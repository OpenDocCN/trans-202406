- en: Chapter 8\. Streaming Data on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you think about data infrastructure, persistence is the first thing that
    comes to mind for many—storing the state of running applications. Accordingly,
    our focus up to this point has been on databases and storage. It’s now time to
    consider the other aspects of the cloud native data stack.
  prefs: []
  type: TYPE_NORMAL
- en: For those of you managing data pipelines, streaming may be your starting point,
    with other parts of your data infrastructure being of secondary concern. Regardless
    of your starting place, data movement is a vitally important part of the overall
    data stack. In this chapter, we’ll examine how to use streaming technologies in
    Kubernetes to share data securely and reliably in your cloud native applications.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra), we defined
    *streaming* as the function of moving data from one point to another and, in some
    cases, processing data in transit. The history of streaming is almost as long
    as that of persistence. As data was pooling in various isolated stores, it became
    evident that moving data reliably was just as important as storing data reliably.
    In those days, it was called *messaging*. Data was transferred slowly but deliberately,
    which resembled something closer to postal mail. Messaging infrastructure put
    data in a place where it could be read asynchronously, in order, with delivery
    guarantees. This met a critical need when using more than one computer and is
    one of the foundations of distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Modern application requirements have evolved from what was known as messaging
    into today’s definition of streaming. Typically, this means managing large volumes
    of data that require more immediate processing, which we call *near real-time*.
    Ordering and delivery guarantees become a critically important feature in the
    distributed applications deployed in Kubernetes and in many cases are a key enabler
    of the scale required. How can adding more infrastructure complexity help scale?
    By providing an orderly way to manage the flow from the creation of data to where
    it can be used and stored. Rarely are streams used as the source of truth, but
    more importantly, they are used as the *conduit* of truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a lot of software and terminology around streaming that can confuse
    first-time users. As with any complex topic, decomposing the parts can be helpful
    as we build understanding. There are three areas to evaluate when choosing a streaming
    system for your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delivery guarantees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature scope for streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a closer look at each of these areas.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use streaming in your application, you will need to understand the delivery
    methods available to you from the long choice list of streaming systems. You will
    need to understand your application requirements to efficiently plan how data
    flows from producer to consumer. For example, “Does my consumer need exclusive
    access?” The answer will drive which system fits the requirements. [Figure 8-1](#delivery_types)
    shows two of the most common choices in streaming systems: point to point and
    publish/subscribe:'
  prefs: []
  type: TYPE_NORMAL
- en: Point to point
  prefs: []
  type: TYPE_NORMAL
- en: In this data flow, data created by the producer is passed through the broker
    and then to a single consumer in a one-to-one relationship. This is primarily
    used as a way to decouple direct connections from producer to consumer. It serves
    as an excellent feature for resilience as consumers can be removed and added with
    no data loss. At the same time, the broker maintains the order and last message
    read, addressable by the consumer using an offset.
  prefs: []
  type: TYPE_NORMAL
- en: Publish/subscribe (pub/sub)
  prefs: []
  type: TYPE_NORMAL
- en: In this delivery method, the broker serves as a distribution hub for a single
    producer and one or more consumers in a one-to-many relationship. Consumers subscribe
    to a topic and receive notifications for any new messages created by the producer—a
    critical component for reactive or event-driven architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Delivery types](assets/mcdk_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Delivery types
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Delivery Guarantees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In conjunction with the delivery types, the broker maintains delivery guarantees
    from producer to consumer per message type in an agreement called a *contract*.
    The typical delivery types are shown in [Figure 8-2](#delivery_guarantees): at-most-once,
    at-least-once, and exactly once. The diagram shows the important relationship
    between when the producer sends a message and the expectation of how the consumer
    receives the message:'
  prefs: []
  type: TYPE_NORMAL
- en: At-most-once
  prefs: []
  type: TYPE_NORMAL
- en: The lowest guarantee is used to avoid any potential data duplication due to
    transient errors that can happen in distributed systems. For example, the producer
    could get a timeout on send. However, the message may have just gone through without
    acknowledgment. In this gray area, the safest choice to avoid duplicate data will
    be for the producer to not attempt a resend and proceed. The critical downside
    to understand is that data loss is possible by design.
  prefs: []
  type: TYPE_NORMAL
- en: At-least-once
  prefs: []
  type: TYPE_NORMAL
- en: This guarantee is the opposite side of at-most-once. Data created by the producer
    is guaranteed to be picked up by a consumer. The added aspect allows for redelivery
    any number of times after the first. For example, this might be used with a unique
    key such as a date stamp or ID number that is considered idempotent on the consumer
    side that multiple processing won’t impact. The consumer will always see data
    delivered by the producer but could see it numerous times. Your application will
    need to account for this possibility.
  prefs: []
  type: TYPE_NORMAL
- en: Exactly once
  prefs: []
  type: TYPE_NORMAL
- en: The strictest of the three guarantees, this means that data created by a producer
    will be delivered only one time to a producer—for example, in exact transactions
    such as money movement, which require subtractions or additions to be delivered
    and processed one time to avoid problems. This guarantee puts a more significant
    burden on the broker to maintain, so you will need to adjust the resources allocated
    to the broker and your expected throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '![Delivery guarantees](assets/mcdk_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Delivery guarantees
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exercise care in selecting delivery guarantees for each type of message. Delivery
    guarantees are ones to carefully evaluate as they can have unexpected downstream
    effects on the consumer if not wholly understood. Questions like “Can my application
    handle duplicate messages?” need a good answer. “Maybe” is not good enough.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Scope
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many streaming technologies are available, some of which have been around for
    quite a few years. On the surface, these technologies may appear similar, but
    each solves a different problem because of new requirements. The majority are
    open source projects, so each has a community of like-minded individuals who join
    in and advance the project. Just as many different persistent data stores fit
    under the large umbrella of “database,” features under the heading of data streaming
    can vary significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature scope is likely the most important selection criterion when evaluating
    which streaming technology to use. Still, you should also challenge yourself to
    add suitability for Kubernetes as a criterion and consider whether more complex
    features are worth the added resource cost. Fortunately, the price for getting
    your decision wrong the first time is relatively low. Streaming data systems tend
    to be some of the easiest to migrate because of their ephemeral nature. The deeper
    into your feature stack the streaming technology goes, the harder it is to move.
    The scope of streaming features can be broken into the two large buckets shown
    in [Figure 8-3](#streaming_types):'
  prefs: []
  type: TYPE_NORMAL
- en: Message broker
  prefs: []
  type: TYPE_NORMAL
- en: This is the simplest form of streaming technology that facilitates the moving
    of data from one point to another with one or more of the delivery methods and
    guarantees listed previously. It’s easy to discount this feature’s simplistic
    appearance, but it’s the backbone of modern cloud native applications. It’s like
    saying FedEx is just a package delivery company, but imagine what would happen
    to the world economy if it stopped for even one day? Example OSS message brokers
    include Apache Kafka, Apache Pulsar, RabbitMQ, and Apache ActiveMQ.
  prefs: []
  type: TYPE_NORMAL
- en: Stream analytics
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the best or only time to analyze data is while it is moving.
    Waiting for data to persist and then begin the analysis could be far too late,
    and the insight’s value is almost useless. Consider fraud detection. The only
    opportunity to stop the fraudulent activity is when it’s happening; waiting for
    a report to run the next day just doesn’t work. Example OSS stream analytics systems
    include the Apache prooducts Spark, Flink, Storm, Kafka Streams, and Pulsar.
  prefs: []
  type: TYPE_NORMAL
- en: '![Streaming types](assets/mcdk_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Streaming types
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Role of Streaming in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered the basic terminology, how does streaming fit into
    a cloud native application running on Kubernetes? Database applications follow
    the pattern of create, read, update and delete (CRUD). For a developer, the database
    provides a single location for data. The addition of streaming assumes some sort
    of motion in the data from one place to another. Data may be short-lived if used
    to create new data. Some data may be transformed in transit, and some may eventually
    be persisted. Streaming assumes a distributed architecture, and the way to scale
    a streaming system is to manage its resource allocation of compute, network, and
    storage. This is landing right into the sweet spot of cloud native architecture.
    In the case of stream-driven applications in Kubernetes, you’re managing the reliable
    flow of data in an environment that can change over time. Allocate what you need
    when you need it.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming and Data Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data engineering is a relatively new and fast-growing discipline, so we want
    to be sure to define it. This is especially applicable to the practice of data
    streaming. Data engineers are concerned with the efficient movement of data in
    complex environments. The two T’s are important in this case: transport and transformation.
    The role of the data scientist is to derive meaning and insights from data. In
    contrast, the data engineer is building the pipeline that collects data from various
    locations, organizes it, and in most cases, persists to something like a data
    lake. Data engineers work with application developers and data scientists to make
    sure application requirements are met in the increasingly distributed nature of
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: The most critical aspect of your speed and agility is how well your tools work
    together. When developers dream up new applications, how fast can that idea turn
    into a production deployment? Deploying and managing separate infrastructure (streaming,
    persistence, microservices) for one application is burdensome and prone to error.
    When asking why you would want to add streaming into your cloud native stack,
    you should consider the cost of not integrating your entire stack in terms of
    technical debt. Creating custom ways of moving data puts a huge burden on application
    and infrastructure teams. Data streaming tools are built for a specific purpose,
    with large communities of users and vendors to aid in your success.
  prefs: []
  type: TYPE_NORMAL
- en: 'For data engineers and site reliability engineers (SREs), your planning and
    implementation of streaming in Kubernetes can greatly impact your organization.
    Cloud native data should allow for more agility and speed while squeezing out
    all the efficiency you can get. As a reader of this book, you are already on your
    way to thinking differently about your infrastructure. Taking some advice from
    Jesse Anderson, there are two areas you should be focusing on as you begin your
    journey into streaming data on Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Resource allocation
  prefs: []
  type: TYPE_NORMAL
- en: Are you planning for peaks as well as the valleys? As you’ll recall from [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra),
    elasticity is one of the more challenging aspects of cloud native data to get
    right. Scaling up is a commonly solved problem in large-scale systems, but scaling
    down can potentially result in data loss, especially with streaming systems. Traffic
    to resources needs to be redirected before they are decommissioned, and any data
    they are managing locally will need to be accounted for in other parts of the
    system. The risk involved with elasticity is what keeps it from being widely used,
    and the result is a lot of unused capacity. Commit yourself to the idea that resources
    should never be idle and build streaming systems that use what they need and no
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Disaster recovery planning
  prefs: []
  type: TYPE_NORMAL
- en: Moving data efficiently is an important problem to solve, but just as important
    is how to manage inevitable failure. Without understanding your data flows and
    durability requirements, you can’t just rely on Kubernetes to handle recovery.
    Disaster recovery is about more than backing up data. How are Pods scheduled so
    that physical server failure has a reduced impact? Can you benefit from geographic
    redundancy? Are you clear on where data is persisted and understand the durability
    of those storage systems? And finally, do you have a clear plan to restore systems
    after a failure? In all cases, writing down the procedure is the first step, but
    testing those procedures is the difference between success and failure.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered the what and why of streaming data on Kubernetes, and it’s time
    we start looking at the how with a particular focus on cloud native deployments.
    We’ll give a quick overview of how to install these technologies on Kubernetes
    and highlight some important details to aid your planning. You’ve already learned
    in previous chapters how to use many of the Kubernetes resources we’ll need, so
    we’ll speed up the pace a bit. Let’s get started on the first cloud native streaming
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming on Kubernetes with Apache Pulsar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Pulsar is an exciting project to watch for cloud native streaming applications.
    Streaming software was mostly built in an era before Kubernetes and cloud native
    architectures. Pulsar was originally developed at Yahoo!, which is no stranger
    to high-scale cloud native workloads. Donated to the Apache Software Foundation,
    it was accepted as a top-level project in 2018\. Additional projects, like Apache
    Kafka or RabbitMQ, may suit your application’s needs, but they will require more
    planning and well-written operators to function at the level of efficiency of
    Pulsar. In terms of the streaming definitions we covered previously, Pulsar supports
    the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Types of delivery: one-to-one and pub/sub'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delivery guarantees: at-least-once, at-most-once, exactly once'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature scope for streaming: message broker, analytics (through functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So what makes Pulsar a good fit for Kubernetes?
  prefs: []
  type: TYPE_NORMAL
- en: We use Kubernetes to create virtual datacenters to efficiently use compute,
    network, and storage. Pulsar was designed from the beginning with a separation
    of compute and storage resource types linked by the network, similar to a microservices
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: These resources can even span multiple Kubernetes clusters or physical datacenters,
    as shown in [Figure 8-4](#apache_pulsar_architecture). Deployment options give
    operators the flexibility to install and scale a running Pulsar cluster based
    on use case and workload. Pulsar was also designed with multitenancy in mind,
    making a big efficiency difference in large deployments. Instead of installing
    a separate Pulsar instance per application, many applications (tenants) can use
    one Pulsar instance with guardrails to prevent resource contention. Finally, built-in
    storage tiering creates automated alternatives for storage persistence as data
    ages, and lower-cost storage can be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: '![Apache Pulsar architecture](assets/mcdk_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Apache Pulsar architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Pulsar’s highest level of abstraction is an instance that consists of one or
    more clusters. We call the local logical administration domain a *cluster* and
    deploy in a Kubernetes cluster, where we’ll concentrate our attention. Clusters
    can share metadata and configuration, allowing producers and consumers to see
    a single system regardless of location. Each cluster is made of several parts
    acting in concert that primarily consume either compute or storage. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Broker (compute)
  prefs: []
  type: TYPE_NORMAL
- en: Producers and consumers pass messages via the broker, a stateless cluster component.
    This means it is purely a compute scaling unit and can be dynamically allocated
    based on the number of tenants and connections. Brokers maintain an HTTP endpoint
    used for client communication, which presents a few options for network traffic
    in a Kubernetes deployment. When multiple clusters are used, the brokers support
    replication between clusters in the instance. Brokers can run in a memory-only
    configuration, or with Apache BookKeeper (labeled as *bookies*) when message durability
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: Apache BookKeeper (storage)
  prefs: []
  type: TYPE_NORMAL
- en: The BookKeeper project provides infrastructure for managing distributed write-ahead
    logs. In Pulsar, the individual instances used are called *bookies*. The storage
    unit is called a *ledger*; each topic can have one or more ledgers. Multiple bookie
    instances provide load-balancing and failure protection. They also offer storage
    tiering functionality, allowing operators to offer a mix of fast and long-term
    storage options based on use case. When brokers interact with bookies, they read
    and write to a topic ledger, an append-only data structure. Bookies provide a
    single reference to the ledger but manage the replication and load balancing behind
    the primary interface. In a Kubernetes environment, knowing where data is stored
    is critical for maintaining resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Apache ZooKeeper (compute)
  prefs: []
  type: TYPE_NORMAL
- en: ZooKeeper is a standalone project used in many distributed systems for coordination,
    leader election, and metadata management. Pulsar uses ZooKeeper for service coordination,
    similar to the way etcd is used in a Kubernetes cluster, storing important metadata
    such as tenants, topics, and cluster configuration state so that the brokers can
    remain stateless. Bookies use ZooKeeper for ledger metadata and coordination between
    multiple storage nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Proxy (network)
  prefs: []
  type: TYPE_NORMAL
- en: The proxy is a solution for dynamic environments like Kubernetes. Instead of
    exposing every broker to HTTP traffic, the proxy serves as a gateway and creates
    an Ingress route to the Pulsar cluster. As brokers are added and removed, the
    proxy uses service discovery to keep the connections flowing to and from the cluster.
    When using Pulsar in Kubernetes, the proxy service IP should be the single access
    for your applications to a running Pulsar cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Functions (compute)
  prefs: []
  type: TYPE_NORMAL
- en: Since Pulsar Functions operate independently and consume their own compute resources,
    we chose not to include them in [Figure 8-4](#apache_pulsar_architecture). However,
    they’re worth mentioning in this context because Pulsar Functions work in conjunction
    with the message broker. When deployed, they take data from a topic, alter it
    with user code, and return it to a different topic. The component added to a Pulsar
    cluster is the worker, which accepts function runtimes on an ad hoc basis. Operators
    can deploy Functions as a part of a larger cluster or standalone for more fine-grained
    resource management.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Your Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When preparing to do your first installation, you need to make some choices.
    Since every user will have unique needs, we recommend you check the [official
    documentation](https://oreil.ly/KCqT2) for the most complete and up-to-date information
    on installing Pulsar in Kubernetes before reading this section. The examples within
    this section will take a closer look at the choices available and how they pertain
    to different cloud native application use cases to help inform your decision making.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, create a local clone directory of the Pulsar Helm chart repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This subproject of Pulsar is well documented, with several helpful examples
    to follow. When using Helm to deploy Pulsar, you will need a *values.yaml* file
    that contains all of the options to customize your deployment. You can include
    as many parameters as you want to change. The Pulsar Helm chart has a complete
    set of defaults for a typical cluster that might work for you, but you will want
    to tune the values for your specific environment. The *examples* directory has
    various deployment scenarios. If you choose the default installation as described
    in the *values-local-cluster.yaml* file, you’ll have a set of resources like that
    shown in [Figure 8-5](#a_simple_pulsar_installation_on_kuberne). As you can see,
    the installation wraps the proxy and brokers in Deployments and presents a unified
    service endpoint for applications.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity is a mechanism built into Kubernetes to create rules for which Pods
    can and cannot be colocated on the same physical node (if needed, refer to the
    more in-depth discussion in [Chapter 4](ch04.html#automating_database_deployment_on_kuber)).
    Pulsar, being a distributed system, has deployment requirements for maximum resilience.
    An example is brokers. When multiple brokers are deployed, each Pod should run
    on a different physical node in case of failure. If all broker Pods were grouped
    on the same node and the node went down, the Pulsar cluster would be unavailable.
    Kubernetes would still recover the runtime state and restart the Pods. However,
    there would be downtime as they came back online.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Simple Pulsar installation on Kubernetes](assets/mcdk_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. A Simple Pulsar installation on Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The easiest thing is not allowing Pods of the same type to group together onto
    the same nodes. When enabled, anti-affinity will keep this from happening. If
    you are running on a single-node system such as a desktop, disabling it will allow
    your cluster to start without blocking based on affinity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Fine-grained control over Pulsar component replica counts lets you tailor your
    deployment based on the use case. Each replica Pod consumes resources and should
    be considered in the application’s lifecycle. For example, starting with a low
    number of brokers and BookKeeper Pods can manage some level of traffic. Still,
    more replicas can be added and configuration updated via Helm as traffic increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You now have a foundational understanding of how to reliably move data to and
    from applications and outside of your Kubernetes cluster. Pulsar is a great fit
    for cloud native application deployments because it can scale compute and storage
    independently. The declarative nature of deployments makes it easy for data engineers
    and SREs to deploy easily with consistency. Now that we have the means for data
    communication, let’s take it a step further with the right kind of network security.
  prefs: []
  type: TYPE_NORMAL
- en: Securing Communications by Default with cert-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An unfortunate reality we face at the end of product development is what gets
    left to complete: security or documentation. Unfortunately, Kubernetes doesn’t
    have much in the way of building documentation, but when it comes to security,
    there has been some great progress on starting earlier without compromise!'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, installing Pulsar has created a lot of infrastructure and communication
    between the elements. High traffic volume is a typical situation. When we build
    out virtual datacenters in Kubernetes, it will create a lot of [internode](https://oreil.ly/YySn7)
    and external network traffic. All traffic should be encrypted with Transport Layer
    Security (TLS) and Secure Socket Layer (SSL) using [X.509 certificates](https://oreil.ly/JG794).
    The most important part of this system is the certificate authority (CA). In a
    public key infrastructure (PKI) arrangement acts as a trusted third party that
    digitally signs certificates used to create a chain of trust between two entities.
    Going through the procedure to have a certificate issued by a CA historically
    has been a manual and arduous process, which unfortunately has led to a lack of
    secure communications in cloud-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*cert-manager* is a tool that uses the Automated Certificate Management Environment
    (ACME) protocol to add certificate management seamlessly to your Kubernetes infrastructure.
    We should always use TLS to secure the data moving from one service to another
    for our streaming application. The cert-manager project is arguably one of the
    most critical pieces of your Kubernetes infrastructure that you will eventually
    forget about. That’s the hallmark of a project that fits the moniker of “it just
    works.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding TLS to your Pulsar deployment has been made incredibly easy with just
    a few configuration steps. Before installing Pulsar, you’ll need to set up the
    cert-manager service inside the target Kubernetes cluster. First, add the cert-manager
    repo to your local Helm installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What Is ACME?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with X.509 certificates, you’ll frequently see references to the
    Automated Certificate Management Environment (ACME). ACME allows for automated
    deployment of certificates between user infrastructure and certificate authorities.
    It was designed by the Internet Security Research Group when it was building its
    free certificate authority, Let’s Encrypt. It would be putting it lightly to say
    this fantastic free service has been a game-changer for cloud native infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation process takes a few parameters, which you should make sure
    to use. First is declaring a separate Namespace to keep the cert-manager neatly
    organized in your virtual datacenter. The second is installing the CRD assets.
    This combination allows you to create services that automate your certificate
    management:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After the cert-manager is installed, you’ll then need to configure the certificate
    issuer that will be called when new certificates are needed. You have many options
    based on the environment you are operating in, and these are covered quite extensively
    in the documentation. One of the custom resources created when installing cert-manager
    is `Issuer`. The most basic `Issuer` is the `selfsigned-issuer` that can create
    a certificate with a user-supplied private key. You can create a basic `Issuer`
    by applying the following YAML configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When installing Pulsar with Helm, you can secure inter-service communication
    with a few lines of YAML configuration. You can pick which services are secured
    by setting the TLS `enabled` to `true` or `false` for each service in the YAML
    that defines your Pulsar cluster. The examples provided by the project are quite
    large, so for brevity, we’ll look at some key areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can secure the entire cluster with just one command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Later in your configuration file, you can use self-signing certificates to
    create TLS connections between components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you have been involved in securing infrastructure communication any time
    in the past, you know the toil in working through all the steps and applying TLS.
    Inside a Kubernetes virtual datacenter, you no longer have an excuse to leave
    network communication unencrypted. With a few lines of configuration, everything
    is secured and maintained.
  prefs: []
  type: TYPE_NORMAL
- en: cert-manager should be one of the first things you install in a new Kubernetes
    cluster. The combination of project maturity and simplicity makes security the
    easy first thing to add to your project instead of the last. This is true not
    only for Pulsar but for every service you deploy in Kubernetes that requires network
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: Using Helm to Deploy Apache Pulsar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have covered how to design a Pulsar cluster to maximize resources,
    you can use Helm to carry out the deployment into Kubernetes. First, add the Pulsar
    Helm repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the special requirements for a Helm install of Pulsar is preparing Kubernetes.
    The Git repository you cloned earlier has a script that will run through all the
    preparations, such as creating the destination Namespace. The more complicated
    setup is the roles with associated keys and tokens. These are important for inter-service
    communication inside the Pulsar cluster. From the docs, you can invoke the prep
    script by using this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Kubernetes cluster has been prepared for Pulsar, the final installation
    can be run. At this point, you should have a YAML configuration file with the
    settings you need for your Pulsar use case as we described earlier. The `helm
    install` command will take that config file and direct Kubernetes to meet the
    desired state you have specified. When creating a new cluster, use `initalize=true`
    to create the base metadata configuration in ZooKeeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In a typical production deployment, you should expect the setup time to take
    10 minutes or more. There are a lot of dependencies to walk through as ZooKeeper,
    bookies, brokers, and finally, proxies are brought online and in order.
  prefs: []
  type: TYPE_NORMAL
- en: Stream Analytics with Apache Flink
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let’s look at a different type of streaming project that is quickly gaining
    popularity in cloud native deployments: Apache Flink. Flink is a system primarily
    designed to focus on stream analytics at an incredible scale. As we discussed
    at the beginning of the chapter, streaming systems come in many flavors, and this
    is a perfect example. Flink has its competencies that overlap very little with
    other systems; in fact, it’s widespread to see Pulsar and Flink deployed together
    to complement each other’s strengths in a cloud native application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a streaming system, the following are available in Flink:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type of delivery: one-to-one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delivery guarantee: exactly once'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature scope for streaming: analytics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two main components of the Flink architecture are shown in [Figure 8-6](#apache_flink_architecture)—the
    JobManager and TaskManager:'
  prefs: []
  type: TYPE_NORMAL
- en: JobManager
  prefs: []
  type: TYPE_NORMAL
- en: This is the control plane for any running Flink application code deployed. A
    JobManager consumes CPU resources but only to maintain Job control; no actual
    processing is done on the JobManager. In high availability (HA) mode, which is
    exclusive to Flink running on Kubernetes, multiple standby JobManagers will be
    provisioned but remain idle until the primary is no longer available.
  prefs: []
  type: TYPE_NORMAL
- en: TaskManager
  prefs: []
  type: TYPE_NORMAL
- en: This is where the work gets done on a running Flink job. The JobManger uses
    TaskManagers to satisfy the chain of tasks needed in the application. A chain
    is the order of operation. In some cases, these operations can be run in parallel,
    and some need to be run in series. The TaskManger will run only one discrete task
    and pass it on. Resource management can be controlled through the number of TaskManagers
    in a cluster and execution slots per TaskManager. The current guidance says that
    you should allocate one CPU to each TaskManager or slot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Apache Flink architecture](assets/mcdk_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Apache Flink architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Flink project is designed for managing stateful computations, which should
    cause you to immediately think of storage requirements. Every transaction in Flink
    is guaranteed to be strongly consistent with no single point of failure. These
    are the features you need when you are trying to build the kind of highly scalable
    systems that Flink was designed to accomplish. There are two types of streaming,
    bounded and unbounded:'
  prefs: []
  type: TYPE_NORMAL
- en: Unbounded streaming
  prefs: []
  type: TYPE_NORMAL
- en: These streaming systems react to new data whenever the data arrives—there is
    no endpoint where you can stop and analyze the data gathered. Every piece of data
    received is independent. The use cases for this can be alerting on values or counting
    when exactness is essential. Reactive processing can be very resource-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Bounded streaming
  prefs: []
  type: TYPE_NORMAL
- en: This is also known as *batch processing* in other systems but is a specific
    case within Flink. Bounded windows can be marked by time or specific values. In
    the case of time windows, they can also slide forward, giving the ability to do
    rolling updates on values. Resource considerations should be given based on the
    data window size to be processed. The limit of the boundary size is constrained
    mainly by memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the foundational tenets of Flink is a strong focus on operations. At
    the scale required for cloud native applications, easy to use and deploy can be
    the difference between using it or not. This includes core support for continuous
    deployment workloads in Kubernetes and feature parity with cloud native applications
    in the areas of reliability and observability:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous deployment
  prefs: []
  type: TYPE_NORMAL
- en: The core unit of work for Flink is called a *job*. Jobs are Java or Scala programs
    that define how the data is read, analyzed, and output. Jobs are chained together
    and compiled into a JAR file to create a Flink application. Flink provides a Docker
    image that encapsulates the application in a form that makes deployment on Kubernetes
    an easy task and facilitates continuous deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs: []
  type: TYPE_NORMAL
- en: Flink also has built-in support for savepoints, which makes updates easier by
    pausing and resuming jobs before and after system updates. Savepoints can also
    be used for fast recovery if a processing Pod fails mid-job. Tighter integration
    with Kubernetes allows Flink to self-heal on failure by restoring Pods and restarting
    Jobs with savepoints.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs: []
  type: TYPE_NORMAL
- en: Cluster metrics are instrumented to output in Prometheus format. Operations
    teams can keep track of lifecycle events inside the Flink cluster with time-based
    details. Application developers can expose custom metrics using the [Flink metric
    system](https://oreil.ly/0x0IS) for further integrated observability.
  prefs: []
  type: TYPE_NORMAL
- en: Flink provides a way for data teams to participate in the overall cloud native
    stack while giving operators everything needed to manage the entire deployment.
    Application developers building microservices can share a CI/CD pipeline with
    developers building the stream analytics of data generated from the application.
    As changes occur in any part of the stack, they can be integration tested entirely
    and deployed as a single unit. Teams can move faster with more confidence knowing
    there aren’t disconnected requirements that may show up in production. This sort
    of outcome is a solid argument to employ cloud native methodologies in your entire
    stack, so it’s time to see how this is done.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Apache Flink on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deploying a Flink cluster into a running Kubernetes cluster, there are
    a few things to consider. The Flink project has gone the route of offering what
    it calls “Kubernetes Native,” which programmatically installs the required Flink
    components without `kubectl` or Helm. These choices may change in the future.
    Side projects in the Flink ecosystem already bring a more typical experience that
    Kubernetes operators might expect, including operators and Helm charts. For now,
    we will discuss the official method endorsed by the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Figure 8-7](#deploying_flink_on_kubernetes), a running Flink cluster
    has two main components we’ll deploy in Pods: the *JobManager* and *TaskManager*.
    These are the basic units, but choosing which deployment mode is the critical
    consideration for your use case. They dictate how compute and network resources
    are utilized. Another thing of note is how to deploy on Kubernetes. As mentioned
    before, there are no official project operators or Helm charts. The Flink [distribution](https://flink.apache.org/downloads.html)
    contains command-line tools that will deploy into a running Kubernetes cluster
    based on the mode for your application.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Flink on Kubernetes](assets/mcdk_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Deploying Flink on Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 8-8](#apache_flink_modes) shows the modes available for deploying Flink
    clusters in Kubernetes: Application Mode and Session Mode. Flink also supports
    a third mode called Per-Job Mode, but this is not available for Kubernetes deployments,
    which leaves us with Application Mode and Session Mode.'
  prefs: []
  type: TYPE_NORMAL
- en: The selection of either Application Mode or Session Mode comes down to resource
    management inside your Kubernetes cluster, so let’s look at both to make an informed
    decision.
  prefs: []
  type: TYPE_NORMAL
- en: '*Application Mode* isolates each Flink application into its own cluster. As
    a reminder, a Flink application JAR can consist of multiple jobs chained together.
    The startup cost of the cluster can be minimized with a single application initialization
    and Job graph. Once deployed, resources are consumed for client traffic and execution
    of the jobs in the application. Network traffic is much more efficient since there
    is only one JobManager and client traffic can be multiplexed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apache Flink Modes](assets/mcdk_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Apache Flink modes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To start in Application Mode, you invoke the `flink` command with the target
    of `kubernetes-application`. You will need the name of the running Kubernetes
    cluster accessible via `kubectl`. The application to be run is contained in a
    Docker image, and the path to the JAR file supplied in the command line. Once
    started, the Flink cluster is created, application code is initialized, and will
    then be ready for client connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Session Mode* changes resource management by creating a single Flink cluster
    that can accept any number of applications on an ad hoc basis. Instead of having
    multiple independent clusters running and consuming resources, you may find it
    more efficient to have a single cluster that can grow and shrink when new applications
    are submitted. The downside for operators is that you now have a single cluster
    that will take several applications with it if it fails. Kubernetes will restart
    the downed Pods, but you will have a recovery time to manage as resources are
    reallocated. To start in Session Mode, use the `kubernetes-session` shell file
    and give it the name of your running Kubernetes cluster. The default is for the
    command to execute and detach from the cluster. To reattach or remain in an interactive
    mode with the running cluster, use the `execution.attached=true` switch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This was a quick fly-by of a massive topic, but hopefully, it inspires you to
    look further. One resource we recommend is [*Stream Processing with Apache Flink*](https://oreil.ly/Iocv6)
    by Fabian Hueske and Vasiliki Kalavri (O’Reilly). Adding Flink to your application
    isn’t just about choosing a platform to perform stream processing. In cloud native
    applications, we should be thinking holistically about the entire application
    stack we are attempting to deploy in Kubernetes. Flink uses containers, as encapsulation
    lends itself to working with other development workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have branched out from persistence-oriented data infrastructure
    into the world of streaming. We defined what streaming is, how to navigate all
    the terminology, and how it fits into Kubernetes. From there, we took a deeper
    look into Apache Pulsar and learned how to deploy it into your Kubernetes cluster
    according to your environment and application needs. As a part of deploying streaming,
    we took a side trip into default secure communications with cert-manager to see
    how it works and how to create self-managed encrypted communication. Finally,
    we looked at Kubernetes deployments of Apache Flink, which is used primarily for
    high-scale stream analytics.
  prefs: []
  type: TYPE_NORMAL
- en: As you saw in this chapter with Pulsar and cert-manager, running cloud native
    data infrastructure on Kubernetes frequently involves the composition of multiple
    components as part of an integrated stack. We’ll discuss more examples of this
    in the next chapter and beyond.
  prefs: []
  type: TYPE_NORMAL
