- en: Chapter 5\. Ingress and Linkerd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whenever you work with Kubernetes, you always have to find a way for your users
    *outside* your cluster to be able to make requests of (some of) the services running
    *inside* your cluster. This is the *ingress problem* (see [Figure 5-1](#ingress-problem)):
    the cluster wants to protect everything inside from the big scary Internet, but
    that’s where your legitimate users are.'
  prefs: []
  type: TYPE_NORMAL
- en: '![luar 0501](assets/luar_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. The ingress problem
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s an entire class of applications out there, unsurprisingly called *ingress
    controllers*, whose sole purpose is solving the ingress problem. Linkerd does
    not include an ingress controller; instead, it allows you to mesh whatever ingress
    controller you like, as long as certain rules are followed. In this chapter, you’ll
    learn how to make Linkerd and the ingress controller of your choice play nicely
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of different ingress controllers, which approach the ingress
    problem in fascinatingly different ways. However, there are some common threads
    across all of them, shown in [Figure 5-2](#ingress-controller-architecture).
  prefs: []
  type: TYPE_NORMAL
- en: '![luar 0502](assets/luar_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Ingress controller high-level architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These common threads include:'
  prefs: []
  type: TYPE_NORMAL
- en: They are all designed to live right at the edge of a cluster (usually behind
    a Kubernetes Service of type `LoadBalancer`), exposed directly to the Internet
    so that their clients can reach them. Security is always a major concern for an
    ingress controller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They always have a way to control which requests from outside get routed to
    which services inside. This is another critical security issue: installing an
    ingress controller cannot mean that all the services in your cluster are open
    to the Internet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the popular ingress controllers support sophisticated routing controls
    at [OSI layer 7](https://oreil.ly/S-sjB), typically focusing on HTTP and gRPC.
    Many also support more limited control for routing OSI layer 4 connections:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At OSI layer 7 (the application layer), the ingress controller might have capabilities
    like “route an HTTP request where the hostname is `foo.example.com` and the `path`
    starts with `/bar/` to the Service named `bar-service`.”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At OSI layer 4 (the transport layer), its capabilities are more likely to be
    along the lines of “route TCP connections arriving on port 1234 to the Service
    named `bar-service`.”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on which ingress controller is in use, the actual way the user configures
    routing can vary significantly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Ingress controllers can always terminate and originate TLS connections (again,
    mostly focusing on HTTPS) to handle security at the edge of the cluster. This
    doesn’t extend Linkerd’s mTLS out to the ingress controller’s clients; rather,
    it creates two separate domains in which TLS is operating and requires the ingress
    controller to translate between them, as shown in [Figure 5-3](#ingress-controller-tls).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![luar 0503](assets/luar_0503.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5-3\. Ingress controllers and TLS
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Keeping the two TLS worlds separate usually ends up making sense because the
    ingress controller needs to be presenting users with certificates that match what
    the user is expecting, but when its proxy interacts with Linkerd, it needs to
    present a properly crafted workload identity. These are not the same thing and
    shouldn’t be conflated. Allowing the ingress controller to manage TLS with its
    client while allowing Linkerd to manage mTLS within the cluster is a powerful
    combination.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, many ingress controllers offer capabilities like end user authentication,
    circuit breaking, rate limiting, etc. These ingress controllers may also be called
    *API gateways*. An example of how one might handle end user authentication is
    shown in [Figure 5-4](#api-gateway-extauth).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![luar 0504](assets/luar_0504.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5-4\. An API gateway providing end user authentication
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: API gateways have enormous latitude over what exactly happens to a user request,
    allowing very sophisticated capabilities indeed—though this is obviously out of
    scope for this book.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Ingress Controllers with Linkerd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linkerd doesn’t have a lot of constraints in terms of which ingress controller
    you use; almost any of them will work, usually without much trouble. From Linkerd’s
    point of view, the ingress is just another meshed workload, and from the ingress
    controller’s point of view, Linkerd is mostly invisible.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Controllers in Other Meshes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some meshes take a very different approach here: they ship with an ingress
    controller that is tightly integrated with the mesh. Linkerd takes a very ingress-agnostic
    approach because it tends to increase flexibility, lessen operational complexity,
    and make it easier to adopt the ingress controller and the service mesh at different
    times.'
  prefs: []
  type: TYPE_NORMAL
- en: The Ingress Controller Is Just Another Meshed Workload
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From Linkerd’s point of view, the ingress controller is mostly just a workload
    in the mesh, as shown in [Figure 5-5](#ingress-is-just-a-workload). The fact that
    clients outside the cluster can talk to the ingress controller is really not something
    that Linkerd worries about: you still need to inject a sidecar into the ingress
    controller, and all the usual Linkerd features like mTLS and metrics just work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![luar 0505](assets/luar_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. The ingress controller is just another workload
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The single way that the ingress controller will almost always need special handling
    is that you’ll almost always want to tell Linkerd to skip the ingress controller’s
    incoming ports. This is because the ingress controller may need access to the
    client’s IP address for routing or authorization purposes, but if Linkerd is handling
    the connection, then the only IP address the ingress controller will ever see
    is that of the Linkerd proxy. See [Figure 5-6](#ingress-skip-incoming).
  prefs: []
  type: TYPE_NORMAL
- en: '![luar 0506](assets/luar_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Skipping incoming traffic for the ingress controller
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ingress Controllers Are Designed for the Edge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember that part of the job of an ingress controller is to sit at the edge
    of the cluster, so it already has to be designed to safely handle connections
    directly from the Internet. Telling Linkerd not to handle the incoming connections
    for the ingress controller shouldn’t be any problem from a security point of view.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll use the `config.linkerd.io/skip-inbound-ports` annotation that we covered
    in [Chapter 4](ch04.html#LUAR_meshing_workloads) to skip the incoming ports. Pay
    attention to the port numbers—you need to skip the port(s) on which the ingress
    controller Pod is actually listening, which will often *not* be the port that
    the client uses! For example, if you associate your ingress controller with a
    Service like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll need to skip inbound port 8080; trying to skip inbound port 80 would
    have no effect whatsoever. So, the correct annotation would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Linkerd Is (Mostly) Invisible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the point of view of the ingress controller, Linkerd is basically invisible.
    This is by design: adding Linkerd to a running application is meant to just work,
    after all! But there are two things to be aware of to make sure that everything
    is working as smoothly as possible: the ingress controller should use cleartext
    within the cluster, and it should route to Services rather than endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: Use Cleartext Within the Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We know: this is probably the only time in years you’ve seen anyone recommend
    using cleartext instead of TLS. To be clear, we’re *not* talking about the connection
    from the client to the ingress controller! (Definitely use HTTPS for that.) Here,
    we’re talking about the connections made from the ingress controller to meshed
    workloads in the cluster, as shown in [Figure 5-7](#use-cleartext-inside-the-cluster).'
  prefs: []
  type: TYPE_NORMAL
- en: '![luar 0507](assets/luar_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Let Linkerd handle mTLS inside the cluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For those connections, you should use cleartext. If the ingress controller originates
    TLS to the workload, Linkerd can’t do anything more than per-connection proxying;
    you miss out on per-request load balancing, proper request metrics, and a lot
    of other really useful things that Linkerd brings to the table. Using cleartext
    connections allows all the advanced functionality and is still safe because Linkerd’s
    mTLS will protect the connection.
  prefs: []
  type: TYPE_NORMAL
- en: Route to Services, Not Endpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is an area where Kubernetes nomenclature and concepts are particularly
    challenging. A Kubernetes Service actually has three entirely distinct parts,
    and all three are relevant for this point:'
  prefs: []
  type: TYPE_NORMAL
- en: The Service causes a name to appear in the cluster’s DNS service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That DNS name is associated with a single IP address for the Service itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Service is also associated with a set of Pods, and each Pod has an IP address
    that is different from every other Pod’s *and* from the Service’s IP address.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collectively, the IP addresses of the Pods are called the *endpoints* of the
    Service. (Kubernetes also has resources called Endpoints and EndpointSlices, but
    we’re just talking about the set of Pod IP addresses for the moment.)
  prefs: []
  type: TYPE_NORMAL
- en: These parts are shown in [Figure 5-8](#k8s-service-architecture-2). Again, all
    three are relevant when considering service mesh routing.
  prefs: []
  type: TYPE_NORMAL
- en: '![luar 0508](assets/luar_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. The three distinct parts of a Kubernetes Service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It matters which IP address the ingress controller uses for its connections
    because normally Linkerd will only load balance connections made to the Service’s
    IP address, *not* connections made directly to an endpoint’s IP address, as shown
    in [Figure 5-9](#linkerd-mesh-routing).
  prefs: []
  type: TYPE_NORMAL
- en: '![luar 0509](assets/luar_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. How Linkerd chooses where to route
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Linkerd handles routing this way to maximize choice for the application designer:
    it’s easy to have the ingress controller simply hand off all load balancing decisions
    to Linkerd (by routing to the Service IP), and it’s still possible to have the
    ingress controller do its own load balancing (by routing directly to endpoint
    IPs).'
  prefs: []
  type: TYPE_NORMAL
- en: In most common cases, having the ingress controller route to the Service IP
    is the simplest way to take full advantage of Linkerd.
  prefs: []
  type: TYPE_NORMAL
- en: Gateway API and Service Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gateway API introduces a wrinkle into this recommendation: it will need to
    support cloud gateway controllers that aren’t really running in the cluster and
    therefore can’t have a Linkerd proxy running next to them. At the same time, these
    cloud gateway controllers can be extremely latency-sensitive, so they’re less
    likely to support Service routing.'
  prefs: []
  type: TYPE_NORMAL
- en: This is an area of active work within the GAMMA initiative and Gateway API as
    a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on which ingress controller you’re using, you might need to specifically
    configure the ingress controller to do this—or you might find that it is not possible
    to configure your ingress controller to route to the Service IP. For these ingress
    controllers, you’ll need to use Linkerd’s *ingress mode*.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When ingress mode is active and Linkerd receives a request to an endpoint IP
    with the `l5d-dst-override` header set to a fully qualified Service DNS name,
    Linkerd will route the request as if it had gone to the Service IP address for
    the service named by the `l5d-dst-override` header, as shown in [Figure 5-10](#ingress-mode-routing).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, given a Service `my-service` in namespace `my-ns`, if you send
    a request directly to one of the endpoint IPs for `my-service` but set its `l5d-dst-override`
    header as shown here, then Linkerd will treat the connection as if it had been
    made to the Service IP for `my-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![luar 0510](assets/luar_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. Linkerd ingress mode routing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Ingress Controller Must Inject l5d-dst-override
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively use ingress mode, the ingress controller must inject the `l5d-dst-override`
    header into every request. An ingress controller that cannot inject this header
    is not compatible with Linkerd ingress mode. Linkerd cannot create the `l5d-dst-override`
    header itself, because in general, it’s not possible to determine the name of
    a Service from one of its endpoint IP addresses. This is because a given Pod can
    be part of multiple Services.
  prefs: []
  type: TYPE_NORMAL
- en: If possible, it’s usually better to configure the ingress controller to route
    to Services than to use ingress mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use ingress mode, inject the proxy with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'rather than:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Specific Ingress Controller Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are some specific examples of configuring different ingress controllers
    for use with Linkerd. This is *not* an exhaustive list by any means—it’s just
    a convenient set to show a fairly wide range of possibilities. The [Linkerd ingress
    documentation](https://oreil.ly/Nl7MR) has more on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: For our examples here, we’ll take a look at Emissary-ingress, NGINX, and Envoy
    Gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Emissary-ingress
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Emissary-ingress](https://oreil.ly/vHmjZ) is an open source, Kubernetes-native
    API gateway that’s been around since 2017\. It’s built on the Envoy proxy, focuses
    on operational simplicity and self-service configuration, and has been a CNCF
    incubating project since 2021\. It defines its own native configuration CRDs but
    can also use Ingress resources or Gateway API. (Full disclosure: Flynn is the
    original author of Emissary.)'
  prefs: []
  type: TYPE_NORMAL
- en: There’s really not too much to dig into as far as setting up Emissary with Linkerd;
    it basically just works. Emissary defaults to routing to Services, so really the
    only thing to consider when adding Emissary to the Linkerd mesh is to skip Emissary’s
    incoming ports if you need Emissary to know client IP addresses. And you’ll want
    to make sure that Emissary isn’t originating TLS to the workloads.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NGINX is an open source API gateway and web server that was around long before
    Kubernetes came along. Though it’s not a CNCF project itself, it served as the
    core of the [`ingress-nginx` Kubernetes ingress controller](https://oreil.ly/m-O2N),
    which was one of the first ingress controllers using the Ingress resource, and
    it has been sufficiently popular for long enough that people generally mean `ingress-nginx`
    when they talk about running NGINX for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `ingress-nginx` will route to endpoint IPs, not Service IPs. To
    tell it to route to Service IPs instead, you’ll need to include an `ingress-nginx`
    annotation on your Ingress resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Installing and meshing `ingress-nginx` after that should be painless. Remember
    to look at skipping incoming ports, too!
  prefs: []
  type: TYPE_NORMAL
- en: Envoy Gateway
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As of this writing, Envoy Gateway has recently reached version 1.0\. It provides
    an interesting opportunity to explore using Gateway API to manage both the ingress
    and the mesh in a Linkerd cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Gateway API has the interesting characteristic that, by design, the user doesn’t
    directly install the Pods that handle data (the data plane). Instead, the user
    installs a Gateway API control plane that understands how to watch Gateway resources.
    Then, when the user creates the Gateway, the Gateway API control plane creates
    the data plane Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy Gateway, as a Gateway API control plane, interprets this design characteristic
    to mean that whenever it sees a change to its Gateway resource, it actually deletes
    and re-creates the data plane Pods. This makes it a touch challenging to manage
    injecting the Envoy Gateway data plane into the mesh! The most effective way to
    handle this is to apply the `linkerd.io/inject` annotation to the `envoy-gateway-system`
    namespace, which is where the data plane Deployment will be created.
  prefs: []
  type: TYPE_NORMAL
- en: Also, Envoy Gateway always routes to endpoint IP addresses in version 1.0\.
    Until this is resolved in a future release of Envoy Gateway, it limits Linkerd’s
    ability to do advanced routing when using Envoy Gateway. (It’s possible to mesh
    Envoy Gateway in ingress mode and then configure HTTPRoutes to inject the `l5d-dst-override`
    header, but it’s a bit manual at present.)
  prefs: []
  type: TYPE_NORMAL
- en: Since Linkerd always gets to manage security though (including encryption and
    policy), Envoy Gateway with Linkerd is still a practical and interesting combination.
    Just pay attention to the incoming ports, as with the other ingress controllers!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of Linkerd’s strengths is its ability to work with a wide variety of ingress
    controllers. As long as a given ingress controller can accept the Linkerd sidecar
    and route to Services, it should work seamlessly with Linkerd. This leaves you
    free to choose whatever ingress controller works well for your team and your application
    and be confident that it’ll get along with Linkerd.
  prefs: []
  type: TYPE_NORMAL
