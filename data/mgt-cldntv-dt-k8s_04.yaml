- en: Chapter 3\. Databases on Kubernetes the Hard Way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra),
    Kubernetes was designed for stateless workloads. A corollary to this is that stateless
    workloads are what Kubernetes does best. Because of this, some have argued that
    you shouldn’t try to run stateful workloads on Kubernetes, and you may hear various
    recommendations about what you should do instead: “Use a managed service,” or
    “Leave data in legacy databases in your on-premises datacenter,” or perhaps even
    “Run your databases in the cloud, but in traditional VMs instead of containers.”'
  prefs: []
  type: TYPE_NORMAL
- en: While these recommendations are still viable options, one of our main goals
    in this book is to demonstrate that running data infrastructure in Kubernetes
    has become not only a viable option, but a preferred option. In his article [“A
    Case for Databases on Kubernetes from a Former Skeptic”](https://oreil.ly/SjQV0),
    Christopher Bradford describes his journey from being skeptical of running any
    stateful workload in Kubernetes, to grudging acceptance of running data infrastructure
    on Kubernetes for development and test workloads, to enthusiastic evangelism around
    deploying databases on Kubernetes in production. This journey is typical of many
    in the Data on Kubernetes Community (DoKC). By the middle of 2020, Boris Kurktchiev
    was able to cite a growing consensus that managing stateful workloads on Kubernetes
    had reached a point of viability, and even maturity, in his article [“3 Reasons
    to Bring Stateful Applications to Kubernetes”](https://oreil.ly/xtm89).
  prefs: []
  type: TYPE_NORMAL
- en: 'How did this change come about? Over the past several years, the Kubernetes
    community has shifted focus toward adding features that support the ability to
    manage state in a cloud native way on Kubernetes. The storage elements represent
    a big part of this shift we introduced in the previous chapter, including the
    Kubernetes PersistentVolume subsystem and the adoption of the CSI. In this chapter,
    we’ll complete this part of the story by looking at Kubernetes resources for building
    stateful applications on top of this storage foundation. We’ll focus in particular
    on a specific type of stateful application: data infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: The Hard Way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The phrase “doing it the hard way” has come to be associated with avoiding the
    easy option in favor of putting in the detailed work required to accomplish a
    result that will have lasting significance. Throughout history, pioneers of all
    persuasions are well known for taking pride in having made the sacrifice of blood,
    sweat, and tears that made life just that little bit more bearable for the generations
    that follow. These elders are often heard to lament when their protégés fail to
    comprehend the depth of what they had to go through.
  prefs: []
  type: TYPE_NORMAL
- en: In the tech world, it’s no different. While new innovations such as APIs and
    “no code” environments have massive potential to grow a new crop of developers
    worldwide, a deeper understanding of the underlying technology is still required
    in order to manage highly available and secure systems at worldwide scale. It’s
    when things go wrong that this detailed knowledge proves its worth. This is why
    many of us who are software developers and never touch a physical server in our
    day jobs gain so much from building our own PC by wiring chips and boards by hand.
    It’s also one of the hidden benefits of serving as informal IT consultants for
    our friends and family.
  prefs: []
  type: TYPE_NORMAL
- en: For the Kubernetes community, of course, “the hard way” has an even more specific
    connotation. Google engineer Kelsey Hightower’s [“Kubernetes the Hard Way”](https://oreil.ly/xd6ne)
    has become a sort of rite of passage for those who want a deeper understanding
    of the elements that make up a Kubernetes cluster. This popular tutorial walks
    you through downloading, installing, and configuring each of the components that
    make up the Kubernetes control plane. The result is a working Kubernetes cluster
    that, although not suitable for deploying a production workload, is certainly
    functional enough for development and learning. The appeal of the approach is
    that all of the instructions are typed by hand. Rather than downloading a bunch
    of scripts that do everything for you, you must understand what is happening at
    each step.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll emulate this approach and walk you through deploying
    some example data infrastructure the hard way ourselves. Along the way, you’ll
    get more hands-on experience with the storage resources you learned about in [Chapter 2](ch02.html#managing_data_storage_on_kubernetes),
    and we’ll introduce additional Kubernetes resource types for managing compute
    and network to complete the compute, network, storage triad we introduced in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra).
    Are you ready to get your hands dirty? Let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: Examples Are Not Production-Grade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The examples we present in this chapter are primarily for introducing new elements
    of the Kubernetes API and are not intended to represent deployments we’d recommend
    running in production. We’ll make sure to highlight any gaps so that we can demonstrate
    how to fill them in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for Running Data Infrastructure on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow along with the examples in this chapter, you’ll want to have a Kubernetes
    cluster to work on. If you’ve never tried it before, perhaps you’ll want to build
    a cluster using the [“Kubernetes the Hard Way”](https://oreil.ly/sLopS) instructions,
    and then use that same cluster to add data infrastructure the hard way as well.
    You could also use a simple desktop Kubernetes, since we won’t be using a large
    amount of resources. If you’re using a shared cluster, you might want to install
    these examples in their own Namespace to isolate them from the work of others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll also need to make sure you have a StorageClass in your cluster. If you’re
    starting from a cluster built the hard way, you won’t have one. You may want to
    follow the instructions in [“StorageClasses”](ch02.html#storageclasses) for installing
    a simple StorageClass and provisioner that expose local storage. The [source code](https://oreil.ly/iV1Tg)
    is in this book’s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You’ll want to use a StorageClass that supports a [`volumeBindingMode`](https://oreil.ly/rpNyc)
    of `WaitForFirstConsumer`. This gives Kubernetes the flexibility to defer provisioning
    storage until we need it. This behavior is generally preferred for production
    deployments, so you might as well start getting in the habit.
  prefs: []
  type: TYPE_NORMAL
- en: Running MySQL on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let’s start with a super simple example. MySQL is one of the most widely
    used relational databases because of its reliability and usability. For this example,
    we’ll build on the [MySQL tutorial](https://oreil.ly/cY6cv) in the official Kubernetes
    documentation, with a couple of twists. You can find the source code used in this
    section at [“Deploying MySQL Example—Data on Kubernetes the Hard Way”](https://oreil.ly/YfjiG).
    The tutorial includes two Kubernetes deployments: one to run a MySQL Pod, and
    another to run a sample client—in this case, WordPress. This configuration is
    shown in [Figure 3-1](#sample_kubernetes_deployment_of_mysql).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample Kubernetes deployment of MySQL](assets/mcdk_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Sample Kubernetes deployment of MySQL
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example, we see that there is a PersistentVolumeClaim for each Pod.
    For the purposes of this example, we’ll assume these claims are satisfied by a
    single volume provided by the default StorageClass. You’ll also notice that each
    Pod is shown as part of a ReplicaSet and that there is a service exposed for the
    MySQL database. Let’s take a pause and introduce these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Production application deployments on Kubernetes do not typically deploy individual
    Pods, because an individual Pod could easily be lost when the node disappears.
    Instead, Pods are typically deployed in the context of a Kubernetes resource that
    manages their lifecycle. ReplicaSet is one of these resources, and the other is
    StatefulSet, which we’ll look at later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of a *ReplicaSet* is to ensure that a specified number of replicas
    of a given Pod are kept running at any given time. As Pods are destroyed, others
    are created to replace them in order to satisfy the desired number of replicas.
    A ReplicaSet is defined by a Pod template, a number of replicas, and a selector.
    The Pod template defines a specification for Pods that will be managed by the
    ReplicaSet, similar to what we saw for individual Pods created in the examples
    in [Chapter 2](ch02.html#managing_data_storage_on_kubernetes). The number of replicas
    can be zero or more. The selector identifies Pods that are part of the ReplicaSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a portion of an example definition of a ReplicaSet for the WordPress
    application shown in [Figure 3-1](#sample_kubernetes_deployment_of_mysql):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A ReplicaSet is responsible for creating or deleting Pods in order to meet the
    specified number of replicas. You can scale the size of a ReplicaSet up or down
    by changing this value. The Pod template is used when creating new Pods. Pods
    that are managed by a ReplicaSet contain a reference to the ReplicaSet in their
    `metadata.ownerReferences` field. A ReplicaSet can actually take responsibility
    for managing a Pod that it did not create if the selector matches and the Pod
    does not reference another owner. This behavior of a ReplicaSet is known as *acquiring*
    a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Define ReplicaSet Selectors Carefully
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you do create ReplicaSets directly, make sure that the selector you use is
    unique and does not match any bare Pods that you do not intend to be acquired.
    Pods that do not match the Pod template could be acquired if the selectors match.
    For more information about managing the lifecycle of ReplicaSets and the Pods
    they manage, see the [Kubernetes documentation](https://oreil.ly/8Bc9D).
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be wondering why we didn’t provide a full definition of a ReplicaSet.
    As it turns out, most application developers do not end up using ReplicaSets directly,
    because Kubernetes provides another resource type that manages ReplicaSets declaratively:
    Deployments.'
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Kubernetes *Deployment* is a resource that builds on top of ReplicaSets with
    additional features for lifecycle management, including the ability to roll out
    new versions and roll back to previous versions. As shown in [Figure 3-2](#deployments_and_replicasets),
    creating a Deployment results in the creation of a ReplicaSet as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deployments and ReplicaSets](assets/mcdk_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Deployments and ReplicaSets
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This figure highlights that ReplicaSets (and therefore the Deployments that
    manage them) operate on cloned replicas of Pods, meaning that the definitions
    of the Pods are the same, even down to the level of PersistentVolumeClaims. The
    definition of a ReplicaSet references a single PVC that is provided to it, and
    there is no mechanism provided to clone the PVC definition for additional Pods.
    For this reason, Deployments and ReplicaSets are not a good choice if your intent
    is for each Pod to have access to its own dedicated storage.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments are a good choice if your application Pods do not need access to
    storage, or if your intent is that they access the same piece of storage. However,
    the cases where this would be desirable are pretty rare, since you likely don’t
    want a situation in which you could have multiple simultaneous writers to the
    same storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create an example Deployment. First, create a Secret that will represent
    the database password (substitute in whatever string you want for the password):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a PVC that represents the storage that the database can use. The
    [source code](https://oreil.ly/CHccy) is in this book’s repository. A single PVC
    is sufficient in this case since you are creating a single node. This should work
    as long as you have an appropriate StorageClass, as referenced earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a Deployment with a Pod template spec that runs MySQL. The [source
    code](https://oreil.ly/v9TEt) is in this book’s repository. Note that it includes
    a reference to the PVC you just created as well as the Secret containing the root
    password for the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a few interesting things to note about this Deployment’s specification:'
  prefs: []
  type: TYPE_NORMAL
- en: The Deployment has a `Recreate` strategy. This refers to the way the Deployment
    handles the replacement of Pods when the Pod template is updated; we’ll discuss
    this shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the Pod template, the password is passed to the Pod as an environment
    variable extracted from the Secret you created in this example. Overriding the
    default password is an important aspect of securing any database deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single port is exposed on the MySQL image for database access, since this
    is a relatively simple example. In other samples in this book, we’ll see cases
    of Pods that expose additional ports for administrative operations, metrics collection,
    and more. The fact that access is disabled by default is a great feature of Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MySQL image mounts a volume for its persistent storage using the PVC defined
    in this example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of replicas was not provided in the specification. This means that
    the default value of 1 will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After applying the configuration, try using a command like `kubectl get deployments,rs,pods`
    to see the items that Kubernetes created for you. You’ll notice a single ReplicaSet
    named after the Deployment that includes a random string (for example, `wordpress-mysql-655c8d9c54`).
    The Pod’s name references the name of the ReplicaSet, adding some additional random
    characters (for example, `wordpress-mysql-655c8d9c54-tgswd`). These names provide
    a quick way to identify the relationships between these resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few of the actions that a Deployment takes to manage the lifecycle
    of ReplicaSets. In keeping with the Kubernetes emphasis on declarative operations,
    most of these are triggered by updating the specification of the Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: Initial rollout
  prefs: []
  type: TYPE_NORMAL
- en: When you create a Deployment, Kubernetes uses the specification you provide
    to create a ReplicaSet. The process of creating this ReplicaSet and its Pods is
    known as a *rollout*. A rollout is also performed as part of a rolling update.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up or down
  prefs: []
  type: TYPE_NORMAL
- en: When you update a Deployment to change the number of replicas, the underlying
    ReplicaSet is scaled up or down accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling update
  prefs: []
  type: TYPE_NORMAL
- en: When you update the Deployment’s Pod template (for example, by specifying a
    different container image for the Pod), Kubernetes creates a new ReplicaSet based
    on the new Pod template. The way that Kubernetes manages the transition between
    the old and new ReplicaSets is described by the Deployment’s `spec.strategy` property,
    which defaults to a value called `RollingUpdate`. In a rolling update, the new
    ReplicaSet is slowly scaled up by creating Pods conforming to the new template,
    as the number of Pods in the existing ReplicaSet is scaled down. During this transition,
    the Deployment enforces a maximum and minimum number of Pods, expressed as percentages,
    as set by the `spec.strategy.rollingupdate.maxSurge` and `maxUnavailable` properties.
    Each of these values defaults to 25%.
  prefs: []
  type: TYPE_NORMAL
- en: Recreate update
  prefs: []
  type: TYPE_NORMAL
- en: The other option for use when you update the Pod template is `Recreate`. This
    is the option that was set in the preceding Deployment. With this option, the
    existing ReplicaSet is terminated immediately before the new ReplicaSet is created.
    This strategy is useful for development environments since it completes the update
    more quickly, whereas `RollingUpdate` is more suitable for production environments
    since it emphasizes high availability. This is also useful for data migration.
  prefs: []
  type: TYPE_NORMAL
- en: Rollback update
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating or updating a Deployment, you could introduce an error—for example,
    by updating a container image in a Pod with a version that contains a bug. In
    this case, the Pods managed by the Deployment might not even initialize fully.
    You can detect these types of errors using commands such as `kubectl` `rollout`
    `status`. Kubernetes provides a series of operations for managing the history
    of rollouts of a Deployment. You can access these via `kubectl` commands such
    as `kubectl rollout history`, which provides a numbered history of rollouts for
    a Deployment, and `kubectl rollout undo`, which reverts a Deployment to the previous
    rollout. You can also `undo` to a specific rollout version with the `--to-version`
    option. Because `kubectl` supports rollouts for other resource types we’ll cover
    later in this chapter (StatefulSets and DaemonSets), you’ll need to include the
    resource type and name when using these commands—for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces output such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, Kubernetes Deployments provide some sophisticated behaviors
    for managing the lifecycle of a set of cloned Pods. You can test out these lifecycle
    operations (other than rollback) by changing the Deployment’s YAML specification
    and reapplying it. Try scaling the number of replicas to 2 and back again, or
    using a different MySQL image. After updating the Deployment, you can use a command
    like `kubectl describe deployment` `wordpress-mysql` to observe the events that
    Kubernetes initiates to bring your Deployment to your desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Other options are available for Deployments that we don’t have space to go into
    here—for example, how to specify what Kubernetes does if you attempt an update
    that fails. For a more in-depth explanation of the behavior of Deployments, see
    the [Kubernetes documentation](https://oreil.ly/ibjpA).
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding steps, you’ve created a PVC to specify the storage needs of
    the database, a Secret to provide administrator credentials, and a Deployment
    to manage the lifecycle of a single MySQL Pod. Now that you have a running database,
    you’ll want to make it accessible to applications. In our scheme of compute, network,
    and storage that we introduced in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra),
    this is the networking part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes *Services* are the primitive that we need to use to expose access
    to our database as a network service. A Service provides an abstraction for a
    group of Pods running behind it. In the case of a single MySQL node as in this
    example, you might wonder why we’d bother creating this abstraction. One key feature
    that a Service supports is to provide a consistently named endpoint that doesn’t
    change. You don’t want to be in a situation of having to update your clients whenever
    the database Pod is restarted and gets a new IP address. You can create a Service
    for accessing MySQL by using a YAML configuration like this. The [source code](https://oreil.ly/FyR9E)
    is in this book’s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are a couple of things to note about this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration specifies a `port` that is exposed on the Service: 3306\.
    In defining a Service, two ports are actually involved: the `port` exposed to
    clients of the Service, and the `targetPort` exposed by the underlying Pods that
    the Service is fronting. Since you haven’t specified a `targetPort`, it defaults
    to the `port` value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `selector` defines what Pods the Service will direct traffic to. In this
    configuration, there will be only a single MySQL Pod managed by the Deployment,
    and that’s just fine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have worked with Kubernetes Services before, you may note that there
    is no `serviceType` defined for this Service, which means that it is of the default
    type, known as `ClusterIP`. Furthermore, since the `clusterIP` property is set
    to `None`, this is what is known as a *headless Service*—that is, the Service’s
    DNS name is mapped directly to the IP addresses of the selected Pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes supports several types of Services to address different use cases,
    which are shown in [Figure 3-3](#kubernetes_service_types). We’ll introduce them
    briefly here in order to highlight their applicability to data infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP Service
  prefs: []
  type: TYPE_NORMAL
- en: This type of Service is exposed on a cluster-internal IP address. ClusterIP
    Services are the type used most often for data infrastructure such as databases
    in Kubernetes, especially headless services, since this infrastructure is typically
    deployed in Kubernetes alongside the application that uses it.
  prefs: []
  type: TYPE_NORMAL
- en: NodePort Service
  prefs: []
  type: TYPE_NORMAL
- en: A NodePort Service is exposed externally to the cluster on the IP address of
    each Worker Node. A ClusterIP service is also created internally, to which the
    NodePort routes traffic. You can allow Kubernetes to select what external port
    is used from a range of ports (30000–32767 by default), or specify the one you
    desire by using the `NodePort` property. NodePort services are most suitable for
    development environments, when you need to debug what is happening on a specific
    instance of a data infrastructure application.
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer Services represent a request from the Kubernetes runtime to set
    up an external load balancer provided by the underlying cloud provider. For example,
    on Amazon’s Elastic Kubernetes Service (EKS), requesting a LoadBalancer Service
    causes an instance of an Elastic Load Balancer (ELB) to be created. Usage of LoadBalancers
    in front of multinode data infrastructure deployments is typically not required,
    as these data technologies often have their own approaches for distributing load.
    For example, Apache Cassandra drivers are aware of the topology of a Cassandra
    cluster and provide load-balancing features to client applications, eliminating
    the need for a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: ExternalName Service
  prefs: []
  type: TYPE_NORMAL
- en: An ExternalName Service is typically used to represent access to a Service that
    is outside your cluster—for example, a database that is running externally to
    Kubernetes. An ExternalName Service does not have a selector, as it is not mapping
    to any Pods. Instead, it maps the Service name to a CNAME record. For example,
    if you create a `my-external-database` Service with an `externalName` of `database.mydomain.com`,
    references in your application Pods to `my-external-database` will be mapped to
    `database.mydomain.com`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes Service types](assets/mcdk_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Kubernetes Service types
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note also the inclusion of *Ingress* in the figure. While Kubernetes Ingress
    is not a type of Service, it is related. An Ingress is used to provide access
    to Kubernetes services from outside the cluster, typically via HTTP. Multiple
    Ingress implementations are available, including Nginx, Traefik, Ambassador (based
    on Envoy) and others. Ingress implementations typically provide features including
    Secure Sockets Layer (SSL) termination and load balancing, even across multiple
    Kubernetes Services. As with LoadBalancer Services, Ingresses are more typically
    used at the application tier.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing MySQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you have deployed the database, you’re ready to deploy an application
    that uses it—the WordPress server. First, the server will need its own PVC. This
    helps illustrate that some applications leverage storage directly—perhaps for
    storing files, applications that use data infrastructure, and applications that
    do both. You can make a small request since this is just for demonstration purposes.
    The [source code](https://oreil.ly/smKtM) is in this book’s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a Deployment for a single WordPress node. The [source code](https://oreil.ly/hLPdW)
    is in this book’s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the database host and password for accessing MySQL are passed to
    WordPress as environment variables. The value of the host is the name of the Service
    you created for MySQL above. This is all that is needed for the database connection
    to be routed to your MySQL instance. The value for the password is extracted from
    the Secret, as with the preceding configuration of the MySQL Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll also notice that WordPress exposes an HTTP interface at port 80, so
    let’s create a service to expose the WordPress server. The [source code](https://oreil.ly/tEigE)
    is in this book’s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that the service is of type LoadBalancer, which should make it fairly simple
    to access from your local machine. Execute the command `kubectl get services`
    to get the LoadBalancer’s IP address; then you can open the WordPress instance
    in your browser with the URL `http://*<ip>*`. Try logging in and creating some
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Services from Kubernetes Distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exact details of accessing Services will depend on the Kubernetes distribution
    you’re using and whether you’re deploying apps in production, or just testing
    something quickly as we’re doing here. If you’re using a desktop Kubernetes distribution,
    you may wish to use a NodePort Service instead of LoadBalancer for simplicity.
    You can also consult the documentation for instructions on accessing services,
    such as those provided for [minikube](https://oreil.ly/euQLB) or [k3d](https://k3d.io).
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’re done experimenting with your WordPress instance, clean up the resources
    specified in the configuration files you’ve used in the local directory using
    the following command, including the data stored in your PersistentVolumeClaim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you might be feeling like this was relatively easy, despite our
    claim to be doing things “the hard way.” And in a sense, you’d be right. So far,
    we’ve deployed a single Node of a simple database with sane defaults that we didn’t
    have to spend much time configuring. Creating a single Node is, of course, fine
    if your application is going to store only a small amount of data. Is that all
    there is to deploying databases on Kubernetes? Of course not! Now that we’ve introduced
    a few of the basic Kubernetes resources via this simple database deployment, it’s
    time to step up the complexity a bit. Let’s get down to business!
  prefs: []
  type: TYPE_NORMAL
- en: Running Apache Cassandra on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll look at running a multinode database on Kubernetes using
    Apache Cassandra. Cassandra is a NoSQL database first developed at Facebook that
    became a top-level project of the Apache Software Foundation (ASF) in 2010\. Cassandra
    is an operational database that provides a tabular data model, and its Cassandra
    Query Language (CQL) is similar to SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra is a database designed for the cloud, as it scales horizontally by
    adding nodes, where each node is a peer. This decentralized design has been proven
    to have near-linear scalability. Cassandra supports high availability by storing
    multiple copies of data or *replicas*, including logic to distribute those replicas
    across multiple Datacenters and cloud regions. Cassandra is built on similar principles
    to Kubernetes in that it is designed to detect failures and continue operating
    while the system can recover to its intended state in the background. All of these
    features make Cassandra an excellent fit for deploying on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To discuss how this deployment works, it’s helpful to understand Cassandra’s
    approach to distributing data from two perspectives: physical and logical. Borrowing
    some of the visuals from [*Cassandra: The Definitive Guide*](https://learning.oreilly.com/library/view/cassandra-the-definitive/9781492097136)
    by Jeff Carpenter and Eben Hewitt (O’Reilly), you can see these perspectives in
    [Figure 3-4](#physical_and_logical_views_of_cassandra). From a physical perspective,
    Cassandra nodes (not to be confused with Kubernetes Worker Nodes) are organized
    using *racks* and *Datacenters*. While the terms betray Cassandra’s origin during
    a time when on-premise datacenters were the dominant way software was deployed
    in the mid-2000s, they can be flexibly applied. In cloud deployments, racks often
    represent an availability zone, while Datacenters represent a cloud region. However
    these are represented, the important part is that they represent physically separate
    failure domains. Cassandra uses awareness of this topology to make sure that it
    stores replicas in multiple physical locations to maximize the availability of
    data in the event of failures, whether those failures are a single machine, a
    rack of servers, an availability zone, or an entire region.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Physical and logical views of Cassandra’s distributed architecture](assets/mcdk_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Physical and logical views of Cassandra’s distributed architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The logical view helps us understand how Cassandra determines what data will
    be placed on each node. Each row of data in Cassandra is identified by a primary
    key, which consists of one or more partition-key columns used to allocate data
    across nodes, as well as optional clustering columns, which can be used to organize
    multiple rows of data within a partition for efficient access. Each write in Cassandra
    (and most reads) references a specific partition by providing the partition-key
    values, which Cassandra hashes together to produce a *token*, which is a value
    between −2^(63) and 2^(63)^(−1). Cassandra assigns each of its nodes responsibility
    for one or more token ranges (shown as a single range per node labeled with letters
    A–H in [Figure 3-4](#physical_and_logical_views_of_cassandra) for simplicity).
    The physical topology is taken into account in the assignment of token ranges
    in order to ensure that copies of your data are distributed across racks and datacenters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’re ready to consider how Cassandra maps onto Kubernetes. It’s important
    to consider two implications of Cassandra’s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Statefulness
  prefs: []
  type: TYPE_NORMAL
- en: Each Cassandra node has state that it is responsible for maintaining. Cassandra
    has mechanisms for replacing a node by streaming data from other replicas to a
    new node, which means that a configuration in which nodes use local ephemeral
    storage is possible, at the cost of longer startup time. However, it’s more common
    to configure each Cassandra node to use persistent storage. In either case, each
    Cassandra node needs to have its own unique PersistentVolumeClaim.
  prefs: []
  type: TYPE_NORMAL
- en: Identity
  prefs: []
  type: TYPE_NORMAL
- en: Although each Cassandra node is the same in terms of its code, configuration,
    and functionality in a fully peer-to-peer architecture, the nodes are different
    in terms of their actual role. Each node has an identity in terms of where it
    fits in the topology of Datacenters and racks, and its assigned token ranges.
  prefs: []
  type: TYPE_NORMAL
- en: These requirements for identity and an association with a specific PersistentVolumeClaim
    present some challenges for Deployments and ReplicaSets that they weren’t designed
    to handle. Starting early in Kubernetes’ existence, there was an awareness that
    another mechanism was needed to manage stateful workloads like Cassandra.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes began providing a resource to manage stateful workloads with the
    alpha release of PetSets in the 1.3 release. This capability has matured over
    time and is now known as *StatefulSets* (see [“Are Your Stateful Workloads Pets
    or Cattle?”](#are_your_stateful_workloads_pets_or_cat)). A StatefulSet has some
    similarities to a ReplicaSet in that it is responsible for managing the lifecycle
    of a set of Pods, but the way in which it goes about this management has some
    significant differences. To address the needs of stateful applications, like those
    of Cassandra that we’ve listed, StatefulSets demonstrate the following key properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Stable identity for Pods
  prefs: []
  type: TYPE_NORMAL
- en: 'First, StatefulSets provide a stable name and network identity for Pods. Each
    Pod is assigned a name based on the name of the StatefulSet, plus an ordinal number.
    For example, a StatefulSet called `cassandra` would have Pods named `cassandra-0`,
    `cassandra-1`, `cassandra-2`, and so on, as shown in [Figure 3-5](#sample_deployment_of_cassandra_on_kuber).
    These are stable names, so if a Pod is lost and needs replacing, the replacement
    will have the same name, even if it’s started on a different Worker Node. A Pod’s
    name is set as its hostname, so if you create a headless service, you can actually
    address individual Pods as needed—for example: `cassandra-1.cqlservice.default.svc.cluster.local`.
    The figure also includes a seed service, which we’ll discuss in [“Accessing Cassandra”](#accessing_cassandra).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample Deployment of Cassandra on Kubernetes with StatefulSets](assets/mcdk_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Sample Deployment of Cassandra on Kubernetes with StatefulSets
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ordered lifecycle management
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets provide predictable behaviors for managing the lifecycle of Pods.
    When scaling up the number of Pods in a StatefulSet, new Pods are added according
    to the next available number, unlike ReplicaSets, where Pod name suffixes are
    based on universally unique identifiers (UUIDs). For example, expanding the StatefulSet
    in [Figure 3-5](#sample_deployment_of_cassandra_on_kuber) would cause the creation
    of Pods such as `cassandra-4` and `cassandra-5`. Scaling down has the reverse
    behavior, as the Pods with the highest ordinal numbers are deleted first. This
    predictability simplifies management—for example, by making it obvious which Nodes
    should be backed up before reducing cluster size.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent disks
  prefs: []
  type: TYPE_NORMAL
- en: Unlike ReplicaSets, which create a single PersistentVolumeClaim shared across
    all their Pods, StatefulSets create a PVC associated with each Pod. If a Pod in
    a StatefulSet is replaced, the replacement is bound to the PVC that has the state
    it is replacing. Replacement could occur because of a Pod failing or the scheduler
    choosing to run a Pod on another node in order to balance the load. For a database
    like Cassandra, this enables quick recovery when a Cassandra node is lost, as
    the replacement node can recover its state immediately from the associated PersistentVolume
    rather than needing data streamed from other replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Data Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When planning your application deployment, make sure you consider whether data
    is being replicated at the data tier or the storage tier. A distributed database
    like Cassandra manages replication itself, storing copies of your data on multiple
    nodes according to the replication factor you request, typically three per Cassandra
    Datacenter. The storage provider you select may also offer replication. If the
    Kubernetes volume for each Cassandra Pod has three replicas, you could end up
    storing nine copies of your data. While this certainly promotes high data survivability,
    this might cost more than you intend.
  prefs: []
  type: TYPE_NORMAL
- en: Defining StatefulSets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve learned a bit about StatefulSets, let’s examine how they can
    be used to run Cassandra. You’ll configure a simple three-node cluster the “hard
    way” using a Kubernetes StatefulSet to represent a single Cassandra datacenter
    containing a single rack. The source code used in this section is located in [the
    book’s repository](https://oreil.ly/yhg3w). This approximates the configuration
    shown in [Figure 3-5](#sample_deployment_of_cassandra_on_kuber).
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a Cassandra cluster in Kubernetes, you’ll first need a headless service.
    This service represents the CQL Service shown in [Figure 3-5](#sample_deployment_of_cassandra_on_kuber),
    providing an endpoint that clients can use to obtain addresses of all the Cassandra
    nodes in the StatefulSet. The [source code](https://oreil.ly/7nXxZ) is in this
    book’s repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll reference this service in the definition of a StatefulSet which will
    manage your Cassandra nodes. The [source code](https://oreil.ly/0r6Cr) is located
    in this book’s repository. Rather than applying this configuration immediately,
    you may want to wait until after we do some quick explanations. The configuration
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the most complex configuration we’ve looked at together so far, so
    let’s simplify it by looking at one portion at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSet metadata
  prefs: []
  type: TYPE_NORMAL
- en: We’ve named and labeled this StatefulSet `cassandra`, and that same string will
    be used as the selector for Pods belonging to the StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing StatefulSet Pods via a Service
  prefs: []
  type: TYPE_NORMAL
- en: The `spec` of the StatefulSet starts with a reference to the headless service
    you created. While `serviceName` is not a required field according to the Kubernetes
    specification, some Kubernetes distributions and tools such as Helm expect it
    to be populated and will generate warnings or errors if you fail to provide a
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Number of replicas
  prefs: []
  type: TYPE_NORMAL
- en: The `replicas` field identifies the number of Pods that should be available
    in this StatefulSet. The value provided (`3`) reflects the smallest Cassandra
    cluster that one might see in an actual production deployment, and most deployments
    are significantly larger, which is when Cassandra’s ability to deliver high performance
    and availability at scale really begin to shine through.
  prefs: []
  type: TYPE_NORMAL
- en: Lifecycle management options
  prefs: []
  type: TYPE_NORMAL
- en: The `podManagementPolicy` and `updateStrategy` describe how Kubernetes should
    manage the rollout of Pods when the cluster is scaling up or down, and how updates
    to the Pods in the StatefulSet should be managed, respectively. We’ll examine
    the significance of these values in [“StatefulSet lifecycle management”](#statefulset_life_cycle_management).
  prefs: []
  type: TYPE_NORMAL
- en: Pod specification
  prefs: []
  type: TYPE_NORMAL
- en: The next section of the StatefulSet specification is the `template` used to
    create each Pod that is managed by the StatefulSet. The template has several subsections.
    First, under `metadata`, each Pod includes a label `cassandra` that identifies
    it as being part of the set.
  prefs: []
  type: TYPE_NORMAL
- en: This template includes a single item in the `containers` field, a specification
    for a Cassandra container. The `image` field selects the latest version of the
    official Cassandra [Docker image](https://oreil.ly/arYaE), which at the time of
    writing is Cassandra 4.0\. This is where we diverge with the Kubernetes StatefulSet
    tutorial referenced previously, which uses a custom Cassandra 3.11 image created
    specifically for that tutorial. Because the image we’ve chosen to use here is
    an official Docker image, you do not need to include registry or account information
    to reference it, and the name `cassandra` by itself is sufficient to identify
    the image that will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each Pod will expose `ports` for various interfaces: a `cql` port for client
    use, `intra-node` and `tls-intra-node` ports for communication between nodes in
    the Cassandra cluster, and a `jmx` port for management via the Java Management
    Extensions (JMX).'
  prefs: []
  type: TYPE_NORMAL
- en: The Pod specification also includes instructions that help Kubernetes manage
    Pod lifecycles, including a `livenessProbe` and a `preStop` command. You’ll learn
    how each of these are used next.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to its [documentation](https://oreil.ly/WuTZo), the image we’re using
    has been constructed to provide two ways to customize Cassandra’s configuration,
    which is stored in the *cassandra.yaml* file within the image. One way is to override
    the entire contents of the *cassandra.yaml* with a file that you provide. The
    second is to use environment variables that the image exposes to override a subset
    of Cassandra configuration options that are used most frequently. Setting these
    values in the `env` field causes the corresponding settings in the *cassandra.yaml*
    file to be updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CASSANDRA_CLUSTER_NAME`'
  prefs: []
  type: TYPE_NORMAL
- en: This setting is used to distinguish which nodes belong to a cluster. Should
    a Cassandra node come into contact with nodes that don’t match its cluster name,
    it will ignore them.
  prefs: []
  type: TYPE_NORMAL
- en: '`CASSANDRA_DC` and `CASSANDRA_RACK`'
  prefs: []
  type: TYPE_NORMAL
- en: These settings identify the Datacenter and rack that each node will be a part
    of. This serves to highlight one interesting wrinkle in the way that StatefulSets
    expose a Pod specification. Since the template is applied to each Pod and container,
    there is no way to vary the configured Datacenter and rack names between Cassandra
    Pods. For this reason, it is typical to deploy Cassandra on Kubernetes using a
    StatefulSet per rack.
  prefs: []
  type: TYPE_NORMAL
- en: '`CASSANDRA_SEEDS`'
  prefs: []
  type: TYPE_NORMAL
- en: These define well-known locations of nodes in a Cassandra cluster that new nodes
    can use to bootstrap themselves into the cluster. The best practice is to specify
    multiple seeds in case one of them happens to be down or offline when a new node
    is joining. However, for this initial example, it’s enough to specify the initial
    Cassandra replica as a seed via the DNS name `cassandra-0.cassandra.default.svc.cluster.local`.
    We’ll look at a more robust way of specifying seeds in [Chapter 4](ch04.html#automating_database_deployment_on_kuber)
    using a service, as implied by the Seed service shown in [Figure 3-5](#sample_deployment_of_cassandra_on_kuber).
  prefs: []
  type: TYPE_NORMAL
- en: The last item in the container specification is a `volumeMount` which requesting
    that a PersistentVolume be mounted at the */var/lib/cassandra* directory, which
    is where the Cassandra image is configured to store its datafiles. Since each
    Pod will need its own PersistentVolumeClaim, the name `cassandra-data` is a reference
    to a PersistentVolumeClaim template, which is defined next.
  prefs: []
  type: TYPE_NORMAL
- en: volumeClaimTemplates
  prefs: []
  type: TYPE_NORMAL
- en: The final piece of the StatefulSet specification is the volumeClaimTemplates.
    The specification must include a template definition for each name referenced
    in one of the preceding container specifications. In this case, the `cassandra-data`
    template references the `standard` StorageClass we’ve been using in these examples.
    Kubernetes will use this template to create a PersistentVolumeClaim of the requested
    size of 1 GB whenever it spins up a new Pod within this StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSet lifecycle management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we’ve had a chance to discuss the components of a StatefulSet specification,
    you can go ahead and apply the source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As this gets applied, you can execute the following to watch as the StatefulSet
    spins up Cassandra Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s describe some of the behavior you can observe from the output of this
    command. First, you’ll see a single Pod, `cassandra-0`. Once that Pod has progressed
    to `Ready` status, you’ll see the `cassandra-1` Pod, followed by `cassandra-2`
    after `cassandra-1` is ready. This behavior is specified by the selection of `podManagementPolicy`
    for the StatefulSet. Let’s explore the available options and some of the other
    settings that help define how Pods in a StatefulSet are managed:'
  prefs: []
  type: TYPE_NORMAL
- en: Pod management policies
  prefs: []
  type: TYPE_NORMAL
- en: The `podManagementPolicy` determines the timing for adding or removing Pods
    from a StatefulSet. The `OrderedReady` policy applied in our Cassandra example
    is the default. When this policy is in place and Pods are added, whether on initial
    creation or scaling up, Kubernetes expands the StatefulSet one Pod at a time.
    As each Pod is added, Kubernetes waits until the Pod reports a status of `Ready`
    before adding subsequent Pods. If the Pod specification contains a `readinessProbe`,
    Kubernetes executes the provided command iteratively to determine when the Pod
    is ready to receive traffic. When the probe completes successfully (i.e., with
    a zero return code), it moves on to creating the next Pod. For Cassandra, readiness
    is typically measured by the availability of the CQL port (9042), which means
    the node is able to respond to CQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when a StatefulSet is removed or scaled down, Pods are removed one
    at a time. As a Pod is being removed, any provided `preStop` commands for its
    containers are executed to give them a chance to shut down gracefully. In our
    current example, the `nodetool drain` command is executed to help the Cassandra
    node exit the cluster cleanly, assigning responsibilities for its token range(s)
    to other nodes. as Kubernetes waits until a Pod has been completely terminated
    before removing the next Pod. The command specified in the `livenessProbe` is
    used to determine when the Pod is alive, and when it no longer completes without
    error, Kubernetes can proceed to removing the next Pod. See the [Kubernetes documentation](https://oreil.ly/SsIuO)
    for more information on configuring readiness and liveness probes.
  prefs: []
  type: TYPE_NORMAL
- en: The other Pod management policy is `Parallel`. When this policy is in effect,
    Kubernetes launches or terminates multiple Pods at the same time in order to scale
    up or down. This has the effect of bringing your StatefulSet to the desired number
    of replicas more quickly, but it may also result in some stateful workloads taking
    longer to stabilize. For example, a database like Cassandra shuffles data between
    nodes when the cluster size changes in order to balance the load, and will tend
    to stabilize more quickly when nodes are added or removed one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: With either policy, Kubernetes manages Pods according to the ordinal numbers,
    always adding Pods with the next unused ordinal numbers when scaling up, and deleting
    the Pods with the highest ordinal numbers when scaling down.
  prefs: []
  type: TYPE_NORMAL
- en: Update strategies
  prefs: []
  type: TYPE_NORMAL
- en: The `updateStrategy` describes how Pods in the StatefulSet will be updated if
    a change is made in the Pod template specification, such as changing a container
    image. The default strategy is `RollingUpdate`, as selected in this example. With
    the other option, `OnDelete`, you must manually delete Pods in order for the new
    Pod template to be applied.
  prefs: []
  type: TYPE_NORMAL
- en: In a rolling update, Kubernetes will delete and re-create each Pod in the StatefulSet,
    starting with the Pod with the largest ordinal number and working toward the smallest.
    Pods are updated one at a time, and you can specify a number of Pods, called a
    *partition*, in order to perform a phased rollout or canary. Note that if you
    discover a bad Pod configuration during a rollout, you’ll need to update the Pod
    template specification to a known good state and then manually delete any Pods
    that were created using the bad specification. Since these Pods will not ever
    reach a `Ready` state, Kubernetes will not decide they are ready to replace with
    the good configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Kubernetes offers similar lifecycle management options for Deployments,
    ReplicaSets, and DaemonSets, including revision history.
  prefs: []
  type: TYPE_NORMAL
- en: 'We recommend getting more hands-on experience with managing StatefulSets in
    order to reinforce your knowledge. For example, you can monitor the creation of
    PersistentVolumeClaims as a StatefulSet scales up. Another thing to try: delete
    a StatefulSet and re-create it, verifying that the new Pods recover previously
    stored data from the original StatefulSet. For more ideas, you may find these
    guided tutorials helpful: [“StatefulSet Basics”](https://oreil.ly/dOovM) from
    the Kubernetes documentation, and [“StatefulSet: Run and Scale Stateful Applications
    Easily in Kubernetes”](https://oreil.ly/TyJj2) from the Kubernetes blog.'
  prefs: []
  type: TYPE_NORMAL
- en: More Sophisticated Lifecycle Management for StatefulSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One interesting set of opinions on additional lifecycle options for StatefulSets
    comes from OpenKruise, a CNCF Sandbox project, which provides an [Advanced StatefulSet](https://oreil.ly/xEqYf).
    The Advanced StatefulSet adds capabilities including these:'
  prefs: []
  type: TYPE_NORMAL
- en: Parallel updates with a maximum number of unavailable Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling updates with an alternate order for replacement, based on a provided
    prioritization policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating Pods “in place” by restarting their containers according to an updated
    Pod template specification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This Kubernetes resource is also named `StatefulSet` to facilitate its use
    with minimal impact to your existing configurations. You just need to change the
    `apiVersion`: from `apps/v1` to `apps.kruise.io/v1beta1`.'
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets are extremely useful for managing stateful workloads on Kubernetes,
    and that’s not even counting some capabilities we didn’t address, such as affinity
    and anti-affinity, managing resource requests for memory and CPU, and availability
    constraints such as PodDisruptionBudgets (PDBs). On the other hand, you might
    desire capabilities that StatefulSets don’t provide, such as backup/restore of
    PersistentVolumes, or secure provisioning of access credentials. We’ll discuss
    how to leverage or build these capabilities on top of Kubernetes in [Chapter 4](ch04.html#automating_database_deployment_on_kuber)
    and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Cassandra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have applied the configurations we’ve listed, you can use Cassandra’s
    CQL shell `cqlsh` to execute CQL commands. If you happen to be a Cassandra user
    and have a copy of `cqlsh` installed on your local machine, you could access Cassandra
    as a client application would, using the CQL Service associated with the StatefulSet.
    However, since each Cassandra node contains `cqlsh` as well, this gives us a chance
    to demonstrate a different way to interact with infrastructure in Kubernetes,
    by connecting directly to an individual Pod in a StatefulSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This should bring up the `cqlsh` prompt, and you can then explore the contents
    of Cassandra’s built-in tables using `DESCRIBE KEYSPACES` and then `USE` to select
    a particular keyspace and run `DESCRIBE TABLES`. Many Cassandra tutorials available
    online can guide you through more examples of creating your own tables, inserting
    and querying data, and more. When you’re done experimenting with `cqlsh`, you
    can type `exit` to exit the shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing a StatefulSet is the same as any other Kubernetes resource—you can
    delete it by name, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You could also delete the StatefulSet referencing the file used to create it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When you delete a StatefulSet with a policy of `Retain` as in this example,
    the PersistentVolumeClaims it creates are not deleted. If you re-create the StatefulSet,
    it will bind to the same PVCs and reuse the existing data. When you no longer
    need the claims, you’ll need to delete them manually. The final cleanup from this
    exercise you’ll want to perform is to delete the CQL Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you’ve learned how to deploy both single-node and multinode
    distributed databases on Kubernetes with hands-on examples. Along the way, you’ve
    gained familiarity with Kubernetes resources such as Deployments, ReplicaSets,
    StatefulSets, and DaemonSets, and learned about the best use cases for each:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Deployments/ReplicaSets to manage stateless workloads or simple stateful
    workloads like single-node databases or caches that can rely on ephemeral storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use StatefulSets to manage stateful workloads that involve multiple nodes and
    require association with specific storage locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use DaemonSets to manage workloads that leverage specific Worker Node functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ve also learned the limits of what each of these resources can provide.
    Now that you’ve gained experience in deploying stateful workloads on Kubernetes,
    the next step is to learn how to automate the so-called “day two” operations involved
    in keeping this data infrastructure running.
  prefs: []
  type: TYPE_NORMAL
