- en: Chapter 16\. Choosing a Shard Key
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important task when using sharding is choosing how your data will
    be distributed. To make intelligent choices about this, you have to understand
    how MongoDB distributes data. This chapter helps you make a good choice of shard
    key by covering:'
  prefs: []
  type: TYPE_NORMAL
- en: How to decide among multiple possible shard keys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shard keys for several use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you can’t use as a shard key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some alternative strategies if you want to customize how data is distributed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to manually shard your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It assumes that you understand the basic components of sharding as covered in
    the previous two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Taking Stock of Your Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you shard a collection you choose a field or two to use to split up the
    data. This key (or keys) is called a *shard key*. Once you shard a collection
    you cannot change your shard key, so it is important to choose correctly.
  prefs: []
  type: TYPE_NORMAL
- en: To choose a good shard key, you need to understand your workload and how your
    shard key is going to distribute your application’s requests. This can be difficult
    to picture, so try to work out some examples—or, even better, try it out on a
    backup dataset with sample traffic. This section has lots of diagrams and explanations,
    but there is no substitute for trying it on your own data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each collection that you’re planning to shard, start by answering the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How many shards are you planning to grow to? A three-shard cluster has a great
    deal more flexibility than a thousand-shard cluster. As a cluster gets larger,
    you should not plan to fire off queries that can hit all shards, so almost all
    queries must include the shard key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you sharding to decrease read or write latency? (Latency refers to how long
    something takes; e.g., a write takes 20 ms, but you need it to take 10 ms.) Decreasing
    write latency usually involves sending requests to geographically closer or more
    powerful machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you sharding to increase read or write throughput? (Throughput refers to
    how many requests the cluster can handle at the same time; e.g., the cluster can
    do 1,000 writes in 20 ms, but you need it to do 5,000 writes in 20 ms.) Increasing
    throughput usually involves adding more parallelization and making sure that requests
    are distributed evenly across the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you sharding to increase system resources (e.g., give MongoDB more RAM per
    GB of data)? If so, you want to keep the working set size as small as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use these answers to evaluate the following shard key descriptions and decide
    whether the shard key you’re considering would work well in your situation. Does
    it give you the targeted queries that you need? Does it change the throughput
    or latency of your system in the ways you need? If you need a compact working
    set, does it provide that?
  prefs: []
  type: TYPE_NORMAL
- en: Picturing Distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common ways people choose to split their data are via ascending, random,
    and location-based keys. There are other types of keys that could be used, but
    most use cases fall into one of these categories. The different types of distributions
    are discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Ascending Shard Keys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ascending shard keys are generally something like a `"date"` field or `ObjectId`—anything
    that steadily increases over time. An autoincrementing primary key is another
    example of an ascending field, albeit one that doesn’t show up in MongoDB much
    (unless you’re importing from another database).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we shard on an ascending field, like `"_id"` on a collection using
    `ObjectId`s. If we shard on `"_id"`, then the data will be split into chunks of
    `"_id"` ranges, as in [Figure 16-1](#ascending-shard-key). These chunks will be
    distributed across our sharded cluster of, let’s say, three shards, as shown in
    [Figure 16-2](#ascending-shard-dist).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-1\. The collection is split into ranges of ObjectIds; each range is
    a chunk
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose we create a new document. Which chunk will it be in? The answer is the
    chunk with the range `ObjectId("5112fae0b4a4b396ff9d0ee5")` through `$maxKey`.
    This is called the *max chunk*, as it is the chunk containing `$maxKey`.
  prefs: []
  type: TYPE_NORMAL
- en: If we insert another document, it will also be in the max chunk. In fact, every
    subsequent insert will be into the max chunk! Every insert’s `"_id"` field will
    be closer to infinity than the previous one (because `ObjectId`s are always ascending),
    so they will all go into the max chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. Chunks are distributed across shards in a random order
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This has a couple of interesting (and often undesirable) properties. First,
    all of your writes will be routed to one shard (*shard0002*, in this case). This
    chunk will be the only one growing and splitting, as it is the only one that receives
    inserts. As you insert data, new chunks will “fall off” of this chunk, as shown
    in [Figure 16-3](#ascending-butt).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. The max chunk continues growing and being split into multiple
    chunks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This pattern often makes it more difficult for MongoDB to keep chunks evenly
    balanced because all the chunks are being created by one shard. Therefore, MongoDB
    must constantly move chunks to other shards instead of correcting the small imbalances
    that might occur in more evenly distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In MongoDB 4.2, the move of the autosplit functionality to the shard primary
    *mongod* added top chunk optimization to address the ascending shard key pattern.
    The balancer will decide in which other shard to place the top chunk. This helps
    avoid a situation in which all new chunks are created on just one shard.
  prefs: []
  type: TYPE_NORMAL
- en: Randomly Distributed Shard Keys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the other end of the spectrum are randomly distributed shard keys. Randomly
    distributed keys could be usernames, email addresses, UUIDs, MD5 hashes, or any
    other key that has no identifiable pattern in your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the shard key is a random number between 0 and 1\. We’ll end up with
    a random distribution of chunks on the various shards, as shown in [Figure 16-4](#random-dist).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. As in the previous section, chunks are distributed randomly around
    the cluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As more data is inserted, the data’s random nature means that inserts should
    hit every chunk fairly evenly. You can prove this to yourself by inserting 10,000
    documents and seeing where they end up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As writes are randomly distributed, the shards should grow at roughly the same
    rate, limiting the number of migrates that need to occur.
  prefs: []
  type: TYPE_NORMAL
- en: The only downside to randomly distributed shard keys is that MongoDB isn’t efficient
    at randomly accessing data beyond the size of RAM. However, if you have the capacity
    or don’t mind the performance hit, random keys nicely distribute load across your
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Location-Based Shard Keys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Location-based shard keys may be things like a user’s IP, latitude and longitude,
    or address. They’re not necessarily related to a physical location field: the
    “location” might be a more abstract way that data should be grouped together.
    In any case, a location-based key is a key where documents with some similarity
    fall into a range based on this field. This can be handy for both putting data
    close to its users and keeping related data together on disk. It may also be a
    legal requirement to remain compliant with GDPR or other similar data privacy
    legislation. MongoDB uses Zoned Sharding to manage this.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In MongoDB 4.0.3+, you can define the zones and the zone ranges prior to sharding
    a collection, which populates chunks for both the zone ranges and for the shard
    key values as well as performing an initial chunk distribution of these. This
    greatly reduces the complexity for sharded zone setup.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we have a collection of documents that are sharded on IP
    address. Documents will be organized into chunks based on their IPs and randomly
    spread across the cluster, as shown in [Figure 16-5](#shard-2-3-0).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-5\. A sample distribution of chunks in the IP address collection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If we wanted certain chunk ranges to be attached to certain shards, we could
    zone these shards and then assign chunk ranges to each zone. In this example,
    suppose that we wanted to keep certain IP blocks on certain shards: say, 56.*.*.*
    (the United States Postal Service’s IP block) on *shard0000* and 17.*.*.* (Apple’s
    IP block) on either *shard0000* or *shard0002*. We do not care where the other
    IPs live. We could request that the balancer do this by setting up zones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This attaches all IPs greater than or equal to 56.0.0.0 and less than 57.0.0.0
    to the shard zoned as `"USPS"`. Next, we add a rule for Apple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When the balancer moves chunks, it will attempt to move chunks with those ranges
    to those shards. Note that this process is not immediate. Chunks that were not
    covered by a zone key range will be moved around normally. The balancer will continue
    attempting to distribute chunks evenly among shards.
  prefs: []
  type: TYPE_NORMAL
- en: Shard Key Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section presents a number of shard key options for various types of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Hashed Shard Key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For loading data as fast as possible, hashed shard keys are the best option.
    A hashed shard key can make any field randomly distributed, so it is a good choice
    if you’re going to be using an ascending key in a lot of queries but want writes
    to be randomly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: The trade-off is that you can never do a targeted range query with a hashed
    shard key. If you will not be doing range queries, though, hashed shard keys are
    a good option.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a hashed shard key, first create a hashed index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, shard the collection with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you create a hashed shard key on a nonexistent collection, `shardCollection`
    behaves interestingly: it assumes that you want evenly distributed chunks, so
    it immediately creates a bunch of empty chunks and distributes them around your
    cluster. For example, suppose our cluster looked like this before creating the
    hashed shard key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Immediately after `shardCollection` returns there are two chunks on each shard,
    evenly distributing the key space across the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that there are no documents in the collection yet, but when you start inserting
    them, writes should be evenly distributed across the shards from the get-go. Ordinarily,
    you would have to wait for chunks to grow, split, and move to start writing to
    other shards. With this automatic priming, you’ll immediately have chunk ranges
    on all shards.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are some limitations on what your shard key can be if you’re using a hashed
    shard key. First, you cannot use the `unique` option. As with other shard keys,
    you cannot use array fields. Finally, be aware that floating-point values will
    be rounded to whole numbers before hashing, so 1 and 1.999999 will both be hashed
    to the same value.
  prefs: []
  type: TYPE_NORMAL
- en: Hashed Shard Keys for GridFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before attempting to shard GridFS collections, make sure that you understand
    how GridFS stores data (see [Chapter 6](ch06.xhtml#chapter-idx-types) for an explanation).
  prefs: []
  type: TYPE_NORMAL
- en: In the following explanation, the term “chunks” is overloaded since GridFS splits
    files into chunks and sharding splits collections into chunks. Thus, the two types
    of chunks are referred to as “GridFS chunks” and “sharding chunks.”
  prefs: []
  type: TYPE_NORMAL
- en: 'GridFS collections are generally excellent candidates for sharding, as they
    contain massive amounts of file data. However, neither of the indexes that are
    automatically created on *fs.chunks* are particularly good shard keys: `{"_id"
    : 1}` is an ascending key and `{"files_id" : 1, "n" : 1}` picks up *fs.files*’s
    `"_id"` field, so it is also an ascending key.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you create a hashed index on the `"files_id"` field, each file
    will be randomly distributed across the cluster, and a file will always be contained
    in a single chunk. This is the best of both worlds: writes will go to all shards
    evenly and reading a file’s data will only ever have to hit a single shard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To set this up, you must create a new index on `{"files_id" : "hashed"}` (as
    of this writing, *mongos* cannot use a subset of the compound index as a shard
    key). Then shard the collection on this field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As a side note, the *fs.files* collection may or may not need to be sharded,
    as it will be much smaller than *fs.chunks*. You can shard it if you would like,
    but it is not likely to be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The Firehose Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have some servers that are more powerful than others, you might want
    to let them handle proportionally more load than your less-powerful servers. For
    example, suppose you have one shard that can handle 10 times the load of your
    other machines. Luckily, you have 10 other shards. You could force all inserts
    to go to the more powerful shard, and then allow the balancer to move older chunks
    to the other shards. This would give lower-latency writes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this strategy, we have to pin the highest chunk to the more powerful
    shard. First, we zone this shard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we pin the current value of the ascending key through infinity to that
    shard, so all new writes go to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now all inserts will be routed to this last chunk, which will always live on
    the shard zoned `"10x"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, ranges from now through infinity will be trapped on this shard unless
    we modify the zone key range. To get around this, we could set up a cron job to
    update the key range once a day, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Then all of the previous day’s chunks would be able to move to other shards.
  prefs: []
  type: TYPE_NORMAL
- en: Another downside of this strategy is that it requires some changes to scale.
    If your most powerful server can no longer handle the number of writes coming
    in, there is no trivial way to split the load between this server and another.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have a high-performance server to firehose into or you are not
    using zone sharding, do not use an ascending key as the shard key. If you do,
    all writes will go to a single shard.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Hotspot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Standalone *mongod* servers are most efficient when doing ascending writes.
    This conflicts with sharding, in that sharding is most efficient when writes are
    spread over the cluster. The technique described here basically creates multiple
    hotspots—optimally several on each shard—so that writes are evenly balanced across
    the cluster but, within a shard, ascending.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this, we use a compound shard key. The first value in the compound
    key is a rough, random value with low-ish cardinality. You can picture each value
    in the first part of the shard key as a chunk, as shown in [Figure 16-6](#rough-key).
    This will eventually work itself out as you insert more data, although it will
    probably never be divided up this neatly (right on the `$minKey` lines). However,
    if you insert enough data, you should eventually have approximately one chunk
    per random value. As you continue to insert data, you’ll end up with multiple
    chunks with the same random value, which brings us to the second part of the shard
    key.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1606.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-6\. A subset of the chunks: each chunk contains a single state and
    a range of “_id” values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The second part of the shard key is an ascending key. This means that within
    a chunk, values are always increasing, as shown in the sample documents in [Figure 16-7](#figure15-7).
    Thus, if you had one chunk per shard, you’d have the perfect setup: ascending
    writes on every shard, as shown in [Figure 16-8](#cafeteria-dist). Of course,
    having *n* chunks with *n* hotspots spread across *n* shards isn’t very extensible:
    add a new shard and it won’t get any writes because there’s no hotspot chunk to
    put on it. Thus, you want a few hotspot chunks per shard (to give you room to
    grow), but not too many. Having a few hotspot chunks will keep the effectiveness
    of ascending writes, but having, say, a thousand hotspots on a shard will end
    up being equivalent to random writes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-7\. A sample list of inserted documents (note that all “_id” values
    are increasing)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. The inserted documents, split into chunks (note that, within each
    chunk, the “_id” values are increasing)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can picture this setup as each chunk being a stack of ascending documents.
    There are multiple stacks on each shard, each ascending until the chunk is split.
    Once a chunk is split, only one of the new chunks will be a hotspot chunk: the
    other chunk will essentially be “dead” and never grow again. If the stacks are
    evenly distributed across the shards, writes will be evenly distributed.'
  prefs: []
  type: TYPE_NORMAL
- en: Shard Key Rules and Guidelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several practical restrictions to be aware of before choosing a shard
    key.
  prefs: []
  type: TYPE_NORMAL
- en: Determining which key to shard on and creating shard keys should be reminiscent
    of indexing because the two concepts are similar. In fact, often your shard key
    may just be the index you use most often (or some variation on it).
  prefs: []
  type: TYPE_NORMAL
- en: Shard Key Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Shard keys cannot be arrays. `sh.shardCollection()` will fail if any key has
    an array value, and inserting an array into that field is not allowed.
  prefs: []
  type: TYPE_NORMAL
- en: Once inserted, a document’s shard key value may be modified unless the shard
    key field is an immutable `_id` field. In older versions of MongoDB prior to 4.2,
    it was not possible to modify a document’s shard key value.
  prefs: []
  type: TYPE_NORMAL
- en: Most special types of indexes cannot be used for shard keys. In particular,
    you cannot shard on a geospatial index. Using a hashed index for a shard key is
    allowed, as covered previously.
  prefs: []
  type: TYPE_NORMAL
- en: Shard Key Cardinality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether your shard key jumps around or increases steadily, it is important to
    choose a key with values that will vary. As with indexes, sharding performs better
    on high-cardinality fields. If, for example, you had a `"logLevel"` key that had
    only values `"DEBUG"`, `"WARN"`, or `"ERROR"`, MongoDB wouldn’t be able to break
    up your data into more than three chunks (because there would be only three different
    values for the shard key). If you have a key with little variation and want to
    use it as a shard key anyway, you can do so by creating a compound shard key on
    that key and a key that varies more, like `"logLevel"` and `"timestamp"`. It is
    important that the combination of keys has high cardinality.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling Data Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, automatic data distribution will not fit your requirements. This
    section gives you some options beyond choosing a shard key and allowing MongoDB
    to do everything automatically.
  prefs: []
  type: TYPE_NORMAL
- en: As your cluster gets larger or busier, these solutions become less practical.
    However, for small clusters, you may want more control.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Cluster for Multiple Databases and Collections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MongoDB evenly distributes collections across every shard in your cluster, which
    works well if you’re storing homogeneous data. However, if you have a log collection
    that is “lower value” than your other data, you might not want it taking up space
    on your more expensive servers. Or, if you have one powerful shard, you might
    want to use it for only a real-time collection and not allow other collections
    to use it. You can create separate clusters, but you can also give MongoDB specific
    directions about where you want it to put certain data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set this up, use the `sh.addShardToZone()` helper in the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can assign different collections to different shards. For instance,
    for your super-important real-time collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This says, “for negative infinity to infinity for this collection, store it
    on shards tagged `"high"`.” This means that no data from the *super.important*
    collection will be stored on any other server. Note that this does not affect
    how other collections are distributed: they will still be evenly distributed between
    this shard and the others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can perform a similar operation to keep the log collection on a low-quality
    server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The log collection will now be split evenly between *shard0004* and *shard0005*.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning a zone key range to a collection does not affect it instantly. It
    is an instruction to the balancer stating that, when it runs, these are the viable
    targets to move the collection to. Thus, if the entire log collection is on *shard0002*
    or evenly distributed among the shards, it will take a little while for all of
    the chunks to be migrated to *shard0004* and *shard0005*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, perhaps you have a collection that you don’t want on the
    shard zoned `"high"`, but you don’t care which other shard it goes on. You can
    zone all of the non-high-performance shards to create a new grouping. Shards can
    have as many zones as you need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can specify that you want this collection (call it *normal.coll*) distributed
    across these five shards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You cannot assign collections dynamically—i.e., you can’t say, “when a collection
    is created, randomly home it to a shard.” However, you could have a cron job that
    went through and did this for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you make a mistake or change your mind, you can remove a shard from a zone
    with `sh.removeShardFromZone()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If you remove all shards from zones described by a zone key range (e.g., if
    you remove *shard0000* from the zone `"high"`), the balancer won’t distribute
    the data anywhere because there aren’t any valid locations listed. All the data
    will still be readable and writable; it just won’t be able to migrate until you
    modify your tags or tag ranges.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove a key range from a zone, use `sh.removeRangeFromZone()`. The following
    is an example. The range specified must be an exact match to a range previously
    defined for the namespace *some.logs* and a given zone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Manual Sharding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, for complex requirements or special situations, you may prefer to
    have complete control over which data is distributed where. You can turn off the
    balancer if you don’t want data to be automatically distributed and use the `moveChunk`
    command to manually distribute data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To turn off the balancer, connect to a *mongos* (any *mongos* is fine) using
    the *mongo* shell and disable the balancer using the shell helper `sh.stopBalancer()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If there is currently a migrate in progress, this setting will not take effect
    until the migrate has completed. However, once any in-flight migrations have finished,
    the balancer will stop moving data around. To verify no migrations are in progress
    after disabling, issue the following in the *mongo* shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the balancer is off, you can move data around manually (if necessary).
    First, find out which chunks are where by looking at *config.chunks*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the `moveChunk` command to migrate chunks to other shards. Specify
    the lower bound of the chunk to be migrated and give the name of the shard that
    you want to move the chunk to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: However, unless you are in an exceptional situation, you should use MongoDB’s
    automatic sharding instead of doing it manually. If you end up with a hotspot
    on a shard that you weren’t expecting, you might end up with most of your data
    on that shard.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, do not combine setting up unusual distributions manually with
    running the balancer. If the balancer detects an uneven number of chunks it will
    simply reshuffle all of your work to get the collection evenly balanced again.
    If you want uneven distribution of chunks, use the zone sharding technique discussed
    in [“Using a Cluster for Multiple Databases and Collections”](#shard-tags).
  prefs: []
  type: TYPE_NORMAL
