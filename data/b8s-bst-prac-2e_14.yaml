- en: Chapter 14\. Running Machine Learning in Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 在 Kubernetes 中运行机器学习
- en: The age of microservices, distributed systems, and the cloud has provided the
    perfect environmental conditions for the democratization of machine learning models
    and tooling. Infrastructure at scale has now become commoditized, and the tooling
    around the machine learning ecosystem is maturing. Kubernetes is one of the platforms
    that has become increasingly popular among developers, data scientists, and the
    wider open source community as the perfect environment to enable the machine learning
    workflow and life cycle. Large machine learning models like [GPT-4](https://oreil.ly/sGzRc)
    and [DALL·E](https://oreil.ly/zTWNx) have brought machine learning into the spotlight
    and organizations like [OpenAI](https://oreil.ly/bCXwF) have been very public
    about their use of Kubernetes to support these models. In this chapter, we will
    cover why Kubernetes is a great platform for machine learning and provide best
    practices for both cluster administrators and data scientists alike on how to
    get the most out of Kubernetes when running machine learning workloads. Specifically,
    we focus on deep learning rather than traditional machine learning because deep
    learning has quickly become the area of innovation on platforms like Kubernetes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务时代、分布式系统和云计算为机器学习模型和工具的民主化提供了完美的环境条件。规模化基础设施现在已经成为商品化，围绕机器学习生态系统的工具正在成熟。Kubernetes
    是其中之一，作为一个平台，它在开发者、数据科学家和更广泛的开源社区中变得越来越流行，成为促进机器学习工作流和生命周期的理想环境。像[GPT-4](https://oreil.ly/sGzRc)和[DALL·E](https://oreil.ly/zTWNx)这样的大型机器学习模型已经将机器学习推上了舞台，而像[OpenAI](https://oreil.ly/bCXwF)这样的组织也公开表明它们使用
    Kubernetes 来支持这些模型。在本章中，我们将探讨为什么 Kubernetes 是一个优秀的机器学习平台，并为集群管理员和数据科学家提供关于在 Kubernetes
    上运行机器学习工作负载时如何获取最大收益的最佳实践。具体而言，我们专注于深度学习而不是传统机器学习，因为深度学习已经迅速成为像 Kubernetes 这样平台上创新的领域。
- en: Why Is Kubernetes Great for Machine Learning?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 为什么是机器学习的绝佳选择？
- en: 'Kubernetes has quickly become the home for rapid innovation in deep learning.
    The confluence of tooling and libraries such as [TensorFlow](https://oreil.ly/nzHaG)
    makes this technology more accessible to a large audience of data scientists.
    What makes Kubernetes such a great place to run your deep learning workloads?
    Let’s cover what Kubernetes provides:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 快速成为深度学习的创新之地。工具和库（如[TensorFlow](https://oreil.ly/nzHaG)）的结合使得这项技术更加易于大众使用。那么，是什么让
    Kubernetes 成为运行深度学习工作负载的绝佳选择呢？让我们看看 Kubernetes 提供了哪些功能：
- en: Ubiquitous
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 普遍存在
- en: Kubernetes is everywhere. All the major public clouds support it, and there
    are distributions for private clouds and infrastructure. Basing ecosystem tooling
    on a platform like Kubernetes allows users to run their deep learning workloads
    anywhere.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 无处不在。所有主要的公共云都支持它，还有专门为私有云和基础设施提供的发行版。将生态系统工具基于 Kubernetes 这样的平台，让用户可以在任何地方运行他们的深度学习工作负载。
- en: Scalable
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展
- en: Deep learning workflows typically need access to large amounts of computing
    power to efficiently train machine learning models. Kubernetes ships with native
    autoscaling capabilities that make it easy for data scientists to achieve and
    fine-tune the level of scale they need to train their models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习工作流通常需要大量的计算资源来高效训练机器学习模型。Kubernetes 自带本地自动缩放功能，使得数据科学家能够轻松实现和微调他们需要的规模级别来训练模型。
- en: Extensible
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展
- en: Efficiently training a machine learning model typically requires access to specialized
    hardware. Kubernetes allows cluster administrators to quickly and easily expose
    new types of hardware to the scheduler without having to change the Kubernetes
    source code. It also allows custom resources and controllers to be seamlessly
    integrated into the Kubernetes API to support specialized workflows, such as hyperparameter
    tuning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高效地训练机器学习模型通常需要访问专门的硬件。Kubernetes 允许集群管理员快速简便地向调度器公开新类型的硬件，而无需修改 Kubernetes
    源代码。它还允许自定义资源和控制器无缝集成到 Kubernetes API 中，以支持特定的工作流程，如超参数调整。
- en: Self-service
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自助式
- en: Data scientists can use Kubernetes to perform self-service machine learning
    workflows on demand, without needing specialized knowledge of Kubernetes itself.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家可以利用 Kubernetes 实现按需自助式的机器学习工作流程，无需专门了解 Kubernetes 本身。
- en: Portable
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 可移植
- en: Machine learning models can be run anywhere, provided that the tooling is based
    on the Kubernetes API. This allows machine learning workloads to be portable across
    Kubernetes providers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以在任何地方运行，只要工具基于 Kubernetes API。这使得机器学习工作负载可以跨 Kubernetes 提供者可移植。
- en: Machine Learning Workflow
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: To effectively understand the needs of deep learning, you must understand the
    complete machine learning workflow. [Figure 14-1](#machine_learning_development_workflow)
    represents a simplified workflow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效地理解深度学习的需求，您必须了解完整的机器学习工作流程。[图 14-1](#machine_learning_development_workflow)
    表示了一个简化的工作流程。
- en: '![Machine learning development workflow](assets/kbp2_1401.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习开发工作流程](assets/kbp2_1401.png)'
- en: Figure 14-1\. Machine learning development workflow
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-1\. 机器学习开发工作流程
- en: 'As you can see, the workflow has the following phases:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，工作流程包括以下阶段：
- en: Dataset preparation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备
- en: This phase includes the storage, indexing, cataloging, and metadata associated
    with the dataset used to train the model. For the purposes of this book, we consider
    only the storage aspect. Datasets vary in size, from hundreds of megabytes to
    hundreds of terabytes, and even petabytes, and need to be provided to the model
    in order for the model to be trained. You must consider storage that provides
    the appropriate properties to meet these needs. Typically, large-scale block and
    object stores are required and must be accessible via Kubernetes-native storage
    abstractions or directly accessible APIs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段包括用于训练模型的数据集的存储、索引、编目和元数据。对于本书的目的，我们只考虑存储方面。数据集的大小各不相同，从几百兆字节到数百 TB，甚至 PB
    不等，并且需要将其提供给模型以进行训练。您必须考虑提供适当属性以满足这些需求的存储。通常需要大规模的块和对象存储，并且必须通过 Kubernetes 本地存储抽象或直接可访问的
    API 进行访问。
- en: Model development
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发
- en: In this phase, data scientists write, share, and collaborate on machine learning
    algorithms. Open source tools like JupyterHub are easy to install on Kubernetes
    because they typically function like any other workload.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，数据科学家编写、分享并协作机器学习算法。像 JupyterHub 这样的开源工具易于在 Kubernetes 上安装，因为它们通常像任何其他工作负载一样运行。
- en: Training
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: For a model to use the dataset to learn how to perform the tasks it’s designed
    to perform, it must be trained. The resulting artifact of the training process
    is usually a checkpoint of the trained model state. The training process is the
    piece that takes advantage of all the capabilities of Kubernetes at the same time.
    Scheduling, access to specialized hardware, dataset volume management, scaling,
    and networking will all be exercised in unison to complete this task. We cover
    more of the specifics of the training phase in the next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要使模型使用数据集学习执行其设计的任务，必须对其进行训练。训练过程的结果通常是训练模型状态的检查点。训练过程是利用 Kubernetes 所有功能的部分。调度、访问专用硬件、数据集卷管理、扩展和网络将同时运行以完成此任务。我们将在下一节中详细介绍训练阶段的具体内容。
- en: Serving
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 服务
- en: This is the process of making the trained model accessible to service requests
    from clients so that it can make an inference based on the data supplied from
    the client. For example, if you have an image-recognition model that’s been trained
    to detect dogs and cats, a client might submit a picture of a dog, and the model
    should be able to determine whether it is a dog, with a certain level of accuracy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将训练好的模型对客户端的服务请求进行访问，以便根据客户端提供的数据进行推断的过程。例如，如果您有一个经过训练以检测狗和猫的图像识别模型，客户端可能会提交一张狗的图片，模型应该能够以一定的准确性确定它是否是一只狗。
- en: Machine Learning for Kubernetes Cluster Admins
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 集群管理员的机器学习
- en: There are a few topics to consider before running machine learning workloads
    on your Kubernetes cluster. This section is specifically targeted to cluster administrators.
    The largest challenge you will face as a cluster administrator responsible for
    a team of data scientists is understanding the terminology. There are myriad new
    terms that you must become familiar with over time, but rest assured, you can
    do it. Let’s look at the main problem areas you’ll need to address when preparing
    a cluster for machine learning workloads.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行机器学习工作负载之前，有几个主题需要考虑。本节专门针对集群管理员。作为负责数据科学家团队的集群管理员，您将面临的最大挑战是理解术语。随着时间的推移，您必须熟悉许多新术语，但请放心，您可以做到。让我们看看准备集群以运行机器学习工作负载时需要解决的主要问题领域。
- en: Model Training on Kubernetes
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上训练模型
- en: Training machine learning models on Kubernetes requires conventional CPUs and
    graphics processing units (GPUs). Typically, the more resources you apply, the
    faster the training will be completed. In most cases, model training can be achieved
    on a single machine that has the required resources. Many cloud providers offer
    multi-GPU virtual machine (VM) types, so we recommend scaling VMs vertically to
    four to eight GPUs before looking into distributed training. Data scientists use
    a technique known as *hyperparameter tuning* when training models. A hyperparameter
    is simply a parameter that has a set value before the training process begins.
    Hyperparameter tuning is the process of finding the optimal set of hyperparameters
    for model training. The technique involves running many of the same training jobs
    with a different set of hyperparameters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上训练机器学习模型需要常规的CPU和图形处理单元（GPU）。通常情况下，你应用的资源越多，训练完成的速度就越快。在大多数情况下，可以在具备所需资源的单台机器上完成模型训练。许多云服务提供商提供多GPU虚拟机（VM）类型，因此我们建议在考虑分布式训练之前，将VM垂直扩展到四到八个GPU。数据科学家在训练模型时使用一种称为
    *超参数调整* 的技术。超参数是在训练过程开始之前就设定好的参数。超参数调整是寻找模型训练的最佳超参数组合的过程。该技术涉及使用不同的超参数集合运行多个相同的训练作业。
- en: Training your first model on Kubernetes
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上训练你的第一个模型
- en: In this example, you are going to use the MNIST dataset to train an image-classification
    model. The MNIST dataset is publicly available and commonly used for image classification.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你将使用 MNIST 数据集来训练一个图像分类模型。MNIST 数据集是公开可用的，常用于图像分类。
- en: 'To train the model, you need GPUs. Let’s confirm that your Kubernetes cluster
    has GPUs available. The following command shows how many GPUs are available in
    a Kubernetes cluster. From the output we can see that this cluster has four GPUs
    available:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，你需要GPU。让我们确认你的 Kubernetes 集群中是否有可用的GPU。以下命令显示了 Kubernetes 集群中有多少个GPU可用。从输出中可以看出，这个集群有四个GPU可用：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Given that training is a batch workload, to run your training you’re going
    to use the `Job` kind in Kubernetes. You will run your training for 500 steps
    and use a single GPU. Create a file called *mnist-demo.yaml* using the following
    manifest, and save it to your filesystem:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于训练是批量工作负载，你将会在 Kubernetes 中使用 `Job` 类型来运行你的训练。你将进行 500 步的训练，并使用单个GPU。创建一个名为
    *mnist-demo.yaml* 的文件，使用以下清单，并将其保存到你的文件系统中：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, create this resource on your Kubernetes cluster:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在你的 Kubernetes 集群上创建这个资源：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Check the status of the job you just created:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 检查刚刚创建的作业的状态：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you look at the pods, you should see the training job running:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查看 Pods，你应该可以看到训练作业正在运行：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Looking at the pod logs, you can see the training happening:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 Pod 日志，你可以看到训练正在进行：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, you can see that the training has completed by looking at the job
    status:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过查看作业状态，你可以看到训练已经完成：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To clean up the training job, simply run the following command:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要清理训练作业，只需运行以下命令：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Congratulations! You just ran your first model training job on Kubernetes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚在 Kubernetes 上运行了你的第一个模型训练作业。
- en: Distributed Training on Kubernetes
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 上的分布式训练
- en: Distributed training is still in its infancy and is difficult to optimize. Running
    a training job that requires eight GPUs will almost always be faster to train
    on a single eight-GPU machine compared to two machines with four GPUs each. The
    only time you should resort to using distributed training is when the model doesn’t
    fit on the biggest machine available. If you are certain you must run distributed
    training, it is important to understand the architecture. [Figure 14-2](#distributed_tensorflow_architecture)
    depicts the distributed TensorFlow architecture, and you can see how the model
    and the parameters are distributed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练目前还处于初期阶段，优化起来相当困难。如果运行一个需要八个GPU的训练作业，与每台有四个GPU的两台机器相比，几乎总是在单台有八个GPU的机器上进行训练速度更快。唯一应当采用分布式训练的时候是当模型无法适应当前最大的单台机器时。如果确实需要运行分布式训练，理解架构是非常重要的。[图 14-2](#distributed_tensorflow_architecture)
    展示了分布式 TensorFlow 的架构，你可以看到模型和参数是如何分布的。
- en: '![Distributed TensorFlow architecture](assets/kbp2_1402.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![分布式 TensorFlow 架构](assets/kbp2_1402.png)'
- en: Figure 14-2\. Distributed TensorFlow architecture
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-2\. 分布式 TensorFlow 架构
- en: Resource Constraints
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源约束
- en: Machine learning workloads demand very specific configurations across all aspects
    of your cluster. The training phases are most certainly the most resource intensive.
    It’s also important to note, as we mentioned a moment ago, that machine learning
    algorithm training is almost always a batch-style workload. Specifically, it will
    have a start time and a finish time. The finish time of a training run depends
    on how quickly you can meet the resource requirements of the model training. This
    means that scaling is almost certainly a quicker way to finish training jobs faster,
    but scaling has its own set of bottlenecks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载在集群的所有方面都需求非常具体的配置。训练阶段肯定是最资源密集的。还值得注意的是，正如我们刚提到的，机器学习算法的训练几乎总是批量式工作负载。具体来说，它将有一个开始时间和一个完成时间。训练运行的完成时间取决于您能多快满足模型训练的资源需求。这意味着扩展几乎肯定是更快完成训练工作的一种方式，但扩展也有其自身的一系列瓶颈。
- en: Specialized Hardware
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专用硬件
- en: 'Training and serving a model is almost always more efficient on specialized
    hardware. A typical example of such specialized hardware would be commodity GPUs.
    Kubernetes allows you to access GPUs via device plug-ins that make the GPU resource
    known to the Kubernetes scheduler and therefore able to be scheduled. A device
    plug-in framework facilitates this capability, which means that vendors do not
    need to modify the core Kubernetes code to implement their specific device. These
    device plug-ins typically run on each node as DaemonSets, which are processes
    that are responsible for advertising these specific resources to the Kubernetes
    API. Let’s look at the [NVIDIA device plug-in for Kubernetes](https://oreil.ly/RgKuz),
    which enables access to NVIDIA GPUs. After they’re running, you can create a pod
    as follows, and Kubernetes will ensure that it is scheduled to a node that has
    these resources available:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和服务模型几乎总是在专用硬件上更为高效。这类专用硬件的典型示例包括通用 GPU。Kubernetes 允许您通过设备插件访问 GPU，这些插件使 GPU
    资源对 Kubernetes 调度器可见，从而可以进行调度。设备插件框架促进了这一能力，这意味着厂商无需修改核心 Kubernetes 代码来实现他们的特定设备。这些设备插件通常作为
    DaemonSets 在每个节点上运行，负责向 Kubernetes API 广告这些特定资源。让我们看看用于 Kubernetes 的[NVIDIA 设备插件](https://oreil.ly/RgKuz)，它使得访问
    NVIDIA GPU 成为可能。一旦它们运行起来，您可以创建一个 Pod 如下，Kubernetes 将确保它被调度到拥有这些资源的节点上：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Device plug-ins are not limited to GPUs; you can use them wherever specialized
    hardware is needed—for example, Field Programmable Gate Arrays (FPGAs) or InfiniBand.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 设备插件不仅限于 GPU；您可以在需要专用硬件的任何地方使用它们，例如可编程门阵列（FPGA）或 InfiniBand。
- en: Scheduling idiosyncrasies
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度的特殊性
- en: It’s important to note that Kubernetes cannot make decisions about resources
    that it does not have knowledge about. One of the things you might notice is that
    the GPUs are not running at capacity when you are training. You are therefore
    not achieving the level of utilization you would like to see. Let’s consider the
    previous example; it exposes only the number of GPU cores and omits the number
    of threads that can be run per core. It also doesn’t expose which bus the GPU
    core is on, so that jobs that need access to one another or to the same memory
    might be colocated on the same Kubernetes nodes. All these considerations might
    be addressed by device plug-ins in the future but for now might leave you wondering
    why you cannot get 100% utilization on that beefy GPU you just purchased. It’s
    also worth mentioning that you cannot request fractions of GPUs (for example,
    0.1), which means that even if the specific GPU supports running multiple threads
    concurrently, you will not be able to utilize that capacity.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Kubernetes 无法对其没有知识的资源做出决策。在训练时，您可能会注意到 GPU 的利用率未达到最大。因此，您没有达到希望看到的利用率水平。让我们考虑之前的例子；它仅公开了
    GPU 核心的数量，但省略了每个核心可运行的线程数量。它也没有公开 GPU 核心所在的总线，因此需要访问彼此或相同内存的作业可能会共同放置在相同的 Kubernetes
    节点上。所有这些考虑可能会在未来由设备插件解决，但目前可能会让您想知道为什么无法在刚购买的强大 GPU 上获得 100% 的利用率。还值得一提的是，您无法请求
    GPU 的分数（例如 0.1），这意味着即使特定的 GPU 支持同时运行多个线程，您也无法利用该容量。
- en: Libraries, Drivers, and Kernel Modules
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 库、驱动程序和内核模块
- en: 'To access specialized hardware, you typically need purpose-built libraries,
    drivers, and kernel modules. You will need to ensure that these are mounted into
    the container runtime so that they are available to the tooling running in the
    container. You might ask, “Why don’t I just add these to the container image itself?”
    The answer is simple: the tools need to match the version on the underlying host
    and must be configured appropriately for that specific system. Container runtimes
    such as [NVIDIA Docker](https://oreil.ly/Re0Ef) remove the burden of having to
    map host volumes into each container. In lieu of having a purpose-built container
    runtime, you might be able to build an admission webhook that provides the same
    functionality. It’s also important to consider that you might need privileged
    containers to access some specialized hardware, which affects the cluster security
    profile. The installation of the associated libraries, drivers, and kernel modules
    might also be facilitated by Kubernetes device plug-ins. Many device plug-ins
    run checks on each machine to confirm that all installations have been completed
    before they advertise the schedulable GPU resources to the Kubernetes scheduler.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问专用硬件，通常需要专为其设计的库、驱动程序和内核模块。您需要确保这些内容被挂载到容器运行时中，以便工具能够在容器中运行时使用。您可能会问：“为什么不直接将这些内容添加到容器映像本身？”答案很简单：这些工具需要与底层主机上的版本匹配，并且必须针对特定系统进行适当配置。像[NVIDIA
    Docker](https://oreil.ly/Re0Ef)这样的容器运行时消除了将主机卷映射到每个容器中的负担。如果没有专为其设计的容器运行时，您可能可以构建一个准入Webhook，提供相同的功能。还要考虑的重要因素是，您可能需要特权容器才能访问某些专用硬件，这会影响集群安全配置文件。安装相关库、驱动程序和内核模块也可能通过Kubernetes设备插件来实现。许多设备插件在每台机器上运行检查，以确认所有安装已完成，然后将可调度的GPU资源广告到Kubernetes调度程序。
- en: Storage
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: 'Storage is one of the most critical aspects of the machine learning workflow.
    You need to consider storage because it directly affects the following pieces
    of the machine learning workflow:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 存储是机器学习工作流程中最关键的方面之一。您需要考虑存储，因为它直接影响机器学习工作流程的以下部分：
- en: Dataset storage and distribution among nodes during training
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练期间数据集的存储和在节点之间的分发
- en: Checkpoints and saving models
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点和模型保存
- en: Dataset storage and distribution among nodes during training
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练期间数据集的存储和在节点之间的分发
- en: During training, the dataset must be retrievable by every node. The storage
    needs are read-only, and, typically, the faster the disk, the better. The type
    of disk that’s providing the storage is almost completely dependent on the size
    of the dataset. Datasets of hundreds of megabytes or gigabytes might be perfect
    for block storage, but datasets that are several or hundreds of terabytes in size
    might be better suited to object storage. Depending on the size and location of
    the disks that hold the datasets, there might be a performance hit on your networking.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，每个节点必须能够检索数据集。存储需求是只读的，通常来说，磁盘越快越好。提供存储的磁盘类型几乎完全取决于数据集的大小。数百兆或几千兆字节大小的数据集可能非常适合块存储，但几百至几千兆字节大小的数据集可能更适合对象存储。根据存放数据集的磁盘的大小和位置，可能会对网络性能造成影响。
- en: Checkpoints and saving models
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查点和模型保存
- en: Checkpoints are created as a model is being trained, and saving models allows
    you to use them for serving. In both cases, you need storage attached to each
    of the nodes to store this data. The data is typically stored under a single directory,
    and each node is writing to a specific checkpoint or save file. Most tools expect
    the checkpoint and save data to be in a single location and require `ReadWriteMany`.
    `ReadWriteMany` simply means that the volume can be mounted as read-write by many
    nodes. When using Kubernetes PersistentVolumes, you will need to determine the
    best storage platform for your needs. The Kubernetes documentation keeps a [list](https://oreil.ly/aMjGd)
    of volume plug-ins that support `ReadWriteMany`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型正在训练时会创建检查点，并且保存模型使您可以将其用于服务。在这两种情况下，您需要每个节点连接的存储来存储这些数据。数据通常存储在单个目录下，每个节点都在向特定的检查点或保存文件写入。大多数工具期望检查点和保存数据位于单个位置，并且需要`ReadWriteMany`。`ReadWriteMany`简单地意味着该卷可以被多个节点挂载为读写。在使用Kubernetes
    PersistentVolumes时，您需要确定适合您需求的最佳存储平台。Kubernetes文档保留了一个[列表](https://oreil.ly/aMjGd)，列出支持`ReadWriteMany`的卷插件。
- en: Networking
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络
- en: 'The training phase of the machine learning workflow has a large impact on the
    network (specifically, when running distributed training). If we consider TensorFlow’s
    distributed architecture, two discrete phases create a lot of network traffic:
    variable distribution from each of the parameter servers to each of the nodes,
    and the application of gradients from each node back to the parameter server (refer
    back to [Figure 14-2](#distributed_tensorflow_architecture)). The time it takes
    for this exchange to happen directly affects the time it takes to train a model.
    So, it’s a simple game of the faster, the better (within reason, of course). With
    most public clouds and servers today supporting 1-Gbps, 10-Gbps, and sometimes
    40-Gbps network interface cards, generally network bandwidth is a concern only
    at lower bandwidths. You might also consider InfiniBand if you need high network
    bandwidth.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流的训练阶段对网络有很大影响（特别是在运行分布式训练时）。如果我们考虑TensorFlow的分布式架构，两个离散阶段会产生大量网络流量：从每个参数服务器到每个节点的变量分布，以及从每个节点到参数服务器的梯度应用（参见[图14-2](#distributed_tensorflow_architecture)）。这种交换所需的时间直接影响模型训练所需的时间。所以，这是一个简单的游戏：越快越好（当然，合理范围内）。如今大多数公共云和服务器支持1-Gbps、10-Gbps甚至40-Gbps的网络接口卡，通常网络带宽只在较低带宽下成为问题。如果需要高网络带宽，可以考虑InfiniBand。
- en: While raw network bandwidth is more often than not a limiting factor, in some
    instances the problem is getting the data onto the wire from the kernel in the
    first place. Some open source projects take advantage of Remote Direct Memory
    Access (RDMA) to further accelerate network traffic without the need to modify
    your nodes or application code. RDMA allows computers in a network to exchange
    data in main memory without using the processor, cache, or operating system of
    either computer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原始网络带宽往往是限制因素，但在某些情况下，问题是如何将数据首先从内核传输到线上。一些开源项目利用远程直接内存访问（RDMA）进一步加速网络流量，而无需修改节点或应用程序代码。RDMA允许网络中的计算机在不使用任何计算机的处理器、缓存或操作系统的情况下在主内存中交换数据。
- en: Specialized Protocols
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专用协议
- en: 'Other specialized protocols you can consider when using machine learning on
    Kubernetes are often vendor specific, but they all seek to address distributed
    training scaling issues by removing areas of the architecture that quickly become
    bottlenecks. For example, parameter servers. These protocols often allow the direct
    exchange of information between GPUs on multiple nodes without the need to involve
    the node CPU and OS. Here are a couple you might want to look into to more efficiently
    scale your distributed training:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Kubernetes上使用机器学习时，您可以考虑其他专用协议，这些协议通常是特定供应商的，但它们都致力于通过消除架构中快速成为瓶颈的区域来解决分布式训练扩展问题。例如，参数服务器。这些协议通常允许多节点上的GPU直接交换信息，而无需涉及节点CPU和操作系统。以下是您可能希望了解的一些协议，以更有效地扩展您的分布式训练：
- en: Message Passing Interface (MPI)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递接口（MPI）
- en: A standardized portable API for the transfer of data between distributed processes
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在分布式进程之间传输数据的标准便携式API
- en: NVIDIA Collective Communications Library (NCCL)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA集体通信库（NCCL）
- en: A library of topology-aware multi-GPU communication primitives
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有拓扑感知的多GPU通信基元库
- en: Data Scientist Concerns
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学家的关注点
- en: 'Earlier in the chapter, we shared considerations you need to make in order
    to be able to run machine learning workloads on your Kubernetes cluster. But what
    about the data scientist? Here we cover some popular tools that make it easy for
    data scientists to utilize Kubernetes for machine learning without having to be
    a Kubernetes expert:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们分享了您需要考虑的一些内容，以便能够在Kubernetes集群上运行机器学习工作负载。但数据科学家如何？在这里，我们涵盖了一些流行工具，这些工具使数据科学家能够在不必成为Kubernetes专家的情况下利用Kubernetes进行机器学习：
- en: '[Kubeflow](https://oreil.ly/UVxjM)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kubeflow](https://oreil.ly/UVxjM)'
- en: A machine learning toolkit for Kubernetes, it is native to Kubernetes and ships
    with several tools necessary to complete the machine learning workflow. Tools
    such as Jupyter Notebooks, pipelines, and Kubernetes-native controllers make it
    simple and easy for data scientists to get the most out of Kubernetes as a platform
    for machine learning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个针对Kubernetes的机器学习工具包，它是Kubernetes原生的，并配备了完成机器学习工作流所需的几个工具。诸如Jupyter Notebooks、流水线和Kubernetes本地控制器等工具，使数据科学家能够简单轻松地充分利用Kubernetes作为机器学习平台。
- en: '[Polyaxon](https://oreil.ly/NZ7Nj)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[Polyaxon](https://oreil.ly/NZ7Nj)'
- en: A tool for managing machine learning workflows that supports many popular libraries
    and runs on any Kubernetes cluster. Polyaxon has both commercial and open source
    offerings.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一种管理机器学习工作流程的工具，支持许多流行的库，并在任何 Kubernetes 集群上运行。Polyaxon 提供商业和开源版本。
- en: '[Pachyderm](https://oreil.ly/CivM_)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pachyderm](https://oreil.ly/CivM_)'
- en: An enterprise-ready data science platform with a rich suite of tools for dataset
    preparation, life cycle, and versioning, along with the ability to build machine
    learning pipelines. Pachyderm has a commercial offering you can deploy to any
    Kubernetes cluster.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个成熟的企业级数据科学平台，具备丰富的数据集准备、生命周期管理和版本控制工具，以及构建机器学习管道的能力。Pachyderm 提供一种商业解决方案，您可以部署到任何
    Kubernetes 集群上。
- en: Machine Learning on Kubernetes Best Practices
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 上的机器学习最佳实践
- en: 'To achieve optimal performance for your machine learning workloads, consider
    the following best practices:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现机器学习工作负载的最佳性能，考虑以下最佳实践：
- en: Smart scheduling and autoscaling
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 智能调度和自动缩放
- en: 'Given that most stages of the machine learning workflow are batch by nature,
    we recommend you utilize a Cluster Autoscaler. GPU-enabled hardware is costly,
    and you certainly do not want to be paying for it when it’s not in use. We recommend
    batching jobs to run at specific times using either taints and tolerations or
    via a time-specific Cluster Autoscaler. That way, the cluster can scale to the
    needs of the machine learning workloads when needed, and not a moment sooner.
    Regarding taints and tolerations, upstream convention is to taint the node with
    the extended resource as the key. For example, a node with NVIDIA GPUs should
    be tainted as follows: `Key: nvidia.com/gpu, Effect: NoSchedule`. Using this method
    means you can also utilize the `ExtendedResourceToleration` admission controller,
    which will automatically add the appropriate tolerations for such taints to pods
    requesting extended resources so that the users don’t need to add them manually.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到机器学习工作流程的大多数阶段本质上是批处理的，我们建议您使用集群自动缩放器。启用 GPU 的硬件成本高昂，当它们不被使用时，您肯定不希望为其付费。我们建议通过特定时间批量运行作业，使用
    taints 和 tolerations 或特定时间的集群自动缩放器。这样，集群可以根据机器学习工作负载的需求进行伸缩，而不是早了一步。关于 taints
    和 tolerations，上游约定是将带有扩展资源的节点 taint。例如，具有 NVIDIA GPU 的节点应按以下方式 tainted：`Key: nvidia.com/gpu,
    Effect: NoSchedule`。使用此方法意味着您还可以利用 `ExtendedResourceToleration` Admission Controller，该控制器将为请求扩展资源的
    Pod 自动添加适当的 tolerations，从而用户无需手动添加它们。'
- en: The truth is that model training is a delicate balance
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，模型训练是一个微妙的平衡
- en: Allowing things to move faster in one area often leads to bottlenecks in others.
    It’s an endeavor of constant observation and tuning. As a general rule, we recommend
    you try to make the GPU become the bottleneck because it is the most costly resource.
    Keep your GPUs saturated. Be prepared to always be on the lookout for bottlenecks,
    and set up your monitoring to track the GPU, CPU, network, and storage utilization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 允许某个区域运行更快通常会导致其他区域的瓶颈。这是一个持续观察和调整的努力。作为一个通用规则，我们建议您尝试让 GPU 成为瓶颈，因为它是最昂贵的资源。保持
    GPU 的饱和。要时刻准备好寻找瓶颈，并设置监控以跟踪 GPU、CPU、网络和存储的利用率。
- en: Mixed workload clusters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 混合工作负载集群
- en: Clusters that are used to run the day-to-day business services might also be
    used for machine learning. Given the high performance requirements of machine
    learning workloads, we recommend using a separate node pool that’s tainted to
    accept only machine learning workloads. This will help protect the rest of the
    cluster from any impact from the machine learning workloads running on the machine
    learning node pool. Furthermore, you should consider multiple GPU-enabled node
    pools, each with different performance characteristics to suit the workload types.
    We also recommend enabling node autoscaling on the machine learning node pool(s).
    Use mixed mode clusters only after you have a solid understanding of the performance
    impact that your machine learning workloads have on your cluster.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 用于日常业务服务的集群也可能用于机器学习。考虑到机器学习工作负载的高性能要求，我们建议使用一个专用的节点池，专门用于接受机器学习工作负载。这将有助于保护集群的其他部分免受运行在机器学习节点池上的机器学习工作负载的任何影响。此外，您应考虑多个支持
    GPU 的节点池，每个节点池具有不同的性能特征以适应不同的工作负载类型。我们还建议在机器学习节点池上启用节点自动缩放。仅在您充分了解机器学习工作负载对集群性能影响之后，才使用混合模式集群。
- en: Achieving linear scaling with distributed training
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实现分布式训练的线性扩展
- en: This is the holy grail of distributed model training. Most libraries unfortunately
    don’t scale linearly when distributed. A lot of work is being done to make scaling
    better, but it’s important to understand the costs because this isn’t as simple
    as throwing more hardware at the problem. In our experience, it’s almost always
    the model itself and not the infrastructure supporting it that is the source of
    the bottleneck. It is, however, important to review the utilization of the GPU,
    CPU, network, and storage before pointing fingers at the model. Open source tools
    such as [Horovod](https://oreil.ly/3NMtg) seek to improve distributed training
    frameworks and provide better model scaling.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分布式模型训练的圣杯。不幸的是，大多数库在分布式环境下并不呈线性扩展。虽然有很多工作正在进行中以改善扩展性，但理解成本是非常重要的，因为这不是简单地投入更多硬件来解决问题。根据我们的经验，几乎总是模型本身而不是支持它的基础设施成为瓶颈的源头。然而，在指责模型之前，审查
    GPU、CPU、网络和存储的利用率是非常重要的。开源工具如[Horovod](https://oreil.ly/3NMtg)致力于改进分布式训练框架，并提供更好的模型扩展能力。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We’ve covered a lot of ground in this chapter and hopefully have provided valuable
    insight into why Kubernetes is a great platform for machine learning, especially
    deep learning, and the considerations you need to be aware of before deploying
    your first machine learning workload. If you exercise the recommendations in this
    chapter, you will be well equipped to build and maintain a Kubernetes cluster
    for these specialized workloads.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了大量内容，希望能够深入理解为什么 Kubernetes 是一个出色的机器学习平台，特别是深度学习，并且在部署首个机器学习工作负载之前需要考虑的因素。如果您按照本章的建议去实施，您将能够为这些特殊工作负载构建和维护一个良好的
    Kubernetes 集群。
