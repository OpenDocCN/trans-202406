- en: Chapter 15\. Load Balancing MySQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different ways to connect to MySQL. For example, to perform a write
    test, a connection is created, the statement is executed, and then the connection
    is closed. To avoid the cost of opening a connection every time it is needed,
    the concept of the *connection pool* was developed. Connection pooling is a technique
    of creating and managing a pool of connections that are ready for use by any thread
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the concept of high availability discussed in [Chapter 13](ch13.xhtml#CH13_HA)
    to connections in order to improve a production system’s resilience, it is possible
    to use *load balancers* to connect to a database cluster. With load balancing
    and MySQL high availability, it is possible to keep the application running without
    interruption (or with only minor downtime). Basically, if the source server or
    one of the nodes of the database cluster fails, the client just needs to connect
    to another database node and it can continue to serve requests.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancers were built to provide transparency for clients when connecting
    to MySQL infrastructure. In this way, the application does not need to be aware
    of the MySQL topology; whether you’re using a classic replication, Group Replication,
    or Galera Cluster does not matter. The load balancer will provide an online node
    where it will be possible to read and write queries. Having a robust MySQL architecture
    and a proper load balancer in place can help DBAs avoid sleepless nights.
  prefs: []
  type: TYPE_NORMAL
- en: Load Balancing with Application Drivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To connect an application to MySQL, you need a driver. A *driver* is an adapter
    used to connect the application to a different system type. It is similar to connecting
    a video card to your computer; you may need to download and install a driver for
    it to work with your application.
  prefs: []
  type: TYPE_NORMAL
- en: Modern MySQL drivers from commonly used programming languages support connection
    pooling, load balancing, and failover. Examples include the [JDBC driver for MySQL
    (MySQL Connector/J)](https://oreil.ly/kaAXI) and the [PDO_MYSQL](https://oreil.ly/xbC7B)
    driver, which implements the PHP Data Objects (PDO) interface to enable access
    from PHP to MySQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: The database drivers we’ve mentioned are built to provide transparency for clients
    when connecting to standalone MySQL Server or MySQL replication setups. We won’t
    show you how to use them in code because that would be outside the scope of this
    book; however, you should be aware that adding a driver library facilitates code
    development, since the driver abstracts away a substantial amount of work for
    the developer.
  prefs: []
  type: TYPE_NORMAL
- en: But for other topologies, such as a clustering setup like Galera Cluster for
    MySQL or MariaDB, the JDBC and PHP drivers are not aware of internal Galera state
    information. For instance, a Galera donor node might be in read-only mode while
    it is helping another node resynchronize (if the SST method is `mysqldump` or
    `rsync`), or it could be up in non-primary state if split-brain happens. Another
    solution is to use a load balancer between the clients and the database cluster.
  prefs: []
  type: TYPE_NORMAL
- en: ProxySQL Load Balancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ProxySQL is a SQL proxy. ProxySQL implements the MySQL protocol, and because
    of this, it can do things that other proxies cannot do. Here are some of its advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides “intelligent” load balancing of application requests to multiple
    databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It understands the MySQL traffic that passes through it and can split reads
    from writes. Understanding the MySQL protocol is especially useful in a source/replica
    replication setup, where writes should only go to the source and reads to the
    replicas, or in the case of Galera Cluster for distributing the read queries evenly
    (linear read scaling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It understands the underlying database topology, including whether the instances
    are up or down, and therefore can route requests to healthy databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides query workload analytics and a query cache, which is useful for
    analyzing and improving performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides administrators with robust, rich query rule definitions to efficiently
    distribute queries and cache data to maximize the database service’s efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ProxySQL runs as a daemon watched by a monitoring process. The process monitors
    the daemon and restarts it in case of a crash to minimize downtime. The daemon
    accepts incoming traffic from MySQL clients and forwards it to backend MySQL servers.
  prefs: []
  type: TYPE_NORMAL
- en: The proxy is designed to run continuously without needing to be restarted. Most
    configurations can be done at runtime using queries similar to SQL statements
    in the ProxySQL admin interface. These include runtime parameters, server grouping,
    and traffic-related settings.
  prefs: []
  type: TYPE_NORMAL
- en: While it is common to install ProxySQL on a standalone node between the application
    and the database, this can affect query performance due to the additional latency
    from network hops. [Figure 15-1](#FIG-PROXYSQL1) shows ProxySQL as a middle layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![lm2e 1501](Images/lm2e_1501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-1\. ProxySQL between the application and MySQL
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To reduce the impact on performance (and avoid the additional network hop),
    another architecture option is installing ProxySQL on the application servers.
    The application then connects to ProxySQL (acting as a MySQL server) on localhost
    using a Unix domain socket, avoiding extra latency. It uses its routing rules
    to reach out and talk to the actual MySQL servers with their connection pooling.
    The application doesn’t have any idea what happens beyond its connection to ProxySQL.
    [Figure 15-2](#FIG-PROXYSQL2) shows ProxySQL on the same server as the application.
  prefs: []
  type: TYPE_NORMAL
- en: '![lm2e 1502](Images/lm2e_1502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-2\. ProxySQL on the same server as the application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Installing and Configuring ProxySQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at how to deploy ProxySQL for a source/replica configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The tool’s developers provide official packages for a variety of Linux distributions
    for all ProxySQL releases on their [GitHub releases page](https://oreil.ly/2EFKJ),
    so we’ll download the latest package version from there and install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before installing, the following instances are the ones we will use in this
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To begin, find the proper distribution for your operating system. In this example,
    we will then install for CentOS 7\. First, we will become root, install the MySQL
    client to connect to ProxySQL, and install ProxySQL itself. We get the URL from
    the downloads page and refer it to `yum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We have all the requirements to run ProxySQL, but the service doesn’t automatically
    start after installation, so we start it manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'ProxySQL should now be running with its default configuration in place. We
    can check it by running this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the ProxySQL process in the active state should be similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'ProxySQL splits the application interface from the admin interface. This means
    that ProxySQL will listen on two network ports: the admin interface will listen
    on 6032, and the application will listen on 6033 (to make it easier to remember,
    that’s the reverse of MySQL’s default port, 3306).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, ProxySQL needs to communicate with the MySQL nodes to be able to check
    their condition. To achieve this, ProxySQL needs to connect to each server with
    a dedicated user.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to create the user on the source server. Connect to the
    MySQL source instance and run these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will configure ProxySQL parameters to recognize the user. First we
    connect to ProxySQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we set the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve set the user in the database and ProxySQL, it is time to tell
    ProxySQL which MySQL servers are present in the topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define who our writer and reader groups. The servers present
    in the writer group will be able to receive DML operations, while `SELECT` queries
    will use the servers in the reader group. In this example, the host group 10 will
    be the writer, and host group 11 will be the reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, ProxySQL must have users that can access backend nodes to manage connections.
    Let’s create the user on the backend source server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we will configure ProxySQL with the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is the most exciting because it is here that we define the rules.
    The rules will tell ProxySQL where to send write and read queries, balancing the
    load on the servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'ProxySQL has a thread responsible for connecting on each server listed in the
    `mysql_servers` table and checking the value of the `read_only` variable. Suppose
    the replica is showing up in the writer group, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Because we do not want ProxySQL writing data to the replica servers, which would
    cause data inconsistency, we need to set the `read_only` option in the replica
    servers, so these servers will serve only read queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to use our application. Running the following command should
    return the hostname that ProxySQL connected from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ProxySQL has a lot more features and flexibility than we’ve shown here; our
    goal in this section was just to present the tool so you’re aware of this option
    when deciding on an architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we mentioned when configuring replication in Chapter 13, we want to reinforce
    the idea that ProxySQL needs to reach the MySQL servers; otherwise, it won’t work.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy Load Balancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*HAProxy* stands for High Availability Proxy, and it is a TCP/HTTP load balancer.
    It distributes a workload across a set of servers to maximize performance and
    optimize resource usage.'
  prefs: []
  type: TYPE_NORMAL
- en: With the intent to expand your knowledge regarding MySQL architectures and different
    topologies, we will configure Percona XtraDB Cluster (Galera Cluster) with HAProxy
    in this section instead of a classic replication topology.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture options are similar to ProxySQL’s. HAProxy can be placed together
    with the application or in a middle layer. [Figure 15-3](#FIG-HAPROXY1) shows
    an example where HAProxy is placed on the same server as the application.
  prefs: []
  type: TYPE_NORMAL
- en: '![lm2e 1503](Images/lm2e_1503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-3\. HAProxy together with the application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: And [Figure 15-4](#FIG-HAPROXY2) shows a topology with HAProxy in a middle layer.
  prefs: []
  type: TYPE_NORMAL
- en: Again, these are archictectures with different pros and cons. While in the first
    one we do not have an extra hop (which reduces latency), we add extra load to
    the application server. Also, you have to configure HAProxy on each application
    server.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, having HAProxy in the middle layer facilitates managing it
    and increases availability, because the application can connect to any HAProxy
    server. However, the extra hop adds latency.
  prefs: []
  type: TYPE_NORMAL
- en: '![lm2e 1504](Images/lm2e_1504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-4\. HAProxy in a middle layer running in dedicated servers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Installing and Configuring HAProxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Common operating systems such as Red Hat/CentOS and Debian/Ubuntu provide the
    HAProxy package, and you can install it using the package manager. The installation
    process is relatively easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Debian or Ubuntu, use these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For Red Hat or CentOS, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: When installed, HAProxy will set the default path for the configuration file
    as */etc/haproxy/haproxy.cfg*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting HAProxy, we need to configure it. For this demonstration, in
    our first scenario HAProxy will be located on the same server as the application.
    Here are the IPs of our three-node Galera Cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s open our */etc/haproxy/haproxy.cfg* file and look at it. There are many
    parameters to customize, split into three sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '`global`'
  prefs: []
  type: TYPE_NORMAL
- en: A section in the configuration file for process-wide parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`defaults`'
  prefs: []
  type: TYPE_NORMAL
- en: A section in the configuration file for default parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`listen`'
  prefs: []
  type: TYPE_NORMAL
- en: A section in the configuration file that defines a complete proxy, including
    its frontend and backend parts
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 15-1](#TABLE-HAPROXY) shows the basic HAProxy parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-1\. HAProxy options (with links to HAProxy documentation)
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`balance`](https://oreil.ly/Ej25a) | Defines the load balancing algorithm
    to be used in a backend. |'
  prefs: []
  type: TYPE_TB
- en: '| [`clitimeout`](https://oreil.ly/HRlC5) | Sets the maximum inactivity time
    on the client side |'
  prefs: []
  type: TYPE_TB
- en: '| [`contimeout`](https://oreil.ly/Hjwtm) | Sets the maximum time to wait for
    a connection attempt to a server to succeed |'
  prefs: []
  type: TYPE_TB
- en: '| [`daemon`](https://oreil.ly/75stw) | Makes the process fork into background
    (recommended mode of operation) |'
  prefs: []
  type: TYPE_TB
- en: '| [`gid`](https://oreil.ly/ikswS) | Changes the process’s group ID to `*<number>*`
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`log`](https://oreil.ly/ovcK5) | Adds a global syslog server |'
  prefs: []
  type: TYPE_TB
- en: '| [`maxconn`](https://oreil.ly/94uQr) | Sets the maximum per-process number
    of concurrent connections to `*<number>*` |'
  prefs: []
  type: TYPE_TB
- en: '| [`mode`](https://oreil.ly/c3CaJ) | Set the running mode or protocol of the
    instance |'
  prefs: []
  type: TYPE_TB
- en: '| [`option dontlognull`](https://oreil.ly/DplEm) | Disable logging of null
    connections |'
  prefs: []
  type: TYPE_TB
- en: '| [`optiontcplog`](https://oreil.ly/BA2mL) | Enables advanced logging of TCP
    connections with session state and timers |'
  prefs: []
  type: TYPE_TB
- en: 'To make HAProxy work, we will use the following configuration file based on
    our settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To start HAProxy, we use the `haproxy` command. We can pass any number of configuration
    parameters on the command line. To use a configuration file, use the `-f` option.
    For example, we can pass one configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'or multiple configuration files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'or a directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With this configuration, HAProxy will balance the load between three nodes.
    In this case, it checks only if the `mysqld` process is listening on port 3306,
    but doesn’t take into account the state of the node. So, it could be sending queries
    to a node that has `mysqld` running even if it’s in the `JOINING` or `DISCONNECTED`
    state.
  prefs: []
  type: TYPE_NORMAL
- en: To check the current status of a node, we need something a little more complex.
    This idea was taken from [Codership’s Google group](https://oreil.ly/7xj0N).
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this setup, we will need two scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clustercheck`, located in */usr/local/bin* and a config for `xinetd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mysqlchk`, located in */etc/xinetd.d* on each node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Both scripts are available](https://oreil.ly/iN1La) in binaries and source
    distributions of Percona XtraDB.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the */etc/services* file by adding the following line for each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If the */etc/services* file does not exist, it’s likely that `xinetd` is not
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install it for CentOS/Red Hat, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For Debian/Ubuntu, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create a MySQL user so the script can check if the node is
    healthy. Ideally, for security reasons, this user should have the minimum privileges
    required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate how our node is performing on the health check, we can run the
    following command and observe the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do this for all nodes, we will be ready to test whether our HAProxy setup
    is working. The easiest way to do this is to connect to it and execute some MySQL
    commands. Let’s run a command that retrieves the hostname from which we are connected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this a second time gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And the third time we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our HAProxy is connecting in a round-robin fashion. If we shut
    down one of the nodes, HAProxy will route only to the remaining ones.
  prefs: []
  type: TYPE_NORMAL
- en: MySQL Router
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MySQL Router is responsible for distributing the traffic between members of
    an InnoDB cluster. It is a proxy-like solution to hide the cluster topology from
    applications, so applications don’t need to know which member of a cluster is
    the primary node and which are secondaries. Note that MySQL Router will *not*
    work with Galera Clusters; it was developed for *InnoDB Cluster only*.
  prefs: []
  type: TYPE_NORMAL
- en: MySQL Router is capable of performing read/write splitting by exposing different
    interfaces. A common setup is to have one read/write interface and one read-only
    interface. This is the default behavior that also exposes two similar interfaces
    to use the X Protocol (used for CRUD operations and async calls).
  prefs: []
  type: TYPE_NORMAL
- en: 'The read/write split is done using the concept of *roles*: primary for writes
    and secondary for read-only. This is analogous to how members of cluster are named.
    Additionally, each interface is exposed via a TCP port so applications only need
    to know the IP:port combination used for writes and the one used for reads. Then
    MySQL Router will take care of connections to cluster members depending on the
    type of traffic to the server.'
  prefs: []
  type: TYPE_NORMAL
- en: When working in a production environment, the MySQL server instances that make
    up an InnoDB Cluster run on multiple host machines as part of a network rather
    than on single machine. So, as with ProxySQL and HAProxy, the MySQL router can
    be a middle layer in the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-5](#FIG-MYSQL-ROUTER-ARCH) illustrates how the production scenario
    works.'
  prefs: []
  type: TYPE_NORMAL
- en: '![lm2e 1505](Images/lm2e_1505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-5\. MySQL InnoDB Cluster production deployment
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, to start our example, let’s take a look at the MySQL members that are
    part of the InnoDB Cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the configuration of the MySQL nodes and the cluster name,
    we can start configuring MySQL Router. For performance purposes it’s recommended
    to set up MySQL Router in the same place as the application, supposing we have
    an instance per application server, so we will place our router on the application
    server. First, we are going to identify the version of MySQL Router compatible
    with our OS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will check the [download page](https://oreil.ly/eCH08) and install
    it using `yum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that MySQL Router is installed, we need to create a dedicated directory
    for its operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to bootstrap MySQL Router. The bootstrap will configure
    the router for operation with a MySQL InnoDB Cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In the command line, we are telling the router to connect with the user `root`,
    in our primary server (`172.16.3.120`), at port 3306\. We are also telling the
    router to create a socket file so we can connect using it. Finally, we are creating
    a new user (`app_router`) to use in our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the contents that the bootstrap process created in our
    configuration directory (*/var/lib/mysqlrouter*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'A generated MySQL Router configuration file (*mysqlrouter.conf*) looks similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In this example, MySQL Router configured four ports (two ports to read/write
    using the regular MySQL protocol, and two to read/write using the X Protocol)
    and four sockets. Ports are added by default, and sockets were added because we
    passed in `--conf-use-sockets`. The InnoDB Cluster named `cluster1` is the source
    of the metadata, and the destinations are using the InnoDB Cluster metadata cache
    to dynamically configure host information.
  prefs: []
  type: TYPE_NORMAL
- en: 'By executing the *start.sh* script we can start the MySQL router daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can observe the process running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'And the ports open:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve configured MySQL Router with the InnoDB Cluster, so now we can test this
    with read and read/write connections. First, we will connect to the writer port
    (6446):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it is possible to execute both reads and writes in the writer
    port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will check the read port (6447) using a `SELECT` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s working, but let’s try to execute a write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the read port only accepts reads. It is also possible to see the router
    load-balancing the reads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In this way, if any downtime occurs in one of the MySQL nodes, MySQL Router
    will route the queries to the remaining active nodes.
  prefs: []
  type: TYPE_NORMAL
