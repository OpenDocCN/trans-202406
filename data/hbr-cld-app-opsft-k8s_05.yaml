- en: Chapter 4\. Single Cluster Availability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章\. 单个集群可用性
- en: A solid foundation for each individual cluster is critical to the availability
    of your applications and services. Even with these advanced distributed application
    capabilities, there are some systems for which multicluster isn’t an option. Most
    stateful applications and databases are not capable of geographic distribution
    and are very complicated to properly operate across multiple clusters. Many modern
    data storage solutions require low latency multizone solutions, making it less
    than optimal to break up the data clusters into higher latency environments. It
    is also important to consider that many data platforms have `StatefulSet` solutions
    to help operate them, requiring a single-cluster architecture. We think that having
    a strong foundation of what it means for a system to be *available* and *highly
    available* is critical to understanding the value and architecture of OpenShift
    and Kubernetes. Note that we’ll discuss how to take advantage of multicluster
    architectures for the ultimate in availability and flexibility in [Chapter 5](ch05.html#continuous_delivery_across_clusters).
    This chapter should help prepare you with the knowledge required to make intelligent
    decisions about what your availability goals are and how to leverage Kubernetes
    and OpenShift to achieve them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单独集群的坚实基础对您的应用程序和服务的可用性至关重要。即使拥有这些先进的分布式应用程序能力，某些系统也不适合多集群选项。大多数有状态应用程序和数据库无法进行地理分布，并且在多个集群上进行适当操作非常复杂。许多现代数据存储解决方案要求低延迟多区域解决方案，这使得将数据集群分散到高延迟环境中变得不太理想。还要考虑到许多数据平台具有`StatefulSet`解决方案来帮助操作它们，需要单集群架构。我们认为理解系统“可用”和“高度可用”的含义对于理解
    OpenShift 和 Kubernetes 的价值和架构至关重要。请注意，我们将讨论如何利用多集群架构实现最高的可用性和灵活性，以及在[第 5 章](ch05.html#continuous_delivery_across_clusters)中准备好您所需的知识，以便能够就您的可用性目标做出明智的决策并利用
    Kubernetes 和 OpenShift。
- en: System Availability
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统可用性
- en: In modern service and application delivery, we typically talk about system availability
    in terms of what percentage of time an application or service is available and
    responding normally. A common standard in the technology service industry is to
    describe SLOs in terms of nines. We hear the terms *four nines* or *five nines*
    to indicate the percentage of time that the service is available. Four nines is
    99.99%, and five nines is 99.999%. We’ll use these standards throughout this chapter.
    This SLO is the agreement within the SLA that the service provider has with its
    stakeholder or customer. These SLOs are often measured on a monthly basis, or
    sometimes annually.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代服务和应用交付中，我们通常根据应用或服务的可用和正常响应时间的百分比来谈论系统可用性。技术服务行业的一个常见标准是使用“9”的术语来描述服务水平目标（SLOs）。我们经常听到“四个9”或“五个9”，以指示服务可用的时间百分比。四个9表示
    99.99%，五个9表示 99.999%。我们将在本章节中沿用这些标准。这些 SLOs 是服务提供商与其利益相关者或客户之间的 SLA 中的协议。这些 SLOs
    通常是按月度或有时是按年度进行衡量。
- en: Measuring System Availability
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量系统可用性
- en: In his day job, one of the authors, Jake, is responsible for the delivery of
    [IBM Cloud Kubernetes Service](https://oreil.ly/7SAB7) and [Red Hat OpenShift
    on IBM Cloud](https://oreil.ly/FMJGW). These are global services that provide
    managed Kubernetes and OpenShift solutions with the simplicity of an API-driven,
    user-friendly cloud experience. For these services, the site reliability engineering
    (SRE) team measures SLOs on a monthly basis as that coincides with the client
    billing cycle. It’s helpful to think about how much downtime is associated with
    these nines. We can see in [Table 4-1](#availability_tablesemicolon_calculations)
    that our disruption budget or allowable downtime can be very low once we get into
    higher levels of availability. When performing this analysis, your team needs
    to carefully consider what it is willing to commit to.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的日常工作中，其中一位作者，Jake，负责交付[IBM 云 Kubernetes 服务](https://oreil.ly/7SAB7)和[Red
    Hat OpenShift on IBM Cloud](https://oreil.ly/FMJGW)。这些是全球服务，提供了管理 Kubernetes 和
    OpenShift 解决方案的简单 API 驱动、用户友好的云体验。对于这些服务，站点可靠性工程（SRE）团队每月都会按客户计费周期测量服务水平目标（SLOs）。考虑到这些目标，思考与这些“9”相关的停机时间量是很有帮助的。我们可以在[表
    4-1](#availability_tablesemicolon_calculations)中看到，一旦进入更高可用性级别，我们的中断预算或允许的停机时间可以非常低。在进行这种分析时，您的团队需要仔细考虑愿意承诺的内容。
- en: Table 4-1\. Availability table; calculations based on an average 730-hour month
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 可用性表；基于平均每月 730 小时计算
- en: '| Availability percentage | Downtime per month |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 可用性百分比 | 每月停机时间 |'
- en: '| --- | --- |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 99% (two nines) | 7 hours 18 minutes |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 99%（两个九） | 7 小时 18 分钟 |'
- en: '| 99.5% (two and a half nines) | 3 hours 39 minutes |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 99.5%（两个半九） | 3 小时 39 分钟 |'
- en: '| 99.9% (three nines) | 43 minutes 48 seconds |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 99.9%（三个九） | 43 分钟 48 秒 |'
- en: '| 99.95% (three and a half nines) | 21 minutes 54 seconds |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 99.95%（三个半九） | 21 分钟 54 秒 |'
- en: '| 99.99% (four nines) | 4 minutes 22.8 seconds |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 99.99%（四个九） | 4 分钟 22.8 秒 |'
- en: '| 99.999% (five nines) | 26.28 seconds |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 99.999%（五个九） | 26.28 秒 |'
- en: 'There are two key performance indicators (KPIs) that a team will strive to
    improve in order to achieve an agreed-upon SLO. They are *mean time to recovery*
    (MTTR) and *mean time between failures* (MTBF). Together, these KPIs directly
    impact availability:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个团队将努力改进的两个关键绩效指标（KPI）是*平均修复时间*（MTTR）和*平均故障间隔时间*（MTBF）。这两个指标直接影响可用性：
- en: <math mode="display"><mi mathvariant="italic">Availability</mi> <mo>=</mo> <mfrac><mrow><mi
    mathvariant="italic">MTBF</mi></mrow> <mrow><mi mathvariant="italic">MTBF</mi>
    <mo>+</mo> <mi mathvariant="italic">MTTR</mi></mrow></mfrac></math>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <math mode="display"><mi mathvariant="italic">Availability</mi> <mo>=</mo> <mfrac><mrow><mi
    mathvariant="italic">MTBF</mi></mrow> <mrow><mi mathvariant="italic">MTBF</mi>
    <mo>+</mo> <mi mathvariant="italic">MTTR</mi></mrow></mfrac></math>
- en: It’s helpful to think about how we can arrive at our desired availability percentage.
    Let’s say we want to get to a 99.99% availability. In [Table 4-2](#four_nines_left_parenthesisnineninedotni),
    we compare some different MTTR and MTBF numbers that can get us to this availability.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们如何达到期望的可用性百分比，进行一些思考会很有帮助。假设我们希望达到 99.99% 的可用性。在[表 4-2](#four_nines_left_parenthesisnineninedotni)中，我们比较了一些可以帮助我们达到这种可用性的不同
    MTTR 和 MTBF 数字。
- en: Table 4-2\. Four nines (99.99%) availability calculations
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. 四个九（99.99%）可用性计算
- en: '| MTBF | MTTR |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| MTBF | MTTR |'
- en: '| --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 day | 8.64 seconds |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 1 天 | 8.64 秒 |'
- en: '| 1 week | 60.48 seconds |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 1 周 | 60.48 秒 |'
- en: '| 1 month | 4 minutes 22.8 seconds |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 1 月 | 4 分钟 22.8 秒 |'
- en: '| 1 year | 52 minutes 33.6 seconds |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 1 年 | 52 分钟 33.6 秒 |'
- en: For those following along at home, this means that to get to four-nines availability
    in a system that will experience a failure once a day, you have to identify and
    repair that failure in under nine seconds. The engineering investment required
    to achieve platform stability and software quality to dramatically extend the
    MTBF is simply massive. For many customers, four nines is prohibitively expensive.
    Getting to that level of availability takes hardware, engineering, and a support
    staff. For contrast, let’s take a look at a two nines comparison in [Table 4-3](#two_nines_nineninepercent_availability_c).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些在家跟随的人来说，这意味着要在一个每天会发生一次故障的系统中，你需要在不到九秒的时间内识别和修复该故障才能达到四个九的可用性。为了实现平台稳定性和软件质量的显著提升
    MTBF，需要大量的工程投入。对许多客户来说，四个九的成本太高。要达到这种可用性水平需要硬件、工程和支持人员。作为对比，让我们来看看[表 4-3](#two_nines_nineninepercent_availability_c)中两个九的比较。
- en: Table 4-3\. Two nines 99% availability calculations
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-3\. 两个九 99% 可用性计算
- en: '| MTBF | MTTR |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| MTBF | MTTR |'
- en: '| --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 30 minutes | 18.18 seconds |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 30 分钟 | 18.18 秒 |'
- en: '| 1 hour | 36.36 seconds |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 1 小时 | 36.36 秒 |'
- en: '| 1 day | 14 minutes 32.4 seconds |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 1 天 | 14 分钟 32.4 秒 |'
- en: '| 1 week | 1 hour 41 minutes 48 seconds |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 1 周 | 1 小时 41 分钟 48 秒 |'
- en: 'What are we to make of these numbers? Well, there are a few things to take
    away from this. First, if you have a target of four nines, then you will need
    an unbelievably reliable system with failures occurring on the order of once a
    year if you are going to have any hope of hitting the target. Even at two nines,
    if you have a single failure a day, then you are going to need to identify and
    repair that failure in under 15 minutes. Second, we need to consider the likelihood
    of these failures. A failure once a day, you say? Who has a failure once a day?
    Well, consider this: the SLA for a single VM instance is typically around 90%,
    which means 73.05 hours of downtime per month. A single VM is going to be down
    on average three days per month. If your application depends on just 10 systems,
    then the odds are that at least one of those is going to be down every day.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据告诉我们什么？首先，如果你的目标是四个九，那么你需要一个异常可靠的系统，如果要达到目标，故障发生的频率应该在一年内仅有一次。即使是两个九，如果每天有一次故障，你也需要在不到
    15 分钟的时间内识别和修复该故障。其次，我们需要考虑这些故障发生的可能性。你说每天发生一次故障？谁每天都会有故障？好吧，请考虑一下：单个 VM 实例的 SLA
    通常约为 90%，这意味着每月将有 73.05 小时的停机时间。一个单独的 VM 平均每月将停机三天。如果你的应用程序仅依赖于 10 个系统，那么至少其中一个系统每天都会停机。
- en: Does this picture look grim and insurmountable? If not, then you are quite the
    optimist. If it is looking challenging, then you have recognized that hitting
    these failure rate numbers and recovery times with standard monitoring and recovery
    runbooks is unlikely. Enter the world of highly available systems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图片看起来令人沮丧且难以逾越吗？如果不是，那么你一定是位极具乐观主义精神的人。如果你觉得这看起来具有挑战性，那么你已经意识到用标准的监控和恢复手册来达到这些故障率和恢复时间是不太可能的。那么，请进入高可用系统的世界吧。
- en: What Is a Highly Available System?
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是高可用系统？
- en: In the simplest terms, *high availability (HA)* refers to a system that has
    the characteristic of not having any single point of failure. Any one “component”
    can sustain a failure and the system as a whole experiences no loss of availability.
    This attribute is sometimes referred to as having the ability to treat individual
    components as cattle rather than pets. This concept was first introduced by Bill
    Baker of Microsoft in his talk “Scaling SQL Server 2012.”^([1](ch04.html#ch01fn26))
    It was later popularized by Gavin McCance when talking about OpenStack at CERN.^([2](ch04.html#ch01fn27))
    As long as the overall herd is healthy, we don’t need to be concerned with the
    health of the individuals. We like to keep *all* of our pets healthy and in top
    shape, but it is rather time-consuming and expensive. If you have only a handful
    of pets, this can be sustainable. However, in modern cloud computing, we typically
    see a herd of hundreds or thousands. There simply are not enough SRE engineers
    on the planet to keep the entire herd healthy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*高可用性（HA）* 指的是一个系统具有没有任何单点故障的特性。任何一个“组件”都可以承受故障，整个系统不会出现可用性损失。这种属性有时被称为能够将个别组件视为牲畜而非宠物。这个概念最初由微软的
    Bill Baker 在他的演讲 “Scaling SQL Server 2012” 中引入^([1](ch04.html#ch01fn26))，后来在 CERN
    的 Gavin McCance 讨论 OpenStack 时得到了普及^([2](ch04.html#ch01fn27))。只要整体群体健康，我们就不需要关心个体的健康状况。我们喜欢保持所有宠物的健康和良好状态，但这需要耗费大量时间和金钱。如果你只有少数几只宠物，这种做法是可持续的。然而，在现代云计算中，我们通常看到数百甚至数千只宠物。这个星球上的
    SRE 工程师根本不足以保持整个群体的健康。
- en: In a system that has a single point of failure, even the most cared-for components
    occasionally have a failure that we cannot prevent. Once discovered via monitoring,
    some time is always required to bring them back to health. Hopefully, our introduction
    to system availability convinces you of the challenges of MTBF and MTTR.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在单点故障的系统中，即使是最精心维护的组件偶尔也会发生我们无法预防的故障。一旦通过监控发现，总会需要一些时间来使它们恢复健康。希望我们对系统可用性的介绍能让您意识到
    MTBF 和 MTTR 的挑战。
- en: It is helpful for us to have a diagram of the simplest of highly available systems.
    A simple stateless web application with a load balancer is a great example, as
    shown in [Figure 4-1](#highly_available_stateless_web_applicati).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，有一个最简单的高可用系统的图示是非常有帮助的。一个简单的无状态 Web 应用程序和负载均衡器就是一个很好的例子，如 [图 4-1](#highly_available_stateless_web_applicati)
    所示。
- en: '![](assets/hcok_0401.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0401.png)'
- en: Figure 4-1\. Highly available stateless web application with load balancer
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 带有负载均衡器的高可用无状态 Web 应用程序
- en: In this simplest of HA systems, if either one of these two web application instances
    fails, the load balancer can detect that failure and stop routing traffic to the
    failed instance. The actor doesn’t know about the individual instances; all of
    the actor’s interactions are with the load balancer. The load balancer is even
    capable of doing retries if a request to one of the instances fails or does not
    respond fast enough.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最简单的高可用系统中，如果这两个 Web 应用实例中的任何一个失败，负载均衡器可以检测到该故障，并停止将流量路由到失败的实例。使用者不了解个别实例的情况；所有的交互都是与负载均衡器进行的。负载均衡器甚至能在一个实例的请求失败或者响应不够快时进行重试。
- en: Now, obviously the load balancer itself could also fail, but load balancers
    generally have much higher availability numbers than your typical web application.
    A typical cloud load balancer has an SLA of 99.99%, which would be the ceiling
    for the availability of any application running behind that load balancer. Where
    a cloud load balancer is not available, you can deploy them in pairs with virtual
    IPs and *Virtual Router Redundancy Protocol* (VRRP), which can move a single IP
    address between two load balancer instances. The public cloud providers can simplify
    all of this with their application and network load balancer services, which are
    capable of handling all of the HA aspects of the load balancer itself.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，负载均衡器本身也可能发生故障，但负载均衡器通常比您典型的Web应用程序具有更高的可用性数字。典型的云负载均衡器具有99.99%的SLA，这将是运行在该负载均衡器后面的任何应用程序的可用性上限。如果云负载均衡器不可用，您可以部署成对的虚拟IP和*虚拟路由冗余协议*（VRRP），它可以在两个负载均衡器实例之间移动单个IP地址。公共云提供商可以通过其应用程序和网络负载均衡器服务简化所有这些操作，这些服务能够处理负载均衡器本身的所有高可用性方面。
- en: All done, right? Well, not quite; there are so many components and services
    to run in a modern application and service platform. In our experience, many large
    cloud services could consist of multiple groups of five to six microservices.
    Then add in the fact that your application or service might be globally distributed
    and/or replicated. Consider our own Red Hat OpenShift on IBM Cloud service availability
    in [Figure 4-2](#red_hat_openshift_on_ibm_cloud_locations).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了吗？嗯，并不完全如此；现代应用程序和服务平台中有许多组件和服务需要运行。根据我们的经验，许多大型云服务可能包括多组五到六个微服务。再加上您的应用程序或服务可能是全球分布和/或复制的事实。考虑我们自己在Red
    Hat OpenShift on IBM Cloud中的服务可用性在[图4-2](#red_hat_openshift_on_ibm_cloud_locations)中展示。
- en: '![](assets/hcok_0402.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0402.png)'
- en: Figure 4-2\. Red Hat OpenShift on IBM Cloud locations
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. IBM云上的Red Hat OpenShift位置
- en: That’s six global multizone regions along with 14 other datacenters worldwide.
    Our team runs over 40 microservices in each of those multizone regions, and about
    half that in the single-zone datacenters. The numbers add up quickly. Imagine
    if we had to manually deploy each microservice and configure a load balancer for
    each one of them in each of these locations. We’d need an army of engineers just
    writing custom configuration data. Enter OpenShift and Kubernetes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在全球六个多区域地区以及全球其他14个数据中心。我们的团队在每个多区域地区运行超过40个微服务，在单区域数据中心运行大约一半的微服务。这些数字很快就会增加。想象一下，如果我们必须在每个位置手动部署每个微服务并为它们中的每一个配置负载均衡器，那我们将需要一支大军的工程师来编写定制配置数据。这就是OpenShift和Kubernetes的用武之地。
- en: OpenShift and Kubernetes Application and Service Availability
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenShift和Kubernetes应用程序与服务可用性
- en: Many components in Kubernetes and OpenShift contribute to the availability of
    the overall system. We’ll discuss how we keep the application healthy, how we
    deal with networking, and how we are able to address potential issues with compute
    nodes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes和OpenShift中的许多组件都有助于整个系统的可用性。我们将讨论如何保持应用程序的健康状态，如何处理网络问题以及如何解决计算节点可能出现的问题。
- en: The application
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序
- en: For our stateless application, the simple load balancer is the key to high availability.
    However, as we discussed, if we rely on manually deploying and configuring load
    balancers and applications, then keeping them up to date won’t be a whole lot
    of fun. In [Chapter 2](ch02.html#getting_started_with_openshift_and_kuber), we
    discussed Kubernetes deployments. This elegant construct makes it simple to deploy
    multiple replicas of an application. Let’s look at the `livenessProbe`, which
    is an advanced capability and one of the more critical features of the deployment-pod-container
    spec. The `livenessProbe` defines a probe that will be executed by the `kubelet`
    against the container to determine if the target container is healthy. If the
    `livenessProbe` fails to return success after the configured `failure​Th⁠reshold`,
    the `kubelet` will stop the container and attempt to restart it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的无状态应用程序，简单的负载均衡器是保证高可用性的关键。然而，正如我们讨论过的那样，如果我们依赖手动部署和配置负载均衡器和应用程序，那么保持它们的更新就不会太有趣了。在[第2章](ch02.html#getting_started_with_openshift_and_kuber)中，我们讨论了Kubernetes部署。这种优雅的结构使得部署多个应用程序副本变得简单。让我们来看看`livenessProbe`，这是一种高级能力，也是部署-容器规范中更为关键的特性之一。`livenessProbe`定义了一个探测器，将由`kubelet`针对容器执行，以确定目标容器是否健康。如果`livenessProbe`在配置的`failure​Th⁠reshold`之后未能成功返回，则`kubelet`将停止容器并尝试重新启动它。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To handle transient issues in a container’s ability to serve requests, you may
    use a `readinessProbe`, which will determine the routing rules for Kubernetes
    services but won’t restart the container if it fails. If the `readinessProbe`
    fails, the state of the pod is marked `NotReady` and the pod is removed from the
    service endpoint list. The probe continues to run and will set the state back
    to `Ready` upon success. This can be useful if a pod has an external dependency
    that may be temporarily unreachable. A restart of this pod won’t help, and thus
    using a `livenessProbe` to restart the pod has no benefit. See the [community
    docs](https://oreil.ly/GqNnW) for more details on the available probes in Kubernetes
    and OpenShift.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理容器在提供请求的能力上的瞬态问题，您可以使用一个`readinessProbe`，它将确定Kubernetes服务的路由规则，但如果它失败了，不会重新启动容器。如果`readinessProbe`失败，Pod的状态将标记为`NotReady`，并且将从服务端点列表中移除该Pod。探测将继续运行，并在成功时将状态设置为`Ready`。如果一个Pod有一个可能暂时无法访问的外部依赖项，这将非常有用。重新启动此Pod将无济于事，因此使用`livenessProbe`来重新启动Pod没有任何好处。有关Kubernetes和OpenShift中可用探测器的更多详细信息，请参见[社区文档](https://oreil.ly/GqNnW)。
- en: 'The following example shows a simple deployment that includes a `livenessProbe`
    with common parameters:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了一个包含具有常见参数的`livenessProbe`的简单部署：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s take a closer look at the options we have chosen in our example for `liveness​P⁠robe`
    and how they translate into real-world behaviors:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地查看我们在`liveness​P⁠robe`示例中选择的选项，以及它们如何转化为真实世界的行为：
- en: '`httpGet`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`httpGet`'
- en: The type of check that will be performed. You can also use `tcp` or `exec` commands
    that will execute to test if the container is healthy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将执行的检查类型。您还可以使用`tcp`或`exec`命令来执行测试，以测试容器是否健康。
- en: '`initialDelaySeconds`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`initialDelaySeconds`'
- en: How long the `kubelet` will wait before attempting the first probe.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet`在尝试第一个探测之前将等待多长时间。'
- en: '`periodSeconds`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`periodSeconds`'
- en: How often the `kubelet` will test the probe.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet`将测试探测的频率。'
- en: '`timeoutSeconds`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`timeoutSeconds`'
- en: How long the `kubelet` client will wait for a response.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet`客户端将等待响应的时间长度。'
- en: '`failureThreshold`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`failureThreshold`'
- en: The number of consecutive failures that the `kubelet` must run through before
    marking the probe as failed. In the case of our `livenessProbe`, the result is
    restarting this container.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在`kubelet`标记探测失败之前，必须运行的连续失败次数。对于我们的`livenessProbe`，结果是重新启动此容器。
- en: Tip
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Be sure to thoroughly test your `livenessProbe` and the timing settings. We
    have heard of numerous cases where a slow application startup results in a `CrashLoopBackOff`
    scenario where a pod never starts up successfully due to never having sufficient
    `initialDelaySeconds` to start completely. The `livenessProbe` never succeeds
    because the `kubelet` starts to check for liveness before the application is ready
    to start receiving requests. It can be painful for rolling updates to set very
    long initial delay settings, but it can improve stability of the system where
    there is a lot of variability in initial liveness times.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要彻底测试您的`livenessProbe`和定时设置。我们听说过许多情况，其中应用程序启动缓慢导致`CrashLoopBackOff`场景，其中Pod由于从未有足够的`initialDelaySeconds`完全启动而从未成功启动。`livenessProbe`永远不会成功，因为`kubelet`开始检查生存状态之前，应用程序尚未准备好开始接收请求。对于滚动更新来说，设置非常长的初始延迟设置可能会很痛苦，但可以提高系统的稳定性，特别是在初始生存时间存在大量变异性的情况下。
- en: 'Together, we have all the makings of a traditional load balancer health check.
    HAProxy is a popular traditional software load balancing solution. If we look
    at a standard HAProxy configuration, we’ll see many of the same critical health-checking
    components:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们具备传统负载均衡器健康检查的所有要素。HAProxy是一种流行的传统软件负载均衡解决方案。如果我们查看标准的HAProxy配置，我们会看到许多相同的关键健康检查组件：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The interesting thing to note here is that we’re looking at the same configuration
    settings for a load balancer configuration in legacy infrastructure as we are
    for a deployment in Kubernetes. In fact, the actual check performed is identical.
    Both initiate an HTTP connection via port 80 and check for a 200 response code.
    However, the net resulting behavior is quite different. In the case of HAProxy,
    it just determines if network traffic should be sent to the backend server. HAProxy
    has no idea what that backend server is and has no ability to change its runtime
    state. In contrast, Kubernetes isn’t changing the network routing directly here.
    Kubernetes is only updating the state of each of the pods in the deployment (`Ready`
    versus `NotReady`), and it will restart that container if it fails its `livenessProbe`.
    Restarting the pod won’t fix all issues that are causing unresponsiveness, but
    it often can solve minor issues.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的有趣之处是，我们在传统基础设施中的负载均衡器配置和 Kubernetes 中的部署中查看的是相同的配置设置。事实上，执行的实际检查是相同的。两者都通过端口
    80 发起 HTTP 连接并检查 200 响应代码。然而，最终的行为却是非常不同的。在 HAProxy 的情况下，它只确定是否应将网络流量发送到后端服务器。HAProxy
    不知道后端服务器是什么，也无法更改其运行时状态。相比之下，Kubernetes 并不直接改变网络路由。Kubernetes 只是更新部署中每个 pod 的状态（`Ready`
    与 `NotReady`），如果 pod 未通过 `livenessProbe`，它将重新启动该容器。重新启动 pod 不能解决导致无响应的所有问题，但通常可以解决一些小问题。
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: '`livenessProbe` isn’t just great for ensuring that your containers are healthy
    and ready to receive traffic. It’s a fantastic programmable solution for the pragmatic
    operator who is tired of cleaning up Java Virtual Machine (JVM) leaking threads
    and memory to the point of becoming unresponsive. It’s a nice backup to the resource
    limits that we set in place in [Chapter 3](ch03.html#advanced_resource_management)
    in an attempt to limit these resource leaks.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`livenessProbe` 不仅适用于确保容器健康且准备好接收流量。对于那些厌倦了清理 Java 虚拟机（JVM）泄漏线程和内存以至变得无响应的实用操作员来说，它是一个极好的可编程解决方案。它是我们在[第
    3 章](ch03.html#advanced_resource_management)中设置的资源限制的一个很好的备份，以尝试限制这些资源泄漏。'
- en: It’s worth noting that associating these health-checking configurations with
    the runtime components rather than a networking feature of the system is a deliberate
    choice. In Kubernetes and OpenShift, the `livenessProbe` is performed by the `kubelet`
    rather than by a load balancer. As we can see in [Figure 4-3](#kubelet_performing_livenessprobe_to_loca),
    this probe is executed on every node where we find a pod from the deployment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，将这些健康检查配置与系统的运行时组件关联起来而不是网络功能是一个有意为之的选择。在 Kubernetes 和 OpenShift 中，`livenessProbe`
    是由 `kubelet` 执行而不是由负载均衡器执行。正如我们在[图 4-3](#kubelet_performing_livenessprobe_to_loca)中所看到的，这个探针在每个包含部署
    pod 的节点上执行。
- en: '![](assets/hcok_0403.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0403.png)'
- en: Figure 4-3\. `kubelet` performing `livenessProbe` to local nginx pod
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. `kubelet` 执行 `livenessProbe` 到本地 nginx pod
- en: Every `kubelet` (one per worker node) is responsible for performing all of the
    probes for all containers running on the `kubelet`’s node. This ensures a highly
    scalable solution that distributes the probe workload and maintains the state
    and availability of pods and containers across the entire fleet.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `kubelet`（每个工作节点一个）负责执行节点上运行的所有容器的所有探针。这确保了一个高度可扩展的解决方案，分发探针工作负载，并在整个机群中维护
    pod 和容器的状态和可用性。
- en: The infrastructure
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础设施
- en: 'Where does the networking come from in Kubernetes? That’s the job of the Kubernetes
    service. Here is where things get interesting. Let’s look at how traffic travels
    to our `nginx` pod. First, our service definition:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的网络是从哪里来的？这是 Kubernetes 服务的工作。这里的事情变得有趣起来。让我们看看流量如何到达我们的 `nginx`
    pod。首先，我们的服务定义：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This service definition tells Kubernetes that it should route any traffic in
    the cluster that is sent to `172.21.102.110` on port `80` to any pods that match
    the selector `app=webserver` on port `8080`. The advantage here is that as we
    scale our webserver application up and down or update it and the pods are replaced,
    the service keeps the endpoints up to date. A great example of endpoint recovery
    can be seen if we delete all the pods and let our deployment replace them automatically
    for us:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务定义告诉 Kubernetes 应该将发送到集群中 `172.21.102.110` 端口 `80` 的任何流量路由到与选择器 `app=webserver`
    匹配的任何 pod 上的端口 `8080`。这里的优势在于，当我们扩展或更新我们的 web 服务器应用程序并且 pod 被替换时，服务会保持端点的最新状态。如果我们删除所有的
    pod 并让我们的部署自动替换它们，可以看到端点恢复的一个很好的例子：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This automatic management of the endpoints for a service begins to show some
    of the power of Kubernetes versus traditional infrastructure. There’s no need
    to update server configurations in HAProxy. The Kubernetes service does all the
    work to keep our “load balancer” up to date. To assemble that list of endpoints,
    the Kubernetes service looks at all pods that are in the `Ready` state and matches
    the label selector from the service definition. If a container fails a probe or
    the node that the pod is hosted on is no longer healthy, then the pod is marked
    `NotReady` and the service will stop routing traffic to that pod. Here we again
    see the role of the `livenessProbe` (as well as the `readinessProbe`), which is
    not to directly affect routing of traffic, but rather to inform the state of the
    pod and thus the list of endpoints used for routing service traffic.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自动管理服务端点的功能开始展示了 Kubernetes 与传统基础设施的一些优势。无需更新 HAProxy 中的服务器配置。Kubernetes 服务会完成所有工作，以保持我们的“负载均衡器”最新。为了组装端点列表，Kubernetes
    服务会查看所有处于 `Ready` 状态的 pod，并匹配服务定义中的标签选择器。如果容器失败了探测，或者托管 pod 的节点不再健康，那么该 pod 将被标记为
    `NotReady`，服务将停止将流量路由到该 pod。在这里，我们再次看到 `livenessProbe`（以及 `readinessProbe`）的作用，它的作用不是直接影响流量路由，而是通知
    pod 的状态，从而影响用于路由服务流量的端点列表。
- en: In a situation where we have multiple replicas of our application spread across
    multiple worker nodes, we have a solution with the service where even if a worker
    node dies, we won’t lose any availability. Kubernetes is smart enough to recognize
    that the pods from that worker node may no longer be available and will update
    the service endpoint list across the full cluster to stop sending traffic to the
    pods on the failed node.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用程序有多个副本分布在多个工作节点的情况下，我们有一个服务的解决方案，即使一个工作节点发生故障，我们也不会丢失任何可用性。Kubernetes
    足够智能，能够意识到来自该工作节点的 pod 可能不再可用，并将在整个集群中更新服务端点列表，停止向故障节点上的 pod 发送流量。
- en: 'We’ve just discussed what happens when a compute node dies: the service itself
    is smart enough to route traffic around any pods on that node. In fact, the deployment
    will then also replace those pods from the failed node with new pods on another
    node after rescheduling them, which is very handy for dealing with compute failures.
    What about the need to provide a “load balancer” for the service? Kubernetes takes
    a very different approach in this matter. It uses the kube-proxy component to
    provide the load-balancing capabilities. The interesting thing about kube-proxy
    is that it is a fully distributed component providing a load-balancing solution
    that blends the distributed nature of client-side load balancers and the frontend
    of a traditional load balancer. There is not a separate client-side load balancer
    for every client instance, but there is one per compute node.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚讨论了当计算节点发生故障时会发生什么：服务本身足够智能，能够绕过在该节点上的任何 pod 进行流量路由。事实上，部署将会在重新调度后，将来自故障节点的那些
    pod 替换为另一个节点上的新 pod，这对处理计算故障非常方便。那么为服务提供“负载均衡器”的需求呢？Kubernetes 在这个问题上采取了非常不同的方法。它使用
    kube-proxy 组件提供负载均衡功能。kube-proxy 的有趣之处在于，它是一个完全分布式的组件，提供了一个将客户端负载均衡器的分布式特性和传统负载均衡器的前端融合在一起的负载均衡解决方案。并不是每个客户端实例都有一个独立的客户端负载均衡器，而是每个计算节点有一个。
- en: Note that kube-proxy is responsible for translating service definitions and
    their endpoints into iptables^([3](ch04.html#ch01fn28)) or IPVS^([4](ch04.html#ch01fn29))
    rules on each worker node to route the traffic appropriately. For simplicity,
    we do not include the iptables/IPVS rules themselves in our diagram; we represent
    them with kube-proxy, as shown in [Figure 4-4](#kube_proxy_routing_traffic_to_data_acces).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，kube-proxy 负责将服务定义及其端点转换为每个工作节点上的 iptables^([3](ch04.html#ch01fn28)) 或 IPVS^([4](ch04.html#ch01fn29))
    规则，以适当地路由流量。为简单起见，我们在图中没有包含 iptables/IPVS 规则本身；我们用 kube-proxy 来代表它们，如 [图 4-4](#kube_proxy_routing_traffic_to_data_acces)
    所示。
- en: '![](assets/hcok_0404.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0404.png)'
- en: Figure 4-4\. kube-proxy routing traffic to data-access endpoints
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. kube-proxy 将流量路由到数据访问端点
- en: In the example in [Figure 4-4](#kube_proxy_routing_traffic_to_data_acces), note
    that the webserver (client) will route traffic to the local kube-proxy instance,
    and then kube-proxy sends traffic along to the data-access service pods running
    on the local and/or remote nodes. Thus, if any one of these nodes fails, then
    the client application and the kube-proxy that does the load balancing for that
    node die together. As a result, the kube-proxy is a key component that provides
    a simplified and distributed solution for high availability in the Kubernetes
    system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4-4](#kube_proxy_routing_traffic_to_data_acces) 的示例中，请注意，Web 服务器（客户端）将流量路由到本地
    kube-proxy 实例，然后 kube-proxy 将流量发送到运行在本地和/或远程节点上的数据访问服务 Pod。因此，如果其中任何一个节点失败，那么客户端应用程序和负责该节点负载均衡的
    kube-proxy 将一同失败。因此，kube-proxy 是在 Kubernetes 系统中提供简化和分布式高可用解决方案的关键组件。
- en: We have now reviewed how Kubernetes has engineered a solution to maintain system
    uptime despite a failure of any single compute node, service instance, or “load
    balancer” (kube-proxy), thus allowing us to maintain the availability of our entire
    system automatically. No SRE intervention is required to identify or resolve the
    failure. Time and again, we see issues where a Kubernetes-hosted system does suffer
    some failure despite this. This is often the result of poorly written probes that
    are unable to account for some edge-case application failure. Kubernetes does
    an excellent job of providing the platform and guidelines for ultimate availability,
    but it is not magic. Users will need to bring well-engineered and properly configured
    applications and services to see this potential.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在审查了 Kubernetes 如何设计解决方案，以确保系统在任何单个计算节点、服务实例或“负载均衡器”（kube-proxy）失败时仍能保持系统的正常运行时间，从而使我们能够自动维持整个系统的可用性。无需
    SRE 干预即可识别或解决故障。然而，我们时常看到 Kubernetes 托管的系统尽管如此仍会遭遇一些故障。这通常是由于编写不良的探针导致无法应对某些边缘情况下的应用程序故障所致。Kubernetes
    在提供平台和最佳可用性指南方面做得非常好，但它并非魔法。用户需要带来良好设计和适当配置的应用程序和服务，才能看到这种潜力。
- en: There are other networking solutions for load balancing as well. The kube-proxy
    is great, but as we have noted, it really works only when routing traffic from
    a client that lives in the cluster. For routing client requests from outside the
    cluster, we’ll need a slightly different approach. Solutions such as an ingress
    controller, OpenShift Router, and Istio Ingress Gateway enable the same integration
    with the Kubernetes service concept but provide a way to route traffic from outside
    the cluster to the service and applications running inside the cluster. It is
    important to note that you will need to combine these Layer 7 routing solutions
    with an external Layer 4 load balancer in order to maintain proper availability.
    [Figure 4-5](#external_load_balancer_and_openshift_rou) shows how an external
    load balancer is combined with an OpenShift Router to get a request from the actor
    outside the cluster to the web server that is running in the cluster. Note that
    when these network solutions are used, they do not route their traffic through
    kube-proxy; they do their own load balancing directly to the endpoints of the
    service.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他网络解决方案用于负载均衡。kube-proxy 很棒，但正如我们所述，它只在从集群中的客户端路由流量时才真正起作用。对于从集群外部的客户端请求路由，我们将需要一种略有不同的方法。诸如
    Ingress Controller、OpenShift Router 和 Istio Ingress Gateway 等解决方案能够与 Kubernetes
    服务概念集成，但提供一种方式将流量从集群外部路由到集群内运行的服务和应用程序。需要注意的是，您将需要将这些 Layer 7 路由解决方案与外部 Layer
    4 负载均衡器结合使用，以保持适当的可用性。[图 4-5](#external_load_balancer_and_openshift_rou) 显示了外部负载均衡器如何与
    OpenShift Router 结合使用，将来自集群外部的请求发送到运行在集群内的 Web 服务器。当使用这些网络解决方案时，请注意它们不会通过 kube-proxy
    路由其流量；它们会直接向服务的端点进行自己的负载均衡。
- en: '![](assets/hcok_0405.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0405.png)'
- en: Figure 4-5\. External load balancer and OpenShift Router
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 外部负载均衡器和 OpenShift Router
- en: All of the same rules and logic for redundancy and availability apply here as
    we have discussed thus far. The external load balancer is just like the HAProxy
    example we provided earlier in this chapter. There simply is no escaping the laws
    of networking. You still need a highly available solution to get traffic from
    the outside world into the cluster. The advantage here is that we don’t need to
    set up a separate load balancer for every application in our cluster. The OpenShift
    Router, ingress controller, and Istio Ingress Gateway can all serve many different
    applications in the cluster because they support Layer 7 routing, not just Layer
    4\. Layer 7 routers can inspect HTTP(S) headers to understand the target URL and
    use that information to send traffic to the correct service. It’s important to
    note that the OpenShift Router and ingress controller still rely on the Kubernetes
    service to do the liveness and readiness probe work to determine which pods the
    Router or ingress controller should send traffic to.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所有我们迄今为止讨论过的冗余性和可用性规则和逻辑在这里同样适用。外部负载均衡器就像我们在本章前面提供的 HAProxy 示例一样。在网络领域，根本无法逃避这些规则。要从外部世界将流量传送到集群中，仍然需要一个高可用解决方案。这里的优势在于，我们不需要为集群中的每个应用程序设置单独的负载均衡器。OpenShift
    Router、入口控制器和 Istio 入口网关都可以为集群中的许多不同应用程序提供服务，因为它们支持第 7 层路由，而不仅仅是第 4 层。第 7 层路由器可以检查
    HTTP(S) 标头以理解目标 URL，并使用该信息将流量发送到正确的服务。重要的是要注意，OpenShift Router 和入口控制器仍然依赖于 Kubernetes
    服务来执行 liveness 和 readiness 探针工作，以确定 Router 或入口控制器应将流量发送到哪些 pod。
- en: While these are more complex solutions, we simply need to have something that
    allows for routing external traffic to services hosted in our cluster. We have
    left discussion of the `NodePort` concept upon which the external load balancer
    solution is based to the reader to investigate from the [Kubernetes documentation](https://oreil.ly/NxmdG).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些是更复杂的解决方案，但我们只需有一些允许将外部流量路由到我们集群中托管服务的东西即可。我们已经将外部负载均衡器解决方案所依赖的 `NodePort`
    概念的讨论留给读者从 [Kubernetes 文档](https://oreil.ly/NxmdG) 中了解。
- en: The result
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果
- en: Looking back to our initial availability discussion, Kubernetes has supplied
    a mechanism to identify and repair an issue. Based on our application `livenessProbe`
    settings, it would take a maximum of six seconds to identify the problem and then
    some amount of time to reroute traffic (mitigating any failed requests), and then
    it would automatically repair the failed container by restarting it. That container-restart
    process in an efficient application can be quite speedy indeed, subsecond. Even
    conservatively at 10 seconds, that looks pretty good for our MTTR. We could be
    more aggressive with our settings and do a `livenessProbe` every two seconds,
    kick a restart on just a single failure, and get to a sub-five-second MTTR.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们最初的可用性讨论，Kubernetes 提供了一种识别和修复问题的机制。根据我们的应用程序 `livenessProbe` 设置，最多需要六秒来识别问题，然后需要一些时间重新路由流量（减少任何失败请求），然后自动通过重启修复失败的容器。在高效应用中，容器重启过程可能非常快，甚至亚秒级别。即使保守估计为
    10 秒，对于我们的 MTTR 看起来也相当不错。我们可以通过调整设置更加积极，每两秒执行一次 `livenessProbe`，在单个故障时执行重启，将 MTTR
    缩短至不到五秒。
- en: 'A quick note on `livenessProbe` and `readinessProbe` settings: remember that
    the `kubelet` is going to be making requests to every instance of these containers
    across the entire cluster. If you have hundreds of replicas running and they are
    all being hit every two seconds by the probes, then things could get very chatty
    in your logging or metrics solution. It may be a nonissue if your probes are hitting
    fairly light endpoints that are comparable to those that may be hit hundreds or
    thousands of times a second to handle user requests, but it’s worth noting all
    the same.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 `livenessProbe` 和 `readinessProbe` 设置的一点快速说明：请记住，`kubelet` 将会向整个集群中的这些容器的每个实例发出请求。如果您运行了数百个副本，并且它们每两秒都受到探针的影响，那么您的日志或指标解决方案可能会变得非常啰嗦。如果您的探针命中的是相对轻的端点，这些端点可能会处理用户请求中的数百甚至数千次，那么这可能不是问题，但无论如何，这也值得注意。
- en: Now that we have some idea of the kind of MTTR we could achieve with Kubernetes
    for our application, let’s take a look at what this does for some of our availability
    calculations to determine MTBF for our system in [Table 4-4](#mtbf_calculations_for_mttr).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对于 Kubernetes 在我们的应用程序中实现的 MTTR 有了一些了解，让我们来看看这对我们的可用性计算中的一些 MTBF 有何影响，以确定我们系统中的
    [表格 4-4](#mtbf_calculations_for_mttr)。
- en: Table 4-4\. MTBF calculations for MTTR
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4-4\. MTTR 计算为 MTBF
- en: '| MTTR | MTBF for 99% availability | MTBF for 99.99% availability |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| MTTR | 99% 可用性的 MTBF | 99.99% 可用性的 MTBF |'
- en: '| --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 5 seconds | 8.26 minutes | 13 hours 53 minutes |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 5 秒 | 8.26 分钟 | 13 小时 53 分钟 |'
- en: '| 10 seconds | 16.5 seconds | 27 hours 46 minutes |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 10 秒 | 16.5 秒 | 27 小时 46 分钟 |'
- en: '| 1 minute | 99 seconds | 6 days 22.65 hours |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 1 分钟 | 99 秒 | 6 天 22.65 小时 |'
- en: '| 10 minutes | 16 minutes 30 seconds | 69 days 10.5 hours |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 10 分钟 | 16 分钟 30 秒 | 69 天 10.5 小时 |'
- en: Looking at this data, we can start to see how with the power of just the simple
    Kubernetes deployment concept, we can get to some reasonable availability calculations
    that we could sustain.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些数据，我们可以看到，仅仅通过简单的 Kubernetes 部署概念，我们可以得出一些合理的可用性计算，我们可以持续维护这些计算。
- en: Done and dusted, right? Not so fast. Different failure modes can have a variety
    of different results for Kubernetes and OpenShift. We’ll take a look at these
    failures next.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 做完了，结束了，对吧？不要那么快。不同的故障模式对 Kubernetes 和 OpenShift 可能会产生不同的结果。接下来我们将看看这些故障。
- en: Failure Modes
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 失败模式
- en: You can experience a variety of different types of failures in your Kubernetes
    and OpenShift systems. We’ll cover many of these failures, how to identify them,
    what the application or service impact is, and how to prevent them.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 Kubernetes 和 OpenShift 系统中可能会遇到各种类型的故障。我们将涵盖许多这些故障，如何识别它们，应用程序或服务的影响以及如何预防它们。
- en: Application Pod Failure
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序 Pod 失败
- en: A single application pod failure ([Figure 4-6](#application_pod_failure)) has
    the potential to result in almost no downtime, as based on our earlier discussion
    around probes and the `kubelet` ability to automatically restart failed pods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 单个应用程序 Pod 的失败（[图 4-6](#application_pod_failure)）可能导致几乎没有停机时间，正如我们早些时候讨论过的探测和
    `kubelet` 自动重新启动失败的 Pod。
- en: '![](assets/hcok_0406.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0406.png)'
- en: Figure 4-6\. Application pod failure
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 应用程序 Pod 失败
- en: 'It is worth noting that longer probe intervals can result in longer MTTR as
    that increases the time to detect the failure. It’s also interesting to note that
    if you are using `readinessProbes`, it can take some time for a readiness failure
    to propagate through the Kubernetes or OpenShift system. The total time for recovery
    can be:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，更长的探测间隔可能会导致更长的 MTTR，因为这会增加检测故障的时间。有趣的是，如果您使用 `readinessProbes`，则需要一些时间才能使就绪失败在
    Kubernetes 或 OpenShift 系统中传播。恢复的总时间可能是：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `iptables-min-sync-period` is a new configuration setting we have not discussed
    yet. This setting (or `ipvs-min-sync-period` in the case of IPVS mode) of kube-proxy
    determines the shortest period of time that can elapse between any two updates
    of the iptables rules for kube-proxy.^([5](ch04.html#ch01fn30)) If one endpoint
    in the cluster is updated at t0 and then another endpoint update occurs five seconds
    later but the `min-sync` is set to 30 seconds, then kube-proxy (iptables/IPVS)
    will still continue to route traffic to potentially invalid endpoints. The default
    for this setting is one second and thus minimal impact. However, in very large
    clusters with huge numbers of services (or huge numbers of endpoints), this interval
    may need to be increased to ensure reasonable performance of the iptables/IPVS
    control plane. The reason for this is that as there are more and more services
    to manage, the size of the iptables or IPVS rule set can become massive, easily
    tens of thousands of rules. If the `iptables-min-sync-period` is not adjusted,
    then kube-proxy can become overwhelmed with constantly trying to update the rules.
    It’s important to remember this relationship. Cluster size can impact performance
    and responsiveness of the entire system.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`iptables-min-sync-period` 是我们尚未讨论的新配置设置。这个设置（或在 IPVS 模式下的 `ipvs-min-sync-period`）是
    kube-proxy 决定在两次更新 kube-proxy 的 iptables 规则之间可以经过的最短时间。^([5](ch04.html#ch01fn30))
    如果集群中的一个端点在 t0 更新，然后另一个端点在五秒钟后更新，但 `min-sync` 设置为 30 秒，那么 kube-proxy（iptables/IPVS）仍将继续将流量路由到可能无效的端点。这个设置的默认值是一秒，因此影响最小。然而，在非常大的集群中，有大量的服务（或大量的端点），可能需要增加这个间隔，以确保
    iptables/IPVS 控制平面的合理性能。这是因为随着要管理的服务越来越多，iptables 或 IPVS 规则集的大小可能会变得非常庞大，轻松地达到数万条规则。如果不调整
    `iptables-min-sync-period`，那么 kube-proxy 可能会因不断尝试更新规则而不堪重负。记住这种关系是很重要的。集群的大小可以影响整个系统的性能和响应能力。'
- en: Worker Node Failure
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作节点故障
- en: One of the most common failures seen in any OpenShift or Kubernetes cluster
    is the single worker node failure ([Figure 4-7](#worker_node_failure-id00002)).
    Single node faults can occur for a variety of reasons. These issues include disk
    failure, network failure, operating system failure, disk space exhaustion, memory
    exhaustion, and more. Let’s take a closer look at what takes place in the Kubernetes
    system when there is a node failure. The process to recover a single node will
    directly impact our availability calculations of the system.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何 OpenShift 或 Kubernetes 集群中最常见的故障之一是单个工作节点故障（[图 4-7](#worker_node_failure-id00002)）。单节点故障可能由多种原因引起。这些问题包括磁盘故障、网络故障、操作系统故障、磁盘空间耗尽、内存耗尽等。让我们更仔细地看看当
    Kubernetes 系统中存在节点故障时会发生什么。恢复单个节点的过程将直接影响我们对系统可用性的计算。
- en: '![](assets/hcok_0407.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0407.png)'
- en: Figure 4-7\. Worker node failure
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 工作节点故障
- en: 'The following are the time intervals that occur when responding to a worker
    node failure:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 响应工作节点故障时发生的时间间隔如下：
- en: T0
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: T0
- en: Failure occurs on node.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 节点发生故障。
- en: T1 = T0 + `nodeLeaseDurationSeconds`
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: T1 = T0 + `nodeLeaseDurationSeconds`
- en: Kubernetes and OpenShift now use a lease system to track the health of worker
    nodes. Each `kubelet` registers a lease object for itself with the control plane.
    If that lease expires, then the node is immediately marked as `NotReady`. As soon
    as the node is marked `NotReady`, then the service controller will denote that
    any pods on that node may have failed and any service endpoints associated with
    the pods on that node should be removed. This is how Kubernetes decides to stop
    routing traffic to pods on a failed node. The result is the same as the `liveness​P⁠robe`
    failing for the pod but affects all pods on the failed node. `nodeLease​Dura⁠tionSeconds`
    is configured on the `kubelet` and determines how long the lease will last. The
    default time is 40 seconds.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 和 OpenShift 现在使用租约系统来跟踪工作节点的健康状况。每个 `kubelet` 向控制平面注册一个自己的租约对象。如果该租约过期，节点将立即标记为
    `NotReady`。一旦节点标记为 `NotReady`，服务控制器将指示该节点上的任何 pod 可能已失败，并且应该移除与该节点上的 pod 相关的任何服务端点。这是
    Kubernetes 决定停止路由流量到故障节点上的 pod 的方式。其结果类似于 pod 的 `livenessProbe` 失败，但会影响到故障节点上的所有
    pod。`nodeLeaseDurationSeconds` 配置在 `kubelet` 上，决定租约的持续时间，默认为 40 秒。
- en: T2 = T1 + `iptables-min-sync-period`
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: T2 = T1 + `iptables-min-sync-period`
- en: This is the additional time that may be required to update the fleet of kube-proxy
    configurations across the cluster. This behavior is identical to the behavior
    that was discussed earlier in our application pod failure scenario where any changes
    in the endpoint list for a service need to be synced to all of the kube-proxy
    instances across the cluster. Once the sync completes, then all traffic is routed
    properly, and any issues from the failed node will be resolved minus the capacity
    lost. The default minimum sync period is 30 seconds.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可能需要的额外时间，用于在集群中同步 kube-proxy 配置的整个飞机队。此行为与我们应用程序 pod 故障场景中讨论的行为完全相同，其中需要将服务的端点列表中的任何更改同步到集群中的所有
    kube-proxy 实例。一旦同步完成，所有流量将被正确路由，任何来自故障节点的问题都将得到解决，但减少的容量除外。默认的最小同步周期为 30 秒。
- en: T3 = T2 + `pod-eviction-timeout`
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: T3 = T2 + `pod-eviction-timeout`
- en: This setting is configured on the kube-controller-manager and determines how
    long the controller will wait after a node is marked `NotReady` before it will
    attempt to terminate pods on the `NotReady` node. If there are controllers managing
    the pods that are terminating, then typically they will start a new replacement
    pod, as is the case with the deployment controller logic. This does not affect
    routing of traffic to the pods on the bad node; that was addressed at T2\. However,
    this will recover any diminished capacity associated with the pods from the failed
    node. The default timeout is five minutes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置配置在 kube-controller-manager 上，决定在节点标记为 `NotReady` 后，控制器将等待多久才会尝试终止 `NotReady`
    节点上的 pod。如果有控制器管理正在终止的 pod，通常它们将启动新的替代 pod，就像部署控制器逻辑的情况一样。这不会影响到流量路由到故障节点上的 pod；这个问题在
    T2 已经解决。然而，这将恢复与来自故障节点的 pod 相关的任何降低的容量。默认超时时间为五分钟。
- en: As we can see, the impact of a node failure is on a similar order of magnitude
    as an application pod failure. Note that here we are calculating the worst-case
    scenario to get to T2 where service is recovered. Some node failures may be detected
    and reported faster, and as a result, the T1 time can be quicker as we don’t have
    to wait for the lease to expire for the node to be marked `NotReady`. Also remember
    that the iptables may sync faster; this is just the worst-case scenario.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，节点故障的影响程度类似于应用容器故障。请注意，在此，我们计算的是达到 T2 时的最坏情况。某些节点故障可能会被检测并迅速报告，因此 T1 时间可以更短，因为我们不必等待租约到期才能将节点标记为
    `NotReady`。另请记住，iptables 可能会同步得更快，这只是最坏情况下。
- en: Worker Zone Failure
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作区故障
- en: A zone failure is handled much the same as a worker node failure. In a total
    zone failure ([Figure 4-8](#worker_zone_failure-id00004)), such as a network failure,
    we would typically expect all nodes to be marked as `NotReady` within a reasonably
    small window of time.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 工作区故障的处理方式与节点故障相似。在完全工作区故障（[图 4-8](#worker_zone_failure-id00004)）的情况下，例如网络故障，我们通常期望在一个相对较短的时间窗口内将所有节点标记为
    `NotReady`。
- en: '![](assets/hcok_0408.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0408.png)'
- en: Figure 4-8\. Worker zone failure
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 4-8 图：工作区故障
- en: A zone failure can be “detected” by Kubernetes and OpenShift if the number of
    nodes in a single zone that is failing exceeds the `unhealthy-zone-threshold`
    percentage.^([6](ch04.html#ch01fn31)) If a zone failure is detected, there is
    no change to the service endpoint evaluation process. All pods on the failed nodes
    will be removed as endpoints for any associated services. The `unhealthy-zone-threshold`
    will change what happens to the pod eviction process. If the cluster is below
    the `large-cluster-size-threshold` (defined as a count of nodes), then all pod
    eviction in the zone will stop. If `unhealthy-zone-threshold` is above the large
    cluster size, then pod eviction starts, but nodes are processed at a much slower
    rate as defined by the `secondary-node-eviction-rate`. This is in an effort to
    prevent a storm of pod scheduling and allows the system some additional time to
    recover without “overreacting” to a temporary failure.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单个区域中故障节点的数量超过 `unhealthy-zone-threshold` 百分比，[Kubernetes](https://kubernetes.io/)和[OpenShift](https://www.openshift.com/)就可以“检测”到区域故障。如果检测到区域故障，则服务端点评估过程没有任何变化。所有位于故障节点上的容器都会被移除作为任何关联服务的端点。`unhealthy-zone-threshold`
    将决定 pod 驱逐过程的变化。如果集群低于 `large-cluster-size-threshold`（定义为节点的数量），则该区域的所有 pod 驱逐将停止。如果
    `unhealthy-zone-threshold` 超过大集群大小，则 pod 驱逐开始，但节点的处理速度会显著减慢，如 `secondary-node-eviction-rate`
    所定义。这是为了防止 pod 调度风暴，并允许系统在“过度反应”到暂时故障之前获得一些额外的恢复时间。
- en: Control Plane Failure
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制平面故障
- en: There are a variety of different control plane failure scenarios. There are
    network-related failures between worker nodes and the control plane, and there
    are control plane component failures. We’ll get into the details of the high availability
    features of the various control plane components later in this section. Before
    we get started with those details, it’s worth talking about what happens when
    there is any kind of total control plane failure. How does the cluster overall
    respond to a failure like the one shown in [Figure 4-9](#complete_control_plane_failure)?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种不同的控制平面故障场景。存在工作节点和控制平面之间的网络相关故障，也存在控制平面组件故障。稍后在本节中，我们将详细介绍各种控制平面组件的高可用性特性。在我们开始这些细节介绍之前，值得谈一谈当出现任何形式的完全控制平面故障时，会发生什么。集群整体如何响应如[图
    4-9](#complete_control_plane_failure)所示的故障？
- en: '![](assets/hcok_0409.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0409.png)'
- en: Figure 4-9\. Complete control plane failure
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图   第 4-9 图：完全控制平面故障
- en: The good news is that all of the workloads running on the worker nodes are unaffected
    in this scenario. All of the kube-proxy configurations, ingress controllers, pods,
    `DaemonSets`, network policies, and so on continue to function exactly as they
    were at the time of the control plane failure. The bad news is that no changes
    or updates can be made to the system in this state. In addition, if a worker node
    fails during this scenario, then there is no detection or updating of the cluster
    to compensate for the failure. Fortunately, the likelihood of such a failure is
    low if the control plane is implemented properly. We’ll cover some of the HA features
    of the control plane now.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，在此场景中，运行在工作节点上的所有工作负载不受影响。所有的 kube-proxy 配置、入口控制器、Pod、`DaemonSets`、网络策略等继续像控制平面故障发生时一样正常运行。坏消息是，在此状态下无法进行任何更改或更新系统。此外，如果在此场景中工作节点发生故障，则无法检测或更新集群以弥补故障。幸运的是，如果控制平面正确实现，这种故障的可能性很低。现在我们将介绍一些控制平面的高可用特性。
- en: Rather than keep everyone waiting, we’ll start with the most complex, potentially
    most challenging, and easily most critical component within the control plane.
    The component worthy of the title of “most critical” is etcd. Kubernetes and OpenShift
    use the open source key value store etcd as their persistent storage solution.
    The etcd datastore is where all objects in the system are persisted. All of the
    data and information related to the nodes, deployments, pods, services, and so
    on are stored in etcd.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 相比让所有人等待，我们将从控制平面中最复杂、潜在最具挑战性且易于最关键的组件开始。称为“最关键”的组件是 etcd。Kubernetes 和 OpenShift
    使用开源键值存储 etcd 作为其持久性存储解决方案。etcd 数据存储是系统中所有对象持久化的地方。与节点、部署、Pod、服务等相关的所有数据和信息都存储在
    etcd 中。
- en: etcd failures
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: etcd 失败
- en: You can encounter many different issues with etcd. Some are easier to mitigate
    than others. Connectivity failures from the kube-apiserver to etcd may be the
    most common. The modern etcd client has load balancing and resiliency built in.
    Note that if you have a traditional load balancer that sits in front of your etcd
    instance, then much of this does not apply, as you likely have a single endpoint
    registered for etcd. Regardless, the kube-apiserver will use the `--etcd-servers`
    setting to determine what IP(s) and/or hostname(s) the etcd go client is configured
    with. There is excellent [documentation about the load balancing behavior](https://oreil.ly/TUc0U)
    in the community. It’s worth noting that most modern Kubernetes and OpenShift
    versions are now using the clientv3-grpc1.23 variant in their client. This version
    of the client maintains active gRPC subconnections at all times to determine connectivity
    to the etcd server endpoints.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到 etcd 的许多不同问题。有些问题比其他问题更容易缓解。从 kube-apiserver 到 etcd 的连接故障可能是最常见的问题。现代的
    etcd 客户端内置了负载均衡和弹性。请注意，如果您有一个传统的负载均衡器位于您的 etcd 实例前面，那么大部分情况下这并不适用，因为您可能已经为 etcd
    注册了单个终端节点。不过，kube-apiserver 将使用 `--etcd-servers` 设置来确定 etcd Go 客户端配置的 IP(s) 和/或主机名(s)。社区中有关于负载均衡行为的优秀[文档](https://oreil.ly/TUc0U)。值得注意的是，大多数现代
    Kubernetes 和 OpenShift 版本现在都在其客户端中使用 clientv3-grpc1.23 变体。该客户端版本始终维持活动的 gRPC 子连接，以确定与
    etcd 服务器端点的连接状态。
- en: 'The failure recovery behavior of the etcd client enables the system to deal
    with communication failures between the kube-apiserver and etcd. Unfortunately,
    the etcd client cannot handle issues like performance degradation of a peer or
    network partitioning. This can be mitigated to some extent with excellent readiness
    checking of the etcd peers. If your system allows for readiness checking (for
    example, etcd hosted on Kubernetes), you can configure readiness checks that can
    look for network partitioning or severe performance degradation using something
    like:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: etcd 客户端的故障恢复行为使系统能够处理 kube-apiserver 与 etcd 之间的通信故障。不幸的是，etcd 客户端无法处理像同行的性能降级或网络分区等问题。可以通过对
    etcd 同行进行出色的就绪检查在某种程度上缓解这些问题。如果您的系统允许就绪检查（例如，在 Kubernetes 上托管的 etcd），您可以配置就绪检查，这些就绪检查可以查找网络分区或严重性能降级，例如使用以下内容：
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Using this code will ensure that the local peer is able to properly perform
    a serialized read and is a stronger check to make sure that not only is this peer
    listening but also it can handle get requests. A put test can be used (which also
    ensures peer connectivity), but this can be a very expensive health check that
    introduces tons of key revisions and may adversely impact the etcd cluster. We
    are conducting an ongoing investigation to determine whether etcd performance
    metrics are a reliable indicator that a peer is suffering performance degradation
    and should be removed from the cluster. At this point, we can only recommend standard
    monitoring and SRE alerting to observe performance issues. There is not yet strong
    evidence that you can safely kill a peer because of performance issues and not
    potentially adversely impact overall availability of the cluster.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码将确保本地对等体能够正确执行序列化读取，并且是一个更强的检查，以确保此对等体不仅在侦听，还可以处理获取请求。可以使用 put 测试（也确保对等体连通性），但这可能是一个非常昂贵的健康检查，引入大量关键修订，并可能对
    etcd 集群产生不利影响。我们正在进行持续调查，以确定 etcd 性能指标是否是对对等体正在遭受性能退化的可靠指标，并应从集群中移除。在此时点上，我们只能建议使用标准监控和
    SRE 警报来观察性能问题。目前还没有强有力的证据表明，由于性能问题，您可以安全地终止对等体而不会可能对集群的整体可用性产生不利影响。
- en: Note
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'It would be poor form to have all of this discussion about improving availability
    of etcd without also mentioning how critical it is to have a proper disaster-recovery
    process in place for etcd. Regular intervals of automated backups are a must-have.
    Ensure that you regularly test your disaster-recovery process for etcd. Here’s
    a bonus tip for etcd disaster recovery: even if quorum is broken for the etcd
    cluster, if etcd processes are still running, you can take a new backup from that
    peer even without quorum to ensure that you minimize data loss for any time delta
    from the last regularly scheduled backup of etcd. Always try to take a backup
    from a remaining member of etcd before restoring. The [Kubernetes documentation](https://oreil.ly/jCvAb)
    has some excellent info on etcd backup and restore options.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果讨论如何改善 etcd 的可用性而不提及为 etcd 设置适当的灾难恢复过程将是不合适的。定期进行自动化备份是必须的。确保定期测试 etcd 的灾难恢复流程。关于
    etcd 灾难恢复的额外提示：即使 etcd 集群的法定人数已经中断，如果 etcd 进程仍在运行，您仍然可以从该对等体获取新的备份，即使没有法定人数也可以确保最小化从上次定期计划备份
    etcd 开始的任何时间间隔的数据丢失。始终尝试从剩余的 etcd 成员获取备份，然后再进行恢复。[Kubernetes 文档](https://oreil.ly/jCvAb)
    提供了关于 etcd 备份和恢复选项的优秀信息。
- en: The most significant concern with etcd is maintaining quorum of the etcd cluster.
    etcd will break quorum any time less than a majority of the peers are healthy.
    If quorum breaks, then the entire Kubernetes/OpenShift control plane will fail
    and all control plane actions will begin to fail. Increasing the number of peers
    will increase the number of peer failures that can be sustained. Note that having
    an even number of peers doesn’t increase fault tolerance and can harm the cluster’s
    ability to detect split brain-network partitions.^([7](ch04.html#ch01fn32))
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对 etcd 最重要的关注点是维护 etcd 集群的法定人数。只要少于大多数对等体健康，etcd 就会中断法定人数。如果法定人数中断，则整个 Kubernetes/OpenShift
    控制平面将失败，并且所有控制平面操作将开始失败。增加对等体的数量将增加可以容忍的对等体故障的数量。请注意，拥有偶数个对等体不会增加容错能力，并且可能损害集群检测到拆分脑网络分区的能力。^([7](ch04.html#ch01fn32))
- en: kube-apiserver failures
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kube-apiserver 失败
- en: The kube-apiserver is responsible for handling all requests for *create, read,
    update, and delete* (CRUD) operations on objects in the Kubernetes or OpenShift
    cluster. This applies to end-user client applications as well as internal components
    of the cluster, such as worker nodes, kube-controller-manager, and kube-scheduler.
    All interactions with the etcd persistent store are handled through the kube-apiserver.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: kube-apiserver 负责处理 Kubernetes 或 OpenShift 集群中对象的所有 *创建、读取、更新和删除*（CRUD）操作的所有请求。这适用于最终用户客户端应用程序以及集群的内部组件，如工作节点、kube-controller-manager
    和 kube-scheduler。与 etcd 持久存储的所有交互都通过 kube-apiserver 处理。
- en: The kube-apiserver is an active/active scale-out application. It requires no
    leader election or other mechanism to manage the fleet of kube-apiserver instances.
    They are all running in an active mode. This attribute makes it straightforward
    to run the kube-apiserver in a highly available configuration. The cluster administrator
    may run as many instances of kube-apiserver as they wish, within reason, and place
    a load balancer in front of the apiserver. If properly configured, a single kube-apiserver
    failure will have no impact on cluster function. See [Figure 4-10](#kube_apiserver_single_failure_with_no_av)
    for an example.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: kube-apiserver 是一个主动/主动的扩展应用程序。它不需要领导者选举或其他机制来管理 kube-apiserver 实例的群体。它们都以活动模式运行。这个特性使得在高可用配置中运行
    kube-apiserver 变得简单。集群管理员可以运行尽可能多的 kube-apiserver 实例，合理地将负载均衡器放置在 apiserver 前面。如果配置正确，单个
    kube-apiserver 失败将不会对集群功能产生影响。参见 [图 4-10](#kube_apiserver_single_failure_with_no_av)
    的示例。
- en: '![](assets/hcok_0410.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0410.png)'
- en: Figure 4-10\. kube-apiserver single failure with no availability impact
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. kube-apiserver 单个故障而无可用性影响
- en: kube-scheduler and kube-controller-manager failures
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kube-scheduler 和 kube-controller-manager 故障
- en: These two components have very similar availability features. They both run
    with a single active instance at all times and use the same leader election features.
    They leverage the lease object to manage the leader of the cluster. If a follower
    instance of either component fails, there is no impact to the overall cluster
    functionality. If the leader fails, then for the `--leader-elect-lease-duration`
    there will be no leader. However, no component interacts with the scheduler or
    controller manager directly. These two components interact asynchronously on object
    state changes in the cluster.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个组件具有非常相似的可用性特性。它们都始终以单个活动实例运行，并使用相同的领导者选举特性。它们利用租约对象来管理集群的领导者。如果任一组件的跟随者实例失败，则不会对整体集群功能产生影响。如果领导者失败，则在
    `--leader-elect-lease-duration` 时间内将没有领导者。然而，没有任何组件直接与调度器或控制器管理器交互。这两个组件在集群中异步地对对象状态变化进行交互。
- en: Loss of leader for the kube-controller-manager can result in a delay of recovery
    from a node failure, as an example. If a pod fails for some reason, then the controller
    manager will not respond to update the endpoints of the associated service. So
    while a concurrent leader failure with other system failures can result in a prolonged
    MTTR for the system, it will not result in an extended failure or outage.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: kube-controller-manager 失去领导者可能导致从节点故障的恢复延迟，例如。如果某个 pod 因某种原因失败，则控制器管理器将无法响应更新相关服务的端点。因此，虽然同时发生领导者故障和其他系统故障可能会导致系统的平均修复时间延长，但不会导致长时间的故障或停机。
- en: A failed kube-scheduler leader should have almost no impact on availability.
    While the leader is down awaiting a new election, no new pods will be scheduled
    to nodes. So while you may experience diminished capacity due to lack of scheduling
    of pods from a concurrent failed node, you’ll still have service and routing updates
    to the cluster at this time.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 失败的 kube-scheduler 领导者对可用性几乎没有影响。当领导者宕机等待新选举时，将不会将新的 pod 调度到节点上。因此，尽管由于并发失败节点的
    pod 调度不足可能会导致容量减少，但在此期间仍会有对集群的服务和路由更新。
- en: Network Failure
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络故障
- en: 'Networking failures come in many different varieties. We could see failures
    of routing-table updates that cause packets to be misrouted. In other scenarios,
    we might see load-balancer failures. There could be a network-switch failure.
    The list of potential issues is endless. To simplify our discussion of this topic,
    we will bundle network failures into two common flavors: *North-South network
    failures* occur when traffic from outside our Kubernetes or OpenShift cluster
    can no longer enter or exit, and *East-West network failures* describe a situation
    where network traffic cannot travel between the nodes in our cluster.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 网络故障有许多不同的类型。我们可能会看到路由表更新失败导致数据包被错误路由。在其他情况下，我们可能会看到负载均衡器故障。也可能是网络交换机故障。潜在问题的列表是无穷无尽的。为了简化我们对这个主题的讨论，我们将网络故障分为两种常见类型：*南北向网络故障*，即来自
    Kubernetes 或 OpenShift 集群外部的流量无法进入或离开，以及 *东西向网络故障*，描述了集群中节点之间无法传输网络流量的情况。
- en: North-South network failure
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 南北向网络故障
- en: This failure scenario refers to the situation where the cluster itself may be
    healthy, control plane and data plane, but there may be issues routing traffic
    into the cluster from external sources. In [Figure 4-11](#north_south_networking_failure),
    we have a situation where connectivity from the external load balancer to the
    worker node has failed.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此故障场景指的是集群本身可能是健康的，控制平面和数据平面也可能是正常的，但可能存在来自外部源的流量路由问题。在[图 4-11](#north_south_networking_failure)中，我们遇到了外部负载均衡器到工作节点的连接失败的情况。
- en: '![](assets/hcok_0411.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0411.png)'
- en: Figure 4-11\. North-South networking failure
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. 南北网络故障
- en: The result is that the load balancer health checks to any `NodePort`s exposed
    from the connection to the failed worker node will fail, the load balancer will
    stop routing traffic to that worker node, and life goes on. In fact, the MTTR
    here will be quite good, as it will take only the time required for the external
    load balancer to fail the health check to stop routing traffic down the failed
    link, or potentially it would have zero downtime if connection time limits and
    retries are integrated into this load-balancer solution. Load balancers were born
    for this stuff.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，负载均衡器对从连接到失败的工作节点暴露的任何`NodePort`的健康检查将失败，负载均衡器将停止将流量路由到该工作节点，生活还在继续。事实上，在这里的
    MTTR 将非常好，因为只需外部负载均衡器失败健康检查来停止将流量路由到失败的链接，或者如果连接时间限制和重试被集成到这个负载均衡器解决方案中，它可能会有零停机时间。负载均衡器就是为这种情况而生的。
- en: If *all* connectivity between the load balancer and the worker nodes fails,
    then all access from outside the cluster will have failed. The result is real
    downtime for any service whose purpose is to be accessed from outside the cluster.
    The good news is that we can implement multizone clusters to mitigate the chances
    of such catastrophic failures. In [Chapter 8](ch08.html#working_example_of_multicluster_applicat),
    we will discuss using multiple clusters with global DNS-based load balancing for
    further improvements to mitigating geographic network outages. In addition, these
    North-South failures only affect access to services from outside the cluster.
    Everything operating within the cluster may still function without any service
    interruption.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果负载均衡器与工作节点之间的*所有*连接都失败，那么来自集群外部的所有访问都将失败。结果是任何旨在从集群外部访问的服务都将真正停机。好消息是，我们可以实现多区域集群以减轻这种灾难性故障的可能性。在[第
    8 章](ch08.html#working_example_of_multicluster_applicat)中，我们将讨论使用全球基于 DNS 的负载均衡来进一步改善减轻地理网络中断的方法。此外，这些南北故障只影响从集群外部访问服务。在集群内运行的所有内容仍可正常运行，没有任何服务中断。
- en: East-West network failure
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 东西网络故障
- en: We may also encounter scenarios where nodes may lose cross-zone connectivity
    or network partitioning. This is most frequently experienced in multizone clusters
    when there is a network failure between zones. In [Figure 4-12](#east_west_total_failure)
    we look at a full zone failure.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能遇到节点失去跨区域连接或网络分区的情况。这在多区域集群中最常见，当区域之间发生网络故障时。在[图 4-12](#east_west_total_failure)中，我们看到了一个完整区域的故障。
- en: '![](assets/hcok_0412.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0412.png)'
- en: Figure 4-12\. East-West total failure
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-12\. 东西总故障
- en: In this scenario, we are faced with a “clean” zone failure where Zone C is fully
    partitioned. The worker node in the zone will continue to try and renew its lease
    to the control plane node in Zone C. If this were successful, it would lead to
    issues. However, the Zone C control plane will drop out of the cluster and stop
    responding to requests. Two things happen for this to occur. First, etcd in Zone
    C will begin to fail as it will not be able to contact etcd in other zones to
    participate in the quorum on the etcd cluster. As a result, the kube-apiserver
    in Zone C will also start to report back as unhealthy. The worker in Zone C won’t
    be able to renew its lease with the control plane. As a result, it will just sit
    tight and do nothing. The `kubelet` will restart any containers that fail. However,
    as we can see, there is no connectivity between the Zone C worker and other zones.
    Any application communication will begin to fail.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们面临着一个“干净”的区域失败，其中 C 区域被完全分区。该区域的工作节点将继续尝试向 C 区域的控制平面节点更新其租约。如果成功，将导致问题。然而，C
    区域的控制平面将退出集群并停止响应请求。发生这种情况有两个原因。首先，C 区域的 etcd 将开始失败，因为它将无法与其他区域的 etcd 联系以参与 etcd
    集群的仲裁。结果，C 区域的 kube-apiserver 也将开始报告为不健康。C 区域的工作节点将无法续约其与控制平面的租约。结果，它将保持静止并且什么也不做。kubelet
    将重新启动任何失败的容器。但是，正如我们所看到的，C 区域的工作节点与其他区域之间没有连接。任何应用程序通信将开始失败。
- en: The good news is that all will be well in Zones A and B. The control plane etcd
    will retain quorum (electing a new leader in A or B if the leader was previously
    in C), and all control plane components will be healthy. In addition, the lease
    for the worker in Zone C will expire, and thus this node will be marked as `NotReady`.
    All Zone C service endpoints will be removed, and kube-proxy will be updated to
    stop routing traffic to pods in Zone C.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是 A 区域和 B 区域一切都将安好。控制平面 etcd 将保持仲裁（在 A 或 B 中选举新的领导者，如果之前的领导者在 C 中），所有控制平面组件将保持健康。此外，C
    区域的工作节点租约将到期，因此此节点将被标记为 `NotReady`。将移除所有 C 区域的服务端点，并更新 kube-proxy 停止将流量路由到 C 区域的
    Pod。
- en: There are probably a number of scenarios that can leave the Kubernetes or OpenShift
    platform slightly out of sorts. We’re going to focus on one such scenario that
    does expose a weakness in the Kubernetes design. This is a failure where East-West
    traffic only fails between worker nodes. It is not uncommon to provide some network
    isolation between the worker nodes and control plane, which can result in this
    inconsistency between control plane connectivity and worker node connectivity.
    We’ve got an example depicting this in [Figure 4-13](#east_west_data_plane_failure).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有多种情况会使 Kubernetes 或 OpenShift 平台略微失调。我们将专注于一种暴露 Kubernetes 设计弱点的场景。这种失败只在工作节点之间的东西向西流量中发生失败并不罕见，可以在工作节点和控制平面之间提供一些网络隔离，这可能导致控制平面连接性和工作节点连接性之间的不一致性。我们在
    [图 4-13](#east_west_data_plane_failure) 中有一个例子。
- en: '![](assets/hcok_0413.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0413.png)'
- en: Figure 4-13\. East-West data plane failure
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-13\. 东西向数据平面失败
- en: The result here is quite interesting. All worker nodes are able to report back
    as healthy to the control plane. The worker nodes have no validation of communication
    between one another. This also includes kube-proxy, as it is not a traditional
    load balancer. Kube-proxy does not perform any health checking that can validate
    connectivity between workers. The result is that all pods, services, and endpoints
    remain fully intact. That sounds great, but the issue is that if any pod tries
    to route requests across the failed zone boundary, that request will fail and
    kube-proxy does not perform retries. This can be a hassle to detect and/or debug.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的结果非常有趣。所有工作节点能够向控制平面报告正常。工作节点之间没有通信验证。这也包括 kube-proxy，因为它不是传统的负载均衡器。Kube-proxy
    不执行任何健康检查，无法验证工作节点之间的连接。结果是所有Pod、服务和端点都保持完好无损。听起来很棒，但问题在于，如果任何Pod尝试跨失败的区域边界路由请求，该请求将失败，kube-proxy
    不会进行重试。这可能很麻烦，需要检测和/或调试。
- en: One approach to take is to include the addition of some cross-zone network validation
    components at the worker level. We also recommend monitoring and alerting for
    your application pods so that you can detect and alert.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是在工作节点级别添加一些跨区域网络验证组件。我们还建议监控和提醒您的应用程序Pod，以便您可以检测并报警。
- en: There is another secret weapon in this battle. If you do not rely on kube-proxy,
    but rather a solution that includes things like circuit breakers and automatic
    retries for timeouts, then you can overcome these situations, and many others,
    with no modification to your applications or Kubernetes or OpenShift. Service
    mesh solutions like [Istio](https://istio.io) and [Red Hat Service Mesh](https://oreil.ly/3drTt)
    introduce a full mesh of sidecar containers to every pod. These sidecar containers
    run Envoy, a small, lightweight, efficient load balancer that includes advanced
    circuit breaking, load balancing, and retry capabilities. In this type of service
    mesh configuration, each sidecar is smart enough to stop routing traffic to any
    endpoint IPs with which it fails to communicate.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这场战斗中还有另一个秘密武器。如果您不依赖kube-proxy，而是选择包括诸如断路器和超时自动重试等功能的解决方案，则可以在不修改应用程序或Kubernetes或OpenShift的情况下克服这些情况及其他情况。像[Istio](https://istio.io)和[Red
    Hat Service Mesh](https://oreil.ly/3drTt)这样的服务网格解决方案向每个Pod引入了一个完整的sidecar容器网格。这些sidecar容器运行Envoy，一个小型、轻量级、高效的负载均衡器，包括先进的断路、负载均衡和重试功能。在这种类型的服务网格配置中，每个sidecar都足够智能，能够停止将流量路由到任何与其通信失败的终端IP。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We now have a solid basis for understanding how we can measure and calculate
    availability of a system. In addition, we have a clear understanding of how Kubernetes
    provides the tools and architecture to implement a highly available application
    and service platform. It’s critical to understand how the system functions and
    what its failure modes are in order for development teams to properly architect
    their applications and define their Kubernetes and OpenShift resources. SRE and
    operations teams should familiarize themselves with these failure modes and plan
    monitoring and alerting accordingly.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个坚实的基础，可以理解如何测量和计算系统的可用性。此外，我们清楚地了解了Kubernetes如何提供工具和架构来实现高可用的应用程序和服务平台。了解系统如何运作以及其故障模式对开发团队正确设计他们的应用程序和定义Kubernetes和OpenShift资源至关重要。SRE和运维团队应熟悉这些故障模式，并相应地计划监控和警报。
- en: With these Kubernetes tools in hand, we should be ready to build a properly
    architected Kubernetes or OpenShift cluster with highly available services. We
    also have the equations necessary to derive an SLO for our services based on the
    choices we have made. How far can we push our availability? Armed with the analysis
    and tools outlined in this chapter, you have the ability to make an informed decision
    about your SLOs and create an implementation that meets your goals.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些Kubernetes工具，我们应该准备好构建一个正确架构的Kubernetes或OpenShift集群，并提供高可用的服务。我们还有必要的方程式来推导基于我们所做选择的服务水平目标（SLO）。凭借本章概述的分析和工具，您有能力对SLO做出明智的决策，并创建符合您目标的实现方案。
- en: ^([1](ch04.html#ch01fn26-marker)) Unfortunately, PASS has ceased operations,
    and the talk given in early 2012 is not available.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#ch01fn26-marker)) 不幸的是，PASS已经停止运营，并且2012年初的演讲内容不可用。
- en: '^([2](ch04.html#ch01fn27-marker)) Gavin McCance, “CERN Data Centre Evolution,”
    talk presented at SCDC12: Supporting Science with Cloud Computing (November 19,
    2012), [*https://oreil.ly/F1Iw9*](https://oreil.ly/F1Iw9).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#ch01fn27-marker)) Gavin McCance在2012年11月19日的演讲“CERN数据中心演进”中提到，[*https://oreil.ly/F1Iw9*](https://oreil.ly/F1Iw9)。
- en: ^([3](ch04.html#ch01fn28-marker)) The [Wikipedia entry on iptables](https://oreil.ly/6Pmwf)
    provides more information.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#ch01fn28-marker)) [iptables的Wikipedia条目](https://oreil.ly/6Pmwf)提供了更多信息。
- en: ^([4](ch04.html#ch01fn29-marker)) The [Wikipedia entry on IP Virtual Server](https://oreil.ly/LSYXT)
    provides more information.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#ch01fn29-marker)) [IP Virtual Server的Wikipedia条目](https://oreil.ly/LSYXT)提供了更多信息。
- en: ^([5](ch04.html#ch01fn30-marker)) See the Kubernetes documentation on [kube-proxy](https://oreil.ly/HReuF)
    for more information.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#ch01fn30-marker)) 查看[Kubernetes文档关于kube-proxy](https://oreil.ly/HReuF)以获取更多信息。
- en: ^([6](ch04.html#ch01fn31-marker)) See the Kubernetes documentation on [Nodes
    Reliability](https://oreil.ly/uzHtJ) for more information.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.html#ch01fn31-marker)) 查看[Kubernetes文档关于节点可靠性](https://oreil.ly/uzHtJ)以获取更多信息。
- en: ^([7](ch04.html#ch01fn32-marker)) For more information, see the [etcd FAQ](https://oreil.ly/XrFaC).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.html#ch01fn32-marker)) 更多信息，请参阅[etcd FAQ](https://oreil.ly/XrFaC)。
