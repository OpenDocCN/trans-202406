- en: Chapter 3\. Container Runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a container orchestrator. Yet, Kubernetes itself does not know
    how to create, start, and stop containers. Instead, it delegates these operations
    to a pluggable component called the *container runtime*. The container runtime
    is a piece of software that creates and manages containers on a cluster node.
    In Linux, the container runtime uses a set of kernel primitives such as control
    groups (cgroups) and namespaces to spawn a process from a container image. In
    essence, Kubernetes, and more specifically, the kubelet, works together with the
    container runtime to run containers.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 1](ch01.html#chapter1), organizations building platforms
    on top of Kubernetes are faced with multiple choices. Which container runtime
    to use is one such choice. Choice is great as it lets you customize the platform
    to your needs, enabling innovation and advanced use cases that might otherwise
    not be possible. However, given the fundamental nature of a container runtime,
    why does Kubernetes not provide an implementation? Why does it choose to provide
    a pluggable interface and offload the responsibility to another component?
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, we will look back and briefly review the history
    of containers and how we got here. We will first discuss the advent of containers
    and how they changed the software development landscape. After all, Kubernetes
    would probably not exist without them. We will then discuss the Open Container
    Initiative (OCI), which arose from the need for standardization around container
    runtimes, images, and other tooling. We will review the OCI specifications and
    how they pertain to Kubernetes. After OCI, we will discuss the Kubernetes-specific
    Container Runtime Interface (CRI). The CRI is the bridge between the kubelet and
    the container runtime. It specifies the interface that the container runtime must
    implement to be compatible with Kubernetes. Finally, we will discuss how to choose
    a runtime for your platform and review the available options in the Kubernetes
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The Advent of Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Control groups (cgroups) and namespaces are the primary ingredients necessary
    to implement containers. Cgroups impose limits on the amount of resources a process
    can use (e.g., CPU, memory, etc.), while namespaces control what a process can
    see (e.g., mounts, processes, network interfaces, etc.). Both these primitives
    have been in the Linux kernel since 2008\. Even earlier in the case of namespaces.
    So why did containers, as we know them today, become popular years later?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we first need to consider the environment surrounding
    the software and IT industry at the time. An initial factor to think about is
    the complexity of applications. Application developers built applications using
    service-oriented architectures and even started to embrace microservices. These
    architectures brought various benefits to organizations, such as maintainability,
    scalability, and productivity. However, they also resulted in an explosion in
    the number of components that made up an application. Meaningful applications
    could easily involve a dozen services, potentially written in multiple languages.
    As you can imagine, developing and shipping these applications was (and continues
    to be) complex. Another factor to remember is that software quickly became a business
    differentiator. The faster you could ship new features, the more competitive your
    offerings. Having the ability to deploy software in a reliable manner was key
    to a business. Finally, the emergence of the public cloud as a hosting environment
    is another important factor. Developers and operations teams had to ensure that
    applications behaved the same across all environments, from a developer’s laptop
    to a production server running in someone else’s datacenter.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping these challenges in mind, we can see how the environment was ripe for
    innovation. Enter Docker. Docker made containers accessible to the masses. They
    built an abstraction that enabled developers to build and run containers with
    an easy-to-use CLI. Instead of developers having to know the low-level kernel
    constructs needed to leverage container technology, all they had to do was type
    `docker run` in their terminal.
  prefs: []
  type: TYPE_NORMAL
- en: While not the answer to all our problems, containers improved many stages of
    the software development life cycle. First, containers and container images allowed
    developers to codify the application’s environment. Developers no longer had to
    wrestle with missing or mismatched application dependencies. Second, containers
    impacted testing by providing reproducible environments for testing applications.
    Lastly, containers made it easier to deploy software to production. As long as
    there was a Docker Engine in the production environment, the application could
    be deployed with minimum friction. Overall, containers helped organizations to
    ship software from zero to production in a more repeatable and efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advent of containers also gave birth to an abundant ecosystem full of different
    tools, container runtimes, container image registries, and more. This ecosystem
    was well received but introduced a new challenge: How do we make sure that all
    these container solutions are compatible with each other? After all, the encapsulation
    and portability guarantees are one of the main benefits of containers. To solve
    this challenge and to improve the adoption of containers, the industry came together
    and collaborated on an open source specification under the umbrella of the Linux
    Foundation: the Open Container Initiative.'
  prefs: []
  type: TYPE_NORMAL
- en: The Open Container Initiative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As containers continued to gain popularity across the industry, it became clear
    that standards and specifications were required to ensure the success of the container
    movement. The Open Container Initiative (OCI) is an open source project established
    in 2015 to collaborate on specifications around containers. Notable founders of
    this initiative included Docker, which donated runc to the OCI, and CoreOS, which
    pushed the needle on container runtimes with rkt.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OCI includes three specifications: the OCI runtime specification, the OCI
    image specification, and the OCI distribution specification. These specs enable
    development and innovation around containers and container platforms such as Kubernetes.
    Furthermore, the OCI aims to allow end users to use containers in a portable and
    interoperable manner, enabling them to move between products and solutions more
    easily when necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explore the runtime and image specifications.
    We will not dig into the distribution specification, as it is primarily concerned
    with container image registries.
  prefs: []
  type: TYPE_NORMAL
- en: OCI Runtime Specification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OCI runtime specification determines how to instantiate and run containers
    in an OCI-compatible fashion. First, the specification describes the schema of
    a container’s configuration. The schema includes information such as the container’s
    root filesystem, the command to run, the environment variables, the user and group
    to use, resource limits, and more. The following snippet is a trimmed example
    of a container configuration file obtained from the OCI runtime specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The runtime specification also determines the operations that a container runtime
    must support. These operations include create, start, kill, delete, and state
    (which provides information about the container’s state). In addition to the operations,
    the runtime spec describes the life cycle of a container and how it progresses
    through different stages. The life cycle stages are (1) `creating`, which is active
    when the container runtime is creating the container; (2) `created`, which is
    when the runtime has completed the `create` operation; (3) `running`, which is
    when the container process has started and is running; and (4) `stopped`, which
    is when the container process has finished.
  prefs: []
  type: TYPE_NORMAL
- en: The OCI project also houses *runc*, a low-level container runtime that implements
    the OCI runtime specification. Other higher-level container runtimes such as Docker,
    containerd, and CRI-O use runc to spawn containers according to the OCI spec,
    as shown in [Figure 3-1](#docker_engine_containerd_and_other_runtimes_use_runc_to_spawn).
    Leveraging runc enables container runtimes to focus on higher-level features such
    as pulling images, configuring networking, handling storage, and so on while conforming
    to the OCI runtime spec.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0301](assets/prku_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Docker Engine, containerd, and other runtimes use runc to spawn
    containers according to the OCI spec.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OCI Image Specification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OCI image specification focuses on the container image. The specification
    defines a manifest, an optional image index, a set of filesystem layers, and a
    configuration. The image manifest describes the image. It includes a pointer to
    the image’s configuration, a list of image layers, and an optional map of annotations.
    The following is an example manifest obtained from the OCI image specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The *image index* is a top-level manifest that enables the creation of multiplatform
    container images. The image index contains pointers to each of the platform-specific
    manifests. The following is an example index obtained from the specification.
    Notice how the index points to two different manifests, one for `ppc64le/linux`
    and another for `amd64/linux`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Each OCI image manifest references a container image configuration. The configuration
    includes the image’s entry point, command, working directory, environment variables,
    and more. The container runtime uses this configuration when instantiating a container
    from the image. The following snippet shows the configuration of a container image,
    with some fields removed for brevity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The OCI image spec also describes how to create and manage container image layers.
    Layers are essentially TAR archives that include files and directories. The specification
    defines different media types for layers, including uncompressed layers, gzipped
    layers, and nondistributable layers. Each layer is uniquely identified by a digest,
    usually a SHA256 sum of the contents of the layer. As we discussed before, the
    container image manifest references one or more layers. The references use the
    SHA256 digest to point to a specific layer. The final container image filesystem
    is the result of applying each of the layers, as listed in the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: The OCI image specification is crucial because it ensures that container images
    are portable across different tools and container-based platforms. The spec enables
    the development of different image build tools, such as kaniko and Buildah for
    userspace container builds, Jib for Java-based containers, and Cloud Native Buildpacks
    for streamlined and automated builds. (We will explore some of these tools in
    [Chapter 15](ch15.html#chapter15)). Overall, this specification ensures that Kubernetes
    can run container images regardless of the tooling used to build them.
  prefs: []
  type: TYPE_NORMAL
- en: The Container Runtime Interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve discussed in prior chapters, Kubernetes offers many extension points
    that allow you to build a bespoke application platform. One of the most critical
    extension points is the Container Runtime Interface (CRI). The CRI was introduced
    in Kubernetes v1.5 as an effort to enable the growing ecosystem of container runtimes,
    which included rkt by CoreOS and hypervisor-based runtimes such as Intel’s Clear
    Containers, which later became Kata Containers.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to CRI, adding support for a new container runtime required a new release
    of Kubernetes and intimate knowledge of the Kubernetes code base. Once the CRI
    was established, container runtime developers could simply adhere to the interface
    to ensure compatibility of the runtime with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the goal of the CRI was to abstract the implementation details of the
    container runtime away from Kubernetes, more specifically the kubelet. This is
    a classic example of the dependency inversion principle. The kubelet evolved from
    having container runtime–specific code and if-statements scattered throughout
    to a leaner implementation that relied on the interface. Thus, the CRI reduced
    the complexity of the kubelet implementation while also making it more extensible
    and testable. These are all important qualities of well-designed software.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CRI is implemented using gRPC and Protocol Buffers. The interface defines
    two services: the *RuntimeService* and the *ImageService*. The kubelet leverages
    these services to interact with the container runtime. The RuntimeService is responsible
    for all the Pod-related operations, including creating Pods, starting and stopping
    containers, deleting Pods, and so on. The ImageService is concerned with container
    image operations, including listing, pulling, and removing container images from
    the node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While we could detail the APIs of both the RuntimeService and ImageService
    in this chapter, it is more useful to understand the flow of perhaps the most
    important operation in Kubernetes: starting a Pod on a node. Thus, let’s explore
    the interaction between the kubelet and the container runtime through CRI in the
    following section.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting a Pod
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following descriptions are based on Kubernetes v1.18.2 and containerd v1.3.4\.
    These components use the v1alpha2 version of the CRI.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Pod is scheduled onto a node, the kubelet works together with the container
    runtime to start the Pod. As mentioned, the kubelet interacts with the container
    runtime through the CRI. In this case, we will explore the interaction between
    the kubelet and the containerd CRI plug-in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The containerd CRI plug-in starts a gRPC server that listens on a Unix socket.
    By default, this socket is located at */run/containerd/containerd.sock*. The kubelet
    is configured to interact with containerd through this socket with the `container-runtime`
    and `container-runtime-endpoint` command-line flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To start the Pod, the kubelet first creates a Pod sandbox using the `RunPodSandbox`
    method of the RuntimeService. Because a Pod is composed of one or more containers,
    the sandbox must be created first to establish the Linux network namespace (among
    other things) for all containers to share. When calling this method, the kubelet
    sends metadata and configuration to containerd, including the Pod’s name, unique
    ID, Kubernetes Namespace, DNS configuration, and more. Once the container runtime
    creates the sandbox, the runtime responds with a Pod sandbox ID that the kubelet
    uses to create containers in the sandbox.
  prefs: []
  type: TYPE_NORMAL
- en: Once the sandbox is available, the kubelet checks whether the container image
    is present on the node using the `ImageStatus` method of the ImageService. The
    `ImageStatus` method returns information about the image. When the image is not
    present, the method returns null and the kubelet proceeds to pull the image. The
    kubelet uses the `PullImage` method of the ImageService to pull the image when
    necessary. Once the runtime pulls the image, it responds with the image SHA256
    digest, which the kubelet then uses to create the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the sandbox and pulling the image, the kubelet creates the containers
    in the sandbox using the `CreateContainer` method of the RuntimeService. The kubelet
    provides the sandbox ID and the container configuration to the container runtime.
    The container configuration includes all the information you might expect, including
    the container image digest, command and arguments, environment variables, volume
    mounts, etc. During the creation process, the container runtime generates a container
    ID that it then passes back to the kubelet. This ID is the one you see in the
    Pod’s status field under container statuses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The kubelet then proceeds to start the container using the `StartContainer`
    method of the RuntimeService. When calling this method, it uses the container
    ID it received from the container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! In this section, we’ve learned how the kubelet interacts with
    the container runtime using the CRI. We specifically looked at the gRPC methods
    invoked when starting a Pod, which include those on the ImageService and the RuntimeService.
    Both of these CRI services provide additional methods that the kubelet uses to
    complete other tasks. Besides the Pod and container management (i.e., CRUD) methods,
    the CRI also defines methods to execute a command inside a container (`Exec` and
    `ExecSync`), attach to a container (`Attach`), forward a specific container port
    (`PortForward`), and others.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the availability of the CRI, platform teams get the flexibility of choice
    when it comes to container runtimes. The reality, however, is that over the last
    couple of years the container runtime has become an implementation detail. If
    you are using a Kubernetes distribution or leveraging a managed Kubernetes service,
    the container runtime will most likely be chosen for you. This is the case even
    for community projects such as Cluster API, which provide prebaked node images
    that include a container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: With that said, if you do have the option to choose a runtime or have a use
    case for a specialized runtime (e.g., VM-based runtimes), you should be equipped
    with information to make that decision. In this section, we will discuss considerations
    you should make when choosing a container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The first question we like to ask when helping organizations in the field is
    which container runtime they have experience with. In most cases, organizations
    that have a long history with containers are using Docker and are familiar with
    Docker’s toolchain and user experience. While Kubernetes supports Docker, we discourage
    its use as it has an extended set of capabilities that Kubernetes does not need,
    such as building images, creating container networks, and so on. In other words,
    the fully fledged Docker daemon is too heavy or bloated for the purposes of Kubernetes.
    The good news is that Docker uses containerd under the covers, one of the most
    prevalent container runtimes in the community. The downside is that platform operators
    have to learn the containerd CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration to make is the availability of support. Depending on where
    you are getting Kubernetes from, you might get support for the container runtime.
    Kubernetes distributions such as VMware’s Tanzu Kubernetes Grid, RedHat’s OpenShift,
    and others usually ship a specific container runtime. You should stick to that
    choice unless you have an extremely compelling reason not to. In that case, ensure
    that you understand the support implications of using a different container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Closely related to support is conformance testing of the container runtime.
    The Kubernetes project, specifically the Node Special Interest Group (sig-node),
    defines a set of CRI validation tests and node conformance tests to ensure container
    runtimes are compatible and behave as expected. These tests are part of every
    Kubernetes release, and some runtimes might have more coverage than others. As
    you can imagine, the more test coverage the better, as any issues with the runtime
    are caught during the Kubernetes release process. The community makes all tests
    and results available through the [Kubernetes Test Grid](https://k8s-testgrid.appspot.com).
    When choosing a runtime, you should consider the container runtime’s conformance
    tests and more broadly, the runtime’s relationship with the overall Kubernetes
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you should determine if your workloads need stronger isolation guarantees
    than those provided by Linux containers. While less common, there are use cases
    that require VM-level isolation for workloads, such as executing untrusted code
    or running applications that require strong multitenancy guarantees. In these
    cases, you can leverage specialized runtimes such as Kata Containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have discussed the considerations you should make when choosing
    a runtime, let’s review the most prevalent container runtimes: Docker, containerd,
    and CRI-O. We will also explore Kata Containers to understand how we can run Pods
    in VMs instead of Linux Containers. Finally, while not a container runtime or
    component that implements CRI, we will learn about Virtual Kubelet, as it provides
    another way to run workloads on Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes supports the Docker Engine as a container runtime through a CRI shim
    called the *dockershim*. The shim is a component that’s built into the kubelet.
    Essentially, it is a gRPC server that implements the CRI services we described
    earlier in this chapter. The shim is required because the Docker Engine does not
    implement the CRI. Instead of special-casing all the kubelet code paths to work
    with both the CRI and the Docker Engine, the dockershim serves as a facade that
    the kubelet can use to communicate with Docker via the CRI. The dockershim handles
    the translation between CRI calls to Docker Engine API calls. [Figure 3-2](#interaction_between_the_kubelet_and_docker_engine_via_the_dockershim)
    depicts how the kubelet interacts with Docker through the shim.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0302](assets/prku_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Interaction between the kubelet and Docker Engine via the dockershim.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As we mentioned earlier in the chapter, Docker leverages containerd under the
    hood. Thus, the incoming API calls from the kubelet are eventually relayed to
    containerd, which starts the containers. In the end, the spawned container ends
    up under containerd and not the Docker daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'From a troubleshooting perspective, you can use the Docker CLI to list and
    inspect containers running on a given node. While Docker does not have the concept
    of Pods, the dockershim encodes the Kubernetes Namespace, Pod name, and Pod ID
    into the name of containers. For example, the following listing shows the containers
    that belong to a Pod called `nginx` in the `default` namespace. The Pod infrastructure
    container (aka, pause container) is the one with the `k8s_POD_` prefix in the
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the containerd CLI, `ctr`, to inspect containers, although
    the output is not as user friendly as the Docker CLI output. The Docker Engine
    uses a containerd namespace called `moby`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can use `crictl` if available on the node. The `crictl` utility
    is a command-line tool developed by the Kubernetes community. It is a CLI client
    for interacting with container runtimes over the CRI. Even though Docker does
    not implement the CRI, you can use `crictl` with the dockershim Unix socket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: containerd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: containerd is perhaps the most common container runtime we encounter when building
    Kubernetes-based platforms in the field. At the time of writing, containerd is
    the default container runtime in Cluster API-based node images and is available
    across various managed Kubernetes offerings (e.g., AKS, EKS, and GKE).
  prefs: []
  type: TYPE_NORMAL
- en: The containerd container runtime implements the CRI through the containerd CRI
    plug-in. The CRI plug-in is a native containerd plug-in that is available since
    containerd v1.1 and is enabled by default. containerd exposes its gRPC APIs over
    a Unix socket at */run/containerd/containerd.sock*. The kubelet uses this socket
    to interact with containerd when it comes to running Pods, as depicted in [Figure 3-3](#interaction_between_the_kubelet_and_containerd_through_the_containerd_cri_plugin).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0303](assets/prku_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Interaction between the kubelet and containerd through the containerd
    CRI plug-in.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The process tree of spawned containers looks exactly the same as the process
    tree when using the Docker Engine. This is expected, as the Docker Engine uses
    containerd to manage containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To inspect containers on a node, you can use `ctr`, the containerd CLI. As
    opposed to Docker, the containers managed by Kubernetes are in a containerd namespace
    called `k8s.io` instead of `moby`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the `crictl` CLI to interact with containerd through the containerd
    unix socket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: CRI-O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CRI-O is a container runtime specifically designed for Kubernetes. As you can
    probably tell from the name, it is an implementation of the CRI. Thus, in contrast
    to Docker and containerd, it does not cater to uses outside of Kubernetes. At
    the time of writing, one of the primary consumers of the CRI-O container runtime
    is the RedHat OpenShift platform.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to containerd, CRI-O exposes the CRI over a Unix socket. The kubelet
    uses the socket, typically located at */var/run/crio/crio.sock*, to interact with
    CRI-O. [Figure 3-4](#interaction_between_the_kubelet_and_crio_using_the_cri_apis)
    depicts the kubelet interacting directly with CRI-O through the CRI.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0304](assets/prku_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Interaction between the kubelet and CRI-O using the CRI APIs.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When spawning containers, CRI-O instantiates a process called *conmon*. Conmon
    is a container monitor. It is the parent of the container process and handles
    multiple concerns, such as exposing a way to attach to the container, storing
    the container’s STDOUT and STDERR streams to logfiles, and handling container
    termination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Because CRI-O was designed as a low-level component for Kubernetes, the CRI-O
    project does not provide a CLI. With that said, you can use `crictl` with CRI-O
    as with any other container runtime that implements the CRI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Kata Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kata Containers is an open source, specialized runtime that uses lightweight
    VMs instead of containers to run workloads. The project has a rich history, resulting
    from the merge of two prior VM-based runtimes: Clear Containers from Intel and
    RunV from Hyper.sh.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the use of VMs, Kata provides stronger isolation guarantees than Linux
    containers. If you have security requirements that prevent workloads from sharing
    a Linux kernel or resource guarantee requirements that cannot be met by cgroup
    isolation, Kata Containers can be a good fit. For example, a common use case for
    Kata containers is to run multitenant Kubernetes clusters that run untrusted code.
    Cloud providers such as [Baidu Cloud](https://oreil.ly/btDL9) and [Huawei Cloud](https://oreil.ly/Mzarh)
    use Kata Containers in their cloud infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: To use Kata Containers with Kubernetes, there is still a need for a pluggable
    container runtime to sit between the kubelet and the Kata runtime, as shown in
    [Figure 3-5](#interaction_between_the_kubelet_and_kata_containers_through_containerd).
    The reason is that Kata Containers does not implement the CRI. Instead, it leverages
    existing container runtimes such as containerd to handle the interaction with
    Kubernetes. To integrate with containerd, the Kata Containers project implements
    the containerd runtime API, specifically the [v2 containerd-shim API](https://oreil.ly/DxGyZ).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0305](assets/prku_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Interaction between the kubelet and Kata Containers through containerd.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Because containerd is required and available on the nodes, it is possible to
    run Linux container Pods and VM-based Pods on the same node. Kubernetes provides
    a mechanism to configure and run multiple container runtimes called Runtime Class.
    Using the RuntimeClass API, you can offer different runtimes in the same Kubernetes
    platform, enabling developers to use the runtime that better fits their needs.
    The following snippet is an example RuntimeClass for the Kata Containers runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a Pod under the `kata-containers` runtime, developers must specify the
    runtime class name in their Pod’s specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Kata Containers supports different hypervisors to run workloads, including
    [QEMU](https://www.qemu.org), [NEMU](https://github.com/intel/nemu), and [AWS
    Firecracker](https://firecracker-microvm.github.io). When using QEMU, for example,
    we can see a QEMU process after launching a Pod that uses the `kata-containers`
    runtime class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: While Kata Containers provides interesting capabilities, we consider it a niche
    and have not seen it used in the field. With that said, if you need VM-level isolation
    guarantees in your Kubernetes cluster, Kata Containers is worth looking into.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Kubelet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Virtual Kubelet](https://github.com/virtual-kubelet/virtual-kubelet) is an
    open source project that behaves like a kubelet but offers a pluggable API on
    the backend. While not a container runtime per se, its main purpose is to surface
    alternative runtimes to run Kubernetes Pods. Because of the Virtual Kubelet’s
    extensible architecture, these alternative runtimes can essentially be any systems
    that can run an application, such as serverless frameworks, edge frameworks, etc.
    For example, as shown in [Figure 3-6](#virtual_kubelet_running_pods_on_a_cloud_service_such_as_azure_container_instances_aws_fargate_etc),
    the Virtual Kubelet can launch Pods on a cloud service such as Azure Container
    Instances or AWS Fargate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0306](assets/prku_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Virtual Kubelet running Pods on a cloud service, such as Azure
    Container Instances, AWS Fargate, etc.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Virtual Kubelet community offers a variety of providers that you can leverage
    if they fit your need, including AWS Fargate, Azure Container Instances, HashiCorp
    Nomad, and others. If you have a more specific use case, you can implement your
    own provider as well. Implementing a provider involves writing a Go program using
    the Virtual Kubelet libraries to handle the integration with Kubernetes, including
    node registration, running Pods, and exporting APIs expected by Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Even though Virtual Kubelet enables interesting scenarios, we have yet to run
    into a use case that needed it in the field. With that said, it is useful to know
    that it exists, and you should keep it in your Kubernetes toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The container runtime is a foundational component of a Kubernetes-based platform.
    After all, it is impossible to run containerized workloads without a container
    runtime. As we learned in this chapter, Kubernetes uses the Container Runtime
    Interface (CRI) to interact with the container runtime. One of the main benefits
    of the CRI is its pluggable nature, which allows you to use the container runtime
    that best fits your needs. To give you an idea of the different container runtimes
    available in the ecosystem, we discussed those that we typically see in the field,
    such as Docker, containerd, etc. Learning about the different options and further
    exploring their capabilities should help you select the container runtime that
    satisfies the requirements of your application platform.
  prefs: []
  type: TYPE_NORMAL
