- en: Chapter 9\. Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ability to observe any software system is critical. If you cannot examine
    the condition of your running applications, you cannot effectively manage them.
    And that is what we are addressing with observability: the various mechanisms
    and systems we use to understand the condition of running software that we are
    responsible for. We should acknowledge that we’re not adhering to the control
    theory definition of observability in this context. We chose to use this term
    simply because it has become popular and we want people to readily understand
    what we’re getting at.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The components of observability can be broken into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating and storing the logged event messages written by programs
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs: []
  type: TYPE_NORMAL
- en: Collecting time series data, making it available in dashboards, and alerting
    upon it
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs: []
  type: TYPE_NORMAL
- en: Capturing data for requests that traverse multiple distinct workloads in the
    cluster
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover how to implement effective observability in Kubernetes-based
    platforms so that you can safely manage a platform and the workloads it hosts
    in production. First, we will explore logging and examine the systems for aggregating
    logs and forwarding them to your company’s logging backend. Next, we’ll cover
    how to collect metrics, how to visualize that data, and how to alert upon it.
    Lastly, we’ll cover tracing requests through distributed systems so as to better
    understand what’s happening when applications are composed of distinct workloads.
    Let’s jump into logging and cover the commonly successful models there.
  prefs: []
  type: TYPE_NORMAL
- en: Logging Mechanics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers logging concerns in a Kubernetes-based platform. We’re primarily
    dealing with the mechanisms for capturing, processing, and forwarding logs from
    your platform components and tenant workloads to a storage backend.
  prefs: []
  type: TYPE_NORMAL
- en: Once upon a time, the software we ran in production usually wrote logs to a
    file on disk. Aggregation of logs—if performed at all—was a simpler exercise because
    there were fewer distinct workloads and fewer instances of those workloads compared
    with today’s systems. In a containerized world, our applications commonly log
    to standard out and standard error the way an interactive CLI would. Indeed, this
    became accepted as best practice for modern service-oriented software even before
    containers became prevalent. In cloud native software ecosystems, there are more
    distinct workloads and instances of each, but they’re also ephemeral and often
    without a disk mounted to persist the logs—hence the shift away from writing logs
    to disk. This introduced challenges in the collection, aggregation, and storage
    of logs.
  prefs: []
  type: TYPE_NORMAL
- en: Often a single workload will have multiple replicas, and there may be multiple
    distinct components to examine. Without centralized log aggregation, analyzing
    (viewing and parsing) logs in this scenario becomes very tedious, if not practically
    impossible. Consider having to analyze logs for a workload that has *dozens* of
    replicas. In these cases, it is essential to have a central collection point that
    allows you to search the log entries across replicas.
  prefs: []
  type: TYPE_NORMAL
- en: In covering logging mechanics, we’ll first look at strategies for capturing
    and routing the logs from the containerized workloads in your platform. This includes
    the logs for the Kubernetes control plane and platform utilities as well as the
    platform tenants. We’ll also cover the Kubernetes API Server audit logs as well
    as Kubernetes Events in this section. Lastly, we’ll address the notion of alerting
    upon conditions found in logged data and alternative strategies for that. We will
    not cover the storage of logs because most enterprises have a log backend that
    we will integrate with—it is generally not a concern of the Kubernetes-based platform
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Container Log Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at three ways log processing could be done for the containerized
    workloads in a Kubernetes-based platform:'
  prefs: []
  type: TYPE_NORMAL
- en: Application forwarding
  prefs: []
  type: TYPE_NORMAL
- en: Send logs to the backend directly from the application.
  prefs: []
  type: TYPE_NORMAL
- en: Sidecar processing
  prefs: []
  type: TYPE_NORMAL
- en: Use a sidecar to manage the logs for an application.
  prefs: []
  type: TYPE_NORMAL
- en: Node agent forwarding
  prefs: []
  type: TYPE_NORMAL
- en: Run a Pod on each Node that forwards logs for all containers on that Node to
    the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Application forwarding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, the application needs to be integrated with the backend storage
    of logs. The developers have to build this functionality into their application
    and maintain that functionality. If the log backend changes, an update to the
    application will likely be required. Since log processing is virtually universal,
    it makes much more sense to offload this from the application. Application forwarding
    is not a good option in most situations and is rarely seen in production environments.
    It makes sense only if you have a heritage application that is being migrated
    to a Kubernetes-based platform that already integrates with a log backend.
  prefs: []
  type: TYPE_NORMAL
- en: Sidecar processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this model, the application runs in one container and writes logs to one
    or more files in the shared storage for the Pod. Another container in the same
    Pod, a sidecar, reads those logs and processes them. The sidecar does one of two
    things with the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: Forwards them directly to the log storage backend
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Writes the logs to standard error and standard out
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forwarding directly to the backend is the primary use case for sidecar log processing.
    This approach is uncommon and is usually a makeshift workaround where the platform
    offers no log aggregation system.
  prefs: []
  type: TYPE_NORMAL
- en: In situations where the sidecar writes the logs to standard out and standard
    error, it does so to leverage Node agent forwarding (which is covered in the next
    section). This is also an uncommon method and is only useful if you are running
    an application that just isn’t able to write logs to standard out and standard
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Node agent forwarding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With node agent forwarding, a log processing workload runs on each node in the
    cluster, reads the logfiles for each container written by the container runtime,
    and forwards the logs to the backend storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the model we generally recommend and is, by far, the most common implementation.
    It is a useful pattern because:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a single point of integration between the log forwarder and the backend,
    as opposed to different sidecars and/or applications having to maintain that integration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The configuration of standardized filtering, attaching metadata, and forwarding
    to multiple backends is centralized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log rotation is taken care of by the kubelet or container runtime. If the application
    is writing logfiles inside the container, the application itself or the sidecar
    (if there is one) needs to handle log rotation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prevailing tools used for this node agent log forwarding are [Fluentd](https://www.fluentd.org)
    and [Fluent Bit](https://fluentbit.io). As the names suggest, they are related
    projects. Fluentd was the original, is written primarily in Ruby, and has a rich
    ecosystem of plug-ins around it. Fluent Bit came from a demand for a more lightweight
    solution for environments like embedded Linux. It is written in C and has a much
    smaller memory footprint than Fluentd, but it does not have as many plug-ins available.
  prefs: []
  type: TYPE_NORMAL
- en: The general guidance we give platform engineers when choosing a log aggregation
    and forwarding tool is to use Fluent Bit unless there are plug-ins for Fluentd
    that have compelling features. If you find a need to leverage Fluentd plug-ins,
    consider running it as a cluster-wide aggregator in conjunction with Fluent Bit
    as the node agent. In this model, you use Fluent Bit as the node agent, which
    is deployed as a DaemonSet. Fluent Bit forwards logs to Fluentd running in the
    cluster as a Deployment or StatefulSet. Fluentd performs further tagging and routes
    the logs to one or more backends where developers access them. [Figure 9-1](#aggregation_of_logs_from_containerized_apps_to_back_end)
    illustrates this pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0901](assets/prku_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Aggregation of logs from containerized apps to backend.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While we strongly prefer the node agent forwarding method, it’s worth calling
    out the potential problems with centralizing log aggregation. You are introducing
    a central point of failure for each node, or for the entire cluster if you use
    a cluster-wide aggregator in your stack. If your node agent gets bogged down by
    one workload that is logging excessively, that could affect the collection of
    logs for all workloads on that node. If you have a Fluentd cluster-wide aggregator
    running as a Deployment, it will be using the ephemeral storage layer in its Pod
    as a buffer. If it gets killed before it can flush the logs from its buffer, you
    will lose logs. For this reason, consider running it as a StatefulSet so those
    logs aren’t lost if the Pod goes down.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Audit Logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers collecting audit logs from the Kubernetes API. These logs
    offer a way to find out who did what in the cluster. You will want to have these
    turned on in production so that you can perform a root cause analysis in the event
    that something goes wrong. You may also have compliance requirements that necessitate
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Audit logs are enabled and configured with flags on the API server. The API
    server allows you to capture a log of every stage of every request sent to it,
    including the request and response bodies. In reality, it’s unlikely you will
    want *every* request logged. There are a lot of calls to the API server so there
    will be a very large number of log entries to store. You can use rules in an audit
    policy to qualify which requests and stages you wish your API server to write
    logs for. Without any audit policy, the API server won’t actually write any logs.
    Tell the API server where your audit policy is on the control plane node’s filesystem
    with the `--audit-policy-file` flag. [Example 9-1](#example_9_1) shows several
    rules that illustrate how the policy rules work so that you can limit the volume
    of log information without excluding important data.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. Example audit policy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_observability_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `None` auditing level means the API server will not log events that match
    this rule. So when the user `system:kube-proxy` requests a watch on the listed
    resources, the event is not logged.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_observability_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `Metadata` level means that only request metadata is logged. When any request
    for the listed resources is received by the API server, it will log which user
    made what type of requests for what resource, but not the body of the request
    or response. The `RequestReceived` stage will not be logged. This means it will
    not write a separate log entry when the request is received. It will write a log
    entry when it starts the response for a long-running watch. It will write a log
    entry after it completes the response to the client. And it will log any panic
    that occurs. But will omit a log entry when the request is first received.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_observability_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The `Request` level will instruct the API server to log the request metadata
    and request body, but *not* the response body. So when any client sends a get,
    list, or watch request, the potentially verbose response body—that contains the
    object/s—is not logged.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_observability_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `RequestResponse` level logs the most information: the request metadata,
    the request body, and the response body. This rule lists the same API groups as
    the previous. So, in effect, this rule says that if a request is *not* a get,
    list, or watch for a resource in one of these groups, additionally log the response
    body. In effect this becomes the default log level for the listed groups.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_observability_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Any other resources that aren’t matched in previous rules will have this default
    applied, which says to skip the additional log message when a request is received
    and log only request metadata and excluded request and response bodies.
  prefs: []
  type: TYPE_NORMAL
- en: As with other logs in your system, you will want to forward the audit logs to
    some backend. You can use either the application forwarding or node agent forwarding
    strategies we covered earlier in this chapter. Much of the same principles and
    patterns apply.
  prefs: []
  type: TYPE_NORMAL
- en: For the application forwarding approach, you can configure the API server to
    send logs directly to a webhook backend. In this case you tell the API server
    with a flag where the config file is that contains the address and credentials
    to connect. This config file uses the kubeconfig format. You will need to spend
    some time tuning the configuration options for buffering and batching to ensure
    all logs arrive at the backend. For example, if you set a buffer size for the
    number of events to buffer before batching that is too low and it overflows, events
    will be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: For node agent forwarding, you can have the API server write logfiles to the
    filesystem on the control plane node. You can provide flags to the API server
    to configure the filepath, maximum retention period, maximum number of files,
    and maximum logfile size. In this case you can aggregate and forward logs with
    tools like Fluent Bit and Fluentd. This is likely a good pattern to follow if
    you are already using these tools to manage logs with the node agent forwarding
    discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Kubernetes, Events are a native resource. They are a way for platform components
    to expose information about what has happened to different objects through the
    Kubernetes API. In effect, they are a kind of platform log. Unlike other logs,
    they are not generally stored in a logging backend. They are stored in etcd and,
    by default, are retained for one hour. They are most commonly used by platform
    operators and users when they want to gather information about actions taken against
    objects. [Example 9-2](#ex_9-2) shows the Events provided when describing a newly
    created Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. Events given with Pod description
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can also retrieve the same Events directly as shown in [Example 9-3](#ex_9-3).
    In this case it includes the Events for the ReplicaSet and Deployment resources
    in addition to the Pod Events we saw when describing that resource.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. Events for a namespace retrieved directly
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Being that Kubernetes Events are available through the Kubernetes API, it is
    entirely possible to build automation to watch for, and react to, specific Events.
    However, in reality, we don’t see this commonly done. One additional way you can
    leverage them is through an Event exporter that exposes them as metrics. See [“Prometheus”](#prometheus)
    for more on Prometheus exporters.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting on Logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Application logs expose important information about the behavior of your software.
    They are especially valuable when unexpected failures occur that require investigation.
    This may lead you to find patterns of events that precipitate problems. If you
    find yourself wanting to set up alerts on Events exposed in logs, first consider
    using metrics instead. If you expose metrics that represent that behavior, you
    can implement alerting rules against them. Log messages are less reliable to alert
    on as they are more subject to change. A slight change to the text of a log message
    may inadvertently break the alerting that uses it.
  prefs: []
  type: TYPE_NORMAL
- en: Security Implications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Don’t forget to give some thought to the access users have to the various logs
    aggregated in your backend. You may not want your production API server audit
    logs accessible to everyone. You may have sensitive systems with information that
    only privileged users should have access to. This may impact the tagging of logs
    or may necessitate using multiple backends, impacting your forwarding configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the various mechanics involved in managing the logs from
    your platform and its tenants, let’s move on to metrics and alerting.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics and alerting services are vital to the usability of the platform. Metrics
    allow us to plot measured data on a timeline and recognize divergences that indicate
    undesirable or unexpected behavior. They help us understand what’s happening with
    our applications, inform us whether they are behaving as we expect, and give us
    insights into how we can remedy problems or improve how we manage our workloads.
    And, critically, metrics give us useful measurements to alert on. Notifications
    of failures, or preferably warnings of impending failures, give us the opportunity
    to avert and/or minimize downtime and errors.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’re going to cover how to provide metrics and alerting as
    a platform service using Prometheus. There is considerable detail to explore here,
    and it will be helpful to reference a particular software stack as we do so. This
    is not to say you cannot or should not use other solutions. There are many circumstances
    where Prometheus may *not* be the right solution. However, Prometheus does provide
    an excellent model for addressing the subject. Regardless of the exact tools you
    use, the Prometheus model provides a clear implementation reference that will
    inform how you approach this topic.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a general look at what Prometheus is, how it collects metrics,
    and what functions it provides. Then we will address various general subtopics,
    including long-term storage and the use case for pushing metrics. Next, we’ll
    cover custom metrics generation and collection as well as organization and federation
    of metrics collection across your infrastructure. Also, we will dive into alerting
    and using metrics for actionable showback and chargeback data. Finally, we will
    break down each of the various components in the Prometheus stack and illustrate
    how they fit together.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prometheus is an open source metrics tool that has become the prevalent open
    source solution for Kubernetes-based platforms. The control plane components expose
    Prometheus metrics, and virtually every production cluster uses Prometheus exporters
    to get metrics from things like the underlying nodes. Due to this, many enterprise
    systems such as Datadog, New Relic, and VMware Tanzu Observability support consuming
    Prometheus metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus metrics are simply a standard format for time series data that can
    actually be used by any system. Prometheus uses a scraping model whereby it collects
    metrics from targets. So applications and infrastructure do not typically *send*
    metrics anywhere, they expose them at an endpoint from which Prometheus can scrape
    them. This model removes the app’s responsibility for knowing anything about the
    metrics system beyond the format in which to present the data.
  prefs: []
  type: TYPE_NORMAL
- en: This scraping model for collecting metrics, its ability to process large amounts
    of data, the use of labels in its data model, and the Prometheus Query Language
    (PromQL) make it a great metrics tool for dynamic, cloud native environments.
    New workloads can be readily introduced and monitored. Expose Prometheus metrics
    from an application or system, add a scrape configuration to a Prometheus server,
    and use PromQL to turn the raw data into meaningful insights and alerts. These
    are some of the core reasons Prometheus became such a popular choice in the Kubernetes
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prometheus provides several critical metrics functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Collects metrics from targets using its scraping model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stores the metrics in a time series database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sends alerts, usually to Alertmanager, which is discussed later in this chapter,
    based on alerting rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposes an HTTP API for other components to access the metrics stored by Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides a dashboard that is useful for executing ad hoc metric queries and
    getting various status info
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most teams use Prometheus for metrics collection paired with Grafana for visualization
    when they start out. However, maintaining organized use of the system in a production
    environment can become challenging for smaller teams. You will have to solve for
    long-term storage of your metrics, scaling Prometheus as the volume of metrics
    grows, as well as organizing the federation of your metrics systems. None of these
    are trivial problems to solve and manage over time. So if the maintenance of the
    metrics stack becomes cumbersome as the system scales, you can migrate to one
    of those commercial systems without changing the type of metrics you use.
  prefs: []
  type: TYPE_NORMAL
- en: Long-Term Storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s important to note that Prometheus is not designed for long-term storage
    of metrics. Instead, it provides support for writing to remote endpoints, and
    there are a [number of solutions](https://oreil.ly/wcaVl) that can be used for
    this kind of integration. The questions you have to answer when providing a metrics
    solution as a part of your application platform are around data retention. Do
    you offer long-term storage only in production? If so, what retention period at
    the Prometheus layer do you offer in nonprod environments? How will you expose
    the metrics in long-term storage to users? Projects such as [Thanos](https://thanos.io)
    and [Cortex](https://cortexmetrics.io) offer tool stacks to help solve these problems.
    Just keep in mind how your platform tenants will be able to leverage these systems
    and let them know what retention policies they can expect.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not every workload is a good fit for the scraping model. In these cases the
    [Prometheus Pushgateway](https://github.com/prometheus/pushgateway) may be used.
    For example, a batch workload that shuts down when it has finished its work may
    not give the Prometheus server the chance to collect all metrics before it disappears.
    For this situation, the batch workload can push its metrics to the Pushgateway
    which, in turn, exposes those metrics for the Prometheus server to retrieve. So
    if your platform will support workloads that call for this support, you will likely
    need to deploy the Pushgateway as a part of your metrics stack and publish information
    for tenants to leverage it. They will need to know where it is in the cluster
    and how to use its REST-like HTTP API. [Figure 9-2](#pushgateway_for_ephemeral_workload)
    illustrates an ephemeral workload leveraging a Prometheus client library that
    supports pushing metrics to a Pushgateway. Those metrics are then scraped by a
    Prometheus server.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0902](assets/prku_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Pushgateway for ephemeral workload.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Custom Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prometheus metrics can be exposed natively by an application. Many applications
    that are developed expressly to run on Kubernetes-based platforms do just this.
    There are several officially supported [client libraries](https://oreil.ly/t9SLv)
    as well as a number of community-supported libraries. Using these, your application
    developers will likely find it trivial to expose custom Prometheus metrics for
    scraping. This is covered in depth in [Chapter 14](ch14.html#application_considerations_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, *exporters* can be used when Prometheus metrics are not natively
    supported by an app or system. Exporters collect data points from an application
    or system, then expose them as Prometheus metrics. A common example of this is
    the Node Exporter. It collects hardware and operating system metrics, then exposes
    those metrics for a Prometheus server to scrape. There are [community-supported
    exporters](https://oreil.ly/JO8sO) for a wide range of popular tools, some of
    which you may find useful.
  prefs: []
  type: TYPE_NORMAL
- en: Once an application that exposes custom metrics is deployed, the next matter
    is adding the application to the scrape configuration of a Prometheus server.
    This is usually done with a ServiceMonitor custom resource used by the Prometheus
    Operator. The Prometheus Operator is covered further in [“Metrics Components”](#metrics_components),
    but for now it is enough to know that you can use a custom Kubernetes resource
    to instruct the operator to auto-discover Services based on their Namespace and
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: In short, instrument the software you develop in-house where possible. Develop
    or leverage exporters where native instrumentation is not feasible. And collect
    the exposed metrics using convenient auto-discovery mechanisms to provide visibility
    into your systems.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the use of labels in the Prometheus data model is powerful, with power
    comes responsibility. You can shoot yourself in the foot with them. If you overuse
    labels, the resource consumption of your Prometheus servers can become untenable.
    Familiarize yourself with the impact of high cardinality of metrics and check
    out the [instrumentation guide](https://oreil.ly/RAskV) in the Prometheus docs.
  prefs: []
  type: TYPE_NORMAL
- en: Organization and Federation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Processing metrics can be particularly compute-intensive, so subdividing this
    computational load can help manage resource consumption for Prometheus servers.
    For example, use one Prometheus server to collect metrics for the platform and
    use other Prometheus servers to collect custom metrics from applications or node
    metrics. This is particularly applicable in larger clusters where there are far
    more scrape targets and much larger volumes of metrics to process.
  prefs: []
  type: TYPE_NORMAL
- en: However, doing this will fragment the locations in which you can see this data.
    One way to solve this is through federation. Federation, in general, refers to
    consolidating data and control into a centralized system. Prometheus federation
    involves collecting important metrics from various Prometheus servers into a central
    Prometheus server. This is accomplished using the same scraping model used to
    collect metrics from workloads. One of the targets that a Prometheus server can
    scrape metrics from is another Prometheus server.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done within a single Kubernetes cluster, among several Kubernetes
    clusters, or both. This provides a very flexible model in that you can organize
    and consolidate your metrics systems in ways that suit the patterns you use to
    manage your Kubernetes clusters. This includes federating in layers or tiers.
    [Figure 9-3](#prometheus_federation) shows an example of a global Prometheus server
    scraping metrics from Prometheus servers in different datacenters which, in turn,
    scrape metrics from targets within their cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0903](assets/prku_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Prometheus federation.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While Prometheus federation is powerful and flexible, it can be complex and
    burdensome to manage. A relatively recent development that offers a compelling
    way to collect metrics from all your Prometheus servers is [Thanos](https://thanos.io),
    an open source project that builds federation-like capabilities on top of Prometheus.
    It is supported by the Prometheus Operator and can be layered onto existing Prometheus
    installations. [Cortex](https://cortexmetrics.io) is another promising project
    in this capacity. Both Thanos and Cortex are incubating projects in the CNCF.
  prefs: []
  type: TYPE_NORMAL
- en: Carefully plan out the organization and federation of your Prometheus servers
    to support scaling and expanding your operations as platform adoption grows. Give
    careful consideration to the consumption model for tenants. Avoid making them
    use a multitude of different dashboards to access metrics for their workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Alerts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prometheus uses alerting rules to generate alerts from metrics. When an alert
    is triggered, the alert will usually be sent to a configured Alertmanager instance.
    Deploying Alertmanager and configuring Prometheus to send alerts to it is somewhat
    trivial when using the Prometheus Operator. Alertmanager will process the alert
    and integrate with messaging systems to make your engineers aware of issues. [Figure 9-4](#alerting_components)
    illustrates the use of distinct Prometheus servers for the platform control plane
    and tenant applications. They both use a common Alertmanager to process alerts
    and notify receivers.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0904](assets/prku_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Alerting components.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, be careful not to *over* alert. Excessive critical alerts will burn
    out your on-call engineers and the noise of false positives can drown out actual
    critical events. So take the time to tune your alerts to be useful. Add useful
    descriptions to the annotations on the alerts so that when engineers are alerted
    to a problem, they have some useful context to understand the situation. Consider
    including links to runbooks or other docs that can aid the resolution of the alerted
    incident.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to alerts for the platform, consider how to expose alerting for
    your tenants so that they may set up alerts on the metrics for their application.
    This involves giving them ways to add alerting rules to Prometheus, which is covered
    more in [“Metrics Components”](#metrics_components). It also includes setting
    up the notification mechanisms through Alertmanager so that application teams
    are alerted according to the rules they set.
  prefs: []
  type: TYPE_NORMAL
- en: Dead man’s switch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One alert in particular is worth addressing as it is universally applicable
    and particularly critical. What happens if your metrics and alerting systems go
    down? How will you get an alert of *that* event? In this case you need to set
    up an alert that fires periodically under normal operating conditions and that,
    if those alerts stop, fires a critical alert to let on-call know your metrics
    and/or alerting systems are down. [PagerDuty](https://oreil.ly/zDJJE) has an integration
    they call Dead Man’s *Snitch* that provides this feature. Alternatively, you could
    set up a custom solution with webhook alerts to a system you install. Regardless
    of the implementation details, ensure you are notified urgently if your alerting
    system goes offline.
  prefs: []
  type: TYPE_NORMAL
- en: Showback and Chargeback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Showback* is a term commonly used to describe resource usage by an organizational
    unit or its workloads. *Chargeback* is the association of costs with that resource
    usage. These are perfect examples of meaningful, actionable expressions of metrics
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes offers the opportunity to dynamically manage compute infrastructure
    used by app development teams. If this readily available capacity is not managed
    well, you may find cluster sprawl and poor utilization of resources. It is greatly
    advantageous to the business to streamline the process of deploying infrastructure
    and workloads for efficiency. However, this streamlining can also lead to waste.
    For this reason, many organizations make their teams and lines of business accountable
    for the usage with showback and chargeback.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make it possible to collect relevant metrics, workloads need to
    be labeled with something useful like a “team” or “owner” name or identifier.
    We recommend establishing a standardized system for this in your organization
    and use admission control to enforce the use of such a label on all Pods deployed
    by platform tenants. There are occasionally other useful methods of identifying
    workloads, such as by Namespace, but labels are the most flexible.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two general approaches to implementing showback:'
  prefs: []
  type: TYPE_NORMAL
- en: Requests
  prefs: []
  type: TYPE_NORMAL
- en: Requests are based on the resources a team reserves with the resource requests
    that are defined for each container in a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Consumption
  prefs: []
  type: TYPE_NORMAL
- en: Consumption is based on what a team actually consumes through resource requests
    *or* actual usage, whichever is higher.
  prefs: []
  type: TYPE_NORMAL
- en: Showback by requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The requests-based method leverages the aggregate resource requests defined
    by a workload. For example, if a Deployment with 10 replicas requests 1 CPU core
    per replica, it is considered to have used 10 cores per unit of time that it was
    running. In this model, if a workload bursts above its requests and uses 1.5 cores
    per replica on average, it would have gotten those resources for free; those additional
    5 cores consumed above its resource requests are *not* attributed to the workload.
    This approach is advantageous in that it is based on what the scheduler can assign
    to nodes in the cluster. The scheduler considers resource requests as reserved
    capacity on a node. If a node has spare resources that aren’t being used, and
    a workload bursts to use that otherwise unused capacity, that workload got those
    resources for free. The solid line in [Figure 9-5](#showback_based_on_cpu_requests)
    indicates the CPU resources attributed to a workload using this method. Consumption
    that bursts above requests is *not* attributed.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0905](assets/prku_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Showback based on CPU requests.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Showback by consumption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a consumption-based model, a workload would be assigned the usage of its
    resource requests *or* its actual usage, whichever is higher. With this approach,
    if a workload commonly and consistently used more than its requested resources,
    it would be shown to have used those resources it actually consumed. This approach
    would remove the possible incentive to game the system by setting resource requests
    low. This could be more likely to lead to resource contention on overcommitted
    nodes. The solid line in [Figure 9-6](#showback_based_on_cpu_consumption_when_bursting_above_requests)
    indicates the CPU resources attributed to a workload using this consumption-based
    method. In this case, consumption that bursts above requests is attributed.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0906](assets/prku_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. Showback based on CPU consumption when bursting above requests.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In [“Metrics Components”](#metrics_components), we will cover kube-state-metrics,
    a platform service that exposes metrics related to Kubernetes resources. If you
    use kube-state-metrics as a part of your metrics stack, you will have the following
    metrics available for resource requests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CPU: `kube_pod_container_resource_requests`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory: `kube_pod_container_resource_requests_memory_bytes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resource usage can be obtained with the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CPU: `container_cpu_usage_seconds_total`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory: `container_memory_usage_bytes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly for showback, you should decide whether to use CPU or memory for determining
    showback for a workload. For this, calculate the percentage of total cluster resource
    consumed by a workload for both CPU and memory. The higher value should apply
    because if a cluster runs out of CPU *or* memory it cannot host more workloads.
    For example, if a workload uses 1% of cluster CPU and 3% of cluster memory, it
    is effectively using 3% of the cluster since a cluster without any more memory
    cannot host any more workloads. This will also help inform whether you should
    employ different node profiles to match the workloads they host, which is discussed
    in [“Infrastructure”](ch02.html#infrastructure).
  prefs: []
  type: TYPE_NORMAL
- en: Chargeback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we solve showback, chargeback becomes possible because we have metrics
    to apply costs to. Costs for machines will usually be pretty straightforward if
    using a public cloud provider. It may be a little more complicated if you are
    buying your own hardware, but somehow or another you need to come up with two
    cost values:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost per unit of time for CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost per unit of time for memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply these costs to the showback value determined and you have a model for
    internally charging your platform tenants.
  prefs: []
  type: TYPE_NORMAL
- en: Network and storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far we’ve looked at showback and chargeback for the compute infrastructure
    used by workloads. This covers a majority of use cases we have seen in the field.
    However, there are workloads that consume considerable networking bandwidth and
    disk storage. This infrastructure can contribute significantly to the true cost
    of running some applications and should be considered in those cases. The model
    will be largely the same: collect the relevant metrics and then decide whether
    to charge according to resources reserved, consumed, or a combination of both.
    How you collect those metrics will depend on the systems used for this infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point we have covered how Prometheus works and the general topics you
    should grasp before diving into the details of the deployed components. Next is
    a tour of those components that are commonly used in a Prometheus metrics stack.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we will examine the components in a very commonly used approach
    to deploying and managing a metrics stack. We’ll also cover some of the management
    tools at your disposal and how all the pieces fit together. [Figure 9-7](#common_components_in_a_prometheus_metrics_stack)
    illustrates a common configuration of components in a Prometheus metrics stack.
    It does not include the Prometheus Operator, which is a utility for deployment
    and management of this stack, rather than a part of the stack itself. The diagram
    does include some autoscaling components to illustrate the role of Prometheus
    Adapter, even though autoscaling is not covered here. See [Chapter 13](ch13.html#autoscaling_chapter)
    for details on that topic.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0907](assets/prku_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Common components in a Prometheus metrics stack.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Prometheus Operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The [Prometheus Operator](https://oreil.ly/k1lMx) is a Kubernetes operator
    that helps deploy and manage the various components of a Kubernetes metrics system
    for the platform itself as well as the tenant workloads. For more information
    about Kubernetes operators in general, see [“The Operator Pattern”](ch11.html#operator_pattern).
    The Prometheus Operator uses several custom resources that represent Prometheus
    servers: Alertmanager deployments, scrape configurations that inform Prometheus
    of the targets to scrape metrics from, and rules for recording metrics and alerting
    on them. This greatly reduces the toil in deploying and configuring Prometheus
    servers in your platform.'
  prefs: []
  type: TYPE_NORMAL
- en: These custom resources are very useful to platform engineers but can also provide
    a very important interface for your platform tenants. If they require a dedicated
    Prometheus server, they can achieve that by submitting a Prometheus resource to
    their Namespace. If they need to add alerting rules to an existing Prometheus
    server, they can do so with a PrometheusRule resource.
  prefs: []
  type: TYPE_NORMAL
- en: The related [kube-prometheus](https://oreil.ly/DITxj) project is a great place
    to start with using the Prometheus Operator. It provides a collection of manifests
    for a complete metrics stack. It includes Grafana dashboard configurations for
    useful visualization out of the box, which is very handy. But treat it as a place
    to start and understand the system so you can mold it to fit your requirements
    so that, once in production, you have confidence that you have comprehensive metrics
    and alerting for your systems.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this section covers the components you will get with a kube-prometheus
    deployment so you can clearly understand and customize these components for your
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus servers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the Prometheus Operator in your clusters, you can create Prometheus custom
    resources that will prompt the operator to create a new StatefulSet for a Prometheus
    server. [Example 9-4](#ex_9-4) is an example manifest for a Prometheus resource.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-4\. Sample Prometheus manifest
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_observability_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Informs the configuration of Prometheus for where to send alerts.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_observability_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The container image to use for Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_observability_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Informs the Prometheus Operator which PrometheusRules apply to this Prometheus
    server. Any PrometheusRule created with the labels shown here will be applied
    to this Prometheus server.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_observability_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This does the same for ServiceMonitors as the `ruleSelector` does for PrometheusRules.
    Any ServiceMonitor resources that have this label will be used to inform the scrape
    config for this Prometheus server.
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus custom resource allows platform operators to readily deploy Prometheus
    servers to collect metrics. As mentioned in [“Organization and Federation”](#organization_and_federation),
    it may be useful to divide metrics collection and processing load among multiple
    deployments of Prometheus within any given cluster. This model is enabled by the
    ability to spin up Prometheus servers using a custom Kubernetes resource.
  prefs: []
  type: TYPE_NORMAL
- en: In some use cases, the ability to spin up Prometheus servers with the Prometheus
    Operator is also helpful to expose to platform tenants. A team’s applications
    may emit a large volume of metrics that will overwhelm existing Prometheus servers.
    And you may want to include a team’s metrics collection and processing in its
    resource budget, so having a dedicated Prometheus server in their Namespace may
    be a useful model. Not every team will have an appetite for this approach where
    they deploy and manage their own Prometheus resources. Many may require further
    abstraction of the details, but it is an option to consider. If using this model,
    don’t discount the added complexity this will introduce for dashboards and alerting
    for the metrics gathered, as well as for federation and long-term storage.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Prometheus servers is one thing, but ongoing management of configuration
    for them is another. For this, the Prometheus Operator has other custom resources,
    the most common being the ServiceMonitor. When you create a ServiceMonitor resource,
    the Prometheus Operator responds by updating the scrape configuration of the relevant
    Prometheus server. [Example 9-5](#ex_9-5) shows a ServiceMonitor that will create
    a scrape configuration for Prometheus to collect metrics from the Kubernetes API
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-5\. Example manifest for a ServiceMonitor resource
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_observability_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the label that is referred to in [Example 9-1](#example_9_1) of a `Prometheus`
    manifest by the `serviceMonitorSelector`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_observability_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `endpoints` provide configuration about the port to use and how to connect
    to the instances that Prometheus will scrape metrics from. This example instructs
    Prometheus to connect using HTTPS and provides a Certificate Authority and server
    name to verify the connection endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_observability_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: In Prometheus terms, a “job” is a collection of instances of a service. For
    example, an individual apiserver is an “instance.” All the apiservers in the cluster
    collectively comprise a “job.” This field indicates which label contains the name
    that should be used for the job in Prometheus. In this case the job will be `apiserver`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_observability_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The `namespaceSelector` instructs Prometheus in which Namespaces to look for
    Services to scrape metrics for this target.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_observability_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The `selector` enables service discovery by way of labels on a Kubernetes Service.
    In other words, any Service (in the default Namespace) that contains the specified
    labels will be used to find the targets to scrape metrics from.
  prefs: []
  type: TYPE_NORMAL
- en: Scrape configurations in a Prometheus server may also be managed with PodMonitor
    resources for monitoring groups of Pods (as opposed to Services with the ServiceMonitor),
    as well as Probe resources for monitoring Ingresses or static targets.
  prefs: []
  type: TYPE_NORMAL
- en: The PrometheusRule resource instructs the operator to generate a rule file for
    Prometheus that contains rules for recording metrics and alerting upon metrics.
    [Example 9-6](#ex_9-6) shows an example of a PrometheusRule manifest that contains
    a recording rule and an alerting rule. These rules will be put in a ConfigMap
    and mounted into the Prometheus server’s Pod/s.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-6\. Example manifest for a PrometheusRule resource
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_observability_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the label that is referred to in [Example 9-1](#example_9_1) of a `Prometheus`
    manifest by the `ruleSelector`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_observability_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of a recording rule for the total LIST and GET requests to
    all Kubernetes API server instances over a 5-minute period. It uses an expression
    on the `apiserver_request_total` metric exposed by the API server and stores a
    new metric called `code_resource:apiserver_request_total:rate5m`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_observability_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an alerting rule that will prompt Prometheus to send a warning alert
    if any Pod gets stuck in a not ready state for more than 15 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Prometheus Operator and these custom resources to manage Prometheus
    servers and their configurations has proven to be a very useful pattern and has
    become very prevalent in the field. If you are using Prometheus as your primary
    metrics tool, we highly recommend it.
  prefs: []
  type: TYPE_NORMAL
- en: Alertmanager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next major component is Alertmanager. This is a separate, distinct workload
    that processes alerts and routes them to receivers that constitute the communication
    medium to the on-call engineers. Prometheus has alerting rules that prompt Prometheus
    to fire off alerts in response to measurable conditions. Those alerts get sent
    to Alertmanager, where they are grouped and deduplicated so that humans don’t
    receive a flood of alerts when outages occur that affect multiple replicas or
    components. Then notifications are sent via the configured receivers. Receivers
    are the supported notification systems, such as email, Slack, or PagerDuty. If
    you want to implement an unsupported or custom notification system, Alertmanager
    has a webhook receiver that allows you to provide a URL to which Alertmanager
    will send a POST request with a JSON payload.
  prefs: []
  type: TYPE_NORMAL
- en: When using the Prometheus Operator, a new Alertmanager can be deployed with
    a manifest, as shown in [Example 9-7](#ex_9-7).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-7\. Example manifest for an Alertmanager resource
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_observability_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple replicas may be requested to deploy Alertmanager in a highly available
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: While this custom resource gives you very convenient methods to deploy Alertmanager
    instances, it is very rarely necessary to deploy multiple Alertmanagers in a cluster,
    especially since it can be deployed in a highly available configuration. You could
    consider a centralized Alertmanager for multiple clusters, but having one per
    cluster is wise since it reduces external dependencies for any given cluster.
    Leveraging a common Alertmanager for a cluster provides the opportunity for tenants
    to leverage a single PrometheusRule resource to configure new alerting rules for
    their app. In this model, each Prometheus server is configured to send alerts
    to the cluster’s Alertmanager.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For platform operators to be able to reason about what is happening in a complex
    Kubernetes-based platform, it is crucial to build charts and dashboards from the
    data stored in Prometheus. [Grafana](https://grafana.com) is an open source visualization
    layer that has become the default solution for viewing Prometheus metrics. The
    kube-prometheus project provides a variety of dashboards to use as a basis and
    starting point, not to mention many others available in the community. And, of
    course, you have the freedom to build your own charts to display the time series
    data from any system you manage as a part of your platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualizing metrics is also critical for application teams. This is relevant
    to how you deploy your Prometheus servers. If you are leveraging multiple Prometheus
    instances in your cluster, how will you expose the metrics gathered to the tenants
    of the platform? On one hand, adding a Grafana dashboard to each Prometheus server
    may be a useful pattern. This could provide a convenient separation of concerns.
    However, on the other hand, if users find they have to routinely log in to multiple
    distinct dashboards, that may be cumbersome. In this case you have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Employ federation to collect metrics from different servers into a single server
    and then add a dashboard to that for a single place to access metrics for a set
    of systems. This is the approach used with projects such as Thanos.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add multiple data sources to a single Grafana dashboard. In this case a single
    dashboard exposes metrics from several Prometheus servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice boils down to whether you prefer the complexity to be in federating
    Prometheus instances or in managing more complex Grafana configurations. There
    is the additional resource consumption to consider with the federation server
    option, but if that is palatable, it is mostly a matter of preference.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a single Prometheus server for a cluster and your platform
    operators and tenants are going to the same place to get metrics, you will need
    to consider permissions around viewing and editing dashboards. You will likely
    need to configure the organizations, teams, and users appropriately for your use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: Node exporter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Node exporter](https://github.com/prometheus/node_exporter) is the node agent
    that usually runs as a Kubernetes DaemonSet and collects machine and operating
    system metrics. It gives you the host-level CPU, memory, disk I/O, disk space,
    network stats, and file descriptor information, to name just a few of the metrics
    it collects by default. As mentioned earlier, this is one of the most common examples
    of an exporter. Linux systems don’t export Prometheus metrics natively. The node
    exporter knows how to collect those metrics from the kernel and then exposes them
    for scraping by Prometheus. It is useful any time you want to monitor the system
    and hardware of Unix-like machines with Prometheus.'
  prefs: []
  type: TYPE_NORMAL
- en: kube-state-metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) provides
    metrics related to a range of Kubernetes resources. It is essentially an exporter
    for information about resources collected from the Kubernetes API. For example,
    kube-state-metrics exposes Pod start times, status, labels, priority class, resource
    requests, and limits; all the information that you would normally use `kubectl
    get` or `kubectl describe` to collect. These metrics can be useful to detect critical
    cluster conditions, such as Pods stuck in crash loops or Namespaces nearing their
    resource quotas.'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus adapter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Prometheus adapter](https://github.com/DirectXMan12/k8s-prometheus-adapter)
    is included here because it is a part of the kube-prometheus stack. However, it
    is not an exporter, nor is it involved in the core functionality of Prometheus.
    Instead, Prometheus adapter is a *client* of Prometheus. It retrieves metrics
    from the Prometheus API and makes them available through the Kubernetes metrics
    APIs. This enables autoscaling functionality for workloads. Refer to [Chapter 13](ch13.html#autoscaling_chapter)
    for more information on autoscaling.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are many components to a production-grade metrics and
    alerting system. We’ve looked at how to accomplish this with Prometheus and the
    patterns demonstrated with the kube-prometheus stack including the Prometheus
    Operator to alleviate toil in managing these concerns. Now that we’ve covered
    logging and metrics, let’s take a look at tracing.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracing in general refers to a specialized kind of event capturing that follows
    an execution path. While tracing can be applicable to a single piece of software,
    in this section we’re dealing with distributed tracing that spans multiple workloads
    and traces requests in microservice architectures. Organizations that have embraced
    distributed systems benefit greatly from this technology. In this section, we’ll
    discuss how to make distributed tracing available as a platform service for your
    application teams.
  prefs: []
  type: TYPE_NORMAL
- en: An important distinction between distributed tracing compared to logging and
    metrics is that the tracing technology between the applications and platform must
    be compatible. As long as an app logs to stdout and stderr, the platform services
    to aggregate logs don’t care how logs are written inside the app. And common metrics
    like CPU and memory consumption can be gathered from workloads without specialized
    instrumentation. However, if an application is instrumented with a client library
    that is incompatible with the tracing system offered by the platform, tracing
    will not work at all. For this reason, close collaboration between platform and
    application development teams is critical in this area.
  prefs: []
  type: TYPE_NORMAL
- en: In covering the topic of distributed tracing, we will first look at the OpenTracing
    and OpenTelemetry specifications along with some of the terminology that is used
    when discussing tracing. We’ll then cover the components that are common with
    the popular projects used for tracing. After that, we’ll touch on the application
    instrumentation necessary to enable tracing as well as the implications of using
    a service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: OpenTracing and OpenTelemetry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[OpenTracing](https://opentracing.io) is an open source specification for distributed
    tracing that helps the ecosystem converge on standards for implementations. The
    spec revolves around three concepts that are important in understanding tracing:'
  prefs: []
  type: TYPE_NORMAL
- en: Trace
  prefs: []
  type: TYPE_NORMAL
- en: When an end user of a distributed application makes a request, that request
    traverses distinct services that process the request and participate in satisfying
    the client request. The trace represents that entire transaction and is the entity
    that we are interested in analyzing. A trace consists of multiple spans.
  prefs: []
  type: TYPE_NORMAL
- en: Span
  prefs: []
  type: TYPE_NORMAL
- en: Each distinct service that processes a request represents a span. The operations
    that occur within the workload boundaries constitute a single span that are part
    of a trace.
  prefs: []
  type: TYPE_NORMAL
- en: Tag
  prefs: []
  type: TYPE_NORMAL
- en: Tags are metadata that are attached to spans to contextualize them within a
    trace and provide searchable indexes.
  prefs: []
  type: TYPE_NORMAL
- en: When traces are visualized, they will generally include each individual trace
    and readily indicate which components in a system are impacting performance the
    most. They will also help track down where errors are occurring and how they are
    affecting other components of an application.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the OpenTracing project merged with OpenCensus to form [OpenTelemetry](https://opentelemetry.io).
    At the time of this writing, OpenTelemetry support is still experimental in Jaeger,
    which is a fair barometer of adoption, but it is reasonable to expect OpenTelemetry
    to become the de facto standard.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To offer distributed tracing as a platform service, there needs to be a few
    platform components in place. The patterns we’ll discuss here are applicable to
    open source projects such as [Zipkin](https://zipkin.io) and [Jaeger](https://www.jaegertracing.io),
    but the same models will often apply for other projects and commercially supported
    products that implement the OpenTracing standards.
  prefs: []
  type: TYPE_NORMAL
- en: Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each component in a distributed application will output a span for each request
    processed. The agent acts as a server to which the application can send the span
    information. In a Kubernetes-based platform it will usually be a node agent that
    runs on each machine in the cluster and receives all spans for workloads on that
    node. The agent will forward batched spans to a central collector.
  prefs: []
  type: TYPE_NORMAL
- en: Collector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The collector processes the spans and stores them in the backend database. It
    is responsible for validating, indexing, and performing any transformations before
    persisting the spans to storage.
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The supported databases vary from project to project, but the usual suspects
    are [Cassandra](https://cassandra.apache.org) and [Elasticsearch](https://www.elastic.co/elasticsearch).
    Even when sampling, distributed tracing systems collect very large amounts of
    data. The databases used need to be able to process and quickly search these large
    volumes of data to produce useful analysis.
  prefs: []
  type: TYPE_NORMAL
- en: API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you might expect, the next component is an API that allows clients to access
    the stored data. It exposes the traces and their spans to other workloads or visualization
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: User interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is where the rubber hits the road for your platform tenants. This visualization
    layer queries the API and reveals the data to app developers. It is where engineers
    can view the data collected in useful charts to analyze their systems and distributed
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-8](#components_of_a_tracing_platform_service) illustrates these tracing
    components, their relationships to one another, and the common deployment methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0908](assets/prku_0908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. Components of a tracing platform service.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Application Instrumentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order for these spans to get collected and correlated together into traces,
    the application has to be instrumented to deliver this information. For this reason,
    it is crucial to get buy-in from your application development teams. The best
    tracing platform service in the world is useless if the applications are not delivering
    the raw data needed. [Chapter 14](ch14.html#application_considerations_chapter)
    goes into this topic in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: Service Meshes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If using a service mesh, you will likely want your mesh data included in your
    traces. Service meshes implement proxies for requests going to and from your workloads,
    and getting timing trace spans on those proxies helps you understand how they
    affect performance. Note that your application will still need to be instrumented,
    even if using a service mesh. The request headers need to be propagated from one
    service request to the next through a trace. Service meshes are covered in detail
    in [Chapter 6](ch06.html#chapter6).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability is a core concern in platform engineering. It’s fair to say that
    no application platform could be considered production-ready without observability
    solved. As a baseline, make sure you are able to reliably collect logs from your
    containerized workloads and forward them to your logging backend along with the
    audit logs from the Kubernetes API server. Also consider metrics and alerting
    a minimum requirement. Collect the metrics exposed by the Kubernetes control plane,
    surface them in a dashboard, and alert on them. Work with your application development
    teams to instrument their applications to expose metrics where applicable and
    collect those, too. Finally, if your teams have embraced microservice architectures,
    work with your app dev teams to instrument their apps for tracing and install
    the platform components to leverage that information as well. With these systems
    in place you will have the visibility to troubleshoot and refine your operations
    to improve performance and stability.
  prefs: []
  type: TYPE_NORMAL
