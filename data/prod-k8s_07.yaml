- en: Chapter 6\. Service Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Service routing is a crucial capability of a Kubernetes-based platform. While
    the container networking layer takes care of the low-level primitives that connect
    Pods, developers need higher-level mechanisms to interconnect services (i.e.,
    east-west service routing) and to expose applications to their clients (i.e.,
    north-south service routing). Service routing encompasses three concerns that
    provide such mechanisms: Services, Ingress, and service mesh.'
  prefs: []
  type: TYPE_NORMAL
- en: Services provide a way to treat a set of Pods as a single unit or network service.
    They provide load balancing and routing features that enable horizontal scaling
    of applications across the cluster. Furthermore, Services offer service discovery
    mechanisms that applications can use to discover and interact with their dependencies.
    Finally, Services also provide layer 3/4 mechanisms to expose workloads to network
    clients outside of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress handles north-south routing in the cluster. It serves as an entry point
    into workloads running in the cluster, mainly HTTP and HTTPS services. Ingress
    provides layer 7 load balancing capabilities that enable more granular traffic
    routing than Services. The load balancing of traffic is handled by an Ingress
    controller, which must be installed in the cluster. Ingress controllers leverage
    proxy technologies such as Envoy, NGINX, or HAProxy. The controller gets the Ingress
    configuration from the Kubernetes API and configures the proxy accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: A service mesh is a service routing layer that provides advanced routing, security,
    and observability features. It is mainly concerned with east-west service routing,
    but some implementations can also handle north-south routing. Services in the
    mesh communicate with each other through proxies that augment the connection.
    The use of proxies makes meshes compelling, as they enhance workloads without
    changes to source code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter digs into these service routing capabilities, which are critical
    in production Kubernetes platforms. First, we will discuss Services, the different
    Service types, and how they are implemented. Next, we will explore Ingress, Ingress
    controllers, and the different considerations to take into account when running
    Ingress in production. Finally, we will cover service meshes, how they work on
    Kubernetes, and considerations to make when adopting a service mesh in a production
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes Service is foundational to service routing. The Service is a
    network abstraction that provides basic load balancing across several Pods. In
    most cases, workloads running in the cluster use Services to communicate with
    each other. Using Services instead of Pod IPs is preferred because of the fungible
    nature of Pods.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will review Kubernetes Services and the different Service
    types. We will also look at Endpoints, another Kubernetes resource that is intimately
    related to Services. We will then dive into the Service implementation details
    and discuss kube-proxy. Finally, we will discuss Service Discovery and considerations
    to make for the in-cluster DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: The Service Abstraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Service is a core API resource in Kubernetes that load balances traffic
    across multiple Pods. The Service does load balancing at the L3/L4 layers in the
    OSI model. It takes a packet with a destination IP and port and forwards it to
    a backend Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancers typically have a frontend and a backend pool. Services do as
    well. The frontend of a Service is the ClusterIP. The ClusterIP is a virtual IP
    address (VIP) that is accessible from within the cluster. Workloads use this VIP
    to communicate with the Service. The backend pool is a collection of Pods that
    satisfy the Service’s Pod selector. These Pods receive the traffic destined for
    the Cluster IP. [Figure 6-1](#the_service_has_a_frontend_and_a_backend_pool) depicts
    the frontend of a Service and its backend pool.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0601](assets/prku_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The Service has a frontend and a backend pool. The frontend is
    the ClusterIP, while the backend is a set of Pods.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Service IP Address Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in the previous chapter, you configure two ranges of IP addresses
    when deploying Kubernetes. On the one hand, the Pod IP range or CIDR block provides
    IP addresses to each Pod in the cluster. On the other hand, the Service CIDR block
    provides the IP addresses for Services in the cluster. This CIDR is the range
    that Kubernetes uses to assign ClusterIPs to Services.
  prefs: []
  type: TYPE_NORMAL
- en: The API server handles the IP Address Management (IPAM) for Kubernetes Services.
    When you create a Service, the API Server (with the help of etcd) allocates an
    IP address from the Service CIDR block and writes it to the Service’s ClusterIP
    field.
  prefs: []
  type: TYPE_NORMAL
- en: When creating Services, you can also specify the ClusterIP in the Service specification.
    In this case, the API Server makes sure that the requested IP address is available
    and within the Services CIDR block. With that said, explicitly setting ClusterIPs
    is an antipattern.
  prefs: []
  type: TYPE_NORMAL
- en: The Service resource
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Service resource contains the configuration of a given Service, including
    the name, type, ports, etc. [Example 6-1](#service_definitions_that_exposes_nginz_on_a_clusterip)
    is an example Service definition in its YAML representation named `nginx`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Service definition that exposes NGINX on a ClusterIP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The Pod selector. Kubernetes uses this selector to find the Pods that belong
    to this Service.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Ports that are accessible through the Service.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_service_routing_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports TCP, UDP, and SCTP protocols in Services.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_service_routing_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Port where the Service can be reached.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_service_routing_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Port where the backend Pod is listening, which can be different than the port
    exposed by the Service (the `port` field above).
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_service_routing_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster IP that Kubernetes allocated for this Service.
  prefs: []
  type: TYPE_NORMAL
- en: The Service’s Pod selector determines the Pods that belong to the Service. The
    Pod selector is a collection of key/value pairs that Kubernetes evaluates against
    Pods in the same Namespace as the Service. If a Pod has the same key/value pairs
    in their labels, Kubernetes adds the Pod’s IP address to the backend pool of the
    Service. The management of the backend pool is handled by the Endpoints controller
    through Endpoints resources. We will discuss Endpoints in more detail later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Service types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to this point, we have mainly talked about the ClusterIP Service, which is
    the default Service type. Kubernetes offers multiple Service types that offer
    additional features besides the Cluster IP. In this section, we will discuss each
    Service type and how they are useful.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have already discussed this Service type in the previous sections. To recap,
    the ClusterIP Service creates a virtual IP address (VIP) that is backed by one
    or more Pods. Usually, the VIP is available only to workloads running inside the
    cluster. [Figure 6-2](#the_clusterip_service_is_a_vip_that_is_accessible_to_workloads_running_within_the_cluster)
    shows a ClusterIP Service.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0602](assets/prku_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. The ClusterIP Service is a VIP that is accessible to workloads
    running within the cluster.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: NodePort
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The NodePort Service is useful when you need to expose a Service to network
    clients outside of the cluster, such as existing applications running in VMs or
    users of a web application.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, the NodePort Service exposes the Service on a port across
    all cluster nodes. The port is assigned randomly from a configurable port range.
    Once assigned, all nodes in the cluster listen for connections on the given port.
    [Figure 6-3](#the_nodeport_service_opens_a_random_port_on_all_cluster_nodes) shows
    a NodePort Service.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0603](assets/prku_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The NodePort Service opens a random port on all cluster nodes.
    Clients outside of the cluster can reach the Service through this port.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The primary challenge with NodePort Services is that clients need to know the
    Service’s node port number and the IP address of at least one cluster node to
    reach the Service. This is problematic because nodes can fail or be removed from
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: A common way to solve this challenge is to use an external load balancer in
    front of the NodePort Service. With this approach, clients don’t need to know
    the IP addresses of cluster nodes or the Service’s port number. Instead, the load
    balancer functions as the single entry point to the Service.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to this solution is that you need to manage external load balancers
    and update their configuration constantly. Did a developer create a new NodePort
    Service? Create a new load balancer. Did you add a new node to the cluster? Add
    the new node to the backend pool of all load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, there are better alternatives to using a NodePort Service. The
    LoadBalancer Service, which we’ll discuss next, is one of those options. Ingress
    controllers are another option, which we’ll explore later in this chapter in [“Ingress”](#ingress).
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The LoadBalancer Service builds upon the NodePort Service to address some of
    its downsides. At its core, the LoadBalancer Service is a NodePort Service under
    the hood. However, the LoadBalancer Service has additional functionality that
    is satisfied by a controller.
  prefs: []
  type: TYPE_NORMAL
- en: The controller, also known as a cloud provider integration, is responsible for
    automatically gluing the NodePort Service with an external load balancer. In other
    words, the controller takes care of creating, managing, and configuring external
    load balancers in response to the configuration of LoadBalancer Services in the
    cluster. The controller does this by interacting with APIs that provision or configure
    load balancers. This interaction is depicted in [Figure 6-4](#the_loadbalancer_service_leverages_a_cloud_provider_integration).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0604](assets/prku_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. The LoadBalancer Service leverages a cloud provider integration
    to create an external load balancer, which forwards traffic to the node ports.
    At the node level, the Service is the same as a NodePort.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes has built-in controllers for several cloud providers, including Amazon
    Web Services (AWS), Google Cloud, and Microsoft Azure. These integrated controllers
    are usually called in-tree cloud providers, as they are implemented inside the
    Kubernetes source code tree.
  prefs: []
  type: TYPE_NORMAL
- en: As the Kubernetes project evolved, out-of-tree cloud providers emerged as an
    alternative to in-tree providers. Out-of-tree providers enabled load balancer
    vendors to provide their implementations of the LoadBalancer Service control loop.
    At this time, Kubernetes supports both in-tree and out-of-tree providers. However,
    the community is quickly adopting out-of-tree providers, given that the in-tree
    providers are deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: ExternalName
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ExternalName Service type does not perform any kind of load balancing or
    proxying. Instead, an ExternalName Service is primarily a service discovery construct
    implemented in the cluster’s DNS. An ExternalName Service maps a cluster Service
    to a DNS name. Because there is no load balancing involved, Services of this type
    lack ClusterIPs.
  prefs: []
  type: TYPE_NORMAL
- en: ExternalName Services are useful in different ways. Piecemeal application migration
    efforts, for example, can benefit from ExternalName Services. If you migrate components
    of an application to Kubernetes while leaving some of its dependencies outside,
    you can use an ExternalName Service as a bridge while you complete the migration.
    Once you migrate the entire application, you can change the Service type to a
    ClusterIP without having to change the application deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Even though useful in creative ways, the ExternalName Service is probably the
    least common Service type in use.
  prefs: []
  type: TYPE_NORMAL
- en: Headless Service
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like the ExternalName Service, the Headless Service type does not allocate a
    ClusterIP or provide any load balancing. The Headless Service merely functions
    as a way to register a Service and its Endpoints in the Kubernetes API and the
    cluster’s DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: Headless Services are useful when applications need to connect to specific replicas
    or Pods of a service. Such applications can use service discovery to find all
    the Pod IPs behind the Service and then establish connections to specific Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Supported communication protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes Services support a specific set of protocols: TCP, UDP, and SCTP.
    Each port listed in a Service resource specifies the port number and the protocol.
    Services can expose multiple ports with different protocols. For example, the
    following snippet shows the YAML definition of the `kube-dns` Service. Notice
    how the list of ports includes TCP port 53 and UDP port 53:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we’ve discussed up to this point, Services load balance traffic across Pods.
    The Service API resource represents the frontend of the load balancer. The backend,
    or the collection of Pods that are behind the load balancer, are tracked by the
    Endpoints resource and controller, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Endpoints resource is another API object that is involved in the implementation
    of Kubernetes Services. Every Service resource has a sibling Endpoints resource.
    If you recall the load balancer analogy, you can think of the Endpoints object
    as the pool of IP addresses that receive traffic. [Figure 6-5](#relationship_between_the_service_and_the_endpoint_resources)
    shows the relationship between a Service and an Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0605](assets/prku_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Relationship between the Service and the Endpoints resources.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Endpoints resource
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An example Endpoints resource for the `nginx` Service in [Example 6-1](#service_definitions_that_exposes_nginz_on_a_clusterip)
    might look like this (some extraneous fields have been removed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, there are two Pods backing the `nginx` Service. Network traffic
    destined to the `nginx` ClusterIP is load balanced across these two Pods. Also
    notice how the port is 8080 and not 80\. This port matches the `targetPort` field
    specified in the Service. It is the port that the backend Pods are listening on.
  prefs: []
  type: TYPE_NORMAL
- en: The Endpoints controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interesting thing about the Endpoints resource is that Kubernetes creates
    it automatically when you create a Service. This is somewhat different from other
    API resources that you usually interact with.
  prefs: []
  type: TYPE_NORMAL
- en: The Endpoints controller is responsible for creating and maintaining the Endpoints
    objects. Whenever you create a Service, the Endpoints controller creates the sibling
    Endpoints resource. More importantly, it also updates the list of IPs within the
    Endpoints object as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The controller uses the Service’s Pod selector to find the Pods that belong
    to the Service. Once it has the set of Pods, the controller grabs the Pod IP addresses
    and updates the Endpoints resource accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Addresses in the Endpoints resource can be in one of two sets: (ready) addresses
    and notReadyAddresses. The Endpoints controller determines whether an address
    is ready by inspecting the corresponding Pod’s Ready condition. The Pod’s Ready
    condition, in turn, depends on multiple factors. One of them, for example, is
    whether the Pod was scheduled. If the Pod is pending (not scheduled), its Ready
    condition is false. Ultimately, a Pod is considered ready when it is running and
    passing its readiness probe.'
  prefs: []
  type: TYPE_NORMAL
- en: Pod readiness and readiness probes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we discussed how the Endpoints controller determines
    whether a Pod IP address is ready to accept traffic. But how does Kubernetes tell
    whether a Pod is ready or not?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two complementary methods that Kubernetes uses to determine Pod readiness:'
  prefs: []
  type: TYPE_NORMAL
- en: Platform information
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has information about the workloads under its management. For example,
    the system knows whether the Pod has been successfully scheduled to a node. It
    also knows whether the Pod’s containers are up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Readiness probes
  prefs: []
  type: TYPE_NORMAL
- en: Developers can configure readiness probes on their workloads. When set, the
    kubelet probes the workload periodically to determine if it is ready to receive
    traffic. Probing Pods for readiness is more powerful than determining readiness
    based on platform information because the probe checks for application-specific
    concerns. For example, the probe can check whether the application’s internal
    initialization process has completed.
  prefs: []
  type: TYPE_NORMAL
- en: Readiness probes are essential. Without them, the cluster would route traffic
    to workloads that might not be able to handle it, which would result in application
    errors and irritated end users. Ensure that you always define readiness probes
    in the applications you deploy to Kubernetes. In [Chapter 14](ch14.html#application_considerations_chapter),
    we will further discuss readiness probes.
  prefs: []
  type: TYPE_NORMAL
- en: The EndpointSlices resource
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The EndpointSlices resource is an optimization implemented in Kubernetes v1.16\.
    It addresses scalability issues that can arise with the Endpoints resource in
    [large cluster deployments](https://oreil.ly/H8rHC). Let’s review these issues
    and explore how EndpointSlices help.
  prefs: []
  type: TYPE_NORMAL
- en: To implement Services and make them routable, each node in the cluster watches
    the Endpoints API and subscribes for changes. Whenever an Endpoints resource is
    updated, it must be propagated to all nodes in the cluster to take effect. A scaling
    event is a good example. Whenever there is a change to the set of Pods in the
    Endpoints resource, the API server sends the entire updated object to all the
    cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach to handling the Endpoints API does not scale well with larger
    clusters for multiple reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Large clusters contain many nodes. The more nodes in the cluster, the more updates
    need to be sent when Endpoints objects change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The larger the cluster, the more Pods (and Services) you can host. As the number
    of Pods increases, the frequency of Endpoints resource updates also grows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of Endpoints resources increases as the number of Pods that belong
    to the Service grows. Larger Endpoints objects require more network and storage
    resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EndpointSlices resource fixes these issues by splitting the set of endpoints
    across multiple resources. Instead of placing all the Pod IP addresses in a single
    Endpoints resource, Kubernetes splits the addresses across various EndpointSlice
    objects. By default, EndpointSlice objects are limited to 100 endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore a scenario to better understand the impact of EndpointSlices.
    Consider a Service with 10,000 endpoints, which would result in 100 EndpointSlice
    objects. If one of the endpoints is removed (due to a scale-in event, for example),
    the API server sends the affected EndpointSlice object to each node. Sending a
    single EndpointSlice with 100 endpoints is much more efficient than sending a
    single Endpoints resource with thousands of endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the EndpointSlices resource improves the scalability of Kubernetes
    by splitting a large number of endpoints into a set of EndpointSlice objects.
    If you are running a platform that has Services with hundreds of endpoints, you
    might benefit from the EndpointSlice improvements. Depending on your Kubernetes
    version, the EndpointSlice functionality is opt-in. If you are running Kubernetes
    v1.18, you must set a feature flag in kube-proxy to enable the use of EndpointSlice
    resources. Starting with Kubernetes v1.19, the EndpointSlice functionality will
    be enabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: Service Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we’ve talked about Services, Endpoints, and what they provide to
    workloads in a Kubernetes cluster. But how does Kubernetes implement Services?
    How does it all work?
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the different approaches available when it
    comes to realizing Services in Kubernetes. First, we will talk about the overall
    kube-proxy architecture. Next, we will review the different kube-proxy data plane
    modes. Finally, we will discuss alternatives to kube-proxy, such as CNI plug-ins
    that are capable of taking over kube-proxy’s responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Kube-proxy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kube-proxy is an agent that runs on every cluster node. It is primarily responsible
    for making Services available to the Pods running on the local node. It achieves
    this by watching the API server for Services and Endpoints and programming the
    Linux networking stack (using iptables, for example) to handle packets accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Historically, kube-proxy acted as a network proxy between Pods running on the
    node and Services. This is where the kube-proxy name came from. As the Kubernetes
    project evolved, however, kube-proxy stopped being a proxy and became more of
    a node agent or localized control plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kube-proxy supports three modes of operation: userspace, iptables, and IPVS.
    The userspace proxy mode is seldom used, since iptables and IPVS are better alternatives.
    Thus, we will only cover the iptables and IPVS modes in the following sections
    of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kube-proxy: iptables mode'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The iptables mode is the default kube-proxy mode at the time of writing (Kubernetes
    v1.18). It is safe to say that the iptables mode is the most prevalent across
    cluster installations today.
  prefs: []
  type: TYPE_NORMAL
- en: In the iptables mode, kube-proxy leverages the network address translation (NAT)
    features of iptables.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP Services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To realize ClusterIP Services, kube-proxy programs the Linux kernel’s NAT table
    to perform Destination NAT (DNAT) on packets destined for Services. The DNAT rules
    replace the packet’s destination IP address with the IP address of a Service endpoint
    (a Pod IP address). Once replaced, the network handles the packet as if it was
    originally sent to the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load balance traffic across multiple Service endpoints, kube-proxy uses
    multiple iptables chains:'
  prefs: []
  type: TYPE_NORMAL
- en: Services chain
  prefs: []
  type: TYPE_NORMAL
- en: Top-level chain that contains a rule for each Service. Each rule checks whether
    the destination IP of the packet matches the ClusterIP of the Service. If it does,
    the packet is sent to the Service-specific chain.
  prefs: []
  type: TYPE_NORMAL
- en: Service-specific chain
  prefs: []
  type: TYPE_NORMAL
- en: Each Service has its iptables chain. This chain contains a rule per Service
    endpoint. Each rule uses the `statistic` iptables extension to select a target
    endpoint randomly. Each endpoint has 1/n probability of being selected, where
    n is the number of endpoints. Once selected, the packet is sent to the Service
    endpoint chain.
  prefs: []
  type: TYPE_NORMAL
- en: Service endpoint chain
  prefs: []
  type: TYPE_NORMAL
- en: Each Service endpoint has an iptables chain that performs DNAT on the packet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following listing of iptables rules shows an example of a ClusterIP Service.
    The Service is called `nginx` and has three endpoints (extraneous iptables rules
    have been removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the top-level chain. It has rules for all the Services in the cluster.
    Notice how the `KUBE-SVC-4N57TFCL4MD7ZTDA` rule specifies a destination IP of
    10.97.85.96\. This is the `nginx` Service’s ClusterIP.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the chain of the `nginx` Service. Notice how there is a rule for each
    Service endpoint with a given probability of matching the rule.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_service_routing_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This chain corresponds to one of the Service endpoints. (SEP stands for Service
    endpoint.) The last rule in this chain is the one that performs DNAT to forward
    the packet to the endpoint (or Pod).
  prefs: []
  type: TYPE_NORMAL
- en: NodePort and LoadBalancer Services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When it comes to NodePort and LoadBalancer Services, kube-proxy configures iptables
    rules similar to those used for ClusterIP Services. The main difference is that
    the rules match packets based on their destination port number. If they match,
    the rule sends the packet to the Service-specific chain where DNAT happens. The
    snippet below shows the iptables rules for a NodePort Service called `nginx` listening
    on port 31767.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Kube-proxy programs iptables rules for NodePort Services in the `KUBE-NODEPORTS`
    chain.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the packet has `tcp: 31767` as the destination port, it is sent to the Service-specific
    chain. This chain is the Service-specific chain we saw in callout 2 in the previous
    code snippet.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to programming the iptables rules, kube-proxy opens the port assigned
    to the NodePort Service and holds it open. Holding on to the port has no function
    from a routing perspective. It merely prevents other processes from claiming it.
  prefs: []
  type: TYPE_NORMAL
- en: 'A key consideration to make when using NodePort and LoadBalancer Services is
    the Service’s external traffic policy setting. The external traffic policy determines
    whether the Service routes external traffic to node-local endpoints (`externalTrafficPolicy:
    Local`) or cluster-wide endpoints (`externalTrafficPolicy: Cluster`). Each policy
    has benefits and trade-offs, as discussed next.'
  prefs: []
  type: TYPE_NORMAL
- en: When the policy is set to `Local`, the Service routes traffic to endpoints (Pods)
    running on the node receiving the traffic. Routing to a local endpoint has two
    important benefits. First, there is no SNAT involved so the source IP is preserved,
    making it available to the workload. And second, there is no additional network
    hop that you would otherwise incur when forwarding traffic to another node. With
    that said, the `Local` policy also has downsides. Mainly, traffic that reaches
    a node that lacks Service endpoints is dropped. For this reason, the `Local` policy
    is usually combined with an external load balancer that health-checks the nodes.
    When the node doesn’t have an endpoint for the Service, the load balancer does
    not send traffic to the node, given that the health check fails. [Figure 6-6](#loadbalancer_service_with_local_external_traffic_policy)
    illustrates this functionality. Another downside of the `Local` policy is the
    potential for unbalanced application load. For example, if a node has three Service
    endpoints, each endpoint receives 33% of the traffic. If another node has a single
    endpoint, it receives 100% of the traffic. This imbalance can be mitigated by
    spreading the Pods with anti-affinity rules or using a DaemonSet to schedule the
    Pods.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0606](assets/prku_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. LoadBalancer Service with `Local` external traffic policy. The
    external load balancer runs health checks against the nodes. Any node that does
    not have a Service endpoint is removed from the load balancer’s backend pool.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you have a Service that handles a ton of external traffic, using the `Local`
    external policy is usually the right choice. However, if you do not have a load
    balancer at your disposal, you should use the `Cluster` external traffic policy.
    With this policy, traffic is load balanced across all endpoints in the cluster,
    as shown in [Figure 6-7](#loadbalancer_service_with_cluster_external_traffic_policy).
    As you can imagine, the load balancing results in the loss of the Source IP due
    to SNAT. It can also result in an additional network hop. However, the `Cluster`
    policy does not drop external traffic, regardless of where the endpoint Pods are
    running.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0607](assets/prku_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. LoadBalancer Service with `Cluster` external traffic policy. Nodes
    that do not have node-local endpoints forward the traffic to an endpoint on another
    node.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Connection tracking (conntrack)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When the kernel’s networking stack performs DNAT on a packet destined to a Service,
    it adds an entry to the connection tracking (conntrack) table. The table tracks
    the translation performed so that it is applied to any additional packet destined
    to the same Service. The table is also used to remove the NAT from response packets
    before sending them to the source Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Each entry in the table maps the pre-NAT protocol, source IP, source port, destination
    IP, and destination port onto the post-NAT protocol, source IP, source port, destination
    IP, and destination port. (Entries include additional information but are not
    relevant in this context.) [Figure 6-8](#connection_tracking_conntrack_table_entry_that_tracks)
    depicts a table entry that tracks the connection from a Pod (`192.168.0.9`) to
    a Service (`10.96.0.14`). Notice how the destination IP and port change after
    the DNAT.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0608](assets/prku_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Connection tracking (conntrack) table entry that tracks the connection
    from a Pod (`192.168.0.9`) to a Service (`10.96.0.14`).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When the conntrack table fills up, the kernel starts dropping or rejecting connections,
    which can be problematic for some applications. If you are running workloads that
    handle many connections and notice connection issues, you may need to tune the
    maximum size of the conntrack table on your nodes. More importantly, you should
    monitor the conntrack table utilization and alert when the table is close to being
    full.
  prefs: []
  type: TYPE_NORMAL
- en: Masquerade
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You may have noticed that we glossed over the `KUBE-MARK-MASQ` iptables rules
    listed in the previous examples. These rules are in place for packets that arrive
    at a node from outside the cluster. To route such packets properly, the Service
    fabric needs to masquerade/source NAT the packets when forwarding them to another
    node. Otherwise, response packets will contain the IP address of the Pod that
    handled the request. The Pod IP in the packet would cause a connection issue,
    as the client initiated the connection to the node and not the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Masquerading is also used to egress from the cluster. When Pods connect to external
    services, the source IP must be the IP address of the node where the Pod is running
    instead of the Pod IP. Otherwise, the network would drop response packets because
    they would have the Pod IP as the destination IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Performance concerns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The iptables mode has served and continues to serve Kubernetes clusters well.
    With that said, you should be aware of some performance and scalability limitations,
    as these can arise in large cluster deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Given the structure of the iptables rules and how they work, whenever a Pod
    establishes a new connection to a Service, the initial packet traverses the iptables
    rules until it matches one of them. In the worst-case scenario, the packet needs
    to traverse the entire collection of iptables rules.
  prefs: []
  type: TYPE_NORMAL
- en: The iptables mode suffers from O(n) time complexity when it processes packets.
    In other words, the iptables mode scales linearly with the number of Services
    in the cluster. As the number of Services grows, the performance of connecting
    to Services gets worse.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps more important, updates to the iptables rules also suffer at large scale.
    Because iptables rules are not incremental, kube-proxy needs to write out the
    entire table for every update. In some cases, these updates can even take minutes
    to complete, which risks sending traffic to stale endpoints. Furthermore, kube-proxy
    needs to hold the iptables lock (`/run/xtables.lock`) during these updates, which
    can cause contention with other processes that need to update the iptables rules,
    such as CNI plug-ins.
  prefs: []
  type: TYPE_NORMAL
- en: Linear scaling is an undesirable quality of any system. With that said, based
    on [tests](https://oreil.ly/YJAu9) performed by the Kubernetes community, you
    should not notice any performance degradation unless you are running clusters
    with tens of thousands of Services. If you are operating at that scale, however,
    you might benefit from the IPVS mode in kube-proxy, which we’ll discuss in the
    following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kube-proxy: IP Virtual Server (IPVS) mode'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IPVS is a load balancing technology built into the Linux kernel. Kubernetes
    added support for IPVS in kube-proxy to address the scalability limitations and
    performance issues of the iptables mode.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous section, the iptables mode uses iptables rules
    to implement Kubernetes Services. The iptables rules are stored in a list, which
    packets need to traverse in its entirety in the worst-case scenario. IPVS does
    not suffer from this problem because it was originally designed for load balancing
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The IPVS implementation in the Linux kernel uses hash tables to find the destination
    of a packet. Instead of traversing the list of Services when a new connection
    is established, IPVS immediately finds the destination Pod based on the Service
    IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss how kube-proxy in IPVS mode handles each of the Kubernetes Service
    types.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP Services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When handling Services that have a ClusterIP, kube-proxy in `ipvs` mode does
    a couple of things. First, it adds the IP address of the ClusterIP Service to
    a dummy network interface on the node called `kube-ipvs0`, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After updating the dummy interface, kube-proxy creates an IPVS virtual service
    with the IP address of the ClusterIP Service. Finally, for each Service endpoint,
    it adds an IPVS real server to the IPVS virtual service. The following snippet
    shows the IPVS virtual service and real servers for a ClusterIP Service with three
    endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the IPVS virtual service. Its IP address is the IP address of the ClusterIP
    Service.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the IPVS real servers. It corresponds to one of the Service endpoints
    (Pods).
  prefs: []
  type: TYPE_NORMAL
- en: NodePort and LoadBalancer Services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For NodePort and LoadBalancer Services, kube-proxy creates an IPVS virtual
    service for the Service’s cluster IP. Kube-proxy also creates an IPVS virtual
    service for each of the node’s IP addresses and the loopback address. For example,
    the following snippet shows a listing of the IPVS virtual services created for
    a NodePort Service listening on TCP port 30737:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: IPVS virtual service listening on the node’s IP address.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: IPVS virtual service listening on the Service’s cluster IP address.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_service_routing_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: IPVS virtual service listening on `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_service_routing_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: IPVS virtual service listening on a secondary network interface on the node.
  prefs: []
  type: TYPE_NORMAL
- en: Running without kube-proxy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Historically, kube-proxy has been a staple in all Kubernetes deployments. It
    is a vital component that makes Kubernetes Services work. As the community evolves,
    however, we could start seeing Kubernetes deployments that do not have kube-proxy
    running. How is this possible? What handles Services instead?
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of extended Berkeley Packet Filters (eBPF), CNI plug-ins such
    as [Cilium](https://oreil.ly/sWoh5) and [Calico](https://oreil.ly/0jrKG) can absorb
    kube-proxy’s responsibilities. Instead of handling Services with iptables or IPVS,
    the CNI plug-ins program Services right into the Pod networking data plane. Using
    eBPF improves the performance and scalability of Services in Kubernetes, given
    that the eBPF implementation uses hash tables for endpoint lookups. It also improves
    Service update processing, as it can handle individual Service updates efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the need for kube-proxy and optimizing Service routing is a worthy
    feat, especially for those operating at scale. However, it is still early days
    when it comes to running these solutions in production. For example, the Cilium
    implementation requires newer kernel versions to support a kube-proxy-less deployment
    (at the time of writing, the latest Cilium version is `v1.8`). Similarly, the
    Calico team discourages the use of eBPF in production because it is still in tech
    preview. (At the time of writing, the latest calico version is `v3.15.1`.) Over
    time, we expect to see kube-proxy replacements become more common. Cilium even
    supports running its proxy replacement capabilities alongside other CNI plug-ins
    (referred to as [CNI chaining](https://oreil.ly/jZ-2r)).
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Service discovery provides a mechanism for applications to discover services
    that are available on the network. While not a *routing* concern, service discovery
    is intimately related to Kubernetes Services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Platform teams may wonder whether they need to introduce a dedicated service
    discovery system to a cluster, such as Consul. While possible, it is typically
    not necessary, as Kubernetes offers service discovery to all workloads running
    in the cluster. In this section, we will discuss the different service discovery
    mechanisms available in Kubernetes: DNS-based service discovery, API-based service
    discovery, and environment variable-based service discovery.'
  prefs: []
  type: TYPE_NORMAL
- en: Using DNS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes provides service discovery over DNS to workloads running inside the
    cluster. Conformant Kubernetes deployments run a DNS server that integrates with
    the Kubernetes API. The most common DNS server in use today is [CoreDNS](https://coredns.io),
    an open source, extensible DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: 'CoreDNS watches resources in the Kubernetes API server. For each Kubernetes
    Service, CoreDNS creates a DNS record with the following format: `<service-name>.<namespace-name>.svc.cluster.local`.
    For example, a Service called `nginx` in the `default` Namespace gets the DNS
    record `nginx.default.svc.cluster.local`. But how can Pods use these DNS records?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable DNS-based service discovery, Kubernetes configures CoreDNS as the
    DNS resolver for Pods. When setting up a Pod’s sandbox, the kubelet writes an
    */etc/resolv.conf* that specifies CoreDNS as the nameserver and injects the config
    file into the container. The */etc/resolv.conf* file of a Pod looks something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Given this configuration, Pods send DNS queries to CoreDNS whenever they try
    to connect to a Service by name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting trick in the resolver configuration is the use of `ndots`
    and `search` to simplify DNS queries. When a Pod wants to reach a Service that
    exists in the same Namespace, it can use the Service’s name as the domain name
    instead of the fully qualified domain name (`nginx.default.svc.cluster.local`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, when a Pod wants to reach a Service in another Namespace, it can
    do so by appending the Namespace name to the Service name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: One thing to consider with the `ndots` configuration is its impact on applications
    that communicate with services outside of the cluster. The `ndots` parameter specifies
    how many dots must appear in a domain name for it to be considered an absolute
    or fully qualified name. When resolving a name that’s not fully qualified, the
    system attempts various lookups using the items in the `search` parameter, as
    seen in the following example. Thus, when applications resolve cluster-external
    names that are not fully qualified, the resolver consults the cluster DNS server
    with multiple futile requests before attempting to resolve the name as an absolute
    name. To avoid this issue, you can use fully qualified domain names in your applications
    by adding a `.` at the end of the name. Alternatively, you can tune the DNS configuration
    of the Pod via the `dnsConfig` field in the Pod’s specification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows the impact of the `ndots` configuration on Pods
    that resolve external names. Notice how resolving a name that has less dots than
    the configured `ndots` results in multiple DNS queries, while resolving an absolute
    name results in a single query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Attempt to resolve a name with less than 5 dots (not fully qualified). The resolver
    performs multiple lookups, one per item in the `search` field of */etc/resolv.conf*.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Attempt to resolve a fully qualified name. The resolver performs a single lookup.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, service discovery over DNS is extremely useful, as it lowers the barrier
    for applications to interact with Kubernetes Services.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Kubernetes API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to discover Services in Kubernetes is by using the Kubernetes API.
    The community maintains various Kubernetes client libraries in different languages,
    including Go, Java, Python, and others. Some application frameworks, such as Spring,
    also support service discovery through the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Kubernetes API for service discovery can be useful in specific scenarios.
    For example, if your applications need to be aware of Service endpoint changes
    as soon as they happen, they would benefit from watching the API.
  prefs: []
  type: TYPE_NORMAL
- en: The main downside of performing service discovery through the Kubernetes API
    is that you tightly couple the application to the underlying platform. Ideally,
    applications should be unaware of the platform. If you do choose to use the Kubernetes
    API for service discovery, consider building an interface that abstracts the Kubernetes
    details away from your business logic.
  prefs: []
  type: TYPE_NORMAL
- en: Using environment variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes injects environment variables into Pods to facilitate service discovery.
    For each Service, Kubernetes sets multiple environment variables according to
    the Service definition. The environment variables for an `nginx` ClusterIP Service
    listening on port 80 look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The downside to this approach is that environment variables cannot be updated
    without restarting the Pod. Thus, Services must be in place before the Pod starts
    up.
  prefs: []
  type: TYPE_NORMAL
- en: DNS Service Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous section, offering DNS-based service discovery to
    workloads on your platform is crucial. As the size of your cluster and number
    of applications grows, the DNS service can become a bottleneck. In this section,
    we will discuss techniques you can use to provide a performant DNS service.
  prefs: []
  type: TYPE_NORMAL
- en: DNS cache on each node
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Kubernetes community maintains a DNS cache add-on called [NodeLocal DNSCache](https://oreil.ly/lQdTH).
    The add-on runs a DNS cache on each node to address multiple problems. First,
    the cache reduces the latency of DNS lookups, given that workloads get their answers
    from the local cache (assuming a cache hit) instead of reaching out to the DNS
    server (potentially on another node). Second, the load on the CoreDNS servers
    goes down, as workloads are leveraging the cache most of the time. Finally, in
    the case of a cache miss, the local DNS cache upgrades the DNS query to TCP when
    reaching out to the central DNS service. Using TCP instead of UDP improves the
    reliability of the DNS query.
  prefs: []
  type: TYPE_NORMAL
- en: The DNS cache runs as a DaemonSet on the cluster. Each replica of the DNS cache
    intercepts the DNS queries that originate from their node. There’s no need to
    change application code or configuration to use the cache. The node-level architecture
    of the NodeLocal DNSCache add-on is depicted in [Figure 6-9](#node_level_architecture_of_the_nodelocal_dnscache_add_on).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0609](assets/prku_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Node-level architecture of the NodeLocal DNSCache add-on. The DNS
    cache intercepts DNS queries and responds immediately if there’s a cache hit.
    In the case of a cache miss, the DNS cache forwards the query to the cluster DNS
    service.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Auto-scaling the DNS server deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to running the node-local DNS cache in your cluster, you can automatically
    scale the DNS Deployment according to the size of the cluster. Note that this
    strategy does not leverage the Horizontal Pod Autoscaler. Instead, it uses the
    [cluster Proportional Autoscaler](https://oreil.ly/432we), which scales workloads
    based on the number of nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Cluster Proportional Autoscaler runs as a Pod in the cluster. It has a configuration
    flag to set the workload that needs autoscaling. To autoscale DNS, you must set
    the target flag to the CoreDNS (or kube-dns) Deployment. Once running, the autoscaler
    polls the API server every 10 seconds (by default) to get the number of nodes
    and CPU cores in the cluster. Then, it adjusts the number of replicas in the CoreDNS
    Deployment if necessary. The desired number of replicas is governed by a configurable
    replicas-to-nodes ratio or replicas-to-cores ratio. The ratios to use depend on
    your workloads and how DNS-intensive they are.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, using node-local DNS cache is sufficient to offer a reliable
    DNS service. However, autoscaling DNS is another strategy you can use when autoscaling
    clusters with a wide-enough range of minimum and maximum nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve discussed in [Chapter 5](ch05.html#chapter5), workloads running in
    Kubernetes are typically not accessible from outside the cluster. This is not
    a problem if your applications do not have external clients. Batch workloads are
    a great example of such applications. Realistically, however, most Kubernetes
    deployments host web services that do have end users.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress is an approach to exposing services running in Kubernetes to clients
    outside of the cluster. Even though Kubernetes does not fulfill the Ingress API
    out of the box, it is a staple in any Kubernetes-based platform. It is not uncommon
    for off-the-shelf Kubernetes applications and cluster add-ons to expect that an
    Ingress controller is running in the cluster. Moreover, your developers will need
    it to be able to run their applications successfully in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: This section aims to guide you through the considerations you must make when
    implementing Ingress in your platform. We will review the Ingress API, the most
    common ingress traffic patterns that you will encounter, and the crucial role
    of Ingress controllers in a Kubernetes-based platform. We will also discuss different
    ways to deploy Ingress controllers and their trade-offs. Finally, we will address
    common challenges you can run into and explore helpful integrations with other
    tools in the ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The Case for Ingress
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes Services already provide ways to route traffic to Pods, so why would
    you need an additional strategy to achieve the same thing? As much as we are fans
    of keeping platforms simple, the reality is that Services have important limitations
    and downsides:'
  prefs: []
  type: TYPE_NORMAL
- en: Limited routing capabilities
  prefs: []
  type: TYPE_NORMAL
- en: Services route traffic according to the destination IP and port of incoming
    requests. This can be useful for small and relatively simple applications, but
    it quickly breaks down for more substantial, microservices-based applications.
    These kinds of applications require smarter routing features and other advanced
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs: []
  type: TYPE_NORMAL
- en: If you are running in a cloud environment, each LoadBalancer Service in your
    cluster creates an external load balancer, such as an ELB in the case of AWS.
    Running a separate load balancer for each Service in your platform can quickly
    become cost-prohibitive.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress addresses both these limitations. Instead of being limited to load balancing
    at layer 3/4 of the OSI model, Ingress provides load balancing and routing capabilities
    at layer 7\. In other words, Ingress operates at the application layer, which
    results in more advanced routing features.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of Ingress is that it removes the need to have multiple load
    balancers or entry points into the platform. Because of the advanced routing capabilities
    available in Ingress, such as the ability to route HTTP requests based on the
    `Host` header, you can route all the service traffic to a single entry point and
    let the Ingress controller take care of demultiplexing the traffic. This dramatically
    reduces the cost of bringing traffic into your platform.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to have a single ingress point into the platform also reduces the
    complexity of noncloud deployments. Instead of potentially having to manage multiple
    external load balancers with a multitude of NodePort Services, you can operate
    a single external load balancer that routes traffic to the Ingress controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Even though Ingress solves most of the downsides related to Kubernetes Services,
    the latter are still needed. Ingress controllers themselves run inside the platform
    and thus need to be exposed to clients that exist outside. And you can use a Service
    (either a NodePort or LoadBalancer) to do so. Besides, most Ingress controllers
    shine when it comes to load balancing HTTP traffic. If you want to be able to
    host applications that use other protocols, you might have to use Services alongside
    Ingress, depending on the capabilities of your Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: The Ingress API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Ingress API enables application teams to expose their services and configure
    request routing according to their needs. Because of Ingress’s focus on HTTP routing,
    the Ingress API resource provides different ways to route traffic according to
    the properties of incoming HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common routing technique is routing traffic according to the `Host` header
    of HTTP requests. For example, given the following Ingress configuration, HTTP
    requests with the `Host` header set to `bookhotels.com` are routed to one service,
    while requests destined to `bookflights.com` are routed to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Hosting applications on specific subdomains of a cluster-wide domain name is
    a common approach we encounter in Kubernetes. In this case, you assign a domain
    name to the platform, and each application gets a subdomain. Keeping with the
    travel theme in the previous example, an example of subdomain-based routing for
    a travel booking application could have `hotels.cluster1.useast.example.com` and
    `flights.cluster1.useast.example.com`. Subdomain-based routing is one of the best
    strategies you can employ. It also enables other interesting use cases, such as
    hosting tenants of a software-as-a-service (SaaS) application on tenant-specific
    domain names (`tenantA.example.com` and `tenantB.example.com`, for example). We
    will further discuss how to implement subdomain-based routing in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: The Ingress API supports features beyond host-based routing. Through the evolution
    of the Kubernetes project, Ingress controllers extended the Ingress API. Unfortunately,
    these extensions were made using annotations instead of evolving the Ingress resource.
    The problem with using annotations is that they don’t have a schema. This can
    result in a poor user experience, as there is no way for the API server to catch
    misconfigurations. To address this issue, some Ingress controllers provide Custom
    Resource Definitions (CRDs). These resources have well-defined APIs offering features
    otherwise not available through Ingress. Contour, for example, provides an `HTTPProxy`
    custom resource. While leveraging these CRDs gives you access to a broader array
    of features, you give up the ability to swap Ingress controllers if necessary.
    In other words, you “lock” yourself into a specific controller.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Controllers and How They Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you can recall the first time you played with Kubernetes, you probably ran
    into a puzzling scenario with Ingress. You downloaded a bunch of sample YAML files
    that included a Deployment and an Ingress and applied them to your cluster. You
    noticed that the Pod came up just fine, but you were not able to reach it. The
    Ingress resource was essentially doing nothing. You probably wondered, What’s
    going on here?
  prefs: []
  type: TYPE_NORMAL
- en: Ingress is one of those APIs in Kubernetes that are left to the platform builder
    to implement. In other words, Kubernetes exposes the Ingress interface and expects
    another component to provide the implementation. This component is commonly called
    an *Ingress* *controller*.
  prefs: []
  type: TYPE_NORMAL
- en: An Ingress controller is a platform component that runs in the cluster. The
    controller is responsible for watching the Ingress API and acting according to
    the configuration defined in Ingress resources. In most implementations, the Ingress
    controller is paired with a reverse proxy, such as NGINX or Envoy. This two-component
    architecture is comparable to other software-defined networking systems, in that
    the controller is the *control plane* of the Ingress controller, while the proxy
    is the *data plane* component. [Figure 6-10](#the_ingress_controller_watches_various_resources_in_the_api)
    shows the control plane and data plane of an Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0610](assets/prku_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The Ingress controller watches various resources in the API server
    and configures the proxy accordingly. The proxy handles incoming traffic and forwards
    it to Pods, according to the Ingress configuration.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The control plane of the Ingress controller connects to the Kubernetes API and
    watches a variety of resources, such as Ingress, Services, Endpoints, and others.
    Whenever these resources change, the controller receives a watch notification
    and configures the data plane to act according to the desired state declared in
    the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: The data plane handles the routing and load balancing of network traffic. As
    mentioned before, the data plane is usually implemented with an off-the-shelf
    proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Because the Ingress API builds on top of the Service abstraction, Ingress controllers
    have a choice between forwarding traffic through Services or sending it directly
    to Pods. Most Ingress controllers opt for the latter. They don’t use the Service
    resource, other than to validate that the Service referenced in the Ingress resource
    exists. When it comes to routing, most controllers forward traffic to the Pod
    IP addresses listed in the corresponding Endpoints object. Routing traffic directly
    to the Pod bypasses the Service layer, which reduces latency and adds different
    load balancing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Traffic Patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A great aspect of Ingress is that each application gets to configure routing
    according to its needs. Typically, each application has different requirements
    when it comes to handling incoming traffic. Some might require TLS termination
    at the edge. Some might want to handle TLS themselves, while others might not
    support TLS at all (hopefully, this is not the case).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the common ingress traffic patterns that we
    have encountered. This should give you an idea of what Ingress can provide to
    your developers and how Ingress can fit into your platform offering.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP proxying
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HTTP proxying is the bread-and-butter of Ingress. This pattern involves exposing
    one or more HTTP-based services and routing traffic according to the HTTP requests’
    properties. We have already discussed routing based on the `Host` header. Other
    properties that can influence routing decisions include the URL path, the request
    method, request headers, and more, depending on the Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: The following Ingress resource exposes the `app1` Service at `app1.example.com`.
    Any incoming request that has a matching `Host` HTTP header is sent to an `app1`
    Pod.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Once applied, the preceding configuration results in the data plane flow depicted
    in [Figure 6-11](#path_of_an_http_request_from_the_client_to_the_target_pod).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0611](assets/prku_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. Path of an HTTP request from the client to the target Pod through
    the Ingress controller.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: HTTP proxying with TLS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supporting TLS encryption is table-stakes for Ingress controllers. This ingress
    traffic pattern is the same as HTTP proxying from a routing perspective. However,
    clients communicate with the Ingress controller over a secure TLS connection instead
    of plain-text HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows an Ingress resource that exposes `app1` with TLS.
    The controller gets the TLS serving certificate and key from the referenced Kubernetes
    Secret.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Ingress controllers support different configurations when it comes to the connection
    between the Ingress controller and the backend service. The connection between
    the external client and the controller is secure (TLS), while the connection between
    the Ingress controller and the backend application does not have to be. Whether
    the connection between the controller and the backend is secure depends on whether
    the application is listening for TLS connections. By default, most Ingress controllers
    terminate TLS and forward requests over an unencrypted connection, as depicted
    in [Figure 6-12](#Ingress_controller_handling_an_HTTPS).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0612](assets/prku_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. Ingress controller handling an HTTPS request by terminating TLS
    and forwarding the request to the backend Pod over an unencrypted connection.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the case where a secure connection to the backend is required, the Ingress
    controller terminates the TLS connection at the edge and establishes a new TLS
    connection with the backend (illustrated in [Figure 6-13](#ingress_controller_terminating_tls_and_establishing_a_new_tls)).
    The reestablishment of the TLS connection is sometimes not appropriate for certain
    applications, such as those that need to perform the TLS handshake with their
    clients. In these situations, TLS passthrough, which we will discuss further later,
    is a viable alternative.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0613](assets/prku_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. Ingress controller terminating TLS and establishing a new TLS
    connection with the backend Pod when handling HTTPS requests.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Layer 3/4 proxying
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though the Ingress API’s primary focus is layer 7 proxying (HTTP traffic),
    some Ingress controllers can proxy traffic at layer 3/4 (TCP/UDP traffic). This
    can be useful if you need to expose applications that do not speak HTTP. When
    evaluating Ingress controllers, you must keep this in mind, as support for layer
    3/4 proxying varies across controllers.
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge with proxying TCP or UDP services is that Ingress controllers
    listen on a limited number of ports, usually 80 and 443\. As you can imagine,
    exposing different TCP or UDP services on the same port is impossible without
    a strategy to distinguish the traffic. Ingress controllers solve this problem
    in different ways. Some, such as Contour, support proxying of only TLS encrypted
    TCP connections that use the Server Name Indication (SNI) TLS extension. The reason
    for this is that Contour needs to know where the traffic is headed. And when using
    SNI, the target domain name is available (unencrypted) in the ClientHello message
    of the TLS handshake. Because TLS and SNI are dependent on TCP, Contour does not
    support UDP proxying.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample HTTPProxy Custom Resource, which is supported by
    Contour. Layer 3/4 proxying is one of those cases where a Custom Resource provides
    a better experience than the Ingress API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding configuration, Contour reads the server name in the SNI extension
    and proxies the traffic to the backend TCP service. [Figure 6-14](#the_ingress_controller_inspects_the_sni_header_to_determine_the_backend_terminates_the_tls_connection)
    illustrates this capability.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0614](assets/prku_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-14\. The Ingress controller inspects the SNI header to determine the
    backend, terminates the TLS connection, and forwards the TCP traffic to the Pod.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other Ingress controllers expose configuration parameters that you can use to
    tell the underlying proxy to bind additional ports for layer 3/4 proxying. You
    then map these additional ports to specific services running in the cluster. This
    is the approach that the community-led NGINX Ingress controller takes for layer
    3/4 proxying.
  prefs: []
  type: TYPE_NORMAL
- en: A common use case of layer 3/4 proxying is TLS passthrough. TLS passthrough
    involves an application that exposes a TLS endpoint and the need to handle the
    TLS handshake directly with the client. As we discussed in the “HTTP proxying
    with TLS” pattern, the Ingress controller usually terminates the client-facing
    TLS connection. The TLS termination is necessary so that the Ingress controller
    can inspect the HTTP request, which would otherwise be encrypted. However, with
    TLS passthrough, the Ingress controller does not terminate TLS and instead proxies
    the secure connection to a backend Pod. [Figure 6-15](#when_tls_passthrough_is_enabled_the_ingress_controller_inspects)
    depicts TLS passthrough.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0615](assets/prku_0615.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-15\. When TLS passthrough is enabled, the Ingress controller inspects
    the SNI header to determine the backend and forwards the TLS connection accordingly.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Choosing an Ingress Controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several Ingress controllers that you can choose from. In our experience,
    the NGINX Ingress controller is one of the most commonly used. However, that does
    not mean it is best for your application platform. Other choices include Contour,
    HAProxy, Traefik, and more. In keeping with this book’s theme, our goal is not
    to tell you which to use. Instead, we aim to equip you with the information you
    need to make this decision. We will also highlight significant trade-offs where
    applicable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stepping back a bit, the primary goal of an Ingress controller is to handle
    application traffic. Thus, it is natural to turn to the applications as the primary
    factor when selecting an Ingress controller. Specifically, what are the features
    and requirements that your applications need? The following is a list of criteria
    that you can use to evaluate Ingress controllers from an application support perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: Do applications expose HTTPS endpoints? Do they need to handle the TLS handshake
    with the client directly, or is it okay to terminate TLS at the edge?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What SSL ciphers do the applications use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do applications need session affinity or sticky sessions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do applications need advanced request routing capabilities, such as HTTP header-based
    routing, cookie-based routing, HTTP method-based routing, and others?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do applications have different load balancing algorithm requirements, such as
    round-robin, weighted least request, or random?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do applications need support for Cross-Origin Resource Sharing (CORS)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do applications offload authentication concerns to an external system? Some
    Ingress controllers provide authentication features that you can leverage to provide
    a common authentication mechanism across applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any applications that need to expose TCP or UDP endpoints?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the application need the ability to rate-limit incoming traffic?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to application requirements, another crucial consideration to make
    is your organization’s experience with the data plane technology. If you are already
    intimately familiar with a specific proxy, it is usually a safe bet to start there.
    You will already have a good understanding of how it works, and more importantly,
    you will know its limitations and how to troubleshoot it.
  prefs: []
  type: TYPE_NORMAL
- en: Supportability is another critical factor to consider. Ingress is an essential
    component of your platform. It exists right in the middle of your customers and
    the services they are trying to reach. When things go wrong with your Ingress
    controller, you want to have access to the support you need when facing an outage.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, remember that you can run multiple Ingress controllers in your platform
    using Ingress classes. Doing so increases the complexity and management of your
    platform, but it is necessary in some cases. The higher the adoption of your platform
    and the more production workloads you are running, the more features they will
    demand from your Ingress tier. It is entirely possible that you will end up having
    a set of requirements that cannot be fulfilled with a single Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Controller Deployment Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regardless of the Ingress controller, there is a set of considerations that
    you should keep in mind when it comes to deploying and operating the Ingress tier.
    Some of these considerations can also have an impact on the applications running
    on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated Ingress nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dedicating (or reserving) a set of nodes to run the Ingress controller and thus
    serve as the cluster’s “edge” is a pattern that we have found very successful.
    [Figure 6-16](#dedicated_ingress_nodes_are_reserved_for_the_ingress_controller)
    illustrates this deployment pattern. At first, it might seem wasteful to use dedicated
    ingress nodes. However, our philosophy is, if you can afford to run dedicated
    control plane nodes, you can probably afford to dedicate nodes to the layer that
    is in the critical path for all workloads on the cluster. Using a dedicated node
    pool for Ingress brings considerable benefits.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0616](assets/prku_0616.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-16\. Dedicated ingress nodes are reserved for the Ingress controller.
    The ingress nodes serve as the “edge” of the cluster or the Ingress tier.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The primary benefit is resource isolation. Even though Kubernetes has support
    for configuring resource requests and limits, we have found that platform teams
    can struggle with getting those parameters right. This is especially true when
    the platform team is at the beginning of their Kubernetes journey and is unaware
    of the implementation details that underpin resource management (e.g., the Completely
    Fair Scheduler, cgroups). Furthermore, at the time of writing, Kubernetes does
    not support resource isolation for network I/O or file descriptors, making it
    challenging to guarantee the fair sharing of these resources.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason for running Ingress controllers on dedicated nodes is compliance.
    We have encountered that a large number of organizations have pre-established
    firewall rules and other security practices that can be incompatible with Ingress
    controllers. Dedicated ingress nodes are useful in these environments, as it is
    typically easier to get exceptions for a subset of cluster nodes instead of all
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, limiting the number of nodes that run the Ingress controller can be
    helpful in bare-metal or on-premises installations. In such deployments, the Ingress
    tier is fronted by a hardware load balancer. In most cases, these are traditional
    load balancers that lack APIs and must be statically configured to route traffic
    to a specific set of backends. Having a small number of ingress nodes eases the
    configuration and management of these external load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, dedicating nodes to Ingress can help with performance, compliance,
    and managing external load balancers. The best approach to implement dedicated
    ingress nodes is to label and taint the ingress nodes. Then, deploy the Ingress
    controller as a DaemonSet that (1) tolerates the taint, and (2) has a node selector
    that targets the ingress nodes. With this approach, ingress node failures must
    be accounted for, as Ingress controllers will not run on nodes other than those
    reserved for Ingress. In the ideal case, failed nodes are automatically replaced
    with new nodes that can continue handling Ingress traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Binding to the host network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To optimize the ingress traffic path, you can bind your Ingress controller
    to the underlying host’s network. By doing so, incoming requests bypass the Kubernetes
    Service fabric and reach the Ingress controller directly. When enabling host networking,
    ensure that the Ingress controller’s DNS policy is set to `ClusterFirstWithHostNet`.
    The following snippet shows the host networking and DNS policy settings in a Pod
    template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: While running the Ingress controller directly on the host network can increase
    performance, you must keep in mind that doing so removes the network namespace
    boundary between the Ingress controller and the node. In other words, the Ingress
    controller has full access to all network interfaces and network services available
    on the host. This has implications on the Ingress controller’s threat model. Namely,
    it lowers the bar for an adversary to perform lateral movement in the case of
    a data plane proxy vulnerability. Additionally, attaching to the host network
    is a privileged operation. Thus, the Ingress controller needs elevated privileges
    or exceptions to run as a privileged workload.
  prefs: []
  type: TYPE_NORMAL
- en: Even then, we’ve found that binding to the host network is worth the trade-off
    and is usually the best way to expose the platform’s Ingress controllers. The
    ingress traffic arrives directly at the controller’s gate, instead of traversing
    the Service stack (which can be suboptimal, as discussed in [“Kubernetes Services”](#kubernetes_services)).
  prefs: []
  type: TYPE_NORMAL
- en: Ingress controllers and external traffic policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unless configured properly, using a Kubernetes Service to expose the Ingress
    controller impacts the performance of the Ingress data plane.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall from [“Kubernetes Services”](#kubernetes_services), a Service’s
    external traffic policy determines how to handle traffic that’s coming from outside
    the cluster. If you are using a NodePort or LoadBalancer Service to expose the
    Ingress controller, ensure that you set the external traffic policy to `Local`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `Local` policy avoids unnecessary network hops, as the external traffic
    reaches the local Ingress controller instead of hopping to another node. Furthermore,
    the `Local` policy doesn’t use SNAT, which means the client IP address is visible
    to applications handling the requests.
  prefs: []
  type: TYPE_NORMAL
- en: Spread Ingress controllers across failure domains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ensure the high-availability of your Ingress controller fleet, use Pod anti-affinity
    rules to spread the Ingress controllers across different failure domains.
  prefs: []
  type: TYPE_NORMAL
- en: DNS and Its Role in Ingress
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have discussed in this chapter, applications running on the platform share
    the ingress data plane, and thus share that single entry point into the platform’s
    network. As requests come in, the Ingress controller’s primary responsibility
    is to disambiguate traffic and route it according to the Ingress configuration.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary ways to determine the destination of a request is by the
    target hostname (the `Host` header in the case of HTTP or SNI in the case of TCP),
    turning DNS into an essential player of your Ingress implementation. We will discuss
    two of the main approaches that are available when it comes to DNS and Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Wildcard DNS record
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most successful patterns we continuously use is to assign a domain
    name to the environment and slice it up by assigning subdomains to different applications.
    We sometimes call this “subdomain-based routing.” The implementation of this pattern
    involves creating a wildcard DNS record (e.g., `*.bearcanoe.com`) that resolves
    to the Ingress tier of the cluster. Typically, this is a load balancer that is
    in front of the Ingress controllers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several benefits to using a wildcard DNS record for your Ingress
    controllers:'
  prefs: []
  type: TYPE_NORMAL
- en: Applications can use any path under their subdomain, including the root path
    (`/`). Developers don’t have to spend engineering hours to make their apps work
    on subpaths. In some cases, applications expect to be hosted at the root path
    and do not work otherwise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DNS implementation is relatively straightforward. There is no integration
    necessary between Kubernetes and your DNS provider.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The single wildcard DNS record removes DNS propagation concerns that could arise
    when using different domain names for each application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes and DNS integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative to using a wildcard DNS record is to integrate your platform
    with your DNS provider. The Kubernetes community maintains a controller that offers
    this integration called [external-dns](https://github.com/kubernetes-sigs/external-dns).
    If you are using a DNS provider that is supported, consider using this controller
    to automate the creation of domain names.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might expect from a Kubernetes controller, external-dns continuously
    reconciles the DNS records in your upstream DNS provider and the configuration
    defined in Ingress resources. In other words, external-dns creates, updates, and
    deletes DNS records according to changes that happen in the Ingress API. External-dns
    needs two pieces of information to configure the DNS records, both of which are
    part of the Ingress resource: the desired hostname, which is in the Ingress specification,
    and the target IP address, which is available in the status field of the Ingress
    resource.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrating the platform with your DNS provider can be useful if you need to
    support multiple domain names. The controller takes care of automatically creating
    DNS records as needed. However, it is important to keep the following trade-offs
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: You have to deploy an additional component (external-dns) into your cluster.
    An additional add-on brings about more complexity into your deployments, given
    that you have to operate, maintain, monitor, version, and upgrade one more component
    in your platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If external-dns does not support your DNS provider, you have to develop your
    own controller. Building and maintaining a controller requires engineering effort
    that could be spent on higher-value efforts. In these situations, it is best to
    simply implement a wildcard DNS record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling TLS Certificates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ingress controllers need certificates and their corresponding private keys to
    serve applications over TLS. Depending on your Ingress strategy, managing certificates
    can be cumbersome. If your cluster hosts a single domain name and implements subdomain-based
    routing, you can use a single wildcard TLS certificate. In some cases, however,
    clusters host applications across a variety of domains, making it challenging
    to manage certificates efficiently. Furthermore, your security team might frown
    upon the usage of wildcard certificates. In any case, the Kubernetes community
    has rallied around a certificate management add-on that eases the minting and
    management of certificates. The add-on is aptly called [cert-manager](https://cert-manager.io).
  prefs: []
  type: TYPE_NORMAL
- en: Cert-manager is a controller that runs in your cluster. It installs a set of
    CRDs that enable declarative management of Certificate Authorities (CAs) and Certificates
    via the Kubernetes API. More importantly, it supports different certificate issuers,
    including ACME-based CAs, HashiCorp Vault, Venafi, etc. It also offers an extension
    point to implement custom issuers, when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The certificate minting features of cert-manager revolve around issuers and
    certificates. Cert-manager has two issuer Custom Resources. The Issuer resource
    represents a CA that signs certificates in a specific Kubernetes Namespace. If
    you want to issue certificates across all Namespaces, you can use the ClusterIssuer
    resource. The following is a sample ClusterIssuer definition that uses a private
    key stored in a Kubernetes Secret named `platform-ca-key-pair`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The great thing about cert-manager is that it integrates with the Ingress API
    to automatically mint certificates for Ingress resources. For example, given the
    following Ingress object, cert-manager automatically creates a certificate key
    pair suitable for TLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `cert-manager.io/cluster-issuer` annotation tells cert-manager to use the
    `prod-ca-issuer` to mint the certificate.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Cert-manager stores the certificate and private key in a Kubernetes Secret called
    `bearcanoe-cert-key-pair`.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, cert-manager handles the certificate request process, which
    includes generating a private key, creating a certificate signing request (CSR),
    and submitting the CSR to the CA. Once the issuer mints the certificate, cert-manager
    stores it in the `bearcanoe-cert-key-pair` certificate. The Ingress controller
    can then pick it up and start serving the application over TLS. [Figure 6-17](#cert_manager_watches_the_ingress_api_and_requests_a_certificate)
    depicts the process in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0617](assets/prku_0617.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-17\. Cert-manager watches the Ingress API and requests a certificate
    from a Certificate Authority when the Ingress resource has the `cert-manager.io/cluster-issuer`
    annotation.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, cert-manager simplifies certificate management on Kubernetes.
    Most platforms we’ve encountered use cert-manager in some capacity. If you leverage
    cert-manager in your platform, consider using an external system such as Vault
    as the CA. Integrating cert-manager with an external system instead of using a
    CA backed by a Kubernetes Secret is a more robust and secure solution.
  prefs: []
  type: TYPE_NORMAL
- en: Service Mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the industry continues to adopt containers and microservices, service meshes
    have gained immense popularity. While the term “service mesh” is relatively new,
    the concepts that it encompasses are not. Service meshes are a rehash of preexisting
    ideas in service routing, load balancing, and telemetry. Before the rise of containers
    and Kubernetes, hyperscale internet companies implemented service mesh precursors
    as they ran into challenges with microservices. Twitter, for example, created
    [Finagle](https://twitter.github.io/finagle), a Scala library that all its microservices
    embedded. It handled load balancing, circuit breaking, automatic retries, telemetry,
    and more. Netflix developed [Hystrix](https://github.com/Netflix/Hystrix), a similar
    library for Java applications.
  prefs: []
  type: TYPE_NORMAL
- en: Containers and Kubernetes changed the landscape. Service meshes are no longer
    language-specific libraries like their precursors. Today, service meshes are distributed
    systems themselves. They consist of a control plane that configures a collection
    of proxies that implement the data plane. The routing, load balancing, telemetry,
    and other capabilities are built into the proxy instead of the application. The
    move to the proxy model has enabled even more apps to take advantage of these
    features, as there’s no need to make code changes to participate in a mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'Service meshes provide a broad set of features that can be categorized across
    three pillars:'
  prefs: []
  type: TYPE_NORMAL
- en: Routing and reliability
  prefs: []
  type: TYPE_NORMAL
- en: Advanced traffic routing and reliability features such as traffic shifting,
    traffic mirroring, retries, and circuit breaking.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs: []
  type: TYPE_NORMAL
- en: Identity and access control features that enable secure communication between
    services, including identity, certificate management, and mutual TLS (mTLS).
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs: []
  type: TYPE_NORMAL
- en: Automated gathering of metrics and traces from all the interactions happening
    in the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the rest of this chapter, we are going to discuss service mesh in
    more detail. Before we do so, however, let’s return to this book’s central theme
    and ask “Do we need a service mesh?” Service meshes have risen in popularity as
    some organizations see them as a golden bullet to implement the aforementioned
    features. However, we have found that organizations should carefully consider
    the impact of adopting a service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: When (Not) to Use a Service Mesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A service mesh can provide immense value to an application platform and the
    applications that run atop. It offers an attractive feature set that your developers
    will appreciate. At the same time, a service mesh brings a ton of complexity that
    you must deal with.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is a complex distributed system. Up to this point in the book, we
    have touched on some of the building blocks you need to create an application
    platform atop Kubernetes, and there are still a bunch of chapters left. The reality
    is that building a successful Kubernetes-based application platform is a lot of
    work. Keep this in mind when you are considering a service mesh. Tackling a service
    mesh implementation while you are beginning your Kubernetes journey will slow
    you down, if not take you down the path to failure.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen these cases firsthand while working in the field. We have worked
    with platform teams who were blinded by the shiny features of a service mesh.
    Granted, those features would make their platform more attractive to developers
    and thus increase the platform’s adoption. However, timing is important. Wait
    until you gain operational experience in production before thinking about service
    mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps more critical is for you to understand your requirements or the problems
    you are trying to solve. Putting the cart before the horse will not only increase
    the chances of your platform failing but also result in wasted engineering effort.
    A fitting example of this mistake is an organization that dove into service mesh
    while developing a Kubernetes-based platform that was not yet in production. “We
    want a service mesh because we need everything it provides,” they said. Twelve
    months later, the only feature they were using was the mesh’s Ingress capabilities.
    No mutual TLS, no fancy routing, no tracing. Just Ingress. The engineering effort
    to get a dedicated Ingress controller ready for production is far less than a
    full-featured mesh implementation. There’s something to be said for getting a
    minimum viable product into production and then iterating to add features moving
    forward.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this, you might feel like we think there’s no place for a service
    mesh in an application platform. Quite the opposite. A service mesh can solve
    a ton of problems if you have them, and it can bring a ton of value if you take
    advantage of it. In the end, we have found that a successful service mesh implementation
    boils down to timing it right and doing it for the right reasons.
  prefs: []
  type: TYPE_NORMAL
- en: The Service Mesh Interface (SMI)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes provides interfaces for a variety of pluggable components. These
    interfaces include the Container Runtime Interface (CRI), the Container Networking
    Interface (CNI), and others. As we’ve seen throughout the book, these interfaces
    are what makes Kubernetes such an extensible foundation. Service mesh is slowly
    but surely becoming an important ingredient of a Kubernetes platform. Thus, the
    service mesh community collaborated to build the Service Mesh Interface, or SMI.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the other interfaces we’ve already discussed, the SMI specifies the
    interaction between Kubernetes and a service mesh. With that said, the SMI is
    different than other Kubernetes interfaces in that it is not part of the core
    Kubernetes project. Instead, the SMI project leverages CRDs to specify the interface.
    The SMI project also houses libraries to implement the interface, such as the
    SMI SDK for Go.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SMI covers the three pillars we discussed in the previous section with
    a set of CRDs. The Traffic Split API is concerned with routing and splitting traffic
    across a number of services. It enables percent-based traffic splitting, which
    enables different deployment scenarios such as blue-green deployments and A/B
    testing. The following snippet is an example of a TrafficSplit that performs a
    canary deployment of the “flights” web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level Service that clients connect to (i.e., `flights.bookings.cluster.svc.local`).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The backend Services that receive the traffic. The v1 version receives 70% of
    traffic and the v2 version receives the rest.
  prefs: []
  type: TYPE_NORMAL
- en: The Traffic Access Control and Traffic Specs APIs work together to implement
    security features such as access control. The Traffic Access Control API provides
    CRDs to control the service interactions that are allowed in the mesh. With these
    CRDs, developers can specify access control policy that determines which services
    can talk to each other and under what conditions (list of allowed HTTP methods,
    for example). The Traffic Specs API offers a way to describe traffic, including
    an `HTTPRouteGroup` CRD for HTTP traffic and a `TCPRoute` for TCP traffic. Together
    with the Traffic Access Control CRDs, these apply policy at the application level.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following HTTPRouteGroup and TrafficTarget allow all requests
    from the bookings service to the payments service. The HTTPRouteGroup resource
    describes the traffic, while the TrafficTarget specifies the source and destination
    services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Allow all requests in this HTTPRouteGroup.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The destination service. In this case, the Pods using the `payments` Service
    Account in the `payments` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_service_routing_CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The HTTPRouteGroups that control the traffic between the source and destination
    services.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_service_routing_CO9-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The source service. In this case, the Pods using the `flights` Service Account
    in the `bookings` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the Traffic Metrics API provides the telemetry functionality of a
    service mesh. This API is somewhat different than the rest in that it defines
    outputs instead of mechanisms to provide inputs. The Traffic Metrics API defines
    a standard to expose service metrics. Systems that need these metrics, such as
    monitoring systems, autoscalers, dashboards, and others, can consume them in a
    standardized fashion. The following snippet shows an example TrafficMetrics resource
    that exposes metrics for traffic between two Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The SMI is one of the newest interfaces in the Kubernetes community. While still
    under development and iteration, it paints the picture of where we are headed
    as a community. As with other interfaces in Kubernetes, the SMI enables platform
    builders to offer a service mesh using portable and provider-agnostic APIs, further
    increasing the value, flexibility, and power of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The Data Plane Proxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data plane of a service mesh is a collection of proxies that connect services
    together. The [Envoy proxy](https://www.envoyproxy.io) is one of the most popular
    service proxies in the cloud native ecosystem. Originally developed at Lyft, it
    has quickly become a prevalent building block in cloud native systems since it
    was open sourced in [late 2016](https://oreil.ly/u5fCD).
  prefs: []
  type: TYPE_NORMAL
- en: Envoy is used in Ingress controllers ([Contour](https://projectcontour.io)),
    API gateways ([Ambassador](https://www.getambassador.io), [Gloo](https://docs.solo.io/gloo/latest)),
    and, you guessed it, service meshes ([Istio](https://istio.io), [OSM](https://github.com/openservicemesh/osm)).
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons why Envoy is such a good building block is its support for
    dynamic configuration over a gRPC/REST API. Open source proxies that predate Envoy
    were not designed for environments as dynamic as Kubernetes. They used static
    configuration files and required restarts for configuration changes to take effect.
    Envoy, on the other hand, offers the xDS (* discovery service) APIs for dynamic
    configuration (depicted in [Figure 6-18](#envoy_supports_dynamic_configuration_via_the_xds_api)).
    It also supports hot restarts, which allow Envoy to reinitialize without dropping
    any active connections.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0618](assets/prku_0618.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-18\. Envoy supports dynamic configuration via the XDS APIs. Envoy connects
    to a configuration server and requests its configuration using LDS, RDS, EDS,
    CDS, and other xDS APIs.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Envoy’s xDS is a collection of APIs that includes the Listener Discovery Service
    (LDS), the Cluster Discovery Service (CDS), the Endpoints Discovery Service (EDS),
    the Route Discovery Service (RDS), and more. An Envoy *configuration server* implements
    these APIs and behaves as the source of dynamic configuration for Envoy. During
    startup, Envoy reaches out to a configuration server (typically over gRPC) and
    subscribes to configuration changes. As things change in the environment, the
    configuration server streams changes to Envoy. Let’s review the xDS APIs in more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: The LDS API configures Envoy’s *Listeners*. Listeners are the entry point into
    the proxy. Envoy can open multiple Listeners that clients can connect to. A typical
    example is listening on ports 80 and 443 for HTTP and HTTPS traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Each Listener has a set of filter chains that determine how to handle incoming
    traffic. The HTTP connection manager filter leverages the RDS API to obtain routing
    configuration. The routing configuration tells Envoy how to route incoming HTTP
    requests. It provides details around virtual hosts and request matching (path-based,
    header-based, and others).
  prefs: []
  type: TYPE_NORMAL
- en: Each route in the routing configuration references a *Cluster*. A cluster is
    a collection of *Endpoints* that belong to the same service. Envoy discovers Clusters
    and Endpoints using the CDS and EDS APIs, respectively. Interestingly enough,
    the EDS API does not have an Endpoint object per se. Instead, Endpoints are assigned
    to clusters using *ClusterLoadAssignment* objects.
  prefs: []
  type: TYPE_NORMAL
- en: While digging into the details of the xDS APIs merits its own book, we hope
    the preceding overview gives you an idea of how Envoy works and its capabilities.
    To summarize, listeners bind to ports and accept connections from clients. Listeners
    have filter chains that determine what to do with incoming connections. For example,
    the HTTP filter inspects requests and maps them to clusters. Each cluster has
    one or more endpoints that end up receiving and handling the traffic. [Figure 6-19](#envoy_configuration_with_a_listener_that_binds_to_port_80)
    shows a graphical representation of these concepts and how they relate to each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0619](assets/prku_0619.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-19\. Envoy configuration with a Listener that binds to port 80\. The
    Listener has an HTTP connection manager filter that references a routing configuration.
    The routing config matches requests with `/` prefix and forwards requests to the
    `my_service` cluster, which has three endpoints.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Service Mesh on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we discussed how the data plane of a service mesh provides
    connectivity between services. We also talked about Envoy as a data plane proxy
    and how it supports dynamic configuration through the xDS APIs. To build a service
    mesh on Kubernetes, we need a control plane that configures the mesh’s data plane
    according to what’s happening inside the cluster. The control plane needs to understand
    Services, Endpoints, Pods, etc. Furthermore, it needs to expose Kubernetes Custom
    Resources that developers can use to configure the service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular service mesh implementations for Kubernetes is Istio.
    Istio implements a control plane for an Envoy-based service mesh. The control
    plane is implemented in a component called istiod, which itself has three primary
    sub-components: Pilot, Citadel, and Galley. Pilot is an Envoy configuration server.
    It implements the xDS APIs and streams the configuration to the Envoy proxies
    running alongside the applications. Citadel is responsible for certificate management
    inside the mesh. It mints certificates that are used to establish service identity
    and mutual TLS. Finally, Galley interacts with external systems such as Kubernetes
    to obtain configuration. It abstracts the underlying platform and translates configuration
    for the other istiod components. [Figure 6-20](#istio_control_plane_interactions)
    shows the interactions between the Istio control plane components.'
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0620](assets/prku_0620.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-20\. Istio control plane interactions.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Istio provides other capabilities besides configuring the data plane of the
    service mesh. First, Istio includes a mutating admission webhook that injects
    an Envoy sidecar into Pods. Every Pod that participates in the mesh has an Envoy
    sidecar that handles all the incoming and outgoing connections. The mutating webhook
    improves the developer experience on the platform, given that developers don’t
    have to manually add the sidecar proxy to all of their application deployment
    manifests. The platform injects the sidecar automatically with both an opt-in
    and opt-out model. With that said, merely injecting the Envoy proxy sidecar alongside
    the workload does not mean the workload will automatically start sending traffic
    through Envoy. Thus, Istio uses an init-container to install iptables rules that
    intercept the Pod’s network traffic and routes it to Envoy. The following snippet
    (trimmed for brevity) shows the Istio init-container configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_routing_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Istio installs an iptables rule that captures all outbound traffic and sends
    it to Envoy at this port.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_routing_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Istio installs an iptables rule that captures all inbound traffic and sends
    it to Envoy at this port.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_service_routing_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: List of CIDRs to redirect to Envoy. In this case, we are redirecting all CIDRs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_service_routing_CO10-4)'
  prefs: []
  type: TYPE_NORMAL
- en: List of ports to redirect to Envoy. In this case, we are redirecting all ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve discussed Istio’s architecture, let’s discuss some of the service
    mesh features that are typically used. One of the more common requirements we
    run into in the field is service authentication and encryption of service-to-service
    traffic. This feature is covered by the Traffic Access Control APIs in the SMI.
    Istio and most service mesh implementations use mutual TLS to achieve this. In
    Istio’s case, mutual TLS is enabled by default for all services that are participating
    in the mesh. The workload sends unencrypted traffic to the sidecar proxy. The
    sidecar proxy upgrades the connection to mTLS and sends it along to the sidecar
    proxy on the other end. By default, the services can still receive non-TLS traffic
    from other services outside of the mesh. If you want to enforce mTLS for all interactions,
    Istio supports a `STRICT` mode that configures all services in the mesh to accept
    only TLS-encrypted requests. For example, you can enforce strict mTLS at the cluster
    level with the following configuration in the `istio-system` Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Traffic management is another key concern handled by a service mesh. Traffic
    management is captured in the Traffic Split API of the SMI, even though Istio’s
    traffic management features are more advanced. In addition to traffic splitting
    or shifting, Istio supports fault injection, circuit breaking, mirroring, and
    more. When it comes to traffic shifting, Istio uses two separate Custom Resources
    for configuration: VirtualService and DestinationRule.'
  prefs: []
  type: TYPE_NORMAL
- en: The *VirtualService* resource creates services in the mesh and specifies how
    traffic is routed to them. It specifies the hostname of the service and rules
    that control the destination of the requests. For example, the VirtualService
    can send 90% of traffic to one destination and send the rest to another. Once
    the VirtualService evaluates the rules and chooses a destination, it sends the
    traffic along to a specific subset of a *DestinationRule*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *DestinationRule* resource lists the “real” backends that are available
    for a given Service. Each backend is captured in a separate subset. Each subset
    can have its own routing configuration, such as load balancing policy, mutual
    TLS mode, and others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, let’s consider a scenario where we want to slowly roll out version
    2 of a service. We can use the following DestinationRule and VirtualService to
    achieve this. The DestinationRule creates two service subsets: v1 and v2\. The
    VirtualService references these subsets. It sends 90% of traffic to the v1 subset
    and 10% of the traffic to the v2 subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Service observability is another feature that is commonly sought after. Because
    there’s a proxy between all services in the mesh, deriving service-level metrics
    is straightforward. Developers get these metrics without having to instrument
    their applications. The metrics are exposed in the Prometheus format, which makes
    them available to a wide range of monitoring systems. The following is an example
    metric captured by the sidecar proxy (some labels removed for brevity). The metric
    shows that there have been 7183 successful requests from the flights booking service
    to the payment processing service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Overall, Istio offers all of the features that are captured in the SMI. However,
    it does not yet implement the SMI APIs (Istio v1.6). The SMI community maintains
    an [adapter](https://github.com/servicemeshinterface/smi-adapter-istio) that you
    can use to make the SMI APIs work with Istio. We discussed Istio mainly because
    it is the service mesh that we’ve most commonly encountered in the field. With
    that said, there are other meshes available in the Kubernetes ecosystem, including
    Linkerd, Consul Connect, Maesh, and more. One of the things that varies across
    these implementations is the data plane architecture, which we’ll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Data Plane Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A service mesh is a highway that services can use to communicate with each
    other. To get onto this highway, services use a proxy that serves as the on-ramp.
    Service meshes follow one of two architecture models when it comes to the data
    plane: the sidecar proxy or the node proxy.'
  prefs: []
  type: TYPE_NORMAL
- en: Sidecar proxy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sidecar proxy is the most common architecture model among the two. As we
    discussed in the previous section, Istio follows this model to implement its data
    plane with Envoy proxies. Linkerd uses this approach as well. In essence, service
    meshes that follow this pattern deploy the proxy inside the workload’s Pod, running
    alongside the service. Once deployed, the sidecar proxy intercepts all the communications
    into and out of the service, as depicted in [Figure 6-21](#pods_participating_in_the_mesh).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0621](assets/prku_0621.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-21\. Pods participating in the mesh have a sidecar proxy that intercepts
    the Pod’s network traffic.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When compared to the node proxy approach, the sidecar proxy architecture can
    have greater impact on services when it comes to data plane upgrades. The upgrade
    involves rolling all the service Pods, as there is no way to upgrade the sidecar
    without re-creating the Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Node proxy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The node proxy is an alternative data plane architecture. Instead of injecting
    a sidecar proxy into each service, the service mesh consists of a single proxy
    running on each node. Each node proxy handles the traffic for all services running
    on their node, as depicted in [Figure 6-22](#the_node_proxy_model_involves_a_single_service_mesh_proxy).
    Service meshes that follow this architecture include [Consul Connect](https://www.consul.io/docs/connect)
    and [Maesh](https://containo.us/maesh). The first version of Linkerd used node
    proxies as well, but the project has since moved to the sidecar model in version
    2.
  prefs: []
  type: TYPE_NORMAL
- en: When compared to the sidecar proxy architecture, the node proxy approach can
    have greater performance impact on services. Because the proxy is shared by all
    the services on a node, services can suffer from noisy neighbor problems and the
    proxy can become a network bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0622](assets/prku_0622.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-22\. The node proxy model involves a single service mesh proxy that
    handles the traffic for all services on the node.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adopting a Service Mesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adopting a service mesh can seem like a daunting task. Should you deploy it
    to an existing cluster? How do you avoid affecting workloads that are already
    running? How can you selectively onboard services for testing?
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the different considerations you should make
    when introducing a service mesh to your application platform.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize one of the pillars
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the first things to do is to prioritize one of the service mesh pillars.
    Doing so will allow you to narrow the scope, both from an implementation and testing
    perspective. Depending on your requirements (which you’ve established if you’re
    adopting a service mesh, right?), you might prioritize mutual TLS, for example,
    as the first pillar. In this case, you can focus on deploying the PKI necessary
    to support this feature. No need to worry about setting up a tracing stack or
    spending development cycles testing traffic routing and management.
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on one of the pillars enables you to learn about the mesh, understand
    how it behaves in your platform, and gain operational expertise. Once you feel
    comfortable, you can implement additional pillars, as necessary. In essence, you
    will be more successful if you follow a piecemeal deployment instead of a big-bang
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy to a new or an existing cluster?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on your platform’s life cycle and topology, you might have a choice
    between deploying the service mesh to a new, fresh cluster or adding it to an
    existing cluster. When possible, prefer going down the new cluster route. This
    eliminates any potential disruption to applications that would otherwise be running
    in an existing cluster. If your clusters are ephemeral, deploying the service
    mesh to a new cluster should be a natural path to follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In situations where you must introduce the service mesh into an existing cluster,
    make sure to perform extensive testing in your development and testing tiers.
    More importantly, offer an onboarding window that allows development teams to
    experiment and test their services with the mesh before rolling it out to the
    staging and production tiers. Finally, provide a mechanism that allows applications
    to opt into being part of the mesh. A common way to enable the opt-in mechanism
    is to provide a Pod annotation. Istio, for example, provides an annotation (`sidecar.istio.io/inject`)
    that determines whether the platform should inject the sidecar proxy into the
    workload, which is visible in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Handling upgrades
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When offering a service mesh as part of your platform, you must have a solid
    upgrade strategy in place. Keep in mind that the service mesh data plane is in
    the critical path that connects your services, including your cluster’s edge (regardless
    of whether you are using the mesh’s Ingress gateway or another Ingress controller).
    What happens when there’s a CVE that affects the mesh’s proxy? How will you handle
    upgrades effectively? Do not adopt a service mesh without understanding these
    concerns and having a well-established upgrade strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The upgrade strategy must account for both the control plane and the data plane.
    The control plane upgrade carries less risk, as the mesh’s data plane should continue
    to function without it. With that said, do not discount control plane upgrades.
    You should understand the version compatibility between the control plane and
    the data plane. If possible, follow a canary upgrade pattern, as recommended by
    the [Istio project](https://oreil.ly/TZj7F). Also make sure to review any service
    mesh Custom Resource Definition (CRD) changes and whether they impact your services.
  prefs: []
  type: TYPE_NORMAL
- en: The data plane upgrade is more involved, given the number of proxies running
    on the platform and the fact that the proxy is handling service traffic. When
    the proxy runs as a sidecar, the entire Pod must be re-created to upgrade the
    proxy as Kubernetes doesn’t support in-place upgrades of containers. Whether you
    do a full data plane upgrade or a slow rollout of the new data plane proxy depends
    on the reason behind the upgrade. One one hand, if you are upgrading the data
    plane to handle a vulnerability in the proxy, you must re-create every single
    Pod that participates in the mesh to address the vulnerability. As you can imagine,
    this can be disruptive to some applications. If, on the other hand, you are upgrading
    to take advantage of new features or bug fixes, you can let the new version of
    the proxy roll out as Pods are created or moved around in the cluster. This slower,
    less disruptive upgrade results in version sprawl of the proxy, which may be acceptable
    as long as the service mesh supports it. Regardless of why you are upgrading,
    always use your development and testing tiers to practice and validate service
    mesh upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to keep in mind is that meshes typically have a narrow set of
    Kubernetes versions they can support. How does a Kubernetes upgrade affect your
    service mesh? Does leveraging a service mesh hinder your ability to upgrade Kubernetes
    as soon as a new version is released? Given that Kubernetes APIs are relatively
    stable, this should not be the case. However, it is possible and something to
    keep in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Resource overhead
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the primary trade-offs of using a service mesh is the resource overhead
    that it carries, especially in the sidecar architecture. As we’ve discussed, the
    service mesh injects a proxy into each Pod in the cluster. To get its job done,
    the proxy consumes resources (CPU and memory) that would otherwise be available
    to other services. When adopting a service mesh, you must understand this overhead
    and whether the trade-off is worth it. If you are running the cluster in a datacenter,
    the overhead is probably palatable. However, the overhead might prevent you from
    using a service mesh in edge deployments where resource constraints are tighter.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps more important, a service mesh introduces latency between services given
    that the service calls are traversing a proxy on both the source and the destination
    services. While the proxies used in service meshes are usually highly performant,
    it is important to understand the latency overhead they introduce and whether
    your application can function given the overhead.
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating a service mesh, spend time investigating its resource overhead.
    Even better, run performance tests with your services to understand how the mesh
    behaves under load.
  prefs: []
  type: TYPE_NORMAL
- en: Certificate Authority for mutual TLS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The identity features of a service mesh are usually based on X.509 certificates.
    Proxies in the mesh use these certificates to establish mutual TLS (mTLS) connections
    between services.
  prefs: []
  type: TYPE_NORMAL
- en: Before being able to leverage the mTLS features of a service mesh, you must
    establish a certificate management strategy. While the mesh is usually responsible
    for minting the service certificates, it is up to you to determine the Certificate
    Authority (CA). In most cases, a service mesh uses a self-signed certificate as
    the CA. However, mature service meshes allow you to bring your own CA, if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Because the service mesh handles service-to-service communications, using a
    self-signed CA is adequate. The CA is essentially an implementation detail that
    is invisible to your applications and their clients. With that said, security
    teams can disapprove of the use of self-signed CAs. When adopting a service mesh,
    make sure to bring your security team into the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: If using a self-signed CA for mTLS is not viable, you will have to provide a
    CA certificate and key that the service mesh can use to mint certificates. Alternatively,
    you can integrate with an external CA, such as Vault, when an integration is available.
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster service mesh
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some service meshes offer multicluster capabilities that you can use to extend
    the mesh across multiple Kubernetes clusters. The goal of these capabilities is
    to connect services running in different clusters through a secure channel that
    is transparent to the application. Multicluster meshes increase the complexity
    of your platform. They can have both performance and fault-domain implications
    that developers might have to be aware of. In any case, while creating multicluster
    meshes might seem attractive, you should avoid them until you gain the operational
    knowledge to run a service mesh successfully within a single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Service routing is a crucial concern when building an application platform atop
    Kubernetes. Services provide layer 3/4 routing and load balancing capabilities
    to applications. They enable applications to communicate with other services in
    the cluster without worrying about changing Pod IPs or failing cluster nodes.
    Furthermore, developers can use NodePort and LoadBalancer Services to expose their
    applications to clients outside of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress builds on top of Services to provide richer routing capabilities. Developers
    can use the Ingress API to route traffic according to application-level concerns,
    such as the `Host` header of the request or the path that the client is trying
    to reach. The Ingress API is satisfied by an Ingress controller, which you must
    deploy before using Ingress resources. Once installed, the Ingress controller
    handles incoming requests and routes them according to the Ingress configuration
    defined in the API.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a large portfolio of microservices-based applications, your developers
    might benefit from a service mesh’s capabilities. When using a service mesh, services
    communicate with each other through proxies that augment the interaction. Service
    meshes can provide a variety of features, including traffic management, mutual
    TLS, access control, automated service metrics gathering, and more. Like other
    interfaces in the Kubernetes ecosystem, the Service Mesh Interface (SMI) aims
    to enable platform operators to use a service mesh without tying themselves to
    specific implementations. However, before adopting a service mesh, ensure that
    you have the operational expertise in your team to operate an additional distributed
    system on top of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
