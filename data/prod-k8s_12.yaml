- en: Chapter 11\. Building Platform Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Platform services are those components that are installed in order to add features
    to the application platform. They are usually deployed as containerized workloads
    into some `*-system` Namespace and are maintained by the platform engineering
    team. These platform services are distinct from the workloads managed by platform
    tenants, which are maintained by the application development teams.
  prefs: []
  type: TYPE_NORMAL
- en: The cloud native ecosystem is rich with projects you can use as part of your
    application platform. Additionally, there are throngs of vendors that will be
    happy to provide platform service solutions. Use these wherever they pass the
    cost benefit analysis. They may even take you all the way to your app platform
    destination. But we have found that it’s common for enterprise users of Kubernetes-based
    platforms to build custom components. You may have to integrate your Kubernetes-based
    platform with some existing in-house system. You may have some unique, sophisticated
    workload requirements to meet. You may have some edge cases to account for that
    are uncommon or specific to your business needs. Whatever the circumstance, this
    chapter addresses the details of extending your application platform with custom
    solutions to fill these gaps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Central to this notion of building custom platform services is the effort to
    remove human toil. There is more to this than just automation. Automation is the
    keystone, but integration of automated components is the mortar. Smooth, reliable
    interaction of systems is both challenging and critical. This concept of API-driven
    software is powerful because it fosters integration of software systems. This
    is part of why Kubernetes has achieved such wide adoption: it enables API-driven
    behavior for your entire platform without the need to build and expose an API
    for every piece of software you add to your platform. This software can leverage
    the Kubernetes API by either managing core resources or adding custom resources
    to represent the state of new objects. If we follow these patterns of integrated
    automation as we build our platform services, we stand to remove tremendous human
    toil. And if we succeed in this effort, we will open up greater opportunities
    for innovation, development, and advancement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we are addressing how to extend the Kubernetes control plane.
    We are taking the effective engineering patterns used by Kubernetes and using
    those same patterns to build upon those systems. We will spend much of this chapter
    exploring Kubernetes operators, their design pattern and use cases, and how to
    develop them. However, it’s important that we first take a tour of the points
    of extension of Kubernetes so that we can maintain a holistic view of building
    platform services. It’s important that we have a clear context and apply solutions
    that are harmonious with the broader system. Finally, we will examine how we may
    extend possibly the most important Kubernetes controller in the ecosystem: the
    scheduler.'
  prefs: []
  type: TYPE_NORMAL
- en: Points of Extension
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a wonderfully extensible system. This is certainly one of its
    most powerful features. A common critical error in software development is attempting
    to add features to meet every imaginable use case. The system can quickly become
    a maze of options with unclear paths to outcomes. Furthermore, it often becomes
    unstable as internal dependencies grow and brittle connections between components
    of the system erode reliability. There is good reason behind the central tenets
    of the Unix philosophy to do one thing well and make it interoperable. Kubernetes
    could never provide for every possible requirement users might encounter while
    orchestrating their containerized workloads. That would be an impossible system
    to build. The core functions it does provide are challenging enough. As it is,
    Kubernetes is a relatively complex distributed software system, even with a pretty
    narrow set of concerns. It could never meet every requirement, and it doesn’t
    need to since it offers points of extension that allow specialized needs to be
    met by specialized solutions that may be readily integrated. It can be extended
    and customized to meet virtually any requirements you may have.
  prefs: []
  type: TYPE_NORMAL
- en: The context of what we are calling plug-in extensions that satisfy defined interfaces
    with Kubernetes are largely covered elsewhere in this book, as are some of the
    popular webhook extension solutions. We present a quick review of these here to
    paint a picture around the operator extension topic, which is where we will spend
    considerable time in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Plug-in Extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a broad class of extensions that generally help integrate Kubernetes
    with adjacent systems that are important, and often essential, to running workloads
    on Kubernetes. They are specifications that third parties can use to implement
    solutions, rather than implementations themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs: []
  type: TYPE_NORMAL
- en: The Container Network Interface (CNI) defines the interface that must be satisfied
    by a plug-in to provide a network for containers to connect to. There are many
    plug-ins that exist to fulfill this requirement but they all must satisfy the
    CNI. This topic is covered in [Chapter 5](ch05.html#chapter5).
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs: []
  type: TYPE_NORMAL
- en: The Container Storage Interface (CSI) provides a system for exposing storage
    systems to containerized workloads. Again, there are many different volume plug-ins
    that expose storage from different providers. This topic is explored in [Chapter 4](ch04.html#chapter4).
  prefs: []
  type: TYPE_NORMAL
- en: Container Runtime
  prefs: []
  type: TYPE_NORMAL
- en: The Container Runtime Interface (CRI) defines a standard for the operations
    that need to be exposed by a container runtime such that the kubelet does not
    care what runtime is in use. Docker has historically been the most popular, but
    there are now others with their own strengths that have become popular. We discuss
    this topic in detail in [Chapter 3](ch03.html#container_runtime_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Devices
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes device plug-in framework allows workloads to access devices on
    underlying nodes. The most common example of this we have found in the field is
    for graphics processing units (GPUs) used by compute-intensive workloads. Node
    pools are often added to clusters for nodes that have these specialized devices,
    allowing workloads to be assigned to them. See [Chapter 2](ch02.html#deployment_models)
    for more on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: The development of these plug-ins is usually carried out by vendors that either
    support or sell products that are integrated. It is very rare in our experience
    to find platform developers building custom solutions in this area. Instead, it
    is generally a matter of evaluating the available options and leveraging those
    that meet your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Webhook Extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Webhook extensions act as a backend server for the Kubernetes API server to
    call to fulfill custom renditions of core API functionality. There are several
    steps that each request goes through when it arrives at the API server. The client
    is authenticated to ensure they are permitted access (AuthN). The API checks to
    ensure the client is authorized to perform the action they are requesting (AuthZ).
    The API server mutates the resource as called for by enabled admission plug-ins.
    The resource schema is validated, and any specialized or custom validation is
    performed by validating admission control. [Figure 11-1](#webhook_extensions_are_back_end_servers)
    illustrates the relationship between the clients, the Kubernetes API, and the
    webhook extensions leveraged by the API.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication extensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Authentication extensions, such as OpenID Connect (OIDC), provide the opportunity
    to offload the task of authenticating requests to the API server. This topic is
    covered in depth in [Chapter 10](ch10.html#chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: You can also have the API server call out to a webhook to authorize actions
    that can be taken on resources by authenticated users. This is an uncommon implementation
    since Kubernetes has a capable Role-Based Access Control system built in. However,
    if you find this system inadequate for any reason you have this option available
    to you.
  prefs: []
  type: TYPE_NORMAL
- en: Admission control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Admission control is a particularly useful and widely used extension point.
    If in use, when a request is sent to the API server to perform an action, the
    API server calls any applicable admission webhooks according to the validating
    and mutating admission webhook configs. This topic is covered in [Chapter 8](ch08.html#chapter8).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1101](assets/prku_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Webhook extensions are backend servers leveraged by the Kubernetes
    API server.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Operator Extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Operators are clients of, as opposed to backend webhooks for, the API server.
    As shown in [Figure 11-2](#operators_extensions_are_clients_of_the_kubernetes_api_server),
    software operators interact as clients of the Kubernetes API, just like human
    operators. These software operators are often called *Kubernetes Operators* and
    follow the officially documented [operator pattern](https://oreil.ly/HLXtJ). Their
    primary purpose is to relieve toil from human operators and perform operations
    on their behalf. These operator extensions follow the same engineering principles
    as the core Kubernetes control plane components. When developing operator extensions
    as platform services, think of them as custom extensions of the control plane
    for your application platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1102](assets/prku_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Operator extensions are clients of the Kubernetes API server.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Operator Pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You could say that the operator pattern boils down to extending Kubernetes
    with Kubernetes. We create new Kubernetes resources and develop Kubernetes controllers
    to reconcile the state defined in them. We use Kubernetes resources called Custom
    Resource Definitions (CRDs) to define our new resources. These CRDs create new
    API types and tell the Kubernetes API how to validate these new objects. Then,
    we take the very same principles and designs that make Kubernetes controllers
    so effective and use those principles to build software extensions to the system.
    These are the two core mechanisms that we employ when building operators: custom
    resources and controllers.'
  prefs: []
  type: TYPE_NORMAL
- en: The notion of an operator was introduced in November 2016 by Brandon Phillips,
    one of the founders of CoreOS. This early definition of an operator was that an
    operator was an app-specific controller that managed a complex stateful application.
    This is still a very useful definition but has broadened somewhat over the years
    to where the Kubernetes docs now classify any controller that uses CRDs as an
    operator. This more general definition is the one we will use as it applies to
    building platform services. Your platform services may not be “complex stateful
    applications” but still can benefit from using this powerful operator pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The following section will look at Kubernetes controllers, which provide a model
    for the functionality we will use in our custom controllers. Then we will examine
    the custom resources that store the desired and existing state our controllers
    reconcile for us.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Controllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes’ core features and functionality are provided by controllers. They
    watch resource types and respond to creation, mutation, and deletion of resources
    by fulfilling that desired state. For example, there is a controller that comes
    bundled with the kube-controller-manager that watches for ReplicaSet resource
    types. When a ReplicaSet is created, the controller creates a number of identical
    Pods—as many as were defined by the `replicas` field in the ReplicaSet. Then at
    some point later, if you change that value, the controller will create or delete
    Pods to satisfy the new desired state.
  prefs: []
  type: TYPE_NORMAL
- en: This watch mechanism is central to the functionality of all Kubernetes controllers.
    It is an etcd feature that is exposed by the Kubernetes API server to the controllers
    that need to respond to changes in resources. The controllers hold a connection
    open with the API server, which allows the API server to notify the controller
    when a change has occurred to a resource it cares about or manages.
  prefs: []
  type: TYPE_NORMAL
- en: This enables very powerful behavior. The user can declare the desired state
    of the system by submitting resource manifests. The controllers responsible for
    fulfilling the desired state are notified and begin working to make the existing
    state match the declared, desired state. Furthermore, in addition to users submitting
    manifests, controllers can also do the same which, in turn, triggers operations
    in other controllers. In this way you end up with a system of controllers that
    can provide sophisticated functionality that is stable and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: One important feature of these controllers is that if they cannot fulfill the
    desired state due to some impediment, they will continue to try on an infinite
    loop. The duration between attempts to fulfill desired state may increase over
    time so that undue load is not placed on the system, but try it will. This provides
    a self-healing behavior that is incredibly important in complex distributed systems
    such as this.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the scheduler is responsible for assigning Pods to nodes in the
    cluster. The scheduler is just another Kubernetes controller, just with a particularly
    important and involved task. If there are insufficient compute resources available
    for one or more Pods, they will go into a “Pending” state and the scheduler will
    continue to attempt to schedule the Pod at some interval. As such, if compute
    resources free up or are added at any point, the Pod will be scheduled and run.
    So if another batch workload completes and resources free up, or if a cluster
    autoscaler adds some worker nodes, the Pending Pod will be assigned with no further
    action required from a human operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'In following the operator pattern to build extensions to your application platform,
    it’s essential to use these design principles used by Kubernetes controllers:
    (1) watch resources in the Kubernetes API to get notified when changes to their
    desired state occur, and (2) work to reconcile existing and desired state on a
    nonterminating loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important features of the Kubernetes API is the ability to extend
    the resource types it will recognize. If you submit a valid CRD you will immediately
    have a new custom API type at your disposal. The CRD contains all the fields that
    you will need in your custom resource both in the `spec` where you will provide
    the desired state for your resource and in the `status` where you can record important
    information about the observed, existing state.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving further into this topic, let’s briefly review Kubernetes resources.
    We are going to be talking a lot about resources in this chapter, so it’s important
    to ensure we’re crystal clear on this topic. When we talk about *resources* in
    Kubernetes, we are referring to the objects that are used to record state. An
    example of a common resource is the Pod resource. When you create a Pod manifest,
    you are defining the attributes of what will become a Pod resource. When you submit
    that to the API server with `kubectl apply -f pod.yaml` or similar, you are creating
    an instance of the Pod API type. On one hand you have the API type, or “kind,”
    which refers to the definition and form of the object as provided in a CRD. On
    the other hand we have the resource that is an instantiation or instance of that
    kind. The Pod is an API type or kind. A Pod you create with the name “my-app”
    is a Kubernetes resource.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a relational database where relationships between objects are recorded
    and linked by foreign keys in the database itself, each object in the Kubernetes
    API exists independently. Relationships are established using labels and selectors,
    and it is the job of the controllers to manage relationships defined this way.
    You cannot query etcd for related objects the way you can with structured query
    language (SQL). So when we talk about resources, we’re talking about actual instances
    of Namespaces, Pods, Deployments, Secrets, ConfigMaps, etc. When we talk about
    custom resources, we’re referring to user-defined resources that have been added
    and defined with CRDs. When you create a CRD, you define new API types that allow
    you to create and manage custom resources as you would other core Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
- en: CRDs use the Open API v3 schema specification for defining fields. This allows
    for features like setting fields as optional or required as well as setting default
    values. This will provide validation instructions for the API server when it receives
    a request to create or update one of your custom resources. In addition, you can
    group your APIs for improved logical organization and, very importantly, version
    your API types as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate what a CRD is and what a manifest for the resulting custom resource
    looks like, let’s look at a fictional example of a custom WebApp API type. In
    this example, a WebApp resource includes the desired state for a web application
    that consists of the following six Kubernetes resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: A stateless application that provides a user interface for clients, processes
    requests, and stores data in a relational database
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSet
  prefs: []
  type: TYPE_NORMAL
- en: The relational database that provides the persistent data store for the web
    application
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMap
  prefs: []
  type: TYPE_NORMAL
- en: Contains the config file for the stateless application, which is mounted into
    each Pod of the Deployment
  prefs: []
  type: TYPE_NORMAL
- en: Secret
  prefs: []
  type: TYPE_NORMAL
- en: Contains credentials for the application to connect to its database
  prefs: []
  type: TYPE_NORMAL
- en: Service
  prefs: []
  type: TYPE_NORMAL
- en: Routes traffic to the Deployment’s backend Pods
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs: []
  type: TYPE_NORMAL
- en: Contains routing rules for the Ingress controller to properly route client requests
    into the cluster
  prefs: []
  type: TYPE_NORMAL
- en: The creation of a WebApp resource would prompt a WebApp Operator to create these
    various child resources. These created resources would constitute a complete,
    functioning instance of the web application that serves clients that are end users
    and customers of the business.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 11-1](#webapp_custom_resources_definition_manifest) illustrates what
    a CRD might look like to define a new WebApp API type.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-1\. WebApp CRD manifest
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the custom resource *definition*, as distinct from the name of the
    custom resource itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_building_platform_services_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the custom resource, as distinct from the definition. This includes
    the variations of the name such as the plural version.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_building_platform_services_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The `deploymentTier` field must contain one of the values listed under `enum`.
    This validation will be carried out by the API server when it receives requests
    to create or update an instance of this custom resource.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_building_platform_services_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The `webAppReplicas` field includes a default value that will be applied if
    the field is not provided.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_building_platform_services_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The required fields are listed here. Note that `webAppReplicas` is not included
    and that it has a default value.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s examine what a manifest for a WebApp would look like. Before submitting
    the manifest shown in [Example 11-2](#ex_11-2) to the API server, you must first
    create the CRD shown in [Example 11-1](#webapp_custom_resources_definition_manifest)
    so that Kubernetes has an API for it. Otherwise, it will not recognize what you
    are trying to create.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-2\. An example of a manifest for a WebApp resource
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This manifest specifies the default value for an optional field, which is unnecessary
    but fine to do if explicitness is desired.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_building_platform_services_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: One of the permitted values for this field is used. Any value that is not permitted
    would prompt the API server to reject the request with an error.
  prefs: []
  type: TYPE_NORMAL
- en: When this WebApp manifest is submitted to the Kubernetes API, the WebApp Operator
    will be notified via its watch that a new instance of the WebApp kind has been
    created. It will fulfill the desired state expressed in the manifest by calling
    the API server to create the various child resources needed to spin up an instance
    of the web application.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the custom resource model is powerful, do not overuse it. Do not use custom
    resources as the primary data store for an end-user application. Kubernetes is
    a container orchestration system. etcd should be storing the state of your software
    deployments, not the internal persistent data for your application. Doing so will
    put significant load on your cluster’s control plane. Stick with relational databases,
    object stores, or whatever data store makes sense for your application. Leave
    the control plane to manage software deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Operator Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing a Kubernetes-based platform, operators offer a compelling model
    for adding features to that platform. If you can represent the state of the system
    you need to implement in the fields of a custom resource, and if you can yield
    value from reconciling changes using a Kubernetes controller, an operator is often
    a great option.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to platform features, operators may be used to facilitate the management
    of software deployments on your platform. They can provide the convenience of
    a generalized abstraction or can be custom-built for the needs of a particular
    sophisticated application. In either case, using software to manage software deployments
    is very useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section we’re going to discuss the three general operator categories
    that you may consider using with your platform:'
  prefs: []
  type: TYPE_NORMAL
- en: Platform utilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General-purpose workload operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: App-specific operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform Utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Operators can be extremely useful in developing your platform. They allow you
    to add features to your cluster and build out functionality on top of Kubernetes
    in a way that leverages and integrates with the control plane seamlessly. There
    is a wealth of open source projects that utilize operators to provide platform
    services atop Kubernetes. These projects are already available and don’t require
    you develop them. The reason we bring them up in a chapter about building them
    is that they help build a good mental model for how they work. Should you find
    yourself having to develop custom platform utilities, looking at existing successful
    projects will be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: The [Prometheus Operator](https://oreil.ly/ClgDL) allows you to provide metrics
    collection, storage, and alerting platform services on your platform. In [Chapter 9](ch09.html#observability_chapter)
    we delve into the value that can be derived from this project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[cert-manager](https://cert-manager.io) provides certificate management as
    a service functionality. It removes significant toil and potential for downtime
    by offering x509 certificate creation and renewal services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Rook](https://rook.io) is storage operator that integrates with providers
    like [Ceph](https://ceph.io) to manage block, object, and filesystem storage as
    a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These open source solutions are examples of what is available in the community.
    There are also countless vendors that can provide and support similar platform
    utilities. However, when a solution is not available or not a good fit, enterprises
    sometimes build their own custom platform utilities.
  prefs: []
  type: TYPE_NORMAL
- en: A common example of a custom platform utility we see in the field is a Namespace
    operator. We have found it quite common for organizations to have a standard set
    of resources that are created with each Namespace, such as ResourceQuotas, LimitRanges,
    and Roles. And it has been a useful pattern to use a controller to take care of
    the routine tedium of creating these resources for each Namespace. In a later
    section, we will use this Namespace operator idea as an example to illustrate
    some implementation details when building operators.
  prefs: []
  type: TYPE_NORMAL
- en: General-Purpose Workload Operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Application developers’ core competency and concern is to add stability and
    features to the software they build. It is not writing YAML for deployment to
    Kubernetes. Learning how to properly define resource limits and requests, learning
    how to mount a ConfigMap volume or Secret as an environment variable, learning
    how to use label selectors to associate Services with Pods—none of these things
    add features or stability to their software.
  prefs: []
  type: TYPE_NORMAL
- en: In an organization that has developed common patterns for deployed workloads,
    the model of abstracting the complexity in a general-purpose manner has considerable
    promise. This is especially relevant in organizations that have embraced microservice
    architectures. In these environments there may be a considerable number of distinct
    pieces of software that are deployed with different functionality, but with very
    similar patterns of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if your company has a large number of workloads that consist of
    a Deployment, Service, and Ingress resource, there are likely patterns that can
    be encoded into an operator that can abstract much of the resource manifests for
    these objects. In each case the Service references labels on the Pods of a Deployment.
    In each case the Ingress references the Service name. All these things can easily
    be handled by an operator—getting these details right is the definition of toil.
  prefs: []
  type: TYPE_NORMAL
- en: App-Specific Operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This type of operator hits at the core of what a Kubernetes operator is: custom
    resources in conjunction with a custom Kubernetes controller for managing a complex
    stateful application. They are purpose-built to manage a particular application.
    Popular examples of this model are the various database operators in the community.
    We have operators for Cassandra, Elasticsearch, MySQL, MariaDB, PostgreSQL, MongoDB
    and many more. Commonly, they handle initial deployment as well as day 2 management
    concerns such as configuration updates, backups, and upgrades.'
  prefs: []
  type: TYPE_NORMAL
- en: Operators for popular community- or vendor-supported projects have gained in
    popularity over the past few years. The area where this is approach is still in
    its infancy is in internal enterprise applications. In cases where your organization
    internally develops and maintains a sophisticated stateful application, an app-specific
    operator may be beneficial. For example, if your company maintains something like
    an ecommerce website, transaction processing app, or inventory management system
    that delivers critical business functionality, you may want to consider this option.
    There is tremendous opportunity for reducing human toil in the deployment and
    day 2 management of these kinds of workloads, especially when they are deployed
    widely and updated frequently.
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t to say that these app-specific operators are the universally right
    choice for managing your workloads. For simpler use cases, they are likely to
    be severe overkill. Production-ready operators are not trivial to develop, so
    weigh the trade-offs. How much time do you spend in routine toil managing deployment
    and day 2 concerns for an application? Is the engineering cost of building an
    operator likely to be less than that routine toil over the long term? Could simpler,
    existing tools such as Helm or Kustomize provide enough automation to sufficiently
    alleviate toil?
  prefs: []
  type: TYPE_NORMAL
- en: Developing Operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking on the task of developing Kubernetes operators is not trivial, especially
    if tackling a full-featured, app-specific operator. The engineering investment
    to get one of these more involved projects into production can be considerable.
    As with other types of software development, if getting started in this area,
    begin with less involved projects while you become familiar with useful patterns
    and successful strategies. In this section we’ll discuss some tools and design
    strategies that will help make developing operators a more efficient and successful
    endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover some specific projects you can leverage to help in development
    of such tools. And then we’ll break down the process of designing and implementing
    this kind of software in detail. We’ll include some code snippets to illustrate
    the concepts and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Operator Development Tooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have a compelling use case for a custom Kubernetes operator, there are
    a couple of community projects that can be very helpful in your endeavor. If you
    are—or have on staff—an experienced Go programmer that is familiar with the Kubernetes
    client-go library and with developing Kubernetes operators, you can certainly
    write your operators from scratch. However, there are common components to every
    operator, and using tools to generate boilerplate source code and utilities are
    expediencies that even seasoned operator developers commonly use. They just save
    time. Software development kits (SDKs) and frameworks can be helpful when they
    fit the pattern of software you’re developing. However, they can be a nuisance
    if they make assumptions that don’t suit your purpose. If your project fits the
    standard model of using one or more custom resources to define configuration,
    and custom controllers to implement behavior associated with these objects, it
    is likely the tools we discuss here will be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Kubebuilder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubebuilder can be described as an SDK for building Kubernetes APIs. This is
    an apt description but is not exactly what you might expect. Using kubebuilder
    begins with the command-line tool that you use to generate boilerplate. It stamps
    out the source code, a Dockerfile, a Makefile, sample Kubernetes manifests—all
    the things you need to write for every such project. As such, it saves a ton of
    time in getting a project started.
  prefs: []
  type: TYPE_NORMAL
- en: Kubebuilder also leverages a collection of tools in a related project called
    controller-runtime. The required imports and common implementations are included
    in the source code generated by the CLI. These help with much of the routine heavy
    lifting of running a controller and interacting with the Kubernetes API. It helps
    with setting up shared caches and clients to provide efficient interaction with
    the API server. The cache allows your controller to list and get objects without
    new requests to the API server for each query, which eases load on the API server
    and speeds up reconciliation. Controller-runtime also provides mechanisms for
    triggering reconcile requests in response to events such as resource changes.
    These reconciliations will be triggered by default for the parent custom resource.
    They can—and usually should—also be triggered when changes occur to child resources
    created by the controller and functions are available to do so. If running your
    controller in highly available (HA) mode, controller-runtime provides the opportunity
    to enable leader election to ensure just one controller is active at any given
    time. Furthermore, controller-runtime includes a package to implement webhooks,
    which are often used for admission control. Lastly, the library includes facilities
    to write structured logs and expose Prometheus metrics for observability.
  prefs: []
  type: TYPE_NORMAL
- en: Kubebuilder is a great choice if you are a Go programmer with Kubernetes experience.
    It is even a good choice if you are an experienced software developer but are
    new to the Go programming language. But it is exclusively for Go—it doesn’t cater
    to other languages.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are going to be developing tools for Kubernetes, you should strongly
    consider learning Go if you don’t know it already. You can certainly use other
    languages. Kubernetes offers a REST API, after all. And there are officially supported
    client libraries for Python, Java, C#, JavaScript, and Haskell, not to mention
    many other community-supported libraries. If you have important reasons for using
    these, you can certainly be successful. However, Kubernetes itself is written
    in Go, and the ecosystem for that language in the world of Kubernetes is rich
    and well-supported.
  prefs: []
  type: TYPE_NORMAL
- en: One of the features of Kubebuilder that make it such a time-saver is its generation
    of CRDs. Writing a CRD manifest by hand is no joke. The OpenAPI v3 spec that is
    used to define these custom APIs is pretty detailed and involved. The Kubebuilder
    CLI will generate the files where you will define the fields for your custom API
    types. You add the various fields to the struct definitions and tag them with
    special markers that provide metadata such as default values. Then you can use
    a make target to generate the CRD manifests. It is very handy.
  prefs: []
  type: TYPE_NORMAL
- en: On the subject of make targets, in addition to generating CRDs, you can generate
    the RBAC and sample custom resource manifests, install the custom resources in
    your development cluster, build and publish images for your operator, and run
    your controller locally against a cluster during development. Having conveniences
    for all these tedious, time-consuming tasks really is a productivity boost, especially
    early in the project.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, we prefer and recommend Kubebuilder for building operators.
    It has been adopted and used with success in a variety of projects.
  prefs: []
  type: TYPE_NORMAL
- en: Metacontroller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If your comfort with a particular programming language besides Go compels you
    to stick with it, another useful option to aid in developing operators is Metacontroller.
    This is an entirely different approach to developing and deploying operators,
    but it is one that is worth considering if you want to use a variety of languages
    and expect to deploy a number of custom in-house operators with your platform.
    Engineers experienced in programming Kubernetes will also sometimes use Metacontroller
    for prototyping and then use Kubebuilder for the final project once design and
    implementation details have been established. And this alludes to one of the strengths
    of Metacontroller: once you have the Metacontroller add-on installed in your cluster,
    it is fast to get going.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That is essentially what Metacontroller is: a cluster add-on that abstracts
    away the interaction with the Kubernetes API. Your job is to write the controller
    webhook that contains your controller’s logic. Metacontroller calls this the *lambda
    controller*. Your lambda controller makes decisions about what to do with the
    resources it cares about. Metacontroller watches the resources under management
    and alerts your controller with an HTTP call when there is a change it needs to
    make a decision about. Metacontroller itself uses custom resources that define
    the characteristics of your webhook, e.g., its URL and the resources it manages.
    As such, once Metacontroller is running in your cluster, adding a controller consists
    of deploying your lambda controller webhook and adding a Metacontroller custom
    resource; e.g., composite controller resource. And all your new controller need
    do is expose an endpoint that will accept requests from Metacontroller, parse
    JSON payloads that contain the Kubernetes resource object in question, and then
    return a response to Metacontroller with any changes to be sent to the Kubernetes
    API. [Figure 11-3](#metacontroller_abstracts_the_kubernetes_api_for_your_lamda_controller)
    illustrates how these components interact when using Metacontroller.'
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1103](assets/prku_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. Metacontroller abstracts the Kubernetes API for your lambda controller.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What Metacontroller does not help with is the creation of any CRDs you may need
    to add to your cluster. You are on your own there. If you are writing a controller
    that responds to changes in core Kubernetes resources, this won’t be an issue.
    But if you’re developing custom resources, this is an area where Kubebuilder has
    a significant advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Operator Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Operator Framework is a collection of open source tools that originated
    at Red Hat and are now under the CNCF umbrella as an incubating project. This
    framework facilitates the development of operators. It includes the Operator SDK,
    which offers similar functionality to Kubebuilder when developing operators using
    Go. Like Kubebuilder, it provides a CLI to generate boilerplate for projects.
    Also like Kubebuilder, it uses the controller-runtime tools to help integrate
    with the Kubernetes API. In addition to Go projects, the Operator SDK allows developers
    to use Helm or Ansible to manage operations. The framework also includes the Operator
    Lifecycle Manager, which is what it sounds like: an operator for your operators.
    It provides abstractions for installing and upgrading your operators. The project
    also maintains an operator hub, which offers a way for users to discover operators
    for software they use. We have not encountered platform teams using these tools
    in the field. Being a Red Hat-maintained project, it is likely more common among
    users of OpenShift, a Red Hat Kubernetes-based offering.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Model Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as you might begin the design of a web application by defining the database
    schema the app will use to persist data, a great place to start when building
    an operator is the data model for the custom resources your operator will use.
    In fact, you will probably have some idea of what fields your custom resource
    will need in its spec before you get started. As soon as you recognize a problem
    to solve or gap to fill, the attributes of the object that defines desired state
    will begin to take shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example from earlier in this chapter, the Namespace operator, it could
    begin as an operator that will create a variety of resources: LimitRange, ResourceQuota,
    Roles, and NetworkPolicies to go along with a new Namespace for an app dev team.
    You may want to bind a team lead to the `namespace-admin` Role right away and
    then hand off management of the Namespace to that person. This would naturally
    lead you to add an `adminUsername` field to the spec of the custom resource. The
    custom resource manifest might look something like [Example 11-3](#ex_11-3).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-3\. An example of a manifest for an AcmeNamespace resource
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: An arbitrary name for the Namespace—in this case it will host a workload, “app-y.”
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_building_platform_services_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This username would correspond to that which is used by the company’s identity
    provider, usually an Active Directory system or similar.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting the manifest in [Example 11-3](#ex_11-3) would result in the username
    `sam` being added to the subjects of a RoleBinding to the `namespace-admin` Role
    in the manner shown in [Example 11-4](#ex_11-4).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-4\. Example of a Role and Rolebinding created for the `team-x` AcmeNamespace
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `adminUsername` provided in the AcmeNamespace manifest would get inserted
    here to be bound to the `namespace-admin` Role.
  prefs: []
  type: TYPE_NORMAL
- en: 'When thinking about the behavior you want, having Sam bound to the `namespace-admin`
    Role, the data needed to accomplish this becomes pretty clear: Sam’s username
    and the name of the Namespace. So start with the obvious pieces of data that you
    will need to provide functionality and define fields in the spec for your CRD
    from that. What that would look like as part of a Kubebuilder project would be
    similar to what is shown in [Example 11-5](#ex_11-5).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-5\. Type definition for the `AcmeNamespaceSpec`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is the source code from which Kubebuilder will generate your CRD manifest
    and the sample AcmeNamespace manifest to use for testing and demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a data model that we *think* will allow us to adequately manage
    state for the behavior we want, it’s time to start writing the controller. It’s
    probable we will find our data model inadequate as we develop and find there are
    additional fields that will be necessary to bring about our desired outcomes.
    But this is a useful place to start for now.
  prefs: []
  type: TYPE_NORMAL
- en: Logic Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logic is implemented in our controller. The primary job of the controller
    is to manage one or more custom resources. The controller will keep a watch on
    those custom resources under management. This part is trivial to implement when
    using tools like Kubebuilder and Metacontroller. It is pretty straightforward
    even if just using the client-go library, the GitHub repo for which has excellent
    code examples to refer to. With a watch on its custom resource/s, your controller
    will be notified of any changes to resources of this type. The job of your controller
    at this point is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather an accurate picture of the existing state of the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine the desired state of the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the required actions to reconcile the existing state with desired state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are essentially three places your controller may gather existing state
    information from:'
  prefs: []
  type: TYPE_NORMAL
- en: The `status` of your custom resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other related resources in the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevant conditions outside the cluster or in other systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `status` field provides a place for controllers in Kubernetes to record
    observed, existing state. For example, Kubernetes uses a `status.phase` field
    on some resources, such as Pods and Namespaces, to keep track of whether the resource
    is `Running` (for Pods) or `Active` (for Namespaces).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to the example of a Namespace operator. The controller is notified
    of a new AcmeNamespace resource along with the spec for it. The controller cannot
    assume it is a new resource and just robotically create the child Namespace and
    Role resources. What if it’s a preexisting resource that has simply been updated
    with some change? Attempting to create the child resources again will get an error
    from the Kubernetes API. However, to follow the preceding Kubernetes example,
    if we include a `phase` field in the `status` of our CRD, the controller can check
    it to evaluate existing state. When it is first created, the controller will find
    the `status.phase` field empty. This will tell the controller it is a new resource
    creation, and should go ahead with creating all child resources. Once all children
    are created with successful responses from the API, the controller can populate
    the `status.phase` field with a value of `Created`. Then if the AcmeNamespace
    resource is later changed, when the controller is notified, it can see from this
    field that it has been previously created and move on to other reconciliation
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: The use of the `status.phase` field to determine existing state as described
    so far has one critical flaw. It assumes the controller itself will never fail.
    What if a problem is encountered while creating the child resources? Say, for
    example, the controller gets notified of a new AcmeNamespace, creates the child
    Namespace, but then goes down before it can create the associated Role resources.
    When the controller comes back up, it will find the AcmeNamespace resource *without*
    `Created` in the `status.phase` field, attempt to create the child Namespace,
    and fail without a satisfactory way to reconcile the situation. In order to prevent
    this, the controller can add a `CreationInProgress` value to the `status.phase`
    as the very first step when it finds a new AcmeNamespace has been created. This
    way, if that failure during creation occurs, when the controller comes back up
    and sees the `CreationInProgress` phase, it will know that existing state cannot
    be accurately determined from the `status` alone. This is where it will need to
    look to other related resources in the cluster to determine existing state.
  prefs: []
  type: TYPE_NORMAL
- en: When existing state cannot be ascertained from the AcmeNamespace `status`, it
    can query the API server—or preferably the local cache of objects in the API server—for
    conditions it cares about. If it finds the phase of an AcmeNamespace set to `CreationInProgress`
    it can start querying the API server for the existence of child resources it expects
    to be there. In the failure example we’re using, it would query for a child Namespace,
    find it exists, and move on. It would query for the Role resource, find it *doesn’t*
    exist, and proceed with creating those resources. In this manner our controller
    can be tolerant of failure. And we should always assume failure will occur and
    develop controller logic accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, sometimes our controller will be interested in existing state outside
    the cluster. Cloud infrastructure controllers are a good example of this. The
    status of infrastructure systems must be queried from cloud provider APIs outside
    the cluster. What this existing state may be will be highly dependent on the purpose
    of the operator in question and will usually be clear.
  prefs: []
  type: TYPE_NORMAL
- en: Desired state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The desired state for a system is expressed in the `spec` for the relevant resources.
    In our Namespace operator, the desired state provided by the `namespaceName` informs
    the controller what the `metadata.name` field should be for the resulting Namespace.
    The `adminUsername` field determines what the `namespace-admin` RoleBinding’s
    `subjects[0].name` should be. These are examples of direct mappings of desired
    state to fields in child resources. Often, the implementation is less direct.
  prefs: []
  type: TYPE_NORMAL
- en: We saw an example of this with the use of the `deploymentTier` field in the
    `AcmeStore` example earlier in this chapter. It allowed a user to specify a single
    variable that informed the controller logic on what default values to use. We
    can apply a very similar idea to the Namespace operator. Our new, modified AcmeNamespace
    manifest could look like [Example 11-6](#ex_11-6).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-6\. The AcmeNamespace manifest with new fields added
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: New addition to the data model for the AcmeNamespace API type.
  prefs: []
  type: TYPE_NORMAL
- en: This will prompt the controller to create a ResourceQuota that could look like
    [Example 11-7](#ex_11-7).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-7\. The ResourceQuota created for the `team-x` AcmeNamespace
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Whereas the default ResourceQuota for `deploymentTier: prod` may look like
    [Example 11-8](#ex_11-8).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-8\. An alternative ResourceQuota created when `deploymentTier:` `prod`
    is given in the AcmeNamespace
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Reconciliation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Kubernetes, reconciliation is the process of altering the existing state
    to match the desired state. This can be as simple as the kubelet requesting the
    container runtime stop containers associated with a deleted Pod. Or it can be
    more complex, such as an operator creating an array of new resources in response
    to a custom resource that represents a stateful application. These are examples
    of reconciliation triggered by creating or deleting the resources that express
    desired state. But very often the reconciliation involves a response to a mutation.
  prefs: []
  type: TYPE_NORMAL
- en: A simple mutation example is if you update the number of replicas on a Deployment
    resource from 5 to 10\. The existing state is 5 Pods for a workload. The desired
    state is 10 Pods. The reconciliation performed by the Deployment controller in
    this case involves updating the replicas on the relevant ReplicaSet. The ReplicaSet
    controller then reconciles state by creating 5 new Pod resources which are, in
    turn, scheduled by the scheduler, which prompts the applicable kubelets to request
    new containers from the container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Another mutation example that is a little more involved is if you change the
    image in a Deployment spec. This is usually to update the version of a running
    application. By default, the Deployment controller will perform a rolling update
    as it reconciles state. It will create a *new* ReplicaSet for the new version
    of the app, increment the replicas on the new ReplicaSet, and decrement the replicas
    on the old ReplicaSet so that the Pods are replaced one at a time. Once all new
    image versions are running with the desired number of replicas, reconciliation
    is complete.
  prefs: []
  type: TYPE_NORMAL
- en: What reconciliation looks like for a custom controller managing a custom resource
    is going to vary greatly according to what the custom resource represents. But
    one thing that should remain constant is that if reconciliation is not successful
    due to a condition that is beyond the domain of the controller, it should retry
    indefinitely. Generally speaking, the reconciliation loop should implement increasing
    delays between iterations. For example, if it is reasonable to expect other systems
    in the cluster to be actively reconciling state that is preventing a controller
    from completing its operations, you may retry 1 second later. However, for the
    sake of preventing gratuitous resource consumption, it is recommended to exponentially
    increase the delay between each iteration until it reaches some reasonable limit
    of, say, 5 minutes. At that point, the controller will retry reconciliation once
    every 5 minutes. This allows for unattended resolving of systems while limiting
    resource consumption and network traffic in circumstances that aren’t resolving
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In broad terms, to implement initial controller functionality for the Namespace
    operator, we want to be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Write or generate a concise AcmeNamespace manifest like the earlier example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submit the manifest to the Kubernetes API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a controller respond by creating a Namespace, ResourceQuota, LimitRange,
    Roles, and a RoleBinding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a kubebuilder project, the logic to create these resources will live in a
    `Reconcile` method. The initial implementation of creating a Namespace with the
    controller could look like [Example 11-9](#ex_11-9).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-9\. The `Reconcile` method for the AcmeNamespace controller
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The variable that will represent the AcmeNamespace object that has been created,
    updated, or deleted.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_building_platform_services_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Fetching the content of the AcmeNamespace object from the request. Error catching
    omitted for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_building_platform_services_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the new Namespace object.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_building_platform_services_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the new Namespace resource in the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: This simplified snippet demonstrates the controller creating the new Namespace.
    Adding the Role and RoleBinding for the Namespace admin to the controller would
    look similar, as shown in [Example 11-10](#ex_11-10).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-10\. The creation of the Role and RoleBinding by the AcmeNamespace
    controller
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: At this point we are able to submit an AcmeNamespace manifest to the API and
    our Namespace operator will create the Namespace, a Role for the Namespace admin,
    and a RoleBinding to the username we provided. As we discussed earlier, this will
    work fine when we create a new AcmeNamespace but will break when it tries to reconcile
    it at any other time in the future. This would occur if the AcmeNamespace was
    changed in any way. It would also happen if the controller restarted for any reason.
    When the controller restarts, it must re-list and reconcile all existing resources
    in case something changed. So at this point, simply restarting our controller
    will break it. Let’s fix that by adding a simple use of the status field. First,
    [Example 11-11](#ex_11-11) shows the addition of the field to `AcmeNamespaceStatus`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-11\. Adding a field to the status of the AcmeNamespace
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now we can leverage this field in our controller as shown in [Example 11-12](#ex_11-12).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-12\. Using the new status field in the AcmeNamespace controller
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a controller that can restart safely. It also now has the beginnings
    of a system to examine existing state using the custom resource’s status and to
    carry out reconciliation steps based on that existing state.
  prefs: []
  type: TYPE_NORMAL
- en: One other thing we should usually do is set ownership for the child resources.
    If we set the AcmeNamespace resource as the owner of the Namespace, Role, and
    RoleBinding, that allows us to delete all the children by simply deleting the
    owner AcmeNamespace resource. This ownership will be managed by the API server.
    Even if the controller is not running, if the owner AcmeNamespace resource is
    deleted, the children will be deleted, too.
  prefs: []
  type: TYPE_NORMAL
- en: This raises the question of scoping for our AcmeNamespace API type. When using
    Kubebuilder, it will default to Namespaced scoping. However, a Namespace-scoped
    API type cannot be an owner of a cluster-scoped resource, such as a Namespace.
    With Kubebuilder we can use convenient markers to generate the CRD manifest with
    the proper scoping for this usage, as shown in [Example 11-13](#updated_api_definition_in_a_kubebuilder_project).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-13\. Updated API definition in a Kubebuilder project
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This marker will set the correct scoping on the CRD when the manifest is generated
    with `make manifests`.
  prefs: []
  type: TYPE_NORMAL
- en: This will generate a CRD that looks like [Example 11-14](#cluster_scoped_crd_for_acmenamespce_api_type).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-14\. Cluster scoped CRD for AcmeNamespace API type
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Resource scoping properly set.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can set the AcmeNamespace as an owner for all child resources. This will
    introduce an `ownerReferences` field into the `metadata` for each child resource.
    At this point our `Reconcile` method looks like [Example 11-15](#setting_ownership_for_child_resources_of_the_acmenamespace).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-15\. Setting ownership for child resources of the AcmeNamespace
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Check to see if resource was not found so we don’t try to reconcile when the
    AcmeNamespace has been deleted.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_building_platform_services_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Set owner reference on Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_building_platform_services_CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Set owner reference on Role.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_building_platform_services_CO9-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Set owner reference on RoleBinding.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we had to add the error checking to see if the AcmeNamespace resource
    was found. This is because when it is deleted, normal reconciliation will fail
    due to there no longer being desired state to reconcile. And, in this case, we
    put an owner reference on the child resources so the API server takes care of
    reconciling state for deletion events.
  prefs: []
  type: TYPE_NORMAL
- en: 'This illustrates the point that reconciliation must not make assumptions about
    existing state. Reconciliation is triggered when:'
  prefs: []
  type: TYPE_NORMAL
- en: The controller starts or restarts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A resource is created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A change is made to a resource, including changes made by the controller itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A resource is deleted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A periodic resync with the API is carried out to ensure an accurate view of
    the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To this end, make sure your reconciliation doesn’t make assumptions about the
    event that triggered reconciliation. Use the `status` field, determine relevant
    conditions in other resources as needed, and reconcile accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Admission webhooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you find your custom resource requires defaults or validation that cannot
    be implemented using the OpenAPI v3 spec in the CRD that creates your new API
    type, you can turn to validating and mutating admission webhooks. The Kubebuilder
    CLI has a `create webhook` command that caters specifically to these use cases
    by generating boilerplate to get you going faster.
  prefs: []
  type: TYPE_NORMAL
- en: An example of where a validating webhook may be useful with our Namespace operator
    example and its AcmeNamespace resource, is in validating the `adminUsername` field.
    As a convenience, your webhook could call out to your corporate identity provider
    to ensure the username provided is valid, preventing mistakes that require human
    intervention to correct.
  prefs: []
  type: TYPE_NORMAL
- en: A defaulting example could be to default the `deploymentTier` to the most common,
    least expensive `dev` option. This is particularly useful for maintaining backward
    compatibility with existing resource definitions when you make a change that adds
    new fields to a custom resource data model.
  prefs: []
  type: TYPE_NORMAL
- en: Admission webhooks are not often included in a prototype or pre-alpha release
    of an operator, but commonly come into play when refining the user experience
    for a stable release of a project. [Chapter 8](ch08.html#chapter8) covers the
    subject of admission control in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Finalizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have looked at examples of a custom resource being set as the owner of child
    resources to ensure they will be deleted when the parent custom resource is removed.
    However, this mechanism is not always sufficient. If the custom resource has relationships
    with other resources in the cluster where an ownership is *not* appropriate, or
    if conditions outside the cluster need to be updated when your custom resource
    is deleted, finalizers will likely be important to use.
  prefs: []
  type: TYPE_NORMAL
- en: Finalizers are added to the metadata of a resource as shown [Example 11-16](#ex_11-16).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-16\. AcmeNamespace manifest with a finalizer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: String value used as a finalizer.
  prefs: []
  type: TYPE_NORMAL
- en: The string value used as your finalizer is not important to anything else in
    the system besides your controller. Just use a value that will safely be unique
    in case other controllers need to apply finalizers to the same resource.
  prefs: []
  type: TYPE_NORMAL
- en: When any finalizers are present on a resource, the API server will not delete
    the resource. If a delete request is received, it will instead update the resource
    to add a `deletionTimestamp` field to its metadata. This update to the resource
    will trigger a reconciliation in your controller. A check for this `deletionTimestamp`
    will need to be added to your controller’s `Reconcile` method so that any pre-delete
    operations can be completed. Once complete, your controller can remove the finalizer.
    This will inform the API server that it may now delete the resource.
  prefs: []
  type: TYPE_NORMAL
- en: Common examples of pre-delete operations are in systems outside the cluster.
    In the Namespace operator example, if there are corporate chargeback systems that
    track Namespace usage and need to be updated when Namespaces are deleted, a finalizer
    could prompt your operator to update that external system before removing the
    Namespace. Other common examples are when a workload uses managed services, such
    as databases or object storage, as a part of the application stack. When an instance
    of the application is deleted, these managed service instances will likely need
    to be cleaned up as well.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the Scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scheduler delivers core functionality in Kubernetes. A huge part of the
    value proposition of Kubernetes is the abstraction of pools of machines on which
    to run workloads. It is the scheduler that makes the determination of where Pods
    will run. It is fair to say that, together with the kubelet, these two controllers
    form the heart of Kubernetes around which all else is built. The scheduler is
    a cornerstone platform service for your application platform. In this section
    we will explore customizing, extending, and replacing the behavior of the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: It’s helpful to keep in mind the parallels between core control plane components,
    like the scheduler, and the custom operators we have examined so far in this chapter.
    In both cases we are dealing with Kubernetes controllers managing Kubernetes resources.
    With our custom operators, we develop entirely new custom controllers, whereas
    the scheduler is a core controller deployed with every Kubernetes cluster. With
    our custom operators, we design and create new custom resources, whereas the scheduler
    manages the core Pod resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have found it uncommon for users of Kubernetes to find a need to extend
    the scheduler or modify its behavior. However, considering how important it is
    to a cluster’s function, it is prudent to both understand how the scheduler reaches
    scheduling decisions it makes and how to modify those decisions if the need arises.
    It bears repeating one more time: a large part of the genius of Kubernetes is
    its extensibility and modularity. If you find the scheduler doesn’t meet your
    needs, you can modify or augment its behavior, or replace it altogether.'
  prefs: []
  type: TYPE_NORMAL
- en: In exploring this topic, we’re going to examine how the scheduler determines
    where to assign Pods so that we can understand what goes into each scheduling
    decision, and then see how we can influence those decisions with scheduling policies.
    We’re also going to address the option of running multiple schedulers and even
    writing your own custom scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Predicates and Priorities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we look at how to extend or modify the scheduler, we first need to understand
    how the scheduler makes its decisions. The scheduler uses a two-step process to
    determine which Node a Pod will get scheduled to.
  prefs: []
  type: TYPE_NORMAL
- en: The first is filtering. In this step, the scheduler filters out Nodes that are
    ineligible to host a Pod using a number of predicates. For example, there is a
    predicate that checks to see if the Pod being scheduled tolerates a Node’s taints.
    The control plane nodes commonly use a taint to ensure regular workloads don’t
    get scheduled there. If a Pod has no tolerations, any tainted Nodes will be filtered
    out as ineligible targets for a Pod. Another predicate checks to ensure the Node
    has sufficient CPU and memory resources for any Pod that requests values for these
    resources. As you’d expect, if the Node has insufficient resources to satisfy
    the Pod spec, it is filtered out as an eligible host. When all predicates have
    checked for Node eligibility, the filtering step is complete. At this point if
    there are no eligible Nodes, the Pod will remain in a `Pending` state until conditions
    change, such as a new eligible Node being added to the cluster. If the list of
    Nodes consists of a single Node, scheduling may occur at this point. If there
    are multiple eligible Nodes, the scheduler proceeds to the second step.
  prefs: []
  type: TYPE_NORMAL
- en: The second step is scoring. This step uses priorities to determine which Node
    is the *best* fit for a particular Pod. One priority that helps to score a Node
    higher is the presence of a container image that is being used by the Pod. Another
    priority that will score a Node higher is the lack of any Pods that share the
    same Service as the Pod being scheduled. That is to say that the scheduler will
    attempt to distribute the Pods that share a Service across multiple Nodes for
    improved Node failure tolerance. The scoring step is also where `preferred...`
    affinity rules on Pods are implemented. At the end of the scoring step, each eligible
    Node has a score associated. The highest scoring Node is deemed the best fit for
    the Pod and it is scheduled there.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling Policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scheduling policies are used to configure the predicates and priorities the
    scheduler is to use. You can write a config file containing a scheduling policy
    to disk on the control plane nodes and provide the scheduler the `--policy-config-file`
    flag, but the preferred method is to use a ConfigMap. Provide the scheduler the
    `--policy-configmap` flag and thereafter you can update the scheduling policy
    via the API server. Note that if you go with the ConfigMap method, you will likely
    need to update the `system:kube-scheduler` ClusterRole to add a rule for getting
    ConfigMaps.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the time of this writing, both the `--policy-config-file` and `--policy-configmap`
    flags for the scheduler still work, but they are marked as deprecated in the official
    documentation. For this reason, if you are implementing new custom scheduling
    behavior, we recommend using the scheduling profiles discussed in the next section
    rather than the policies discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the policy ConfigMap in [Example 11-17](#ex_11-17) will make a
    Node eligible for selection with a `nodeSelector` by a Pod only if it has a label
    with the key: `selectable`.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-17\. An example ConfigMap defining a scheduling policy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_building_platform_services_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The filename the scheduler will expect to use for policies.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_building_platform_services_CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The predicate name that implements `nodeSelectors`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_building_platform_services_CO11-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The label key you wish to use for adding a constraint to selection. In this
    example, if a node does not have this label key present, it will not be selectable
    by a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_building_platform_services_CO11-4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This indicates the provided label must be present. If `false` it would need
    to be absent. If using the example configuration of `presence: true`, a Node without
    the label `selectable: ""` will not be eligible for selection by a Pod.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this scheduling policy in place, a Pod defined with the manifest in [Example 11-18](#ex_11-18)
    would be scheduled to an eligible Node only with *both* the `device: gpu` *and*
    the `selectable: ""` labels present.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-18\. A Pod manifest using the `nodeSelector` field to direct scheduling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Scheduling Profiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scheduling profiles allow you to enable or disable plug-ins that are compiled
    into the scheduler. You can specify a profile by passing a filename to the `--config`
    flag when running the scheduler. These plug-ins implement the various extension
    points that include—but are not limited to—the filter and scoring steps we covered
    earlier. In our experience it is rarely necessary to customize the scheduler in
    this way. But should you find a need for it, the Kubernetes documentation provides
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Schedulers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It should be noted that you are not limited to one scheduler. You can deploy
    any number of schedulers that are either Kubernetes schedulers with different
    policies and profiles, or even custom-built schedulers. If running multiple schedulers
    you can supply the `schedulerName` in the spec for a Pod, which will determine
    which scheduler carries out the scheduling for that Pod. Given the added complexity
    of following this multischeduler model, consider using dedicated clusters for
    workloads with such specialized scheduling requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the unlikely event that you are unable to use the Kubernetes scheduler, even
    with the use of policies and profiles, you have the option to develop and use
    your own scheduler. This would entail developing a controller that watches Pod
    resources and whenever a new Pod is created, determine where the Pod should run
    and update the `nodeName` field for that Pod. While this is a narrow scope, this
    is not a simple exercise. As we have seen in this section, the core scheduler
    is a sophisticated controller that routinely evaluates numerous complex factors
    into account when making scheduling decisions. If your requirements are specialized
    enough to demand a custom scheduler, it’s likely you will have to spend considerable
    engineering effort to refine its behavior. We recommend proceeding with this approach
    only if you have exhausted your options with the existing scheduler and have deep
    Kubernetes expertise to tap into for the project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s important to understand the points of extension available with Kubernetes
    and how best to add the platform services required to meet your tenants’ needs.
    Study the operator pattern and the use cases for Kubernetes operators. If you
    find a compelling need to build an operator, decide what development tooling and
    language you will use, design your custom resource’s data model, and then build
    a Kubernetes controller to manage that custom resource. Finally, if the default
    scheduler behavior does not meet your requirements, look into scheduling policies
    and profiles to modify its behavior. In extreme edge cases, you have the option
    to develop your own custom scheduler to replace, or run in conjunction with, the
    default scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Using the principles and practices laid out in this chapter, you are no longer
    constrained by the utilities and software provided by the community or your company’s
    vendors. If you encounter important requirements for which there is no existing
    solution, you have at your disposal the tools and guidelines to add any specialized
    platform services your business needs may require.
  prefs: []
  type: TYPE_NORMAL
