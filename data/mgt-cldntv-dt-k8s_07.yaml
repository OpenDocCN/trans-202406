- en: Chapter 6\. Integrating Data Infrastructure in a Kubernetes Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we are illuminating a future of modern, cloud native applications
    that run on Kubernetes. Up until this point, we’ve noted that historically, data
    has been one of the hardest parts of making this a reality. In previous chapters,
    we’ve introduced the primitives Kubernetes provides for managing compute, network,
    and storage ([Chapter 2](ch02.html#managing_data_storage_on_kubernetes)) resources,
    and considered how databases ([Chapter 3](ch03.html#databases_on_kubernetes_the_hard_way))
    can be deployed on Kubernetes using these resources. We’ve also examined the automation
    of infrastructure using controllers and the operator pattern ([Chapter 4](ch04.html#automating_database_deployment_on_kuber)).
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s expand our focus to consider how data infrastructure fits into your
    overall application architecture in Kubernetes. In this chapter, we’ll explore
    how to assemble the building blocks discussed in previous chapters into integrated
    data infrastructure stacks that are easy to deploy and tailor to the unique needs
    of each application. These stacks represent a step toward the vision of the virtual
    datacenter we introduced in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra).
    To learn the considerations involved in building and using these larger assemblies,
    let’s take an in-depth look at [K8ssandra](https://k8ssandra.io). This open source
    project provides an integrated data stack based on Apache Cassandra, a database
    we first discussed in [“Running Apache Cassandra on Kubernetes”](ch03.html#running_apache_cassandra_on_kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: 'K8ssandra: Production-Ready Cassandra on Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To set the context, let’s consider some of the practical challenges of moving
    application workloads into Kubernetes. As organizations have begun to migrate
    existing applications to Kubernetes and create new cloud native applications in
    Kubernetes, modernizing the data tier is a step that is often deferred. Whatever
    the causes of these delays—the belief that Kubernetes is not ready for stateful
    workloads, a lack of development resources, or other factors—the result has been
    mismatched architectures in which applications are running in Kubernetes with
    databases and other data infrastructure running externally. This leads to a division
    of focus for developers and SREs that can limit productivity. It’s also common
    to see distinct toolsets for monitoring applications and database infrastructure,
    which increases cloud computing costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This adoption challenge became evident in the Cassandra community. Despite
    the growing collaboration and consensus around building a single Cassandra operator
    as discussed in [Chapter 5](ch05.html#automating_database_management_on_kuber),
    developers were still confronted with key questions about how the database and
    operator would fit in the larger application context:'
  prefs: []
  type: TYPE_NORMAL
- en: How can you have an integrated view of the health of your entire stack, including
    both applications and data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you tailor the automation of installation, upgrades, and other operational
    tasks in a Kubernetes native way that fits the way we manage your Datacenters?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help address these questions, John Sanda and a team of engineers at DataStax
    launched an open source project called K8ssandra with the goal of providing a
    production-ready deployment of Cassandra that embodies best practices for running
    Cassandra in Kubernetes. K8ssandra provides custom resources that help manage
    tasks including cluster deployment, upgrades, scaling up and down, data backup
    and restore, and more. You can read more about the motivations for the project
    in Jeff Carpenter’s blog post [“Why K8ssandra?”](https://oreil.ly/dB6mJ).
  prefs: []
  type: TYPE_NORMAL
- en: K8ssandra Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K8ssandra is deployed in units known as *clusters*, which is similar terminology
    to that used by Kubernetes and Cassandra. A K8ssandra cluster includes a Cassandra
    cluster along with additional components depicted in [Figure 6-1](#keightssandra_architecture)
    to provide a full data management ecosystem. Let’s consider these in roughly clockwise
    order starting from the top center:'
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes operator first introduced in [Chapter 5](ch05.html#automating_database_management_on_kuber).
    It manages the lifecycle of Cassandra nodes on Kubernetes, including provisioning
    new nodes and storage, and scaling up and down.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra Reaper
  prefs: []
  type: TYPE_NORMAL
- en: This manages the details of repairing Cassandra nodes in order to maintain high
    data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra Medusa
  prefs: []
  type: TYPE_NORMAL
- en: Provides backup and restore for data stored in Cassandra.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus and Grafana
  prefs: []
  type: TYPE_NORMAL
- en: Used for the collection and visualization of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Stargate
  prefs: []
  type: TYPE_NORMAL
- en: A data gateway that provides API access to client applications as an alternative
    to CQL.
  prefs: []
  type: TYPE_NORMAL
- en: K8ssandra Operator
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrates all of the other components, including multicluster support for
    managing Cassandra clusters that span multiple Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![K8ssandra architecture](assets/mcdk_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. K8ssandra architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the following sections, we’ll take a look at each component of the K8ssandra
    project to understand the role that it plays within the architecture and its relationship
    to other components.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the K8ssandra Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s dive in with some hands-on experience of installing K8ssandra. To get
    a basic installation of K8ssandra running that fully demonstrates the power of
    the operator, you’ll need a Kubernetes cluster with several Worker Nodes.
  prefs: []
  type: TYPE_NORMAL
- en: To make the deployment simpler, the K8ssandra team has provided scripts to automate
    the process of creating Kubernetes clusters and then deploying the operator to
    these clusters. These scripts use [kind clusters](https://kind.sigs.k8s.io) for
    simplicity, so you’ll want to make sure you have this installed before starting.
  prefs: []
  type: TYPE_NORMAL
- en: Instructions for installing on various clouds are also available on the K8ssandra
    website. The instructions we provide here are based on an [installation guide](https://oreil.ly/4z2oH)
    in the K8ssandra Operator [repository](https://oreil.ly/kgeWY).
  prefs: []
  type: TYPE_NORMAL
- en: K8ssandra 2.0 Release Status
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focuses on the K8ssandra 2.0 release, including the K8ssandra Operator.
    At the time of writing, K8ssandra 2.0 is still in beta status. As K8ssandra 2.0
    moves toward a full GA release, the instructions on the [“Get Started” section
    of the K8ssandra website](https://oreil.ly/nT1n5) will be updated to reference
    the new version.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, start by cloning the K8ssandra operator repository from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you’ll want to use the provided Makefile to create a Kubernetes cluster
    and deploy the K8ssandra Operator into it (this assumes you have `make` installed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you examine the Makefile, you’ll notice the operator is installed using
    Kustomize, which we discussed in [“Additional Deployment Tools: Kustomize and
    Skaffold”](ch04.html#additional_deployment_tools_kustomize_a). The target you
    just executed creates a kind cluster with four Worker Nodes and changes your current
    context to point to that cluster, as you can see by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now examine the list of CRDs that have been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, several CRDs are associated with the cert-manager and K8ssandra.
    There is also the CassandraDatacenter CRD used by Cass Operator. The K8ssandra
    and Cass Operator CRDs are all Namespaced, which you can verify using the `kubectl
    api-resources` command, meaning that resources created according to these definitions
    are assigned to a specific Namespace. That command will also show you the acceptable
    abbreviations for each resource type (for example, `k8c` for `k8ssandracluster`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can examine the contents that have been installed within the kind
    cluster. If you list the Namespaces using `kubectl get ns`, you’ll note two new
    Namespaces: `cert-manager` and `k8ssandra-operator`. As you may suspect, K8ssandra
    is using the same cert-manager project as Pulsar, as described in [“Securing Communications
    by Default with cert-manager”](ch08.html#securing_communications_by_default_with).
    Let’s examine the contents of the `k8ssandra-operator` Namespace, which are summarized
    in [Figure 6-2](#keightssandra_operator_architecture) along with related K8ssandra
    CRDs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examine the workloads and you’ll notice that two Deployments have been created:
    one for the K8ssandra Operator and one for Cass Operator. Take a look at the K8ssandra
    Operator source code, and you’ll see that it contains multiple controllers, while
    the Cass Operator consists of a single controller. This packaging reflects the
    fact that Cass Operator is an independent project which can be used by itself
    without having to adopt the entire K8ssandra framework—otherwise, it could have
    been included as a controller within the K8ssandra Operator.'
  prefs: []
  type: TYPE_NORMAL
- en: '![K8ssandra Operator architecture](assets/mcdk_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. K8ssandra Operator architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 6-1](#mapping_keightssandra_crds_to_controlle) describes the mapping
    of these various controllers to the key resources with which they interact.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Mapping K8ssandra CRDs to controllers
  prefs: []
  type: TYPE_NORMAL
- en: '| Operator | Controller | Key custom resources |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| K8ssandra Operator | K8ssandra controller | `K8ssandraCluster`, `CassandraDatacenter`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Medusa controller | `CassandraBackup`, `CassandraRestore` |'
  prefs: []
  type: TYPE_TB
- en: '| Reaper controller | `Reaper` |'
  prefs: []
  type: TYPE_TB
- en: '| Replication controller | `ClientConfig`, `ReplicatedSecret` |'
  prefs: []
  type: TYPE_TB
- en: '| Stargate controller | `Stargate` |'
  prefs: []
  type: TYPE_TB
- en: '| Cass Operator | Cass Operator controller manager | `CassandraDatacenter`
    |'
  prefs: []
  type: TYPE_TB
- en: 'We’ll introduce each K8ssandra and Cass Operator CRD in more detail in the
    followiing sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a K8ssandraCluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you’ve installed the K8ssandra Operator, the next step is to create a
    K8ssandraCluster. The source code used in this section is available in the [“Vitess
    Operator Example” section of the book’s repository](https://oreil.ly/1n3k7), based
    on samples available in the [K8ssandra Operator GitHub repo](https://oreil.ly/5WxRO).
    First, have a look at the *k8ssandra-cluster.yaml* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code specifies a K8ssandraCluster resource consisting of a single Datacenter
    `dc1` running three nodes of Cassandra 4.0.1, where the Pod specification for
    each Cassandra node requests 1 GB of storage using a PersistentVolumeClaim that
    references the `standard` StorageClass. This configuration also includes a single
    Stargate node to provide API access to the Cassandra cluster. This is a minimal
    configuration that accepts the chart defaults for most of the other components.
    Create the `demo` K8ssandraCluster in the `k8ssandra-operator` Namespace with
    this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once the command completes, you can check on the installation of the K8ssandraCluster
    using commands such as `kubectl get k8ssandraclusters` (or `kubectl get k8c` for
    short). [Figure 6-3](#a_simple_keightssandracluster) depicts some of the key compute,
    network, and storage resources that the operator built on your behalf when you
    created the `demo` K8ssandraCluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![A simple K8ssandraCluster](assets/mcdk_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. A simple K8ssandraCluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are some key items to note:'
  prefs: []
  type: TYPE_NORMAL
- en: A single StatefulSet has been created to represent the Cassandra Datacenter
    `dc1`, with three Pods containing the replicas you specified. As you’ll learn
    in the next section, K8ssandra uses a CassandraDatacenter CRD to manage this StatefulSet
    via the Cass Operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the figure shows a single Service `demo-dc1-service` exposing access to
    the Cassandra cluster as a single endpoint, this is a simplification. You will
    find multiple Services configured to provide access for various clients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a Deployment managing a single Stargate Pod, as well as Services that
    provide client endpoints to the various API services provided by Stargate. This
    is another simplification, and we’ll explore this part of configuration in more
    detail in [“Enabling Developer Productivity with Stargate APIs”](#enabling_developer_productivity_with_st).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to examples of infrastructure we’ve shown in previous chapters, the
    K8ssandra Operator also creates additional supporting security resources such
    as ServiceAccounts, Roles, and RoleBindings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have a K8ssandraCluster created, you can point client applications
    at the Cassandra interfaces and Stargate APIs, and perform cluster maintenance
    operations. You can remove a K8ssandraCluster just by deleting its resource, but
    you won’t want to do that yet as we have a lot more to explore! We’ll describe
    several of these interactions as we examine each of the K8ssandra components in
    more detail. Along the way, we’ll make sure to note some of the interesting design
    choices made by contributors to K8ssandra and related projects in terms of how
    they use Kubernetes resources and how they adapt data infrastructure that predates
    Kubernetes into the Kubernetes way of doing things.
  prefs: []
  type: TYPE_NORMAL
- en: 'StackGres: An Integrated Kubernetes Stack for Postgres'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The K8ssandra project is not the only instance of an integrated data stack that
    runs on Kubernetes. Another example can be found in [StackGres](https://stackgres.io),
    a project managed by OnGres. StackGres uses [Patroni](https://github.com/zalando/patroni)
    to support clustered, highly available Postgres deployments and adds automated
    backup functionality. StackGres supports integration with Prometheus and Grafana
    for metrics aggregation and visualization, along with an optional Envoy proxy
    for getting more fine-grained metrics at the protocol level. StackGres is composed
    of open source components and uses the [AGPLv3 License](https://www.gnu.org/licenses/agpl-3.0.en.html)
    for its community edition.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Cassandra in Kubernetes with Cass Operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Cass Operator* is the shorthand name for the DataStax Kubernetes Operator
    for Apache Cassandra. This open source project available on [GitHub](https://oreil.ly/xWjZr)
    was brought under the umbrella of the K8ssandra project in 2021, replacing its
    previous home under the [DataStax GitHub organization](https://oreil.ly/JAF3Y).'
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator is a key part of K8ssandra, since a Cassandra cluster is the basic
    data infrastructure around which all the other infrastructure elements and tools
    are added. However, Cass Operator was developed before K8ssandra and will continue
    to exist as a separately deployable project. This is helpful since not every capability
    of Cass Operator is exposed via K8ssandra, especially more advanced Cassandra
    configuration options. Cass Operator is listed as its own project in [Operator
    Hub](https://oreil.ly/gPtl3) and can be installed via Kustomize.
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator provides a mapping of Cassandra’s topology concepts including
    clusters, Datacenters, racks, and nodes onto Kubernetes resources. The key construct
    is the CassandraDatacenter CRD, which represents a Datacenter within the topology
    of a Cassandra cluster. (Reference [Chapter 3](ch03.html#databases_on_kubernetes_the_hard_way)
    if you need a refresher on Cassandra topology.)
  prefs: []
  type: TYPE_NORMAL
- en: 'When you created a K8ssandraCluster resource in the previous section, the K8ssandra
    Operator created a single CassandraDatacenter resource, which would have looked
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Since you didn’t specify a rack in the K8ssandraCluster definition, K8ssandra
    interprets this as a single rack named `default`. By creating the CassandraDatacenter,
    K8ssandra Operator is delegating the operation of the Cassandra nodes in this
    Datacenter to Cass Operator.
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator and Multiple Datacenters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be wondering why Cass Operator does not define a CRD representing a
    Cassandra cluster. From the perspective of the Cass Operator, the Cassandra cluster
    is basically just a piece of metadata—the CassandraDatacenter’s `clusterName`—rather
    than an actual resource. This reflects the convention that Cassandra clusters
    used in production systems are typically deployed across multiple physical datacenters,
    which is beyond the scope of a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: While you can certainly create multiple CassandraDatacenters and link them together
    using the same `clusterName`, they must be in the same Kuberneters cluster for
    Cass Operator to be able to manage them. It’s also recommended to use a separate
    Namespace to install a dedicated instance of Cass Operator to manage each cluster.
    You’ll see how K8ssandra supports the ability to create Cassandra clusters that
    span multiple physical datacenters (and Kubernetes clusters) in multicluster topologies.
  prefs: []
  type: TYPE_NORMAL
- en: When Cass Operator is notified by the API server of the creation of the CassandraDatacenter
    resource, it creates resources used to implement the datacenter, including a StatefulSet
    to manage the nodes in each rack, as well as various Services and security-related
    resources. The StatefulSet will start the requested number of Pods in parallel.
    This brings up a situation in which Cass Operator provides logic to adapt between
    how Cassandra and Kubernetes operate.
  prefs: []
  type: TYPE_NORMAL
- en: If you have worked with Cassandra previously, you may be aware that the best
    practice for adding nodes to a cluster is to do so one one at a time, to simplify
    the process of a node joining the cluster. This process, called *bootstrapping*,
    includes the step of negotiating which data the node will be responsible for,
    and may include streaming data from other nodes to the new node. However, since
    the StatefulSet is not aware of these constraints, how can adding multiple nodes
    to a new or existing cluster one at a time be accomplished?
  prefs: []
  type: TYPE_NORMAL
- en: The answer lies in the composition of the Pod specification that Cass Operator
    passes to the StatefulSet, which is then used to create each Cassandra node, as
    shown in [Figure 6-4](#cass_operator_interactions_with_cassand).
  prefs: []
  type: TYPE_NORMAL
- en: '![Cass Operator interactions with Cassandra Pods](assets/mcdk_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Cass Operator interactions with Cassandra Pods
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Cass Operator deploys a custom image of Cassandra in each Cassandra Pod that
    it manages. The Pod specification includes at least two containers: an init container
    called `server-config-init` and a Cassandra container called `cassandra`.'
  prefs: []
  type: TYPE_NORMAL
- en: As an init container, `server-config-init` is started before the Cassandra container.
    It’s responsible for generating the *cassandra.yaml* configuration file based
    on the selected configuration options for the CassandraDatacenter. You can specify
    additional configuration values using the `config` section of the CassandraDatacenter
    resource, as described in the [K8ssandra documentation](https://oreil.ly/SlN0F).
  prefs: []
  type: TYPE_NORMAL
- en: Additional Sidecar Containers in Cassandra Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you’ll learn in the following sections, the Cassandra Pod may have additional
    sidecar containers when deployed in a K8ssandraCluster, depending on which of
    the additional K8ssandra components you have enabled. For right now, though, we
    are focusing on the most basic installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cassandra` container actually contains two separate processes: the daemon
    that runs the Cassandra instance and a Management API. This goes somewhat against
    the traditional best practice of running a single process per container, but there
    is a good reason for this exception.'
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra’s management interface is exposed via the Java Management Extensions
    (JMX). While this was a legitimate design choice for a Java-based application
    like Cassandra when the project was just starting out, JMX has fallen out of favor
    because of its complexity and security issues. While there has been some progress
    toward an alternate management interface for Cassandra, the work is not yet complete,
    so the developers of Cass Operator decided to integrate another open source project,
    the [Management API for Apache Cassandra](https://oreil.ly/1XIPi).
  prefs: []
  type: TYPE_NORMAL
- en: The Management API project provides a RESTful API that translates HTTP-based
    invocations into calls on Cassandra’s legacy JMX interface. By running the Management
    API inside the Cassandra container, we avoid having to expose the JMX port outside
    of the Cassandra containers. This is an instance of a pattern frequently used
    in cloud native architectures to adapt custom protocols into HTTP-based interfaces,
    for which there is much better support for routing and security in ingress controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator discovers and connects to the Management API on each Cassandra
    Pod in order to perform management operations that are not related to Kubernetes.
    When adding new nodes, this involves the simple action of using the Management
    API to verify that the node is up and running successfully and updating the CassandraDatacenter’s
    status accordingly. This sequence is described in more detail in the [K8ssandra
    documentation](https://oreil.ly/T7io1).
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the Cassandra Image Used by Cass Operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Management API project provides images for recent Cassandra versions in
    the 3.*x* and 4.*x* series, which are available on [Docker Hub](https://oreil.ly/FQa3q).
    While it is possible to override the Cassandra image that Cass Operator uses with
    one of your own, Cass Operator does require that the Management API is available
    on each Cassandra Pod. If you need to build your own custom image including the
    Management API, you could use the Dockerfiles and supporting scripts from the
    [GitHub repository](https://oreil.ly/GKRP1) as a starting point.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this section focused largely on the startup and scaling of Cassandra
    clusters just described, Cass Operator provides several features for deploying
    and managing Cassandra clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Topology management
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator uses Kubernetes affinity principles to manage the placement of
    Cassandra nodes (Pods) across Kubernetes Worker Nodes to maximize availability
    of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling down
  prefs: []
  type: TYPE_NORMAL
- en: Just as nodes are added one at a time to scale up, Cass Operator manages scaling
    down one node at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing nodes
  prefs: []
  type: TYPE_NORMAL
- en: If a Cassandra node is lost because it crashes or the Worker Node on which it
    is running goes down, Cass Operator relies on the StatefulSet to replace the node
    and bind the new node to the appropriate PersistentVolumeClaim.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading images
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator also leverages the capabilities of StatefulSet to perform rolling
    upgrades of the images used by the Cassandra Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Managing seed nodes
  prefs: []
  type: TYPE_NORMAL
- en: Cass Operator creates Kubernetes Services to expose the seed nodes in each Datacenter
    according to Cassandra’s recommended conventions of one seed node per rack, for
    a minimum of three per Datacenter.
  prefs: []
  type: TYPE_NORMAL
- en: You can read about these and other features in the [Cass Operator documentation](https://oreil.ly/UafkF).
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Developer Productivity with Stargate APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our focus so far in this book has been primarily on deployment of data infrastructure
    such as databases in Kubernetes, more than on the way that infrastructure is used
    in cloud native applications. The usage of [Stargate](https://stargate.io) in
    K8ssandra gives us a good opportunity to have that discussion.
  prefs: []
  type: TYPE_NORMAL
- en: In many organizations, there is an ongoing conversation about the pros and cons
    of direct application access to databases versus abstracting the details of database
    interactions. This debate occurs especially frequently in larger organizations
    that divide responsibilities between application development teams and teams that
    manage platforms including data infrastructure. However, it can also be observed
    in organizations that employ modern practices including DevOps and microservice
    architectures, where each microservice may have a different data store behind
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of providing abstractions over direct database access has taken many
    forms over the years. Even in the days of monolithic client-server applications,
    it was common to use stored procedures or isolate data access and complex query
    logic behind object-relational mapping tools such as Hibernate, or to use patterns
    like data access objects (DAOs).
  prefs: []
  type: TYPE_NORMAL
- en: More recently, as the software industry has moved toward service oriented architecture
    (SOA) and microservices, similar patterns for abstracting data access have appeared.
    As described in Jeff’s article [“Data Services for the Masses”](https://oreil.ly/u6K58),
    many teams have found themselves creating a layer of microservices in their architecture
    dedicated to data access, providing create, read, update, and delete (CRUD) operations
    on specific data types or entities. These services abstract the details of interacting
    with a specific database backend, and if well executed and maintained, can help
    increase developer productivity and facilitate migration to a different database
    when needed.
  prefs: []
  type: TYPE_NORMAL
- en: The Stargate project was born out of the realization that multiple teams were
    building very similar abstraction layers to provide data access via APIs. The
    goal of the Stargate project is to provide an open source *data API gateway—*a
    common set of APIs for data access to help eliminate the need for teams to develop
    and maintain their own custom API layers. While the initial implementation of
    Stargate is based on Cassandra, the goal of the project is to support multiple
    database backends, and even other types of data infrastructure such as caches
    and streaming.
  prefs: []
  type: TYPE_NORMAL
- en: With Cassandra used as the backend data store, the Stargate architecture can
    be described as having three layers, as shown in [Figure 6-5](#stargate_conceptual_architecture_with_c).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stargate conceptual architecture with Cassandra](assets/mcdk_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Stargate conceptual architecture with Cassandra
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *API layer* is the outermost layer, consisting of services that implement
    various APIs on top of the underlying Cassandra cluster. Available APIs include
    a [REST API](https://oreil.ly/qTEY6), a [Document API](https://oreil.ly/ekhlV)
    that provides access to JSON documents over HTTP, a [GraphQL API](https://oreil.ly/BescX),
    and a [gRPC API](https://oreil.ly/k2fNY). The *routing layer* (or *coordination
    layer*) consists of a set of nodes that act as Cassandra nodes, but perform only
    routing of queries, not data storage. The *storage layer* consists of a traditional
    Cassandra cluster, which can currently be Cassandra 3.11, Cassandra 4.0, or DataStax
    Enterprise 6.8.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the key benefits of this architecture is that it recognizes the separation
    of concerns for managing usage of compute and storage resources and provides the
    ability to scale this usage independently based on the needs of client applications:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of storage nodes can be scaled up or down to provide the storage
    capacity required by the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of coordinator nodes and API instances can be scaled up or down to
    match the application’s read and write load and optimize throughput.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APIs that are not used by the application can be scaled to zero (disabled) to
    reduce resource consumption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K8ssandra supports the provision of Stargate on top of an underlying Cassandra
    cluster via the Stargate CRD. The CassandraDatacenter deployed by Cass Operator
    serves as the storage layer, and the Stargate CRD specifies the configuration
    of the routing and API layers. An example configuration is shown in [Figure 6-6](#stargate_deployment_on_kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stargate deployment on Kubernetes](assets/mcdk_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Stargate deployment on Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The installation includes a Deployment to manage the coordinator nodes, and
    a Service to provide access to the Bridge API, a private gRPC interface exposed
    on the coordinator nodes that can be used to create new API implementations. See
    the [Stargate v2 design](https://oreil.ly/6ct5m) for more details on the Bridge
    API. There is also a Deployment for each of the APIs that is enabled in the installation,
    along with a Service to provide access to client applications.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Stargate project provides a promising framework for extending
    your data infrastructure with developer-friendly APIs that can scale along with
    the underlying database.
  prefs: []
  type: TYPE_NORMAL
- en: Unified Monitoring Infrastructure with Prometheus and Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve considered the addition of infrastructure that makes life easier
    for application developers, let’s look at some of the more operations-focused
    aspects of integrating data infrastructure in a Kubernetes stack. We’ll start
    with monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observability is a key attribute of any application deployed on Kubernetes,
    since it has implications for your awareness of its availability, performance,
    and cost. Your goal should be to have an integrated view across both your application
    and the infrastructure it depends on. Observability is often described as consisting
    of three types of data: metrics, logs, and tracing. Kubernetes itself provides
    capabilities for logging as well as associating events with resources, and you’ve
    already learned how the Cass Operator facilitates the collection of logs from
    Cassandra nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll focus on how K8ssandra incorporates the Prometheus/Grafana
    stack, which provides metrics. *Prometheus* is a popular open source monitoring
    platform. It supports a variety of interfaces for collecting data from applications
    and services and stores them in a time series database which can be queried efficiently
    using the Prometheus Query Language (PromQL). It also includes an Alertmanager
    which generates alerts and other notifications based on metric thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: While previous releases of K8ssandra in the 1.*x* series incorporated the Prometheus
    stack as part of a K8ssandra, K8ssandra 2.*x* provides the capability to integrate
    with an existing Prometheus installation.
  prefs: []
  type: TYPE_NORMAL
- en: One easy way to install the Prometheus Operator is to use [kube-prometheus](https://oreil.ly/tzDmJ),
    a repository provided as part of the Prometheus Operator project. Kube-prometheus
    is intended as a comprehensive monitoring stack for Kubernetes including the control
    plane and applications. You can clone this repository and use the library of manifests
    (YAML files) that it contains to install the integrated stack of components shown
    in [Figure 6-7](#components_of_the_kube_prometheus_stack).
  prefs: []
  type: TYPE_NORMAL
- en: '![Components of the kube-prometheus stack](assets/mcdk_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Components of the kube-prometheus stack
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These components include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus Operator
  prefs: []
  type: TYPE_NORMAL
- en: The operator, which is set apart in the figure, manages the other components.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs: []
  type: TYPE_NORMAL
- en: The metrics database is run in a high-availability configuration managed via
    a StatefulSet. Prometheus stores data using a time series database with a backing
    PersistentVolume.
  prefs: []
  type: TYPE_NORMAL
- en: Node exporter
  prefs: []
  type: TYPE_NORMAL
- en: The [node exporter](https://oreil.ly/Yt0YO) runs on each Kubernetes Worker Node,
    allowing Prometheus to pull operating system metrics via HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: Client library
  prefs: []
  type: TYPE_NORMAL
- en: Applications can embed a Prometheus client library, which allows Prometheus
    to pull metrics via HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: Alert manager
  prefs: []
  type: TYPE_NORMAL
- en: This can be configured to generate alerts based on thresholds for specific metrics
    for delivery via email or third-party tools such as PagerDuty. The kube-prometheus
    stack comes with built-in alerts for the Kubernetes cluster; application-specific
    alerts can also be added.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs: []
  type: TYPE_NORMAL
- en: This is deployed to provide charts that are used to display metrics to human
    operators. Grafana uses PromQL to access metrics from Prometheus, and this interface
    is available to other clients as well.
  prefs: []
  type: TYPE_NORMAL
- en: While not shown in the figure, the stack also includes the [Prometheus Adapter
    for Kubernetes Metrics APIs](https://oreil.ly/g033n), an optional component that
    exposes metrics collected by Prometheus to the Kubernetes control plane so that
    they can be used to auto-scale applications.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting K8ssandra with Prometheus can be accomplished in a few quick steps.
    The [instructions](https://oreil.ly/UOt4t) in the K8ssandra documentation walk
    you through installing the Prometheus Operator using kube-prometheus if you do
    not have it already. Since kube-prometheus installs Prometheus Operator in its
    own Namespace, you’ll want to make sure the operator has permissions to manage
    resources in other Namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate K8ssandra with Prometheus, you set attributes on your K8ssandraCluster
    resource to enable monitoring on Cassandra and Stargate nodes. For example, you
    could do something like the following to enable monitoring for nodes in all Datacenters
    in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It’s also possible to selectively enable monitoring on individual datacenters.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at how the integration works. First, let’s consider how the
    Cassandra nodes expose metrics. As discussed in [“Managing Cassandra in Kubernetes
    with Cass Operator”](#managing_cassandra_in_kubernetes_with_c), Cassandra exposes
    management capabilities via JMX, and this includes metrics reporting. The [Metric
    Collector for Apache Cassandra (MCAC)](https://oreil.ly/CHNMQ) is an open source
    project that exposes metrics so that they can be accessed by Prometheus or other
    backends that use the Prometheus protocol via HTTP. K8ssandra and Cass Operator
    use a Cassandra Docker image that includes MCAC as well as the Management API
    as additional processes that run in the Cassandra container. This configuration
    is shown on the left side of [Figure 6-8](#monitoring_cassandra_with_kube_promethe).
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Cassandra with kube-prometheus stack](assets/mcdk_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Monitoring Cassandra with the kube-prometheus stack
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The right side of [Figure 6-8](#monitoring_cassandra_with_kube_promethe) shows
    how Prometheus and Grafana are configured to consume and expose the Cassandra
    metrics. The K8ssandra Operator creates ServiceMonitor resources for each CassandraDatacenter
    for which monitoring has been enabled. The ServiceMonitor, a CRD defined by the
    Prometheus Operator, contains configuration details describing how to collect
    metrics from a set of Pods, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A `selector` referencing the name of a label which identifies the Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connection information such as the `scheme` (protocol), `port`, and `path` to
    use to gather metrics from each Pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `interval` at which metrics should be pulled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional `metricRelabelings`, which are instructions that indicate any desired
    renaming of metrics, or even indicate metrics that should be dropped and not ingested
    by Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K8ssandra creates separate ServiceMonitor instances for Cassandra and Stargate
    nodes, since the metrics exposed are slightly different. To observe the ServiceMonitors
    deployed in your cluster, you can execute a command such as `kubectl get servicemonitors
    -n monitoring`.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus provides access to its metrics to Grafana and other tools via a PromQL
    endpoint exposed as a Kubernetes service. The kube-prometheus installation configures
    an instance of Grafana to connect to Prometheus using an instance of the Grafana
    Datasource CRD. Grafana accepts dashboards defined using YAML files, which you
    can provide as ConfigMaps. See the K8ssandra [documentation](https://oreil.ly/vCfmR)
    for guidance on loading dashboard definitions that display Cassandra and Stargate
    metrics. You may also wish to create dashboards that display your application
    metrics alongside the data tier metrics provided by K8ssandra for an integrated
    view of application performance.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, kube-prometheus provides a comprehensive and extensible monitoring
    stack for Kubernetes clusters, much as K8ssandra provides a stack for data management.
    The integration of K8ssandra with kube-prometheus is a great example of how you
    can assemble integrated stacks of Kubernetes resources to form even more powerful
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Performing Repairs with Cassandra Reaper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a NoSQL database, Cassandra emphasizes high performance (especially for writes)
    and high availability by default. If you’re familiar with the CAP theorem, you’ll
    understand that this means that sometimes Cassandra will temporarily sacrifice
    consistency of data across nodes in order to deliver this high performance and
    high availability at scale, an approach known as *eventual consistency*. Cassandra
    does provide the ability to tune the amount of consistency to your needs via options
    for specifying replication strategies and the consistency level required per query.
    Users and administrators should be aware of these options and their behavior in
    order to use Cassandra effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra has multiple built-in “anti-entropy” mechanisms such as hinted handoff
    and repair that help maintain consistency of data between nodes over time. Repair
    is a background process by which a node compares a portion of the data it owns
    with the latest contents of other nodes that are also responsible for that data.
    While these checks can be somewhat optimized through the use of checksums, repair
    can still be a performance-intensive process and is best performed when a cluster
    is under reduced or off-peak load. Combined with the fact that multiple options
    are available, including full and incremental repairs, executing repairs has traditionally
    required some tailoring for each cluster. It also has tended to be a manual process
    that was unfortunately frequently neglected by some Cassandra cluster administrators.
  prefs: []
  type: TYPE_NORMAL
- en: More Detail on Repairs in Cassandra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a deeper treatment of repair, see [*Cassandra: The* *Definitive* *Guide*](https://learning.oreilly.com/library/view/cassandra-the-definitive/9781492097136),
    where repair concepts and the available options are described in Chapters 6 and
    12, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Cassandra Reaper](http://cassandra-reaper.io) was created to take the difficulty
    out of executing repairs on Cassandra clusters and optimize repair performance
    to minimize the impact of running repairs on heavily used clusters. Reaper was
    created by Spotify and enhanced by The Last Pickle, which currently manages the
    project on [GitHub](https://oreil.ly/2MttB). Reaper exposes a RESTful API for
    configuring repair schedules for one or more Cassandra clusters, and also provides
    a command-line tool and web interface which guides administrators through the
    process of creating schedules.'
  prefs: []
  type: TYPE_NORMAL
- en: K8ssandra provides the option to incorporate an instance of Cassandra Reaper
    as part of a K8ssandraCluster. The K8ssandra Operator includes a Reaper controller
    that is responsible for managing the local Cassandra Reaper manager process through
    its associated Reaper CRD. By default, enabling Reaper in a K8ssandraCluster will
    cause an instance of Reaper to be installed in each Kubernetes cluster represented
    in the installation, but you can also use a single instance of Reaper to manage
    repairs across multiple Datacenters, or even across multiple Cassandra clusters,
    provided they are accessible via the network.
  prefs: []
  type: TYPE_NORMAL
- en: Backing Up and Restoring Data with Cassandra Medusa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing backups is an important part of maintaining high availability and disaster
    recovery planning for any system that stores data. Cassandra supports both full
    and differential backups by creating hard links to the SSTable files it uses for
    data persistence. Cassandra itself does not take responsibility for copying the
    SSTable files to backup storage. Instead, this is left to the user. Similarly,
    recovering from backup involves copying the SSTable files to the Cassandra node
    where the data is to be reloaded; then Cassandra can be pointed to the local files
    to restore their contents.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra’s backup and restore operations are traditionally executed on individual
    nodes using nodetool, a command-line tool that leverages Cassandra’s JMX interface.
    [Cassandra Medusa](https://oreil.ly/tmP91) is an open source command-line tool
    created by Spotify and The Last Pickle that executes nodetool commands to perform
    backups, including synchronization of backups across multiple nodes. Medusa supports
    Amazon S3, Google Cloud Storage (GCS), Azure Storage, and S3-compatible storage
    such as MinIO and Ceph Object Gateway, and can be extended to support other storage
    providers via the Apache Libcloud project.
  prefs: []
  type: TYPE_NORMAL
- en: Medusa can restore either individual nodes to support fast replacement of a
    downed node, or entire clusters in a disaster recovery scenario. Restoring to
    a cluster can either be to the original cluster or to a new cluster. Medusa is
    able to restore data to a cluster with a different size or topology than the original
    cluster, which has traditionally been a challenge to figure out manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'K8ssandra has incorporated Medusa in order to provide backup and restore capabilities
    for Cassandra clusters running in Kubernetes. To configure the use of Medusa in
    a K8ssandraCluster, you’ll want to configure the `medusa` properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The options shown here include the storage provider, the bucket to use for backups,
    an optional prefix to add to directory names used to organize backup files, and
    the name of a Kubernetes Secret containing login credentials for the bucket. See
    the [documentation](https://oreil.ly/ujZYw) for details on the contents of the
    Secret. Other available options include enabling SSL on the bucket connection,
    and setting the policies for purging old backups such as a maximum age or number
    of backups.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Backup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the K8ssandraCluster has been started, you can create backups using the
    CassandraBackup CRD. For example, you could initiate a backup of the CassandraDatacenter
    `dc1` using a command like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The steps in processing of this resource are shown in [Figure 6-9](#performing_a_datacenter_backup_using_me).
  prefs: []
  type: TYPE_NORMAL
- en: '![Performing a Datacenter backup using Medusa](assets/mcdk_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Performing a Datacenter backup using Medusa
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you apply the resource definition (1), `kubectl` registers the resource
    with the API Server (2). The API server notifies the Medusa Controller running
    as part of the K8ssandra Operator (3).
  prefs: []
  type: TYPE_NORMAL
- en: The Medusa Controller contacts a sidecar container (4), which K8ssandra has
    injected into the Cassandra Pod because you chose to enable Medusa on the K8ssandraCluster.
    The Medusa sidecar container uses nodetool commands to a backup on the Cassandra
    node via JMX (5) (the JMX interface is exposed only within the Pod).
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra performs a backup (6), marking the SSTable files on the PersistentVolume
    that mark the current snapshot. The Medusa sidecar copies the snapshot files from
    the PV to the bucket (7). Steps 4–7 are repeated for each Cassandra Pod in the
    CassandraDatacenter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can monitor the progress of the backup by checking the status of the resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You’ll know the backup is complete when the `finishTime` attribute is populated.
    The Pods that have been backed up are listed under the `finished` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring from Backup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of restoring data from a backup is similar. To restore an entire
    Datacenter from backed-up data, you could create a CassandraRestore resource like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When the Medusa Controller is notified of the new resource, it locates the CassandraDatacenter
    and updates the Pod spec template within the StatefulSet that is managing the
    Cassandra Pods. The updates consist of adding a new init container called `medusa-restore`
    and setting environment variables that `medusa-restore` will use to locate the
    datafiles that are to be restored. The update to the Pod spec template causes
    the StatefulSet controller to perform a rolling update of the Cassandra Pods in
    the StatefulSet. As each Pod restarts, `medusa-restore` copies the files from
    object storage onto the PersistentVolume for the node, and then the Cassandra
    container starts as usual. You can monitor the progress of the restore by checking
    the status of the CassandraRestore resource.
  prefs: []
  type: TYPE_NORMAL
- en: A Common Language for Data Recovery?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is interesting to note the similarities and differences between the ways
    backup and restore operations are supported by the K8ssandra Operator we’ve discussed
    in this chapter and the Vitess Operator discussed in [Chapter 5](ch05.html#automating_database_management_on_kuber).
  prefs: []
  type: TYPE_NORMAL
- en: In K8ssandra, the CassandraBackup and CassandraRestore resources function in
    a manner similar to Kubernetes Jobs—they represent a task that you would like
    to have performed as well as the results of the task. In contrast, the VitessBackup
    resource represents a record of a backup that the Vitess Operator has performed
    based on the configuration of a VitessCluster resource. There is no equivalent
    resource to the CassandraRestore operator in Vitess.
  prefs: []
  type: TYPE_NORMAL
- en: Although K8ssandra and Vitess differ significantly in their approach to managing
    backups, both represent each backup task as a resource. Perhaps this common ground
    could be the starting point toward the development of common resource definitions
    for backup and restore operations, helping fulfill the vision introduced in [Chapter 5](ch05.html#automating_database_management_on_kuber).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the behavior of Cassandra Reaper, a single instance of Medusa can
    be configured to manage backup and restore operations across multiple Datacenters
    or Cassandra clusters. See the K8ssandra [documentation](https://oreil.ly/Y2EkE)
    for more details on performing backup and restore operations with Medusa.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Multicluster Applications in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main selling points of a distributed database like Cassandra is its
    ability to support deployments across multiple Datacenters. Many users take advantage
    of this in order to promote high availability across geographically distributed
    Datacenters, to provide lower-latency reads and writes for applications and their
    users.
  prefs: []
  type: TYPE_NORMAL
- en: However, Kubernetes itself was not originally designed to support applications
    that span multiple Kubernetes clusters. This has traditionally meant that creating
    such multiregion applications leaves a lot of work to development teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'This work takes two main forms: creating the network infrastructure to connect
    the Kubernetes clusters, and coordinating interactions between resources in those
    clusters. Let’s examine these requirements and the implications for an application
    like Cassandra:'
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster networking requirements
  prefs: []
  type: TYPE_NORMAL
- en: From a networking perspective, the key is to have secure, reliable networking
    between Datacenters. If you’re using a single cloud provider for your application,
    this may be relatively simple to achieve using VPC capabilities offered by the
    major cloud vendors.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using multiple clouds, you’ll need a third-party solution. For the
    most part, Cassandra requires routable IPs between its nodes and does not rely
    on name resolution, but it is helpful to have DNS in place as well to simplify
    the process of managing Cassandra’s seed nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Jeff’s blog post [“Deploy a Multi-Data Center Cassandra Cluster in Kubernetes”](https://oreil.ly/HpCYX)
    describes an example configuration in Google Cloud Platform (GCP) using the CloudDNS
    service, while Raghavan Srinivas’s blog post [“Multi-Region Cassandra on EKS with
    K8ssandra and Kubefed”](https://oreil.ly/9byYo) describes a similar configuration
    on Amazon EKS.
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster resource coordination requirements
  prefs: []
  type: TYPE_NORMAL
- en: Managing an application that spans multiple Kubernetes clusters means that there
    are distinct resources in each cluster which have no relationship to resources
    in other clusters that the Kubernetes control plane is aware of. To manage the
    lifecycle of an application including deployment, upgrade, scaling up and down,
    and teardown, you need to coordinate resources across multiple Datacenters.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes Cluster Federation project ([KubeFed](https://oreil.ly/yvUCm)
    for short) provides one approach to providing a set of APIs for managing resources
    across clusters that can be leveraged to build multicluster applications. This
    includes mechanisms that represent Kubernetes clusters themselves as resources.
    While KubeFed is still in beta, the K8ssandra Operator uses a similar design approach
    for managing resources across clusters. We’ll examine this in more detail in [“Kubernetes
    Cluster Federation”](#kubernetes_cluster_federation).
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve a multicluster Kubernetes deployment of Cassandra, you’ll need to
    establish networking between Datacenters according to your specific situation.
    Given that foundation, the K8ssandra Operator provides the facilities to manage
    the lifecycle of resources across the Kubernetes clusters. For a simple example
    of deploying a multiregion K8ssandraCluster, use the [instructions](https://oreil.ly/bmcil)
    found in the K8ssandra documentation, again using the Makefile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This builds two kind clusters, deploys the K8ssandra Operator in each of them,
    and creates a multicluster K8ssandraCluster. One advantage of using kind for a
    simple demonstration is that Docker provides the networking between clusters.
    We’ll walk through some of the key steps in this process in order to describe
    how the K8ssandra Operator accomplishes this work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The K8ssandra Operator supports two modes of installation: control plane (the
    default) and data plane. For a multicluster deployment, one Kubernetes cluster
    must be designated as the control plane cluster, and the others as data plane
    clusters. The control plane cluster can optionally include a CassandraDatacenter,
    as in the configuration shown in [Figure 6-10](#keightssandra_multicluster_architecture).'
  prefs: []
  type: TYPE_NORMAL
- en: '![K8ssandra multicluster architecture](assets/mcdk_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. K8ssandra multicluster architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When installed in control plane mode, the K8ssandra Operator uses two additional
    CRDs to manage multicluster deployments: ReplicatedSecret and ClientConfig. You
    can see evidence of the ClientConfig in the K8ssandraCluster configuration that
    was used, which looks something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This configuration specifies a K8ssandraCluster `demo` consisting of two CassandraDatacenters,
    `dc1` and `dc2`. Each Datacenter has its own configuration so that you can select
    a different number of Cassandra and Stargate nodes, or different resource allocations
    for the Pods. In the `demo` configuration, `dc1` is running in the control plane
    cluster `kind-k8ssandra-0`, and `dc2` is running in the data plane cluster `kind-k8ssandra-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the `k8sContext: kind-k8ssandra-1` line in the configuration. This is
    a reference to a ClientConfig resource that was created by the `make` command.
    A ClientConfig is a resource that represents the information needed to connect
    to the API server of another cluster, similar to the way `kubectl` stores information
    about different clusters on your local machine. The ClientConfig resource references
    a Secret that is used to store access credentials securely. The K8ssandra Operator
    repo includes a [convenience script](https://oreil.ly/wPINU) that can be used
    to create ClientConfig resources for Kubernetes clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: When you create a K8ssandraCluster in the control plane cluster, it uses the
    ClientConfigs to connect to each remote Kubernetes cluster in order to create
    the specified resources. For the preceding configuration, this includes CassandraDatacenter
    and Stargate resources, but can also include other resources such as Medusa and
    Prometheus ServiceMonitor.
  prefs: []
  type: TYPE_NORMAL
- en: The ReplicatedSecret is another resource involved in sharing access credentials.
    The control plane K8ssandra Operator uses this resource to keep track of Secrets
    that it creates in each remote cluster. These Secrets are used by the various
    K8ssandra components to securely communicate information such as the default Cassandra
    administrator credentials with each other. The K8ssandra Operator creates and
    manages ReplicatedSecret resources itself; you don’t need to interact with them.
  prefs: []
  type: TYPE_NORMAL
- en: The K8ssandraCluster, ClientConfig, and ReplicatedSecret resources exist only
    in the control plane cluster, and when the K8ssandra Operator is deployed in data
    plane mode, it does not even run the controllers associated with those resource
    types.
  prefs: []
  type: TYPE_NORMAL
- en: More Detail on the K8ssandra Operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a quick summary of a complex design for a multicluster operator. For
    more details on the approach, see the K8ssandra Operator [architecture overview](https://oreil.ly/ACAD2)
    and John Sanda’s [presentation](https://oreil.ly/RMK3E) at the Data on Kubernetes
    Community (DoKC) meetup.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s consider a more general approach to building multicluster applications
    that we can compare and contrast with K8ssandra’s approach.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is a lot of potential for growth in the area of Kubernetes
    federation and the ability to manage resources across Kubernetes cluster boundaries.
    For example, as a database whose primary superpower is running across multiple
    Datacenters, Cassandra seems like a great match for a multicluster solution like
    KubeFed.
  prefs: []
  type: TYPE_NORMAL
- en: The K8ssandra Operator and KubeFed have taken similar architectural approaches,
    where custom “federated” resources provide templates used to define resources
    in other clusters. This commonality points to the possibility for future collaboration
    across these projects and others based on similar design principles. Perhaps in
    the future, CRDs like K8ssandra’s ClientConfig and ReplicatedSecret can be replaced
    by equivalent functionality provided by KubeFed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned how data infrastructure can be composed with
    other infrastructure to build reusable stacks on Kubernetes. Using the K8ssandra
    project as an example, you’ve learned about aspects including integrating data
    infrastructure with API gateways and monitoring solutions to provide more full-featured
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve also learned some of the opportunities and challenges with adapting existing
    technologies onto Kubernetes and creating multicluster data infrastructure deployments.
    In the next chapter, we’ll explore how to design new cloud native data infrastructure
    that takes advantage of everything that Kubernetes provides without requiring
    adaptation and discover what new possibilities that opens up.
  prefs: []
  type: TYPE_NORMAL
