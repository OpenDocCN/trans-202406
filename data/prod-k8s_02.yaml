- en: Chapter 1\. A Path to Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the years, the world has experienced wide adoption of Kubernetes within
    organizations. Its popularity has unquestionably been accelerated by the proliferation
    of containerized workloads and microservices. As operations, infrastructure, and
    development teams arrive at this inflection point of needing to build, run, and
    support these workloads, several are turning to Kubernetes as part of the solution.
    Kubernetes is a fairly young project relative to other, massive, open source projects
    such as Linux. Evidenced by many of the clients we work with, it is still early
    days for most users of Kubernetes. While many organizations have an existing Kubernetes
    footprint, there are far fewer that have reached production and even less operating
    at scale. In this chapter, we are going to set the stage for the journey many
    engineering teams are on with Kubernetes. Specifically, we are going to chart
    out some key considerations we look at when defining a path to production.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is Kubernetes a platform? Infrastructure? An application? There is no shortage
    of thought leaders who can provide you their precise definition of what Kubernetes
    is. Instead of adding to this pile of opinions, let’s put our energy into clarifying
    the problems Kubernetes solves. Once defined, we will explore how to build atop
    this feature set in a way that moves us toward production outcomes. The ideal
    state of “Production Kubernetes” implies that we have reached a state where workloads
    are successfully serving production traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name *Kubernetes* can be a bit of an umbrella term. A quick browse on GitHub
    reveals the `kubernetes` organization contains (at the time of this writing) 69
    repositories. Then there is `kubernetes-sigs`, which holds around 107 projects.
    And don’t get us started on the hundreds of Cloud Native Compute Foundation (CNCF)
    projects that play in this landscape! For the sake of this book, *Kubernetes*
    will refer exclusively to the core project. So, what is the core? The core project
    is contained in the [kubernetes/kubernetes](https://github.com/kubernetes/kubernetes)
    repository. This is the location for the key components we find in most Kubernetes
    clusters. When running a cluster with these components, we can expect the following
    functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling workloads across many hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing a declarative, extensible, API for interacting with the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a CLI, `kubectl`, for humans to interact with the API server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconciliation from current state of objects to desired state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a basic service abstraction to aid in routing requests to and from
    workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing multiple interfaces to support pluggable networking, storage, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These capabilities create what the project itself claims to be, a *production-grade
    container orchestrator*. In simpler terms, Kubernetes provides a way for us to
    run and schedule containerized workloads on multiple hosts. Keep this primary
    capability in mind as we dive deeper. Over time, we hope to prove how this capability,
    while foundational, is only part of our journey to production.
  prefs: []
  type: TYPE_NORMAL
- en: The Core Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What are the components that provide the functionality we have covered? As
    we have mentioned, core components reside in the `kubernetes/kubernetes` repository.
    Many of us consume these components in different ways. For example, those running
    managed services such as Google Kubernetes Engine (GKE) are likely to find each
    component present on hosts. Others may be downloading binaries from repositories
    or getting signed versions from a vendor. Regardless, anyone can download a Kubernetes
    release from the `kubernetes/kubernetes` repository. After downloading and unpacking
    a release, binaries may be retrieved using the `cluster/get-kube-binaries.sh`
    command. This will auto-detect your target architecture and download server and
    client components. Let’s take a look at this in the following code, and then explore
    the key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the downloaded server components, likely saved to *server/kubernetes-server-${ARCH}.tar.gz*,
    you’ll find the key items that compose a Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: API Server
  prefs: []
  type: TYPE_NORMAL
- en: The primary interaction point for all Kubernetes components and users. This
    is where we get, add, delete, and mutate objects. The API server delegates state
    to a backend, which is most commonly etcd.
  prefs: []
  type: TYPE_NORMAL
- en: kubelet
  prefs: []
  type: TYPE_NORMAL
- en: The on-host agent that communicates with the API server to report the status
    of a node and understand what workloads should be scheduled on it. It communicates
    with the host’s container runtime, such as Docker, to ensure workloads scheduled
    for the node are started and healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Controller Manager
  prefs: []
  type: TYPE_NORMAL
- en: A set of controllers, bundled in a single binary, that handle reconciliation
    of many core objects in Kubernetes. When desired state is declared, e.g., three
    replicas in a Deployment, a controller within handles the creation of new Pods
    to satisfy this state.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs: []
  type: TYPE_NORMAL
- en: Determines where workloads should run based on what it thinks is the optimal
    node. It uses filtering and scoring to make this decision.
  prefs: []
  type: TYPE_NORMAL
- en: Kube Proxy
  prefs: []
  type: TYPE_NORMAL
- en: Implements Kubernetes services providing virtual IPs that can route to backend
    Pods. This is accomplished using a packet filtering mechanism on a host such as
    `iptables` or `ipvs`.
  prefs: []
  type: TYPE_NORMAL
- en: While not an exhaustive list, these are the primary components that make up
    the core functionality we have discussed. Architecturally, [Figure 1-1](#the_primary_components_that_make_up_the_kubernetes_cluster)
    shows how these components play together.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes architectures have many variations. For example, many clusters run
    kube-apiserver, kube-scheduler, and kube-controller-manager as containers. This
    means the control-plane may also run a container-runtime, kubelet, and kube-proxy.
    These kinds of deployment considerations will be covered in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0101](assets/prku_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The primary components that make up the Kubernetes cluster. Dashed
    borders represent components that are not part of core Kubernetes.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Beyond Orchestration—Extended Functionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are areas where Kubernetes does more than just orchestrate workloads.
    As mentioned, the component kube-proxy programs hosts to provide a virtual IP
    (VIP) experience for workloads. As a result, internal IP addresses are established
    and route to one or many underlying Pods. This concern certainly goes beyond running
    and scheduling containerized workloads. In theory, rather than implementing this
    as part of core Kubernetes, the project could have defined a Service API and required
    a plug-in to implement the Service abstraction. This approach would require users
    to choose between a variety of plug-ins in the ecosystem rather than including
    it as core functionality.
  prefs: []
  type: TYPE_NORMAL
- en: This is the model many Kubernetes APIs, such as Ingress and NetworkPolicy, take.
    For example, creation of an Ingress object in a Kubernetes cluster does not guarantee
    action is taken. In other words, while the API exists, it is not core functionality.
    Teams must consider what technology they’d like to plug in to implement this API.
    For Ingress, many use a controller such as [ingress-nginx](https://kubernetes.github.io/ingress-nginx),
    which runs in the cluster. It implements the API by reading Ingress objects and
    creating NGINX configurations for NGINX instances pointed at Pods. However, `ingress-nginx`
    is one of many options. [Project Contour](https://projectcontour.io) implements
    the same Ingress API but instead programs instances of envoy, the proxy that underlies
    Contour. Thanks to this pluggable model, there are a variety of options available
    to teams.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Interfaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Expanding on this idea of adding functionality, we should now explore interfaces.
    Kubernetes interfaces enable us to customize and build on the core functionality.
    We consider an interface to be a definition or contract on how something can be
    interacted with. In software development, this parallels the idea of defining
    functionality, which classes or structs may implement. In systems like Kubernetes,
    we deploy plug-ins that satisfy these interfaces, providing functionality such
    as networking.
  prefs: []
  type: TYPE_NORMAL
- en: A specific example of this interface/plug-in relationship is the [Container
    Runtime Interface](https://github.com/kubernetes/cri-api) (CRI). In the early
    days of Kubernetes, there was a single container runtime supported, Docker. While
    Docker is still present in many clusters today, there is growing interest in using
    alternatives such as [containerd](https://containerd.io) or [CRI-O](https://github.com/cri-o/cri-o).
    [Figure 1-2](#two_workloads_nodes_running_two_different_container_runtimes) demonstrates
    this relationship with these two container runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0102](assets/prku_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Two workload nodes running two different container runtimes. The
    kubelet sends commands defined in the CRI such as `CreateContainer` and expects
    the runtime to satisfy the request and respond.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In many interfaces, commands, such as `CreateContainerRequest` or `PortForwardRequest`,
    are issued as remote procedure calls (RPCs). In the case of CRI, the communication
    happens over GRPC and the kubelet expects responses such as `CreateContainerResponse`
    and `PortForwardResponse`. In [Figure 1-2](#two_workloads_nodes_running_two_different_container_runtimes),
    you’ll also notice two different models for satisfying CRI. CRI-O was built from
    the ground up as an implementation of CRI. Thus the kubelet issues these commands
    directly to it. containerd supports a plug-in that acts as a shim between the
    kubelet and its own interfaces. Regardless of the exact architecture, the key
    is getting the container runtime to execute, without the kubelet needing to have
    operational knowledge of how this occurs for *every possible* runtime. This concept
    is what makes interfaces so powerful in how we architect, build, and deploy Kubernetes
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, we’ve even seen some functionality removed from the core project
    in favor of this plug-in model. These are things that historically existed “in-tree,”
    meaning within the `kubernetes/kubernetes` code base. An example of this is [cloud-provider
    integrations](https://github.com/kubernetes/cloud-provider) (CPIs). Most CPIs
    were traditionally baked into components such as the kube-controller-manager and
    the kubelet. These integrations typically handled concerns such as provisioning
    load balancers or exposing cloud provider metadata. Sometimes, especially prior
    to the creation of the [Container Storage Interface (CSI)](https://kubernetes-csi.github.io/docs/introduction.html),
    these providers provisioned block storage and made it available to the workloads
    running in Kubernetes. That’s a lot of functionality to live in Kubernetes, not
    to mention it needs to be re-implemented for every possible provider! As a better
    solution, support was moved into its own interface model, e.g., [kubernetes/cloud-provider](https://github.com/kubernetes/cloud-provider),
    that can be implemented by multiple projects or vendors. Along with minimizing
    sprawl in the Kubernetes code base, this enables CPI functionality to be managed
    out of band of the core Kubernetes clusters. This includes common procedures such
    as upgrades or patching vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, there are several interfaces that enable customization and additional
    functionality in Kubernetes. What follows is a high-level list, which we’ll expand
    on throughout chapters in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: The Container Networking Interface (CNI) enables networking providers to define
    how they do things from IPAM to actual packet routing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Container Storage Interface (CSI) enables storage providers to satisfy intra-cluster
    workload requests. Commonly implemented for technologies such as ceph, vSAN, and
    EBS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Container Runtime Interface (CRI) enables a variety of runtimes, common
    ones including Docker, containerd, and CRI-O. It also has enabled a proliferation
    of less traditional runtimes, such as firecracker, which leverages KVM to provision
    a minimal VM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Service Mesh Interface (SMI) is one of the newer interfaces to hit the Kubernetes
    ecosystem. It hopes to drive consistency when defining things such as traffic
    policy, telemetry, and management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Cloud Provider Interface (CPI) enables providers such as VMware, AWS, Azure,
    and more to write integration points for their cloud services with Kubernetes
    clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Open Container Initiative Runtime Spec. (OCI) standardizes image formats
    ensuring that a container image built from one tool, when compliant, can be run
    in any OCI-compliant container runtime. This is not directly tied to Kubernetes
    but has been an ancillary help in driving the desire to have pluggable container
    runtimes (CRI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have focused in on the scope of Kubernetes. It is a *container orchestrator*,
    with a couple extra features here and there. It also has the ability to be extended
    and customized by leveraging plug-ins to interfaces. Kubernetes can be foundational
    for many organizations looking for an elegant means of running their applications.
    However, let’s take a step back for a moment. If we were to take the current systems
    used to run applications in your organization and replace them with Kubernetes,
    would that be enough? For many of us, there is much more involved in the components
    and machinery that make up our current “application platform.”
  prefs: []
  type: TYPE_NORMAL
- en: Historically, we have witnessed a lot of pain when organizations hold the view
    of having a “Kubernetes” strategy—or when they assume that Kubernetes will be
    an adequate forcing function for modernizing how they build and run software.
    Kubernetes is a technology, a great one, but it really should not be the focal
    point of where you’re headed in the modern infrastructure, platform, and/or software
    realm. We apologize if this seems obvious, but you’d be surprised how many executive
    or higher-level architects we talk to who believe that Kubernetes, by itself,
    is the answer to problems, when in actuality their problems revolve around application
    delivery, software development, or organizational/people issues. Kubernetes is
    best thought of as a piece of your puzzle, one that enables you to deliver platforms
    for your applications. We have been dancing around this idea of an application
    platform, which we’ll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Application Platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our path to production, it is key that we consider the idea of an application
    platform. We define an application platform as a viable place to run workloads.
    Like most definitions in this book, how that’s satisfied will vary from organization
    to organization. Targeted outcomes will be vast and desirable to different parts
    of the business—for example, happy developers, reduction of operational costs,
    and quicker feedback loops in delivering software are a few. The application platform
    is often where we find ourselves at the intersection of apps and infrastructure.
    Concerns such as developer experience (devx) are typically a key tenet in this
    area.
  prefs: []
  type: TYPE_NORMAL
- en: Application platforms come in many shapes and sizes. Some largely abstract underlying
    concerns such as the IaaS (e.g., AWS) or orchestrator (e.g., Kubernetes). Heroku
    is a great example of this model. With it you can easily take a project written
    in languages like Java, PHP, or Go and, using one command, deploy them to production.
    Alongside your app runs many platform services you’d otherwise need to operate
    yourself. Things like metrics collection, data services, and continuous delivery
    (CD). It also gives you primitives to run highly available workloads that can
    easily scale. Does Heroku use Kubernetes? Does it run its own datacenters or run
    atop AWS? Who cares? For Heroku users, these details aren’t important. What’s
    important is delegating these concerns to a provider or platform that enables
    developers to spend more time solving business problems. This approach is not
    unique to cloud services. RedHat’s OpenShift follows a similar model, where Kubernetes
    is more of an implementation detail and developers and platform operators interact
    with a set of abstractions on top.
  prefs: []
  type: TYPE_NORMAL
- en: Why not stop here? If platforms like Cloud Foundry, OpenShift, and Heroku have
    solved these problems for us, why bother with Kubernetes? A major trade-off to
    many prebuilt application platforms is the need to conform to their view of the
    world. Delegating ownership of the underlying system takes a significant operational
    weight off your shoulders. At the same time, if how the platform approaches concerns
    like service discovery or secret management does not satisfy your organizational
    requirements, you may not have the control required to work around that issue.
    Additionally, there is the notion of vendor or opinion lock-in. With abstractions
    come opinions on how your applications should be architected, packaged, and deployed.
    This means that moving to another system may not be trivial. For example, it’s
    significantly easier to move workloads between Google Kubernetes Engine (GKE)
    and Amazon Elastic Kubernetes Engine (EKS) than it is between EKS and Cloud Foundry.
  prefs: []
  type: TYPE_NORMAL
- en: The Spectrum of Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, it is clear there are several approaches to establishing a successful
    application platform. Let’s make some big assumptions for the sake of demonstration
    and evaluate theoretical trade-offs between approaches. For the average company
    we work with, say a mid to large enterprise, [Figure 1-3](#the_multitude_of_options_available_to_provider_an_applications)
    shows an arbitrary evaluation of approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In the bottom-left quadrant, we see deploying Kubernetes clusters themselves,
    which has a relatively low engineering effort involved, especially when managed
    services such as EKS are handling the control plane for you. These are lower on
    production readiness because most organizations will find that more work needs
    to be done on top of Kubernetes. However, there are use cases, such as teams that
    use dedicated cluster(s) for their workloads, that may suffice with just Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0103](assets/prku_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. The multitude of options available to provide an application platform
    to developers.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the bottom right, we have the more established platforms, ones that provide
    an end-to-end developer experience out of the box. Cloud Foundry is a great example
    of a project that solves many of the application platform concerns. Running software
    in Cloud Foundry is more about ensuring the software fits within its opinions.
    OpenShift, on the other hand, which for most is far more production-ready than
    just Kubernetes, has more decision points and considerations for how you set it
    up. Is this flexibility a benefit or a nuisance? That’s a key consideration for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, in the top right, we have building an application platform on top of
    Kubernetes. Relative to the others, this unquestionably requires the most engineering
    effort, at least from a platform perspective. However, taking advantage of Kubernetes
    extensibility means you can create something that lines up with your developer,
    infrastructure, and business needs.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning Your Organizational Needs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What’s missing from the graph in [Figure 1-3](#the_multitude_of_options_available_to_provider_an_applications)
    is a third dimension, a z-axis that demonstrates how aligned the approach is with
    your requirements. Let’s examine another visual representation. [Figure 1-4](#the_added_complexity_of_the_alignment_of_these)
    maps out how this might look when considering platform alignment with organizational
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0104](assets/prku_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. The added complexity of the alignment of these options with your
    organizational needs, the z-axis.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In terms of requirements, features, and behaviors you’d expect out of a platform,
    building a platform is almost always going to be the most aligned. Or at least
    the most capable of aligning. This is because you can build anything! If you wanted
    to re-implement Heroku in-house, on top of Kubernetes, with minor adjustments
    to its capabilities, it is technically possible. However, the cost/reward should
    be weighed out with the other axes (x and y). Let’s make this exercise more concrete
    by considering the following needs in a next-generation platform:'
  prefs: []
  type: TYPE_NORMAL
- en: Regulations require you to run mostly on-premise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need to support your baremetal fleet along with your vSphere-enabled datacenter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Want to support growing demand for developers to package applications in containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need ways to build self-service API mechanisms that move you away from “ticket-based”
    infrastructure provisioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Want to ensure APIs you’re building atop of are vendor agnostic and not going
    to cause lock-in because it has cost you millions in the past to migrate off these
    types of systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are open to paying enterprise support for a variety of products in the stack,
    but unwilling to commit to models where the entire stack is licensed per node,
    core, or application instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must understand our engineering maturity, appetite for building and empowering
    teams, and available resources to qualify whether building an application platform
    is a sensible undertaking.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing Application Platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Admittedly, what constitutes an application platform remains fairly gray. We’ve
    focused on a variety of platforms that we believe bring an experience to teams
    far beyond just workload orchestration. We have also articulated that Kubernetes
    can be customized and extended to achieve similar outcomes. By advancing our thinking
    beyond “How do I get a Kubernetes” into concerns such as “What is the current
    developer workflow, pain points, and desires?” platform and infrastructure teams
    will be more successful with what they build. With a focus on the latter, we’d
    argue, you are far more likely to chart a proper path to production and achieve
    nontrivial adoption. At the end of the day, we want to meet infrastructure, security,
    and developer requirements to ensure our customers—typically developers—are provided
    a solution that meets their needs. Often we do not want to simply provide a “powerful”
    engine that every developer must build their own platform atop of, as jokingly
    depicted in [Figure 1-5](#when_developers_desire_an_end_to_end_experience).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0105](assets/prku_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. When developers desire an end-to-end experience (e.g., a driveable
    car), do not expect an engine without a frame, wheels, and more to suffice.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Building Application Platforms on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we’ve identified Kubernetes as one piece of the puzzle in our path to production.
    With this, it would be reasonable to wonder “Isn’t Kubernetes just missing stuff
    then?” The Unix philosophy’s principle of “make each program do one thing well”
    is a compelling aspiration for the Kubernetes project. We believe its best features
    are largely the ones it does not have! Especially after being burned with one-size-fits-all
    platforms that try to solve the world’s problems for you. Kubernetes has brilliantly
    focused on being a great orchestrator while defining clear interfaces for how
    it can be built on top of. This can be likened to the foundation of a home.
  prefs: []
  type: TYPE_NORMAL
- en: A good foundation should be structurally sound, able to be built on top of,
    and provide appropriate interfaces for routing utilities to the home. While important,
    a foundation alone is rarely a habitable place for our applications to live. Typically,
    we need some form of home to exist on top of the foundation. Before discussing
    *building* on top of a foundation such as Kubernetes, let’s consider a pre-furnished
    apartment as shown in [Figure 1-6](#an_apartment_that_is_move_in_ready_similar).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0106](assets/prku_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. An apartment that is move-in ready. Similar to platform as a service
    options like Heroku. Illustration by Jessica Appelbaum.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This option, similar to our examples such as Heroku, is habitable with no additional
    work. There are certainly opportunities to customize the experience inside; however,
    many concerns are solved for us. As long as we are comfortable with the price
    of rent and are willing to conform to the nonnegotiable opinions within, we can
    be successful on day one.
  prefs: []
  type: TYPE_NORMAL
- en: Circling back to Kubernetes, which we have likened to a foundation, we can now
    look to build that habitable home on top of it, as depicted in [Figure 1-7](#building_a_house_similar_to_establishing_an_application_platform).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0107](assets/prku_0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. Building a house. Similar to establishing an application platform,
    which Kubernetes is foundational to. Illustration by Jessica Appelbaum.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the cost of planning, engineering, and maintaining, we can build remarkable
    platforms to run workloads throughout organizations. This means we’re in complete
    control of every element in the output. The house can and should be tailored to
    the needs of the future tenants (our applications). Let’s now break down the various
    layers and considerations that make this possible.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the Bottom
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we must start at the bottom, which includes the technology Kubernetes
    expects to run. This is commonly a datacenter or cloud provider, which offers
    compute, storage, and networking. Once established, Kubernetes can be bootstrapped
    on top. Within minutes you can have clusters living atop the underlying infrastructure.
    There are several means of bootstrapping Kubernetes, and we’ll cover them in depth
    in [Chapter 2](ch02.html#deployment_models).
  prefs: []
  type: TYPE_NORMAL
- en: From the point of Kubernetes clusters existing, we next need to look at a conceptual
    flow to determine what we should build on top. The key junctures are represented
    in [Figure 1-8](#a_flow_our_teams_may_go_through_in_their_path).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0108](assets/prku_0108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. A flow our teams may go through in their path to production with
    Kubernetes.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the point of Kubernetes existing, you can expect to quickly be receiving
    questions such as:'
  prefs: []
  type: TYPE_NORMAL
- en: “How do I ensure workload-to-workload traffic is fully encrypted?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How do I ensure egress traffic goes through a gateway guaranteeing a consistent
    source CIDR?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How do I provide self-service tracing and dashboards to applications?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How do I let developers onboard without being concerned about them becoming
    Kubernetes experts?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list can be endless. It is often incumbent on us to determine which requirements
    to solve at a platform level and which to solve at an application level. The key
    here is to deeply understand exiting workflows to ensure what we build lines up
    with current expectations. If we cannot meet that feature set, what impact will
    it have on the development teams? Next we can start the building of a platform
    on top of Kubernetes. In doing so, it is key we stay paired with development teams
    willing to onboard early and understand the experience to make informed decisions
    based on quick feedback. After reaching production, this flow should not stop.
    Platform teams should not expect what is delivered to be a static environment
    that developers will use for decades. In order to be successful, we must constantly
    be in tune with our development groups to understand where there are issues or
    potential missing features that could increase development velocity. A good place
    to start is considering what level of interaction with Kubernetes we should expect
    from our developers. This is the idea of how much, or how little, we should abstract.
  prefs: []
  type: TYPE_NORMAL
- en: The Abstraction Spectrum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past, we’ve heard posturing like, “If your application developers know
    they’re using Kubernetes, you’ve failed!” This can be a decent way to look at
    interaction with Kubernetes, especially if you’re building products or services
    where the underlying orchestration technology is meaningless to the end user.
    Perhaps you’re building a database management system (DBMS) that supports multiple
    database technologies. Whether shards or instances of a database run via Kubernetes,
    Bosh, or Mesos probably doesn’t matter to your developers! However, taking this
    philosophy wholesale from a tweet into your team’s success criteria is a dangerous
    thing to do. As we layer pieces on top of Kubernetes and build platform services
    to better serve our customers, we’ll be faced with many points of decision to
    determine what appropriate abstractions looks like. [Figure 1-9](#the_various_ends_of_the_spectrum_starting_with_giving)
    provides a visualization of this spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0109](assets/prku_0109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. The various ends of the spectrum. Starting with giving each team
    its own Kubernetes cluster to entirely abstracting Kubernetes from your users,
    via a platform as a service (PaaS) offering.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This can be a question that keeps platform teams up at night. There’s a lot
    of merit in providing abstractions. Projects like Cloud Foundry provide a fully
    baked developer experience—an example being that in the context of a single `cf
    push` we can take an application, build it, deploy it, and have it serving production
    traffic. With this goal and experience as a primary focus, as Cloud Foundry furthers
    its support for running on top of Kubernetes, we expect to see this transition
    as more of an implementation detail than a change in feature set. Another pattern
    we see is the desire to offer more than Kubernetes at a company, but not make
    developers explicitly choose between technologies. For example, some companies
    have a Mesos footprint alongside a Kubernetes footprint. They then build an abstraction
    enabling transparent selection of where workloads land without putting that onus
    on application developers. It also prevents them from technology lock-in. A trade-off
    to this approach includes building abstractions on top of two systems that operate
    differently. This requires significant engineering effort and maturity. Additionally,
    while developers are eased of the burden around knowing how to interact with Kubernetes
    or Mesos, they instead need to understand how to use an abstracted company-specific
    system. In the modern era of open source, developers from all over the stack are
    less enthused about learning systems that don’t translate between organizations.
    Lastly, a pitfall we’ve seen is an obsession with abstraction causing an inability
    to expose key features of Kubernetes. Over time this can become a cat-and-mouse
    game of trying to keep up with the project and potentially making your abstraction
    as complicated as the system it’s abstracting.
  prefs: []
  type: TYPE_NORMAL
- en: On the other end of the spectrum are platform groups that wish to offer self-service
    clusters to development teams. This can also be a great model. It does put the
    responsibility of Kubernetes maturity on the development teams. Do they understand
    how Deployments, ReplicaSets, Pods, Services, and Ingress APIs work? Do they have
    a sense for setting millicpus and how overcommit of resources works? Do they know
    how to ensure that workloads configured with more than one replica are always
    scheduled on different nodes? If yes, this is a perfect opportunity to avoid over-engineering
    an application platform and instead let application teams take it from the Kubernetes
    layer up.
  prefs: []
  type: TYPE_NORMAL
- en: This model of development teams owning their own clusters is a little less common.
    Even with a team of humans that have a Kubernetes background, it’s unlikely that
    they want to take time away from shipping features to determine how to manage
    the life cycle of their Kubernetes cluster when it comes time to upgrade. There’s
    so much power in all the knobs Kubernetes exposes, but for many development teams,
    expecting them to become Kubernetes experts on top of shipping software is unrealistic.
    As you’ll find in the coming chapters, abstraction does not have to be a binary
    decision. At a variety of points we’ll be able to make informed decisions on where
    abstractions make sense. We’ll be determining where we can provide developers
    the right amount of flexibility while still streamlining their ability to get
    things done.
  prefs: []
  type: TYPE_NORMAL
- en: Determining Platform Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building on top of Kubernetes, a key determination is what features should
    be built into the platform relative to solved at the application level. Generally
    this is something that should be evaluated at a case-by-case basis. For example,
    let’s assume every Java microservice implements a library that facilitates mutual
    TLS (mTLS) between services. This provides applications a construct for identity
    of workloads and encryption of data over the network. As a platform team, we need
    to deeply understand this usage to determine whether it is something we should
    offer or implement at a platform level. Many teams look to solve this by potentially
    implementing a technology called a service mesh into the cluster. An exercise
    in trade-offs would reveal the following considerations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros to introducing a service mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: Java apps no longer need to bundle libraries to facilitate mTLS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-Java applications can take part in the same mTLS/encryption system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lessened complexity for application teams to solve for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons to introducing a service mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: Running a service mesh is not a trivial task. It is another distributed system
    with operational complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service meshes often introduce features far beyond identity and encryption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mesh’s identity API might not integrate with the same backend system as
    used by the existing applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighing these pros and cons, we can come to the conclusion as to whether solving
    this problem at a platform level is worth the effort. The key is we don’t need
    to, and should not strive to, solve every application concern in our new platform.
    This is another balancing act to consider as you proceed through the many chapters
    in this book. Several recommendations, best practices, and guidance will be shared,
    but like anything, you should assess each based on the priorities of your business
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: The Building Blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s wrap up this chapter by concretely identifying key building blocks you
    will have available as you build a platform. This includes everything from the
    foundational components to optional platform services you may wish to implement.
  prefs: []
  type: TYPE_NORMAL
- en: The components in [Figure 1-10](#many_of_the_key_building_blocks_involved_in_establishing)
    have differing importance to differing audiences.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0110](assets/prku_0110.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-10\. Many of the key building blocks involved in establishing an application
    platform.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some components such as container networking and container runtime are required
    for every cluster, considering that a Kubernetes cluster that can’t run workloads
    or allow them to communicate would not be very successful. You are likely to find
    some components to have variance in whether they should be implemented at all.
    For example, secret management might not be a platform service you intend to implement
    if applications already get their secrets from an external secret management solution.
  prefs: []
  type: TYPE_NORMAL
- en: Some areas, such as security, are clearly missing from [Figure 1-10](#many_of_the_key_building_blocks_involved_in_establishing).
    This is because security is not a feature but more so a result of how you implement
    everything from the IAAS layer up. Let’s explore these key areas at a high level,
    with the understanding that we’ll dive much deeper into them throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: IAAS/datacenter and Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IAAS/datacenter and Kubernetes form the foundational layer we have called out
    many times in this chapter. We don’t mean to trivialize this layer because its
    stability will directly correlate to that of our platform. However, in modern
    environments, we spend much less time determining the architecture of our racks
    to support Kubernetes and a lot more time deciding between a variety of deployment
    options and topologies. Essentially we need to assess how we are going to provision
    and make available Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The container runtime will faciliate the life cycle management of our workloads
    on each host. This is commonly implemented using a technology that can manage
    containers, such as CRI-O, containerd, and Docker. The ability to choose between
    these different implementations is thanks to the Container Runtime Interface (CRI).
    Along with these common examples, there are specialized runtimes that support
    unique requirements, such as the desire to run a workload in a micro-vm.
  prefs: []
  type: TYPE_NORMAL
- en: Container networking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our choice of container networking will commonly address IP address management
    (IPAM) of workloads and routing protocols to facilitate communication. Common
    technology choices include Calico or Cilium, which is thanks to the Container
    Networking Interface (CNI). By plugging a container networking technology into
    the cluster, the kubelet can request IP addresses for the workloads it starts.
    Some plug-ins go as far as implementing service abstractions on top of the Pod
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Storage integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Storage integration covers what we do when the on-host disk storage just won’t
    cut it. In modern Kubernetes, more and more organizations are shipping stateful
    workloads to their clusters. These workloads require some degree of certainty
    that the state will be resilient to application failure or rescheduling events.
    Storage can be supplied by common systems such as vSAN, EBS, Ceph, and many more.
    The ability to choose between various backends is facilitated by the Container
    Storage Interface (CSI). Similar to CNI and CRI, we are able to deploy a plug-in
    to our cluster that understands how to satisfy the storage needs requested by
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: Service routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Service routing is the facilitation of traffic to and from the workloads we
    run in Kubernetes. Kubernetes offers a Service API, but this is typically a stepping
    stone for support of more feature-rich routing capabilities. Service routing builds
    on container networking and creates higher-level features such as layer 7 routing,
    traffic patterns, and much more. Many times these are implemented using a technology
    called an Ingress controller. At the deeper side of service routing comes a variety
    of service meshes. This technology is fully featured with mechanisms such as service-to-service
    mTLS, observability, and support for applications mechanisms such as circuit breaking.
  prefs: []
  type: TYPE_NORMAL
- en: Secret management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Secret management covers the management and distribution of sensitive data needed
    by workloads. Kubernetes offers a Secrets API where sensitive data can be interacted
    with. However, out of the box, many clusters don’t have robust enough secret management
    and encryption capabilities demanded by several enterprises. This is largely a
    conversation around defense in depth. At a simple level, we can ensure data is
    encrypted before it is stored (encryption at rest). At a more advanced level,
    we can provide integration with various technologies focused on secret management,
    such as Vault or Cyberark.
  prefs: []
  type: TYPE_NORMAL
- en: Identity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identity covers the authentication of humans and workloads. A common initial
    ask of cluster administrators is how to authenticate users against a system such
    as LDAP or a cloud provider’s IAM system. Beyond humans, workloads may wish to
    identify themselves to support zero-trust networking models where impersonation
    of workloads is far more challenging. This can be facilitated by integrating an
    identity provider and using mechanisms such as mTLS to verify a workload.
  prefs: []
  type: TYPE_NORMAL
- en: Authorization/admission control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Authorization is the next step after we can verify the identity of a human or
    workload. When users or workloads interact with the API server, how do we grant
    or deny their access to resources? Kubernetes offers an RBAC feature with resource/verb-level
    controls, but what about custom logic specific to authorization inside our organization?
    Admission control is where we can take this a step further by building out validation
    logic that can be as simple as looking over a static list of rules to dynamically
    calling other systems to determine the correct authorization response.
  prefs: []
  type: TYPE_NORMAL
- en: Software supply chain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The software supply chain covers the entire life cycle of getting software in
    source code to runtime. This involves the common concerns around continuous integration
    (CI) and continuous delivery (CD). Many times, developers’ primary interaction
    point is the pipelines they establish in these systems. Getting the CI/CD systems
    working well with Kubernetes can be paramount to your platform’s success. Beyond
    CI/CD are concerns around the storage of artifacts, their safety from a vulnerability
    standpoint, and ensuring integrity of images that will be run in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Observability is the umbrella term for all things that help us understand what’s
    happening with our clusters. This includes at the system and application layers.
    Typically, we think of observability to cover three key areas. These are logs,
    metrics, and tracing. Logging typically involves forwarding log data from workloads
    on the host to a target backend system. From this system we can aggregate and
    analyze logs in a consumable way. Metrics involves capturing data that represents
    some state at a point in time. We often aggregate, or scrape, this data into some
    system for analysis. Tracing has largely grown in popularity out of the need to
    understand the interactions between the various services that make up our application
    stack. As trace data is collected, it can be brought up to an aggregate system
    where the life of a request or response is shown via some form of context or correlation
    ID.
  prefs: []
  type: TYPE_NORMAL
- en: Developer abstractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Developer abstractions are the tools and platform services we put in place to
    make developers successful in our platform. As discussed earlier, abstraction
    approaches live on a spectrum. Some organizations will choose to make the usage
    of Kubernetes completely transparent to the development teams. Other shops will
    choose to expose many of the powerful knobs Kubernetes offers and give significant
    flexibility to every developer. Solutions also tend to focus on the developer
    onboarding experience, ensuring they can be given access and secure control of
    an environment they can utilize in the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored ideas spanning Kubernetes, application platforms,
    and even building application platforms on Kubernetes. Hopefully this has gotten
    you thinking about the variety of areas you can jump into in order to better understand
    how to build on top of this great workload orchestrator. For the remainder of
    the book we are going to dive into these key areas and provide insight, anecdotes,
    and recommendations that will further build your perspective on platform building.
    Let’s jump in and start down this path to production!
  prefs: []
  type: TYPE_NORMAL
