- en: Chapter 15\. Software Supply Chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a Kubernetes platform should never be the goal of your team or
    company (assuming you are not a vendor or consultant!). This might seem a strange
    claim for a book exclusively devoted to Kubernetes to make, but let’s step back
    a moment. All companies are in the business of delivering their *core-competency*.
    This might be an ecommerce platform, a SaaS monitoring system, or an insurance
    website. Platforms like Kubernetes (and almost any other tooling) exist to *enable*
    the delivery of core business value, a truth that is often forgotten by teams
    when designing and implementing IT solutions.
  prefs: []
  type: TYPE_NORMAL
- en: With that sentiment in mind, this chapter will focus on the actual process of
    getting code from developers to production on Kubernetes. To best cover each stage
    that we think is relevant, we’ll follow the model of a pipeline that many are
    familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: First we’ll look at some of the considerations when building container images
    (our deployed assets) from source code. If you’re already utilizing Kubernetes
    or other container platforms you’ll probably be familiar with some of the concepts
    in this section, but hopefully we’ll cover some questions that you may not have
    considered. If you’re *new* to containers this will be a paradigm shift from the
    way that you currently build software (WAR files, Go binaries, etc.) to thinking
    about the container image and the nuances involved with building and maintaining
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have built our assets we need somewhere to store them. We’ll discuss
    container registries (e.g., DockerHub, Harbor, Quay) and the functionality that
    we think is important when choosing one. Many of the attributes of container registries
    are related to security, and we’ll discuss options like image scanning, updates,
    and signing.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll dedicate some time to reviewing continuous delivery and how those
    practices and associated tooling intersect with Kubernetes. We’ll look at emerging
    ideas like GitOps (deployments through syncing cluster state from git repositories)
    and more traditional imperative pipeline approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you are not yet running Kubernetes, you will likely have considered
    and/or solved for all of the high-level areas just mentioned (build, asset storage,
    deployment). It’s reasonable that everyone has investments and expertise in existing
    tooling and approaches, and we very rarely encounter a situation where an organization
    wants to start afresh with its entire software supply chain. One of the things
    we’ll try to emphasize in this chapter is that there are clean handoff points
    in the pipeline and we can pick and choose the most effective approaches for each
    phase. As with many of the topics covered in this book, it is entirely possible
    (and recommended) to enact *incremental* positive change while remaining focused
    on delivering business value.
  prefs: []
  type: TYPE_NORMAL
- en: Building Container Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before containers we would package applications as a binary, compressed asset,
    or raw source code for it to be deployed onto a server. This would either run
    standalone or inside of an application server. Alongside the application itself
    we’d need to ensure the environment contained the correct dependencies and configuration
    available for it to run successfully in the environment we were deploying to.
  prefs: []
  type: TYPE_NORMAL
- en: In a container-based environment, the container image is the deployable asset.
    It contains not only the application binary itself but also the execution environment
    and any associated dependencies. The image itself is a compressed set of *filesystem
    layers* alongside some metadata, which together conform to the Open Container
    Initiative (OCI) Image Specification. This is an agreed standard within the cloud
    native community to ensure that image building can be implemented in many different
    ways (we’ll see some of these in the following sections) while still producing
    an artifact that is runnable by all the different container runtimes (more information
    on this can be found in [Chapter 3](ch03.html#container_runtime_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: Typically, building a container image involves creating a Dockerfile that describes
    the image and using Docker Engine to execute the Dockerfile. With that said, there
    is an ecosystem of tools (each with their own approaches) that you can use to
    create container images in different scenarios. To borrow a concept from BuildKit
    (one such tool, built by Docker) we can think about building in terms of *frontend*
    and *backend*. The *frontend* is the method for defining the high-level process
    that should be used to build the image, e.g., a Dockerfile or Buildpack (more
    on these later in this chapter). The *backend* is the actual build engine that
    takes the definition generated by the *frontend* and executes commands on the
    filesystem to construct the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many cases the *backend* is the Docker daemon, which may not be suitable
    for all cases. For example, if we want to run builds in Kubernetes we need either
    to run a Docker daemon inside a container (Docker in Docker) or mount the Docker
    Unix socket from the host machine into the build container. Both of these approaches
    have drawbacks, and in the latter case exposes potential security issues. In response
    to these issues, other build backends like Kaniko have emerged. Kaniko uses the
    same *frontend* (a Dockerfile) but utilizes different techniques to create the
    image under the hood, making it a solid choice for running in a Kubernetes Pod.
    When deciding how you want to build images, you should answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Can we run our builder as root?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are we OK mounting the Docker socket?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we care about running a daemon?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we want to containerize builds?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we want to run them among workloads in Kubernetes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much do we intend to leverage layer caching?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will our tooling choice affect distributing builds?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What *frontends* or image definition mechanisms do we want to use? What is supported?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will first cover some of the patterns and antipatterns we’ve
    seen when building container images (Cloud Native Buildpacks) that will hopefully
    help you on your journey to build better container images. Then, we will review
    an alternative method for building container images, and how all of these techniques
    can be integrated into a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: One question that often comes up early on within organizations is, Who should
    be responsible for building images? Early on as Docker was becoming popular it
    was largely embraced as a developer-focused tool. In our experience, smaller organizations
    still have developers responsible for writing Dockerfiles and defining the build
    process for their application images. However, as organizations look to adopt
    containers (and Kubernetes) at scale, having individual developers or development
    teams all creating their own Dockerfiles becomes unsustainable. Firstly it creates
    extra work for developers, which pulls them away from their core responsibility,
    and secondly, it results in a huge variance in produced images with little to
    no standardization.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we are seeing a move toward abstracting the build process from
    development teams and instead moving the responsibility toward operations and
    platform teams to implement *source to image* patterns and tooling that receive
    a code repository as an input and are capable of producing a container image ready
    to move through the pipeline. We’ll discuss this pattern more in [“Cloud Native
    Buildpacks”](#cloud_native_buildpacks). In the interim we have also commonly seen
    a pattern of platform teams running workshops and/or assisting development teams
    with Dockerfile and image creation. As organizations scale this can be an effective
    first step but is usually not sustainable given the ratio of development teams
    to platform personnel.
  prefs: []
  type: TYPE_NORMAL
- en: The Golden Base Images Antipattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field we have encountered several antipatterns that are usually the result
    of teams not adjusting their thinking to embrace patterns that have emerged in
    the container and cloud native landscape. Maybe the most common of these is the
    concept of pre-ordained, *gold* images. The scenario is that in a pre-container
    environment specific base images (for example, a preconfigured CentOS base) would
    be approved for use within an organization, and all applications going into production
    would have to be based on that image. This approach is usually adopted for *security*
    reasons, as the tools and libraries in the image have been well vetted. However,
    when moving to containers, teams found themselves consigned to reinventing the
    wheel by pulling useful upstream images from third parties and vendors and rebasing
    their applications and configurations onto them.
  prefs: []
  type: TYPE_NORMAL
- en: This introduces a few related issues. Firstly, there is the additional work
    involved with the initial conversion from the upstream image to an internal customized
    version. Secondly, there is now an onus of maintenance placed on the internal
    platform team to store and maintain these internal images. Because this situation
    can sprawl (given how many images are in use in a typical environment), this approach
    usually ends up resulting in a *worse* security posture as updates are performed
    infrequently (if at all) given the extra work involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our recommendation in this area is usually to partner with security teams and
    identify what specific requirements the gold images are serving. Usually several
    of the following will apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure specific agents/software is installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure no vulnerable libraries are present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure user accounts have the correct permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding the reasoning behind the restrictions, we can instead codify
    these requirements into tooling that will sit in the pipeline and reject and/or
    alert on non-compliant images and maintain the desired security posture, while
    still broadly allowing teams to reuse images (and the work that has gone into
    crafting them) from the upstream community. We’ll take a deeper look at one example
    workflow in [“Image Registries”](#image_registries).
  prefs: []
  type: TYPE_NORMAL
- en: One of the more compelling reasons to specify a base OS is to ensure that operational
    knowledge exists in the organization should troubleshooting be required. However,
    when digging a little deeper this is not as useful as it may seem. Very rarely
    should it be necessary to `exec` into containers to troubleshoot specific issues,
    and even then the differences between Linux-based operating systems are fairly
    trivial for the kinds of support required. Additionally, more and more applications
    are being packaged in ultra-lightweight scratch or distroless images to reduce
    the overhead inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to refactor all *upstream*/vendor images onto your own base(s) should
    be avoided for the reasons described in this section. However, we’re not asserting
    that maintaining an internal set of curated base images is a bad idea. These can
    be great to use as a foundation for your own applications, and we talk about some
    of the considerations when building these internal bases in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Base Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base image of the container determines the bottom layers on which the application’s
    container image is to be built. The base image is critical as it usually contains
    operating system libraries and tools that will be part of your application container
    image. If you are not mindful when choosing a base image, it can be the source
    of unnecessary libraries and tools that not only bloat your container image but
    also can become security vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your organization’s maturity and security posture, you might not
    have a choice when it comes to base images. We have worked with many organizations
    that have a dedicated team responsible for curating and maintaining a set of approved
    base images that must be used across the organization. With that said, if you
    do have a choice or you are part of the team that is vetting base images, consider
    the following guidelines when evaluating base images:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure the images are published by a reputable vendor. You don’t want to use
    a base image from a random DockerHub user. After all, these images will be the
    foundation for most, if not all, your applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the update cycle and prefer images that are updated continuously.
    As mentioned earlier, the base image typically contains libraries and tools that
    must be patched whenever new vulnerabilities are discovered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefer images that have an open source build process or specification. This
    is typically a Dockerfile that you can inspect to understand how the image is
    built.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid images that have unnecessary tools or libraries. Prefer minimal images
    that provide a small footprint that your developers can build upon, when necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time if you are building your own images we’ve seen scratch or distroless
    to be a solid choice as both embody the preceding principles. The scratch image
    contains absolutely nothing, so with a simpler static binary scratch can be the
    leanest possible image. However, you may encounter issues if you need root CA
    certificates or some other assets. These can be copied in, but are something to
    think about. The distroless base is what we’d recommend in most cases as they
    contain some sensible users precreated (`nonroot`, `nobody`, etc.) and a set of
    minimal required libraries that vary depending on the flavor of the base image
    chosen. Distroless has several language-specific base variants for you to choose
    from.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections we’ll continue to talk about best-practice patterns,
    starting with the importance of specifying an appropriate user for your application
    to run under.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime User
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because of the container isolation model (mainly the fact that containers share
    the underlying Linux kernel), the runtime user of a container has important implications
    that some developers don’t think about. In most cases, when the container’s runtime
    user is left unspecified, the process runs as the root user. This is problematic
    because it increases the attack surface of the container. For example, if an attacker
    were to compromise the application and escape the container, they could gain root
    access on the underlying host.
  prefs: []
  type: TYPE_NORMAL
- en: 'When building your container image, it is critical for you to consider the
    runtime user of the container. Does the application need to run as root? Does
    the application depend on the contents of */etc/passwd*? Do you need to add a
    nonroot user to the container image? As you answer these questions, ensure that
    you specify the runtime user in the container image’s configuration. If you are
    using a Dockerfile to build your image, you can use the `USER` directive to specify
    the runtime user, as in the following example, which runs the `my-app` binary
    with the user and group ID `nonroot` (which is configured by default as part of
    the distroless set of images):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Even though you can specify the runtime user in your Kubernetes deployment manifests,
    defining it as part of the container image specification is valuable as it results
    in a self-documenting container image. It also ensures that developers use the
    same user and group ID as they work with the container in their local or development
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Pinning Package Versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If your application leverages external packages you will most likely install
    them using a package manager such as `apt`, `yum`, or `apk`. As you build your
    container image, it is important that you pin or specify the version of these
    packages. For example, the following example shows an application that depends
    on imagemagick. The `apk` instruction in the Dockerfile pins imagemagick to the
    version that is compatible with the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you leave package versions unspecified, you risk getting different packages
    that might break your application. Thus, always specify the versions of the packages
    you install in your container image. By doing so, you ensure that your container
    image builds are repeatable and produce container images with compatible package
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: Build Versus Runtime Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to packaging applications for deployment, development teams can
    also leverage containers to build their applications. Containers can provide a
    well-defined build environment that can be codified into a Dockerfile, for example.
    This is useful as developers are not required to install the build tooling in
    their systems. More importantly, containers can provide a standardized build environment
    across the entire development team and their continuous integration (CI) systems.
  prefs: []
  type: TYPE_NORMAL
- en: While using containers to build applications can be useful, it is important
    to distinguish between the build container image and the runtime image. The build
    image contains all the tooling and libraries that are necessary to compile the
    application, whereas the runtime image contains the application to be deployed.
    For example in a Java application we might have a build image that contains the
    JDK, Gradle/Maven, and all of our compilation and testing tooling. Then our runtime
    image can contain only the Java runtime and our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the application typically does not need the build tooling at runtime,
    the runtime image should not contain these tools. This results in a leaner container
    image that is faster to distribute and has a tighter attack surface. If you are
    using docker to build images, you can leverage its multistage build feature to
    separate the build from the runtime image. The following snippet shows a Dockerfile
    for a Go application. The build stage uses the `golang` image, which includes
    the Go toolchain, while the runtime stage uses the scratch base image and contains
    nothing more than the application binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_software_supply_chain_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The main `golang` image contains all the Go build tools, which are not required
    at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_software_supply_chain_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We copy the *go.mod* file first and download so that we can cache this step
    if the code changes but the dependencies don’t.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_software_supply_chain_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We can use `distroless` as a runtime image to take advantage of a minimal base
    but with no unnecessary extra dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_software_supply_chain_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We want to run our apps as a nonroot user if possible.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_software_supply_chain_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Only the compiled file (*my-app*) is being copied from the build stage to the
    deploy stage.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Containers run a single process and usually have no supervisor or init system.
    For this reason you need to ensure that signals are handled correctly and orphaned
    processes are correctly reparented and reaped. There are several minimal init
    scripts capable of fulfilling these requirements and acting as a bootstrap for
    your application instance.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Native Buildpacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An additional method of building container images involves tooling that analyze
    the application’s source code and automatically produce a container image. Similar
    to application-specific build tools, this approach greatly simplifies the developer
    experience as developers don’t have to create and maintain Dockerfiles. Cloud
    Native Buildpacks is an implementation of such an approach, and the high-level
    flow is shown in [Figure 15-1](#buildpack_flow).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1501](assets/prku_1501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-1\. Buildpack flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cloud Native Buildpacks (CNB) is a container-focused implementation of Buildpacks,
    a technology that Heroku and Cloud Foundry have used for years to package applications
    for those platforms. In the case of CNB, it packages applications into OCI container
    images, ready to run on Kubernetes. To build the image, CNB analyzes the application
    source code and executes the buildpacks accordingly. For example, the Go buildpack
    is executed if there are Go files present in your source code. Similarly, the
    Maven (Java) buildpack is executed if CNB finds a *pom.xml* file. This all happens
    behind the scenes, and developers can initiate this process using a CLI tool called
    `pack`. The great thing about this approach is that the buildpacks are tightly
    scoped, which enables building high-quality images that follow best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to improving the developer experience and lowering the barrier to
    platform adoption, platform teams can leverage custom buildpacks to enforce policy,
    ensure compliance, and standardize the container images running in their platform.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, providing a solution that builds container images from source code
    can be a worthy endeavor. Furthermore, we find that the value of such a solution
    increases with the size of the organization. At the end of the day, development
    teams want to focus on building value in the application and not on how to containerize
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Image Registries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are already using containers, then you’ll likely have a registry you
    prefer. It’s one of the core requirements for utilizing Docker and Kubernetes
    because we need somewhere to store the images we build on one machine and want
    to run on many others (either standalone or in a cluster). As is the case for
    images, the OCI also defines a standard spec for registry operations (to ensure
    interoperability), and there are many proprietary and open source solutions available,
    most of which share a common set of core features. Most image registries are comprised
    of three major components: a server (for the user interface and API logic), a
    blob store (for the images themselves), and a database (for user and image metadata).
    Usually, the storage backends are configurable, and this can impact how you design
    your registry architecture. We’ll talk more about this in a minute.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll look at some of the most important features offered by
    registries and some of the patterns for integrating them into your pipeline. We’re
    not going to look deeply at any *specific* registry implementations, as functionality
    is generally similar; however, there are scenarios where you may want to lean
    in a certain direction based on your existing setup or requirements.
  prefs: []
  type: TYPE_NORMAL
- en: If you are already leveraging an artifact store like Artifactory or Nexus you
    may want to take advantage of their image hosting capabilities for ease of management.
    Similarly, if your environments are heavily cloud-based there may be cost benefits
    to utilizing cloud provider registries like AWS Elastic Container Registry (ECR),
    Google Container Registry (GCR), or Azure Container Registry (ACR).
  prefs: []
  type: TYPE_NORMAL
- en: Another key factor to consider when choosing a registry is the topology, architecture,
    and failure domains of your environments and clusters. You may choose to place
    registries in each failure domain to ensure high availability. When doing this
    you’ll need to decide whether you want a centralized blob store or whether you
    want blob stores in each region and set up image replication between the registries.
    Replication is a feature of most registries that allows you to push an image to
    one of a set of registries and have that image automatically pushed to the others
    in the set. Even if this is not directly supported in your registry of choice,
    it is fairly trivial to set up basic replication by using a pipeline tool (e.g.,
    Jenkins) and webhooks that trigger on each image push.
  prefs: []
  type: TYPE_NORMAL
- en: The decision of one versus many registries is also impacted by how much throughput
    you need to support. In organizations with many thousands of developers triggering
    code and image builds on every code commit, the number of concurrent operations
    (pulls and pushes) can be significant. It is important to therefore understand
    that an image registry, while playing only a limited role in the pipeline, is
    in the critical path for not only production deployments but also development
    activities. It is a core component that must be monitored and maintained in the
    same way as other critical components to achieve a high level of service availability.
  prefs: []
  type: TYPE_NORMAL
- en: Many registries are built with the intention of being easily run *inside* a
    cluster or containerized environment. This approach (which we’ll cover again in
    [“Continuous Delivery”](#continuous_delivery)) has many advantages. Primarily,
    we are able to leverage all of the primitives and conventions inside Kubernetes
    to keep the services running, discoverable, and easily configurable. The obvious
    downside here is that we now have a reliance on a service *inside* the cluster
    to provide images to start new services inside that cluster. It’s more common
    to see registries run on a shared services cluster and have a failover system
    to a backup cluster to ensure that *some* instance of the registry will always
    be able to service requests.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also commonly seen registries run *outside* of Kubernetes and treated
    as more of a standalone *bootstrap* component that is required by all clusters.
    This is usually the case where an organization is already using an existing instance
    of Artifactory or another registry and repurposes it for image hosting. Utilizing
    cloud registries is also a common pattern here, although you also need to be aware
    of their availability guarantees (as the same topology questions apply) and potential
    extra latency.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections we’ll look at some of the most common concerns
    when choosing and working with a registry. These concerns are all security related
    as securing our software supply chain revolves around our deployed artifacts (images).
    First we’ll look at vulnerability scanning and how to ensure that our images don’t
    contain known security flaws. Then we’ll describe a commonly used quarantine flow
    that can be effective at bringing external/vendor images into our environments.
    Finally, we’ll discuss image trust and signing. This is an area that many organizations
    are interested in but where the upstream tooling and approaches are still maturing.
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerability Scanning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scanning images for known vulnerabilities is a key competency of most image
    registries. Usually the scanning itself along with the database of common vulnerabilities
    and exposures (CVEs) are delegated to a third-party component. Clair is a popular
    open source choice and, in many cases, is pluggable should you have specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Every organization will have its own requirements about what constitutes an
    acceptable risk when considering CVE scores. Registries will commonly expose controls
    that allow you to disable the pulling of images that contain CVEs over a defined
    score threshold. Additionally, the ability to add CVEs to an allowlist can be
    useful to bypass issues that are flagged but not relevant in your environment,
    or for those CVEs that are deemed acceptable risk and/or have no fixes released
    and available.
  prefs: []
  type: TYPE_NORMAL
- en: This static scanning at initial pull time can be useful to begin with, but what
    happens if vulnerabilities are discovered over time in the images we’re already
    using in the environment? Scans can be scheduled to detect these changes, but
    then we need to have a plan for updating and replacing the images. It can be tempting
    to automatically remediate (patch) and push out updated images, and there are
    solutions that will always try to keep images up-to-date. However, this can be
    problematic as image updates can introduce changes that may be incompatible and/or
    break the running application. These automated image update systems may also work
    outside of your designated deployment change process and could be hard to audit
    in the environment. Even the blocking of image pulls (described previously) can
    cause issues. If a core application’s image has a new CVE discovered and pulls
    are suddenly prohibited, this could cause availability issues in the application
    if those workloads are scheduled to new nodes and images are unavailable for pulling.
    As we’ve discussed many times in this book, it’s imperative to understand the
    trade-offs that you encounter when implementing each solution (in this case, security
    versus availability) and make informed, well-documented decisions.
  prefs: []
  type: TYPE_NORMAL
- en: A more common model than the automatic remediation described briefly is to alert
    and/or monitor on image vulnerability scans and bubble these up to operations
    and security teams. The implementation of this alerting may differ depending on
    the capabilities offered by your choice of registry. Some registries can be configured
    to trigger a webhook call on completion of a scan with the payload including details
    of the vulnerable images and discovered CVEs. Others may expose a scrapeable set
    of metrics with image and CVE details that can be alerted on using standard tooling
    (take a look at [Chapter 9](ch09.html#observability_chapter) for more details
    on metrics and alerting tools). While this method requires more manual intervention,
    it allows good visibility into the security state of images in your environment
    while also affording more control over how and when they are patched.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have CVE information for an image, then decisions about whether or not
    to patch the image (and when) can be made based on the impact of the vulnerability.
    If we need to patch and update the image we can trigger the update, testing, and
    deployment via our regular deployment pipelines. This ensures that we have full
    transparency and auditability and that those changes all go through our regular
    processes. We will discuss CI/CD and deployment models in more detail later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: While the static image vulnerability scanning covered in this subsection is
    a commonly implemented part of an organization’s software supply chain, it is
    only one layer of what should be a *defense in depth* strategy to container security.
    Images may download malicious content post-deployment or containerized applications
    may be compromised/hijacked at runtime. It’s essential therefore to implement
    some type of runtime scanning. In a more naive form, this could take the form
    of periodic filesystem scanning on running containers to ensure that no vulnerable
    binaries and/or libraries are introduced post-deployment. However, for more robust
    protection it’s necessary to limit the *actions* and *behaviors* that a container
    is capable of performing. This eliminates the inevitable game of whack-a-mole
    that can occur with CVEs being discovered and patched and instead focuses on the
    capabilities a containerized application should possess. Runtime scanning is a
    larger topic that we don’t have the space to fully cover here, but you should
    look into tools like [Falco](https://falco.org) and the [Aqua Security suite](https://github.com/aquasecurity).
  prefs: []
  type: TYPE_NORMAL
- en: Quarantine Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned, most registries provide a mechanism to scan images for known vulnerabilities
    and restrict image pulls. However, there may be additional requirements that images
    must satisfy before they can be used. We have also encountered the scenario where
    developers are unable to directly pull images from the public internet and must
    use an internal registry. Both of these use cases can be solved by using a multiregistry
    setup with a quarantine workflow pipeline described next.
  prefs: []
  type: TYPE_NORMAL
- en: First, we can provide developers with a self-service portal to request images.
    Something like ServiceNow or a Jenkins job works fine here, and we’ve seen this
    many times. Chatbots can also offer a more seamless integration for developers
    and are gaining popularity. Once an image is requested, it is automatically pulled
    to a *quarantine* registry where checks can be run on the image and the pipeline
    can spin up environments to pull and verify the image meets specific criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Once the checks pass, the image can be signed (this is optional, see [“Image
    Signing”](#image_signing) for more information) and pushed to an approved registry.
    The developer can also be notified (either via the chatbot, or an updated ticket/job,
    etc.) that the image has been approved (or rejected, and the reasoning). The whole
    flow can be seen in [Figure 15-2](#quarantine_flow).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1502](assets/prku_1502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-2\. Quarantine flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This flow can be combined with admission controllers to ensure that only images
    that are signed, or those that come from a specific registry, are allowed to be
    run on a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Image Signing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The issue of supply chain security is becoming more prevalent as applications
    come to rely on an increasing number of external dependencies, be those code libraries
    or container images.
  prefs: []
  type: TYPE_NORMAL
- en: One of the security features often mentioned when discussing images is the notion
    of signing. Very simply, the concept with signing is that an image publisher can
    cryptographically sign an image by generating a hash of the image and associating
    their identity with it before pushing it to a registry. Users are then able to
    verify the authenticity of an image by validating the signed hash against the
    publisher’s public key.
  prefs: []
  type: TYPE_NORMAL
- en: This workflow is attractive because it means we can create an image at the start
    of our software supply chain and sign it after each stage of the pipeline. Perhaps
    we can sign it after testing has been completed, and again after it has been approved
    for deployment by a release management team. Then at deploy time we are able to
    gate the deployment of the image into production based on whether it has been
    signed by the various parties that we specify. Not only do we ensure that it has
    passed those approvals, but we ensure that it is exactly the same image that is
    now being promoted to a production environment. This high-level flow is shown
    in [Figure 15-3](#signing_flow).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1503](assets/prku_1503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-3\. Signing flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The primary project in this area is Notary, which was originally developed by
    Docker and is built upon The Update Framework (TUF), a system designed to facilitate
    the secure distribution of software updates.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its benefits, we have not encountered much adoption of image signing
    in the field for a couple of reasons. First, Notary has several components including
    a server and multiple databases. These are additional components that need to
    be installed, configured, and maintained. Not only that, but because the ability
    to sign and verify images is usually in the critical path for software deployment,
    the Notary system must be configured for high availability and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, Notary requires each image be identified with a Globally Unique Name
    (GUN), which includes the registry URL as part of the name. This makes signing
    more problematic if you have multiple registries (e.g., caches, edge locations)
    as the signatures are tied to the registry and cannot be moved/copied.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Notary and TUF require different sets of key pairs to be used across
    the signing process. Each of the keys have different security requirements and
    can be challenging to rotate in the case of a security breach. While it provides
    an academically well-designed solution, the current Notary/TUF implementation
    posed too high of a barrier to entry for many organizations that were only just
    getting comfortable with some of the base technologies they were using. Thus,
    many weren’t ready to trade more convenience and knowledge for the additional
    security benefits that the signing workflow provided.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, there are efforts underway to develop and release a
    second version of Notary. This updated version should improve the user experience
    by solving many of the issues just discussed, like reducing the complexity of
    key management and eliminating the constraint that signatures are not transferable
    by bundling them with the OCI images themselves.
  prefs: []
  type: TYPE_NORMAL
- en: There are already several existing projects that implement an admission webhook
    that will check images to ensure they have been signed before allowing them to
    be run in a Kubernetes cluster. Once the issues are addressed, we anticipate signing
    becoming a more oft-implemented property of the software supply chain and these
    signing admission webhooks to mature even further.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections we’ve discussed in detail the process of converting
    source code into a container image. We also looked at where images are stored
    and the architectural and procedural decisions we need to make around choosing
    and deploying image registries. In this final section we’ll turn to examining
    the entire pipeline that ties these early steps up with the actual deployment
    of the image to potentially multiple Kubernetes clusters across many environments
    (test, staging, production).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover how to integrate the build process into the automated pipeline before
    looking at imperative, push-driven pipelines that many folks will already be familiar
    with. Lastly, we’ll take a look at some of the principles and tooling emerging
    in the field of GitOps, a relatively new approach to deployments that leverages
    version control repositories as the source of truth for the assets that should
    be deployed to our environments.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that continuous delivery is a huge area and is the sole subject
    of many books. In this section we’re assuming some knowledge of CD principles,
    and we’ll focus on how to implement those principles within Kubernetes and associated
    tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Builds into a Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For local development and testing phases, developers may build images with Docker
    locally. However, anything beyond those early phases and organizations will want
    to have builds performed as part of an automated pipeline triggered by the committing
    of code into a central version control repository. We’ll talk later in this chapter
    about more advanced patterns around the actual *deployment* of images into an
    environment, but in this section want to focus purely on how the build phases
    can also be run in cluster using cloud native pipeline automated tooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We typically want new image builds to be triggered with a code commit. Some
    pipeline tools will intermittently poll a set of configured repositories and trigger
    a task run when changes are detected. In other cases it might be possible to trigger
    a process to begin by firing a webhook from the version control system. We’ll
    use a few examples from Tekton, a popular open source pipeline tool that is designed
    to run on Kubernetes to illustrate some concepts in this section. Tekton (and
    many other Kubernetes native tools) utilize CRDs to describe components in the
    pipeline. In the following code, we can see a (heavily edited) instance of a `Task`
    CRD that can be reused across multiple pipelines. Tekton maintains a catalog of
    common actions (such as cloning a git repository, as shown in the following snippet)
    that can be utilized in your own pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in previous sections, there are many different ways of building
    OCI images. Some of these require a Dockerfile, and some do not. You may also
    need to perform additional actions as part of a build. Almost all pipeline tools
    expose the notion of stages, steps, or tasks that allow users to configure discrete
    pieces of functionality that can be chained together. The following code snippet
    shows an example `Task` definition that uses Cloud Native Buildpacks to build
    an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can then tie together this task (and others) with our input repository as
    part of a `Pipeline` (not shown here). This involves mapping the workspace we
    cloned our git repository into earlier with the workspace that our buildpack builder
    will use as a source. We can also specify that the image be pushed to a registry
    at the end of the process.
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of this approach (configurable task blocks) means that pipelines
    become very powerful tools for defining a build flow on Kubernetes. We could add
    testing and/or linting stages to the build, or some kind of static code analysis.
    We could also easily add a signing step to our image (as described in [“Image
    Signing”](#image_signing)) if desired. We can also define our own tasks to run
    other build tools such as Kaniko or BuildKit (if not utilizing buildpacks as in
    this example).
  prefs: []
  type: TYPE_NORMAL
- en: Push-Based Deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section we saw how to automate builds in a pipeline. In this
    section we’ll see how this can be extended to actually perform the deployment
    to a cluster and some of the patterns you’ll want to implement to make these types
    of automated delivery pipelines easier.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the flexibility of the task/step-based approach that we saw previously
    (which is present in almost every tool), it is trivial to create a step at the
    end of the pipeline that reads the tag of the newly created (and pushed) image
    and updates the image for the Deployment. This *could* be achieved by updating
    the Deployment directly in the cluster using `kubectl set image`, and several
    articles/tutorials still demonstrate this approach. A better alternative is to
    have our pipeline write the image tag change back into the YAML file describing
    the Deployment and then committing this change *back into version control*. We
    can then trigger a `kubectl apply` over the new version of the repository to enact
    the change. The latter approach is preferred as we can keep our YAML as the approximate
    source of truth for our cluster in that case (we’ll discuss more on this in [“GitOps”](#gitops))
    but the former is an acceptable iterative step when migrating to this type of
    Kubernetes-native automated pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'When deploying applications to Kubernetes we have two distinct types of artifacts
    to consider: the code and configuration required for the application and how to
    *build* it, and the configuration for how to *deploy* it. We are often asked to
    weigh in on how best to organize these artifacts, with some folks preferring to
    keep *everything* related to an application together in a single tree, and others
    preferring to keep them separate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our advice is usually to choose the latter route for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Each concern is usually the responsibility of a separate domain or team in the
    organization. While developers should be aware of how their applications will
    be deployed and will have input into the process, the configuration around sizing,
    environments, the injection of secrets, etc. mostly sit as responsibilities of
    the platform or operations team.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security permissions and audit requirements are likely different for code repositories
    versus those containing deployment pipeline artifacts, secrets, and environment
    configurations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have our deployment configurations in a separate repository, it’s easy
    to follow how the deployment pipeline might first checkout this repository, then
    run an update of the image tag (using `sed` or something similar), and finally
    *check the change back in to git* to ensure that is our source of truth. We can
    then run `kubectl apply -f` over the changed manifests. This imperative (or *push-based*)
    model provides great auditability as we can leverage the built-in reporting and
    logging capabilities provided by our version control system and easily see the
    changes flow through our pipeline, as shown in [Figure 15-4](#push_based_deployments).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1504](assets/prku_1504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-4\. Push-based deployments.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on the level of automation within your organization, you may want
    to have promotions between environments handled by your pipelines, and even deployments
    executed against different Kubernetes clusters. There are certainly ways to achieve
    this with most tools, and some will have better native support for this than others.
    However, this is an area where the imperative pipeline model described in this
    subsection can be more challenging to implement because we have to keep an inventory
    (and credentials) for each of the clusters we want to use as a target.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge of this imperative (where we have a centralized tool pushing
    changes out to our environments) is that if the pipeline is interrupted for some
    reason, we need to ensure that it is restarted or reconciled back to a healthy
    state. We also need to maintain monitoring and alerting on our deployment pipelines
    (however they are implemented) to make sure that we’re aware of deployment issues
    if they arise.
  prefs: []
  type: TYPE_NORMAL
- en: Rollout Patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mentioned briefly at the end of the previous section the need to monitor
    pipelines to ensure that they successfully complete. However, when deploying new
    versions of applications we also need a way to monitor their health and decide
    whether we need to resolve issues or roll back to a previous working state.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several patterns that organizations may want to implement. There
    are entire books dedicated to these patterns, but we’ll cover them briefly here
    to show how you might implement them in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Canary
  prefs: []
  type: TYPE_NORMAL
- en: Canary releases are where a new version of an application is deployed to the
    cluster and a small subset of traffic (based on metadata, users, or some other
    attribute) is directed to the new version. This can be monitored closely to ensure
    that the new version (Canary) behaves the same way as the previous version, or
    at least does not result in an error scenario. The percentage of traffic can slowly
    be increased over time as confidence increases.
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green
  prefs: []
  type: TYPE_NORMAL
- en: This approach is similar to the Canary but involves more of a big bang cutover
    of traffic. This could be achieved over multiple clusters (one on the old version,
    *blue*, and one of the new version, *green*) or could be achieved in the same
    cluster. The idea here is that we can test that deployment of the service works
    as intended and perform some tests on the environment which is not user facing
    before cutting traffic across to the new version. If we see elevated errors, we
    can cut the traffic back. There is additional nuance in this approach, of course,
    as your applications may need to gracefully handle state, sessions, and other
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing
  prefs: []
  type: TYPE_NORMAL
- en: Again similar to the Canary, we can roll out a version of the application that
    may contain some different behavior that targets a subset of consumers. We can
    gather metrics and analysis on the usage patterns of the new version to make decisions
    about whether to roll back or forward, or expand the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: These patterns move us toward the desired state of being able to decouple the
    *deployment* of our applications from their *release* to consumers by giving us
    control about when features and/or new versions are enabled. These practices are
    great at de-risking the deployment of changes into our environments.
  prefs: []
  type: TYPE_NORMAL
- en: Most of these patterns are implemented via some kind of network traffic shifting.
    In Kubernetes, we have some very rich networking primitives and capabilities that
    make the implementation of these patterns possible. One open source tool that
    enables these patterns (on top of various service mesh solutions) is Flagger.
    Flagger runs as a controller within the Kubernetes cluster and watches for changes
    to the image field of Deployment resources. It exposes many tweakable options
    to enable the preceding patterns by programmatically configuring an underlying
    service mesh to appropriately shift traffic. It also adds the ability to monitor
    the health of newly rolled-out versions and either continue or halt and revert
    the rollout process.
  prefs: []
  type: TYPE_NORMAL
- en: We definitely see the value of looking into Flagger and other solutions in this
    space. However, we see these approaches more typically broached as part of a second
    or third phase of an organization’s Kubernetes journey, given the additional complexity
    they both depend on (service mesh is required to enable most patterns) and introduce.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have looked at adding a push-based deployment stage into your delivery
    pipelines for Kubernetes. An emerging alternative model in the deployment space
    is GitOps. Rather than imperatively push out changes to the cluster, the GitOps
    model features a controller that constantly reconciles the contents of a git repository
    with resources running in the cluster, as shown in [Figure 15-5](#gitops_flow).
    This model brings it closely in line with the control-loop reonciliation experience
    that we get with Kubernetes itself. The two primary tools in the GitOps space
    are ArgoCD and Flux, and both teams are working together on a common engine to
    underpin their respective tools.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1505](assets/prku_1505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-5\. GitOps flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are a couple of major benefits to this model:'
  prefs: []
  type: TYPE_NORMAL
- en: It is declarative in nature, so that any issues with the deployment itself (tooling
    going down, etc.) or deployments being deleted ad hoc will result in (attempted)
    reconciliation to a good state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git becomes our single source of truth, and we can leverage existing expertise
    and familiarity with the tooling, in addition to getting a strong audit log of
    changes by default. We can use a pull request workflow as a gate for changes to
    our clusters and integrate with external tooling as required through the extension
    points that most version control systems expose (webhooks, workflows, actions,
    etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, this model is not without downsides. For those organizations that truly
    want to use git as their *single* source of truth, this means keeping secret data
    in version control. Several projects have emerged over recent years to solve this
    problem, with the most well-known of these being Bitnami’s Sealed Secrets. That
    project allows encrypted versions of Secrets to be committed to a repository and
    then decrypted once applied to the cluster (so as to be available to applications).
    We discuss this approach more in [Chapter 5](ch05.html#chapter5).
  prefs: []
  type: TYPE_NORMAL
- en: We also need to ensure that we are monitoring the current health of state synchronization.
    If the pipeline was push-based and fails, we’ll see a failure in the pipeline.
    However, because the GitOps approach is declarative, we need to make sure we get
    alerted if the observed state (in cluster) and the declared state (in git) remain
    diverged for an extended period of time.
  prefs: []
  type: TYPE_NORMAL
- en: Increased embracing of GitOps is a trend we see in the field, although it’s
    definitely a paradigm shift from more traditional push-based models. Not all applications
    deploy cleanly as-is with a flat application of YAML resources, and it may be
    necessary to build in ordering and some scripting initially as organizations transition
    to this approach.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to be cognizant of tools that may create, modify, or delete
    resources as part of their life cycle as these sometimes require some massaging
    to fit into the GitOps model. An example of this would be a controller that runs
    in the cluster and watches for a specific CRD and then creates multiple other
    resources directly via the Kubernetes API. If running in *strict* mode, GitOps
    tooling may delete those dynamically created resources as they are not in the
    single source of truth (the git repo). This deletion of *unknown* resources may,
    of course, be desirable in most cases and is one of the *positive* attributes
    of GitOps. However, you should absolutely be mindful of situations where changes
    may intentionally occur out of band from the git repository that may break the
    model and need to be worked around.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we looked at the process of getting source code into a container
    and deployed onto a Kubernetes cluster. Many of the stages and principles that
    you will already be familiar with (build/test, CI, CD, etc.) still apply in a
    container/Kubernetes environment but with different tooling. At the same time
    some concepts (like GitOps) will likely be new, building on concepts that are
    present in Kubernetes itself to enhance reliability and security within existing
    deployment patterns.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools in this space that can enable many different flows and
    patterns. However, one of the key takeaways from this chapter should be the importance
    of deciding how much (or little) to expose each part of this pipeline to different
    groups in the organization. Maybe development teams are engaged with Kubernetes
    and confident enough to write build and deployment artifacts (or at least have
    significant input). Or maybe the desire is toward abstracting all the underlying
    details away from development teams to ease scaling and standardization concerns,
    at the expense of additional burden on the platform teams to put relevant foundations
    and automation in place.
  prefs: []
  type: TYPE_NORMAL
