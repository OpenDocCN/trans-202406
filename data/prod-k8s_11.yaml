- en: Chapter 10\. Identity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Establishing the identity of both users and application workloads is a key
    concern when designing and implementing a Kubernetes platform. No one wants to
    be in the news for having their systems breached. So it’s vital that we ensure
    that only the appropriately privileged entities (human or application) can access
    particular systems or take certain actions. For this we need to ensure that there
    are both Authentication and Authorization systems implemented. As a refresher:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Authentication* is the process of establishing the identity of an application
    or user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Authorization* is the process of determining what actions an application or
    user are able to do, after they have been authenticated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter is solely focused on authentication. That’s not to say that authorization
    is not important, and we will touch on it briefly where appropriate. For more
    information you should definitely research Role Based Access Control (RBAC) in
    Kubernetes (there are many great resources available) and ensure that you have
    a solid strategy for implementing it for your own applications, so that you understand
    the permissions that are required by any external applications that you might
    deploy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Establishing identity for the purposes of authentication is a key requirement
    of almost every distributed system. A simple example of this that everyone has
    used is a username and password. Together, the information identifies you as a
    user of the system. In this context then, *identity* needs to have a couple of
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: It needs to be verifiable. If a user enters their username and password, we
    need to be able to go to a database or source of truth and compare the values
    to make sure they’re correct. In the case of a TLS certificate that might be presented,
    we need to be able to verify that certificate against a trusted issuing Certificate
    Authority (CA).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It needs to be unique. If an identity provided to us is not unique, we cannot
    specifically identify the bearer. However, we need to maintain uniqueness only
    *within our desired scope*—for example, a username or email address.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing identity is also a crucial precursor to handling authorization
    concerns. Before we can determine what scope of resource access should be granted,
    we need to uniquely identify the entity that is authenticating to the system.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes clusters commonly serve multiple tenants where many users and teams
    are deploying and operating multiple applications in a single cluster. Solving
    for tenancy in Kubernetes presents challenges (many covered in this book), of
    which one is identity. Given the matrix of privileges and resources that must
    be considered, we must solve for many deployment and configuration scenarios.
    Development teams should have access to their applications. Operations teams should
    have access to all applications and might need access to platform services. Application-to-application
    communication should be limited among applications. Then the list goes on. What
    about shared services? Security teams? Deployment tooling?
  prefs: []
  type: TYPE_NORMAL
- en: These are all common concerns and add significant complexity to cluster configuration
    and maintenance. Remember, we have to keep these privileges updated somehow as
    well. These things are easy to get wrong. But the good news is that Kubernetes
    has capabilities that allow us to integrate with external systems and to model
    identity and access controls in a secure fashion.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll begin by discussing *user* identity and the different
    methods for authenticating users to Kubernetes. We’ll then move on to options
    and patterns for establishing *application* identity within a Kubernetes cluster.
    We’ll see how to authenticate applications to the Kubernetes API server (for writing
    tools that interact directly with Kubernetes, such as operators). We’ll also cover
    how to establish unique application identities to enable those applications to
    authenticate to each other within the cluster, in addition to authenticating to
    *external* services like AWS.
  prefs: []
  type: TYPE_NORMAL
- en: User Identity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we’ll cover the methods and patterns for implementing a robust
    system of *user* identity across your Kubernetes cluster(s). In this context we’re
    defining a user as a human who will be interacting with the cluster directly (either
    through the Kubectl CLI or the API). The properties of identity (described in
    the previous section) are common to user and application identity, but some of
    the methods will differ. For example, we always want our identities to be verifiable
    and unique; however, these properties will be achieved in different ways for a
    user utilizing OpenID Connect (OIDC) versus an application using service account
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of different authentication methods available to Kubernetes
    operators, and each have their own strengths and weaknesses. In keeping with the
    core theme of this book, it’s essential to understand your specific use cases,
    evaluate what’s going to work for you, integrate with your systems, provide the
    user experience (UX), and deliver the security posture that your organization
    requires.
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll cover each method of establishing *user* identity and
    their trade-offs while describing some commonly used patterns we’ve implemented
    in the field. Some of the methods described here are platform-specific and tied
    to functionality available in certain cloud vendors, while others are platform-agnostic.
    How well a system integrates into your existing technology landscape is definitely
    going to be a factor in determining whether to adopt it. The trade-off is between
    extra functionality available in new tooling versus the ease-of-maintenance of
    integrations with the incumbent stack.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to providing identity, some of the methods described may also provide
    encryption. For example, the flow described for the public key infrastructure
    (PKI) method provides certificates that could be used in Mutual Transport Layer
    Security (mTLS) communication. However, encryption is not the focus of this chapter
    and is an incidental benefit from those methods of identity grants.
  prefs: []
  type: TYPE_NORMAL
- en: Shared secrets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A shared secret is a unique piece (or set) of information that is held by the
    calling entity and the server. For example, when an application needs to connect
    to a MySQL database, it can use a username and password combination to authenticate.
    This method necessitates that both parties have access to that combination in
    some form. You must create an entry in MySQL with that information, and then distribute
    the secret to any calling application that may need it. [Figure 10-1](#shared_secrets_flow)
    shows this pattern, with the backend application storing valid credentials that
    need to be presented by the frontend to gain access.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1001](assets/prku_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Shared secrets flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes provides two options that allow you to utilize a shared secret model
    to authenticate to the API server. In the first method you can give the API server
    a list of comma-separated values (CSV) mapping usernames (and optionally, groups)
    to static tokens. When you want to authenticate to the API server, you can provide
    the token as a Bearer token within the HTTP Authorization header. Kubernetes will
    treat the request as coming from the mapped user and act accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The other method is to supply the API server with a CSV of username (and optionally,
    groups) and password mappings. With this method configured, users can supply the
    credentials base64-encoded in the HTTP Basic Authorization header.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes has no resource or object called User or Group. These are just predefined
    names for the purposes of identification within RBAC RoleBindings. The user can
    be mapped from a static file to token or password (as described previously), can
    be pulled from the CN of an x509 cert, or can be read as a field from an OAuth
    request, etc. The method of determining the user and group is entirely dependent
    on the authentication method in use, and Kubernetes has no way to define or manage
    them internally. In our opinion this pattern is a strength of the API because
    it allows us to plug in a variety of different implementations and delegate those
    concerns to systems specifically designed to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these methods have serious weaknesses and are not recommended. Some
    of these weaknesses are due to the Kubernetes-specific implementation, and some
    are inherent to the shared secret model, which we’ll discuss shortly. In Kubernetes,
    the main issues are:'
  prefs: []
  type: TYPE_NORMAL
- en: Static token and/or password files must be stored (in plain text) somewhere
    accessible to the API server. This is less of a risk than it initially seems,
    because if someone was able to compromise an API server and gain access to that
    node you would have greater things to worry about than an unencrypted password
    file. However, Kubenetes installations are mostly automated, and all assets required
    for setup should be stored in a repository. This repository must be secured, audited,
    and updated. This opens another potential area for carelessness or bad practices
    to leak credentials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the static tokens and the username/password combinations have no expiration
    date. If any credentials are compromised, the breach must be identified quickly
    and remediated by removing the relevant credentials and restarting the API server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any modifications to these credential files require that the API server is restarted.
    In practice (and in isolation) this is fairly trivial. However, many organizations
    are rightly moving away from manual intervention into their running software and
    servers. Changing configurations is now mostly a rebuild and redeploy process
    versus simply SSH’ing into the machines (cattle over pets). Therefore, modifying
    API server configurations and restarting the processes is likely a more involved
    action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outside of the Kubernetes-specific disadvantages just described, the shared
    secrets model suffers from another drawback. If I am an untrusted entity, how
    can I authenticate to a secret store *in the first place* to receive an appropriate
    identity? We’ll look more at this *secure introduction* problem and how to solve
    it in [“Application/Workload Identity”](#app_workload_security).
  prefs: []
  type: TYPE_NORMAL
- en: Public key infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section assumes you are already familiar with PKI concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The PKI model uses certificates and keys to uniquely identify and authenticate
    users to Kubernetes. Kubernetes makes extensive use of PKI to secure communications
    between all of the core components of the system. It’s possible to configure the
    Certificate Authorities (CA) and certificates in multiple ways, but we will demonstrate
    it using kubeadm, the method most commonly seen in the field (and the de facto
    installation method for upstream Kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing a Kubernetes cluster, you typically get a kubeconfig file
    with the `kubernetes-admin` user details. This file is essentially the root key
    to the cluster. Usually, this kubeconfig file is called *admin.conf* and is similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine the user that this will authenticate us to the cluster with, we
    need to first base64 decode the `client-certificate-data` field and then display
    the contents using something like `openssl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We see from the certificate that it was issued by the Kubernetes CA, and identifies
    the User as `kubernetes-admin` (the subject `CN` field) in the `system:masters`
    group. When using x509 certificates, any Organizations (O=) present are treated
    by Kubernetes as groups that the user should be considered part of. We will discuss
    some advanced methods around user and group configuration and permissions later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example we saw the default configuration for the `kubernetes-admin`
    user, a reserved default name that enables cluster-wide administrative privileges.
    It would also be useful to see how to configure the provisioning of certificates
    to identify other regular system users who can then be given appropriate permissions
    using the RBAC system. Provisioning and maintaining a large set of certificate
    artifacts is an arduous task, but one that Kubernetes can help us with by using
    some built-in resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for the CSR flow described next to function correctly, the controller-manager
    needs to be configured with the `--cluster-signing-cert-file` and `--cluster-signing-key-file`
    parameters as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Any entity with appropriate RBAC permissions can submit a Certificate Signing
    Request object to the Kubernetes API. If a user should be able to *self submit*,
    this means we need to provide a mechanism for the user to submit those requests.
    One way of doing this is to explicitly configure permissions to allow the `system:anonymous`
    User and / or `system:unauthenticated` group to submit and retrieve CSRs.
  prefs: []
  type: TYPE_NORMAL
- en: Without this, any unauthenticated user would by definition be unable to initiate
    the process that would allow them to become authenticated. We should definitely
    be wary of this approach, though, as we never want to give unauthenticated users
    any access to the Kubernetes API server. A common way of providing self-service
    for CSRs is therefore to provide a thin abstraction or portal on top of Kubernetes
    that will run with the appropriate permissions. Users can log in to the portal
    using some other credentials (usually SSO) and initiate this CSR flow (as shown
    in [Figure 10-2](#csr_flow)).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1002](assets/prku_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. CSR flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this flow the user could generate a private key locally and then submit
    this through the portal. Or, the portal could generate private keys for each user
    and return them with the approved certificate to the user. Generation can be done
    using `openssl` or any number of other tools/libraries. The CSR should contain
    the metadata the user wants encoded into their x509 certificate, including their
    user name and any additional groups they should be part of. The following example
    creates a certificate request that identifies the user as *john*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After generating the CSR we can submit it to the cluster via our portal in
    a CertificateSigningRequest resource. Following is an example of the request as
    a YAML object, but our portal would likely programatically apply this via the
    Kubernetes API rather than constructing the YAML manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This creates a CSR object in Kubernetes with a `pending` state, awaiting approval.
    This CSR object contains the (base64-encoded) signing request and the username
    of the requestor. If using a Service Account token to authenticate to the Kubernetes
    API (as a Pod would in an automated flow), then the username will be the Service
    Account name. In the following example, I was authenticated to the Kubernetes
    API as the `kubernetes-admin` user and it appears in the Requestor field. If using
    a portal we’d see the Service Account assigned to that portal component.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: While the request is pending, the user has not been granted any certificate.
    The next stage involves a cluster administrator (or a user with appropriate permissions)
    approving the CSR. This may also be automated if the user’s identity can be programatically
    determined. Approval will issue a certificate back to the user that can be used
    to assert identity on that Kubernetes cluster. For this reason, it’s important
    to perform verification that the submitter of the request *is* who they claim
    to be. This could be achieved by adding some additional identifying metadata to
    the CSR and having an automated process validate the information against the claimed
    identity, or by having an out-of-band process to verify the user’s identity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the CSR has been approved, the certificate (in the `status` field of the
    CSR) can be retrieved and used (in conjunction with their private key) for TLS
    communications with the Kubernetes API. In our portal implementation, the CSR
    would be pulled by the portal system and made available for the requesting user
    once they log back in and recheck the portal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When decoding the certificate we can see that it contains the relevant identity
    information (*john*) in the CN field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can craft a kubeconfig containing our private key and the approved
    certificate that will allow us to communicate with the Kubernetes API server as
    the *john* user. The certificate we get back from the preceding CSR process goes
    into the `client-certificate-data` field shown here in the kubeconfig:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We have seen implementations of this model in the field whereby an automated
    system provisions certificates based on some verifiable SSO credentials or other
    authentication method. When automated, these systems can be successful, but we
    do not recommend them. Relying on x509 certificates as a primary authentication
    method for users of Kubernetes introduces a number of issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Certificates provisioned through the Kubernetes CSR flow cannot be revoked prior
    to their expiry. There is currently no support for certificate revocation lists
    of Online Certificate Status Protocol (OSCP) stapling in Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional PKI needs to be provisioned, supported, and maintained, in addition
    to creating and maintaining a component responsible for provisioning certificates
    based on external authentication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x509 certificates have expiry timestamps, and these should be kept relatively
    short to reduce the risk should a pair (key/cert) be compromised. This short life
    span means that there is a high churn in certificates, and these must be distributed
    out to users regularly to ensure that consistent access to the cluster is maintained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There needs to be a way to verify the identity of anyone requesting a certificate.
    In an automated system, you can engineer ways of doing this via externally verifiable
    metadata. In the absence of such metadata, out-of-band verification is often too
    time-consuming to be practical, especially given the short life span of certificates
    as noted previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certificates are are localized to one cluster. In the field we see many (10s–100s)
    Kubernetes clusters across projects and groups. Requiring unique credentials for
    each cluster multiplies the complexity of storing and maintaining the relevant
    credentials. This leads to a degraded user experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that even when not using certificates as the primary authentication
    method, you should still keep the *admin.conf* kubeconfig somewhere secure. If
    for whatever reason other authentication methods become unavailable, this can
    act as an admin break-glass solution to access the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: OpenID Connect (OIDC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our opinion, the best choice when setting up user authentication and identity
    with Kubernetes is to integrate with an existing Single Sign-On system or provider.
    Almost every organization already has a solution such as Okta, Auth0, Google,
    or even internal LDAP/AD that provides a single place for users to authenticate
    and gain access to internal systems. For something like authentication (where
    security is a strong factor), outsourcing the complexity is a solid choice unless
    you have very specialized requirements.
  prefs: []
  type: TYPE_NORMAL
- en: These systems have many advantages. They are built on well-understood and widely
    supported standards. They consolidate all management of user accounts and access
    to a single well-secured system, making maintenance and removal of accounts/access
    straightforward. Additionally, when using the common OIDC framework, they also
    allow users to access downstream applications without exposing credentials to
    those systems. Another advantage is that many Kubernetes clusters across multiple
    environments can leverage a single identity provider, reducing variance between
    cluster configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports OIDC directly as an authentication mechanism (as shown in
    [Figure 10-3](#oidc_flow)). If your organization is using an identity provider
    that natively exposes the relevant OIDC endpoints, then configuring Kubernetes
    to take advantage of this is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1003](assets/prku_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. OIDC flow. Reproduced from the [official Kubernetes documentation](https://oreil.ly/VZCz5).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However, there are several scenarios where some extra tooling may be required
    or desired to provide additional functionality or improve user experience. Firstly,
    if your organization has multiple identity providers it is necessary to utilize
    an OIDC aggregator. Kubernetes only supports defining a single identity provider
    in its configuration options, and an OIDC aggregator is capable of acting as a
    single intermediary to multiple other providers (either OIDC or other methods).
    We have used Dex ([a sandbox project](https://oreil.ly/_maX6) within the Cloud
    Native Computing Foundation) with success many times before, although other popular
    options like Keycloak and UAA offer similiar functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that authentication is in the critical path to cluster access. Dex,
    Keycloak, and UAA are all configurable to variable degrees and you should optimize
    for availability and stability when implementing these solutions. These tools
    are additional maintenance burdens and must be configured, updated, and secured.
    In the field we always try to emphasize the need to understand and own any additional
    complexity that is introduced to your environment and clusters.
  prefs: []
  type: TYPE_NORMAL
- en: While configuring the API server to utilize OIDC is straightforward, attention
    must be given to providing a seamless user experience for the users of the cluster.
    OIDC solutions will return a token identifying us (given a successful login);
    however, we require a properly formatted kubeconfig in order to access and perform
    operations on the cluster. When we hit this use case in the field early on, our
    colleagues developed a simple web UI called Gangway to automate the process of
    logging in through an OIDC provider and generating a conformant kubeconfig from
    the returned token (complete with relevant endpoints and certificates).
  prefs: []
  type: TYPE_NORMAL
- en: Despite OIDC being our preferred method of authentication, it does not suit
    all cases, and secondary methods may be required. OIDC (as defined in the specification)
    requires users to log in directly through the web interface of the identity provider.
    This is for obvious reasons, to ensure the user is actually providing the credentials
    only to the trusted provider and not to the consuming application. This requirement
    can cause issues in the case where robot users require access to the system. This
    is common for automated tooling like CI/CD systems and others who are unable to
    respond to the web-based credential challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these cases, we have seen a couple of different models/solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: In cases where robot users are tied to centrally managed accounts, it’s possible
    to implement a kubectl authentication plug-in that would log in to the external
    system and receive a token in response. Kubernetes can be configured to verify
    this token via the webhook token authenticator method. This method will likely
    require some custom coding to create the token generator/webhook server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For other cases, we have seen users fall back to using a certificate-based auth
    for robot accounts that don’t need to be centrally managed. This of course means
    you need to manage certificate issuance and rotation, but it doesn’t require any
    custom components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another manual but effective alternative solution is to create a Service Account
    for the tool and utilize the token generated for API access. If the tool is running
    *in* cluster it can use the credential directly mounted into the Pod. If the tool
    is *outside* the cluster we can manually copy and paste the token into a secure
    location accessible by the tool and utilize that when making kubectl or API calls.
    Service Accounts are covered in more detail in [“Service Account Tokens (SAT)”](#service_account_tokens).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Least Privilege Permissions for Users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve seen the different ways it’s possible to implement identity and
    authentication, let’s turn to the related topic of authorization. It’s out of
    scope for this book to go deep into how you should be configuring RBAC across
    your clusters. This will likely vary significantly between applications, environments,
    and teams. However, we do want to describe a pattern we’ve implemented successfully
    in the field around the principle of least privilege when designing administrative
    access roles.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you have chosen to go with a cluster-per-team approach, or a multitenant
    cluster approach, you will likely have *super-admin* users on the operations team
    who are responsible for configuring, upgrading, and maintaining the environment.
    While individual teams should have restricted permissions based on the access
    they require, these admins will have full reign over the entire cluster and therefore
    greater potential to accidentally perform destructive actions.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, all cluster access and operations would be performed by an
    automated process—GitOps or something similar, perhaps. However, practically speaking,
    we regularly see users individually accessing clusters and found the following
    pattern to be an effective way to limit potential issues. It is tempting to bind
    an administrator role to a particular operator’s username/identity directly, only
    for them to delete something important while mistakenly having loaded the wrong
    kubeconfig, for example. It should never happen until it does!
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports the concept of *impersonation*, and with this we can create
    an experience that behaves closely to *sudo* on Linux systems by restricting the
    default permissions of the user and requiring them to elevate permissions to perform
    sensitive commands. Practically speaking, we want to enable these users to view
    everything by default but deliberately elevate their privileges to be able to
    write. This model significantly reduces the chances of the preceding scenario
    occurring.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s work through how you might implement the privilege elevation pattern just
    described. We’ll assume that our operations team’s user identities are all part
    of an `ops-team` group in Kubernetes. As mentioned earlier, Kubernetes has no
    defined concept of a group per se, so we mean that those users all have additional
    attributes in their Kubernetes identity (x509 cert, OIDC claim, etc.) that identify
    them as being part of the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the ClusterRoleBinding that allows users in the `ops-team` group
    access to the `view` built-in ClusterRole, which is what gives us our default
    read-only access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create a ClusterRoleBinding to allow our `cluster-admin` user to have
    `cluster-admin` ClusterRole permissions on the cluster. Remember, we’re not binding
    this ClusterRole directly to our `ops-team` group. No user can *directly* identify
    as the `cluster-admin` user; this will be a user that is *impersonated* and their
    permissions *assumed* by another authenticated user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create a ClusterRole called `cluster-admin-impersonator` that allows
    the impersonation of the `cluster-admin` user, and a ClusterRoleBinding that binds
    that capability to everyone in the `ops-team` group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s use a kubeconfig for a user (john) in the `ops-team` group to see
    how the elevation of privileges works in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We used the preceding setup for admin users, although implementing something
    similiar for every user (having a *team-a* group, a *team-a* view role, and a
    *team-a* admin user) is a solid pattern that removes a lot of the potential for
    costly mistakes. Additionally, one of the great things about the impersonation
    approach just described is that all of this is played out in the Kubernetes audit
    logs, so we can see the original user log in, impersonate the cluster-admin, and
    then take action.
  prefs: []
  type: TYPE_NORMAL
- en: Application/Workload Identity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we saw the main methods and patterns for establishing
    the identity of human users of Kubernetes, and how they can authenticate to the
    cluster. In this section, we’re going to take at look how we can establish identity
    for our workloads that run in the cluster. There are three main use cases we’ll
    be examining:'
  prefs: []
  type: TYPE_NORMAL
- en: Workloads identifying themselves to other workloads within the cluster, to potentially
    establish a mutual authentication between them for additional security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workloads identifying themselves to obtain appropriate access to the Kubernetes
    API itself. This is a common use case for custom controllers that need to watch
    and act on Kubernetes resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workloads identifying themselves and authenticating to external services. This
    could be anything outside of the cluster but will be primarily cloud vendor services
    running on AWS, GCP, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [“Network Identity”](#networking), we’ll look at two of the most popular
    Container Networking Interface (CNI) tools (Calico and Cilium) and see how they
    can assign identity and restrict access, primarily for the first use case we just
    described.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we’ll move on to service account tokens (SAT) and projected service
    account tokens (PSAT). These are flexible and important Kubernetes primitives
    that enable workload-to-workload identity (the first use case) in addition to
    being the primary mechanism for workloads identifying to the Kubernetes API itself
    (the second use case).
  prefs: []
  type: TYPE_NORMAL
- en: Next we’ll cover options where an application’s identity is provided by the
    platform itself. The most common use case we see in the field is workloads that
    need access to AWS services, and we’ll look at the three main methods that are
    possible today.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we’ll extend the concept of platform-mediated identity to consider tooling
    that aims to provide a consistent model of identity across multiple platforms
    and environments. The flexibility of this approach can be used to cover all of
    the use cases we mentioned, and we’ll show how this can be a very powerful capability.
  prefs: []
  type: TYPE_NORMAL
- en: Before implementing any of the patterns described in this section you should
    definitely evaluate your requirements as they relate to establishing workload-to-workload
    identity. Often, establishing this capability is an advanced-level activity, and
    the majority of organizations may not need to solve this topic, at least initially.
  prefs: []
  type: TYPE_NORMAL
- en: Shared Secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the discussion around shared secrets for user identity also applies
    to application identity; however, there are some additional nuances and guidance
    based on field experience.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have secrets in place that are known by the client and the server, how
    do we safely rotate them upon expiry? Ideally we want these secrets to have a
    fixed life span to mitigate the potential damage caused if one were to be compromised.
    Additionally, because they are *shared*, they need to be redistributed to both
    the client application and the server. Hashicorp’s Vault is a prominent example
    of an enterprise secret store and features integrations with many tools that get
    close to the goal of solving this re-syncing problem. However, Vault also suffers
    from the secure introduction problem we first encountered in [“User Identity”](#user_identity).
  prefs: []
  type: TYPE_NORMAL
- en: This is the problem we have when trying to ensure that a shared secret is securely
    distributed to both the client and the serving entity *before* we have any model
    of identity and authentication established (the chicken and the egg). Any attempt
    to initially seed a secret between two entities could be compromised, breaking
    our guarantee of identity and unique authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the flaws already discussed, shared secrets have one strong advantange
    in that it is a model that is well supported and understood by almost all users
    and applications. This makes it a strong choice for cross-platform operability.
    We will see how to solve the secure introduction problem for Vault and Kubernetes
    with more advanced methods of authentication later in this chapter. Once Vault
    is securely configured with those methods, it is a fine choice (and one we have
    implemented many times) as many of the issues with shared secrets are mitigated.
  prefs: []
  type: TYPE_NORMAL
- en: Network Identity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network primitives like IP addresses, VPNs, firewalls, etc., have historically
    been used as a form of identity for controlling which applications have access
    to what services. However, in a cloud native ecosystem these methods are breaking
    down and paradigms are changing. In our experience it is important to educate
    teams across the organization (especially networking and security) on these changes
    and how practices can (and should) adapt to embrace and accommodate them. Too
    often this is met with resistance around concerns over security and/or control.
    In reality it’s possible to achieve almost any posture if required, and time should
    be taken to understand the *actual requirements* of the teams, rather than getting
    stuck in implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: In container-based environments, workloads share networking stacks and underlying
    machines. Workloads are increasingly ephemeral and move between nodes often. This
    results in a constant churn of IP addresses and network changes.
  prefs: []
  type: TYPE_NORMAL
- en: In a multicloud and API-driven world, the network is no longer a primary boundary.
    Calls commonly occur to external services across multiple providers, each of which
    may need a way to prove identity of our calling applications.
  prefs: []
  type: TYPE_NORMAL
- en: Existing traditional (platform level) network primitives (host IP addresses,
    firewalls, etc.) are no longer suitable for establishing workload identity and,
    if used at all, should be used only as an additional layer of defense in depth.
    This is not to say that network primitives *in general* are bad but that they
    must have additional workload context to be effective. In this section we’ll look
    at how CNI options provide degrees of identity for Kubernetes clusters and how
    best to leverage them. CNI providers are able to contextualize requests and provide
    identity by combining network primitives and metadata retrieved from the Kubernetes
    API. We’ll take a brief look at some of the most popular CNI implementations and
    see what capabilities can they can provide.
  prefs: []
  type: TYPE_NORMAL
- en: Calico
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Calico](https://www.projectcalico.org) provides network policy enforcement
    at layers 3 (Network) and 4 (Transport) of the OSI Model, enabling users to restrict
    communication between Pods based on their Namespace, labels, and other metadata.
    This enforcement is all enabled by modifying the network configuration `iptables`/`ipvs`)
    to allow/disallow IP addresses.'
  prefs: []
  type: TYPE_NORMAL
- en: Calico also supports making policy decisions based on Service Accounts using
    a component called Dikastes when used in combination with [Envoy proxy](https://www.envoyproxy.io)
    (either standalone Envoy or deployed as part of a service mesh like [Istio](https://istio.io)).
    This approach enables enforcement at layer 7 (Application), based on attributes
    of the application protocol (headers, etc.) and relevant cryptographic identities
    (certificates, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Istio (Envoy) will only perform mTLS and ensure that workloads
    present certificates signed by the Istio CA (Citadel). Dikastes runs as a sidecar
    alongside Envoy as a plug-in, as we can see in the architecture diagram in [Figure 10-3](#oidc_flow).
    Envoy verifies the CA before consulting Dikastes for a decision on whether to
    admit or reject the request. Dikastes makes this decision based on user-defined
    Calico NetworkPolicy or GlobalNetworkPolicy objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding rule is specifying that the policy be applied to any Pods with
    the label `app: summary` and restricts access to Pods calling from the `customer`
    Service Account (in Namespaces with the label `app: bank`). This works because
    the Calico control plane (the Felix node agent) computes rules by reconciling
    Pods that are running under a specific Service Account with their IP addresses
    and subsequently syncing this information to Dikastes via a Unix socket.'
  prefs: []
  type: TYPE_NORMAL
- en: This out-of-band verification is important as it mitigates a potential attack
    vector in an Istio environment. Istio stores each Service Account’s PKI assets
    in a Secret in the cluster. Without this additional verification, an attacker
    who was able to steal that Secret would be able to masquerade as the asserted
    Service Account (by presenting those PKI assets), even though it may not be running
    as that account.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1004](assets/prku_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Architecture diagram using Dikastes with Envoy.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your team is leveraging Calico already, then Dikastes can provide an extra
    layer of defense in depth and should definitely be considered. However, it requires
    Istio or some other mesh solution (e.g., standalone Envoy) to be available and
    running in the environment to validate the identity presented by the workload.
    These claims are not independently cryptographically verifiable, relying on the
    mesh to be present with every connected Service. This in itself adds a nontrivial
    level of complexity, and the trade-offs should be carefully evaluated. One strength
    of this approach is that Calico and Istio are both cross-platform, so this setup
    could be used to establish identity for applications running both on and off Kubernetes
    within an environment (whereas some options we’ll see are Kubernetes-only).
  prefs: []
  type: TYPE_NORMAL
- en: Cilium
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like Calico, [Cilium](https://docs.cilium.io) also provides network policy enforcement
    at layers 3 and 4, enabling users to restrict communication between Pods based
    on their Namespace and other metadata (labels, for example). Cilium also supports
    (without additional tooling) the ability to apply policy at layer 7 and restrict
    access to Services via Service Accounts.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Calico, enforcement in Cilium is not based on IP address (and updating
    node networking configurations). Instead, Cilium calculates identities for each
    unique Pod/endpoint (based on a number of selectors) and encodes these identities
    into each packet. It then enforces whether packets should be allowed based on
    these identities using [eBPF](https://oreil.ly/Jl9yw) kernel hooks at various
    points in the datapath.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s briefly explore how Cilium calculates identities for an endpoint (Pod).
    The output of listing Cilium endpoints for an application is shown in the following
    code. We have omitted the list of labels in the snippet but have added an additional
    label to the last Pod in the list (`deathstar-657477f57d-zzz65`) that is not present
    on the other four Pods. As a result of this, we can see that the last Pod is therefore
    assigned a *different* identity to the previous four. Aside from that single differing
    label, all the Pods in the Deployment share a Namespace, Service Account, and
    several other arbitrary Kubernetes labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If we removed the divergent label, the `deathstar-657477f57d-zzz65` Pod would
    be reassigned the same identity as its four peers. This level of granularity gives
    us a lot of power and flexibility when assigning identities to individual Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cilium implements the Kubernetes-native NetworkPolicy API, and like Calico
    also exposes more fully featured capabilities in the form of CiliumNetworkPolicy
    and CiliumClusterwideNetworkPolicy objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are using special `io.cilium.k8s.policy.*` label selectors
    to target specific Service Accounts in the cluster. Cilium then uses its registry
    of identities (that we saw previously) to restrict/allow access as necessary.
    In the policy shown, we are restricting access to the path `/public` on port 80
    for Pods with the `leia` Service Account. We are allowing access only from Pods
    with the `luke` Service Account.
  prefs: []
  type: TYPE_NORMAL
- en: Like Calico, Cilium is cross-platform so can be used across Kubernetes and non-Kubernetes
    environments. Cilium *is* required to be present with every connected Service
    for identities to be verifiable, so the overall complexity of your networking
    setup can increase with this approach. However, Cilium doesn’t require a service
    mesh component to operate.
  prefs: []
  type: TYPE_NORMAL
- en: Service Account Tokens (SAT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Service Accounts are primitives in Kubernetes that provide identity for groups
    of Pods. Every Pod runs under a Service Account. If a Service Account is not pre-created
    by an administrator and assigned to a Pod, they are assigned a default Service
    Account for the Namespace they reside in.
  prefs: []
  type: TYPE_NORMAL
- en: Service Account tokens are JSON Web Tokens (JWT) that are created as Kubernetes
    Secrets. Each Service Account (including the default Service Account) has a corresponding
    Secret that contains the JWT. Unless otherwise specified, these tokens are mounted
    into each Pod running under that Service Account and can be used to make requests
    to the Kubernetes API (and as this section shows, other services).
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes Service Accounts provide a way of assigning identity to a set of
    workloads. Role-Based Access Control (RBAC) rules then can be applied within the
    cluster to limit the scope of access for a specific Service Account. Service Accounts
    are the way that Kubernetes itself usually authenticates in-cluster access to
    the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When a Service Account is created, an associated Secret is also created containing
    a unique JWT identifying the account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: By default, Pods will automatically get the `default` Service Account token
    for their Namespace mounted if they do not specify a specific Service Account
    to use. This can (and should) [be disabled](https://oreil.ly/kX5mI) to ensure
    that all Service Account tokens are explicitly mounted to Pods and their access
    scopes are well understood and defined (rather than falling back and assuming
    a default).
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify a Service Account for a Pod, use the `serviceAccountName` field
    in the Pod spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This will cause the Service Account’s Secret (containing the token) to be mounted
    into the Pod at `/var/run/secrets/kubernetes.io/serviceaccount/`. The application
    can retrieve the token and use it in a request to other applications/services
    in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The destination application can verify the provided token by calling the Kubernetes
    `TokenReview` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_identity_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This token is the Secret mounted into the destination application’s Pod, allowing
    it to communicate with the API server.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_identity_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This token is the one the calling application has presented as proof of identity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kubernetes API will respond with metadata about the token to be verified,
    in addition to whether or not it has been authenticated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding flow is shown in [Figure 10-5](#service_account_tokens_fig).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1005](assets/prku_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Service Account tokens.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Service Account tokens have been part of Kubernetes since very early on and
    provide a tight integration with the platform in a consumable format (JWT). We
    as operators also have a fairly tight control on their validity as tokens are
    invalidated if the Service Account or Secret is deleted. However, they have some
    features that make their use as identifiers suboptimal. Most importantly, the
    tokens are scoped to a specific Service Account, so are unable to validate anything
    with a more granular scope, for example a Pod or a single container. We also need
    to add functionality to our applications if we want to use and verify tokens as
    a form of client identity. This involves calling the TokenReview API with some
    custom component.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens are also scoped to a single cluster, so we’re not able to use Service
    Account tokens issued by one cluster as identity documents for services calling
    from other clusters without exposing each cluster’s TokenReview API and encoding
    some additional metadata about the cluster where the request originated. All of
    this adds significant complexity to the setup, so we’d recommend not going down
    this path as a method of cross-cluster service identity/authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To ensure that permissions can be granted to applications in an appropriately
    granular way, unique Service Accounts should be created for each workload that
    requires access to the Kubernetes API server. Additionally, if a workload *does
    not* require access to the Kubernetes API server, disable the mounting of a Service
    Account token by specifying the `automountServiceAccountToken: false` field on
    the `ServiceAccount` object.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, this can be set on the default Service Account for a Namespace
    to disable the auto-mounting of the credential token. This field can also be set
    on the `Pod` object, but note that the `Pod` field takes precedence if it’s set
    in both places.
  prefs: []
  type: TYPE_NORMAL
- en: Projected Service Account Tokens (PSAT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beginning with Kubernetes v1.12 there is an additional method of identity available
    that builds on the ideas in service account tokens but seeks to address some of
    the weaknesses (such as lack of TTL, wide scoping, and persistence).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for the PSAT flow to function correctly, the Kubernetes API server
    needs to be configured with the parameter keys shown here (all are configurable,
    though):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The flow for establishing and verifying identity is similar to the SAT method.
    However, instead of having our Pod/application read the automounted Service Account
    token, you instead mount a projected Service Account token as a Volume. This also
    injects a token into the Pod, but you can specify a TTL and custom audience for
    the token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_identity_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `audience` field is important because it prevents destination applications
    using the token from the calling application and attempting to masquerade as the
    calling application. The audience should always be scoped correctly depending
    on the destination application. In this case, we are scoping to communicate with
    the API server itself.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using PSAT, a designated Service Account must be created and used. Kubernetes
    does not mount PSATs for Namespace default Service Accounts.
  prefs: []
  type: TYPE_NORMAL
- en: The calling application can read the projected token and use that in requests
    within the cluster. Destination applications can verify the token by calling the
    `TokenReview` API and passing the received token. With the PSAT method, the review
    will also verify that the TTL has not expired and will return additional metadata
    about the presenting application, including specific Pod information. This provides
    a tighter scope than regular SATs (which only assert a Service Account).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As shown in [Figure 10-6](#projected_service_account_tokens), there is no real
    difference between the SAT and PSAT flows themselves (aside from the server verifying
    the `audience` field), only in the validity and granularity of the identity asserted
    by the token. The `audience` field is important as it identifies the intended
    recipient of the token. In keeping with the [JWT official specification](https://oreil.ly/gKlA7),
    the API will reject a token whose audience does not match the audience specified
    in the API server configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1006](assets/prku_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Projected Service Account tokens.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Projected Service Account tokens are a relatively recent but incredibly strong
    addition to Kubernetes’ feature set. On their own they provide tight integration
    with the platform itself, they provide configurable TTLs, and they have a tight
    scope (individual Pods). They can also be used as building blocks to construct
    even more robust patterns (as we’ll see in later sections).
  prefs: []
  type: TYPE_NORMAL
- en: Platform Mediated Node Identity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cases where all workloads are running on a homogeneous platform (for example,
    AWS), it is possible for the platform itself to determine and assign identities
    to workloads because of the contextual metadata they possess about the workload.
  prefs: []
  type: TYPE_NORMAL
- en: Identity is not asserted by the workload itself but is determined based on its
    properties by an out-of-band provider. The provider returns the workload a credential
    to prove identity that may be used to communicate with other services on the platform.
    It then becomes trivial for the other services to verify that credential because
    they too are on the same underlying platform.
  prefs: []
  type: TYPE_NORMAL
- en: On AWS, an EC2 instance may request credentials to connect to a different service
    like an S3 bucket. The AWS platform inspects the metadata of the instance and
    can provide role-specific credentials back to the instance with which to make
    the connection, as shown in [Figure 10-7](#platform_mediated_identify).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1007](assets/prku_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Platform mediated identity.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that the platform still has to perform *authorization* on the request
    to ensure that the identity being used has the appropriate permissions. This method
    is only being used to *authenticate* the request.
  prefs: []
  type: TYPE_NORMAL
- en: Many cloud vendors expose functionality described in this section. We’re choosing
    to focus on tooling that applies to and integrates with Amazon Web Services (AWS)
    because it is the vendor we most commonly see in the field.
  prefs: []
  type: TYPE_NORMAL
- en: AWS platform authentication methods/tooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS provides a strong identity solution at the node level via the EC2 metadata
    API. This is an example of a platform-mediated system, whereby the platform (AWS)
    is able to determine the identity of a calling entity based on a number of intrinsic
    properties without the entity asserting any credentials/identity claim itself.
    The platform can then deliver secure credentials to the instance (in the form
    of a role, for example) that allows it to access any services defined by the relevant
    policies. As a whole this is referred to as Identity and Access Management (IAM).
  prefs: []
  type: TYPE_NORMAL
- en: This model underpins how AWS (and many other vendors) provide secure access
    to their own cloud services. However, with the rise of containers and other multitenant
    application models, this per-node identity/authentication system breaks down and
    requires additional tooling and alternative approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll look at the three main tooling options we encounter in
    the field. We’ll cover kube2iam and kiam, two separate tools that share the same
    approximate implementation model (and therefore have similar advantages and disadvantages).
    We’ll also describe why we don’t recommend those tools today and why you should
    consider a more integrated solution such as the final option we cover, IAM Roles
    for Service Accounts (IRSA).
  prefs: []
  type: TYPE_NORMAL
- en: kube2iam
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[kube2iam](https://github.com/jtblin/kube2iam) is an open source (OSS) tool
    that acts as a proxy between running workloads and the AWS EC2 metadata API. The
    architecture is shown in [Figure 10-8](#kube2iam).'
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: kube2iam requires that every node in the cluster be able to assume a superset
    of all the roles that Pods may require. This security model means that the scope
    of access provided should a container breakout occur is potentially huge. For
    this reason it is strongly advised not to use kube2iam. We are discussing it here
    as we regularly encounter it in the field and want to ensure that you are aware
    of the limitations of the implementation before diving in.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1008](assets/prku_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. kube2iam architecture and data flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: kube2iam Pods run on every node via a DaemonSet. Each Pod injects an iptables
    rule to capture outbound traffic to the metadata API and redirect it to the running
    instance of kube2iam on that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pods that want to interact with AWS APIs should specify the role they want
    to assume as an annotation in the spec. For example, in the following Deployment
    spec you can see the role is specified in the `iam.amazonaws.com/role` annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: kiam
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like kube2iam, [kiam](https://github.com/uswitch/kiam) is an open source (OSS)
    tool that acts as a proxy to the AWS EC2 metadata API, although its architecture
    (and as a result, its security model) are different and slightly improved, as
    shown in [Figure 10-9](#kiam).
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While safer than kube2iam, kiam also introduces a potentially serious security
    flaw. This section describes a mitigation of the flaw, but you should still use
    caution and understand the attack vector when using kiam.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1009](assets/prku_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. kiam architecture and data flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: kiam has both server and agent components. The agents run as a DaemonSet on
    every node in the cluster. The server component can (and should) be restricted
    to the either the control-plane nodes or a subset of cluster nodes. Agents capture
    EC2 metadata API requests and forward them to the server components to complete
    the appropriate authentication with AWS. Only the server nodes require access
    to assume AWS IAM roles (again, a superset of all roles that Pods may require),
    as shown in [Figure 10-10](#kiam_flow).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1010](assets/prku_1010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. kiam flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this model, there should be controls in place to ensure that no workloads
    are able to run on the server nodes (and thereby obtain unfettered AWS API access).
    Assumption of roles is achieved (like kube2iam) by annotating Pods with the desired
    role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: While the security model is better than kube2iam, kiam still has a potential
    attack vector whereby if a user is able to directly schedule a Pod onto a node
    (by populating its `nodeName` field, bypassing the Kubernetes scheduler and any
    potential guards) they would have unrestricted access to the EC2 metadata API.
  prefs: []
  type: TYPE_NORMAL
- en: The mitigation for this issue is to run a mutating or validating admission webhook
    that ensures the `nodeName` field is not prepopulated on Pod create and update
    requests to the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: kiam provides a strong story for enabling individual Pods to access AWS APIs,
    using a model that existing AWS users will be familiar with (role assumption).
    This is a viable solution in many cases, provided the preceding mitigation is
    put in place prior to use.
  prefs: []
  type: TYPE_NORMAL
- en: IAM Roles for Service Accounts (IRSA)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since late 2019, AWS has provided a native integration between Kubernetes and
    IAM called [IAM Roles for Service Accounts](https://oreil.ly/dUoJJ) (IRSA).
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, IRSA exposes a similiar experience to kiam and kube2iam, in
    that users can annotate their Pods with an AWS IAM role they want it to assume.
    The implementation is very different, though, eliminating the security concerns
    of the earlier approaches.
  prefs: []
  type: TYPE_NORMAL
- en: AWS IAM supports federating identity out to a third-party OIDC provider, in
    this case the Kubernetes API server. As you saw already with PSATs, Kubernetes
    is capable of creating and signing short-lived tokens on a per-Pod basis.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS IRSA combines these features with an additional credential provider in
    their SDKs that calls `sts:AssumeRoleWithWebIdentity`, passing the PSAT. The PSAT
    and desired role need to be injected as environment variables within the Pod (there
    is a webhook that will do this automatically based on the `serviceAccountName`
    desired):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes does not natively expose a `.well-known` OIDC endpoint, so there
    is some additional work required to configure this at a public location (static
    S3 bucket) so that AWS IAM can verify the token using Kubernetes’ public Service
    Account signing key.
  prefs: []
  type: TYPE_NORMAL
- en: Once verified, AWS IAM responds to the application’s request, exchanging the
    PSAT for the desired IAM role credentials as shown in [Figure 10-11](#iam_roles_for_service_accounts).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1011](assets/prku_1011.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-11\. IAM roles for Service Accounts.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although the setup for IRSA is a little clunky, it possesses the best security
    model of all approaches to Pod IAM Role assumption.
  prefs: []
  type: TYPE_NORMAL
- en: IRSA is a strong choice for organizations already leveraging AWS services as
    it uses patterns and primitives that will be familiar with your operations and
    development teams. The model employed (mapping Service Accounts to IAM roles)
    is also a straightforward one to understand with a strong security model.
  prefs: []
  type: TYPE_NORMAL
- en: The main downside is that IRSA can be somewhat cumbersome to deploy and configure
    if you are not utilizing the Amazon Elastic Kubernetes Service (EKS). However,
    recent additions to Kubernetes itself will alleviate some of the technical challenges
    here, such as exposing Kubernetes itself as an OIDC provider.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in this section, mediating identity through a common platform (AWS
    in this case) has many strengths. In the next section we’ll dive into tooling
    that is aiming to implement this same model but capable of spanning *multiple*
    underlying platforms. This brings the control of a centralized identity system
    with the flexibility of running it for any workload across any cloud or platform.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-platform identity with SPIFFE and SPIRE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Secure Production Identity Framework for Everyone (SPIFFE) is a standard that
    specifies a syntax for identity (SPIFFE Verifiable Identity Document, SVID) that
    can leverage existing cryptographic formats such as x509 and JWT. It also specifies
    a number of APIs for providing and consuming these identities. A SPIFFE ID takes
    the form `spiffe://trust-domain/hierarchical/workload`, where all sections after
    the `spiffe://` are arbitrary string identifiers that can be used in multiple
    ways (although creating some kind of hierarchy is most common).
  prefs: []
  type: TYPE_NORMAL
- en: SPIFFE Runtime Environment (SPIRE) is the reference implementation of SPIFFE
    and has a number of SDKs and integrations to allow applications to make use of
    (both providing, and consuming) SVIDs.
  prefs: []
  type: TYPE_NORMAL
- en: This section will assume use of SPIFFE and SPIRE together unless otherwise noted.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture and concepts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SPIRE runs a server component that acts as a signing authority for identities
    and maintains a registry of all workload identities and the conditions required
    for an identity document to be issued.
  prefs: []
  type: TYPE_NORMAL
- en: SPIRE agents run on every node as a DaemonSet where they expose an API for workloads
    to request identity via a Unix socket. The agent is also configured with read-only
    access to the kubelet to determine metadata about Pods on the node. The SPIRE
    architecture is shown in [Figure 10-12](#spire_architecture).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1012](assets/prku_1012.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-12\. SPIRE Architecture. Reproduced from the [official SPIRE documentation](https://oreil.ly/6VY4A).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When agents come online they verify and register themselves to the server by
    a process called *node attestation* (as shown in [Figure 10-13](#node_attestation)).
    This process utilizes environmental context (for example, the AWS EC2 metadata
    API or Kubernetes PSATs) to identify a node and assign it a SPIFFE ID. The server
    then issues the node an identity in the form of an x509 SVID. Following is an
    example registration for a node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This tells the SPIRE server to assign the SPIFFE ID `spiffe://production-trust-domain/nodes`
    to any node where the agent Pod satisfies the selectors specified; in this case,
    we are selecting when the Pod is running in the SPIRE Namespace on the `production-cluster`
    under the `spire-agent` Service account (verified via the PSAT).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1013](assets/prku_1013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-13\. Node attestation. Reproduced from the [official SPIRE documentation](https://oreil.ly/Q5eEW).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When workloads come online they call the node-local workload API to request
    an SVID. The SPIRE agent uses information available to it on the platform (from
    the kernel, kubelet, etc.) to determine the properties of the calling workload.
    This process is referred to as *workload attestation* (as shown in [Figure 10-14](#workload_attestation)).
    The SPIRE server then matches the properties against known workload identities
    based on their selectors and returns an SVID to the workload (via the agent) that
    can be used for authentication against other systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells the SPIRE server to assign the SPIFFE ID `spiffe://production-trust-domain/service-a`
    to any workload that:'
  prefs: []
  type: TYPE_NORMAL
- en: Is running on a node with ID `spiffe://production-trust-domain/nodes`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is running in the `default` Namespace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is running under the `service-a` Service Account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Has the Pod label `app: frontend`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Was built using the `docker.io/johnharris85/service-a:v0.0.1` image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![prku 1014](assets/prku_1014.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. Workload attestation. Reproduced from the [official SPIRE documentation](https://oreil.ly/Eh7Xl).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the workload attestor plug-in can query the kubelet (to discover workload
    information) using its Service Account. The kubelet then uses the `TokenReview`
    API to validate bearer tokens. This requires reachability to the Kubernetes API
    server. Therefore, API server downtime can interrupt workload attestation.
  prefs: []
  type: TYPE_NORMAL
- en: The `--authentication-token-webhook-cache-ttl` kubelet flag controls how long
    the kubelet caches TokenReview responses and may help to mitigate this issue.
    A large cache TTL value is not recommended, however, as that can impact permission
    revocation. See the [SPIRE workload attestor documentation](https://oreil.ly/Pn1ZP)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: The patterns described in this section have significant advantages when trying
    to build a robust identity system for your workloads, both on and off Kubernetes.
    The SPIFFE specification leverages well-understood and widely supported cryptographic
    standards in x509 and JWT, and the SPIRE implementation also supports many different
    methods of application integrations. Another key property is the ability to scope
    identity to a very granular level by combining projected service account tokens
    with its own selectors to identify individual Pods. This can be especially useful
    in scenarios where sidecar containers are present in a Pod and each container
    needs varying levels of access.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is also undeniably the most labor-intensive and requires expertise
    in the tooling and effort to maintain another component in the environment. There
    may also be work required to register each workload, although this could be automated
    (and work is already underway in the community around the area of automated registration
    of workloads).
  prefs: []
  type: TYPE_NORMAL
- en: SPIFFE/SPIRE have a number of integration points with workload applications.
    Which integration point is appropriate will depend on the desired level of coupling
    to the platform and the amount of control users have over the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Direct application access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SPIRE provides SDKs for Go, C, and Java for applications to directly integrate
    with the SPIFFE workload API. These wrap existing HTTP libraries but provide native
    support for obtaining and verifying identities. Following is an example in Go
    calling a Kubernetes Service `service-b` and expecting a specific SPIFFE ID to
    be presented (through an x509 SVID):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The SPIRE agent also exposes a [gRPC](https://grpc.io/about) API for those users
    who want a tighter integration with the platform but are working in a language
    without SDK availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Direct integration (as described in this subsection) is *not* a recommended
    approach for end-user applications for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It tightly couples the application with the platform/implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It requires mounting the SPIRE agent Unix socket into the Pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s not easily extensible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main area where using these libraries directly is appropriate is if building
    out some intermediate platform tooling that wraps or extends some of the existing
    functionality of the toolset.
  prefs: []
  type: TYPE_NORMAL
- en: Sidecar proxy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SPIRE natively supports the Envoy SDS API for publishing certificates to be
    consumed by an Envoy proxy. Envoy can then use the SVID x509 certificate to establish
    TLS connections with other Services and use the trust bundle to verify incoming
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Envoy also supports verifying that only specific SPIFFE IDs (encoded into the
    SVID) should be able to connect. There are two methods to implement this verification:'
  prefs: []
  type: TYPE_NORMAL
- en: By specifying a list of `verify_subject_alt_name` values in the Envoy configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By utilizing Envoy’s External Authorization API to delegate admission decisions
    to an external system (for example, Open Policy Agent). Following is an example
    of a Rego policy to achieve this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In this example, Envoy verifies the request’s TLS certificate against the SPIRE
    trust bundle, then delegates authorization to Open Policy Agent (OPA). The Rego
    policy inspects the SVID and allows the request if the SPIFFE ID matches `spiffe://production-trust-domain/frontend`.
    The architecture for this flow is shown in [Figure 10-15](#spire_with_envoy).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This approach inserts OPA into the critical request path, so that should be
    taken into consideration when designing the flow/ architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1015](assets/prku_1015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. SPIRE with Envoy.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Service mesh (Istio)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Istio’s CA creates SVIDs for all Service Accounts, encoding a SPIFFE ID in the
    format `spiffe://cluster.local/ns/<namespace>/sa/<service_account>`. Therefore,
    Services in an Istio mesh can leverage SPIFFE-aware endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While service meshes are out of scope for this chapter, many attempt to address
    the issue of identity and authentication. Most of these attempts include or build
    on the methods and tooling detailed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Other application integration methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In addition to the primary methods just discussed, SPIRE also supports the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Pulling SVIDs and trust bundles directly to a filesystem, enabling applications
    to detect changes and reload. While this enables applications to be somewhat agnostic
    to SPIRE, it also opens an attack vector for certificates to be stolen from the
    filesystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nginx module that allows for certificates to be streamed from SPIRE (similiar
    to the Envoy integration described earlier). There are custom modules for Nginx
    that enable users to specify the SPIFFE IDs that should be allowed to connect
    to the server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with secrets store (Vault)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SPIRE can be used to solve the secure introduction problem when an application
    needs to obtain some shared secret material from [HashiCorp Vault](https://www.vaultproject.io).
    Vault can be configured to authenticate clients using OIDC federation with the
    SPIRE server as an OIDC provider.
  prefs: []
  type: TYPE_NORMAL
- en: Roles in Vault can be bound to specific subjects (SPIFFE IDs) so that when a
    workload requests a JWT SVID from SPIRE, that is valid to obtain a role and therefore
    accessor credentials to Vault.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with AWS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SPIRE can also be used to establish identity and authenticate to AWS services.
    This process utilizes the same OIDC federation idea in the AWS IRSA and Vault
    sections. Workloads request JWT SVIDs that are then verified by AWS by validating
    against the federated OIDC provider (SPIRE server). The downside of this approach
    is that SPIRE must be publicly accessible for AWS to discover the JSON Web Key
    Set (JWKS) material required to validate the JWTs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have dived into the patterns and tooling that we have successfully
    seen and implemented in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Identity is a multilayered topic, and your approach will evolve over time as
    you become more comfortable with the complexity of the different patterns and
    how that fits with each individual organization’s requirements. Typically on the
    user identity side you will already have a third-party SSO of some kind, but directly
    integrating this into Kubernetes via OIDC might seem nontrivial. In these situations
    we’ve seen Kubernetes sit *outside* of the main organizational identity strategy.
    Depending on requirements this may be fine, but integrating directly will give
    greater visibility and control over environments, especially those with multiple
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: On the workload/application side we have often experienced this being treated
    as an afterthought (beyond default Service Accounts). Again, depending on internal
    requirements this may be fine. It’s definitely true that implementing a robust
    solution for workload identity both in-cluster and cross-platform introduces (in
    some cases) significant complexity and requires deeper knowledge of external tooling.
    However, when organizations reach a level of maturity with Kubernetes, we think
    implementing the patterns described in this chapter can significantly increase
    the security posture of your Kubernetes environments and provide additional layers
    of defense in depth should breaches occur.
  prefs: []
  type: TYPE_NORMAL
