<html><head></head><body><section data-pdf-bookmark="Chapter 8. Resource Management" data-type="chapter" epub:type="chapter"><div class="chapter" id="resource_management">&#13;
<h1><span class="label">Chapter 8. </span>Resource Management</h1>&#13;
&#13;
&#13;
<p>In this chapter, we focus on the best practices for managing and optimizing&#13;
Kubernetes resources. We discuss workload scheduling, cluster&#13;
management, pod resource management, namespace management, and scaling&#13;
applications. We also dive into some of the advanced scheduling&#13;
techniques that Kubernetes provides through affinity, anti-affinity, taints,&#13;
tolerations, and nodeSelectors.</p>&#13;
&#13;
<p>We show you how to implement resource limits, resource&#13;
requests, pod Quality of Service, <code>PodDisruptionBudget</code>s, <code>LimitRanger</code>s, and&#13;
anti-affinity policies.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Scheduler" data-type="sect1"><div class="sect1" id="id356">&#13;
<h1>Kubernetes Scheduler</h1>&#13;
&#13;
<p>The Kubernetes scheduler is one of the main components that is hosted in&#13;
the control plane. The scheduler allows Kubernetes to make placement&#13;
decisions for pods deployed to the cluster. It deals with&#13;
optimization of resources based on constraints of the cluster as well as user-specified constraints. It uses a scoring algorithm that is based on&#13;
predicates and priorities.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Predicates" data-type="sect2"><div class="sect2" id="id55">&#13;
<h2>Predicates</h2>&#13;
&#13;
<p>The <a data-primary="resource management" data-secondary="scheduling" data-tertiary="predicates" data-type="indexterm" id="id696"/><a data-primary="scheduling" data-secondary="predicates" data-type="indexterm" id="id697"/><a data-primary="predicates" data-type="indexterm" id="id698"/>first function Kubernetes uses to make a scheduling decision is the&#13;
predicate function, which determines what nodes the pods can be scheduled on.&#13;
It implies a hard constraint, so it returns a value of true or false.&#13;
An example would be when a pod requests 4 GB of memory and a&#13;
node cannot satisfy this requirement. The node would return a false&#13;
value and would be removed from viable nodes for the pod to be scheduled&#13;
to. Another example would be if the node is set to unschedulable; it would then be removed from the scheduling decision.</p>&#13;
&#13;
<p>The scheduler checks the predicates based on order of restrictiveness and&#13;
complexity. As of this writing, the following are the predicates that the&#13;
scheduler checks for:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="w">    </code>CheckNodeConditionPred,<code class="w"/>&#13;
<code class="w">    </code>CheckNodeUnschedulablePred,<code class="w"/>&#13;
<code class="w">    </code>GeneralPred,<code class="w"/>&#13;
<code class="w">    </code>HostNamePred,<code class="w"/>&#13;
<code class="w">    </code>PodFitsHostPortsPred,<code class="w"/>&#13;
<code class="w">    </code>MatchNodeSelectorPred,<code class="w"/>&#13;
<code class="w">    </code>PodFitsResourcesPred,<code class="w"/>&#13;
<code class="w">    </code>NoDiskConflictPred,<code class="w"/>&#13;
<code class="w">    </code>PodToleratesNodeTaintsPred,<code class="w"/>&#13;
<code class="w">    </code>PodToleratesNodeNoExecuteTaintsPred,<code class="w"/>&#13;
<code class="w">    </code>CheckNodeLabelPresencePred,<code class="w"/>&#13;
<code class="w">    </code>CheckServiceAffinityPred,<code class="w"/>&#13;
<code class="w">    </code>MaxEBSVolumeCountPred,<code class="w"/>&#13;
<code class="w">    </code>MaxGCEPDVolumeCountPred,<code class="w"/>&#13;
<code class="w">    </code>MaxCSIVolumeCountPred,<code class="w"/>&#13;
<code class="w">    </code>MaxAzureDiskVolumeCountPred,<code class="w"/>&#13;
<code class="w">    </code>MaxCinderVolumeCountPred,<code class="w"/>&#13;
<code class="w">    </code>CheckVolumeBindingPred,<code class="w"/>&#13;
<code class="w">    </code>NoVolumeZoneConflictPred,<code class="w"/>&#13;
<code class="w">    </code>CheckNodeMemoryPressurePred,<code class="w"/>&#13;
<code class="w">    </code>CheckNodePIDPressurePred,<code class="w"/>&#13;
<code class="w">    </code>CheckNodeDiskPressurePred,<code class="w"/>&#13;
<code class="w">    </code>MatchInterPodAffinityPred<code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Priorities" data-type="sect2"><div class="sect2" id="id56">&#13;
<h2>Priorities</h2>&#13;
&#13;
<p>Whereas <a data-primary="resource management" data-secondary="scheduling" data-tertiary="priorities" data-type="indexterm" id="id699"/><a data-primary="scheduling" data-secondary="priorities" data-type="indexterm" id="id700"/><a data-primary="priorities (in scheduling)" data-type="indexterm" id="id701"/>predicates indicate a true or false value and dismiss a node for&#13;
scheduling, the priority value ranks all the valid nodes based on&#13;
a relative value. The following priorities are scored for nodes:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="w">    </code>EqualPriority<code class="w"/>&#13;
<code class="w">    </code>MostRequestedPriority<code class="w"/>&#13;
<code class="w">    </code>RequestedToCapacityRatioPriority<code class="w"/>&#13;
<code class="w">    </code>SelectorSpreadPriority<code class="w"/>&#13;
<code class="w">    </code>ServiceSpreadingPriority<code class="w"/>&#13;
<code class="w">    </code>InterPodAffinityPriority<code class="w"/>&#13;
<code class="w">    </code>LeastRequestedPriority<code class="w"/>&#13;
<code class="w">    </code>BalancedResourceAllocation<code class="w"/>&#13;
<code class="w">    </code>NodePreferAvoidPodsPriority<code class="w"/>&#13;
<code class="w">    </code>NodeAffinityPriority<code class="w"/>&#13;
<code class="w">    </code>TaintTolerationPriority<code class="w"/>&#13;
<code class="w">    </code>ImageLocalityPriority<code class="w"/>&#13;
<code class="w">    </code>ResourceLimitsPriority<code class="w"/></pre>&#13;
&#13;
<p>The scores will be added, and then a node is given its final score to&#13;
indicate its priority. For example, if a pod requires 600 millicores and&#13;
there are two nodes, one with 900 millicores available and one with 1,800&#13;
millicores, the node with 1,800 millicores available will have a&#13;
higher priority.</p>&#13;
&#13;
<p>If nodes are returned with the same priority, the scheduler will use&#13;
a <code>selectHost()</code> function, which selects a node in a round-robin fashion.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Advanced Scheduling Techniques" data-type="sect1"><div class="sect1" id="id357">&#13;
<h1>Advanced Scheduling Techniques</h1>&#13;
&#13;
<p>For most cases, Kubernetes does a good job of optimally scheduling pods&#13;
for you. It takes into account pods that are placed only on nodes that have&#13;
sufficient resources. It also tries to spread pods from the same ReplicaSet&#13;
across nodes to increase availability and will balance resource utilization. When this is&#13;
not good enough, Kubernetes gives you the flexibility to influence how&#13;
resources are scheduled. For example, you might want to schedule pods&#13;
across availability zones to mitigate a zonal failure causing downtime&#13;
to your application. You might also want to&#13;
colocate pods to a specific host for performance benefits.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Affinity and Anti-Affinity" data-type="sect2"><div class="sect2" id="id57">&#13;
<h2>Pod Affinity and Anti-Affinity</h2>&#13;
&#13;
<p>Pod affinity<a data-primary="resource management" data-secondary="scheduling" data-tertiary="pod affinity/anti-affinity" data-type="indexterm" id="resource-mgmt-schedule-affinity"/><a data-primary="scheduling" data-secondary="pod affinity/anti-affinity" data-type="indexterm" id="schedule-affinity"/><a data-primary="pod affinity rules" data-type="indexterm" id="pod-affinity"/><a data-primary="anti-affinity rules" data-type="indexterm" id="anti-affinity"/> and anti-affinity let you set rules to place pods relative&#13;
to other pods. These rules allow you to modify the scheduling behavior&#13;
and override the scheduler’s placement decisions.</p>&#13;
&#13;
<p>For example, an anti-affinity rule would allow you to spread pods from a&#13;
ReplicaSet across multiple datacenter zones. It does this by utilizing&#13;
keylabels set on the pods. Setting the key/value pairs instructs the&#13;
scheduler to schedule the pods on the same node (affinity) or prevent the&#13;
pods from scheduling on the same nodes (anti-affinity).</p>&#13;
&#13;
<p>Following is an example of setting a pod anti-affinity rule:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">4</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">affinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">podAntiAffinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">requiredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">labelSelector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">              </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">              </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">app</code><code class="w"/>&#13;
<code class="w">                </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">In</code><code class="w"/>&#13;
<code class="w">                </code><code class="nt">values</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">                </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">topologyKey</code><code class="p">:</code><code class="w"> </code><code class="s">"kubernetes.io/hostname"</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:alpine</code><code class="w"/></pre>&#13;
&#13;
<p>This manifest of an NGINX deployment has four replicas and the selector&#13;
label <code>app=frontend</code>. The deployment has a PodAntiAffinity stanza&#13;
configured that will ensure that the scheduler does not colocate replicas on&#13;
a single node. This ensures that if a node fails, there are still&#13;
enough replicas of NGINX to serve data from its <a data-primary="resource management" data-secondary="scheduling" data-startref="resource-mgmt-schedule-affinity" data-tertiary="pod affinity/anti-affinity" data-type="indexterm" id="id702"/><a data-primary="scheduling" data-secondary="pod affinity/anti-affinity" data-startref="schedule-affinity" data-type="indexterm" id="id703"/><a data-primary="pod affinity rules" data-startref="pod-affinity" data-type="indexterm" id="id704"/><a data-primary="anti-affinity rules" data-startref="anti-affinity" data-type="indexterm" id="id705"/>cache.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="nodeSelector" data-type="sect2"><div class="sect2" id="id58">&#13;
<h2>nodeSelector</h2>&#13;
&#13;
<p>A nodeSelector <a data-primary="resource management" data-secondary="scheduling" data-tertiary="nodeSelectors" data-type="indexterm" id="id706"/><a data-primary="scheduling" data-secondary="nodeSelectors" data-type="indexterm" id="id707"/><a data-primary="nodeSelectors" data-type="indexterm" id="id708"/>is the easiest way to schedule pods to a particular node.&#13;
It uses label selectors with key/value pairs to make the&#13;
scheduling decision. For example, you might want to schedule pods to a&#13;
specific node that has specialized hardware, such as a GPU. You might ask,&#13;
“Can’t I do this with a node taint?” The answer is, yes, you can. The&#13;
difference is that you use a nodeSelector when you want to <em>request</em> a GPU-enabled node, whereas a taint <em>reserves</em> a node for only GPU workloads.&#13;
You can use both node taints and nodeSelectors together to reserve the&#13;
nodes for only GPU workloads, and use the nodeSelector to automatically&#13;
select a node with a GPU.</p>&#13;
&#13;
<p>Following is an example of labeling a node and using a nodeSelector in the pod <span class="keep-together">specification:</span></p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>label<code class="w"> </code>node<code class="w"> </code>&lt;node_name&gt;<code class="w"> </code><code class="nv">disktype</code><code class="o">=</code>ssd<code class="w"/></pre>&#13;
&#13;
<p>Now, let’s create a pod specification with a nodeSelector key/value of <code>disktype: ssd</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">redis</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">env</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">prod</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:alpine</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">imagePullPolicy</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">IfNotPresent</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">nodeSelector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">disktype</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ssd</code><code class="w"/></pre>&#13;
&#13;
<p>Using the nodeSelector schedules the pod to only nodes that have the&#13;
label <span class="keep-together"><code>disktype=ssd</code>:</span></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Taints and Tolerations" data-type="sect2"><div class="sect2" id="id59">&#13;
<h2>Taints and Tolerations</h2>&#13;
&#13;
<p><em>Taints</em> are <a data-primary="resource management" data-secondary="scheduling" data-tertiary="taints/tolerations" data-type="indexterm" id="resource-mgmt-schedule-taint"/><a data-primary="scheduling" data-secondary="taints/tolerations" data-type="indexterm" id="schedule-taint"/><a data-primary="taints" data-type="indexterm" id="taint"/><a data-primary="tolerations" data-type="indexterm" id="toleration"/>used on nodes to repel pods from being scheduled on them. But isn’t that what anti-affinity is for?&#13;
Yes, but taints take a different approach than pod anti-affinity and serve a&#13;
different use case. For example, you might have pods that require a&#13;
specific performance profile, and you do not want to schedule any other&#13;
pods to the specific node. Taints work in conjunction with <em>tolerations</em>,&#13;
which allow you to override tainted nodes. The combination of the two&#13;
gives you fine-grained control over anti-affinity rules.</p>&#13;
&#13;
<p>In general, you will use taints and tolerations for the following use cases:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Specialized node hardware</p>&#13;
</li>&#13;
<li>&#13;
<p>Dedicated node resources</p>&#13;
</li>&#13;
<li>&#13;
<p>Avoiding degraded nodes</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Multiple taint types affect scheduling and running&#13;
containers:</p>&#13;
<dl>&#13;
<dt>NoSchedule</dt>&#13;
<dd>&#13;
<p>A hard taint that prevents scheduling on the node</p>&#13;
</dd>&#13;
<dt>PreferNoSchedule</dt>&#13;
<dd>&#13;
<p>Schedules only if pods cannot be scheduled on other nodes</p>&#13;
</dd>&#13;
<dt>NoExecute</dt>&#13;
<dd>&#13;
<p>Evicts pods already running on the node</p>&#13;
</dd>&#13;
<dt>NodeCondition</dt>&#13;
<dd>&#13;
<p>Taints a node if it meets a specific condition</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#kubernetes_taints_and_tolerations">Figure 8-1</a> shows an example of a node that is tainted with&#13;
<code>gpu=true:NoSchedule</code>. Pod Spec 1 has a toleration key with&#13;
<code>gpu</code>, so it will be scheduled to the tainted node. Pod Spec 2&#13;
has a toleration key of <code>no-gpu</code>, so it will not be scheduled to the&#13;
node.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="kubernetes_taints_and_tolerations">&#13;
<img alt="Kubernetes taints and tolerations" src="assets/kbp2_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Kubernetes taints and tolerations</h6>&#13;
</div></figure>&#13;
&#13;
<p>When a pod cannot be scheduled due to tainted nodes, you’ll see an error message like the following:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">Warning:<code class="w">  </code>FailedScheduling<code class="w">  </code>10s<code class="w"> </code><code class="o">(</code>x10<code class="w"> </code>over<code class="w"> </code>2m<code class="o">)</code><code class="w">  </code>default-scheduler<code class="w"/>&#13;
<code class="m">0</code>/2<code class="w"> </code>nodes<code class="w"> </code>are<code class="w"> </code>available:<code class="w"> </code><code class="m">2</code><code class="w"> </code>node<code class="o">(</code>s<code class="o">)</code><code class="w"> </code>had<code class="w"> </code>taints<code class="w"> </code>that<code class="w"> </code>the<code class="w"> </code>pod<code class="w"> </code>did<code class="w"> </code>not<code class="w"> </code>tolerate.<code class="w"/></pre>&#13;
&#13;
<p>Now that we’ve seen how we can manually add taints to affect scheduling,&#13;
there is also the powerful concept <a data-primary="taint-based eviction" data-type="indexterm" id="id709"/>of <em>taint-based eviction</em>, which allows&#13;
the eviction of running pods. For example, if a node becomes unhealthy&#13;
due to a bad disk drive, the taint-based eviction can reschedule the pods&#13;
on the host to another healthy node in the<a data-primary="resource management" data-secondary="scheduling" data-startref="resource-mgmt-schedule-taint" data-tertiary="taints/tolerations" data-type="indexterm" id="id710"/><a data-primary="scheduling" data-secondary="taints/tolerations" data-startref="schedule-taint" data-type="indexterm" id="id711"/><a data-primary="taints" data-startref="taint" data-type="indexterm" id="id712"/><a data-primary="tolerations" data-startref="toleration" data-type="indexterm" id="id713"/> cluster.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Resource Management" data-type="sect1"><div class="sect1" id="id209">&#13;
<h1>Pod Resource Management</h1>&#13;
&#13;
<p>One of<a data-primary="resource management" data-secondary="importance of" data-type="indexterm" id="id714"/> the most important aspects of managing applications in Kubernetes&#13;
is appropriately managing pod resources. Managing pod resources consists&#13;
of managing CPU and memory to optimize the overall utilization of your&#13;
Kubernetes cluster. You can manage these resources at the&#13;
container level and at the namespace level. There are other resources,&#13;
such as network and storage, but Kubernetes doesn’t yet have a way to&#13;
set requests and limits for those resources.</p>&#13;
&#13;
<p>For the scheduler to optimize resources and make intelligent placement&#13;
decisions, it needs to understand the requirements of an application. As&#13;
an example, if a container (application) needs a minimum of 2 GB to&#13;
perform, we need to define this in our pod specification so the scheduler&#13;
knows that the container requires 2 GB of memory on the host to which it schedules&#13;
the container.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Request" data-type="sect2"><div class="sect2" id="id60">&#13;
<h2>Resource Request</h2>&#13;
&#13;
<p>A <a data-primary="resource management" data-secondary="requests" data-type="indexterm" id="resource-mgmt-request"/><a data-primary="requests (for resources)" data-type="indexterm" id="requests"/>Kubernetes resource <em>request</em> defines that a container requires <em>X</em> amount&#13;
of CPU or memory to be scheduled. If you were to specify in the pod&#13;
specification that a container requires 8 GB for its resource request and all&#13;
your nodes have 7.5 GB of memory, the pod would not be scheduled. If&#13;
the pod is not able to be scheduled, it will go into a <em>pending</em> state&#13;
until the required resources are available. So let’s look at how this works in our cluster.</p>&#13;
&#13;
<p>To determine the available free resources in your cluster, use&#13;
<code>kubectl top</code>:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>top<code class="w"> </code>nodes<code class="w"/></pre>&#13;
&#13;
<p>The output should look like this (the memory size might be different for&#13;
your <span class="keep-together">cluster):</span></p>&#13;
<pre>NAME                       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%&#13;
aks-nodepool1-14849087-0   524m         27%    7500Mi          33%&#13;
aks-nodepool1-14849087-1   468m         24%    3505Mi          27%&#13;
aks-nodepool1-14849087-2   406m         21%    3051Mi          24%&#13;
aks-nodepool1-14849087-3   441m         22%    2812Mi          22%</pre>&#13;
&#13;
<p>As this example shows, the largest amount of memory available&#13;
to a host is 7,500 Mi, so let’s schedule a pod that requests&#13;
8,000 Mi of memory:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">memory-request</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">memory-request</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">polinux/stress</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">requests</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="s">"8000Mi"</code><code class="w"/></pre>&#13;
&#13;
<p>Notice that the pod will stay pending, and if you look at the events on&#13;
the pods, you’ll see that no nodes are available to schedule the pods:</p>&#13;
&#13;
<pre data-type="programlisting">kubectl describe pods memory-request</pre>&#13;
&#13;
<p>The output of the event should&#13;
look <a data-primary="resource management" data-secondary="requests" data-startref="resource-mgmt-request" data-type="indexterm" id="id715"/><a data-primary="requests (for resources)" data-startref="requests" data-type="indexterm" id="id716"/>like this:</p>&#13;
<pre>Events:&#13;
  Type     Reason        Age                From              Message&#13;
  Warning  FailedSch...  27s (x2 over 27s)  default-sched...  0/3 nodes are&#13;
                                                              available: 3&#13;
                                                              Insufficient memory</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Limits and Pod Quality of Service" data-type="sect2"><div class="sect2" id="id61">&#13;
<h2>Resource Limits and Pod Quality of Service</h2>&#13;
&#13;
<p>Kubernetes <a data-primary="resource management" data-secondary="limits" data-type="indexterm" id="id717"/><a data-primary="limits (resource management)" data-type="indexterm" id="id718"/>resource <em>limits</em> define the maximum CPU or memory that a pod is&#13;
given. When you specify limits for CPU and memory, each takes a different action when it reaches the specified limit. With CPU&#13;
limits, the container is throttled from using more than its specified&#13;
limit. With memory limits, the pod is restarted if it reaches its&#13;
limit. The pod might be restarted on the same host or a different host&#13;
within the <span class="keep-together">cluster.</span></p>&#13;
&#13;
<p>Specifying limits for containers is a best practice to ensure that&#13;
applications are allotted their fair share of resources within the cluster:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cpu-demo</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cpu-example</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:alpine</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">limits</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"1"</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">requests</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"0.5"</code><code class="w"/></pre>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">qos-demo</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">qos-example</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">qos-demo-ctr</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:alpine</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">limits</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="s">"200Mi"</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"700m"</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">requests</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="s">"200Mi"</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"700m"</code><code class="w"/></pre>&#13;
&#13;
<p>When a <a data-primary="resource management" data-secondary="QoS (Quality of Service)" data-type="indexterm" id="id719"/><a data-primary="QoS (Quality of Service)" data-type="indexterm" id="id720"/>pod is created, it’s assigned one of the following Quality of Service (QoS) classes:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Guaranteed</p>&#13;
</li>&#13;
<li>&#13;
<p>Burstable</p>&#13;
</li>&#13;
<li>&#13;
<p>Best effort</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The pod is assigned a QoS<a data-primary="guaranteed QoS" data-type="indexterm" id="id721"/> of  <em>guaranteed</em> when CPU and memory both have request and limits that match. A <em>burstable</em> QoS<a data-primary="burstable QoS" data-type="indexterm" id="id722"/> is when the limits are set higher than the request, meaning that the container is guaranteed its request, but it can also burst to the limit set for the container. A pod is assigned <em>best effort</em> when no request or limits are set for the containers in the pod.</p>&#13;
&#13;
<p><a data-type="xref" href="#kubernetes_qos">Figure 8-2</a> depicts how QoS is assigned to pods.</p>&#13;
&#13;
<figure class="width-45"><div class="figure" id="kubernetes_qos">&#13;
<img alt="Kubernetes QoS" src="assets/kbp2_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Kubernetes QoS</h6>&#13;
</div></figure>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>With guaranteed QoS, if you have multiple containers in your pod, you’ll need to have memory request and limits set for each container, and you’ll also need CPU request and limits set for each container. If the request and limits are not set for all containers, they will not be assigned guaranteed QoS.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PodDisruptionBudgets" data-type="sect2"><div class="sect2" id="id62">&#13;
<h2>PodDisruptionBudgets</h2>&#13;
&#13;
<p>At some<a data-primary="resource management" data-secondary="PodDisruptionBudgets" data-type="indexterm" id="resource-mgmt-pod-disrupt"/><a data-primary="PodDisruptionBudgets" data-type="indexterm" id="pod-disrupt"/><a data-primary="pod eviction" data-type="indexterm" id="pod-evict"/><a data-primary="evicting pods" data-type="indexterm" id="evict-pod"/> point in time, Kubernetes might need to <em>evict</em> pods from a host.&#13;
There are two types of <a data-primary="voluntary evictions" data-type="indexterm" id="id723"/><a data-primary="involuntary evictions" data-type="indexterm" id="id724"/>evictions: <em>voluntary</em> and&#13;
<em>involuntary</em> disruptions. Involuntary disruptions can be caused by&#13;
hardware failure, network partitions, kernel panics, or a node being out&#13;
of resources. Voluntary evictions can be caused by performing&#13;
maintenance on the cluster, the Cluster Autoscaler deallocating nodes, or&#13;
updating pod templates. To minimize the impact to your application, you&#13;
can set a <code>PodDisruptionBudget</code> to ensure uptime of the application when&#13;
pods need to be evicted. A <code>PodDisruptionBudget</code> allows you to set a&#13;
policy on the minimum available and maximum unavailable pods&#13;
during voluntary eviction events. An example of a voluntary eviction would be when draining a node to perform maintenance on the node.</p>&#13;
&#13;
<p>For example, you might specify that no more than 20% of pods belonging to&#13;
your application can be down at a given time. You could also specify&#13;
this policy in terms of <em>X</em> number of replicas that must always be available.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Minimum available" data-type="sect3"><div class="sect3" id="id358">&#13;
<h3>Minimum available</h3>&#13;
&#13;
<p>In the following example, we set a <code>PodDisruptionBudget</code> to handle a minimum&#13;
available to 5 for app: frontend:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">policy/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PodDisruptionBudget</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend-pdb</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">minAvailable</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">5</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/></pre>&#13;
&#13;
<p>In this example, the <code>PodDisruptionBudget</code> specifies that for the frontend app&#13;
there must always be five replica pods available at any given time. In this&#13;
scenario, an eviction can evict as many pods as it wants, as long as five&#13;
are available.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Maximum unavailable" data-type="sect3"><div class="sect3" id="id63">&#13;
<h3>Maximum unavailable</h3>&#13;
&#13;
<p>In the next example, we set a <code>PodDisruptionBudget</code> to handle a maximum&#13;
unavailable to 20% for the frontend app:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">policy/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PodDisruptionBudget</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend-pdb</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">maxUnavailable</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">20%</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/></pre>&#13;
&#13;
<p>In this example, the <code>PodDisruptionBudget</code> specifies that no more than 20% of replica pods can be unavailable at any given&#13;
time. In this scenario, an eviction can evict a maximum of 20% of pods during a&#13;
voluntary disruption.</p>&#13;
&#13;
<p>It’s essential that when designing your Kubernetes cluster you think&#13;
about the sizing of the cluster resources so that you can handle a number of&#13;
failed nodes. For example, if you have a four-node cluster and one node&#13;
fails, you will be losing a quarter of your cluster capacity.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>When specifying a <code>PodDisruptionBudget</code> as a percentage, it might not correlate to a specific number of pods. For example, if your application has seven pods and you specify <code>maxAvailable</code> to <code>50%</code>, it’s not clear whether that is three or four pods. In this case, Kubernetes rounds up to the closest integer, so the <code>maxAvailable</code> would be <a data-primary="resource management" data-secondary="PodDisruptionBudgets" data-startref="resource-mgmt-pod-disrupt" data-type="indexterm" id="id725"/><a data-primary="PodDisruptionBudgets" data-startref="pod-disrupt" data-type="indexterm" id="id726"/><a data-primary="pod eviction" data-startref="pod-evict" data-type="indexterm" id="id727"/><a data-primary="evicting pods" data-startref="evict-pod" data-type="indexterm" id="id728"/>four pods.</p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Managing Resources by Using Namespaces" data-type="sect2"><div class="sect2" id="id210">&#13;
<h2>Managing Resources by Using Namespaces</h2>&#13;
&#13;
<p><em>Namespaces</em> in <a data-primary="resource management" data-secondary="namespaces" data-type="indexterm" id="resource-mgmt-namespace"/><a data-primary="namespaces" data-secondary="resource management with" data-type="indexterm" id="namespace-resource-mgmt"/>Kubernetes give you a nice logical separation of&#13;
resources deployed to a cluster. This allows you to set resource quotas&#13;
per namespace, Role-Based Access Control (RBAC) per namespace, and also network policies per&#13;
namespace. It gives you soft multitenancy features so you can separate&#13;
out workloads in a cluster without dedicating specific infrastructure to&#13;
a team or application. This allows you to get the most out of your&#13;
cluster resource while also maintaining a logical form of separation.</p>&#13;
&#13;
<p>For example, you could create a namespace per team and give each team a&#13;
quota on the number of resources that it can utilize, such as CPU and&#13;
memory.</p>&#13;
&#13;
<p>When designing how you want to configure a namespace, you should think about how you want to control access to a specific set of applications. If you have multiple teams that will be using a single cluster, it is typically best to allocate a namespace to each team. If the cluster is dedicated to only one team, it might make sense to allocate a namespace for each service deployed to the cluster. There’s no single solution to this; your team organization and responsibilities will drive the design.</p>&#13;
&#13;
<p class="pagebreak-before">After deploying a Kubernetes cluster, you’ll see the following namespaces&#13;
in your cluster:</p>&#13;
<dl>&#13;
<dt><code>kube-system</code></dt>&#13;
<dd>&#13;
<p>    Kubernetes internal components are deployed here, such as&#13;
<code>coredns</code>, <code>kube-proxy</code>, and <code>metrics-server</code>.</p>&#13;
</dd>&#13;
<dt><code>default</code></dt>&#13;
<dd>&#13;
<p>    This is the default namespace that is used when you don’t specify&#13;
a namespace in the resource object.</p>&#13;
</dd>&#13;
<dt><code>kube-public</code></dt>&#13;
<dd>&#13;
<p>    Used for anonymous and unauthenticated content, and&#13;
reserved for system usage.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>You’ll want to avoid using the default namespace because users are not mandated to deploy applications within specific resource constraints, and it can lead to resource contention. You should also avoid using the <code>kube-system</code> namespace for your applications because it is used for Kubernetes internal components.</p>&#13;
&#13;
<p>When working with namespaces, you need to use the <code>–namespace</code> flag, or <code>-n</code>&#13;
for short, when working with <code>kubectl</code>:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>create<code class="w"> </code>ns<code class="w"> </code>team-1<code class="w"/></pre>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>get<code class="w"> </code>pods<code class="w"> </code>--namespace<code class="w"> </code>team-1<code class="w"/></pre>&#13;
&#13;
<p>You can also set your <code>kubectl</code> context to a specific namespace, which is&#13;
useful so that you don’t need to add the <code>–namespace</code> flag with every command.&#13;
You can set your namespace context by using the following command:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>config<code class="w"> </code>set-context<code class="w"> </code>my-context<code class="w"> </code>--namespace<code class="o">=</code>team-1<code class="w"/></pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>When dealing with multiple namespaces and clusters, it can be a pain to set different namespaces and cluster context. We’ve found that using <a href="https://oreil.ly/ryavL">kubens</a> and <a href="https://oreil.ly/kVBiL">kubectx</a> can help make it easy to switch between these different <a data-primary="resource management" data-secondary="namespaces" data-startref="resource-mgmt-namespace" data-type="indexterm" id="id729"/><a data-primary="namespaces" data-secondary="resource management with" data-startref="namespace-resource-mgmt" data-type="indexterm" id="id730"/>namespaces and contexts.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ResourceQuota" data-type="sect2"><div class="sect2" id="id64">&#13;
<h2>ResourceQuota</h2>&#13;
&#13;
<p>When<a data-primary="resource management" data-secondary="ResourceQuotas" data-type="indexterm" id="resource-mgmt-quota"/><a data-primary="ResourceQuotas" data-type="indexterm" id="resource-quota"/> multiple teams or applications share a single cluster, it’s important to set up <code>ResourceQuota</code>s on your namespaces. <code>ResourceQuota</code>s allow you to divvy up the cluster in logical units so that no single namespace can consume more than its share of resources in the cluster.&#13;
The following resources can have a quota set for them:</p>&#13;
&#13;
<ul class="less_space pagebreak-before">&#13;
<li>&#13;
<p>Compute resources:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>requests.cpu</code>: Sum of CPU requests cannot exceed this amount</p>&#13;
</li>&#13;
<li>&#13;
<p><code>limits.cpu</code>: Sum of CPU limits cannot exceed this amount</p>&#13;
</li>&#13;
<li>&#13;
<p><code>requests.memory</code>: Sum of memory requests cannot exceed this amount</p>&#13;
</li>&#13;
<li>&#13;
<p><code>limit.memory</code>: Sum of memory limits cannot exceed this amount</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Storage resources:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>requests.storage</code>: Sum of storage requests cannot exceed this value</p>&#13;
</li>&#13;
<li>&#13;
<p><code>persistentvolumeclaims</code>: The total number of PersistentVolume claims&#13;
that can exist in the namespace</p>&#13;
</li>&#13;
<li>&#13;
<p><code>storageclass.request</code>: Volume claims associated with the specified&#13;
storage-class cannot exceed this value</p>&#13;
</li>&#13;
<li>&#13;
<p><code>storageclass.pvc</code>:  The total number of PersistentVolume claims that&#13;
can exist in the namespace</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Object count quotas (only an example set):</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>count/pvc</p>&#13;
</li>&#13;
<li>&#13;
<p>count/services</p>&#13;
</li>&#13;
<li>&#13;
<p>count/deployments</p>&#13;
</li>&#13;
<li>&#13;
<p>count/replicasets</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As you can see from this list, Kubernetes gives you fine-grained control over how you carve up resource quotas per namespace.&#13;
This allows you to more efficiently operate resource usage in a&#13;
multitenant cluster.</p>&#13;
&#13;
<p>Let’s see how these quotas actually work by setting up a quota on a&#13;
namespace. Apply the following YAML file to the <code>team-1</code> namespace:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ResourceQuota</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mem-cpu-demo</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">team-1</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">hard</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">requests.cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"1"</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">requests.memory</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1Gi</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">limits.cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"2"</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">limits.memory</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2Gi</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">persistentvolumeclaims</code><code class="p">:</code><code class="w"> </code><code class="s">"5"</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">requests.storage</code><code class="p">:</code><code class="w"> </code><code class="s">"10Gi</code><code class="w"/></pre>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>apply<code class="w"> </code>quota.yaml<code class="w"> </code>-n<code class="w"> </code>team-1<code class="w"/></pre>&#13;
&#13;
<p>This example sets quotas for CPU, memory, and storage on the <code>team-1</code>&#13;
namespace.</p>&#13;
&#13;
<p>Now let’s try to deploy an application to see how the resource quotas&#13;
affect the deployment:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>run<code class="w"> </code>nginx-quotatest<code class="w"> </code>--image<code class="o">=</code>nginx<code class="w"> </code>--restart<code class="o">=</code>Never<code class="w"> </code>--replicas<code class="o">=</code><code class="m">1</code><code class="w"> </code>--port<code class="o">=</code><code class="m">80</code><code class="w"/>&#13;
<code class="w">    </code>--requests<code class="o">=</code><code class="s1">'cpu=500m,memory=4Gi'</code><code class="w"> </code>--limits<code class="o">=</code><code class="s1">'cpu=500m,memory=4Gi'</code><code class="w"> </code>-n<code class="w"> </code>team-1<code class="w"/></pre>&#13;
&#13;
<p>This deployment will fail with the following error due to the memory quota exceeding 2 Gi of memory:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">Error<code class="w"> </code>from<code class="w"> </code>server<code class="w"> </code><code class="o">(</code>Forbidden<code class="o">)</code>:<code class="w"> </code>pods<code class="w"> </code><code class="s2">"nginx-quotatest"</code><code class="w"> </code>is<code class="w"> </code>forbidden:<code class="w"/>&#13;
<code class="w">    </code>exceeded<code class="w"> </code>quota:<code class="w"> </code>mem-cpu-demo<code class="w"/></pre>&#13;
&#13;
<p>As this example demonstrates, setting resource quotas can let&#13;
you deny deployment of resources based on policies you set for <a data-primary="resource management" data-secondary="ResourceQuotas" data-startref="resource-mgmt-quota" data-type="indexterm" id="id731"/><a data-primary="ResourceQuotas" data-startref="resource-quota" data-type="indexterm" id="id732"/>the&#13;
namespace.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="LimitRange" data-type="sect2"><div class="sect2" id="id65">&#13;
<h2>LimitRange</h2>&#13;
&#13;
<p>We’ve <a data-primary="resource management" data-secondary="LimitRanges" data-type="indexterm" id="id733"/><a data-primary="LimitRanges" data-type="indexterm" id="id734"/>discussed setting <code>request</code> and <code>limits</code> at the container&#13;
level, but what happens if the user forgets to set these in the pod&#13;
specification? Kubernetes provides an admission controller that allows you to&#13;
automatically set these when none are indicated in the specification.</p>&#13;
&#13;
<p>First, create a namespace to work with quotas and <code>LimitRange</code>s:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>create<code class="w"> </code>ns<code class="w"> </code>team-1<code class="w"/></pre>&#13;
&#13;
<p>Apply a <code>LimitRange</code> to the namespace to apply <code>defaultRequest</code>&#13;
in <code>limits</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">LimitRange</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">team-1-limit-range</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">limits</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">default</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">512Mi</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">defaultRequest</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">256Mi</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Container</code><code class="w"/></pre>&#13;
&#13;
<p>Save this to <em>limitranger.yaml</em> and then run <code>kubectl apply</code>:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>apply<code class="w"> </code>-f<code class="w"> </code>limitranger.yaml<code class="w"> </code>-n<code class="w"> </code>team-1<code class="w"/></pre>&#13;
&#13;
<p>Verify that the <code>LimitRange</code> applies default limits and requests:</p>&#13;
<pre> kubectl run team-1-pod --image=nginx -n team-1</pre>&#13;
&#13;
<p>Next, let’s describe the pod to see what requests and limits were set on it:</p>&#13;
<pre>kubectl describe pod team-1-pod -n team-1</pre>&#13;
&#13;
<p>You should see the following requests and limits set on the pod specification:</p>&#13;
<pre>Limits:&#13;
      memory:  512Mi&#13;
    Requests:&#13;
      memory:  256Mi</pre>&#13;
&#13;
<p>It’s important to use <code>LimitRange</code> when using <code>ResourceQuota</code>s, because if&#13;
no request or limits are set in the specification, the deployment will be&#13;
rejected.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Scaling" data-type="sect2"><div class="sect2" id="id211">&#13;
<h2>Cluster Scaling</h2>&#13;
&#13;
<p>One of<a data-primary="resource management" data-secondary="scaling" data-tertiary="cluster scaling" data-type="indexterm" id="resource-mgmt-scale-cluster"/><a data-primary="scaling" data-secondary="clusters" data-type="indexterm" id="scale-cluster"/><a data-primary="clusters" data-secondary="scaling" data-type="indexterm" id="cluster-scale"/> the first decisions you need to make when deploying a cluster&#13;
is the instance size you’ll want to use within your cluster. This&#13;
becomes more of an art than science, especially when you’re&#13;
mixing workloads in a single cluster. You’ll first want to identify a good starting point for the cluster; aiming for a good&#13;
balance of CPU and memory is one option. After you’ve decided&#13;
on a sensible size for the cluster, you can use a couple of&#13;
Kubernetes core primitives to manage the scaling of your cluster.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Manual scaling" data-type="sect3"><div class="sect3" id="id359">&#13;
<h3>Manual scaling</h3>&#13;
&#13;
<p>Kubernetes makes it easy to scale your cluster, especially if you’re&#13;
using tools like Kops or a managed Kubernetes offering. Scaling your&#13;
cluster manually is typically just choosing a new number of nodes, and&#13;
the service will add the new nodes to your cluster.</p>&#13;
&#13;
<p>These tools also allow you to create node pools, which allows you to&#13;
add new instance types to an already running cluster. This becomes very&#13;
useful when running mixed workloads within a single cluster. For&#13;
example, one workload might be more CPU driven, whereas the other workloads&#13;
might be memory-driven applications. Node pools allow you to mix multiple&#13;
instance types within a single cluster.</p>&#13;
&#13;
<p>But perhaps you don’t want to manually do this and want it to autoscale. There are things that you need to take into consideration with&#13;
cluster autoscaling, and we have found that most users are better off&#13;
starting with just manually scaling their nodes proactively when&#13;
resources are needed. If your workloads are highly variable, cluster&#13;
autoscaling can be very useful.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster autoscaling" data-type="sect3"><div class="sect3" id="id66">&#13;
<h3>Cluster autoscaling</h3>&#13;
&#13;
<p>Kubernetes <a data-primary="autoscaling" data-type="indexterm" id="autoscale-cluster"/>provides a Cluster Autoscaler add-on that allows you to set&#13;
the minimum nodes available to a cluster and also the maximum number of nodes&#13;
to which your cluster can scale. The Cluster Autoscaler bases its scale&#13;
decision on when a pod goes pending. For example, if the Kubernetes&#13;
scheduler tries to schedule a pod with a memory request of 4,000 Mib and&#13;
the cluster has only 2,000 Mib available, the pod will go into a pending&#13;
state. After the pod is pending, the Cluster Autoscaler will add a node to&#13;
the cluster. As soon as the new node is added to the cluster, the pending pod&#13;
is scheduled to the node. The downside of the Cluster Autoscaler is&#13;
that a new node is added only before a pod goes pending, so your workload may end up waiting for a new node to come online when it is scheduled. As of Kubernetes v1.15, the Cluster Autoscaler doesn’t support scaling based on custom metrics.</p>&#13;
&#13;
<p>The Cluster Autoscaler can also reduce the size of the cluster after&#13;
resources are no longer needed. When the resources are no longer needed,&#13;
it will drain the node and reschedule the pods to new nodes in the&#13;
cluster. You’ll want to use a <code>PodDisruptionBudget</code> to&#13;
ensure that you don’t negatively affect your application when it performs&#13;
its drain operation to remove the node from the <a data-primary="resource management" data-secondary="scaling" data-startref="resource-mgmt-scale-cluster" data-tertiary="cluster scaling" data-type="indexterm" id="id735"/><a data-primary="scaling" data-secondary="clusters" data-startref="scale-cluster" data-type="indexterm" id="id736"/><a data-primary="clusters" data-secondary="scaling" data-startref="cluster-scale" data-type="indexterm" id="id737"/><a data-primary="autoscaling" data-startref="autoscale-cluster" data-type="indexterm" id="id738"/>cluster.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Application Scaling" data-type="sect2"><div class="sect2" id="id67">&#13;
<h2>Application Scaling</h2>&#13;
&#13;
<p>Kubernetes <a data-primary="resource management" data-secondary="scaling" data-tertiary="application scaling" data-type="indexterm" id="id739"/><a data-primary="scaling" data-secondary="applications" data-type="indexterm" id="id740"/><a data-primary="application scaling" data-type="indexterm" id="id741"/>provides multiple ways to scale applications in your cluster.&#13;
You can scale an application by manually changing the number of replicas&#13;
within a deployment. You can also change the ReplicaSet or replication&#13;
controller, but we don’t recommend managing your applications through&#13;
those implementations. Manual scaling is perfectly fine for workloads&#13;
that are static or when you know the times that the workload spikes, but for&#13;
workloads that experience sudden spikes or workloads that are not static,&#13;
manual scaling is not ideal for the application. Happily, Kubernetes&#13;
also provides a Horizontal Pod Autoscaler (HPA) to automatically scale&#13;
workloads for you.</p>&#13;
&#13;
<p>Let’s first look at how you can manually scale a deployment by&#13;
applying the following Deployment manifest:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">apiVersion:<code class="w"> </code>apps/v1<code class="w"/>&#13;
kind:<code class="w"> </code>Deployment<code class="w"/>&#13;
metadata:<code class="w"/>&#13;
<code class="w">  </code>name:<code class="w"> </code>frontend<code class="w"/>&#13;
spec:<code class="w"/>&#13;
<code class="w">  </code>replicas:<code class="w"> </code><code class="m">3</code><code class="w"/>&#13;
<code class="w">  </code>selector:<code class="w"/>&#13;
<code class="w">    </code>matchlables:<code class="w"/>&#13;
<code class="w">      </code>app:<code class="w"> </code>frontend<code class="w"/>&#13;
<code class="w">  </code>template:<code class="w"/>&#13;
<code class="w">    </code>metadata:<code class="w"/>&#13;
<code class="w">      </code>name:<code class="w"> </code>frontend<code class="w"/>&#13;
<code class="w">      </code>labels:<code class="w"/>&#13;
<code class="w">        </code>app:<code class="w"> </code>frontend<code class="w"/>&#13;
<code class="w">    </code>spec:<code class="w"/>&#13;
<code class="w">      </code>containers:<code class="w"/>&#13;
<code class="w">      </code>-<code class="w"> </code>image:<code class="w"> </code>nginx:alpine<code class="w"/>&#13;
<code class="w">        </code>name:<code class="w"> </code>frontend<code class="w"/>&#13;
<code class="w">        </code>resources:<code class="w"/>&#13;
<code class="w">          </code>requests:<code class="w"/>&#13;
<code class="w">            </code>cpu:<code class="w"> </code>100m<code class="w"/></pre>&#13;
&#13;
<p>This example deploys three replicas of our frontend service. We then can scale this deployment by using the <code>kubectl scale</code> command:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>scale<code class="w"> </code>deployment<code class="w"> </code>frontend<code class="w"> </code>--replicas<code class="w"> </code><code class="m">5</code><code class="w"/></pre>&#13;
&#13;
<p>This results in five replicas of our frontend service. This is great,&#13;
but let’s look at how we can add some intelligence and automatically&#13;
scale the application based on <span class="keep-together">metrics.</span></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scaling with HPA" data-type="sect2"><div class="sect2" id="id68">&#13;
<h2>Scaling with HPA</h2>&#13;
&#13;
<p>The <a data-primary="resource management" data-secondary="scaling" data-tertiary="with HPA" data-tertiary-sortas="HPA" data-type="indexterm" id="id742"/><a data-primary="scaling" data-secondary="with HPA" data-secondary-sortas="HPA" data-type="indexterm" id="id743"/><a data-primary="HPA (Horizontal Pod Autoscaler)" data-type="indexterm" id="id744"/>Kubernetes HPA allows you to scale your&#13;
deployments based on CPU, memory, or custom metrics. It performs a watch&#13;
on the deployment and pulls metrics from the Kubernetes <code>metrics-server</code>.&#13;
It also allows you to set the minimum and maximum number of pods available.&#13;
For example, you can define an HPA policy that sets the minimum number&#13;
of pods to 3 and the maximum number of pods to 10, and it scales&#13;
when the deployment reaches 80% CPU usage. Setting the minimum and maximum is&#13;
critical because you don’t want the HPA to scale  the replicas to an&#13;
infinite amount due to an application bug or issue.</p>&#13;
&#13;
<p>The HPA has the following default setting for sync metrics, upscaling, and&#13;
downscaling replicas:</p>&#13;
<dl>&#13;
<dt><code>horizontal-pod-autoscaler-sync-period</code></dt>&#13;
<dd>&#13;
<p>    Default of 30 seconds for&#13;
syncing metrics</p>&#13;
</dd>&#13;
<dt><code>horizontal-pod-autoscaler-upscale-delay</code></dt>&#13;
<dd>&#13;
<p>    Default of three minutes&#13;
between two upscale operations</p>&#13;
</dd>&#13;
<dt><code>horizontal-pod-autoscaler-downscale-delay</code></dt>&#13;
<dd>&#13;
<p>    Default of five minutes&#13;
between two downscale operations</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>You can change the defaults by using their relative flags, but you need to be&#13;
careful when doing so. If your workload is extremely&#13;
variable, it’s worth playing around with the settings to optimize them for your specific&#13;
use case.</p>&#13;
&#13;
<p>Let’s go ahead and set up an HPA policy for the frontend application you deployed in the previous exercise.</p>&#13;
&#13;
<p>First, expose the deployment on port 80:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="w"> </code>kubectl<code class="w"> </code>expose<code class="w"> </code>deployment<code class="w"> </code>frontend<code class="w"> </code>--port<code class="w"> </code><code class="m">80</code><code class="w"/></pre>&#13;
&#13;
<p>Next, set the autoscale policy:</p>&#13;
<pre>kubectl autoscale deployment frontend --cpu-percent=50 --min=1 --max=10</pre>&#13;
&#13;
<p>This sets the policy to scale your app from a minimum of 1 replica to&#13;
a maximum of 10 replicas and will invoke the scale operation when the&#13;
CPU load reaches 50%.</p>&#13;
&#13;
<p>Let’s generate some load so that we can see the deployment autoscale:</p>&#13;
<pre>kubectl run -i --tty load-generator --image=busybox /bin/sh</pre>&#13;
<pre>Hit enter for command prompt&#13;
while true; do wget -q -O- http://frontend.default.svc.cluster.local; done</pre>&#13;
<pre>kubectl get hpa</pre>&#13;
&#13;
<p>You might need to wait a few minutes to see the replicas scale up&#13;
automatically.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="HPA with Custom Metrics" data-type="sect2"><div class="sect2" id="id212">&#13;
<h2>HPA with Custom Metrics</h2>&#13;
&#13;
<p>In <a data-type="xref" href="ch04.html#configuration_secrets_and_rbac">Chapter 4</a>, we<a data-primary="resource management" data-secondary="scaling" data-tertiary="with custom metrics" data-tertiary-sortas="custom metrics" data-type="indexterm" id="id745"/><a data-primary="scaling" data-secondary="with custom metrics" data-secondary-sortas="custom metrics" data-type="indexterm" id="id746"/><a data-primary="metrics" data-secondary="scaling with" data-type="indexterm" id="id747"/> introduced the role that the metrics server plays in monitoring our systems in Kubernetes. With the Metrics Server API, we can also support scaling our applications with custom metrics. The Custom Metrics API and Metrics Aggregator allow third-party providers to plug in and extend the metrics, and HPA can then scale based on these external metrics. For example, instead of just basic CPU and memory metrics, you could scale based on a metric you’re collecting on an external storage queue. By utilizing custom metrics for autoscaling, you have the ability to scale application-specific metrics or external service metrics.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vertical Pod Autoscaler" data-type="sect2"><div class="sect2" id="id69">&#13;
<h2>Vertical Pod Autoscaler</h2>&#13;
&#13;
<p>The <a data-primary="resource management" data-secondary="scaling" data-tertiary="with VPA" data-tertiary-sortas="VPA" data-type="indexterm" id="id748"/><a data-primary="scaling" data-secondary="with VPA" data-secondary-sortas="VPA" data-type="indexterm" id="id749"/><a data-primary="VPA (Vertical Pod Autoscaler)" data-type="indexterm" id="id750"/>Vertical Pod Autoscaler (VPA) differs from the HPA in that it doesn’t scale replicas; instead, it automatically scales requests. Earlier&#13;
in the chapter, we talked about setting requests on our pods and how&#13;
that guarantees <em>X</em> amount of resources for a given container. The VPA frees you from manually adjusting these requests and&#13;
automatically scales up and scales down pod requests&#13;
for you. For workloads that can’t scale out due to their architecture,&#13;
this works well for automatically scaling the resources. For example, a&#13;
MySQL database doesn’t scale the same way as a stateless web&#13;
frontend. With MySQL, you might want to set the Master nodes to&#13;
automatically scale up based on workload.</p>&#13;
&#13;
<p>The VPA is more complex than the HPA, and it consists of three components:</p>&#13;
<dl>&#13;
<dt><code>Recommender</code></dt>&#13;
<dd>&#13;
<p>    Monitors the current and past resource consumption, and&#13;
provides recommended values for the container’s CPU and memory requests</p>&#13;
</dd>&#13;
<dt><code>Updater</code></dt>&#13;
<dd>&#13;
<p>    Checks which of the pods have the correct resources set, and if they don’t, kills them so that they can be re-created by their controllers with&#13;
the updated requests</p>&#13;
</dd>&#13;
<dt><code>Admission Plugin</code></dt>&#13;
<dd>&#13;
<p>Sets the correct resource requests on new pods</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p class="pagebreak-before">Vertical scaling has two objectives:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Reducing the maintenance cost, by automating configuration of resource requirements.</p>&#13;
</li>&#13;
<li>&#13;
<p>Improving utilization of cluster resources, while minimizing the risk of containers running out of memory or getting CPU starved.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Management Best Practices" data-type="sect1"><div class="sect1" id="id213">&#13;
<h1>Resource Management Best Practices</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Utilize <a data-primary="resource management" data-secondary="best practices" data-type="indexterm" id="id751"/><a data-primary="best practices" data-secondary="resource management" data-type="indexterm" id="id752"/>pod anti-affinity to spread workloads across multiple&#13;
availability zones to ensure high availability for your application.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you’re using specialized hardware, such as GPU-enabled nodes, ensure that only&#13;
workloads that need GPUs are scheduled to those nodes by utilizing&#13;
taints.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use <code>NodeCondition</code> taints to proactively avoid failing or degraded&#13;
nodes.</p>&#13;
</li>&#13;
<li>&#13;
<p>Apply nodeSelectors to your pod specifications to schedule pods to specialized&#13;
hardware that you have deployed in the cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>Before going to production, experiment with different node sizes to&#13;
find a good mix of cost and performance for node types.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you’re deploying a mix of workloads with different performance&#13;
characteristics, utilize node pools to have mixed node types in a single&#13;
cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>Ensure that you set memory and CPU limits for all pods deployed to your&#13;
cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>Utilize <code>ResourceQuota</code>s to ensure that multiple teams or applications are allotted&#13;
their fair share of resources in the cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>Implement <code>LimitRange</code> to set default limits and requests for pod specifications&#13;
that don’t set limits or requests.</p>&#13;
</li>&#13;
<li>&#13;
<p>Start with manual cluster scaling until you understand your workload&#13;
profiles on Kubernetes. You can use autoscaling, but it comes with additional&#13;
considerations around node spin-up time and cluster scale down.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use the HPA for workloads that are variable and that&#13;
have unexpected spikes in their usage.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id360">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we discussed how you can optimally manage&#13;
Kubernetes and application resources. Kubernetes provides many built-in features to manage resources that you can use to maintain a reliable, highly utilized, and efficient cluster. Cluster&#13;
and pod sizing can be difficult at first, but through monitoring your&#13;
applications in production you can discover ways to optimize&#13;
your resources.</p>&#13;
</div></section>&#13;
</div></section></body></html>