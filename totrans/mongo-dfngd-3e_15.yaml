- en: Chapter 12\. Connecting to a Replica Set from Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers how applications interact with replica sets, including:'
  prefs: []
  type: TYPE_NORMAL
- en: How connections and failovers work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waiting for replication on writes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routing reads to the correct member
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client−to−Replica Set Connection Behavior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB client libraries (“drivers” in MongoDB parlance) are designed to manage
    communication with MongoDB servers, regardless of whether the server is a standalone
    MongoDB instance or a replica set. For replica sets, by default, drivers will
    connect to the primary and route all traffic to it. Your application can perform
    reads and writes as though it were talking to a standalone server while your replica
    set quietly keeps hot standbys ready in the background.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connections to a replica set are similar to connections to a single server.
    Use the `MongoClient` class (or equivalent) in your driver and provide a seed
    list for the driver to connect to. A seed list is simply a list of server addresses.
    *Seeds* are members of the replica set your application will read from and write
    data to. You do not need to list all members in the seed list (although you can).
    When the driver connects to the seeds, it will discover the other members from
    them. A connection string usually looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: See your driver’s documentation for details.
  prefs: []
  type: TYPE_NORMAL
- en: To provide further resilience, you should also use the [DNS Seedlist Connection
    format](https://oreil.ly/Uq4za) to specify how your applications connect to your
    replica set. The advantage to using DNS is that servers hosting your MongoDB replica
    set members can be changed in rotation without needing to reconfigure the clients
    (specifically, their connection strings).
  prefs: []
  type: TYPE_NORMAL
- en: All MongoDB drivers adhere to the [server discovery and monitoring (SDAM) spec](https://oreil.ly/ZsS8p).
    They persistently monitor the topology of your replica set to detect any changes
    in your application’s ability to reach all members of the set. In addition, the
    drivers monitor the set to maintain information on which member is the primary.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of replica sets is to make your data highly available in the face
    of network partitions or servers going down. In ordinary circumstances, replica
    sets respond gracefully to such problems by electing a new primary so that applications
    can continue to read and write data. If a primary goes down, the driver will automatically
    find the new primary (once one is elected) and will route requests to it as soon
    as possible. However, while there is no reachable primary, your application will
    be unable to perform writes.
  prefs: []
  type: TYPE_NORMAL
- en: There may be no primary available for a brief time (during an election) or for
    an extended period of time (if no reachable member can become primary). By default,
    the driver will not service any requests—read or write—during this period. If
    necessary to your application, you can configure the driver to use secondaries
    for read requests.
  prefs: []
  type: TYPE_NORMAL
- en: A common desire is to have the driver hide the entire election process (the
    primary going away and a new primary being elected) from the user. However, no
    driver handles failover this way, for a few reasons. First, a driver can only
    hide a lack of primary for so long. Second, a driver often finds out that the
    primary went down because an operation failed, which means that the driver doesn’t
    know whether or not the primary processed the operation before going down. This
    is a fundamental distributed systems problem that is impossible to avoid, so we
    need a strategy for dealing with it when it emerges. Should we retry the operation
    on the new primary, if one is elected quickly? Assume it got through on the old
    primary? Check and see if the new primary has the operation?
  prefs: []
  type: TYPE_NORMAL
- en: 'The correct strategy, it turns out, is to retry at most one time. Huh? To explain,
    let’s consider our options. These boil down to the following: don’t retry, give
    up after retrying some fixed number of times, or retry at most once. We also need
    to consider the type of error that could be the source of our problem. There are
    three types of errors we might see in attempting to write to a replica set: a
    transient network error, a persistent outage (either network or server), or an
    error caused by a command the server rejects as incorrect (e.g., not authorized).
    For each type of error, let’s consider our retry options.'
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of this discussion, let’s look at the example of a write to simply
    increment a counter. If our application attempts to increment our counter but
    gets no response from the server, we don’t know whether the server received the
    message and performed the update. So, if we follow a strategy of not retrying
    this write, for a transient network error, we might undercount. For a persistent
    outage or a command error not retrying is the correct strategy, because no amount
    of retrying the write operation will have the desired effect.
  prefs: []
  type: TYPE_NORMAL
- en: If we follow a strategy of retrying some fixed number of times, for transient
    network errors, we might overcount (in the case where our first attempt succeeded).
    For a persistent outage or command error, retrying multiple times will simply
    waste cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look now at the strategy of retrying just once. For a transient network
    error, we might overcount. For a persistent outage or command error, this is the
    correct strategy. However, what if we could ensure that our operations are idempotent?
    Idempotent operations are those that have the same outcome whether we do them
    once or multiple times. With idempotent operations, retrying network errors once
    has the best chance of correctly dealing with all three types of errors.
  prefs: []
  type: TYPE_NORMAL
- en: As of MongoDB 3.6, the server and all MongoDB drivers support a retryable writes
    option. See your driver’s documentation for details on how to use this option.
    With retryable writes, the driver will automatically follow the retry-at-most-once
    strategy. Command errors will be returned to the application for client-side handling.
    Network errors will be retried once after an appropriate delay that should accommodate
    a primary election under ordinary circumstances. With retryable writes turned
    on, the server maintains a unique identifier for each write operation and can
    therefore determine when the driver is attempting to retry a command that already
    succeeded. Rather than apply the write again, it will simply return a message
    indicating the write succeeded and thereby overcome the problem caused by the
    transient network issue.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for Replication on Writes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the needs of your application, you might want to require that all
    writes are replicated to a majority of the replica set before they are acknowledged
    by the server. In the rare circumstance where the primary of a set goes down and
    the newly elected primary (formerly a secondary) did not replicate the very last
    writes to the former primary, those writes will be rolled back when the former
    primary comes back up. They can be recovered, but it requires manual intervention.
    For many applications, having a small number of writes rolled back is not a problem.
    In a blog application, for example, there is little real danger in rolling back
    one or two comments from one reader.
  prefs: []
  type: TYPE_NORMAL
- en: However, for other applications, rollback of any writes should be avoided. Suppose
    your application sends a write to the primary. It receives confirmation that the
    write was written, but the primary crashes before any secondaries have had a chance
    to replicate that write. Your application thinks that it’ll be able to access
    that write, but the current members of the replica set don’t have a copy of it.
  prefs: []
  type: TYPE_NORMAL
- en: At some point, a secondary may be elected primary and start taking new writes.
    When the former primary comes back up, it will discover that it has writes that
    the current primary does not. To correct this, it will undo any writes that do
    not match the sequence of operations on the current primary. These operations
    are not lost, but they are written to special rollback files that have to be manually
    applied to the current primary. MongoDB cannot automatically apply these writes,
    since they may conflict with other writes that have happened since the crash.
    Thus, the writes essentially disappear until an admin gets a chance to apply the
    rollback files to the current primary (see [Chapter 11](ch11.xhtml#chapter-repl-comp)
    for more details on rollbacks).
  prefs: []
  type: TYPE_NORMAL
- en: 'The requirement of writing to a majority prevents this situation: if the application
    gets a confirmation that a write succeeded, then the new primary will have to
    have a copy of the write to be elected (a member must be up to date to be elected
    primary). If the application does not receive acknowledgment from the server or
    receives an error, then it will know to try again, because the write was not propagated
    to a majority of the set before the primary crashed.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to ensure that writes will be persisted no matter what happens to the
    set, we must ensure that each write propagates to a majority of the members of
    the set. We can achieve this using `writeConcern`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of MongoDB 2.6, `writeConcern` is integrated with write operations. For
    example, in JavaScript, we can use `writeConcern` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The specific syntax in your driver will vary depending on the programming language,
    but the semantics remain the same. In the example here, we specify a write concern
    of `"majority"`. Upon success, the server will respond with a message such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'But the server will not respond until this write operation has replicated to
    a majority of the members of the replica set. Only then will our application receive
    acknowledgment that this write succeeded. If the write does not succeed within
    the timeout we’ve specified, the server will respond with an error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Write concern majority and the replica set election protocol ensure that in
    the event of a primary election, only secondaries that are up to date with acknowledged
    writes can be elected primary. In this way, we guarantee that rollback will not
    happen. With the timeout option, we also have a tunable setting that enables us
    to detect and flag any long-running writes at the application layer.
  prefs: []
  type: TYPE_NORMAL
- en: Other Options for “w”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`"majority"` is not the only `writeConcern` option. MongoDB also lets you specify
    an arbitrary number of servers to replicate to by passing `"w"` a number, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will wait until two members (the primary and one secondary) have the write.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `"w"` value includes the primary. If you want the write propagated
    to ``*`n`*`` secondaries, you should set `"w"` to ``*`n`*``+1 (to include the
    primary). Setting ``"`w`" : 1`` is the same as not passing the `"w"` option at
    all because it just checks that the write was successful on the primary.'
  prefs: []
  type: TYPE_NORMAL
- en: The downside to using a literal number is that you have to change your application
    if your replica set configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Replication Guarantees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Writing to a majority of a set is considered “safe.” However, some sets may
    have more complex requirements: you may want to make sure that a write makes it
    to at least one server in each data center or a majority of the nonhidden nodes.
    Replica sets allow you to create custom rules that you can pass to `"getLastError"`
    to guarantee replication to whatever combination of servers you need.'
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteeing One Server per Data Center
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network issues between data centers are much more common than within data centers,
    and it is more likely for an entire data center to go dark than an equivalent
    smattering of servers across multiple data centers. Thus, you might want some
    data center−specific logic for writes. Guaranteeing a write to every data center
    before confirming success means that, in the case of a write followed by the data
    center going offline, every other data center will have at least one local copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set this up, we first classify the members by data center. We do this by
    adding a `"tags"` field to their replica set configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `"tags"` field is an object, and each member can have multiple tags. It
    might be a “high quality” server in the `"us-east"` data center, for example,
    in which case we’d want a `"tags"` field such as `{"dc": "us-east", "quality"
    : "high"}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to add a rule by creating a `"getLastErrorModes"` field
    in our replica set config. The name `"getLastErrorModes"` is vestigial in the
    sense that prior to MongoDB 2.6, applications used a method called `"getLastError"`
    to specify write concern. In replica configs, for `"getLastErrorModes"` each rule
    is of the form ``"*`name`*" : {"*`key`*" : *`number`*}}``. ``"*`name`*"`` is the
    name for the rule, which should describe what the rule does in a way that clients
    can understand, as they’ll be using this name when they call `getLastError`. In
    this example, we might call this rule `"eachDC"` or something more abstract such
    as `"user-level safe"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The ``"*`key`*"`` field is the key field from the tags, so in this example it
    will be `"dc"`. The *`number`* is the number of groups that are needed to fulfill
    this rule. In this case, *`number`* is 2 (because we want at least one server
    from `"us-east"` and one from `"us-west"`). *`number`* always means “at least
    one server from each of *`number`* groups.”
  prefs: []
  type: TYPE_NORMAL
- en: 'We add `"getLastErrorModes"` to the replica set config as follows and reconfigure
    to create the rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`"getLastErrorModes"` lives in the `"settings"` subobject of a replica set
    config, which contains a few set-level optional settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use this rule for writes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that rules are somewhat abstracted away from the application developer:
    they don’t have to know which servers are in `"eachDC"` to use the rule, and the
    rule can change without their application having to change. We could add a data
    center or change set members and the application would not have to know.'
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteeing a Majority of Nonhidden Members
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often, hidden members are somewhat second-class citizens: you’re never going
    to fail over to them and they certainly aren’t taking any reads. Thus, you may
    only care that nonhidden members received a write and let the hidden members sort
    it out for themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have five members, *host0* through *host4*, *host4* being a hidden
    member. We want to make sure that a majority of the nonhidden members have a write—that
    is, at least three of *host0*, *host1*, *host2*, and *host3*. To create a rule
    for this, first we tag each of the nonhidden members with its own tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The hidden member, *host4*, is not given a tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we add a rule for the majority of these servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use this rule in our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will wait until at least three of the nonhidden members have the write.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Other Guarantees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rules you can create are limitless. Remember that there are two steps to
    creating a custom replication rule:'
  prefs: []
  type: TYPE_NORMAL
- en: Tag members by assigning them key/value pairs. The keys describe classifications;
    for example, you might have keys such as `"data_center"` or `"region"` or `"serverQuality"`.
    Values determine which group a server belongs to within a classification. For
    example, for the key `"data_center"`, you might have some servers tagged `"us-east"`,
    some `"us-west"`, and others `"aust"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a rule based on the classifications you create. Rules are always of
    the form ``{"*`name`*" : {"*`key`*" : *`number`*}}``, where at least one server
    from *`number`* groups must have a write before it has succeeded. For example,
    you could create a rule `{"twoDCs" : {"data_center" : 2}}`, which would mean that
    at least one server in two of the data centers tagged must confirm a write before
    it is successful.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then you can use this rule in `getLastErrorModes`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rules are immensely powerful ways to configure replication, although they are
    complex to understand and set up. Unless you have fairly involved replication
    requirements, you should be perfectly safe sticking with `"w" : "majority"`.'
  prefs: []
  type: TYPE_NORMAL
- en: Sending Reads to Secondaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, drivers will route all requests to the primary. This is generally
    what you want, but you can configure other options by setting read preferences
    in your driver. Read preferences let you specify the types of servers queries
    should be sent to.
  prefs: []
  type: TYPE_NORMAL
- en: Sending read requests to secondaries is generally a bad idea. There are some
    specific situations in which it makes sense, but you should generally send all
    traffic to the primary. If you are considering sending reads to secondaries, make
    sure to weigh the pros and cons very carefully before allowing it. This section
    covers why it’s a bad idea and the specific conditions when it makes sense to
    do so.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applications that require strongly consistent reads should not read from secondaries.
  prefs: []
  type: TYPE_NORMAL
- en: Secondaries should usually be within a few milliseconds of the primary. However,
    there is no guarantee of this. Sometimes secondaries can fall behind by minutes,
    hours, or even days due to load, misconfiguration, network errors, or other issues.
    Client libraries cannot tell how up to date a secondary is, so clients will cheerfully
    send queries to secondaries that are far behind. Hiding a secondary from client
    reads can be done but is a manual process. Thus, if your application needs data
    that is predictably up to date, it should not read from secondaries.
  prefs: []
  type: TYPE_NORMAL
- en: If your application needs to read its own writes (e.g., insert a document and
    then query for it and find it) you should not send the read to a secondary (unless
    the write waits for replication to all secondaries using `"w"` as shown earlier).
    Otherwise, an application may perform a successful write, attempt to read the
    value, and not be able to find it (because it sent the read to a secondary that
    hasn’t replicated yet). Clients can issue requests faster than replication can
    copy operations.
  prefs: []
  type: TYPE_NORMAL
- en: To always send read requests to the primary, set your read preference to `primary`
    (or leave it alone, since `primary` is the default). If there is no primary, queries
    will error out. This means that your application cannot perform queries if the
    primary goes down. However, it is certainly an acceptable option if your application
    can deal with downtime during failovers or network partitions or if getting stale
    data is unacceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Load Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many users send reads to secondaries to distribute load. For example, if your
    servers can only handle 10,000 queries a second and you need to handle 30,000,
    you might set up a couple of secondaries and have them take some of the load.
    However, this is a dangerous way to scale because it’s easy to accidentally overload
    your system and difficult to recover from once you do.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose that you have the situation just described: 30,000 reads
    per second. You decide to create a replica set with four members (one of these
    would be configured as nonvoting, to prevent ties in elections) to handle this:
    each secondary is well below its maximum load and the system works perfectly.'
  prefs: []
  type: TYPE_NORMAL
- en: Until one of the secondaries crashes.
  prefs: []
  type: TYPE_NORMAL
- en: Now each of the remaining members are handling 100% of their possible load.
    If you need to rebuild the member that crashed, it may need to copy data from
    one of the other servers, overwhelming the remaining servers. Overloading a server
    often makes it perform slower, lowering the set’s capacity even further and forcing
    other members to take on more load, causing them to slow down in a death spiral.
  prefs: []
  type: TYPE_NORMAL
- en: Overloading can also cause replication to slow down, making the remaining secondaries
    fall behind. Suddenly you have a member down and a member lagging, and everything
    is too overloaded to have any wiggle room.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a good idea of how much load a server can take, you might feel
    like you can plan this out better: use five servers instead of four and the set
    won’t be overloaded if one goes down. However, even if you plan it out perfectly
    (and only lose the number of servers you expected), you still have to fix the
    situation with the other servers under more stress than they would be otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: A better choice is to use sharding to distribute load. We’ll cover how to set
    sharding up in [Chapter 14](ch14.xhtml#chapter_d1e10482).
  prefs: []
  type: TYPE_NORMAL
- en: Reasons to Read from Secondaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few cases in which it’s reasonable to send application reads to
    secondaries. For instance, you may want your application to still be able to perform
    reads if the primary goes down (and you do not care if those reads are somewhat
    stale). This is the most common case for distributing reads to secondaries: you’d
    like a temporary read-only mode when your set loses a primary. This read preference
    is called `primaryPreferred`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One common argument for reading from secondaries is to get low-latency reads.
    You can specify `nearest` as your read preference to route requests to the lowest-latency
    member based on average ping time from the driver to the replica set member. If
    your application needs to access the same document with low latency in multiple
    data centers, this is the only way to do it. If, however, your documents are more
    location-based (application servers in this data center need low-latency access
    to some of your data, or application servers in another data center need low-latency
    access to other data), this should be done with sharding. Note that you must use
    sharding if your application requires low-latency reads *and* low-latency writes:
    replica sets only allow writes to one location (wherever the primary is).'
  prefs: []
  type: TYPE_NORMAL
- en: You must be willing to sacrifice consistency if you are reading from members
    that may not have replicated all the writes yet. Alternatively, you could sacrifice
    write speed if you wanted to wait until writes had been replicated to all members.
  prefs: []
  type: TYPE_NORMAL
- en: If your application can truly function acceptably with arbitrarily stale data,
    you can use the `secondary` or `secondaryPreferred` read preferences. `secondary`
    will always send read requests to a secondary. If there are no secondaries available,
    this will error out rather than send reads to the primary. It can be used for
    applications that do not care about stale data and want to use the primary for
    writes only. If you have any concerns about staleness of data, this is not recommended.
  prefs: []
  type: TYPE_NORMAL
- en: '`secondaryPreferred` will send read requests to a secondary if one is available.
    If no secondaries are available, requests will be sent to the primary.'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, read load is drastically different than write load—i.e., you’re reading
    entirely different data than you’re writing. You might want dozens of indexes
    for offline processing that you don’t want to have on the primary. In this case,
    you might want to set up a secondary with different indexes than the primary.
    If you’d like to use a secondary for this purpose, you’d probably create a connection
    directly to it from the driver, instead of using a replica set connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider which of the options makes sense for your application. You can also
    combine options: if some read requests must be from the primary, use `primary`
    for those. If you are OK with other reads not having the most up-to-date data,
    use `primaryPreferred` for those. And if certain requests require low latency
    over consistency, use `nearest` for those.'
  prefs: []
  type: TYPE_NORMAL
