- en: Chapter 13\. Container Platform Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When implementing any technology in production, you’ll often gain the most mileage
    by designing a resilient platform that can withstand the unexpected issues that
    will inevitably occur. Docker can be a powerful tool but requires attention to
    detail to get the whole platform right around it. As a technology that is going
    through very rapid growth, it is bound to produce frustrating bugs that crop up
    between the various components that make up your container platform.
  prefs: []
  type: TYPE_NORMAL
- en: If instead of simply deploying Docker into your existing environment, you take
    the time to build a well-designed container platform utilizing Docker as one of
    the core components, you can enjoy the many benefits of a container-based workflow
    while simultaneously protecting yourself from some of the sharper edges that can
    exist in such high-velocity projects.
  prefs: []
  type: TYPE_NORMAL
- en: Like all other technology, Docker doesn’t magically solve all your problems.
    To reach its true potential, organizations must make very conscious decisions
    about why and how to use it. For small projects, it is possible to use Docker
    in a simple manner; however, if you plan to support a large project that can scale
    with demand, it’s crucial that you design your applications and the platform very
    deliberately. This ensures that you can maximize the return on your investment
    in the technology. Taking the time to intentionally design your platform will
    also make it much easier to modify your production workflow over time. A well-designed
    container platform and deployment process will be as lightweight and straightforward
    as possible while still supporting the features required to meet all the technical
    and compliance requirements. A well-thought-out design will help ensure that your
    software is running on a dynamic foundation that can easily be upgraded as technology
    and company processes develop.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore two open documents, [“The Twelve-Factor App”](https://12factor.net)
    and [“The Reactive Manifesto”](https://www.reactivemanifesto.org) (a companion
    document to [“The Reactive Principles”](https://www.reactiveprinciples.org)),
    and discuss how they relate to Docker and building robust container platforms.
    Both documents contain a lot of ideas that should help guide you through the design
    and implementation of your container platform and ensure more resiliency and supportability
    across the board.
  prefs: []
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In November of 2011, well before the release of Docker, Heroku cofounder Adam
    Wiggins and his colleagues released an article called [“The Twelve-Factor App”](https://12factor.net).
    This document describes a series of 12 practices, distilled from the experiences
    of the [Heroku](https://www.heroku.com) engineers, for designing applications
    that will thrive and grow in a modern container-based SaaS environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although not required, applications built with these 12 steps in mind are ideal
    candidates for the Docker workflow. Throughout this chapter, we will explore each
    of the following steps and explain why these practices can, in numerous ways,
    help improve your development cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: Codebase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Config
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build, release, run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Port binding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disposability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development/production parity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admin processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codebase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*One codebase tracked in revision control.*'
  prefs: []
  type: TYPE_NORMAL
- en: Many instances of your application will be running at any given time, but they
    should all come from the same code repository. Every single Docker image for a
    given application should be built from a single source code repository that contains
    all the code required to build the Linux container. This ensures that the code
    can easily be rebuilt and that all third-party requirements are well-defined within
    the repository and will automatically be pulled in during a build.
  prefs: []
  type: TYPE_NORMAL
- en: What this means is that building your application shouldn’t require stitching
    together code from multiple source repositories. That is not to say that you can’t
    have a dependency on an artifact from another repo. But it does mean that there
    should be a clear mechanism for determining which pieces of code were shipped
    when you built your application. Docker’s ability to simplify dependency management
    is much less useful if building your application requires pulling down multiple
    source code repositories and stitching pieces together. It also is not very repeatable
    if you must know a magic incantation to get the build to work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: A good test might be to give a new developer in your company a clean laptop
    and a paragraph of directions and then see if they can successfully build your
    application in under an hour. If they can’t, then the process probably needs to
    be refined and simplified.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Explicitly declare and isolate dependencies*.'
  prefs: []
  type: TYPE_NORMAL
- en: Never rely on the belief that a dependency will be made available via some other
    avenue, like the operating system install. Any dependencies that your application
    requires should be well defined in the codebase and pulled in by the build process.
    This will help ensure that your application will run when deployed, without relying
    on libraries being installed by other people or processes. This is particularly
    important within a container since the container’s processes are isolated from
    the rest of the host operating system and will usually not have access to anything
    outside of the host’s kernel and the container image’s filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: The *Dockerfile* and language-dependent configuration files like Node’s *package.json*
    or Ruby’s *Gemfile* should define every nonexternal dependency required by your
    application. This ensures that your image will run correctly on any system to
    which it is deployed. Gone will be the days when you try to deploy and run your
    application in production only to find out that important libraries are missing
    or installed with the wrong version. This pattern has huge reliability and repeatability
    advantages and very positive ramifications for system security. If to fix a security
    issue, you update the OpenSSL or *libyaml* libraries that your containerized application
    uses, then you can be assured that it will always be running with that version
    wherever you deploy that particular application.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that many Docker base images are larger than they
    need to be. Remember that your application process will be running on a shared
    kernel, and the only files that you need inside your image are the ones that the
    process will require to run. It’s good that base images are so readily available,
    but they can sometimes mask hidden dependencies. Although people often start with
    a minimal install of Alpine, Ubuntu, or Fedora, these images still contain a lot
    of operating system files and applications that your process almost certainly
    does not need, or possibly some files that your application is making use of that
    you aren’t consciously aware of, like compiling your application using the *musl*
    system library in Alpine versus the *glibc* system library in many other base
    images. You need to be fully aware of your dependencies, even when containerizing
    your application. It is also important to consider what support tools, if any,
    you are including in your images, as there can be a fine line between making things
    easier to debug and increasing the security attack surface of your application
    and environments.
  prefs: []
  type: TYPE_NORMAL
- en: A good way to shed light on what files are required inside an image is to compare
    a “small” base image with an image for a statically linked program written in
    a language like Go or C. These applications can be designed to run directly on
    the Linux kernel without any additional libraries or files.
  prefs: []
  type: TYPE_NORMAL
- en: To help drive this point home, it might be useful to review the exercises in
    [“Keeping Images Small”](ch04.html#small_images), where we explored one of these
    ultra-light containers, `spkane/scratch-helloworld`, and then dived into the underlying
    filesystem a bit and compared it with the popular `alpine` base image.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to being conscientious about how you manage the filesystem layers
    in your images, keeping your images stripped down to the bare necessities is another
    great way to keep everything streamlined and your `docker image pull` commands
    fast. Applications written with interpreted languages will require many more files
    because of the large runtimes and dependency graphs you often need to install,
    but you should try to keep as minimal a base layer as needed for your use case
    so that you can reason about your dependencies. Docker helps you package them
    up, but you still need to be in charge of reasoning about them.
  prefs: []
  type: TYPE_NORMAL
- en: Config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Store configuration in environment variables, not in files checked into the
    codebase.*'
  prefs: []
  type: TYPE_NORMAL
- en: This makes it simple to deploy the same codebase to different environments,
    like staging and production, without maintaining complicated configuration in
    code or rebuilding your container for each environment. This keeps your codebase
    much cleaner by keeping environment-specific information like database names and
    passwords out of your source code repository. More importantly, though, it means
    that you don’t bake deployment environment assumptions into the repository, and
    thus it is extremely easy to deploy your applications anywhere that it might be
    useful. You also want to be able to test the same image you will ship to production.
    You can’t do that if you have to build an image for each environment with all
    of its configuration already baked in.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 4](ch04.html#docker_images), you can achieve this by
    launching `docker container run` commands that leverage the `-e` command-line
    argument. Using `-e APP_ENV=` `*production*` tells Docker to set the environment
    variable `APP_ENV` to the value `production` within the newly launched container.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a real-world example, let’s assume we pulled the image for the chat robot
    Hubot with the [Rocket.Chat](https://www.rocket.chat) adapter installed. We’d
    issue something like the following command to get it running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are passing a whole set of environment variables into the container
    when it is created. When the process is launched in the container, it will have
    access to these environment variables so that it can properly configure itself
    at runtime. These configuration items are now an external dependency that we can
    inject at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many other ways to provide this data to a container, including using
    key/value stores like `etcd` and `consul`. Environment variables are simply a
    universal option that acts as a very good starting point for most projects. They
    are the easy path for container configuration because they are well supported
    by the platform and every programming language in common use. They also aid in
    the observability of your applications because the configuration can easily be
    inspected with `docker container inspect`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of a Node.js application like `hubot`, you could then write the
    following code to make decisions based on these environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The exact method used to pass this configuration data into your container will
    vary depending on the specific tooling that you’ve chosen for your projects, but
    almost all of them will make it easy to ensure that every deployment contains
    the proper settings for that environment.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping specific configuration information out of your source code makes it
    very easy to deploy the exact same container to multiple environments, with no
    changes and no sensitive information committed into your source code repository.
    Crucially, it supports testing your container images thoroughly before deploying
    to production by allowing the same image to be used in all environments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you need a process for managing secrets that need to be provided to your
    containers, you might want to look into the [documentation](https://docs.docker.com/engine/swarm/secrets)
    for the `docker secret` command, which works with Docker Swarm mode, and HashiCorp’s
    [Vault](https://www.vaultproject.io).
  prefs: []
  type: TYPE_NORMAL
- en: Backing Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Treat backing services as attached resources.*'
  prefs: []
  type: TYPE_NORMAL
- en: Local databases are no more reliable than third-party services and should be
    treated as such. Applications should handle the loss of an attached resource gracefully.
    By implementing graceful degradation in your application and never assuming that
    any resource, including filesystem space, is available, you ensure that your application
    will continue to perform as many of its functions as it can, even when external
    resources are unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t something that Docker helps you with directly, and although it is
    always a good idea to write robust services, it is even more important when you
    are using containers. When using containers, you achieve high availability most
    often through horizontal scaling and rolling deployments, instead of relying on
    the live migration of long-running processes, like on traditional VMs. This means
    that specific instances of a service will often come and go over time, and your
    service should be able to handle this gracefully.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, because Linux containers have limited filesystem resources, you
    can’t simply rely on having some local storage available. You need to plan that
    into your application’s dependencies and handle it explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Build, Release, Run
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Strictly separate build and run stages.*'
  prefs: []
  type: TYPE_NORMAL
- en: Build the code, release it with the proper configuration, and then deploy it.
    This ensures that you maintain control of the process and can perform any single
    step without triggering the whole workflow. By ensuring that each of these steps
    is self-contained in a distinct process, you can tighten the feedback loop and
    react more quickly to any problems within the deployment flow.
  prefs: []
  type: TYPE_NORMAL
- en: As you design your Docker workflow, you want to clearly separate each step in
    the deployment process. It is perfectly fine to have a single button that builds
    a container, tests it, and then deploys it, assuming that you trust your testing
    processes—but you don’t want to be forced to rebuild a container simply to deploy
    it to another environment.
  prefs: []
  type: TYPE_NORMAL
- en: Docker supports the 12-factor ideal well in this area because the image registry
    provides a clean handoff point between building an image and shipping it to production.
    If your build process generates images and pushes them to the registry, then deployment
    can simply be pulling the image down to servers and running it.
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Execute the app as one or more stateless processes.*'
  prefs: []
  type: TYPE_NORMAL
- en: All shared data must be accessed via a stateful backing store so that application
    instances can easily be redeployed without losing any important session data.
    You don’t want to keep any critical state on disk in your ephemeral container
    or in the memory of one of its processes. Containerized applications should always
    be considered ephemeral. A truly dynamic container environment requires the ability
    to destroy and re-create containers at a moment’s notice. This flexibility helps
    enable the rapid deployment cycle and outage recovery demanded by modern, Agile
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: As much as possible, it is preferable to write applications that do not need
    to keep state longer than the time required to process and respond to a single
    request. This ensures that the impact of stopping any given container in your
    application pool is very minimal. When you must maintain state, the best approach
    is to use a remote datastore like Redis, PostgreSQL, Memcache, or even Amazon
    S3, depending on your resiliency needs.
  prefs: []
  type: TYPE_NORMAL
- en: Port Binding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Export services via port binding.*'
  prefs: []
  type: TYPE_NORMAL
- en: Your application needs to be addressable by a port specific to itself. Applications
    should bind directly to a port to expose the service and should not rely on an
    external daemon like `inetd` to handle that for them. You should be certain that
    when you’re talking to that port, you’re talking to your application. Most modern
    web platforms are quite capable of directly binding to a port and servicing their
    own requests.
  prefs: []
  type: TYPE_NORMAL
- en: To expose a port from your container, as discussed in [Chapter 4](ch04.html#docker_images),
    you can launch `docker container run` commands that use the `--publish` command-line
    argument. Using `--publish mode=ingress,published=80,target=8080`, for example,
    would tell Docker to proxy the container’s port 8080 on the host’s port 80.
  prefs: []
  type: TYPE_NORMAL
- en: The statically linked Go Hello World container that we discussed in [“Keeping
    Images Small”](ch04.html#small_images) is a great example of this, because the
    container contains nothing but our application to serve its content to a web browser.
    We did not need to include any additional web servers, which would require further
    configuration, introduce additional complexity, and increase the number of potential
    failure points in our system.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Scale out via the process model.*'
  prefs: []
  type: TYPE_NORMAL
- en: Design for concurrency and horizontal scaling within your applications. Increasing
    the resources of an existing instance can be difficult and hard to reverse. Adding
    and removing instances as scale fluctuates is much easier and helps maintain flexibility
    in the infrastructure. Launching another container on a new server is incredibly
    inexpensive compared to the effort and expense required to add resources to an
    underlying virtual or physical system. Designing for horizontal scaling allows
    the platform to react much faster to changes in resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, in [Chapter 10](ch10.html#containers_scale), you saw how easily
    a service could be scaled using Docker Swarm mode by simply running a command
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is where tools like Docker Swarm mode, Mesos, and Kubernetes truly begin
    to shine. Once you have implemented a Docker cluster with a dynamic scheduler,
    it is very easy to add three more instances of a container to the cluster as load
    increases and then to be able to easily remove two instances of your application
    from the cluster as load starts to decrease again.
  prefs: []
  type: TYPE_NORMAL
- en: Disposability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Maximize robustness with fast startup and graceful shutdown.*'
  prefs: []
  type: TYPE_NORMAL
- en: Services should be designed to be ephemeral. We already talked a little bit
    about this when discussing external state with containers. Responding well to
    dynamic horizontal scaling, rolling deploys, and unexpected problems requires
    applications that can quickly and easily be started or shut down. Services should
    respond gracefully to a `SIGTERM` signal from the operating system and even handle
    hard failures confidently. Most importantly, we shouldn’t care if any given container
    for our application is up and running. As long as requests are being served, the
    developer should be freed of concerns about the health of any single component
    within the system. If an individual node is behaving poorly, turning it off or
    redeploying it should be an easy decision that doesn’t entail long planning sessions
    and concerns about the health of the rest of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 7](ch07.html#debug_docker), Docker sends standard Unix
    signals to containers when it is stopping or killing them; therefore, any containerized
    application can detect these signals and take the appropriate steps to shut down
    gracefully.
  prefs: []
  type: TYPE_NORMAL
- en: Development/Production Parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Keep development, staging, and production as similar as possible.*'
  prefs: []
  type: TYPE_NORMAL
- en: The same processes and artifacts should be used to build, test, and deploy services
    into all environments. The same people should do the work in all environments,
    and the physical nature of the environments should be as similar as reasonably
    possible. Repeatability is incredibly important. Almost any issue discovered in
    production points to a failure in the process. Every area where production diverges
    from staging is an area where risk is being introduced into the system. These
    inconsistencies blind you to certain types of issues that could occur in your
    production environment until it is too late to proactively deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, this advice essentially repeats a few of the early recommendations.
    However, the specific point here is that any environment divergence introduces
    risks, and although these differences are common in many organizations, they are
    much less necessary in a containerized environment. Docker servers can normally
    be created so that they are identical in all of your environments, and environment-based
    configuration changes should typically impact only which endpoints your service
    connects to without specifically changing the application’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Treat logs as event streams.*'
  prefs: []
  type: TYPE_NORMAL
- en: Services should not concern themselves with routing or storing logs. Instead,
    events should be streamed, unbuffered, to `STDOUT` and `STDERR` for handling by
    the hosting process. In development, `STDOUT` and `STDERR` can be easily viewed,
    whereas in staging and production, the streams can be routed to anything, including
    a central logging service. Different environments have different exceptions for
    log handling. This logic should never be hardcoded into the application. Streaming
    everything to `STDOUT` and `STDERR` enables the top-level process manager to handle
    the logs via whatever method is best for the environment, allowing the application
    developer to focus on core functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.html#exploring_docker), we discussed the `docker container
    logs` command, which collects the output from your container’s `STDOUT` and `STDERR`
    and records it as logs. If you write logs to random files within the container’s
    filesystem, you will not have easy access to them. It is also possible to configure
    Docker to send logs to a local or remote logging system using tools like `rsyslog`,
    `journald`, or `fluentd`.
  prefs: []
  type: TYPE_NORMAL
- en: If you use a process manager or initialization system on your servers, like
    `systemd` or `upstart`, it is usually very easy to direct all process output to
    `STDOUT` and `STDERR` and then have your process monitor capture them and send
    them to a remote logging host.
  prefs: []
  type: TYPE_NORMAL
- en: Admin Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Run admin/management tasks as one-off processes.*'
  prefs: []
  type: TYPE_NORMAL
- en: One-off administration tasks should be run via the same codebase and configuration
    that the application uses. This helps avoid synchronization problems and code/schema
    drift problems. Oftentimes, management tools exist as one-off scripts or live
    in a completely different codebase. It is much safer to build management tools
    within the application’s codebase and utilize the same libraries and functions
    to perform the required work. This can significantly improve the reliability of
    these tools by ensuring that they leverage the same code paths that the application
    relies on to perform its core functionality.
  prefs: []
  type: TYPE_NORMAL
- en: What this means is that you should never rely on random `cron`-like scripts
    to perform administrative and maintenance functions. Instead, include all of these
    scripts and functionality in your application codebase. Assuming that these don’t
    need to be run on every instance of your application, you can launch a special
    short-lived container, or use `docker container exec` with the existing container,
    whenever you need to run a maintenance job. This command can trigger the required
    job, report its status somewhere, and then exit.
  prefs: []
  type: TYPE_NORMAL
- en: Twelve-Factor Wrap-Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While “The Twelve-Factor App” wasn’t written as a Docker-specific manifesto,
    almost all of this advice can be applied to writing and deploying applications
    on a Docker platform. This is in part because the article heavily influenced Docker’s
    design, and in part because the manifesto itself codified many of the best practices
    promoted by modern software architects.
  prefs: []
  type: TYPE_NORMAL
- en: The Reactive Manifesto
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Riding alongside “The Twelve-Factor App,” another pertinent document was released
    in July of 2013 by Typesafe cofounder and CTO Jonas Bonér, entitled [“The Reactive
    Manifesto”](https://www.reactivemanifesto.org). Jonas originally worked with a
    small group of contributors to solidify a manifesto that discusses how the expectations
    for application resiliency have evolved over the last few years and how applications
    should be engineered to react predictably to various forms of interaction, including
    events, users, load, and [failures](https://www.lightbend.com/blog/why-do-we-need-a-reactive-manifesto).
  prefs: []
  type: TYPE_NORMAL
- en: “The Reactive Manifesto” states that “reactive systems” are responsive, resilient,
    elastic, and message driven.
  prefs: []
  type: TYPE_NORMAL
- en: Responsive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The system responds in a timely manner if at all possible.*'
  prefs: []
  type: TYPE_NORMAL
- en: In general, this means that the application should respond to requests very
    quickly. Users simply don’t want to wait, and there is rarely a good reason to
    make them. If you have a containerized service that renders large PDF files, design
    it so that it immediately responds with a “job submitted” message so that users
    can go about their day, and then provide a message or banner that informs them
    when the job is finished and where they can download the resulting PDF.
  prefs: []
  type: TYPE_NORMAL
- en: Resilient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The system stays responsive in the face of failure*.'
  prefs: []
  type: TYPE_NORMAL
- en: When your application fails for any reason, the situation will always be worse
    if it becomes unresponsive. It is much better to handle the failure gracefully
    and dynamically reduce the application’s functionality or even display a simple
    but clear problem message to the user while reporting the issue internally.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The system stays responsive under varying workload.*'
  prefs: []
  type: TYPE_NORMAL
- en: With Docker, you achieve this by dynamically deploying and decommissioning containers
    as requirements and load fluctuate so that your application is always able to
    handle server requests quickly, without deploying a lot of underutilized resources.
  prefs: []
  type: TYPE_NORMAL
- en: Message Driven
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Reactive systems rely on asynchronous message passing to establish a boundary
    between components that ensures loose coupling, isolation, and location transparency.*'
  prefs: []
  type: TYPE_NORMAL
- en: Although not directly addressed by Docker, the idea here is that there are times
    when an application can become busy or unavailable. If you utilize asynchronous
    message passing between your services, you can help ensure that your services
    will not lose requests and that they will be processed as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Wrap-Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All four of the design features in “The Reactive Manifesto” require application
    developers to design graceful degradation and define a clear separation of responsibilities
    in their applications. By treating all dependencies as properly designed, attached
    resources, dynamic container environments allow you to easily maintain *N*+2 status
    across your application stack, reliably scale individual services in your environment,
    and quickly replace unhealthy nodes.
  prefs: []
  type: TYPE_NORMAL
- en: A service is only as reliable as its least reliable dependency, so it is vital
    to incorporate these ideas into every component of your platform.
  prefs: []
  type: TYPE_NORMAL
- en: The core ideas in “The Reactive Manifesto” merge very nicely with “The Twelve-Factor
    App” and the Docker workflow. These documents successfully summarize many of the
    most important discussions about the way you need to think and work if you want
    to be successful in meeting new expectations in the industry. The Docker workflow
    provides a practical way to implement many of these ideas in any organization
    in a completely approachable manner.
  prefs: []
  type: TYPE_NORMAL
