<html><head></head><body><section data-pdf-bookmark="Chapter 3. Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch03">&#13;
<h1><span class="label">Chapter 3. </span>Data</h1>&#13;
&#13;
&#13;
<p>This chapter begins the second part of the journey: <em>indirect query optimization</em>.&#13;
As mentioned in <a data-type="xref" href="ch01.html#query-optimization">“Improving Query Response Time”</a>, direct query optimization solves a lot of problems, but not all.&#13;
Even when you surpass the knowledge and skills in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>, which focuses on direct query optimization, you will encounter queries that are simple and properly indexed but still slow.&#13;
That’s when you begin to optimize <em>around</em> the query, starting with the data that it accesses.&#13;
To understand why, let’s think about <a data-primary="rocks" data-type="indexterm" id="rocks-ch3"/><a data-primary="pebbles" data-type="indexterm" id="pebbles-ch3"/><a data-primary="cobbles" data-type="indexterm" id="cobbles-ch3"/><a data-primary="boulders" data-type="indexterm" id="boulders-ch3"/>rocks.</p>&#13;
&#13;
<p>Imagine that your job is to move rocks, and you have three piles of different sized rocks.&#13;
The first pile contains pebbles: very light, no larger than your thumbnail.&#13;
The second pile contains cobbles: heavy but light enough to pick up, no larger than your head.&#13;
The third pile contains boulders: too large and heavy to pick up; you need leverage or a machine to move them.&#13;
Your job is to move one pile from the bottom of a hill to the top (no matter why; but if it helps, imagine that you’re Sisyphus).&#13;
Which pile do you choose?</p>&#13;
&#13;
<p>I presume that you choose the pebbles because they’re light and easy to move.&#13;
But there’s a critical detail that might change your decision: weight.&#13;
The pile of pebbles weighs two metric tons (the weight of a mid-size SUV).&#13;
The pile of cobbles weighs one metric ton (the weight of a very small car).&#13;
And there’s only one boulder that weighs half a metric ton (the weight of ten adult humans).&#13;
Now which pile do you choose?</p>&#13;
&#13;
<p>On the one hand, the pebbles are still a lot easier to move.&#13;
You can shovel them into a wheelbarrow and roll it up the hill.&#13;
There’s just a lot of them (pebbles, not wheelbarrows).&#13;
The boulder is a fraction of the weight, but its singular size makes it unwieldy.&#13;
Special equipment is need to move it up the hill, but it’s a one-time task.&#13;
Tough decision.&#13;
<a data-type="xref" href="ch05.html#ch05">Chapter 5</a> provides an answer and an explanation, but we have much more to cover before that chapter.</p>&#13;
&#13;
<p>Data is analogous to a pile of rocks, and executing queries is analogous to moving the rocks uphill.&#13;
When data size is small, direct query optimization is usually sufficient because the data is trivial to handle—like walking (or running) up a hill with a handful of pebbles.&#13;
But as data size increases, indirect query optimization becomes increasingly important—like lugging a heavy cobble up a hill and stopping midway to ask, “Can we do something about these <a data-primary="rocks" data-startref="rocks-ch3" data-type="indexterm" id="idm45829112726992"/><a data-primary="pebbles" data-startref="pebbles-ch3" data-type="indexterm" id="idm45829112726016"/><a data-primary="cobbles" data-startref="cobbles-ch3" data-type="indexterm" id="idm45829112725072"/><a data-primary="boulders" data-startref="boulders-ch3" data-type="indexterm" id="idm45829112724128"/>rocks?”</p>&#13;
&#13;
<p><a data-type="xref" href="ch01.html#ch01">Chapter 1</a> provided a “proof” that data size affects performance: <code>TRUNCATE TABLE</code> dramatically increases performance—but don’t use this “optimization.”&#13;
That’s a joke, but it also proves a point that is not frequently followed through to its logical consequence: <em>less data is more performance</em>.&#13;
That’s the tagline; the full statement is: you can improve performance by reducing data because less data requires fewer system resources (CPU, memory, storage, and so on).</p>&#13;
&#13;
<p>You can tell by now that this chapter is going to argue for <em>less</em> data.&#13;
But isn’t <em>more</em> data the reality and reason that drives engineers to learn about performance optimization?&#13;
Yes, and <a data-type="xref" href="ch05.html#ch05">Chapter 5</a> addresses MySQL at scale, but first it’s imperative to learn to reduce and optimize data when it’s relatively small and problems are tractable.&#13;
The most stressful time to learn is when you’ve ignored data size until it’s crushing the application.</p>&#13;
&#13;
<p>This chapter examines data with respect to performance and argues that reducing data access and storage is a technique—an indirect query optimization—for improving performance.&#13;
There are three major sections.&#13;
The first reveals three secrets about MySQL performance.&#13;
The second introduces what I call the <em>principle of least data</em> and its numerous implications.&#13;
The third covers how to quickly <em>and safely</em> delete or archive data.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Three Secrets" data-type="sect1"><div class="sect1" id="idm45829112716352">&#13;
<h1>Three Secrets</h1>&#13;
&#13;
<p>To keep a secret is to conceal a truth.&#13;
The following truths are not always revealed in books about MySQL performance for two reasons.&#13;
First, they complicate matters.&#13;
It’s a lot easier to write about and explain performance without mentioning the caveats and gotchas.&#13;
Second, they’re counterintuitive.&#13;
That doesn’t make them false, but it does make them difficult to clarify.&#13;
Nevertheless, the following truths are important for MySQL performance, so let’s dig into the details with an open mind.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Indexes May Not Help" data-type="sect2"><div class="sect2" id="index-may-not-help">&#13;
<h2>Indexes May Not Help</h2>&#13;
&#13;
<p>Ironically, <a data-primary="data" data-secondary="indexes may not help" data-type="indexterm" id="data-indexes-may-not-help-ch3"/><a data-primary="indexes" data-secondary="possibility indexes may not help" data-type="indexterm" id="indexes-possibility-indexes-may-not-help-ch3"/>you can expect the majority of slow queries to use an index lookup.&#13;
That’s ironic for two reasons.&#13;
First, indexes are the key to performance, but a query can be slow even with a good index.&#13;
Second, after learning about indexes and indexing (as discussed in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>), engineers become so good at avoiding index scans and table scans that only index lookups remain, which is a good problem but ironic nonetheless.</p>&#13;
&#13;
<p>Performance cannot be achieved without indexes, but that doesn’t mean that indexes provide infinite leverage for infinite data size.&#13;
Don’t lose faith in indexes, but be aware of the following cases in which indexes may not help.&#13;
For each case, presuming the query and its indexes cannot be optimized any further, the next step is indirect query optimization.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Index scan" data-type="sect3"><div class="sect3" id="idm45829112707488">&#13;
<h3>Index scan</h3>&#13;
&#13;
<p>An index scan <a data-primary="index scan" data-type="indexterm" id="idm45829112706160"/>provides diminishing leverage as a table grows because the index also grows: more table rows, more index values.<sup><a data-type="noteref" href="ch03.html#idm45829112705168" id="idm45829112705168-marker">1</a></sup>&#13;
(By contrast, the leverage that an index lookup provides almost never diminishes as long as the index fits in memory.)&#13;
Even an index-only scan tends not to scale because it almost certainly reads a large number of values—a safe presumption because MySQL would have done an index lookup to read fewer rows if possible.&#13;
An index scan only delays the inevitable: as the number of rows in the table increases, response time for queries that use an index scan also increases.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Finding rows" data-type="sect3"><div class="sect3" id="idm45829112703760">&#13;
<h3>Finding rows</h3>&#13;
&#13;
<p>When I optimize a <a data-primary="rows examined" data-type="indexterm" id="rows-examined2"/>slow query that uses an index lookup, the first query metric I check is rows examined (see <a data-type="xref" href="ch01.html#Rows-examined">“Rows examined”</a>).&#13;
Finding matching rows is the fundamental purpose of a query, but even with a good index, a query can examine too many rows.&#13;
<em>Too many</em> is the point at which response time becomes unacceptable (and the root cause is not something else, like insufficient memory or disk IOPS).&#13;
This happens because several index lookup access types can match many rows.&#13;
Only the access types listed in <a data-type="xref" data-xrefstyle="select:nopage" href="#one-row-idx">Table 3-1</a> match <em>at most</em> one row.</p>&#13;
<table id="one-row-idx">&#13;
<caption><span class="label">Table 3-1. </span>Index lookup access types that match at most one row</caption>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p><code>system</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p><code>const</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p><code>eq_ref</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p><code>unique_subquery</code></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>If the <code>type</code> field in an EXPLAIN plan is not one of the access types listed in <a data-type="xref" data-xrefstyle="select:nopage" href="#one-row-idx">Table 3-1</a>, then pay close attention to the <code>rows</code> field and the query metric rows examined (see <a data-type="xref" href="ch01.html#Rows-examined">“Rows examined”</a>).&#13;
Examining a very large number of rows is slow regardless of the index lookup.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a href="https://oreil.ly/8dkRy">“EXPLAIN Output Format”</a> in the MySQL manual enumerates access types, which it calls <em>join types</em> because MySQL treats every query as a join.&#13;
In this book, for precision and consistency I use only two terms: <em>access method</em> and <em>access type</em>, as written throughout <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>.</p>&#13;
</div>&#13;
&#13;
<p>Very low index selectivity is a likely accomplice.&#13;
Recall <a data-type="xref" href="ch02.html#extreme-selectivity">“Extreme Selectivity”</a>: index selectivity <a data-primary="index selectivity" data-type="indexterm" id="idm45829112679984"/>is cardinality divided by the number of rows in the table.&#13;
MySQL is unlikely to chose an index with very low selectivity because it can match too many rows.&#13;
Since secondary indexes require a second lookup in the primary key to read rows, it can be faster to eschew an index with extremely low selectivity and do a full table scan instead—presuming there’s no better index.&#13;
You can detect this in an EXPLAIN plan when the access method is a table scan (<code>type: ALL</code>) but there are indexes that MySQL could use (<code>possible_keys</code>).&#13;
To see the execution plan that MySQL is not choosing, <code>EXPLAIN</code> the query with <a href="https://oreil.ly/nv1uy"><code>FORCE INDEX</code></a> to use an index listed in the <code>possible_keys</code> field.&#13;
Most likely, the resulting execution plan will be an index scan (<code>type: index</code>) with a large number of <code>rows</code>, which is why MySQL chooses a table scan instead.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Recall <a data-type="xref" href="ch02.html#its-a-trap">“It’s a Trap! (When MySQL Chooses Another Index)”</a>: in very rare cases, MySQL chooses the wrong index.&#13;
If a query examines too many rows but you’re certain there’s a better index that MySQL should use, there’s a small chance that the index statistics are wrong, which causes MySQL to not choose the better index.&#13;
Run <code>ANALYZE TABLE</code> to update index statistics.</p>&#13;
</div>&#13;
&#13;
<p>Remember that index selectivity is a function of cardinality <a data-primary="cardinality" data-type="indexterm" id="idm45829112672144"/>and the number of rows in the table.&#13;
If cardinality remains constant but the number of rows increases, then selectivity decreases.&#13;
Consequently, an index that helped when the table was small may not help when the table is huge.<a data-primary="rows examined" data-startref="rows-examined2" data-type="indexterm" id="idm45829112670992"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Joining tables" data-type="sect3"><div class="sect3" id="less-data-join">&#13;
<h3>Joining tables</h3>&#13;
&#13;
<p>When joining tables, <a data-primary="table join" data-type="indexterm" id="table-join"/>a few rows in each table quickly obliterate performance.&#13;
If you recall from <a data-type="xref" href="ch02.html#table-join-algos">“Table Join Algorithms”</a>, the nested-loop join (NLJ) algorithm (<a data-type="xref" href="ch02.html#NLJ">Example 2-22</a>) entails that the total number of rows accessed for a join is the product of rows accessed for each table.&#13;
In other words, multiply the values for <code>rows</code> in an EXPLAIN plan.&#13;
A three-table join with only one hundred rows per table can access one <em>million</em> rows: 100 × 100 × 100 = 1,000,000.&#13;
To avoid this, the index lookup on each table joined should match only one row—one of the access types listed in <a data-type="xref" href="#one-row-idx">Table 3-1</a> is best.</p>&#13;
&#13;
<p>MySQL can join tables in almost any order.&#13;
Use this to your advantage: sometimes the solution to a poor join is a better index on another table that allows MySQL to change the join order.</p>&#13;
&#13;
<p>Without an index lookup, a table join is doomed.&#13;
The result is a full join, as forewarned in <a data-type="xref" href="ch01.html#Select-full-join">“Select full join”</a>.&#13;
But even with an index, a table join will struggle if the index does not match a single row.<a data-primary="table join" data-startref="table-join" data-type="indexterm" id="idm45829112661104"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Working set size" data-type="sect3"><div class="sect3" id="working-set-size">&#13;
<h3>Working set size</h3>&#13;
&#13;
<p>Indexes <a data-primary="working set size" data-type="indexterm" id="idm45829112658224"/>are only useful when they’re in memory.&#13;
If the index values that a query looks up are not in memory, then MySQL reads them from disk.&#13;
(More accurately, the B-tree nodes that constitute the index are stored in 16 KB pages, and MySQL swaps pages between memory and disk as needed.)&#13;
Reading from disk is orders of magnitude slower than reading from memory, which is one problem, but the main problem is that indexes compete for memory.</p>&#13;
&#13;
<p>If memory is limited but indexes are numerous and frequently used to look up a large percentage of values (relative to the table size), then index usage can increase storage I/O as MySQL attempts to keep frequently used index values in memory.&#13;
This is possible but rare for two reasons.&#13;
First, MySQL is exceptionally good at keeping frequently used index values in memory.&#13;
Second, frequently used index values and the primary key rows to which they refer are called the <em>working set</em>, and it’s usually a small percentage of the table size.&#13;
For example, a database can be 500 GB large, but the application frequently accesses only 1 GB of data.&#13;
In light of this fact, MySQL DBAs commonly allocate memory for only 10% of total data size, usually rounded to standard memory values (64 GB, 128 GB, and so forth).&#13;
10% of 500 GB is 50 GB, so a DBA would probably err on the side of caution and round up to 64 GB of memory.&#13;
This works surpassingly well and is a good starting point.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>As a starting point, allocate memory for 10% of total data size.&#13;
The working set size is usually a small percentage of total data size.</p>&#13;
</div>&#13;
&#13;
<p>When the working set size becomes significantly larger than available memory, indexes may not help.&#13;
Instead, like a fire that burns so hot that water fuels it rather than extinguishing it, index usage puts pressure on storage I/O and everything slows down.&#13;
More memory is a quick fix, but remember <a data-type="xref" href="ch02.html#better-faster-hardware">“Better, Faster Hardware!”</a>: scaling up is not a sustainable approach.&#13;
The best solution is to address the data size and access patterns responsible for the large working set.&#13;
If the application truly needs to store and access so much data that the working set size cannot fit within a reasonable amount of memory on a single MySQL instance, then the solution is sharding, which is covered in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a>.<a data-primary="data" data-secondary="indexes may not help" data-startref="data-indexes-may-not-help-ch3" data-type="indexterm" id="idm45829112651088"/><a data-primary="indexes" data-secondary="possibility indexes may not help" data-startref="indexes-possibility-indexes-may-not-help-ch3" data-type="indexterm" id="idm45829112649872"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Less Data Is Better" data-type="sect2"><div class="sect2" id="less-data-is-better">&#13;
<h2>Less Data Is Better</h2>&#13;
&#13;
<p>Experienced <a data-primary="data" data-secondary="less data is better" data-type="indexterm" id="idm45829112646736"/>engineers don’t celebrate a huge database, they cope with it.&#13;
They celebrate when data size is dramatically reduced because less data is better.&#13;
Better for what?&#13;
Everything: performance, management, cost, and so on.&#13;
It’s simply a lot faster, easier, and cheaper to deal with 100 GB of data than 100 <em>TB</em> on a single MySQL instance.&#13;
The former is so small that a smartphone can handle it.&#13;
The latter requires specialized handling: optimizing performance is more challenging, managing the data can be risky (what’s the backup and restore time?), and good luck finding affordable hardware for 100 TB.&#13;
It’s easier to keep data size reasonable than to cope with a huge database.</p>&#13;
&#13;
<p>Any amount of data that’s legitimately required is worth the time and effort to optimize and manage.&#13;
The problem is less about data size and more about unbridled data growth.&#13;
It’s not uncommon for engineers to hoard data: storing any and all data.&#13;
If you’re thinking, “Not me. I don’t hoard data,” then wonderful.&#13;
But your colleagues may not share your laudable sense of data asceticism.&#13;
If not, raise the issue of unbridled data growth before data size becomes a problem.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Don’t let an unwieldy database catch you by surprise.&#13;
Monitor data size (see <a data-type="xref" href="ch06.html#metrics-data-size">“Data Size”</a>) and, based on the current rate of growth, estimate data size for the next four years.&#13;
If future data size is not feasible with the current hardware and application design, then address the issue now before it becomes a problem.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Less QPS Is Better" data-type="sect2"><div class="sect2" id="less-qps-is-better">&#13;
<h2>Less QPS Is Better</h2>&#13;
&#13;
<p>You may never <a data-primary="data" data-secondary="less QPS is better" data-type="indexterm" id="less-QPS-is-better"/>find another book or engineer that says <em>less</em> QPS is better.&#13;
Cherish the moment.</p>&#13;
&#13;
<p>I realize that this secret is counterintuitive, perhaps even unpopular.&#13;
To see its truth and wisdom, consider three less objectionable points about QPS:</p>&#13;
<dl>&#13;
<dt><em>QPS is only a number—a measurement of raw throughput</em></dt>&#13;
<dd>&#13;
<p>It reveals nothing qualitative about the queries or performance in general.&#13;
One application can be effectively idle at 10,000 QPS, while another is overloaded and having an outage at half that throughput.&#13;
Even at the same QPS, there are numerous qualitative differences.&#13;
Executing <code>SELECT 1</code> at 1,000 QPS requires almost zero system resources, but a complex query at the same QPS could be very taxing on all system resources.&#13;
And high QPS—no matter how high—is only as good as query response time.</p>&#13;
</dd>&#13;
</dl>&#13;
<dl class="pagebreak-before less_space">&#13;
<dt><em>QPS values have no objective meaning</em></dt>&#13;
<dd>&#13;
<p>They’re neither good nor bad, high nor low, typical nor atypical.&#13;
QPS values are only meaningful relative to an application.&#13;
If one application averages 2,000 QPS, then 100 QPS could be a precipitous drop that indicates a outage.&#13;
But if another application averages 300 QPS, then 100 QPS could be a normal fluctuation.&#13;
QPS can also be relative to external events: time of day, day of week, seasons, holidays, and so on.</p>&#13;
</dd>&#13;
<dt><em>It is difficult to increase QPS</em></dt>&#13;
<dd>&#13;
<p>By contrast, data size can increase with relative ease from 1 GB to 100 GB—a 100x increase.&#13;
But it’s incredibly difficult to increase QPS by 100x (except for extremely low values, like 1 QPS to 100 QPS).&#13;
Even a 2x increase in QPS can be very challenging to achieve.&#13;
Maximum QPS—relative to an application—is even more challenging to increase because you cannot purchase more QPS, unlike storage and memory.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In summary of these points: QPS is not qualitative, only relative to an application, and difficult to increase.&#13;
To put a point on it: <em>QPS does not help you</em>.&#13;
It’s more of a liability than an asset.&#13;
Therefore, less QPS is better.</p>&#13;
&#13;
<p>Experienced engineers celebrate when QPS is reduced (intentionally) because less QPS is more capacity for growth.<a data-primary="data" data-secondary="less QPS is better" data-startref="less-QPS-is-better" data-type="indexterm" id="idm45829112628288"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Principle of Least Data" data-type="sect1"><div class="sect1" id="principle-of-least-data">&#13;
<h1>Principle of Least Data</h1>&#13;
&#13;
<p>I define <a data-primary="data" data-secondary="principle of least data" data-type="indexterm" id="principle-of-least-data_index1"/><a data-primary="principle of least data" data-type="indexterm" id="principle-of-least-data_index2"/>the principle of least data as: <em>store and access only needed data</em>.&#13;
That sounds obvious in theory, but it’s far from the norm in practice.&#13;
It’s also deceptively simple, which is why the next two sections have many fine details.</p>&#13;
<blockquote>&#13;
<p>Common sense is not so common.</p>&#13;
<p data-type="attribution">Voltaire</p>&#13;
</blockquote>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Access" data-type="sect2"><div class="sect2" id="data-access">&#13;
<h2>Data Access</h2>&#13;
&#13;
<p>Do not access more data <a data-primary="data" data-secondary="access" data-type="indexterm" id="data-access_index1"/>than needed.&#13;
<em>Access</em> refers to all the work that MySQL does to execute a query: find matching rows, process matching rows, and return the result set—for both reads (<code>SELECT</code>) and writes.&#13;
Efficient data access is especially important for writes because it’s more difficult to scale writes.</p>&#13;
&#13;
<p><a data-type="xref" data-xrefstyle="select:nopage" href="#data-access-checklist">Table 3-2</a> is a checklist that you can apply to a query—hopefully every query—to verify its data access efficiency.</p>&#13;
<table class="pagebreak-before less_space" id="data-access-checklist">&#13;
<caption><span class="label">Table 3-2. </span>Efficient data access checklist</caption>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Return only needed columns</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Reduce query complexity</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Limit row access</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Limit the result set</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Avoid sorting rows</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>To be fair and balanced, ignoring a single checklist item is unlikely to affect performance.&#13;
For example, the fifth item—avoid sorting rows—is commonly ignored without affecting performance.&#13;
These items are best practices.&#13;
If you practice them until they become habit, you will have greater success and performance with MySQL than engineers who ignore them completely.</p>&#13;
&#13;
<p>Before I explain each item in <a data-type="xref" data-xrefstyle="select:nopage" href="#data-access-checklist">Table 3-2</a>, let’s take one paragraph to revisit an example in <a data-type="xref" href="ch01.html#ch01">Chapter 1</a> that I deferred to this chapter.</p>&#13;
&#13;
<p>Perhaps you recall this example from <a data-type="xref" href="ch01.html#query-profile">“Query profile”</a>: “As I write this, I’m looking at a query with load 5,962. How is that possible?”&#13;
That query load is possible thanks to <em>incredibly</em> efficient data access and an extremely busy application.&#13;
The query is like <code>SELECT col1, col2 WHERE pk_col = 5</code>: a primary key look up that returns only two columns from a single row.&#13;
When data access is that efficient, MySQL functions <em>almost</em> like an in-memory cache, and it executes the query at incredible QPS and query load.&#13;
<em>Almost</em>, but not entirely, because every query is a transaction that entails overhead.&#13;
(<a data-type="xref" href="ch08.html#ch08">Chapter 8</a> focuses on transactions.)&#13;
To optimize a query like this, you must change access patterns because the query cannot be optimized any further and the data size cannot be reduced.&#13;
I revisit this query one more time in <a data-type="xref" href="ch04.html#ch04">Chapter 4</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Return only needed columns" data-type="sect3"><div class="sect3" id="data-access-check-1">&#13;
<h3>Return only needed columns</h3>&#13;
&#13;
<p>Queries should return only needed columns.</p>&#13;
&#13;
<p>Do not <code>SELECT *</code>.&#13;
This is especially important if the table has any <code>BLOB</code>, <code>TEXT</code>, or <code>JSON</code> columns.</p>&#13;
&#13;
<p>You’ve probably heard this best practice before because the database industry (not just MySQL) has been harping on it for decades.&#13;
I can’t recall the last time I saw <code>SELECT *</code> in production, but it’s important enough to keep repeating.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reduce query complexity" data-type="sect3"><div class="sect3" id="data-access-check-2">&#13;
<h3>Reduce query complexity</h3>&#13;
&#13;
<p>Queries should be as simple as possible.</p>&#13;
&#13;
<p><em>Query complexity</em> refers <a data-primary="query complexity" data-type="indexterm" id="idm45829112587712"/>to all tables, conditions, and SQL clauses that constitute a query.&#13;
In this context, complexity is relative only to a query, not to engineers.&#13;
Query <code>SELECT col FROM tbl WHERE id = 1</code> is less complex than a query that joins five tables with many <code>WHERE</code> conditions.</p>&#13;
&#13;
<p>Complex queries are a problem for engineers, not MySQL.&#13;
The more complex a query, the more difficult it is to analyze and optimize.&#13;
If you’re lucky, a complex query works well and never shows up as a slow query (see <a data-type="xref" href="ch01.html#query-profile">“Query profile”</a>).&#13;
But luck is not a best practice.&#13;
Keep queries simple from the start (when first written), and reduce query complexity when possible.</p>&#13;
&#13;
<p>With respect to data access, simple queries tend to access less data because they have fewer tables, conditions, and SQL clauses—less work for MySQL.&#13;
But be careful: the wrong simplification can yield a worse EXPLAIN plan.&#13;
For example, <a data-type="xref" href="ch02.html#idx-order-by-pk-not">Figure 2-21</a> in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a> demonstrates how removing a condition negates an <code>ORDER BY</code> optimization, resulting in a (slightly) worse EXPLAIN plan.&#13;
Always confirm that a simpler query has an equivalent or better EXPLAIN plan—and the same result set.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Limit row access" data-type="sect3"><div class="sect3" id="data-access-check-3">&#13;
<h3>Limit row access</h3>&#13;
&#13;
<p>Queries should access as few rows as possible.</p>&#13;
&#13;
<p>Accessing too many rows usually comes as a surprise; it’s not something engineers do intentionally.&#13;
Data growth over time is a common cause: a fast query starts by accessing a few rows, but years and gigabytes later, it becomes a slow query because it accesses too many rows.&#13;
Simple mistakes are another cause: an engineer writes a query that they think will access a few rows, but they’re wrong.&#13;
At the intersection of data growth and simple mistakes is the most important cause: <em>not limiting ranges and lists</em>.&#13;
An open-ended range like <code>col &gt; 75</code> can access countless rows if MySQL does a range scan on <code>col</code>.&#13;
Even if this is intended because the table is presumed to be small, be aware that row access is virtually unbounded as the table grows, especially if the index on <code>col</code> is nonunique.</p>&#13;
&#13;
<p>A <code>LIMIT</code> clause <a data-primary="LIMIT clause" data-type="indexterm" id="LIMIT-clause"/>does not limit row access because <code>LIMIT</code> applies to the result set <em>after</em> matching rows.&#13;
The exception is the <code>ORDER BY</code>…<code>LIMIT</code> optimization: if MySQL can access rows in index order, then it stops reading rows when the <code>LIMIT</code> number of matching rows are found.&#13;
But here’s the fun part: <code>EXPLAIN</code> does not report when this optimization is used.&#13;
You must infer the optimization from what an EXPLAIN does and does not report.&#13;
Let’s take a moment to see this optimization in action and prove that it limits row access.</p>&#13;
&#13;
<p>Using table <code>elem</code> (<a data-type="xref" href="ch02.html#elem">Example 2-1</a>) from <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>, let’s first execute a query that does not have a <code>LIMIT</code> clause.&#13;
<a data-type="xref" href="#order-by-limit-all-rows">Example 3-1</a> shows that the query returns eight rows.</p>&#13;
<div data-type="example" id="order-by-limit-all-rows">&#13;
<h5><span class="label">Example 3-1. </span>Rows for query without <code>LIMIT</code></h5>&#13;
&#13;
<pre data-type="programlisting">SELECT * FROM elem WHERE a &gt; 'Ag' ORDER BY a;&#13;
&#13;
+----+----+----+----+&#13;
| id | a  | b  | c  |&#13;
+----+----+----+----+&#13;
|  8 | Al | B  | Cd |&#13;
|  9 | Al | B  | Cd |&#13;
|  3 | Al | Br | Cr |&#13;
| 10 | Ar | B  | Cd |&#13;
|  4 | Ar | Br | Cd |&#13;
|  5 | Ar | Br | C  |&#13;
|  7 | At | Bi | Ce |&#13;
|  2 | Au | Be | Co |&#13;
+----+----+----+----+&#13;
8 rows in set (0.00 sec)</pre></div>&#13;
&#13;
<p>Without a <code>LIMIT</code> clause, the query accesses (and returns) eight rows.&#13;
Accordingly, <code>EXPLAIN</code> reports <code>rows: 8</code> even with a <code>LIMIT 2</code> clause—as shown in <a data-type="xref" href="#order-by-limit-explain">Example 3-2</a>—because MySQL cannot know how many rows in the range will <em>not</em> match until it executes the query.&#13;
Worst case: MySQL reads all rows because none match.&#13;
But for this simple example, we can see that the first two rows (<code>id</code> values 8 and 9) will match the only table condition.&#13;
If we’re right, query metrics will report two rows examined, not eight.&#13;
But first, let’s see how to infer the optimization from the EXPLAIN plan in <a data-type="xref" href="#order-by-limit-explain">Example 3-2</a>.</p>&#13;
<div data-type="example" id="order-by-limit-explain">&#13;
<h5><span class="label">Example 3-2. </span>EXPLAIN plan for <code>ORDER BY</code>…<code>LIMIT</code> optimization</h5>&#13;
&#13;
<pre data-type="programlisting">EXPLAIN SELECT * FROM elem WHERE a &gt; 'Ag' ORDER BY a LIMIT 2\G&#13;
&#13;
*************************** 1. row ***************************&#13;
           id: 1&#13;
  select_type: SIMPLE&#13;
        table: elem&#13;
   partitions: NULL&#13;
         type: range&#13;
possible_keys: a&#13;
          key: a&#13;
      key_len: 8&#13;
          ref: NULL&#13;
         rows: 8&#13;
     filtered: 100.00&#13;
        Extra: Using index condition</pre></div>&#13;
&#13;
<p>You can infer that MySQL uses the <code>ORDER BY</code>…<code>LIMIT</code> optimization to access only two rows (<code>LIMIT 2</code>) because:</p>&#13;
&#13;
<ul class="pagebreak-before less_space">&#13;
<li>&#13;
<p>The query uses an index (<code>type: range</code>)</p>&#13;
</li>&#13;
<li>&#13;
<p>The <code>ORDER BY</code> column is a leftmost prefix of that index (<code>key: a</code>)</p>&#13;
</li>&#13;
<li>&#13;
<p>The <code>Extra</code> field does <em>not</em> report “Using filesort”</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The proof is shown in <a data-type="xref" href="#order-by-limit-slowlog">Example 3-3</a>: a snippet of the slow query log after MySQL executed the query.</p>&#13;
<div data-type="example" id="order-by-limit-slowlog">&#13;
<h5><span class="label">Example 3-3. </span>Query metrics for <code>ORDER BY</code>…<code>LIMIT</code> optimization</h5>&#13;
&#13;
<pre data-type="programlisting"># Query_time: 0.000273  Lock_time: 0.000114  Rows_sent: 2  Rows_examined: 2&#13;
SELECT * FROM elem WHERE a &gt; 'Ag' ORDER BY a LIMIT 2;</pre></div>&#13;
&#13;
<p><code>Rows_examined: 2</code> at the end of the first line in <a data-type="xref" href="#order-by-limit-slowlog">Example 3-3</a> proves that MySQL used the <code>ORDER BY</code>…<code>LIMIT</code> optimization to access only two rows instead of all eight rows.&#13;
To learn more about this query optimization, read <a href="https://oreil.ly/AnurD">“LIMIT Query Optimization”</a> in the MySQL manual.</p>&#13;
&#13;
<p>With respect to limiting ranges and lists, there’s an important factor to verify: <em>does the application limit the input used in a query?</em>&#13;
Way back in <a data-type="xref" href="ch01.html#avg-p-max-distro">“Average, Percentile, and Maximum”</a>, I related a story: “Long story short, the query was used to look up data for fraud detection, and occasionally a big case would look up several thousand rows at once, which caused MySQL to switch query execution plans.”&#13;
In that case, the solution was simple: limit application input to one thousand values per request.&#13;
That case also highlights the fact that a human can input a flood of values.&#13;
Normally, engineers are careful to limit input when the user is another computer, but their caution relaxes when the user is another human because they think a human wouldn’t or couldn’t input too many values.&#13;
But they’re wrong: with copy-paste and a looming deadline, the average human can overload any computer.<a data-primary="LIMIT clause" data-startref="LIMIT-clause" data-type="indexterm" id="idm45829112538288"/></p>&#13;
&#13;
<p>For writes, limiting row access is critical because, generally speaking, InnoDB locks every row that it accesses before it updates matching rows.&#13;
Consequently, InnoDB can lock more rows than you might expect.&#13;
<a data-type="xref" href="ch08.html#row-locking">“Row Locking”</a> goes into detail.</p>&#13;
&#13;
<p>For table joins, limiting row access is also critical: recall from <a data-type="xref" href="#less-data-join">“Joining tables”</a> that, on join, a few rows in each table quickly obliterates performance.&#13;
In that section, I was pointing out that a table join is doomed without an index lookup.&#13;
In this section, I’m pointing out that a table join <a data-primary="table join" data-type="indexterm" id="idm45829112534336"/>is double-doomed unless it also accesses <em>very</em> few rows.&#13;
Remember: an index lookup on a nonunique index can access any number of duplicate rows.</p>&#13;
&#13;
<p>Know your access patterns: for each query, what limits row access?&#13;
Use <code>EXPLAIN</code> to see estimated row access (the <code>rows</code> field), and monitor rows examined (see <a data-type="xref" href="ch01.html#Rows-examined">“Rows examined”</a>) to avoid the surprise of accessing too many rows.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Limit the result set" data-type="sect3"><div class="sect3" id="data-access-check-4">&#13;
<h3>Limit the result set</h3>&#13;
&#13;
<p>Queries should return as few rows as possible.</p>&#13;
&#13;
<p>This is more involved than putting a <code>LIMIT</code> clause on a query, although that certainly helps.&#13;
It refers to the application not using the entire <em>result set</em>: the rows returned by a query.&#13;
This problem has three variations.</p>&#13;
&#13;
<p>The first variation occurs when the application uses some rows, but not all.&#13;
This can be done intentionally or unintentionally.&#13;
Unintentionally, it indicates that the <code>WHERE</code> clause needs better (or more) conditions to match only needed rows.&#13;
You can spot this in application code that filters rows instead of using <code>WHERE</code> conditions.&#13;
If you spot this, talk with your team to make sure it’s not intentional.&#13;
Intentionally, an application might select more rows to avoid a complex query by shifting row matching from MySQL to the application.&#13;
This technique is useful only when it reduces response time—akin to MySQL choosing a table scan in rare cases.</p>&#13;
&#13;
<p>The second variation occurs when a query has an <code>ORDER BY</code> clause and the application uses an ordered subset of rows.&#13;
Row order doesn’t matter for the first variation, but it’s the defining characteristic of the second variation.&#13;
For example, a query returns 1,000 rows but the application only uses the first 20 rows in order.&#13;
In this case, the solution might be as simple as adding a <code>LIMIT 20</code> clause to the query.</p>&#13;
&#13;
<p>What does the application do with the remaining 980 rows?&#13;
If those rows are never used, then definitely the query should not return them—add the <code>LIMIT 20</code> clause.&#13;
But if those rows are used, then the application is most likely paginating: using 20 rows at a time (for example, showing 20 results per page).&#13;
In that case, it might be faster and more efficient to use <code>LIMIT 20 OFFSET N</code> to fetch pages on demand—where N = 20 × (page number – 1)—only if the <code>ORDER BY</code>…<code>LIMIT</code> optimization can be used (see the previous section, <a data-type="xref" href="#data-access-check-3">“Limit row access”</a>).&#13;
The optimization is required because, without it, MySQL must find and sort all matching rows before it can apply the <code>OFFSET</code> part of the <code>LIMIT</code> clause—a lot of wasted work to return only 20 rows.&#13;
But even without the optimization, there’s another solution: a large but reasonable <code>LIMIT</code> clause.&#13;
If, for example, you measure application usage and find that most requests only use the first five pages, then use a <code>LIMIT 100</code> clause to fetch the first five pages and reduce the result set size by 90% for most requests.</p>&#13;
&#13;
<p>The third variation occurs when the application <em>only</em> aggregates the result set.&#13;
If the application aggregates the result set <em>and</em> uses the individual rows, that’s acceptable.&#13;
The antipattern is <em>only</em> aggregating the result set instead of using a SQL aggregate function, which limits the result set.&#13;
<a data-type="xref" href="#result-set-anti-patterns">Table 3-3</a> lists four antipatterns and corresponding SQL solutions.</p>&#13;
<table id="result-set-anti-patterns">&#13;
<caption><span class="label">Table 3-3. </span>Four result set antipatterns in an application</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Antipattern in Application</th>&#13;
<th>Solution in SQL</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Adding a column value</p></td>&#13;
<td><p><code>SUM(column)</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Counting the number of rows</p></td>&#13;
<td><p><code>COUNT(*)</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Counting the number of values</p></td>&#13;
<td><p><code>COUNT(column)</code>…<code>GROUP BY column</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Counting the number of distinct values</p></td>&#13;
<td><p><code>COUNT(DISTINCT column)</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Extracting distinct values</p></td>&#13;
<td><p><code>DISTINCT</code></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Adding a column value applies to other statistical functions: <code>AVG()</code>, <code>MAX()</code>, <code>MIN()</code>, and so on.&#13;
Let MySQL do the calculation rather than returning the rows.</p>&#13;
&#13;
<p>Counting the number of rows is an extreme antipattern, but I’ve seen it, so I’m sure there are other applications quietly wasting network bandwidth on needless rows.&#13;
Never use the application only to count rows; use <code>COUNT(*)</code> in the query.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>As of MySQL 8.0.14, <code>SELECT COUNT(*) FROM table</code> (without a <code>WHERE</code> clause) uses multiple threads to read the primary key in parallel.&#13;
This is not parallel query execution; the MySQL manual calls it “parallel clustered index reads.”</p>&#13;
</div>&#13;
&#13;
<p>Counting the number of values is, perhaps, easier for programmers to express in code than a SQL <code>GROUP BY</code> clause, but the latter should be used to limit the result set.&#13;
Using table <code>elem</code> (<a data-type="xref" href="ch02.html#elem">Example 2-1</a>) again, <a data-type="xref" href="#count-distinct-values">Example 3-4</a> demonstrates how to count the number of values for a column using <code>COUNT(column)</code>…<code>GROUP BY column</code>.</p>&#13;
<div data-type="example" id="count-distinct-values">&#13;
<h5><span class="label">Example 3-4. </span>Counting the number of values</h5>&#13;
&#13;
<pre data-type="programlisting">SELECT a, COUNT(a) FROM elem GROUP BY a;&#13;
&#13;
+----+----------+&#13;
| a  | COUNT(a) |&#13;
+----+----------+&#13;
| Ag |        2 |&#13;
| Al |        3 |&#13;
| Ar |        3 |&#13;
| At |        1 |&#13;
| Au |        1 |&#13;
+----+----------+</pre></div>&#13;
&#13;
<p>For column <code>a</code> in table <code>elem</code>, two rows have value “Ag,” three rows have value “Al,” and so forth.&#13;
The SQL solution returns five rows, whereas the antipattern would return all ten rows.&#13;
These aren’t dramatic numbers—five versus ten rows—but they make the point: a query can limit its result set by aggregating in SQL, not application code.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45829112488960">&#13;
<h5>COUNT(*) Versus COUNT(column)</h5>&#13;
<p><code>COUNT(*)</code> counts the number of matching rows—the result set size.&#13;
<code>COUNT(column)</code> counts the number of non-NULL values in the column of the matching rows.&#13;
When <code>COUNT(column)</code> is used with other columns (including itself), you need a <code>GROUP BY</code> clause for proper aggregation, as shown in <a data-type="xref" href="#count-distinct-values">Example 3-4</a>.</p>&#13;
</div></aside>&#13;
&#13;
<p>Extracting distinct values—deduplicating column values—is trivial in the application with an associative array; but MySQL can do it, too, with <code>DISTINCT</code>, which limits the result set.&#13;
(<code>DISTINCT</code> qualifies as an aggregate function because it’s a special case of <code>GROUP BY</code>.)&#13;
<code>DISTINCT</code> is especially clear and useful with a single column.&#13;
For example, <code>SELECT DISTINCT a FROM elem</code> returns a list of unique values from column <code>a</code>.&#13;
(If you’re curious, column <code>a</code> has five unique values: “Ag,” “Al,” “Ar,” “At,” and “Au.”)&#13;
The gotcha with <code>DISTINCT</code> is that it applies to all columns.&#13;
<code>SELECT DISTINCT a, b FROM elem</code> returns a list of unique <em>rows</em> with values from columns <code>a</code> and <code>b</code>.&#13;
To learn more, check out <a href="https://oreil.ly/j3IjK">“DISTINCT Optimization”</a> in the MySQL manual.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Avoid sorting rows" data-type="sect3"><div class="sect3" id="data-access-check-5">&#13;
<h3>Avoid sorting rows</h3>&#13;
&#13;
<p>Queries should avoid sorting rows.</p>&#13;
&#13;
<p>Sorting rows in the application instead of MySQL reduces query complexity by removing the <code>ORDER BY</code> clause, and it scales better by distributing work to application instances, which are much easier to scale out than MySQL.</p>&#13;
&#13;
<p>An <code>ORDER BY</code> clause without a <code>LIMIT</code> clause is a telltale sign that the <code>ORDER BY</code> clause can be dropped and the application can sort the rows.&#13;
(It might also be the second variation of the problem discussed in the preceding section.)&#13;
Look for queries with an <code>ORDER BY</code> clause but no <code>LIMIT</code> clause, then determine whether the application can sort the rows instead of MySQL—the answer should be yes.<a data-primary="data" data-secondary="access" data-startref="data-access_index1" data-type="indexterm" id="idm45829112471824"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Storage" data-type="sect2"><div class="sect2" id="data-storage">&#13;
<h2>Data Storage</h2>&#13;
&#13;
<p>Do not store more data <a data-primary="data" data-secondary="storage" data-type="indexterm" id="data-storage_index1"/>than needed.</p>&#13;
&#13;
<p>Although data is valuable to you, it’s dead weight <a data-primary="dead weight" data-type="indexterm" id="idm45829112466912"/>to MySQL.&#13;
<a data-type="xref" data-xrefstyle="select:nopage" href="#data-storage-checklist">Table 3-4</a> is a checklist for efficient data storage.</p>&#13;
&#13;
<p>I highly encourage you to audit your data storage because surprises are easy to discover.&#13;
I mentioned one such surprise at the beginning of <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>: the application I created that accidentally stored one <em>billion</em> rows.</p>&#13;
<table class="pagebreak-before less_space" id="data-storage-checklist">&#13;
<caption><span class="label">Table 3-4. </span>Efficient data storage checklist</caption>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Only needed rows are stored</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Every column is used</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Every column is compact and practical</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Every value is compact and practical</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Every secondary index is used and not a duplicate</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>☐</p></td>&#13;
<td><p>Only needed rows are kept</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>If you can check off all six items, then you will be very well positioned to scale data to any size.&#13;
But it’s not easy: some items are easier to ignore than to implement, especially when the database is small.&#13;
But don’t delay: the very best time to find and correct storage inefficiencies is when the database is small.&#13;
At scale, a byte or two can make a big difference when multiplied by high throughput and all 86,400 seconds in a typical Earth day.&#13;
Design for scale and plan for success.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Only needed rows are stored" data-type="sect3"><div class="sect3" id="data-checklist-1">&#13;
<h3>Only needed rows are stored</h3>&#13;
&#13;
<p>As an application changes and grows, engineers can lose track of what it stores.&#13;
And when data storage is not an issue, engineers have no reason to look at or ask about what it stores.&#13;
If it’s been a long time since you or anyone else reviewed what the application is storing, or if you’re new to the team or application, then take a look.&#13;
I have seen, for example, forgotten services writing data (for years, no less) that no one was using.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Every column is used" data-type="sect3"><div class="sect3" id="data-checklist-2">&#13;
<h3>Every column is used</h3>&#13;
&#13;
<p>One level deeper than storing only needed rows is having only needed columns.&#13;
Again, as the application changes and grows, engineers can lose track of columns, especially when using object-relational mapping (ORM).</p>&#13;
&#13;
<p>Unfortunately, there’s no tool or automated way to find unused columns in MySQL.&#13;
MySQL tracks which databases, tables, and indexes are used, but it does not track column usage.&#13;
Nothing is more furtive than an unused column.&#13;
The only solution is a manual review: compare columns used by application queries to columns that exist in the tables.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Every column is compact and practical" data-type="sect3"><div class="sect3" id="data-checklist-3">&#13;
<h3>Every column is compact and practical</h3>&#13;
&#13;
<p>Two levels deeper than storing only needed rows is having every <a data-primary="compact column" data-type="indexterm" id="compact-column"/><a data-primary="practical column" data-type="indexterm" id="practical-column"/>column be compact and practical.&#13;
<em>Compact</em> means using the smallest data type to store values.&#13;
<em>Practical</em> means not using a data type so small that it’s onerous or error-prone for you or the application.&#13;
For example, using an unsigned <code>INT</code> as a bit field is compact (nothing smaller than a bit) but usually not practical.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Familiarize yourself with all the <a href="https://oreil.ly/x7fTF">MySQL data types</a>.</p>&#13;
</div>&#13;
&#13;
<p>The classic antipattern is data type <code>VARCHAR(255)</code>.&#13;
This specific data type and size are a common but inefficient default for many programs and engineers, who likely copied the practice from another program or engineer.&#13;
You will see it used to store anything and everything, which is why it’s inefficient.</p>&#13;
&#13;
<p>For example, let’s reuse table <code>elem</code> (<a data-type="xref" href="ch02.html#elem">Example 2-1</a>).&#13;
Atomic symbols are one or two characters.&#13;
Column definition <code>atomic_symbol VARCHAR(255)</code> is technically compact—a <code>VARCHAR</code> is variable length, so it would use only one or two characters—but it allows <em>garbage in, garbage out</em>: invalid values like “Carbon” instead of “C,” which could have unknown consequences for the application.&#13;
A better column definition is <code>atomic_symbol CHAR(2)</code>, which is compact and practical.</p>&#13;
&#13;
<p>Is column definition <code>atomic_symbol ENUM(</code>…<code>)</code> even better for table <code>elem</code>?&#13;
<code>ENUM</code> is more compact than <code>CHAR(2)</code>, but is it more practical with over one hundred atomic symbols?&#13;
That’s a trade-off you could decide; either choice is patently better than <code>VAR⁠CHAR(255)</code>.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p><a href="https://oreil.ly/WMXfA"><code>ENUM</code></a> is one of the great unsung heroes of efficient data storage.</p>&#13;
</div>&#13;
&#13;
<p>Beware the column character set.&#13;
If not explicitly defined, it defaults to the table character set which, if also not explicitly defined, defaults to the server character set.&#13;
As of MySQL 8.0, the default server character set is <code>utf8mb4</code>.&#13;
For MySQL 5.7 and older, the default server character set is <code>latin1</code>.&#13;
Depending on the character set, a single character like <em>é</em> might be stored as multiple bytes.&#13;
For example, using the <code>latin1</code> character set, MySQL stores <em>é</em> as a single byte: 0xE9.&#13;
But using the <code>utf8mb4</code> character set, MySQL stores <em>é</em> as two bytes: 0xC3A9.&#13;
(Emoji use four bytes per character.)&#13;
Character sets are a special and erudite world beyond the scope of most books.&#13;
For now, all you need to know is this: <em>one character</em> can require <em>several bytes</em> of storage, depending on the character and character set.&#13;
Bytes add up quickly in large tables.</p>&#13;
&#13;
<p>Be very conservative with <code>BLOB</code>, <code>TEXT</code>, and <code>JSON</code> data types.&#13;
Do not use them as a dumping ground, a catch-all, or generic buckets.&#13;
For example, do not store images in a <code>BLOB</code>—you can, it works, but don’t.&#13;
There are far better solutions, like <a href="https://aws.amazon.com/s3">Amazon S3</a>.</p>&#13;
&#13;
<p>Compact and practical extend all the way down to the bit level.&#13;
Another surprisingly common yet easily avoidable column storage inefficiency is wasting the high-order bit of <a href="https://oreil.ly/6CdwC">integer data types</a>.&#13;
For example, using <code>INT</code> instead of <code>INT UNSIGNED</code>: the maximum value is roughly two billion versus four billion, respectively.&#13;
If the value cannot be negative, then use an unsigned data type.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>As of MySQL 8.0.17, <code>UNSIGNED</code> is deprecated for data types <code>FLOAT</code>, <code>DOUBLE</code>, and <code>DECIMAL</code>.</p>&#13;
</div>&#13;
&#13;
<p>In the world of software engineering, details like these might be considered micro-optimizations or premature optimization, which are frowned upon, but in the world of schema design and database performance, they’re best practices.<a data-primary="compact column" data-startref="compact-column" data-type="indexterm" id="idm45829112413232"/><a data-primary="practical column" data-startref="practical-column" data-type="indexterm" id="idm45829112412256"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Every value is compact and practical" data-type="sect3"><div class="sect3" id="data-checklist-4">&#13;
<h3>Every value is compact and practical</h3>&#13;
&#13;
<p>Three levels deeper than storing only needed rows is having every value be compact and practical.<a data-primary="practical value" data-type="indexterm" id="practical-value"/><a data-primary="compact value" data-type="indexterm" id="compact-value"/>&#13;
<em>Practical</em> has the same meaning as defined in the previous section, but <em>compact</em> means the smallest representation of the value.&#13;
Compact values are highly dependent on how the application uses them.&#13;
For example, consider a string with one leading and one trailing space: <code>“ and ”</code>.&#13;
<a data-type="xref" href="#compact-string">Table 3-5</a> lists six ways that an application could compact this string.</p>&#13;
<table id="compact-string">&#13;
<caption><span class="label">Table 3-5. </span>Six ways to compact the string <code>“ and ”</code></caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Compact value</th>&#13;
<th>Possible use</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p><code>“and”</code></p></td>&#13;
<td><p>Strip all whitespace. This is common for strings.</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>“ and”</code></p></td>&#13;
<td><p>Strip trailing whitespace. In many syntaxes (like YAML and Markdown), leading whitespace is syntactically significant.</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>“and ”</code></p></td>&#13;
<td><p>Strip leading whitespace. Perhaps less common but still possible. Sometimes used by programs to join space-separated arguments (like command-line arguments).</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>“”</code></p></td>&#13;
<td><p>Delete the value (empty string). Maybe the value is optional, like <em>AS</em> in <code>FROM table AS table_alias</code>, which can be written as <code>FROM table table_alias</code>.</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>“&amp;”</code></p></td>&#13;
<td><p>Replace string with equivalent symbol. In written language, the ampersand character is semantically equivalent to the word “and”.</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>NULL</code></p></td>&#13;
<td><p>No value. Maybe the value is completely superfluous and can be removed, resulting in no value (not even an empty string, which is still technically a value).</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The transformations in <a data-type="xref" href="#compact-string">Table 3-5</a> represent three ways to compact a value: minimize, encode, and deduplicate.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Minimize" data-type="sect4"><div class="sect4" id="idm45829112387120">&#13;
<h4>Minimize</h4>&#13;
&#13;
<p>To minimize a <a data-primary="minimized value" data-type="indexterm" id="minimized-value"/>value, remove superfluous and extraneous data: white space, comments, headers, and so on.&#13;
Let’s consider a more difficult yet familiar value in <a data-type="xref" href="#sample-sql">Example 3-5</a>.</p>&#13;
<div data-type="example" id="sample-sql">&#13;
<h5><span class="label">Example 3-5. </span>Formatted SQL statement (not minimized)</h5>&#13;
&#13;
<pre data-code-language="sql" data-type="programlisting"><code class="k">SELECT</code>&#13;
  <code class="cm">/*!40001 SQL_NO_CACHE */</code>&#13;
  <code class="n">col1</code><code class="p">,</code>&#13;
  <code class="n">col2</code>&#13;
<code class="k">FROM</code>&#13;
  <code class="n">tbl1</code>&#13;
<code class="k">WHERE</code>&#13;
  <code class="cm">/* comment 1 */</code>&#13;
  <code class="n">foo</code> <code class="o">=</code> <code class="s1">' bar '</code>&#13;
<code class="k">ORDER</code> <code class="k">BY</code> <code class="n">col1</code>&#13;
<code class="k">LIMIT</code> <code class="mi">1</code><code class="p">;</code> <code class="err">—</code> <code class="k">comment</code> <code class="mi">2</code></pre></div>&#13;
&#13;
<p>If an application stores only the the functional parts of the SQL statement in <a data-type="xref" href="#sample-sql">Example 3-5</a>, then it can minimize the value by collapsing white space between keywords (not within values) and removing the last two comments (not the first).&#13;
<a data-type="xref" href="#sample-sql-mini">Example 3-6</a> is the minimized (compact) value.</p>&#13;
<div data-type="example" id="sample-sql-mini">&#13;
<h5><span class="label">Example 3-6. </span>Minimized SQL statement</h5>&#13;
&#13;
<pre data-code-language="sql" data-type="programlisting"><code class="k">SELECT</code> <code class="cm">/*!40001 SQL_NO_CACHE */</code> <code class="n">col1</code><code class="p">,</code> <code class="n">col2</code> <code class="k">FROM</code> <code class="n">tbl1</code> <code class="k">WHERE</code> <code class="n">foo</code><code class="o">=</code><code class="s1">' bar '</code> <code class="k">LIMIT</code> <code class="mi">1</code></pre></div>&#13;
&#13;
<p>Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#sample-sql">3-5</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#sample-sql-mini">3-6</a> are functionally equivalent (same EXPLAIN plan), but the data size of the minimized value is almost <em>50% smaller</em> (48.9%): 137 bytes to 70 bytes, respectively.&#13;
For long-term data growth, a 50% reduction—or even just 25%—is significant and impactful.</p>&#13;
&#13;
<p>Minimizing a SQL statement illustrates an important point: minimizing a value is not always trivial.&#13;
A SQL statement isn’t a meaningless string: it’s a syntax that requires syntactical awareness to minimize correctly.&#13;
The first comment cannot be removed because it’s functional.&#13;
(See <a href="https://oreil.ly/3l8zy">“Comments”</a> in the MySQL manual.)&#13;
Likewise, the white space in the quoted value <code>' bar '</code> is functional: <code>' bar '</code> is not equal to <code>'bar'</code>.&#13;
And you might have noticed a tiny detail: the trailing semicolon was removed because it’s not functional in this context, but it is functional in other contexts.</p>&#13;
&#13;
<p>When considering how to minimize a value, begin with its data format.&#13;
The syntax and semantics of the data format dictate which data is superfluous and extraneous.&#13;
In YAML, for example, comments <code># like this</code> are pure comments (unlike certain SQL comments) and can be removed if the application doesn’t need them.&#13;
Even if your data format is custom-built, it must have some syntax and semantics, else the application could not programmatically read and write it.&#13;
It’s necessary to know the data format to minimize a value correctly.</p>&#13;
&#13;
<p>The most minimal value is no value at all: <code>NULL</code>.&#13;
I know that dealing with <code>NULL</code> can be a challenge, but there’s an elegant solution that I highly encourage you to use: <a href="https://oreil.ly/muYZW"><code>COALESCE()</code></a>.&#13;
For example, if column <code>middle_name</code> is nullable (not all people have middle names), then use <code>COALESCE(middle_name, '')</code> to return the value if set, else return an empty string.&#13;
This way, you get the benefits of <code>NULL</code> storage—which requires only one bit—without the hassle of handling null strings (or pointers) in the application.&#13;
Use <code>NULL</code> instead of empty strings, zero values, and magical values when practical.&#13;
It requires a little extra work, but it’s the best practice.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p><code>NULL</code> and <code>NULL</code> are unique; that is, two null values are unique.&#13;
Avoid unique indexes on nullable columns, or be certain that the application properly handles duplicate rows with <code>NULL</code> values.</p>&#13;
</div>&#13;
&#13;
<p>If you really want to avoid using <code>NULL</code>, the previous warning is your technical reason.&#13;
These two sets of values are <em>unique</em>: <code>(1, NULL)</code> and <code>(1, NULL)</code>.&#13;
That is not a typo.&#13;
To humans, those values look identical, but to MySQL they are unique because the comparison of <code>NULL</code> to <code>NULL</code> is undefined.&#13;
Check out <a href="https://oreil.ly/oyTPZ">“Working with NULL Values”</a> in the MySQL manual.&#13;
It begins with a humble admission: “The <code>NULL</code> value can be surprising until you get used to it.”<a data-primary="minimized value" data-startref="minimized-value" data-type="indexterm" id="idm45829112274144"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Encode" data-type="sect4"><div class="sect4" id="idm45829112386528">&#13;
<h4>Encode</h4>&#13;
&#13;
<p>To encode a <a data-primary="encoded value" data-type="indexterm" id="encoded-value"/>value, convert it from human-readable to machine-encoded.&#13;
Data can be encoded and stored one way for computers, and decoded and displayed another way for humans.&#13;
The most efficient way to store data on a computer is to encode it for the computer.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Store for the machine, display for the human.</p>&#13;
</div>&#13;
&#13;
<p>The quintessential example and antipattern is storing an IP address as a string.&#13;
For example, storing <code>127.0.0.1</code> as a string in a <code>CHAR(15)</code> column.&#13;
IP addresses are four-byte unsigned integers—that’s the true machine encoding.&#13;
(If you’re curious, <code>127.0.0.1</code> is decimal value 2130706433.)&#13;
To encode and store IP addresses, use data type <code>INT UNSIGNED</code> and functions <code>INET_ATON()</code> and <code>INET_NTOA()</code> to convert to and from a string, respectively.&#13;
If encoding IP addresses is impractical, then data type <code>CHAR(15)</code> is an acceptable alternative.</p>&#13;
&#13;
<p>Another similar example and antipattern is storing a UUID as a string.&#13;
A UUID is a multibyte integer represented as a string.&#13;
Since UUID byte lengths vary, you need to use data type <code>BINARY(N)</code>, where <code>N</code> is the byte length, and functions <code>HEX()</code> and <code>UNHEX()</code> to convert the value.&#13;
Or, if you’re using MySQL 8.0 (or newer) and RFC 4122 UUIDs (which MySQL <code>UUID()</code> generates), you can use functions <code>UUID_TO_BIN()</code> and <code>BIN_TO_UUID()</code>.&#13;
If encoding UUIDs is impractical, at least store the string representation using data type <code>CHAR(N)</code>, where <code>N</code> is the string length in characters.</p>&#13;
&#13;
<p>There is a more compact, computer-encoded method to store data: compression.&#13;
But this is an extreme method that creeps into the gray zone of space-speed trade-offs, which are beyond the scope of this book.&#13;
I have not seen a case where compression was required for performance or scale.&#13;
A rigorous application of the efficient data storage checklist (<a data-type="xref" data-xrefstyle="select:nopage" href="#data-storage-checklist">Table 3-4</a>) scales data to sizes so large that other problems become blockers: backup and restore time, online schema changes, and so forth.&#13;
If you think you need compression to scale performance, consult with an expert to verify.</p>&#13;
&#13;
<p>While we’re on the topic of encoding, there’s an important best practice that I’ll shoehorn into this section: store and access dates and times only as UTC.&#13;
Convert dates and times to local time (or whatever time zone is appropriate) only on display (or on print).&#13;
Also be aware that the MySQL <code>TIMESTAMP</code> data type ends on January 19, 2038.&#13;
If you received this book as a holiday gift in December 2037 and your databases have <code>TIMESTAMP</code> columns, you might want to go back to work a little earlier.<a data-primary="encoded value" data-startref="encoded-value" data-type="indexterm" id="idm45829112256512"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deduplicate" data-type="sect4"><div class="sect4" id="idm45829112255408">&#13;
<h4>Deduplicate</h4>&#13;
&#13;
<p>To deduplicate <a data-primary="deduplicated value" data-type="indexterm" id="deduplicated-value"/>a value, normalize the column into another table with a one-to-one relationship.&#13;
This method is entirely application-specific, so let’s consider a concrete example.&#13;
Imagine an overly simple catalogue of books stored in a table with only two columns: <code>title</code> and <code>genre</code>.&#13;
(Let’s focus on the data and ignore the details like data types and indexes.)&#13;
<a data-type="xref" href="#books-dupe">Example 3-7</a> shows a table with five books and three unique genres.</p>&#13;
<div data-type="example" id="books-dupe">&#13;
<h5><span class="label">Example 3-7. </span>Book catalogue with duplicate <code>genre</code> values</h5>&#13;
&#13;
<pre data-type="programlisting">+--------------------------------+-----------+&#13;
| title                          | genre     |&#13;
+--------------------------------+-----------+&#13;
| Efficient MySQL Performance    | computers |&#13;
| TCP/IP Illustrated             | computers |&#13;
| The C Programming Language     | computers |&#13;
| Illuminations                  | poetry    |&#13;
| A Little History of the World  | history   |&#13;
+--------------------------------+-----------+</pre></div>&#13;
&#13;
<p>Column <code>genre</code> has duplicate values: three instances of value <code>computers</code>.&#13;
To deduplicate, normalize the column into another table with a one-to-one-relationship.&#13;
<a data-type="xref" href="#books-dedupe">Example 3-8</a> shows the new table at top and the modified original table at bottom.&#13;
The two tables have a one-to-one relationship on column <code>genre_id</code>.</p>&#13;
<div data-type="example" id="books-dedupe">&#13;
<h5><span class="label">Example 3-8. </span>Normalized book catalogue</h5>&#13;
&#13;
<pre data-type="programlisting">+----------+-----------+&#13;
| genre_id | genre     |&#13;
+----------+-----------+&#13;
|        1 | computers |&#13;
|        2 | poetry    |&#13;
|        3 | history   |&#13;
+----------+-----------+&#13;
&#13;
+--------------------------------+-----------+&#13;
| title                          | genre_id  |&#13;
+--------------------------------+-----------+&#13;
| Efficient MySQL Performance    | 1         |&#13;
| TCP/IP Illustrated             | 1         |&#13;
| The C Programming Language     | 1         |&#13;
| Illuminations                  | 2         |&#13;
| A Little History of the World  | 3         |&#13;
+--------------------------------+-----------+</pre></div>&#13;
&#13;
<p>The original table (at bottom) still has duplicate values for column <code>genre_id</code>, but the reduction in data size at scale is huge.&#13;
For example, it takes 9 bytes to store the string “computers” but only 2 bytes to store the integer 1 as data type <code>SMALLINT UNSIGNED</code>, which allows for 65,536 unique genres (probably enough).&#13;
That’s a 77.7% reduction in data size: 9 bytes to 2 bytes.</p>&#13;
&#13;
<p>Deduplicating values in this way is accomplished by <em>database normalization</em>: separating data into tables based on logical relationships (one to one, one to many, and so forth).&#13;
However, deduplicating values data is <em>not</em> the goal or purpose of database normalization.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Database normalization is beyond the scope of this book, so I won’t explain it further.&#13;
There are many books on the subject, so you won’t have any trouble finding a great one to learn about database normalization.</p>&#13;
</div>&#13;
&#13;
<p>From this example, it looks like database normalization <em>causes</em> deduplication of values, but that’s not strictly true.&#13;
The single table in <a data-type="xref" href="#books-dupe">Example 3-7</a> is technically valid first, second, and third normal forms (presuming there’s a primary key)—fully normalized, just poorly designed.&#13;
It’s more accurate to say that deduplication of values is a common (and desired) side effect of database normalization.&#13;
And since you should normalize your databases in any case, you’re likely to avoid duplicate values.</p>&#13;
&#13;
<p>There’s an interesting flip side: <em>denormalization</em>.&#13;
Denormalization <a data-primary="denormalization" data-type="indexterm" id="idm45829112235440"/>is the opposite of normalization: combining related data into one table.&#13;
The single table in <a data-type="xref" href="#books-dupe">Example 3-7</a> could be a denormalized table, if that was the intention behind its design.&#13;
Denormalization is a technique to increase performance by eliminating table joins and attendant complexities.&#13;
But don’t rush to denormalize your schemas because there are details and trade-offs to consider that are beyond the scope of this book.&#13;
In fact, denormalization is the opposite of <em>less data</em> because it intentionally duplicates data to trade space for speed.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>The safe bet and best practice is database normalization and less data.&#13;
Incredible scale and performance are possible with both.<a data-primary="practical value" data-startref="practical-value" data-type="indexterm" id="idm45829112231744"/><a data-primary="compact value" data-startref="compact-value" data-type="indexterm" id="idm45829112230768"/><a data-primary="deduplicated value" data-startref="deduplicated-value" data-type="indexterm" id="idm45829112229824"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Every secondary index is used and not a duplicate" data-type="sect3"><div class="sect3" id="data-checklist-5">&#13;
<h3>Every secondary index <a data-primary="secondary indexes" data-type="indexterm" id="secondary-index2"/>is used and not a duplicate</h3>&#13;
&#13;
<p>Second to last on the efficient data storage checklist (<a data-type="xref" href="#data-storage-checklist">Table 3-4</a>): every secondary index is used and not a duplicate.&#13;
Avoiding unused indexes and duplicate indexes is always a great idea, but it’s especially important for data size because indexes are copies of data.&#13;
Granted, secondary indexes are much smaller than the full table (the primary key) because they only contain index column values and corresponding primary key column values, but these add up as the table grows.</p>&#13;
&#13;
<p>Dropping unused and duplicate secondary indexes is an easy way to reduce data size, but be careful.&#13;
As mentioned in <a data-type="xref" href="ch02.html#idx-too-many">“Excessive, Duplicate, and Unused”</a>, finding unused indexes is tricky because an index might not be used frequently, so be sure to check index usage over a sufficiently long period.&#13;
By contrast, duplicate indexes are easier to find: use <a href="https://oreil.ly/qSStI">pt-duplicate-key-checker</a>.&#13;
Again: be careful when dropping indexes.</p>&#13;
&#13;
<p>Dropping an index only recovers a data size equal to the index size.&#13;
There are three methods to see index sizes.&#13;
Let’s use the <a href="https://oreil.ly/lwWxR"><code>employees</code> sample database</a> because it has a few megabytes of index data.&#13;
The first and preferred method to see index sizes is querying table <code>INFORMATION_SCHEMA.TABLES</code>, as shown in <a data-type="xref" href="#show-index-size-1">Example 3-9</a>.</p>&#13;
<div data-type="example" id="show-index-size-1">&#13;
<h5><span class="label">Example 3-9. </span>Index sizes of employees sample database (<code>INFORMATION_SCHEMA</code>)</h5>&#13;
&#13;
<pre data-type="programlisting">SELECT&#13;
  TABLE_NAME, DATA_LENGTH, INDEX_LENGTH&#13;
FROM&#13;
  INFORMATION_SCHEMA.TABLES&#13;
WHERE&#13;
  TABLE_TYPE = 'BASE TABLE' AND TABLE_SCHEMA = 'employees';&#13;
&#13;
+--------------+-------------+--------------+&#13;
| TABLE_NAME   | DATA_LENGTH | INDEX_LENGTH |&#13;
+--------------+-------------+--------------+&#13;
| departments  |       16384 |        16384 |&#13;
| dept_emp     |    12075008 |      5783552 |&#13;
| dept_manager |       16384 |        16384 |&#13;
| employees    |    15220736 |            0 |&#13;
| salaries     |   100270080 |            0 |&#13;
| titles       |    20512768 |            0 |&#13;
+--------------+-------------+--------------+</pre></div>&#13;
&#13;
<p><code>TABLE_NAME</code> is the table name in the <code>employees</code> sample database—only six tables.&#13;
(The database has some views that are filtered out by condition <code>TABLE_TYPE = 'BASE TABLE'</code>.)&#13;
<code>DATA_LENGTH</code> is the size of the primary key (in bytes).&#13;
<code>INDEX_LENGTH</code> is the size of all secondary indexes (in bytes).&#13;
The last four tables have no secondary indexes, only a primary key.</p>&#13;
&#13;
<p>The second and historical (but still widely used) method to see index sizes is <code>SHOW TABLES STATUS</code>.&#13;
You can add a <code>LIKE</code> clause to show only one table, as demonstrated in <a data-type="xref" href="#show-index-size-2">Example 3-10</a>.</p>&#13;
<div data-type="example" id="show-index-size-2">&#13;
<h5><span class="label">Example 3-10. </span>Index sizes of table <code>employees.dept_emp</code> (<code>SHOW TABLE STATUS</code>)</h5>&#13;
&#13;
<pre data-type="programlisting">SHOW TABLE STATUS LIKE 'dept_emp'\G&#13;
&#13;
*************************** 1. row ***************************&#13;
           Name: dept_emp&#13;
         Engine: InnoDB&#13;
        Version: 10&#13;
     Row_format: Dynamic&#13;
           Rows: 331143&#13;
 Avg_row_length: 36&#13;
    Data_length: 12075008&#13;
Max_data_length: 0&#13;
   Index_length: 5783552&#13;
      Data_free: 4194304&#13;
 Auto_increment: NULL&#13;
    Create_time: 2021-03-28 11:15:15&#13;
    Update_time: 2021-03-28 11:15:24&#13;
     Check_time: NULL&#13;
      Collation: utf8mb4_0900_ai_ci&#13;
       Checksum: NULL&#13;
 Create_options:&#13;
        Comment:</pre></div>&#13;
&#13;
<p>The fields <code>Data_length</code> and <code>Index_length</code> in the <code>SHOW TABLE STATUS</code> output are the same columns and values from <code>INFORMATION_SCHEMA.TABLES</code>.&#13;
It’s better to query <code>INFORMATION_SCHEMA.TABLES</code> because you can use functions in the <code>SELECT</code> clause like <code>ROUND(DATA_LENGTH / 1024 / 1024)</code> to convert and round the values from bytes to other units.</p>&#13;
&#13;
<p>The third method to see index sizes is currently the only method to see the size of each index: query table <code>mysql.innodb_index_stats</code>, as shown in <a data-type="xref" href="#show-index-size-3">Example 3-11</a> for table <code>employees.dept_emp</code>.</p>&#13;
<div data-type="example" id="show-index-size-3">&#13;
<h5><span class="label">Example 3-11. </span>Size of each index on table <code>employees.dept_emp</code> (<code>mysql.innodb_index_stats</code>)</h5>&#13;
&#13;
<pre data-type="programlisting">SELECT&#13;
  index_name, SUM(stat_value) * @@innodb_page_size size&#13;
FROM&#13;
  mysql.innodb_index_stats&#13;
WHERE&#13;
      stat_name = 'size'&#13;
  AND database_name = 'employees'&#13;
  AND table_name = 'dept_emp'&#13;
GROUP BY index_name;&#13;
&#13;
+------------+----------+&#13;
| index_name | size     |&#13;
+------------+----------+&#13;
| PRIMARY    | 12075008 |&#13;
| dept_no    |  5783552 |&#13;
+------------+----------+</pre></div>&#13;
&#13;
<p>Table <code>employees.dept_emp</code> has two indexes: a primary key and a secondary index named <code>dept_no</code>.&#13;
Column <code>size</code> contains the size of each index in bytes, which is actually the number of index pages multiplied by the InnoDB page size (16 KB by default).</p>&#13;
&#13;
<p>The <code>employees</code> sample database is not a spectacular display of secondary index size, but real-world databases can be overflowing with secondary indexes that account for a significant amount of total data size.&#13;
Regularly check index usage and index sizes, and reduce total data size by carefully dropping unused and duplicate indexes.<a data-primary="secondary indexes" data-startref="secondary-index2" data-type="indexterm" id="idm45829112195136"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Only needed rows are kept" data-type="sect3"><div class="sect3" id="data-checklist-6">&#13;
<h3>Only needed rows are kept</h3>&#13;
&#13;
<p>Last item on the efficient data storage checklist (<a data-type="xref" href="#data-storage-checklist">Table 3-4</a>): only needed rows are kept.&#13;
This item brings us full circle, closing the loop with the first item: <a data-type="xref" href="#data-checklist-1">“Only needed rows are stored”</a>.&#13;
A row might be needed when stored, but that need changes over time.&#13;
Delete (or archive) rows that are no longer needed.&#13;
That sounds obvious, but it’s common to find tables with forgotten or abandoned data.&#13;
I’ve lost count of how many times I’ve seen teams drop entire tables that were forgotten.</p>&#13;
&#13;
<p>Deleting (or archiving) data is a lot easier said than done, and the next section takes on the challenge.<a data-primary="data" data-secondary="principle of least data" data-startref="principle-of-least-data_index1" data-type="indexterm" id="idm45829112189936"/><a data-primary="principle of least data" data-startref="principle-of-least-data_index2" data-type="indexterm" id="idm45829112188624"/><a data-primary="data" data-secondary="storage" data-startref="data-storage_index1" data-type="indexterm" id="idm45829112187664"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Delete or Archive Data" data-type="sect1"><div class="sect1" id="delete-or-archive">&#13;
<h1>Delete or Archive Data</h1>&#13;
&#13;
<p>I hope <a data-primary="data" data-secondary="delete or archive" data-type="indexterm" id="data-delete-or-archive-ch3"/>this chapter instills in you a desire to delete or archive data.&#13;
Too much data has woken me from too many pleasant dreams: it’s as if MySQL has a mind of its own and waits until 3 a.m. to fill <a data-primary="backfill (data)" data-type="indexterm" id="idm45829112182928"/>up the disk.&#13;
I once had an application page me in the middle of the night in three different time zones (my time zone changed due to meetings in different parts of the world).&#13;
But enough about me; let’s talk about how to delete or archive data without negatively impacting the application.</p>&#13;
&#13;
<p>For brevity, I refer only to deleting data, not deleting <em>or archiving</em> data, because the challenge lies almost entirely in the former: deleting data.&#13;
Archiving data requires copying the data first, then deleting it.&#13;
Copying data should use nonlocking <code>SELECT</code> statements to avoid impacting the application, then write the copied rows to another table or data store that the application doesn’t access.&#13;
Even with nonlocking <code>SELECT</code> statements, you must rate-limit the copy process to avoid increasing QPS beyond what MySQL and the application can handle.&#13;
(Recall from <a data-type="xref" href="#less-qps-is-better">“Less QPS Is Better”</a> that QPS is relative to the application and difficult to increase.)</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tools" data-type="sect2"><div class="sect2" id="idm45829112178704">&#13;
<h2>Tools</h2>&#13;
&#13;
<p>You will have to write your own tools to delete or archive data.&#13;
Sorry to lead with bad news, but it’s the truth.&#13;
The good news is that deleting and archiving data is not difficult—it’s probably trivial compared to your application.&#13;
The <em>critically important</em> part is throttling the loop that executes SQL statements.&#13;
Never do this:</p>&#13;
&#13;
<pre data-type="programlisting">for {&#13;
    rowsDeleted = execute(“DELETE FROM table LIMIT 1000000”)&#13;
    if rowsDeleted == 0 {&#13;
        break&#13;
    }&#13;
}</pre>&#13;
&#13;
<p>The <code>LIMIT 1000000</code> clause is probably too large, and the <code>for</code> loop has no delay between statements.&#13;
That pseudocode is likely to cause an application outage.&#13;
<em>Batch size</em> is the key to a safe and effective data archiving tool.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Batch Size" data-type="sect2"><div class="sect2" id="batch-size">&#13;
<h2>Batch Size</h2>&#13;
&#13;
<p>First, <a data-primary="batch size" data-type="indexterm" id="batch-size2"/>a shortcut that might allow you to skip reading this section until needed: it’s safe to <em>manually</em> delete 1,000 rows or less in a single <code>DELETE</code> statement if the rows are small (no <code>BLOB</code>, <code>TEXT</code>, or <code>JSON</code> columns) and MySQL is not heavily loaded.&#13;
<em>Manually</em> means that you execute each <code>DELETE</code> statement in series (one after the other), not in parallel.&#13;
Do not write a program to execute the <code>DELETE</code> statements.&#13;
Most humans are too slow for MySQL to notice, so no matter how fast you are, you cannot manually execute <code>DELETE</code>…<code>LIMIT 1000</code> statements fast enough to overload MySQL.&#13;
Use this shortcut judiciously, and have another engineer review any manual deletes.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The method described in this section focuses on <code>DELETE</code> but applies in general to <code>INSERT</code> and <code>UPDATE</code>.&#13;
For <code>INSERT</code>, batch size is controlled by the number of rows inserted, not a <code>LIMIT</code> clause.</p>&#13;
</div>&#13;
&#13;
<p>The rate at which you can quickly <em>and safely</em> delete rows is determined by the batch size that MySQL and the application can sustain without impacting query response time or replication lag.&#13;
(<a data-type="xref" href="ch07.html#ch07">Chapter 7</a> covers replication lag.)&#13;
<em>Batch size</em> is the number of rows deleted per <code>DELETE</code> statement, which is controlled by a <code>LIMIT</code> clause and throttled by a simple delay, if necessary.</p>&#13;
&#13;
<p>Batch size is calibrated to an execution time; 500 milliseconds is a good starting point.&#13;
This means that each <code>DELETE</code> statement should take no longer than 500 ms to execute.&#13;
This is critically important for two reasons:</p>&#13;
<dl>&#13;
<dt>Replication lag</dt>&#13;
<dd>&#13;
<p>Execution <a data-primary="replication lag" data-type="indexterm" id="idm45829112156320"/>time on a source MySQL instance creates replication lag on replica MySQL instances.&#13;
If a <code>DELETE</code> statement takes 500 ms to execute on the source, then it also takes 500 ms to execute on a replica, which creates 500 ms of replication lag.&#13;
You cannot avoid replication lag, but you must minimize it because replication lag is data loss.&#13;
(For now, I gloss over many details about replication that I clarify in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.)</p>&#13;
</dd>&#13;
<dt>Throttling</dt>&#13;
<dd>&#13;
<p>In some cases, <a data-primary="throttling" data-type="indexterm" id="idm45829112152400"/>it’s safe to execute <code>DELETE</code> statements with no delay—no throttling—because the calibrated batch size limits query execution time, which limits QPS.&#13;
A query that takes 500 ms to execute can only execute at 2 QPS in series.&#13;
But these are no ordinary queries: they’re purpose-built to access and write (delete) as many rows as possible.&#13;
Without throttling, bulk <a data-primary="bulk (load) data" data-type="indexterm" id="idm45829112150768"/>writes can disrupt other queries and impact the application.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Throttling is paramount when deleting data: always begin with a delay between <code>DELETE</code> statements, and monitor replication lag.<sup><a data-type="noteref" href="ch03.html#idm45829112148736" id="idm45829112148736-marker">2</a></sup></p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Always build a throttle into bulk operations.</p>&#13;
</div>&#13;
&#13;
<p>To calibrate the batch size to a 500 ms execution time (or whatever execution time you chose), start with batch size 1,000 (<code>LIMIT 1000</code>) and a 200 ms delay between <code>DELETE</code> statements:&#13;
200 ms is a long delay, but you decrease it after calibrating the batch size.&#13;
Let that run for at least 10 minutes while monitoring replication lag and MySQL stability—don’t let MySQL lag or destabilize.&#13;
(Replication lag and MySQL stability are covered in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.html#ch07">7</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#ch06">6</a>, respectively.)&#13;
Use query reporting (see <a data-type="xref" href="ch01.html#query-reporting">“Reporting”</a>) to inspect the maximum execution time of the <code>DELETE</code> statement, or measure it directly in your data archiving tool.&#13;
If the maximum execution time is well below the target—500 ms—then double the batch size and re-run for another 10 minutes.&#13;
Keep doubling the batch size—or making smaller adjustments—until the maximum execution time is consistently on target—preferably just a little below target.&#13;
When you’re done, record the calibrated batch size and execution time because deleting old data should be a recurring event.</p>&#13;
&#13;
<p>To set the throttle using the calibrated batch size, repeat the process by slowly reducing the delay on each 10-minute rerun.&#13;
Depending on MySQL and the application, you might reach zero (no throttling).&#13;
Stop at the first sign of replication lag or MySQL destabilizing, then increase the delay to the previous value that didn’t cause either problem.&#13;
When you’re done, record the delay for the same reason as before: deleting old data should be a recurring event.</p>&#13;
&#13;
<p>With the batch size calibrated and the throttle set, you can finally calculate the rate: how many rows per second you can delete without impacting query response time: <code>batch size * DELETE QPS</code>.&#13;
(Use query reporting to inspect the QPS of the <code>DELETE</code> statement, or measure it directly in your data archiving tool.)&#13;
Expect the rate to change throughout the day.&#13;
If the application is extremely busy during business hours, the only sustainable rate might be zero.&#13;
If you’re an ambitious go-getter who’s on a rocket ride to the top of your career, your industry, and the world, then wake up in the middle of the night and try a higher rate when the database is quiet: larger batch size, lower delay, or both.&#13;
Just remember to reset the batch size and delay before the sun rises and database load increases.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>MySQL backups almost always run in the middle of the night.&#13;
Even if the application is quiet in the dead of night, the database might be busy.<a data-primary="batch size" data-startref="batch-size2" data-type="indexterm" id="idm45829112135712"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Row Lock Contention" data-type="sect2"><div class="sect2" id="idm45829112173136">&#13;
<h2>Row Lock Contention</h2>&#13;
&#13;
<p>For write-heavy workloads, <a data-primary="row lock contention" data-type="indexterm" id="idm45829112133088"/>bulk <a data-primary="bulk (load) data" data-type="indexterm" id="idm45829112132256"/>operations can cause elevated <em>row lock contention</em>: queries waiting to acquire row locks on the same (or nearby) rows.&#13;
This problem mainly affects <code>INSERT</code> and <code>UPDATE</code> statements, but <code>DELETE</code> statements could be affected, too, if deleted rows are interspersed with kept rows.&#13;
The problem is that the batch size is too large even though it executes within the calibrated time.&#13;
For example, MySQL might be able to delete 100,000 rows in 500 ms, but if the locks for those rows overlap with rows that the application is updating, then it causes row lock contention.</p>&#13;
&#13;
<p>The solution is to reduce the batch size by calibrating for a much smaller execution time—100 ms, for example.&#13;
In extreme cases, you might need to increase the delay, too: small batch size, long delay.&#13;
This reduces row lock contention, which is good for the application, but it makes data archiving slower.&#13;
There’s no magical solution for this extreme case; it’s best to avoid with <em>less data</em> and <em>fewer QPS</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Space and Time" data-type="sect2"><div class="sect2" id="space-and-time">&#13;
<h2>Space and Time</h2>&#13;
&#13;
<p>Deleting data does not free disk space.&#13;
Row deletes are logical, not physical, which is a common performance optimization in many databases.&#13;
When you delete 500 GB of data, you don’t get 500 GB of disk space, you get 500 GB of free pages.<a data-primary="free pages" data-type="indexterm" id="idm45829112125248"/>&#13;
Internal details are more complex and beyond the scope of this book, but the general idea is correct: deleting data yields free pages, not free disk space.</p>&#13;
&#13;
<p>Free pages do not affect performance, and InnoDB reuses free pages when new rows are inserted.&#13;
If deleted rows will soon be replaced by new rows, and disk space isn’t limited, then free pages and unclaimed disk space are not a concern.&#13;
But please be mindful of your colleagues: if your company runs its own hardware and MySQL for your application shares disk space with MySQL for other applications, then don’t waste disk space that can be used by other applications.&#13;
In the cloud, storage costs money, so don’t waste money: reclaim the disk space.</p>&#13;
&#13;
<p>The best way to reclaim disk space from InnoDB is to rebuild the table by executing a no-op <code>ALTER TABLE</code>…<code>ENGINE=INNODB</code> statement.&#13;
This is a solved problem with three great solutions:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/8EJph">pt-online-schema-change</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/IsV83"><code>gh-ost</code></a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/JhWdg"><code>ALTER TABLE</code>…<code>ENGINE=INNODB</code></a></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Each solution works differently, but they have one thing in common: all of them can rebuild huge InnoDB tables <em>online</em>: in production without impacting the application.&#13;
Read the documentation for each to decide which one works best for you.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>To rebuild a table with <code>ALTER TABLE</code>…<code>ENGINE=INNODB</code>, replace … with the table name.&#13;
Do not make any other changes.</p>&#13;
</div>&#13;
&#13;
<p>Deleting large amounts of data takes time.&#13;
You might read or hear about how fast MySQL can write data, but that’s usually for benchmarks (see <a data-type="xref" href="ch02.html#mysql-tuning">“MySQL Tuning”</a>).&#13;
In the glamorous world of laboratory research, sure: MySQL will consume every clock cycle and disk IOP you can give it.&#13;
But in the quotidian world that you and I slog through, data must be deleted with significant restraint to avoid impacting the application.&#13;
To put it bluntly: it’s going to take a lot longer than you think.&#13;
The good news is: if done correctly—as detailed in <a data-type="xref" href="#batch-size">“Batch Size”</a>—then time is on your side.&#13;
A well-calibrated, sustainable bulk <a data-primary="bulk (load) data" data-type="indexterm" id="idm45829112111056"/>operation can run for days and weeks.&#13;
This includes the solution that you use to reclaim disk space from InnoDB because rebuilding the table is just another type of bulk operation.&#13;
It takes time to delete rows, and it takes additional time to reclaim the disk space.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Binary Log Paradox" data-type="sect2"><div class="sect2" id="idm45829112109760">&#13;
<h2>The Binary Log Paradox</h2>&#13;
&#13;
<p>Deleting data creates <a data-primary="binary log paradox" data-type="indexterm" id="binary-log-paradox"/>data.&#13;
This paradox happens because data changes are written to the binary logs.&#13;
Binary logging can be disabled, but it never is in production because the binary logs are required for replication, and no sane production system runs without replicas.</p>&#13;
&#13;
<p>If the table contains large <code>BLOB</code>, <code>TEXT</code>, or <code>JSON</code> columns, then binary log size could increase dramatically because the MySQL system variable <a href="https://oreil.ly/0bNcG"><code>binlog_row_image</code></a> defaults to <code>full</code>.&#13;
That variable determines how row images are written to the binary logs; it has three settings:</p>&#13;
<dl>&#13;
<dt><code>full</code></dt>&#13;
<dd>&#13;
<p>Write the value of every column (the full row).</p>&#13;
</dd>&#13;
<dt><code>minimal</code></dt>&#13;
<dd>&#13;
<p>Write the value of columns that changed and columns required to identify the row.</p>&#13;
</dd>&#13;
<dt><code>noblob</code></dt>&#13;
<dd>&#13;
<p>Write the value of every column <em>except</em> <code>BLOB</code> and <code>TEXT</code> columns that aren’t required.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>It’s both safe and recommended to use <code>minimal</code> (or <code>noblob</code>) if there are no external services that rely on full row images in the binary logs—for example, a data pipeline service that stream changes to a data lake or big data store.</p>&#13;
&#13;
<p>If you use <a href="https://oreil.ly/2EB4l">pt-online-schema-change</a> or <a href="https://oreil.ly/nUuvv"><code>gh-ost</code></a> to rebuild the table, these tools copy the table (safely and automatically), and that copy process writes even more data changes to the binary logs.&#13;
However, <code>ALTER TABLE</code>…<code>ENGINE=INNODB</code> defaults to an in-place alter—no table copy.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>When deleting a lot of data, disk usage will <em>increase</em> because of binary logging and the fact that deleting data does not free disk space.</p>&#13;
</div>&#13;
&#13;
<p>Paradoxically, you must ensure that the server has enough free disk space to delete data and rebuild the table.<a data-primary="data" data-secondary="delete or archive" data-startref="data-delete-or-archive-ch3" data-type="indexterm" id="idm45829112090672"/><a data-primary="binary log paradox" data-startref="binary-log-paradox" data-type="indexterm" id="idm45829112089408"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch03-summary">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>This chapter examined data with respect to performance and argued that reducing data access and storage is a technique—an indirect query optimization—for improving performance.&#13;
The primary takeaway points are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Less data yields better performance.</p>&#13;
</li>&#13;
<li>&#13;
<p>Less QPS is better because it’s a liability, not an asset.</p>&#13;
</li>&#13;
<li>&#13;
<p>Indexes are necessary for maximum MySQL performance, but there are cases when indexes may not help.</p>&#13;
</li>&#13;
<li>&#13;
<p>The principle of least data means: store and access only needed data.</p>&#13;
</li>&#13;
<li>&#13;
<p>Ensure that queries access as few rows as possible.</p>&#13;
</li>&#13;
<li>&#13;
<p>Do not store more data than needed: data is valuable to you, but it’s dead weight to MySQL.</p>&#13;
</li>&#13;
<li>&#13;
<p>Deleting or archiving data is important and improves performance.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The next chapter centers on access patterns that determine how you can change the application to use MySQL efficiently.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Practice: Audit Query Data Access" data-type="sect1"><div class="sect1" id="ch03-ai">&#13;
<h1>Practice: Audit Query Data Access</h1>&#13;
&#13;
<p>The goal of this practice <a data-primary="audit query data access" data-type="indexterm" id="idm45829112076528"/>is to audit queries for inefficient data access.&#13;
This is the efficient data access checklist (<a data-type="xref" href="#data-access-checklist">Table 3-2</a>):</p>&#13;
<ul class="simplelist">&#13;
<li>☐ Return only needed columns</li>&#13;
<li>☐ Reduce query complexity</li>&#13;
<li>☐ Limit row access</li>&#13;
<li>☐ Limit the result set</li>&#13;
<li>☐ Avoid sorting rows</li>&#13;
</ul>&#13;
&#13;
<p>Apply the checklist to the top 10 slow queries.&#13;
(To get slow queries, refer back to <a data-type="xref" href="ch01.html#query-profile">“Query profile”</a> and <a data-type="xref" href="ch01.html#ch01-ai">“Practice: Identify Slow Queries”</a>.)&#13;
An easy fix is any <code>SELECT *</code>: explicitly select only the columns needed.&#13;
Also pay close attention to any query with an <code>ORDER BY</code> clause: is it using an index? Does it have a <code>LIMIT</code>? Can the application sort rows instead?</p>&#13;
&#13;
<p>Unlike <a data-type="xref" href="ch01.html#ch01-ai">“Practice: Identify Slow Queries”</a> and <a data-type="xref" href="ch02.html#ch02-ai">“Practice: Find Duplicate Indexes”</a>, there is no tool to audit query data access.&#13;
But the checklist is only five items, so it doesn’t take long to audit queries manually.&#13;
Carefully and methodically auditing queries for optimal data access is expert-level MySQL performance practice.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45829112705168"><sup><a href="ch03.html#idm45829112705168-marker">1</a></sup> MySQL does not support sparse or partial indexes.</p><p data-type="footnote" id="idm45829112148736"><sup><a href="ch03.html#idm45829112148736-marker">2</a></sup> Check out <a href="https://oreil.ly/vSmUb"><code>freno</code></a> by GitHub Engineering: an open source throttle for MySQL.</p></div></div></section></body></html>