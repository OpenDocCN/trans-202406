- en: Chapter 14\. Running Machine Learning in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The age of microservices, distributed systems, and the cloud has provided the
    perfect environmental conditions for the democratization of machine learning models
    and tooling. Infrastructure at scale has now become commoditized, and the tooling
    around the machine learning ecosystem is maturing. Kubernetes is one of the platforms
    that has become increasingly popular among developers, data scientists, and the
    wider open source community as the perfect environment to enable the machine learning
    workflow and life cycle. Large machine learning models like [GPT-4](https://oreil.ly/sGzRc)
    and [DALL·E](https://oreil.ly/zTWNx) have brought machine learning into the spotlight
    and organizations like [OpenAI](https://oreil.ly/bCXwF) have been very public
    about their use of Kubernetes to support these models. In this chapter, we will
    cover why Kubernetes is a great platform for machine learning and provide best
    practices for both cluster administrators and data scientists alike on how to
    get the most out of Kubernetes when running machine learning workloads. Specifically,
    we focus on deep learning rather than traditional machine learning because deep
    learning has quickly become the area of innovation on platforms like Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Why Is Kubernetes Great for Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes has quickly become the home for rapid innovation in deep learning.
    The confluence of tooling and libraries such as [TensorFlow](https://oreil.ly/nzHaG)
    makes this technology more accessible to a large audience of data scientists.
    What makes Kubernetes such a great place to run your deep learning workloads?
    Let’s cover what Kubernetes provides:'
  prefs: []
  type: TYPE_NORMAL
- en: Ubiquitous
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is everywhere. All the major public clouds support it, and there
    are distributions for private clouds and infrastructure. Basing ecosystem tooling
    on a platform like Kubernetes allows users to run their deep learning workloads
    anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning workflows typically need access to large amounts of computing
    power to efficiently train machine learning models. Kubernetes ships with native
    autoscaling capabilities that make it easy for data scientists to achieve and
    fine-tune the level of scale they need to train their models.
  prefs: []
  type: TYPE_NORMAL
- en: Extensible
  prefs: []
  type: TYPE_NORMAL
- en: Efficiently training a machine learning model typically requires access to specialized
    hardware. Kubernetes allows cluster administrators to quickly and easily expose
    new types of hardware to the scheduler without having to change the Kubernetes
    source code. It also allows custom resources and controllers to be seamlessly
    integrated into the Kubernetes API to support specialized workflows, such as hyperparameter
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Self-service
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists can use Kubernetes to perform self-service machine learning
    workflows on demand, without needing specialized knowledge of Kubernetes itself.
  prefs: []
  type: TYPE_NORMAL
- en: Portable
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models can be run anywhere, provided that the tooling is based
    on the Kubernetes API. This allows machine learning workloads to be portable across
    Kubernetes providers.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively understand the needs of deep learning, you must understand the
    complete machine learning workflow. [Figure 14-1](#machine_learning_development_workflow)
    represents a simplified workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning development workflow](assets/kbp2_1401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Machine learning development workflow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, the workflow has the following phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  prefs: []
  type: TYPE_NORMAL
- en: This phase includes the storage, indexing, cataloging, and metadata associated
    with the dataset used to train the model. For the purposes of this book, we consider
    only the storage aspect. Datasets vary in size, from hundreds of megabytes to
    hundreds of terabytes, and even petabytes, and need to be provided to the model
    in order for the model to be trained. You must consider storage that provides
    the appropriate properties to meet these needs. Typically, large-scale block and
    object stores are required and must be accessible via Kubernetes-native storage
    abstractions or directly accessible APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Model development
  prefs: []
  type: TYPE_NORMAL
- en: In this phase, data scientists write, share, and collaborate on machine learning
    algorithms. Open source tools like JupyterHub are easy to install on Kubernetes
    because they typically function like any other workload.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs: []
  type: TYPE_NORMAL
- en: For a model to use the dataset to learn how to perform the tasks it’s designed
    to perform, it must be trained. The resulting artifact of the training process
    is usually a checkpoint of the trained model state. The training process is the
    piece that takes advantage of all the capabilities of Kubernetes at the same time.
    Scheduling, access to specialized hardware, dataset volume management, scaling,
    and networking will all be exercised in unison to complete this task. We cover
    more of the specifics of the training phase in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Serving
  prefs: []
  type: TYPE_NORMAL
- en: This is the process of making the trained model accessible to service requests
    from clients so that it can make an inference based on the data supplied from
    the client. For example, if you have an image-recognition model that’s been trained
    to detect dogs and cats, a client might submit a picture of a dog, and the model
    should be able to determine whether it is a dog, with a certain level of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning for Kubernetes Cluster Admins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few topics to consider before running machine learning workloads
    on your Kubernetes cluster. This section is specifically targeted to cluster administrators.
    The largest challenge you will face as a cluster administrator responsible for
    a team of data scientists is understanding the terminology. There are myriad new
    terms that you must become familiar with over time, but rest assured, you can
    do it. Let’s look at the main problem areas you’ll need to address when preparing
    a cluster for machine learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training machine learning models on Kubernetes requires conventional CPUs and
    graphics processing units (GPUs). Typically, the more resources you apply, the
    faster the training will be completed. In most cases, model training can be achieved
    on a single machine that has the required resources. Many cloud providers offer
    multi-GPU virtual machine (VM) types, so we recommend scaling VMs vertically to
    four to eight GPUs before looking into distributed training. Data scientists use
    a technique known as *hyperparameter tuning* when training models. A hyperparameter
    is simply a parameter that has a set value before the training process begins.
    Hyperparameter tuning is the process of finding the optimal set of hyperparameters
    for model training. The technique involves running many of the same training jobs
    with a different set of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Training your first model on Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, you are going to use the MNIST dataset to train an image-classification
    model. The MNIST dataset is publicly available and commonly used for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the model, you need GPUs. Let’s confirm that your Kubernetes cluster
    has GPUs available. The following command shows how many GPUs are available in
    a Kubernetes cluster. From the output we can see that this cluster has four GPUs
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that training is a batch workload, to run your training you’re going
    to use the `Job` kind in Kubernetes. You will run your training for 500 steps
    and use a single GPU. Create a file called *mnist-demo.yaml* using the following
    manifest, and save it to your filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create this resource on your Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the status of the job you just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look at the pods, you should see the training job running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the pod logs, you can see the training happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can see that the training has completed by looking at the job
    status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To clean up the training job, simply run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You just ran your first model training job on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed training is still in its infancy and is difficult to optimize. Running
    a training job that requires eight GPUs will almost always be faster to train
    on a single eight-GPU machine compared to two machines with four GPUs each. The
    only time you should resort to using distributed training is when the model doesn’t
    fit on the biggest machine available. If you are certain you must run distributed
    training, it is important to understand the architecture. [Figure 14-2](#distributed_tensorflow_architecture)
    depicts the distributed TensorFlow architecture, and you can see how the model
    and the parameters are distributed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed TensorFlow architecture](assets/kbp2_1402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. Distributed TensorFlow architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Resource Constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning workloads demand very specific configurations across all aspects
    of your cluster. The training phases are most certainly the most resource intensive.
    It’s also important to note, as we mentioned a moment ago, that machine learning
    algorithm training is almost always a batch-style workload. Specifically, it will
    have a start time and a finish time. The finish time of a training run depends
    on how quickly you can meet the resource requirements of the model training. This
    means that scaling is almost certainly a quicker way to finish training jobs faster,
    but scaling has its own set of bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized Hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training and serving a model is almost always more efficient on specialized
    hardware. A typical example of such specialized hardware would be commodity GPUs.
    Kubernetes allows you to access GPUs via device plug-ins that make the GPU resource
    known to the Kubernetes scheduler and therefore able to be scheduled. A device
    plug-in framework facilitates this capability, which means that vendors do not
    need to modify the core Kubernetes code to implement their specific device. These
    device plug-ins typically run on each node as DaemonSets, which are processes
    that are responsible for advertising these specific resources to the Kubernetes
    API. Let’s look at the [NVIDIA device plug-in for Kubernetes](https://oreil.ly/RgKuz),
    which enables access to NVIDIA GPUs. After they’re running, you can create a pod
    as follows, and Kubernetes will ensure that it is scheduled to a node that has
    these resources available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Device plug-ins are not limited to GPUs; you can use them wherever specialized
    hardware is needed—for example, Field Programmable Gate Arrays (FPGAs) or InfiniBand.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling idiosyncrasies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to note that Kubernetes cannot make decisions about resources
    that it does not have knowledge about. One of the things you might notice is that
    the GPUs are not running at capacity when you are training. You are therefore
    not achieving the level of utilization you would like to see. Let’s consider the
    previous example; it exposes only the number of GPU cores and omits the number
    of threads that can be run per core. It also doesn’t expose which bus the GPU
    core is on, so that jobs that need access to one another or to the same memory
    might be colocated on the same Kubernetes nodes. All these considerations might
    be addressed by device plug-ins in the future but for now might leave you wondering
    why you cannot get 100% utilization on that beefy GPU you just purchased. It’s
    also worth mentioning that you cannot request fractions of GPUs (for example,
    0.1), which means that even if the specific GPU supports running multiple threads
    concurrently, you will not be able to utilize that capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries, Drivers, and Kernel Modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To access specialized hardware, you typically need purpose-built libraries,
    drivers, and kernel modules. You will need to ensure that these are mounted into
    the container runtime so that they are available to the tooling running in the
    container. You might ask, “Why don’t I just add these to the container image itself?”
    The answer is simple: the tools need to match the version on the underlying host
    and must be configured appropriately for that specific system. Container runtimes
    such as [NVIDIA Docker](https://oreil.ly/Re0Ef) remove the burden of having to
    map host volumes into each container. In lieu of having a purpose-built container
    runtime, you might be able to build an admission webhook that provides the same
    functionality. It’s also important to consider that you might need privileged
    containers to access some specialized hardware, which affects the cluster security
    profile. The installation of the associated libraries, drivers, and kernel modules
    might also be facilitated by Kubernetes device plug-ins. Many device plug-ins
    run checks on each machine to confirm that all installations have been completed
    before they advertise the schedulable GPU resources to the Kubernetes scheduler.'
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storage is one of the most critical aspects of the machine learning workflow.
    You need to consider storage because it directly affects the following pieces
    of the machine learning workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset storage and distribution among nodes during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpoints and saving models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset storage and distribution among nodes during training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During training, the dataset must be retrievable by every node. The storage
    needs are read-only, and, typically, the faster the disk, the better. The type
    of disk that’s providing the storage is almost completely dependent on the size
    of the dataset. Datasets of hundreds of megabytes or gigabytes might be perfect
    for block storage, but datasets that are several or hundreds of terabytes in size
    might be better suited to object storage. Depending on the size and location of
    the disks that hold the datasets, there might be a performance hit on your networking.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints and saving models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Checkpoints are created as a model is being trained, and saving models allows
    you to use them for serving. In both cases, you need storage attached to each
    of the nodes to store this data. The data is typically stored under a single directory,
    and each node is writing to a specific checkpoint or save file. Most tools expect
    the checkpoint and save data to be in a single location and require `ReadWriteMany`.
    `ReadWriteMany` simply means that the volume can be mounted as read-write by many
    nodes. When using Kubernetes PersistentVolumes, you will need to determine the
    best storage platform for your needs. The Kubernetes documentation keeps a [list](https://oreil.ly/aMjGd)
    of volume plug-ins that support `ReadWriteMany`.
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training phase of the machine learning workflow has a large impact on the
    network (specifically, when running distributed training). If we consider TensorFlow’s
    distributed architecture, two discrete phases create a lot of network traffic:
    variable distribution from each of the parameter servers to each of the nodes,
    and the application of gradients from each node back to the parameter server (refer
    back to [Figure 14-2](#distributed_tensorflow_architecture)). The time it takes
    for this exchange to happen directly affects the time it takes to train a model.
    So, it’s a simple game of the faster, the better (within reason, of course). With
    most public clouds and servers today supporting 1-Gbps, 10-Gbps, and sometimes
    40-Gbps network interface cards, generally network bandwidth is a concern only
    at lower bandwidths. You might also consider InfiniBand if you need high network
    bandwidth.'
  prefs: []
  type: TYPE_NORMAL
- en: While raw network bandwidth is more often than not a limiting factor, in some
    instances the problem is getting the data onto the wire from the kernel in the
    first place. Some open source projects take advantage of Remote Direct Memory
    Access (RDMA) to further accelerate network traffic without the need to modify
    your nodes or application code. RDMA allows computers in a network to exchange
    data in main memory without using the processor, cache, or operating system of
    either computer.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized Protocols
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Other specialized protocols you can consider when using machine learning on
    Kubernetes are often vendor specific, but they all seek to address distributed
    training scaling issues by removing areas of the architecture that quickly become
    bottlenecks. For example, parameter servers. These protocols often allow the direct
    exchange of information between GPUs on multiple nodes without the need to involve
    the node CPU and OS. Here are a couple you might want to look into to more efficiently
    scale your distributed training:'
  prefs: []
  type: TYPE_NORMAL
- en: Message Passing Interface (MPI)
  prefs: []
  type: TYPE_NORMAL
- en: A standardized portable API for the transfer of data between distributed processes
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA Collective Communications Library (NCCL)
  prefs: []
  type: TYPE_NORMAL
- en: A library of topology-aware multi-GPU communication primitives
  prefs: []
  type: TYPE_NORMAL
- en: Data Scientist Concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier in the chapter, we shared considerations you need to make in order
    to be able to run machine learning workloads on your Kubernetes cluster. But what
    about the data scientist? Here we cover some popular tools that make it easy for
    data scientists to utilize Kubernetes for machine learning without having to be
    a Kubernetes expert:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kubeflow](https://oreil.ly/UVxjM)'
  prefs: []
  type: TYPE_NORMAL
- en: A machine learning toolkit for Kubernetes, it is native to Kubernetes and ships
    with several tools necessary to complete the machine learning workflow. Tools
    such as Jupyter Notebooks, pipelines, and Kubernetes-native controllers make it
    simple and easy for data scientists to get the most out of Kubernetes as a platform
    for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[Polyaxon](https://oreil.ly/NZ7Nj)'
  prefs: []
  type: TYPE_NORMAL
- en: A tool for managing machine learning workflows that supports many popular libraries
    and runs on any Kubernetes cluster. Polyaxon has both commercial and open source
    offerings.
  prefs: []
  type: TYPE_NORMAL
- en: '[Pachyderm](https://oreil.ly/CivM_)'
  prefs: []
  type: TYPE_NORMAL
- en: An enterprise-ready data science platform with a rich suite of tools for dataset
    preparation, life cycle, and versioning, along with the ability to build machine
    learning pipelines. Pachyderm has a commercial offering you can deploy to any
    Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning on Kubernetes Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To achieve optimal performance for your machine learning workloads, consider
    the following best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Smart scheduling and autoscaling
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that most stages of the machine learning workflow are batch by nature,
    we recommend you utilize a Cluster Autoscaler. GPU-enabled hardware is costly,
    and you certainly do not want to be paying for it when it’s not in use. We recommend
    batching jobs to run at specific times using either taints and tolerations or
    via a time-specific Cluster Autoscaler. That way, the cluster can scale to the
    needs of the machine learning workloads when needed, and not a moment sooner.
    Regarding taints and tolerations, upstream convention is to taint the node with
    the extended resource as the key. For example, a node with NVIDIA GPUs should
    be tainted as follows: `Key: nvidia.com/gpu, Effect: NoSchedule`. Using this method
    means you can also utilize the `ExtendedResourceToleration` admission controller,
    which will automatically add the appropriate tolerations for such taints to pods
    requesting extended resources so that the users don’t need to add them manually.'
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that model training is a delicate balance
  prefs: []
  type: TYPE_NORMAL
- en: Allowing things to move faster in one area often leads to bottlenecks in others.
    It’s an endeavor of constant observation and tuning. As a general rule, we recommend
    you try to make the GPU become the bottleneck because it is the most costly resource.
    Keep your GPUs saturated. Be prepared to always be on the lookout for bottlenecks,
    and set up your monitoring to track the GPU, CPU, network, and storage utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed workload clusters
  prefs: []
  type: TYPE_NORMAL
- en: Clusters that are used to run the day-to-day business services might also be
    used for machine learning. Given the high performance requirements of machine
    learning workloads, we recommend using a separate node pool that’s tainted to
    accept only machine learning workloads. This will help protect the rest of the
    cluster from any impact from the machine learning workloads running on the machine
    learning node pool. Furthermore, you should consider multiple GPU-enabled node
    pools, each with different performance characteristics to suit the workload types.
    We also recommend enabling node autoscaling on the machine learning node pool(s).
    Use mixed mode clusters only after you have a solid understanding of the performance
    impact that your machine learning workloads have on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving linear scaling with distributed training
  prefs: []
  type: TYPE_NORMAL
- en: This is the holy grail of distributed model training. Most libraries unfortunately
    don’t scale linearly when distributed. A lot of work is being done to make scaling
    better, but it’s important to understand the costs because this isn’t as simple
    as throwing more hardware at the problem. In our experience, it’s almost always
    the model itself and not the infrastructure supporting it that is the source of
    the bottleneck. It is, however, important to review the utilization of the GPU,
    CPU, network, and storage before pointing fingers at the model. Open source tools
    such as [Horovod](https://oreil.ly/3NMtg) seek to improve distributed training
    frameworks and provide better model scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve covered a lot of ground in this chapter and hopefully have provided valuable
    insight into why Kubernetes is a great platform for machine learning, especially
    deep learning, and the considerations you need to be aware of before deploying
    your first machine learning workload. If you exercise the recommendations in this
    chapter, you will be well equipped to build and maintain a Kubernetes cluster
    for these specialized workloads.
  prefs: []
  type: TYPE_NORMAL
