<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Exposing Services to External Clients"><div class="chapter" id="exposing_services_to_external_clients">
    <h1><span class="label">Chapter 9. </span>Exposing Services to External Clients</h1>
    <p>In earlier chapters we explored how network policy is one of the primary tools for securing Kubernetes.<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="about" id="idm45326825570560"/><a contenteditable="false" data-type="indexterm" data-primary="clients connecting to services" data-see="services exposed to external clients" id="idm45326825569168"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="exposing services to external clients" data-tertiary="about" id="idm45326825567760"/><a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="exposing services to external clients" data-tertiary="about" id="idm45326825566096"/> This is true for
        both pod-to-pod traffic within the cluster (east-west traffic) and for traffic between pods and external
        entities outside of the cluster (north-south traffic). For all of these traffic types, the best practice is the
        same: Use network policy to limit which network connections are allowed to the narrowest scope needed, so the
        only connections that are allowed are the ones you expect and need for your specific applications or
        microservices to work.</p>
    <p>In the case of pods that need to be accessed by external clients outside of the cluster, this means restricting
        connections:</p>
    <ul>
        <li>
            <p>To the specific port(s) that the corresponding microservice is expecting incoming connections to</p>
        </li>
        <li>
            <p>From the specific clients that need to connect to the microservice</p>
        </li>
    </ul>
    <p>It’s not uncommon for a particular microservice to be consumed just within the enterprise (whether on-prem or in
        a public cloud) by a limited number of clients. In this case the Kubernetes network policy rules ideally should
        limit incoming connections to just the IP addresses, or IP address range, associated with the clients. Even if a
        microservice is being exposed to the public internet (for example, exposing the frontend microservices for a
        publicly accessible SaaS or website), there are still cases where access may need to be restricted to some
        extent. For example, it may be a requirement to block access from certain geographies for compliance reasons, or
        it may be desirable to block access from known bad actors or threat feeds.</p>
    <p>Unfortunately, how you go about implementing this best practice needs to include the consideration of which
        network plug-ins and Kubernetes primitives are used to expose the microservice outside the cluster. In
        particular, in some cases the original client source IP address is preserved all the way to the pod, which
        allows Kubernetes network policies to easily limit access to specific clients. In other cases the client source
        IP gets obscured by network address translation (NAT) associated with network load balancing, or by connection
        termination associated with application layer load balancing.</p>
    <p>In this chapter we will explore different client source IP behaviors available across the three main options for
        exposing an application or microservice outside of the cluster: direct pod connections, Kubernetes services, and
        Kubernetes Ingress.</p>
    <section data-type="sect1" data-pdf-bookmark="Understanding Direct Pod Connections"><div class="sect1" id="understanding_direct_pod_connections">
        <h1>Understanding Direct Pod Connections</h1>
        <p>It’s relatively uncommon for pods to be directly accessed by clients<a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="exposing services to external clients" data-tertiary="direct pod connections" id="idm45326825556064"/><a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="exposing services to external clients" data-tertiary="direct pod connections" id="idm45326825554272"/><a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="direct pod connections" id="idm45326825552608"/><a contenteditable="false" data-type="indexterm" data-primary="direct pod connections" id="idm45326825551216"/> outside
            of the cluster rather than being accessed via a Kubernetes service or Kubernetes Ingress. However, there are
            scenarios where this may be desired or required. For example, some types of distributed data stores may
            require multiple pods, each with specific IP addresses that can be configured for data distribution or
            clients to peer with.</p>
        <p>Supporting direct connections to pod IP addresses from outside of the cluster <a contenteditable="false" data-type="indexterm" data-primary="plug-ins" data-secondary="direct pod connections" id="idm45326825549152"/><a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="direct pod connections" id="idm45326825547776"/>requires a pod network that makes pod IP addresses routable
            beyond the boundary of the cluster. This typically means using one of the following:</p>
        <ul>
            <li>
                <p>A cloud provider network plug-in in public cloud clusters (e.g., the Amazon VPC CNI plug-in, as used
                    by default in EKS)</p>
            </li>
            <li>
                <p>A network plug-in that can use BGP to integrate with an on-prem enterprise network (e.g.,
                    Kube-router, Calico CNI plug-in).</p>
            </li>
        </ul>
        <p>In addition to the underlying networking supporting the connectivity, the clients need a way of finding out
            the pod IP addresses. This may be done via DNS, explicit configuration of the client, or some other
            third-party service discovery mechanism.</p>
        <p>From a security point of view, connections from clients directly to pods are straightforward: They have the
            original client source IP address in place all the way to the pod, which means network policy can easily be
            used to restrict access to clients with particular IP addresses or from particular IP address ranges.</p>
        <p>Note though that in any cluster where pod IP addresses are routable beyond the boundary of the cluster, it becomes
            even more important to ensure network policy best practices are followed. Without network policy in place,
            pods that should only be receiving east-west connections could be accessed from outside of the cluster
            without the need for configuring a corresponding externally accessible Kubernetes service type or Kubernetes
            Ingress.</p>
    </div></section>
    <section data-type="sect1" data-pdf-bookmark="Understanding Kubernetes Services"><div class="sect1" id="understanding_kubernetes_services">
        <h1>Understanding Kubernetes Services</h1>
        <p>Kubernetes services provide a convenient mechanism for accessing<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="about" id="idm45326825539200"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="exposing services to external clients" data-tertiary="about Kubernetes services" id="idm45326825537408"/><a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="exposing services to external clients" data-tertiary="about Kubernetes services" id="idm45326825535728"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="services to external clients" data-see="services exposed to external clients" id="idm45326825534048"/><a contenteditable="false" data-type="indexterm" data-primary="kube-proxy" data-secondary="Kubernetes services implemented by" id="idm45326825532368"/> pods from outside of the cluster using
            services of type NodePort or LoadBalancer or by explicitly configuring an External IP for the service. By
            default, Kubernetes services are implemented by kube-proxy. Kube-proxy runs on every node in the cluster and
            is responsible for intercepting connections to Kubernetes services and load-balancing them across the pods
            backing the corresponding service. This connection handling has a well-defined behavior for when source IP
            addresses are preserved and when they are not, which we will look at now for each service type.</p>
        <section data-type="sect2" data-pdf-bookmark="Cluster IP Services"><div class="sect2" id="cluster_ip_services">
            <h2>Cluster IP Services</h2>
            <p>Before we dig into exposing pods to external clients using Kubernetes services,<a contenteditable="false" data-type="indexterm" data-primary="network policies" data-secondary="Cluster IP services and" id="idm45326825528080"/><a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="Cluster IP services" id="idm45326825526704"/><a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="cluster IP services" id="idm45326825525328"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="Cluster IP services" id="idm45326825523712"/> it is worth understanding how Kubernetes services
                behave for connections originating from inside the cluster. The primary mechanism for service discovery
                and load balancing of these connections within the cluster (i.e., pod-to-pod connectivity) makes use of Kubernetes 
                services of type Cluster IP. <a contenteditable="false" data-type="indexterm" data-primary="destination network address translation (DNAT)" id="idm45326825521840"/><a contenteditable="false" data-type="indexterm" data-primary="network address translation (NAT)" data-secondary="destination network address translation" id="idm45326825520640"/><a contenteditable="false" data-type="indexterm" data-primary="network address translation (NAT)" data-secondary="destination network address translation" data-tertiary="Cluster IP services" id="idm45326825519232"/>For Cluster IP services, kube-proxy is able to use
                destination network address translation (DNAT) to map connections to the service’s Cluster IP to the
                pods backing the service. This mapping is reversed for any return packets on the connection. The mapping
                is done without changing the source IP address, as illustrated in <a data-type="xref" href="#network_path_for_a_kubernetes_service">Figure 9-1</a>.</p>
            <figure><div id="network_path_for_a_kubernetes_service" class="figure">
                <img src="Images/ksao_0901.png" alt="" width="1258" height="368"/>
                <h6><span class="label">Figure 9-1. </span>Network path for a Kubernetes service advertising Cluster IP</h6>
            </div></figure>
            <p>Importantly, the destination pod sees the connection has originated from the IP address of the client
                pod. <a contenteditable="false" data-type="indexterm" data-primary="network policies" data-secondary="pod labels for identification" id="idm45326825513664"/>This means that any network policy applied
                to the destination pod behaves as expected and is not impacted by the fact that the connection was load
                balanced via the service’s Cluster IP. In addition, any network policy egress rules that apply to the
                client pod are evaluated after the mapping from Cluster IP to destination pod has happened. This means
                that network policy applied to the client pod also behaves as expected, independent of the fact that the
                connection was load balanced via the service’s cluster IP. (As a reminder, network policy rules match on
                pod labels, not on service labels.)</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Node Port Services"><div class="sect2" id="node_port_services">
            <h2>Node Port Services</h2>
            <p>The most basic way to access a service from outside the cluster<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="node port services" id="idm45326825509312"/><a contenteditable="false" data-type="indexterm" data-primary="node port services" id="idm45326825507616"/><a contenteditable="false" data-type="indexterm" data-primary="kube-proxy" data-secondary="load balancing" id="idm45326825506512"/><a contenteditable="false" data-type="indexterm" data-primary="load balancer services" data-secondary="kube-proxy load balancing" id="idm45326825505136"/><a contenteditable="false" data-type="indexterm" data-primary="NAT" data-see="network address translation" id="idm45326825503744"/><a contenteditable="false" data-type="indexterm" data-primary="network address translation (NAT)" data-secondary="node port services" id="idm45326825502352"/><a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="node port services" id="idm45326825500960"/> is to use a Kubernetes service of type NodePort. A node port is
                a port reserved on each node in the cluster through which the service can be accessed. In a typical
                Kubernetes deployment, kube-proxy is responsible for intercepting connections to node ports and
                load-balancing them across the pods backing each service.</p>
            <p>As part of this process, NAT is used to map the destination IP address and port from the node IP and node
                port to the chosen backing pod and service port. However, unlike connections to cluster IPs, where the
                NAT maps only the destination IP address, in the case of node ports the source IP address is also mapped
                from the client IP to the node IP.</p>
            <p>If the source IP address was not mapped in this way, then any response packets on the connection would
                flow directly back to the external client, bypassing the ability for kube-proxy on the original ingress
                node to reverse the mapping of the destination IP address. (It’s the node that performed the NAT that
                has the connection tracking state needed to reverse the NAT.) As a result, the external client would
                drop the packets because it would not recognize them as being part of the connection it made to the node
                port on the original ingress node.</p>
            <p>The process is illustrated in <a data-type="xref" href="#network_path_for_a_kubernetes_service_u">Figure 9-2</a>.</p>
            <figure><div id="network_path_for_a_kubernetes_service_u" class="figure">
                <img src="Images/ksao_0902.png" alt="" width="1077" height="368"/>
                <h6><span class="label">Figure 9-2. </span>Network path for a Kubernetes service using node ports</h6>
            </div></figure>
            <p>Since the NAT changes the source IP address, any network policy that applies<a contenteditable="false" data-type="indexterm" data-primary="network policies" data-secondary="node port services and" id="idm45326825493200"/> to the destination pod cannot match on the
                original client IP address. Typically this <span class="keep-together">means that</span> any such policy is limited to restricting the
                destination protocol and port and cannot restrict based on the external client’s IP address. This in
                turn means the best practice of limiting access to the specific clients that need to connect to the
                microservice cannot easily be implemented with Kubernetes network policy in this <span class="keep-together">configuration.</span></p>
            <p class="pagebreak-before">Fortunately, there are a number of solutions that can be used to circumvent the limitations of this
                default behavior of node ports:</p>
            <ul>
                <li>
                    <p>Configuring the service with <code>externalTrafficPolicy:local</code></p>
                </li>
                <li>
                    <p>Using a network plug-in that supports node-port-aware network policy <span class="keep-together">extensions</span></p>
                </li>
                <li>
                    <p>Using an alternative implementation for service load balancing in place of kube-proxy that
                        preserves client source IP addresses</p>
                </li>
            </ul>
            <p>We will cover each of these later in this chapter. But before that, to complete our picture of how the
                default behavior of mainline Kubernetes services work, let’s look at services of type LoadBalancer.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Load Balancer Services"><div class="sect2" id="load_balancer_services">
            <h2>Load Balancer Services</h2>
            <p>Services of type LoadBalancer build on the behavior of node ports by<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="load balancer services" id="idm45326825482128"/><a contenteditable="false" data-type="indexterm" data-primary="load balancer services" id="idm45326825480336"/><a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="load balancer services" id="idm45326825479232"/> integrating with external network load balancers.
                The exact type of network load balancer depends on which public cloud provider, or if on-prem, which
                specific hardware load balancer integration, is integrated with your cluster.</p>
            <p>The service can be accessed from outside of the cluster via a specific IP address on the network load
                balancer, which by default will load-balance evenly across the nodes to the service’s node port.</p>
            <p>Most network load balancers are located at a point in the network where return traffic on a connection
                will always be routed via the network load balancer, and therefore they can implement their load
                balancing using only DNAT, leaving the client source IP address unaltered by the network load balancer,
                as illustrated in <a data-type="xref" href="#network_path_for_a_kubernetes_service_o">Figure 9-3</a>.</p>
            <figure><div id="network_path_for_a_kubernetes_service_o" class="figure">
                <img src="Images/ksao_0903.png" alt="" width="1445" height="368"/>
                <h6><span class="label">Figure 9-3. </span>Network path for a Kubernetes service of type LoadBalancer</h6>
            </div></figure>
            <p class="pagebreak-before">However, because the network load balancer is load-balancing to the service’s node port, and kube-proxy’s
                default node port behavior changes the source IP address as part of its load balancing implementation,
                the destination pod still cannot match on the original client source IP address. Just like with vanilla
                node port services, this in turn means the best practice of limiting access to the specific clients that
                need to connect to the microservice cannot easily be implemented with Kubernetes network policy in this
                configuration.</p>
            <p>Fortunately, the same solutions that can be used to circumvent the limitations of the default behavior of
                services of type NodePort can be used in conjunction with services of type LoadBalancer:</p>
            <ul>
                <li>
                    <p>Configuring the service with <code>externalTrafficPolicy:local</code></p>
                </li>
                <li>
                    <p>Using a network plug-in that supports node-port-aware network policy <span class="keep-together">extensions</span></p>
                </li>
                <li>
                    <p>Using an alternative implementation for service load balancing in place of kube-proxy that
                        preserves client source IP addresses</p>
                </li>
            </ul>
            <p>Let’s look at each of those now.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="externalTrafficPolicy:local"><div class="sect2" id="externaltrafficpolicylocal">
            <h2>externalTrafficPolicy:local</h2>
            <p>By default, the node port associated with a service is available<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="externalTrafficPolicy:local" id="idm45326825464496"/><a contenteditable="false" data-type="indexterm" data-primary="externalTrafficPolicy:local" id="idm45326825462688"/><a contenteditable="false" data-type="indexterm" data-primary="load balancer services" data-secondary="externalTrafficPolicy:local" id="idm45326825461568"/><a contenteditable="false" data-type="indexterm" data-primary="policies" data-secondary="externalTrafficPolicy:local" id="idm45326825460176"/> on every node in the cluster, and services
                of type LoadBalancer load-balance to the service’s node port evenly across all of the nodes, independent
                of which nodes may actually be hosting backing pods for the service. This behavior can be changed by
                configuring the service with <code>externalTrafficPolicy:local</code>, which specifies that connections should only
                be load balanced to pods backing the service on the local node.</p>
            <p>When combined with services of type LoadBalancer, connections to the service are only directed to nodes
                that host at least one pod backing the service. This reduces the potential extra network hop between
                nodes associated with kube-proxy’s normal node port handling. Perhaps more importantly, since each
                node’s kube-proxy is only load-balancing to pods on the same node, kube-proxy does not need to perform
                source network address translation as part of the load balancing, meaning that the client source IP
                address is preserved all the way to the pod. (As a reminder, kube-proxy’s default handling of node ports
                on the ingress node normally needs to NAT the source IP address so that return traffic flows back via
                the original ingress node, since that is the node that has the required traffic state to reverse the
                NAT.)</p>
            <p>Network flow is illustrated in <a data-type="xref" href="#network_path_for_a_kubernetes_service_l">Figure 9-4</a>.</p>
                    <figure><div id="network_path_for_a_kubernetes_service_l" class="figure">
                <img src="Images/ksao_0904.png" alt="" width="1445" height="368"/>
                <h6><span class="label">Figure 9-4. </span>Network path for a Kubernetes service leveraging the optimization to route to the node
                    backing the pod</h6>
            </div></figure>
                    <p>As the original client source IP address is preserved all the way to the backing pod, network policy
                applied to the backing pod is now able to restrict access to the service to only the specific client IP
                addresses or address ranges that need to be able to access the service.</p>
            
            
            <p>Note that not all load balancers support this mode of operation. So it is important to check whether this
                is supported by the specific public cloud provider, or if on-prem, the specific hardware load balancer
                integration, that is integrated with your cluster. The good news is that most of the large public
                providers do support this mode. Some load balancers can even go a step further, bypassing kube-proxy and
                load-balancing directly to the backing pods without using the node port.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Network Policy Extensions"><div class="sect2" id="network_policy_extensions">
            <h2>Network Policy Extensions</h2>
            <p>Some Kubernetes network plug-ins provide extensions to the standard<a contenteditable="false" data-type="indexterm" data-primary="load balancer services" data-secondary="network policy extensions" id="idm45326825449184"/><a contenteditable="false" data-type="indexterm" data-primary="plug-ins" data-secondary="network policy extensions" id="idm45326825447664"/><a contenteditable="false" data-type="indexterm" data-primary="network policies" data-secondary="network policy extensions" id="idm45326825446272"/><a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="network policy extensions" id="idm45326825444880"/> Kubernetes
                network policy capabilities, which can be used to help secure access to services from outside of the
                cluster.</p>
            <p>There are many solutions that provide network policy extensions (e.g., Weave Net, Kuberouter, Calico). <a contenteditable="false" data-type="indexterm" data-primary="Calico Enterprise" data-secondary="network policies" data-tertiary="network policy extensions" id="idm45326825442368"/>Let’s look at
                Calico once again, as it’s our area of expertise. Calico includes support for host endpoints, which
                allow network policies to be applied to the nodes within a cluster, not just pods within the cluster.
                Whereas standard Kubernetes network policy can be thought of as providing a virtual firewall within the
                pod network in front of every pod, Calico’s host endpoint extensions can be thought of as providing a
                virtual firewall in front of every node/host, as illustrated in <a data-type="xref" href="#virtual_firewall_using_host_endpoint_pr">Figure 9-5</a>.</p>
            <p>In addition, Calico’s network policy extensions support the ability to specify whether the policy rules
                applied to host endpoints apply before or after the NAT associated with kube-proxy’s load balancing.
                This means that they can be used to limit which clients can connect to specific node ports, unencumbered
                by whatever load-balancing decisions kube-proxy may be about to make.</p>
            <figure><div id="virtual_firewall_using_host_endpoint_pr" class="figure">
                <img src="Images/ksao_0905.png" alt="" width="1182" height="475"/>
                <h6><span class="label">Figure 9-5. </span>Virtual firewall using host endpoint protection</h6>
            </div></figure>
            
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Alternatives to kube-proxy"><div class="sect2" id="alternatives_to_kube_proxy">
            <h2>Alternatives to kube-proxy</h2>
            <p>Kube-proxy provides the default implementation for Kubernetes services<a contenteditable="false" data-type="indexterm" data-primary="kube-proxy" data-secondary="about" id="idm45326825434016"/><a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="kube-proxy alternatives" id="idm45326825432560"/><a contenteditable="false" data-type="indexterm" data-primary="kube-proxy" data-secondary="alternatives to" id="idm45326825430848"/><a contenteditable="false" data-type="indexterm" data-primary="plug-ins" data-secondary="kube-proxy alternatives" id="idm45326825429472"/> and is included as
                standard in most clusters. However, some network plug-ins provide alternative implementations of
                Kubernetes services to replace kube-proxy.</p>
            <p>For some network plug-ins, this alternative implementation is necessary because the particular way the
                plug-in implements pod networking is not compatible with kube-proxy’s dataplane (which uses the standard
                Linux networking pipeline controlled by iptables and/or IPVS). For other network plug-ins, the
                alternative implementation is optional. For example, a CNI that implements a Linux eBPF dataplane will
                choose to replace kube-proxy in favor of its native service implementation.</p>
            <p>Some of these alternative implementations provide additional capabilities beyond kube-proxy’s behavior.
                One such additional capability that is relevant from a security perspective is the ability to preserve
                the client source IP address all the way to the back pods when load-balancing from external clients.</p>
            <p>For example, <a data-type="xref" href="#network_path_for_a_kubernetes_servic">Figure 9-6</a> illustrates
                how an eBPF-based dataplane implements this behavior.</p>
            <figure><div id="network_path_for_a_kubernetes_servic" class="figure">
                <img src="Images/ksao_0906.png" alt="" width="1423" height="368"/>
                <h6><span class="label">Figure 9-6. </span>Network path for a Kubernetes service with an eBPF-based implementation</h6>
            </div></figure>
            <p>This allows the original client source IP address to be preserved all the way to the packing pod for
                services of type NodePort or LoadBalancer, without requiring support for <code>externalTrafficPolicy:local</code> in
                network load balancers or node selection for node ports. This in turn means that network policy applied
                to the backing pod is able to restrict access to the service to only the specific clients, IP addresses,
                or address ranges that need to be able to access the service.</p>
            <p>Beyond the security considerations, these alternative Kubernetes services implementations (e.g.,
                eBPF-based dataplanes) provide other advantages over kube-proxy’s implementation, such as:</p>
            <ul>
                <li>
                    <p>Improved performance when running with very high numbers of services, including reduced first
                        packet latencies and reduced control plane CPU usage</p>
                </li>
                <li>
                    <p>Direct server return (DSR), which reduces the number of network hops for return traffic</p>
                </li>
            </ul>
            <p>We will look at DSR more closely next, since it does have some security implications.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Direct Server Return"><div class="sect2" id="direct_server_return">
            <h2>Direct Server Return</h2>
            <p>DSR allows the return traffic from the destination pod to flow directly<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="direct server return" id="idm45326825415872"/><a contenteditable="false" data-type="indexterm" data-primary="direct server return (DSR)" id="idm45326825413984"/>
                back to the client rather than going via the original ingress node. There are several network plug-ins
                that are able to replace kube-proxy’s service handling with their own implementations that support DSR.
                For example, a eBPF dataplane that includes native service handling and (optionally) can use DSR for
                return traffic is illustrated in <a data-type="xref" href="#network_path_for_a_kubernetes_service_w">Figure 9-7</a>.</p>
            <figure><div id="network_path_for_a_kubernetes_service_w" class="figure">
                <img src="Images/ksao_0907.png" alt="" width="1423" height="576"/>
                <h6><span class="label">Figure 9-7. </span>Network path for a Kubernetes service with direct server return</h6>
            </div></figure>
            <p class="pagebreak-before">Eliminating one network hop for the return traffic reduces:</p>
            <ul>
                <li>
                    <p>The overall latency for the service (since every network hop introduces latency)</p>
                </li>
                <li>
                    <p>The CPU load on the original ingress node (since it is no longer dealing with return traffic)</p>
                </li>
                <li>
                    <p>The east-west network traffic within the cluster</p>
                </li>
            </ul>
            <p>For particularly network-intensive or latency-sensitive applications, this can be a big win. However,
                there are also security implications of DSR. <a contenteditable="false" data-type="indexterm" data-primary="reverse path filtering (RPF)" id="idm45326825404416"/><a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="reverse path filtering" id="idm45326825403296"/>In particular, the underlying network may need to
                be configured with fairly relaxed reverse path filtering (RPF) settings.</p>
            <p>RPF is a network routing mechanism that blocks any traffic from a particular source IP address where
                there is not a corresponding route to that IP address over the same link. That is, if the router doesn’t
                have a route that says it can send traffic to a particular IP address over the network link, then it
                will not allow traffic from that IP address over the network link. RPF makes it harder for attackers to
                “spoof” IP addresses—i.e., pretend to be a different IP address than what the device has been allocated.
            </p>
            <p>In the context of DSR and Kubernetes services, <a data-type="xref" href="#network_path_for_a_kubernetes_service_w">Figure 9-7</a>
                illustrates a few key points:</p>
            <ul>
                <li>
                    <p>If the service is being accessed via a node port on Node 1, then the return traffic from Node 2
                        will have the source IP address of Node 1. So the underlying network must be configured with
                        relaxed RPF settings, otherwise the network will filter out the return traffic because the
                        network would not normally route traffic to Node 1 via the network link to Node 2.</p>
                </li>
                <li>
                    <p>If the service is being accessed via service IP advertisement (e.g., sending traffic directly to
                        a service’s cluster IP, external IP, or load balancer IP), then the return traffic from Node 2
                        will have the source IP address of the service IP. In this case, no relaxation of RPF is
                        required, since the service IP should be advertised from all nodes in the cluster, meaning the
                        network will have routes to the service IP via all nodes. We’ll cover service IP advertising in
                        more detail later in this chapter.</p>
                </li>
            </ul>
            <p>As explained earlier, DSR is an excellent optimization that you can use, but you need to review your use
                case and ensure that you are comfortable with disabling the RPF check.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Limiting Service External IPs"><div class="sect2" id="limiting_service_external_ips">
            <h2>Limiting Service External IPs</h2>
            <p>So far in this chapter we have focused on how service types and<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="limiting service external IPs" id="idm45326825393296"/><a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="limiting service external IPs" id="idm45326825391488"/> implementations impact how network policy
                can be used to restrict access to services to only the specific client IP addresses or address ranges
                that need to be able to access each service.</p>
            <p>Another important security consideration is the power associated with users who have permissions to
                create or configure Kubernetes services. In particular, any user who has RBAC permissions to modify a
                Kubernetes service effectively has control over which pods that service is load balanced to. If used
                maliciously, this could mean the user is able to divert traffic that was intended for a particular
                microservice to their own malicious pods.</p>
            <p>As Kubernetes services are namespaces resources, this rarely equates to a genuine security issue for
                mainline service capabilities. For example, a user who has been granted permissions to define services
                in a particular namespace will typically also have permission to modify pods in that namespace. So for
                standard service capabilities such as handling of cluster IPs, node ports, or load balancers, the
                permissions to define and modify services in the namespace doesn’t really represent any more trust than
                having permissions to define or modify pods in the namespace.</p>
            <p>There is one notable exception, though, which is the ability to specify external IPs for services. The
                externalIP field in a service definition allows the user to associate an arbitrary IP address with the
                service. Connections to this IP address received on any node in the cluster are load balanced to the
                pods backing the service.</p>
            <p>The normal use case is to provide an IP-oriented alternative to node ports that can be used by external
                clients to connect to a service. This use case usually requires special handling within the underlying
                network in order to route connections to the external IP to the nodes in the cluster. This may be
                achieved by programming static routes into the underlying network, or in a BGP-capable network, using
                BGP to dynamically advertise the external IP. (See the next section for more details on advertising
                service IP addresses.)</p>
            <p>Like the mainline service capabilities, this use case is relatively benign in terms of the level of trust
                for users. It allows them to offer an additional way to reach the pods in the namespaces they have
                permission to manage, but does not interfere with traffic destined to pods in other namespaces.</p>
            <p>However, just like with node ports, connections from pods to external IPs are also intercepted and load
                balanced to the service backing pods. As Kubernetes does not police or attempt to provide any level of
                validation on external IP addresses, this means a malicious user can effectively intercept traffic to
                any IP address, without any namespace or other scope restrictions. This is an extremely powerful tool
                for a malicious user and represents a correspondingly large security risk.</p>
            <p>If you are following the best practice of having default deny–style policies for both ingress and egress
                traffic that apply across all pods in the cluster, then this significantly hampers the malicious user’s
                attempt to get access to traffic that should have been between two other pods. However, although the
                network policy will stop them from accessing the traffic, it doesn’t stop the service load balancing
                from diverting the traffic from its intended destination, which means that the malicious user can
                effectively block traffic between any two pods even though they cannot receive the traffic themselves.
            </p>
            <p>So in addition to following network policy best practices, it is recommended to use an admission
                controller to restrict which users can specify or modify the externalIP field. For users who are allowed
                to specify external IP addresses, it may also be desirable to restrict the IP address values to a
                specific IP address range that is deemed safe (i.e., a range that is not being used for any other
                purpose). For more discussion of admission controllers, see <a data-type="xref" href="ch08.xhtml#managing_trust_across_teams">Chapter 8</a>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Advertising Service IPs"><div class="sect2" id="advertising_service_ips">
            <h2>Advertising Service IPs</h2>
            <p>One alternative to using node ports or network load balancers is to <a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="advertising service IPs" id="idm45326825379328"/><a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="advertising service IPs" id="idm45326825377600"/>advertise service IP addresses over BGP. This
                requires the cluster to be running on an underlying network that supports BGP, which typically means an
                on-prem deployment with standard top-of-rack routers.</p>
            <p>For example, Calico supports advertising the service<a contenteditable="false" data-type="indexterm" data-primary="Calico Enterprise" data-secondary="advertising service IPs" id="idm45326825375328"/> clusterIP,
                loadBalancerIP, or externalIP for services configured with one. If you are not using Calico as your
                network plug-in, then MetalLB provides similar capabilities that work with a variety of different
                network plug-ins.</p>
            <p>Advertising service IPs effectively allows the underlying network routers to act as load balancers,
                without the need for an actual network load balancer.</p>
            <p>The security considerations for advertising service IPs are equivalent to those of normal node port– or
                load balancer–based services discussed earlier in this chapter. When using kube-proxy, the original
                client IP address is obscured by default, as illustrated in <a data-type="xref" href="#network_path_for_a_kubernetes_servi">Figure 9-8</a>.</p>
            <figure><div id="network_path_for_a_kubernetes_servi" class="figure">
                <img src="Images/ksao_0908.png" alt="" width="1423" height="368"/>
                <h6><span class="label">Figure 9-8. </span>Network path for a Kubernetes service advertising Cluster IP via BGP</h6>
            </div></figure>
            <p class="pagebreak-before">This behavior can be changed using <code>externalTrafficPolicy:local</code>, which (at the time of writing) is
                supported by kube-proxy for both loadBalancerIP and externalIP addresses but not clusterIP addresses.
                However, it should be noted that when using <code>externalTrafficPolicy:local</code>, the evenness of the load
                balancing becomes topology-dependent. To circumvent this, pod anti-affinity rules can be used to ensure
                even distribution of backing pods across your topology, but this does add some complexity to deploying
                the service.</p>
            <p>Alternatively, a network plug-in with native service handling (replacing kube-proxy) that supports source
                IP address preservation can be used. This combination can be very appealing for on-prem deployments due
                to its operational simplicity and removal of the need to build network load balancer appliances into the
                network <span class="keep-together">topology.</span></p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Understanding Kubernetes Ingress"><div class="sect2" id="understanding_kubernetes_ingress">
            <h2>Understanding Kubernetes Ingress</h2>
            <p>Kubernetes Ingress builds on top of Kubernetes services to provide<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="Kubernetes Ingress" id="ch09-ing"/><a contenteditable="false" data-type="indexterm" data-primary="Ingress" id="ch09-ing2"/> load
                balancing at the application layer, mapping HTTP and HTTPS requests with particular domains or URLs to
                Kubernetes services. Kubernetes Ingress can be a convenient way of exposing multiple microservices via a
                single external point of contact, if for example multiple microservices make up a single web
                application. In addition, they can be used to terminate SSL/TLS (for receiving HTTPS encrypted
                connections from external clients) before load balancing to the backing microservices.</p>
            <p>The details of how Kubernetes Ingress is implemented depend on which Ingress Controller you are using.
                The Ingress Controller is responsible for monitoring Kubernetes Ingress resources and
                provisioning/configuring one or more ingress load balancers to implement the desired load balancing
                behavior.</p>
            <p>Unlike Kubernetes services, which are handled at the network layer (L3–L4), ingress load balancers
                operate at the application layer (L5–L7). Incoming connections are terminated at the load balancer so it
                can inspect the individual HTTP/HTTPS requests. The requests are then forwarded via separate connections
                from the load balancer to the chosen service. As a result, network policy applied to the pods backing a
                service sees the ingress load balancer as the client source IP address, rather than the original
                external client IP address. This means they can restrict access to only allow connections from the load
                balancer, but cannot restrict access to specific external <span class="keep-together">clients.</span></p>
            <p>To restrict access to specific external clients, the access control needs to be enforced either within
                the application load balancer or in front of the application load balancer. In case you choose an
                IP-based access control, it needs to happen before the traffic is forwarded to the backing services. How
                you do this depends on the specific Ingress Controller you are using.</p>
            <p>Broadly speaking, there are two types of ingress solutions:</p>
            <dl>
                <dt>In-cluster ingress</dt>
                <dd>Ingress load balancing is performed by pods within the cluster itself.</dd>
                <dt>External ingress</dt>
                <dd>Ingress load balancing is implemented outside of the cluster by appliances or cloud provider capabilities.</dd>
            </dl>
            <p>Now that we have covered Kubernetes Ingress, let’s review ingress solutions.</p>
            <section data-type="sect3" data-pdf-bookmark="In-cluster ingress solutions"><div class="sect3" id="in_cluster_ingress_solutions">
                <h3>In-cluster ingress solutions</h3>
                <p>In-cluster ingress solutions use software application load balancers running in pods within the
                    cluster itself. There are many different Ingress Controllers that follow this pattern. For example,
                    the NGINX Ingress Controller instantiates and configures NGINX pods to act as application load
                    balancers.</p>
                <p>The advantages of in-cluster ingress solutions are that:</p>
                <ul>
                    <li>
                        <p>You can horizontally scale your Ingress solution up to the limits of Kubernetes.</p>
                    </li>
                    <li>
                        <p>There are many different in-cluster Ingress Controller solutions, so you can choose the
                            Ingress Controller that best suits your specific needs— for example, with particular load
                            balancing algorithms, security options, or observability <span class="keep-together">capabilities.</span></p>
                    </li>
                </ul>
                <p>To get your ingress traffic to the in-cluster Ingress pods, the Ingress pods are normally exposed
                    externally as a Kubernetes service, as illustrated in <a data-type="xref" href="#an_example_of_an_in_cluster_ingress_imp">Figure 9-9</a>.
                </p>
                <figure><div id="an_example_of_an_in_cluster_ingress_imp" class="figure">
                    <img src="Images/ksao_0909.png" alt="" width="1097" height="216"/>
                    <h6><span class="label">Figure 9-9. </span>An example of an in-cluster ingress implementation in a Kubernetes cluster</h6>
                </div></figure>
                <p>This means you can use any of the standard ways of accessing the service from outside of the cluster.
                    One common approach is to use an external network load balancer or service IP advertisement, along
                    with one of the following:</p>
                <ul>
                    <li>
                        <p>A network plug-in with native Kubernetes service handling that always preserves the original
                            client source IP</p>
                    </li>
                    <li>
                        <p><code>externalTrafficPolicy:local</code> (and pod anti-affinity rules to ensure even load balancing across
                            the ingress pods) to preserve the original client source IP</p>
                    </li>
                </ul>
                <p>Network policy applied to the ingress pods can then restrict access to specific external clients as
                    described earlier in this chapter, and the pods backing any microservices being exposed via Ingress
                    can restrict connections to just those from the ingress pods.</p>
            </div></section>
            <section data-type="sect3" data-pdf-bookmark="External ingress solutions"><div class="sect3" id="external_ingress_solutions">
                <h3>External ingress solutions</h3>
                <p>External ingress solutions use application load balancers outside<a contenteditable="false" data-type="indexterm" data-primary="services exposed to external clients" data-secondary="Kubernetes services" data-tertiary="external ingress solutions" id="idm45326825336592"/><a contenteditable="false" data-type="indexterm" data-primary="Ingress" data-secondary="external ingress solutions" id="idm45326825334784"/> of the cluster, as illustrated in <a data-type="xref" href="#an_example_of_an_external_ingress_in_a">Figure 9-10</a>.</p>
                <figure><div id="an_example_of_an_external_ingress_in_a" class="figure">
                    <img src="Images/ksao_0910.png" alt="" width="1169" height="191"/>
                    <h6><span class="label">Figure 9-10. </span>An example of an external ingress in a Kubernetes cluster</h6>
                </div></figure>
                <p>The exact details and features depend on which Ingress Controller you are using. Most public cloud
                    providers have their own Ingress Controllers that automate the provisioning and management of the
                    cloud provider’s application load balancers to provide ingress.</p>
                <p>Most application load balancers support a basic operation mode of forwarding traffic to the chosen
                    service backing pods via the node port of the corresponding service. In addition to this basic
                    approach of load balancing to service node ports, some cloud providers support a second mode of
                    application load balancing that load-balances directly to the pods backing each service, without
                    going via node ports or other kube-proxy service handling. This has the advantage of eliminating the
                    potential second network hop associated with node ports load-balancing to a pod on a different node.
                </p>
                <p>The main advantage of an external ingress solution is that the cloud provider handles the operational
                    complexity of the ingress for you. The potential downsides are as <span class="keep-together">follows:</span></p>
                <ul>
                    <li>
                        <p>The set of features available is usually more limited, compared with the rich range of
                            available in-cluster ingress solutions. For example, if you require a specific load
                            balancing algorithm, security controls, or observability capabilities, these may or may not
                            be supported by the cloud provider’s implementation.</p>
                    </li>
                    <li>
                        <p>The maximum supported number of services (and potentially the number of pods backing the
                            services) is constrained by cloud provider–specific limits. For example, if you are
                            operating at very high scales, with hundreds of pods backing a service, you may exceed the
                            application layer load balancer’s maximum limit of IPs it can load balance to in this mode.
                            In this case switching to an in-cluster ingress solution is likely the better fit for you.
                        </p>
                    </li>
                    <li>
                        <p>Since the application load balancer is not hosted within the Kubernetes cluster, if you need
                            to restrict access to specific external clients, you cannot use Kubernetes network policy
                            and instead must use the cloud provider’s specific mechanisms. It is still possible to
                            follow the best practices laid out at the start of this chapter, but doing so will be cloud
                            provider–specific and will likely introduce a little additional operational complexity,
                            compared with being able to use native Kubernetes capabilities independent of the cloud
                            provider’s capabilities and APIs.</p>
                    </li>
                </ul>
                <p>In this section we covered how Kubernetes ingress works and the solutions available. We recommend you
                    review the sections and decide if the in-cluster ingress solution works for you or if you should go
                    with an external ingress solution.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch09-ing" id="idm45326825321536"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch09-ing2" id="idm45326825320160"/></p>
            </div></section>
        </div></section>
    </div></section>
    <section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="conclusion-id000014">
        <h1>Conclusion</h1>
        <p>In this chapter, we covered the topic of exposing Kubernetes services outside the cluster. The following are
            the key concepts covered:</p>
        <ul>
            <li>
                <p>Kubernetes services concepts like direct pod connections, advertising service IPs, and node ports are
                    techniques you can leverage to expose Kubernetes services outside the cluster.</p>
            </li>
            <li>
                <p>We recommend using an eBPF-based dataplane to optimize the ingress path to route traffic to the pods
                    hosting the service backend.</p>
            </li>
            <li>
                <p>An eBPF dataplane is an excellent alternative to the default Kubernetes services implementation,
                    kube-proxy, due to its ability to preserve source IP to the pod.</p>
            </li>
            <li>
                <p>The choice of a Kubernetes ingress implementation will depend on your use case. We recommend that you
                    consider an in-cluster ingress solution as it is more native to Kubernetes and will give you more
                    control than using an external ingress solution.</p>
            </li>
        </ul>
        <p>We hope you are able to leverage these concepts based on your use case as you implement Kubernetes services.
        </p>
    </div></section>
</div></section></div></body></html>