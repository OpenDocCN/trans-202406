- en: Chapter 2\. Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](ch01.xhtml#introduction_to_cloud_native), cloud
    native applications are applications that are distributed in nature and utilize
    cloud infrastructure. There are many technologies and tools that are being used
    to implement cloud native applications, but from a compute perspective, it is
    mainly *functions* and *containers*. From an architectural perspective, *microservices
    architectures* have gained a lot of popularity. More often than not, those terms
    are mistakenly used, and often believed to be one and the same. In reality, functions
    and containers are different technologies, each serving a particular purpose,
    whereas microservices describes an architectural style. That said, understanding
    how to best use functions and containers, along with *eventing* or *messaging*
    technologies, allows developers to design, develop, and operate a new generation
    of cloud native microservices-based applications in the most efficient and agile
    way. To make the correct architectural decisions to design those types of applications,
    it is important to understand the basics of the underlying terms and technologies.
    This chapter explains important technologies used with cloud native applications
    and concludes by providing an overview of the microservices architectural style.
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Initially, containers were brought into the spotlight by startups and born-in-the-cloud
    companies, but over the past couple of years, containers have become synonymous
    with application modernization. Today there are very few companies that are not
    using containers or at least considering using containers in the future, which
    means that architects and developers alike need to understand what containers
    offer and what they don’t offer.
  prefs: []
  type: TYPE_NORMAL
- en: When people talk about containers today, they refer to “Docker containers” most
    of the time, because it’s Docker that has really made containers popular. However,
    in the Linux operating system (OS) world, containers date back more than 10 years.
    The initial idea of containers was to slice up an OS so that you can securely
    run multiple applications without them interfering with one another. The required
    isolation is accomplished through namespaces and control groups, which are Linux
    kernel features. Namespaces allow the different components of the OS to be sliced
    up and thus create isolated workspaces. Control groups then allow fine-grained
    control of resource utilization, effectively stopping one container from consuming
    all system resources.
  prefs: []
  type: TYPE_NORMAL
- en: Because the interaction with kernel features was not exactly what we would call
    developer friendly, Linux containers (LXC) were introduced to abstract away some
    of the complexity of composing the various technology underpinnings of what is
    now commonly call a “container.” Eventually it was Docker that made containers
    mainstream by introducing a developer-friendly packaging of the kernel features.
    Docker defines containers as a “standardized unit of software.” The “unit of software”—or,
    more accurately, the service or application running within a container—has full,
    private access to their own isolated view of OS constructs. In other words, you
    can view containers as encapsulated, individually deployable components running
    as isolated instances on the same kernel with virtualization happening on the
    OS level.
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0201](Images/clna_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. VMs and containers on a single host
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition, containers use the copy-on-write filesystem strategy, which allows
    multiple containers to share the same data, and the OS provides a copy of the
    data to the container that needs to modify or write data. This allows containers
    to be very lightweight in terms of memory and disk space usage, resulting in faster
    startup times, which is one of the great benefits of using containers. Other benefits
    are *deterministic deployments*, allowing portability between environments, isolation,
    and higher density. For modern cloud native applications, container images have
    become the unit of deployment encapsulating the application or service code, its
    runtime, dependencies, system libraries, and so on. Due to their fast startup
    times, containers are an ideal technology for scale-out scenarios, which are very
    common in cloud native applications. [Figure 2-1](#vms_and_containers_in_a_single_host)
    shows the difference between virtual machines (VMs) and containers on a single
    host.
  prefs: []
  type: TYPE_NORMAL
- en: Container Isolation Levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because containers are based on OS virtualization, they share the same kernel
    when running on the same host. Although this is sufficient enough isolation for
    most scenarios, it falls short of the isolation level that hardware-based virtualization
    options such as VMs provide. Following are some of the downsides of using VMs
    as the foundation of cloud native applications:'
  prefs: []
  type: TYPE_NORMAL
- en: VMs can take a considerable amount of time to start because they boot a full
    OS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the VM can be an issue. A VM contains an entire OS, which can easily
    be several gigabytes in size. Copying this image across a network—for example,
    if they are kept in a central image repository—will take a lot of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling of VMs has its challenges. Scaling up (adding more resources) requires
    a new, larger VM (more CPU, memory, storage, etc.) to be provisioned and booted.
    Scaling out might not be fast enough to respond to demand; it takes time for new
    instances to start.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMs have more overhead and use considerably more resources such as memory, CPU,
    and disk. This limits the density, or number of VMs that can run on a single host
    machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most common scenarios that demand high isolation on a hardware virtualization
    level are hostile multitenant scenarios in which you typically need to protect
    against malicious escape and breakout attempts into other targets on the same
    host or on the shared infrastructure. Cloud providers have been using technologies
    internally that provide VM-level isolation while maintaining the expected speed
    and efficiency of containers. These technologies are known as *Hyper-V containers*,
    *sandboxed containers*, or *MicroVMs*. Here are the most popular MicroVM technologies
    (in nonspecific order):'
  prefs: []
  type: TYPE_NORMAL
- en: '[Nabla containers](https://nabla-containers.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: These enable better isolation by taking advantage of unikernel techniques, specifically
    those from the [Solo5 project](https://github.com/Solo5/solo5), to limit system
    calls from the container to host kernel. The Nabla container runtime (runc) is
    an Open Container Initiative (OCI)-compliant runtime. OCI will be explained in
    a bit more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[Google’s gVisor](https://github.com/google/gvisor)'
  prefs: []
  type: TYPE_NORMAL
- en: This is a container runtime and user space kernel written in Go. The new kernel
    is a “user space” process that addresses the container’s system call needs, preventing
    direct interaction with the host OS. The gVisor runtime (runSC) is an OCI-compliant
    runtime, and it supports Kubernetes orchestration as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[Microsoft’s Hyper-V containers](https://oreil.ly/5njcd)'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s Hyper-V containers were introduced a couple of years ago and are
    based on VM Worker Process (*vmwp.exe*). Those containers provide full VM-level
    isolation and are OCI compliant. As for running [Hyper-V containers in Kubernetes](http://bit.ly/33vZb7U)
    in production, you will want to wait for general availability of Kubernetes on
    Windows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Kata containers](https://katacontainers.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Kata containers are a combination of Hyper.sh and Intel’s clear containers and
    provide classic hardware-assisted virtualization. Kata containers are compatible
    with the OCI specification for Docker containers and CRI for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon’s Firecracker
  prefs: []
  type: TYPE_NORMAL
- en: Firecracker is powering Amazon’s Lambda infrastructure and has been open sourced
    under the Apache 2.0 license. Firecracker is a user-mode VM solution that sits
    on top of the KVM API and is designed to run modern Linux kernels. The goal of
    Firecracker is to provide support for running Linux containers in a hypervisor-isolated
    fashion similar to other more isolated container technologies such as Kata containers.
    Note that, as of this writing, you are not able to use Firecracker with Kubernetes,
    Docker, or Kata containers.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-2](#isolation_levels_for_vms_comma_container) provides an overview
    of the isolation levels of the these technologies.'
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0202](Images/clna_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Isolation levels for VMs, containers, and processes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Container Orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To manage the life cycle of containers at scale, you need to use a container
    orchestrator. The tasks of a container orchestrator are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The provisioning and deployment of containers onto the cluster nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource management of containers, meaning placing containers on nodes that
    provide sufficient resources or moving containers to other nodes if the resource
    limits of a node is reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health monitoring of the containers and the nodes to initiaing restarted and
    rescheduling in case of failures on a container or node level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling in or out containers within a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing mappings for containers to connect to networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal load balancing between containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple container orchestrators available, but there is no doubt
    that Kubernetes is by far the most popular choice for cluster management and the
    scheduling of container-centric workloads in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes (often abbreviated as k8s) is an open source project for running
    and managing containers. Google open sourced the project in 2014, and Kubernetes
    is often viewed as a container platform, microservices platform, and/or a cloud
    portability layer. All of the major cloud vendors have a managed Kubernetes offering
    today.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Kubernetes cluster runs multiple components that can be grouped in one of
    three categories: *master components*, *node components*, or *addons*. Master
    components provide the cluster control plane. These components are responsible
    for making cluster-wide decisions like scheduling tasks in the cluster or responding
    to events, such as starting new tasks if one fails or does not meet the desired
    number of replicas. The master components can run on any node in the cluster,
    but are commonly deployed to dedicated master nodes. Managed Kubernetes offerings
    from cloud providers will handle the management of the control plane, including
    on-demand upgrades and patches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes master components include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: kube-apiserver
  prefs: []
  type: TYPE_NORMAL
- en: Exposes the Kubernetes API and is the frontend for the Kubernetes control plane
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs: []
  type: TYPE_NORMAL
- en: A key/value store used for all cluster data
  prefs: []
  type: TYPE_NORMAL
- en: kube-scheduler
  prefs: []
  type: TYPE_NORMAL
- en: Monitors newly created *pods* (a Kubernetes-specific management wrapper around
    containers, which we explain in more detail later in this chapter) that are not
    assigned to a node and finds an available node
  prefs: []
  type: TYPE_NORMAL
- en: kube-controller-manager
  prefs: []
  type: TYPE_NORMAL
- en: Manages a number of controllers that are responsible for responding to nodes
    that go down or maintaining the correct number of replicas
  prefs: []
  type: TYPE_NORMAL
- en: cloud-controller-manager
  prefs: []
  type: TYPE_NORMAL
- en: Run controllers that interact with the underlying cloud providers
  prefs: []
  type: TYPE_NORMAL
- en: Node components run on every node in the cluster, which is also referred to
    as the *data plane*, and are responsible for maintaining running pods and the
    environment for the node to which they are deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes node components include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: kubelet
  prefs: []
  type: TYPE_NORMAL
- en: An agent that runs on each node in the cluster and is responsible for running
    containers in pods based on their pod specification
  prefs: []
  type: TYPE_NORMAL
- en: kube-proxy
  prefs: []
  type: TYPE_NORMAL
- en: Maintains network rules on the nodes and performs connection forwarding
  prefs: []
  type: TYPE_NORMAL
- en: container runtime
  prefs: []
  type: TYPE_NORMAL
- en: The software responsible for running containers (see [“Kubernetes and Containers”](#kubernetes_and_containers))
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-3](#kubernetes_master_and_worker_node_compon) shows the Kubernetes
    master and worker node components.'
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0203](Images/clna_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Kubernetes master and worker node components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes is commonly deployed with addons that are managed by the master and
    worker node components. These addons will include services like Domain Name System
    (DNS) and a management user interface (UI).
  prefs: []
  type: TYPE_NORMAL
- en: 'A deep dive into Kubernetes is beyond the scope of this book. There are, however,
    some fundamental concepts that are important for you to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs: []
  type: TYPE_NORMAL
- en: A pod is basically a management wrapper around one or multiple containers, storage
    resources, or a unique network IP, that governs the container life cycle. Although
    Kubernetes supports multiple containers per pod, most of the time there is only
    one application container per pod. That said, the pattern of *sidecar containers*,
    which extends or enhances the functionality of the application container, is very
    popular. Service meshes like Istio rely heavily on sidecars, as you can see in
    [Chapter 3](ch03.xhtml#designing_cloud-native_applications).
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes service provides a steady endpoint to a grouping of pods that are
    running on the cluster. Kubernetes uses label selectors to identify which pods
    are targeted by a service.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSets
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to think about ReplicaSets is to think about service instances.
    You basically define how many replicas of a pod you need, and Kubernetes makes
    sure that you have that number of replicas running at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes Deployment documentation states that you “describe a desired
    state in a Deployment object, and the Deployment controller changes the actual
    state to the desired state at a controlled rate.” In other words, you should use
    Deployments for rolling out and monitoring ReplicaSets, scaling ReplicaSets, updating
    pods, rolling back to earlier Deployments versions, and cleaning up older ReplicaSets.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-4](#fundamental_kubernetes_concepts) provides a logical view of the
    fundamental Kubernetes concepts and how they interact with one another.'
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0204](Images/clna_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Fundamental Kubernetes concepts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes and Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is simply the orchestration platform for containers, so it needs
    a container runtime to manage the container life cycle. The Docker runtime was
    supported from day one in Kubernetes, but it isn’t the only container runtime
    available on the market. As a consequence, the Kubernetes community has pushed
    for a generic way to integrate container runtimes into Kubernetes. Interfaces
    have proven to be a good software pattern for providing contracts between two
    systems, so the community created the [Container Runtime Interface (CRI)](http://bit.ly/31y3pdC).
    The CRI avoids “hardcoding” specific runtime requirements into the Kubernetes
    codebase, with the consequence of always needing to update the Kubernetes codebase
    when there are changes to a container runtime. Instead, the CRI describes the
    functions that need to be implemented by a container runtime to be CRI compliant.
    The functions that the CRI describes handle the life cycle of container pods (start,
    stop, pause, kill, delete), container image management (e.g., download images
    from a registry), and some helper functions around observability, such as log
    and metric collections and networking. [Figure 2-5](#docker_versus_kata_container_on_kubernet)
    shows high-level CRI example architectures for Docker and Kata containers.
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0205](Images/clna_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Docker versus Kata container on Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following list provides other container-related technologies that might
    be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: OCI
  prefs: []
  type: TYPE_NORMAL
- en: The OCI is a [Linux Foundation](https://en.wikipedia.org/wiki/Linux_Foundation)
    project that aims to design open standards for container images and runtimes.
    Many container technologies implement an OCI-compliant runtime and image specification.
  prefs: []
  type: TYPE_NORMAL
- en: containerd
  prefs: []
  type: TYPE_NORMAL
- en: containerd is an industry-standard container runtime used by Docker and Kubernetes
    CRI, just to name the most popular ones. It is available as a daemon for Linux
    and Windows, which can manage the complete container life cycle of its host system,
    including container image management, container execution, low-level storage,
    and network attachments.
  prefs: []
  type: TYPE_NORMAL
- en: Moby
  prefs: []
  type: TYPE_NORMAL
- en: Moby is a set of open source tools created by Docker to enable and accelerate
    software containerization. The toolkit includes container build tools, a container
    registry, orchestration tools, a runtime, and more, and you can use these as building
    blocks in conjunction with other tools and projects. Moby is using containerd
    as the default container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless computing means that scale and the underlying infrastructure is managed
    by the cloud provider; that is, your application automatically drives the allocation
    and deallocation of resources, and you do not need to worry about managing the
    underlying infrastructure at all. All management and operations are abstracted
    away from the user and managed by cloud providers such as Microsoft Azure, Amazon
    Web Services (AWS), and Google Cloud Platform (GCP). From a developer perspective,
    serverless often adds an event-driven programming model, and from an economic
    perspective, you pay only per execution (CPU time consumed).
  prefs: []
  type: TYPE_NORMAL
- en: Many people think Function as a Service (FaaS) is serverless. This is technically
    true, but FaaS is only one variation of serverless computing. Microsoft Azure’s
    Container Instances (ACI) and Azure SF Mesh, as well as AWS Fargate and GCP’s
    Serverless Containers on Cloud Functions, are good examples. ACI and AWS Fargate
    are serverless container offerings also known as Container as a Service (CaaS),
    which allow you to deploy containerized applications without needing to know about
    the underlying infrastructure. Other examples of serverless offerings are API
    management and machine learning services—basically, any service that lets you
    consume functionality without managing the underlying infrastructure and a pay-only-for-what-you-use
    model qualifies as serverless offering.
  prefs: []
  type: TYPE_NORMAL
- en: Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When talking about functions, people typically talk about FaaS offerings such
    as AWS Lambda, Azure Functions, and Google Cloud Functions, which are implemented
    on serverless infrastructure. The advantages of serverless computing—fast startup
    and execution time, plus the simplification of their applications—makes FaaS offerings
    very compelling to developers because it allows them to focus solely on writing
    code.
  prefs: []
  type: TYPE_NORMAL
- en: From a development perspective, a function is the unit of work, which means
    that your code has a start and a finish. Functions are usually triggered by events
    that are emitted by either other functions or platform services. For example,
    a function can be triggered by adding an entry to a database service or eventing
    service. There are quite a few things to consider when you want to build a large,
    complex application just with functions. You will need to manage more independent
    code, you will need to ensure state is being taken care of, and you will need
    to implement patterns if functions must depend on one another, just to name a
    few. Containerized microservices share a lot of the same patterns, so there have
    been quite a few discussions around when to use FaaS or a container. [Table 2-1](#comparison_of_faas_and_containerized_ser)
    provides some high-level guidance between FaaS and containers, and [Chapter 3](ch03.xhtml#designing_cloud-native_applications)
    covers the trade-offs in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Comparison of FaaS and containerized services
  prefs: []
  type: TYPE_NORMAL
- en: '| FaaS | Containerized service |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Does one thing | Does more than one thing |'
  prefs: []
  type: TYPE_TB
- en: '| Can’t deploy dependencies | Can deploy dependencies |'
  prefs: []
  type: TYPE_TB
- en: '| Must respond to one kind of event | Can respond to more than one kind of
    event |'
  prefs: []
  type: TYPE_TB
- en: There are two scenarios in which using FaaS offerings might not be ideal, although
    it offers the best economics. First, you want to avoid vendor lock-in. Because
    you need to develop your function specific to the FaaS offering and consume higher-level
    cloud services from a provider, your entire application becomes less portable.
    Second, you want to run functions on-premises or your own clusters. There are
    a bunch of FaaS runtimes that are available as open source runtimes and that you
    can run on any Kubernetes cluster. Kubeless, OpenFaaS, Serverless, and Apache
    OpenWhisk are among the most popular installable FaaS platforms, with Azure Functions
    gaining more popularity since it has been open sourced. Installable FaaS platforms
    are typically deployed through containers and allow the developer to simply deploy
    small bits of code (functions) without needing to worry about the underlying infrastructure.
    Many installable FaaS frameworks use Kubernetes resources for routing, autoscaling,
    and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: A critical aspect of any FaaS implementation, no matter whether it runs on a
    cloud provider’s serverless infrastructure or is installed on your own clusters,
    is the startup time. In general, you expect functions to execute very quickly
    after they have been triggered, which implies that their underlying technology
    needs to provide very fast boot-up times. As previously discussed, containers
    provide good startup times, but do not necessarily offer the best isolation.
  prefs: []
  type: TYPE_NORMAL
- en: From VMs to Cloud Native
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how we ended up with the next generation of cloud native applications,
    it is worth looking at how applications evolve from running on VMs to functions.
    Describing the journey should give you a good idea of how the IT industry is changing
    to put developer productivity into focus and how you can take advantage of all
    the new technologies. There are really two different paths to the cloud native
    world. The first one is mainly used for *brownfield scenarios*, which means that
    you have an existing application, and typically follows a lift-and-shift, application
    modernization, and eventually an application optimization process. The second
    one is a *greenfield scenario* in which you start your application from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Lift-and-Shift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Installing software directly on machines in the cloud is still the very first
    step for many customers to move to the cloud. The benefits are mainly in the capital
    and operational expense areas given that customers do not need to operate their
    own datacenters or can at least reduce operations and, therefore, the costs. From
    a technical perspective, lift-and-shift into Infrastructure as a Service (IaaS)
    gives you the most control over the entire stack. With control comes responsibility,
    and installing software directly on machines often resulted in errors caused by
    missing dependencies, runtime versioning conflicts, resource contention, and isolation.
    The next logical step is to move applications into a Platform as a Service (PaaS)
    environment. PaaS existed long before containers became popular; for example,
    Azure Cloud Services dates back to 2010\. In most past PaaS environments, access
    to the underlying VMs is restricted or in some cases prohibited so that moving
    to the cloud requires some rewriting of the applications. The benefit for developers
    is not to worry about the underlying infrastructure anymore. Operational tasks
    such as patching the OS were handled by the cloud providers, but some of the problems,
    like missing dependencies, remained. Because many PaaS services were based on
    VMs, scaling in burst scenarios was still a challenge due to the downsides of
    VMs, which we discussed previously, and for economic reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Application Modernization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides offering super-fast startup times, containers drastically removed the
    issues of missing dependencies, because everything an application needed is packaged
    inside a container. It didn’t take long for developers to begin to love the concept
    of containers as a packaging format, and now pretty much every new application
    is using containers, and more and more monolithic legacy applications are being
    containerized. Many customers see the containerization of an existing application
    as an opportunity to also move to a more suitable architecture for cloud native
    environments. Microservices is the obvious choice, but as you will see later in
    the chapter, moving to such an architecture comes with some disadvantages. There
    are a few very obvious reasons, though, why you want to break up your monolith:'
  prefs: []
  type: TYPE_NORMAL
- en: Time to deployment is faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain components need to update more frequently than others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain components need different scale requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain components should be developed in a different technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The codebase has gotten too big and complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the methodology to break up a monolith goes beyond the scope of this
    book, it is worth mentioning the two major patterns to move from a monolithic
    application to microservices.
  prefs: []
  type: TYPE_NORMAL
- en: '*Strangler* pattern'
  prefs: []
  type: TYPE_NORMAL
- en: With the Strangler pattern, you strangle the monolithic application. New services
    or existing components are implemented as microservices. A facade or gateway routes
    user requests to the correct application. Over time, more and more features are
    moved to the new architecture until the monolithic application has been entirely
    transformed into a microservices application.
  prefs: []
  type: TYPE_NORMAL
- en: '*Anticorruption Layer* pattern'
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to the Strangler pattern but is used when new services need
    to access the legacy application. The layer then translates the concepts from
    existing app to new, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: We describe both patterns in more detail in [Chapter 6](ch06.xhtml#best_practices).
  prefs: []
  type: TYPE_NORMAL
- en: With applications being packaged in container images, orchestrators began to
    play a more important role. Even though there were several choices in the beginning,
    Kubernetes has become the most popular choice today; in fact, it is considered
    the new cloud OS. Orchestrators, however, added another variable to the equation
    insomuch as development and operations teams needed to understand them. The management
    part of the environment has become better, as pretty much every cloud vendor now
    offers “orchestrators” as a service. As with any cloud provider, “managed” Kubernetes
    means that the setup and runtime part of the Kubernetes service is managed. From
    an economical point of view, users are typically being charged for compute hours,
    which means that you pay as long as the nodes of the cluster are up and running
    even though the application might be sitting idle or utilizing low resources.
  prefs: []
  type: TYPE_NORMAL
- en: From a developer perspective, you still need to understand how Kubernetes works
    if you want to build microservices applications on top of it given that Kubernetes
    does not offer any PaaS or CaaS features out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a Kubernetes service does not really represent the service code
    within a container, it just provides an endpoint to it, so that the code within
    the container can always be accessed through the same endpoint. In addition to
    needing to understand Kubernetes, developers are also being introduced to distributed
    systems patterns to handle resiliency, diagnostics, and routing, just to name
    a few.
  prefs: []
  type: TYPE_NORMAL
- en: Service meshes such as Istio or Linkerd are gaining popularity because they
    are moving some of the distributed systems complexity into the platform layer.
    [Chapter 3](ch03.xhtml#designing_cloud-native_applications) covers service meshes
    in great detail, but for now you can think of a service mesh as being a dedicated
    networking infrastructure layer that handles the service-to-service communication.
    Among other things, service meshes enable resiliency features such as retries
    and circuit breakers, distributed tracing, and routing.
  prefs: []
  type: TYPE_NORMAL
- en: The next step of application evolution is to use serverless infrastructure for
    containerized workloads, aka CaaS offerings such as Azure Container Instances
    or AWS Fargate. Microsoft Azure has done a great job to meld the world of its
    managed Kubernetes Service (AKS) with its CaaS offering, ACI, by using *virtual
    nodes*. Virtual nodes is based on Microsoft’s open source project called Virtual
    Kubelet, which allows any compute resource to act as a Kubernetes node and use
    the Kubernetes control plane. In the case of AKS virtual nodes, you are able to
    schedule your application on AKS and burst into ACI without needing to set up
    additional nodes in case your cluster cannot offer any more resources in a scale-out
    scenario. [Figure 2-6](#modernized_application_with_feature_3_be) shows how an
    existing monolithic application (Legacy App) is broken down into smaller microservices
    (Feature 3). The legacy application and the new microservice (Feature 3) are on
    a service mesh on Kubernetes. In this case Feature 3 has independent scale needs
    and can be scaled out into a CaaS offering using Virtual Kubelet.
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0206](Images/clna_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Modernized application with Feature 3 being scaled out into CaaS
    using Virtual Kubelet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Application Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to improve the application in terms not only of further cost
    optimization, but also of code optimization. Functions really excel in short-lived
    compute scenarios, such as updating records, sending emails, transforming messages,
    and so on. To take advantage of functions, you can identify short-lived compute
    functionality in your service codebase and implement it using functions. A good
    example is an order service in which the containerized microservice does all the
    Create, Read, Update, and Delete (CRUD) operations, and a function sends the notification
    of a successfully placed order. To trigger the function, eventing or messaging
    systems are being used. Eventually, you could decide to build the entire order
    service using functions, with each function executing one of the CRUD operations.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices is a term commonly used to refer to a microservices architecture
    style, or the individual services in a microservices architecture. A microservices
    architecture is a service-oriented architecture in which applications are decomposed
    into small, loosely coupled services by area of functionality. It’s important
    that services remain relatively small, are loosely coupled, and are decomposed
    around business capability.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices architectures are often compared and contrasted with monolithic
    architectures. Instead of managing a single codebase, a shared datastore, and
    data structure, as in a monolith, in a microservices architecture an application
    is composed of smaller codebases, created and managed by independent teams. Each
    service is owned and operated by a small team, with all elements of the service
    contributing to a single well-defined task. Services run in separate processes
    and communicate through APIs that are either synchronous or asynchronous message
    based.
  prefs: []
  type: TYPE_NORMAL
- en: Each service can be viewed as its very own application with an independent team,
    tests, builds, data, and deployments. [Figure 2-7](#inventory_service_in_a_microservices_arc)
    shows the concept of a microservices architecture, using the Inventory service
    as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0207](Images/clna_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Inventory service in a microservices architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Benefits of a Microservices Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A properly implemented microservices architecture will increase the release
    velocity of large applications, enabling businesses to deliver value to customers
    faster and more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Agility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fast, reliable deployments can be challenging with large, monolithic applications.
    A deployment of a small change to a module in one feature area can be held up
    by a change to another feature. As an application grows, testing of the application
    will increase and it can take a considerable amount of time to deliver new value
    to stakeholders. A change to one feature will require the entire application to
    be redeployed and rolled forward or back if there is an issue with that change.
    By decomposing an application into smaller services, the time needed to verify
    and release changes can be reduced and deployed more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous innovation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Companies need to move increasingly faster in order to remain relevant today.
    This requires organizations to be agile and capable of quickly adapting to fast-changing
    market conditions. Companies can no longer wait years or months to deliver new
    value to customers: they must often deliver new value daily. A microservices architecture
    can make it easier to deliver value to stakeholders in a reliable way. Small independent
    teams are able to release features and perform A/B testing to improve conversions
    or user experience during even the busiest times.'
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a large monolithic application, it can be very difficult to adopt new technologies
    or techniques because this will often require that the entire application be rewritten
    or care needs to be taken to ensure that some new dependency can run side-by-side
    with a previous one. Loose coupling and high functional cohesion is important
    to a system design that is able to evolve through changing technologies. By decomposing
    an application by features into small, loosely coupled services, it can be much
    easier to change individual services without affecting the entire application.
    Different languages, frameworks, and libraries can be used across the different
    services if needed to support the business.
  prefs: []
  type: TYPE_NORMAL
- en: Small, focused teams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structuring engineering teams at scale and keeping them focused and productive
    can be challenging. Making people responsible for designing, running, and operating
    what they build can also be challenging if what you are building is heavily intertwined
    with what everyone else is building. It can sometimes take new team members days,
    weeks, or even months to get up to speed and begin contributing because they are
    burdened with understanding aspects of a system that are unrelated to their area
    of focus. By decomposing an application into smaller services, small agile teams
    are able to focus on a smaller concern and move quickly. It can be much easier
    for a new member joining because they need to be concerned with only a smaller
    service. Team members can more easily operate and take accountability for the
    services they build.
  prefs: []
  type: TYPE_NORMAL
- en: Fault isolation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a monolithic application, a single library or module can cause problems for
    the entire application. A memory leak in one module not only can affect the stability
    and performance of the entire application, but can often be difficult to isolate
    and identify. By decomposing features of the application into independent services,
    teams can isolate a defect in one service to that service.
  prefs: []
  type: TYPE_NORMAL
- en: Improved scale and resource usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Applications are generally scaled up or out. They are scaled up by increasing
    the size or type of machine, and scaled out by increasing the number of instances
    deployed and routing users across these instances. Different features of an application
    will sometimes have different resource requirements; for example, memory, CPU,
    disk, and so on. Application features will often have different scale requirements.
    Some features might easily scale out with very few resources required for each
    instance, whereas other features might require large amounts of memory with limited
    ability to scale out. By decoupling these features into independent services,
    teams can configure the services to run in environments that best meet the services,
    individual resource and scale requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Improved observability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a monolithic application it can be difficult to measure and observe the individual
    components of an application without careful and detailed instrumentation throughout
    the application. By decomposing features of an application into separate services,
    teams can use tools to gain deeper insights into the behavior of the individual
    features and interactions with other features. System metrics such as process
    utilization and memory usage can now easily be tied back to the feature team because
    it’s running in a separate process or container.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with a Microservices Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite all the benefits of a microservices architecture, there are trade-offs,
    and a microservices architecture does have its own set of challenges. Tooling
    and technologies have begun to address some of these challenges, but many of them
    still remain. A microservices architecture might not be the best choice for all
    applications today, but we can still apply many of the concepts and practices
    to other architectures. The best approach often lies somewhere in between.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distributed systems are inherently complex. As we decompose the application
    into individual services, network calls are necessary for the individual services
    to communicate. Networks calls add latency and experience transient failures,
    and the operations can run on different machines with a different clock, each
    having a slightly different sense of the current time. We cannot assume that the
    network is reliable, latency is zero, bandwidth is infinite, the network is secure,
    the topology will not change, there is one administrator, transport costs are
    zero, and that the network is homogenous. Many developers are not familiar with
    distributed systems and often make false assumptions when entering that world.
    The *Fallacies of Distributed Computing*, as discussed in [Chapter 1](ch01.xhtml#introduction_to_cloud_native),
    is a set of assertions describing those false assumptions commonly made by developers.
    They were first documented by L. Peter Deutsch and other Sun Microsystems engineers
    and are covered in numerous blog articles. [Chapter 6](ch06.xhtml#best_practices)
    provides more information about best practices, tools, and techniques for dealing
    with the complexities of distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity and consistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decentralized data means that data will often exist in multiple places with
    relationships spanning different systems. Performing transactions across these
    systems can be difficult, and we need to employ a different approach to data management.
    One service might have a relationship to data in another service; for example,
    an order service might have a reference to a customer in an account service. Data
    might have been copied from the account service in order to satisfy some performance
    requirements. If the customer is removed or disabled, it can be important that
    the order service is updated to indicate this status. Dealing with data will require
    a different approach. [Chapter 4](ch04.xhtml#working_with_data) covers patterns
    for dealing with this.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Networking requests and data serialization add overhead. In a microservices-based
    architecture the number of network requests will increase. Remember, components
    are libraries that are no longer making direct calls; this is happening over a
    network. A call to one service can result in a call to another dependent service.
    It might take a number of requests to multiple services in order to satisfy the
    original request. We can implement some patterns and best practices to mitigate
    potential performance overhead in a microservices architecture, which we look
    at in [Chapter 6](ch06.xhtml#best_practices).
  prefs: []
  type: TYPE_NORMAL
- en: Development and testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Development can be a bit more challenging because the tools and practices used
    today don’t work with a microservices architecture. Given the velocity of change
    and the fact that there are many more external dependencies, it can be challenging
    to run a complete test suite on versions of the dependent services that will be
    running in production. We can implement a different approach to testing to address
    these challenges, and a proper Continuous Integration/Continuous Deployment (CI/CD)
    pipeline will be necessary. Development tooling and test strategies have evolved
    over the years to better accommodate a microservices architecture. [Chapter 5](ch05.xhtml#devops)
    covers many of the tools, techniques, and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning and integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Changing an interface in a monolithic application can require some refactoring,
    but the changes are often built, tested, and deployed as a single cohesive unit.
    In a microservices architecture service, dependencies are changing and evolving
    independently of the consumers. Careful attention to forward and backward compatibility
    is necessary when dealing with service versioning. In addition to maintaining
    forward and backward compatibility with service changes, it might be possible
    to deploy an entirely new version of the service, running it side-by-side with
    the previous version for some period of time. [Chapter 5](ch05.xhtml#devops) explores
    service versioning and integration strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and logging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many organizations struggle with monitoring and logging of monolithic applications,
    even when they are using a common shared logging library. Inconsistencies in naming,
    data types, and values make it difficult to correlate relevant log events. In
    a microservices architecture, when relevant events span multiple services—all
    potentially using different logging implementations—correlating these events can
    be even more challenging. Planning and early attention to the importance of logging
    and monitoring can help address much of this, which we examine in [Chapter 5](ch05.xhtml#devops).
  prefs: []
  type: TYPE_NORMAL
- en: Service dependency management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a monolithic application, dependencies on libraries are generally compiled
    into a single package and tested. In a microservices architecture, service dependencies
    are managed differently, requiring environment-specific routing and discovery.
    Service discovery and routing tools and technologies have come a long way in addressing
    these challenges. [Chapter 3](ch03.xhtml#designing_cloud-native_applications)
    looks at these in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although a microservices architecture can help isolate faults to individual
    services, if other services or the application as a whole is unable to function
    without that service, the application will be unavailable. As the number of services
    increases, the chance that one of those services experiences a failure also increases.
    Services will need to implement resilient design patterns, or some functionality
    downgraded in the event of a service outage. [Chapter 6](ch06.xhtml#best_practices)
    covers patterns and best practices for building highly available applications
    and provides more detail on the specific challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every application, whether cloud native or traditional, needs infrastructure
    on which to be hosted, technology that addresses pain points with development
    and deployment, and an architectural style that helps with achieving the business
    objectives, such as time to market. The goal of this chapter was to provide the
    basic knowledge for cloud native applications. By now you should understand that
    there are various container technologies with different isolation levels, how
    functions relate to containers, and that serverless infrastructure does not always
    need to be FaaS. Further, you should have a basic understanding of microservices
    architectures and of how you can migrate and modernize an existing application
    to be a cloud native application.
  prefs: []
  type: TYPE_NORMAL
- en: The upcoming chapters build on this knowledge and go deep into how to design,
    develop, and operate cloud native applications.
  prefs: []
  type: TYPE_NORMAL
