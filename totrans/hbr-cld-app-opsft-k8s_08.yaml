- en: 'Chapter 6\. Multicluster Fleets: Provision and Upgrade Life Cycles'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章。多集群群集：配置和升级生命周期
- en: 'The terms *multicluster* and *multicloud* have become common in today’s landscape.
    For the purposes of this discussion, we will define these terms as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*多集群* 和 *多云* 这两个术语已经在当今的环境中变得很普遍。对于本次讨论，我们将以下列定义来定义这些术语：'
- en: Multicluster
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群
- en: Refers to scenarios where more than a single cluster is under management or
    an application is made up of parts that are hosted on more than one cluster
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是管理多个集群或应用程序由多个部分组成并托管在多个集群上的情况。
- en: Multicloud
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 多云
- en: Refers to scenarios where the multiple clusters in use also span infrastructure
    substrates, which might include a private datacenter and a single public cloud
    provider or multiple public cloud providers
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是使用的多个集群还跨越基础设施基板的情况，这可能包括一个私有数据中心和一个单一的公共云提供商，或多个公共云提供商。
- en: The differences here are more academic; the reality is that you are more likely
    than not to have to manage many clusters just as your organization has had to
    manage multiple VMware ESXi hosts that run VMs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的区别更多是学术性的；事实上，您很可能不得不像您的组织不得不管理运行虚拟机的多个 VMware ESXi 主机一样，管理许多集群。
- en: The differences will matter when your container orchestration platform has variation
    across infrastructure substrates. We’ll talk about some of the places where this
    currently comes up and may affect some of your management techniques or application
    architectures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的容器编排平台在基础设施基板上有变化时，这些差异就会很重要。我们将讨论当前出现这些变化的一些地方，以及可能影响到一些管理技术或应用架构的地方。
- en: Why Multicluster?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要使用多集群？
- en: Let’s discuss the use cases that lead to multiple clusters under management.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论导致多个集群管理的使用案例。
- en: 'Use Case: Using Multiple Clusters to Provide Regional Availability for Your
    Applications'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例：使用多个集群为您的应用程序提供区域性可用性
- en: As discussed in [Chapter 4](ch04.html#single_cluster_availability), a single
    cluster can span multiple availability zones. Each availability zone has independent
    failure characteristics. A failure in the power supply, network provider, and
    even physical space (e.g., the datacenter building) should be isolated to one
    availability zone. Typically, the network links across availability zones still
    provide for significant throughput and low latency, allowing the etcd cluster
    for the Kubernetes API server to span hosts running in different availability
    zones. However, your application may need to tolerate an outage that affects more
    than two availability zones within a region or tolerate an outage of the entire
    region.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第四章](ch04.html#single_cluster_availability) 所讨论的，单个集群可以跨越多个可用区。每个可用区都有独立的故障特性。例如，电源供应、网络提供商，甚至物理空间（例如，数据中心建筑）的故障应该被隔离到一个可用区。通常情况下，跨可用区的网络链接仍然提供了显著的吞吐量和低延迟，允许
    Kubernetes API 服务器的 etcd 集群跨越运行在不同可用区的主机。然而，您的应用程序可能需要容忍影响超过两个可用区的故障，或容忍整个区域的故障。
- en: So perhaps one of the most easily understood use cases is to create more than
    one multiavailability zone cluster in two or more regions. You will commonly find
    applications that are federated across two “swim lanes,” sometimes referred to
    as a [*blue-green architecture*](https://oreil.ly/82hDU). The “blue-green” pairing
    pattern can often be found within the same region, with alternate blue-green pairs
    in other regions. You may choose to bring that same architecture to OpenShift
    where you run two separate clusters that host the same set of components for the
    application, effectively running two complete end-to-end environments, either
    of which can support most of the load of your users. Additional issues concerning
    load balancing and data management arise around architectural patterns required
    to support cross-region deployments and will be covered in [Chapter 8](ch08.html#working_example_of_multicluster_applicat).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，创建两个或多个区域内的多个多可用区集群可能是最容易理解的使用案例之一。您经常会发现应用程序在两个“泳道”上联合，有时称为 [*蓝绿架构*](https://oreil.ly/82hDU)。这种“蓝绿”配对模式通常可以在同一区域内找到，在其他区域中有备用的蓝绿配对。您可以选择将同样的架构带到
    OpenShift，您可以在其中运行两个独立的集群来托管应用程序的相同组件集，从而有效地运行两个完整的端到端环境，其中任何一个都可以支持大多数用户的负载。围绕支持跨区域部署所需的架构模式，涉及到额外的负载平衡和数据管理问题将在
    [第八章](ch08.html#working_example_of_multicluster_applicat) 中进行讨论。
- en: 'Use Case: Using Multiple Clusters for Multiple Tenants'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例：为多租户使用多个集群
- en: The Kubernetes community boundary for tenancy is a single cluster. In general,
    the API constructs within Kubernetes focus on dividing the compute resources of
    the cluster into namespaces (also called *projects* in OpenShift). Users are then
    assigned roles or `ClusterRole`s to access their namespaces. However, cluster-scoped
    resources like `ClusterRole`s, CRDs, namespaces/projects, webhook configurations,
    and so on really cannot be managed by independent parties. Each API resource must
    have a unique name within the collection of the same kind of API resources. If
    there were true multitenancy within a cluster, then some API concept (like a tenant)
    would group things like `ClusterRole`s, CRDs, and webhook configurations and prevent
    collisions (in naming or behavior) across each tenant, much like projects do for
    applications (e.g., deployments, services, and `PersistentVolumeClaim`s can duplicate
    names or behavior across different namespaces/projects).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes社区的租户边界是单个集群。一般来说，Kubernetes内的API构建专注于将集群的计算资源划分为命名空间（在OpenShift中称为*项目*）。然后，用户被分配角色或`ClusterRole`以访问他们的命名空间。然而，像`ClusterRole`、CRD、命名空间/项目、Webhook配置等集群范围的资源确实不能由独立方管理。每个API资源在同类API资源的集合中必须有唯一的名称。如果集群内有真正的多租户，那么某些API概念（如租户）将会将`ClusterRole`、CRD和Webhook配置等分组，并防止跨每个租户的名称或行为冲突，就像应用程序（例如部署、服务和`PersistentVolumeClaim`）的项目所做的那样。
- en: So Kubernetes is easiest to consume when you can assign a cluster to a tenant.
    A tenant might be a line of business or a functional team within your organization
    (e.g., quality engineering or performance and scale test). Then, a set of cluster-admins
    or similarly elevated `ClusterRole`s can be assigned to the owners of the cluster.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当您能为一个租户分配一个集群时，Kubernetes使用起来最为简单。一个租户可以是您组织内的业务线或功能团队（例如质量工程或性能和规模测试）。然后，一组集群管理员或类似的高级`ClusterRole`可以分配给集群的所有者。
- en: Hence, an emerging pattern is that platform teams that manage OpenShift clusters
    will define a process where a consumer may request a cluster for their purposes.
    As a result, multiple clusters now require consistent governance and policy management.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个新兴的模式是，管理OpenShift集群的平台团队将定义一个流程，消费者可以为其目的请求一个集群。因此，现在多个集群需要一致的治理和策略管理。
- en: 'Use Case: Supporting Far-Edge Use Cases Where Clusters Do Not Run in Traditional
    Datacenters or Clouds'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例：支持远端用例，在这些用例中，集群不在传统的数据中心或云中运行。
- en: There are some great examples of how technology is being applied to a variety
    of use cases where computing power is coupled with sensor data from cameras, audio
    sensors, or environmental sensors and machine learning or AI to improve efficiency,
    provide greater safety, or create novel consumer interactions.^([1](ch06.html#ch01fn36))
    These use cases are often referred to generically as *edge computing* because
    the computing power is outside the datacenter and closer to the “edge” of the
    consumer experience.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些很好的例子展示了技术如何应用于各种使用案例，其中计算能力与来自摄像头、音频传感器或环境传感器的传感器数据以及机器学习或人工智能结合，以提高效率、提供更大安全性或创建新型消费者互动。[^1]
- en: The introduction of high-bandwidth capabilities with 5G also creates scenarios
    where an edge-computing solution can use a localized 5G network within a space
    like a manufacturing plant and where edge-computing applications help track product
    assembly, automate safety controls for employees, or protect sensitive machinery.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 引入5G的高带宽能力也创造了一些场景，其中边缘计算解决方案可以利用类似制造厂的空间内的本地化5G网络，边缘计算应用有助于跟踪产品装配、自动化员工安全控制或保护敏感设备。
- en: Just as containers provide a discrete package for enterprise web-based applications,
    there are significant benefits of using containers in edge-based applications.
    Similarly, the automated recovery of services by your container orchestration
    is also beneficial, even more so when the computing source is not easily accessible
    within your datacenter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如容器为企业Web应用程序提供了一个独立的包装，使用容器在基于边缘的应用程序中也有显著的好处。类似地，您的容器编排的自动恢复服务也是有益的，尤其是当计算资源不易在您的数据中心内访问时。
- en: Architectural Characteristics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构特征
- en: Now that we have seen some of the reasons why you may use multiple clusters
    or clouds to support your needs, let’s take a look at some of the architectural
    benefits and challenges of such an approach.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了您可能使用多个集群或云来支持您的需求的一些原因，让我们来看看这种方法的一些架构优势和挑战。
- en: Region availability versus availability zones
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域可用性与可用性区域
- en: With multiple clusters hosting the application, you can spread instances of
    the application across multiple cloud regions. Each cluster within a region will
    still spread compute capacity across multiple availability zones. See [Figure 6-1](#cluster_region_availability_allows_multi)
    for a visual representation of this topology.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序托管在多个集群中时，您可以将应用程序的实例分布在多个云区域中。每个区域内的集群仍会将计算能力分布在多个可用性区域中。请参见[图 6-1](#cluster_region_availability_allows_multi)以查看此拓扑结构的可视表示。
- en: '![](assets/hcok_0601.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0601.png)'
- en: Figure 6-1\. Cluster region availability allows multiple clusters to run across
    independent cloud regions
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 集群区域可用性允许多个集群在独立的云区域中运行
- en: Under this style of architecture, each cluster can tolerate the total loss of
    any one availability zone (AZ1, AZ2, or AZ3 could become unavailable but not more
    than one), and the workload will continue to run and serve requests. As a result
    of two availability zone failures, the etcd cluster would lose quorum and the
    control plane would become unavailable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构风格下，每个集群可以容忍任何一个可用性区域的完全丢失（AZ1、AZ2或AZ3可能不可用，但不会超过一个），并且工作负载将继续运行和提供请求。由于两个可用性区域的失败，etcd集群将失去法定人数，控制平面将变得不可用。
- en: The reason that a Kubernetes control plane becomes inoperable with more than
    a single availability zone failure is because of quorum requirements for etcd.
    Typically, etcd will have three replicas that are maintained in the control plane,
    each replica supported by exactly one availability zone. If a single availability
    zone is lost, there are still two out of three replicas present and distributed
    writes can still be sure that the write transaction is accepted. If two availability
    zones fail, then write attempts will be rejected. Pods running on worker nodes
    in the cluster may still be able to serve traffic, but no updates related to the
    Kubernetes API will be accepted or take place. However, the independent cluster
    running in one of the other regions could continue to respond to user requests.
    See [Chapter 4](ch04.html#single_cluster_availability) for a deeper analysis of
    how this process works.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes控制平面在超过一个可用性区域失败时变得无法操作的原因是etcd的法定人数要求。通常，etcd将在控制平面中维护三个副本，每个副本由一个可用性区域支持。如果一个可用性区域丢失，仍然有三个副本中的两个存在，并且分布式写入仍然可以确保写入事务被接受。如果两个可用性区域失败，则写入尝试将被拒绝。在集群中运行在工作节点上的Pod可能仍然能够提供流量服务，但不会接受或发生与Kubernetes
    API相关的任何更新。然而，在另一个区域中运行的独立集群仍可以继续响应用户请求。请参见[第4章](ch04.html#single_cluster_availability)以深入分析此过程的工作原理。
- en: Mitigating latency for users based on geography
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据地理位置减轻用户的延迟
- en: If you have users in different locations, using more than one cloud region can
    also improve response times for your users. When a user attempts to access the
    web user experience of your application or an API exposed by your workload, their
    request can be routed to the nearest available instance of your application. Typically,
    a *Global Server Load Balancer* (GSLB) is used to efficiently route traffic in
    these scenarios. When the user attempts to access the service, a DNS lookup will
    be delegated to the nameservers hosted by your GSLB. Then, based on a heuristic
    of where the request originated, the nameserver will respond with the IP address
    of the nearest hosted instance of your application. You can see a visual representation
    of this in [Figure 6-2](#requests_to_resolve_the_address_of_a_glo).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的用户位于不同的位置，使用多个云区域也可以提高用户的响应时间。当用户尝试访问您的应用程序的Web用户体验或工作负载提供的API时，他们的请求可以被路由到最近可用的应用程序实例。通常，在这些情况下会使用*全局服务器负载均衡器*（GSLB）来有效地路由流量。当用户尝试访问服务时，DNS查找将被委托给您的GSLB托管的名称服务器。然后，根据请求的来源地的启发式，名称服务器将返回最近托管的应用程序实例的IP地址。您可以在[图 6-2](#requests_to_resolve_the_address_of_a_glo)中看到这一过程的可视表示。
- en: '![](assets/hcok_0602.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0602.png)'
- en: Figure 6-2\. Requests to resolve the address of a global service using a GSLB
    will return the closest instance based on proximity to the request originator
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 使用 GSLB 解析全局服务地址的请求将返回距离请求发起者最近的实例。
- en: Consistency of your platform (managed Kubernetes versus OpenShift plus cloud
    identity providers)
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平台的一致性（托管 Kubernetes 与 OpenShift 加云身份提供商）
- en: One of the major benefits of the OpenShift Container Platform is that it deploys
    and runs consistently across all cloud providers and substrates like VMware and
    bare metal. When you consider whether to consume a managed Kubernetes provider
    or OpenShift, be aware that each distribution of Kubernetes makes various architectural
    decisions that can require greater awareness of your application to ensure cross-provider
    portability.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 容器平台的主要优势之一是它在所有云提供商和基础设施（如 VMware 和裸金属）上部署和运行一致。在考虑是否使用托管 Kubernetes
    提供商或 OpenShift 时，请注意 Kubernetes 的每个分发版本都会做出各种架构决策，可能需要更多关注应用程序，以确保跨提供商的可移植性。
- en: Provisioning Across Clouds
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨云部署
- en: Choosing a Kubernetes strategy affords a great way to simplify how applications
    consume elastic cloud-based infrastructure. To some extent, the problem of how
    you consume a cloud’s resources shifts from solving the details for every application
    to one platform—namely, how your organization will adopt and manage Kubernetes
    across infrastructure substrates.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 Kubernetes 策略提供了简化应用程序如何消费弹性云基础设施的有效方式。在某种程度上，解决如何为每个应用程序解决云资源的细节问题，转变为解决一个平台问题，即你的组织如何在基础设施底层采用和管理
    Kubernetes。
- en: There are several ways to provision Kubernetes from community-supported projects.
    For the purposes of this section, we’ll focus on how to provision Kubernetes using
    the Red Hat OpenShift Container Platform. Then we’ll discuss how you could alternatively
    consume managed OpenShift or managed Kubernetes services as part of your provisioning
    life cycle.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种从社区支持的项目中部署 Kubernetes 的方法。在本节中，我们将重点介绍如何使用 Red Hat OpenShift 容器平台来部署 Kubernetes。然后，我们将讨论如何在你的部署生命周期中，作为替代选择，使用托管
    OpenShift 或托管 Kubernetes 服务。
- en: User-Managed OpenShift
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户管理的 OpenShift
- en: When you provision an OpenShift Container Platform 4.x cluster, you have two
    options for how infrastructure resources are created. *User-provisioned infrastructure*
    (UPI) allows you more control to spin up VMs, network resources, and storage and
    then give these details to the install process and allow them to be bootstrapped
    into a running cluster. Alternatively, you can rely on the more automated approach
    of *installer-provisioned infrastructure* (IPI). Using IPI, the installer accepts
    cloud credentials with the appropriate privileges to create the required infrastructure
    resources. The IPI process will typically define a *virtual private cloud* (VPC).
    Note that you can specify the VPC as an input parameter if your organization has
    its own conventions for how these resources are created and managed. Within the
    VPC, resources, including network load balancers, object store buckets, virtual
    computing resources, elastic IP addresses, and so forth, are all created and managed
    by the install process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署 OpenShift 容器平台 4.x 集群时，有两种选择如何创建基础设施资源。*用户提供的基础设施*（UPI）允许你更多地控制，以便启动虚拟机、网络资源和存储，然后将这些详细信息提供给安装过程，并允许它们引导到运行中的集群。或者，你可以依赖更自动化的*安装程序提供的基础设施*（IPI）方法。使用
    IPI，安装程序接受具有创建所需基础设施资源所需权限的云凭据。IPI 过程通常会定义一个*虚拟私有云*（VPC）。请注意，如果你的组织有自己的资源创建和管理约定，你可以将
    VPC 指定为输入参数。在 VPC 内部，资源包括网络负载均衡器、对象存储桶、虚拟计算资源、弹性 IP 地址等，都由安装过程创建和管理。
- en: 'Let’s take a look at provisioning an OpenShift cluster across three cloud providers:
    AWS, Microsoft Azure, and Google Cloud Platform. For this discussion, we will
    review how the install process makes use of declarative configuration (just as
    Kubernetes does in general) and how this relates to the `ClusterVersionOperator`
    (CVO), which manages the life cycle of the OpenShift cluster itself.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 AWS、Microsoft Azure 和 Google Cloud Platform 三个云提供商上部署 OpenShift 集群。在本讨论中，我们将审查安装过程如何利用声明性配置（与
    Kubernetes 一般情况下一样），以及这与管理 OpenShift 集群生命周期的 *ClusterVersionOperator*（CVO）的关系。
- en: First, you will need to download the openshift-installer binary for your appropriate
    version. Visit [Red Hat](https://cloud.redhat.com), create an account, and follow
    the steps to Create Cluster and download the binary for local use. Specific details
    about the options available for installation are available in the [product documentation](https://oreil.ly/HerIm).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要为适当版本下载openshift-installer二进制文件。访问[Red Hat](https://cloud.redhat.com)，创建帐户，并按照创建集群和下载本地使用二进制文件的步骤进行操作。有关可用于安装的选项的具体详细信息，请参阅[产品文档](https://oreil.ly/HerIm)。
- en: Let’s demonstrate how this approach works by looking at a few example configuration
    files for the openshift-installer binary. The full breadth of options for installing
    and configuring OpenShift is beyond the scope of this book. See the [OpenShift
    Container Platform documentation](https://oreil.ly/wOJ3P) for a thorough reference
    of all supported options. The following examples will highlight how the declarative
    nature of the OpenShift 4.x install methodology simplifies provisioning clusters
    across multiple substrates. Further, a walk-through example of the `MachineSet`
    API will demonstrate how operators continue to manage the life cycle and health
    of the cluster after provisioning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看openshift-installer二进制文件的一些示例配置文件来演示这种方法的工作原理。 OpenShift 4.x安装方法的所有支持选项超出了本书的范围。请查看[OpenShift容器平台文档](https://oreil.ly/wOJ3P)详细参考所有支持的选项。以下示例将突出显示OpenShift
    4.x安装方法的声明性本质如何简化在多个基础设施上配置集群。此外，将演示`MachineSet` API的步骤示例，展示操作员在配置集群后继续管理其生命周期和健康状况。
- en: '[Example 6-1](#an_example_install_configdotyaml_to_pro) defines a set of options
    for provisioning an OpenShift cluster on AWS. [Example 6-2](#an_example_install_configdotyaml_to_pr)
    defines how to provision an OpenShift cluster on Microsoft Azure, while [Example 6-3](#an_example_install_configdotyaml_to_p)
    defines the equivalent configuration for Google Cloud Platform. [Example 6-4](#an_example_install_configdotyaml_to)—you
    guessed it!—provides an example configuration for VMware vSphere. With the exception
    of the VMware vSphere example (which is more sensitive to your own environment),
    you can use these examples to provision your own clusters with minimal updates.
    Refer to the OpenShift Container Platform product documentation for a full examination
    of install methods.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-1](#an_example_install_configdotyaml_to_pro)定义了在AWS上配置OpenShift集群的一组选项。[示例 6-2](#an_example_install_configdotyaml_to_pr)定义了如何在Microsoft
    Azure上配置OpenShift集群，而[示例 6-3](#an_example_install_configdotyaml_to_p)定义了Google
    Cloud Platform的等效配置。[示例 6-4](#an_example_install_configdotyaml_to)—你猜对了！—提供了一个VMware
    vSphere的配置示例。除了VMware vSphere示例（更依赖于您自己的环境），您可以使用这些示例以最少的更新来配置自己的集群。请参考OpenShift
    Container Platform产品文档，详细查看安装方法。'
- en: Example 6-1\. An example *install-config.yaml* to provision an OpenShift cluster
    on AWS
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 一个示例*install-config.yaml*，用于在AWS上配置OpenShift集群。
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Example 6-2\. An example *install-config.yaml* to provision an OpenShift cluster
    on Microsoft Azure
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 一个示例*install-config.yaml*，用于在Microsoft Azure上配置OpenShift集群。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 6-3\. An example *install-config.yaml* to provision an OpenShift cluster
    on Google Cloud Platform
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. 一个示例*install-config.yaml*，用于在Google Cloud Platform上配置OpenShift集群。
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example 6-4\. An example *install-config.yaml* to provision an OpenShift cluster
    on VMware vSphere
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. 一个示例*install-config.yaml*，用于在VMware vSphere上配置OpenShift集群。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Any one of these *install-config.yaml* files can be used to provision your
    cluster using the following command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些*install-config.yaml*文件中的任何一个都可以使用以下命令来配置您的集群：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note how each example shares some of the same options, notably the `clusterName`
    and `baseDomain` that will be used to derive the default network address of the
    cluster (applications will be hosted by default at `https://*.apps.clusterName.baseDomain`
    and the API endpoint for OpenShift will be available at `https://api.clusterName.baseDomain:6443`).
    When the openshift-installer runs, DNS entries on the cloud provider (e.g., Route
    53 in the case of AWS) will be created and linked to the appropriate network load
    balancers (also created by the install process), that in turn resolve to IP addresses
    running within the VPC.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个示例都共享一些相同的选项，特别是`clusterName`和`baseDomain`，这些选项将用于派生集群的默认网络地址（应用程序将默认托管在`https://*.apps.clusterName.baseDomain`，OpenShift的API端点将在`https://api.clusterName.baseDomain:6443`可用）。当openshift-installer运行时，云提供商的DNS条目（例如AWS的Route
    53）将被创建并链接到由安装过程创建的适当网络负载均衡器，后者又解析为VPC内运行的IP地址。
- en: Each example defines sections for the `controlPlane` and `compute` that correspond
    to `MachineSet`s that will be created and managed. We’ll talk about how these
    relate to operators within the cluster shortly. More than one `MachinePool` within
    the `compute` section can be specified. Both the `controlPlane` and `compute`
    sections provide configurability for the compute hosts and can customize which
    availability zones are used. Settings including the type (or size) for each host
    and the options for what kind of storage is attached to the hosts are also available,
    but reasonable defaults will be chosen if omitted.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个示例定义了与将要创建和管理的 `MachineSet` 相对应的 `controlPlane` 和 `compute` 部分。我们将讨论这些与集群内运算符的关系。在
    `compute` 部分可以指定一个以上的 `MachinePool`。`controlPlane` 和 `compute` 部分提供了计算主机的可配置性，并可以自定义使用哪些可用区。如果省略，将选择合理的默认设置，包括每个主机的类型（或大小）以及与主机连接的存储选项。
- en: Now, if we compare where the *install-config.yaml* properties vary for each
    substrate, we will find cloud-specific options within the `platform` sections.
    There is a global `platform` to specify which region the cluster should be created
    within, as well as `platform` sections under each of the `controlPlane` and `compute`
    sections to override settings for each provisioned host.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们比较每个基础设施在 *install-config.yaml* 属性上的不同之处，我们会发现在 `platform` 部分中有特定于云的选项。全局的
    `platform` 指定了集群应创建在哪个区域，同时在每个 `controlPlane` 和 `compute` 部分下也有 `platform` 部分，用于覆盖每个预配主机的设置。
- en: As introduced in [Chapter 5](ch05.html#continuous_delivery_across_clusters),
    the [Open Cluster Management](https://oreil.ly/D1UvC) project is a new approach
    to managing the multicluster challenges that most cluster maintainers encounter.
    [Chapter 5](ch05.html#continuous_delivery_across_clusters) discussed how applications
    can be distributed easily across clusters. Now let’s look at how the cluster provisioning,
    upgrade, and decommissioning process can be driven using Open Cluster Management.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 5 章](ch05.html#continuous_delivery_across_clusters) 中介绍的，[Open Cluster
    Management](https://oreil.ly/D1UvC) 项目是管理大多数集群维护者面临的多集群挑战的新方法。[第 5 章](ch05.html#continuous_delivery_across_clusters)
    讨论了如何轻松地在多个集群之间分发应用程序。现在让我们看看如何使用 Open Cluster Management 驱动集群预配、升级和退役过程。
- en: In the following example, we will walk through creating a new cluster on a cloud
    provider. The underlying behavior is leveraging the same openshift-install process
    that we just discussed. Once provisioned, the Open Cluster Management framework
    will install an agent that runs as a set of pods on the new cluster. We refer
    to this agent as a `klusterlet`, mimicking the naming of the `kubelet` process
    that runs on nodes that are part of a Kubernetes cluster.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将演示如何在云提供商上创建一个新的集群。其底层行为利用了我们刚刚讨论过的 openshift-install 进程。一旦预配完成，Open
    Cluster Management 框架将安装一个代理作为新集群上一组 Pod 运行。我们将这个代理称为 `klusterlet`，模仿运行在 Kubernetes
    集群节点上的 `kubelet` 进程的命名。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The following assumes the user has already set up the Open Cluster Management
    project or RHACM for Kubernetes as described in [Chapter 5](ch05.html#continuous_delivery_across_clusters).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下假设用户已经按 [第 5 章](ch05.html#continuous_delivery_across_clusters) 中描述的设置好了 Open
    Cluster Management 项目或 RHACM for Kubernetes。
- en: From the RHACM for Kubernetes web console, open the Automate Infrastructure
    > Clusters page and click on the action to “Create cluster” as shown in [Figure 6-3](#the_cluster_overview_page_allows_you_to).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从 RHACM for Kubernetes 的 Web 控制台，打开“自动化基础设施 > 集群”页面，并点击“创建集群”操作，如 [图 6-3](#the_cluster_overview_page_allows_you_to)
    所示。
- en: '![](assets/hcok_0603.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0603.png)'
- en: Figure 6-3\. The cluster overview page allows you to provision new clusters
    from the console
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 集群概览页面允许您从控制台预配新集群
- en: The “Create cluster” action shown in [Figure 6-4](#cluster_creation_form_via_rhacm_for_kube)
    opens to a form where you provide a name and select one of the available cloud
    providers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 [图 6-4](#cluster_creation_form_via_rhacm_for_kube) 中显示的“创建集群”操作，进入一个表单，您需要提供一个名称并选择可用的云提供商。
- en: '![](assets/hcok_0604.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0604.png)'
- en: Figure 6-4\. Cluster creation form via RHACM for Kubernetes
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 通过 RHACM for Kubernetes 的集群创建表单
- en: 'Next, select a version of OpenShift to provision. The available list maps directly
    to the `ClusterImageSet`s available on the hub cluster. You can introspect these
    images with the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，选择要预配的 OpenShift 版本。可用列表直接映射到 Hub 集群上的 `ClusterImageSet`，您可以使用以下命令检查这些镜像：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Further down the page, as shown in [Figure 6-5](#select_your_release_image_left_parenthes),
    you will also need to specify a provider connection. In the case of AWS, you will
    need to provide the Access ID and Secret Key to allow API access by the installation
    process with your AWS account.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 页面下方如图 [6-5](#select_your_release_image_left_parenthes) 所示，您还需要指定提供者连接。在AWS的情况下，您需要提供访问ID和密钥，以允许安装过程通过您的AWS账户访问API。
- en: '![](assets/hcok_0605.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0605.png)'
- en: Figure 6-5\. Select your release image (the version to provision) and your provider
    connection
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 选择您的发布镜像（要预配的版本）和提供者连接
- en: At this point, you can simply click Create and the cluster will be provisioned.
    However, let’s walk through how the `MachinePool` operator allows you to manage
    `MachineSet`s within the cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可以简单地点击“创建”，即可开始集群的预配过程。然而，让我们来看一下 `MachinePool` 操作员如何允许您在集群内管理 `MachineSet`。
- en: Customize the “Worker pool1” `NodePool` for your desired region and availability
    zones. See Figures [6-6](#customizing_the_region_and_zones_for_the) and [6-7](#customizing_the_availability_zone)
    for an example of what this will look like in the form. You can amend these options
    after the cluster is provisioned as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义“Worker pool1” `NodePool`，以符合您期望的区域和可用性区域。参见 [图 6-6](#customizing_the_region_and_zones_for_the)
    和 [图 6-7](#customizing_the_availability_zone)，了解此表单中的示例。在集群预配后，您也可以修改这些选项。
- en: '![](assets/hcok_0606.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0606.png)'
- en: Figure 6-6\. Customizing the region and zones for the cluster workers
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 自定义集群工作节点的区域和区域可用性区域
- en: '![](assets/hcok_0607.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0607.png)'
- en: Figure 6-7\. Customize the availability zones within the region that are valid
    to host workers
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 自定义区域内的可用性区域，以便托管工作节点
- en: A final summary of your confirmed choices is presented for review in [Figure 6-8](#confirmed_options_in_form).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了审查确认的选择，您可以查看在 [图 6-8](#confirmed_options_in_form) 中呈现的最终摘要。
- en: '![](assets/hcok_0608.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0608.png)'
- en: Figure 6-8\. Confirmed options selected in the form
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. 确认在表单中选择的选项
- en: Once you have made your final customizations, click Create to begin the provisioning
    process as shown in [Figure 6-9](#rhacm_web_console_view_that_includes_lin). The
    web console provides a view that includes links to the provisioning logs for the
    cluster. If the cluster fails to provision (e.g., due to a quota restriction in
    your cloud account), the provisioning logs provide clues on what to troubleshoot.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 完成最终的自定义后，点击“创建”即可开始预配过程，如 [图 6-9](#rhacm_web_console_view_that_includes_lin)
    所示。Web 控制台提供的视图包括指向集群预配日志的链接。如果集群无法预配（例如，由于云账户中的配额限制），预配日志将提供故障排除的线索。
- en: '![](assets/hcok_0609.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0609.png)'
- en: Figure 6-9\. RHACM web console view that includes links to the provisioning
    logs for the cluster
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. RHACM Web 控制台视图，包括集群预配日志的链接
- en: Behind the form editor, a number of Kubernetes API objects are created. A small
    number of these API objects are cluster scoped (`ManagedCluster` in particular).
    The `ManagedCluster` controller will ensure that a project (namespace) exists
    that maps to the cluster name. Other controllers, including the controller that
    begins the provisioning process, will use the `cluster` project (namespace) to
    store resources that provide an API control surface for provisioning. Let’s take
    a look at a subset of these that you should become familiar with.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在表单编辑器后面，会创建多个 Kubernetes API 对象。其中少量的 API 对象是集群范围的（特别是 `ManagedCluster`）。`ManagedCluster`
    控制器将确保存在映射到集群名称的项目（命名空间）。其他控制器，包括启动预配过程的控制器，将使用 `cluster` 项目（命名空间）存储提供用于预配的 API
    控制表面的资源。让我们看一下您应该熟悉的这些对象的子集。
- en: ManagedCluster
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ManagedCluster
- en: '`ManagedCluster` (API group: cluster.open-cluster-management.io/v1; cluster
    scoped) recognizes that a remote cluster is under the control of the hub cluster.
    The agent that runs on the remote cluster will attempt to create `ManagedCluster`
    if it does not exist on the hub, and it must be accepted by a user identity on
    the hub with appropriate permissions. You can see the example created in [Example 6-5](#example_of_the_managedcluster_api_object).
    Note that labels for this object will drive placement decisions later in this
    chapter.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`ManagedCluster`（API 组：cluster.open-cluster-management.io/v1；集群范围）识别远程集群在中心集群控制下。在远程集群上运行的代理将尝试创建
    `ManagedCluster`，如果在中心集群上不存在，则必须由具有适当权限的用户身份接受。您可以在 [示例 6-5](#example_of_the_managedcluster_api_object)
    中看到创建的示例。请注意，此对象的标签将在本章后期的位置决策中起作用。'
- en: Example 6-5\. Example of the `ManagedCluster` API object
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. `ManagedCluster` API 对象示例
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ClusterDeployment
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClusterDeployment
- en: '`ClusterDeployment` (API group: hive.openshift.io/v1; namespace scoped) controls
    the provisioning and decommissioning phases of the cluster. A controller on the
    hub takes care of running the openshift-installer on your behalf. If the cluster
    creation process fails for any reason (e.g., you encounter a quota limit within
    your cloud account), the cloud resources will be destroyed and another attempt
    will be made after a waiting period to reattempt successful creation of the cluster.
    Unlike traditional automation methods that “try once” and require user intervention
    upon failure, the Kubernetes reconcile loop for this API kind will continue to
    attempt to create the cluster (with appropriate waiting periods in between) as
    shown in [Example 6-6](#example_clusterdeployment_created_by_the). You can also
    create these resources directly through the `oc` or `kubectl` like any Kubernetes
    native resource.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterDeployment`（API 组：hive.openshift.io/v1；命名空间范围）控制集群的配置和销毁阶段。中心上的控制器负责代表您运行
    openshift-installer。如果由于任何原因（例如，在您的云帐户中遇到配额限制），集群创建过程失败，那么将销毁云资源，并在等待一段时间后尝试重新创建集群。与“仅尝试一次”且失败后需要用户干预的传统自动化方法不同，此
    API 种类的 Kubernetes 协调循环将继续尝试创建集群（在之间适当的等待期之后），如[示例 6-6](#example_clusterdeployment_created_by_the)所示。您还可以像任何
    Kubernetes 本机资源一样直接通过 `oc` 或 `kubectl` 创建这些资源。'
- en: Example 6-6\. Example `ClusterDeployment` created by the form
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-6\. 通过表单创建的 `ClusterDeployment` 示例
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: KlusterletAddonConfig
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KlusterletAddonConfig
- en: '`KlusterletAddonConfig` (API group: agent.open-cluster-management.io/v1; namespace
    scoped) represents the capabilities that should be provided on the remote agent
    that manages the cluster. In [Example 6-7](#an_example_of_the_klusterletaddonconfig),
    the Open Cluster Management project refers to the remote agent as a `klusterlet`,
    mirroring the language of `kubelet`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`KlusterletAddonConfig`（API 组：agent.open-cluster-management.io/v1；命名空间范围）表示应在管理集群的远程代理上提供的功能。在[示例
    6-7](#an_example_of_the_klusterletaddonconfig)中，Open Cluster Management 项目将远程代理称为
    `klusterlet`，反映了 `kubelet` 的语言。'
- en: Example 6-7\. An example of the `KlusterletAddonConfig` API object
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. `KlusterletAddonConfig` API 对象示例
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: MachinePool
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MachinePool
- en: '`MachinePool` (API group: hive.openshift.io/v1; namespace -scoped) allows you
    to create a collection of hosts that work together and share characteristics.
    You might use a `MachinePool` to group a set of compute capacity that supports
    a specific team or line of business. As we will see in the next section, `MachinePool`
    also allows you to dynamically size your cluster. Finally, the status provides
    a view into the `MachineSet`s that are available on `ManagedCluster`. See [Example 6-8](#the_machinepool_api_object_provides_a_co)
    for the example `MachinePool` created earlier, which provides a control surface
    to scale the number of replicas up or down within the pool and status about the
    `MachineSet`s under management on the remote cluster.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`MachinePool`（API 组：hive.openshift.io/v1；命名空间范围）允许您创建一组共同工作并共享特征的主机。您可以使用 `MachinePool`
    将支持特定团队或业务线的计算能力集中在一起。正如我们将在下一节中看到的那样，`MachinePool` 还允许您动态调整集群的大小。最后，状态提供了对 `ManagedCluster`
    上可用的 `MachineSet` 的视图。有关之前创建的示例 `MachinePool` 的控制界面以及有关远程集群管理下的 `MachineSet` 的状态，请参见[示例
    6-8](#the_machinepool_api_object_provides_a_co)。'
- en: Example 6-8\. The example `MachinePool` API object
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-8\. `MachinePool` API 对象示例
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once provisioned, the address for the Kubernetes API server and OpenShift web
    console will be available from the cluster details page. You can use these coordinates
    to open your web browser and authenticate with the new cluster as the kubeadmin
    user. You can also access the `KUBECONFIG` certificates that allow you command-line
    access to the cluster.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置完成，可以从集群详细信息页面获取 Kubernetes API 服务器和 OpenShift Web 控制台的地址。您可以使用这些坐标打开 Web
    浏览器，并作为 kubeadmin 用户进行认证与新集群通信。您还可以访问 `KUBECONFIG` 证书，以便通过命令行访问集群。
- en: You can download the `KUBECONFIG` authorization as shown in [Figure 6-10](#downloading_the_kubeconfig_authorization)
    for the new cluster from the RHACM web console under the cluster overview page
    or access it from the command line. From the web console, click on the cluster
    name in the list of clusters to view an overview of that cluster. Once the provisioning
    process has completed, you will be able to download the `KUBECONFIG` file that
    will allow you command-line access to the cluster.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从RHACM Web控制台的群集概览页面下载新群集的`KUBECONFIG`授权，如[图 6-10](#downloading_the_kubeconfig_authorization)所示，或通过命令行访问。从Web控制台，单击群集名称以查看该群集的概述。一旦完成配置过程，您将能够下载`KUBECONFIG`文件，从而可以通过命令行访问群集。
- en: '![](assets/hcok_0610.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0610.png)'
- en: Figure 6-10\. Downloading the kubeconfig authorization from the RHACM web console
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. 从RHACM Web控制台下载kubeconfig授权
- en: From the command line, you can retrieve the information stored in a secret under
    the cluster project (namespace) as in [Example 6-9](#output_of_the_cluster_kubeconfig_filedot).
    Save the file contents and configure your `KUBECONFIG` environment variable to
    point to the location of the file. Then `oc` will be able to run commands against
    the remote cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令行，您可以像在[示例 6-9](#output_of_the_cluster_kubeconfig_filedot)中那样检索存储在群集项目（命名空间）下的秘密信息。保存文件内容并配置您的`KUBECONFIG`环境变量以指向文件的位置。然后`oc`将能够对远程群集运行命令。
- en: Example 6-9\. Output of the cluster `KUBECONFIG` file
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-9\. 群集`KUBECONFIG`文件的输出
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that our cluster is up and running, let’s walk through how we can scale
    the cluster. We will review this concept from the context of the oc CLI.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的群集已经启动运行，让我们从oc CLI的上下文中讨论如何扩展群集。我们将从以下示例中审视这个概念。
- en: First, open two terminals and configure the `KUBECONFIG` or context for each
    of the hub clusters and our newly minted `mycluster`. See Examples [6-10](#example_six_onezero_an_example_of_what_t)
    and [6-11](#an_example_of_how_terminal_two_will_look) for examples of what each
    of the two separate terminals will look like after you run these commands. Note
    the tip to override your PS1 shell prompt temporarily to avoid confusion when
    running commands on each cluster.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，打开两个终端，并为每个中心群集和我们新建的`mycluster`配置`KUBECONFIG`或上下文。查看示例 [6-10](#example_six_onezero_an_example_of_what_t)
    和 [6-11](#an_example_of_how_terminal_two_will_look) 来了解运行这些命令后两个独立终端的外观示例。请注意，临时覆盖您的PS1
    shell提示以避免在每个群集上运行命令时混淆。
- en: Example 6-10\. An example of what Terminal 1 will look like
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-10\. 终端1的外观示例
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Example 6-11\. An example of how Terminal 2 will look
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-11\. 终端2的外观示例
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you should have Terminal 1 with a prompt including `hubcluster` and Terminal
    2 with a prompt including `mycluster`. We will refer to these terminals by the
    appropriate names through the rest of the example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该有一个包含`hubcluster`的终端1和一个包含`mycluster`的终端2。我们将在后续示例中使用适当的名称引用这些终端。
- en: In the following walk-through, we will review the `MachineSet` API, which underpins
    how an OpenShift cluster understands compute capacity. We will then scale the
    size of our managed cluster from the hub using the `MachinePool` API that we saw
    earlier.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将审视`MachineSet` API，这是OpenShift群集了解计算能力的基础。然后，我们将使用我们之前看到的`MachinePool`
    API来扩展我们管理的群集的大小。
- en: 'In the `mycluster` terminal, review the `MachineSet`s for your cluster:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mycluster`终端中，查看您的群集的`MachineSet`：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Each `MachineSet` will have a name following the pattern: `<clusterName>-<five-character
    identifier>-<machinePoolName>-<availabilityZone>`. In your cluster, you should
    see counts for the desired number of machines per `MachineSet`, the current number
    of machines that are available, and the current number that are considered `Ready`
    to be integrated as nodes into the OpenShift cluster. Note that these three counts
    should generally be equivalent and should only vary when the cluster is in a transition
    state (adding or removing machines) or when an underlying availability problem
    in the cluster causes one or more machines to be considered unhealthy. For example,
    when you edit a `MachineSet` to increase the desired replicas, you will see the
    `Desired` count increment by one for that `MachineSet`. As the machine is provisioned
    and proceeds to boot and configure the `kubelet`, the `Current` count will increment
    by one. Finally, as the `kubelet` registers with the Kubernetes API control plane
    and marks the node as `Ready`, the `Ready` count will increment by one. If at
    any point the machine becomes unhealthy, the `Ready` count may decrease. Similarly,
    if you reduced the `Desired` count by one, you would see the same staggered reduction
    in counts as the machine proceeds through various life-cycle states until it is
    removed.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`MachineSet`将按照以下模式命名：`<clusterName>-<five-character identifier>-<machinePoolName>-<availabilityZone>`。在您的集群中，您应该看到每个`MachineSet`的期望机器数量、当前可用的机器数量以及准备作为节点集成到OpenShift集群中的当前数量。请注意，这三个计数通常应该相等，只有在集群处于过渡状态（添加或删除机器）或者当集群中的底层可用性问题导致一个或多个机器被视为不健康时才会有所不同。例如，当您编辑`MachineSet`以增加所需副本时，您将看到该`MachineSet`的`Desired`计数增加一个。当机器被配置和启动`kubelet`并注册到Kubernetes
    API控制平面并将节点标记为`Ready`时，`Current`计数将增加一个。最后，如果机器变得不健康，`Ready`计数可能会减少。类似地，如果将`Desired`计数减少一个，则会看到与机器通过各种生命周期状态直到移除的减少计数相同的错位减少。
- en: 'Next, in the hub terminal, review the `worker MachinePool` defined for the
    managed cluster:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在中心终端中查看为托管集群定义的`worker MachinePool`：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will increase the size of the managed cluster `mycluster` by one node:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将增加托管集群`mycluster`的大小一个节点：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The size of the worker node will be determined by the values set in the `MachinePool
    mycluster-worker`. The availability zone of the new node will be determined by
    the `MachinePool` controller, where nodes are distributed across availability
    zones as evenly as possible.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点的大小将由`MachinePool mycluster-worker`中设置的值确定。新节点的可用区将由`MachinePool`控制器确定，其中节点尽可能均匀地分布在可用区。
- en: 'Immediately after you have patched the `MachinePool` to increase the number
    of desired replicas, rerun the command to view the `MachineSet` on your managed
    cluster:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在您修补了`MachinePool`以增加所需副本数量后，请重新运行命令以查看托管集群上的`MachineSet`：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Over the course of a few minutes, you should see the new node on the managed
    cluster transition from `Desired` to `Current` to `Ready`, with a final result
    that looks like the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，您应该会看到托管集群上的新节点从`Desired`到`Current`再到`Ready`的过渡，最终结果看起来如下输出：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s recap what we’ve just seen. First, we used a declarative method (*install-config.yaml*)
    to provision our first cluster, called a hub. Next, we used the hub to provision
    the first managed cluster in our fleet. That managed cluster was created under
    the covers using the same IPI method but with the aid of Kubernetes API and continuous
    reconcilers that ensure that the running cluster matches the `Desired` state.
    One of the APIs that governs the `Desired` state is the `MachinePool` API on the
    hub cluster. Because our first fleet member, `mycluster`, was created from the
    hub cluster, we can use the `MachinePool API` to govern how `mycluster` adds or
    removes nodes. Or indeed, we can create additional `MachinePool`s that add capacity
    to the cluster.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下刚才看到的内容。首先，我们使用声明性方法（*install-config.yaml*）来配置我们的第一个集群，称为中心集群。接下来，我们使用中心集群来为我们的舰队中的第一个托管集群进行配置。该托管集群是在幕后使用相同的IPI方法创建的，但借助于Kubernetes
    API和持续协调器来确保运行中的集群与`Desired`状态匹配。管理`Desired`状态的API之一是中心集群上的`MachinePool` API。因为我们的第一个舰队成员`mycluster`是从中心集群创建的，所以我们可以使用`MachinePool
    API`来管理`mycluster`如何添加或移除节点。或者，我们可以创建额外的`MachinePool`来增加集群的容量。
- en: Throughout the process, the underlying infrastructure substrate was completely
    managed through operators. The `MachineSet` operator on the managed cluster was
    given updated instructions by the `MachinePool` operator on the hub to grow the
    number of machines available in one of the `MachineSet`s supporting `mycluster`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，底层基础设施基板完全通过操作符进行管理。在管理的集群上，`MachineSet`操作符通过来自中心的`MachinePool`操作符的更新指令，以增加支持`mycluster`中一个`MachineSet`中的机器数量。
- en: Note
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We will use the term *infrastructure substrate* as a catchall term to refer
    to the compute, network, and storage resources provided by bare metal virtualization
    within your datacenter or virtualization offered by a public cloud provider.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用术语*基础设施基板*作为一个统称术语，用来指代由您的数据中心提供的裸金属虚拟化或公共云提供商提供的虚拟化中的计算、网络和存储资源。
- en: Upgrading Your Clusters to the Latest Version of Kubernetes
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将您的集群升级到最新的Kubernetes版本
- en: 'As we saw with `MachinePool`s and `MachineSet`s, operators provide a powerful
    way to abstract the differences across infrastructure substrates, allowing an
    administrator to declaratively specify the desired outcome. An OpenShift cluster
    is managed by the CVO, which acts as an “operator of operators” pattern to manage
    operators for each dimension of the cluster’s configuration (authentication, networking,
    machine creation, bootstrapping, and removal, and so on). Every cluster will have
    a `Cluster​Ver⁠sion` API object named `version`. You can retrieve the details
    for this object with the command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在`MachinePool`和`MachineSet`中看到的那样，运算符提供了一种强大的抽象方式，可以在基础设施基板之间隐藏差异，允许管理员声明性地指定期望的结果。OpenShift集群由CVO管理，其作为“操作符的操作符”模式来管理集群配置的每个维度的操作符（包括认证、网络、机器创建、引导和移除等）。每个集群都将有一个名为`version`的`ClusterVersion`
    API对象。您可以使用以下命令检索此对象的详细信息：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`ClusterVersion` specifies a “channel” to seek available versions for the cluster
    and a desired version from that channel. Think of a channel as an ongoing list
    of available versions (e.g., 4.5.1, 4.5.2, 4.5.7, and so on). There are channels
    for “fast” adoption of new versions, as well as “stable” versions. The fast channels
    produce new versions quickly. Coupled with connected telemetry data from a broad
    source of OpenShift clusters running across infrastructure substrates and industries,
    fast channels allow the delivery and validation of new releases very quickly (on
    order of weeks or days). As releases in fast channels have enough supporting evidence
    that they are broadly acceptable across the global fleet of OpenShift clusters,
    versions are promoted to stable channels. Hence, the list of versions within a
    channel is not always consecutive. An example `ClusterVersion` API object is represented
    in [Example 6-12](#an_example_clusterversion_api_object_tha).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterVersion`指定了一个“通道”，以寻找集群可用版本以及从该通道获取的期望版本。将通道视为可用版本的持续列表（例如，4.5.1、4.5.2、4.5.7等）。有用于“快速”采用新版本的通道，以及“稳定”版本的通道。快速通道快速产生新版本。结合来自运行在不同基础设施基板和行业的OpenShift集群的广泛数据的连接遥测数据，快速通道允许非常快速地交付和验证新版本（以周或天为单位）。由于快速通道中的发布具有足够的支持证据，可以广泛接受全球OpenShift集群，因此版本被提升到稳定通道。因此，通道内版本列表并不总是连续的。示例`ClusterVersion`
    API对象在[示例 6-12](#an_example_clusterversion_api_object_tha)中表示。'
- en: Example 6-12\. An example `ClusterVersion` API object that records the version
    history for the cluster and the desired version—changing the desired version will
    cause the operator to begin applying updates to achieve the goal
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-12. 记录集群版本历史和期望版本的示例`ClusterVersion` API对象-更改期望版本将导致操作符开始应用更新以实现目标
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Upgrading a version of Kubernetes along with all other supporting APIs and infrastructure
    around it can be a daunting task. The operator that controls the life cycle of
    all of the container images is known informally as [“Cincinnati”](https://oreil.ly/6HIZd)
    and formally as the *OpenShift Update Service* (OSUS). OSUS (or Cincinnati) maintains
    a connected graph of versions and tracks which “walks” or “routes” within the
    graph are known as good upgrade paths. For example, an issue may be detected in
    early release channels that indicates that the upgrade from 4.4.23 to 4.5.0 to
    4.5.18 may be associated with a specific problem. A fix can be released to create
    a new release 4.4.24 that then allows a successful and predictable upgrade from
    4.4.23 to 4.4.24 to 4.5.0 to 4.5.18\. The graph records the successive nodes that
    must be walked to ensure success.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 升级Kubernetes版本以及其周围的所有其他支持API和基础设施可能是一项艰巨的任务。负责所有容器映像生命周期的操作符通常称为[“Cincinnati”](https://oreil.ly/6HIZd)，正式称为*OpenShift更新服务*（OSUS）。
    OSUS（或Cincinnati）维护版本的连接图，并跟踪在图中已知的良好升级路径。例如，可能在早期发布通道中检测到问题，表明从4.4.23升级到4.5.0到4.5.18可能与特定问题相关。可以发布修复程序以创建新版本4.4.24，然后允许成功和可预测地从4.4.23升级到4.4.24到4.5.0到4.5.18。图记录必须走过的连续节点，以确保成功。
- en: However, the OSUS operator removes the guesswork, allowing the cluster administrator
    to specify the desired version from the channel. From there, the CVO will carry
    out the following tasks:^([2](ch06.html#ch01fn37))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OSUS运算符消除了猜测工作，允许集群管理员从通道中指定所需的版本。从那里，CVO将执行以下任务：^([2](ch06.html#ch01fn37))
- en: Upgrade the Kubernetes and OpenShift control plane pods, including etcd.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级Kubernetes和OpenShift控制平面Pod，包括etcd。
- en: Upgrade the operating system for the nodes running the control plane pods.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级运行控制平面Pod的节点操作系统。
- en: Upgrade the cluster operators controlling aspects like authentication, networking,
    storage, and so on.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级控制认证、网络、存储等方面的集群运算符。
- en: For nodes managed by the `MachineConfigOperator`, upgrade the operating system
    for the nodes running the data plane pods (user workload).
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于由`MachineConfigOperator`管理的节点，需要升级运行数据平面Pod（用户工作负载）的操作系统。
- en: The upgrade takes place in a rolling fashion, avoiding bursting the size of
    the cluster or taking out too much capacity at the same time. Because the control
    plane is spread across three machines, as each machine undergoes an operating
    system update and reboot, the other two nodes maintain the availability of the
    Kubernetes control plane, including the datastore (etcd), the scheduler, the controller,
    the Kubernetes API server, and the network ingress controller.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 升级以滚动方式进行，避免集群规模暴增或同时削减太多容量。由于控制平面分布在三台机器上，每台机器进行操作系统更新和重启时，其他两个节点维护Kubernetes控制平面的可用性，包括数据存储（etcd）、调度器、控制器、Kubernetes
    API服务器和网络入口控制器。
- en: When the data plane is upgraded, the upgrade process will respect `PodDisruptionBudget`s
    and look for feedback about the health of OpenShift and user workloads running
    on each node by means of liveness and readiness probes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据平面进行升级时，升级过程将尊重`PodDisruptionBudget`并通过活跃性和就绪性探针查找有关OpenShift和运行在每个节点上的用户工作负载健康状态的反馈。
- en: Note
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Sometimes the group of clusters under management is referred to as a *fleet*.
    Individual clusters under management may be referred to as *fleet members*, primarily
    to distinguish them from the hub cluster that is responsible for management.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有时管理下的集群组被称为*fleet*。主要将管理下的单个集群称为*fleet members*，主要是为了区别它们与负责管理的中枢集群。
- en: From the RHACM web console, you can manipulate the desired version of a managed
    cluster for a single fleet member or the entire fleet. From the console, choose
    the “Upgrade cluster” action for any cluster that shows “Upgrade available.” Recall
    from the discussion around channels that not every channel may have an upgrade
    currently available. Additionally, the list of versions may not be consecutive.
    Figures [6-11](#actions_permitted_on_a_cluster_allow_a_f), [6-12](#the_list_of_available_versions_is_provid),
    and [6-13](#multiple_clusters_may_be_selected_for_up) provide examples of what
    this actually looks like for a specific cluster or for multiple clusters.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 RHACM Web 控制台，您可以为单个成员或整个集群控制期望的托管集群版本。从控制台中选择任何显示“可升级”的集群的“升级集群”操作。回想一下关于通道的讨论，并非每个通道当前都有可用的升级。此外，版本列表可能不是连续的。图示
    [6-11](#actions_permitted_on_a_cluster_allow_a_f)、[6-12](#the_list_of_available_versions_is_provid)
    和 [6-13](#multiple_clusters_may_be_selected_for_up) 提供了针对特定集群或多个集群的实际展示示例。
- en: '![](assets/hcok_0611.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0611.png)'
- en: Figure 6-11\. Actions permitted on a cluster allow a fleet manager to upgrade
    the desired version of a cluster
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-11\. 可以在集群上执行的操作允许集群管理员升级所需的版本
- en: '![](assets/hcok_0612.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0612.png)'
- en: Figure 6-12\. The list of available versions is provided for user selection
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-12\. 提供给用户选择的可用版本列表
- en: '![](assets/hcok_0613.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0613.png)'
- en: Figure 6-13\. Multiple clusters may be selected for upgrade, and the versions
    available will vary based on the cluster’s attached channel configuration in the
    `ClusterVersion` object
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-13\. 可以选择多个集群进行升级，可用的版本会根据`ClusterVersion`对象中附加的通道配置而变化
- en: A core topic for this book is how to manage your clusters as a fleet, and for
    that, we will rely on policies. The preceding discussion should provide a foundation
    for you to understand the moving parts and see that you can explicitly trigger
    upgrade behavior across the fleet. In [Chapter 7](ch07.html#multicluster_policy_configuration),
    we will discuss how we can control upgrade behavior across the fleet by policy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的核心主题之一是如何将您的集群作为一个整体进行管理，为此，我们将依赖策略。前面的讨论应该为您提供了一个理解各个部分并明确如何在整个集群中触发升级行为的基础。在[第7章](ch07.html#multicluster_policy_configuration)中，我们将讨论如何通过策略控制整个集群的升级行为。
- en: Summary of Multicloud Cluster Provisioning
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多云集群配置总结
- en: Throughout our example, the specific infrastructure substrate showed up in a
    few declarative APIs, specifically represented by the *install-config.yaml* for
    the hub cluster and as part of the secrets referenced by the `ClusterDeployment`
    API object for the managed cluster. However, the action of provisioning a new
    cluster and adding or removing nodes for that fleet member was completely driven
    through Kubernetes API objects.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，特定的基础设施底层技术通过几个声明性 API 出现，具体来说是作为*install-config.yaml*用于中心集群，并作为`ClusterDeployment`
    API 对象的一部分引用的秘密。然而，通过 Kubernetes API 对象来进行新集群的配置和添加或移除成员节点完全是由此驱动的。
- en: In addition, the upgrade life cycle managed through the CVO is consistent across
    supported infrastructure substrates. Hence, regardless if you provision an OpenShift
    cluster on a public cloud service or in your datacenter, you can still declaratively
    manage the upgrade process. The powerful realization you should now understand
    is that managing the infrastructure substrate for OpenShift clusters in multicloud
    scenarios can be completely abstracted away from many basic cluster-provisioning
    life-cycle operations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过 CVO 管理的升级生命周期在支持的基础设施底层技术上保持一致。因此，无论您是在公共云服务上还是在您的数据中心中提供 OpenShift 集群，您仍然可以完全声明式地管理升级过程。您现在应该理解到的强大功能是，在多云场景下管理
    OpenShift 集群的基础设施底层技术可以完全抽象出许多基本的集群配置生命周期操作。
- en: Beyond controlling the capacity of your fleet from the hub, you can assign policies
    with Open Cluster Management and drive behavior like fleet upgrades. We will see
    an example of fleet upgrade by policy in [Chapter 7](ch07.html#multicluster_policy_configuration).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从中心控制您的集群的容量外，您还可以使用 Open Cluster Management 分配策略并驱动像集群升级这样的行为。我们将在[第7章](ch07.html#multicluster_policy_configuration)中看到一个通过策略进行集群升级的示例。
- en: OpenShift as a Service
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenShift 作为服务
- en: The previous section described how you can abstract the provisioning and life
    cycle of OpenShift across multiple infrastructure substrates. Under the model
    we’ve outlined, you are responsible for the availability of your clusters. For
    budget or organizational reasons, you may choose to consider a managed service
    for OpenShift or Kubernetes. Using a vendor-provided “OpenShift as a Service”
    or “Kubernetes as a Service” can change how you interact with some dimensions,
    including cluster creation or decommissioning. However, your applications will
    run consistently regardless of whether the vendor manages the underlying infrastructure
    or you manage it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节描述了如何在多个基础设施基板上抽象化OpenShift的供应和生命周期。根据我们所概述的模型，您需要负责集群的可用性。基于预算或组织原因，您可能选择考虑使用OpenShift或Kubernetes的托管服务。使用供应商提供的“OpenShift即服务”或“Kubernetes即服务”可以改变您与某些维度（包括集群创建或退役）的交互方式。然而，无论供应商是否管理基础架构，您的应用程序将始终保持一致运行。
- en: Azure Red Hat OpenShift
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure Red Hat OpenShift
- en: '[Azure Red Hat OpenShift](https://oreil.ly/0OWW7) is integrated into the Microsoft
    Azure ecosystem, including Azure billing. Other aspects, including single sign-on,
    are automatically configured with Azure Active Directory, simplifying how you
    expose capabilities to your organization, particularly if you are already consuming
    other services on Azure. The underlying service is maintained by a partnership
    between Microsoft and Red Hat.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[Azure Red Hat OpenShift](https://oreil.ly/0OWW7)已整合到Microsoft Azure生态系统中，包括Azure计费。其他方面，包括单点登录，与Azure
    Active Directory自动配置，简化了您向组织公开功能的方式，特别是如果您已经在Azure上消费其他服务。底层服务由微软和Red Hat的合作关系维护。'
- en: Red Hat OpenShift on AWS
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Red Hat OpenShift on AWS
- en: '[Red Hat OpenShift on AWS](https://oreil.ly/fX0a2) was announced at the end
    of 2020 with planned availability in 2021\. It integrates OpenShift into the Amazon
    ecosystem, allowing for access and creation through the Amazon cloud console and
    consistent billing with the rest of your Amazon account. The underlying service
    is maintained by a partnership between Amazon and Red Hat.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[Red Hat OpenShift on AWS](https://oreil.ly/fX0a2)于2020年底宣布，计划于2021年可用。它将OpenShift整合到亚马逊生态系统中，允许通过亚马逊云控制台访问和创建，并与您的亚马逊账户的其余部分一致计费。底层服务由亚马逊和Red
    Hat的合作关系维护。'
- en: Red Hat OpenShift on IBM Cloud
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Red Hat OpenShift on IBM Cloud
- en: '[Red Hat OpenShift on IBM Cloud](https://oreil.ly/gZBHU) integrates OpenShift
    consumption into the IBM Cloud ecosystem, including integration with IBM Cloud
    single sign-on and billing. In addition, IBM Cloud APIs are provided to manage
    cluster provisioning, worker nodes, and the upgrade process. These APIs allow
    separate access controls via IBM Cloud Identity and Access Management for management
    of the cluster versus the access controls used for managing resources in the cluster.
    The underlying service is maintained by IBM.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[Red Hat OpenShift on IBM Cloud](https://oreil.ly/gZBHU)将OpenShift消费整合到IBM
    Cloud生态系统中，包括与IBM Cloud单点登录和计费的集成。此外，提供了IBM Cloud API来管理集群的供应、工作节点和升级过程。这些API允许通过IBM
    Cloud身份和访问管理分别访问控制集群管理与管理集群资源所用的访问控制。底层服务由IBM维护。'
- en: OpenShift Dedicated
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenShift Dedicated
- en: '[OpenShift Dedicated](https://cloud.redhat.com) is a managed OpenShift as a
    Service offering provided by Red Hat. OpenShift clusters can be created across
    a variety of clouds from this service, in some cases under your own preexisting
    cloud account. Availability and maintenance of the cluster are handled by the
    Red Hat SRE team. The underlying service is maintained by Red Hat with options
    to bring your own cloud accounts on some supported infrastructure providers like
    AWS.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenShift Dedicated](https://cloud.redhat.com)是Red Hat提供的托管OpenShift即服务。可以从该服务在各种云中创建OpenShift集群，在某些情况下可以在您自己现有的云账户下操作。集群的可用性和维护由Red
    Hat的SRE团队处理。底层服务由Red Hat维护，并提供在一些支持的基础设施提供商如AWS上带上自己的云账户的选项。'
- en: Kubernetes as a Service
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes即服务
- en: 'In addition to vendor-managed OpenShift as a Service, many vendors offer managed
    Kubernetes as a Service distributions. These are typically where the vendor adopts
    Kubernetes and integrates it into its ecosystem. The following are some examples
    of these services:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了供应商管理的OpenShift即服务外，许多供应商还提供管理的Kubernetes即服务分发。这些通常是供应商采用Kubernetes并将其整合到其生态系统中的地方。以下是这些服务的一些示例：
- en: Amazon Elastic Kubernetes Service
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon弹性Kubernetes服务
- en: Azure Kubernetes Service
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Kubernetes Service
- en: Google Kubernetes Engine
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Kubernetes Engine
- en: IBM Cloud Kubernetes Service
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM Cloud Kubernetes服务
- en: 'Because the Kubernetes community leaves some decisions up to vendors or users
    who assemble their own distributions, each of these managed services can introduce
    some variations that you should be aware of when adopting them as part of a larger
    multicloud strategy. In particular, several specific areas in Kubernetes have
    been evolving quickly:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kubernetes 社区将一些决策留给供应商或用户来组装自己的发行版，因此在采用它们作为更大的多云战略的一部分时，您应意识到这些管理服务中可能引入的一些变化。特别是，在
    Kubernetes 的几个特定领域已经迅速发展：
- en: Cluster creation
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群创建
- en: User identity and access management
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户身份和访问管理
- en: Network routing
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络路由
- en: Pod security management
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 安全管理
- en: Role-based access control
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于角色的访问控制
- en: Value-added admission controllers
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增值准入控制器
- en: Operating system management of the worker nodes
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点的操作系统管理
- en: Different security apparatus to manage compliance
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的安全设备来管理合规性
- en: Across each dimension, a vendor providing a managed Kubernetes service must
    decide how to best integrate that aspect of Kubernetes into the cloud provider’s
    ecosystem. The core API should respond consistently by way of the [CNCF Kubernetes
    Certification process](https://oreil.ly/sWAXA). In practice, differences tend
    to arise where Kubernetes is integrated into a particular cloud ecosystem.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个维度，提供托管 Kubernetes 服务的供应商必须决定如何最佳地将 Kubernetes 的这一方面集成到云提供商的生态系统中。核心 API
    应通过 [CNCF Kubernetes 认证流程](https://oreil.ly/sWAXA) 保持一致响应。实际上，当 Kubernetes 集成到特定的云生态系统中时，差异往往会产生。
- en: For instance, in some cases a managed Kubernetes service will come with Kubernetes
    RBAC deployed and configured out of the box. Other vendors may leave it to the
    cluster creator to configure RBAC for Kubernetes. Across vendors that automatically
    configure the Kubernetes with RBAC, the set of out-of-the-box `ClusterRole`s and
    roles can vary.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在某些情况下，托管 Kubernetes 服务将自带 Kubernetes RBAC 并配置好。其他供应商可能会将 RBAC 的配置留给集群创建者来完成。对于自动配置
    Kubernetes RBAC 的供应商，出厂的 ClusterRole 和角色集合可能会有所不同。
- en: 'In other cases, the network ingress for a Kubernetes cluster can vary from
    cloud-specific extensions to use of the community network ingress controller.
    Hence, your application may need to provide alternative network ingress behavior
    based on the cloud providers that you choose to provide Kubernetes. When using
    Ingress (API group: networking.k8s.io/v1) on a particular cloud vendor–managed
    Kubernetes, the set of respected annotations can vary across providers, requiring
    additional validation for apps that must tolerate different managed Kubernetes
    services. With an OpenShift cluster (managed by a vendor or by you), all applications
    define the standard Ingress API with a fixed set of annotations or Route (API
    group: route.openshift.io/v1) API, which will be correctly exposed into the specific
    infrastructure substrate.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，Kubernetes 集群的网络入口可能因云特定扩展或使用社区网络入口控制器而异。因此，根据您选择提供 Kubernetes 的云提供商，您的应用可能需要提供基于不同云提供商的替代网络入口行为。在特定云供应商管理的
    Kubernetes 上使用 Ingress（API 组：networking.k8s.io/v1）时，受尊重的注释集合可能因提供商而异，需要对必须容忍不同托管
    Kubernetes 服务的应用程序进行额外验证。对于由供应商或您管理的 OpenShift 集群，所有应用程序都定义了带有一组固定注释的标准 Ingress
    API 或 Route（API 组：route.openshift.io/v1）API，这些将正确暴露到特定的基础设施基板中。
- en: The variation that you must address in your application architectures and multicloud
    management strategies is not insurmountable. However, be aware of these aspects
    as you plan your adoption strategy. Whether you adopt an OpenShift as a Service
    provider or run OpenShift within your own cloud accounts, all of the API-facing
    applications, including RBAC and networking, will behave the same.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您在应用架构和多云管理策略中必须解决的这种变化并不是不可逾越的。但是，在制定采用策略时，请注意这些方面。无论您是采用作为服务提供商的 OpenShift
    还是在自己的云帐户中运行 OpenShift，包括 RBAC 和网络在内的所有 API 面向的应用程序将表现相同。
- en: Operating System Currency for Nodes
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点的操作系统货币
- en: 'As your consumption of an OpenShift cluster grows, practical concerns around
    security and operating system currency must be addressed. With an OpenShift 4.x
    cluster, the control plane hosts are configured with Red Hat CoreOS as the operating
    system. When upgrades occur for the cluster, the operating system of the control
    plane nodes are also upgraded. The CoreOS package manager uses a novel approach
    to applying updates: updates are packaged into containers and applied transactionally.
    Either the entire update succeeds or fails. When managing the update of an OpenShift
    control plane, the result of this approach limits the potential for partially
    completed or failed installs from the interaction of unknown or untested configurations
    within the operating system. By default, the operating system provisioned for
    workers will also use Red Hat CoreOS, allowing the data plane of your cluster
    the same transactional update benefits.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您对OpenShift集群的消耗增长，必须解决围绕安全性和操作系统当前性的实际问题。对于OpenShift 4.x集群，控制平面主机配置了Red Hat
    CoreOS作为操作系统。当集群进行升级时，控制平面节点的操作系统也会升级。CoreOS软件包管理器使用一种新颖的方法来应用更新：将更新打包到容器中，并以事务方式应用。整个更新要么成功完成，要么失败。在管理OpenShift控制平面的更新时，这种方法的结果限制了由操作系统内未知或未经测试配置的交互而导致部分完成或失败安装的潜力。默认情况下，为工作节点配置的操作系统也将使用Red
    Hat CoreOS，从而使您集群的数据平面获得相同的事务更新优势。
- en: It is possible to add workers to an OpenShift cluster configured with RHEL.
    The process to add a RHEL worker node is covered in the product documentation
    and is beyond the scope of this book.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置了RHEL的OpenShift集群中可以添加工作节点。添加RHEL工作节点的过程在产品文档中有所覆盖，超出了本书的范围。
- en: 'If you integrate a managed Kubernetes service into your multicluster strategy,
    pay attention to the division of responsibilities between your vendor and your
    teams: who owns the currency/compliance status of the worker nodes in the cluster?
    Virtually all of the managed Kubernetes service providers manage the operating
    system of the control plane nodes. However, there is variance across the major
    cloud vendors on who is responsible for the operating system of the worker nodes.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将托管的Kubernetes服务集成到您的多集群策略中，请注意供应商和您的团队之间的责任划分：谁负责集群中工作节点的操作系统的当前/合规状态？几乎所有的托管Kubernetes服务提供商管理控制平面节点的操作系统。然而，在主要云供应商之间在工作节点操作系统的责任上存在差异。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have covered quite a bit in this chapter. By now, you should understand how
    IPI provides a consistent abstraction of many infrastructure substrates. Once
    provisioned, operators within OpenShift manage the life-cycle operations for key
    functions of the cluster, including machine management, authentication, networking,
    and storage. We can also use the API exposed by these operators to request and
    drive upgrade operations against the control plane of the cluster and the operating
    system of the supporting nodes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们已经涵盖了很多内容。到目前为止，您应该了解IPI如何提供多个基础设施基板的一致抽象。一旦被配置，OpenShift内的操作员管理集群的关键功能的生命周期操作，包括机器管理、认证、网络和存储。我们还可以使用这些操作员暴露的API来请求和驱动对集群控制平面和支持节点操作系统的升级操作。
- en: We also introduced cluster life-cycle management from [Open Cluster Management](https://oreil.ly/3J1SW)
    using a supporting offering, Red Hat Advanced Cluster Management. Using RHACM,
    we saw how to trigger the upgrade behavior for user-managed clusters on any infrastructure
    substrate.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了使用支持服务提供的[Red Hat高级集群管理](https://oreil.ly/3J1SW)实现的[开放集群管理](https://oreil.ly/3J1SW)中的集群生命周期管理。使用RHACM，我们看到如何在任何基础设施基板上触发用户管理的集群的升级行为。
- en: In the next chapter, we will continue to leverage cluster operators to configure
    and drive cluster behavior by defining Open Cluster Management policies that we
    can apply to one or more clusters under management.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续利用集群操作员通过定义可以应用于一个或多个受管理集群的开放集群管理策略来配置和驱动集群行为。
- en: ^([1](ch06.html#ch01fn36-marker)) Ted Dunning, “Using Data Fabric and Kubernetes
    in Edge Computing,” The Newstack (May 21, 2020), [*https://oreil.ly/W3J7f*](https://oreil.ly/W3J7f);
    “Edge Computing at Chick-fil-A,” Chick-fil-A Tech Blog (July 30, 2018), [*https://oreil.ly/HcJqZ*](https://oreil.ly/HcJqZ).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#ch01fn36-marker)) Ted Dunning，“在边缘计算中使用数据织物和Kubernetes”，The
    Newstack（2020年5月21日），[*https://oreil.ly/W3J7f*](https://oreil.ly/W3J7f)；“Chick-fil-A的边缘计算”，Chick-fil-A技术博客（2018年7月30日），[*https://oreil.ly/HcJqZ*](https://oreil.ly/HcJqZ)。
- en: ^([2](ch06.html#ch01fn37-marker)) Rob Szumski, “The Ultimate Guide to OpenShift
    Release and Upgrade Process for Cluster Administrators,” Red Hat OpenShift Blog
    (November 9, 2020), [*https://oreil.ly/hKCex*](https://oreil.ly/hKCex).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#ch01fn37-marker)) Rob Szumski，“集群管理员的OpenShift版本和升级流程终极指南”，红帽OpenShift博客（2020年11月9日），[*https://oreil.ly/hKCex*](https://oreil.ly/hKCex)。
