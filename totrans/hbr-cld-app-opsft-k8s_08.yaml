- en: 'Chapter 6\. Multicluster Fleets: Provision and Upgrade Life Cycles'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The terms *multicluster* and *multicloud* have become common in today’s landscape.
    For the purposes of this discussion, we will define these terms as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Refers to scenarios where more than a single cluster is under management or
    an application is made up of parts that are hosted on more than one cluster
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Multicloud
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Refers to scenarios where the multiple clusters in use also span infrastructure
    substrates, which might include a private datacenter and a single public cloud
    provider or multiple public cloud providers
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The differences here are more academic; the reality is that you are more likely
    than not to have to manage many clusters just as your organization has had to
    manage multiple VMware ESXi hosts that run VMs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The differences will matter when your container orchestration platform has variation
    across infrastructure substrates. We’ll talk about some of the places where this
    currently comes up and may affect some of your management techniques or application
    architectures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Why Multicluster?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s discuss the use cases that lead to multiple clusters under management.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case: Using Multiple Clusters to Provide Regional Availability for Your
    Applications'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in [Chapter 4](ch04.html#single_cluster_availability), a single
    cluster can span multiple availability zones. Each availability zone has independent
    failure characteristics. A failure in the power supply, network provider, and
    even physical space (e.g., the datacenter building) should be isolated to one
    availability zone. Typically, the network links across availability zones still
    provide for significant throughput and low latency, allowing the etcd cluster
    for the Kubernetes API server to span hosts running in different availability
    zones. However, your application may need to tolerate an outage that affects more
    than two availability zones within a region or tolerate an outage of the entire
    region.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: So perhaps one of the most easily understood use cases is to create more than
    one multiavailability zone cluster in two or more regions. You will commonly find
    applications that are federated across two “swim lanes,” sometimes referred to
    as a [*blue-green architecture*](https://oreil.ly/82hDU). The “blue-green” pairing
    pattern can often be found within the same region, with alternate blue-green pairs
    in other regions. You may choose to bring that same architecture to OpenShift
    where you run two separate clusters that host the same set of components for the
    application, effectively running two complete end-to-end environments, either
    of which can support most of the load of your users. Additional issues concerning
    load balancing and data management arise around architectural patterns required
    to support cross-region deployments and will be covered in [Chapter 8](ch08.html#working_example_of_multicluster_applicat).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case: Using Multiple Clusters for Multiple Tenants'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kubernetes community boundary for tenancy is a single cluster. In general,
    the API constructs within Kubernetes focus on dividing the compute resources of
    the cluster into namespaces (also called *projects* in OpenShift). Users are then
    assigned roles or `ClusterRole`s to access their namespaces. However, cluster-scoped
    resources like `ClusterRole`s, CRDs, namespaces/projects, webhook configurations,
    and so on really cannot be managed by independent parties. Each API resource must
    have a unique name within the collection of the same kind of API resources. If
    there were true multitenancy within a cluster, then some API concept (like a tenant)
    would group things like `ClusterRole`s, CRDs, and webhook configurations and prevent
    collisions (in naming or behavior) across each tenant, much like projects do for
    applications (e.g., deployments, services, and `PersistentVolumeClaim`s can duplicate
    names or behavior across different namespaces/projects).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: So Kubernetes is easiest to consume when you can assign a cluster to a tenant.
    A tenant might be a line of business or a functional team within your organization
    (e.g., quality engineering or performance and scale test). Then, a set of cluster-admins
    or similarly elevated `ClusterRole`s can be assigned to the owners of the cluster.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Hence, an emerging pattern is that platform teams that manage OpenShift clusters
    will define a process where a consumer may request a cluster for their purposes.
    As a result, multiple clusters now require consistent governance and policy management.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case: Supporting Far-Edge Use Cases Where Clusters Do Not Run in Traditional
    Datacenters or Clouds'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some great examples of how technology is being applied to a variety
    of use cases where computing power is coupled with sensor data from cameras, audio
    sensors, or environmental sensors and machine learning or AI to improve efficiency,
    provide greater safety, or create novel consumer interactions.^([1](ch06.html#ch01fn36))
    These use cases are often referred to generically as *edge computing* because
    the computing power is outside the datacenter and closer to the “edge” of the
    consumer experience.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of high-bandwidth capabilities with 5G also creates scenarios
    where an edge-computing solution can use a localized 5G network within a space
    like a manufacturing plant and where edge-computing applications help track product
    assembly, automate safety controls for employees, or protect sensitive machinery.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Just as containers provide a discrete package for enterprise web-based applications,
    there are significant benefits of using containers in edge-based applications.
    Similarly, the automated recovery of services by your container orchestration
    is also beneficial, even more so when the computing source is not easily accessible
    within your datacenter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Architectural Characteristics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen some of the reasons why you may use multiple clusters
    or clouds to support your needs, let’s take a look at some of the architectural
    benefits and challenges of such an approach.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Region availability versus availability zones
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With multiple clusters hosting the application, you can spread instances of
    the application across multiple cloud regions. Each cluster within a region will
    still spread compute capacity across multiple availability zones. See [Figure 6-1](#cluster_region_availability_allows_multi)
    for a visual representation of this topology.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0601.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Cluster region availability allows multiple clusters to run across
    independent cloud regions
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under this style of architecture, each cluster can tolerate the total loss of
    any one availability zone (AZ1, AZ2, or AZ3 could become unavailable but not more
    than one), and the workload will continue to run and serve requests. As a result
    of two availability zone failures, the etcd cluster would lose quorum and the
    control plane would become unavailable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The reason that a Kubernetes control plane becomes inoperable with more than
    a single availability zone failure is because of quorum requirements for etcd.
    Typically, etcd will have three replicas that are maintained in the control plane,
    each replica supported by exactly one availability zone. If a single availability
    zone is lost, there are still two out of three replicas present and distributed
    writes can still be sure that the write transaction is accepted. If two availability
    zones fail, then write attempts will be rejected. Pods running on worker nodes
    in the cluster may still be able to serve traffic, but no updates related to the
    Kubernetes API will be accepted or take place. However, the independent cluster
    running in one of the other regions could continue to respond to user requests.
    See [Chapter 4](ch04.html#single_cluster_availability) for a deeper analysis of
    how this process works.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating latency for users based on geography
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have users in different locations, using more than one cloud region can
    also improve response times for your users. When a user attempts to access the
    web user experience of your application or an API exposed by your workload, their
    request can be routed to the nearest available instance of your application. Typically,
    a *Global Server Load Balancer* (GSLB) is used to efficiently route traffic in
    these scenarios. When the user attempts to access the service, a DNS lookup will
    be delegated to the nameservers hosted by your GSLB. Then, based on a heuristic
    of where the request originated, the nameserver will respond with the IP address
    of the nearest hosted instance of your application. You can see a visual representation
    of this in [Figure 6-2](#requests_to_resolve_the_address_of_a_glo).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0602.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Requests to resolve the address of a global service using a GSLB
    will return the closest instance based on proximity to the request originator
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consistency of your platform (managed Kubernetes versus OpenShift plus cloud
    identity providers)
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the major benefits of the OpenShift Container Platform is that it deploys
    and runs consistently across all cloud providers and substrates like VMware and
    bare metal. When you consider whether to consume a managed Kubernetes provider
    or OpenShift, be aware that each distribution of Kubernetes makes various architectural
    decisions that can require greater awareness of your application to ensure cross-provider
    portability.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning Across Clouds
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing a Kubernetes strategy affords a great way to simplify how applications
    consume elastic cloud-based infrastructure. To some extent, the problem of how
    you consume a cloud’s resources shifts from solving the details for every application
    to one platform—namely, how your organization will adopt and manage Kubernetes
    across infrastructure substrates.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to provision Kubernetes from community-supported projects.
    For the purposes of this section, we’ll focus on how to provision Kubernetes using
    the Red Hat OpenShift Container Platform. Then we’ll discuss how you could alternatively
    consume managed OpenShift or managed Kubernetes services as part of your provisioning
    life cycle.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: User-Managed OpenShift
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you provision an OpenShift Container Platform 4.x cluster, you have two
    options for how infrastructure resources are created. *User-provisioned infrastructure*
    (UPI) allows you more control to spin up VMs, network resources, and storage and
    then give these details to the install process and allow them to be bootstrapped
    into a running cluster. Alternatively, you can rely on the more automated approach
    of *installer-provisioned infrastructure* (IPI). Using IPI, the installer accepts
    cloud credentials with the appropriate privileges to create the required infrastructure
    resources. The IPI process will typically define a *virtual private cloud* (VPC).
    Note that you can specify the VPC as an input parameter if your organization has
    its own conventions for how these resources are created and managed. Within the
    VPC, resources, including network load balancers, object store buckets, virtual
    computing resources, elastic IP addresses, and so forth, are all created and managed
    by the install process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at provisioning an OpenShift cluster across three cloud providers:
    AWS, Microsoft Azure, and Google Cloud Platform. For this discussion, we will
    review how the install process makes use of declarative configuration (just as
    Kubernetes does in general) and how this relates to the `ClusterVersionOperator`
    (CVO), which manages the life cycle of the OpenShift cluster itself.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: First, you will need to download the openshift-installer binary for your appropriate
    version. Visit [Red Hat](https://cloud.redhat.com), create an account, and follow
    the steps to Create Cluster and download the binary for local use. Specific details
    about the options available for installation are available in the [product documentation](https://oreil.ly/HerIm).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Let’s demonstrate how this approach works by looking at a few example configuration
    files for the openshift-installer binary. The full breadth of options for installing
    and configuring OpenShift is beyond the scope of this book. See the [OpenShift
    Container Platform documentation](https://oreil.ly/wOJ3P) for a thorough reference
    of all supported options. The following examples will highlight how the declarative
    nature of the OpenShift 4.x install methodology simplifies provisioning clusters
    across multiple substrates. Further, a walk-through example of the `MachineSet`
    API will demonstrate how operators continue to manage the life cycle and health
    of the cluster after provisioning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 6-1](#an_example_install_configdotyaml_to_pro) defines a set of options
    for provisioning an OpenShift cluster on AWS. [Example 6-2](#an_example_install_configdotyaml_to_pr)
    defines how to provision an OpenShift cluster on Microsoft Azure, while [Example 6-3](#an_example_install_configdotyaml_to_p)
    defines the equivalent configuration for Google Cloud Platform. [Example 6-4](#an_example_install_configdotyaml_to)—you
    guessed it!—provides an example configuration for VMware vSphere. With the exception
    of the VMware vSphere example (which is more sensitive to your own environment),
    you can use these examples to provision your own clusters with minimal updates.
    Refer to the OpenShift Container Platform product documentation for a full examination
    of install methods.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. An example *install-config.yaml* to provision an OpenShift cluster
    on AWS
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Example 6-2\. An example *install-config.yaml* to provision an OpenShift cluster
    on Microsoft Azure
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 6-3\. An example *install-config.yaml* to provision an OpenShift cluster
    on Google Cloud Platform
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example 6-4\. An example *install-config.yaml* to provision an OpenShift cluster
    on VMware vSphere
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Any one of these *install-config.yaml* files can be used to provision your
    cluster using the following command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note how each example shares some of the same options, notably the `clusterName`
    and `baseDomain` that will be used to derive the default network address of the
    cluster (applications will be hosted by default at `https://*.apps.clusterName.baseDomain`
    and the API endpoint for OpenShift will be available at `https://api.clusterName.baseDomain:6443`).
    When the openshift-installer runs, DNS entries on the cloud provider (e.g., Route
    53 in the case of AWS) will be created and linked to the appropriate network load
    balancers (also created by the install process), that in turn resolve to IP addresses
    running within the VPC.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Each example defines sections for the `controlPlane` and `compute` that correspond
    to `MachineSet`s that will be created and managed. We’ll talk about how these
    relate to operators within the cluster shortly. More than one `MachinePool` within
    the `compute` section can be specified. Both the `controlPlane` and `compute`
    sections provide configurability for the compute hosts and can customize which
    availability zones are used. Settings including the type (or size) for each host
    and the options for what kind of storage is attached to the hosts are also available,
    but reasonable defaults will be chosen if omitted.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we compare where the *install-config.yaml* properties vary for each
    substrate, we will find cloud-specific options within the `platform` sections.
    There is a global `platform` to specify which region the cluster should be created
    within, as well as `platform` sections under each of the `controlPlane` and `compute`
    sections to override settings for each provisioned host.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: As introduced in [Chapter 5](ch05.html#continuous_delivery_across_clusters),
    the [Open Cluster Management](https://oreil.ly/D1UvC) project is a new approach
    to managing the multicluster challenges that most cluster maintainers encounter.
    [Chapter 5](ch05.html#continuous_delivery_across_clusters) discussed how applications
    can be distributed easily across clusters. Now let’s look at how the cluster provisioning,
    upgrade, and decommissioning process can be driven using Open Cluster Management.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will walk through creating a new cluster on a cloud
    provider. The underlying behavior is leveraging the same openshift-install process
    that we just discussed. Once provisioned, the Open Cluster Management framework
    will install an agent that runs as a set of pods on the new cluster. We refer
    to this agent as a `klusterlet`, mimicking the naming of the `kubelet` process
    that runs on nodes that are part of a Kubernetes cluster.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following assumes the user has already set up the Open Cluster Management
    project or RHACM for Kubernetes as described in [Chapter 5](ch05.html#continuous_delivery_across_clusters).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: From the RHACM for Kubernetes web console, open the Automate Infrastructure
    > Clusters page and click on the action to “Create cluster” as shown in [Figure 6-3](#the_cluster_overview_page_allows_you_to).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0603.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The cluster overview page allows you to provision new clusters
    from the console
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The “Create cluster” action shown in [Figure 6-4](#cluster_creation_form_via_rhacm_for_kube)
    opens to a form where you provide a name and select one of the available cloud
    providers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0604.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Cluster creation form via RHACM for Kubernetes
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, select a version of OpenShift to provision. The available list maps directly
    to the `ClusterImageSet`s available on the hub cluster. You can introspect these
    images with the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Further down the page, as shown in [Figure 6-5](#select_your_release_image_left_parenthes),
    you will also need to specify a provider connection. In the case of AWS, you will
    need to provide the Access ID and Secret Key to allow API access by the installation
    process with your AWS account.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0605.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Select your release image (the version to provision) and your provider
    connection
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At this point, you can simply click Create and the cluster will be provisioned.
    However, let’s walk through how the `MachinePool` operator allows you to manage
    `MachineSet`s within the cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Customize the “Worker pool1” `NodePool` for your desired region and availability
    zones. See Figures [6-6](#customizing_the_region_and_zones_for_the) and [6-7](#customizing_the_availability_zone)
    for an example of what this will look like in the form. You can amend these options
    after the cluster is provisioned as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0606.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Customizing the region and zones for the cluster workers
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/hcok_0607.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Customize the availability zones within the region that are valid
    to host workers
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A final summary of your confirmed choices is presented for review in [Figure 6-8](#confirmed_options_in_form).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0608.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Confirmed options selected in the form
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you have made your final customizations, click Create to begin the provisioning
    process as shown in [Figure 6-9](#rhacm_web_console_view_that_includes_lin). The
    web console provides a view that includes links to the provisioning logs for the
    cluster. If the cluster fails to provision (e.g., due to a quota restriction in
    your cloud account), the provisioning logs provide clues on what to troubleshoot.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0609.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. RHACM web console view that includes links to the provisioning
    logs for the cluster
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Behind the form editor, a number of Kubernetes API objects are created. A small
    number of these API objects are cluster scoped (`ManagedCluster` in particular).
    The `ManagedCluster` controller will ensure that a project (namespace) exists
    that maps to the cluster name. Other controllers, including the controller that
    begins the provisioning process, will use the `cluster` project (namespace) to
    store resources that provide an API control surface for provisioning. Let’s take
    a look at a subset of these that you should become familiar with.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: ManagedCluster
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ManagedCluster` (API group: cluster.open-cluster-management.io/v1; cluster
    scoped) recognizes that a remote cluster is under the control of the hub cluster.
    The agent that runs on the remote cluster will attempt to create `ManagedCluster`
    if it does not exist on the hub, and it must be accepted by a user identity on
    the hub with appropriate permissions. You can see the example created in [Example 6-5](#example_of_the_managedcluster_api_object).
    Note that labels for this object will drive placement decisions later in this
    chapter.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-5\. Example of the `ManagedCluster` API object
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ClusterDeployment
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ClusterDeployment` (API group: hive.openshift.io/v1; namespace scoped) controls
    the provisioning and decommissioning phases of the cluster. A controller on the
    hub takes care of running the openshift-installer on your behalf. If the cluster
    creation process fails for any reason (e.g., you encounter a quota limit within
    your cloud account), the cloud resources will be destroyed and another attempt
    will be made after a waiting period to reattempt successful creation of the cluster.
    Unlike traditional automation methods that “try once” and require user intervention
    upon failure, the Kubernetes reconcile loop for this API kind will continue to
    attempt to create the cluster (with appropriate waiting periods in between) as
    shown in [Example 6-6](#example_clusterdeployment_created_by_the). You can also
    create these resources directly through the `oc` or `kubectl` like any Kubernetes
    native resource.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-6\. Example `ClusterDeployment` created by the form
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: KlusterletAddonConfig
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`KlusterletAddonConfig` (API group: agent.open-cluster-management.io/v1; namespace
    scoped) represents the capabilities that should be provided on the remote agent
    that manages the cluster. In [Example 6-7](#an_example_of_the_klusterletaddonconfig),
    the Open Cluster Management project refers to the remote agent as a `klusterlet`,
    mirroring the language of `kubelet`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-7\. An example of the `KlusterletAddonConfig` API object
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: MachinePool
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`MachinePool` (API group: hive.openshift.io/v1; namespace -scoped) allows you
    to create a collection of hosts that work together and share characteristics.
    You might use a `MachinePool` to group a set of compute capacity that supports
    a specific team or line of business. As we will see in the next section, `MachinePool`
    also allows you to dynamically size your cluster. Finally, the status provides
    a view into the `MachineSet`s that are available on `ManagedCluster`. See [Example 6-8](#the_machinepool_api_object_provides_a_co)
    for the example `MachinePool` created earlier, which provides a control surface
    to scale the number of replicas up or down within the pool and status about the
    `MachineSet`s under management on the remote cluster.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-8\. The example `MachinePool` API object
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once provisioned, the address for the Kubernetes API server and OpenShift web
    console will be available from the cluster details page. You can use these coordinates
    to open your web browser and authenticate with the new cluster as the kubeadmin
    user. You can also access the `KUBECONFIG` certificates that allow you command-line
    access to the cluster.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: You can download the `KUBECONFIG` authorization as shown in [Figure 6-10](#downloading_the_kubeconfig_authorization)
    for the new cluster from the RHACM web console under the cluster overview page
    or access it from the command line. From the web console, click on the cluster
    name in the list of clusters to view an overview of that cluster. Once the provisioning
    process has completed, you will be able to download the `KUBECONFIG` file that
    will allow you command-line access to the cluster.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0610.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. Downloading the kubeconfig authorization from the RHACM web console
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the command line, you can retrieve the information stored in a secret under
    the cluster project (namespace) as in [Example 6-9](#output_of_the_cluster_kubeconfig_filedot).
    Save the file contents and configure your `KUBECONFIG` environment variable to
    point to the location of the file. Then `oc` will be able to run commands against
    the remote cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-9\. Output of the cluster `KUBECONFIG` file
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that our cluster is up and running, let’s walk through how we can scale
    the cluster. We will review this concept from the context of the oc CLI.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: First, open two terminals and configure the `KUBECONFIG` or context for each
    of the hub clusters and our newly minted `mycluster`. See Examples [6-10](#example_six_onezero_an_example_of_what_t)
    and [6-11](#an_example_of_how_terminal_two_will_look) for examples of what each
    of the two separate terminals will look like after you run these commands. Note
    the tip to override your PS1 shell prompt temporarily to avoid confusion when
    running commands on each cluster.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-10\. An example of what Terminal 1 will look like
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Example 6-11\. An example of how Terminal 2 will look
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you should have Terminal 1 with a prompt including `hubcluster` and Terminal
    2 with a prompt including `mycluster`. We will refer to these terminals by the
    appropriate names through the rest of the example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In the following walk-through, we will review the `MachineSet` API, which underpins
    how an OpenShift cluster understands compute capacity. We will then scale the
    size of our managed cluster from the hub using the `MachinePool` API that we saw
    earlier.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `mycluster` terminal, review the `MachineSet`s for your cluster:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Each `MachineSet` will have a name following the pattern: `<clusterName>-<five-character
    identifier>-<machinePoolName>-<availabilityZone>`. In your cluster, you should
    see counts for the desired number of machines per `MachineSet`, the current number
    of machines that are available, and the current number that are considered `Ready`
    to be integrated as nodes into the OpenShift cluster. Note that these three counts
    should generally be equivalent and should only vary when the cluster is in a transition
    state (adding or removing machines) or when an underlying availability problem
    in the cluster causes one or more machines to be considered unhealthy. For example,
    when you edit a `MachineSet` to increase the desired replicas, you will see the
    `Desired` count increment by one for that `MachineSet`. As the machine is provisioned
    and proceeds to boot and configure the `kubelet`, the `Current` count will increment
    by one. Finally, as the `kubelet` registers with the Kubernetes API control plane
    and marks the node as `Ready`, the `Ready` count will increment by one. If at
    any point the machine becomes unhealthy, the `Ready` count may decrease. Similarly,
    if you reduced the `Desired` count by one, you would see the same staggered reduction
    in counts as the machine proceeds through various life-cycle states until it is
    removed.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in the hub terminal, review the `worker MachinePool` defined for the
    managed cluster:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will increase the size of the managed cluster `mycluster` by one node:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The size of the worker node will be determined by the values set in the `MachinePool
    mycluster-worker`. The availability zone of the new node will be determined by
    the `MachinePool` controller, where nodes are distributed across availability
    zones as evenly as possible.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Immediately after you have patched the `MachinePool` to increase the number
    of desired replicas, rerun the command to view the `MachineSet` on your managed
    cluster:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Over the course of a few minutes, you should see the new node on the managed
    cluster transition from `Desired` to `Current` to `Ready`, with a final result
    that looks like the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s recap what we’ve just seen. First, we used a declarative method (*install-config.yaml*)
    to provision our first cluster, called a hub. Next, we used the hub to provision
    the first managed cluster in our fleet. That managed cluster was created under
    the covers using the same IPI method but with the aid of Kubernetes API and continuous
    reconcilers that ensure that the running cluster matches the `Desired` state.
    One of the APIs that governs the `Desired` state is the `MachinePool` API on the
    hub cluster. Because our first fleet member, `mycluster`, was created from the
    hub cluster, we can use the `MachinePool API` to govern how `mycluster` adds or
    removes nodes. Or indeed, we can create additional `MachinePool`s that add capacity
    to the cluster.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the process, the underlying infrastructure substrate was completely
    managed through operators. The `MachineSet` operator on the managed cluster was
    given updated instructions by the `MachinePool` operator on the hub to grow the
    number of machines available in one of the `MachineSet`s supporting `mycluster`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will use the term *infrastructure substrate* as a catchall term to refer
    to the compute, network, and storage resources provided by bare metal virtualization
    within your datacenter or virtualization offered by a public cloud provider.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Your Clusters to the Latest Version of Kubernetes
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw with `MachinePool`s and `MachineSet`s, operators provide a powerful
    way to abstract the differences across infrastructure substrates, allowing an
    administrator to declaratively specify the desired outcome. An OpenShift cluster
    is managed by the CVO, which acts as an “operator of operators” pattern to manage
    operators for each dimension of the cluster’s configuration (authentication, networking,
    machine creation, bootstrapping, and removal, and so on). Every cluster will have
    a `Cluster​Ver⁠sion` API object named `version`. You can retrieve the details
    for this object with the command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`ClusterVersion` specifies a “channel” to seek available versions for the cluster
    and a desired version from that channel. Think of a channel as an ongoing list
    of available versions (e.g., 4.5.1, 4.5.2, 4.5.7, and so on). There are channels
    for “fast” adoption of new versions, as well as “stable” versions. The fast channels
    produce new versions quickly. Coupled with connected telemetry data from a broad
    source of OpenShift clusters running across infrastructure substrates and industries,
    fast channels allow the delivery and validation of new releases very quickly (on
    order of weeks or days). As releases in fast channels have enough supporting evidence
    that they are broadly acceptable across the global fleet of OpenShift clusters,
    versions are promoted to stable channels. Hence, the list of versions within a
    channel is not always consecutive. An example `ClusterVersion` API object is represented
    in [Example 6-12](#an_example_clusterversion_api_object_tha).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-12\. An example `ClusterVersion` API object that records the version
    history for the cluster and the desired version—changing the desired version will
    cause the operator to begin applying updates to achieve the goal
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Upgrading a version of Kubernetes along with all other supporting APIs and infrastructure
    around it can be a daunting task. The operator that controls the life cycle of
    all of the container images is known informally as [“Cincinnati”](https://oreil.ly/6HIZd)
    and formally as the *OpenShift Update Service* (OSUS). OSUS (or Cincinnati) maintains
    a connected graph of versions and tracks which “walks” or “routes” within the
    graph are known as good upgrade paths. For example, an issue may be detected in
    early release channels that indicates that the upgrade from 4.4.23 to 4.5.0 to
    4.5.18 may be associated with a specific problem. A fix can be released to create
    a new release 4.4.24 that then allows a successful and predictable upgrade from
    4.4.23 to 4.4.24 to 4.5.0 to 4.5.18\. The graph records the successive nodes that
    must be walked to ensure success.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: However, the OSUS operator removes the guesswork, allowing the cluster administrator
    to specify the desired version from the channel. From there, the CVO will carry
    out the following tasks:^([2](ch06.html#ch01fn37))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Upgrade the Kubernetes and OpenShift control plane pods, including etcd.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade the operating system for the nodes running the control plane pods.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade the cluster operators controlling aspects like authentication, networking,
    storage, and so on.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For nodes managed by the `MachineConfigOperator`, upgrade the operating system
    for the nodes running the data plane pods (user workload).
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The upgrade takes place in a rolling fashion, avoiding bursting the size of
    the cluster or taking out too much capacity at the same time. Because the control
    plane is spread across three machines, as each machine undergoes an operating
    system update and reboot, the other two nodes maintain the availability of the
    Kubernetes control plane, including the datastore (etcd), the scheduler, the controller,
    the Kubernetes API server, and the network ingress controller.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: When the data plane is upgraded, the upgrade process will respect `PodDisruptionBudget`s
    and look for feedback about the health of OpenShift and user workloads running
    on each node by means of liveness and readiness probes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sometimes the group of clusters under management is referred to as a *fleet*.
    Individual clusters under management may be referred to as *fleet members*, primarily
    to distinguish them from the hub cluster that is responsible for management.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: From the RHACM web console, you can manipulate the desired version of a managed
    cluster for a single fleet member or the entire fleet. From the console, choose
    the “Upgrade cluster” action for any cluster that shows “Upgrade available.” Recall
    from the discussion around channels that not every channel may have an upgrade
    currently available. Additionally, the list of versions may not be consecutive.
    Figures [6-11](#actions_permitted_on_a_cluster_allow_a_f), [6-12](#the_list_of_available_versions_is_provid),
    and [6-13](#multiple_clusters_may_be_selected_for_up) provide examples of what
    this actually looks like for a specific cluster or for multiple clusters.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hcok_0611.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. Actions permitted on a cluster allow a fleet manager to upgrade
    the desired version of a cluster
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/hcok_0612.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. The list of available versions is provided for user selection
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/hcok_0613.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. Multiple clusters may be selected for upgrade, and the versions
    available will vary based on the cluster’s attached channel configuration in the
    `ClusterVersion` object
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A core topic for this book is how to manage your clusters as a fleet, and for
    that, we will rely on policies. The preceding discussion should provide a foundation
    for you to understand the moving parts and see that you can explicitly trigger
    upgrade behavior across the fleet. In [Chapter 7](ch07.html#multicluster_policy_configuration),
    we will discuss how we can control upgrade behavior across the fleet by policy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Summary of Multicloud Cluster Provisioning
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout our example, the specific infrastructure substrate showed up in a
    few declarative APIs, specifically represented by the *install-config.yaml* for
    the hub cluster and as part of the secrets referenced by the `ClusterDeployment`
    API object for the managed cluster. However, the action of provisioning a new
    cluster and adding or removing nodes for that fleet member was completely driven
    through Kubernetes API objects.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the upgrade life cycle managed through the CVO is consistent across
    supported infrastructure substrates. Hence, regardless if you provision an OpenShift
    cluster on a public cloud service or in your datacenter, you can still declaratively
    manage the upgrade process. The powerful realization you should now understand
    is that managing the infrastructure substrate for OpenShift clusters in multicloud
    scenarios can be completely abstracted away from many basic cluster-provisioning
    life-cycle operations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Beyond controlling the capacity of your fleet from the hub, you can assign policies
    with Open Cluster Management and drive behavior like fleet upgrades. We will see
    an example of fleet upgrade by policy in [Chapter 7](ch07.html#multicluster_policy_configuration).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift as a Service
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous section described how you can abstract the provisioning and life
    cycle of OpenShift across multiple infrastructure substrates. Under the model
    we’ve outlined, you are responsible for the availability of your clusters. For
    budget or organizational reasons, you may choose to consider a managed service
    for OpenShift or Kubernetes. Using a vendor-provided “OpenShift as a Service”
    or “Kubernetes as a Service” can change how you interact with some dimensions,
    including cluster creation or decommissioning. However, your applications will
    run consistently regardless of whether the vendor manages the underlying infrastructure
    or you manage it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Azure Red Hat OpenShift
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Azure Red Hat OpenShift](https://oreil.ly/0OWW7) is integrated into the Microsoft
    Azure ecosystem, including Azure billing. Other aspects, including single sign-on,
    are automatically configured with Azure Active Directory, simplifying how you
    expose capabilities to your organization, particularly if you are already consuming
    other services on Azure. The underlying service is maintained by a partnership
    between Microsoft and Red Hat.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Red Hat OpenShift on AWS
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Red Hat OpenShift on AWS](https://oreil.ly/fX0a2) was announced at the end
    of 2020 with planned availability in 2021\. It integrates OpenShift into the Amazon
    ecosystem, allowing for access and creation through the Amazon cloud console and
    consistent billing with the rest of your Amazon account. The underlying service
    is maintained by a partnership between Amazon and Red Hat.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Red Hat OpenShift on IBM Cloud
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Red Hat OpenShift on IBM Cloud](https://oreil.ly/gZBHU) integrates OpenShift
    consumption into the IBM Cloud ecosystem, including integration with IBM Cloud
    single sign-on and billing. In addition, IBM Cloud APIs are provided to manage
    cluster provisioning, worker nodes, and the upgrade process. These APIs allow
    separate access controls via IBM Cloud Identity and Access Management for management
    of the cluster versus the access controls used for managing resources in the cluster.
    The underlying service is maintained by IBM.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift Dedicated
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[OpenShift Dedicated](https://cloud.redhat.com) is a managed OpenShift as a
    Service offering provided by Red Hat. OpenShift clusters can be created across
    a variety of clouds from this service, in some cases under your own preexisting
    cloud account. Availability and maintenance of the cluster are handled by the
    Red Hat SRE team. The underlying service is maintained by Red Hat with options
    to bring your own cloud accounts on some supported infrastructure providers like
    AWS.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes as a Service
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to vendor-managed OpenShift as a Service, many vendors offer managed
    Kubernetes as a Service distributions. These are typically where the vendor adopts
    Kubernetes and integrates it into its ecosystem. The following are some examples
    of these services:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Elastic Kubernetes Service
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Kubernetes Service
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Kubernetes Engine
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM Cloud Kubernetes Service
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because the Kubernetes community leaves some decisions up to vendors or users
    who assemble their own distributions, each of these managed services can introduce
    some variations that you should be aware of when adopting them as part of a larger
    multicloud strategy. In particular, several specific areas in Kubernetes have
    been evolving quickly:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Cluster creation
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User identity and access management
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network routing
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod security management
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Role-based access control
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value-added admission controllers
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating system management of the worker nodes
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different security apparatus to manage compliance
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Across each dimension, a vendor providing a managed Kubernetes service must
    decide how to best integrate that aspect of Kubernetes into the cloud provider’s
    ecosystem. The core API should respond consistently by way of the [CNCF Kubernetes
    Certification process](https://oreil.ly/sWAXA). In practice, differences tend
    to arise where Kubernetes is integrated into a particular cloud ecosystem.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in some cases a managed Kubernetes service will come with Kubernetes
    RBAC deployed and configured out of the box. Other vendors may leave it to the
    cluster creator to configure RBAC for Kubernetes. Across vendors that automatically
    configure the Kubernetes with RBAC, the set of out-of-the-box `ClusterRole`s and
    roles can vary.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'In other cases, the network ingress for a Kubernetes cluster can vary from
    cloud-specific extensions to use of the community network ingress controller.
    Hence, your application may need to provide alternative network ingress behavior
    based on the cloud providers that you choose to provide Kubernetes. When using
    Ingress (API group: networking.k8s.io/v1) on a particular cloud vendor–managed
    Kubernetes, the set of respected annotations can vary across providers, requiring
    additional validation for apps that must tolerate different managed Kubernetes
    services. With an OpenShift cluster (managed by a vendor or by you), all applications
    define the standard Ingress API with a fixed set of annotations or Route (API
    group: route.openshift.io/v1) API, which will be correctly exposed into the specific
    infrastructure substrate.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The variation that you must address in your application architectures and multicloud
    management strategies is not insurmountable. However, be aware of these aspects
    as you plan your adoption strategy. Whether you adopt an OpenShift as a Service
    provider or run OpenShift within your own cloud accounts, all of the API-facing
    applications, including RBAC and networking, will behave the same.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Operating System Currency for Nodes
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As your consumption of an OpenShift cluster grows, practical concerns around
    security and operating system currency must be addressed. With an OpenShift 4.x
    cluster, the control plane hosts are configured with Red Hat CoreOS as the operating
    system. When upgrades occur for the cluster, the operating system of the control
    plane nodes are also upgraded. The CoreOS package manager uses a novel approach
    to applying updates: updates are packaged into containers and applied transactionally.
    Either the entire update succeeds or fails. When managing the update of an OpenShift
    control plane, the result of this approach limits the potential for partially
    completed or failed installs from the interaction of unknown or untested configurations
    within the operating system. By default, the operating system provisioned for
    workers will also use Red Hat CoreOS, allowing the data plane of your cluster
    the same transactional update benefits.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to add workers to an OpenShift cluster configured with RHEL.
    The process to add a RHEL worker node is covered in the product documentation
    and is beyond the scope of this book.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'If you integrate a managed Kubernetes service into your multicluster strategy,
    pay attention to the division of responsibilities between your vendor and your
    teams: who owns the currency/compliance status of the worker nodes in the cluster?
    Virtually all of the managed Kubernetes service providers manage the operating
    system of the control plane nodes. However, there is variance across the major
    cloud vendors on who is responsible for the operating system of the worker nodes.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered quite a bit in this chapter. By now, you should understand how
    IPI provides a consistent abstraction of many infrastructure substrates. Once
    provisioned, operators within OpenShift manage the life-cycle operations for key
    functions of the cluster, including machine management, authentication, networking,
    and storage. We can also use the API exposed by these operators to request and
    drive upgrade operations against the control plane of the cluster and the operating
    system of the supporting nodes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced cluster life-cycle management from [Open Cluster Management](https://oreil.ly/3J1SW)
    using a supporting offering, Red Hat Advanced Cluster Management. Using RHACM,
    we saw how to trigger the upgrade behavior for user-managed clusters on any infrastructure
    substrate.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue to leverage cluster operators to configure
    and drive cluster behavior by defining Open Cluster Management policies that we
    can apply to one or more clusters under management.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.html#ch01fn36-marker)) Ted Dunning, “Using Data Fabric and Kubernetes
    in Edge Computing,” The Newstack (May 21, 2020), [*https://oreil.ly/W3J7f*](https://oreil.ly/W3J7f);
    “Edge Computing at Chick-fil-A,” Chick-fil-A Tech Blog (July 30, 2018), [*https://oreil.ly/HcJqZ*](https://oreil.ly/HcJqZ).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.html#ch01fn37-marker)) Rob Szumski, “The Ultimate Guide to OpenShift
    Release and Upgrade Process for Cluster Administrators,” Red Hat OpenShift Blog
    (November 9, 2020), [*https://oreil.ly/hKCex*](https://oreil.ly/hKCex).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
