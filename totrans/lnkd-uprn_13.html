<html><head></head><body><section data-pdf-bookmark="Chapter 13. Linkerd CNI Versus Init Containers" data-type="chapter" epub:type="chapter" class="preface"><div class="preface" id="LUAR_cni_vs_init_containers">
<h1 class="calibre7"><span class="calibre">Chapter 13. </span>Linkerd CNI Versus Init Containers</h1>


<p class="author1">In <a data-type="xref" href="ch02.html#LUAR_intro_to_linkerd" class="calibre4">Chapter 2</a>, we mentioned<a data-primary="mesh networking" data-secondary="about two mechanisms" data-type="indexterm" id="id1665" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="mesh networking" data-tertiary="about two mechanisms" data-type="indexterm" id="id1666" class="calibre4"/><a data-primary="init container" data-secondary="about" data-type="indexterm" id="id1667" class="calibre4"/> the init container a couple of
times without ever talking about it in detail. The init container is one of
the two mechanisms Linkerd provides for handling mesh networking in
Kubernetes, with the other being the Linkerd CNI plugin. To understand what
these do and why you’d choose one over the other, you need to understand what
happens when a meshed Pod starts running.</p>

<p class="author1">As it happens, that’s a much bigger, thornier issue than you might expect.
We’ll start by looking at vanilla Kubernetes, <em class="hyperlink">without</em> Linkerd.</p>






<section data-pdf-bookmark="Kubernetes sans Linkerd" data-type="sect1" class="preface"><div class="preface" id="id183">
<h1 class="calibre8">Kubernetes sans Linkerd</h1>

<p class="author1">At its core, Kubernetes has a<a data-primary="mesh networking" data-secondary="Kubernetes without Linkerd" data-type="indexterm" id="id1668" class="calibre4"/><a data-primary="Kubernetes" data-secondary="without Linkerd" data-type="indexterm" id="id1669" class="calibre4"/><a data-primary="init container" data-secondary="Kubernetes without Linkerd" data-type="indexterm" id="id1670" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="mesh networking" data-tertiary="Kubernetes sans Linkerd" data-type="indexterm" id="ch13-sans" class="calibre4"/> straightforward goal: manage user workloads so
that developers can concentrate on Pods and Services without needing to worry
too much about the underlying hardware. This is one of those things that’s
easy to describe, and fairly easy to use, but <em class="hyperlink">extremely</em> complex to
implement. Kubernetes relies on several different open source technologies to
get it all done. Remember that we’re talking about Kubernetes <em class="hyperlink">without</em>
Linkerd at this point—this is essentially your standard Kubernetes
functionality.</p>








<section data-pdf-bookmark="Nodes, Pods, and More" data-type="sect2" class="preface"><div class="preface" id="id99">
<h2 class="calibre27">Nodes, Pods, and More</h2>

<p class="author1">The first area that Kubernetes<a data-primary="init container" data-secondary="Kubernetes without Linkerd" data-tertiary="orchestrating workload execution" data-type="indexterm" id="id1671" class="calibre4"/><a data-primary="mesh networking" data-secondary="Kubernetes without Linkerd" data-tertiary="orchestrating workload execution" data-type="indexterm" id="id1672" class="calibre4"/><a data-primary="Kubernetes" data-secondary="without Linkerd" data-tertiary="orchestrating workload execution" data-type="indexterm" id="id1673" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="mesh networking" data-tertiary="orchestrating workload execution sans Linkerd" data-type="indexterm" id="id1674" class="calibre4"/> has to manage is orchestrating the actual
execution of workloads within a cluster. It relies extensively on OS-level
isolation mechanisms for this task. Here are some key points to keep in mind:<a data-primary="Nodes" data-type="indexterm" id="id1675" class="calibre4"/></p>

<ul class="printings">
<li class="calibre6">
<p class="author1">Clusters comprise one or more <em class="hyperlink">Nodes</em>, which are physical or virtual
machines running Kubernetes itself. We’ll discuss Linux Nodes here.</p>
</li>
<li class="calibre6">
<p class="author1">Since Nodes are entirely distinct from one another, everything on one Node
is isolated from others.</p>
</li>
<li class="calibre6">
<p class="author1">Pods consist of one or more <em class="hyperlink">containers</em>, and they’re isolated within the same
Node using Linux <a href="https://oreil.ly/K1z3T" class="calibre4"><code class="calibre9">cgroup</code>s</a> and <code class="calibre9">namespace</code>s.</p>
</li>
<li class="calibre6">
<p class="author1">Containers within the same Pod are allowed to communicate using <em class="hyperlink">loopback</em>
networking. Containers in different Pods need to use non-loopback addresses
because Pods are isolated from each other. Pod-to-Pod communication is the
same whether the Pods are on the same Node or not.</p>
</li>
<li class="calibre6">
<p class="author1">An important point is that Linux itself operates at the Node level: Pods and
containers don’t have to run separate instances of the OS. This is the
reason that isolation between them is so critical.</p>
</li>
</ul>

<p class="author1">This layered approach, shown in <a data-type="xref" href="#clusters-nodes-etc-diagram" class="calibre4">Figure 13-1</a>, lets
Kubernetes orchestrate the distribution of workloads within the cluster, while
keeping an eye on resource availability and usage: workload containers map to
Pods, Pods are scheduled onto Nodes, and all Nodes connect to a single flat
network.</p>

<figure class="calibre23"><div class="figure" id="clusters-nodes-etc-diagram">
<img alt="luar 1301" src="assets/luar_1301.png" class="calibre24"/>
<h6 class="calibre25"><span class="calibre">Figure 13-1. </span>Clusters, Nodes, Pods, and containers</h6>
</div></figure>

<p class="author1">(What about Deployments, ReplicaSets, DaemonSets, and such? They’re all about
hinting to Kubernetes where the Pods they create should be scheduled; the
actual scheduling mechanism underneath is the same.)</p>
</div></section>








<section data-pdf-bookmark="Networking in Kubernetes" data-type="sect2" class="preface"><div class="preface" id="id100">
<h2 class="calibre27">Networking in Kubernetes</h2>

<p class="author1">The other major area that <a data-primary="init container" data-secondary="Kubernetes without Linkerd" data-tertiary="networking" data-type="indexterm" id="ch13-net" class="calibre4"/><a data-primary="mesh networking" data-secondary="Kubernetes without Linkerd" data-tertiary="networking" data-type="indexterm" id="ch13-net2" class="calibre4"/><a data-primary="Kubernetes" data-secondary="without Linkerd" data-tertiary="networking" data-type="indexterm" id="ch13-net3" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="mesh networking" data-tertiary="Kubernetes networking sans Linkerd" data-type="indexterm" id="ch13-net4" class="calibre4"/>Kubernetes manages is the network, starting with the
fundamental tenet that every Pod must see a flat and transparent network.
Every Pod should be able to communicate with all others, on any Node. This
means that every Pod must have its own IP address (the <em class="hyperlink">Pod IP</em>).</p>
<div data-type="note" epub:type="note" class="calibre16"><h1 class="calibre26">Containers or Pods?</h1>
<p class="author1">The requirement is actually<a data-primary="IP addresses" data-secondary="same for multiple containers in one Pod" data-type="indexterm" id="id1676" class="calibre4"/> that <em class="hyperlink">any two containers</em> must be able to talk to
each other, but IP addresses are allocated at the Pod level—multiple
containers within one Pod share the same IP address.</p>
</div>

<p class="author1">While it’s possible to have a workload use Pod IPs directly to communicate
with other workloads, it’s not a good idea due to the dynamic nature of Pod
IPs: they change as Pods cycle.<a data-primary="IP addresses" data-secondary="same for multiple containers in one Pod" data-tertiary="communication via Kubernetes Services" data-type="indexterm" id="id1677" class="calibre4"/><a data-primary="Services" data-secondary="Pod communication via Kubernetes Services" data-type="indexterm" id="id1678" class="calibre4"/><a data-primary="communication" data-secondary="Pod-to-Pod via Kubernetes Services" data-type="indexterm" id="id1679" class="calibre4"/> It’s a better idea to use Kubernetes Services
in most cases.</p>

<p class="author1">Services are rather complex, as we discussed briefly in
<a data-type="xref" href="ch05.html#LUAR_ingress_and_linkerd" class="calibre4">Chapter 5</a>:<a data-primary="DNS services and Services" data-type="indexterm" id="id1680" class="calibre4"/><a data-primary="IP addresses" data-secondary="DNS names" data-type="indexterm" id="id1681" class="calibre4"/><a data-primary="Services" data-secondary="definition" data-tertiary="three distinct parts" data-type="indexterm" id="id1682" class="calibre4"/></p>

<ul class="printings">
<li class="calibre6">
<p class="author1">Creating a Service triggers the allocation of a DNS entry, so workloads can refer to
the Service by name.</p>
</li>
<li class="calibre6">
<p class="author1">Creating a Service also triggers allocation of a unique IP address for the Service,
distinct from any other IP address in the cluster. We call this the Service IP.</p>
</li>
<li class="calibre6">
<p class="author1">The Service includes a <em class="hyperlink">selector</em>, which defines which Pods will be
associated with the Service.</p>
</li>
<li class="calibre6">
<p class="author1">Lastly, the Service gathers the Pod IP addresses of all its matching Pods
and maintains them as its endpoints.</p>
</li>
</ul>

<p class="author1">This is all shown in <a data-type="xref" href="#k8s-service-diagram-1" class="calibre4">Figure 13-2</a>—which, sadly, is still a
<em class="hyperlink">simplified</em> view of Services.</p>

<figure class="calibre23"><div class="figure" id="k8s-service-diagram-1">
<img alt="luar 1302" src="assets/luar_1302.png" class="calibre24"/>
<h6 class="calibre25"><span class="calibre">Figure 13-2. </span>Kubernetes Services and addressing</h6>
</div></figure>

<p class="author1">When a workload attempts to connect to a Service,<a data-primary="Services" data-secondary="Pod IP addresses as endpoints of Service" data-type="indexterm" id="id1683" class="calibre4"/> Kubernetes
will, by default, select one of the Service’s endpoints and route the
connection there. This allows Kubernetes to perform basic load balancing of
connections, as shown in <a data-type="xref" href="#k8s-routing-diagram" class="calibre4">Figure 13-3</a>:</p>

<ul class="printings">
<li class="calibre6">
<p class="author1">Connections within Pods happen over localhost so that they stay within the
Pod.</p>
</li>
<li class="calibre6">
<p class="author1">Connections to other workloads hosted on the same Node stay internal to the
Node.</p>
</li>
<li class="calibre6">
<p class="author1">Connections to workloads hosted on other Nodes are the only ones that
transit the network.</p>
</li>
</ul>

<figure class="calibre23"><div class="figure" id="k8s-routing-diagram">
<img alt="luar 1303" src="assets/luar_1303.png" class="calibre24"/>
<h6 class="calibre25"><span class="calibre">Figure 13-3. </span>Kubernetes basic network routing</h6>
</div></figure>

<p class="author1">To make this all work, Kubernetes relies on the networking mechanisms built
into the core of the Linux kernel.<a data-startref="ch13-net" data-type="indexterm" id="id1684" class="calibre4"/><a data-startref="ch13-net2" data-type="indexterm" id="id1685" class="calibre4"/><a data-startref="ch13-net3" data-type="indexterm" id="id1686" class="calibre4"/><a data-startref="ch13-net4" data-type="indexterm" id="id1687" class="calibre4"/></p>
</div></section>








<section data-pdf-bookmark="The Role of the Packet Filter" data-type="sect2" class="preface"><div class="preface" id="role_of_the_packet_filter_ch13">
<h2 class="calibre27">The Role of the Packet Filter</h2>

<p class="author1">The Linux kernel has long included<a data-primary="mesh networking" data-secondary="Kubernetes without Linkerd" data-tertiary="packet filter role" data-type="indexterm" id="ch13-pfr" class="calibre4"/><a data-primary="init container" data-secondary="Kubernetes without Linkerd" data-tertiary="packet filter role" data-type="indexterm" id="ch13-pfr2" class="calibre4"/><a data-primary="Kubernetes" data-secondary="without Linkerd" data-tertiary="packet filter role" data-type="indexterm" id="ch13-pfr3" class="calibre4"/><a data-primary="packet filter role" data-type="indexterm" id="ch13-pfr4" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="mesh networking" data-tertiary="packet filter role sans Linkerd" data-type="indexterm" id="ch13-pfr5" class="calibre4"/> a powerful <em class="hyperlink">packet filter</em> mechanism to
inspect network packets and make decisions about what to do with each one.
Possible actions the packet filter system can take include letting the packet
continue as is, modifying the packet, rerouting the packet, or even discarding
it entirely.</p>

<p class="author1">Kubernetes takes extensive advantage of the packet filter to handle the
complexities of routing traffic among an ever-changing set of Pods within a
cluster. For instance, the filter can intercept a packet sent to a Service
and rewrite it to go to a specific Pod IP instead. It can also
distinguish between a Pod IP on the same Node as the sender and one on a
different Node, and manage routing appropriately. If we zoom in a bit on
<a data-type="xref" href="#k8s-routing-diagram" class="calibre4">Figure 13-3</a>, we get the more detailed view in
<a data-type="xref" href="#k8s-routing-pf-diagram" class="calibre4">Figure 13-4</a>.</p>

<figure class="calibre23"><div class="figure" id="k8s-routing-pf-diagram">
<img alt="luar 1304" src="assets/luar_1304.png" class="calibre24"/>
<h6 class="calibre25"><span class="calibre">Figure 13-4. </span>Kubernetes and the packet filter</h6>
</div></figure>

<p class="author1">Let’s follow the dotted-line connection shown in <a data-type="xref" href="#k8s-routing-pf-diagram" class="calibre4">Figure 13-4</a>, from Pod B-1 all the way to Pod C-2:</p>

<ul class="printings">
<li class="calibre6">
<p class="author1">The application container in Pod B-1 makes a connection to the Service IP address for Service C.</p>
</li>
<li class="calibre6">
<p class="author1">The packet filter sees a connection from a local container to the Service
IP, so it redirects that connection to the Pod IP of either Pod C-1 or Pod C-2. By default, the choice is random for each new connection (though the exact configuration of the cluster’s networking layer can change this).</p>
</li>
<li class="calibre6">
<p class="author1">In this case, the Pod IP is on a different Node, so the network hardware gets
involved to communicate over the network to the second Node.</p>
</li>
<li class="calibre6">
<p class="author1">On the second Node, the packet filter sees the connection coming over the
network to a Pod IP address, so it hands the connection directly to the Pod,
choosing a container based on the port number.</p>
</li>
</ul>

<p class="author1">For the dashed-line connection shown between Pod B-1 and Pod A-1, the process is the same, except that the network hardware
has no role to play since the connection is entirely contained within
one Node. In all cases, the containers see a simple, flat network, with all
containers living in the same IP address range—which, of course, requires
Kubernetes to continuously update the packet filter rules as
Pods are created and removed.</p>
<div data-type="note" epub:type="note" class="calibre16"><h1 class="calibre26">Alphabet Soup: iptables, nftables, and eBPF</h1>
<p class="author1">There have been several implementations<a data-primary="packet filter role" data-secondary="iptables, nftables, eBPF" data-type="indexterm" id="id1688" class="calibre4"/> of the packet filter over time, and
you may hear people use the name of a specific implementation when talking about this
topic. The most common as of this writing is <code class="calibre9">iptables</code>, but a newer
<code class="calibre9">nftables</code> implementation is becoming more popular.</p>

<p class="author1">You might also find this whole bit reminding you of the filtering technology
known as eBPF, which makes a lot of sense since eBPF is particularly good at
this kind of packet wizardry. However, many implementations predate eBPF and
don’t rely on it.<a data-startref="ch13-pfr" data-type="indexterm" id="id1689" class="calibre4"/><a data-startref="ch13-pfr2" data-type="indexterm" id="id1690" class="calibre4"/><a data-startref="ch13-pfr3" data-type="indexterm" id="id1691" class="calibre4"/><a data-startref="ch13-pfr4" data-type="indexterm" id="id1692" class="calibre4"/><a data-startref="ch13-pfr5" data-type="indexterm" id="id1693" class="calibre4"/></p>
</div>
</div></section>








<section data-pdf-bookmark="The Container Networking Interface" data-type="sect2" class="preface"><div class="preface" id="id102">
<h2 class="calibre27">The Container Networking Interface</h2>

<p class="author1">Since networking configuration<a data-primary="mesh networking" data-secondary="Kubernetes without Linkerd" data-tertiary="Kubernetes CNI" data-type="indexterm" id="id1694" class="calibre4"/><a data-primary="Kubernetes CNI (Container Network Interface)" data-type="indexterm" id="id1695" class="calibre4"/><a data-primary="Kubernetes" data-secondary="without Linkerd" data-tertiary="Kubernetes CNI" data-type="indexterm" id="id1696" class="calibre4"/> is a rather low-level aspect of Kubernetes,
the details tend to depend on which Kubernetes implementation is in use. The
Container Network Interface (CNI) is a standard
designed to offer a consistent interface for managing dynamic network
configurations; for example, the CNI provides mechanisms for allocating and
releasing IP addresses within specific ranges, which Kubernetes uses in turn
to manage the IP addresses associated with Services and Pods.</p>

<p class="author1">The CNI doesn’t directly provide mechanisms for managing packet filtering
functionality, but it does allow for <em class="hyperlink">CNI plugins</em>. Service meshes—including
Linkerd—can use these plugins to implement the packet filtering configuration
they need to function.</p>
<div class="calibre16" data-type="note" epub:type="note"><h1 class="calibre26">CNI Versus CNI</h1>
<p class="author1">There are many implementations of the CNI, and a given Kubernetes solution
often allows a choice between several different CNI implementations <a data-primary="k3d clusters in multicluster setup" data-secondary="Flannel CNI used by k3d" data-type="indexterm" id="id1697" class="calibre4"/><a data-primary="Flannel CNI used by k3d" data-type="indexterm" id="id1698" class="calibre4"/><a data-primary="k3d clusters in multicluster setup" data-secondary="Calico CNI available" data-type="indexterm" id="id1699" class="calibre4"/><a data-primary="Kubernetes" data-secondary="k3d clusters in multicluster setup" data-tertiary="Calico CNI available" data-type="indexterm" id="id1700" class="calibre4"/><a data-primary="Kubernetes" data-secondary="k3d clusters in multicluster setup" data-tertiary="Flannel CNI used by k3d" data-type="indexterm" id="id1701" class="calibre4"/><a data-primary="Calico CNI" data-type="indexterm" id="id1702" class="calibre4"/>(for
example, k3d uses <a href="https://oreil.ly/GIVvg" class="calibre4">Flannel</a> by default
as its networking layer, but it can be easily switched to
<a href="https://oreil.ly/YSSts" class="calibre4">Calico</a>).</p>
</div>
</div></section>








<section data-pdf-bookmark="The Kubernetes Pod Startup Process" data-type="sect2" class="preface"><div class="preface" id="id103">
<h2 class="calibre27">The Kubernetes Pod Startup Process</h2>

<p class="author1">When all is said and done, here’s what Kubernetes needs to do to start a Pod:<a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="Pod startup process" data-type="indexterm" id="id1703" class="calibre4"/><a data-primary="mesh networking" data-secondary="Pod startup process" data-type="indexterm" id="id1704" class="calibre4"/><a data-primary="init container" data-secondary="Pod startup process" data-type="indexterm" id="id1705" class="calibre4"/><a data-primary="Pods" data-secondary="startup process" data-type="indexterm" id="id1706" class="calibre4"/><a data-primary="Kubernetes" data-secondary="Pod startup process" data-type="indexterm" id="id1707" class="calibre4"/></p>
<ol class="calibre54">
<li class="calibre55">
<p class="author1">Locate a Node to host the new Pod.</p>
</li>
<li class="calibre55">
<p class="author1">Run any CNI plugins defined by the Node within the new Pod’s context. The
process fails if any plugin doesn’t work.</p>
</li>
<li class="calibre55">
<p class="author1">Execute any init containers defined for the new Pod, in the order they’re
defined. Again, the startup process fails if any don’t work.</p>
</li>
<li class="calibre55">
<p class="author1">Launch all the containers defined by the Pod.</p>
</li>

</ol>

<p class="author1">During the initiation of the Pod’s containers, it’s important to note that the
containers will start in the order outlined by the Pod’s <code class="calibre9">spec</code>. However,<a data-primary="Pods" data-secondary="startup process" data-tertiary="postStartHook" data-type="indexterm" id="id1708" class="calibre4"/>
Kubernetes will <em class="hyperlink">not</em> wait for one container to start before moving on to the
next, unless a container has a <code class="calibre9">postStartHook</code> defined. In that case,
Kubernetes will start that container, run the <code class="calibre9">postStartHook</code> to completion,
and only then proceed to start the next container. We’ll talk more about this
in <a data-type="xref" href="#container_ordering_ch13" class="calibre4">“Container ordering”</a>.<a data-startref="ch13-sans" data-type="indexterm" id="id1709" class="calibre4"/></p>
</div></section>
</div></section>






<section data-pdf-bookmark="Kubernetes and Linkerd" data-type="sect1" class="preface"><div class="preface" id="id184">
<h1 class="calibre8">Kubernetes and Linkerd</h1>

<p class="author1">Any service mesh introduces complexities<a data-primary="mesh networking" data-secondary="Kubernetes and Linkerd" data-type="indexterm" id="id1710" class="calibre4"/><a data-primary="init container" data-secondary="Kubernetes and Linkerd" data-type="indexterm" id="id1711" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="mesh networking" data-tertiary="Kubernetes and Linkerd" data-type="indexterm" id="id1712" class="calibre4"/><a data-primary="Kubernetes" data-secondary="Linkerd and" data-type="indexterm" id="id1713" class="calibre4"/> into startup, and Linkerd is no exception. The first concern is that Linkerd has to inject its proxy into application Pods, and the proxy has to intercept network traffic going into and out of the Pod. Injection is managed using a mutating admission controller. Interception is more complex, and Linkerd has two ways to manage it: you can use either an init container or a CNI plugin.</p>








<section data-pdf-bookmark="The Init Container Approach" data-type="sect2" class="preface"><div class="preface" id="id216">
<h2 class="calibre27">The Init Container Approach</h2>

<p class="author1">The most straightforward way <a data-primary="Kubernetes" data-secondary="Linkerd and" data-tertiary="init container approach" data-type="indexterm" id="id1714" class="calibre4"/><a data-primary="mesh networking" data-secondary="Kubernetes and Linkerd" data-tertiary="init container approach" data-type="indexterm" id="id1715" class="calibre4"/><a data-primary="init container" data-secondary="Kubernetes and Linkerd" data-tertiary="init container approach" data-type="indexterm" id="id1716" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="Kubernetes and Linkerd" data-tertiary="init container approach" data-type="indexterm" id="id1717" class="calibre4"/>for Linkerd to configure networking is via an
init container, as shown in <a data-type="xref" href="#init-container-diagram" class="calibre4">Figure 13-5</a>. Kubernetes ensures all
init containers are run to completion, in the order they’re mentioned in the
Pod’s <code class="calibre9">spec</code>, before any other containers start. This makes the init container
an ideal way to configure the packet filter.</p>

<figure class="calibre23"><div class="figure" id="init-container-diagram">
<img alt="luar 1305" src="assets/luar_1305.png" class="calibre24"/>
<h6 class="calibre25"><span class="calibre">Figure 13-5. </span>Startup with the init container</h6>
</div></figure>

<p class="author1">The downside here is that the init container requires the <code class="calibre9">NET_ADMIN</code>
capability to perform the required configuration. In many Kubernetes runtimes,
this capability simply isn’t available, and you’ll need to resort to the
Linkerd CNI plugin.</p>

<p class="author1">Also, the OS used in some Kubernetes clusters may not support the older
<code class="calibre9">iptables</code> binary used by default in Linkerd (this typically comes into play with the
Red Hat family). In these instances, you’ll need to set
<code class="calibre9">proxyInit.iptablesMode=nft</code> to instruct Linkerd to use <code class="calibre9">iptables-nft</code>
instead. (This isn’t the default setting because <code class="calibre9">iptables-nft</code> isn’t
universally supported yet.)</p>
</div></section>








<section data-pdf-bookmark="The Linkerd CNI Plugin Method" data-type="sect2" class="preface"><div class="preface" id="id217">
<h2 class="calibre27">The Linkerd CNI Plugin Method</h2>

<p class="author1">In contrast, the Linkerd CNI plugin<a data-primary="mesh networking" data-secondary="Kubernetes and Linkerd" data-tertiary="Linkerd CNI plugin approach" data-type="indexterm" id="id1718" class="calibre4"/><a data-primary="init container" data-secondary="Kubernetes and Linkerd" data-tertiary="Linkerd CNI plugin approach" data-type="indexterm" id="id1719" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="Kubernetes and Linkerd" data-tertiary="Linkerd CNI plugin approach" data-type="indexterm" id="id1720" class="calibre4"/><a data-primary="Kubernetes" data-secondary="Linkerd and" data-tertiary="Linkerd CNI plugin approach" data-type="indexterm" id="id1721" class="calibre4"/> simply requires that you install the
plugin prior to installing Linkerd itself. It doesn’t need any special
capabilities, and the CNI plugin will operate every time a Pod starts,
configuring the packet filter as required, as shown in <a data-type="xref" href="#cni-startup-diagram" class="calibre4">Figure 13-6</a>.</p>

<figure class="calibre23"><div class="figure" id="cni-startup-diagram">
<img alt="luar 1306" src="assets/luar_1306.png" class="calibre24"/>
<h6 class="calibre25"><span class="calibre">Figure 13-6. </span>Startup with the CNI plugin</h6>
</div></figure>

<p class="author1">The main complication here is that the CNI was originally designed for the
people setting up the cluster in the first place, rather than people using it
after it’s been created. As a result, the CNI assumes that the ordering of CNI
plugins is handled completely outside the Kubernetes environment. This has
turned out to be less than ideal, so most CNI plugins these days (including
the Linkerd CNI plugin) are written to try to do the right thing no matter
what the cluster operators did.</p>

<p class="author1">In the case of the Linkerd CNI plugin, when it’s enabled Linkerd installs a
DaemonSet designed to arrange for the Linkerd CNI plugin to always run last.
This allows other plugins the chance to configure what they need before
Linkerd jumps in to enable the Linkerd proxy to intercept traffic.</p>

<p class="author1">When using the CNI plugin, Linkerd will still inject an init container. If
you’re using a version of Linkerd prior to <code class="calibre9">stable-2.13.0</code>, this will be a no-op init
container that, as the name suggests, essentially doesn’t do much. From
<code class="calibre9">stable-2.13.0</code> onward, the init container will verify that the packet filter is
correctly configured. If it’s not, the container will fail, prompting
Kubernetes to restart the Pod. This helps to avoid a startup race condition
(more on this in the next section).</p>
</div></section>








<section data-pdf-bookmark="Races and Ordering" data-type="sect2" class="preface"><div class="preface" id="id218">
<h2 class="calibre27">Races and Ordering</h2>

<p class="author1">As you can see, the startup process in Kubernetes can be complex—which
means that there are several different ways things can fail.<a data-primary="debugging" data-secondary="data plane issues" data-tertiary="Pods failing to start" data-type="indexterm" id="id1722" class="calibre4"/><a data-primary="Kubernetes" data-secondary="Pod startup process" data-tertiary="Pods failing to start" data-type="indexterm" id="id1723" class="calibre4"/><a data-primary="Pods" data-secondary="startup process" data-tertiary="debugging startup failures" data-type="indexterm" id="id1724" class="calibre4"/></p>










<section data-pdf-bookmark="Container ordering" data-type="sect3" class="preface"><div class="preface" id="container_ordering_ch13">
<h3 class="calibre33">Container ordering</h3>

<p class="author1">As mentioned previously, containers are launched in<a data-primary="init container" data-secondary="Pod startup process" data-tertiary="container ordering" data-type="indexterm" id="id1725" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="Pod startup process" data-tertiary="container ordering" data-type="indexterm" id="id1726" class="calibre4"/><a data-primary="mesh networking" data-secondary="Pod startup process" data-tertiary="container ordering" data-type="indexterm" id="id1727" class="calibre4"/><a data-primary="Kubernetes" data-secondary="Pod startup process" data-tertiary="container ordering" data-type="indexterm" id="id1728" class="calibre4"/><a data-primary="Pods" data-secondary="startup process" data-tertiary="container ordering" data-type="indexterm" id="id1729" class="calibre4"/> the order they appear in the Pod’s <code class="calibre9">spec</code>, but
Kubernetes doesn’t wait for a given container to start before launching the next one
(except for init containers). This can cause trouble during Linkerd’s startup:
what if the application container begins running and tries to use the network
before the Linkerd proxy container is functioning?</p>

<p class="author1">Starting with Linkerd 2.12, <a data-primary="Pods" data-secondary="startup process" data-tertiary="postStartHook" data-type="indexterm" id="id1730" class="calibre4"/>there’s a <code class="calibre9">postStartHook</code> on the Linkerd proxy
container to deal with this. When a container has a <code class="calibre9">postStartHook</code>,
Kubernetes starts the container, then runs the <code class="calibre9">postStartHook</code> to completion
before starting the next container. This gives containers a straightforward
way to ensure ordering.</p>

<p class="author1">The Linkerd proxy’s <code class="calibre9">postStartHook</code> won’t complete until the proxy is actually
running, which forces Kubernetes to wait until the proxy is functional before
starting the application container. If necessary, this functionality can be
disabled by setting the annotation <code class="calibre9">config.linkerd.io/proxy-await=disabled</code>.
However, we recommend leaving it enabled unless there’s a compelling reason to
do otherwise!</p>
</div></section>










<section data-pdf-bookmark="CNI plugin ordering" data-type="sect3" class="preface"><div class="preface" id="id220">
<h3 class="calibre33">CNI plugin ordering</h3>

<p class="author1">There are several ways CNI plugin ordering can cause confusion:<a data-primary="mesh networking" data-secondary="Pod startup process" data-tertiary="CNI plugin ordering" data-type="indexterm" id="id1731" class="calibre4"/><a data-primary="init container" data-secondary="Pod startup process" data-tertiary="CNI plugin ordering" data-type="indexterm" id="id1732" class="calibre4"/><a data-primary="Linkerd CNI (Container Network Interface) plugin" data-secondary="Pod startup process" data-tertiary="CNI plugin ordering" data-type="indexterm" id="id1733" class="calibre4"/><a data-primary="Kubernetes" data-secondary="Pod startup process" data-tertiary="CNI plugin ordering" data-type="indexterm" id="id1734" class="calibre4"/><a data-primary="Pods" data-secondary="startup process" data-tertiary="CNI plugin ordering" data-type="indexterm" id="id1735" class="calibre4"/></p>
<dl class="calibre10">
<dt class="calibre11">DaemonSets versus other Pods</dt>
<dd class="calibre12">
<p class="calibre13">Kubernetes treats DaemonSet Pods just like any other Pods, which means that an
application Pod might be scheduled before the Linkerd CNI DaemonSet can
install the Linkerd CNI plugin! This implies that the Linkerd CNI plugin won’t
run for the application Pod, which in turn means that the application
container(s) won’t have a functioning Linkerd proxy.</p>

<p class="calibre13">Before Linkerd <code class="calibre9">stable-2.13.0</code>, there was no way to catch this, and the
application container would simply never be present in the mesh. As of
<code class="calibre9">stable-2.13.0</code>, though, the init container checks that the packet filter has been
configured correctly. If it’s not, the init container will exit, causing a crash
loop from Kubernetes’s point of view, which will make the failure obvious.</p>
</dd>
<dt class="calibre11">Multiple CNI plugins</dt>
<dd class="calibre12">
<p class="calibre13">In most cases, a given Kubernetes installation will use more than one CNI
plugin. While the Linkerd CNI DaemonSet puts a lot of effort into allowing the
Linkerd CNI plugin to run last, and to not disrupting other CNI plugins, it’s not
perfect. If this goes wrong, the Pod will (again) probably never appear to be
in the mesh.</p>
</dd>
<dt class="calibre11">Misconfigured CNI</dt>
<dd class="calibre12">
<p class="calibre13">It’s always possible to simply misconfigure the Linkerd CNI plugin when you
install it in the first place. For example, when running k3d, it’s necessary
to supply the plugin with certain paths, and if these are wrong, the plugin
itself won’t work. This might cause application Pods to silently fail to
launch, or it might cause “corrupt message” errors to show up in the proxy
logs:</p>

<pre data-code-language="json" data-type="programlisting" class="calibre36"><code class="p">{</code><code class="w"> </code><code class="nt">"message"</code><code class="p">:</code><code class="w"> </code><code class="s">"Failed to connect"</code><code class="p">,</code><code class="w"> </code><code class="nt">"error"</code><code class="p">:</code><code class="w"> </code><code class="s">"received corrupt message"</code><code class="w"> </code><code class="p">}</code><code class="w"/></pre>
</dd>
</dl>

<p class="author1">The only real saving grace of CNI issues is that they’re typically pretty
obvious, conspicuous errors: you’ll see <code class="calibre9">linkerd check</code> fail, or Pods won’t
start, or similar things. On the other hand, <em class="hyperlink">resolving</em> the failures can be
tricky and depends greatly on the specific CNI involved, so in general we
recommend sticking with the init container mechanism where possible.</p>
</div></section>
</div></section>
</div></section>






<section data-pdf-bookmark="Summary" data-type="sect1" class="preface"><div class="preface" id="id329">
<h1 class="calibre8">Summary</h1>

<p class="author1">There’s a lot of complexity to the Kubernetes startup process—especially
with Linkerd—but there are also some simple recommendations to help keep
everything going smoothly:</p>

<ul class="printings">
<li class="calibre6">
<p class="author1">Keep Linkerd up-to-date! Recent versions have added some really helpful
things for startup.</p>
</li>
<li class="calibre6">
<p class="author1">Use <code class="calibre9">proxy-await</code> unless you have a <em class="hyperlink">very</em> good reason to disable it.
It’ll make sure that your application code has a working mesh before
starting.</p>
</li>
<li class="calibre6">
<p class="author1">Stick with the init container if you can. If not, just use the CNI plugin,
but if your cluster can run with the init container, it’s probably simplest
to do so.</p>
</li>
</ul>
</div></section>
</div></section></body></html>