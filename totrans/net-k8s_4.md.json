["```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: go-web\n  namespace: default\nspec:\n  containers:\n  - name: go-web\n    image: go-web:v0.0.1\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  nodeName: \"node-1\"\n  containers:\n    - name: example\n      image: example:1.0\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: go-web\nspec:\n  containers:\n  - name: go-web\n    image: go-web:v0.0.1\n    ports:\n    - containerPort: 8080\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 5\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 5\n```", "```\nkind: Pod\n…\nspec:\n  readinessGates:\n  - conditionType: www.example.com/feature-X\n  - conditionType: www.example.com/feature-Y\n…\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: 2021-04-25T00:00:00Z\n    status: \"False\"\n    type: Ready\n  - lastProbeTime: null\n    lastTransitionTime: 2021-04-25T00:00:00Z\n    status: \"False\"\n    type: www.example.com/feature-X\n  - lastProbeTime: null\n    lastTransitionTime: 2021-04-25T00:00:00Z\n    status: \"True\"\n    type: www.example.com/feature-Y\n  containerStatuses:\n  - containerID: docker://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n    ready : true\n```", "```\n{\n  \"cniVersion\": \"0.4.0\",\n  \"ips\": [\n      {\n          \"version\": \"<4-or-6>\",\n          \"address\": \"<ip-and-prefix-in-CIDR>\",\n          \"gateway\": \"<ip-address-of-the-gateway>\"  (optional)\n      },\n      ...\n  ],\n  \"routes\": [                                       (optional)\n      {\n          \"dst\": \"<ip-and-prefix-in-cidr>\",\n          \"gw\": \"<ip-of-next-hop>\"                  (optional)\n      },\n      ...\n  ]\n  \"dns\": {                                          (optional)\n    \"nameservers\": <list-of-nameservers>            (optional)\n    \"domain\": <name-of-local-domain>                (optional)\n    \"search\": <list-of-search-domains>              (optional)\n    \"options\": <list-of-options>                    (optional)\n  }\n}\n```", "```\nkind: Cluster ![1](Images/1.png)\napiVersion: kind.x-k8s.io/v1alpha4 ![2](Images/2.png)\nnodes: ![3](Images/3.png)\n- role: control-plane ![4](Images/4.png)\n- role: worker ![5](Images/5.png)\n- role: worker ![6](Images/6.png)\n- role: worker ![7](Images/7.png)\nnetworking: ![8](Images/8.png)\ndisableDefaultCNI: true ![9](Images/9.png)\n```", "```\n$ kind create cluster --config=kind-config.yaml\nCreating cluster \"kind\" ...\n✓ Ensuring node image (kindest/node:v1.18.\n2) Preparing nodes\n✓ Writing configuration Starting control-plane\nInstalling StorageClass Joining worker nodes Set kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a question, bug, or feature request?\nLet us know! https://kind.sigs.k8s.io/#community ߙ⊭---\n\nAlways verify that the cluster is up and running with kubectl.\n```", "```\n$ kubectl cluster-info --context kind-kind\nKubernetes master -> control plane is running at https://127.0.0.1:59511\nKubeDNS is running at\nhttps://127.0.0.1:59511/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump.'\n```", "```\n$ helm repo add cilium https://helm.cilium.io/\n# Pre-pulling and loading container images is optional.\n$ docker pull cilium/cilium:v1.9.1\nkind load docker-image cilium/cilium:v1.9.1\n```", "```\n$ helm install cilium cilium/cilium --version 1.10.1 \\\n  --namespace kube-system\n\nNAME: Cilium\nLAST DEPLOYED: Fri Jan  1 15:39:59 2021\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nYou have successfully installed Cilium with Hubble.\n\nYour release version is 1.10.1.\n\nFor any further help, visit https://docs.cilium.io/en/v1.10/gettinghelp/\n```", "```\n$ kubectl -n kube-system get pods --watch\nNAME                                         READY   STATUS\ncilium-65kvp                                 0/1     Init:0/2\ncilium-node-init-485lj                       0/1     ContainerCreating\ncilium-node-init-79g68                       1/1     Running\ncilium-node-init-gfdl8                       1/1     Running\ncilium-node-init-jz8qc                       1/1     Running\ncilium-operator-5b64c54cd-cgr2b              0/1     ContainerCreating\ncilium-operator-5b64c54cd-tblbz              0/1     ContainerCreating\ncilium-pg6v8                                 0/1     Init:0/2\ncilium-rsnqk                                 0/1     Init:0/2\ncilium-vfhrs                                 0/1     Init:0/2\ncoredns-66bff467f8-dqzql                     0/1     Pending\ncoredns-66bff467f8-r5nl6                     0/1     Pending\netcd-kind-control-plane                      1/1     Running\nkube-apiserver-kind-control-plane            1/1     Running\nkube-controller-manager-kind-control-plane   1/1     Running\nkube-proxy-k5zc2                             1/1     Running\nkube-proxy-qzhvq                             1/1     Running\nkube-proxy-v54p4                             1/1     Running\nkube-proxy-xb9tr                             1/1     Running\nkube-scheduler-kind-control-plane            1/1     Running\ncilium-operator-5b64c54cd-tblbz              1/1     Running\n```", "```\n$ kubectl create ns cilium-test\nnamespace/cilium-test created\n\n$ kubectl apply -n cilium-test \\\n-f \\\nhttps://raw.githubusercontent.com/strongjz/advanced_networking_code_examples/\nmaster/chapter-4/connectivity-check.yaml\n\ndeployment.apps/echo-a created\ndeployment.apps/echo-b created\ndeployment.apps/echo-b-host created\ndeployment.apps/pod-to-a created\ndeployment.apps/pod-to-external-1111 created\ndeployment.apps/pod-to-a-denied-cnp created\ndeployment.apps/pod-to-a-allowed-cnp created\ndeployment.apps/pod-to-external-fqdn-allow-google-cnp created\ndeployment.apps/pod-to-b-multi-node-clusterip created\ndeployment.apps/pod-to-b-multi-node-headless created\ndeployment.apps/host-to-b-multi-node-clusterip created\ndeployment.apps/host-to-b-multi-node-headless created\ndeployment.apps/pod-to-b-multi-node-nodeport created\ndeployment.apps/pod-to-b-intra-node-nodeport created\nservice/echo-a created\nservice/echo-b created\nservice/echo-b-headless created\nservice/echo-b-host-headless created\nciliumnetworkpolicy.cilium.io/pod-to-a-denied-cnp created\nciliumnetworkpolicy.cilium.io/pod-to-a-allowed-cnp created\nciliumnetworkpolicy.cilium.io/pod-to-external-fqdn-allow-google-cnp created\n```", "```\n$ kubectl get pods -n cilium-test -w\nNAME                                                     READY   STATUS\necho-a-57cbbd9b8b-szn94                                  1/1     Running\necho-b-6db5fc8ff8-wkcr6                                  1/1     Running\necho-b-host-76d89978c-dsjm8                              1/1     Running\nhost-to-b-multi-node-clusterip-fd6868749-7zkcr           1/1     Running\nhost-to-b-multi-node-headless-54fbc4659f-z4rtd           1/1     Running\npod-to-a-648fd74787-x27hc                                1/1     Running\npod-to-a-allowed-cnp-7776c879f-6rq7z                     1/1     Running\npod-to-a-denied-cnp-b5ff897c7-qp5kp                      1/1     Running\npod-to-b-intra-node-nodeport-6546644d59-qkmck            1/1     Running\npod-to-b-multi-node-clusterip-7d54c74c5f-4j7pm           1/1     Running\npod-to-b-multi-node-headless-76db68d547-fhlz7            1/1     Running\npod-to-b-multi-node-nodeport-7496df84d7-5z872            1/1     Running\npod-to-external-1111-6d4f9d9645-kfl4x                    1/1     Running\npod-to-external-fqdn-allow-google-cnp-5bc496897c-bnlqs   1/1     Running\n```", "```\n$ sudo iptables -t nat -L KUBE-SERVICES\nChain KUBE-SERVICES (2 references)\ntarget     prot opt source destination\n\n/* kube-system/kube-dns:dns cluster IP */ udp dpt:domain\nKUBE-MARK-MASQ  udp  -- !10.217.0.0/16        10.96.0.10\n/* kube-system/kube-dns:dns cluster IP */ udp dpt:domain\nKUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  anywhere  10.96.0.10\n/* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:domain\nKUBE-MARK-MASQ  tcp  -- !10.217.0.0/16        10.96.0.10\n/* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:domain\nKUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  anywhere 10.96.0.10 ADDRTYPE\n    match dst-type LOCAL\n/* kubernetes service nodeports; NOTE: this must be the\n    last rule in this chain */\nKUBE-NODEPORTS  all  --  anywhere             anywhere\n```", "```\n$ sudo iptables -t nat -L KUBE-SVC-TCOU7JCQXEZGVUNU\nChain KUBE-SVC-TCOU7JCQXEZGVUNU (1 references)\ntarget     prot opt source destination\n\n/* kube-system/kube-dns:dns */\nKUBE-SEP-OCPCMVGPKTDWRD3C  all -- anywhere anywhere  statistic mode\n    random probability 0.50000000000\n/* kube-system/kube-dns:dns */\nKUBE-SEP-VFGOVXCRCJYSGAY3  all -- anywhere anywhere\n```", "```\n$ sudo iptables -t nat -L KUBE-SEP-OCPCMVGPKTDWRD3C\nChain KUBE-SEP-OCPCMVGPKTDWRD3C (1 references)\ntarget     prot opt source destination\n\n/* kube-system/kube-dns:dns */\nKUBE-MARK-MASQ  all  --  10.0.1.141           anywhere\n/* kube-system/kube-dns:dns */ udp to:10.0.1.141:53\nDNAT       udp  --  anywhere             anywhere\n```", "```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: demo\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: demo\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress: []NetworkPolicyIngressRule # Not expanded\n  egress: []NetworkPolicyEgressRule # Not expanded\n```", "```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: demo-db\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: demo-db\n  policyTypes:\n  - Ingress\n  - Egress\n```", "```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: demo-db\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: demo-db\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: demo\n```", "```\n$ kubectl apply -f database.yaml\nservice/postgres created\nconfigmap/postgres-config created\nstatefulset.apps/postgres created\n```", "```\n$ kubectl apply -f web.yaml\ndeployment.apps/app created\n```", "```\n$ kubectl apply -f dnsutils.yaml\npod/dnsutils created\n```", "```\nkubectl port-forward app-5878d69796-j889q 8080:8080\n```", "```\n$ curl localhost:8080/\nHello\n$ curl localhost:8080/healthz\nHealthy\n$ curl localhost:8080/data\nDatabase Connected\n```", "```\n$ kubectl get pods -l app=app -o wide\nNAME                  READY  STATUS   RESTARTS  AGE  IP            NODE\napp-5878d69796-j889q  1/1    Running  0         87m  10.244.1.188  kind-worker3\n```", "```\n$ kubectl exec dnsutils -- nc -z -vv 10.244.1.188 8080\n10.244.1.188 (10.244.1.188:8080) open\nsent 0, rcvd 0\n```", "```\n$ kubectl exec dnsutils -- wget -qO- 10.244.1.188:8080/\nHello\n\n$ kubectl exec dnsutils -- wget -qO- 10.244.1.188:8080/data\nDatabase Connected\n\n$ kubectl exec dnsutils -- wget -qO- 10.244.1.188:8080/healthz\nHealthy\n```", "```\n$ kubectl get pods -l app=postgres -o wide\nNAME         READY   STATUS    RESTARTS   AGE   IP             NODE\npostgres-0   1/1     Running   0          98m   10.244.2.189   kind-worker\n```", "```\n$ kubectl exec dnsutils -- nc -z -vv 10.244.2.189 5432\n10.244.2.189 (10.244.2.189:5432) open\nsent 0, rcvd 0\n```", "```\n$ kubectl apply -f layer_3_net_pol.yaml\nciliumnetworkpolicy.cilium.io/l3-rule-app-to-db created\n```", "```\n$ kubectl describe ciliumnetworkpolicies.cilium.io l3-rule-app-to-db\nName:         l3-rule-app-to-db\nNamespace:    default\nLabels:       <none>\nAnnotations:  API Version:  cilium.io/v2\nKind:         CiliumNetworkPolicy\nMetadata:\nCreation Timestamp:  2021-01-10T01:06:13Z\nGeneration:          1\nManaged Fields:\nAPI Version:  cilium.io/v2\nFields Type:  FieldsV1\nfieldsV1:\nf:metadata:\nf:annotations:\n.:\nf:kubectl.kubernetes.io/last-applied-configuration:\nf:spec:\n.:\nf:endpointSelector:\n.:\nf:matchLabels:\n.:\nf:app:\nf:ingress:\nManager:         kubectl\nOperation:       Update\nTime:            2021-01-10T01:06:13Z\nResource Version:  47377\nSelf Link:\n/apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/l3-rule-app-to-db\nUID:       71ee6571-9551-449d-8f3e-c177becda35a\nSpec:\nEndpoint Selector:\nMatch Labels:\nApp:  postgres\nIngress:\nFrom Endpoints:\nMatch Labels:\nApp:  app\nEvents:       <none>\n```", "```\n$ kubectl exec dnsutils -- nc -z -vv -w 5 10.244.2.189 5432\nnc: 10.244.2.189 (10.244.2.189:5432): Operation timed out\nsent 0, rcvd 0\ncommand terminated with exit code 1\n```", "```\n$ kubectl exec dnsutils -- wget -qO- 10.244.1.188:8080/data\nDatabase Connected\n\n$ curl localhost:8080/data\nDatabase Connected\n```", "```\n$ kubectl apply -f layer_7_netpol.yml\nciliumnetworkpolicy.cilium.io/l7-rule created\n```", "```\n$ kubectl get ciliumnetworkpolicies.cilium.io\nNAME      AGE\nl7-rule   6m54s\n\n$ kubectl describe ciliumnetworkpolicies.cilium.io l7-rule\nName:         l7-rule\nNamespace:    default\nLabels:       <none>\nAnnotations:  API Version:  cilium.io/v2\nKind:         CiliumNetworkPolicy\nMetadata:\n  Creation Timestamp:  2021-01-10T00:49:34Z\n  Generation:          1\n  Managed Fields:\n    API Version:  cilium.io/v2\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:kubectl.kubernetes.io/last-applied-configuration:\n      f:spec:\n        .:\n        f:egress:\n        f:endpointSelector:\n          .:\n          f:matchLabels:\n            .:\n            f:app:\n    Manager:         kubectl\n    Operation:       Update\n    Time:            2021-01-10T00:49:34Z\n  Resource Version:  43869\n  Self Link:/apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/l7-rule\n  UID:               0162c16e-dd55-4020-83b9-464bb625b164\nSpec:\n  Egress:\n    To Ports:\n      Ports:\n        Port:      8080\n        Protocol:  TCP\n      Rules:\n        Http:\n          Method:  GET\n          Path:    /\n          Method:  GET\n          Path:    /data\n  Endpoint Selector:\n    Match Labels:\n      App:  app\nEvents:     <none>\n```", "```\n$ kubectl exec dnsutils -- wget -qO- 10.244.1.188:8080/data\nDatabase Connected\n\n$kubectl exec dnsutils -- wget -qO- 10.244.1.188:8080/\nHello\n\n$ kubectl exec dnsutils -- wget -qO- -T 5 10.244.1.188:8080/healthz\nwget: error getting response\ncommand terminated with exit code 1\n```", "```\nmetadata:\n  labels:\n    colour: purple\n    shape: square\n```", "```\nmatchLabels:\n  colour: purple\n  shape: square\n```", "```\nmatchExpressions:\n  - key: colour\n    operator: In\n    values:\n      - purple\n  - key: shape\n    operator: In\n    values:\n      - square\n```", "```\nmatchExpressions:\n  - key: colour\n    operator: NotIn\n    values:\n      - red\n      - orange\n      - yellow\n  - key: shape\n    operator: Exists\n```", "```\nfrom:\n  - ipBlock:\n    - cidr: \"10.0.0.0/24\"\n    - except: \"10.0.0.10\"\n```", "```\n#\nfrom:\n  - namespaceSelector:\n    - matchLabels:\n      group: x\n```", "```\nfrom:\n  - podSelector:\n    - matchLabels:\n      service: y\n```", "```\nfrom:\n  - namespaceSelector:\n    - matchLabels:\n      group: monitoring\n    podSelector:\n    - matchLabels:\n      service: logscraper\n```", "```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: store-api\n  namespace: store\nspec:\n  podSelector:\n    matchLabels: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          app: frontend\n      podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          app: downstream-1\n      podSelector:\n        matchLabels:\n          app: downstream-1\n    - namespaceSelector:\n        matchLabels:\n          app: downstream-2\n      podSelector:\n        matchLabels:\n          app: downstream-2\n    ports:\n    - protocol: TCP\n      port: 8080\n```", "```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: store-to-downstream-1\n  namespace: downstream-1\nspec:\n  podSelector:\n    app: downstream-1\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          app: store\n    ports:\n    - protocol: TCP\n      port: 8080\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n  hostNetwork: true\n  dnsPolicy: ClusterFirstWithHostNet\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: busybox\nspec:\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  dnsPolicy: \"None\"\n  dnsConfig:\n    nameservers:\n      - 1.1.1.1\n    searches:\n      - ns1.svc.cluster-domain.example\n      - my.dns.search.suffix\n```", "```\n<service>.default.svc.cluster.local\n                ↓\n        svc.cluster.local\n                ↓\n          cluster.local\n                ↓\n        The host search path\n```", "```\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  labels:\n    app: MyApp\nspec:\n  ipFamilyPolicy: PreferDualStack\n  selector:\n    app: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n```"]