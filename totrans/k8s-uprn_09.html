<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 9. ReplicaSets" data-type="chapter" epub:type="chapter"><div class="chapter" id="replica_set">
<h1><span class="label">Chapter 9. </span>ReplicaSets</h1>
<p>We have covered how to run individual containers as Pods, but these Pods are essentially one-off singletons.<a data-primary="ReplicaSets" data-type="indexterm" id="ix_RepS"/>  More often than not, you want multiple replicas of a container running at a particular time <a data-primary="ReplicaSets" data-secondary="benefits of" data-type="indexterm" id="idm45664077273296"/>for a variety of reasons:</p>
<dl>
<dt>Redundancy</dt>
<dd>
<p>Failure toleration by running multiple instances.</p>
</dd>
<dt>Scale</dt>
<dd>
<p>Higher request-processing capacity by running multiple instances.</p>
</dd>
<dt>Sharding</dt>
<dd>
<p>Different replicas can handle different parts of a computation in parallel.</p>
</dd>
</dl>
<p>Of course, you could manually create multiple copies of a Pod using multiple different (though largely similar) Pod manifests, but doing so is both tedious and error-prone. Logically, a user managing a replicated set of Pods considers them as a single entity to be defined and managed—and that’s precisely what a ReplicaSet is. A ReplicaSet acts as a cluster-wide Pod manager, ensuring that the right types and numbers of Pods are running at all times.</p>
<p>Because ReplicaSets make it easy to create and manage replicated sets of Pods, they are the building blocks for common application deployment patterns and for self-healing applications at the infrastructure level. Pods managed by ReplicaSets are automatically rescheduled under certain failure conditions, such as node failures and network partitions.</p>
<p>The easiest way to think of a ReplicaSet is that it combines a cookie cutter and a desired number of cookies into a single API object.  When we define a ReplicaSet, we define a specification for the Pods we want to create (the “cookie cutter”) and a desired number of replicas. Additionally, we need to define a way of finding Pods that the ReplicaSet should control.<a data-primary="reconciliation loops" data-type="indexterm" id="idm45664077266160"/> The actual act of managing the replicated Pods is an example of a <em>reconciliation loop</em>. Such loops are fundamental to most of the design and implementation of Kubernetes.</p>
<section data-pdf-bookmark="Reconciliation Loops" data-type="sect1"><div class="sect1" id="idm45664077264816">
<h1>Reconciliation Loops</h1>
<p>The central concept behind a reconciliation loop is the notion of <em>desired</em> state versus <em>observed</em> or <em>current</em> state.<a data-primary="ReplicaSets" data-secondary="reconciliation loops" data-type="indexterm" id="idm45664077261776"/><a data-primary="desired state versus observed or current state" data-type="indexterm" id="idm45664077260768"/><a data-primary="state" data-secondary="desired state versus observed or current state" data-type="indexterm" id="idm45664077260000"/>  Desired state is the state you want. With a ReplicaSet, it is the desired number of replicas and the definition of the Pod to replicate. For example, “the desired state is that there are three replicas of a Pod running the <code>kuard</code> server.” In contrast, the current state is the currently observed state of the system.  For example, “there are only two <code>kuard</code> Pods currently running.”</p>
<p>The reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state.  For instance, with the previous examples, the reconciliation loop would create a new <code>kuard</code> Pod in an effort to make the observed state match the desired state of three replicas.</p>
<p>There are many benefits to the reconciliation-loop approach to managing state.  It is an inherently goal-driven, self-healing system, yet it can often be easily expressed in a few lines of code.<a data-primary="state" data-secondary="benefits of reconciliation loop approach to" data-type="indexterm" id="idm45664077256320"/> For example, the reconciliation loop for ReplicaSets is a single loop, yet it handles user actions to scale up or scale down the ReplicaSet as well as node failures or nodes rejoining the cluster after being absent.</p>
<p>We’ll see numerous examples of reconciliation loops in action throughout the rest of the book.</p>
</div></section>
<section data-pdf-bookmark="Relating Pods and ReplicaSets" data-type="sect1"><div class="sect1" id="idm45664077254320">
<h1>Relating Pods and ReplicaSets</h1>
<p>Decoupling is a key theme in Kubernetes.<a data-primary="ReplicaSets" data-secondary="relating Pods and" data-type="indexterm" id="idm45664077253024"/><a data-primary="Pods" data-secondary="relating ReplicaSets to" data-type="indexterm" id="idm45664077252048"/> In particular, it’s important that all of the core concepts of Kubernetes are modular with respect to each other and that they are swappable and replaceable with other components. In this spirit, the relationship between ReplicaSets and Pods is loosely coupled.  Though ReplicaSets create and manage Pods, they do not own the Pods they create.  ReplicaSets use label queries to identify the set of Pods they should be managing.  They then use the exact same Pod API that you used directly in <a data-type="xref" href="ch05.xhtml#pods">Chapter 5</a> to create the Pods that they are managing. This notion of “coming in the front door” is another central design concept in Kubernetes. In a similar decoupling, ReplicaSets that create multiple Pods and the services that load balance to those Pods are also totally separate, decoupled API objects.<a data-primary="decoupled architectures" data-secondary="decoupling of Pods and ReplicaSets" data-type="indexterm" id="idm45664077250176"/> In addition to supporting modularity, decoupling Pods and ReplicaSets enables several important behaviors, discussed in the following <span class="keep-together">sections</span>.</p>
<section data-pdf-bookmark="Adopting Existing Containers" data-type="sect2"><div class="sect2" id="idm45664077248112">
<h2>Adopting Existing Containers</h2>
<p>Although declarative configuration is valuable, there are times when it is easier to build something up imperatively.<a data-primary="containers" data-secondary="ReplicaSets adopting existing containers" data-type="indexterm" id="idm45664077246656"/>  In particular, early on you may be simply deploying a single Pod with a container image without a ReplicaSet managing it. You might even define a load balancer to serve traffic to that single Pod.</p>
<p>But at some point, you may want to expand your singleton container into a replicated service and create and manage an array of similar containers. If ReplicaSets owned the Pods they created, then the only way to start replicating your Pod would be to delete it and relaunch it via a ReplicaSet. This might be disruptive, as there would be a moment when there would be no copies of your container running. However, because ReplicaSets are decoupled from the Pods they manage, you can simply create a ReplicaSet that will “adopt” the existing Pod and scale out additional copies of those containers. In this way, you can seamlessly move from a single imperative Pod to a replicated set of Pods managed by a ReplicaSet.</p>
</div></section>
<section data-pdf-bookmark="Quarantining Containers" data-type="sect2"><div class="sect2" id="idm45664077244816">
<h2>Quarantining Containers</h2>
<p>Oftentimes, when a server misbehaves, Pod-level health checks will automatically restart that Pod.<a data-primary="debugging" data-secondary="of Pods that are misbehaving" data-secondary-sortas="Pods" data-type="indexterm" id="idm45664077243376"/> But if your health checks are incomplete, a Pod can be misbehaving but still be part of the replicated set. In these situations, while it would work to simply kill the Pod, that would leave your developers with only logs to debug the problem. Instead, you can modify the set of labels on the sick Pod.<a data-primary="labels" data-secondary="changing on sick Pods" data-type="indexterm" id="idm45664077242032"/> Doing so will disassociate it from the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet controller will notice that a Pod is missing and create a new copy, but because the Pod is still running, it is available to developers for interactive debugging, which is significantly more valuable than debugging from logs.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Designing with ReplicaSets" data-type="sect1"><div class="sect1" id="idm45664077240576">
<h1>Designing with ReplicaSets</h1>
<p>ReplicaSets are designed to represent a single, scalable microservice inside your architecture. <a data-primary="ReplicaSets" data-secondary="designing with" data-type="indexterm" id="idm45664077239200"/>Their key characteristic is that every Pod the ReplicaSet controller creates is entirely homogeneous. Typically, these Pods are then fronted by a Kubernetes service load balancer, which spreads traffic across the Pods that make up the service. Generally speaking, ReplicaSets are designed for stateless (or nearly stateless) services. The elements they create are interchangeable; when a ReplicaSet is scaled down, an arbitrary Pod is selected for deletion. Your application’s behavior shouldn’t change because of such a scale-down operation.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Typically you will see applications use the Deployment object because it allows you to manage the release of new versions. <a data-primary="deployments" data-secondary="ReplicaSets powering Deployment objects" data-type="indexterm" id="idm45664077236416"/>Repli⁠ca​Sets power Deployments under the hood, and it’s important to understand how they operate so that you can debug them should you need to troubleshoot.</p>
</div>
</div></section>
<section data-pdf-bookmark="ReplicaSet Spec" data-type="sect1"><div class="sect1" id="idm45664077234960">
<h1>ReplicaSet Spec</h1>
<p>Like all objects in Kubernetes, ReplicaSets are defined using a specification.<a data-primary="ReplicaSets" data-secondary="specification" data-type="indexterm" id="idm45664077233088"/>  All <span class="keep-together">ReplicaSets</span> must have a unique name (defined using the <code>metadata.name</code> field), a <code>spec</code> section that describes the number of Pods (replicas) that should be running cluster-wide at any given time, and a Pod template that describes the Pod to be created when the defined number of replicas is not met. <a data-type="xref" href="#minimal_ReplicaSet">Example 9-1</a> shows a minimal ReplicaSet definition. Pay attention to the replicas, selector, and template sections of the definition because they provide more insight into how ReplicaSets operate.</p>
<div data-type="example" id="minimal_ReplicaSet">
<h5><span class="label">Example 9-1. </span>kuard-rs.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ReplicaSet</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kuard</code><code class="w"/>
<code class="w">    </code><code class="nt">version</code><code class="p">:</code><code class="w"> </code><code class="s">"2"</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kuard</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w"/>
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kuard</code><code class="w"/>
<code class="w">      </code><code class="nt">version</code><code class="p">:</code><code class="w"> </code><code class="s">"2"</code><code class="w"/>
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kuard</code><code class="w"/>
<code class="w">        </code><code class="nt">version</code><code class="p">:</code><code class="w"> </code><code class="s">"2"</code><code class="w"/>
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kuard</code><code class="w"/>
<code class="w">          </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="s">"gcr.io/kuar-demo/kuard-amd64:green"</code><code class="w"/></pre></div>
<section data-pdf-bookmark="Pod Templates" data-type="sect2"><div class="sect2" id="idm45664077201696">
<h2>Pod Templates</h2>
<p>As mentioned previously, when the number of <a data-primary="Pods" data-secondary="templates in ReplicaSet specification" data-type="indexterm" id="idm45664077200560"/>Pods in the current state is less than the number of Pods in the desired state, the ReplicaSet controller will create new Pods  using a template contained in the ReplicaSet specification.  The Pods are created in exactly the same manner as when you created a Pod from a YAML file in previous chapters, but instead of using a file, the Kubernetes ReplicaSet controller creates and submits a Pod manifest based on the Pod template directly to the API server. Here is an example of a Pod template in a ReplicaSet:</p>
<pre data-type="programlisting">template:
  metadata:
    labels:
      app: helloworld
      version: v1
  spec:
    containers:
      - name: helloworld
        image: kelseyhightower/helloworld:v1
        ports:
          - containerPort: 80</pre>
</div></section>
<section data-pdf-bookmark="Labels" data-type="sect2"><div class="sect2" id="idm45664077097872">
<h2>Labels</h2>
<p>In any reasonably sized cluster, many different Pods are running simultaneously—so how does the ReplicaSet reconciliation loop discover the set of Pods for a particular ReplicaSet?<a data-primary="Pods" data-secondary="discovering for particular ReplicaSet using labels" data-type="indexterm" id="idm45664077096352"/><a data-primary="labels" data-secondary="filtering labels in ReplicaSet specifications" data-type="indexterm" id="idm45664077095408"/> ReplicaSets monitor cluster state using a set of Pod labels to filter Pod listings and track Pods running within a cluster. When initially created, a ReplicaSet fetches a Pod listing from the Kubernetes API and filters the results by labels. Based on the number of Pods the query returns, the ReplicaSet deletes or creates Pods to meet the desired number of replicas. These filtering labels are defined in the ReplicaSet <code>spec</code> section and are the key to understanding how ReplicaSets work.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The selector in the ReplicaSet <code>spec</code> should be a proper subset of the labels in the Pod template.</p>
</div>
</div></section>
</div></section>
<section data-pdf-bookmark="Creating a ReplicaSet" data-type="sect1"><div class="sect1" id="idm45664077091744">
<h1>Creating a ReplicaSet</h1>
<p>ReplicaSets are created by submitting a ReplicaSet object to the Kubernetes API.<a data-primary="ReplicaSets" data-secondary="creating" data-type="indexterm" id="idm45664077090176"/>  In this section, we will create a ReplicaSet using a configuration file and the <code>kubectl apply</code> command.</p>
<p>The ReplicaSet configuration file in <a data-type="xref" href="#minimal_ReplicaSet">Example 9-1</a> will ensure one copy of the <code>gcr.io/kuar-demo/kuard-amd64:green</code> container is running at any given time. Use the <code>kubectl apply</code> command to <a data-primary="kubectl tool" data-secondary="commands" data-tertiary="apply -f" data-type="indexterm" id="idm45664077086416"/>submit the <code>kuard</code> ReplicaSet to the
Kubernetes API:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f kuard-rs.yaml</strong>
replicaset "kuard" created</pre>
<p>Once the <code>kuard</code> ReplicaSet has been accepted, the ReplicaSet controller will detect that there are no <code>kuard</code> Pods running that match the desired state and create a new <code>kuard</code> Pod based on the contents of the Pod template:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods</strong>
NAME          READY     STATUS    RESTARTS   AGE
kuard-yvzgd   1/1       Running   0          11s</pre>
</div></section>
<section data-pdf-bookmark="Inspecting a ReplicaSet" data-type="sect1"><div class="sect1" id="idm45664077058464">
<h1>Inspecting a ReplicaSet</h1>
<p>As with Pods and <a data-primary="ReplicaSets" data-secondary="inspecting" data-type="indexterm" id="idm45664077056736"/>other Kubernetes API objects, if you are interested in further
details about a ReplicaSet, you can use the <code>describe</code> command to provide much more information about its state.<a data-primary="kubectl tool" data-secondary="commands" data-tertiary="describe" data-type="indexterm" id="idm45664077055088"/> Here is an example of using <code>describe</code> to obtain the details of the <span class="keep-together">ReplicaSet</span> we previously created:</p>
<pre data-type="programlisting">$ <strong>kubectl describe rs kuard</strong>
Name:         kuard
Namespace:    default
Selector:     app=kuard,version=2
Labels:       app=kuard
              version=2
Annotations:  &lt;none&gt;
Replicas:     1 current / 1 desired
Pods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:</pre>
<p>You can see the label selector for the ReplicaSet, as well as the state of all of the replicas it manages.</p>
<section data-pdf-bookmark="Finding a ReplicaSet from a Pod" data-type="sect2"><div class="sect2" id="idm45664077050688">
<h2>Finding a ReplicaSet from a Pod</h2>
<p>Sometimes you may wonder if a Pod is being managed by a ReplicaSet, and if it is, which one.<a data-primary="Pods" data-secondary="finding a ReplicaSet from a Pod" data-type="indexterm" id="idm45664077049280"/> To enable this kind of discovery, the ReplicaSet controller adds an <code>ownerReferences</code> section
to every Pod that it creates. <a data-primary="ownerReferences section (ReplicaSet controller)" data-type="indexterm" id="idm45664077047728"/>If you run the following, look for the <code>ownerReferences</code> section:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods &lt;<em>pod-name</em>&gt; -o=jsonpath='{.metadata.ownerReferences[0].name}'</strong></pre>
<p>If applicable, this will list the name of the ReplicaSet that is managing this Pod.</p>
</div></section>
<section data-pdf-bookmark="Finding a Set of Pods for a ReplicaSet" data-type="sect2"><div class="sect2" id="idm45664077044240">
<h2>Finding a Set of Pods for a ReplicaSet</h2>
<p>You can also determine the set of Pods managed by a ReplicaSet. <a data-primary="Pods" data-secondary="finding Pods managed by a ReplicaSet" data-type="indexterm" id="idm45664077042672"/><a data-primary="kubectl tool" data-secondary="commands" data-tertiary="get pods" data-type="indexterm" id="idm45664077041680"/><a data-primary="labels" data-secondary="label selectors" data-tertiary="finding Pods that match" data-type="indexterm" id="idm45664077040464"/>First, get the set of labels using the <code>kubectl describe</code> command. In the previous example, the label selector was <code>app=kuard,version=2</code>. To find the Pods that match this selector, use the <code>--selector</code> flag or the shorthand <code>-l</code>:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods -l app=kuard,version=2</strong></pre>
<p>This is exactly the same query that the ReplicaSet executes to determine the current number of Pods.<a data-primary="ReplicaSets" data-secondary="scaling" data-type="indexterm" id="ix_ResSsca"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Scaling ReplicaSets" data-type="sect1"><div class="sect1" id="idm45664077034480">
<h1>Scaling ReplicaSets</h1>
<p>You can scale ReplicaSets up or down by updating the <code>spec.replicas</code> key on the <span class="keep-together">ReplicaSet</span> object stored in Kubernetes. <a data-primary="scaling" data-secondary="of ReplicaSets" data-secondary-sortas="ReplicaSets" data-type="indexterm" id="ix_scaleRS"/>When you scale up a ReplicaSet, it submits new Pods to the Kubernetes API using the Pod template defined on the ReplicaSet.</p>
<section data-pdf-bookmark="Imperative Scaling with kubectl scale" data-type="sect2"><div class="sect2" id="idm45664077030016">
<h2>Imperative Scaling with kubectl scale</h2>
<p>The easiest way to achieve this is using the <code>scale</code> command in <code>kubectl</code>. <a data-primary="kubectl tool" data-secondary="commands" data-tertiary="scale" data-type="indexterm" id="idm45664077027600"/><a data-primary="scaling" data-secondary="of ReplicaSets" data-secondary-sortas="ReplicaSets" data-tertiary="imperatively scaling with kubectl scale" data-type="indexterm" id="idm45664077026320"/>For example, to scale up to four replicas, you could run:</p>
<pre data-type="programlisting">$ <strong>kubectl scale replicasets kuard --replicas=4</strong></pre>
<p>While such imperative commands are useful for demonstrations and quick
reactions to emergency situations (such as a sudden increase in load), it is important to also update any text file configurations to match the number of replicas that you set via the imperative <code>scale</code> command.  The reason for this becomes obvious when you consider the following scenario.</p>
<p>Alice is on call, when suddenly there is a large increase in load on the service she is managing. Alice uses the <code>scale</code> command to increase the number of servers responding to requests to 10, and the situation is resolved. However, Alice forgets to update the ReplicaSet configurations checked into source control.</p>
<p>Several days later, Bob is preparing the weekly rollouts. Bob edits the ReplicaSet configurations stored in version control to use the new container image, but he doesn’t notice that the number of replicas in the file is currently 5, not the 10 that Alice set in response to the increased load.  Bob proceeds with the rollout, which both updates the container image and reduces the number of replicas by half. This causes an immediate overload, which leads to an outage.</p>
<p>This fictional case study illustrates the need to ensure that any imperative changes are immediately followed by a declarative change in source control. Indeed, if the need is not acute, we generally recommend only making declarative changes as described in the following section.</p>
</div></section>
<section data-pdf-bookmark="Declaratively Scaling with kubectl apply" data-type="sect2"><div class="sect2" id="idm45664077019680">
<h2>Declaratively Scaling with kubectl apply</h2>
<p>In a declarative world, you make changes by editing the configuration file in
version control and then applying those changes to your cluster.<a data-primary="scaling" data-secondary="of ReplicaSets" data-secondary-sortas="ReplicaSets" data-tertiary="declaratively scaling with kubectl apply" data-type="indexterm" id="idm45664077017760"/><a data-primary="kubectl tool" data-secondary="commands" data-tertiary="apply -f" data-type="indexterm" id="idm45664077016144"/> To scale the <code>kuard</code> ReplicaSet, edit the <em>kuard-rs.yaml</em> configuration file and set the <code>replicas</code> count to <code>3</code>:</p>
<pre class="pagebreak-before less_space" data-type="programlisting">...
spec:
  replicas: 3
...</pre>
<p>In a multiuser setting, you would likely have a documented code review of this change and eventually check the changes into version control.  Either way, you can then use the <code>kubectl apply</code> command to submit the updated <code>kuard</code> ReplicaSet to the API server:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f kuard-rs.yaml</strong>
replicaset "kuard" configured</pre>
<p>Now that the updated <code>kuard</code> ReplicaSet is in place, the ReplicaSet controller will detect that the number of desired Pods has changed and that it needs to take action to realize that desired state. If you used the imperative <code>scale</code> command in the previous section, the ReplicaSet controller will destroy one Pod to get the number to three. Otherwise, it will submit two new Pods to the Kubernetes API using the Pod template defined on the <code>kuard</code> ReplicaSet. Regardless, use the <code>kubectl get pods</code> command to list the running <code>kuard</code> Pods.<a data-primary="kubectl tool" data-secondary="commands" data-tertiary="get pods" data-type="indexterm" id="idm45664077006752"/> You should see output similar to the following with three Pods in running state; two will have a smaller age because they were recently started:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods</strong>
NAME          READY     STATUS    RESTARTS   AGE
kuard-3a2sb   1/1       Running   0          26s
kuard-wuq9v   1/1       Running   0          26s
kuard-yvzgd   1/1       Running   0          2m</pre>
</div></section>
<section data-pdf-bookmark="Autoscaling a ReplicaSet" data-type="sect2"><div class="sect2" id="autoscaling">
<h2>Autoscaling a ReplicaSet</h2>
<p>While there will be times when you want to have explicit control over the number of replicas in a ReplicaSet, often you simply want to have “enough” replicas.<a data-primary="scaling" data-secondary="of ReplicaSets" data-secondary-sortas="ReplicaSets" data-tertiary="autoscaling" data-type="indexterm" id="idm45664077002208"/><a data-primary="autoscaling" data-secondary="ReplicaSets" data-type="indexterm" id="idm45664077000688"/> The definition varies depending on the needs of the containers in the ReplicaSet. For example, with a web server like NGINX, you might want to scale due to CPU usage.  <a data-primary="HPA (Horizontal Pod Autoscaling)" data-type="indexterm" id="idm45664076999424"/>For an in-memory cache, you might want to scale with memory consumption. In some cases, you might want to scale in response to custom application metrics. Kubernetes can handle all of these scenarios via <em>Horizontal Pod Autoscaling</em> (HPA).</p>
<p>“Horizontal Pod Autoscaling” is kind of a mouthful, and you might wonder why it is not simply called “autoscaling.”  Kubernetes makes a distinction between <em>horizontal</em> scaling, which involves creating additional replicas of a Pod, and <em>vertical</em> scaling, which involves increasing the resources required for a particular Pod (such as increasing the CPU required for the Pod). <a data-primary="clusters" data-secondary="autoscaling" data-type="indexterm" id="idm45664076996784"/> Many solutions also enable <em>cluster</em> autoscaling, where the number of machines in the cluster is scaled in response to resource needs, but that solution is outside the scope of this chapter.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Autoscaling requires the presence of the <code>metrics-server</code> in
your cluster.<a data-primary="metrics-server, required for autoscaling" data-type="indexterm" id="idm45664076993472"/><a data-primary="autoscaling" data-secondary="metrics-server required for" data-type="indexterm" id="idm45664076992640"/> The <code>metrics-server</code> keeps track of metrics and provides an API for consuming metrics that HPA uses when making scaling decisions. Most installations of Kubernetes include <code>metrics-server</code> by default. <a data-primary="kube-system namespace" data-secondary="listing Pods in" data-type="indexterm" id="idm45664076990672"/>You can validate its presence by listing the Pods in the <code>kube-system</code> namespace:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods --namespace=kube-system</strong></pre>
<p>You should see a Pod with a name that starts with <code>metrics-server</code> somewhere in that list.  If you do not see it, autoscaling will not work correctly.</p>
</div>
<p>Scaling based on CPU usage is the most common use case for Pod autoscaling.
You can also scale based on memory usage.<a data-primary="CPUs" data-secondary="autoscaling based on CPU usage" data-type="indexterm" id="idm45664076986496"/> CPU-based autoscaling  is most useful for request-based systems that consume CPU proportionally to the number of requests they are receiving, while using a relatively static amount of memory.<a data-primary="kubectl tool" data-secondary="commands" data-tertiary="autoscale rs" data-type="indexterm" id="idm45664076985376"/></p>
<p>To scale a ReplicaSet, you can run a command like the following:</p>
<pre data-type="programlisting">$ <strong>kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80</strong></pre>
<p>This command creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%.  To view, modify, or delete this resource, you can use the standard <code>kubectl</code> commands and the <code>horizontalpodautoscalers</code> resource. It is quite a bit to type <code>horizontalpodautoscalers</code>, but it can be shortened to <code>hpa</code>:</p>
<pre data-type="programlisting">$ <strong>kubectl get hpa</strong></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Because of the decoupled nature of Kubernetes, there is no direct link between the HPA and the ReplicaSet. <a data-primary="decoupled architectures" data-secondary="no direct link between HPA and ReplicaSet" data-type="indexterm" id="idm45664076978080"/><a data-primary="HPA (Horizontal Pod Autoscaling)" data-secondary="no direct link between HPA and ReplicaSet" data-type="indexterm" id="idm45664076977008"/>While this is great for modularity and composition, it also enables some antipatterns. In particular, it’s a bad idea to combine autoscaling with imperative or declarative management of the number of replicas.  If both you and an autoscaler are attempting to modify the number of replicas, it’s highly likely that you will clash, resulting in unexpected behavior.<a data-primary="scaling" data-secondary="of ReplicaSets" data-secondary-sortas="ReplicaSets" data-startref="ix_scaleRS" data-type="indexterm" id="idm45664076975840"/><a data-primary="ReplicaSets" data-secondary="scaling" data-startref="ix_ResSsca" data-type="indexterm" id="idm45664076974352"/></p>
</div>
</div></section>
</div></section>
<section data-pdf-bookmark="Deleting ReplicaSets" data-type="sect1"><div class="sect1" id="idm45664076972880">
<h1>Deleting ReplicaSets</h1>
<p>When a ReplicaSet is no longer required, it can be deleted using the <code>kubectl
delete</code> command. <a data-primary="ReplicaSets" data-secondary="deleting" data-type="indexterm" id="idm45664076970768"/><a data-primary="kubectl tool" data-secondary="commands" data-tertiary="delete" data-type="indexterm" id="idm45664076969760"/><a data-primary="Pods" data-secondary="managed by a ReplicaSet, deleting" data-type="indexterm" id="idm45664076968544"/>By default, this also deletes the Pods that are managed by the ReplicaSet:</p>
<pre data-type="programlisting">$ <strong>kubectl delete rs kuard</strong>
replicaset "kuard" deleted</pre>
<p>Running the <code>kubectl get pods</code> command shows <a data-primary="kubectl tool" data-secondary="commands" data-tertiary="get pods" data-type="indexterm" id="idm45664076965536"/>that all the <code>kuard</code> Pods created by the <code>kuard</code> ReplicaSet have also been deleted:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods</strong></pre>
<p>If you don’t want to delete the Pods that the ReplicaSet is managing, you can set the <code>--cascade</code> flag to <code>false</code> to ensure only the ReplicaSet object is deleted and not the Pods:</p>
<pre data-type="programlisting">$ <strong>kubectl delete rs kuard --cascade=false</strong></pre>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45664076959552">
<h1>Summary</h1>
<p>Composing Pods with ReplicaSets provides the foundation for building robust applications with automatic failover, and makes deploying those applications a breeze by enabling scalable and sane deployment patterns. Use ReplicaSets for any Pod you care about, even if it is a single Pod! Some people even default to using ReplicaSets instead of Pods. A typical cluster will have many ReplicaSets, so apply them liberally to the affected area.<a data-primary="ReplicaSets" data-startref="ix_RepS" data-type="indexterm" id="idm45664076958288"/></p>
</div></section>
</div></section></div></body></html>