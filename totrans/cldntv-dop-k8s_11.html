<html><head></head><body><section data-pdf-bookmark="Chapter 9. Managing Pods" data-type="chapter" epub:type="chapter"><div class="chapter" id="managing">&#13;
<h1><span class="label">Chapter 9. </span>Managing Pods</h1>&#13;
&#13;
<blockquote class="epigraph">&#13;
<p>There are no big problems, there are just a lot of little problems.</p>&#13;
<p data-type="attribution">Henry Ford</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="Kubernetes" data-secondary="Pod management" data-type="indexterm" id="ix_09-pods-adoc0"/><a data-primary="Pod objects" data-secondary="Pod management" data-type="indexterm" id="ix_09-pods-adoc1"/>In the previous chapter, we covered containers in some detail and explained how in Kubernetes, containers are composed together to form Pods. There are a few other interesting aspects of Pods, which we’ll turn to in this chapter, including labels, guiding Pod scheduling using node affinities, barring Pods from running on certain nodes with taints and tolerations, keeping Pods together or apart using Pod affinities, and orchestrating applications using Pod controllers such as DaemonSets and StatefulSets. We’ll also cover some advanced networking features including Ingress controllers and service mesh tools.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Labels" data-type="sect1"><div class="sect1" id="labels">&#13;
<h1>Labels</h1>&#13;
&#13;
<p><a data-primary="labels" data-secondary="working with" data-type="indexterm" id="ix_09-pods-adoc2"/>You know that Pods (and other Kubernetes resources) can have labels attached to them, and that these play an important role in connecting related resources (for example, sending requests from a Service to the appropriate backends). Let’s take a closer look at labels and selectors in this section.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Are Labels?" data-type="sect2"><div class="sect2" id="idm45979383685536">&#13;
<h2>What Are Labels?</h2>&#13;
<blockquote data-type="quote">&#13;
<p>Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system.</p>&#13;
<p data-type="attribution">The Kubernetes <a href="https://oreil.ly/C4j1y">documentation</a></p>&#13;
</blockquote>&#13;
&#13;
<p>In other words, labels exist to tag resources with information that’s meaningful to us, but they don’t mean anything to Kubernetes. For example, it’s common to label Pods with the application they belong to:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><strong><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code></strong></pre>&#13;
&#13;
<p>Now, by itself, this label has no effect. It’s still useful as documentation: someone can look at this Pod and see what application it’s running. But the real power of a label comes when we use it with a <em>selector</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Selectors" data-type="sect2"><div class="sect2" id="selectors">&#13;
<h2>Selectors</h2>&#13;
&#13;
<p><a data-primary="selectors" data-type="indexterm" id="ix_09-pods-adoc3"/>A selector is an expression that matches a label (or set of labels). It’s a way of specifying a group of resources by their labels. For example, a Service resource has a selector that identifies the Pods it will send requests to. Remember our demo Service from <a data-type="xref" href="ch04.html#services">“Service Resources”</a>?</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w">&#13;
</code><code class="nn">...</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="l-Scalar-Plain">...</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="l-Scalar-Plain">selector</code><code class="p-Indicator">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code></strong></pre>&#13;
&#13;
<p>This is a very simple selector that matches any resource that has the <code>app</code> label with a value of <code>demo</code>. If a resource doesn’t have the <code>app</code> label at all, it won’t match this selector. If it has the <code>app</code> label, but its value is not <code>demo</code>, it won’t match the selector either. Only suitable resources (in this case, Pods) with the label <code>app: demo</code> will match, and all such resources will be selected by this Service.</p>&#13;
&#13;
<p><a data-primary="kubectl" data-secondary="flags" data-tertiary="--selector" data-type="indexterm" id="idm45979383608240"/>Labels aren’t just used for connecting Services and Pods; you can use them directly when querying the cluster with <code>kubectl get</code>, using the <code>--selector</code> flag:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods --all-namespaces --selector app=demo</code></strong><code class="go">&#13;
</code><code class="go">NAMESPACE   NAME                    READY     STATUS    RESTARTS   AGE&#13;
</code><code class="go">demo        demo-5cb7d6bfdd-9dckm   1/1       Running   0          20s</code></pre>&#13;
&#13;
<p>You may recall from <a data-type="xref" href="ch07.html#shortflags">“Using Short Flags”</a> that <code>--selector</code> can be abbreviated to just <code>-l</code> (for <em><strong>l</strong>abels</em>).</p>&#13;
&#13;
<p><a data-primary="kubectl" data-secondary="flags" data-tertiary="--show-labels" data-type="indexterm" id="idm45979383570592"/>If you want to see what labels are defined on your Pods, use the <code>--show-labels</code> flag to <code>kubectl get</code>:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods --show-labels</code></strong><code class="go">&#13;
</code><code class="go">NAME                    ... LABELS&#13;
</code><code class="go">demo-5cb7d6bfdd-9dckm   ... app=demo,environment=development</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="More Advanced Selectors" data-type="sect2"><div class="sect2" id="idm45979383661280">&#13;
<h2>More Advanced Selectors</h2>&#13;
&#13;
<p>Most of the time, a simple selector like <code>app: demo</code> (known as an <em>equality selector</em>) will be all you need. You can combine different labels to make more specific selectors:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods -l app=demo,environment=production</code></strong></pre>&#13;
&#13;
<p>This will return only Pods that have <em>both</em> <code>app: demo</code> and <code>environment: production</code> labels. The YAML equivalent of this (in a Service, for example) would be:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">environment</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">production</code></strong></pre>&#13;
&#13;
<p>Equality selectors like this are the only kind available with a Service, but for interactive queries with <code>kubectl</code>, or more sophisticated resources such as Deployments, there are other options.</p>&#13;
&#13;
<p>One is selecting for label <em>inequality</em>:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods -l app!=demo</code></strong></pre>&#13;
&#13;
<p>This will return all Pods that have an <code>app</code> label with a different value to <code>demo</code>, or that don’t have an <code>app</code> label at all.</p>&#13;
&#13;
<p>You can also ask for label values that are in a <em>set</em>:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong>kubectl get pods -l <code><code><em>environment in (staging, production)</em></code></code></strong></pre>&#13;
&#13;
<p>The YAML equivalent would be:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="p-Indicator">{</code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="nv">environment</code><code class="p-Indicator">,</code><code class="nt"> operator</code><code class="p">:</code><code class="w"> </code><code class="nv">In</code><code class="p-Indicator">,</code><code class="nt"> values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="nv">staging</code><code class="p-Indicator">,</code><code class="w"> </code><code class="nv">production</code><code class="p-Indicator">]</code><code class="p-Indicator">}</code></strong></pre>&#13;
&#13;
<p>You can also ask for label values not in a given set:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong>kubectl get pods -l <code><code><em>environment notin (production)</em></code></code></strong></pre>&#13;
&#13;
<p>The YAML equivalent of this would be:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="p-Indicator">{</code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="nv">environment</code><code class="p-Indicator">,</code><code class="nt"> operator</code><code class="p">:</code><code class="w"> </code><code class="nv">NotIn</code><code class="p-Indicator">,</code><code class="nt"> values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="nv">production</code><code class="p-Indicator">]</code><code class="p-Indicator">}</code></strong></pre>&#13;
&#13;
<p>You can see another example of using <code>matchExpressions</code> in <a data-type="xref" href="ch05.html#nodeaffinities-intro">“Using node affinities to control scheduling”</a>.<a data-startref="ix_09-pods-adoc3" data-type="indexterm" id="idm45979383364880"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other Uses for Labels" data-type="sect2"><div class="sect2" id="idm45979383547072">&#13;
<h2>Other Uses for Labels</h2>&#13;
&#13;
<p>We’ve seen how to link Pods to Services using an <code>app</code> label (actually, you can use any label, but <code>app</code> is common). But what other uses are there for labels?</p>&#13;
&#13;
<p>In our Helm chart for the demo application (see <a data-type="xref" href="ch12.html#helmcharts">“What’s Inside a Helm Chart?”</a>), we set an <code>environment</code> label, which can be, for example, <code>staging</code> or <code>production</code>. If you’re running staging and production Pods in the same cluster (see <a data-type="xref" href="ch06.html#multiclusters">“Do I need multiple clusters?”</a>), you might want to use a label like this to distinguish between the two environments. For example, your Service selector for production might be:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">environment</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">production</code><code class="w"/></pre>&#13;
&#13;
<p>Without the extra <code>environment</code> selector, the Service would match any and all Pods with <code>app: demo</code>, including the staging ones, which you probably don’t want.</p>&#13;
&#13;
<p>Depending on your applications, you might want to use labels to slice and dice your resources in a number of different ways. Here are some examples:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">tier</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">frontend</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">environment</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">production</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">version</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1.12.0</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">role</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">primary</code><code class="w"/></pre>&#13;
&#13;
<p>This allows you to query the cluster along these various different dimensions to see what’s going on.</p>&#13;
&#13;
<p>You could also use labels as a way of doing canary deployments (see <a data-type="xref" href="ch13.html#canary">“Canary Deployments”</a>). If you want to roll out a new version of the application to just a small percentage of Pods, you could use labels like <code>track: stable</code> and <code>track: canary</code> for two separate Deployments.</p>&#13;
&#13;
<p>If your Service’s selector matches only the <code>app</code> label, it will send traffic to all Pods matching that selector, including both <code>stable</code> and <code>canary</code>. You can alter the number of replicas for both Deployments to gradually increase the proportion of <code>canary</code> Pods. Once all running Pods are on the canary track, you can relabel them as <code>stable</code> and begin the process again with the next version.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Labels and Annotations" data-type="sect2"><div class="sect2" id="labelsandannotations">&#13;
<h2>Labels and Annotations</h2>&#13;
&#13;
<p><a data-primary="annotations" data-secondary="labels versus" data-type="indexterm" id="idm45979383266880"/><a data-primary="labels" data-secondary="annotations versus" data-type="indexterm" id="idm45979383265904"/>You might be wondering what the difference is between labels and annotations. They’re both sets of key-value pairs that provide metadata about resources.</p>&#13;
&#13;
<p>The difference is that <em>labels identify resources</em>. They’re used to select groups of related resources, like in a Service’s selector. Annotations, on the other hand, are for non-identifying information, to be used by tools or services outside Kubernetes. For example, in <a data-type="xref" href="ch13.html#helmhooks">“Helm Hooks”</a> there’s an example of using annotations to control Helm workflows.</p>&#13;
&#13;
<p>Because labels are often used in internal queries that are performance-critical to Kubernetes, there are some fairly tight restrictions on valid labels. For example, label names are limited to 63 characters, though they may have an optional 253-character prefix in the form of a DNS subdomain, separated from the label by a slash character. Labels can only begin with an alphanumeric character (a letter or a digit), and can only contain alphanumeric characters plus dashes, underscores, and dots. Label values are <a href="https://oreil.ly/pR82Y">similarly restricted</a>.</p>&#13;
&#13;
<p>In practice, we doubt you’ll run out of characters for your labels, since most labels in common use are just a single word (for example, <code>app</code>).<a data-startref="ix_09-pods-adoc2" data-type="indexterm" id="idm45979383218304"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Node Affinities" data-type="sect1"><div class="sect1" id="nodeaffinities">&#13;
<h1>Node Affinities</h1>&#13;
&#13;
<p><a data-primary="node affinities" data-type="indexterm" id="ix_09-pods-adoc4"/>We mentioned node affinities briefly in <a data-type="xref" href="ch05.html#nodeaffinities-intro">“Using node affinities to control scheduling”</a>, in relation to preemptible nodes. In that section, you learned how to use node affinities to preferentially schedule Pods on certain nodes (or not). Let’s take a more detailed look at node affinities now.</p>&#13;
&#13;
<p>In most cases, you don’t need node affinities. Kubernetes is pretty smart about scheduling Pods onto the right nodes. If all your nodes are equally suitable to run a given Pod, then don’t worry about it.</p>&#13;
&#13;
<p>There are exceptions, however (like preemptible nodes in the previous example). If a Pod is expensive to restart, you probably want to avoid scheduling it on a preemptible node wherever possible; preemptible nodes can disappear from the cluster without warning. You can express this kind of preference using node affinities.</p>&#13;
&#13;
<p><a data-primary="preferredDuringSchedulingIgnoredDuringExecution" data-type="indexterm" id="idm45979383212480"/><a data-primary="requiredDuringSchedulingIgnoredDuringExecution" data-type="indexterm" id="idm45979383211808"/>There are two types of affinity: hard and soft, and in Kubernetes these are called:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>requiredDuringSchedulingIgnoredDuringExecution</code> (hard)</p>&#13;
</li>&#13;
<li>&#13;
<p><code>preferredDuringSchedulingIgnoredDuringExecution</code> (soft)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>It may help you to remember that <code>required</code> means a hard affinity (the rule <em>must</em> be satisfied to schedule this Pod) and <code>preferred</code> means a soft affinity (it would be <em>nice</em> if the rule were satisfied, but it’s not critical).</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>The long names of the hard and soft affinity types make the point that these rules apply <em>during scheduling</em>, but not <em>during execution</em>. That is, once the Pod has been scheduled to a particular node satisfying the affinity, it will stay there. If things change while the Pod is running so that the rule is no longer satisfied, Kubernetes won’t move the Pod.</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hard Affinities" data-type="sect2"><div class="sect2" id="idm45979383203616">&#13;
<h2>Hard Affinities</h2>&#13;
&#13;
<p>An affinity is expressed by describing the kind of nodes that you want the Pod to run on. There might be several rules about how you want Kubernetes to select nodes for the Pod. <a data-primary="nodeSelectorTerms" data-type="indexterm" id="idm45979383202288"/>Each one is expressed using the <code>nodeSelectorTerms</code> field. Here’s an example:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>&#13;
<code class="nn">...</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">affinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">nodeAffinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">requiredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">nodeSelectorTerms</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="s">"topology.kubernetes.io/zone"</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">In</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"us-central1-a"</code><code class="p-Indicator">]</code><code class="w"/></pre>&#13;
&#13;
<p>Only nodes that are in the <code>us-central1-a</code> zone will match this rule, so the overall effect is to ensure that this Pod is only scheduled in that zone.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Soft Affinities" data-type="sect2"><div class="sect2" id="idm45979383190496">&#13;
<h2>Soft Affinities</h2>&#13;
&#13;
<p><a data-primary="weights" data-secondary="soft affinities" data-type="indexterm" id="idm45979383176992"/>Soft affinities are expressed in much the same way, except that each rule is assigned a numerical <em>weight</em> from 1 to 100 that determines the effect it has on the result. Here’s an example:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">preferredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w">&#13;
</code><code class="p-Indicator">-</code><code class="w"> </code><strong><code class="nt">weight</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10</code></strong><code class="w">&#13;
</code><code class="w">  </code><code class="nt">preference</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">topology.kubernetes.io/zone</code><code class="s">"</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">In</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"</code><code class="s">us-central1-a</code><code class="s">"</code><code class="p-Indicator">]</code><code class="w">&#13;
</code><code class="p-Indicator">-</code><code class="w"> </code><strong><code class="nt">weight</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">100</code></strong><code class="w">&#13;
</code><code class="w">  </code><code class="nt">preference</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">topology.kubernetes.io/zone</code><code class="s">"</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">In</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"</code><code class="s">us-central1-b</code><code class="s">"</code><code class="p-Indicator">]</code></pre>&#13;
&#13;
<p>Because this is a <code>preferred...</code> rule, it’s a soft affinity: Kubernetes can schedule the Pod on any node, but it will give priority to nodes that match these rules.</p>&#13;
&#13;
<p><a data-primary="weights" data-secondary="purpose of" data-type="indexterm" id="idm45979383021136"/>You can see that the two rules have different <code>weight</code> values. The first rule has weight 10, but the second has weight 100. If there are nodes that match both rules, Kubernetes will give 10 times the priority to nodes that match the second rule (being in availability zone <code>us-central1-b</code>).</p>&#13;
&#13;
<p>Weights are a useful way of expressing the relative importance of your preferences.<a data-startref="ix_09-pods-adoc4" data-type="indexterm" id="idm45979383035728"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Affinities and Anti-Affinities" data-type="sect1"><div class="sect1" id="idm45979383034768">&#13;
<h1>Pod Affinities and Anti-Affinities</h1>&#13;
&#13;
<p><a data-primary="Pod objects" data-secondary="affinity" data-type="indexterm" id="ix_09-pods-adoc5"/>We’ve seen how you can use node affinities to nudge the scheduler toward or away from running a Pod on certain kinds of nodes. But is it possible to influence scheduling decisions based on what other Pods are already running on a node?</p>&#13;
&#13;
<p>Sometimes there are pairs of Pods that work better when they’re together on the same node; for example, a web server and a content cache, such as Redis. It would be useful if you could add information to the Pod spec that tells the scheduler it would prefer to be colocated with a Pod matching a particular set of labels.</p>&#13;
&#13;
<p>Conversely, sometimes you want Pods to avoid each other. In <a data-type="xref" href="ch05.html#balanced">“Keeping Your Workloads Balanced”</a>, we saw the kind of problems that can arise if Pod replicas end up together on the same node, instead of distributed across the cluster. Can you tell the scheduler to avoid scheduling a Pod where another replica of that Pod is already running?</p>&#13;
&#13;
<p>That’s exactly what you can do with Pod affinities. Like node affinities, Pod affinities are expressed as a set of rules: either hard requirements, or soft preferences with a set of weights.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Keeping Pods Together" data-type="sect2"><div class="sect2" id="idm45979383029360">&#13;
<h2>Keeping Pods Together</h2>&#13;
&#13;
<p>Let’s take the first case first: scheduling Pods together. Suppose you have one Pod, labeled <code>app: server</code>, which is your web server, and another, labeled <code>app: cache</code>, which is your content cache. They can still work together even if they’re on separate nodes, but it’s better if they’re on the same node because they can communicate without having to go over the network. How do you ask the scheduler to colocate them?</p>&#13;
&#13;
<p>Here’s an example of the required Pod affinity, expressed as part of the <code>server</code> Pod spec. The effect would be just the same if you added it to the <code>cache</code> spec, or to both Pods:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">server</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">server</code><code class="w">&#13;
</code><code class="nn">...</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">affinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><strong><code class="nt">podAffinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">requiredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">labelSelector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">app</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">In</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"</code><code class="s">cache</code><code class="s">"</code><code class="p-Indicator">]</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">topologyKey</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubernetes.io/hostname</code></strong></pre>&#13;
&#13;
<p>The overall effect of this affinity is to ensure that the <code>server</code> Pod is scheduled, if possible, on a node that is also running a Pod labeled <code>cache</code>. If there is no such node, or if there is no matching node that has sufficient spare resources to run the Pod, it will not be able to run.</p>&#13;
&#13;
<p>This probably isn’t the behavior you want in a real-life situation. If the two Pods absolutely must be colocated, put their containers in the same Pod. If it’s just preferable for them to be colocated, use a soft Pod affinity (<code>preferredDuringSchedulingIgnoredDuringExecution</code>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Keeping Pods Apart" data-type="sect2"><div class="sect2" id="idm45979382896336">&#13;
<h2>Keeping Pods Apart</h2>&#13;
&#13;
<p><a data-primary="anti-affinities" data-type="indexterm" id="ix_09-pods-adoc6"/>Now let’s take the anti-affinity case: keeping certain Pods apart. Instead of <code>podAffinity</code>, we use <code>podAntiAffinity</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">server</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">server</code><code class="w">&#13;
</code><code class="nn">...</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">affinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><strong><code class="nt">podAntiAffinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">requiredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">labelSelector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">app</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">In</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"</code><code class="s">server</code><code class="s">"</code><code class="p-Indicator">]</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">topologyKey</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubernetes.io/hostname</code></strong></pre>&#13;
&#13;
<p>It’s very similar to the previous example, except that it’s a <code>podAntiAffinity</code>, so it expresses the opposite sense, and the match expression is different. This time, the expression is: “The <code>app</code> label must have the value <code>server</code>.”</p>&#13;
&#13;
<p>The effect of this affinity is to ensure that the Pod will <em>not</em> be scheduled on any node matching this rule. In other words, no Pod labeled <code>app: server</code> can be scheduled on a node that already has an <code>app: server</code> Pod running. This will enforce an even distribution of <code>server</code> Pods across the cluster, at the possible expense of the desired number of replicas.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Soft Anti-Affinities" data-type="sect2"><div class="sect2" id="idm45979382795344">&#13;
<h2>Soft Anti-Affinities</h2>&#13;
&#13;
<p>However, we usually care more about having enough replicas available than distributing them as fairly as possible. A hard rule is not really what we want here. Let’s modify it slightly to make it a soft anti-affinity:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">affinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="nt">podAntiAffinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">preferredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">weight</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">podAffinityTerm</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">labelSelector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">app</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">In</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"</code><code class="s">server</code><code class="s">"</code><code class="p-Indicator">]</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">topologyKey</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubernetes.io/hostname</code></strong></pre>&#13;
&#13;
<p>Notice that now the rule is <code>preferred...</code>, not <code>required...</code>, making it a soft anti-affinity. If the rule can be satisfied, it will be, but if not, Kubernetes will schedule the Pod anyway.</p>&#13;
&#13;
<p><a data-primary="weights" data-secondary="soft anti-affinities" data-type="indexterm" id="idm45979382686672"/>Because it’s a preference, we specify a <code>weight</code> value, just as we did for soft node affinities. If there were multiple affinity rules to consider, Kubernetes would prioritize them according to the weight you assign each rule.<a data-startref="ix_09-pods-adoc6" data-type="indexterm" id="idm45979382699264"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When to Use Pod Affinities" data-type="sect2"><div class="sect2" id="idm45979382698432">&#13;
<h2>When to Use Pod Affinities</h2>&#13;
&#13;
<p>Just as with node affinities, you should treat Pod affinities as a fine-tuning enhancement for special cases. The scheduler is already good at placing Pods to get the best performance and availability from the cluster. Pod affinities restrict the scheduler’s freedom, trading one application for another.<a data-startref="ix_09-pods-adoc5" data-type="indexterm" id="idm45979382696848"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Taints and Tolerations" data-type="sect1"><div class="sect1" id="taints">&#13;
<h1>Taints and Tolerations</h1>&#13;
&#13;
<p>In <a data-type="xref" href="#nodeaffinities">“Node Affinities”</a>, you learned about a property of Pods that can steer them toward (or away from) a set of nodes. <a data-primary="kubectl" data-secondary="commands" data-tertiary="taint" data-type="indexterm" id="idm45979382692944"/><a data-primary="taints" data-type="indexterm" id="idm45979382655584"/>Conversely, <em>taints</em> allow a node to repel a set of Pods, based on certain properties of the node.</p>&#13;
&#13;
<p>For example, you could use taints to set aside particular nodes: nodes that are reserved only for specific kinds of Pods. Kubernetes also creates taints for you if certain problems exist on the node, such as low memory, or a lack of network connectivity.</p>&#13;
&#13;
<p>To add a taint to a particular node, use the <code>kubectl taint</code> command:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl taint nodes docker-for-desktop dedicated=true:NoSchedule</code></strong></pre>&#13;
&#13;
<p><a data-primary="tolerations" data-type="indexterm" id="idm45979382650528"/>This adds a taint called <code>dedicated=true</code> to the <code>docker-for-desktop</code> node, with the effect <code>NoSchedule</code>: no Pod can now be scheduled there unless it has a matching <em>toleration</em>.</p>&#13;
&#13;
<p>To see the taints configured on a particular node, use <code>kubectl describe node...</code>.</p>&#13;
&#13;
<p>To remove a taint from a node, repeat the <code>kubectl taint</code> command but with a trailing minus sign after the name of the taint:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl taint nodes docker-for-desktop dedicated:NoSchedule-</code></strong></pre>&#13;
&#13;
<p>Tolerations are properties of Pods that describe the taints that they’re compatible with. For example, to make a Pod tolerate the <code>dedicated=true</code> taint, add this to the Pod’s spec:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nn">...</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="nt">tolerations</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">dedicated</code><code class="s">"</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">Equal</code><code class="s">"</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">value</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">true</code><code class="s">"</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">effect</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">NoSchedule</code><code class="s">"</code></strong></pre>&#13;
&#13;
<p>This is effectively saying, “This Pod is allowed to run on nodes that have the <span class="keep-together"><code>dedicated=true</code></span> taint with the effect <code>NoSchedule</code>.” Because the toleration <em>matches</em> the taint, the Pod can be scheduled. Any Pod without this toleration will not be allowed to run on the tainted node.</p>&#13;
&#13;
<p>When a Pod can’t run at all because of tainted nodes, it will stay in <code>Pending</code> status, and you’ll see a message like this in the Pod description:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><code class="go">Warning  FailedScheduling  4s (x10 over 2m)  default-scheduler  0/1 nodes are</code>&#13;
<code class="go">available: 1 node(s) had taints that the pod didn't tolerate.</code></pre>&#13;
&#13;
<p>Other uses for taints and tolerations include marking nodes with specialized hardware (such as GPUs), and allowing certain Pods to tolerate certain kinds of node problems.</p>&#13;
&#13;
<p>For example, if a node falls off the network, Kubernetes automatically adds the taint <code>node.kubernetes.io/unreachable</code>. Normally, this would result in its <code>kubelet</code> evicting all Pods from the node. However, you might want to keep certain Pods running, in the hope that the network will come back in a reasonable time. To do this, you could add a toleration to those Pods that matches the <code>unreachable</code> taint.</p>&#13;
&#13;
<p>You can read more about taints and tolerations in the Kubernetes <a href="https://oreil.ly/qbqpz">documentation</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Controllers" data-type="sect1"><div class="sect1" id="idm45979382695296">&#13;
<h1>Pod Controllers</h1>&#13;
&#13;
<p><a data-primary="Pod objects" data-secondary="controllers" data-type="indexterm" id="ix_09-pods-adoc7"/>We’ve talked a lot about Pods in this chapter, and that makes sense: all Kubernetes applications run in a Pod. You might wonder, though, why we need other kinds of objects at all. Isn’t it enough just to create a Pod for an application and run it?</p>&#13;
&#13;
<p>That’s effectively what you get by running a container directly with <span class="keep-together"><code>docker container run</code></span>, as we did in <a data-type="xref" href="ch02.html#runningcontainer">“Running a Container Image”</a>. It works, but it’s very limited:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>If the container exits for some reason, you have to manually restart it.</p>&#13;
</li>&#13;
<li>&#13;
<p>There’s only one replica of your container and no way to load-balance traffic across multiple replicas if you run them manually.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you want highly available replicas, you have to decide which nodes to run them on, and take care of keeping the cluster balanced.</p>&#13;
</li>&#13;
<li>&#13;
<p>When you update the container, you have to take care of stopping each running image in turn, pulling the new image and restarting it.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>That’s the kind of work that Kubernetes is designed to take off your hands using <em>controllers</em>. In <a data-type="xref" href="ch04.html#replicaset-intro">“ReplicaSets”</a>, we introduced the ReplicaSet controller, which manages a group of replicas of a particular Pod. It works continuously to make sure there are always the specified number of replicas, starting new ones if there aren’t enough, and killing off replicas if there are too many.</p>&#13;
&#13;
<p>You’re also now familiar with Deployments, which as we saw in <a data-type="xref" href="ch04.html#deployments-intro">“Deployments”</a>, manage ReplicaSets to control the rollout of application updates. When you update a Deployment—for example, with a new container spec—it creates a new ReplicaSet to start up the new Pods, and eventually closes down the ReplicaSet that was managing the old Pods.</p>&#13;
&#13;
<p>For most simple applications, a Deployment is all you need. But there are a few other useful kinds of Pod controllers, and we’ll look briefly at a few of them in this section.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DaemonSets" data-type="sect2"><div class="sect2" id="idm45979382488960">&#13;
<h2>DaemonSets</h2>&#13;
&#13;
<p><a data-primary="DaemonSet" data-type="indexterm" id="idm45979382487584"/>Suppose you want to send logs from all your applications to a centralized log server, like an Elasticsearch-Logstash-Kibana (ELK) stack, or a SaaS monitoring product such as Datadog (see <a data-type="xref" href="ch16.html#datadog">“Datadog”</a>). There are a few ways to do that.</p>&#13;
&#13;
<p>You could have each application include code to connect to the logging service, authenticate, write logs, and so on, but this results in a lot of duplicated code, which is inefficient.</p>&#13;
&#13;
<p>Alternatively, you could run an extra container in every Pod that acts as a logging agent (this is called a <em>sidecar</em> pattern). This means that the application doesn’t need built-in knowledge of how to talk to the logging service, but it does mean you would potentially have several copies of the same logging agent running on a node.</p>&#13;
&#13;
<p>Since all it does is manage a connection to the logging service and pass on log messages to it, you really only need one copy of the logging agent on each node. This is such a common requirement that Kubernetes provides a special controller object for it: the <em>DaemonSet</em>.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>The term <em>daemon</em> traditionally refers to long-running background processes on a server that handle things like logging, so by analogy, Kubernetes DaemonSets run a <em>daemon</em> container on each node in the cluster.</p>&#13;
</div>&#13;
&#13;
<p>The manifest for a DaemonSet, as you might expect, looks very much like that for a Deployment:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">DaemonSet</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluentd-elasticsearch</code><code class="w"/>&#13;
<code class="w">  </code><code class="l-Scalar-Plain">...</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="l-Scalar-Plain">...</code><code class="w"/>&#13;
<code class="w">  </code><code class="l-Scalar-Plain">template</code><code class="p-Indicator">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="l-Scalar-Plain">...</code><code class="w"/>&#13;
<code class="w">    </code><code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluentd-elasticsearch</code><code class="w"/>&#13;
<code class="w">        </code><code class="l-Scalar-Plain">...</code><code class="w"/></pre>&#13;
&#13;
<p>Use a DaemonSet when you need to run one copy of a Pod on each of the nodes in your cluster. If you’re running an application where maintaining a given number of replicas is more important than exactly which node the Pods run on, use a Deployment instead.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="StatefulSets" data-type="sect2"><div class="sect2" id="statefulsets">&#13;
<h2>StatefulSets</h2>&#13;
&#13;
<p><a data-primary="StatefulSet" data-type="indexterm" id="idm45979382456416"/>Like a Deployment or DaemonSet, a StatefulSet is a kind of Pod controller. What a StatefulSet adds is the ability to start and stop Pods in a specific sequence.</p>&#13;
&#13;
<p>With a Deployment, for example, all your Pods are started and stopped in a random order. This is fine for stateless services, where every replica is identical and does the same job.</p>&#13;
&#13;
<p>Sometimes, though, you need to start Pods in a specific numbered sequence, and be able to identify them by their number. For example, distributed applications such as Redis, MongoDB, or Cassandra create their own clusters, and need to be able to identify the cluster leader by a predictable name.</p>&#13;
&#13;
<p>A StatefulSet is ideal for this. For example, if you create a StatefulSet named <code>redis</code>, the first Pod started will be named <code>redis-0</code>, and Kubernetes will wait until that Pod is ready before starting the next one, <code>redis-1</code>.</p>&#13;
&#13;
<p>Depending on the application, you can use this property to cluster the Pods in a reliable way. For example, each Pod can run a startup script that checks if it is running on <code>redis-0</code>. If it is, it will be the redis cluster leader. If not, it will attempt to join the cluster as a replica by contacting <code>redis-0</code>.</p>&#13;
&#13;
<p>Each replica in a StatefulSet must be running and ready before Kubernetes starts the next one, and similarly when the StatefulSet is terminated, the replicas will be shut down in reverse order, waiting for each Pod to finish before moving on to the next.</p>&#13;
&#13;
<p>Apart from these special properties, a StatefulSet looks very similar to a normal Deployment:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">StatefulSet</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">redis</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">redis</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="s">"redis"</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="l-Scalar-Plain">...</code><code class="w"/></pre>&#13;
&#13;
<p><a data-primary="clusterIP" data-type="indexterm" id="idm45979382323072"/>To be able to address each of the Pods by a predictable DNS name, such as <code>redis-1</code>, <a data-primary="Service objects" data-secondary="headless" data-type="indexterm" id="idm45979382344304"/>you also need to create a Service with a <code>clusterIP</code> type of <code>None</code> (known as a <span class="keep-together"><em>headless service</em></span>).</p>&#13;
&#13;
<p>With a nonheadless Service, you get a single DNS entry (such as <code>redis</code>) that load-balances across all the backend Pods. With a headless service, you still get that single service DNS name, but you also get individual DNS entries for each numbered Pod, like <code>redis-0</code>, <code>redis-1</code>, <code>redis-2</code>, and so on.</p>&#13;
&#13;
<p>Pods that need to join the Redis cluster can contact <code>redis-0</code> specifically, but applications that simply need a load-balanced Redis service can use the <code>redis</code> DNS name to talk to a randomly selected Redis Pod.</p>&#13;
&#13;
<p>StatefulSets can also manage disk storage for their Pods, using a VolumeClaimTemplate object that automatically creates a PersistentVolumeClaim (see <a data-type="xref" href="ch08.html#persistent">“Persistent Volumes”</a>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Jobs" data-type="sect2"><div class="sect2" id="jobs">&#13;
<h2>Jobs</h2>&#13;
&#13;
<p>Another useful type of Pod controller in Kubernetes is the Job. Whereas a Deployment runs a specified number of Pods and restarts them continually, a Job only runs a Pod for a specified number of times. After that, it is considered completed.</p>&#13;
&#13;
<p>For example, a batch-processing task or queue-worker Pod usually starts up, does its work, and then exits. This is an ideal candidate to be managed by a Job.</p>&#13;
&#13;
<p><a data-primary="completions" data-type="indexterm" id="idm45979382333840"/><a data-primary="Jobs" data-secondary="completions field" data-type="indexterm" id="idm45979382333136"/><a data-primary="Jobs" data-secondary="parallelism field" data-type="indexterm" id="idm45979382332192"/><a data-primary="parallelism" data-type="indexterm" id="idm45979382288144"/>There are two fields that control Job execution: <code>completions</code> and <code>parallelism</code>. The first, <code>completions</code>, determines the number of times the specified Pod needs to run successfully before the Job is considered complete. The default value is 1, meaning the Pod will run once.</p>&#13;
&#13;
<p>The <code>parallelism</code> field specifies how many Pods should run at once. Again, the default value is 1, meaning that only one Pod will run at a time.</p>&#13;
&#13;
<p>For example, suppose you want to run a queue-worker Job whose purpose is to consume work items from a queue. You could set <code>parallelism</code> to 10, and leave <code>completions</code> unset. This will start 10 Pods, each of which will keep consuming work from the queue until there is no more work to do, and then exit, at which point the Job will be completed:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">batch/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Job</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">queue-worker</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">completions</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">queue-worker</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="l-Scalar-Plain">...</code><code class="w"/></pre>&#13;
&#13;
<p>Alternatively, if you want to run a single one-off task, you could leave both <span class="keep-together"><code>completions</code></span> and <code>parallelism</code> at 1. This will start one copy of the Pod, and wait for it to complete successfully. If it crashes, fails, or exits in any nonsuccessful way, the Job will restart it, just like a Deployment does. Only successful exits count toward the required number of <code>completions</code>.</p>&#13;
&#13;
<p>How do you start a Job? You could do it manually, by applying a Job manifest using <code>kubectl</code> or Helm. Alternatively, a Job might be triggered by automation; your continuous deployment pipeline, for example (see <a data-type="xref" href="ch14.html#continuous">Chapter 14</a>).</p>&#13;
&#13;
<p>When your Job is finished, if you want Kubernetes to automatically clean up after itself you can use the <code>ttlSecondsAfterFinished</code> setting. Once the specified number of seconds passes after the Job exits, it will automatically be deleted. You can also set <code>ttlSecondsAfterFinished</code> to <code>0</code>, which means your Job will be deleted as soon as it completes.</p>&#13;
&#13;
<p>When you need to run a Job periodically, at a given time of day, or at a given interval, Kubernetes also has a <code>CronJob</code> object.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CronJobs" data-type="sect2"><div class="sect2" id="cronjobs">&#13;
<h2>CronJobs</h2>&#13;
&#13;
<p><a data-primary="CronJob" data-type="indexterm" id="idm45979382239760"/>In Unix environments, scheduled jobs are run by the <code>cron</code> daemon (whose name comes from the Greek word χρόνος, meaning “time”). Accordingly, they’re known as <em>CronJobs</em>, and the Kubernetes <code>CronJob</code> object does exactly the same thing.</p>&#13;
&#13;
<p>A <code>CronJob</code> looks like this:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">batch/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">CronJob</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-cron</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">schedule</code><code class="p">:</code><code class="w"> </code><code class="s">"*/1</code><code class="nv"> </code><code class="s">*</code><code class="nv"> </code><code class="s">*</code><code class="nv"> </code><code class="s">*</code><code class="nv"> </code><code class="s">*"</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">jobTemplate</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="l-Scalar-Plain">...</code><code class="w"/></pre>&#13;
&#13;
<p>The two important fields to look at in the <code>CronJob</code> manifest are <code>spec.schedule</code> and <code>spec.jobTemplate</code>. The <code>schedule</code> field specifies when the job will run, using the same <a href="https://oreil.ly/TaRek">format</a> as the Unix <code>cron</code> utility.</p>&#13;
&#13;
<p>The <code>jobTemplate</code> specifies the template for the Job that is to be run, and is exactly the same as a normal Job manifest (see <a data-type="xref" href="#jobs">“Jobs”</a>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Horizontal Pod Autoscalers" data-type="sect2"><div class="sect2" id="horizontalscaling">&#13;
<h2>Horizontal Pod Autoscalers</h2>&#13;
&#13;
<p><a data-primary="Horizontal Pod Autoscalers (HPAs)" data-type="indexterm" id="ix_09-pods-adoc8"/>Remember that a Deployment controller maintains a specified number of Pod replicas. If one replica fails, another will be started to replace it in order to achieve the target number of replicas.</p>&#13;
&#13;
<p>The desired replica count is set in the Deployment manifest, and we’ve seen that you can adjust this to increase the number of Pods if there is heavy traffic, or reduce it to scale down the Deployment if there are idle Pods.</p>&#13;
&#13;
<p><a data-primary="autoscaling" data-secondary="replicas" data-type="indexterm" id="idm45979382146192"/>But what if Kubernetes could adjust the number of replicas for you automatically, responding to increased demand? This is exactly what the <a href="https://oreil.ly/d4cEE">Horizontal Pod Autoscaler</a> does. (<em>Horizontal</em> scaling refers to adjusting the number of replicas of a service, in contrast to <em>vertical</em> scaling, which makes individual replicas bigger or smaller in terms of CPU or memory.)</p>&#13;
&#13;
<p>A Horizontal Pod Autoscaler (HPA) watches a specified Deployment, constantly monitoring a given metric to see if it needs to scale the number of replicas up or down.</p>&#13;
&#13;
<p>One of the most common autoscaling metrics is CPU utilization. Remember from <a data-type="xref" href="ch05.html#resourcerequests">“Resource Requests”</a> that Pods can request a certain amount of CPU resources; for example, 500 millicpus. As the Pod runs, its CPU usage will fluctuate, meaning that, at any given moment, the Pod is actually using some percentage of its original CPU request.</p>&#13;
&#13;
<p>You can autoscale the Deployment based on this value: for example, you could create an HPA that targets 80% CPU utilization for the Pods. If the mean CPU usage over all the Pods in the Deployment is only 70% of their requested amount, the HPA will scale down by decreasing the target number of replicas. If the Pods aren’t working very hard, we don’t need so many of them and the HPA can scale them down.</p>&#13;
&#13;
<p>On the other hand, if the average CPU utilization is 90%, this exceeds the target of 80%, so we need to add more replicas until the average CPU usage comes down. The HPA will modify the Deployment to increase the target number of replicas.</p>&#13;
&#13;
<p>Each time the HPA determines that it needs to do a scaling operation, it adjusts the replicas by a different amount, based on the ratio of the actual metric value to the target. If the Deployment is very close to the target CPU utilization, the HPA will only add or remove a small number of replicas; but if it’s way out of scale, the HPA will adjust it by a larger number.</p>&#13;
&#13;
<p>The HPA uses another popular Kubernetes project called the Metrics Server for getting the data it needs for making autoscaling decisions. You can install it following the instructions in the <a href="https://oreil.ly/6nZ20">metrics-server repo</a>.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Here’s an example of an HPA based on CPU utilization:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">autoscaling/v2beta2</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">HorizontalPodAutoscaler</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-hpa</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">scaleTargetRef</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">minReplicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">maxReplicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">metrics</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Resource</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">resource</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cpu</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">target</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Utilization</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">averageUtilization</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/></pre>&#13;
&#13;
<p>The interesting fields here are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>spec.scaleTargetRef</code> specifies the Deployment to scale</p>&#13;
</li>&#13;
<li>&#13;
<p><code>spec.minReplicas</code> and <code>spec.maxReplicas</code> specify the limits of scaling</p>&#13;
</li>&#13;
<li>&#13;
<p><code>spec.metrics</code> determines the metrics that will be used for scaling</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Although CPU utilization is the most common scaling metric, you can use any metrics available to Kubernetes, including both the built-in <em>system metrics</em> like CPU and memory usage, and app-specific <em>service metrics</em>, which you define and export from your application (see <a data-type="xref" href="ch16.html#metrics">Chapter 16</a>). For example, you could scale based on the application error rate or number of incoming requests per second.</p>&#13;
&#13;
<p>You can read more about autoscalers and custom metrics in the Kubernetes <a href="https://oreil.ly/17zTB"><span class="keep-together">documentation</span></a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Autoscaling on a known schedule" data-type="sect3"><div class="sect3" id="idm45979381976400">&#13;
<h3>Autoscaling on a known schedule</h3>&#13;
&#13;
<p>The <code>HorizontalPodAutoscaler</code> when combined with a <code>CronJob</code> can be useful in cases where your traffic patterns for an application are predictable based on the time of day. If you know, for example, that you definitely need 20 Pods to be already up and running by 8 a.m. for a big flood of requests that always come in at the start of your business day, then you could create a <code>CronJob</code> that runs the <code>kubectl</code> command with an internal service account (see <a data-type="xref" href="ch08.html#serviceaccounts">“Pod Service Accounts”</a>) for scaling up just before that time:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">batch/v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">CronJob</code><code class="w">&#13;
</code><code class="nn">...</code><code class="w">&#13;
</code><code class="nt">args</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"</code><code class="s">kubectl</code><code class="nv"> </code><code class="s">patch</code><code class="nv"> </code><code class="s">hpa</code><code class="nv"> </code><code class="s">service-hpa</code><code class="nv"> </code><code class="s">--patch</code><code class="nv"> </code><em><code class="s">{</code><code class="s">\"</code><code class="s">spec</code><code class="s">\"</code><code class="s">:{</code><code class="s">\"</code><code class="s">minReplicas</code><code class="s">\"</code><code class="s">:20}}</code></em><code class="s">"</code><code class="w">&#13;
</code><code class="nn">...</code></pre>&#13;
&#13;
<p>Then you could create a similar <code>CronJob</code> at the end of your business day to scale the <code>minReplicas</code> back down. When combined with cluster autoscaling as discussed in <a data-type="xref" href="ch06.html#autoscaling">“Autoscaling”</a>, you could use this trick to save on your total compute costs.</p>&#13;
&#13;
<p>Using the plain HPA without a cron may work fine for your use-cases, but remember that scaling up new nodes and Pods does not happen instantly. In the cases when you already know that you will need a certain capacity to handle an upcoming load spike, adding a <code>CronJob</code> can help ensure that you have everything up and running at the beginning of the spike.<a data-startref="ix_09-pods-adoc8" data-type="indexterm" id="idm45979381911984"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Operators and Custom Resource Definitions (CRDs)" data-type="sect2"><div class="sect2" id="crds">&#13;
<h2>Operators and Custom Resource Definitions (CRDs)</h2>&#13;
&#13;
<p><a data-primary="CRDs (custom resource definitions)" data-type="indexterm" id="idm45979381909632"/><a data-primary="Custom Resource Definitions (CRDs)" data-type="indexterm" id="idm45979381908960"/><a data-primary="operator" data-type="indexterm" id="idm45979381908320"/>We saw in <a data-type="xref" href="#statefulsets">“StatefulSets”</a> that, while the standard Kubernetes objects such as Deployment and Service are fine for simple, stateless applications, they have their limitations. Some applications require multiple, collaborating Pods that have to be initialized in a particular order (for example, replicated databases or clustered services).</p>&#13;
&#13;
<p>For applications that need more complicated management or complex types of resources, Kubernetes allows you to create your own new types of object. These are called <em>Custom Resource Definitions</em> (CRDs). For example, the Velero backup tool creates and uses new custom Kubernetes objects it calls <code>Configs</code> and <code>Backups</code> (see <a data-type="xref" href="ch11.html#velero">“Velero”</a>).</p>&#13;
&#13;
<p>Kubernetes is designed to be extensible, and you’re free to define and create any type of object you want to, using the CRD mechanism. Some CRDs just exist to store data, like the Velero <code>BackupStorageLocation</code> object. But you can go further and create objects that act as Pod controllers, just like a Deployment or StatefulSet.</p>&#13;
&#13;
<p>For example, if you wanted to create a controller object that sets up replicated, high-availability MySQL database clusters in Kubernetes, how would you go about it?</p>&#13;
&#13;
<p>The first step would be to create a CRD for your custom controller object. In order to make it do anything, you then need to write a program that communicates with the Kubernetes API. This is easy to do, as we saw in <a data-type="xref" href="ch07.html#client-go">“Building Your Own Kubernetes Tools”</a>. Such a program is called an <em>Operator</em> (perhaps because it automates the kinds of actions that a human operator might perform).</p>&#13;
&#13;
<p>You can see lots of examples of Operators built and maintained by the community in the <a href="https://operatorhub.io">OperatorHub.io site</a>. This is a repository of hundreds of Operators that you can install on your clusters, or just browse their code to get ideas for your building your own Operators.<a data-startref="ix_09-pods-adoc7" data-type="indexterm" id="idm45979381869872"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress" data-type="sect1"><div class="sect1" id="ingress">&#13;
<h1>Ingress</h1>&#13;
&#13;
<p><a data-primary="Ingress" data-type="indexterm" id="ix_09-pods-adoc9"/>While Services (see <a data-type="xref" href="ch04.html#services">“Service Resources”</a>) are for routing <em>internal</em> traffic in your cluster (for example, from one microservice to another), Ingress is used for routing <em>external</em> traffic into your cluster and to the appropriate microservice (see <a data-type="xref" href="#img-ingress">Figure 9-1</a>). You can think of the concept of Ingress as a load balancer that works in coordination with a Service to get requests from external clients to the correct Pods based on their label selectors. All of this happens using an Ingress controller, which we will cover shortly. For now, let’s see what a typical Ingress resource looks like for exposing your applications outside of the cluster.</p>&#13;
&#13;
<figure><div class="figure" id="img-ingress">&#13;
<img alt="Kubernetes Ingress diagram" src="assets/cndk_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>The Ingress resource</h6>&#13;
</div></figure>&#13;
&#13;
<p>Here is a manifest for a generic Ingress resource:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-ingress</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">rules</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">http</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">paths</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">pathType</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Prefix</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">              </code><code class="nt">service</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">                </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-service</code><code class="w"/>&#13;
<code class="w">                </code><code class="nt">port</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">                  </code><code class="nt">number</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w"/></pre>&#13;
&#13;
<p>This Ingress in this example looks at a Service named <code>demo-service</code>, and that Service then uses the selector labels and readiness status (see <a data-type="xref" href="ch05.html#readiness-probes">“Readiness Probes”</a>) to determine a suitable Pod for receiving the request. Pods that do not match the selector labels defined in the <code>demo-service</code>, and any Pods that have a failing “Ready” status, will not receive any requests. In addition to this basic routing of requests, Ingress can also handle more advanced tasks, such as managing SSL <span class="keep-together">certificates</span>, <span class="keep-together">rate-limiting</span>, and other features commonly associated with load balancers. The specifics of how these work are handled by an Ingress controller.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress Controllers" data-type="sect2"><div class="sect2" id="idm45979381816992">&#13;
<h2>Ingress Controllers</h2>&#13;
&#13;
<p><a data-primary="Ingress" data-secondary="controllers" data-type="indexterm" id="ix_09-pods-adoc10"/>An Ingress controller is responsible for managing Ingress resources in a cluster. Depending on where you are running your clusters, and the functionality you need, the controller you use may vary.</p>&#13;
&#13;
<p>Usually, selecting which Ingress controller to use, and configuring the behavior of the controller, is done using annotations in the Ingress manifest. <a data-primary="load balancer" data-type="indexterm" id="idm45979381813632"/>For example, to have an Ingress in an EKS cluster use a public-facing AWS Application Load Balancer, you would add annotations like this:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nn">...</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-aws-ingress</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">annotations</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">kubernetes.io/ingress.class</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">alb</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">alb.ingress.kubernetes.io/scheme</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">internet-facing</code><code class="w"/>&#13;
<code class="nn">...</code><code class="w"/></pre>&#13;
&#13;
<p>Each Ingress controller will have its own sets of annotations that configure the various features available to that controller.</p>&#13;
&#13;
<p>Hosted GKE clusters in GCP can use <a href="https://oreil.ly/CsWhb">Google Cloud’s Load Balancer Controller (GLBC)</a> for Ingress resources. AWS has a similar product we mentioned above called the <a href="https://oreil.ly/ttxp4">AWS Load Balancer Controller</a> and Azure also has its own <a href="https://oreil.ly/7u1qA">Application Gateway Ingress Controller (AGIC)</a>. If you are using one of these major public cloud providers, and you have applications that you need to expose outside of your clusters, then we recommend exploring using the particular Ingress controller maintained by your cloud provider first.</p>&#13;
&#13;
<p>You also have the option to install and use a different Ingress controller inside your clusters, or even run multiple controllers if you like. There are <a href="https://oreil.ly/Eu9k0">lots of different Ingress controller options</a> out there, and some of the more popular ones include:</p>&#13;
<dl>&#13;
<dt><a href="https://oreil.ly/ZSpg6">nginx-ingress</a></dt>&#13;
<dd>&#13;
<p><a data-primary="Ingress" data-secondary="nginx-ingress" data-type="indexterm" id="idm45979381721424"/><a data-primary="nginx-ingress" data-type="indexterm" id="idm45979381720448"/>NGINX has long been a popular load balancer tool, even before Kubernetes came on the scene. The <code>nginx-ingress</code> project is maintained by the Kubernetes community.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/8k7JH">NGINX Ingress Controller</a></dt>&#13;
<dd>&#13;
<p><a data-primary="Ingress" data-secondary="NGINX Ingress Controller" data-type="indexterm" id="idm45979381717600"/><a data-primary="NGINX Ingress Controller" data-type="indexterm" id="idm45979381716656"/>This controller is backed by the NGINX company itself. There are some differences between this project and the Kubernetes community one mentioned in the previous paragraph.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/wbTye">Contour</a></dt>&#13;
<dd>&#13;
<p><a data-primary="Contour" data-type="indexterm" id="idm45979381714288"/><a data-primary="Envoy load balancer" data-type="indexterm" id="idm45979381670480"/><a data-primary="Ingress" data-secondary="Contour" data-type="indexterm" id="idm45979381669872"/><a data-primary="Ingress" data-secondary="Envoy load balancer" data-type="indexterm" id="idm45979381669024"/>Contour actually uses another tool under the hood called <a href="https://www.envoyproxy.io">Envoy</a> to proxy requests between clients and Pods.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/PBTPR">Traefik</a></dt>&#13;
<dd>&#13;
<p><a data-primary="Ingress" data-secondary="Traefik" data-type="indexterm" id="idm45979381665872"/><a data-primary="Traefik" data-type="indexterm" id="idm45979381664896"/>This is a lightweight proxy tool that can also automatically manage TLS certificates for your Ingress.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/DQ2KE">Kong</a></dt>&#13;
<dd>&#13;
<p><a data-primary="Ingress" data-secondary="Kong" data-type="indexterm" id="idm45979381662432"/><a data-primary="Kong" data-type="indexterm" id="idm45979381661456"/>Kong hosts the <a href="https://docs.konghq.com/hub">Kong Plugin Hub</a> with plugins that integrate with their Ingress controller to configure things like OAuth authentication, LetsEncrypt certificates, IP restriction, metrics, and other useful features for load balancers.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/IbvBL">HAProxy</a></dt>&#13;
<dd>&#13;
<p><a data-primary="HAProxy" data-type="indexterm" id="idm45979381658416"/><a data-primary="Ingress" data-secondary="HAProxy" data-type="indexterm" id="idm45979381657712"/>HAProxy has been another popular tool for load balancers for several years, and they also have their own Ingress controller for Kubernetes along with a Helm chart for installing it in your clusters.<a data-startref="ix_09-pods-adoc10" data-type="indexterm" id="idm45979381656640"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress Rules" data-type="sect2"><div class="sect2" id="idm45979381655712">&#13;
<h2>Ingress Rules</h2>&#13;
&#13;
<p><a data-primary="Ingress" data-secondary="rules" data-type="indexterm" id="idm45979381654576"/>Ingress can also be used to forward traffic to different backend services, depending on certain rules that you specify. One common use for this is to route requests to different places, depending on the request URL (known as a <em>fanout</em>):</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fanout-ingress</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">rules</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">http</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">paths</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/hello</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hello</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">servicePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/goodbye</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">goodbye</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">servicePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Terminating TLS with Ingress" data-type="sect2"><div class="sect2" id="idm45979381634000">&#13;
<h2>Terminating TLS with Ingress</h2>&#13;
&#13;
<p><a data-primary="Ingress" data-secondary="TLS" data-type="indexterm" id="idm45979381632992"/><a data-primary="TLS (Transport Layer Security)" data-secondary="termination" data-type="indexterm" id="idm45979381554528"/><a data-primary="Transport Layer Security (TLS)" data-secondary="termination" data-type="indexterm" id="idm45979381553616"/>Most Ingress controllers can also handle securing connections using Transport Layer Security (TLS) (the protocol formerly known as Secure Sockets Layer [SSL]). This is typically done using a Kubernetes Secret (we will cover these in <a data-type="xref" href="ch10.html#secrets">“Kubernetes Secrets”</a>) containing the contents of the certificate and key, and the <code>tls</code> section of the Ingress manifest:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-ingress</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">tls</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">secretName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-tls-secret</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-service</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">servicePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/>&#13;
<code class="nn">...</code><code class="w"/>&#13;
<code class="nn">---</code><code class="w"/>&#13;
<code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Secret</code><code class="w"/>&#13;
<code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubernetes.io/tls</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-tls-secret</code><code class="w"/>&#13;
<code class="nt">data</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">tls.crt</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">LS0tLS1CRUdJTiBDRV...LS0tCg==</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">tls.key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">LS0tLS1CRUdJTiBSU0...LS0tCg==</code><code class="w"/></pre>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Automating LetsEncrypt certificates with Cert-Manager" data-type="sect3"><div class="sect3" id="idm45979381547376">&#13;
<h3>Automating LetsEncrypt certificates with Cert-Manager</h3>&#13;
&#13;
<p><a data-primary="cert-manager" data-type="indexterm" id="idm45979381546368"/><a data-primary="TLS (Transport Layer Security)" data-secondary="automatic certificate requests" data-type="indexterm" id="idm45979381420240"/><a data-primary="Transport Layer Security (TLS)" data-secondary="automatic certificate requests" data-type="indexterm" id="idm45979381419360"/>If you want to automatically request and renew TLS certificates using the popular <a href="https://letsencrypt.org">LetsEncrypt</a> authority (or another ACME certificate provider), you can use <a href="http://docs.cert-manager.io/en/latest"><code>cert-manager</code></a>.</p>&#13;
&#13;
<p>If you run <code>cert-manager</code> in your cluster, it will automatically detect TLS Ingresses that have no certificate, and request one from the specified provider. It can also handle automatically renewing these certificates when they are close to expiring.<a data-startref="ix_09-pods-adoc9" data-type="indexterm" id="idm45979381415776"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service Mesh" data-type="sect1"><div class="sect1" id="service-mesh">&#13;
<h1>Service Mesh</h1>&#13;
&#13;
<p><a data-primary="service mesh" data-type="indexterm" id="ix_09-pods-adoc11"/>Kubernetes Ingress and Services may be all that you need for routing requests from clients to your applications, depending on the complexity of your organization. But there is also a growing interest in a newer concept commonly referred to as a <em>service mesh</em>. A service mesh is responsible for managing more complex network operations such as rate-limiting and encrypting network traffic between microservices. Service mesh tools can also add metrics and logging for requests flowing through the network, keeping track of how long requests are taking, or tracing where a request started and what path it took through the various microservices along the way. Some service mesh tools can handle automatic retries of failed requests, and have the ability to deny or block inbound or outbound requests as needed.</p>&#13;
&#13;
<p>There are several options for implementing a service mesh that we will list here. We expect that the service mesh tooling landscape will continue to rapidly evolve in the coming years and will become a central part of any cloud native infrastructure stack. If you are just starting out with a few applications to deploy, then you can likely begin with just using the standard Service and Ingress resources provided by Kubernetes. But if you find yourself needing to delve into these more advanced capabilities of a service mesh, these are some good options to explore.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Istio" data-type="sect2"><div class="sect2" id="istio">&#13;
<h2>Istio</h2>&#13;
&#13;
<p><a data-primary="Istio" data-type="indexterm" id="idm45979381409376"/><a data-primary="service mesh" data-secondary="Istio" data-type="indexterm" id="idm45979381408672"/>Istio was one of the first tools associated with providing a service mesh. It is available as an optional add-on component to many hosted Kubernetes clusters, including <a href="https://oreil.ly/BnQMV">GKE</a>. If you want to install Istio yourself, see the <a href="https://oreil.ly/SWldX">Istio installation docs</a> for more info. <a href="https://oreil.ly/KuTMV"><em>Istio: Up and Running</em></a> (O’Reilly) is a great book for learning more about Istio, as well as the broader concepts commonly associated with service meshes.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linkerd" data-type="sect2"><div class="sect2" id="linkerd">&#13;
<h2>Linkerd</h2>&#13;
&#13;
<p><a data-primary="Linkerd" data-type="indexterm" id="idm45979381381968"/><a data-primary="service mesh" data-secondary="Linkerd" data-type="indexterm" id="idm45979381381360"/><a href="https://linkerd.io">Linkerd</a> offers many of the key service mesh features, but with a much lighter footprint and less complexity involved compared to Istio. It can be used for setting up mutual TLS between services, gathering metrics for request rates and latency, blue-green deployments, and request retries.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Consul Connect" data-type="sect2"><div class="sect2" id="idm45979381379568">&#13;
<h2>Consul Connect</h2>&#13;
&#13;
<p><a data-primary="Consul Connect" data-type="indexterm" id="idm45979381378336"/><a data-primary="service mesh" data-secondary="Consul Connect" data-type="indexterm" id="idm45979381377632"/>Before Kubernetes was widely known and used, HashiCorp was offering a popular tool called Consul, which was focused on service discovery. It handled application health checks and the automatic routing of requests to the right place in a distributed compute environment. That piece of functionality is now handled natively in Kubernetes, but Consul has now expanded to include a newer tool called <a href="https://oreil.ly/vxlsB">Consul Connect</a> with service mesh capabilities. If you run a mixed environment with applications running outside of Kubernetes, or are already familiar with using Consul, then Consul Connect may be worth exploring for your service mesh.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="NGINX Service Mesh" data-type="sect2"><div class="sect2" id="idm45979381374992">&#13;
<h2>NGINX Service Mesh</h2>&#13;
&#13;
<p><a data-primary="NGINX Service Mesh" data-type="indexterm" id="idm45979381373792"/><a data-primary="service mesh" data-secondary="NGINX Service Mesh" data-type="indexterm" id="idm45979381373088"/>In addition to the Ingress controllers, NGINX also offers a full <a href="https://oreil.ly/HOmvE">Service Mesh</a> product for Kubernetes. It also uses the sidecar pattern where an NGINX container runs alongside your applications and handles routing the network traffic. This service mesh container provides mTLS encryption, traffic-splitting capabilities, and tracks metrics on network performance for observability.<a data-startref="ix_09-pods-adoc11" data-type="indexterm" id="idm45979381371360"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45979381370400">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Ultimately, everything in Kubernetes is about running Pods. Those Pods can be configured and managed using Kubernetes controllers and objects, either for long-running processes or short-lived jobs and CronJobs. More complex setups may use Custom Resource Definitions and Operators. Routing network requests to Pods involves using Services and Ingress controllers.</p>&#13;
&#13;
<p>The basic ideas to remember:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Labels are key-value pairs that identify resources, and can be used with selectors to match a specified group of resources.</p>&#13;
</li>&#13;
<li>&#13;
<p>Node affinities attract or repel Pods to or from nodes with specified attributes. For example, you can specify that a Pod can only run on a node in a specified availability zone.</p>&#13;
</li>&#13;
<li>&#13;
<p>While hard node affinities can block a Pod from running, soft node affinities are more like suggestions to the scheduler. You can combine multiple soft affinities with different weights.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pod affinities express a preference for Pods to be scheduled on the same node as other Pods. For example, Pods that benefit from running on the same node can express that using a Pod affinity for each other.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pod anti-affinities repel other Pods instead of attracting. For example, an anti-affinity to replicas of the same Pod can help spread your replicas evenly across the cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>Taints are a way of tagging nodes with specific information, usually about node problems or failures. By default, Pods won’t be scheduled on tainted nodes.</p>&#13;
</li>&#13;
<li>&#13;
<p>Tolerations allow a Pod to be scheduled on nodes with a specific taint. You can use this mechanism to run certain Pods only on dedicated nodes.</p>&#13;
</li>&#13;
<li>&#13;
<p>DaemonSets allow you to schedule one copy of a Pod on every node (for example, a logging agent).</p>&#13;
</li>&#13;
<li>&#13;
<p>StatefulSets start and stop Pod replicas in a specific numbered sequence, allowing you to address each by a predictable DNS name. This is ideal for clustered applications, such as databases.</p>&#13;
</li>&#13;
<li>&#13;
<p>Jobs run a Pod once (or a specified number of times) before completing. Similarly, CronJobs run a Pod periodically at specified times.</p>&#13;
</li>&#13;
<li>&#13;
<p>Horizontal Pod Autoscalers (HPAs) watch a set of Pods, trying to optimize a given metric (such as CPU utilization). They increase or decrease the desired number of replicas to achieve the specified goal.</p>&#13;
</li>&#13;
<li>&#13;
<p>Custom Resource Definitions (CRDs) allow you to create your own custom Kubernetes objects, to store any data you wish. Operators are Kubernetes client programs that can implement orchestration behavior for your specific application. OperatorHub.io is a great resource for searching community-built <span class="keep-together">Operators.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Ingress resources route requests to different services, depending on a set of rules, for example, matching parts of the request URL. They can also terminate TLS connections for your application.</p>&#13;
</li>&#13;
<li>&#13;
<p>Istio, Linkerd, and Consul Connect are advanced service mesh tools that provide networking features for microservice environments such as encryption, QoS, metrics, logging, and more complex routing strategies.<a data-startref="ix_09-pods-adoc1" data-type="indexterm" id="idm45979381355088"/><a data-startref="ix_09-pods-adoc0" data-type="indexterm" id="idm45979381354384"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>