<html><head></head><body><section data-pdf-bookmark="Chapter 9. Scaling" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch_scaling">&#13;
<h1><span class="label">Chapter 9. </span>Scaling</h1>&#13;
&#13;
&#13;
<p>In Kubernetes, scaling can mean different things to different users. <a data-primary="scaling" data-secondary="cluster scaling and pod scaling" data-type="indexterm" id="id993"/><a data-primary="cluster scaling" data-type="indexterm" id="id994"/>We distinguish between two cases:</p>&#13;
<dl>&#13;
<dt>Cluster scaling</dt>&#13;
<dd>&#13;
<p>Sometimes called <em>cluster elasticity</em>, this refers to the (automated) process of adding or removing worker nodes based on cluster utilization.<a data-primary="cluster elasticity" data-seealso="cluster scaling" data-type="indexterm" id="id995"/></p>&#13;
</dd>&#13;
<dt>Application-level scaling</dt>&#13;
<dd>&#13;
<p>Sometimes called <em>pod scaling</em>, this refers to the (automated) process of manipulating pod characteristics based on a variety of metrics, from low-level signals such as CPU utilization to higher-level ones, such as HTTP requests served per second, for a given pod.<a data-primary="application-level scaling" data-type="indexterm" id="id996"/><a data-primary="pod scaling" data-type="indexterm" id="id997"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Two kinds of pod-level scalers exist:</p>&#13;
<dl>&#13;
<dt>Horizontal pod autoscalers (HPAs)</dt>&#13;
<dd>&#13;
<p>HPAs automatically increase or decrease the number of pod replicas depending on certain metrics.</p>&#13;
</dd>&#13;
<dt>Vertical pod autoscalers (VPAs)</dt>&#13;
<dd>&#13;
<p>VPAs automatically increase or decrease the resource requirements of containers running in a pod.<a data-primary="horizontal pod autoscalers (HPAs)" data-type="indexterm" id="id998"/><a data-primary="vertical pod autoscalers (VPAs)" data-type="indexterm" id="id999"/><a data-primary="VPAs (vertical pod autoscalers)" data-type="indexterm" id="id1000"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In this chapter, we first examine cluster elasticity for GKE, AKS, and EKS and then discuss pod scaling with HPAs.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="9.1 Scaling a Deployment" data-type="sect1"><div class="sect1" id="scaling-deployments">&#13;
<h1>9.1 Scaling a Deployment</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id234">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You have a deployment and want to scale it horizontally.<a data-primary="deployments" data-secondary="scaling" data-type="indexterm" id="id1001"/><a data-primary="scaling" data-secondary="horizontally scaling a deployment" data-type="indexterm" id="id1002"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id82">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use the <code>kubectl scale</code> command to scale out a deployment.<a data-primary="kubectl" data-secondary="scale command" data-type="indexterm" id="id1003"/></p>&#13;
&#13;
<p>Let’s reuse the <code>fancyapp</code> deployment from <a data-type="xref" href="ch04.html#deployments">Recipe 4.5</a>, with five replicas. If it’s not running yet, create it with <code>kubectl apply -f fancyapp.yaml</code>.</p>&#13;
&#13;
<p>Now suppose that the load has decreased and you don’t need five replicas anymore; three is enough. <a data-primary="replicas, scaling for a deployment" data-type="indexterm" id="id1004"/>To scale the deployment down to three replicas, do this:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get deploy fancyapp</strong>&#13;
NAME       READY   UP-TO-DATE   AVAILABLE   AGE&#13;
fancyapp   5/5     5            5           59s&#13;
&#13;
$ <strong>kubectl scale deployment fancyapp --replicas=3</strong>&#13;
deployment "fancyapp" scaled&#13;
&#13;
$ <strong>kubectl get deploy fancyapp</strong>&#13;
NAME       READY   UP-TO-DATE   AVAILABLE   AGE&#13;
fancyapp   3/3     3            3           81s&#13;
</pre>&#13;
&#13;
<p>Rather than manually scaling a deployment, you can automate this process; see <a data-type="xref" href="#auto_app_scaling_hpa">Recipe 9.2</a> for an example.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="9.2 Using Horizontal Pod Autoscaling" data-type="sect1"><div class="sect1" id="auto_app_scaling_hpa">&#13;
<h1>9.2 Using Horizontal Pod Autoscaling</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id83">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You want to automatically increase or decrease the number of pods in a deployment, depending on the load present.<a data-primary="scaling" data-secondary="using horizontal pod autoscaling" data-type="indexterm" id="ix_sclHPA"/><a data-primary="pod scaling" data-secondary="using horizontal pod autoscaling" data-type="indexterm" id="ix_podsclhor"/><a data-primary="horizontal pod autoscaling, using" data-type="indexterm" id="ix_HPA"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id84">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use an HPA, as described here.</p>&#13;
&#13;
<p>To use HPAs, the Kubernetes Metrics API must be available.<a data-primary="Metrics Server" data-secondary="installing" data-type="indexterm" id="id1005"/><a data-primary="Kubernetes Metrics Server" data-see="Metrics Server" data-type="indexterm" id="id1006"/> To install the Kubernetes Metrics Server, see <a data-type="xref" href="ch02.html#metrics_server">Recipe 2.7</a>.</p>&#13;
&#13;
<p>First, create<a data-primary="PHP app (PHP environment and server)" data-type="indexterm" id="id1007"/> an app—a PHP environment and server—that you can use as the target of the HPA:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl create deployment appserver --image=registry.k8s.io/hpa-example \&#13;
    --port 80</strong>&#13;
deployment.apps/appserver created&#13;
$ <strong>kubectl expose deployment appserver --port=80 --target-port=80</strong>&#13;
$ <strong>kubectl set resources deployment appserver -c=hpa-example --requests=cpu=200m</strong>&#13;
</pre>&#13;
&#13;
<p>Next, create an HPA and define the trigger parameter <code>--cpu-percent=40</code>, which means <a data-primary="horizontal pod autoscalers (HPAs)" data-secondary="creating and defining trigger parameter" data-type="indexterm" id="id1008"/>that the CPU utilization should not exceed 40%:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl autoscale deployment appserver --cpu-percent=40 --min=1 --max=5</strong>&#13;
horizontalpodautoscaler.autoscaling/appserver autoscaled&#13;
&#13;
$ <strong>kubectl get hpa --watch</strong>&#13;
NAME        REFERENCE              TARGETS   MINPODS   MAXPODS  REPLICAS   AGE&#13;
appserver   Deployment/appserver   1%/40%    1         5        1          2m29s&#13;
</pre>&#13;
&#13;
<p>In a second terminal session, keep an eye on the deployment:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get deploy appserver --watch</strong>&#13;
</pre>&#13;
&#13;
<p>Finally, in a third terminal <a data-primary="terminal sessions for setting up HPA" data-type="indexterm" id="id1009"/>session, launch <a data-primary="load generator" data-type="indexterm" id="id1010"/>the load generator:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl run -i -t loadgen --rm --image=busybox:1.36 --restart=Never -- \&#13;
    /bin/sh -c "while sleep 0.01; do wget -q -O- http://appserver; done"</strong>&#13;
</pre>&#13;
&#13;
<p>Since there are three terminal sessions involved in parallel, an overview of the whole situation is provided in <a data-type="xref" href="#horizontal-pod-autoscaler-terminal">Figure 9-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="horizontal-pod-autoscaler-terminal">&#13;
<img alt="Screenshot of the terminal sessions for setting up an HPA" src="assets/kcb2_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>Terminal sessions for setting up an HPA</h6>&#13;
</div></figure>&#13;
&#13;
<p>In <a data-type="xref" href="#horizontal-pod-autoscaler-dashboard">Figure 9-2</a> showing the Kubernetes dashboard, you can see the effect of the HPA on the <code>appserver</code> deployment.<a data-primary="dashboard" data-secondary="showing effect of HPA" data-type="indexterm" id="id1011"/></p>&#13;
&#13;
<figure><div class="figure" id="horizontal-pod-autoscaler-dashboard">&#13;
<img alt="Screen Shot Of The Kubernetes Dashboard, Showing The Effect Of An HPA" src="assets/kcb2_0902.png"/>&#13;
<h6><span class="label">Figure 9-2. </span>Kubernetes dashboard, showing the effect of an HPA</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1012">&#13;
<h2>See Also</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://keda.sh">Kubernetes Event-driven Autoscaling</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/b6Pwx">The HPA walkthrough in the Kubernetes documentation</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="9.3 Automatically Resizing a Cluster in GKE" data-type="sect1"><div class="sect1" id="auto_cluster_scaling_gke">&#13;
<h1>9.3 Automatically Resizing a Cluster in GKE</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id85">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You want the number of nodes in your GKE cluster to automatically grow or shrink, depending on the utilization.<a data-primary="scaling" data-secondary="using horizontal pod autoscaling" data-startref="ix_sclHPA" data-type="indexterm" id="id1013"/><a data-primary="pod scaling" data-secondary="using horizontal pod autoscaling" data-startref="ix_podsclhor" data-type="indexterm" id="id1014"/><a data-primary="horizontal pod autoscaling, using" data-startref="ix_HPA" data-type="indexterm" id="id1015"/><a data-primary="scaling" data-secondary="automatically resizing cluster in GKE" data-type="indexterm" id="ix_sclclsGKE"/><a data-primary="cluster scaling" data-secondary="automatically resizing cluster in GKE" data-type="indexterm" id="ix_clstrsclGKE"/><a data-primary="Google Kubernetes Engine (GKE)" data-secondary="GKE Cluster Autoscaler" data-type="indexterm" id="ix_GKEclsscl"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id86">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use the GKE Cluster Autoscaler. This recipe assumes you’ve got the <code>gcloud</code> command installed and the environment set up (i.e., you’ve created a project and enabled billing).<a data-primary="gcloud CLI" data-type="indexterm" id="id1016"/></p>&#13;
&#13;
<p>Create a cluster with one worker node and cluster autoscaling enabled:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>gcloud container clusters create supersizeme --zone=us-west1-a \&#13;
    --machine-type=e2-small --num-nodes=1 \&#13;
    --min-nodes=1 --max-nodes=3 --enable-autoscaling</strong>&#13;
Creating cluster supersizeme in us-west1-a... Cluster is being health-checked&#13;
(master is healthy)...done.&#13;
Created [https://container.googleapis.com/v1/projects/k8s-cookbook/zones/&#13;
us-west1-a/clusters/supersizeme].&#13;
To inspect the contents of your cluster, go to: https://console.cloud.google.com/&#13;
kubernetes/workload_/gcloud/us-west1-a/supersizeme?project=k8s-cookbook&#13;
kubeconfig entry generated for supersizeme.&#13;
NAME         LOCATION    ...  MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS&#13;
supersizeme  us-west1-a  ...  e2-small      1.26.5-gke.1200  1          RUNNING&#13;
</pre>&#13;
&#13;
<p>At this point in time, when looking at the Google Cloud console, you should see something like what is shown in <a data-type="xref" href="#gke-cluster-autoscale-1-node">Figure 9-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="gke-cluster-autoscale-1-node">&#13;
<img alt="Screenshot Of The Google Cloud Console, Showing The Initial Cluster Size of One Node" src="assets/kcb2_0903.png"/>&#13;
<h6><span class="label">Figure 9-3. </span>Google Cloud console, showing the initial cluster size of one node</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now, launch three pods using a deployment and request <a data-primary="resources" data-secondary="triggering cluster autoscaling" data-type="indexterm" id="id1017"/>cluster resources to trigger the cluster autoscaling:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl create deployment gogs --image=gogs/gogs:0.13 --replicas=3</strong>&#13;
$ <strong>kubectl set resources deployment gogs -c=gogs --requests=cpu=200m,memory=256Mi</strong>&#13;
</pre>&#13;
&#13;
<p class="pagebreak-before">After a while, the deployment will be updated:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get deployment gogs</strong>&#13;
NAME   READY   UP-TO-DATE   AVAILABLE   AGE&#13;
gogs   3/3     3            3           2m27s&#13;
</pre>&#13;
&#13;
<p>You should now have a cluster of two nodes, as depicted in <a data-type="xref" href="#gke-cluster-autoscale-2-nodes">Figure 9-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="gke-cluster-autoscale-2-nodes">&#13;
<img alt="Screenshot of the Google Cloud console, showing the resulting cluster scaled to two nodes" src="assets/kcb2_0904.png"/>&#13;
<h6><span class="label">Figure 9-4. </span>Google Cloud console, showing the resulting cluster scaled to two nodes</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id235">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>Cluster autoscaling can be enabled or updated on a GKE cluster after it has been created:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>gcloud container clusters update supersizeme --zone=us-west1-a \&#13;
    --min-nodes=1 --max-nodes=3 --enable-autoscaling</strong>&#13;
</pre>&#13;
&#13;
<p>The choice of <a href="https://oreil.ly/lz7wQ">machine type</a> used in the cluster nodes is an important factor to consider and depends on the resources required to run your workloads. If your workloads demand more resources, then you should consider using a larger machine type.<a data-primary="resources" data-secondary="cluster scaling adding or removing resources" data-type="indexterm" id="id1018"/></p>&#13;
&#13;
<p>Unlike pod scaling, cluster scaling dynamically adds resources to your cluster, which could significantly increase your cloud bill. Ensure that you configure the maximum node count of your GKE cluster appropriately to avoid exceeding your spending limit.</p>&#13;
&#13;
<p>When you don’t need the cluster anymore, you should delete it to avoid being charged for unused compute resources:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>gcloud container clusters delete supersizeme</strong>&#13;
</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1019">&#13;
<h2>See Also</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/QHik5">Cluster Autoscaler</a> in the <em>kubernetes/autoscaler</em> repo</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/g8lfr">Cluster Autoscaler</a> in the GKE docs</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="9.4 Automatically Resizing an Amazon EKS Cluster" data-type="sect1"><div class="sect1" id="auto_cluster_scaling_aws">&#13;
<h1>9.4 Automatically Resizing an Amazon EKS Cluster</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id236">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You want the number of nodes in your AWS EKS cluster to automatically grow or shrink, depending on the utilization.<a data-primary="Google Kubernetes Engine (GKE)" data-secondary="GKE Cluster Autoscaler" data-startref="ix_GKEclsscl" data-type="indexterm" id="id1020"/><a data-primary="scaling" data-secondary="automatically resizing cluster in GKE" data-startref="ix_sclclsGKE" data-type="indexterm" id="id1021"/><a data-primary="AWS (Amazon Web Services)" data-secondary="automatically resizing AWS EKS cluster" data-type="indexterm" id="ix_AWSEKScls"/><a data-primary="scaling" data-secondary="automatically resizing AWS EKS cluster" data-type="indexterm" id="ix_sclclsAWSEKS"/><a data-primary="cluster scaling" data-secondary="automatically resizing cluster in GKE" data-startref="ix_clstrsclGKE" data-type="indexterm" id="id1022"/><a data-primary="EKS (Elastic Kubernetes Service)" data-see="AWS" data-type="indexterm" id="id1023"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id237">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use the <a href="https://oreil.ly/6opBo">Cluster Autoscaler</a>, a Helm package leveraging AWS autoscaling groups. Follow <a data-type="xref" href="ch06.html#helm_install">Recipe 6.1</a> to install the Helm client that’s required to install the package.</p>&#13;
&#13;
<p>First, create a cluster with one worker node and make sure you can access it with <code>kubectl</code>:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>eksctl create cluster --name supersizeme \&#13;
    --region eu-central-1 --instance-types t3.small \&#13;
    --nodes 1 --nodes-min 1 --nodes-max 3</strong>&#13;
2023-04-11 12:00:50 [i]  eksctl version 0.136.0-dev+3f5a7c5e0.2023-03-31T10...&#13;
2023-04-11 12:00:50 [i]  using region eu-central-1&#13;
...&#13;
2023-04-11 12:17:31 [i]  kubectl command should work with "/Users/sameersbn/&#13;
.kube/config", try 'kubectl get nodes'&#13;
2023-04-11 12:17:31 [✔]  EKS cluster "supersizeme" in "eu-central-1" region&#13;
is ready&#13;
&#13;
$ <strong>aws eks update-kubeconfig --name supersizeme --region eu-central-1</strong>&#13;
</pre>&#13;
&#13;
<p class="pagebreak-before">Next, deploy the Cluster Autoscaler<a data-primary="Helm" data-secondary="deploying Cluster Autoscaler chart" data-type="indexterm" id="id1024"/><a data-primary="charts" data-secondary="deploying Cluster Autoscaler Helm chart" data-type="indexterm" id="id1025"/> Helm chart:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>helm repo add autoscaler https://kubernetes.github.io/autoscaler</strong>&#13;
$ <strong>helm install autoscaler autoscaler/cluster-autoscaler \&#13;
    --set autoDiscovery.clusterName=supersizeme \&#13;
    --set awsRegion=eu-central-1 \&#13;
    --set awsAccessKeyID=&lt;<em>YOUR AWS KEY ID&gt;</em> \&#13;
    --set awsSecretAccessKey=&lt;<em>YOUR AWS SECRET KEY&gt;</em></strong>&#13;
</pre>&#13;
&#13;
<p>At this point, the cluster has only one node:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get nodes</strong>&#13;
NAME                                 STATUS   ROLES    AGE   VERSION&#13;
ip...eu-central-1.compute.internal   Ready    &lt;none&gt;   31m   v1.25.9-eks-0a21954&#13;
</pre>&#13;
&#13;
<p>Now, launch five pods using a deployment and<a data-primary="resources" data-secondary="triggering cluster autoscaling" data-type="indexterm" id="id1026"/> request cluster resources to trigger the cluster autoscaling:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl create deployment gogs --image=gogs/gogs:0.13 --replicas=5</strong>&#13;
$ <strong>kubectl set resources deployment gogs -c=gogs --requests=cpu=200m,memory=512Mi</strong>&#13;
</pre>&#13;
&#13;
<p>After a while, the deployment will be updated:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get deployment gogs</strong>&#13;
NAME   READY   UP-TO-DATE   AVAILABLE   AGE&#13;
gogs   5/5     5            5           2m7s&#13;
</pre>&#13;
&#13;
<p>Now your cluster should have scaled up to accommodate the requested resources:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get nodes</strong>&#13;
NAME                                 STATUS   ROLES    AGE   VERSION&#13;
ip...eu-central-1.compute.internal   Ready    &lt;none&gt;   92s   v1.25.9-eks-0a21954&#13;
ip...eu-central-1.compute.internal   Ready    &lt;none&gt;   93s   v1.25.9-eks-0a21954&#13;
ip...eu-central-1.compute.internal   Ready    &lt;none&gt;   36m   v1.25.9-eks-0a21954&#13;
</pre>&#13;
&#13;
<p>To avoid being charged for <a data-primary="scaling" data-secondary="automatically resizing AWS EKS cluster" data-startref="ix_sclclsAWSEKS" data-type="indexterm" id="id1027"/><a data-primary="AWS (Amazon Web Services)" data-secondary="automatically resizing AWS EKS cluster" data-startref="ix_AWSEKScls" data-type="indexterm" id="id1028"/>unused resources, delete the cluster if you don’t need &#13;
<span class="keep-together">it anymore:</span></p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>eksctl delete cluster --name supersizeme --region eu-central-1</strong>&#13;
</pre>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section></body></html>