- en: Chapter 20\. Chaos Testing, Load Testing, and Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers three different methods of testing applications in your
    Kubernetes cluster: chaos testing, load testing, and experiments. All these tools
    can be used to help you build more useful, more resilient, and more performant
    applications. They can also provide insight into your application and help you
    better understand your users and anticipate the impact of changes before you roll
    them out broadly. This insight enables you to make better decisions and identify
    areas for future improvements. The following sections will describe the details
    of each type of test, their goals, and the prerequisites necessary before starting
    each test.'
  prefs: []
  type: TYPE_NORMAL
- en: Chaos Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chaos testing, as its name indicates, is testing your application’s ability
    to respond to chaos in the world, But what exactly does chaos mean? Broadly speaking,
    for an application chaos means introducing unusual, but not wholly unexpected,
    edge conditions to your application and seeing how it responds. This enables you
    to understand if your application is resilient to these edge conditions that may
    not have previously occurred during development of the application but may occur
    at some point during the operation of your application. Often our application
    development occurs during idealized conditions. Unfortunately, when exposed to
    the real world for long enough, these idealized conditions are challenged by errors
    and failures that were not present during initial development. These errors can
    include communication errors, network disconnections, storage problems, and application
    crashes and failures. Chaos testing is the art of artificially introducing these
    errors into your test environments and observing how well your application copes
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: Goals for Chaos Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goals for chaos testing are to introduce extreme conditions into your application’s
    environment and to observe how your application behaves in these conditions, especially,
    how it fails. It may seem unusual to test in such a way that failures are expected
    and desirable. While application failures in general are something that we try
    to avoid, it is far better to observe those failures in a test environment where
    customers or users are not impacted. We hope to observe failures when chaos testing
    because they offer an opportunity to fix those problems before they affect our
    users or customers.
  prefs: []
  type: TYPE_NORMAL
- en: Of course the goal is to introduce a *realistic* level of error into our applications
    to see how they behave. Introducing a level of error that is not expected to ever
    occur in practice, while interesting, isn’t a great use of time or resources.
    Excessive levels of error can help us harden our applications for extreme environments,
    but if such extremes never occur, the effort to harden the application is wasted.
    Of course each application has a different level of both variability and resilience
    that is desired. The level of resiliency expected of a mobile game is dramatically
    less than the level of resilience expected of an aircraft or automobile. Understanding
    both the resilience requirements and expected environment for your application
    is a critical prerequisite for high-quality chaos testing.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for Chaos Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build a useful chaos test it is critical to understand the environmental
    conditions that your application may encounter. This includes both the expected
    frequency of errors and also the types of errors that may occur. For example,
    is your storage already resilient? If you are building a stateless application
    that uses cloud-backed storage as a service, you may not need to test your application
    for disk failures, but you will likely want to introduce chaos in the communication
    with the cloud storage solution.
  prefs: []
  type: TYPE_NORMAL
- en: Before beginning chaos testing think about the risks in your application, and
    identify places where you want to introduce error and at what frequency. When
    thinking about frequency, remember that we’re not trying to test for the average
    case. The average case is already well represented in your existing integration
    tests. Instead we are looking to simulate the kind of environment that may occur
    only once a year or once in a decade. You need to understand your application
    well enough to describe what is plausible.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of understanding your application, the other important prerequisite
    for chaos testing is high-quality monitoring for the correctness and behavior
    of your application. It is one thing to introduce chaos into your environment,
    but to make this chaos useful you also need to be able to observe the operation
    of your application with sufficient detail to determine the impact of the chaos
    and to identify the areas where your application needs hardening to be able to
    deal with the chaos. In general, this monitoring is necessary for any production
    application. In addition to its core contributions around resiliency, chaos testing
    can also be a good test to see if your monitoring and logging are sufficient to
    handle a real outage.
  prefs: []
  type: TYPE_NORMAL
- en: Chaos Testing Your Application’s Communication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the easiest ways to inject chaos into your application’s communication
    is to place a proxy between each client and your service. This proxy handles all
    the network traffic between your client and the server and injects random faults
    like extra latency, disconnects, or other errors. There are several different
    open source options for such a proxy, but one of the most popular is [ToxiProxy](https://oreil.ly/N8QNF),
    which was created by Shopify. The easiest way to add ToxiProxy to your system
    is to effectively run a ToxiProxy layer in front of each actual service in your
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, you first need to rename each service to which you want to
    add chaos. To see this in more detail, suppose you have a service named `backend`
    that serves traffic on port 8080. You can update a Kubernetes Service named `backend`
    to be called `backend-real`. Then you can create new Deployment of ToxiProxy Pods
    that are configured using the ToxiProxy command-line tool as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When you build the Pod definition for this Deployment of ToxiProxy, you can
    run this command as a PostStart life-cycle hook. This command configures ToxiProxy
    to listen on port 8080 within the pod and then forward traffic to your actual
    backend service, which has the DNS name `backend-real`.
  prefs: []
  type: TYPE_NORMAL
- en: Next you create a new service named `backend` to replace the one that you renamed,
    and you point this service at the Deployment of ToxiProxy Pods that you just created.
    In this way, any client in your application that communicates with `backend` will
    automatically start communicating with the chaos proxy instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can start adding chaos to your application using the ToxiProxy
    command-line tool by issuing commands like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will add 2,000 milliseconds of latency to all traffic through this proxy.
    If you create multiple pods in your proxy Deployment, you will need to run this
    command for each pod, or automate it using scripts or code.
  prefs: []
  type: TYPE_NORMAL
- en: Chaos Testing Your Application’s Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to testing the operation of your application when communication
    is flaky, it is also a good idea to test your application in situations where
    the infrastructure it is running on is flaky or overloaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to start with infrastructure failures is to simply delete pods.
    Starting with a single Deployment, you can delete random pods within the Deployment
    based on its label selector using a simple bash script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Of course if you’d rather have something more complete you can write code using
    the various [Kubernetes clients](https://oreil.ly/Ib1kp) out there or even an
    existing open source tool like [Chaos Mesh](https://chaos-mesh.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have moved through all the microservice Deployments in your application,
    you can move on to deleting pods within the different services at once. This simulates
    a more broad outage. You can extend the previous script to randomly delete pods
    within a particular namespace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you can simulate complete failures in your infrastructure by causing
    entire nodes in your cluster to fail. There are a variety of ways to accomplish
    this. If you are running in a cloud-based Kubernetes, you can use cloud VM APIs
    to shut down or reboot a machine in your cluster. If you are running on physical
    infrastructure, you can literally pull the power plug on a particular machine,
    or reboot it by logging in and running commands. On both physical and virtual
    hardware you can also cause your kernel to panic by running `sudo sh -c 'echo
    c > /proc/sysrq-trigger'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple script that will randomly panic approximately 10% of the machines
    in a Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Fuzz Testing Your Application for Security and Resiliency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One final type of testing in the same spirit as chaos testing is fuzz testing.
    Fuzz testing is like chaos testing in that it introduces randomness and chaos
    into your application, but instead of introducing failures, fuzz testing focuses
    on introducing inputs that are technically legal but extreme in one way or another.
    For example, you might send an endpoint a legal JSON request but include duplicate
    fields or data that is especially long or contains random values. The goal of
    fuzz testing is to test the resiliency of your application to random extreme or
    malicious inputs. Fuzz testing is most often used in the context of security testing
    because random inputs can cause unexpected code paths to be executed and to introduce
    vulnerabilities or crashes. Fuzz testing can help you ensure that your application
    is resilient to chaos from malicious or erroneous input in addition to failures
    in the environment. Fuzz testing can be added at both the cluster service level
    as well as the unit test level.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chaos testing is the art of introducing unexpected but not impossible conditions
    into the runtime of your application and observing what happens. Introducing potential
    errors and failures into an environment before any failures can impact actual
    usage of your application helps you identify problem areas before they become
    critical.
  prefs: []
  type: TYPE_NORMAL
- en: Load Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load testing is used to determine how your application behaves under load. A
    load-testing tool is used to generate realistic application traffic that is equivalent
    to real production usage of your application. This traffic can either be artificially
    generated or recorded traffic from actual production traffic that is replayed.
    Load testing can be used to either identify areas that may become problems in
    the future or to ensure that new code and features do not cause regressions.
  prefs: []
  type: TYPE_NORMAL
- en: Goals for Load Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core goal for load testing is to understand how your application behaves
    under load. When you are building an application, it is generally exposed to occasional
    traffic from only a few users. This traffic is sufficient for understanding the
    correctness of the application, but it doesn’t help us understand how the application
    behaves under realistic load. Thus, to understand how your application works when
    deployed in production, load testing is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two fundamental uses of load testing are estimating current capacity and regression
    prevention. Regression prevention is the use of load testing to ensure that a
    new release of software can sustain the same load as the previous version of the
    software. Whenever we roll out a new version of our software there is new code
    and configuration in the release (if there wasn’t, then what is the point of the
    release?). While these code changes introduce new features and fix bugs, they
    can also introduce performance regressions: the new version cannot serve the same
    level of load as the previous version. Of course sometimes these performance regressions
    are known and expected; for example, a new feature may have made a computation
    more complex and thus slower, but even in such cases, load testing is necessary
    to determine how the infrastructure (e.g., the number of pods, the resources they
    require) needs to be scaled up to sustain production traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to regression prevention, which is used to catch problems newly
    introduced into your application, predictive load testing is used to anticipate
    problems before they occur. For many services, there is a steady growth in the
    use of the service. Each month there are more users and more requests to your
    service. In general this is a good thing, but keeping those users happy means
    continuing to improve your infrastructure to keep up with the new load. Predictive
    load testing takes the historical growth trends from your application and uses
    them to test your application as if it were operating in the future. For example,
    if your application’s traffic is growing 10% each month, you might run a predictive
    load test at 110% of the current peak traffic to simulate how your application
    will work in the next month. While scaling up your application can be as easy
    as adding more replicas and more resources, often fundamental bottlenecks in your
    application require rearchitecting. Predictive load testing allows you to anticipate
    the future and perform these changes without the emergency of a user-facing outage
    due to increased load.
  prefs: []
  type: TYPE_NORMAL
- en: Predictive load testing can also be used to anticipate how an application will
    behave prior to launch. Rather than using historical information, you can use
    your predictions about usage at launch to ensure that such a launch is successful
    and not a disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for Load Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Load testing is used to ensure that your application can perform while operating
    under significant load. Further, like chaos testing, load testing can also introduce
    failure conditions in your application due to that load. Consequently, load testing
    shares the same prerequisites as chaos testing around application observability.
    To successfully use a load test, you need to be able to verify that your application
    is operating correctly and have enough information to gain insight into where
    and why failures occur if they do.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the core observability of your application, another critical
    prerequisite for load testing is the ability to generate realistic load for your
    test. If your load test doesn’t closely mimic real-world user behaviors, then
    it is of little use. As a concrete example, imagine if your load test continuously
    makes repeated requests for a single user. In many applications, such traffic
    will produce an unrealistic cache hit rate, and your load test will seem to show
    an ability to handle large amounts of load that is not possible under more realistic
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Realistic Traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Methods for generating real-world traffic patterns for your application vary
    depending on your application. For certain types of more read-only sites, for
    example, a news site, it may be sufficient to repeatedly access each of the different
    pages using some sort of probability distribution. But for many applications,
    especially those that involve both reading and writing operations, the only way
    to generate a realistic load test is to record real-world traffic and play it
    back. One of the easiest ways to do this is to write the complete details of each
    HTTP request to a file, and then resend those requests back to the server at a
    later time.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, such an approach can have complications. The first and foremost
    consequence of recording all the requests to your application is user privacy
    and security. In many cases requests to an application contain both private information
    as well as security tokens. If you record all this information to a file for playback,
    you must be very, very careful in handling these files to ensure that user privacy
    and security are respected.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge with recording and playing back actual user requests has to
    do with the timeliness of the requests themselves. If there is a time component
    to the requests, for example, search queries about the latest news events, these
    requests will have a very different behavior several weeks (or months) after those
    events have occurred. There will be many fewer messages related to old news. Timeliness
    also affects the correct behavior of your application. Requests often contain
    security tokens and if you are doing security properly, those tokens are short
    lived. This means that recorded tokens will likely not work correctly when verified.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when requests write data to backend storage systems, replaying requests
    that modify storage must be performed in a copy or snapshot of the production
    storage infrastructure. If you are not careful about how you set this up, you
    can cause significant problems with customer data.
  prefs: []
  type: TYPE_NORMAL
- en: For all these reasons, simply recording and playing back requests, though easy,
    is not a best practice. Instead the more useful way to use requests is to build
    up a model of the ways in which your service is used. How many read requests?
    For what resources? How many writes? Using this model you can generate synthetic
    load that has realistic characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Load Testing Your Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have generated the requests to power your load test, it is simply a
    matter of applying that load to your service. Unfortunately, it is rarely that
    simple. In most real-world applications there are databases and other storage
    systems involved. To correctly simulate your application under load, you also
    need to write into storage systems, but not to the production data store since
    this is artificial load. Thus, to correctly load test your application, you need
    to be able to turn up a true copy of your application with all its dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Once your application clone is up and running, it is a matter of sending all
    the requests. It turns out large-scale load testing is also a distributed systems
    problem. You will want to use a large number of different pods to send load onto
    your application. This is to ensure an even distribution of requests through the
    load balancers and to make it feasible to send more load than a single pod’s network
    can support. One of the choices you will need to make is whether to run these
    load testing pods within the same cluster as your application or in a separate
    cluster. Running the pods within the same cluster maximizes the load that you
    can send to your application, but it does exercise the edge load balancers that
    bring traffic from the internet onto your application. Depending on which parts
    of your application you wish to test, you may want to run the load within the
    cluster, outside of the cluster, or both.
  prefs: []
  type: TYPE_NORMAL
- en: Two popular tools for running distributed load tests in Kubernetes are [JMeter](https://oreil.ly/MXBgj)
    and [Locust](https://locust.io). Both provide ways to describe the load that you
    want to send to your service and allow you to deploy distributed load test bots
    to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning Your Application Using Load Tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to using load tests to prevent performance regressions and to anticipate
    future performance problems, load testing can also be used to optimize the resource
    utilization of your application. For any given service multiple variables can
    be tuned and can impact system performance. For the purposes of this discussion
    we consider three: number of pods, number of cores, and memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first it might seem that an application would perform the same given the
    same number of replicas times cores. That is, an application with five pods, each
    with three cores, would perform the same as an application with three pods, each
    with five cores. In some cases this is true, but in many cases it is not; the
    specific details of the service and location of its bottlenecks often cause differences
    in behavior that are hard to anticipate. For example, an application built in
    a language like Java, dotnet, or Go that provides garbage collection: with one
    or two cores, the application is going to tune the garbage collector significantly
    differently than if it has many cores.'
  prefs: []
  type: TYPE_NORMAL
- en: The same thing is true of memory. More memory means that more things can be
    kept in cache, and this often leads to more performance, but this benefit has
    an asymptotic limit. You cannot simply throw more memory at a service and expect
    it to continue to improve in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Often times the only way to understand how your application will behave under
    different configurations is to actually do the experimentation. To do this properly
    you can set up an experimental set of configurations with different values for
    pods, cores, and memory and run each configuration through a load test. Using
    the data from these experiments you often can identify patterns of behavior that
    can drive insight into the particular details of your system’s performance, and
    you can use the results to select the most efficient configuration for your service.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance is a critical part of building an application that delights users.
    Load testing ensures that you do not introduce regressions that impact performance
    and lead to poor user experiences. Load testing can also serve as a time machine,
    enabling you to imagine your application’s behavior in the future and make changes
    to your architecture to support additional growth. Load testing can also help
    you understand and optimize your resource usage, lowering costs and improving
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to chaos testing and load testing, experiments are used not to discover
    problems in your service’s architecture and operation but to identify ways to
    improve how your users use your service. An experiment is a long-running change
    to your service, generally in the user experience, in which a small percentage
    of users (for example, 1% of all traffic) receive a slightly different experience.
    From examining the difference between the control (the group with no changes)
    and the experiment (the group that had a different experience) you can understand
    the impact of the changes and decide whether to continue to experiment or to roll
    out the changes more broadly.
  prefs: []
  type: TYPE_NORMAL
- en: Goals for Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we build a service, we build it with a goal in mind. That goal more often
    than not is to provide something that is useful, easy to use, and pleasing to
    our customers or users. But how can we know if we have achieved that goal? It’s
    relatively easy to see that our site breaks in the presence of chaos or that it
    can only handle a small amount of load before failing, but understanding how a
    user experiences our services can be tricky to determine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several traditional methods for understanding user experience include surveys,
    in which you ask users how they feel about the current service. While this can
    be useful in understanding the current performance of our service, it is much
    harder to use surveys to predict the impact of future changes. Much like performance
    regressions, it is far better to know the impact *before* the change is rolled
    out everywhere. That is the main goal of any experiment: to learn with minimal
    impact on our users’ experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for an Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like when we were kids in a science fair, every good experiment starts
    with a good hypothesis, and that is a natural prerequisite for our service experiments
    also. There is some change that we are thinking about making, and we need to have
    a guess as to what impact it will have on user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Of course to understand the impact on user experience, we also need to be able
    to measure the user experience. This data can come in the form of the surveys
    mentioned previously, through which you can gather metrics like satisfaction (“please
    rate us one through five”) or net promoter score (“how likely are you to recommend
    this to a friend?”). Or it can come from passive metrics associated with user
    behavior (“how long did they spend on our site?” or “how many pages did they click
    on?” etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a hypothesis and a way to measure user experience, you’re ready
    to begin the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up an Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two different ways to set up an experiment. The approach you take
    depends on the specific things being tested. You can include multiple possible
    experiences in a single service, or you can deploy two copies of your service
    and use a service mesh to direct traffic between them.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach is to check both versions of the code into your release binary
    and switch between the experiment and control using some property of the requests
    that your service is receiving. You can use HTTP headers, cookies, or query parameters
    to enable users to explicitly opt in to the experiment. Alternatively you can
    use characteristics of the requests, such as the source IP, to randomly select
    users for your experiments. For example, you could choose people for the experiments
    whose IP addresses ended in one.
  prefs: []
  type: TYPE_NORMAL
- en: A common way to implement experiments is to use explicit feature flagging where
    a user decides to opt in to an experiment by supplying the query parameter or
    cookie that turns on the experiment. This is a good way to allow specific customers
    to try new functionality or to demonstrate a new feature without releasing it
    broadly. Feature flags can also be used to rapidly turn features on or off in
    the case of instability. Numerous open source projects, for example [Flagger](https://flagger.app),
    can be used to implement feature flagging.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of placing the experiment in the same binary as your control code
    is that it is simplest to roll it out into production, but this simplicity also
    leads to two drawbacks. The first is that if the experimental code is unstable
    and crashes, it can also impact your production traffic. The other is that because
    any changes are tied to a complete release of your service it is much slower to
    make changes to update the experiment or to roll out new experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach to experiments is to deploy two (or more) different versions
    of your service. In this approach, you have the control production service that
    receives the bulk of the traffic and a separate experimental deployment of your
    service that receives only a fraction of the traffic. You can use a service mesh
    (described in [Chapter 9](ch09.html#networking_network_security_and_service_mesh))
    to route a small percentage of traffic to this experimental deployment instead
    of the production deployment. Though this approach is more complex to implement,
    it is significantly more agile and robust than including experimental code in
    your production binary. Because it requires a completely new deployment of code
    the upfront cost of setting up an experiment is increased, but because it has
    no impact on anything except the experimental traffic, you can easily deploy new
    versions of the experiment (or even multiple versions of the experiment) at any
    time without impacting the bulk of your traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, because the service mesh can measure whether requests are successful,
    if the experimental code starts failing it can quickly be removed from use and
    user impact is minimized. Of course detecting these failures can be a challenge.
    You need to make sure that the experimental infrastructure is monitored independently
    from the standard production monitoring; otherwise the experimental failures may
    be lost in successful requests that are processed by the current production infrastructure.
    Ideally the name of the pod or the deployment provides sufficient context to determine
    if the monitoring signals are from production or an experiment.
  prefs: []
  type: TYPE_NORMAL
- en: In general, using separate deployments and some sort of traffic router like
    a service mesh is the best practice for experiments, but it is a lot of infrastructure
    to set up. For your initial experiments, or if you are a small team that is already
    fairly agile, it may be that checking in experimental code is the easiest path
    to experimentation and iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experiments enable you to understand the impact of changes on your users’ experience
    before those changes are rolled out to the broad user base. Experiments play a
    critical role in helping us quickly understand what changes are possible and how
    we can update our services to better serve our users. Experiments make the improvement
    of our services easier, quicker, and safer.
  prefs: []
  type: TYPE_NORMAL
- en: Chaos Testing, Load Testing, and Experiments Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ve covered a variety of different ways to learn more about
    your service to make it more resilient, more performant, and more useful. Just
    as testing your code with unit tests is a critical part of the software development
    process, testing your service with chaos, load, and experiments is a critical
    part of service design and operation.
  prefs: []
  type: TYPE_NORMAL
