<html><head></head><body><section data-pdf-bookmark="Chapter 3. Advanced Resource Management" data-type="chapter" epub:type="chapter"><div class="chapter" id="advanced_resource_management">&#13;
<h1><span class="label">Chapter 3. </span>Advanced Resource Management</h1>&#13;
<p>Managing available resources is a critical aspect of effectively running <a contenteditable="false" data-primary="resource management" data-secondary="about" data-type="indexterm" id="idm45358201795640"/><a contenteditable="false" data-primary="service-level agreements (SLAs)" data-secondary="resource management aspect" data-type="indexterm" id="idm45358201794264"/><a contenteditable="false" data-primary="allocatable resources" data-seealso="resource management" data-type="indexterm" id="idm45358201792808"/><a contenteditable="false" data-primary="management of resources" data-see="resource management" data-type="indexterm" id="idm45358201791432"/><a contenteditable="false" data-primary="pods" data-secondary="resource management" data-see="resource management" data-type="indexterm" id="idm45358201790056"/>Kubernetes workloads in production. Without the proper sizing of workloads and management of CPU, memory, disk, graphics processing units (GPUs), pods, containers, and other resources, it is impossible for engineers and operations teams to control the service-level agreement (SLA) of applications and services between a client and provider. <a contenteditable="false" data-primary="service-level agreements (SLAs)" data-secondary="about" data-type="indexterm" id="idm45358201787928"/>This SLA is the defining contract that determines the level of availability and performance that the client can expect from the system and is often backed by financial penalties.</p>&#13;
<p>We’ll discuss a variety of tools and techniques available to the Kubernetes engineer for controlling the allocation of these resources. In this chapter, we begin with a discussion of proper scheduling of pod resources, where we cover topics like priority scheduling, quality of service, and the impact resource limits can have on scheduling pods. Next, we provide an overview of capacity planning and management approaches for ensuring the scalability of your Kubernetes platform. We then conclude this chapter with a discussion of admission controllers and how they can be used to enforce additional constraints on resources.</p>&#13;
<section data-pdf-bookmark="Pod Resources and Scheduling" data-type="sect1"><div class="sect1" id="pod_resources_and_scheduling">&#13;
<h1>Pod Resources and Scheduling</h1>&#13;
<p>Proper scheduling of workload is critical to maintaining the availability <a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="about" data-type="indexterm" id="idm45358201782984"/><a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="about" data-type="indexterm" id="idm45358201781336"/><a contenteditable="false" data-primary="errors" data-secondary="insufficient memory" data-type="indexterm" id="idm45358201779688"/><a contenteditable="false" data-primary="memory utilization" data-secondary="scheduling worker nodes" data-type="indexterm" id="idm45358201778312"/><a contenteditable="false" data-primary="worker nodes" data-secondary="memory errors" data-type="indexterm" id="idm45358201776936"/>and performance of your applications. Without effective scheduling, you can end up with an overloaded worker node with insufficient memory and CPU resources. The most desired outcome in these situations is a graceful shutdown of workload instances that have additional replicas running in the system, resulting in little or no service disruption. <a contenteditable="false" data-primary="Linux operating system" data-secondary="Out of Memory Killer" data-type="indexterm" id="idm45358201775064"/><a contenteditable="false" data-primary="Out of Memory (OOM) Killer" data-secondary="about" data-type="indexterm" id="idm45358201773688"/>In contrast, the worst-case scenario is that the Linux Out of Memory (OOM) Killer<sup><a data-type="noteref" href="ch03.html#ch01fn21" id="ch01fn21-marker">1</a></sup> comes through and starts randomly destroying processes. <a contenteditable="false" data-primary="kubelet" data-secondary="destroying worker node" data-type="indexterm" id="idm45358201770888"/>In extreme cases, an improperly configured <code>kubelet</code> component on a Kubernetes node could actually destroy the worker node itself.</p>&#13;
<section data-pdf-bookmark="Driving Scheduler Decisions via Resource Requests" data-type="sect2"><div class="sect2" id="driving_scheduler_decisions_via_resource">&#13;
<h2>Driving Scheduler Decisions via Resource Requests</h2>&#13;
<p>One of the key mechanisms that Kubernetes uses for making scheduling <a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="resource requests driving" data-type="indexterm" id="idm45358201766904"/><a contenteditable="false" data-primary="resource requests" data-secondary="scheduling driven by" data-type="indexterm" id="idm45358201765208"/><a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="resource requests driving" data-type="indexterm" id="idm45358201763832"/><a contenteditable="false" data-primary="containers" data-secondary="resource requests" data-type="indexterm" id="idm45358201762216"/>decisions is the resource request construct. A <em>resource request</em> for a container is the mechanism that the developer uses to inform Kubernetes how much CPU, memory, and disk resources will be needed to run the associated container. <a contenteditable="false" data-primary="resource management" data-secondary="documentation online" data-type="indexterm" id="idm45358201760040"/><a contenteditable="false" data-primary="online resources" data-secondary="Kubernetes" data-tertiary="resource management documentation" data-type="indexterm" id="idm45358201758664"/><a contenteditable="false" data-primary="resources online" data-see="online resources" data-type="indexterm" id="idm45358201757000"/><a contenteditable="false" data-primary="resource requests" data-secondary="documentation online" data-type="indexterm" id="idm45358201755624"/>The <a href="https://oreil.ly/Sxnzu">Kubernetes official documentation on pod resources</a> provides an excellent overview of resource requests. <a contenteditable="false" data-primary="YAML files" data-secondary="pod resource requests" data-type="indexterm" id="idm45358201753272"/><a contenteditable="false" data-primary="resource requests" data-secondary="YAML file" data-type="indexterm" id="idm45358201751928"/>Let’s take a look at a basic Pod that uses resource requests:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: frontend&#13;
spec:&#13;
  containers:&#13;
  - name: web&#13;
    image: icr.io/sample/web:v1&#13;
    env:&#13;
    resources:&#13;
      requests:&#13;
        memory: "50Mi"&#13;
        cpu: "150m"&#13;
        ephemeral-storage: “50Mi”&#13;
  - name: logging&#13;
    image: icr.io/sample/logging:v2&#13;
    resources:&#13;
      requests:&#13;
        memory: "40Mi"&#13;
        cpu: "100m"&#13;
        ephemeral-storage: “200Mi”</pre>&#13;
<p>All of this information provided in the pod and container definition are the hints used by the Kubernetes scheduler for placement in the cluster. When we look more closely at scheduling, we’ll gain a better understanding of how the scheduler processes this information.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Node Available Resources" data-type="sect2"><div class="sect2" id="node_available_resources">&#13;
<h2>Node Available Resources</h2>&#13;
<p>During the scheduling process, Kubernetes is looking for nodes that fit the <a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="node available resources" data-type="indexterm" id="idm45358201746536"/><a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="node-allocatable resources" data-type="indexterm" id="idm45358201744824"/><a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="node-allocatable resources" data-type="indexterm" id="idm45358201743160"/><a contenteditable="false" data-primary="allocatable resources" data-type="indexterm" id="idm45358201741496"/><a contenteditable="false" data-primary="worker nodes" data-secondary="allocatable resources" data-type="indexterm" id="idm45358201740392"/><a contenteditable="false" data-primary="container runtime" data-secondary="allocatable resources" data-type="indexterm" id="idm45358201739016"/>requested resources of the pod to be scheduled. To determine this, the scheduler is looking at allocatable resources minus allocated resources to find available resources. <em>Allocatable resources</em> are the resources that can be consumed by user-controlled pods on a worker node. In Kubernetes, the allocatable resource quantity for a worker node is defined by the total resource available in the node minus the capacity that is reserved for system daemons and <a contenteditable="false" data-primary="Kubernetes" data-secondary="system daemon documentation online" data-type="indexterm" id="idm45358201736632"/>Kubernetes runtime components.<sup><a data-type="noteref" href="ch03.html#ch01fn22" id="ch01fn22-marker">2</a></sup></p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
&#13;
<p>The <code>--kube-reserved</code> and <code>--system-reserved</code> <code>kubelet</code> flags are critical to maintaining the <a contenteditable="false" data-primary="--kube-reserved flag" data-primary-sortas="kube-reserved flag" data-type="indexterm" id="idm45358201731032"/><a contenteditable="false" data-primary="--system-reserved flag" data-primary-sortas="system-reserved flag" data-type="indexterm" id="idm45358201729656"/><a contenteditable="false" data-primary="production" data-secondary="resource reservations" data-type="indexterm" id="idm45358201728280"/><a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="resource reservations" data-type="indexterm" id="idm45358201726904"/><a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="resource reservations" data-type="indexterm" id="idm45358201725256"/>stability of your worker nodes. Without proper configuration, the user pods can easily overwhelm the resources available on the node and start to compete with system processes and <code>kubelet</code>, kube-proxy, and other container runtime components for resources. We recommend a healthy reservation for kube and system components to ensure the health of the node when faced with hungry user container components.</p>&#13;
</div>&#13;
<p>For administrators managing Kubernetes in production, it’s critical to properly configure these resource reservations for both the Kubernetes platform and the system. What’s the best way to determine how to set these flags? Experience and real-world testing are invaluable. Trial and error aren’t fun, but given how varied every Kubernetes worker may be, it’s worth spending some time testing out various settings. A good starting place may lie with your friendly cloud provider. <a contenteditable="false" data-primary="online resources" data-secondary="Kubernetes" data-tertiary="worker node resource reservations" data-type="indexterm" id="idm45358201721704"/><a contenteditable="false" data-primary="IBM Cloud" data-secondary="resource reservations" data-type="indexterm" id="idm45358201719992"/>IBM Cloud has done extensive scale testing and evaluation of production systems to come up with <a href="https://oreil.ly/7g71U">a set of safe resource reservations</a>. As mentioned in the upstream documentation, these reservations are largely based on pod limits per node, kube resource reservations, and kernel memory for system resource reservations. If your configuration has any significant system-level runtime components, you may need to adjust.</p>&#13;
<p>These reservations play two key roles. Here we are looking at how they impact allocatable resources for each worker. Later in this chapter, we’ll talk about the post-scheduling life cycle where these reservations play a role in the life cycle of a pod with pod quality of service.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Scheduling" data-type="sect2"><div class="sect2" id="scheduling">&#13;
<h2>Scheduling</h2>&#13;
<p>We’re now armed with critical knowledge for the scheduling process with <a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="process of scheduling" data-type="indexterm" id="idm45358201714728"/><a contenteditable="false" data-primary="worker nodes" data-secondary="scheduling process" data-type="indexterm" id="idm45358201713080"/><a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="scheduling process" data-type="indexterm" id="idm45358201711704"/>resource reservations and node-allocatable resources. Finding a node that fits our resource requirements is not the only metric at play in scheduling.</p>&#13;
<p class="pagebreak-before less_space">Kubernetes uses several different factors to determine the placement of pods on worker nodes. Let’s cover some of the basic concepts of scheduling and how you can leverage this knowledge to build applications and services with higher availability and performance.</p>&#13;
<p>The <a href="https://oreil.ly/jca9A">kube-scheduler</a> uses<a contenteditable="false" data-primary="kube-scheduler" data-secondary="scheduling process" data-type="indexterm" id="idm45358201707416"/><a contenteditable="false" data-primary="predicates for placing pods on worker nodes" data-type="indexterm" id="idm45358201706008"/><a contenteditable="false" data-primary="priorities for placing pods on worker nodes" data-type="indexterm" id="idm45358201704872"/> a two-phase process that first filters for nodes that can run the pod and then scores the filtered nodes to determine which is a best fit. A number of <a href="https://oreil.ly/oVThs">predicates</a> are used to determine if a node is fit for running a given pod.  The <a href="https://oreil.ly/NA3Z9">priorities</a> then rank the remaining nodes to determine final placement. There are numerous predicates and policies. We’ll cover a few that have the greatest impact on the day-to-day operation of a Kubernetes or OpenShift cluster:</p>&#13;
<dl>&#13;
<dt><code>PodFitsResources</code></dt>&#13;
<dd>The most commonly considered predicate, <code>PodFitsResources</code> <a contenteditable="false" data-primary="PodFitsResources predicate" data-type="indexterm" id="idm45358201699912"/>evaluates the resource requests of the pods and filters out any nodes that do not have sufficient available resources. There may be instances where there are sufficient total available resources in the cluster but not one node has enough resources available to fit the pod. We’ll discuss pod priority and preemption, which can assist with this, in the next section.</dd>&#13;
<dt><code>PodMatchNodeSelector</code></dt>&#13;
<dd>While seemingly not that complex, this predicate is responsible for <a contenteditable="false" data-primary="PodMatchNodeSelector predicate" data-type="indexterm" id="idm45358201697304"/>handling all of the pod and node affinity and anti-affinity rules that can be specified. These rules are critical for handling blast radius control and the availability of applications across zones.</dd>&#13;
<dt><code>PodToleratesNodeTaints</code></dt>&#13;
<dd>Taints are often used to isolate groups of nodes that may be <a contenteditable="false" data-primary="PodToleratesNodeTaints" data-type="indexterm" id="idm45358201694776"/><a contenteditable="false" data-primary="taints" data-type="indexterm" id="idm45358201693592"/>dedicated to specific workloads in a cluster. In some cases, administrators may use this to reserve a set of nodes for a specific namespace or tenant.<sup><a data-type="noteref" href="ch03.html#ch01fn23" id="ch01fn23-marker">3</a></sup></dd>&#13;
</dl>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
&#13;
<p>When you think about availability, you should consider the “blast radius” of any specific failure.<a contenteditable="false" data-primary="errors" data-secondary="node failure" data-type="indexterm" id="idm45358201689528"/><a contenteditable="false" data-primary="failure" data-secondary="node failure" data-type="indexterm" id="idm45358201688152"/><a contenteditable="false" data-primary="node failure" data-type="indexterm" id="idm45358201686776"/><a contenteditable="false" data-primary="debugging" data-secondary="node failure" data-type="indexterm" id="idm45358201685672"/><a contenteditable="false" data-primary="control plane failure" data-secondary="failure blast radius" data-type="indexterm" id="idm45358201684296"/> If a node fails, all pods on that node will become unavailable. If an availability zone fails, all nodes supporting the control plane and the application pods will become unavailable. When you think about the “blast radius,” it really is just a way of reasoning about the impact (and the cascading effects) of that failure.</p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Pod Priority and Preemption" data-type="sect2"><div class="sect2" id="pod_priority_and_preemption">&#13;
<h2>Pod Priority and Preemption</h2>&#13;
<p>There are occasions where a pod with resource needs that cannot <a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="pod priority and preemption" data-type="indexterm" id="idm45358201680248"/><a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="pod priority and preemption" data-type="indexterm" id="idm45358201678456"/>be met needs to be scheduled. If <a href="https://oreil.ly/nrgbg">pod priority and preemption</a> are used, the scheduler can evict lower-priority pods to make room for the higher-priority pod. Priority classes will also factor into which pods will be scheduled first. If a high-priority pod and a low-priority pod are both pending, the low-priority pod will not be scheduled until the high-priority pod is running.</p>&#13;
<p>One interesting use case for priority classes is to have pods with larger resource requests be assigned to the higher priority class. In some situations, this can help by moving smaller pods onto other nodes and making room for a larger pod to fit. <a contenteditable="false" data-primary="Pending state" data-secondary="pod priorities in scheduling" data-type="indexterm" id="idm45358201674936"/>While this can result in smaller, lower-priority pods being unscheduled and stuck in Pending, it does help to better use the capacity of your cluster by compressing smaller pods into the gaps available on nodes. Let’s consider an example.</p>&#13;
<p>Initial state:</p>&#13;
<ul>&#13;
<li><p>Pod P has resource requests of CPU: 1,000, priority 100</p></li>&#13;
<li><p>Pod Q has resource requests of CPU: 100, priority 10</p></li>&#13;
<li><p>Node N has 900 available CPU out of 1,000 total</p></li>&#13;
<li><p>Node O has 300 available CPU out of 300 total</p></li>&#13;
<li><p>Pod P is Pending</p></li>&#13;
<li><p>Pod Q is Running on Node N</p></li>&#13;
</ul>&#13;
<p>In this example, Pod P will not fit on either Node N or Node O. However, because Pod P has higher priority than Pod Q, Q will be evicted from Node N; thus, Pod P can fit onto Node N and be scheduled. Pod Q will then enter the scheduler again and can fit on Node O. The result is that both pods are now scheduled.</p>&#13;
<p>End state:</p>&#13;
<ul>&#13;
<li><p>Pod P is running on Node N</p></li>&#13;
<li><p>Pod Q is running on Node O</p></li>&#13;
<li><p>Node N has 0 available CPU</p></li>&#13;
<li><p>Node O has 200 available CPU</p></li>&#13;
</ul>&#13;
<p>This may not be a common use case for users, but it is an interesting process that can be used to maximize the utilization of the available node resources.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Post-Scheduling Pod Life Cycle" data-type="sect1"><div class="sect1" id="post_scheduling_pod_life_cycle">&#13;
<h1>Post-Scheduling Pod Life Cycle</h1>&#13;
<p>Now that our pod has been scheduled to a node, we’re done, right? <a contenteditable="false" data-primary="resource management" data-secondary="post-scheduling pod life cycle" data-tertiary="about" data-type="indexterm" id="idm45358201662280"/><a contenteditable="false" data-primary="scheduling" data-secondary="post-scheduling pod life cycle" data-tertiary="about" data-type="indexterm" id="idm45358201660488"/>Not so. Once the pod is running on a node, a number of factors will determine the ongoing life cycle of the pod. Kubernetes controls the resource consumption of pods, may evict pods to protect the health of the node, and may even preempt a running pod to make way for a higher-priority pod, as discussed in <a data-type="xref" href="#scheduling">“Scheduling”</a>.</p>&#13;
<p>We’ve already reviewed resource requests, which are used for making scheduling decisions in Kubernetes. <a contenteditable="false" data-primary="resource limits" data-type="indexterm" id="idm45358201656840"/>Once pods are in the Running state, resource limits are the attributes most critical to the pod’s life cycle. It’s worth noting that resource requests continue to serve a valuable purpose as they help to determine the quality of service of the pod and are factored into eviction decisions.</p>&#13;
<section data-pdf-bookmark="Pod Quality of Service" data-type="sect2"><div class="sect2" id="pod_quality_of_service">&#13;
<h2>Pod Quality of Service</h2>&#13;
<p>The intersection of requests and limits is <a href="https://oreil.ly/QukmD"><em>quality of service</em> (QoS)</a>.<a contenteditable="false" data-primary="resource management" data-secondary="post-scheduling pod life cycle" data-tertiary="pod quality of service" data-type="indexterm" id="ch03-qos"/><a contenteditable="false" data-primary="quality of service (QoS)" data-type="indexterm" id="ch03-qos2"/><a contenteditable="false" data-primary="resource management" data-secondary="overcommit" data-type="indexterm" id="idm45358201648824"/><a contenteditable="false" data-primary="quality of service (QoS)" data-secondary="overcommits" data-type="indexterm" id="idm45358201647448"/><a contenteditable="false" data-primary="overcommit" data-type="indexterm" id="idm45358201646056"/><a contenteditable="false" data-primary="VMs and overcommit" data-type="indexterm" id="idm45358201644952"/><a contenteditable="false" data-primary="scheduling" data-secondary="post-scheduling pod life cycle" data-tertiary="pod quality of service" data-type="indexterm" id="ch03-qos3"/> QoS does not have any impact on scheduling; only requests are factored here. However, QoS does determine the pod eviction selection process and what we would have called overcommit in the world of virtualized infrastructure. There are yet other factors in eviction that we’ll discuss in detail in <a data-type="xref" href="#node_eviction">“Node Eviction”</a>.</p>&#13;
<p>Before containers, there were VMs. A VM was a huge leap forward in efficiency for developers and operations teams alike. The VM allowed users to take a single physical computer system and subdivide it into multiple logical operating system instances. If you ever worked with a VM infrastructure management platform, especially OpenStack, then you have likely run across the concept of overcommit. Overcommit is a number or multiplier that determines how much more CPU, memory, and disk would be allocated than is actually available for a given node. Kubernetes does not have the notion of overcommit, but rather QoS. However, you will notice some similarities between the two.</p>&#13;
<p>In the world of VMs, the VM creator picks a CPU and memory size for a given VM, and that is how much memory is carved out of the physical host system for that VM: no more, no less. <a contenteditable="false" data-primary="resource requests" data-secondary="overcommit" data-type="indexterm" id="idm45358201638888"/>In Kubernetes, the creator of a pod or container can choose how much CPU and memory they would like to have for their pod or container (resource request) and chooses limits separately. This allows for more efficient utilization of resources in the Kubernetes cluster where there are pods that are less sensitive to their available CPU and memory. The delta between the resource requests and resource limits is how Kubernetes provides the ability to overcommit node resources.</p>&#13;
<p>It’s important to note that some resources are compressible and some are incompressible. What does this mean? Well, unless it’s 1990 and you are running RAM Doubler and Disk Doubler, then these resources are finite and cannot be shared. <a contenteditable="false" data-primary="resource management" data-secondary="incompressible resources" data-type="indexterm" id="idm45358201636216"/><a contenteditable="false" data-primary="incompressible resources" data-type="indexterm" id="idm45358201634872"/>These nonshareable resources (RAM and disk) are known as <em>incompressible</em>. <a contenteditable="false" data-primary="resource management" data-secondary="compressible resources" data-type="indexterm" id="idm45358201633128"/><a contenteditable="false" data-primary="compressible resources" data-type="indexterm" id="idm45358201631720"/>When there is competition for disk or memory, then processes will lose out and be evicted to make room for other processes. However, CPU can be split, or compressed, to allow multiple processes to compete for CPU cycles. Let’s say that there are two processes that each wants to do one thousand pieces of work per time unit. If the CPU can do one thousand cycles per unit of time, then both processes continue to run, but they will take double the time to complete their task.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Pod QoS Levels" data-type="sect2"><div class="sect2" id="pod_qos_levels">&#13;
<h2>Pod QoS Levels</h2>&#13;
<p>Now that we have completed our detailed investigation of resource types, we can discuss <a contenteditable="false" data-primary="quality of service (QoS)" data-secondary="levels of" data-type="indexterm" id="idm45358201627640"/><a contenteditable="false" data-primary="Guaranteed level of QoS" data-type="indexterm" id="idm45358201626296"/><a contenteditable="false" data-primary="Burstable level of QoS" data-type="indexterm" id="idm45358201625192"/><a contenteditable="false" data-primary="BestEffort level of QoS" data-type="indexterm" id="idm45358201624088"/><a contenteditable="false" data-primary="resource limits" data-secondary="QoS levels" data-type="indexterm" id="idm45358201622968"/><a contenteditable="false" data-primary="resource requests" data-secondary="QoS levels" data-type="indexterm" id="idm45358201621592"/>QoS a bit further. There are three QoS levels. <em>Guaranteed</em> is the highest level of QoS. These are pods that have their resource requests and limits set to the same value for all resources. <em>Burstable</em> pods have requests set, but their limit values are higher, which permits them to consume more resources if needed. <em>BestEffort</em> pods have no requests or limits set. Following are some examples of resource settings for container specs within a pod:</p>&#13;
<p>Here is a Guaranteed example:</p>&#13;
<pre data-type="programlisting">&#13;
resources:&#13;
   limits:&#13;
     memory: "200Mi"&#13;
   requests:&#13;
     memory: "200Mi"</pre>&#13;
<p>Here is a Burstable example:</p>&#13;
<pre data-type="programlisting">&#13;
resources:&#13;
   limits:&#13;
     memory: "200Mi"&#13;
   requests:&#13;
     memory: "100Mi"</pre>&#13;
<p>And here is a BestEffort example:</p>&#13;
<pre data-type="programlisting">resources: {}</pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>When <code>requests</code> and <code>limits</code> are equal, you do not need to explicitly set both. If only the limit is set, then the requests are automatically assumed to be the same as the limit by the Kubernetes scheduler.</p>&#13;
</div>&#13;
<p>Remember, we mentioned that if there is competition for memory or disk, <a contenteditable="false" data-primary="quality of service (QoS)" data-secondary="processes to be killed" data-type="indexterm" id="idm45358201612424"/>then one or more processes will be killed. How does Kubernetes decide which processes to kill? It uses QoS to make this decision. If a Kubernetes worker node comes under resource pressure, then it will first kill off BestEffort pods, then Burstable, and then Guaranteed. For a Guaranteed pod to be evicted from a node, it would require some system-level resource pressures.</p>&#13;
<p>Because this QoS is defined on a container-by-container basis and is under the container creator’s control, we have the opportunity for much higher resource utilization without putting our critical containers at risk of being starved for resources or potentially having their performance diminished. We get the best of both worlds in <span class="keep-together">Kubernetes</span>.</p>&#13;
<p>What about pod priority? It sounds a lot like QoS. However, as we saw in <a data-type="xref" href="#scheduling">“Scheduling”</a>, <a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="pod priority and preemption" data-type="indexterm" id="idm45358201607512"/><a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="pod priority and preemption" data-type="indexterm" id="idm45358201605816"/>pod priority affects preemption during scheduling but does not affect the eviction algorithms.</p>&#13;
<p>The takeaway from all this QoS discussion is that your developers’ <a contenteditable="false" data-primary="clusters" data-secondary="behavior from resource configuration" data-type="indexterm" id="idm45358201603496"/><a contenteditable="false" data-primary="debugging" data-secondary="pods constantly dying" data-type="indexterm" id="idm45358201602024"/><a contenteditable="false" data-primary="errors" data-secondary="pods constantly dying" data-type="indexterm" id="idm45358201600648"/>configuration of resource requests and limits will have a significant impact on how your cluster behaves and handles pods. It’s also worth noting that using anything other than Guaranteed QoS can make debugging your workloads and managing capacity very difficult. Your developers keep asking why their pods are constantly dying, only to find out that they are being evicted to make room for higher QoS pods from other teams. As an admin, you are trying to figure out how to manage the capacity of your cluster, but you can’t tell how much room is really available because half of your pods are Burstable. Yes, there are monitoring tools that will calculate the cluster’s true available capacity based on allocatable resources versus requested resources on all your nodes, but your users are in for a rude awakening when their Burstable capacity starts getting reclaimed to make way for Guaranteed pods. Sure, you may end up leaving a bit of CPU or memory on the table, but your cluster admin and your development teams will have a much more predictable result in production.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
&#13;
<p>Avoid the use of BestEffort QoS at all costs in production. It can <a contenteditable="false" data-primary="production" data-secondary="BestEffort QoS avoided" data-type="indexterm" id="idm45358201597032"/><a contenteditable="false" data-primary="BestEffort level of QoS" data-secondary="production use avoided" data-type="indexterm" id="idm45358201595480"/>result in very unpredictable scheduling. The result is an environment that may be very unstable as pods compete for resources.<a contenteditable="false" data-primary="" data-startref="ch03-qos" data-type="indexterm" id="idm45358201593816"/><a contenteditable="false" data-primary="" data-startref="ch03-qos2" data-type="indexterm" id="idm45358201592440"/><a contenteditable="false" data-primary="" data-startref="ch03-qos3" data-type="indexterm" id="idm45358201591064"/></p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Testing Resource Limits" data-type="sect2"><div class="sect2" id="testing_resource_limits">&#13;
<h2>Testing Resource Limits</h2>&#13;
<p>Limits control how many resources a container is given access to after it is running.<a contenteditable="false" data-primary="scheduling" data-secondary="post-scheduling pod life cycle" data-tertiary="testing resource limits" data-type="indexterm" id="ch03-rclim"/><a contenteditable="false" data-primary="resource management" data-secondary="post-scheduling pod life cycle" data-tertiary="testing resource limits" data-type="indexterm" id="ch03-rclim2"/><a contenteditable="false" data-primary="resource limits" data-secondary="testing" data-type="indexterm" id="ch03-rclim3"/><a contenteditable="false" data-primary="requests" data-see="resource requests" data-type="indexterm" id="idm45358201582168"/><a contenteditable="false" data-primary="Linux operating system" data-secondary="testing resource limits" data-type="indexterm" id="ch03-rclim4"/><a contenteditable="false" data-primary="containerd" data-type="indexterm" id="idm45358201579144"/><a contenteditable="false" data-primary="CRI-O" data-type="indexterm" id="idm45358201578040"/> Various container runtimes may have different methods for implementing this. For our purposes, we’ll focus on traditional Linux container runtimes. The following rules apply for the <a href="https://cri-o.io">CRI-O</a> and <a href="https://containerd.io">containerd</a> container runtimes. The basic implementation of limits for CPU and memory are implemented using Linux control groups (cgroups). Specifically, in these examples, we are using an OpenShift 4.7 cluster and the CRI-O 1.17.4-19 container runtime.</p>&#13;
<section class="pagebreak-before" data-pdf-bookmark="CPU limits" data-type="sect3"><div class="sect3" id="cpu_limits">&#13;
<h3 class="less_space">CPU limits</h3>&#13;
<p>Let’s see what this means for actual running pods. We will start by looking at <a contenteditable="false" data-primary="Linux operating system" data-secondary="testing resource limits" data-tertiary="CPU limits" data-type="indexterm" id="idm45358201572392"/><a contenteditable="false" data-primary="resource management" data-secondary="testing resource limits" data-tertiary="CPU limits" data-type="indexterm" id="idm45358201570648"/><a contenteditable="false" data-primary="resource management" data-secondary="compressible resources" data-tertiary="testing CPU limits" data-type="indexterm" id="idm45358201569000"/><a contenteditable="false" data-primary="compressible resources" data-secondary="testing CPU limits" data-type="indexterm" id="idm45358201567352"/><a contenteditable="false" data-primary="CPU utilization" data-secondary="CPU limits tested" data-type="indexterm" id="idm45358201565976"/><a contenteditable="false" data-primary="Burstable level of QoS" data-secondary="testing CPU limits" data-type="indexterm" id="idm45358201564600"/>compressible CPU resources in a Burstable configuration, as it is the most interesting. We have created a test deployment we can use to scale and view the impact on our CPU resources:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
 labels:&#13;
   run: cpu-use&#13;
 name: cpu-use&#13;
spec:&#13;
 replicas: 1&#13;
 selector:&#13;
   matchLabels:&#13;
     run: cpu-use&#13;
 template:&#13;
   metadata:&#13;
     labels:&#13;
       run: cpu-use&#13;
   spec:&#13;
     containers:&#13;
     - command:&#13;
       - stress&#13;
       - --cpu&#13;
       - "5"&#13;
       image: kitch/stress&#13;
       imagePullPolicy: Always&#13;
       name: cpu-use&#13;
       resources:&#13;
         limits:&#13;
           cpu: 1000m&#13;
         requests:&#13;
           cpu: 200m&#13;
     nodeSelector:&#13;
       kubernetes.io/hostname: "&lt;worker node&gt;"</pre>&#13;
<p>This sample will let us scale our workload up and down on a four-vCPU worker node and see what the impact is on CPU.</p>&#13;
<p>At three replicas, all pods are hitting their CPU limit:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods</strong>&#13;
NAME                      CPU(cores)   MEMORY(bytes)&#13;
cpu-use-ffd7fd8f8-b2wds   998m         0Mi&#13;
cpu-use-ffd7fd8f8-cw6lz   999m         0Mi&#13;
cpu-use-ffd7fd8f8-wcn2x   999m         0Mi</pre>&#13;
<p>But when we scale up to higher numbers of pods, we can see there is competition for resources and the cgroups start slicing the CPU thinner. And if we max out the schedulable pods based on our CPU request of 200m, we still end up with even distribution of CPU:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods</strong>&#13;
NAME                       CPU(cores)   MEMORY(bytes)&#13;
cpu-use-575444f9c6-2fctp   264m         0Mi&#13;
cpu-use-575444f9c6-4x2w6   264m         0Mi&#13;
cpu-use-575444f9c6-89q8z   263m         0Mi&#13;
cpu-use-575444f9c6-bw6fl   265m         0Mi&#13;
cpu-use-575444f9c6-dq4pn   265m         0Mi&#13;
cpu-use-575444f9c6-g968p   265m         0Mi&#13;
cpu-use-575444f9c6-jmpwl   265m         0Mi&#13;
cpu-use-575444f9c6-ktmbp   264m         0Mi&#13;
cpu-use-575444f9c6-lmjlz   265m         0Mi&#13;
cpu-use-575444f9c6-rfvx6   264m         0Mi&#13;
cpu-use-575444f9c6-rg77n   264m         0Mi&#13;
cpu-use-575444f9c6-skt25   263m         0Mi&#13;
cpu-use-575444f9c6-srhhf   264m         0Mi&#13;
cpu-use-575444f9c6-svz9z   264m         0Mi</pre>&#13;
<p>Now let’s take a look at what happens when we add BestEffort load.<a contenteditable="false" data-primary="BestEffort level of QoS" data-secondary="testing CPU limits" data-type="indexterm" id="idm45358201556376"/> Let’s start with <code>cpu-noise</code>, which has no requests or limits (BestEffort) and has enough load to consume five vCPU if available. We start with the following load:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods</strong>&#13;
NAME                         CPU(cores)   MEMORY(bytes)&#13;
cpu-noise-6575cc6657-2qhl8   3724m        0Mi</pre>&#13;
<p>Once we add a <code>cpu-use</code> pod to the mix with requests and limits, this new pod is given not only its requested CPU, but also its limit:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods</strong>&#13;
NAME                         CPU(cores)   MEMORY(bytes)&#13;
cpu-noise-6575cc6657-2qhl8   2491m        0Mi&#13;
cpu-use-679cbc8b6d-95bpk     999m         0Mi</pre>&#13;
<p>Finally, we scale up <code>cpu-use</code> and get to see the real difference between Burstable and BestEffort QoS:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods</strong>&#13;
NAME                         CPU(cores)   MEMORY(bytes)&#13;
cpu-noise-6575cc6657-2qhl8   7m           0Mi&#13;
cpu-use-679cbc8b6d-6nnkp     850m         0Mi&#13;
cpu-use-679cbc8b6d-n6gwp     844m         0Mi&#13;
cpu-use-679cbc8b6d-rl7vv     863m         0Mi&#13;
cpu-use-679cbc8b6d-z7hhb     865m         0Mi</pre>&#13;
<p>In these results, we see that the Burstable pods are well past their requested CPU resources, but the <code>cpu-noise</code> BestEffort pod is just getting scraps.</p>&#13;
<p>This is the part where you take note and remember that CPU requests are your friend. You’ll be ensuring that your pod won’t be at the bottom of the CPU barrel.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Memory limits" data-type="sect3"><div class="sect3" id="memory_limits">&#13;
<h3>Memory limits</h3>&#13;
<p>We’ve taken a closer look at the rather interesting control of compressible CPU resources. <a contenteditable="false" data-primary="Linux operating system" data-secondary="testing resource limits" data-tertiary="memory limits" data-type="indexterm" id="idm45358201544840"/><a contenteditable="false" data-primary="resource management" data-secondary="testing resource limits" data-tertiary="memory limits" data-type="indexterm" id="idm45358201543192"/><a contenteditable="false" data-primary="resource management" data-secondary="incompressible resources" data-tertiary="testing memory limits" data-type="indexterm" id="idm45358201541544"/><a contenteditable="false" data-primary="incompressible resources" data-secondary="testing memory limits" data-type="indexterm" id="idm45358201539832"/><a contenteditable="false" data-primary="memory utilization" data-secondary="memory limits tested" data-type="indexterm" id="idm45358201538440"/>It’s worth looking at memory, but it is not quite as interesting. Let’s get started with our <code>memory-use</code> workload first:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  labels:&#13;
    app: memory-use&#13;
  name: memory-use&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: memory-use&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: memory-use&#13;
    spec:&#13;
      containers:&#13;
      - command:&#13;
        - stress&#13;
        - --cpu&#13;
        - "1"&#13;
        - --vm&#13;
        - "5"&#13;
        - --vm-keep&#13;
        image: kitch/stress&#13;
        imagePullPolicy: Always&#13;
        name: memory-use&#13;
        resources:&#13;
          limits:&#13;
            cpu: 10m&#13;
            memory: 1290Mi&#13;
          requests:&#13;
            cpu: 10m&#13;
            memory: 1290Mi&#13;
      nodeSelector:&#13;
        kubernetes.io/hostname: "10.65.59.69"</pre>&#13;
<p>The result is that we can fit right around six pods per 16 GB host after accounting for node reservations and other critical pods:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods</strong>&#13;
NAME                          CPU(cores)   MEMORY(bytes)&#13;
memory-use-66b45dbd56-j4jj7   3m           943Mi&#13;
memory-use-774b6549db-bqpj5   9m           1280Mi&#13;
memory-use-774b6549db-k9f78   9m           1280Mi&#13;
memory-use-774b6549db-qmq62   9m           1280Mi&#13;
memory-use-774b6549db-vtm96   9m           1280Mi&#13;
memory-use-774b6549db-wwj2r   9m           1280Mi</pre>&#13;
<p>If we start to apply memory pressure with our <code>memory-noise</code> deployment (same as above, just no resource requests or limits), then we’ll start to see eviction and OOM Killer take over; our Guaranteed pods are spared in the mayhem:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods</strong>&#13;
NAME                            READY   STATUS    RESTARTS   AGE&#13;
memory-noise-85df694f5d-2szg2   0/1     Evicted   0          12m&#13;
memory-noise-85df694f5d-598mz   1/1     Running   1          3m49s&#13;
memory-noise-85df694f5d-7njvb   1/1     Running   1          7m23s&#13;
memory-noise-85df694f5d-7pjjc   0/1     Evicted   0          12m&#13;
memory-noise-85df694f5d-8vl8h   0/1     Evicted   0          12m&#13;
memory-noise-85df694f5d-7njvb   0/1     OOMKilled 1          8m23s&#13;
memory-use-774b6549db-bqpj5     1/1     Running   0          59m&#13;
memory-use-774b6549db-k9f78     1/1     Running   0          62m&#13;
memory-use-774b6549db-qmq62     1/1     Running   0          62m&#13;
memory-use-774b6549db-vtm96     1/1     Running   0          62m&#13;
memory-use-774b6549db-wwj2r     1/1     Running   0          62m</pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>“OOMKilled” is a status condition indicating that the process <a contenteditable="false" data-primary="Out of Memory (OOM) Killer" data-secondary="memory limits tested" data-type="indexterm" id="idm45358201528904"/><a contenteditable="false" data-primary="OOM Killer" data-see="Out of Memory (OOM) Killer" data-type="indexterm" id="idm45358201527336"/><a contenteditable="false" data-primary="Linux operating system" data-secondary="Out of Memory Killer" data-type="indexterm" id="idm45358201525944"/>running within the pod was killed because it reached an OOM error. The supporting node could have had all memory exhausted because of unrestricted pods running on the same compute host, or the container process could have reached its specified memory limit (and was thus terminated). The oom_killer is a process in the Linux kernel that takes over whenever a process is identified for termination due to memory constraints.</p>&#13;
</div>&#13;
<p>Hopefully, these examples help to clarify what you can expect from the <code>kubelet</code> and Linux memory allocation and management techniques. Ensure that your developers have a clear understanding as well. Once developers realize they will be first in line for OOM Killer’s wrath when they don’t provide strong requests and limits for their containers, they typically give much better container definitions.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Node Eviction" data-type="sect2"><div class="sect2" id="node_eviction">&#13;
<h2>Node Eviction</h2>&#13;
<p>Eventually, we’ll exhaust the noncompressible resources of our worker node, <a contenteditable="false" data-primary="resource management" data-secondary="pod eviction" data-type="indexterm" id="idm45358201520664"/><a contenteditable="false" data-primary="incompressible resources" data-secondary="pod eviction" data-type="indexterm" id="idm45358201519288"/><a contenteditable="false" data-primary="kubelet" data-secondary="pod eviction" data-type="indexterm" id="idm45358201517944"/><a contenteditable="false" data-primary="worker nodes" data-secondary="pod eviction" data-type="indexterm" id="idm45358201516568"/><a contenteditable="false" data-primary="evicting pods" data-type="indexterm" id="idm45358201515192"/>and it is at that point that eviction starts to take over. We won’t go into the details of things like eviction thresholds here; <a contenteditable="false" data-primary="resource management" data-secondary="documentation online" data-type="indexterm" id="idm45358201513816"/><a contenteditable="false" data-primary="online resources" data-secondary="Kubernetes" data-tertiary="resource management documentation" data-type="indexterm" id="idm45358201512440"/>the <a href="https://oreil.ly/imjls">official Kubernetes documentation</a> provides plenty of information about these settings.</p>&#13;
<p>It is, however, worth reviewing <a href="https://oreil.ly/G9chi">the process for evicting end-user pods</a>. <a contenteditable="false" data-primary="quality of service (QoS)" data-secondary="levels of" data-tertiary="pod eviction" data-type="indexterm" id="idm45358201508696"/><a contenteditable="false" data-primary="Burstable level of QoS" data-secondary="pod eviction" data-type="indexterm" id="idm45358201506984"/><a contenteditable="false" data-primary="BestEffort level of QoS" data-secondary="pod eviction" data-type="indexterm" id="idm45358201505608"/><a contenteditable="false" data-primary="Guaranteed level of QoS" data-secondary="pod eviction" data-type="indexterm" id="idm45358201504216"/>In short, when the <code>kubelet</code> is unable to free sufficient resources on the node to alleviate any resource pressure, it will first evict BestEffort or Burstable pods that are exceeding their resource requests. The last pods it will evict are those Guaranteed pods that are underutilizing their resource requests or limits. <a contenteditable="false" data-primary="--system-reserved flag" data-primary-sortas="system-reserved flag" data-secondary="pod eviction" data-type="indexterm" id="idm45358201501960"/><a contenteditable="false" data-primary="--kube-reserved flag" data-primary-sortas="kube-reserved flag" data-secondary="pod eviction" data-type="indexterm" id="idm45358201500312"/>If only Guaranteed pods are used on a node, then resource pressure may be coming from system-reserved or kube-reserved processes on the node. It will start evicting user Guaranteed pods in <span class="keep-together">self-preservation</span>. It’s worth investigating the <code>kubelet</code> configuration for reserved resources to ensure a more stable and predictable node in the future.</p>&#13;
<p>The last resort in the situation where a system out of memory situation is encountered before pods can be gracefully evicted is the OOM Killer process itself. <a contenteditable="false" data-primary="Out of Memory (OOM) Killer" data-secondary="pod eviction" data-type="indexterm" id="idm45358201496424"/><a contenteditable="false" data-primary="Linux operating system" data-secondary="Out of Memory Killer" data-tertiary="pod eviction" data-type="indexterm" id="idm45358201495080"/><a contenteditable="false" data-primary="Out of Memory (OOM) Killer" data-secondary="documentation online" data-type="indexterm" id="idm45358201493432"/>More details can be found in the <a href="https://oreil.ly/hO0h2">Kubernetes documentation</a>. Typically, this comes from a process that is consuming resources at a very rapid pace, such that the <code>kubelet</code> cannot address the memory-pressure situation as fast as the process is consuming memory. <a contenteditable="false" data-primary="debugging" data-secondary="Out of Memory Killer" data-type="indexterm" id="idm45358201490568"/>Debugging can be quite a chore. If your pods all have limits in place, then this becomes less of an issue since the cgroup limits placed on individual pods will OOM Kill the process before the node reaches memory pressure. Containers and pods with no memory limits are your most likely culprits and should be <span class="keep-together">avoided</span>.<a contenteditable="false" data-primary="" data-startref="ch03-rclim" data-type="indexterm" id="idm45358201488056"/><a contenteditable="false" data-primary="" data-startref="ch03-rclim2" data-type="indexterm" id="idm45358201486648"/><a contenteditable="false" data-primary="" data-startref="ch03-rclim3" data-type="indexterm" id="idm45358201485272"/><a contenteditable="false" data-primary="" data-startref="ch03-rclim4" data-type="indexterm" id="idm45358201483896"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Capacity Planning and Management" data-type="sect1"><div class="sect1" id="capacity_planning_and_management">&#13;
<h1>Capacity Planning and Management</h1>&#13;
<p>As with many modern platforms and service frameworks, our focus<a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="about" data-type="indexterm" id="idm45358201480744"/><a contenteditable="false" data-primary="capacity planning and management" data-secondary="about" data-type="indexterm" id="idm45358201479000"/><a contenteditable="false" data-primary="scaling" data-secondary="capacity management" data-type="indexterm" id="idm45358201477608"/><a contenteditable="false" data-primary="multicluster management" data-secondary="about multicluster design" data-type="indexterm" id="idm45358201476232"/> is more about capacity management than it is about predicting the future. You can plan for how you will scale your Kubernetes platform for your enterprise. Part of this planning should include how and when to scale master and/or worker nodes, as well as when it is the right time to add additional clusters to your fleet.</p>&#13;
<p>Single cluster or multiple clusters? <a contenteditable="false" data-primary="single cluster design" data-secondary="capacity planning and management" data-type="indexterm" id="idm45358201473960"/>It is unrealistic to expect to run a single cluster for all your workloads. There are some factors to consider. Geographic distribution, tenancy, single-cluster scalability, and blast radius are other factors to consider.</p>&#13;
<section data-pdf-bookmark="Kubernetes Worker Node Capacity" data-type="sect2"><div class="sect2" id="kubernetes_worker_node_capacity">&#13;
<h2>Kubernetes Worker Node Capacity</h2>&#13;
<p>The starting point for effective and consistent worker node <a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="Kubernetes worker node capacity" data-type="indexterm" id="ch03-wkr"/><a contenteditable="false" data-primary="worker nodes" data-secondary="capacity planning and management" data-type="indexterm" id="ch03-wkr2"/><a contenteditable="false" data-primary="capacity planning and management" data-secondary="Kubernetes worker node capacity" data-type="indexterm" id="ch03-wkr3"/><a contenteditable="false" data-primary="Guaranteed level of QoS" data-secondary="worker node capacity management" data-type="indexterm" id="idm45358201464776"/><a contenteditable="false" data-primary="quality of service (QoS)" data-secondary="levels of" data-tertiary="worker node capacity management" data-type="indexterm" id="idm45358201463384"/><a contenteditable="false" data-primary="Burstable level of QoS" data-secondary="worker node capacity management" data-type="indexterm" id="idm45358201461704"/><a contenteditable="false" data-primary="BestEffort level of QoS" data-secondary="worker node capacity management" data-type="indexterm" id="idm45358201460312"/>capacity management comes from using Guaranteed QoS for your pods. While it’s possible to manage capacity with Burstable or BestEffort pods, it can be extremely challenging to decide when a cluster needs to be scaled.</p>&#13;
<p>What is it like to manage capacity without Guaranteed QoS? Administrators<a contenteditable="false" data-primary="monitoring capacity management" data-type="indexterm" id="idm45358201458120"/><a contenteditable="false" data-primary="capacity planning and management" data-secondary="monitoring" data-type="indexterm" id="idm45358201457000"/><a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="capacity management" data-type="indexterm" id="idm45358201455608"/><a contenteditable="false" data-primary="memory utilization" data-secondary="capacity management" data-type="indexterm" id="idm45358201454216"/><a contenteditable="false" data-primary="CPU utilization" data-secondary="capacity management" data-type="indexterm" id="idm45358201452840"/> are left trying to formulate a guess on when scale is needed based on a combination of monitoring tools for the worker node resources and application metrics coming from the services and applications running on the cluster. Do your monitoring tools show that you have 25% unused memory and CPU and your application metrics are all hitting their <em>service-level objectives</em> (SLOs)? Fantastic, you’ve gotten lucky. It’s only a matter of time before those resources start getting pinched and your SLOs start to suffer. It’s at this point where you can just start throwing more and/or bigger worker nodes at the cluster in hopes that your SLOs come back in line. However, it may be that there are ill-behaving pods elsewhere in the cluster that are forcing you into this position and you are just throwing money away. With Guaranteed QoS, your CPU will never be compressed, and your memory utilization will never result in randomly evicted pods.</p>&#13;
<p>A number of tools are at our disposal to help monitor and manage the capacity of our cluster. Let’s consider some of the options available to us.</p>&#13;
<section data-pdf-bookmark="Monitoring" data-type="sect3"><div class="sect3" id="monitoring">&#13;
<h3>Monitoring</h3>&#13;
<p>Monitoring is critical to having a strong understanding of how your <a contenteditable="false" data-primary="memory utilization" data-secondary="monitoring" data-type="indexterm" id="idm45358201447464"/><a contenteditable="false" data-primary="CPU utilization" data-secondary="monitoring" data-type="indexterm" id="idm45358201446008"/><a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="monitoring" data-type="indexterm" id="idm45358201444632"/>capacity is being used. Even with all Guaranteed QoS pods, you’ll want to have monitoring data to see if you have significantly underutilized resources or rogue system or kube processes that are consuming resources. Monitoring systems gather metrics from the entire system and aggregate this data into a centralized tool. These metrics can include CPU consumption, available memory, disk performance, network throughput, and application-specific data. With this centralized tool, we typically have the ability to view historical data, as well as to define alarms and thresholds to notify engineers of impending issues. Together, this historical data and alerting provide powerful insight into the performance and scale of the system. <a contenteditable="false" data-primary="online resources" data-secondary="monitoring tools" data-type="indexterm" id="idm45358201442088"/><a contenteditable="false" data-primary="monitoring capacity management" data-secondary="tools for" data-type="indexterm" id="idm45358201440712"/><a contenteditable="false" data-primary="Prometheus monitoring tool" data-type="indexterm" id="idm45358201439320"/><a contenteditable="false" data-primary="Sysdig Monitor tool" data-type="indexterm" id="idm45358201438200"/>Two first-rate monitoring options to consider are <a href="https://prometheus.io">Prometheus</a> and <a href="https://oreil.ly/3p9YR">Sysdig Monitor</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Limit ranges and quotas" data-type="sect3"><div class="sect3" id="limit_ranges_and_quotas">&#13;
<h3>Limit ranges and quotas</h3>&#13;
<p>We’ve now talked extensively about resource requests and limits. <a contenteditable="false" data-primary="capacity planning and management" data-secondary="limit ranges" data-type="indexterm" id="idm45358201433560"/><a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="limit ranges" data-type="indexterm" id="idm45358201432136"/><a contenteditable="false" data-primary="resource limits" data-secondary="limit ranges" data-type="indexterm" id="idm45358201430472"/><a contenteditable="false" data-primary="limit ranges" data-seealso="resource limits" data-type="indexterm" id="idm45358201429096"/>Fortunately, Kubernetes has even more tools at your disposal to assist with controlling resource usage.</p>&#13;
<p><a href="https://oreil.ly/BvBNH">Limit ranges</a> enable an administrator to enforce the use of requests and limits. This includes setting defaults for all containers and pods to inject them at runtime, as well as setting minimum and maximum values for a namespace.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Limit ranges are really a safeguard against poorly configured pods. Administrators should strongly encourage application owners to set their own proper requests and limits based on performance testing data.</p>&#13;
</div>&#13;
<p>Quotas allow an administrator to set maximum requests and limits <a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="quotas" data-type="indexterm" id="idm45358201424488"/><a contenteditable="false" data-primary="quotas" data-type="indexterm" id="idm45358201422744"/><a contenteditable="false" data-primary="namespaces" data-secondary="quotas" data-type="indexterm" id="idm45358201421640"/><a contenteditable="false" data-primary="capacity planning and management" data-secondary="quotas" data-type="indexterm" id="idm45358201420264"/>per namespace. While potentially aggravating for the developer who expects unlimited access to resources, this gives the cluster administrator ultimate control over preventing resource contention. Administrators should consider setting alerts in combination with total cluster capacity as well as on individual namespace quotas to help plan for future capacity needs. Quotas provide the control needed to ensure that additional compute resources can be onboarded before user demand for resources exceeds the cluster supply.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Autoscaling" data-type="sect3"><div class="sect3" id="autoscaling">&#13;
<h3>Autoscaling</h3>&#13;
<p>Kubernetes autoscaling comes in two flavors. The first of these is the <a contenteditable="false" data-primary="workload autoscaling" data-type="indexterm" id="ch03-workl"/><a contenteditable="false" data-primary="scaling" data-secondary="autoscaling" data-type="indexterm" id="ch03-aut2"/><a contenteditable="false" data-primary="autoscaling" data-secondary="about" data-type="indexterm" id="idm45358201413288"/><a contenteditable="false" data-primary="clusters" data-secondary="cluster autoscaler" data-type="indexterm" id="idm45358201411912"/><a contenteditable="false" data-primary="horizontal pod autoscaling (HPA)" data-type="indexterm" id="ch03-hor"/><a contenteditable="false" data-primary="autoscaling" data-secondary="cluster autoscaling" data-type="indexterm" id="idm45358201409144"/><a contenteditable="false" data-primary="autoscaling" data-secondary="workload autoscaling" data-type="indexterm" id="ch03-aut3"/><a contenteditable="false" data-primary="replicas" data-secondary="horizontal pod autoscaling" data-type="indexterm" id="ch03-aut4"/>cluster autoscaler, which modifies the number of worker nodes in the cluster. The other is workload autoscaling, which takes many forms, such as horizontal pod autoscaler, vertical pod autoscaler, addon-resizer, and cluster proportional autoscaler. Typically, a cluster administrator is more concerned with the cluster autoscaler, and the workload autoscaling options are more the domain of those responsible for the applications and services running on the cluster.</p>&#13;
<section data-pdf-bookmark="Cluster autoscaler" data-type="sect4"><div class="sect4" id="idm45358201403720">&#13;
<h4>Cluster autoscaler</h4>&#13;
<p>The <a href="https://oreil.ly/ccMvo">cluster autoscaler</a> works<a contenteditable="false" data-primary="Pending state" data-secondary="cluster autoscaler" data-type="indexterm" id="idm45358201401576"/> by evaluating pods that remain in Pending state due to insufficient resources and responds by adding additional worker nodes or removing underutilized worker nodes. <a contenteditable="false" data-primary="Guaranteed level of QoS" data-secondary="cluster autoscaler" data-type="indexterm" id="idm45358201399864"/>In clusters where most pods are using Guaranteed QoS, the cluster autoscaler can be a very efficient solution to the management of worker node capacity.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Horizontal pod autoscaler" data-type="sect4"><div class="sect4" id="idm45358201397928">&#13;
<h4>Horizontal pod autoscaler</h4>&#13;
<p>The most common form of autoscaling used is <a href="https://oreil.ly/Y6vOg">horizontal pod autoscaling (HPA)</a>. Autoscaling<a contenteditable="false" data-primary="CPU utilization" data-secondary="horizontal pod autoscaler" data-type="indexterm" id="idm45358201395320"/> factors in the actual CPU utilization of a pod based on metrics provided via the <a contenteditable="false" data-primary="metrics" data-secondary="metrics.k8s.io API" data-type="indexterm" id="idm45358201393752"/>metrics API <code>metrics.k8s.io</code> (or directly from heapster, pre-Kubernetes 1.11 only). With this approach, the resource requests and limits just need to be reasonable for the given workload, and the autoscaler will look at real-world CPU utilization to determine when to scale. <a contenteditable="false" data-primary="autoscaling" data-secondary="example" data-type="indexterm" id="idm45358201391544"/><a contenteditable="false" data-primary="horizontal pod autoscaling (HPA)" data-secondary="example" data-type="indexterm" id="idm45358201390168"/>Let’s look at an example via our application:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  labels:&#13;
    app: hello&#13;
  name: hello&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: hello&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: hello&#13;
    spec:&#13;
      containers:&#13;
      - image: kitch/hello-app:1.0&#13;
        name: hello&#13;
        resources:&#13;
          requests:&#13;
            cpu: 20m&#13;
            memory: 50Mi&#13;
---&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  labels:&#13;
    run: hello&#13;
  name: hello&#13;
spec:&#13;
  ports:&#13;
  - port: 80&#13;
    protocol: TCP&#13;
    targetPort: 8080&#13;
  selector:&#13;
    run: hello</pre>&#13;
<p>We now have a simple web app to begin exploring autoscaling.  Let’s see what we can do from a scaling perspective. Step one—create an autoscaling policy for this <span class="keep-together">deployment</span>:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl autoscale deploy hello --min=1 --max=5 --cpu-percent=80</strong>&#13;
deployment.apps "hello" autoscaled&#13;
 &#13;
$ <strong>kubectl get hpa hello</strong>&#13;
NAME      REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS  &#13;
hello     Deployment/hello   0%/80%    1         5         1</pre>&#13;
<p>Excellent! We are ready to scale! Let’s throw some load, using <a href="http://locust.io">Locust</a> or similar, at our fancy new web application and see what happens next. Now when we check to see the CPU utilization of our pod, we can see it is using 43m cores:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods -l run=hello</strong>&#13;
NAME                     CPU(cores)   MEMORY(bytes)&#13;
hello-7b68c766c6-mgtdk   43m          6Mi</pre>&#13;
<p>This is more than double the resource request we specified:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl get hpa hello</strong>&#13;
NAME      REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  &#13;
hello     Deployment/hello   215%/80%   1         5         1        </pre>&#13;
<p>Note that the HPA has increased the number of replicas:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl get hpa hello</strong>&#13;
NAME      REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE&#13;
hello     Deployment/hello   86%/80%   1         5         3          10m</pre>&#13;
<p>Utilization is still above our policy limit, and thus in time the HPA will continue to scale up and reduce the load below the threshold of the policy:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl get hpa hello</strong>&#13;
NAME      REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE&#13;
hello     Deployment/hello   62%/80%   1         5         4          15m</pre>&#13;
<p>It’s important to note that the metrics collection and HPA are not real-time systems.<a contenteditable="false" data-primary="metrics" data-secondary="not real-time" data-type="indexterm" id="idm45358201375976"/><a contenteditable="false" data-primary="horizontal pod autoscaling (HPA)" data-secondary="not real-time" data-type="indexterm" id="idm45358201374504"/><a contenteditable="false" data-primary="horizontal pod autoscaling (HPA)" data-secondary="documentation online" data-type="indexterm" id="idm45358201373112"/><a contenteditable="false" data-primary="online resources" data-secondary="Kubernetes" data-tertiary="horizontal pod autoscaling" data-type="indexterm" id="idm45358201371720"/> The <a href="https://oreil.ly/9R9cE">Kubernetes documentation</a> provides a bit more detail about the controller-manager settings and other intricacies of the HPA.</p>&#13;
<p>Finally, we reduce the load on the deployment by killing the load-generating pod, and it is automatically scaled down again:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl get hpa hello</strong>&#13;
NAME      REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE&#13;
hello     Deployment/hello   0%/80%    1         5         1          45m</pre>&#13;
<p>How does this help us in hybrid scenarios? It guarantees that regardless of the inherent performance of any one cluster or worker node, the autoscaler will ensure that we have the appropriate resources allocated to support our workload.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>HPA can be configured to use all kinds of metrics, including request response times or request rates, in order to meet application service-level requirements.<a contenteditable="false" data-primary="" data-startref="ch03-hor" data-type="indexterm" id="idm45358201365416"/><a contenteditable="false" data-primary="" data-startref="ch03-aut4" data-type="indexterm" id="idm45358201363944"/></p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Vertical pod autoscaler" data-type="sect4"><div class="sect4" id="idm45358201397304">&#13;
<h4>Vertical pod autoscaler</h4> &#13;
<p>The <a href="https://oreil.ly/ostge">vertical pod autoscaler (VPA)</a> <a contenteditable="false" data-primary="vertical pod autoscaler (VPA)" data-type="indexterm" id="idm45358201360184"/><a contenteditable="false" data-primary="memory utilization" data-secondary="vertical pod autoscaler" data-type="indexterm" id="idm45358201359016"/><a contenteditable="false" data-primary="CPU utilization" data-secondary="vertical pod autoscaler" data-type="indexterm" id="idm45358201357640"/>is an excellent solution for a situation in which you have a deployment that needs to scale up rather than out. Whereas the HPA adds more replicas as memory and CPU utilization increase, the VPA increases the memory and CPU requests of your deployment. For this example, let’s reuse our hello example. We begin by installing the VPA according to the steps provided. If you recall, we started with requests of 20m. First, let’s apply our VPA:</p>&#13;
<pre data-type="programlisting">apiVersion: poc.autoscaling.k8s.io/v1alpha1&#13;
kind: VerticalPodAutoscaler&#13;
metadata:&#13;
  name: hello-vpa&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      run: hello&#13;
  updatePolicy:&#13;
    updateMode: Auto</pre>&#13;
<p>Now, let’s apply load using a load generation tool such as <a href="http://locust.io">Locust</a> to the application and observe the appropriate response:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl top pods -l run=hello</strong>&#13;
NAME                     CPU(cores)   MEMORY(bytes)&#13;
hello-7b68c766c6-mgtdk   74m          6Mi</pre>&#13;
<p>We can then view the resource requests for our <code>hello</code> deployment and see that they have been automatically adjusted to match real-world utilization of our application:</p>&#13;
<pre data-type="programlisting">resources:&#13;
      requests:&#13;
        cpu: 80m&#13;
        memory: 50Mi</pre>&#13;
<p>The VPA is a powerful tool that can enable us to scale our applications and services in a manner that is best suited to their performance characteristics. Scaling out an application doesn’t always provide the most efficient use of available resources. Leveraging the VPA in these situations can lead to improved performance and lower costs if applied appropriately.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Cluster proportional autoscaler" data-type="sect4"><div class="sect4" id="idm45358201349640">&#13;
<h4>Cluster proportional autoscaler</h4> &#13;
<p>A couple of other common autoscaler implementations are used in addition to the HPA. The first of these is the <a href="https://oreil.ly/pvIDT">cluster proportional autoscaler</a>, which <a contenteditable="false" data-primary="clusters" data-secondary="cluster proportional autoscaler" data-type="indexterm" id="idm45358201347480"/><a contenteditable="false" data-primary="replicas" data-secondary="cluster proportional autoscaler" data-type="indexterm" id="idm45358201346040"/>looks at the size of the cluster in terms of workers and resource capacity to decide how many replicas of a given service are needed. <a contenteditable="false" data-primary="CoreDNS" data-type="indexterm" id="idm45358201344376"/>Famously, this is used by CoreDNS; for example:</p>&#13;
<pre data-type="programlisting">spec:&#13;
  containers:&#13;
  - command:&#13;
    - /cluster-proportional-autoscaler&#13;
    - --namespace=kube-system&#13;
    - --configmap=coredns-autoscaler&#13;
    - --target=Deployment/coredns&#13;
    - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,&#13;
"preventSinglePointFailure":true}}&#13;
    - --logtostderr=true&#13;
    - --v=2</pre>&#13;
<p>The number of cores and nodes are used to determine how many replicas of CoreDNS are needed.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Addon-resizer" data-type="sect4"><div class="sect4" id="idm45358201341272">&#13;
<h4>Addon-resizer</h4> &#13;
<p>Another great example is the <a href="https://oreil.ly/3sF7E">addon-resizer</a> (aka <code>pod_nanny</code>),<a contenteditable="false" data-primary="addon-resizer" data-type="indexterm" id="idm45358201338584"/><a contenteditable="false" data-primary="pod_nanny" data-type="indexterm" id="idm45358201337448"/> which performs vertical scaling of resource requests based on cluster size. It scales the resource’s requests of a singleton based on the number of workers in the cluster. This autoscaler has been used by the Kubernetes <a href="https://oreil.ly/GIyey">metrics-server</a>, <a contenteditable="false" data-primary="metrics" data-secondary="Kubernetes metrics-server addon-resizer" data-type="indexterm" id="idm45358201335288"/><a contenteditable="false" data-primary="metrics-server addon-resizer" data-type="indexterm" id="idm45358201333864"/>which is a core component responsible for providing a simple API frontend to basic worker node and pod metrics. Here we can see the use of the <code>pod_nanny</code> to scale the metrics-server as the cluster grows in size, with <code>extra-cpu</code> and <code>extra-memory</code> defining how much additional CPU and memory resources should be allocated to the metrics-server for each additional worker node in the cluster:</p>&#13;
<pre data-type="programlisting">- /pod_nanny&#13;
    - --config-dir=/etc/config&#13;
    - --cpu=100m&#13;
    - --extra-cpu=1m&#13;
    - --memory=40Mi&#13;
    - --extra-memory=6Mi&#13;
    - --threshold=5&#13;
    - --deployment=metrics-server&#13;
    - --container=metrics-server&#13;
    - --poll-period=300000&#13;
    - --estimator=exponential&#13;
    - --use-metrics=true</pre>&#13;
<p>This autoscaling technique is very helpful as it can resize applications based on known scaling measurements as cluster size grows. This can help to avoid issues with OOM or CPU starvation for an application before a traditional horizontal or vertical autoscaler would even be able to collect enough metrics to make a judgment on the need to scale up.<a contenteditable="false" data-primary="" data-startref="ch03-wkr" data-type="indexterm" id="idm45358201329288"/><a contenteditable="false" data-primary="" data-startref="ch03-wkr2" data-type="indexterm" id="idm45358201327912"/><a contenteditable="false" data-primary="" data-startref="ch03-wkr3" data-type="indexterm" id="idm45358201326536"/><a contenteditable="false" data-primary="" data-startref="ch03-aut2" data-type="indexterm" id="idm45358201325160"/><a contenteditable="false" data-primary="" data-startref="ch03-aut3" data-type="indexterm" id="idm45358201323784"/><a contenteditable="false" data-primary="" data-startref="ch03-workl" data-type="indexterm" id="idm45358201322408"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Kubernetes Master Capacity" data-type="sect2"><div class="sect2" id="kubernetes_master_capacity">&#13;
<h2>Kubernetes Master Capacity</h2>&#13;
<p>The Kubernetes <a href="https://oreil.ly/IjmVi">scalability special interest group (sig-scalability)</a> <a contenteditable="false" data-primary="master capacity of Kubernetes" data-type="indexterm" id="idm45358201317768"/><a contenteditable="false" data-primary="online resources" data-secondary="Kubernetes" data-tertiary="Special Interest Groups" data-type="indexterm" id="idm45358201316664"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="Special Interest Groups" data-type="indexterm" id="idm45358201315016"/><a contenteditable="false" data-primary="Special Interest Groups of Kubernetes" data-type="indexterm" id="idm45358201313640"/><a contenteditable="false" data-primary="capacity planning and management" data-secondary="Kubernetes master capacity" data-type="indexterm" id="idm45358201312568"/><a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="Kubernetes master capacity" data-type="indexterm" id="idm45358201311160"/>is a group of Kubernetes community members focused on measuring and improving the scalability of Kubernetes itself. The sig-scalability team has done fantastic work to quantify and improve the limits of a Kubernetes cluster, yet it still has its boundaries.<sup><a data-type="noteref" href="ch03.html#ch01fn24" id="ch01fn24-marker">4</a></sup> The<a contenteditable="false" data-primary="scaling" data-secondary="large clusters" data-type="indexterm" id="idm45358201307384"/><a contenteditable="false" data-primary="scaling" data-secondary="master capacity" data-type="indexterm" id="idm45358201306008"/><a contenteditable="false" data-primary="clusters" data-secondary="scaling large clusters" data-tertiary="master capacity" data-type="indexterm" id="idm45358201304632"/> limits specified by the sig-scalability team are based primarily on the impact of adding nodes and pods. <a contenteditable="false" data-primary="etcd" data-secondary="scalability and" data-type="indexterm" id="idm45358201302728"/><a contenteditable="false" data-primary="kube-apiserver" data-secondary="scalability and" data-type="indexterm" id="idm45358201301352"/>The limiting factor in kube scalability is the performance of etcd/kube-apiserver to handle the load of the controllers and objects they are managing. In the standard tests, there are a limited number of objects and controllers at work. In these tests, the <code>kubelet</code>, controller-manager, and scheduler are the primary consumers of etcd/apiserver resources.</p>&#13;
<p><a data-type="xref" href="#recommended_resources_needed_for_kuberne">Table 3-1</a> lists the recommended master node systems based on the number of worker nodes based on <a href="https://oreil.ly/lXkTu">the data from sig-scalability</a>.<a contenteditable="false" data-primary="worker nodes" data-secondary="capacity planning and management" data-tertiary="resource recommendations per node counts" data-type="indexterm" id="idm45358201297224"/><a contenteditable="false" data-primary="capacity planning and management" data-secondary="resource recommendations per node counts" data-type="indexterm" id="idm45358201295416"/><a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="resource recommendations per node counts" data-type="indexterm" id="idm45358201293992"/><a contenteditable="false" data-primary="control planes" data-secondary="resources needed per worker node" data-type="indexterm" id="idm45358201292296"/></p>&#13;
<table class="border" id="recommended_resources_needed_for_kuberne">&#13;
<caption><span class="label">Table 3-1. </span>Recommended resources needed for Kubernetes control plane based on cluster worker node counts</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Worker nodes</th>&#13;
<th>Master vCPU</th>&#13;
<th>Master memory</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>1–10</td>&#13;
<td>2</td>&#13;
<td>8</td>&#13;
</tr>&#13;
<tr>&#13;
<td>11–100</td>&#13;
<td>4</td>&#13;
<td>16</td>&#13;
</tr>&#13;
<tr>&#13;
<td>101–250</td>&#13;
<td>8</td>&#13;
<td>32</td>&#13;
</tr>&#13;
<tr>&#13;
<td>251–500</td>&#13;
<td>16</td>&#13;
<td>64</td>&#13;
</tr>&#13;
<tr>&#13;
<td>500+</td>&#13;
<td>32</td>&#13;
<td>128</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p><a contenteditable="false" data-primary="clusters" data-secondary="scaling large clusters" data-tertiary="maximum cluster size" data-type="indexterm" id="idm45358201277224"/><a contenteditable="false" data-primary="maximum cluster size" data-type="indexterm" id="idm45358201275576"/><a contenteditable="false" data-primary="pods" data-secondary="about" data-tertiary="maximum number" data-type="indexterm" id="idm45358201274472"/><a contenteditable="false" data-primary="node maximum count" data-type="indexterm" id="idm45358201272824"/><a contenteditable="false" data-primary="containers" data-secondary="about" data-tertiary="maximum number" data-type="indexterm" id="idm45358201271720"/>The <a href="https://oreil.ly/DxmGf">maximum cluster size documented officially</a> is as <span class="keep-together">follows</span>:</p>&#13;
<ul>&#13;
<li><p>No more than 5,000 nodes</p></li>&#13;
<li><p>No more than 150,000 total pods</p></li>&#13;
<li><p>No more than 300,000 total containers</p></li>&#13;
<li><p>No more than 100 pods per node</p></li>&#13;
</ul>&#13;
<p>However, as we’ll discuss in this chapter, there are far more factors to consider.</p>&#13;
<p>In addition to pods and worker nodes, it’s important to consider the <a contenteditable="false" data-primary="Kubernetes" data-secondary="services" data-tertiary="master capacity" data-type="indexterm" id="idm45358201264680"/>number of Kubernetes services, secrets, <code>ConfigMaps</code>, persistent volumes, and other standard Kubernetes objects and controllers. <a contenteditable="false" data-primary="kube-proxy" data-secondary="master capacity" data-type="indexterm" id="idm45358201262344"/>There have been considerable improvements in kube-proxy to mitigate the performance impact of a large number of services, but it’s still a factor. Remember that kube-proxy needs to track changes to the endpoints of all services in order to keep iptables or IP Virtual Server (IPVS) configurations up to date. There is no hard limit on the number of services that we know of, but more endpoints and more ports will have a huge impact on the scalability of each kube-proxy and increase the load on etcd/apiserver. Consider also the load of secrets and <code>ConfigMaps</code> on etcd/apiserver. Every mounted secret means another watch on that secret for the <code>kubelet</code>. In aggregate, these secret watches can have a significant impact on etcd/apiserver performance and scale.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>One option to consider to help Kubernetes scale more efficiently is to <a contenteditable="false" data-primary="service accounts" data-secondary="secrets in scaling" data-type="indexterm" id="idm45358201258424"/><a contenteditable="false" data-primary="secrets" data-secondary="scaling considerations" data-type="indexterm" id="idm45358201256968"/>skip mounting the service account secret into your pods. This means the <code>kubelet</code> will not need to watch this service account secret for changes.</p>&#13;
</div>&#13;
<p>Here is an example of how to skip mounting the service account token into the pod:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: ServiceAccount&#13;
metadata:&#13;
  name: build-robot&#13;
automountServiceAccountToken: false</pre>&#13;
<p>This will help with scalability of the kube-apiserver as well as the node itself.<a contenteditable="false" data-primary="kube-apiserver" data-secondary="scalability and" data-tertiary="service account mounting skipped" data-type="indexterm" id="idm45358201253112"/> Workload for kube-apiserver is reduced because the <code>kubelet</code> will not need to call the kube-apiserver to get the token. The node performance is improved because there are fewer volume mounts for tokens.</p>&#13;
<p>Not convinced that many factors contribute to the scalability of the cluster? <a contenteditable="false" data-primary="custom resource definition (CRD)" data-secondary="scalability and" data-type="indexterm" id="idm45358201250120"/>Consider that the modern Kubernetes workload often includes a significant number of other objects and controllers. Kubernetes 1.7 introduced the ability for users to define their own objects via <em>custom resource definitions</em> (CRDs). These CRDs, along with the Kubernetes controller model for reconciling declared states, enabled users to extend the capabilities of Kubernetes to manage almost any resource imaginable. Although they are very powerful tools, CRDs and controllers can sometimes lead to an explosion of objects in the Kubernetes data model. Along with all those new objects may come many active actors writing and reading to and from that model. Left unchecked, these controllers and new objects can have a massive impact on scale and performance.</p>&#13;
<p>With so many moving parts contributing to the scale limitations of your kube cluster, <a contenteditable="false" data-primary="production" data-secondary="sharding" data-type="indexterm" id="idm45358201246712"/><a contenteditable="false" data-primary="sharding in production" data-type="indexterm" id="idm45358201245336"/><a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="sharding in production" data-type="indexterm" id="idm45358201244232"/><a contenteditable="false" data-primary="capacity planning and management" data-secondary="sharding in production" data-type="indexterm" id="idm45358201242568"/>the one thing you can count on is this: in production, you will need to shard. In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#continuous_delivery_across_clusters">5</a>, <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#multicluster_fleets_provision_and_upgrad">6</a>, and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#working_example_of_multicluster_applicat">8</a>, we go into more detail on how to manage multiple clusters and distribute workload across them. We provide a formula for sharding that allows for further expansion of our scaling capabilities. For the purposes of this discussion, we simply need to establish what the scalability limits are of a single Kubernetes cluster running <em>your</em> unique workload. There is simply no substitute for automated scale testing that you can repeat regularly. One of your teams wants to introduce a new set of CRDs and a controller for them? Time to run scale testing! Before they can ship that new code to production, you need to understand the impacts of those changes. It may be that you find that it is a significant enough impact that you can break the scale capabilities of your clusters in production. In such cases, you may need to roll out a new set of clusters for these services. What if this service is already in production? You may need to migrate this service off of an existing set of multitenant clusters.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Admission Controller Best Practices" data-type="sect1"><div class="sect1" id="admission_controller_best_practices">&#13;
<h1>Admission Controller Best Practices</h1>&#13;
<p>Admission controllers are powerful tools available to the administrator<a contenteditable="false" data-primary="admission controllers" data-secondary="best practices" data-type="indexterm" id="ch03-adcn2"/><a contenteditable="false" data-primary="resource management" data-secondary="admission controllers" data-tertiary="best practices" data-type="indexterm" id="ch03-adcn"/><a contenteditable="false" data-primary="admission controllers" data-secondary="about" data-type="indexterm" id="idm45358201230248"/><a contenteditable="false" data-primary="resource management" data-secondary="admission controllers" data-type="indexterm" id="idm45358201228872"/> of Kubernetes clusters to enforce additional constraints and behaviors on objects in the cluster. Most commonly, these controllers are used to enforce security, performance, and other best practices. These add-ons to the cluster can either modify or constrain objects that are being created, updated, or deleted in the kube-apiserver.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
&#13;
<p>It’s important to note that as with many features of Kubernetes, enabling admission controllers can have an impact on the performance and scale of your cluster. This is especially true when using webhook-based admission controllers.</p>&#13;
</div>&#13;
<section data-pdf-bookmark="Standard Admission Controllers" data-type="sect2"><div class="sect2" id="standard_admission_controllers">&#13;
<h2>Standard Admission Controllers</h2>&#13;
<p>Upstream Kubernetes includes a number of admission controllers as <a contenteditable="false" data-primary="admission controllers" data-secondary="standard admission controllers" data-type="indexterm" id="idm45358201223672"/><a contenteditable="false" data-primary="resource management" data-secondary="admission controllers" data-tertiary="standard admission controllers" data-type="indexterm" id="idm45358201222152"/><a contenteditable="false" data-primary="control planes" data-secondary="standard admission controllers" data-type="indexterm" id="idm45358201220488"/>part of all control planes. As of Kubernetes 1.18, <a href="https://oreil.ly/DzrON">the set of admission controllers enabled by default</a> are as follows:</p>&#13;
<ul>&#13;
<li><p><code>NamespaceLifecycle</code></p></li>&#13;
<li><p><code>LimitRanger</code></p></li>&#13;
<li><p><code>ServiceAccount</code></p></li>&#13;
<li><p><code>TaintNodesByCondition</code></p></li>&#13;
<li><p><code>Priority</code></p></li>&#13;
<li><p><code>DefaultTolerationSeconds</code></p></li>&#13;
<li><p><code>DefaultStorageClass</code></p></li>&#13;
<li><p><code>StorageObjectInUseProtection</code></p></li>&#13;
<li><p><code>PersistentVolumeClaimResize</code></p></li>&#13;
<li><p><code>RuntimeClass</code></p></li>&#13;
<li><p><code>CertificateApproval</code></p></li>&#13;
<li><p><code>CertificateSigning</code></p></li>&#13;
<li><p><code>CertificateSubjectRestriction</code></p></li>&#13;
<li><p><code>DefaultIngressClass</code></p></li>&#13;
<li><p><code>MutatingAdmissionWebhook</code></p></li>&#13;
<li><p><code>ValidatingAdmissionWebhook</code></p></li>&#13;
<li><p><code>ResourceQuota</code></p></li>&#13;
</ul>&#13;
<p>Not all of these are particularly interesting, and <a contenteditable="false" data-primary="admission controllers" data-secondary="documentation online" data-type="indexterm" id="idm45358201205000"/><a contenteditable="false" data-primary="resource management" data-secondary="admission controllers" data-tertiary="documentation online" data-type="indexterm" id="idm45358201203624"/><a contenteditable="false" data-primary="online resources" data-secondary="Kubernetes" data-tertiary="admission controller documentation" data-type="indexterm" id="idm45358201201976"/>the <a href="https://oreil.ly/4EHzv">official documentation</a> has plenty of information about what they all do. There are two on this list that we’ll spend more time on: <code>MutatingAdmissionWebhook</code> and <span class="keep-together"><code>ValidatingAdmissionWebhook</code></span>.</p>&#13;
<p>Before we move on to those dynamic admission controllers, we’ll cover a few of the standard controllers in more detail:</p>&#13;
<dl>&#13;
<dt><code>LimitRanger</code></dt>&#13;
<dd>Earlier in this chapter, we talked about the importance of <a contenteditable="false" data-primary="LimitRanger admission controller" data-type="indexterm" id="idm45358201196088"/><a contenteditable="false" data-primary="resource limits" data-secondary="LimitRanger admission controller" data-type="indexterm" id="idm45358201194936"/>resource limits to ensure that pod resource consumption is kept in check. <code>LimitRanger</code> ensures that the <code>LimitRange</code> settings configured on a namespace are adhered to. This provides one more tool for cluster administrators to keep workloads in check.<sup><a data-type="noteref" href="ch03.html#ch01fn25" id="ch01fn25-marker">5</a></sup></dd>&#13;
<dt><code>Priority</code></dt>&#13;
<dd>As we discussed earlier, <code>PriorityClasses</code> play a key <a contenteditable="false" data-primary="Priority admission controller" data-type="indexterm" id="idm45358201189336"/><a contenteditable="false" data-primary="resource management" data-secondary="scheduling" data-tertiary="pod priority and preemption" data-type="indexterm" id="idm45358201188136"/><a contenteditable="false" data-primary="scheduling" data-secondary="resource management" data-tertiary="pod priority and preemption" data-type="indexterm" id="idm45358201186472"/>role in the scheduling and post-scheduling behavior of pods. The <code>Priority</code> admission controller helps to map useful names to an integer that represents the relative value of one class versus another.</dd>&#13;
<dt><code>ResourceQuota</code></dt>&#13;
<dd>Critical to the function of a production-ready cluster, this <a contenteditable="false" data-primary="capacity planning and management" data-secondary="quotas" data-tertiary="ResourceQuota admission controller" data-type="indexterm" id="idm45358201183048"/><a contenteditable="false" data-primary="ResourceQuota admission controller" data-type="indexterm" id="idm45358201181288"/><a contenteditable="false" data-primary="quotas" data-secondary="ResourceQuota admission controller" data-type="indexterm" id="idm45358201180168"/><a contenteditable="false" data-primary="namespaces" data-secondary="quotas" data-tertiary="ResourceQuota admission controller" data-type="indexterm" id="idm45358201178776"/><a contenteditable="false" data-primary="resource management" data-secondary="capacity planning and management" data-tertiary="quotas" data-type="indexterm" id="idm45358201177112"/>controller is responsible for enforcing the quotas we discussed in <a data-type="xref" href="#limit_ranges_and_quotas">“Limit ranges and quotas”</a>.</dd>&#13;
<dt><code>PodSecurityPolicy</code></dt>&#13;
<dd>This controller is not enabled by default but should be in any <a contenteditable="false" data-primary="PodSecurityPolicy admission controller" data-type="indexterm" id="idm45358201173272"/><a contenteditable="false" data-primary="production" data-secondary="PodSecurityPolicy admission controller" data-type="indexterm" id="idm45358201172024"/><a contenteditable="false" data-primary="security" data-secondary="PodSecurityPolicy admission controller" data-type="indexterm" id="idm45358201170632"/>production-ready Kubernetes or OpenShift cluster. <a href="https://oreil.ly/fLhIC">Pod security policies</a> are essential for enforcing security and compliance.</dd>&#13;
</dl>&#13;
<p>Collectively, these additional admission controllers provide a great deal of the core behaviors we have come to take as standard for Kubernetes. These admission controllers help to build the rules that govern how administrators and users deploy their workloads within set boundaries of performance, scale, and security.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Admission Webhooks" data-type="sect2"><div class="sect2" id="admission_webhooks">&#13;
<h2>Admission Webhooks</h2>&#13;
<p>It’s helpful to talk a bit about what admission webhooks are and how they work.<a contenteditable="false" data-primary="admission controllers" data-secondary="admission webhooks" data-type="indexterm" id="idm45358201165352"/><a contenteditable="false" data-primary="admission webhooks" data-type="indexterm" id="idm45358201163976"/><a contenteditable="false" data-primary="webhooks" data-secondary="admission webhooks" data-type="indexterm" id="idm45358201162872"/><a contenteditable="false" data-primary="resource management" data-secondary="admission controllers" data-tertiary="admission webhooks" data-type="indexterm" id="idm45358201161496"/><a contenteditable="false" data-primary="dynamic admission controllers" data-type="indexterm" id="idm45358201159848"/><a contenteditable="false" data-primary="kube-apiserver" data-secondary="admission webhooks" data-type="indexterm" id="idm45358201158776"/> The <a href="https://oreil.ly/pSA4X">upstream documentation</a> also refers to them as <em>dynamic admission controllers</em> and provides excellent information about these tools. An <em>admission</em> <em>webhook</em> is a Kubernetes configuration that references a web service that can run either on the cluster or external to the cluster and provides modification (mutating) or validation of objects as they are modified in the kube-apiserver. A simple example of a webhook configuration is provided for reference:</p>&#13;
<pre data-type="programlisting">apiVersion: admissionregistration.k8s.io/v1&#13;
kind: ValidatingWebhookConfiguration&#13;
metadata:&#13;
  name: "pod-policy.example.com"&#13;
webhooks:&#13;
- name: "pod-policy.example.com"&#13;
  rules:&#13;
  - apiGroups:   [""]&#13;
    apiVersions: ["v1"]&#13;
    operations:  ["CREATE"]&#13;
    resources:   ["pods"]&#13;
    scope:       "Namespaced"&#13;
  clientConfig:&#13;
    service:&#13;
      namespace: "example-namespace"&#13;
      name: "example-service"&#13;
    caBundle: "xxxxxxxxxx"&#13;
  admissionReviewVersions: ["v1", "v1beta1"]&#13;
  sideEffects: None&#13;
  timeoutSeconds: 5</pre>&#13;
<p>Notable features of this config are the rules that determine which objects the kube-apiserver should call the webhook for and the <code>clientConfig</code>, which determines the target endpoint and authentication mechanism to be used for the calls.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In our example, we have a <code>clientConfig</code> that references an in-cluster service to contact for webhook calls. Alternatively, the <span class="keep-together"><code>service:</code></span> stanza can be replaced with a <code>url: “https://myexamplewebhook:9443/path”</code> to refer via a specific address rather than a service.</p>&#13;
</div>&#13;
<section data-pdf-bookmark="ValidatingAdmissionWebhook" data-type="sect3"><div class="sect3" id="validatingadmissionwebhook">&#13;
<h3>ValidatingAdmissionWebhook</h3>&#13;
<p>Validating webhooks are powerful tools that can be used to <a contenteditable="false" data-primary="webhooks" data-secondary="validating webhooks" data-type="indexterm" id="idm45358201147848"/><a contenteditable="false" data-primary="validating webhooks" data-type="indexterm" id="idm45358201146392"/><a contenteditable="false" data-primary="security" data-secondary="validating webhooks" data-type="indexterm" id="idm45358201145288"/><a contenteditable="false" data-primary="Portieris" data-type="indexterm" id="idm45358201143912"/><a contenteditable="false" data-primary="kube-apiserver" data-secondary="validating webhooks" data-type="indexterm" id="idm45358201142808"/>enforce security constraints, resource constraints, or best practices for a cluster. A great example is <a href="https://oreil.ly/xpu1F">Portieris</a>, which is used to validate that only images from approved registries are used for containers. The kube-apiserver calls the validating webhook and expects a Boolean response to determine if the request should be admitted.</p>&#13;
<p>An important operational note is that a dependency chain could inadvertently <a contenteditable="false" data-primary="namespaces" data-secondary="validating webhooks" data-type="indexterm" id="idm45358201139832"/>be built that prevents the validating application from starting in the cluster. As noted, it is not uncommon for admission controllers to be run in the cluster; if we were to run Portieris in our cluster in the <code>portieris</code> namespace and then created a webhook rule that required validation before starting any pods in the <code>portieris</code> namespace, we would have to remove the webhook configuration before we could even restart the pods that were serving the webhook. Be sure to avoid such circular dependencies by excluding namespaces from the webhook configuration where appropriate.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="MutatingAdmissionWebhook" data-type="sect3"><div class="sect3" id="mutatingadmissionwebhook">&#13;
<h3>MutatingAdmissionWebhook</h3>&#13;
<p>Mutating webhooks are commonly used to inject sidecars into <a contenteditable="false" data-primary="webhooks" data-secondary="mutating webhooks" data-type="indexterm" id="idm45358201135000"/><a contenteditable="false" data-primary="mutating webhooks" data-type="indexterm" id="idm45358201133544"/>pods, modify resource constraints, apply default settings, and other useful enforcement. There may often be a validating and a mutating option for similar purposes. There may be a validating webhook that ensures that pods are not started without having a specific service account in use, and a mutating alternative might modify the service account of the pod dynamically. Both have similar results: one requires the user to make the modifications required on their own before running their applications, while the other modifies on the fly. There are pros and cons to both approaches. Mutating is typically best reserved for adding function to a pod, such as a sidecar. Validating is better suited for enforcing best practices and forcing users to make the modifications on their own, thus raising awareness of those practices for the team.<a contenteditable="false" data-primary="" data-startref="ch03-adcn" data-type="indexterm" id="idm45358201131448"/><a contenteditable="false" data-primary="" data-startref="ch03-adcn2" data-type="indexterm" id="idm45358201130072"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00007">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, we explored the tools that govern how user applications consume resources in the cluster and how administrators can put rules in place with admission controllers to guide their users to success. We also covered how the Kubernetes scheduler uses these user-provided details to efficiently schedule workload in the cluster. Finally, we covered some of the tools of the trade that can be used to help ensure strong performance of our applications with monitoring and autoscaling.</p>&#13;
<p>When combined, these resource definitions, autoscaling techniques, and admission controls provide the groundwork for more advanced management of resources in any Kubernetes or OpenShift cluster. Proper application of all of these tools can lead to a higher level of application performance, more stability for the system, and lower costs through efficient resource utilization. Now that we have a foundation for understanding how resources are managed in Kubernetes and OpenShift, we will move on to how to use these tools to increase the availability of applications and services.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch01fn21"><sup><a href="ch03.html#ch01fn21-marker">1</a></sup> The Out of Memory Killer is a Linux process with the job of weighing all running processes on the system and selecting one or more for termination when the system is critically low on available memory.</p><p data-type="footnote" id="ch01fn22"><sup><a href="ch03.html#ch01fn22-marker">2</a></sup> See the Kubernetes documentation, <a href="https://oreil.ly/YQjrt">“Reserve Compute Resources for System Daemons”</a>.</p><p data-type="footnote" id="ch01fn23"><sup><a href="ch03.html#ch01fn23-marker">3</a></sup> See the Kubernetes documentation on <a href="https://oreil.ly/gSnJ9">Taints and Tolerations Example Use Cases</a>.</p><p data-type="footnote" id="ch01fn24"><sup><a href="ch03.html#ch01fn24-marker">4</a></sup> See the Kubernetes documentation, <a href="https://oreil.ly/qaWKY">“Considerations for Large Clusters”</a>.</p><p data-type="footnote" id="ch01fn25"><sup><a href="ch03.html#ch01fn25-marker">5</a></sup> See the Kubernetes documentation on <a href="https://oreil.ly/gPLXG">Limit Ranges</a>.</p></div></div></section></body></html>