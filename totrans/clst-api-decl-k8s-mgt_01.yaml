- en: Chapter 1\. Why Kubernetes Adoption Is Complex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern application design has moved from the creation of huge monoliths to a
    more flexible architecture based on microservices running in containers. *Containers*
    are small runtime environments that include the dependencies and configuration
    files the services need to run. Containers are the building blocks of the cloud
    native approach, enabling scalable applications in diverse environments, including
    public, private, and hybrid clouds, as well as bare metal and edge locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the significant advantage of empowering application development teams
    to work in parallel on different services without having to update the entirety
    of an application, the cloud native model offers a number of advantages over monolithic
    architecture from an infrastructure perspective. Containerized applications use
    resources more efficiently than virtual machines (VMs), can run in a broader variety
    of environments, and can be scaled more easily. These advantages have driven wide
    adoption of microservice-based architecture, containers, and the predominant container
    orchestration platform: Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes facilitates the management of these distributed applications, allowing
    you to scale dynamically both horizontally and vertically as needed. Containers
    bring consistency of management to different applications, simplifying operational
    and lifecycle tasks. By orchestrating containers, Kubernetes can operationalize
    the management of applications across an entire environment, controlling and balancing
    resource consumption, providing automatic failover, and simplifying deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Although Kubernetes provides a foundation for resilient and flexible cloud native
    application development, it introduces its own complexities to the organization.
    Running and managing Kubernetes at scale is no easy task, and the difficulties
    are compounded by the inconsistencies between different providers and environments.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes manages a cluster of physical or virtual servers, called *worker
    nodes*, each one of which hosts containers organized into *pods*. A separate,
    smaller number of servers are reserved as *control plane nodes* that make up the
    control plane for the cluster. To support multitenancy, a Kubernetes cluster offers
    logical separation between workloads using *namespaces*—a mechanism for separating
    resources based on ownership—to provide a virtual cluster for each team.
  prefs: []
  type: TYPE_NORMAL
- en: The *control plane* is the main access point that lets administrators and others
    manage the cluster. The control plane also stores state and configuration data
    for the cluster, tells worker nodes when to create and destroy containers, and
    routes traffic in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The control plane consists mainly of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: API Server
  prefs: []
  type: TYPE_NORMAL
- en: The access point through which the control plane, worker agents (kubelets),
    and users communicate with the cluster
  prefs: []
  type: TYPE_NORMAL
- en: Controller manager
  prefs: []
  type: TYPE_NORMAL
- en: A service that manages the cluster using the API server using *controllers**,*
    which bring the state of the cluster in line with specifications
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs: []
  type: TYPE_NORMAL
- en: A distributed key-value store that contains cluster state and configuration
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs: []
  type: TYPE_NORMAL
- en: A service that manages node resources, assigning work based on availability
  prefs: []
  type: TYPE_NORMAL
- en: Kubelet
  prefs: []
  type: TYPE_NORMAL
- en: The agent running on all worker nodes to run pods via a container runtime
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-1](#fig_1_the_components_of_a_kubernetes_cluster) shows the basic
    components of a Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: For high availability, the control plane is often replicated by maintaining
    multiple copies of the essential services and data required to run the cluster
    (mainly the API server and etcd).
  prefs: []
  type: TYPE_NORMAL
- en: '![The components of a Kubernetes cluster](assets/cdkm_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The components of a Kubernetes cluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You manage every aspect of a Kubernetes cluster’s configuration *declaratively*
    (such as deployments, pods, StatefulSets, PersistentVolumeClaim, etc.), meaning
    that you declare the desired state of each component, leaving Kubernetes to ensure
    that reality matches your specification. Kubernetes maintains a controller for
    each object type to bring the state of every object in the cluster in line with
    the declared state. For example, if you declare a certain number of pods, Kubernetes
    ensures that when a node fails, its pods are moved to a healthy node.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Objects and Custom Resource Definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes represents the cluster as *objects*. You create an object declaratively
    by writing a *manifest* file—a YAML document that describes the intended state
    of the object—and running a command to create the object from the file.
  prefs: []
  type: TYPE_NORMAL
- en: A controller makes sure the object exists and matches the state declared in
    the manifest. A controller is essentially a control loop, similar to a voltage
    regulator or thermostat, that knows how to maintain the state of an object within
    specified parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes *resource* is an endpoint in the Kubernetes API that stores a certain
    type of object. You can create a custom resource using a *custom resource definition*
    (CRD) to represent a new kind of object. In fact, some core Kubernetes resources
    now use CRDs because they make it easier to extend and update the capabilities
    of the objects.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes Adoption Journey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many organizations follow a well-traveled path in their adoption of Kubernetes,
    starting with experimentation before they decide whether to rely on it. The journey
    nearly always leads from a single cluster to the complexity of managing many clusters
    in different environments. [Figure 1-2](#fig_2_the_journey_to_kubernetes_adoption)
    shows a typical journey to Kubernetes adoption, beginning with experimentation,
    moving to productization, and finally to developing a managed platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![The journey to Kubernetes adoption ](assets/cdkm_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. The journey to Kubernetes adoption
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Experimenting with Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the experimentation phase, developers drive investigation into the capabilities
    of Kubernetes by containerizing a few projects. The open nature of Kubernetes
    makes it easy to manage on a small scale, using the command-line interface and
    writing scripts to make changes to the cluster and to integrate other open source
    components. At this stage, the organization often has not yet engaged with security,
    upgrades, availability, and other concerns that become important later. As their
    needs change, the team makes configuration changes and integrates components gradually,
    often without documenting the evolution of the cluster. When the time comes to
    support a broader array of environments or expand access to more teams, it becomes
    apparent that the cluster is tailor-made for the small set of use cases it has
    been serving thus far.
  prefs: []
  type: TYPE_NORMAL
- en: Productizing Kubernetes for the Organization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the organization begins to recognize the value of Kubernetes, teams begin
    to investigate how to scale general-purpose clusters that can serve the needs
    of the entire company. Departments such as service reliability and IT operations
    begin looking for ways to make Kubernetes secure, supportable, and manageable.
    These teams begin advocating for prescriptive, off-the-shelf solutions that trade
    flexibility for reliability. As the organization prepares to scale Kubernetes
    to fit its needs, it might find that these solutions are rigid and tend to silo
    each cluster, making cross-team work more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Developing Kubernetes as a Managed Platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make Kubernetes work for the business needs of the organization, it must
    be possible to easily deploy, maintain, and scale clusters that are highly available
    and can handle applications and workloads that dynamically meet changing demands.
    The goal of the organization must therefore be to make Kubernetes operate as a
    modern platform at scale, providing resources to multiple teams.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the investigation often focuses on commercial Kubernetes platforms
    that can solve the organization’s problems out of the box. The team likely has
    enough experience to look for features such as flexibility, repeatable deployment,
    and the ability to manage multiple clusters across different environments, including
    diverse cloud platforms, virtualized or bare metal data centers, and increasingly,
    edge locations. IT operations and platform engineering teams will especially be
    looking at securing cluster access, configuration management, and Day 2 concerns
    like scaling, upgrades, quota control, logging, monitoring, continuity, and others.
  prefs: []
  type: TYPE_NORMAL
- en: As the platform takes shape and evolves, the tendency is to break larger multitenant
    clusters into smaller special-purpose clusters. This approach allows more flexible
    management, more efficient use of resources, and a more tailored experience for
    the teams using the clusters within the organization. From a security standpoint,
    smaller clusters help make defense in depth policies easier to implement, reducing
    the “blast radius” in the case of a breach. Operating multiple clusters also makes
    it possible to deploy apps wherever needed, whether in the cloud or on premises,
    or even at the edge.
  prefs: []
  type: TYPE_NORMAL
- en: The Challenges of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every environment deals with infrastructure differently, meaning different challenges
    for every type of Kubernetes deployment. Provisioning, upgrading, and maintaining
    the infrastructure to support the control plane can be difficult and time-consuming.
    Once the cluster is up, integrating basic components like storage, networking,
    and security presents significant hurdles. Finally, in a deployment of multiple
    clusters across a variety of environments, each cluster must be managed individually.
    There is no native tool in Kubernetes itself for managing the clusters as a group,
    and differences between environments introduce operational nuances to each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below the layer of Kubernetes are the resources that the cluster needs, mainly
    storage, networking, and the physical or virtual machines where the cluster runs.
    This means that running a cluster involves two separate lifecycles that must be
    managed concurrently: Kubernetes and the hardware and operating system supporting
    it. Because Kubernetes is quickly evolving, these two layers must be kept in sync.
    Many problems with node or cluster availability come from incompatibilities between
    the versions of the operating system and the Kubernetes components. These problems
    become exponentially more difficult in multicluster deployments because the problem
    of keeping Kubernetes and the operating system in sync is multiplied by the number
    of platforms, each with its own complexities, then multiplied again by any specific
    versions or flavors required by the teams using the clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, Kubernetes didn’t include any tools for managing the cluster infrastructure.
    Bringing up machines and installing Kubernetes were manual procedures. Creating
    clusters outside of managed Kubernetes environments required a lot of effort and
    custom tooling. The Kubernetes community needed a way to provide common tools
    to bootstrap a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To meet this challenge, the Kubernetes community began developing tools to
    simplify provisioning and maintaining clusters. Here are some examples that were
    developed over the last few years:'
  prefs: []
  type: TYPE_NORMAL
- en: Kube-up
  prefs: []
  type: TYPE_NORMAL
- en: The first tool to bring up a Kubernetes cluster (2015)
  prefs: []
  type: TYPE_NORMAL
- en: kubeadm
  prefs: []
  type: TYPE_NORMAL
- en: A tool for initializing, configuring, and upgrading Kubernetes clusters (2016)
  prefs: []
  type: TYPE_NORMAL
- en: Kops
  prefs: []
  type: TYPE_NORMAL
- en: A tool for Kubernetes lifecycle management, including infrastructure and dependencies
    (2016)
  prefs: []
  type: TYPE_NORMAL
- en: Kubicorn
  prefs: []
  type: TYPE_NORMAL
- en: A cluster management framework with modular support for cloud providers (2018)
  prefs: []
  type: TYPE_NORMAL
- en: These tools were a great step in the direction of integrated cluster management
    but fell short of a complete solution. More complex workloads began to require
    more sophisticated deployments, including multiple clusters. Managing these deployments,
    in turn, became more difficult. In particular, it was clear there was a need to
    be able to manage clusters consistently and declaratively using a cohesive API—the
    way that Kubernetes manages nodes, pods, and containers.
  prefs: []
  type: TYPE_NORMAL
