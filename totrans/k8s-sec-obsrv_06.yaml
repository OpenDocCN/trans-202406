- en: Chapter 6\. Observability and Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will explain how an observability platform can help improve the
    security of your Kubernetes cluster. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.xhtml#observability-id000002) we covered best practices
    for implementing log collection. In this chapter, we will focus on how to build
    a system that helps generate high-fidelity alerts. We will also discuss the use
    of machine learning for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Security operations center
  prefs: []
  type: TYPE_NORMAL
- en: We will review a reference implementation of a security operations center (SOC)
    and how observability can help you build an SOC for your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral analytics
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the concept of user and entity behavior analytics (UEBA) and how
    to implement it in your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter we discussed how to implement logging for your Kubernetes
    cluster. An effective alerting system must include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The system should be able to automatically run queries across various log data
    sources (e.g., Kubernetes activity logs, network logs, application logs, DNS logs,
    etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system must be able to support a state machine that is used to generate
    events for a specified number of threshold violations in a specified duration.
    The system must also support setting a time period for the query (known as *lookback*).
    We will cover an example in the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system must be able to export actionable alerts to external security, information,
    and event management (SIEM) so that they can be a part of the incident response
    process in an enterprise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerting systems are available from major cloud providers that can help you
    define alerts on logs collected in the cloud provider environment. Google Cloud
    has an excellent resource to [learn about its alerting capabilities](https://oreil.ly/DyLh3).
    Amazon Web Services (AWS) also has similar alerting capabilities. These alerts
    work on logs collected in the cloud provider’s logging system and allow you to
    define rules to trigger alerts based on thresholds. For example, the number of
    API calls to an API endpoint in a given time period can be an indicator of a potential
    denial of service (DoS) attack. While these systems are good for general logging
    and alerting, a log collection system that is native to Kubernetes, like the one
    we covered in [Chapter 5](ch05.xhtml#observability-id000002), is necessary to
    detect security-based events in your Kubernetes cluster, as it correlates data
    at the time of collection and makes it easy to define alerts on one log source.
    Also, an alerting system native to Kubernetes will help you define alerts using
    Kubernetes constructs like deployments, labels, etc., as it will be able to enhance
    log data with the right context so queries are simple. (For example, you do not
    need to join a set of labels to services and labels to IPs in network flow logs
    to query network activity for a service.)
  prefs: []
  type: TYPE_NORMAL
- en: Figures [6-1](#configuring_an_alert_operation) and [6-2](#query_configuration_for_an_alert)
    show you an example of how you can define an alert to detect lateral movement
    in your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Configuring an alert operation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-1](#configuring_an_alert_operation) shows how you can configure the
    operation of an alert and metadata like name, description, severity, and time
    period to poll for the data and the lookback period, which is how far back the
    system will look when querying the data. You can also define thresholds for occurrences
    of threshold violations before an alert is triggered. It is also important to
    be able to configure the format of the output; in this example the data will be
    aggregated by the source of the traffic (namespace and deployment) when the alert
    is reported. The output alert data helps facilitate the management of the alert
    by downstream systems (e.g., SIEM).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Query configuration for an alert
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-2](#query_configuration_for_an_alert) shows an example of query configuration
    for the alert. The things to note in the query are the ability to use Kubernetes
    metadata (e.g., labels) with network flow activity (e.g., destination, protocol)
    and policy verdict (e.g., action) in a single query. This gives you a lot of flexibility
    in defining alerts that are effective in your Kubernetes cluster. Please note
    that this is a representative example of how you should think about building an
    effective alerting system. In addition to the cloud provider’s alerting systems,
    there are several other tools like Datadog, Sysdig, and Calico Enterprise that
    provide Kubernetes-native alerting systems.'
  prefs: []
  type: TYPE_NORMAL
- en: The alerting systems we covered previously are great at detecting and reporting
    alerts when your system has predictable behavior and you can easily define thresholds
    for normal activity of the system. It would be great for an alerting system to
    be able to “learn” the behavior of the system and be able to dynamically define
    thresholds; this will help generate high-fidelity alerts and reduce false positives
    due to thresholds not changing with the state of the system. Let’s explore how
    machine learning can be used to help with this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning fundamentals and how it works are outside the scope of this
    book. In this section we will review a few concepts of machine learning that will
    show us how it can help with learning the behavior of a given metric and alerting
    on deviations from the expected behavior. Before we do that, let’s review the
    high-level techniques in machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs: []
  type: TYPE_NORMAL
- en: This is a technique where the system is trained by labeling test data over a
    period of time. It allows the system to use the learnings to classify new data
    and predict outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a technique where algorithms are used to detect and classify patterns
    in data that is not labeled. Please note that there are many resources to understand
    these concepts; one example is Julianna Delua’s[“Supervised vs. Unsupervised Learning:
    What’s the Difference?”](https://oreil.ly/9aBfa). Given the ephemeral nature of
    entities (e.g., pods) in a Kubernetes cluster and our goal of detecting anomalies
    in the data generated from that activity, we recommend using the technique of
    unsupervised learning for detecting anomalies.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselining
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselining is a technique used to build a model in machine learning that can
    continuously predict values for a given metric (e.g., connections per second)
    and detect anomalies (deviations) from the expected value. [CMU ML’s blog post
    “3–Baselines”](https://oreil.ly/zpeNO) is a great resource for understanding how
    baselines work and the different types of models that can be built using baselining.
    As mentioned in the blog, it is possible to create simple models that are very
    effective in achieving human-level performance. This is exactly what we want in
    our alerting system: The system should automatically define thresholds and alert
    on deviations from the baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the high-level techniques we should use, let’s look at
    some example machine learning jobs that help with implementing observability and
    securing a Kubernetes cluster. In a dynamic environment like Kubernetes, where
    workloads are ephemeral and can be restarted/scheduled on a different node, it
    is not practical in most cases to use a rule-based engine to detect anomalies.
    What is needed is the anomaly-detection engine layered over a machine learning
    engine that reports deviations from the baseline for any given metric.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Machine Learning Jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How to create a machine learning model is outside the scope of this book; you
    should have the data science team build models for your deployments. Major cloud
    providers like Google Cloud offer a [service to build machine learning models
    for Kubernetes workloads](https://oreil.ly/0JxKq) that can help the data science
    team implement the right ML model. The following are some examples of ML jobs
    that are effective to detect anomalous events in your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: IP sweep detection
  prefs: []
  type: TYPE_NORMAL
- en: The job looks for pods in your cluster that are sending packets to many destinations.
    This may indicate an attacker has gained control of a pod and is gathering reconnaissance
    on what else they can reach. The job compares pods both with other pods in their
    replica set and with other pods in the cluster generally.
  prefs: []
  type: TYPE_NORMAL
- en: Port scan detection
  prefs: []
  type: TYPE_NORMAL
- en: The job looks for pods in your cluster that are sending packets to one destination
    on multiple ports. This may indicate an attacker has gained control of a pod and
    is gathering reconnaissance on what else they can reach. The job compares pods
    both with other pods in their replica set and with other pods in the cluster generally.
  prefs: []
  type: TYPE_NORMAL
- en: Service bytes anomaly
  prefs: []
  type: TYPE_NORMAL
- en: The job looks for services that receive/send an anomalously high amount of data.
    This could indicate a denial of service attack, data exfiltrating, or other attacks.
    The job looks for services that are unusual with respect to their replica set,
    and replica sets that are unusual with respect to the rest of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Process restarts anomaly
  prefs: []
  type: TYPE_NORMAL
- en: The job looks for pods with an excessive number of the process restarts. This
    could indicate problems with the processes, such as resource problems or attacks.
    The job looks for pods that are unusual with respect to their process restart
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: DNS latency anomaly
  prefs: []
  type: TYPE_NORMAL
- en: The job looks for the clients that have too-high latency of DNS requests. This
    could indicate a denial of service attack.
  prefs: []
  type: TYPE_NORMAL
- en: L7 latency anomaly
  prefs: []
  type: TYPE_NORMAL
- en: The job looks for the pods that have too-high latency of L7 requests. All HTTP
    requests are measured here. This anomaly could indicate a denial of service attack
    or other attacks.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP connection spike anomaly
  prefs: []
  type: TYPE_NORMAL
- en: The job looks for services that get too many HTTP inbound connections. This
    anomaly could indicate a denial of service attack.
  prefs: []
  type: TYPE_NORMAL
- en: This list gives you examples of jobs you can use to detect anomalies. You can
    use [this resource offered by Google Cloud](https://oreil.ly/1FaTm) to build a
    machine learning job for your Kubernetes cluster. Note the descriptions of each
    job rely on a set of context-rich logs that are native to Kubernetes (e.g., comparing
    pods to other pods in a replica set, using bytes sent to/from a service).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered how to build an effective alerting system to detect
    and report anomalies, let’s look at an example implementation of a security operations
    center for your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Security Operations Center
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we will review a reference implementation for a security operations
    center (SOC) for a SaaS service based on Kubernetes. An SOC is used to detect
    and respond to security events; we will explore how to leverage observability
    when you implement an SOC for your Kubernetes-based services. Note this is a sample
    and should be used as an example to guide your implementation. When you implement
    this in production, you should use these concepts but will need to design and
    implement an SOC suited to your use case. [Figure 6-3](#sample_implementation_of_an_soc_in_goog)
    shows an SOC implementation for your service hosted in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Sample implementation of an SOC in Google Cloud
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-3](#sample_implementation_of_an_soc_in_goog) shows a set of Kubernetes
    clusters running in Google Cloud with a namespace representing a tenant. Each
    tenant cluster can be deployed in Google Kubernetes Engine (GKE) or as an upstream
    Kubernetes cluster in Google Cloud. There is an ingress representing how the service
    is accessed by external entities. The details of the workload deployment and provisioning
    are omitted from the figure, as we want to focus on how you can secure the service.
    In order to secure the service, you need logging and monitoring and alerting.
    This can be achieved by using the [Google Cloud operations suite](https://oreil.ly/9rTas),
    which provides capabilities to support logging and monitoring and alerting. In
    case you are using GKE, [Google Cloud’s blog](https://oreil.ly/QCGa0) describes
    how to leverage these services to detect and manage alerts for your Kubernetes
    clusters. As mentioned before, you need to leverage ML for baselining and to improve
    the quality of alerts. Google offers a set of ML services known as [AI Hub](https://oreil.ly/ICHTe).
    Note you still need to build ML models that are relevant and effective for your
    SaaS service (see the example ML jobs earlier in this chapter). You can then use
    well-known tools like [OpsGenie](https://oreil.ly/gNSbr) to route alerts for alert
    management to SIEM, Slack, PagerDuty, JIRA, and other tools. These alerts will
    then trigger the remediation workflows as defined by the security team. Note we
    have used Google Cloud as an example, but you can use the previously mentioned
    approach to build an SOC for AWS and Azure. These cloud providers also have a
    similar set of services available to users.'
  prefs: []
  type: TYPE_NORMAL
- en: The previously mentioned approach is very effective in case you are using only
    one cloud provider and do not have any workloads running on-premise or in other
    cloud provider environments. Also, all the services mentioned earlier will increase
    the cost of deployment, and you also need to delegate some of your DevOps/DevSecOps
    resources to implement and manage these services. Therefore, we recommend that
    you build your SOC using a tool that is agnostic to any cloud provider and can
    be used across cloud providers/on-premise environments.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-4](#soc_using_a_kubernetes_native_platform) shows how you can replace
    some of the cloud provider–specific components and create an SOC using a Kubernetes-native
    observability and security platform. You can build the platform yourself or you
    can choose to use products that offer these platforms, such as Datadog, VMware,
    and Calico Enterprise. When you choose products, keep in mind the concepts covered
    in the previous section about alerting, and ensure that the platform supports
    integrations to your remediation/management systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed how you build an SOC that is effective for your Kubernetes
    cluster, let’s review another application of observability to secure your Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. SOC using a Kubernetes-native platform
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: User and Entity Behavior Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: User and entity behavior analytics (UEBA) is an area where you use ML- and AI-based
    techniques to profile the behavior of a user or an entity (e.g., a pod, service,
    or deployment) over time and detect anomalous behavior by the user/entity. Microsoft
    Azure offers UEBA as a part of its cloud platform. Microsoft Azure’s blog post,
    [Identify Advanced Threats with User and Entity Behavior Analytics (UEBA) in Azure
    Sentinel”](https://oreil.ly/FDspV), and an excellent resources that describes
    how you can use UEBA for security use cases. Note that anomalous behavior by an
    entity is not always suspicious behavior; you need to map the behavior to frameworks
    like the MITRE attack framework or other indicators of compromise to confirm it
    is a security issue.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a simple example of how you can implement UEBA for an entity in Kubernetes,
    such as a service.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-5](#profiling_the_behavior_of_a_kubernetes) shows a service in your
    Kubernetes cluster and the various interactions of the service we will consider
    when we profile the behavior of the service. As a part of its normal operation,
    the service will interact with the Kubernetes API server and the Kubernetes datastore.
    In addition, it will interact with the ingress resource to communicate with entities
    external to the cluster and use cluster networking to interact with other entities
    inside the cluster. The service will also use the DNS service in the cluster for
    its operation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Profiling the behavior of a Kubernetes service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In order to build a profile for the service, we would need to consider the following
    aspects of the service. These are known as features in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Service composition (number of endpoints such as pods, RBAC, policies)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filesystem activity, process information, and system call activity for the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service accounts associated with the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service life cycle operations (e.g., create, delete, scale up/down)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic to and from the service (network, application)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS activity by pods in the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The UEBA engine shown in [Figure 6-5](#profiling_the_behavior_of_a_kubernetes)
    will collect logs from various data sources (network flow logs, application flow
    logs, Kubernetes audit logs, DNS activity logs, process information, filesystem,
    syscall activity logs) and store them in the datastore. These logs are aggregated
    and correlated by the analytics engine to generate correlated logs for the service
    across various features.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning engine uses a complex model to baseline the behavior of
    the service across various features. This is an advanced concept in machine learning
    where the model considers each feature and interactions between features, among
    other things, to build a profile for the service. This is best implemented by
    your data science team. This profile is then used to predict anomalies and generate
    alerts for deviations. There is a dashboard to allow SOC operators to review analyzed
    data and use it for forensics or to hunt threats. Please note a security and observability
    platform built using the concepts described in [Chapter 5](ch05.xhtml#observability-id000002)
    will help build an effective UEBA system.
  prefs: []
  type: TYPE_NORMAL
- en: UEBA is an advanced technique and is complex to implement, but it is a very
    effective way to quickly find out which entities in your cluster are potentially
    vulnerable. This makes SOC operation very efficient and scalable. Once your deployment
    scales up to several clusters (50+), it is not practical to use alerts/manual
    reviews of dashboards to find real issues. UEBA will alert you to entities that
    are abnormal and need immediate attention.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you think about how you can use observability to help secure your cluster,
    please consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The alerting system you use must be Kubernetes-native and must support baselining
    using ML so that you do not have to manually define thresholds for various features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended that you consider a Kubernetes-native platform that works
    across your cloud and on-premise deployments to build your SOC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UEBA is an advanced concept and is complex to implement, but it can be very
    effective in securing a Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
