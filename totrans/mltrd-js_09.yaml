- en: Chapter 8\. Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now you should be pretty familiar with building multithreaded applications
    using JavaScript, whether it be code that runs in a user’s browser or your server,
    or even applications that employ both. And, while this book provides a lot of
    use cases and reference material, at no point did it say “you should add multithreading
    to your application,” and there’s an important reason for this.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: By and large the main reason to add workers to an application is to increase
    performance. But this trade-off comes with a cost of added complexity. The *KISS
    principle*, meaning “Keep It Simple, Stupid,” suggests that your applications
    should be so stupidly simple that anyone can quickly look at the code and get
    an understanding of it. Being able to read code after it has been written is of
    paramount importance and simply adding threads to a program without purpose is
    an absolute violation of KISS.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are absolutely good reasons to add threads to an application, and as long
    as you’re measuring performance and confirming that speed gains outweigh added
    maintenance costs, then you’ve found yourself a situation deserving of threads.
    But how do you identify situations where threads will or will not help without
    going through all the work of implementing them? And how do you go about measuring
    performance impact?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: When Not to Use
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threading is not a magic bullet capable of solving an application’s performance
    problems. It is usually not the lowest-hanging fruit when it comes to performance,
    either, and should often be done as a final effort. This is particularly true
    in JavaScript, where multithreading isn’t as widely understood by the community
    as other languages. Adding threading support may require heavy changes to an application,
    which means your effort-to-performance gains will likely be higher if you first
    hunt down other code inefficiencies first.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Once that’s done, and you’ve made your application performant in other areas,
    you are then left with the question, “Is now a good time to add multithreading?”
    The rest of this section contains some situations where adding threads will most
    likely not provide any performance benefits. This can help you avoid going through
    some of the discovery work.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Low Memory Constraints
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is some additional memory overhead incurred when instantiating multiple
    threads in JavaScript. This is because the browser needs to allocate additional
    memory for the new JavaScript environment—this includes things like globals and
    APIs available to your code as well as under-the-hood memory used by the engine
    itself. This overhead might prove to be minimal in a normal server environment
    in the case of Node.js or a beefy laptop in the case of browsers. But it could
    be a hindrance if you’re running code on an embedded ARM device with 512 MB of
    RAM or donated netbooks in a K–12 classroom.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: What’s the memory impact of additional threads? It’s a little hard to quantify,
    and it changes depending on the JavaScript engine and platform. The safe answer
    is that, like most performance aspects, you should measure it in a real-world
    environment. But we can certainly try to get some concrete numbers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s consider a dead simple Node.js program that just kicks off a timer
    and doesn’t pull in any third-party modules. This program looks like the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Running the program and measuring memory usage looks like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `pstree` command displays the threads used by the program. It displays
    the main V8 JavaScript thread, as well as some of the background threads covered
    in [“Hidden Threads”](ch01.xhtml#sec_hidden_threads). Here is an example output
    from the command:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `ps` command displays information about the process, notably the memory
    usage of the process. Here’s an example of the output from the command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are two important variables here used to measure the memory usage of a
    program, both of them measured in kilobytes. The first here is `VSZ`, or *virtual
    memory size*, which is the memory the process can access including swapped memory,
    allocated memory, and even memory used by shared libraries (such as TLS), approximately
    1.4 GB. The next is `RSS`, or *resident set size*, which is the amount of physical
    memory currently being used by the process, approximately 48 MB.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Measuring memory can be a little hand wavy, and it’s tricky to estimate how
    many processes can actually fit in memory. In this case, we’ll mostly be looking
    at the RSS value.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider a more complicated version of the program using threads.
    Again, the same dead simple timer will be used, but in this case there will be
    a total of four threads created. In this case a new *worker.js* file is required:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Running the *leader.js* program with a numerical argument greater than 0 allows
    the program to create additional workers. [Table 8-1](#table_thread_overhead)
    is a listing of the memory usage output from `ps` for each of the different iterations
    of additional threads.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Thread memory overhead with Node.js v16.5
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '| Add Threads | VSZ | RSS | SIZE |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| 0 | 318,124 KB | 31,836 KB | 47,876 KB |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| 1 | 787,880 KB | 38,372 KB | 57,772 KB |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| 2 | 990,884 KB | 45,124 KB | 68,228 KB |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1,401,500 KB | 56,160 KB | 87,708 KB |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2,222,732 KB | 78,396 KB | 126,672 KB |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| 16 | 3,866,220 KB | 122,992 KB | 205,420 KB |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '[Figure 8-1](#chart_threads_memory) displays the correlation between RSS memory
    and thread count.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison of thread count with memory usage](Images/mtjs_0801.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Memory usage increases with each additional thread
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With this information it appears that the added RSS memory overhead of instantiating
    each new thread, using Node.js 16.5 on an x86 processor, is approximately 6 MB.
    Again, this number is a bit hand wavy, and you’ll need to measure it in your particular
    situation. Of course, the memory overhead is compounded when the threads pull
    in more modules. If you were to instantiate heavy frameworks and web servers in
    each thread you may end up adding hundreds of megabytes of memory to your process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While it’s becoming increasingly rare to find them, programs running on a 32-bit
    computer or smart phone have a maximum addressable memory space of 4 GB. This
    limit is shared across any threads in the program.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Low Core Count
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your application will run slower in situations where it has fewer cores. This
    is especially true if the machine has a single core, and it can also be true if
    it has two cores. Even if you employ a thread pool in your application and scale
    the pool based on the core count, the application will be slower if it creates
    a single worker thread. When creating an additional thread, the application now
    has at least two threads (the main and the worker), and the two threads will compete
    with each other for attention.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Another reason your application will slow down is that there is additional overhead
    when it comes to communicating between threads. With a single core and two threads,
    even if the two never compete for resources, i.e., the main thread has no work
    to do while the worker is running and vice versa, there is still an overhead when
    performing message passing between the two threads.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: This might not be a huge deal. For example, if you create a distributable application
    that runs in many environments, often running on multicore systems and infrequently
    on single-core systems, then this overhead might be OK. But if you’re building
    an application that almost entirely runs in a single-core environment, you would
    likely be better off by not adding threading at all. That is, you probably shouldn’t
    build an app that takes advantage of your beefy multicore developer laptop and
    then ship it to production where a container orchestrator limits the app to a
    single core.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: How much of a performance loss are we talking? On the Linux operating system
    it’s straightforward to tell the OS that a program, and all of its threads, should
    only run on a subset of CPU cores. The use of this command allows developers to
    test the effects of running a multithreaded application in a low core environment.
    If you’re using a Linux-based computer, then feel free to run these examples;
    if not, a summary will be provided.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'First, go back to the *ch6-thread-pool/* example that you created in [“Thread
    Pool”](ch06.xhtml#ch_patterns_sec_threadpool). Execute the application so that
    it creates a worker pool with two workers:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note that with a thread pool of 2, the application has three JavaScript environments
    available, and `libuv` should have a default pool of 5, leading to a total of
    about eight threads as of Node.js v16\. With the program running and able to access
    all of the cores on your machine, you’re ready to run a quick benchmark. Execute
    the following command to send a barrage of requests to the server:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this case we’re just interested in the average request rate, identified in
    the last table of the output with the Req/Sec row and the Avg column. In one sample
    run the value of 17.5 was returned.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Kill the server with Ctrl+C and run it again. But this time use the `taskset`
    command to force the process (and all of its child threads) to use the same CPU
    core:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this case the two environment variables `THREADS` and `STRATEGY` are set,
    then the `taskset` command is run. The `-c 0` flag tells the command to only allow
    the program to use the 0th CPU. The arguments that follow are then treated as
    the command to run. Note that the `taskset` command can also be used to modify
    an already running process. When that happens the command displays some useful
    output to tell you what happens. Here’s a copy of that output when the command
    is used on a computer with 16 cores:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case it says that the program used to have access to all 16 cores (0–15),
    but now it only has access to one (0).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'With the program running and locked to a single CPU core to emulate an environment
    with fewer cores available, run the same benchmark command again:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In one such run the average requests per second has been reduced to 8.32\. This
    means that the throughput of this particular program, when trying to use three
    JavaScript threads in a single-core environment, leads to a performance of 48%
    when compared to having access to all cores!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural question might be: in order to maximize the throughput of the *ch6-thread-pool*
    application, how large should the thread pool be and how many cores should be
    provided to the application? To find an answer, 16 permutations of the benchmark
    were applied to the application and the performance was measured. The length of
    the test was doubled to two minutes to help reduce any outlying requests. A tabular
    version of this data is provided in [Table 8-2](#table_core_vs_thread_perf).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-2\. Available cores versus thread pool size and how it affects throughput
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 1 core | 2 cores | 3 cores | 4 cores |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| 1 thread | 8.46 | 9.08 | 9.21 | 9.19 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| 2 threads | 8.69 | 9.60 | 17.61 | 17.28 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| 3 threads | 8.23 | 9.38 | 16.92 | 16.91 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| 4 threads | 8.47 | 9.57 | 17.44 | 17.75 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: A graph of the data has been reproduced in [Figure 8-2](#graph_core_vs_thread_perf).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: In this case there is an obvious performance benefit when the number of threads
    dedicated to the thread pool is at least two and the number of cores available
    to the application is at least three. Other than that, there isn’t anything too
    interesting about the data. When measuring the effects of cores versus threads
    in a real-world application, you will likely see more interesting performance
    trade-offs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'One question posed by this data is: why doesn’t adding more than two threads
    or three threads make the application any faster? Answering questions like these
    will require hypotheses, experimenting with application code, and trying to erase
    any bottlenecks. In this case it may be that the main thread is so busy coordinating,
    handling requests, and communicating with threads, that the worker threads aren’t
    able to get much work done.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![Two Threads and three cores](Images/mtjs_0802.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Available cores versus thread pool size and how it affects throughput
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Containers Versus Threads
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to writing server software, like with Node.js, the rule of thumb
    is that processes should scale horizontally. This is a fancy term meaning you
    should run multiple redundant versions of the program in an isolated manner—such
    as within a Docker container. Horizontal scaling benefits performance in a way
    that allows developers to fine-tune the performance of the whole fleet of applications.
    Such tuning can’t be performed as easily when the scaling primitive happens within
    the program, in the form of a thread pool.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrators, such as Kubernetes, are tools that run containers across multiple
    servers. They make it easy to scale an application on demand; during the holiday
    season an engineer can manually increase the number of instances running. Orchestrators
    can also dynamically change the scale depending on other heuristics like CPU usage,
    traffic throughput, and even the size of a work queue.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: How might this dynamic scaling look if it were performed within an application
    at runtime? Well, certainly the available thread pool would need to be resized.
    There would also need to be some sort of communication in place, allowing an engineer
    to send messages to the processes to resize the pool; perhaps an additional server
    needs to listen on a port for such administrative commands. Such functionality
    then requires additional complexity to be added to the application code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: While adding additional processes instead of increasing thread count increases
    overall resource consumption, not to mention the overhead of wrapping processes
    in a container, larger companies usually prefer the scaling flexibility of this
    approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: When to Use
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes you’ll get lucky and will end up with a problem that benefits greatly
    from a multithreaded solution. Here are some of the most straightforward characteristics
    of such a problem to keep an eye out for:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Embarrassingly parallel
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a class of problems where a large task can be broken down into smaller
    tasks and very little or no sharing of state is required. One such problem is
    the Game of Life simulation covered in [“Example Application: Conway’s Game of
    Life”](ch05.xhtml#ch_adv_shared_mem_sec_app). With that problem, the game grid
    can be subdivided into smaller grids, and each grid can be dedicated to an individual
    thread.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Heavy math
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Another characteristic of problems that are a good fit for threads are those
    that involve a heavy use of math, aka CPU-intensive work. Sure, one might say
    that everything a computer does is math, but the inverse of a math-heavy application
    is one that is I/O heavy, or one that mostly deals with network operations. Consider
    a password hash cracking tool that has a weak SHA1 digest of a password. Such
    tools may work by running the Secure Hash Algorithm 1 (SHA1) algorithm over every
    possible combination of 10 character passwords, which is a lot of number crunching
    indeed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce-friendly problems
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce is a programming model that is inspired by functional programming.
    This model is often used for large-scale data processing that has been spread
    across many different machines. MapReduce is broken into two pieces. The first
    is Map, which accepts a list of values and produces a list of values. The second
    is Reduce, where the list of values are iterated on again, and a singular value
    is produced. A single-threaded version of this could be created in JavaScript
    using `Array#map()` and `Array#reduce()`, but a multithreaded version requires
    different threads processing subsets of the lists of data. A search engine uses
    Map to scan millions of documents for keywords, then Reduce to score and rank
    them, providing a user with a page of relevant results. Database systems like
    Hadoop and MongoDB benefit from MapReduce.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Graphics processing
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: A lot of graphics processing tasks also benefit from multiple threads. Much
    like the Game of Life problem, which operates on a grid of cells, images are represented
    as a grid of pixels. In both cases the value at each coordinate can be represented
    as a number, though Game of Life uses a single 1-bit number while images are more
    likely to use 3 or 4 bytes (red, green, blue, and optional alpha transparency).
    Image filtering then becomes a problem of subdividing an image into smaller images,
    having threads in a thread-pool process with the smaller images in parallel, then
    updating the interface once the change is complete.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t a complete list of all the situations in which you should use multithreading;
    it’s just a list of some of the most obvious use cases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: One of the repeating themes is that problems that don’t require shared data,
    or at least that don’t require coordinated reads and writes to shared data, are
    easier to model using multiple threads. Though it’s generally beneficial to write
    code that doesn’t have many side effects, this benefit is compounded when writing
    multithreaded code.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Another use case that’s particularly beneficial to JavaScript applications is
    that of template rendering. Depending on the library used, the rendering of a
    template might be done using a string that represents the raw template and an
    object that contains variables to modify the template. With such use cases there
    usually isn’t much global state to consider, just the two inputs, while a single
    string output is returned. This is the case with the popular template rendering
    packages `mustache` and `handlebars`. Offloading template rendering from the main
    thread of a Node.js application seems like a reasonable place to gain performance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test this assumption out. Create a new directory named *ch8-template-render/*.
    Inside this directory, copy and paste the existing *ch6-thread-pool/rpc-worker.js*
    file from [Example 6-3](ch06.xhtml#ex_threadpool_rpcworker_1). Although the file
    will work fine unmodified, you should comment out the `console.log()` statement
    so that it doesn’t slow down the benchmark.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll also want to initialize an npm project and install some basic packages.
    You can do this by running the following commands:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, create a file named *server.js*. This represents an HTTP application that
    performs basic HTML rendering when it receives a request. This benchmark is going
    to use some real-world packages instead of loading built-in modules for everything.
    Start the file off with the contents of [Example 8-1](#ex_template_server_1).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-1\. *ch8-template-render/server.js* (part 1)
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The file starts off by instantiating the Fastify web framework, as well as a
    worker pool with four workers. The application also loads a module named *template.js*
    that will be used to render templates used by the web application.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Now, you’re ready to declare some routes and to tell the server to listen for
    requests. Keep editing the file by adding the content from [Example 8-2](#ex_template_server_2)
    to it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-2\. *ch8-template-render/server.js* (part 2)
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Two routes have been introduced to the application. The first is `GET /main`
    and will perform the rendering of the request in the main thread. This represents
    a single-threaded application. The second route is `GET /offload`, where the rendering
    work will be offloaded to a separate worker thread. Finally, the server is instructed
    to listen on port 3000.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: At this point the application is functionally complete. But as an added bonus,
    it would be nice to be able to quantify the amount of work that the server is
    busy doing. While it’s true that we can primarily test the efficiency of this
    application by using an HTTP request benchmark, sometimes it’s nice to look at
    other numbers as well. Add the content from [Example 8-3](#ex_template_server_3)
    to finish off the file.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-3\. *ch8-template-render/server.js* (part 3)
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code uses a `setInterval` call that runs every second. It wraps a `setImmediate()`
    call, measuring current time in nanoseconds before and after the call is made.
    It’s not perfect, but it is one way to approximate how much load the process is
    currently receiving. As the event loop for the process gets busier, the number
    that is reported will get higher. Also, the busyness of the event loop affects
    the delay of asynchronous operations throughout the process. Keeping this number
    lower therefore correlates to a more performant application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Next, create a file named *worker.js*. Add the content from [Example 8-4](#ex_template_worker)
    to it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-4\. *ch8-template-render/worker.js*
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is a modified version of the worker file that you created before. In this
    case a single command is used, `renderLove()`, which accepts an object with key
    value pairs to be used by the template rendering function.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Finally, create a file named *template.js*, and add the content from [Example 8-5](#ex_template_template)
    to it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-5\. *ch8-template-render/template.js*
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In a real-world application, this file might be used for reading template files
    from disk and substituting values, exposing a complete list of templates. For
    this simple example just a single template renderer is exported and a single hard-coded
    template is used. This template uses two variables, `me` and `you`. The string
    is repeated many times to approach the length of a template that a real application
    might use. The longer the template, the longer it takes to render.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the files have been created, you’re ready to run the application.
    Run the following commands to run the server and then to launch a benchmark against
    it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: On a test run on a beefy 16-core laptop, when rendering templates entirely in
    the main thread, the application had an average throughput of 13,285 requests
    per second. However, when running the same test while offloading template rendering
    to a worker thread, the average throughput was 18,981 requests per second. In
    this particular situation it means the throughput increased by about 43%.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The event loop latency also decreased significantly. Sampling the time it takes
    to call `setImmediate()` while the process is idle gets us about 87 μs on average.
    When performing template rendering in the main thread, the latency averages 769
    μs. The same samples taken when offloading rendering to a worker thread are on
    average 232 μs. Subtracting out the idle state from both values means it’s about
    a 4.7x improvement when using threads. [Figure 8-3](#chart_eventloop_delay) compares
    these samples over time during the 60-second benchmark.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![The event loop is always further delayed when single threaded](Images/mtjs_0803.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Event loop delay when using single thread versus multiple threads
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Does this mean you should run out and refactor your applications to offload
    rendering to another thread? Not necessarily. With this contrived example the
    application was made faster with the additional threads, but this was done on
    a 16-core machine. It’s very likely that your production applications have access
    to fewer cores.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: That said, the biggest performance differentiator while testing this was the
    size of the templates. When they’re a lot smaller, like without repeating the
    string, it’s faster to render the templates in a single thread. The reason it’s
    going to be slower is that the overhead of passing the template data between threads
    is going to be much larger than the time it takes to render a tiny template.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: As with all benchmarks, take this one with a grain of salt. You’ll need to test
    such changes with your application in a production environment to know for sure
    if it benefits from additional threads or not.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Summary of Caveats
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a combined list of the aforementioned caveats when working with threads
    in JavaScript:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Complexity
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Applications tend to be more complex when using shared memory. This is especially
    true if you are hand-writing calls with `Atomics` and manually working with `SharedBufferArray`
    instances. Now, admittedly, a lot of this complexity can be hidden from the application
    through the use of a third-party module. In such a case it can be possible to
    represent your workers in a clean manner, communicating with them from the main
    thread, and having all the intercommunication and coordination abstracted away.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Memory overhead
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: There is additional memory overhead with each thread that is added to a program.
    This memory overhead is compounded if a lot of modules are being loaded in each
    thread. Although the overhead might not be a huge deal on modern computers, it
    is worth testing on the end hardware the code will ultimately run on just to be
    safe. One way to help alleviate this issue is to audit the code that is being
    loaded in separate threads. Make sure you’re not unnecessarily loading the kitchen
    sink!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: No shared objects
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The inability to share objects between threads can make it difficult to easily
    convert a single-threaded application to a multithreaded one. Instead, when it
    comes to mutating objects, you’ll need to pass messages around that end up mutating
    an object that lives in a single location.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: No DOM access
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Only the main thread of a browser-based application has access to the DOM. This
    can make it difficult to offload UI rendering tasks to another thread. That said,
    it’s entirely possible for the main thread to be in charge of DOM mutation while
    additional threads can do the heavy lifting and return data changes to the main
    thread to update the UI.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Modified APIs
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Along the same lines as the lack of DOM access, there are slight changes to
    APIs available in threads. In the browser this means no calls to `alert()`, and
    individual worker types have even more rules, like disallowing blocking `XMLHttpRequest#open()`
    requests, `localStorage` restrictions, top-level `await`, etc. While some concerns
    are a little fringe, it does mean that not all code can run unmodified in every
    possible JavaScript context. Documentation is your friend when dealing with this.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与缺乏 DOM 访问的情况类似，可用的线程 API 有一些细微的变化。在浏览器中，这意味着不能调用 `alert()`，而且各个工作线程类型还有更多的规则，比如不允许阻塞
    `XMLHttpRequest#open()` 请求、`localStorage` 限制、顶级 `await` 等。虽然有些问题看起来比较边缘，但这意味着并不是所有代码都能在所有可能的
    JavaScript 环境中未经修改地运行。当处理这些问题时，文档是你的朋友。
- en: Structured clone algorithm constraints
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化克隆算法的限制。
- en: There are some constraints on the structured clone algorithm that may make it
    difficult to pass certain class instances between different threads. Currently,
    even if two threads have access to the same class definition, instances of the
    class passed between threads become plain `Object` instances. While it’s possible
    to rehydrate the data back into a class instance, it does require manual effort.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化克隆算法有一些限制，可能会使得在不同线程之间传递某些类实例变得困难。目前，即使两个线程访问的是相同的类定义，传递给线程的类实例也会变成普通的 `Object`
    实例。虽然可以将数据重新恢复为类实例，但这确实需要手动操作。
- en: Browsers require special headers
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器需要特殊的头信息。
- en: When working with shared memory in the browser via `SharedArrayBuffer`, the
    server must supply two additional headers in the request for the HTML document
    used by the page. If you have complete control of the server, then these headers
    may be easy to introduce. However, in certain hosting environments, it might be
    difficult or impossible to supply such headers. Even the package used in this
    book to host a local server required modifications to enable the headers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中通过 `SharedArrayBuffer` 操作共享内存时，服务器必须在页面使用的 HTML 文档请求中提供两个额外的头信息。如果你完全控制服务器，那么这些头信息可能很容易添加。然而，在某些托管环境中，可能很难或不可能提供这些头信息。即使是这本书中用来托管本地服务器的包也需要修改才能启用这些头信息。
- en: Thread preparedness detection
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 线程准备就绪检测。
- en: There is no built-in functionality to know when a spawned thread is ready to
    work with shared memory. Instead, a solution must first be built that essentially
    pings the thread and then waits until a response has been received.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 没有内置功能来知道一个被创建的线程何时准备好与共享内存工作。相反，必须首先构建一个解决方案，基本上是向线程发送 ping，然后等待收到响应。
