- en: Chapter 7\. Hard Multitenancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharing a Kubernetes cluster securely is hard. By default, Kubernetes is not
    configured to host multiple tenants, and work is needed to make it secure. “Secure”
    means it should be divided fairly between isolated tenants, who shouldn’t be able
    to see each other and shouldn’t be able to break shared resources for anybody
    else.
  prefs: []
  type: TYPE_NORMAL
- en: Each tenant may run their own choice of workloads, confined to their own set
    of namespaces. The combination of security settings in the namespace configuration
    and the cluster’s access to external and cloud services defines how securely tenants
    are separated.
  prefs: []
  type: TYPE_NORMAL
- en: Each tenant in a cluster can be considered friendly or hostile, and cluster
    admins deploy appropriate controls to keep other tenants and the cluster components
    free from harm. The level of these controls is set for the type of tenants expected
    by the system’s threat model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A tenant is the cluster’s customer. They may be a team, test or production environment,
    a hosted tool, or any logical grouping of resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter you will sail the shark-infested waters of Kubernetes multitenancy
    and their namespaced “security boundaries.” The control plane’s lockdown techniques
    are inspected for signs of fraying, we compare the data classification of workloads
    and their cargo, and look at how to monitor our resources.
  prefs: []
  type: TYPE_NORMAL
- en: Defaults
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Namespaces exist to group resources, and Kubernetes doesn’t have an inherent
    namespace tenancy model. The namespaced tenancy concept only works for interactions
    within the Kubernetes API, not the entire cluster.
  prefs: []
  type: TYPE_NORMAL
- en: By default, cross-tenant visibility is not protected by networking, DNS, and
    some namespaced policy unless the cluster is hardened with specific configuration
    that we’ll examine in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A careful defender will segregate a tenant application into multiple namespaces
    to more clearly delineate the RBAC permissions each service account has and to
    make it easier to reason and deploy network policy, quotas and limits, and other
    security tooling. You should only allow one tenant to use each namespace because
    of those namespace-bound policies and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A tenant could be a single application, a complex application divided in multiple
    namespaces, a test environment required for its development, a project, or any
    trust boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Threat Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The [Kubernetes multitenancy working group](https://oreil.ly/cHmPD) considers
    two categories of multitenancy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Soft multitenancy* is easier for tenants to use and allows greater configuration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hard multitenancy* aims to be “secure by default,” with security settings
    preconfigured and immutable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft multitenancy is a friendly, more permissive security model. It assumes
    tenants are partially trusted and have the cluster’s best interest at heart, and
    it permits them to configure parts of their own namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hard multitenancy is locked down and assumes tenants are hostile. Multiple
    controls reduce opportunity for attackers: workload isolation, admission control,
    network policy, security monitoring, and intrusion detection systems (IDS) are
    configured in the platform, and tenants only perform a restricted set of operations.
    The trade-off for such a restrictive configuration is tenant usability.'
  prefs: []
  type: TYPE_NORMAL
- en: Our threat model is scoped for hard multitenancy, to strengthen every possible
    defense against our arch nemesis, the scourge of the digital seas, Dread Pirate
    Captain Hashjack.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaced Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get into hard and soft multitenancy, let’s have a look at how we
    can separate resources: namespaces and nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Networking doesn’t adhere the idea of namespacing: we can apply policy to shape
    it, but fundamentally it’s a flat subnet.'
  prefs: []
  type: TYPE_NORMAL
- en: Visibility of your Kubernetes RBAC resources (covered in [Chapter 8](ch08.xhtml#ch-policy))
    is either scoped to a namespace such as pods or service accounts or to the whole
    cluster like nodes or persistent volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Spreading a single tenant across multiple namespaces reduces the impact of stolen
    or compromised credentials and increases the resistance of the system to compromise,
    at the cost of some operational complexity. Your teams should be able to automate
    their jobs, which will result in a secure and fast-to-patch system.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GitOps operators may be deployed in a dedicated per-operator namespace. For
    example, an application may be deployed into the namespaces `myapp-front-end`,
    `myapp-middleware`, and `myapp-data`. A privileged operator that deploys and modifies
    the application in those namespaces may be deployed into a `myapp-gitops` namespace,
    so compromise of any namespaces under its control (e.g., the `myapp-front-end`)
    doesn’t directly or indirectly lead to the compromise of the privileged operator.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps deploys whatever is committed to the repository it is monitoring, so
    control of production assets extends to the the source repository. For more on
    securing Git, see [“Hardening Git for GitOps”](https://oreil.ly/FRDzv).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Kubernetes RBAC model, namespaces are cluster-scoped and so suffer coarse
    cluster-level RBAC: if a user in a tenant namespace has permission to view their
    own namespace, they can also view all other namespaces on the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OpenShift introduces the “project” concept, which is a namespace with additional
    annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The API server can tell you which of its resources are not namespaced with
    this query (output edited to fit):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes’ shared DNS model also exposes other namespaces and services and
    is an example of the difficulties of hard multitenancy. CoreDNS’s firewall plug-in
    can be configured to “prevent Pods in certain Namespaces from looking up Services
    in other Namespaces.” IP and DNS addresses are useful to an attacker who surveys
    the visible horizon for their next target.
  prefs: []
  type: TYPE_NORMAL
- en: Node Pools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pods in a namespace can span multiple nodes, as [Figure 7-1](#multitenancy-shared-namespace)
    shows. If an attacker can escape from a container onto the underlying node, they
    may be able to jump between namespaces, and possibly even nodes, of a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![multitenancy-shared-namespace](Images/haku_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. A namespace often spans multiple nodes, however a single instance
    of a pod only ever runs on one node
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Node pools are groups of nodes with the same configuration, and that can scale
    independently of other node pools. They can be used to keep workloads of the same
    risk, or classification, on the same nodes. For example, web-facing applications
    should be separated from internal APIs and middleware workloads that are not accessible
    to internet traffic, and the control plane should be on a dedicated pool. This
    means in the event of container breakout, the attacker can only access resources
    on those nodes, and not more sensitive workloads or Secrets, and traversing between
    node pools is not a simple escalation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can assign workloads to node pools with labels and node selectors (output
    edited):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine a deployment to see its `NodeSelector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker11)'
  prefs: []
  type: TYPE_NORMAL
- en: See the labels applied to each node.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker22)'
  prefs: []
  type: TYPE_NORMAL
- en: Set a node’s class.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker33)'
  prefs: []
  type: TYPE_NORMAL
- en: One or more key-value pairs targeting a Node’s labels, to direct the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Later in this chapter we look at how to prevent a hostile `kubelet` relabeling
    itself, by using well-known labels from the [`NodeRestriction`](https://oreil.ly/VOjYB)
    admission plug-in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [`PodNodeSelector`](https://oreil.ly/TRNUq) admission controller can limit
    which nodes can be targeted by selectors in a namespace, to prevent hostile tenants
    scheduling on other’s namespace-restricted nodes (edited to fit):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For hard multitenant systems these values should be set by an admission controller
    based upon a property of the workload: its labels, who or where it was deployed
    from, or the image name. Or it may be that only an allowlist of fully qualified
    images with their digests may ever be deployed to high-risk or web-facing nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A cluster’s security boundary slides based upon its threat model and current
    scope, as [Mark Manning](https://oreil.ly/5YvCA) demonstrates in [“Command and
    KubeCTL: Real-World Kubernetes Security for Pentesters”](https://oreil.ly/viBnl).'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes system’s prime directive is to keep pods running. A pod’s tolerance
    to infrastructure failure relies upon an effective distribution of workloads across
    hardware. This availability-centric style rightly prioritises utilization above
    security isolation. Security is more expensive and requires dedicated nodes for
    each isolated workload classification.
  prefs: []
  type: TYPE_NORMAL
- en: Node Taints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, namespaces share all nodes in a cluster. As illustrated here, a
    taint can be used to prevent the scheduler from placing pods on certain nodes
    in Kubernetes’ default configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This works because a self-hosted control plane runs using filesystem-hosted
    static pod manifests in the `kubelet`’s `staticPodPath`, defaulted to */etc/kubernetes/manifests*,
    which ignore these taints.
  prefs: []
  type: TYPE_NORMAL
- en: Pods can be prevented from co-scheduling on the same node with advanced scheduler
    hints, which isolate them on their own compute hardware (a virtual or bare-metal
    machine).
  prefs: []
  type: TYPE_NORMAL
- en: This level of isolation is too expensive for most (it prevents “bin packing”
    workload by reducing the number of possible nodes for each workloads, and the
    under-utilization is likely to lead to unused compute), and so provides a consistent
    mechanism to traverse namespaces on default Kubernetes configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An attacker who can compromise a `kubelet` can remain in-cluster in many ingenious
    ways, as detailed in the talk [“Advanced Persistence Threats”](https://oreil.ly/1GvRP)
    by [Brad Geesaman](https://oreil.ly/gEIBb) and [Ian Coldwater](https://oreil.ly/8nz0p).
  prefs: []
  type: TYPE_NORMAL
- en: Admission control prevents wide-open avenues of exploitation such as sharing
    host namespaces. You should mitigate potential container breakout to the host
    with secure pod configuration, image scanning, supply chain verification, admission
    control and policy for incoming pods and operators, and intrusion detection for
    when all else fails.
  prefs: []
  type: TYPE_NORMAL
- en: If Captain Hashjack can’t easily break out of the container through vulnerable
    container runtime or kernel versions, they’ll pretty quickly start attacking the
    network. They may choose to attack other tenants on the cluster or other network-accessible
    services like the control plane and API server, compute nodes, cluster-external
    datastores, or anything else accessible on the same network segment.
  prefs: []
  type: TYPE_NORMAL
- en: Attackers look for the next weak link in the chain, or any overprivileged pod,
    so enforcing secure “hard” multitenancy between tenants hardens the cluster to
    this escalation. To give us a comparison, let’s first look at the goals of “soft”
    multitenancy.
  prefs: []
  type: TYPE_NORMAL
- en: Soft Multitenancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should use a soft multitenancy model to prevent avoidable accidents resulting
    from overprivileged tenants. It is much easier to build and run than hard multitenancy,
    which we will come to next, because its threat model doesn’t consider motivated
    threat actors like Dread Pirate Hashjack.
  prefs: []
  type: TYPE_NORMAL
- en: Soft multitenancy is often a “tenant per namespace” model. Across the cluster,
    tenants are likely to have the best interests of the cluster or administrators
    at heart. Hostile tenants, however, can probably break out of this type of cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of tenants include different projects in a team, or teams in a company,
    that are enforced by RBAC roles and bindings and grouped by namespaces. Resources
    in the namespace are limited, so tenants can’t exhaust a cluster’s resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Namespaces are tools used to build security boundaries in your cluster, but
    they are not an enforcement point. They scope many security and policy features:
    admission control webhooks, RBAC and access control, network policy, resource
    quotas and LimitRanges, Pod Security Policy, pod anti-affinity, dedicated nodes
    with taints and tolerations, and more. So they are an abstraction grouping of
    other mechanisms and resources: your threat models should consider these trust
    boundaries as walls on the defensive landscape.'
  prefs: []
  type: TYPE_NORMAL
- en: Under the lenient soft multitenancy model, namespace isolation techniques may
    not be strictly enforced, permitting tenants visibility of each other’s DNS records,
    and possibly permitting network routing between them if network policy is absent.
    DNS enumeration and requests for malicious domains should be monitored. For Captain
    Hashjack, quietly scanning a network may be a more effective way to evade detection,
    although CNI and IDS tools should detect this anomalous behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Network policy is strongly recommended for even soft multitenant deployments.
    Kubernetes nodes require a flat network space between their `kubelet`s, and the
    `kubelet`’s CNI plug-in for pod networking is responsible for enforcing tenant
    namespace separation at OSI layers 2 (ARP), 3/4 (IP addresses and TCP/UDP ports),
    and 7 (application, and TLS/x509). Network traffic is encrypted to off-cluster
    snoopers by network plug-ins like Cilium, Weave, or Calico that route pod traffic
    over virtual overlay networks, or VPN tunnels for all traffic between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The CNI protects data in transit, but must trust any workload Kubernetes has
    permitted to run. As malicious workloads are inside a CNI’s trust boundary, they
    are served trusted traffic. Your tolerance to the impact of that workload going
    rogue in its surrounding environment should guide your level of security controls.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We go into depth on network policies in [Chapter 5](ch05.xhtml#ch-networking).
  prefs: []
  type: TYPE_NORMAL
- en: Hard Multitenancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this model, cluster tenants don’t trust each other, and namespace configurations
    are “secure by default.” Security guardrails in CI/CD pipelines and admission
    control enforce policy. This level of separation is required by sensitive and
    private workloads across industry and state sectors, public compute services,
    and regulatory and accrediting bodies.
  prefs: []
  type: TYPE_NORMAL
- en: Hostile Tenants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It helps to threat model all workloads in a hard multitenant system as aggressively
    hostile. This explores more branches of an attack tree to inform an optimal balance
    of cluster security controls, which will help limit a workload’s potentially permissive
    cloud or cluster authorizations.
  prefs: []
  type: TYPE_NORMAL
- en: It also covers unknown potential events, such as a failure in the RBAC subsystem
    CVE-2019-11247 that leaked access to cluster-scoped resource for non–cluster-scoped
    roles, CVE-2018-1002105 and CVE-2019-1002100, which enabled API server DOS, or
    CVE-2018-1002105, a partially exploitable API authentication bypass (covered later
    in the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In 2019, all Kubernetes API servers were at grave risk of being honked. [Rory
    McCune](https://oreil.ly/Kf8cP) discovered v1.13.7 was vulnerable to the Billion
    Laughs YAML deserialization attack, and [Brad Geesaman](https://oreil.ly/KaOWm)
    weaponized it with [sig-honk](https://oreil.ly/dB7CX). For a malicious tenant
    with API server visibility, this would be trivial to exploit.
  prefs: []
  type: TYPE_NORMAL
- en: Admission controllers are executed by the API server after authentication and
    authorization, and validate the inbound API server request with “deep payload
    inspection.” This additional step is more powerful than traditional RESTful API
    architectures as it inspects the content of the request with specific policies,
    which can catch misconfigurations and malicious YAML.
  prefs: []
  type: TYPE_NORMAL
- en: Hard multitenant systems may use advanced sandboxes to isolate pods in a different
    way to `runc` containers and increase their resistance to zero-day attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Advanced sandboxing techniques are covered in more depth in [Chapter 3](ch03.xhtml#ch-container-runtime-isolation).
  prefs: []
  type: TYPE_NORMAL
- en: Sandboxing and Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sandboxes such as gVisor, Firecracker, and Kata Containers ingeniously combine
    KVM with namespaces and LSMs to further abstract workloads from high-risk interfaces
    and trust boundaries like the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '![captain](Images/haku_0000.png)'
  prefs: []
  type: TYPE_IMG
- en: These sandboxes are designed to resist vulnerabilities in their system call,
    filesystem, and network subsystems. As with every project, they have had CVEs,
    but they are well maintained and quick to fix. Their threat models are well documented
    and architectures theoretically solid, but every security abstraction comes at
    the cost of system simplicity, workload debuggability, and filesystem and network
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Sandboxing a pod or namespace is weighted against the additional resources required.
    Captain Hashjack’s potential cryptolocking ransom should be valued in the equation,
    against the sandboxing protect from unknown kernel and driver vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'No sandbox is inescapable, and sandboxing weighs the likelihood of simultaneous
    exploitable bugs being found in both the sandbox and the underlying Linux kernel.
    This is a reasonable approach and has analogies to browser sandboxing: Chromium
    uses the same namespaces, `cgroups`, and `seccomp` as containers do. Chromium
    breakouts have been demonstrated at Pwn2Own and the Tianfu Cup often, but the
    risk window for exploitation is a few days (very roughly) every 2–5 years.'
  prefs: []
  type: TYPE_NORMAL
- en: Hard multitenant systems should implement tools like OPA for complex and extensible
    admission control. OPA uses the Rego language to define policy and, like any code,
    policies may contain bugs. Security tools rarely “fail open” unless they’re misconfigured.
  prefs: []
  type: TYPE_NORMAL
- en: Policy risks include permissive regexes and loose comparison of objects or values
    in admission controller policy. Many YAML properties such as image names, tags,
    and metadata are string values prone to comparison mistakes. Like all static analysis,
    policy engines are as robust as you configure them to be.
  prefs: []
  type: TYPE_NORMAL
- en: Multitenancy also involves the monitoring of workloads for potentially hostile
    behavior. Supporting intrusion detection and observability services sometimes
    require potentially dangerous eBPF privileges, and eBPF’s in-kernel execution
    has been a source of container breakouts. The `CAP_BPF` capability (since Linux
    5.8) will reduce the impact of bugs in eBPF systems and require less usage of
    the “overloaded `CAP_SYS_ADMIN` capability” (says the manpage).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: eBPF is covered further in Chapters [5](ch05.xhtml#ch-networking) and [9](ch09.xhtml#ch-intrusion-detection).
  prefs: []
  type: TYPE_NORMAL
- en: Despite the runtime risks of running introspection and observability tooling
    with elevated privileges, it is safer to understand cluster risks in real time
    with these permissions, than to be unaware of them.
  prefs: []
  type: TYPE_NORMAL
- en: Public Cloud Multitenancy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Public hard multitenancy services like Google Cloud Run do not trust their workloads.
    They naturally assume the tenant and their activity is malicious and build controls
    to restrict them to the container, pod, and namespace. The threat model considers
    that attackers will try every known attack in an attempt to break out.
  prefs: []
  type: TYPE_NORMAL
- en: Privately run hard multitenancy pioneers include [*https://contained.af*](https://contained.af)—a
    web page with a terminal, connected to a container secured with kernel primitives
    and LSMs. Adventurers are invited to break their way out of the container if their
    cunning and skill enables them. So far there have been no escapes, which is testament
    to the work Jess Frazelle, the site’s host, contributed to the `runc` runtime
    at Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Although a criminal might be motivated to use or sell a container breakout zero
    day, a prerequisite to most container escapes is absence of LSM and capability
    controls. Containers configured to security best-practice, as enforced by admission
    control, have these controls enabled and are at low risk of breakout.
  prefs: []
  type: TYPE_NORMAL
- en: CTF or shared compute platforms such as [*https://ctf.af*](https://ctf.af) should
    be considered compromised and regularly “repaved” (rebuilt from scratch with no
    infrastructure persisting) in the expectation of escalation. This makes an attacker’s
    persistence attacks difficult as they must regularly re-use the same point of
    entry, increasing the likelihood of detection.
  prefs: []
  type: TYPE_NORMAL
- en: Control Plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Captain Hashjack wants to run code in your pods to poke around the rest of
    the system. Stealing service account authentication information from a pod (at
    */var/run/secrets/kubernetes.io/serviceaccount/token*) enables an attacker to
    spoof your pod’s identity to the API server and any cloud integrations, as in
    the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pod and machine identity credentials are like treasure to a pirate adversary.
    Only the service account token (a JWT) is needed to communicate with the API server
    as server certificate verification can be disabled with `--insecure`, although
    this is not recommended for legitimate use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_hard_multitenancy_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubernetes.default` is the API server DNS name in the pod network.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_hard_multitenancy_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` defaults to `kubernetes.default` and credentials in */run/secrets/kubernetes.io/serviceaccount*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_hard_multitenancy_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: With `kubectl` YOLO (no certificate authority verification of the API server).
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_hard_multitenancy_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: If the workload identity is able to access CRDs for cloud-managed resources,
    stealing a token could unlock access to S3 buckets and another connected infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Access to the API server can also leak information through its SAN, revealing
    internal and external IP addresses, any other domains DNS records point to, as
    well as the standard internal domains stemming from *kubernetes.default.svc.cluster.local*
    (output edited):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are many authentication endpoints in a cluster in addition to the API
    server’s Kubernetes resource-level RBAC, each of which allows an attacker to use
    stolen credentials to attempt privilege escalation.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By default unauthenticated users are placed into the `system:anonymous` group
    and able to read the API server’s `/version` endpoint as seen here, and use any
    roles that have been accidentally bound to the group. Anonymous authentication
    should be disabled if possible for your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Each `kubelet` has its own locally exposed API port, which can allow unauthenticated
    access to read node-local pods. Historically this was due to `cAdvisor` (Container
    Advisor) requesting resource and performance statistics, and some observability
    tools still use this endpoint. If there’s no network policy to restrict pods from
    the node’s network, the `kubelet` can be attacked from a pod.
  prefs: []
  type: TYPE_NORMAL
- en: By the same logic, the API server can be attacked from a pod with no network
    policy restrictions. Any admin interface should be restricted in as many ways
    as is practical.
  prefs: []
  type: TYPE_NORMAL
- en: API Server and etcd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`etcd` is the robust distributed datastore backing every version of Kubernetes
    and many other cloud native projects. It may be deployed on a dedicated cluster,
    as `systemd` units on Kubernetes control plane nodes, or as self-hosted pods in
    a Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hosting `etcd` as pods inside a Kubernetes cluster is the riskiest deployment
    option: it offers an attacker direct access to `etcd` on the CNI. An accidental
    Kubernetes RBAC misconfiguration could expose the whole cluster via `etcd` tampering.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`etcd`’s API has experienced remotely exploitable CVEs. CVE-2020-15115 allows
    remote brute-force of user passwords, CVE-2020-15106 was a remote DoS. Historically
    CVE-2018-1098 also permitted cross-site request forgery, with a resulting elevation
    of privilege.'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd` should be secured by firewalling it to the API server only, enabling
    all encryption methods, and finally integrating the API server with a KMS or Vault
    so sensitive values are encrypted before reaching `etcd`. Guidance is published
    in the [`etcd` Security model](https://oreil.ly/stnc5).'
  prefs: []
  type: TYPE_NORMAL
- en: The API server handles the system’s core logic and persists its state in `etcd`.
    Only the API server should ever need access, so `etcd` should not be accessible
    over the network in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s obvious that every software is vulnerable to bugs, so these sorts of attacks
    can be reduced when `etcd` is not generally available on the network. If Captain
    Hashjack can’t see the socket, they can’t attack it.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a trust boundary encompassing the API server and `etcd`. Root access
    to `etcd` may compromise the API server’s data or allow injection of malicious
    workloads, so the API server holds an encryption key: if the key is compromised,
    `etcd` data can be read by an attacker. This means that if `etcd`’s memory and
    backups are partially encrypted to protect against theft, values of Secrets are
    encrypted with the API server’s symmetric key. That Secret key is passed to the
    API server in a configuration YAML at startup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This file contains the symmetric keys used to encrypt Secrets in `etcd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Base64 is encoding to simplify binary data over text links and, in ye olden
    days of Kubernetes yore, was the only way that secrets were “protected.” If Secret
    values are not encrypted at rest, then `etcd`’s memory can be dumped and Secret
    values read by an attacker, and backups can be plundered for Secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers are just processes, but the root user on the host is omniscient.
    They must be able to see everything in order to debug and maintain the system.
    As you can see, dumping the strings in a process’s memory space is trivial:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: See [“Container Forensics”](ch09.xhtml#container-forensics) for a simple example
    of dumping process memory.
  prefs: []
  type: TYPE_NORMAL
- en: All memory is readable by the root user, and so unencrypted values in a container’s
    memory are easy to discover. You must detect attackers that attempt this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The hardest Secrets for an attacker to steal are those hidden in managed provider-hosted
    Key Management Services (KMS), which can perform cryptographic operations on a
    consumer’s behalf. Dedicated, physical hardware security modules (HSMs) are used
    to minimize risk to the cloud KMS system. Applications such as HashiCorp Vault
    can be configured as a frontend for a KMS, and services must explicitly authenticate
    to retrieve these Secrets. They are not in-memory on the local host, they cannot
    be easily enumerated, and each request is logged for audit. An attacker that compromises
    a node has not yet stolen all the Secrets that node can access.
  prefs: []
  type: TYPE_NORMAL
- en: 'KMS integration makes cloud Secrets much harder to steal from `etcd`. The API
    server uses a local proxy to interact with KMS, by which it decrypts values stored
    in `etcd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let’s move on to other control plane components.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler and Controller Manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Controller manager and scheduler components are hard to attack as they do not
    have a public network API. They can be manipulated by affecting data in `etcd`
    or tricking the API server, but do not accept network input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The controller manager service accounts are exemplary implementations of “least
    privilege.” A single controller manager process actually runs many individual
    controllers. In the event of privilege escalation in the controller manager, the
    service accounts used are well segregated in case one of them is leaked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'However, that’s not the greatest risk to that service. As with most Linux attacks,
    a malicious user with root privileges can access everything: memory of running
    processes, files on disk, network adapters, and mounted devices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An attacker that compromises the node running the controller manager can impersonate
    that component, as it shares essential key and authentication material with the
    API server, using the master node’s filesystem to share:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As root on the control plane host, examining the controller manager, we are
    able to dump the container’s filesystem and explore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The scheduler has fewer permissions and keys than the controller manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: These limited permissions give Kubernetes least privilege configuration within
    the cluster and make Dread Pirate Hashjack’s work harder.
  prefs: []
  type: TYPE_NORMAL
- en: The RBAC ClusterRole for the cloud controller manager is allowed to create service
    accounts, which can be used by attackers to pivot or persist access. It may also
    access cloud interaction to control computer nodes for autoscaling, cloud storage
    access, network routing (e.g., between nodes), and load balancer config (for routing
    internet or external traffic to the cluster).
  prefs: []
  type: TYPE_NORMAL
- en: Historically, this controller was part of the API server, which means cluster
    compromise may be escalated to cloud account compromise. Segregating permissions
    like this makes an attacker’s life more difficult, and ensuring the control plane
    nodes are not compromised will protect these services.
  prefs: []
  type: TYPE_NORMAL
- en: Data Plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes trusts a worker once it’s joined the cluster. If your worker node
    is hacked, then the `kubelet` it hosts is compromised, as well as the pods and
    data the `kubelet` was running or has access to. All `kubelet` and workload credentials
    fall under the attacker’s control.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubelet`’s kubeconfig, keys, and service account details are not IP-bound
    by default, and neither are default workload service accounts. These identities
    (service account JWTs) can be exfiltrated and used from anywhere the API server
    is accessible. Post-exploitation, Captain Hashjack can masquerade as the `kubelet`’s
    workloads everywhere that workload’s identity is accepted. API server, other clusters
    and `kubelet`s, cloud and datacenter integrations, and external systems.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the API server uses the NodeRestriction plug-in and node authorization
    in the admission controller. These restrict a `kubelet`’s service account credentials
    (which must be in the `system:nodes` group) to only the pods that are scheduled
    on that `kubelet`. An attacker may only pull Secrets associated with a workload
    scheduled on the `kubelet`’s node, and those Secrets are already mounted from
    the host’s filesystem into the container, which root can read anyway.
  prefs: []
  type: TYPE_NORMAL
- en: This makes Captain Hashjack’s tyrannical plans more difficult. A compromised
    `kubelet`’s blast radius is limited by this policy. Attackers may work around
    this by attempting to attract sensitive pods to schedule on it.
  prefs: []
  type: TYPE_NORMAL
- en: This is not using the API server to reschedule the pods—the stolen `kubelet`
    credentials have no authorization in the `kube-system` namespace—but instead changing
    the `kubelet`’s labels to pretend to be a different host or an isolated workload
    type (frontend, database, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'A compromised `kubelet` is able to relabel itself by updating its command-line
    flags and restarting. This may trick the API server into scheduling sensitive
    pods and Secrets on the node (output edited):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker1)'
  prefs: []
  type: TYPE_NORMAL
- en: Check user by making a failing API call, we’re the `kubelet` node `kube-node-2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker2)'
  prefs: []
  type: TYPE_NORMAL
- en: Modify ourselves with a new label.
  prefs: []
  type: TYPE_NORMAL
- en: Admins may use labels to assign specific workloads to certain matching `kubelet`
    nodes and namespaces, grouping workloads with similar data classifications together,
    or increasing performance by keeping network traffic in the same zone or datacenter.
    Attackers should not be able to jump between these sensitive and isolated namespaces
    or nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NodeRestriction admission plug-in defends against nodes relabeling themselves
    as part of these trusted node groups by enforcing an immutable label format. The
    documentation uses regulatory tags as examples (like `example.com.node-restriction.kubernetes.io/fips=true`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Without this added control, a compromised `kubelet` could potentially compromise
    sensitive workloads and possibly even the cluster or cloud account. The plug-in
    still allows modifications to some less-sensitive labels.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Any hard-multitenant system should consider the impact of transitive permissions
    for RBAC roles, as tools like [gcploit](https://oreil.ly/DTFz2) show in GCP. It
    chains IAM policies assigned to service accounts, and uses their permissions to
    explore the “transitive permissions” of the original service account.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Isolation Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When thinking about data classification, it helps to ask yourself, “what’s the
    worst that could happen?” and work backward from there. As [Figure 7-2](#multitenancy-4-cs)
    shows, code will try to escape its container, which might attack a cluster and
    could compromise the account. Don’t put high-value data where it can be accessed
    by breaching a low-value data system.
  prefs: []
  type: TYPE_NORMAL
- en: '![The 4C''s of Cloud Native Security](Images/haku_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-2\. Kubernetes data flow diagram (source: [cncf/financial-user-group](https://oreil.ly/BI4lj))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clusters should be segregated on data classification and impact of breach. Taken
    to the extreme, that’s a cluster per tenant, which is costly and creates management
    overhead for SRE and security teams. The art of delineating clusters is a balance
    of security and maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: Having many clusters isolated from each other is expensive to run and wastes
    much fallow compute, but this is sometimes necessary for sensitive workloads.
    Each cluster must have consistent policy applied, and the hierarchical namespace
    controller offers a measured path to rolling out similar configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Uniform configuration is needed to sync Kubernetes resources to subordinate
    clusters and apply consistent admission control and network policy.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 7-3](#multitenancy-hierarchical-ns) we see how policy objects can
    be propagated between namespaces to provide uniform enforcement of security requirements
    like RBAC, network policy, and team-specific Secrets.
  prefs: []
  type: TYPE_NORMAL
- en: '![haku 0703](Images/haku_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-3\. Kubernetes hierarchical namespace controller (source: [Kubernetes
    Multitenancy Working Group: Deep Dive](https://oreil.ly/7E2e2))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cluster Support Services and Tooling Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shared tooling environments that are connected to clusters of different sensitivities
    can offer a chance for attackers to pivot between environments or harvest leaked
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tooling environments are a prime candidate for attack, especially the supply
    chain of registries and internal packages, and logging and observability systems.
  prefs: []
  type: TYPE_NORMAL
- en: CVE-2019-11250 is a side-channel information disclosure, leaking HTTP authentication
    headers. This sensitive data is logged by default in Kubernetes components running
    the client-go library. Excess logging of headers and environment variables is
    a frequent issue in any system and highlights the need for separation in sensitive
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Tooling environments that can route traffic back to their client clusters may
    be able to reach administrative interfaces and datastores with credentials from
    CI/CD systems. Or, without direct network access, Captain Hashjack might poison
    container images, compromise source code, pillage monitoring systems, access security
    and scanning services, and drop backdoors.
  prefs: []
  type: TYPE_NORMAL
- en: Security Monitoring and Visibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large organizations run Security Operations Centers (SOC) and Security Information
    and Event Management (SIEM) technology to support compliance, threat detection,
    and security management. Your clusters emit events, audit, log, and observability
    data to these systems where they are monitored and reacted to.
  prefs: []
  type: TYPE_NORMAL
- en: Overloading these systems with data, exceeding rate limits, or increased event
    latency (adding a delay to audit, event, or container logs) may overwhelm an SOC
    or SIEM’s capacity to respond.
  prefs: []
  type: TYPE_NORMAL
- en: As stateless applications prefer to store their data in somebody else’s database,
    cloud datastores accessible from Kubernetes are a prime target. The workloads
    that can read or write to these are juicy, and service account credentials and
    workload identities are easy to harvest data with. Your SOC and SIEM should correlate
    cloud events with their calling Kubernetes workload identities to understand how
    your systems are being used, and how they may be attacked.
  prefs: []
  type: TYPE_NORMAL
- en: We will go into greater detail on this topic in [Chapter 9](ch09.xhtml#ch-intrusion-detection).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Segregating workloads is hard and requires you to invest in testing your own
    security. Validate your configuration files with static analysis, and revalidate
    clusters at runtime with tests to ensure they’re still configured correctly.
  prefs: []
  type: TYPE_NORMAL
- en: The API server and `etcd` are the brains and memory of Kubernetes, and must
    be isolated from hostile tenants. Some multitenancy options run many control planes
    in a larger Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The hierarchical namespace controller brings distributed management to multicluster
    policy.
  prefs: []
  type: TYPE_NORMAL
