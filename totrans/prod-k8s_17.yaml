- en: Chapter 16\. Platform Abstractions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many times we have seen organizations adopt a *build it and they will come*
    approach to designing and building a Kubernetes platform. However, this philosophy
    is usually fraught with risk as it often fails to deliver on the key requirements
    of the many teams (e.g., development, information security, networking, etc.)
    that will be interacting with the platform, resulting in rework and additional
    effort. It’s important to bring the other groups along on the journey and ensure
    that the platform constructed is fit for purpose.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll cover some of the angles you should consider when designing
    the onboarding and usage experience for other teams (specifically developers)
    to your Kubernetes platform. First we’ll look at some of the more philosophical
    aspects and ask the question, How much should developers know about Kubernetes?
    Then we’ll move on to discuss how we can build a smooth onramp for developers
    to get started deploying to Kubernetes and deploying clusters themselves. Finally,
    we’ll revisit the complexity spectrum we mentioned in [Chapter 1](ch01.html#chapter1)
    and look at some of the levels of abstractions we can put in place. Our objective
    is to strike a good balance between complexity and flexibility when offering a
    Kubernetes platform to development teams that have varying degrees of knowledge
    and desired engagement with the underlying implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the areas in this chapter are covered elsewhere in the book, which we’ll
    call out where appropriate. Here we aim to cover aspects from the specific stance
    of increasing team collaboration and building a platform that serves the needs
    of everyone in the organization. Although on the surface this might seem a light
    topic, the issues discussed within are often some of the hardest hurdles for many
    companies to overcome and can make or break the successful adoption of a Kubernetes-based
    application platform.
  prefs: []
  type: TYPE_NORMAL
- en: Platform Exposure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve talked many times in this book about the need to evaluate your individual
    requirements and ask questions in different areas when designing and implementing
    a Kubernetes platform. One major question that will inform many choices is deciding
    how much exposure you want development teams to have to the underlying Kubernetes
    systems and resources. There are several factors that will impact this decision.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is a relatively new technology. In some cases the drive to adopt
    Kubernetes will come from the infrastructure side of the house, to simplify infrastructure
    usage and efficiency, or standardize workloads. In other cases the drive may be
    from development teams that are keen to implement a new technology that they feel
    can accommodate and accelerate their development and deployment of cloud native
    applications. Wherever the drive is coming from, impact will be felt in the other
    teams, whether it be adapting to a new paradigm, learning new tools, or just a
    change in user experience when interacting with a new platform.
  prefs: []
  type: TYPE_NORMAL
- en: In some organizations, there is a strong requirement that development teams
    should not be exposed to the underlying platform. The driver for this is the belief
    that developers should focus on delivering business value and not be distracted
    by the implementation details of the platform being developed. There is some value
    to this approach, but in our experience, we don’t always completely agree with
    it. For example, it’s necessary for developers to have at least *some* understanding
    of the base platform in order to effectively develop applications that target
    it. This doesn’t mean increasing the coupling of the application and the platform,
    but purely understanding how to maximize the platform capabilities. [Chapter 14](ch14.html#application_considerations_chapter)
    covers this application and platform relationship in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: For the approach of *no developer exposure* to be successful, there must be
    sufficient capacity in the platform team. Firstly because they will be solely
    responsible for maintaining and supporting the environments, and secondly, that
    team will also be responsible for building the necessary abstractions required
    for developers to seamlessly interact with the platform. This point is important,
    as even when developers are not directly exposed to Kubernetes, they will still
    need ways of analyzing application performance, debugging issues, and troubleshooting.
    If giving developers `kubectl` access to a cluster exposes too much underlying
    detail, there needs to be an intermediate layer that allows developers to *own*
    the application into production while simultaneously not overwhelming them with
    implementation details. In [Chapter 9](ch09.html#observability_chapter) we cover
    many of the main ways to expose debugging tools to development teams in an effective
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Simply streamlining the troubleshooting experience for developers may not be
    enough in some organizations. Deploying applications to Kubernetes can also be
    complex, potentially requiring many components. Maybe an application needs a StatefulSet,
    PersistentVolumeClaim, Service, and ConfigMap to be successfully deployed. When
    exposing these concepts to developers is not desirable, platform teams may go
    a step further and create abstractions in this area. This could be achieved through
    self-service pipelines or building custom resources (this is covered in [Chapter 11](ch11.html#building_platform_services))
    to more simply encapsulate the required pieces. We’ll cover both of these approaches
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A limiting factor when deciding how much of the platform to expose is the skillset
    and experience of the teams creating the abstractions. For example, platform teams
    will need to have some development skills and knowledge of Kubernetes API best
    practices if they want to go down the custom resource route. If they don’t, you
    may be limited in what abstractions you can build and as such have to expose more
    of the platform internals to development teams.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we’ll look at some of the ways we’ve seen teams offer a
    self-service model to developers (and other end users) in order to streamline
    and standardize the deployment of both clusters and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Service Onboarding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Early on in an organization’s Kubernetes journey it’s likely that platform teams
    will be responsible for the provisioning and configuration of clusters for all
    the teams that need them. They’ll probably also be responsible for at least helping
    with the deployment of applications to those clusters. Depending on the tenancy
    model adopted (read more about workload tenancy in [Chapter 12](ch12.html#multi_tenancy_chapter)),
    there will be different requirements in this setup process. In a single-tenant
    model, cluster provisioning and configuration may be more straightforward, with
    a set of common permissions, core services (logging, monitoring, ingress, etc.),
    and access (e.g., Single Sign-On) setup. However, in a multitenant cluster we
    may need to create multiple additional components (e.g., Namespaces, Network Policies,
    Quotas, etc.) for each team and application onboarded.
  prefs: []
  type: TYPE_NORMAL
- en: However, as the organization begins to scale, provisioning and configuring manually
    is not sustainable. It represents repetitive toil for the platform team and blocks
    the development teams waiting for manual tasks to complete. Once a base level
    of maturity has been reached, we usually see teams start to offer some kind of
    self-service offering to their internal users. An effective way of providing this
    is through an existing CI/CD tool or process like Jenkins or GitLab. Both tools
    allow the easy creation of pipelines and provide the capability for additional
    customized inputs at execution time.
  prefs: []
  type: TYPE_NORMAL
- en: The maturity of tools like `kubeadm` and Cluster API make the automation of
    cluster creation relatively straightforward and consistent. Teams can expose tweakable
    parameters like cluster name and sizing, for example, and the pipeline can invoke
    those tools to provision clusters with sensible defaults before outputting appropriate
    credentials or access to the requesting team. Like many things, this automation
    can be as sophisticated as you choose to make it. We have seen pipelines that
    create load balancers and DNS automatically based on the requesting user’s LDAP
    information, including automatically tagging the underlying infrastructure with
    the relevant cost centers. Sizing can be open to the user but constrained with
    certain ranges depending on the team, environment, or project. We can even choose
    whether to provision in the private or public cloud depending on the classification
    or security profile of the application. There is a wide array of possibilities
    for platform teams to create a flexible yet powerful automated provisioning process
    for development teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multitenant scenarios we’ll not be creating clusters but rather creating
    Namespaces and all of the associated objects that allow us to provide a soft-isolation
    environment for the new application. Again, we can use a similiar pipeline approach,
    but this time allow development teams to choose the cluster (or clusters) where
    their application will be deployed to. At a base level we’ll want to generate
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Namespace
  prefs: []
  type: TYPE_NORMAL
- en: For the application to reside in and to provide the logical isolation for our
    other components to build on.
  prefs: []
  type: TYPE_NORMAL
- en: RBAC
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that only the appropriate authorized groups can access resources in
    their own application’s Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Network policies
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that the application is only allowed to communicate with itself or
    other shared cluster services, but *not* other applications on the same cluster
    (if required).
  prefs: []
  type: TYPE_NORMAL
- en: Quotas
  prefs: []
  type: TYPE_NORMAL
- en: To limit the amount of resources that one Namespace or application may consume
    in the cluster, reducing the potential for a *noisy neighbor* situation to arise.
  prefs: []
  type: TYPE_NORMAL
- en: Limit ranges
  prefs: []
  type: TYPE_NORMAL
- en: To set sensible defaults for specific objects created in the Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Security Policies
  prefs: []
  type: TYPE_NORMAL
- en: To ensure workloads conform to sensible default security settings, such as not
    running as a root user.
  prefs: []
  type: TYPE_NORMAL
- en: Not all of these are required or necessary in every scenario, although combined
    they allow cluster administrators and platform teams to create a seamless onboarding
    experience and deployment environment for new development teams without requiring
    manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'As organizations mature in their usage and knowledge of Kubernetes, these pipelines
    can be implemented in a Kubernetes native way using operators. For example, we
    might define a `Team` resource in the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example we might define a specific `Team` that we want to be onboarded
    with an owner (user) and some resource quotas. Our controller that lives in the
    cluster would be responsible for reading this object and creating the relevant
    Namespace, RBAC resources, and quotas and tying them together. This approach can
    be powerful because it allows us to tie in closely with the Kubernetes API and
    expose a native way of managing and reconciling resources. For instance, if a
    role were to be accidentally deleted or a quota were to get modified, the controller
    would be able to automatically remediate the situation. These higher-level types
    of resources (like a `Team` or `Application`) can be great for bootstrapping a
    cluster also, but just adding several team objects and our controller we’re able
    to automate all of the relevant configuration ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: We can definitely dig deeper down this rabbit hole to produce a sophisticated
    automation setup. For instance, let’s think about some of the observability tooling
    that might need to be configured for new applications. Perhaps we could have our
    team controller generate and submit customized dashboards for a new team or application
    and have Grafana automatically reload them. We might dynamically add new alert
    targets in Alertmanager for new teams or Namespaces. We can create very powerful
    functionality behind these simpler, more user-friendly onboarding abstractions.
  prefs: []
  type: TYPE_NORMAL
- en: The Spectrum of Abstraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#chapter1) we introduced the idea of a *spectrum of
    abstraction*. In [Figure 16-1](#spectrum_of_abstraction) we have expanded on that
    original concept and added some concrete levels of abstraction along the spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding sections we talked about some of the philosophical decisions
    and organization constraints that might influence where on this spectrum you might
    land. In this section we’ll walk through this more detailed spectrum from left
    (no abstractions) to right (fully abstracted platform) and discuss some of the
    options and trade-offs as we go.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 1601](assets/prku_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-1\. Spectrum of abstraction.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Command-Line Tooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By exposing the Kubernetes API through native command-line tooling we are at
    the far-left end of the spectrum with no abstractions in place. In some organizations
    `kubectl` will be the primary point of entry to Kubernetes for developers. This
    might be due to constraints (lack of available support from the platform teams)
    or choice (familiarity with and desire to work directly on Kubernetes from the
    developers). There may still be some automation or guardrails in place on the
    cluster, but developers will interact with it using native tooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some downsides to this approach (even if your development teams *are*
    a little familiar with Kubernetes):'
  prefs: []
  type: TYPE_NORMAL
- en: The manual overhead of having to set up and configure the authentication methods
    for potentially multiple clusters can be cumbersome. This includes switching contexts
    between multiple clusters and ensuring that individuals are always targeting the
    intended cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output format of `kubectl` commands can be cumbersome to view and work with.
    By default we are given tabular output, but this can be marshaled into different
    formats and piped to external tooling like `jq` to more concisely display the
    information. However, this requires that developers know about `kubectl`’s options
    and how to use them (in addition to the external tooling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw `kubectl` opens all the tweaks and knobs of Kubernetes to the user without
    abstraction/mediation. As such we need to ensure not only that there are appropriate
    RBAC rules in place to restrict unauthorized access, but also that there is a
    layer of admission control vetting all the requests coming into the API server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various tools can enhance this experience. There are many `kubectl` plug-ins
    that can provide a better user experience in the local shell, such as `kubens`
    and `kubectx`, that provide better usability and visibility into Namespaces and
    contexts, respectively. There are plug-ins that will aggregate logs from multiple
    Pods, or provide a terminal UI for application health. While not advanced tools,
    they can remove common pain points and eliminate the need for developers to know
    the intricacies of some of the underlying implementation details. These additional
    tools are useful helpers, but we’re still exposing the Kubernetes API directly
    with barely any abstraction on top.
  prefs: []
  type: TYPE_NORMAL
- en: There are also plug-ins that tie into external auth systems to streamline the
    authentication flow to abstract the complexity of kubeconfigs, certificates, tokens,
    etc., away from the user. This is an area where we regularly see some augmentation
    of the vanilla tooling, as enabling developers to have secure access to multiple
    clusters (especially those that may come and go dynamically) can be challenging.
    In noncritical environments, access may be based on key pairs (which must be generated,
    managed, and distributed), whereas in more stable environments access is likely
    to be linked to a Single Sign-On system. We have developed command-line utilities
    for several clients to pull credentials from a central cluster registry based
    on a local user’s login credentials.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you may decide to go down the route of Airbnb. In a [recent QCon
    talk](https://oreil.ly/OxTSc), Melanie Cebula shared Airbnb’s approach of building
    out more advanced toolsets both as standalone binaries and `kubectl` plug-ins
    to interact with its clusters, hook into image building, deployment, and more.
  prefs: []
  type: TYPE_NORMAL
- en: There is an additional class of tooling that allows developers a graphical interface
    to interact with the cluster. Recent popular choices here are [Octant](https://octant.dev)
    and [Lens](https://k8slens.dev). Rather than sitting in the cluster as the Kubernetes
    dashboard does, these tools run locally on a workstation and utilize a `kubeconfig`
    to access the cluster. These tools can be a great onramp for developers newer
    to the platform who want to see a visual representation of the cluster and their
    applications. Enhancing the client-side experience is the first step that organizations
    can take to simplify developer interactions with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Abstraction Through Templating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deploying a single application to Kubernetes can require the creation of multiple
    Kubernetes objects. For example, a *simple* Wordpress application may need the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: For describing the image, commands, and properties of the Wordpress instance.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSet
  prefs: []
  type: TYPE_NORMAL
- en: For deploying MySQL as a datastore for Wordpress.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs: []
  type: TYPE_NORMAL
- en: To provide discovery and load balancing for both Wordpress and MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: PVC
  prefs: []
  type: TYPE_NORMAL
- en: To dynamically create a Volume for the Wordpress data.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps
  prefs: []
  type: TYPE_NORMAL
- en: To hold configuration for both Wordpress and MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  prefs: []
  type: TYPE_NORMAL
- en: To hold admin credentials for both Wordpress and MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: In this list we have nearly 10 different objects all to support an extremely
    small application. Not only that, but there are nuances and expert knowledge needed
    to configure them. For example, when using a StatefulSet we need to create a special
    *headless* Service to front it. We want our developers to be able to deploy their
    application to the cluster *without* having to know how to create and configure
    all of these different Kubernetes object types themselves.
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways we can ease the user experience when deploying these applications
    is by only exposing a small set of inputs and generating the rest of the boilerplate
    behind the scenes. This approach doesn’t require developers to know about all
    the fields in all the objects, but it still exposes some of the underlying objects
    and uses tools that are a level up from pure kubectl. The tools that have some
    maturity in this area are templating tools like Helm and Kustomize.
  prefs: []
  type: TYPE_NORMAL
- en: Helm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Helm has become a popular tool in the Kubernetes ecosystem over the past couple
    of years. We realize that it has capabilities over and above just templating,
    but in our experience have found the templating use case (followed by editing
    and application of manifests) more compelling over some of its life cycle management
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a snippet from the Wordpress Helm chart (package describing an
    application) describing a Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This template is not directly exposed to developers but is a template that
    will use an injected or defined value from elsewhere. In the case of Helm, this
    can be passed on the command line or more commonly through a values file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Charts contain a default *Values.yaml* file with sensible settings, but developers
    can provide an override where they modify only the settings they need. This allows
    powerful customization via the templating without needing in-depth knowledge.
    Rather than purely templating values, Helm also contains functionality for basic
    logic operations, allowing a single tweak in the values file to generate or modify
    large sections in the underlying templates.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in the example values file just shown, there is a `type: Loadbalancer`
    declaration. This is injected directly into the template in several places, but
    it is also responsible for triggering more complex templating through the use
    of conditionals and built-in functions, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This inline logic may look complex and certainly has its detractors. However,
    the complexity is owned by the *creators* of the charts, and not the end-user
    development teams. The construction of the complex YAML structures in the template
    is keyed from a single `type` key in the values file, which is the interface for
    the developer to modify the configuration. The values file can be specified at
    runtime so different files (with different configurations) can be used for different
    clusters, teams, or environments.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Helm for configuring and deploying both third-party and internal
    applications can be an effective first step to abstracting some of the underlying
    platform from developers and allowing them to focus more closely on only the options
    they need. However, there are still some disadvantages. The interface (*Values.yaml*)
    is still YAML and can be an unfriendly user experience if developers need to explore
    the templates to understand the impact of a change (although good documentation
    can mitigate this).
  prefs: []
  type: TYPE_NORMAL
- en: For those who want to go a step further, we’ve seen tools developed that will
    abstract these tweakable items to a user interface. This allows a more native
    approach under the hood, but the user experience can be customized depending on
    the requirements of the audience. For example, workflows can be built into an
    existing deployment tool (like Jenkins) or a ticketing type of service, but the
    underlying output can still be Kubernetes manifests that are then applied to a
    cluster. While powerful, these models can get complex to maintain, and the abstractions
    can eventually leak through to the user.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting take on this model that has recently appeared is the [K8s Initializer
    by Ambassador Labs](https://app.getambassador.io/initializer). Using a browser-based
    UI workflow, the user is asked multiple questions about the type of service they
    want to deploy and the target platform. The site then outputs a downloadable package
    for the user to apply to the cluster with all the customizations applied.
  prefs: []
  type: TYPE_NORMAL
- en: All the templating approaches have many of the same strengths and weaknesses.
    We are still dealing with Kubernetes native objects that are applied to the cluster.
    For example, when outputting our Helm files with completed values we’re still
    exposed to Services, StatefulSets, and more. This isn’t a complete abstraction
    of the platform, so developers are still required to have *some level* of underlying
    knowledge. However, on the flip side, that’s also an *advantage* of this approach
    (either with Helm, or the more abstracted approach from K8s Initializer). If upstream
    Helm charts or the Initializer do not output exactly what we need, we still have
    the full flexibility to modify the results before applying to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kustomize
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kustomize is a flexible tool that can be used standalone or as part of `kubectl`
    to apply abitrary additions, deletions, and modifications to fields in any Kubernetes
    YAML objects. It is not a templating tool but is useful when leveraged on a set
    of manifests that have been templated by Helm as a method of modifying fields
    that Helm does not otherwise expose.
  prefs: []
  type: TYPE_NORMAL
- en: For the reasons previously discussed, we have seen Helm as a templating tool
    piped into something like Kustomize for additional customizations as a very powerful
    abstraction that also allows full flexibility. This approach sits somewhere in
    the middle of the spectrum and is often a sweet spot for organizations. In the
    next section we’ll move further to the right on the abstraction spectrum and see
    how we can start encapsulating underlying objects with custom resources tailored
    specifically to each organization/use case.
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting Kubernetes Primitives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve spoken about many times in this book, Kubernetes provides a set of
    primitive objects and API patterns. In combination these allow us to build higher-level
    abstractions and custom resources to capture types and ideas that are not built
    in. In late 2019 the social media company Pinterest published an interesting blog
    post describing how it had created CRDs (and associated controllers) to model
    its internal workloads as a way of abstracting Kubernetes native building blocks
    away from its development teams. Pinterest summarized its rationale for this approach
    as such:'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the Kubernetes native workload model, such as deployment,
    jobs and daemonsets, are not enough for modeling our own workloads. Usability
    issues are huge blockers on the way to adopt Kubernetes. For example, we’ve heard
    service developers complaining about missing or misconfigured Ingress messing
    up their endpoints. We’ve also seen batch job users using template tools to generate
    hundreds of copies of the same job specification and ending up with a debugging
    nightmare.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lida Li, June Liu, Rodrigo Menezes, Suli Xu, Harry Zhang, and Roberto Rodriguez
    Alcala; [“Building a Kubernetes platform at Pinterest”](https://oreil.ly/Ovmgh)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the following code snippet, `PinterestService` is an example of Pinterest’s
    custom internal resources. The 25-line object creates multiple Kubernetes native
    objects that would equate to more than 350 lines if created directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is an extension of the templating model we saw in the previous section
    where only certain inputs are exposed to the end user. However, in this case we
    can construct an input object that makes sense in the context of the application
    (rather than a relatively unstructured *Values.yaml* file) and be more intuitively
    understood by the developers. While it’s still possible for leaky abstractions
    to occur with this approach, it’s less likely as the platform team (creating the
    CRDs/operator) have full control of how to create and modify the underlying resources
    rather than having to work within the constraints of the existing objects as with
    the Helm approach. They also have the ability (with the controller) to craft much
    more sophisticated logic through a general-purpose programming language instead
    of being limited by Helm’s built-in functions. However, as we discussed earlier,
    this comes with the trade-off that platform teams must now have programming expertise.
    For more depth on creating platform services and operators take a look at [Chapter 11](ch11.html#building_platform_services).
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing an operator, we can also call out to external APIs to integrate
    richer functionality into our abstracted object types. For example, one client
    had an internal DNS system that all applications needed to be registered with
    to work correctly and be exposed to external clients. The incumbent flow would
    have developers visit a web portal and manually add the location of their service
    and the ports they needed forwarded from their assigned DNS name. We have a couple
    of options to enhance the developer experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we’re utilizing native Kubernetes objects (like Ingress in this case), we
    can create an operator that will read a special annotation on the applied Ingress
    and automatically register the application with the DNS service. This might look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our controller would read the `company.ingress.required: true` annotation,
    and depending on the name of the application, the Namespace or some other metadata
    could go and register the appropriate DNS records, as well as potentially modifying
    the host field depending on certain rules. While it reduces a lot of the manual
    work (of creating the records) required by the developer, it still requires some
    knowledge/creation of Kubernetes objects (in this case, the Ingress). In that
    way, it is more in line with the level of abstraction described in the previous
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to use a custom resource like the `PinterestService`. We have
    all the information we need encapsulated there, and we can create the Ingress
    via our operator, as well as configuring external services like the DNS system.
    We haven’t leaked any of the underlying abstraction through to the developer and
    have full flexibility with our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Even with the custom resource and operator approach we’ve discussed in this
    section, we are still exposing some of the core mechanics of the platform to development
    teams. We need to specify valid Kubernetes metadata, API versions, and resource
    types. We also expose YAML (unless we’re also providing a pipeline, wrapper, or
    UI to build it) and the quirks associated with it. In the next (and final) section
    we’ll move fully to the right on our abstraction spectrum and talk about some
    of the options that allow developers to go directly from their application code
    to the platform and not even necessarily know about Kubernetes at all.
  prefs: []
  type: TYPE_NORMAL
- en: Making Kubernetes Invisible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous sections we’ve been moving from left to right on the spectrum of
    abstraction, starting with the least abstracted (raw `kubectl` access) and now
    ending with a fully abstracted platform. In this section we’ll talk about cases
    and tooling where developers don’t even know they’re using Kubernetes and whose
    only interface to the platform (more or less) is committing/pushing code, allowing
    them to retain a fairly narrow (and deep) focus without being exposed to platform
    nuances.
  prefs: []
  type: TYPE_NORMAL
- en: SaaS providers like Heroku and tools like Cloud Foundry popularized the developer-focused
    *push* experience over 10 years ago. The concept is that the tooling (once configured)
    would provide a platform as a service (PaaS, now a nebulous term) that contained
    all of the necessary complementary components for an application to function (observability
    stacks, some form of routing/traffic management, software catalogs, etc.) and
    would allow developers to *simply* push code to a source repository. Specialized
    components within the platform would set appropriate resource limits, provision
    (if necessary) environments for the code to run, and plumb together the standard
    PaaS components to enable a streamlined end-user experience.
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that there is some crossover with Kubernetes here, which
    also provides us some primitives to enable some similar functionality. When the
    original PaaS platforms were built, Docker and Kubernetes didn’t exist and the
    prevalence of more rudimentary containerized workloads was very limited. Hence,
    these tools were built from the ground up for a virtual machine–based environment.
    We are now increasingly seeing these tools (and other new ones) be ported to or
    rewritten for Kubernetes for exactly the reason we identified earlier. Kubernetes
    provides very strong mechanical foundations, API conventions, and raw primitives
    to *build* these higher-level platforms atop of it.
  prefs: []
  type: TYPE_NORMAL
- en: One of the criticisms that is often leveled at Kubernetes is that it introduces
    a non-trivial amount of additional complexity into an environment, both for operations
    and development teams (on top of the paradigm shift to containers that must also
    be negotiated). However, this perspective misses one of the primary aims of Kubernetes
    which (as cofounder Joe Beda has articulated many times) is to be a platform *for
    building platforms*. Complexity will always exist somewhere, but through its architectural
    decisions and primitives, Kubernetes allows us to abstract the complexity to platform
    developers, vendors, and the open source community for them to build seamless
    development and deployment experiences *upon* Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already mentioned Cloud Foundry, which is probably the most popular and
    successful open source PaaS (now ported to Kubernetes), and there are other fairly
    mature options like Google App Engine (and some other serverless technologies)
    and parts of RedHat OpenShift. In addition to these we’re seeing more platforms
    appear as the space matures. One such popular platform is [Backstage](https://backstage.io),
    originally created by Spotify. Now a CNCF Sandbox project, it is a platform for
    building portals that provide tailored abstractions for developers to deploy and
    manage applications. Even as we write this chapter, HashiCorp (developer of many
    cloud native OSS tools like Vault and Consul) has just announced Project Waypoint,
    a new tool to separate end users from the underlying deployment platforms and
    provide a high-level abstraction for development teams. In their announcement
    blog post they wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We built Waypoint for one simple reason: developers just want to deploy.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mitchell Hashimoto, [“Announcing HashiCorp Waypoint”](https://oreil.ly/ZhTJ4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Waypoint aims to encapsulate the build, deploy, and release stages of software
    development. With Waypoint the developers still have to create (or have help creating)
    a configuration file that describes their process, akin to a Dockerfile except
    it describes the full set of stages in a minimal way, soliciting only the essential
    inputs. An example of this configuration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that Waypoint’s approach is still to push *some* complexity onto the developer
    (writing this file); however, they have abstracted a huge number of decisions
    away. Abstracting the platform doesn’t always mean that all complexity is removed
    from the process, or that no one has to learn anything new. Instead, as in this
    case, we can introduce a new, simplified interface *at the right level* of abstraction
    that hits the sweet spot of speed and flexibility. In Waypoint’s case even the
    underlying platform can be switched out in the deploy and release stages to use
    something like Hashicorp’s own Nomad, or some other orchestration engine. All
    of the underlying details and logic are abstracted by the platform. As Kubernetes
    and these other platforms evolve and become more stable and *boring* (some would
    argue we are nearly there), then the real innovation will continue in the development
    of higher-level platforms to better enable development teams to deliver more rapid
    business value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ve discussed the different layers of abstraction that platform
    teams can offer to their users (usually development teams) and the common tools
    and patterns we’ve seen used to implement them. Probably more than any other area,
    this is where organizational culture, history, tooling, skillsets, and more will
    all inform any decisions and trade-offs that you choose, and almost every client
    we’ve worked with has chosen to solve the issues described in this chapter in
    slightly different ways.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to note that while we often have espoused the value of having
    development teams not having to concern themselves too much with the underlying
    deployment platform, that is *not* to say that this is always the right choice
    or that developers should never understand where and how their applications are
    running. This information is key to being able to leverage specific features of
    the platform, for instance, or being able to debug issues with their software.
    As always, maintaining a strong balance is often the most successful way forward.
  prefs: []
  type: TYPE_NORMAL
