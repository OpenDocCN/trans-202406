<html><head></head><body><section data-pdf-bookmark="Chapter 13. Service Discovery" data-type="chapter" epub:type="chapter"><div class="chapter" id="ServiceDiscovery">&#13;
<h1><span class="label">Chapter 13. </span>Service Discovery</h1>&#13;
&#13;
&#13;
<p>The<a data-primary="Service Discovery" data-type="indexterm" id="servdisc13"/> <em>Service Discovery</em> pattern provides a stable endpoint through which consumers of a service can access the instances providing the service. For this purpose, Kubernetes provides multiple mechanisms, depending on whether the service consumers and producers are located on or off the cluster.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect1"><div class="sect1" id="idm45902102808800">&#13;
<h1>Problem</h1>&#13;
&#13;
<p>Applications<a data-primary="problems" data-secondary="endpoints, tracking, registering and discovering" data-type="indexterm" id="idm45902102807232"/><a data-primary="Pods" data-secondary="tracing, registering, and discovering endpoints" data-type="indexterm" id="idm45902102806256"/><a data-primary="endpoints, discovering" data-type="indexterm" id="idm45902102805280"/> deployed on Kubernetes rarely exist on their own, and usually they have to interact with other services within the cluster or systems outside the cluster. The interaction can be initiated internally within the service or through external stimulus. Internally initiated interactions are usually performed through a polling consumer: either after startup or later, an application  connects to another system and starts sending and receiving data. Typical examples are an application running within a Pod that reaches a file server and starts consuming files, or a message that connects to a message broker and starts receiving or sending messages, or an application that uses a relational database or a key-value store and starts reading or writing data.</p>&#13;
&#13;
<p>The critical distinction here is that the application running within the Pod decides at some point to open an outgoing connection to another Pod or external system and starts exchanging data in either direction. In this scenario, we don’t have an external stimulus for the application, and we don’t need any additional setup in Kubernetes.</p>&#13;
&#13;
<p>To<a data-primary="Batch Job" data-type="indexterm" id="idm45902102803456"/><a data-primary="Batch Job" data-secondary="Service Discovery" data-type="indexterm" id="idm45902102802720"/><a data-primary="Periodic Job" data-type="indexterm" id="idm45902102801776"/><a data-primary="Periodic Job" data-secondary="Service Discovery" data-type="indexterm" id="idm45902102801104"/> implement the patterns described in <a data-type="xref" data-xrefstyle="chap-num-title" href="ch07.html#BatchJob">Chapter 7, “Batch Job”</a>, or <a data-type="xref" data-xrefstyle="chap-num-title" href="ch08.html#PeriodicJob">Chapter 8, “Periodic Job”</a>, we often use this technique. In addition, long-running Pods in<a data-primary="DaemonSets" data-secondary="Service Discovery" data-type="indexterm" id="idm45902102797568"/> DaemonSets or<a data-primary="ReplicaSet" data-secondary="Service Discovery" data-type="indexterm" id="Rsetsrvdisc13"/> ReplicaSets sometimes actively connect to other systems over the network. The more common use case for Kubernetes workloads occurs when we have long-running services expecting external stimulus, most commonly in the form of incoming HTTP connections from other Pods within the cluster or external systems. In these cases, service consumers need a mechanism for discovering Pods that are dynamically placed by the<a data-primary="scheduler" data-secondary="Service Discovery" data-type="indexterm" id="idm45902102795088"/> scheduler and sometimes elastically scaled up and down.</p>&#13;
&#13;
<p>It would be a significant challenge if we had to track, register, and discover endpoints of dynamic Kubernetes Pods ourselves. That is why Kubernetes implements the <em>Service Discovery</em> pattern through different mechanisms, which we explore in this chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect1"><div class="sect1" id="idm45902102793088">&#13;
<h1>Solution</h1>&#13;
&#13;
<p>If we look at the “Before Kubernetes Era,” the most common mechanism of service discovery was through client-side discovery. In this architecture, when a service consumer had to call another service that might be scaled to multiple instances, the service consumer would have a discovery agent capable of looking at a registry for service instances and then choosing one to call. Classically, that would be done, for example, either with an embedded agent within the consumer service (such as a ZooKeeper client, Consul client, or Ribbon) or with another colocated process looking up the service in a registry, as shown in <a data-type="xref" href="#img-service-discovery-client">Figure 13-1</a>.</p>&#13;
&#13;
<figure class="width-80"><div class="figure" id="img-service-discovery-client">&#13;
<img alt="Client-side service discovery" src="assets/kup2_1301.png"/>&#13;
<h6><span class="label">Figure 13-1. </span>Client-side service discovery</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the “Post Kubernetes Era,” many of the nonfunctional responsibilities of distributed systems such as placement, health checks, healing, and resource isolation are moving into the platform, and so is service discovery and load balancing. If we use the definitions from<a data-primary="SOA (service oriented architecture)" data-type="indexterm" id="idm45902102787648"/><a data-primary="service oriented architecture (SOA)" data-type="indexterm" id="idm45902102786880"/> service-oriented architecture (SOA), a service provider instance still has to register itself with a service registry while providing the service capabilities, and a service consumer has to access the information in the registry to reach the service.</p>&#13;
&#13;
<p class="pagebreak-before">In the Kubernetes world, all that happens behind the scenes so that a service consumer calls a fixed virtual Service endpoint that can dynamically discover service instances implemented as Pods. <a data-type="xref" href="#img-service-discovery-server">Figure 13-2</a> shows how<a data-primary="registration and lookup" data-type="indexterm" id="idm45902102784352"/><a data-primary="lookup and registration" data-type="indexterm" id="idm45902102783648"/> registration and lookup are embraced by Kubernetes.</p>&#13;
&#13;
<figure><div class="figure" id="img-service-discovery-server">&#13;
<img alt="Server-side service discovery on Kubernetes" src="assets/kup2_1302.png"/>&#13;
<h6><span class="label">Figure 13-2. </span>Server-side service discovery</h6>&#13;
</div></figure>&#13;
&#13;
<p>At first glance, <em>Service Discovery</em> may seem like a simple pattern. However, multiple mechanisms can be used to implement this pattern, which depends on whether a service consumer is within or outside the cluster and whether the service provider is within or outside the cluster.<a data-primary="" data-startref="Rsetsrvdisc13" data-type="indexterm" id="idm45902102779936"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Internal Service Discovery" data-type="sect2"><div class="sect2" id="idm45902102778832">&#13;
<h2>Internal Service Discovery</h2>&#13;
&#13;
<p>Let’s<a data-primary="internal service discovery" data-type="indexterm" id="idm45902102777120"/> assume we have a web application and want to run it on Kubernetes. As soon as we create a Deployment with a few replicas, the scheduler places the Pods on the suitable nodes, and each Pod gets a cluster-internal IP address assigned before starting up. If another client service within a different Pod wishes to consume the web application endpoints, there isn’t an easy way to know the IP addresses of the service provider Pods in advance.</p>&#13;
&#13;
<p>This challenge is what the Kubernetes Service resource addresses. It provides a constant and stable entry point for a collection of Pods offering the same functionality. The easiest way to create a Service is through<a data-primary="kubectl" data-secondary="internal service discovery" data-type="indexterm" id="idm45902102775584"/> <code>kubectl expose</code>, which creates a Service for a Pod or multiple Pods of a Deployment or ReplicaSet. The command creates a virtual IP address referred to as the<a data-primary="clusterIP" data-type="indexterm" id="idm45902102773936"/> <code>clusterIP</code>, and it pulls both Pod selectors and port numbers from the resources to create the Service definition. However, to have full control over the definition, we create the Service manually, as shown in <a data-type="xref" href="#ex-service-discovery-service">Example 13-1</a>.</p>&#13;
<div class="less_space pagebreak-before" data-type="example" id="ex-service-discovery-service">&#13;
<h5><span class="label">Example 13-1. </span>A simple Service</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">                </code><a class="co" href="#callout_service_discovery_CO1-1" id="co_service_discovery_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w">               </code><a class="co" href="#callout_service_discovery_CO1-2" id="co_service_discovery_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">targetPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w">       </code><a class="co" href="#callout_service_discovery_CO1-3" id="co_service_discovery_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">protocol</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">TCP</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_discovery_CO1-1" id="callout_service_discovery_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Selector matching Pod labels.</p></dd>&#13;
<dt><a class="co" href="#co_service_discovery_CO1-2" id="callout_service_discovery_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Port over which this Service can be contacted.</p></dd>&#13;
<dt><a class="co" href="#co_service_discovery_CO1-3" id="callout_service_discovery_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Port on which the Pods are listening.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The definition in this example will create a Service named <code>random-generator</code> (the name is important for discovery later) and <code>type: ClusterIP</code> (which is the default) that accepts TCP connections on port 80 and routes them to port 8080 on all the matching Pods with the selector <code>app: random-generator</code>. It doesn’t matter when or how the Pods are created—any matching Pod becomes a routing target, as illustrated in <a data-type="xref" href="#img-service-discovery-internal">Figure 13-3</a>.</p>&#13;
&#13;
<figure class="width-45"><div class="figure" id="img-service-discovery-internal">&#13;
<img alt="Internal service discovery" src="assets/kup2_1303.png"/>&#13;
<h6><span class="label">Figure 13-3. </span>Internal service discovery</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">The essential points to remember here are that once a Service is created, it gets a <code>clusterIP</code> assigned that is accessible only from within the Kubernetes cluster (hence the name), and that IP remains unchanged as long as the Service definition exists. However, how can other applications within the cluster figure out what this dynamically allocated <code>clusterIP</code> is? There are two ways:</p>&#13;
<dl>&#13;
<dt>Discovery through environment variables</dt>&#13;
<dd>&#13;
<p>When<a data-primary="environment variables" data-secondary="service discovery through" data-type="indexterm" id="idm45902102656272"/><a data-primary="discovery" data-secondary="through environment variables" data-type="indexterm" id="idm45902102655200"/> Kubernetes starts a Pod, its environment variables get populated with the details of all Services that exist up to that moment. For example, our <code>random-generator</code> Service listening on port 80 gets injected into any newly starting Pod, as the environment variables shown in <a data-type="xref" href="#ex-service-discovery-env">Example 13-2</a> demonstrate. The application running that Pod would know the name of the Service it needs to consume and can be coded to read these environment variables. This lookup is a simple mechanism that can be used from applications written in any language and is also easy to emulate outside the Kubernetes cluster for development and testing purposes. The main issue with this mechanism is the temporal dependency on Service creation. Since environment variables cannot be injected into already-running Pods, the Service coordinates are available only for Pods started after the Service is created in Kubernetes. That requires the Service to be defined before starting the Pods that depend on the Service—or if this is not the case, the Pods need to be restarted.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="example" id="ex-service-discovery-env">&#13;
<h5><span class="label">Example 13-2. </span>Service-related environment variables set automatically in Pod</h5>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">RANDOM_GENERATOR_SERVICE_HOST</code><code class="o">=</code><code class="m">10</code>.109.72.32<code class="w"/>&#13;
<code class="nv">RANDOM_GENERATOR_SERVICE_PORT</code><code class="o">=</code><code class="m">80</code><code class="w"/></pre></div>&#13;
<dl>&#13;
<dt>Discovery through DNS lookup</dt>&#13;
<dd>&#13;
<p>Kubernetes<a data-primary="discovery" data-secondary="through DNS lookup" data-type="indexterm" id="idm45902096001280"/><a data-primary="DNS lookup" data-type="indexterm" id="idm45902095405856"/> runs a DNS server that all the Pods are automatically configured to use. Moreover, when a new Service is created, it automatically gets a new DNS entry that all Pods can start using. Assuming a client knows the name of the Service it wants to access, it can reach the Service by a<a data-primary="FQDN (fully qualified domain name)" data-type="indexterm" id="idm45902095070048"/> fully qualified domain name (FQDN) such as <code>random-generator.default.svc.cluster.local</code>. Here, <code>random-generator</code> is the name of the Service, <code>default</code> is the name of the namespace, <code>svc</code> indicates it is a Service resource, and <code>cluster.local</code> is the cluster-specific suffix. We can omit the cluster suffix if desired, and the namespace as well when accessing the Service from the same namespace.</p>&#13;
&#13;
<p class="pagebreak-before">The DNS discovery mechanism doesn’t suffer from the drawbacks of the environment-variable-based mechanism, as the DNS server allows lookup of all Services to all Pods as soon as a Service is defined. However, you may still need to use the environment variables to look up the port number to use if it is a nonstandard one or unknown by the service consumer.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here are some other high-level characteristics of the Service with <code>type: ClusterIP</code> that other types build upon:</p>&#13;
<dl>&#13;
<dt>Multiple ports</dt>&#13;
<dd>&#13;
<p>A single Service definition can support multiple source and target ports. For example, if your Pod supports both HTTP on port 8080 and HTTPS on port 8443, there is no need to define two Services. A single Service can expose both ports on 80 and 443, for example.</p>&#13;
</dd>&#13;
<dt>Session affinity</dt>&#13;
<dd>&#13;
<p>When there is a new request, the Service randomly picks a Pod to connect to by default. That can be changed with <code>sessionAffinity: ClientIP</code>, which makes all requests originating from the same client IP stick to the same Pod. Remember that Kubernetes Services performs L4 transport layer load balancing, and it cannot look into the network packets and perform application-level load balancing such as HTTP cookie-based session affinity.</p>&#13;
</dd>&#13;
<dt>Readiness probes</dt>&#13;
<dd>&#13;
<p>In<a data-primary="Health Probe" data-type="indexterm" id="idm45902094825872"/><a data-primary="Health Probe" data-secondary="Service Discovery" data-type="indexterm" id="idm45902093311120"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch04.html#HealthProbe">Chapter 4, “Health Probe”</a>, you learned how to define a <code>readinessProbe</code> for a container. If a Pod has defined readiness checks, and they are failing, the Pod is removed from the list of Service endpoints to call even if the<a data-primary="label selectors" data-secondary="internal service discovery" data-type="indexterm" id="idm45902096063520"/> label selector matches the Pod.</p>&#13;
</dd>&#13;
<dt>Virtual IP</dt>&#13;
<dd>&#13;
<p>When we create a Service with <code>type: ClusterIP</code>, it gets a stable virtual IP address. However, this IP address does not correspond to any network interface and doesn’t exist in reality. It is the kube-proxy that runs on every node that picks this new Service and updates the iptables of the node with rules to catch the network packets destined for this virtual IP address and replaces it with a selected Pod IP address. The rules in the iptables do not add ICMP rules, but only the protocol specified in the Service definition, such as TCP or UDP. As a consequence, it is not possible to <code>ping</code> the IP address of the Service as that operation uses the ICMP.</p>&#13;
</dd>&#13;
<dt>Choosing ClusterIP</dt>&#13;
<dd>&#13;
<p>During Service creation, we can specify an IP to use with the field <code>.spec.clusterIP</code>. It must be a valid IP address and within a predefined range. While not recommended, this option can turn out to be handy when dealing with legacy applications configured to use a specific IP address, or if there is an existing DNS entry we wish to reuse.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Kubernetes Services with <code>type: ClusterIP</code> are accessible only from within the cluster; they are used for discovery of Pods by matching selectors and are the most commonly used type. Next, we will look at other types of Services that allow discovery of endpoints that are manually specified.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Manual Service Discovery" data-type="sect2"><div class="sect2" id="idm45902093471584">&#13;
<h2>Manual Service Discovery</h2>&#13;
&#13;
<p>When<a data-primary="manual service discovery" data-type="indexterm" id="idm45902094324464"/><a data-primary="discovery" data-secondary="manual" data-type="indexterm" id="idm45902097469952"/> we create a Service with <code>selector</code>, Kubernetes tracks the list of matching and ready-to-serve Pods in the list of endpoint resources. For <a data-type="xref" href="#ex-service-discovery-service">Example 13-1</a>, you can check all endpoints created on behalf of the Service<a data-primary="kubectl" data-secondary="manual service discovery" data-type="indexterm" id="idm45902097851696"/> with <code>kubectl get endpoints random-generator</code>. Instead of redirecting connections to Pods within the cluster, we could also redirect connections to external IP addresses and ports. We can do that by omitting the <code>selector</code> definition of a Service and manually creating endpoint resources, as shown in <a data-type="xref" href="#ex-service-discovery-service-plain">Example 13-3</a>.</p>&#13;
<div data-type="example" id="ex-service-discovery-service-plain">&#13;
<h5><span class="label">Example 13-3. </span>Service without selector</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">external-service</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ClusterIP</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">ports</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">protocol</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">TCP</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/></pre></div>&#13;
&#13;
<p>Next, in <a data-type="xref" href="#ex-service-discovery-endpoints">Example 13-4</a>, we define an endpoint resource with the same name as the Service and containing the target IPs and ports.</p>&#13;
<div data-type="example" id="ex-service-discovery-endpoints">&#13;
<h5><span class="label">Example 13-4. </span>Endpoints for an external service</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Endpoints</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">external-service</code><code class="w">   </code><a class="co" href="#callout_service_discovery_CO2-1" id="co_service_discovery_CO2-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="nt">subsets</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">addresses</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">ip</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1.1.1.1</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">ip</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2.2.2.2</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_discovery_CO2-1" id="callout_service_discovery_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Name must match the Service that accesses these endpoints.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>This Service is also accessible only within the cluster and can be consumed in the same way as the previous ones, through environment variables or DNS lookup. The difference is that the list of endpoints is manually maintained and those values usually point to IP addresses outside the cluster, as demonstrated in <a data-type="xref" href="#img-service-discovery-manual-discovery">Figure 13-4</a>.</p>&#13;
&#13;
<p>While connecting to an external resource is this mechanism’s most common use, it is not the only one. Endpoints can hold IP addresses of Pods but not virtual IP addresses of other Services. One good thing about the Service is that it allows you to add and remove selectors and point to external or internal providers without deleting the resource definition that would lead to a Service IP address change. So service consumers can continue using the same Service IP address they first pointed to while the actual service provider implementation is migrated from on-premises to Kubernetes without affecting the client.</p>&#13;
&#13;
<figure class="width-45"><div class="figure" id="img-service-discovery-manual-discovery">&#13;
<img alt="Manual service discovery" src="assets/kup2_1304.png"/>&#13;
<h6><span class="label">Figure 13-4. </span>Manual service discovery</h6>&#13;
</div></figure>&#13;
&#13;
<p>In this category of manual destination configuration, there is one more type of Service, as shown in <a data-type="xref" href="#ex-service-discovery-external-name">Example 13-5</a>.</p>&#13;
<div data-type="example" id="ex-service-discovery-external-name">&#13;
<h5><span class="label">Example 13-5. </span>Service with an external destination</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">database-service</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ExternalName</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">externalName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">my.database.example.com</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">ports</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/></pre></div>&#13;
&#13;
<p>This Service definition does not have a <code>selector</code> either, but its type is <code>ExternalName</code>. That is an important difference from an implementation point of view. This Service definition maps to the content pointed by <code>externalName</code> using DNS only, or more specifically, <code>database-service.&lt;namespace&gt;.svc.cluster.local</code> will now point to <code>my.database.example.com</code>. It is a way of creating an alias for an external endpoint using<a data-primary="CNAME (Canonical Name) record" data-type="indexterm" id="idm45902092996240"/><a data-primary="Canonical Name (CNAME) record" data-type="indexterm" id="idm45902092995568"/> DNS CNAME rather than going through the proxy with an IP address. But fundamentally, it is another way of providing a Kubernetes abstraction for endpoints located outside the cluster.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service Discovery from Outside the Cluster" data-type="sect2"><div class="sect2" id="idm45902092994672">&#13;
<h2>Service Discovery from Outside the Cluster</h2>&#13;
&#13;
<p>The service discovery mechanisms discussed so far in this chapter all use a virtual IP address that points to Pods or external endpoints, and the virtual IP address itself is accessible only from within the Kubernetes cluster. However, a Kubernetes cluster doesn’t run disconnected from the rest of the world, and in addition to connecting to external resources from Pods, very often the opposite is also required—external applications wanting to reach to endpoints provided by the Pods. Let’s see how to make Pods accessible for clients living outside the cluster.</p>&#13;
&#13;
<p>The<a data-primary="NodePort" data-type="indexterm" id="idm45902092992592"/> first method to create a Service and expose it outside of the cluster is through <code>type: NodePort</code>. The definition in <a data-type="xref" href="#ex-service-discovery-node-port">Example 13-6</a> creates a Service as earlier, serving Pods that match the selector <code>app: random-generator</code>, accepting connections on port 80 on the virtual IP address and routing each to port 8080 of the selected Pod. However, in addition to all of that, this definition also reserves port 30036 on all the nodes and forwards incoming connections to the Service. This reservation makes the Service accessible internally through the virtual IP address, as well as externally through a dedicated port on every node.</p>&#13;
<div data-type="example" id="ex-service-discovery-node-port">&#13;
<h5><span class="label">Example 13-6. </span>Service with type NodePort</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">NodePort</code><code class="w">           </code><a class="co" href="#callout_service_discovery_CO3-1" id="co_service_discovery_CO3-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">targetPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">nodePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">30036</code><code class="w">        </code><a class="co" href="#callout_service_discovery_CO3-2" id="co_service_discovery_CO3-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">protocol</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">TCP</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_discovery_CO3-1" id="callout_service_discovery_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Open port on all nodes.</p></dd>&#13;
<dt><a class="co" href="#co_service_discovery_CO3-2" id="callout_service_discovery_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Specify a fixed port (which needs to be available) or leave this out to get a randomly selected port assigned.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>While this method of exposing services (illustrated in <a data-type="xref" href="#img-service-discovery-node-port">Figure 13-5</a>) may seem like a good approach, it has drawbacks.</p>&#13;
&#13;
<figure class="width-45"><div class="figure" id="img-service-discovery-node-port">&#13;
<img alt="Node port Service Discovery" src="assets/kup2_1305.png"/>&#13;
<h6><span class="label">Figure 13-5. </span>Node port service discovery</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s see some of its distinguishing characteristics:</p>&#13;
<dl>&#13;
<dt>Port number</dt>&#13;
<dd>&#13;
<p>Instead of picking a specific port with <code>nodePort: 30036</code>, you can let Kubernetes pick a free port within its range.</p>&#13;
</dd>&#13;
<dt>Firewall rules</dt>&#13;
<dd>&#13;
<p>Since this method opens a port on all the nodes, you may have to configure additional firewall rules to let external clients access the node ports.</p>&#13;
</dd>&#13;
<dt>Node selection</dt>&#13;
<dd>&#13;
<p>An external client can open connection to any node in the cluster. However, if the node is not available, it is the responsibility of the client application to connect to another healthy node. For this purpose, it may be a good idea to put a load balancer in front of the nodes that picks healthy nodes and performs failover.</p>&#13;
</dd>&#13;
<dt>Pods selection</dt>&#13;
<dd>&#13;
<p>When a client opens a connection through the node port, it is routed to a randomly chosen Pod that may be on the same node where the connection was open or a different node. It is possible to avoid this extra hop and always force Kubernetes to pick a Pod on the node where the connection was opened by adding <code>externalTrafficPolicy: Local</code> to the Service definition. When this option is set, Kubernetes does not allow you to connect to Pods located on other nodes, which can be an issue. To resolve that, you have to either make sure there are Pods placed on every node (e.g., by using<a data-primary="Daemon Service" data-type="indexterm" id="idm45902092867712"/><a data-primary="Daemon Service" data-secondary="Pod selector" data-type="indexterm" id="idm45902092867104"/> daemon services) or make sure the client knows which nodes have healthy Pods placed on them.</p>&#13;
</dd>&#13;
<dt>Source addresses</dt>&#13;
<dd>&#13;
<p>There are some peculiarities around the source addresses of packets sent to different types of Services. Specifically, when we use type <code>NodePort</code>, client addresses are source NAT’d, which means the source IP addresses of the network packets containing the client IP address are replaced with the node’s internal addresses. For example, when a client application sends a packet to node 1, it replaces the source address with its node address, replaces the destination address with the Pod’s address, and forwards the packet to node 2, where the Pod is located. When the Pod receives the network packet, the source address is not equal to the original client’s address but is the same as node 1’s address. To prevent this from happening, we can set <code>externalTrafficPolicy: Local</code> as described earlier and forward traffic only to Pods located on node 1.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Another way to perform Service Discovery for external clients is through a load balancer. You have seen how a <code>type: NodePort</code> Service builds on top of a regular Service with <code>type: ClusterIP</code> by also opening a port on every node. The limitation of this approach is that we still need a load balancer for client applications to pick a healthy node. The Service type<a data-primary="LoadBalancer" data-type="indexterm" id="idm45902092862560"/> <code>LoadBalancer</code> addresses this limitation.</p>&#13;
&#13;
<p>In addition to creating a regular Service, and opening a port on every node, as with <code>type: NodePort</code>, it also exposes the service externally using a cloud provider’s load balancer. <a data-type="xref" href="#img-service-discovery-load-balancer">Figure 13-6</a> shows this setup: a proprietary load balancer serves as a gateway to the Kubernetes cluster.</p>&#13;
&#13;
<figure class="width-45"><div class="figure" id="img-service-discovery-load-balancer">&#13;
<img alt="Load balancer Service Discovery" src="assets/kup2_1306.png"/>&#13;
<h6><span class="label">Figure 13-6. </span>Load balancer service discovery</h6>&#13;
</div></figure>&#13;
&#13;
<p>So this type of Service works only when the cloud provider has Kubernetes support and provisions a load balancer. We can create a Service with a load balancer by specifying the type <code>LoadBalancer</code>. Kubernetes then will add IP addresses to the <code>.spec</code> and <code>.status</code> fields, as shown in <a data-type="xref" href="#ex-service-discovery-load-balancer">Example 13-7</a>.</p>&#13;
<div data-type="example" id="ex-service-discovery-load-balancer">&#13;
<h5><span class="label">Example 13-7. </span>Service of type LoadBalancer</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">LoadBalancer</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">clusterIP</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10.0.171.239</code><code class="w">      </code><a class="co" href="#callout_service_discovery_CO4-1" id="co_service_discovery_CO4-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">loadBalancerIP</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">78.11.24.19</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">targetPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w">&#13;
</code><code class="nt">status</code><code class="p">:</code><code class="w">                        </code><a class="co" href="#callout_service_discovery_CO4-2" id="co_service_discovery_CO4-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">loadBalancer</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">ingress</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">ip</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">146.148.47.155</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_discovery_CO4-1" id="callout_service_discovery_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Kubernetes assigns <code>clusterIP</code> and <code>loadBalancerIP</code> when they are available.</p></dd>&#13;
<dt><a class="co" href="#co_service_discovery_CO4-2" id="callout_service_discovery_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>status</code> field is managed by Kubernetes and adds the Ingress IP.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>With this definition in place, an external client application can open a connection to the load balancer, which picks a node and locates the Pod. The exact way that load-balancer provisioning and service discovery are performed varies among cloud providers. Some cloud providers will allow you to define the load-balancer address and some will not. Some offer mechanisms for preserving the source address, and some replace that with the load-balancer address. You should check the specific implementation provided by your cloud provider of choice.</p>&#13;
<div class="less_space pagebreak-before" data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Yet another type of Service<a data-primary="headless services" data-type="indexterm" id="idm45902092769584"/> is available: <em>headless</em> services, for which you don’t request a dedicated IP address. You create a headless service by specifying <code>clusterIP None</code> within the Service’s <code>spec</code> section. For headless services, the backing Pods are added to the internal DNS server and are most useful for implementing Services to<a data-primary="Stateful Service" data-type="indexterm" id="idm45902092767376"/><a data-primary="Stateful Service" data-secondary="Service Discovery" data-type="indexterm" id="idm45902092717440"/> StatefulSets, as described in detail in &#13;
<span class="keep-together"><a data-type="xref" data-xrefstyle="chap-num-title" href="ch12.html#StatefulService">Chapter 12, “Stateful Service”</a>.</span></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Application Layer Service Discovery" data-type="sect2"><div class="sect2" id="idm45902092994080">&#13;
<h2>Application Layer Service Discovery</h2>&#13;
&#13;
<p>Unlike<a data-primary="application layer service discovery" data-type="indexterm" id="idm45902092713616"/> the mechanisms discussed so far, Ingress<a data-primary="Ingress" data-type="indexterm" id="idm45902092712880"/> is not a service type but a separate Kubernetes resource that sits in front of Services and acts as a smart router and entry point to the cluster. Ingress typically provides HTTP-based access to Services through externally reachable URLs, load balancing, TLS termination, and name-based virtual hosting, but there are also other specialized Ingress implementations. For Ingress to work, the cluster must have one or more Ingress controllers running. A simple Ingress that exposes a single Service is shown in <a data-type="xref" href="#ex-service-discovery-ingress">Example 13-8</a>.</p>&#13;
<div data-type="example" id="ex-service-discovery-ingress">&#13;
<h5><span class="label">Example 13-8. </span>An Ingress definition</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1beta1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">servicePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w"/></pre></div>&#13;
&#13;
<p>Depending on the infrastructure Kubernetes is running on, and the Ingress controller implementation, this definition allocates an externally accessible IP address and exposes the <code>random-generator</code> Service on port 80. But this is not very different from a Service with <code>type: LoadBalancer</code>, which requires an external IP address per Service definition. The real power of Ingress comes from reusing a single external load balancer and IP to service multiple Services and reduce the infrastructure costs. A simple fan-out configuration for routing a single IP address to multiple Services based on HTTP URI paths looks like <a data-type="xref" href="#ex-service-discovery-ingress-with-mapping">Example 13-9</a>.</p>&#13;
<div class="less_space pagebreak-before" data-type="example" id="ex-service-discovery-ingress-with-mapping">&#13;
<h5><span class="label">Example 13-9. </span>A definition for Nginx Ingress controller</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1beta1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">annotations</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">nginx.ingress.kubernetes.io/rewrite-target</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">rules</code><code class="p">:</code><code class="w">                       </code><a class="co" href="#callout_service_discovery_CO5-1" id="co_service_discovery_CO5-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">http</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">paths</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/</code><code class="w">                </code><a class="co" href="#callout_service_discovery_CO5-2" id="co_service_discovery_CO5-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">backend</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">servicePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/cluster-status</code><code class="w">  </code><a class="co" href="#callout_service_discovery_CO5-3" id="co_service_discovery_CO5-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">backend</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cluster-status</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">servicePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_discovery_CO5-1" id="callout_service_discovery_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Dedicated rules for the Ingress controller for dispatching requests based on the request path.</p></dd>&#13;
<dt><a class="co" href="#co_service_discovery_CO5-2" id="callout_service_discovery_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Redirect every request to Service <code>random-generator</code>…​</p></dd>&#13;
<dt><a class="co" href="#co_service_discovery_CO5-3" id="callout_service_discovery_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>…​ except <code>/cluster-status</code>, which goes to another Service.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Since every Ingress controller implementation is different, apart from the usual Ingress definition, a controller may require additional configuration, which is passed through annotations. Assuming the Ingress is configured correctly, the preceding definition would provision a load balancer and get an external IP address that services two Services under two different paths, as shown in <a data-type="xref" href="#img-service-discovery-ingress">Figure 13-7</a>.</p>&#13;
&#13;
<p>Ingress is the most powerful and at the same time most complex service discovery mechanism on Kubernetes. It is most useful for exposing multiple services under the same IP address and when all services use the same L7 (typically HTTP) protocol.</p>&#13;
&#13;
<figure class="width-45"><div class="figure" id="img-service-discovery-ingress">&#13;
<img alt="Application layer service discovery" src="assets/kup2_1307.png"/>&#13;
<h6><span class="label">Figure 13-7. </span>Application layer service discovery</h6>&#13;
</div></figure>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45902092496864">&#13;
<h1>OpenShift Routes</h1>&#13;
<p>Red Hat OpenShift<a data-primary="Routes" data-type="indexterm" id="idm45902092495360"/><a data-primary="Red Hat OpenShift" data-secondary="Routes" data-type="indexterm" id="idm45902092494624"/> is a popular enterprise distribution of Kubernetes.&#13;
Besides being fully compliant with Kubernetes, OpenShift provides some additional features. One of these features is Routes, which are very similar to Ingress.&#13;
They are so similar, in fact, the differences might be difficult to spot. First of all, Routes predates the introduction of the Ingress object in Kubernetes, so Routes can be considered a kind of predecessor of Ingress.</p>&#13;
&#13;
<p>However, some technical differences still exist between Routes and Ingress objects:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A Route is picked up automatically by the OpenShift-integrated HAProxy load balancer, so there is no requirement for an extra Ingress controller to be installed.</p>&#13;
</li>&#13;
<li>&#13;
<p>You can use additional TLS termination modes like re-encryption or pass-through for the leg to the Service.</p>&#13;
</li>&#13;
<li>&#13;
<p>Multiple weighted backends for splitting traffic can be used.</p>&#13;
</li>&#13;
<li>&#13;
<p>Wildcard domains are supported.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Having said all that, you can use Ingress on OpenShift too. So you have the choice when using OpenShift.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Discussion" data-type="sect1"><div class="sect1" id="idm45902102792464">&#13;
<h1>Discussion</h1>&#13;
&#13;
<p>In<a data-primary="discovery" data-secondary="service discovery in Kubernetes" data-type="indexterm" id="idm45902092486672"/> this chapter, we  covered the favorite service discovery mechanisms on Kubernetes. Discovery of dynamic Pods from within the cluster is always achieved through the Service resource, though different options can lead to different implementations. The Service abstraction is a high-level cloud native way of configuring low-level details such as virtual IP addresses, iptables, DNS records, or environment variables. Service discovery from outside the cluster builds on top of the Service abstraction and focuses on exposing the Services to the outside world. While a <code>NodePort</code> provides the basics of exposing Services, a highly available setup requires integration with the platform infrastructure provider.</p>&#13;
&#13;
<p><a data-type="xref" href="#table-service-discovery-types">Table 13-1</a> summarizes the various ways service discovery is implemented in Kubernetes. This table aims to organize the various service discovery mechanisms in this chapter from more straightforward to more complex. We hope it can help you build a mental model and understand them better.</p>&#13;
<table id="table-service-discovery-types">&#13;
<caption><span class="label">Table 13-1. </span>Service Discovery mechanisms</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Name</th>&#13;
<th>Configuration</th>&#13;
<th>Client type</th>&#13;
<th>Summary</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>ClusterIP</p></td>&#13;
<td><div>&#13;
<p><code>type: ClusterIP</code><br/>&#13;
<code>.spec.selector</code></p></div></td>&#13;
<td><p>Internal</p></td>&#13;
<td><p>The most common internal discovery mechanism</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Manual IP</p></td>&#13;
<td><div>&#13;
<p><code>type: ClusterIP</code><br/>&#13;
<code>kind: Endpoints</code></p></div></td>&#13;
<td><p>Internal</p></td>&#13;
<td><p>External IP discovery</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Manual FQDN</p></td>&#13;
<td><div>&#13;
<p><code>type: ExternalName</code><br/>&#13;
<code>.spec.externalName</code></p></div></td>&#13;
<td><p>Internal</p></td>&#13;
<td><p>External FQDN discovery</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Headless Service</p></td>&#13;
<td><div>&#13;
<p><code>type: ClusterIP</code><br/>&#13;
<code>.spec.clusterIP: None</code></p></div></td>&#13;
<td><p>Internal</p></td>&#13;
<td><p>DNS-based discovery without a virtual IP</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>NodePort</p></td>&#13;
<td><p><code>type: NodePort</code></p></td>&#13;
<td><p>External</p></td>&#13;
<td><p>Preferred for non-HTTP traffic</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>LoadBalancer</p></td>&#13;
<td><p><code>type: LoadBalancer</code></p></td>&#13;
<td><p>External</p></td>&#13;
<td><p>Requires supporting cloud infrastructure</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Ingress</p></td>&#13;
<td><p><code>kind: Ingress</code></p></td>&#13;
<td><p>External</p></td>&#13;
<td><p>L7/HTTP-based smart routing mechanism</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>This chapter gave a comprehensive overview of all the core concepts in Kubernetes for accessing and discovering services. However, the journey does not stop here. With the<a data-primary="Knative" data-secondary="new primitives" data-type="indexterm" id="idm45902092455264"/> <em>Knative</em> project, new primitives on top of Kubernetes have been introduced, which help application developers with advanced serving and eventing.</p>&#13;
&#13;
<p>In the context of the <em>Service Discovery</em> pattern, the<a data-primary="Knative" data-secondary="serving" data-type="indexterm" id="idm45902092452656"/> <em>Knative Serving</em> subproject is of particular interest as it introduces a new Service resource with the same kind as the Services introduced here (but with a different API group). Knative Serving provides support for application revision but also for a very flexible scaling of services behind a load balancer. We give a short shout-out to Knative Serving in <a data-type="xref" href="ch29.html#elastic-scale-knative">“Knative”</a>, but a full discussion of Knative is beyond the scope of this book. In <a data-type="xref" href="ch29.html#elastic-scale-more-information">“More Information”</a>, you will find links that point to detailed information about Knative.<a data-primary="" data-startref="servdisc13" data-type="indexterm" id="idm45902092449504"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="More Information" data-type="sect1"><div class="sect1" id="idm45902092448560">&#13;
<h1>More Information</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/nagmD">Service Discovery Example</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/AEDi5">Kubernetes Service</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/WRT5H">DNS for Services and Pods</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/voVbw">Debug Services</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/mGjzg">Using Source IP</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/pzOiM">Create an External Load Balancer</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/Idv2c">Ingress</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/W4i8U">Kubernetes NodePort Versus LoadBalancer Versus Ingress? When Should I Use What?</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/fXicP">Kubernetes Ingress Versus OpenShift Route</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section></body></html>