<html><head></head><body><section data-pdf-bookmark="Chapter 12. Multitenancy" data-type="chapter" epub:type="chapter"><div class="chapter" id="multi_tenancy_chapter">&#13;
<h1><span class="label">Chapter 12. </span>Multitenancy</h1>&#13;
&#13;
&#13;
<p>When building a production application platform atop Kubernetes, you must consider how to handle the tenants that will run on the platform.<a data-primary="multitenancy" data-type="indexterm" id="ix_mltiten"/><a data-primary="tenants" data-seealso="multitenancy" data-type="indexterm" id="idm45611975893304"/> As we’ve discussed throughout this book, Kubernetes provides a set of foundational features you can use to implement many requirements. Workload tenancy is no different. Kubernetes offers various knobs you can use to ensure tenants can safely coexist on the same platform. With that said, Kubernetes does not define a tenant. A tenant can be an application, a development team, a business unit, or something else. Defining a tenant is up to you and your organization, and we hope this chapter will help you with that task.</p>&#13;
&#13;
<p>Once you establish who your tenants are, you must determine whether multiple tenants should run on the same platform. In our experience helping large organizations build application platforms, we’ve found that platform teams are usually interested in operating a multitenant platform. With that said, this decision is firmly rooted in the nature of the different tenants and the trust that exists between them. For example, an enterprise offering a shared application platform is a different story than a company offering containers-as-a-service to external customers.</p>&#13;
&#13;
<p>In this chapter, we will first explore the degrees of tenant isolation you can achieve with Kubernetes. The nature of your workloads and your specific requirements will dictate how much isolation you need to provide. The stronger the isolation, the higher the investment you need to make in this area. We will then discuss Kubernetes Namespaces, an essential building block that enables a large portion of the multitenancy capabilities in Kubernetes. Finally, we will dig into the different Kubernetes features you can leverage to isolate tenants on a multitenant cluster, including Role-Based Access Control (RBAC), Resource Requests and Limits, Pod Security Policies, and others.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Degrees of Isolation" data-type="sect1"><div class="sect1" id="idm45611975889464">&#13;
<h1>Degrees of Isolation</h1>&#13;
&#13;
<p>Kubernetes lends itself to various tenancy models, each with pros and cons. The most critical factor that determines which model to implement is the degree of isolation demanded by your workloads.<a data-primary="tenants" data-secondary="degrees of isolation" data-type="indexterm" id="idm45611976190520"/><a data-primary="multitenancy" data-secondary="degrees of isolation for tenants" data-type="indexterm" id="ix_mltitendeg"/> For example, running untrusted code developed by different third parties usually requires more robust isolation than hosting your organization’s internal applications. Broadly speaking, there are two tenancy models you can follow: single-tenant clusters and multitenant clusters. Let’s discuss the strengths and weaknesses of each model.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Single-Tenant Clusters" data-type="sect2"><div class="sect2" id="idm45611976187864">&#13;
<h2>Single-Tenant Clusters</h2>&#13;
&#13;
<p>The single-tenant cluster model (depicted in <a data-type="xref" href="#each_tenant_runs_in_a_separate_cluster">Figure 12-1</a>) provides the strongest isolation between tenants, as there is no sharing of cluster resources.<a data-primary="clusters" data-secondary="single-tenant" data-type="indexterm" id="idm45611975887880"/><a data-primary="multitenancy" data-secondary="degrees of isolation for tenants" data-tertiary="single-tenant clusters" data-type="indexterm" id="idm45611975886936"/><a data-primary="single-tenant clusters" data-type="indexterm" id="idm45611975885752"/><a data-primary="tenants" data-secondary="degrees of isolation" data-tertiary="single-tenant clusters" data-type="indexterm" id="idm45611975885080"/> This model is rather appealing as you do not have to solve the complex multitenancy problems that can otherwise arise. In other words, there is no tenant isolation problem to solve.</p>&#13;
&#13;
<figure><div class="figure" id="each_tenant_runs_in_a_separate_cluster">&#13;
<img alt="prku 1201" src="assets/prku_1201.png"/>&#13;
<h6><span class="label">Figure 12-1. </span>Each tenant runs in a separate cluster (CP represents a control plane node).</h6>&#13;
</div></figure>&#13;
&#13;
<p>Single-tenant clusters can be viable if you have a small number of tenants. However, the model can suffer from the following downsides:</p>&#13;
<dl>&#13;
<dt>Resource overhead</dt>&#13;
<dd>&#13;
<p>Each single-tenant cluster has to run its own control plane, which in most cases, requires at least three dedicated nodes.<a data-primary="resources" data-secondary="overhead for single-tenant clusters" data-type="indexterm" id="idm45611975898920"/> The more tenants you have, the more resources dedicated to cluster control planes—resources that you could otherwise use to run workloads. In addition to the control plane, each cluster hosts a set of workloads to provide platform services. These platform services also incur overhead as they could otherwise be shared among different tenants in a multitenant cluster. Monitoring tools, policy controllers (e.g., Open Policy Agent), and Ingress controllers are good examples.</p>&#13;
</dd>&#13;
<dt>Increased management complexity</dt>&#13;
<dd>&#13;
<p>Managing a large number of clusters can become a challenge for platform teams. Each cluster needs to be deployed, tracked, upgraded, etc. Imagine having to remediate a security vulnerability across hundreds of clusters. Investing in advanced tooling is necessary for platform teams to do this effectively.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Even with the drawbacks just mentioned, we have seen many successful implementations of single-tenant clusters in the field. And with cluster life cycle tooling such as <a href="https://oreil.ly/8QRz7">Cluster API</a> reaching <a data-primary="Cluster API" data-type="indexterm" id="idm45611975906904"/>maturity, the single-tenant model has become easier to adopt. With that said, most of our focus in the field has been helping organizations with multitenant clusters, which we’ll discuss next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multitenant Clusters" data-type="sect2"><div class="sect2" id="idm45611975905704">&#13;
<h2>Multitenant Clusters</h2>&#13;
&#13;
<p>Clusters that host multiple tenants can address the downsides of single-tenant clusters we previously discussed.<a data-primary="multitenancy" data-secondary="degrees of isolation for tenants" data-tertiary="multitenant clusters" data-type="indexterm" id="idm45611975904008"/><a data-primary="tenants" data-secondary="degrees of isolation" data-tertiary="multitenant clusters" data-type="indexterm" id="idm45611975902792"/><a data-primary="clusters" data-secondary="multitenant" data-type="indexterm" id="idm45611974639144"/> Instead of deploying and managing one cluster per tenant, the platform team can focus on a smaller number of clusters, which reduces the resource overhead and management complexity (as seen in <a data-type="xref" href="#a_single_cluster_shared_by_multiple_tenants">Figure 12-2</a>). With that said, there is a trade-off being made. The implementation of multitenant clusters is more complicated and nuanced, as you have to ensure that tenants can coexist without affecting each other.</p>&#13;
&#13;
<figure><div class="figure" id="a_single_cluster_shared_by_multiple_tenants">&#13;
<img alt="prku 1202" src="assets/prku_1202.png"/>&#13;
<h6><span class="label">Figure 12-2. </span>A single cluster shared by multiple tenants (CP represents a control plane node).</h6>&#13;
</div></figure>&#13;
&#13;
<p>Multitenancy <a data-primary="soft multitenancy" data-type="indexterm" id="idm45611974634552"/>comes in two broad flavors, soft multitenancy and hard multitenancy. <em>Soft multitenancy</em>, sometimes referred to as “multiteam,” assumes that some level of trust exists between the tenants on the platform. This model is usually viable when tenants belong to the same organization. For example, an enterprise application platform hosting different tenants can generally assume a soft multitenancy posture. This is because the tenants are incentivized to be good neighbors as they move their organization toward success. Nevertheless, even though the intent is positive, tenant isolation is still necessary given that unintentional issues can arise (e.g., vulnerabilities, bugs, etc.).</p>&#13;
&#13;
<p>On the other hand, the <em>hard multitenancy</em> model establishes that there is no trust between tenants.<a data-primary="hard multitenancy" data-type="indexterm" id="idm45611976019400"/> From a security point of view, the tenants are even considered adversaries to ensure the proper isolation mechanisms are put in place. A platform running untrusted code that belongs to different organizations is a good example. In this case, strong isolation between tenants is critical to ensure they can share the cluster safely.</p>&#13;
&#13;
<p>Building on our housing analogy theme from <a data-type="xref" href="ch01.html#chapter1">Chapter 1</a>, we can say that the soft multitenancy model is equivalent to a family living together. They share the kitchen, living room, and utilities, but each family member has their own bedroom. In contrast, the hard multitenancy model is better represented by an apartment building. Multiple families share the building, but each family lives behind a locked front door.</p>&#13;
&#13;
<p>While the soft and hard multitenancy models can help guide conversations about multitenant platforms, the implementation is not as clear-cut. The reality is that multitenancy is best described as a spectrum. On the one end, we have no isolation at all. Tenants are free to do anything on the platform and consume all its resources. On the other end, we have full tenant isolation, where tenants are strictly controlled and isolated across all layers of the platform.</p>&#13;
&#13;
<p>As you can imagine, establishing a production multitenant platform with no tenant isolation is not viable. At the same time, building a multitenant platform with complete tenant isolation can be a costly (or even futile) endeavor. Thus, it is important to find the sweet spot in the multitenancy spectrum that will work for your workloads and organization as a whole.</p>&#13;
&#13;
<p>To determine the isolation required for your workloads, you must consider the different<a data-primary="workloads" data-secondary="isolation requirements" data-type="indexterm" id="idm45611975920568"/> layers where you <em>can</em> apply isolation in a Kubernetes-based platform:</p>&#13;
<dl>&#13;
<dt>Workload plane</dt>&#13;
<dd>&#13;
<p>The workload plane consists of the nodes where the workloads get to run.<a data-primary="workload plane" data-type="indexterm" id="idm45611975917320"/> In a multitenant scenario, workloads are typically scheduled across the shared pool of nodes. Isolation at this level involves fair sharing of node resources, security and network boundaries, etc.</p>&#13;
</dd>&#13;
<dt>Control plane</dt>&#13;
<dd>&#13;
<p>The control plane<a data-primary="control plane" data-type="indexterm" id="idm45611975927704"/> encompasses the components that make up a Kubernetes cluster, such as the API server, the controller manager, and the scheduler.<a data-primary="API server" data-type="indexterm" id="idm45611975926808"/> There are different mechanisms available in Kubernetes to segregate tenants at this level, including authorization (i.e., RBAC), admission control, and API priority and fairness.</p>&#13;
</dd>&#13;
<dt>Platform services</dt>&#13;
<dd>&#13;
<p>Platform services include centralized logging, monitoring, ingress, in-cluster DNS, and others.<a data-primary="services" data-secondary="multitenant platform services" data-type="indexterm" id="idm45611975924424"/> Depending on the workloads, these services or capabilities might also require some level of isolation. For example, you might want to prevent tenants from inspecting each other’s logs or discovering each other’s services via the cluster’s DNS server.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Kubernetes provides different primitives you can use to implement isolation at each of these layers. Before digging into them, we will discuss the Kubernetes Namespace, the foundational boundary that allows you to segregate tenants on a cluster.<a data-primary="multitenancy" data-secondary="degrees of isolation for tenants" data-startref="ix_mltitendeg" data-type="indexterm" id="idm45611975948152"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Namespace Boundary" data-type="sect1"><div class="sect1" id="idm45611975905032">&#13;
<h1>The Namespace Boundary</h1>&#13;
&#13;
<p>Namespaces enable a<a data-primary="multitenancy" data-secondary="Namespace boundary" data-type="indexterm" id="idm45611975945512"/><a data-primary="Namespaces" data-secondary="multitenancy and" data-type="indexterm" id="idm45611975944504"/> number of different capabilities in the Kubernetes API. They allow you to organize your cluster, enforce policy, control access, etc. More importantly, they are a critical building block when implementing a multitenant Kubernetes platform, as they provide the foundation to onboard and isolate tenants.</p>&#13;
&#13;
<p>When it comes to tenant isolation, however, it is important to keep in mind that the Namespace is a logical construct in the Kubernetes control plane. Without additional policy or configuration, the Namespace has no implications on the workload plane. For example, workloads that belong to different Namespaces are likely to run on the same node unless advanced scheduling constraints are put in place. In the end, the Namespace is merely a piece of metadata attached to resources in the Kubernetes API.</p>&#13;
&#13;
<p>Having said that, many of the isolation mechanisms that we will explore in this chapter hinge on the Namespace construct. RBAC, resource quotas, and network policies are examples of such mechanisms. Thus, one of the first decisions to make when designing your tenancy strategy is establishing how to leverage Namespaces. When helping organizations in the field, we have seen the following approaches:</p>&#13;
<dl>&#13;
<dt>Namespace per team</dt>&#13;
<dd>&#13;
<p>In this model, each team has access to a single Namespace in the cluster. This approach makes it simple to apply policy and quota to specific teams. However, it can be challenging for teams to exist within a single Namespace if they own many services. Overall, we find that this model can be viable for small organizations that are getting started with Kubernetes.</p>&#13;
</dd>&#13;
<dt>Namespace per application</dt>&#13;
<dd>&#13;
<p>This approach provides a Namespace for each application in the cluster, making it easier to apply application-specific policy and quota. The downside is that this model usually results in tenants having access to multiple Namespaces, which can complicate the tenant onboarding process and the ability to apply tenant-level policy and quota. With that said, this approach is perhaps the most viable for large organizations and enterprises building multitenant platforms.</p>&#13;
</dd>&#13;
<dt>Namespace per tier</dt>&#13;
<dd>&#13;
<p>This pattern establishes different runtime tiers (or environments) using Namespaces. We usually avoid this approach, as we prefer to use separate clusters for development, staging, and production tiers.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The approach to use largely depends on your isolation requirements and the structure of your organization. If you are leaning toward the Namespace per team model, remember that all resources in the Namespace are accessible by all team members or workloads in the Namespace. For example, assuming Alice and Bob are on the same team, there’s no way to prevent Alice from looking at Bob’s Secrets if they are both authorized to get Secrets in the team’s Namespace.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multitenancy in Kubernetes" data-type="sect1"><div class="sect1" id="idm45611975852968">&#13;
<h1>Multitenancy in Kubernetes</h1>&#13;
&#13;
<p>Up to this point, we have discussed the different tenancy models you can implement when building a Kubernetes-based platform.<a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-type="indexterm" id="ix_mltitenKu"/> In the rest of this chapter, we will focus on multitenant clusters and the various Kubernetes capabilities you can leverage to safely and effectively host your tenants. As you read through these sections, you will find that we have covered some of these capabilities in other chapters. In those cases, we will brush up on them once more, but we will focus on the multitenancy aspect of them.</p>&#13;
&#13;
<p>First, we will focus on the isolation mechanisms available in the control plane layer. Mainly, RBAC, resource quotas, and validating admission webhooks. We will then move onto the workload plane, where we will discuss resource requests and limits, Network Policies, and Pod Security Policies. Finally, we will touch on monitoring and centralized logging as example platform services that you can design with multitenancy in mind.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Role-Based Access Control (RBAC)" data-type="sect2"><div class="sect2" id="idm45611977210776">&#13;
<h2>Role-Based Access Control (RBAC)</h2>&#13;
&#13;
<p>When hosting multiple tenants in the same cluster, you must enforce isolation at the API server layer to prevent tenants from modifying resources that do not belong to them.<a data-primary="API server" data-secondary="multitenancy and" data-tertiary="preventing resource modification by tenants" data-type="indexterm" id="idm45611977209048"/><a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-tertiary="role-based access control" data-type="indexterm" id="ix_mltitenKuRBAC"/><a data-primary="RBAC (role-based access control)" data-secondary="multitenancy and" data-type="indexterm" id="ix_RBAC"/> The RBAC authorization mechanism enables you to configure this policy. As we discussed in <a data-type="xref" href="ch10.html#chapter10">Chapter 10</a>, the API server supports different mechanisms to establish a user’s or tenant’s identity. Once established, the tenant’s identity is passed on to the RBAC system, which determines whether the tenant is authorized to perform the requested action.</p>&#13;
&#13;
<p>As you onboard tenants onto the cluster, you can grant them access to one or more Namespaces in which they can create and manage API resources.<a data-primary="identity" data-secondary="tenants, binding ClusterRoles" data-type="indexterm" id="idm45611976830184"/> To authorize each tenant, you must bind Roles or ClusterRoles with their identities.<a data-primary="ClusterRole" data-type="indexterm" id="idm45611976829112"/> The binding is achieved with the RoleBinding resource.<a data-primary="RoleBindings" data-type="indexterm" id="idm45611976828312"/> The following snippet shows an example RoleBinding that grants the <code>app1-viewer</code> Group view access to the <code>app1</code> Namespace.<a data-primary="ClusterRoleBinding" data-type="indexterm" id="idm45611976826712"/> Unless you have a good use case, avoid using ClusterRoleBindings for tenants, as it authorizes the tenant to leverage the bound role across all Namespaces.</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">RoleBinding</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">viewers</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">app1</code>&#13;
<code class="nt">roleRef</code><code class="p">:</code>&#13;
  <code class="nt">apiGroup</code><code class="p">:</code> <code class="l-Scalar-Plain">rbac.authorization.k8s.io</code>&#13;
  <code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">ClusterRole</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">view</code>&#13;
<code class="nt">subjects</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">apiGroup</code><code class="p">:</code> <code class="l-Scalar-Plain">rbac.authorization.k8s.io</code>&#13;
  <code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Group</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">app1-viewer</code></pre>&#13;
&#13;
<p>You’ll notice in the example that the RoleBinding references a ClusterRole named <code>view</code>. This is a built-in role that is available in Kubernetes. Kubernetes provides a set of built-in roles that cover common use cases:</p>&#13;
<dl>&#13;
<dt>view</dt>&#13;
<dd>&#13;
<p>The view role <a data-primary="view role" data-type="indexterm" id="idm45611976286424"/>grants tenants read-only access to Namespace-scoped resources. This role can be bound to all the developers in a team, for example, as it allows them to inspect and troubleshoot their resources in production clusters.</p>&#13;
</dd>&#13;
<dt>edit</dt>&#13;
<dd>&#13;
<p>The edit role<a data-primary="edit role" data-type="indexterm" id="idm45611976284040"/> allows tenants to create, modify, and delete Namespace-scoped resources, in addition to viewing them. Given this role’s abilities, binding of this role is highly dependent on your approach to application deployment.</p>&#13;
</dd>&#13;
<dt>admin</dt>&#13;
<dd>&#13;
<p>In addition to viewing and editing resources, the admin role can create Roles and RoleBindings.<a data-primary="admin role" data-type="indexterm" id="idm45611976421400"/> This role is usually bound to the tenant administrator to delegate Namespace-management concerns.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These built-in roles are a good starting point. With that said, they can be considered too broad, as they grant access to a vast number of resources in the Kubernetes API. To follow the principle of least privilege, you can create tightly scoped roles that allow the minimum set of resources and actions required to get the job done. However, keep in mind that this can result in management overhead as you potentially need to manage many unique roles.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>In most Kubernetes deployments, tenants are typically authorized to list all Namespaces on the cluster.<a data-primary="tenants" data-secondary="listing all Namespaces on the cluster" data-type="indexterm" id="idm45611976418360"/> This is problematic if you need to prevent tenants from knowing what other Namespaces exist, as there is currently no way of achieving this using the Kubernetes RBAC system. If you do have this requirement, you must build a higher-level abstraction to handle it (OpenShift’s <a href="https://oreil.ly/xIAT8">Project</a> resource is an example abstraction that addresses this).</p>&#13;
</div>&#13;
&#13;
<p>RBAC is a must when running multiple tenants in the same cluster. It provides isolation at the control plane layer, which is necessary to prevent tenants from viewing and modifying each other’s resources. Make sure to leverage RBAC when building a multitenant Kubernetes-based platform.<a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-startref="ix_mltitenKuRBAC" data-tertiary="role-based access control" data-type="indexterm" id="idm45611976066232"/><a data-primary="RBAC (role-based access control)" data-secondary="multitenancy and" data-startref="ix_RBAC" data-type="indexterm" id="idm45611976064728"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Quotas" data-type="sect2"><div class="sect2" id="idm45611976063368">&#13;
<h2>Resource Quotas</h2>&#13;
&#13;
<p>As a platform operator offering a multitenant platform, you need to ensure that each tenant gets an appropriate share of the limited cluster resources.<a data-primary="resources" data-secondary="quotas on" data-type="indexterm" id="idm45611976061640"/><a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-tertiary="resource quotas" data-type="indexterm" id="idm45611976060664"/> Otherwise, nothing prevents an ambitious (or perhaps malicious) tenant from consuming the entire cluster and effectively starving the other tenants.</p>&#13;
&#13;
<p>To place a limit on resource consumption, you can use the resource quotas feature of Kubernetes. Resource quotas apply at the Namespace level, and they can limit two kinds of resources. On one hand, you can control the amount of compute resources available to a Namespace, such as CPU, memory, and storage. On the other hand, you can limit the number of API objects that can be created within a Namespace, such as the number of Pods, Services, etc. A common scenario that calls for limiting API objects is to control the number of LoadBalancer Services in cloud environments, which can get expensive.</p>&#13;
&#13;
<p>Because quotas apply at the Namespace level, your Namespace strategy impacts how you configure quotas<a data-primary="Namespaces" data-secondary="resource quotas and" data-type="indexterm" id="idm45611976403576"/>. If tenants get access to a single Namespace, applying quotas to each tenant is straightforward, as you can create a ResourceQuota for each tenant in their Namespace. The story is more complicated when tenants have access to multiple Namespaces.<a data-primary="Hierarchical Namespace Controller" data-type="indexterm" id="idm45611976402216"/> In this case, you need extra automation or an additional controller to enforce quota across different Namespaces. (The <a href="https://oreil.ly/PyPDK">Hierarchical Namespace Controller</a> is an attempt at addressing this issue).</p>&#13;
&#13;
<p>To further explore ResourceQuotas, let’s explore them in action. The following example shows a ResourceQuota that limits the Namespace to consume up to 1 CPU and 512 MiB of memory:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">ResourceQuota</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">cpu-mem</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">app1</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">hard</code><code class="p">:</code>&#13;
    <code class="nt">requests.cpu</code><code class="p">:</code> <code class="s">"1"</code>&#13;
    <code class="nt">requests.memory</code><code class="p">:</code> <code class="l-Scalar-Plain">512Mi</code>&#13;
    <code class="nt">limits.cpu</code><code class="p">:</code> <code class="s">"1"</code>&#13;
    <code class="nt">limits.memory</code><code class="p">:</code> <code class="l-Scalar-Plain">512Mi</code></pre>&#13;
&#13;
<p>As Pods in the <code>app1</code> Namespace start to get scheduled, the quota is consumed accordingly. For example, if we create a Pod that requests 0.5 CPUs and 256 MiB, we can see the updated quota as follows:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>kubectl describe resourcequota cpu-mem&#13;
Name:            cpu-mem&#13;
Namespace:       app1&#13;
Resource         Used   Hard&#13;
--------         ----   ----&#13;
limits.cpu       500m   1&#13;
limits.memory    512Mi  512Mi&#13;
requests.cpu     500m   1&#13;
requests.memory  512Mi  512Mi</pre>&#13;
&#13;
<p>Attempts to consume resources beyond the configured quota are blocked by an admission controller, as shown in the following error message. In this case, we were trying to consume 2 CPUs and 2 GiB of memory but were limited by the quota:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>kubectl apply -f my-app.yaml&#13;
Error from server <code class="o">(</code>Forbidden<code class="o">)</code>:&#13;
  error when creating <code class="s2">"my-app.yaml"</code>: pods <code class="s2">"my-app"</code> is forbidden:&#13;
    exceeded quota: cpu-mem,&#13;
      requested: limits.cpu<code class="o">=</code>2,limits.memory<code class="o">=</code>2Gi,&#13;
                 requests.cpu<code class="o">=</code>2,requests.memory<code class="o">=</code>2Gi,&#13;
      used: limits.cpu<code class="o">=</code>0,limits.memory<code class="o">=</code>0,&#13;
                 requests.cpu<code class="o">=</code>0,requests.memory<code class="o">=</code>0,&#13;
      limited: limits.cpu<code class="o">=</code>1,limits.memory<code class="o">=</code>512Mi,&#13;
                 requests.cpu<code class="o">=</code>1,requests.memory<code class="o">=</code>512Mi</pre>&#13;
&#13;
<p>As you can see, ResourceQuotas give you the ability to control how tenants consume cluster resources. They are critical when running a multitenant cluster, as they ensure tenants can safely share the cluster’s limited resources.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Admission Webhooks" data-type="sect2"><div class="sect2" id="idm45611976062776">&#13;
<h2>Admission Webhooks</h2>&#13;
&#13;
<p>Kubernetes has a set of built-in admission controllers that you can use to enforce policy.<a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-tertiary="admission webhooks" data-type="indexterm" id="idm45611975995304"/><a data-primary="webhooks" data-secondary="admission" data-type="indexterm" id="idm45611975994040"/><a data-primary="admission control" data-secondary="webhooks" data-type="indexterm" id="idm45611975993096"/> The ResourceQuota functionality we just covered is implemented using an admission controller. While the built-in controllers help solve common use cases, we typically find that organizations need to extend the admission layer to isolate and limit tenants further.<a data-primary="API server" data-secondary="multitenancy and" data-tertiary="enforcing policy using admission webhooks" data-type="indexterm" id="idm45611976334552"/></p>&#13;
&#13;
<p>Validating and mutating admission webhooks are the mechanisms that enable you to inject custom logic into the admission pipeline. We will not dig into the implementation details of these webhooks, as we have already covered them in <a data-type="xref" href="ch08.html#chapter8">Chapter 8</a>. Instead, we will explore some of the multitenancy use cases we’ve solved in the field with custom admission webhooks:</p>&#13;
<dl>&#13;
<dt>Standardized labels</dt>&#13;
<dd>&#13;
<p>You can enforce a standard set of labels across all API objects using a validating admission webhook.<a data-primary="labels" data-secondary="standardized for all API objects" data-type="indexterm" id="idm45611976329944"/> For example, you could require all resources to have an <code>owner</code> label. Having a standard set of labels is useful, as labels provide a way to query the cluster and even support higher-level features, such as network policies and scheduling constraints.</p>&#13;
</dd>&#13;
<dt>Require fields</dt>&#13;
<dd>&#13;
<p>Like enforcing a standard set of labels, you can use a validating admission webhook to mark fields of certain resources as required.<a data-primary="validating webhooks" data-secondary="marking required fields in resources" data-type="indexterm" id="idm45611976248312"/> For example, you can require all tenants to set the <code>https</code> field of their Ingress resources. Or perhaps require tenants to always set readiness and liveness probes in their Pod &#13;
<span class="keep-together">specifications</span>.</p>&#13;
</dd>&#13;
<dt>Set guardrails</dt>&#13;
<dd>&#13;
<p>Kubernetes has a broad set of features that you might want to limit or even disable. Webhooks allow you to set guardrails around specific functionality. Examples include disabling specific Service types (e.g., NodePorts), disallowing node selectors, controlling Ingress hostnames, and others.</p>&#13;
</dd>&#13;
<dt>MultiNamespace resource quotas</dt>&#13;
<dd>&#13;
<p>We have experienced cases in the field where organizations needed to enforce resource quotas across multiple Namespaces.<a data-primary="MultiNamespace resource quotas" data-type="indexterm" id="idm45611976005208"/> You can use a custom admission webhook/controller to implement this functionality, as the ResourceQuota object in Kubernetes is Namespace-scoped.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Overall, admission webhooks are a great way to enforce custom policy in your multi-tenant clusters.<a data-primary="policy engines" data-type="indexterm" id="idm45611976003528"/> And the emergence of policy engines such as <a href="https://www.openpolicyagent.org">Open Policy Agent (OPA)</a> and <a href="https://github.com/kyverno/kyverno">Kyverno</a> make it even simpler to implement them. Consider leveraging such engines to isolate and limit tenants in your clusters.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611976001064">&#13;
<h5>API Priority and Fairness</h5>&#13;
<p>The <a href="https://oreil.ly/lA7jy">API Priority and Fairness</a> feature in Kubernetes is another mechanism you can leverage to isolate tenants at the control plane layer.<a data-primary="API server" data-secondary="multitenancy and" data-tertiary="API Priority and Fairness" data-type="indexterm" id="idm45611976242728"/><a data-primary="API Priority and Fairness feature" data-type="indexterm" id="idm45611976241448"/> This feature prevents the API server from being overloaded by limiting the number of concurrent requests it handles according to a configurable policy.</p>&#13;
&#13;
<p>The API server sits at the heart of control plane functionality.  If one tenant overloads it, this is likely to have significant consequences for other tenants.  The API Priority and Fairness capability can thwart any attempt from a malicious tenant or buggy API client from causing this overload.  Instead, the client’s requests are queued or rejected according to the configured policy.</p>&#13;
&#13;
<p>The API Priority and Fairness feature is relatively new. As of this writing, the feature is in alpha and we have yet to see it implemented in the field. Thus, we would recommend holding off on enabling it unless you have a strong reason to use it. Even then, if you find you need this capability, we would encourage you to evaluate whether running multiple clusters instead of leveraging this capability would result in a simpler implementation.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Requests and Limits" data-type="sect2"><div class="sect2" id="idm45611976238536">&#13;
<h2>Resource Requests and Limits</h2>&#13;
&#13;
<p>Kubernetes schedules workloads onto a shared pool of cluster nodes.<a data-primary="resources" data-secondary="requests and limits" data-type="indexterm" id="idm45611976237352"/><a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-tertiary="resource requests and limits" data-type="indexterm" id="idm45611974914968"/> Commonly, workloads from different tenants get scheduled onto the same node and thus share the node’s resources. Ensuring that the resources are shared fairly is one of the most critical concerns when running a multitenant platform. Otherwise, tenants can negatively affect other tenants that are colocated on the same node.</p>&#13;
&#13;
<p>Resource requests and limits in Kubernetes are the mechanisms that isolate tenants from one another when it comes to compute resources. Resource requests are generally fulfilled at the Kubernetes scheduler level (CPU requests are also reflected at runtime, as we will see later). <a data-primary="Linux" data-secondary="resource limits" data-type="indexterm" id="idm45611974912520"/>In contrast, resource limits are implemented at the node level using Linux control groups (cgroups) and the Linux Completely Fair Scheduler (CFS).</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>While requests and limits provide adequate isolation for production workloads, it should be known that this isolation is not as strict as that provided by a hypervisor.<a data-primary="hypervisors" data-type="indexterm" id="idm45611974910088"/> Completely removing noisy neighbor symptoms from workloads can be challenging in containerized environments. Be sure to experiment and understand the implication of multiple workloads under load on a given Kubernetes node.</p>&#13;
</div>&#13;
&#13;
<p>In addition to providing resource isolation, resource requests and limits determine a Pod’s Quality of Service (QoS) class.<a data-primary="Quality of Service (QoS) class (Pods)" data-type="indexterm" id="idm45611976103976"/><a data-primary="Pods" data-secondary="Quality of Service (QoS) class" data-type="indexterm" id="idm45611976103256"/> The QoS class is important because it determines the order in which the kubelet evicts Pods when a node is running low on resources. Kubernetes offers the following QoS classes:</p>&#13;
<dl>&#13;
<dt>Guaranteed</dt>&#13;
<dd>&#13;
<p>Pods with CPU limits equal to CPU requests and memory limits equal to memory requests.<a data-primary="CPU requests and limits" data-type="indexterm" id="idm45611976100280"/> This must be true across all containers. The kubelet seldom evicts Guaranteed Pods.</p>&#13;
</dd>&#13;
<dt>Burstable</dt>&#13;
<dd>&#13;
<p>Pods that do not qualify as Guaranteed and have at least one container with CPU or memory requests. The kubelet evicts Burstable Pods based on how many resources they are consuming above their requests. Pods bursting higher above their requests are evicted before Pods bursting closer to their requests.</p>&#13;
</dd>&#13;
<dt>BestEffort</dt>&#13;
<dd>&#13;
<p>Pods that have no CPU or memory limits or requests. These Pods run on a “best effort” basis. They are the first to be evicted by the kubelet.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Pod eviction is a complex process. In addition to using QoS classes to rank Pods, the kubelet also considers Pod priorities when making eviction decisions.<a data-primary="Pods" data-secondary="evictions" data-type="indexterm" id="idm45611976010424"/> The Kubernetes documentation has an excellent article that discusses <a href="https://oreil.ly/LuCD9">“Out of Resource” handling</a> in greater detail.</p>&#13;
</div>&#13;
&#13;
<p>Now that we know that resource requests and limits provide tenant isolation and determine a Pod’s QoS class, let’s dive into the details of resource requests and limits. Even though Kubernetes supports requesting and limiting different resources, we will focus our discussion on CPU and memory, the essential resources that all workloads need at runtime. Let’s discuss memory requests and limits first.<a data-primary="memory requests and limits" data-type="indexterm" id="idm45611976007688"/></p>&#13;
&#13;
<p>Each container in a Pod can specify memory requests and limits.<a data-primary="Pods" data-secondary="memory requests and limits" data-type="indexterm" id="idm45611976176136"/> When memory requests are set, the scheduler adds them up to get the Pod’s overall memory request. With this information, the scheduler finds a node with enough memory capacity to host the Pod. If none of the cluster nodes have enough memory, the Pod remains in a pending state. Once scheduled, though, the containers in the Pod are guaranteed the requested memory.</p>&#13;
&#13;
<p>A Pod’s memory request represents a guaranteed lower bound for the memory resource. However, they can consume additional memory if it’s available on the node. This is problematic because the Pod uses memory that the scheduler can assign to other workloads or tenants. When a new Pod is scheduled onto the same node, the Pods may fight over the memory. To honor the memory requests of both Pods, the Pod consuming memory above its request is terminated. <a data-type="xref" href="#pod_consuming_memory_above_its_request_is_terminated_to_reclaim_memory_for_the_new_pod">Figure 12-3</a> depicts this &#13;
<span class="keep-together">process</span>.</p>&#13;
&#13;
<figure><div class="figure" id="pod_consuming_memory_above_its_request_is_terminated_to_reclaim_memory_for_the_new_pod">&#13;
<img alt="prku 1203" src="assets/prku_1203.png"/>&#13;
<h6><span class="label">Figure 12-3. </span>Pod consuming memory above its request is terminated to reclaim memory for the new Pod.</h6>&#13;
</div></figure>&#13;
&#13;
<p>In order to control the amount of memory that tenants can consume, we must include memory limits on the workloads, which enforce an upper bound on the amount of memory available to a given workload.<a data-primary="workloads" data-secondary="memory limits on" data-type="indexterm" id="idm45611976133352"/> If the workload attempts to consume memory above the limit, the workload is terminated. This is because memory is a noncompressible resource.<a data-primary="containers" data-secondary="out-of-memory killed (OOMKilled)" data-type="indexterm" id="idm45611976132088"/> There is no way to throttle memory, and thus the process must be terminated when the node’s memory is under contention. The following snippet shows a container that was out-of-memory killed (OOMKilled). Notice the “Reason” in the “Last State” section of the output:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>kubectl describe pod memory&#13;
Name:         memory&#13;
Namespace:    default&#13;
Priority:     0&#13;
... &lt;snip&gt; ...&#13;
Containers:&#13;
  stress:&#13;
    ... &lt;snip&gt; ...&#13;
    Last State:     Terminated&#13;
      Reason:       OOMKilled&#13;
      Exit Code:    1&#13;
      Started:      Fri, <code class="m">23</code> Oct <code class="m">2020</code> 10:11:51 -0400&#13;
      Finished:     Fri, <code class="m">23</code> Oct <code class="m">2020</code> 10:11:56 -0400&#13;
    Ready:          True&#13;
    Restart Count:  1&#13;
    Limits:&#13;
      memory:  100Mi&#13;
    Requests:&#13;
      memory:     100Mi</pre>&#13;
&#13;
<p>A common question we encounter in the field is whether one should allow tenants to set memory limits higher than requests. In other words, whether nodes should be oversubscribed on memory. This question boils down to a trade-off between node density and stability. When you oversubscribe your nodes, you increase node density but decrease workload stability. As we’ve seen, workloads that consume memory above their requests get terminated when memory comes under contention. In most cases, we encourage platform teams to avoid oversubscribing nodes, as they typically consider stability more important than tightly packing nodes. This is especially the case in clusters hosting production workloads.</p>&#13;
&#13;
<p>Now that we’ve covered memory requests and limits, let’s shift our discussion to CPU.<a data-primary="CPU requests and limits" data-type="indexterm" id="idm45611976277896"/> In contrast to memory, CPU is a compressible resource. You can throttle processes when CPU is under contention. For this reason, CPU requests and limits are somewhat more complex than memory requests and limits.</p>&#13;
&#13;
<p>CPU requests and limits are specified using CPU units. In most cases, 1 CPU unit is equivalent to 1 CPU core. Requests and limits can be fractional (e.g., 0.5 CPU) and they can be expressed using millis by adding an <code>m</code> suffix. 1 CPU unit equals 1000m CPU.</p>&#13;
&#13;
<p>When containers within a Pod specify CPU requests, the scheduler finds a node with enough capacity to place the Pod. Once placed, the kubelet converts the requested CPU units into cgroup CPU shares. CPU shares is a mechanism in the Linux kernel that grants CPU time to cgroups (i.e., the processes within the cgroup). The following are critical aspects of CPU shares to keep in mind:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>CPU shares are relative. 1000 CPU shares does not mean 1 CPU core or 1000 CPU cores. Instead, the CPU capacity is proportionally divided among all cgroups according to their relative shares. For example, consider two processes in different cgroups. If process 1 (P1) has 2000 shares, and process 2 (P2) has 1000 shares, P1 will get twice the CPU time as P2.</p>&#13;
</li>&#13;
<li>&#13;
<p>CPU shares come into effect only when the CPU is under contention. If the CPU is not fully utilized, processes are not throttled and can consume additional CPU cycles. Following the preceding example, P1 will get twice the CPU time as P2 only when the CPU is 100% busy.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>CPU shares (CPU requests) provide the CPU resource isolation necessary to run different tenants on the same node. As long as tenants declare CPU requests, the CPU capacity is shared according to those requests. Consequently, tenants are unable to starve other tenants from getting CPU time.</p>&#13;
&#13;
<p>CPU limits work differently. They set an upper bound on the CPU time that each container can use. Kubernetes leverages the bandwidth control feature of the Completely Fair Scheduler (CFS) to implement CPU limits.<a data-primary="CFS (Completely Fair Scheduler) bandwidth control" data-type="indexterm" id="idm45611976269480"/> CFS bandwidth control uses time periods to limit CPU consumption. Each container gets a quota within a configurable period. The quota determines how much CPU time can be consumed in every period. If the container exhausts the quota, the container is throttled for the rest of the period.</p>&#13;
&#13;
<p>By default, Kubernetes sets the period to 100 ms. A container with a limit of 0.5 CPUs gets 50 ms of CPU time every 100 ms, as depicted in <a data-type="xref" href="#cpu_consumption_and_throttling_of_a_process_running">Figure 12-4</a>. A container with a limit of 3 CPUs gets 300 ms of CPU time in every 100 millisecond period, effectively allowing the container to consume up to 3 CPUs every 100 ms.</p>&#13;
&#13;
<figure><div class="figure" id="cpu_consumption_and_throttling_of_a_process_running">&#13;
<img alt="prku 1204" src="assets/prku_1204.png"/>&#13;
<h6><span class="label">Figure 12-4. </span>CPU consumption and throttling of a process running in a cgroup that has a CFS period of 100 milliseconds and a CPU quota of 50 milliseconds.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Due to the nature of CPU limits, they can sometimes result in surprising behavior or unexpected throttling. This is usually the case in multithreaded applications that can consume the entire quota at the very beginning of the period. For example, a container with a limit of 1 CPU will get 100 ms of CPU time every 100 ms. Assuming the container has 5 threads using CPU, the container consumes the 100 ms quota in 20 ms and gets throttled for the remaining 80 ms. This is depicted in <a data-type="xref" href="#multi_threaded_application_consumes_the_entire">Figure 12-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="multi_threaded_application_consumes_the_entire">&#13;
<img alt="prku 1205" src="assets/prku_1205.png"/>&#13;
<h6><span class="label">Figure 12-5. </span>Multithreaded application consumes the entire CPU quota in the first 20 milliseconds of the 100-millisecond period.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Enforcing CPU limits is useful to minimize the variability of an application’s performance, especially when running multiple replicas across different nodes. This variability in performance stems from the fact that, without CPU limits, replicas can burst and consume idle CPU cycles, which <em>might</em> be available at different times. By setting the CPU limits equal to the CPU requests, you remove the variability as the workloads get precisely the CPU they requested. (Google and IBM published an excellent <a href="https://oreil.ly/39Pu7">whitepaper</a> that discusses CFS bandwidth control in more detail.) In a similar vein, CPU limits play a critical role in performance testing and benchmarking. Without any CPU limits, your benchmarks will produce inconclusive results, as the CPU available to your workloads will vary based on the nodes where they got scheduled and the amount of idle CPU available.</p>&#13;
&#13;
<p>If your workloads require predictable access to CPU (e.g., latency-sensitive applications), setting CPU limits equal to CPU requests is helpful. Otherwise, placing an upper bound on CPU cycles is not necessary. When the CPU resources on a node are under contention, the CPU shares mechanism ensures that workloads get their fair share of CPU time, according to their container’s CPU requests. When the CPU is not under contention, the idle CPU cycles are not wasted as workloads opportunistically consume them.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611976233000">&#13;
<h5>Linux Kernel Bug Impacting Kubernetes CPU Limits</h5>&#13;
<p>Another issue with CPU limits is a <a href="https://oreil.ly/EPWrm">Linux kernel bug</a> that throttles containers unnecessarily.<a data-primary="CPU requests and limits" data-secondary="Linux kernel bug affecting CPU limits" data-type="indexterm" id="idm45611976230952"/><a data-primary="Linux" data-secondary="kernel bug impacting CPU limits" data-type="indexterm" id="idm45611976230008"/> This has a significant impact on latency-sensitive workloads, such as web services. To avoid this issue, Kubernetes users resorted to different workarounds, including:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Removing CPU limits from Pod specifications</p>&#13;
</li>&#13;
<li>&#13;
<p>Disabling enforcement of CPU limits by setting the kubelet flag <code>--cpu-cfs-quota=false</code></p>&#13;
</li>&#13;
<li>&#13;
<p>Reducing the CFS period to 5–10ms by setting the kubelet flag <code>--cpu-cfs-quota-period</code></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Depending on your Linux kernel version, you might not have to implement these workarounds, as the bug has been <a href="https://oreil.ly/xekUx">fixed</a> in version 5.4 of the Linux kernel and backported to versions 4.14.154+, 4.19.84+, and 5.3.9+. If you need to enforce CPU limits, consider upgrading your Linux kernel version to avoid this bug.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Network Policies" data-type="sect2"><div class="sect2" id="idm45611974916312">&#13;
<h2>Network Policies</h2>&#13;
&#13;
<p>In most deployments, Kubernetes assumes all Pods running on the platform can communicate with each other.<a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-tertiary="network policies" data-type="indexterm" id="ix_mltitenKuNP"/><a data-primary="network policies" data-type="indexterm" id="ix_netpol"/> As you can imagine, this stance is problematic for multitenant clusters, where you might want to enforce network-level isolation between the tenants. The NetworkPolicy API is the mechanism you can leverage to ensure tenants are isolated from each other at the network layer.</p>&#13;
&#13;
<p>We explored Network Policies in <a data-type="xref" href="ch05.html#chapter5">Chapter 5</a>, where we discussed the role of the Container Networking Interface (CNI) plug-ins in enforcing network policies. In this section, we will discuss the <em>default deny-all</em> network policy model, a common approach to Network Policy, especially in multitenant clusters.</p>&#13;
&#13;
<p>As a platform operator, you can establish a default deny-all network policy across the entire cluster. By doing so, you take the strongest stance regarding network security and isolation, given that tenants are fully isolated as soon as they are onboarded onto the platform. Furthermore, you drive tenants to a model where they have to declare their workloads’ network interactions, which improves their applications’ network security.</p>&#13;
&#13;
<p>When it comes to implementing a default deny-all policy, you can follow two different paths, each with its pros and cons. The first approach leverages the NetworkPolicy API available in Kubernetes.<a data-primary="NetworkPolicy API" data-secondary="leveraging to implement deny-all policy" data-type="indexterm" id="idm45611976321576"/> Because this is a core API, this implementation is portable across different CNI plug-ins. However, given that the NetworkPolicy object is Namespace-scoped, it requires you to create and manage multiple default deny-all NetworkPolicy resources, one per Namespace. Additionally, because tenants need the authorization to create their own NetworkPolicy objects, you must implement additional controls (usually via admission webhooks, as discussed earlier) to prevent tenants from modifying or deleting the default deny-all policy. The following snippet shows a default deny-all NetworkPolicy object.<a data-primary="NetworkPolicy objects" data-secondary="default deny-all" data-type="indexterm" id="idm45611976227752"/> The empty Pod selector selects all the Pods in the Namespace:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">NetworkPolicy</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">default-deny-all</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">tenant-a</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">podSelector</code><code class="p">:</code> <code class="p-Indicator">{}</code>&#13;
  <code class="nt">policyTypes</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Egress</code></pre>&#13;
&#13;
<p>The alternative approach is to leverage CNI plug-in-specific Custom Resource Definitions (CRDs).<a data-primary="custom resource definitions (CRDs)" data-secondary="CNI plug-in-spacific, leveraging for network policy" data-type="indexterm" id="idm45611976083592"/><a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-tertiary="leveraging plug-in-specific CRDS for network policy" data-type="indexterm" id="idm45611977097448"/> Some CNI plug-ins, such as Antrea, Calico, and Cilium, provide CRDs that enable you to specify cluster-level or “global” network policy. These CRDs help you reduce the implementation and management complexity of the default deny-all policy, but they tie you to a specific CNI plug-in.<a data-primary="Calico" data-secondary="GlobalNetworkPolicy CRD" data-type="indexterm" id="idm45611976125528"/> The following snippet shows an example Calico GlobalNetworkPolicy CRD that implements the default deny-all policy:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">projectcalico.org/v3</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">GlobalNetworkPolicy</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">default-deny</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">selector</code><code class="p">:</code> <code class="l-Scalar-Plain">all()</code>&#13;
  <code class="nt">types</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Egress</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Typically, default deny-all network policy implementations make exceptions to allow fundamental network traffic, such as DNS queries to the cluster’s DNS server. Additionally, they are not applied to the kube-system Namespace and any other system-level Namespaces to prevent breaking the cluster. The YAML snippets in the preceding code do not address these concerns.</p>&#13;
</div>&#13;
&#13;
<p>As with most choices, whether to use the built-in NetworkPolicy object or a CRD results in a trade-off between portability and simplicity. In our experience, we’ve found that the simplicity gained by leveraging the CNI-specific CRD is usually worth the trade-off, given that switching CNI plug-ins is an uncommon event. With that said, you might not have to make this choice in the future, as the Kubernetes &#13;
<span class="keep-together">Networking</span> Special Interest Group (sig-network) is <a href="https://oreil.ly/jVP_f">looking at evolving</a> the NetworkPolicy APIs to support cluster-scoped network policies.</p>&#13;
&#13;
<p>Once the default deny-all policy is in place, tenants are responsible for poking holes in the network fabric to ensure their applications can function. They achieve this using the NetworkPolicy resource, in which they specify ingress and egress rules that apply to their workloads.<a data-primary="NetworkPolicy objects" data-secondary="tenants specifying ingress and egress rules in" data-type="indexterm" id="idm45611975233864"/><a data-primary="NetworkPolicy objects" data-type="indexterm" id="idm45611975232792"/> For example, the following snippet shows a NetworkPolicy that could be applied to a web service. It allows Ingress or incoming traffic from the web frontend, and it allows Egress or outgoing traffic to its database:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">NetworkPolicy</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">webservice</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">reservations</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">podSelector</code><code class="p">:</code>&#13;
    <code class="nt">matchLabels</code><code class="p">:</code>&#13;
      <code class="nt">role</code><code class="p">:</code> <code class="l-Scalar-Plain">webservice</code>&#13;
  <code class="nt">policyTypes</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Egress</code>&#13;
  <code class="nt">ingress</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">from</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">podSelector</code><code class="p">:</code>&#13;
        <code class="nt">matchLabels</code><code class="p">:</code>&#13;
          <code class="nt">role</code><code class="p">:</code> <code class="l-Scalar-Plain">frontend</code>&#13;
    <code class="nt">ports</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
      <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>&#13;
  <code class="nt">egress</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">to</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">podSelector</code><code class="p">:</code>&#13;
        <code class="nt">role</code><code class="p">:</code> <code class="l-Scalar-Plain">database</code>&#13;
    <code class="nt">ports</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
      <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">3306</code></pre>&#13;
&#13;
<p>Enforcing a default deny-all network policy is a critical tenant isolation mechanism. As you build your platform atop Kubernetes, we strongly encourage you to follow this pattern, especially if you plan to host multiple tenants.<a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-startref="ix_mltitenKuNP" data-tertiary="network policies" data-type="indexterm" id="idm45611975230216"/><a data-primary="network policies" data-startref="ix_netpol" data-type="indexterm" id="idm45611975476472"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Security Policies" data-type="sect2"><div class="sect2" id="idm45611976565640">&#13;
<h2>Pod Security Policies</h2>&#13;
&#13;
<p>Pod Security Policies (PSPs) are another important mechanism to ensure tenants can coexist safely on the same cluster.<a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-tertiary="Pod Security Policies" data-type="indexterm" id="ix_mltitenKuPSP"/><a data-primary="Pod Security Policies (PSPs)" data-type="indexterm" id="ix_PSPs"/> PSPs control critical security parameters of Pods at runtime, such as their ability to run as privileged, access host volumes, bind to the host network, and more. Without PSPs (or a similar policy enforcement mechanism), workloads are free to do virtually anything on a cluster node.<a data-primary="PSPs" data-see="Pod Security Policies" data-type="indexterm" id="idm45611975470872"/></p>&#13;
&#13;
<p>Kubernetes enforces most of the controls implemented via PSPs using an admission controller. (The rule that requires a nonroot user is sometimes enforced by the kubelet, which verifies the runtime user of the container after downloading the image.) Once the admission controller is enabled, attempts to create a Pod are blocked unless they are allowed by a PSP. <a data-type="xref" href="#sample_restrictive_podsecuritypolicy">Example 12-1</a> shows a restrictive PSP that we typically define as the <em>default</em> policy in multitenant clusters.</p>&#13;
<div data-type="example" id="sample_restrictive_podsecuritypolicy">&#13;
<h5><span class="label">Example 12-1. </span>Sample restrictive PodSecurityPolicy</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">policy/v1beta1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">PodSecurityPolicy</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">default</code><code>&#13;
</code><code>  </code><code class="nt">annotations</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">seccomp.security.alpha.kubernetes.io/allowedProfileNames</code><code class="p">:</code><code> </code><code class="p-Indicator">|</code><code>&#13;
</code><code>      </code><code class="no">'docker/default,runtime/default'</code><code>&#13;
</code><code>    </code><code class="nt">apparmor.security.beta.kubernetes.io/allowedProfileNames</code><code class="p">:</code><code> </code><code class="s">'</code><code class="s">runtime/default</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="nt">seccomp.security.alpha.kubernetes.io/defaultProfileName</code><code class="p">:</code><code>  </code><code class="s">'</code><code class="s">runtime/default</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="nt">apparmor.security.beta.kubernetes.io/defaultProfileName</code><code class="p">:</code><code>  </code><code class="s">'</code><code class="s">runtime/default</code><code class="s">'</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">privileged</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">false</code><code> </code><a class="co" href="#callout_multitenancy_CO1-1" id="co_multitenancy_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">allowPrivilegeEscalation</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">false</code><code>&#13;
</code><code>  </code><code class="nt">requiredDropCapabilities</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">ALL</code><code>&#13;
</code><code>  </code><code class="nt">volumes</code><code class="p">:</code><code> </code><a class="co" href="#callout_multitenancy_CO1-2" id="co_multitenancy_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">configMap</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">emptyDir</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">projected</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">secret</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">downwardAPI</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">persistentVolumeClaim</code><code class="s">'</code><code>&#13;
</code><code>  </code><code class="nt">hostNetwork</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">false</code><code> </code><a class="co" href="#callout_multitenancy_CO1-3" id="co_multitenancy_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="nt">hostIPC</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">false</code><code>&#13;
</code><code>  </code><code class="nt">hostPID</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">false</code><code>&#13;
</code><code>  </code><code class="nt">runAsUser</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">rule</code><code class="p">:</code><code> </code><code class="s">'</code><code class="s">MustRunAsNonRoot</code><code class="s">'</code><code> </code><a class="co" href="#callout_multitenancy_CO1-4" id="co_multitenancy_CO1-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>  </code><code class="nt">seLinux</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">rule</code><code class="p">:</code><code> </code><code class="s">'</code><code class="s">RunAsAny</code><code class="s">'</code><code> </code><a class="co" href="#callout_multitenancy_CO1-5" id="co_multitenancy_CO1-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>  </code><code class="nt">supplementalGroups</code><code class="p">:</code><code> </code><a class="co" href="#callout_multitenancy_CO1-6" id="co_multitenancy_CO1-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>    </code><code class="nt">rule</code><code class="p">:</code><code> </code><code class="s">'</code><code class="s">MustRunAs</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="nt">ranges</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">min</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">1</code><code>&#13;
</code><code>        </code><code class="nt">max</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">65535</code><code>&#13;
</code><code>  </code><code class="nt">fsGroup</code><code class="p">:</code><code> </code><a class="co" href="#callout_multitenancy_CO1-7" id="co_multitenancy_CO1-7"><img alt="7" src="assets/7.png"/></a><code>&#13;
</code><code>    </code><code class="nt">rule</code><code class="p">:</code><code> </code><code class="s">'</code><code class="s">MustRunAs</code><code class="s">'</code><code>&#13;
</code><code>    </code><code class="nt">ranges</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">min</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">1</code><code>&#13;
</code><code>        </code><code class="nt">max</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">65535</code><code>&#13;
</code><code>  </code><code class="nt">readOnlyRootFilesystem</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">false</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_multitenancy_CO1-1" id="callout_multitenancy_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Disallow privileged containers.</p></dd>&#13;
<dt><a class="co" href="#co_multitenancy_CO1-2" id="callout_multitenancy_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Control the volume types that Pods can use.</p></dd>&#13;
<dt><a class="co" href="#co_multitenancy_CO1-3" id="callout_multitenancy_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Prevent Pods from binding to the underlying host’s network stack.</p></dd>&#13;
<dt><a class="co" href="#co_multitenancy_CO1-4" id="callout_multitenancy_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Ensure that containers run as a nonroot user.</p></dd>&#13;
<dt><a class="co" href="#co_multitenancy_CO1-5" id="callout_multitenancy_CO1-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>This policy assumes the nodes are using AppArmor rather than SELinux.</p></dd>&#13;
<dt><a class="co" href="#co_multitenancy_CO1-6" id="callout_multitenancy_CO1-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>Specify the allowed group IDs that containers can use. The root gid (0) is &#13;
<span class="keep-together">disallowed</span>.</p></dd>&#13;
<dt><a class="co" href="#co_multitenancy_CO1-7" id="callout_multitenancy_CO1-7"><img alt="7" src="assets/7.png"/></a></dt>&#13;
<dd><p>Control the group IDs applied to volumes. The root gid (0) is disallowed.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The existence of a PSP that allows the Pod is not enough for the Pod to be admitted. The Pod must be authorized to <em>use</em> the PSP as well. PSP authorization is handled using RBAC.<a data-primary="RBAC (role-based access control)" data-secondary="PSP authorization" data-type="indexterm" id="idm45611975641352"/> Pods can use a PSP if their Service Account is authorized to use it. Pods can also use a PSP if the actor creating the Pod is authorized to use the PSP. <a data-primary="Service Accounts" data-secondary="using for PSP authorization" data-type="indexterm" id="idm45611975640104"/>However, given that Pods are seldom created by cluster users, using Service Accounts for PSP authorization is the more common approach.<a data-primary="Roles" data-type="indexterm" id="idm45611975638920"/><a data-primary="RoleBindings" data-type="indexterm" id="idm45611975638248"/> The following snippet shows a Role and RoleBinding that authorizes a Service Account to use a specific PSP named <code>sample-psp</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Role</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">sample-psp</code>&#13;
<code class="nt">rules</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">apiGroups</code><code class="p">:</code> <code class="p-Indicator">[</code><code class="s">'policy'</code><code class="p-Indicator">]</code>&#13;
  <code class="nt">resources</code><code class="p">:</code> <code class="p-Indicator">[</code><code class="s">'podsecuritypolicies'</code><code class="p-Indicator">]</code>&#13;
  <code class="nt">resourceNames</code><code class="p">:</code> <code class="p-Indicator">[</code><code class="s">'sample-psp'</code><code class="p-Indicator">]</code>&#13;
  <code class="nt">verbs</code><code class="p">:</code> <code class="p-Indicator">[</code><code class="s">'use'</code><code class="p-Indicator">]</code>&#13;
<code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1beta1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">RoleBinding</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">sample-psp</code>&#13;
<code class="nt">subjects</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">ServiceAccount</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
<code class="nt">roleRef</code><code class="p">:</code>&#13;
  <code class="nt">apiGroup</code><code class="p">:</code> <code class="l-Scalar-Plain">rbac.authorization.k8s.io</code>&#13;
  <code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Role</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">sample-psp</code></pre>&#13;
&#13;
<p>In most cases, the platform team is responsible for creating and managing the PSPs and enabling tenants to use them. As you design the policies, always follow the &#13;
<span class="keep-together">principle</span> of least privilege. Only allow the minimum set of privileges and capabilities required for Pods to complete their work. As a starting point, we typically recommend creating the following policies:</p>&#13;
<dl>&#13;
<dt>Default</dt>&#13;
<dd>&#13;
<p>The default policy is usable by all tenants on the cluster. It should be a restrictive policy that blocks all privileged operations, drops all Linux capabilities, disallows running as the root user, etc. (See <a data-type="xref" href="#sample_restrictive_podsecuritypolicy">Example 12-1</a> for the YAML definition of this policy.) To make it the default policy, you can authorize all Pods in the cluster to use this PSP using a ClusterRole and ClusterRoleBinding.</p>&#13;
</dd>&#13;
<dt>Kube-system</dt>&#13;
<dd>&#13;
<p>The kube-system policy is for the system components that exist within the kube-system Namespace. Due to the nature of these components, this policy needs to be more permissive than the default policy. For example, it must allow Pods to mount <code>hostPath</code> volumes and run as root. In contrast to the default policy, the RBAC authorization is achieved using a RoleBinding scoped to all Service Accounts in the kube-system Namespace.</p>&#13;
</dd>&#13;
<dt>Networking</dt>&#13;
<dd>&#13;
<p>The networking policy is geared toward the cluster’s networking components, such as the CNI plug-in. These Pods require even more privileges to manipulate the networking stack of cluster nodes. To isolate this policy to networking Pods, create a RoleBinding that authorizes only the networking Pods Service Accounts to use the policy.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>With these policies in place, tenants can deploy unprivileged workloads into the cluster. If there is a workload that needs additional privileges, you must determine whether you can tolerate the risk of running that privileged workload in the same cluster. If so, create a different policy tailored to that workload. Grant the privileges required by the workload and only authorize that workload’s Service Account to use the PSP.</p>&#13;
&#13;
<p>PSPs are a critical enforcement mechanism in multitenant platforms. They control what tenants can and cannot do at runtime, as they run alongside other tenants on shared nodes. When building your platform, you should leverage PSPs to ensure tenants are isolated and protected from each other.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The Kubernetes community is <a href="https://oreil.ly/ayN8j">discussing</a> the possibility of removing the PodSecurityPolicy API and admission controller from the core project.<a data-primary="Pod Security Policies (PSPs)" data-secondary="PodSecurityPolicy API" data-type="indexterm" id="idm45611975390312"/><a data-primary="policy engines" data-type="indexterm" id="idm45611975389368"/> If removed, you can leverage a policy engine such as <a href="https://oreil.ly/wrz23">Open Policy Agent</a> or <a href="https://oreil.ly/v7C2H">Kyverno</a> to implement similar functionality.<a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-startref="ix_mltitenKuPSP" data-tertiary="Pod Security Policies" data-type="indexterm" id="idm45611975387192"/><a data-primary="Pod Security Policies (PSPs)" data-startref="ix_PSPs" data-type="indexterm" id="idm45611975385656"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multitenant Platform Services" data-type="sect2"><div class="sect2" id="idm45611975474936">&#13;
<h2>Multitenant Platform Services</h2>&#13;
&#13;
<p>In addition to<a data-primary="services" data-secondary="multitenant platform services" data-type="indexterm" id="ix_serMPS"/><a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-tertiary="multitenant platform services" data-type="indexterm" id="ix_mltitenKuMPS"/> isolating the Kubernetes control plane and workload plane, you can enforce isolation in the different services you offer on the platform. These include services such as logging, monitoring, ingress, etc. A significant determining factor in implementing this isolation is the technology you use to provide the service. In some cases, the tool or technology might support multitenancy out of the box, vastly simplifying your implementation.</p>&#13;
&#13;
<p>Another important consideration to make is whether you <em>need</em> to isolate tenants at this layer. Is it okay for tenants to look at each other’s logs and metrics? Is it acceptable for them to freely discover each other’s services over DNS? Can they share the ingress data path? Answering these and similar questions will help clarify your requirements. In the end, it boils down to the level of trust between the tenants you are hosting on the platform.</p>&#13;
&#13;
<p>A typical scenario we run into when helping platform teams is multitenant monitoring with Prometheus. Out of the box, Prometheus does not support multitenancy. Metrics are ingested and stored in a single time-series database, which is accessible by anyone who has access to the Prometheus HTTP endpoint. In other words, if the Prometheus instance is scraping metrics from multiple tenants, there’s no way to prevent different tenants from seeing each other’s data. To address this issue, we need to deploy separate Prometheus instances per tenant.</p>&#13;
&#13;
<p>When approaching<a data-primary="Prometheus Operator" data-type="indexterm" id="idm45611975377032"/> this problem, we typically leverage the <a href="https://oreil.ly/j38-Q">prometheus-operator</a>. As discussed in <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a>, the prometheus-operator allows you to deploy and manage multiple instances of Prometheus using Custom Resource Definitions.<a data-primary="Prometheus" data-secondary="tenants deploying multiple instances of" data-type="indexterm" id="idm45611975374504"/> With this capability, you can offer a monitoring platform service that can safely support various tenants. Tenants are completely isolated as they get a dedicated monitoring stack that includes Prometheus, Grafana, Alertmanager, etc.</p>&#13;
&#13;
<p>Depending on the platform’s target user experience, you can either allow tenants to deploy their Prometheus instance using the operator, or you can automatically create an instance when you onboard a new tenant. When the platform team has the capacity, we recommend the latter, as it removes the burden from the platform tenants and provides an improved user experience.</p>&#13;
&#13;
<p>Centralized logging is another platform service that you can implement with multi-tenancy in mind.<a data-primary="logging" data-secondary="centralized logging platform service" data-type="indexterm" id="idm45611975371864"/> Typically, this involves sending logs for different tenants to different backends or datastores. Most log forwarders have routing features that you can use to implement a multitenant solution.</p>&#13;
&#13;
<p>In the<a data-primary="Fluentd" data-type="indexterm" id="idm45611975370152"/><a data-primary="Fluent Bit" data-type="indexterm" id="idm45611975369416"/> case of Fluentd and Fluent Bit, you can leverage their tag-based routing features when configuring the forwarder. The following snippet shows a sample Fluent Bit output configuration that routes Alice’s logs (Pods in the <code>alice-ns</code> Namespace) to one backend and Bob’s logs (Pods in the <code>bob-ns</code> Namespace) to another backend:</p>&#13;
&#13;
<pre data-type="programlisting">[OUTPUT]&#13;
    Name            es&#13;
    Match           kube.var.log.containers.**alice-ns**.log&#13;
    Host            alice.es.internal.cloud.example.com&#13;
    Port            ${FLUENT_ELASTICSEARCH_PORT}&#13;
    Logstash_Format On&#13;
    Replace_Dots    On&#13;
    Retry_Limit     False&#13;
&#13;
[OUTPUT]&#13;
    Name            es&#13;
    Match           kube.var.log.containers.**bob-ns**.log&#13;
    Host            bob.es.internal.cloud.example.com&#13;
    Port            ${FLUENT_ELASTICSEARCH_PORT}&#13;
    Logstash_Format On&#13;
    Replace_Dots    On&#13;
    Retry_Limit     False</pre>&#13;
&#13;
<p>In addition to isolating the logs at the backend, you can also implement rate-limiting or throttling to prevent one tenant from hogging the log forwarding infrastructure. Both Fluentd and Fluent Bit have plug-ins you can use to enforce such limits. Finally, if you have a use case that warrants it, you can leverage a logging operator to support more advanced use cases, such as exposing the logging configuration via a Kubernetes CRD.</p>&#13;
&#13;
<p>Multitenancy in the platform services layer is sometimes overlooked by platform teams. As you build your multitenant platform, consider your requirements and their implications on the platform services you want to offer. In some cases, it can drive decisions around approaches and tooling that are fundamental to your platform.<a data-primary="services" data-secondary="multitenant platform services" data-startref="ix_serMPS" data-type="indexterm" id="idm45611975364712"/><a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-startref="ix_mltitenKuMPS" data-tertiary="multitenant platform services" data-type="indexterm" id="idm45611975363400"/><a data-primary="multitenancy" data-secondary="Kubernetes capabilities for" data-startref="ix_mltitenKu" data-type="indexterm" id="idm45611975361880"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611975852472">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Workload tenancy is a crucial concern you must consider when building a platform atop Kubernetes. On one hand, you can operate single-tenant clusters for each of your platform tenants. While this approach is viable, we discussed its downsides, including resource and management overhead. The alternative is multitenant clusters, where tenants share the cluster’s control plane, workload plane, and platform services.</p>&#13;
&#13;
<p>When hosting multiple tenants on the same cluster, you must ensure tenant isolation such that tenants cannot negatively affect each other. We discussed the Kubernetes Namespace as the foundation upon which we can build the isolation. We then discussed many of the isolation mechanisms available in Kubernetes that allow you to build a multitenant platform. These mechanisms are available in different layers, mainly the control plane, the workload plane, and the platform services.</p>&#13;
&#13;
<p class="pagebreak-before">The control plane isolation mechanisms include RBAC to control what tenants can do, resource quotas to divvy up the cluster resources, and admission webhooks to enforce policy. On the workload plane, you can segregate tenants by using Resource Requests and Limits to ensure fair-sharing of node resources, Network Policies to segment the Pod network, and Pod Security Policies to limit Pods capabilities. Finally, when it comes to platform services, you can leverage different technologies to implement multitenant offerings. We explored monitoring and centralized logging as example platform service that you can build to support multiple tenants.<a data-primary="multitenancy" data-startref="ix_mltiten" data-type="indexterm" id="idm45611975356760"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>