<html><head></head><body><section data-pdf-bookmark="Chapter 4. Container Storage" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter4">&#13;
<h1><span class="label">Chapter 4. </span>Container Storage</h1>&#13;
&#13;
&#13;
<p>While Kubernetes cut its teeth in the world of stateless workloads, running stateful services has become increasingly common.<a data-primary="storage" data-type="indexterm" id="ix_stor"/> Even complex stateful workloads such as databases and message queues are finding their way to Kubernetes clusters. To support these workloads, Kubernetes needs to provide storage capabilities beyond ephemeral options. Namely, systems that can provide increased resilience and availability in the face of various events such as an application crashing or a workload being rescheduled to a different host.</p>&#13;
&#13;
<p>In this chapter we are going to explore how our platform can offer storage services to applications.<a data-primary="container storage" data-see="storage" data-type="indexterm" id="idm45612000287176"/> We’ll start by covering key concerns of application persistence and storage system expectations before moving on to address the storage primitives available in Kubernetes. As we get into more advanced storage needs, we will look to the <a href="https://kubernetes-csi.github.io/docs">Container Storage Interface (CSI)</a>, which enables our integration with various storage providers. Lastly, we’ll explore using a CSI plug-in to provide self-service storage to our applications.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Storage is a vast subject in itself. Our intentions are to give you just enough detail to make informed decisions about the storage you may offer to workloads. If storage is not your background, we highly recommend going over these concepts with your infrastructure/storage team. Kubernetes does not negate the need for storage expertise in your organization!</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Storage Considerations" data-type="sect1"><div class="sect1" id="idm45612000283416">&#13;
<h1>Storage Considerations</h1>&#13;
&#13;
<p>Before getting into Kubernetes <a data-primary="storage" data-secondary="considerations" data-type="indexterm" id="ix_storcons"/>storage patterns and options, we should take a step back and analyze some key considerations around potential storage needs. At an infrastructure and application level, it is important to think through the following requirements.</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Access modes</p>&#13;
</li>&#13;
<li>&#13;
<p>Volume expansion</p>&#13;
</li>&#13;
<li>&#13;
<p>Dynamic provisioning</p>&#13;
</li>&#13;
<li>&#13;
<p>Backup and recovery</p>&#13;
</li>&#13;
<li>&#13;
<p>Block, file, and object storage</p>&#13;
</li>&#13;
<li>&#13;
<p>Ephemeral data</p>&#13;
</li>&#13;
<li>&#13;
<p>Choosing a provider</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Access Modes" data-type="sect2"><div class="sect2" id="idm45612000243560">&#13;
<h2>Access Modes</h2>&#13;
&#13;
<p>There are three access modes<a data-primary="access modes for storage" data-type="indexterm" id="idm45612000241960"/><a data-primary="storage" data-secondary="considerations" data-tertiary="access modes" data-type="indexterm" id="idm45612000241192"/> that can be supported for applications:</p>&#13;
<dl>&#13;
<dt>ReadWriteOnce (RWO)</dt>&#13;
<dd>&#13;
<p>A single Pod can read and write to the volume.</p>&#13;
</dd>&#13;
<dt>ReadOnlyMany (ROX)</dt>&#13;
<dd>&#13;
<p>Multiple Pods can read the volume.</p>&#13;
</dd>&#13;
<dt>ReadWriteMany (RWX)</dt>&#13;
<dd>&#13;
<p>Multiple Pods can read and write to the volume.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>For cloud native applications, RWO is by far the most common pattern. When leveraging common providers such <a href="https://aws.amazon.com/ebs">Amazon Elastic Block Storage (EBS)</a> or <a href="https://oreil.ly/wAtBg">Azure Disk Storage</a>, you are limited to RWO because the disk may only be attached to one node.<a data-primary="Azure Disk Storage" data-type="indexterm" id="idm45612000233416"/><a data-primary="AWS Elastic Block Storage (EBS)" data-type="indexterm" id="idm45612000232712"/> While this limitation may seem problematic, most cloud native applications work best with this kind of storage, where the volume is exclusively theirs and offers high-performance read/write.</p>&#13;
&#13;
<p>Many times, we find legacy applications that have a requirement for RWX.<a data-primary="network file system (NFS)" data-type="indexterm" id="idm45612000231032"/> Often, they are built to assume access to a <a href="https://oreil.ly/OrsBR">network file system (NFS)</a>. When services need to share state, there are often more elegant solutions than sharing data over NFS; for example, the use of message queues or databases. Additionally, should an application wish to share data, it’s typically best to expose this over an API, rather than grant access to its file system. This makes many use cases for RWX, at times, questionable. Unless NFS is the correct design choice, platform teams may be confronted with the tough choice of whether to offer RWX-compatible storage or request their developers re-architect applications. Should the call be made that supporting ROX or RWX is required, there are several providers that can be integrated with, such as <a href="https://aws.amazon.com/efs">Amazon Elastic File System (EFS)</a> and <a href="https://oreil.ly/u6HiQ">Azure File Share</a>.<a data-primary="Azure File Share" data-type="indexterm" id="idm45612000227464"/><a data-primary="Amazon Elastic File System (EFS)" data-type="indexterm" id="idm45612000226728"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Volume Expansion" data-type="sect2"><div class="sect2" id="idm45612000225992">&#13;
<h2>Volume Expansion</h2>&#13;
&#13;
<p>Over time, an application may begin to fill up its volume.<a data-primary="storage" data-secondary="considerations" data-tertiary="volume expansion" data-type="indexterm" id="idm45612000224696"/><a data-primary="volumes" data-secondary="expansion of" data-type="indexterm" id="idm45612000223448"/> This can pose a challenge since replacing the volume with a larger one would require migration of data. One solution to this is supporting volume expansion. From the perspective of a container orchestrator such as Kubernetes, this involves a few steps:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Request additional storage from the orchestrator (e.g., via a PersistentVolumeClaim).</p>&#13;
</li>&#13;
<li>&#13;
<p>Expand the size of the volume via the storage provider.</p>&#13;
</li>&#13;
<li>&#13;
<p>Expand the filesystem to make use of the larger volume.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Once complete, the Pod will have access to the additional space. This feature is contingent on our choice of storage backend and whether the integration in Kubernetes can facilitate the preceding steps. We will explore an example of volume expansion later in this chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Volume Provisioning" data-type="sect2"><div class="sect2" id="idm45612000218008">&#13;
<h2>Volume Provisioning</h2>&#13;
&#13;
<p>There are two provisioning models available to you: dynamic and static provisioning.<a data-primary="volumes" data-secondary="provisioning" data-type="indexterm" id="idm45612000216680"/><a data-primary="storage" data-secondary="considerations" data-tertiary="volume provisioning" data-type="indexterm" id="idm45612000215704"/><a data-primary="static provisioning" data-type="indexterm" id="idm45612000214488"/> Static provisioning assumes volumes are created on nodes for Kubernetes to consume.<a data-primary="dynamic provisioning" data-type="indexterm" id="idm45612000213688"/> Dynamic provisioning is when a driver runs within the cluster and can satisfy storage requests of workloads by talking to a storage provider. Out of these two models, dynamic provisioning, when possible, is preferred. Often, the choice between the two is a matter of whether your underlying storage system has a compatible driver for Kubernetes. We’ll dive into these drivers later in the chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Backup and Recovery" data-type="sect2"><div class="sect2" id="idm45612000212216">&#13;
<h2>Backup and Recovery</h2>&#13;
&#13;
<p>Backup is one of the most complex aspects of storage, especially when automated restores are a requirement.<a data-primary="storage" data-secondary="considerations" data-tertiary="backup and recovery" data-type="indexterm" id="idm45612000210792"/><a data-primary="backup and recovery, storage considerations" data-type="indexterm" id="idm45612000209544"/> In general terms, a backup is a copy of data that is stored for use in case of data loss. Typically, we balance backup strategies with the availability guarantees of our storage systems. For example, while backups are always important, they are less critical when our storage system has a replication guarantee where loss of hardware will <em>not</em> result in loss of data. Another consideration is that applications may require different procedures to facilitate backup and restores. The idea that we can take a backup of an entire cluster and restore it at any time is typically a naive outlook, or at minimum, one that requires mountains of engineering effort to &#13;
<span class="keep-together">achieve</span>.</p>&#13;
&#13;
<p>Deciding who should be responsible for backup and recovery of applications can be one of the most challenging debates within an organization. Arguably, offering restore features as a platform service can be a “nice to have.” However, it can tear at the seams when we get into application-specific complexity—for example, when an app cannot restart and needs actions to take place that are known only to developers.</p>&#13;
&#13;
<p>One of the most popular backup solutions for both Kubernetes state and application state is <a href="https://velero.io">Project Velero</a>. <a data-primary="Velero" data-type="indexterm" id="idm45612000204696"/>Velero can back up Kubernetes objects should you have a desire to migrate or restore them across clusters. Additionally, Velero supports the scheduling of volume snapshots. As we dive deeper into volume snapshotting in this chapter, we’ll learn that the ability to schedule and manage snapshots is <em>not</em> taken care of for us. More so, we are often given the snapshotting primitives but need to define an orchestration flow around them. Lastly, Velero supports backup and restore hooks. These enable us to run commands in the container before performing backup or recovery. For example, some applications may require stopping traffic or triggering a flush before a backup should be taken. This is made possible using hooks in Velero.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Block Devices and File and Object Storage" data-type="sect2"><div class="sect2" id="idm45612000202424">&#13;
<h2>Block Devices and File and Object Storage</h2>&#13;
&#13;
<p>The storage types our applications expect are key to selecting the appropriate underlying storage and Kubernetes integration.<a data-primary="storage" data-secondary="considerations" data-tertiary="block devices, file, and object storage" data-type="indexterm" id="idm45612000200984"/> The most common storage type used by applications is file storage.<a data-primary="file storage" data-type="indexterm" id="idm45612000199640"/> File storage is a block device with a filesystem on top. This enables applications to write to files in the way we are familiar with on any operating system.</p>&#13;
&#13;
<p>Underlying a filesystem is a block device.<a data-primary="block devices" data-type="indexterm" id="idm45612000198216"/> Rather than establishing a filesystem on top, we can offer the device such that applications may communicate directly with raw block. Filesystems inherently add overhead to writing data. In modern software development, it’s pretty rare to be concerned about filesystem overhead. However, if your use case warrants direct interaction with raw block devices, this is something certain storage systems can support.</p>&#13;
&#13;
<p>The final storage type is object storage.<a data-primary="object storage" data-type="indexterm" id="idm45612000196344"/> Object storage deviates from files in the sense that there is not the conventional hierarchy. Object storage enables developers to take unstructured data, give it a unique identifier, add some metadata around it, and store it.<a data-primary="cloud computing" data-secondary="object stores" data-type="indexterm" id="idm45612000195272"/> Cloud-provider object stores such as <a href="https://aws.amazon.com/s3">Amazon S3</a> have become popular locations for organizations to host images, binaries, and more.<a data-primary="Amazon S3" data-type="indexterm" id="idm45612000193576"/> This popularity has been accelerated by its fully featured web API and access control. Object stores are <em>most commonly</em> interacted with from the application itself, where the application uses a library to authenticate and interact with the provider. Since there is less standardization around interfaces for interaction with object stores, it is less &#13;
<span class="keep-together">common</span> to see them integrated as platform services that applications can interact with transparently.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ephemeral Data" data-type="sect2"><div class="sect2" id="idm45612000191048">&#13;
<h2>Ephemeral Data</h2>&#13;
&#13;
<p>While storage may imply a level of persistence that is beyond the life cycle of a Pod, there are valid use cases for supporting ephemeral data usage.<a data-primary="storage" data-secondary="considerations" data-tertiary="ephemeral data" data-type="indexterm" id="idm45612000189560"/><a data-primary="ephemeral data, storage of" data-type="indexterm" id="idm45612000188312"/> By default, containers that write to their own filesystem will utilize ephemeral storage. If the container were to restart, this storage would be lost. The <a href="https://oreil.ly/86zjA">emptyDir</a> volume type is available for ephemeral storage that is resilient to restarts. Not only is this resilient to container restarts, but it can be used to share files between containers in the same Pod.</p>&#13;
&#13;
<p>The biggest risk with ephemeral data is ensuring your Pods don’t consume too much of the host’s storage capacity. While numbers like 4Gi per Pod might not seem like much, consider a node can run hundreds, in some cases thousands, of Pods. Kubernetes supports the ability to limit the cumulative amount of ephemeral storage available to Pods in a Namespace. Configuration of these concerns are covered in <a data-type="xref" href="ch12.html#multi_tenancy_chapter">Chapter 12</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Choosing a Storage Provider" data-type="sect2"><div class="sect2" id="idm45612000184472">&#13;
<h2>Choosing a Storage Provider</h2>&#13;
&#13;
<p>There is no shortage of storage providers available to you.<a data-primary="storage" data-secondary="considerations" data-tertiary="choosing a storage provider" data-type="indexterm" id="idm45612000183176"/> Options span from storage solutions you might manage yourself such as Ceph to fully managed systems like Google Persistent Disk or Amazon Elastic Block Store. The variance in options is far beyond the scope of this book. However, we do recommend understanding the capabilities of storage systems along with which of those capabilities are easily integrated with Kubernetes. This will surface perspective on how well one solution may satisfy your application requirements relative to another. Additionally, in the case you may be managing your own storage system, consider using something you have operational experience with when possible. Introducing Kubernetes alongside a new storage system adds a lot of new operational complexity to your organization.<a data-primary="storage" data-secondary="considerations" data-startref="ix_storcons" data-type="indexterm" id="idm45612000180968"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Storage Primitives" data-type="sect1"><div class="sect1" id="idm45612000282792">&#13;
<h1>Kubernetes Storage Primitives</h1>&#13;
&#13;
<p>Out of the box, Kubernetes provides multiple primitives to support workload storage.<a data-primary="storage" data-secondary="Kubernetes storage primitives" data-type="indexterm" id="ix_storKSP"/> These primitives provide the building blocks we will utilize to offer sophisticated storage solutions. In this section, we are going to cover PersistentVolumes, PersistentVolumeClaims, and StorageClasses using an example of allocating fast pre-provisioned storage to containers.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Persistent Volumes and Claims" data-type="sect2"><div class="sect2" id="idm45612000176504">&#13;
<h2>Persistent Volumes and Claims</h2>&#13;
&#13;
<p>Volumes and claims live at the foundation of storage in Kubernetes.<a data-primary="storage" data-secondary="Kubernetes storage primitives" data-tertiary="persistent volumes and claims" data-type="indexterm" id="idm45612000174616"/><a data-primary="volumes" data-secondary="persistent volumes and claims" data-type="indexterm" id="idm45612000173336"/> These are exposed using the <a href="https://oreil.ly/7_OAz">PersistentVolume</a> and <a href="https://oreil.ly/PKtAr">PersistentVolumeClaim</a> APIs.<a data-primary="PersistentVolumeClaim API" data-type="indexterm" id="idm45612000170872"/><a data-primary="PersistentVolume API" data-type="indexterm" id="idm45612000170072"/> The PersistentVolume resource represents a storage volume known to Kubernetes. Let’s assume an administrator has prepared a node to offer 30Gi of fast, on-host, storage. Let’s also assume the administrator has provisioned this storage at <em>/mnt/fast-disk/pod-0</em>. To &#13;
<span class="keep-together">represent</span> this volume in Kubernetes, the administrator can then create a PersistentVolume object:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">PersistentVolume</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">pv0</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">capacity</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">storage</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">30Gi</code><code> </code><a class="co" href="#callout_container_storage_CO1-1" id="co_container_storage_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">volumeMode</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Filesystem</code><code> </code><a class="co" href="#callout_container_storage_CO1-2" id="co_container_storage_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="nt">accessModes</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">ReadWriteOnce</code><code> </code><a class="co" href="#callout_container_storage_CO1-3" id="co_container_storage_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="nt">storageClassName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">local-storage</code><code> </code><a class="co" href="#callout_container_storage_CO1-4" id="co_container_storage_CO1-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>  </code><code class="nt">local</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">path</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">/mnt/fast-disk/pod-0</code><code>&#13;
</code><code>  </code><code class="nt">nodeAffinity</code><code class="p">:</code><code> </code><a class="co" href="#callout_container_storage_CO1-5" id="co_container_storage_CO1-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>    </code><code class="nt">required</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">nodeSelectorTerms</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">matchExpressions</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="p-Indicator">-</code><code> </code><code class="nt">key</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">kubernetes.io/hostname</code><code>&#13;
</code><code>          </code><code class="nt">operator</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">In</code><code>&#13;
</code><code>          </code><code class="nt">values</code><code class="p">:</code><code>&#13;
</code><code>          </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">test-w</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO1-1" id="callout_container_storage_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The amount of storage available in this volume. Used to determine whether a claim can bind to this volume.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO1-2" id="callout_container_storage_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Specifies whether the volume is a <a href="https://oreil.ly/mrHwE">block device</a> or &#13;
<span class="keep-together">filesystem</span>.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO1-3" id="callout_container_storage_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Specifies the access mode of the volume. Includes <code>ReadWriteOnce</code>, <code>ReadMany</code>, and <code>ReadWriteMany</code>.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO1-4" id="callout_container_storage_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Associates this volume with a storage class. Used to pair an eventual claim to this volume.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO1-5" id="callout_container_storage_CO1-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Identifies which node this volume should be associated with.</p></dd>&#13;
</dl>&#13;
&#13;
<p class="pagebreak-before">As you can see, the PersistentVolume contains details around the implementation of the volume. To provide one more layer of abstraction, a PersistentVolumeClaim is introduced, which binds to an appropriate volume based on its request. Most commonly, this will be defined by the application team, added to their Namespace, and referenced from their Pod:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">PersistentVolumeClaim</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">pvc0</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">storageClassName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">local-storage</code><code> </code><a class="co" href="#callout_container_storage_CO2-1" id="co_container_storage_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">accessModes</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">ReadWriteOnce</code><code>&#13;
</code><code>  </code><code class="nt">resources</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">requests</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">storage</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">30Gi</code><code> </code><a class="co" href="#callout_container_storage_CO2-2" id="co_container_storage_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code class="nn">---</code><code>&#13;
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Pod</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">task-pv-pod</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">volumes</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">fast-disk</code><code>&#13;
</code><code>      </code><code class="nt">persistentVolumeClaim</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">claimName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">pvc0</code><code> </code><a class="co" href="#callout_container_storage_CO2-3" id="co_container_storage_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="nt">containers</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ml-processer</code><code>&#13;
</code><code>      </code><code class="nt">image</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ml-processer-image</code><code>&#13;
</code><code>      </code><code class="nt">volumeMounts</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="p-Indicator">-</code><code> </code><code class="nt">mountPath</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">/var/lib/db</code><code class="s">"</code><code>&#13;
</code><code>          </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">fast-disk</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO2-1" id="callout_container_storage_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Checks for a volume that is of the class <code>local-storage</code> with the access mode <code>ReadWriteOnce</code>.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO2-2" id="callout_container_storage_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Binds to a volume with &gt;= <code>30Gi</code> of storage.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO2-3" id="callout_container_storage_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Declares this Pod a consumer of the PersistentVolumeClaim.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Based on the PersistentVolume’s <code>nodeAffinity</code> settings, the Pod will be automatically scheduled on the host where this volume is available. There is no additional affinity configuration required from the developer.</p>&#13;
&#13;
<p>This process has demonstrated a very manual flow for how administrators could make this storage available to developers. We refer to this as static provisioning.<a data-primary="static provisioning" data-type="indexterm" id="idm45611999872488"/> With proper automation this could be a viable way to expose fast disk on hosts to Pods. For example, the <a href="https://oreil.ly/YiQ0G">Local Persistence Volume Static Provisioner</a> can be deployed to the &#13;
<span class="keep-together">cluster</span> to detect preallocated storage and expose it, automatically, as PersistentVolumes. It also provides some life cycle management capabilities such as deleting data upon destruction of the PersistentVolumeClaim.<a data-primary="Local Persistence Volume Static Provisioner" data-type="indexterm" id="idm45611999837128"/></p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>There are multiple ways to achieve local storage that can lead you into a bad practice. For example, it can seem compelling to allow developers to use <a href="https://oreil.ly/PAU8Y">hostPath</a> rather than needing to preprovision a local storage. <code>hostPath</code> enables you to specify a path on the host to bind to rather than having to use a PersistentVolume and PersistentVolumeClaim.<a data-primary="hostPath" data-type="indexterm" id="idm45611999833848"/> This can be a huge security risk as it enables developers to bind to directories on the host, which can have a negative impact on the host and other Pods.<a data-primary="EmptyDir" data-type="indexterm" id="idm45611999832840"/> If you desire to provide developers ephemeral storage that can withstand a Pod restart but not the Pod being deleted or moved to a different node, you can use <a href="https://oreil.ly/mPwBg">EmptyDir</a>. This will allocate storage in the filesystem managed by Kube and be transparent to the Pod.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Storage Classes" data-type="sect2"><div class="sect2" id="idm45612000175912">&#13;
<h2>Storage Classes</h2>&#13;
&#13;
<p>In many environments, expecting nodes to be prepared ahead of time with disks and volumes is unrealistic.<a data-primary="storage" data-secondary="Kubernetes storage primitives" data-tertiary="storage classes" data-type="indexterm" id="idm45611999829432"/> These cases often warrant dynamic provisioning, where volumes can be made available based on the needs of our claims. To facilitate this model, we can make classes of storage available to our developers. These are defined using the <a href="https://oreil.ly/MoG_T">StorageClass</a> API. <a data-primary="StorageClass API" data-type="indexterm" id="idm45611999827176"/><a data-primary="AWS Elastic Block Storage (EBS)" data-secondary="offering EBS volumes to Pods dynamically" data-type="indexterm" id="idm45611999826440"/>Assuming your cluster runs in AWS and you want to offer EBS volumes to Pods dynamically, the following StorageClass can be added:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">storage.k8s.io/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">StorageClass</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ebs-standard</code><code> </code><a class="co" href="#callout_container_storage_CO3-1" id="co_container_storage_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">annotations</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">storageclass.kubernetes.io/is-default-class</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">true</code><code> </code><a class="co" href="#callout_container_storage_CO3-2" id="co_container_storage_CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code class="nt">provisioner</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">kubernetes.io/aws-ebs</code><code> </code><a class="co" href="#callout_container_storage_CO3-3" id="co_container_storage_CO3-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code class="nt">parameters</code><code class="p">:</code><code> </code><a class="co" href="#callout_container_storage_CO3-4" id="co_container_storage_CO3-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>  </code><code class="nt">type</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">io2</code><code>&#13;
</code><code>  </code><code class="nt">iopsPerGB</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">17</code><code class="s">"</code><code>&#13;
</code><code>  </code><code class="nt">fsType</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ext4</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO3-1" id="callout_container_storage_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The name of the StorageClass that can be referenced from claims.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO3-2" id="callout_container_storage_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Sets this StorageClass as the default. If a claim does not specify a class, this will be used.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO3-3" id="callout_container_storage_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Uses the <code>aws-ebs</code> provisioner to create the volumes based on claims.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO3-4" id="callout_container_storage_CO3-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Provider-specific configuration for how to provision volumes.</p></dd>&#13;
</dl>&#13;
&#13;
<p>You can offer a variety of storage options to developers by making multiple StorageClasses available. This includes supporting more than one provider in a single cluster—for example, running Ceph alongside VMware vSAN. Alternatively, you may offer different tiers of storage via the same provider. An example would be offering cheaper storage alongside more expensive options. Unfortunately, Kubernetes lacks granular controls to limit what classes developers can request. Control can be implemented as validating admission control, which is covered in <a data-type="xref" href="ch08.html#chapter8">Chapter 8</a>.</p>&#13;
&#13;
<p>Kubernetes offers a wide variety of providers including AWS EBS, Glusterfs, GCE PD, Ceph RBD, and many more.<a data-primary="storage providers" data-type="indexterm" id="idm45611999730008"/> Historically, these providers were implemented in-tree. This means storage providers needed to implement their logic in the core Kubernetes project. This code would then get shipped in the relevant Kubernetes control plane components.</p>&#13;
&#13;
<p>There were several downsides to this model. For one, the storage provider could not be managed out of band. All changes to the provider needed to be tied to a Kubernetes release. Also, every Kubernetes deployment shipped with unnecessary code. For example, clusters running AWS still had the provider code for interacting with GCE PDs. It quickly became apparent there was high value in externalizing these provider integrations and deprecating the in-tree functionality. <a href="https://oreil.ly/YnnCq">FlexVolume drivers</a> were an out-of-tree implementation specification that initially aimed to solve this problem. However, FlexVolumes have been put into maintenance mode in favor of our next topic, the Container Storage Interface (CSI).<a data-primary="storage" data-secondary="Kubernetes storage primitives" data-startref="ix_storKSP" data-type="indexterm" id="idm45611999727112"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Container Storage Interface (CSI)" data-type="sect1"><div class="sect1" id="idm45611999725592">&#13;
<h1>The Container Storage Interface (CSI)</h1>&#13;
&#13;
<p>The Container Storage Interface is the answer to how we provide block and file storage to our workloads.<a data-primary="storage" data-secondary="CSI (Container Storage Interface)" data-type="indexterm" id="ix_StorCSI"/><a data-primary="Container Storage Interface (CSI)" data-type="indexterm" id="ix_CSI"/> The implementations of CSI are referred to as drivers, which have the operational knowledge for talking to storage providers. These providers span from cloud systems such as <a href="https://cloud.google.com/persistent-disk">Google Persistent Disks</a> to storage systems (such as <a href="https://ceph.io">Ceph</a>) deployed and managed by you. The drivers are implemented by storage providers in projects that live out-of-tree. They can be entirely managed out of band from the cluster they are deployed within.</p>&#13;
&#13;
<p>At a high level, CSI implementations feature a controller plug-in and a node plug-in. CSI driver developers have a lot of flexibility in how they implement these components. Typically, implementations bundle the controller and node plug-ins in the same binary and enable either mode via an environment variable such as <code>X_CSI_MODE</code>. The only expectations are that the driver registers with the kubelet and the endpoints in the CSI specification are implemented.</p>&#13;
&#13;
<p>The controller service is responsible for managing the creation and deletion of volumes in the storage provider. This functionality extends into (optional) features such as taking volume snapshots and expanding volumes. The node service is responsible for preparing volumes to be consumed by Pods on the node. Often this means setting up the mounts and reporting information about volumes on the node. Both the node and controller service also implement identity services that report plug-in info, capabilities, and whether the plug-in is healthy. With this in mind, <a data-type="xref" href="#cluster_running_a_csi_plugin_the_driver">Figure 4-1</a> represents a cluster architecture with these components deployed.</p>&#13;
&#13;
<figure><div class="figure" id="cluster_running_a_csi_plugin_the_driver">&#13;
<img alt="prku 0401" src="assets/prku_0401.png"/>&#13;
<h6><span class="label">Figure 4-1. </span>Cluster running a CSI plug-in. The driver runs in a node and controller mode. The controller is typically run as a Deployment. The node service is deployed as a DaemonSet, which places a Pod on each host.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s take a deeper look at these two components, the controller and the node.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CSI Controller" data-type="sect2"><div class="sect2" id="idm45611999714024">&#13;
<h2>CSI Controller</h2>&#13;
&#13;
<p>The CSI Controller service provides APIs for managing volumes in a persistent storage system.<a data-primary="Container Storage Interface (CSI)" data-secondary="CSI Controller service" data-type="indexterm" id="idm45611999712584"/><a data-primary="CSI Controller" data-type="indexterm" id="idm45611999711592"/> The Kubernetes control plane <em>does not</em> interact with the CSI Controller service directly. Instead, controllers maintained by the Kubernetes storage community react to Kubernetes events and translate them into CSI instructions, such as CreateVolumeRequest when a new PersistentVolumeClaim is created.<a data-primary="Unix socket" data-secondary="CSI Controller service exposing APIs over" data-type="indexterm" id="idm45611999710136"/> Because the CSI Controller service exposes its APIs over UNIX sockets, the controllers are usually deployed as sidecars alongside the CSI Controller service. There are multiple external controllers, each with different behavior:</p>&#13;
<dl>&#13;
<dt>external-provisioner</dt>&#13;
<dd>&#13;
<p>When PersistentVolumeClaims are created, this requests a volume be created from the CSI driver. Once the volume is created in the storage provider, this provisioner creates a PersistentVolume object in Kubernetes.</p>&#13;
</dd>&#13;
<dt>external-attacher</dt>&#13;
<dd>&#13;
<p>Watches the VolumeAttachment objects, which declare that a volume should be attached or detached from a node. Sends the attach or detach request to the CSI driver.</p>&#13;
</dd>&#13;
<dt>external-resizer</dt>&#13;
<dd>&#13;
<p>Detects storage-size changes in PersistentVolumeClaims. Sends requests for expansion to the CSI driver.</p>&#13;
</dd>&#13;
<dt>external-snapshotter</dt>&#13;
<dd>&#13;
<p>When VolumeSnapshotContent objects are created, snapshot requests are sent to the driver.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>When implementing CSI plug-ins, developers are not required to use the aforementioned controllers. However, their use is encouraged to prevent duplication of logic in every CSI plug-in.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CSI Node" data-type="sect2"><div class="sect2" id="idm45611999700840">&#13;
<h2>CSI Node</h2>&#13;
&#13;
<p>The Node plug-in typically<a data-primary="Node plug-in (CSI)" data-type="indexterm" id="idm45611999699464"/> runs the same driver code as the controller plug-in.<a data-primary="Container Storage Interface (CSI)" data-secondary="Node plug-in" data-type="indexterm" id="idm45611999698632"/> However, running in the “node mode” means it is focused on tasks such as mounting attached volumes, establishing their filesystem, and mounting volumes to Pods. Requests for these behaviors is done via the kubelet. Along with the driver, the following sidecars are often included in the Pod:</p>&#13;
<dl>&#13;
<dt>node-driver-registrar</dt>&#13;
<dd>&#13;
<p>Sends a <a href="https://oreil.ly/kmkJh">registration request</a> to the kubelet to make it aware of the CSI driver.</p>&#13;
</dd>&#13;
<dt>liveness-probe</dt>&#13;
<dd>&#13;
<p>Reports the health of the CSI driver.<a data-primary="Container Storage Interface (CSI)" data-startref="ix_CSI" data-type="indexterm" id="idm45611999693672"/><a data-primary="storage" data-secondary="CSI (Container Storage Interface)" data-startref="ix_StorCSI" data-type="indexterm" id="idm45611999692632"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Implementing Storage as a Service" data-type="sect1"><div class="sect1" id="idm45611999690888">&#13;
<h1>Implementing Storage as a Service</h1>&#13;
&#13;
<p>We have now covered key considerations for application storage, storage primitives available in <a data-primary="storage" data-secondary="implementing storage as a service" data-type="indexterm" id="ix_storSaaS"/>Kubernetes, and driver integration using the CSI. Now it’s time to bring these ideas together and look at an implementation that offers developers storage as a service. We want to provide a declarative way to request storage and make it available to workloads. We also prefer to do this dynamically, not requiring an administrator to preprovision and attach volumes. Rather, we’d like to achieve this on demand based on the needs of workloads.</p>&#13;
&#13;
<p>In order to get started with this implementation, we’ll use Amazon Web Services (AWS).<a data-primary="AWS Elastic Block Storage (EBS)" data-type="indexterm" id="idm45611999687032"/> This example integrates with AWS’s <a href="https://oreil.ly/I4VVw">elastic block</a> storage system. If your choice in provider differs, the majority of this content will still be relevant! We are simply using this provider as a concrete example of how all the pieces fit together.</p>&#13;
&#13;
<p>Next we are going to dive into installation of the integration/driver, exposing storage options to developers, consuming the storage with workloads, resizing volumes, and taking volume snapshots.<a data-primary="storage" data-secondary="implementing storage as a service" data-tertiary="installation of the integration/driver" data-type="indexterm" id="idm45611999684696"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Installation" data-type="sect2"><div class="sect2" id="idm45611999683240">&#13;
<h2>Installation</h2>&#13;
&#13;
<p>Installation is a fairly straightforward<a data-primary="access control" data-secondary="configuring access to storage provider" data-type="indexterm" id="idm45611999681544"/> process consisting of two key steps:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Configure access to the provider.</p>&#13;
</li>&#13;
<li>&#13;
<p>Deploy the driver components to the cluster.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>The provider, in this case AWS, will require the driver to identify itself, ensuring it has appropriate access. In this case, we have three options available to us. One is to update the <a href="https://oreil.ly/fGWYd">instance profile</a> of the Kubernetes nodes. This will prevent us from worrying about credentials at the Kubernetes level but will provide universal privileges to workloads that can reach the AWS API.<a data-primary="identity" data-secondary="identity service providing IAM permissions" data-type="indexterm" id="idm45611999676504"/> The second and likely most secure option is to introduce an identity service that can provide IAM permissions to specific workloads. A project that is an example of this is <a href="https://github.com/uswitch/kiam">kiam</a>. This approach is covered in <a data-type="xref" href="ch10.html#chapter10">Chapter 10</a>. Lastly, you can add credentials in a secret that gets mounted into the CSI driver.<a data-primary="secrets" data-secondary="adding credentials in secret mounted into CSI driver" data-type="indexterm" id="idm45611999673736"/> In this model, the secret would look as follows:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Secret</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">aws-secret</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">kube-system</code>&#13;
<code class="nt">stringData</code><code class="p">:</code>&#13;
  <code class="nt">key_id</code><code class="p">:</code> <code class="s">"AKIAWJQHICPELCJVKYNU"</code>&#13;
  <code class="nt">access_key</code><code class="p">:</code> <code class="s">"jqWi1ut4KyrAHADIOrhH2Pd/vXpgqA9OZ3bCZ"</code></pre>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>This account will have access to manipulating an underlying storage system. Access to this secret should be carefully managed. See <a data-type="xref" href="ch07.html#chapter7">Chapter 7</a> for more information.</p>&#13;
</div>&#13;
&#13;
<p>With this configuration in place, the CSI components may be installed. First, the controller is installed as a Deployment. When running multiple replicas, it will use leader-election to determine which instance should be active. Then, the node plug-in is installed, which comes in the form of a DaemonSet running a Pod on every node. Once initialized, the instances of the node plug-in will register with their kubelets. The kubelet will then report the CSI-enabled node by creating a CSINode object for every Kubernetes node<a data-primary="nodes" data-secondary="CSINode objects" data-type="indexterm" id="idm45611999653960"/>.<a data-primary="CSINode objects" data-type="indexterm" id="idm45611999650168"/> The output of a three-node cluster is as follows:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>kubectl get csinode&#13;
&#13;
NAME                                       DRIVERS   AGE&#13;
ip-10-0-0-205.us-west-2.compute.internal   <code class="m">1</code>         97m&#13;
ip-10-0-0-224.us-west-2.compute.internal   <code class="m">1</code>         79m&#13;
ip-10-0-0-236.us-west-2.compute.internal   <code class="m">1</code>         98m</pre>&#13;
&#13;
<p>As we can see, there are three nodes listed with one driver registered on each node. Examining the YAML of one CSINode exposes the following:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">storage.k8s.io/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">CSINode</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ip-10-0-0-205.us-west-2.compute.internal</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">drivers</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">allocatable</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">count</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">25</code><code> </code><a class="co" href="#callout_container_storage_CO4-1" id="co_container_storage_CO4-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>      </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ebs.csi.aws.com</code><code>&#13;
</code><code>      </code><code class="nt">nodeID</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">i-0284ac0df4da1d584</code><code>&#13;
</code><code>      </code><code class="nt">topologyKeys</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">topology.ebs.csi.aws.com/zone</code><code> </code><a class="co" href="#callout_container_storage_CO4-2" id="co_container_storage_CO4-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO4-1" id="callout_container_storage_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The maximum number of volumes allowed on this node.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO4-2" id="callout_container_storage_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>When a node is picked for a workload, this value will be passed in the CreateVolumeRequest so that the driver knows <em>where</em> to create the volume. This is important for storage systems where nodes in the cluster won’t have access to the same storage. For example, in AWS, when a Pod is scheduled in an availability zone, the Volume must be created in the same zone.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Additionally, the driver is officially registered with the cluster. The details can be <a data-primary="CSIDriver object" data-type="indexterm" id="idm45611999540040"/>found in the CSIDriver object:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">storage.k8s.io/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">CSIDriver</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">aws-ebs-csi-driver</code><code> </code><a class="co" href="#callout_container_storage_CO5-1" id="co_container_storage_CO5-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">labels</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">app.kubernetes.io/name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">aws-ebs-csi-driver</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">attachRequired</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">true</code><code> </code><a class="co" href="#callout_container_storage_CO5-2" id="co_container_storage_CO5-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="nt">podInfoOnMount</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">false</code><code> </code><a class="co" href="#callout_container_storage_CO5-3" id="co_container_storage_CO5-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="nt">volumeLifecycleModes</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">Persistent</code><code> </code><a class="co" href="#callout_container_storage_CO5-4" id="co_container_storage_CO5-4"><img alt="4" src="assets/4.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO5-1" id="callout_container_storage_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The name of the provider representing this driver. This name will be bound to class(es) of storage we offer to platform users.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO5-2" id="callout_container_storage_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Specifies that an attach operation must be completed before volumes are &#13;
<span class="keep-together">mounted</span>.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO5-3" id="callout_container_storage_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Does not need to pass Pod metadata in as context when setting up a mount.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO5-4" id="callout_container_storage_CO5-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The default model for provisioning persistent volumes. <a href="https://oreil.ly/Z_pDY">Inline support</a> can be enabled by setting this option to <code>Ephemeral</code>. In the ephemeral mode, the storage is expected to last only as long as the Pod.</p></dd>&#13;
</dl>&#13;
&#13;
<p>The settings and objects we have explored so far are artifacts of our bootstrapping process. The CSIDriver object makes for easier discovery of driver details and was included in the driver’s deployment bundle. The CSINode objects are managed by the kubelet. A generic registrar sidecar is included in the node plug-in Pod and gets details from the CSI driver and registers the driver with the kubelet. The kubelet then reports up the quantity of CSI drivers available on each host. <a data-type="xref" href="#csidriver_object_is_deployed_and_part_of_the_bundle_while_the_node_plugin_registers_with_the_kubelet">Figure 4-2</a> demonstrates this bootstrapping process.</p>&#13;
&#13;
<figure><div class="figure" id="csidriver_object_is_deployed_and_part_of_the_bundle_while_the_node_plugin_registers_with_the_kubelet">&#13;
<img alt="prku 0402" src="assets/prku_0402.png"/>&#13;
<h6><span class="label">Figure 4-2. </span>CSIDriver object is deployed and part of the bundle while the node plug-in registers with the kubelet. This in turn creates/manages the CSINode objects.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Exposing Storage Options" data-type="sect2"><div class="sect2" id="idm45611999682648">&#13;
<h2>Exposing Storage Options</h2>&#13;
&#13;
<p>In order to provide storage options to developers, we need to create StorageClasses.<a data-primary="storage" data-secondary="implementing storage as a service" data-tertiary="exposing storage options" data-type="indexterm" id="idm45611999416744"/> For this scenario we’ll assume there are two types of storage we’d like to expose.<a data-primary="disk storage" data-type="indexterm" id="idm45611999415336"/>The first option is to expose cheap disk that can be used for workload persistence needs. Many times, applications don’t need an SSD as they are just persisting some files that do not require quick read/write. As such, the cheap disk (HDD) will be the default option. Then we’d like to offer faster SSD with a custom <a href="https://oreil.ly/qXMcQ">IOPS</a> per gigabyte configured. <a data-type="xref" href="#table_4-1">Table 4-1</a> shows our offerings; prices reflect AWS costs at the time of this writing.</p>&#13;
<table id="table_4-1">&#13;
<caption><span class="label">Table 4-1. </span>Storage offerings</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Offering name</th>&#13;
<th>Storage type</th>&#13;
<th>Max throughput per volume</th>&#13;
<th>AWS cost</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>default-block</p></td>&#13;
<td><p>HDD (optimized)</p></td>&#13;
<td><p>40–90 MB/s</p></td>&#13;
<td><p>$0.045 per GB per month</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>performance-block</p></td>&#13;
<td><p>SSD (io1)</p></td>&#13;
<td><p>~1000 MB/s</p></td>&#13;
<td><p>$0.125 per GB per month + $0.065 per provisioned IOPS per month</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>In order <a data-primary="StorageClass API" data-secondary="creating storage classes for storage options" data-type="indexterm" id="idm45611999402536"/>to create these offerings, we’ll create a storage class for each. Inside each storage class is a <code>parameters</code> field. This is where we can configure settings that satisfy the features in <a data-type="xref" href="#table_4-1">Table 4-1</a>.</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">StorageClass</code><code>&#13;
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">storage.k8s.io/v1</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">default-block</code><code> </code><a class="co" href="#callout_container_storage_CO6-1" id="co_container_storage_CO6-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">annotations</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">storageclass.kubernetes.io/is-default-class</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">true</code><code class="s">"</code><code> </code><a class="co" href="#callout_container_storage_CO6-2" id="co_container_storage_CO6-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code class="nt">provisioner</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ebs.csi.aws.com</code><code> </code><a class="co" href="#callout_container_storage_CO6-3" id="co_container_storage_CO6-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code class="nt">allowVolumeExpansion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">true</code><code> </code><a class="co" href="#callout_container_storage_CO6-4" id="co_container_storage_CO6-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code class="nt">volumeBindingMode</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">WaitForFirstConsumer</code><code> </code><a class="co" href="#callout_container_storage_CO6-5" id="co_container_storage_CO6-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code class="nt">parameters</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">type</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">st1</code><code> </code><a class="co" href="#callout_container_storage_CO6-6" id="co_container_storage_CO6-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code class="nn">---</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">StorageClass</code><code> </code><a class="co" href="#callout_container_storage_CO6-7" id="co_container_storage_CO6-7"><img alt="7" src="assets/7.png"/></a><code>&#13;
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">storage.k8s.io/v1</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">performance-block</code><code>&#13;
</code><code class="nt">provisioner</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ebs.csi.aws.com</code><code>&#13;
</code><code class="nt">parameters</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">type</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">io1</code><code>&#13;
</code><code>  </code><code class="nt">iopsPerGB</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">20</code><code class="s">"</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO6-1" id="callout_container_storage_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This is the name of the storage offering we are providing to platform users. It will be referenced from PeristentVolumeClaims.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO6-2" id="callout_container_storage_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This sets the offering as the default. If a PersistentVolumeClaim is created <em>without</em> a StorageClass specified, <code>default-block</code> will be used.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO6-3" id="callout_container_storage_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Mapping to which CSI driver should be executed.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO6-4" id="callout_container_storage_CO6-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Allow expansion of the volume size via changes to a PersistentVolumeClaim.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO6-5" id="callout_container_storage_CO6-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Do not provision the volume until a Pod consumes the PersistentVolumeClaim. This will ensure the volume is created in the appropriate availability zone of the scheduled Pod. It also prevents orphaned PVCs from creating volumes in AWS, which you will be billed for.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO6-6" id="callout_container_storage_CO6-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>Specifies what type of storage the driver should acquire to satisfy claims.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO6-7" id="callout_container_storage_CO6-7"><img alt="7" src="assets/7.png"/></a></dt>&#13;
<dd><p>Second class, tuned to high-performance SSD.<a data-primary="performance" data-secondary="high-performance SSD" data-type="indexterm" id="idm45611999250600"/></p></dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Consuming Storage" data-type="sect2"><div class="sect2" id="idm45611999285960">&#13;
<h2>Consuming Storage</h2>&#13;
&#13;
<p>With the preceding pieces in place, we are now ready for users to consume these different classes of storage.<a data-primary="storage" data-secondary="implementing storage as a service" data-tertiary="consuming storage" data-type="indexterm" id="idm45611999247928"/> We will start by looking at the developer experience of requesting storage. Then we’ll walk through the internals of how it is satisfied. To start off, let’s see what a developer gets when listing available StorageClasses:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>kubectl get storageclasses.storage.k8s.io&#13;
&#13;
NAME                      PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE&#13;
default-block <code class="o">(</code>default<code class="o">)</code>   ebs.csi.aws.com   Delete          Immediate&#13;
performance-block         ebs.csi.aws.com   Delete          WaitForFirstConsumer&#13;
&#13;
ALLOWVOLUMEEXPANSION&#13;
<code class="nb">true</code>&#13;
<code class="nb">true</code></pre>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>By enabling developers to create PVCs, we will be allowing them to reference <em>any</em> StorageClass.<a data-primary="PersistentVolumeClaim API" data-secondary="allowing developers to create PVCs" data-type="indexterm" id="idm45611999239752"/> If this is problematic, you may wish to consider implementing Validating Admission control to assess whether requests are appropriate. This topic is covered in &#13;
<span class="keep-together"><a data-type="xref" href="ch08.html#chapter8">Chapter 8</a></span>.</p>&#13;
</div>&#13;
&#13;
<p>Let’s assume the developer wants to make a cheaper HDD and more performant SSD available for an application. In this case, two PersistentVolumeClaims are created. We’ll refer to these as <code>pvc0</code> and <code>pvc1</code>, respectively:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">PersistentVolumeClaim</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">pvc0</code><code> </code><a class="co" href="#callout_container_storage_CO7-1" id="co_container_storage_CO7-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">resources</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">requests</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">storage</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">11Gi</code><code>&#13;
</code><code class="nn">---</code><code>&#13;
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">PersistentVolumeClaim</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">pvc1</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">resources</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">requests</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">storage</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">14Gi</code><code>&#13;
</code><code>  </code><code class="nt">storageClassName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">performance-block</code><code> </code><a class="co" href="#callout_container_storage_CO7-2" id="co_container_storage_CO7-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO7-1" id="callout_container_storage_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This will use the default storage class (<code>default-block</code>) and assume other defaults such as RWO and filesystem storage type.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO7-2" id="callout_container_storage_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Ensure <code>performance-block</code> is requested to the driver rather than <code>default-block</code>.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Based on the StorageClass settings, these two will exhibit different provisioning behaviors. The performant storage (from <code>pvc1</code>) is created as an unattached volume in AWS. This volume can be attached quickly and is ready to use. The default storage (from <code>pv0</code>) will sit in a <code>Pending</code> state where the cluster waits until a Pod consumes the PVC to provision storage in AWS. While this will require more work to provision when a Pod finally consumes the claim, you will not be billed for the unused storage! The relationship between the claim in Kubernetes and volume in AWS can be seen in <a data-type="xref" href="#pv1_is_provisioned_as_a_volume_in_aws">Figure 4-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="pv1_is_provisioned_as_a_volume_in_aws">&#13;
<img alt="prku 0403" src="assets/prku_0403.png"/>&#13;
<h6><span class="label">Figure 4-3. </span><code>pv1</code> is provisioned as a volume in AWS, and the CSIVolumeName is propagated for ease of correlation. <code>pv0</code> will not have a respective volume created until a Pod references it.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now let’s assume the developer creates two Pods. One Pod references <code>pv0</code> while the other references <code>pv1</code>. Once each Pod is scheduled on a Node, the volume will be attached to that node for consumption. For <code>pv0</code>, before this can occur the volume will also be created in AWS. With the Pods scheduled and volumes attached, a filesystem is established and the storage is mounted into the container. Because these are persistent volumes, we have now introduced a model where even if the Pod is rescheduled to another node, the volume can come with it. The end-to-end flow for how we’ve satisfied the self-service storage request is shown in <a data-type="xref" href="#end_to_end_flow_of_the_driver_and_kubernetes">Figure 4-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="end_to_end_flow_of_the_driver_and_kubernetes">&#13;
<img alt="prku 0404" src="assets/prku_0404.png"/>&#13;
<h6><span class="label">Figure 4-4. </span>End-to-end flow of the driver and Kubernetes working together to satisfy the storage request.</h6>&#13;
</div></figure>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Events are particularly helpful in debugging storage interaction with CSI.<a data-primary="events" data-secondary="using to debug storage interaction with CSI" data-type="indexterm" id="idm45611999184232"/> Because provisioning, attaching, and mounting are all happening in order to satisfy a PVC, you should view events on these objects as different components report what they have done. <code>kubectl describe -n $NAMESPACE pvc $PVC_NAME</code> is an easy way to view these events.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resizing" data-type="sect2"><div class="sect2" id="idm45611999249032">&#13;
<h2>Resizing</h2>&#13;
&#13;
<p>Resizing is a supported feature in the <code>aws-ebs-csi-driver</code>. <a data-primary="storage" data-secondary="implementing storage as a service" data-tertiary="resizing" data-type="indexterm" id="idm45611999104792"/><a data-primary="resizing PersistentClaimVolume objects" data-type="indexterm" id="idm45611999103544"/><a data-primary="PersistentClaimVolume objects, resizing" data-type="indexterm" id="idm45611999102904"/>In most CSI implementations, the <code>external-resizer</code> controller is used to detect changes in PersistentVolumeClaim objects. When a size change is detected, it is forwarded to the driver, which will expand the volume.<a data-primary="volumes" data-secondary="expansion of" data-type="indexterm" id="idm45611999101528"/> In this case, the driver running in the controller plug-in will facilitate expansion with the AWS EBS API.</p>&#13;
&#13;
<p>Once the volume is expanded in EBS, the new space is <em>not</em> immediately usable to the container.<a data-primary="file storage" data-secondary="expanding the filesystem" data-type="indexterm" id="idm45611999099624"/> This is because the filesystem still occupies only the original space. In order for the filesystem to expand, we’ll need to wait for the node plug-in’s driver instance to expand the filesystem. This can all be done <em>without</em> terminating the Pod. The filesystem expansion can be seen in the following logs from the node plug-in’s CSI driver:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">mount_linux.go: Attempting to determine if disk "/dev/nvme1n1" is formatted&#13;
using blkid with args: ([-p -s TYPE -s PTTYPE -o export /dev/nvme1n1])&#13;
&#13;
mount_linux.go: Output: "DEVNAME=/dev/nvme1n1\nTYPE=ext4\n", err: &lt;nil&gt;&#13;
&#13;
resizefs_linux.go: ResizeFS.Resize - Expanding mounted volume /dev/nvme1n1&#13;
&#13;
resizefs_linux.go: Device /dev/nvme1n1 resized successfully</pre>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Kubernetes does not support downsizing a PVC’s size field. Unless the CSI-driver provides a workaround for this, you may not be able to downsize without re-creating a volume. Keep this in mind when growing volumes.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Snapshots" data-type="sect2"><div class="sect2" id="idm45611999079624">&#13;
<h2>Snapshots</h2>&#13;
&#13;
<p>To facilitate periodic backups of volume data used by containers, snapshot functionality is available.<a data-primary="storage" data-secondary="implementing storage as a service" data-tertiary="snapshots" data-type="indexterm" id="idm45611999077784"/><a data-primary="snapshots of storage volumes" data-type="indexterm" id="idm45611999051880"/> The functionality is often broken into two controllers, which are responsible for two different CRDs.<a data-primary="custom resource definitions (CRDs)" data-secondary="for snapshots" data-secondary-sortas="snapshots" data-type="indexterm" id="idm45611999051000"/> The CRDs include VolumeSnapshot and VolumeContentSnapshot.<a data-primary="VolumeContentSnapshot" data-type="indexterm" id="idm45611999049688"/><a data-primary="VolumeSnapshot" data-type="indexterm" id="idm45611999049016"/> At a high-level, the VolumeSnapshot is responsible for the life cycle of volumes. Based on these objects, VolumeContentSnapshots are managed by the external-snapshotter controller. This controller is typically run as a sidecar in the CSI’s controller plug-in and forwards requests to the driver.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>At the time of this writing, these objects are implemented as CRDs and not core Kubernetes API objects. This requires the CSI driver or Kubernetes distribution to deploy the CRD definitions ahead of time.</p>&#13;
</div>&#13;
&#13;
<p>Similar to offering storage via StorageClasses, snapshotting is offered by introducing a Snapshot class. The following YAML represents this class:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">snapshot.storage.k8s.io/v1beta1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">VolumeSnapshotClass</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">default-snapshots</code><code>&#13;
</code><code class="nt">driver</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ebs.csi.aws.com</code><code> </code><a class="co" href="#callout_container_storage_CO8-1" id="co_container_storage_CO8-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="nt">deletionPolicy</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Delete</code><code> </code><a class="co" href="#callout_container_storage_CO8-2" id="co_container_storage_CO8-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO8-1" id="callout_container_storage_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Which driver to delegate snapshot request to.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO8-2" id="callout_container_storage_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Whether the VolumeSnapshotContent should be deleted when the VolumeSnapshot is deleted. In effect, the actual volume could be deleted (depending on support from the provider).</p></dd>&#13;
</dl>&#13;
&#13;
<p>In the Namespace of the application and PersistentVolumeClaim, a VolumeSnapshot may be created. An example is as follows:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">snapshot.storage.k8s.io/v1beta1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">VolumeSnapshot</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">snap1</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">volumeSnapshotClassName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">default-snapshots</code><code> </code><a class="co" href="#callout_container_storage_CO9-1" id="co_container_storage_CO9-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">source</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">persistentVolumeClaimName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">pvc0</code><code> </code><a class="co" href="#callout_container_storage_CO9-2" id="co_container_storage_CO9-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO9-1" id="callout_container_storage_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Specifies the class, which informs the driver to use.</p></dd>&#13;
<dt><a class="co" href="#co_container_storage_CO9-2" id="callout_container_storage_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Specifies the volume claim, which informs the volume to snapshot.</p></dd>&#13;
</dl>&#13;
&#13;
<p>The existence of this object will inform the need to create a VolumeSnapshotContent object. This object has a scope of cluster-wide. The detection of a VolumeSnapshotContent object will cause a request to create a snapshot and the driver will satisfy this by communicating with AWS EBS. Once satisfied, the VolumeSnapshot will report ReadyToUse. <a data-type="xref" href="#the_various_objects_and_their_relations_that_make_up_the_snapshot_flow">Figure 4-5</a> demonstrates the relationship between the various objects.</p>&#13;
&#13;
<figure><div class="figure" id="the_various_objects_and_their_relations_that_make_up_the_snapshot_flow">&#13;
<img alt="prku 0405" src="assets/prku_0405.png"/>&#13;
<h6><span class="label">Figure 4-5. </span>The various objects and their relations that make up the snapshot flow.</h6>&#13;
</div></figure>&#13;
&#13;
<p>With a snapshot in place, we can explore a scenario of data loss.<a data-primary="volumes" data-secondary="loss of data" data-type="indexterm" id="idm45611998951992"/> Whether the original volume was accidentally deleted, had a failure, or was removed due to an accidental deletion of a PersistentVolumeClaim, we can reestablish the data. To do this, a new PersistentVolumeClaim is created with the <code>spec.dataSource</code> specified. <code>dataSource</code> supports referencing a VolumeSnapshot that can populate data into the new claim. The following manifest recovers from the previously created snapshot:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">PersistentVolumeClaim</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">pvc-reclaim</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">accessModes</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">ReadWriteOnce</code><code>&#13;
</code><code>  </code><code class="nt">storageClassName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">default-block</code><code>&#13;
</code><code>  </code><code class="nt">resources</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">requests</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">storage</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">600Gi</code><code>&#13;
</code><code>  </code><code class="nt">dataSource</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">snap1</code><code> </code><a class="co" href="#callout_container_storage_CO10-1" id="co_container_storage_CO10-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>    </code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">VolumeSnapshot</code><code>&#13;
</code><code>    </code><code class="nt">apiGroup</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">snapshot.storage.k8s.io</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_container_storage_CO10-1" id="callout_container_storage_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The VolumeSnapshot instance that references the EBS snapshot to replenish the new PVC.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Once the Pod is re-created to reference this new claim, the last snapshotted state will return to the container! Now we have access to all the primitives for creating a robust backup and recovery solution. Solutions could range from scheduling snapshots via a CronJob, writing a custom controller, or using tools such as <a href="https://velero.io">Velero</a> to back up Kubernetes objects along with data volumes on a schedule.<a data-primary="storage" data-secondary="implementing storage as a service" data-startref="ix_storSaaS" data-type="indexterm" id="idm45611998900552"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611999079000">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we’ve explored a variety of container storage topics.  First, we want to have a deep understanding of application requirements to best inform our technical decision. Then we want to ensure that our underlying storage provider can satisfy these needs and that we have the operational expertise (when required) to operate them. Lastly, we should establish an integration between the orchestrator and the storage system, ensuring developers can get the storage they need without being proficient in an underlying storage system.<a data-primary="storage" data-startref="ix_stor" data-type="indexterm" id="idm45611998885208"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>