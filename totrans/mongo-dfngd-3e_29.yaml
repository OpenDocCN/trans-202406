- en: Chapter 23\. Making Backups
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to make regular backups of your system. Backups are good protection
    against most types of failure, and very little can’t be solved by restoring from
    a clean backup. This chapter covers the common options for making backups:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Single-server backups, including snapshot backup and restore procedure
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special considerations for backing up replica sets
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baking up a sharded cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backups are only useful if you are confident about deploying them in an emergency.
    Thus, for any backup technique you choose, be sure to practice both making backups
    and restoring from them until you are comfortable with the restore procedure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Backup Methods
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of options for backing up clusters in MongoDB. MongoDB Atlas,
    the official MongoDB cloud service, provides both continuous backups and cloud
    provider snapshots. Continuous backups take incremental backups of data in your
    cluster, ensuring your backups are typically just a few seconds behind the operating
    system. Cloud provider snapshots provide localized backup storage using the snapshot
    functionality of the cluster’s cloud service provider (e.g., Amazon Web Services,
    Microsoft Azure, or Google Cloud Platform). The best backup solution for the majority
    of scenarios is continuous backups.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB also provides backup capability through Cloud Manager and Ops Manager.
    Cloud Manager is a hosted backup, monitoring, and automation service for MongoDB.
    Ops Manager is an on-premise solution that has similar functionality to Cloud
    Manager.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: For individuals and teams managing MongoDB clusters directly, there are several
    backup strategies. We will outline these strategies in the rest of this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Backing Up a Server
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a variety of ways to create backups. Regardless of the method, making
    a backup can cause strain on a system: it generally requires reading all your
    data into memory. Thus, backups should generally be done on replica set secondaries
    (as opposed to the primary) or, for standalone servers, at an off time.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The techniques in this section apply to any *mongod*, whether a standalone server
    or a member of a replica set, unless otherwise noted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Filesystem Snapshot
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filesystem snapshots use system-level tools to create copies of the device that
    holds MongoDB’s data files. These methods complete quickly and work reliably,
    but require additional system configuration outside of MongoDB.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB 3.2 added support for volume-level backup of MongoDB instances using
    the WiredTiger storage engine when those instances’ data files and journal files
    reside on separate volumes. However, to create a coherent backup, the database
    must be locked and all writes to the database must be suspended during the backup
    process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Prior to MongoDB 3.2, creating volume-level backups of MongoDB instances using
    WiredTiger required that the data files and journal reside on the same volume.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots work by creating pointers between the live data and a special snapshot
    volume. These pointers are theoretically equivalent to “hard links.” As the working
    data diverges from the snapshot, the snapshot process uses a copy-on-write strategy.
    As a result, the snapshot only stores modified data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: After making the snapshot, you mount the snapshot image on your filesystem and
    copy data from the snapshot. The resulting backup contains a full copy of all
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'The database must be valid when the snapshot takes place. This means that all
    writes accepted by the database need to be fully written to disk: either to the
    journal or to data files. If there are writes that are not on disk when the backup
    occurs, the backup will not reflect these changes.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: For the WiredTiger storage engine, the data files reflect a consistent state
    as of the last checkpoint. Checkpoints occur every minute.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Snapshots create an image of an entire disk or volume. Unless you need to back
    up your entire system, consider isolating your MongoDB data files, journal (if
    applicable), and configuration on one logical disk that doesn’t contain any other
    data. Alternatively, store all MongoDB data files on a dedicated device so that
    you can make backups without duplicating extraneous data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you copy data from snapshots onto other systems. This ensures that
    data is safe from site failures.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: If your *mongod* instance has journaling enabled, then you can use any kind
    of filesystem or volume/block-level snapshot tool to create backups.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: If you manage your own infrastructure on a Linux-based system, configure your
    system using the Linux Logical Volume Manager (LVM) to provide your disk packages
    and provide snapshot capability. LVM allows for the flexible combination and division
    of physical disk partitions, enabling dynamically resizable filesystems. You can
    also use LVM-based setups within a cloud/virtualized environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In the initial setup of LVM, first we assign disk partitions to physical volumes
    `(pvcreate)`, then one or more of these are then assigned to a volume group `(vgcreate)`,
    and then we create logical volumes `(lvcreate)` referring to the volume groups.
    We can build a filesystem on the logical volume `(mkfs)`, which when created can
    be mounted for use `(mount)`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Snapshot backup and restore procedure
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section provides an overview of a simple backup process using LVM on a
    Linux system. While the tools, commands, and paths may be (slightly) different
    on your system, the following steps provide a high-level overview of the backup
    operation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Only use the following procedure as a guideline for a backup system and infrastructure.
    Production backup systems must consider a number of application-specific requirements
    and factors unique to specific environments.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a snapshot with LVM, issue a command as `root` in the following format:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command creates an LVM snapshot (with the `--snapshot` option) named `mdb-snap01`
    of the `mongodb` volume in the `vg0` volume group, which will be located at `/dev/vg0/mdb-snap01`.
    The location and paths to your systems, volume groups, and devices may vary slightly
    depending on your operating system’s LVM configuration.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The snapshot has a cap of 100 MB, because of the parameter `--size 100M`. This
    size does not reflect the total amount of the data on the disk, but rather the
    amount of differences between the current state of */dev/vg0/mongodb* and the
    snapshot (*/dev/vg0/mdb-snap01*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: The snapshot will exist when the command returns. You can restore directly from
    the snapshot at any time, or create a new logical volume and restore from the
    snapshot to the alternate image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: While snapshots are great for creating high-quality backups quickly, they are
    not ideal as a format for storing backup data. Snapshots typically depend and
    reside on the same storage infrastructure as the original disk images. Therefore,
    it’s crucial that you archive these snapshots and store them elsewhere.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a snapshot, mount the snapshot and copy the data to separate
    storage. Alternatively, take a block-level copy of the snapshot image, such as
    with the following procedure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '`# umount /dev/vg0/mdb-snap01`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '`# dd if=/dev/vg0/mdb-snap01 | gzip > mdb-snap01.gz`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'This command sequence does the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Ensures that the */dev/vg0/mdb-snap01* device is not mounted
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a block-level copy of the entire snapshot image using the `dd` command
    and compresses the result in a gzipped file in the current working directory
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `dd` command will create a large *.gz* file in your current working directory.
    Make sure that you run *this command* in a filesystem that has enough free space.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore a snapshot created with LVM, issue the following sequence of commands:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '`# lvcreate --size 1G --name mdb-new vg0`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '`# gzip -d -c mdb-snap01.gz | dd of=/dev/vg0/mdb-new`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '`# mount /dev/vg0/mdb-new /srv/mongodb`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'This sequence does the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Creates a new logical volume named *mdb-new*, in the */dev/vg0* volume group.
    The path to the new device will be */dev/vg0/mdb-new*. You can use a different
    name, and change 1G to your desired volume size.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncompresses and unarchives the *mdb-snap01.gz* file into the *mdb-new* disk
    image.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mounts the *mdb-new* disk image to the */srv/mongodb* directory. Modify the
    mount point to correspond to your MongoDB data file location or other location
    as needed.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The restored snapshot will have a stale `mongod.lock` file. If you do not remove
    this file from the snapshot, MongoDB may assume that the stale lock file indicates
    an unclean shutdown. If you’re running with `storage.journal.enabled` enabled
    and you do not use `db.fsyncLock()`, you do not need to remove the `mongod.lock`
    file. If you use `db.fsyncLock()` you will need to remove the lock.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore a backup without writing to a compressed *.gz* file, use the following
    sequence of commands:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '`# umount /dev/vg0/mdb-snap01`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '`# lvcreate --size 1G --name mdb-new vg0`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '`# dd if=/dev/vg0/mdb-snap01 of=/dev/vg0/mdb-new`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '`# mount /dev/vg0/mdb-new /srv/mongodb`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'You can implement off-system backups using the combined process and SSH. This
    sequence is identical to procedures explained previously, except that it archives
    and compresses the backup on a remote system using SSH:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '`umount /dev/vg0/mdb-snap01`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '`dd if=/dev/vg0/mdb-snap01 | ssh username@example.com gzip > /opt/backup/mdb-snap01.gz`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '`lvcreate --size 1G --name mdb-new vg0`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '`ssh username@example.com gzip -d -c /opt/backup/mdb-snap01.gz | dd of=/dev/vg0/mdb-new`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '`mount /dev/vg0/mdb-new /srv/mongodb`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Starting in MongoDB 3.2, for the purpose of volume-level backup of MongoDB instances
    using WiredTiger, the data files and the journal are no longer required to reside
    on a single volume. However, the database must be locked and all writes to the
    database must be suspended during the backup process to ensure the consistency
    of the backup.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: If your *mongod* instance is either running without journaling or has the journal
    files on a separate volume, you must flush all writes to disk and lock the database
    to prevent writes during the backup process. If you have a replica set configuration,
    then for your backup use a secondary that is not receiving reads (i.e., a hidden
    member).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, issue the `db.fsyncLock()` method in the `mongo` shell:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then perform the backup operation described previously.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'After the snapshot completes, unlock the database by issuing the following
    command in the `mongo` shell:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This process is described more fully in the following section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Copying Data Files
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another way of creating single-server backups is to make a copy of everything
    in the data directory. Because you cannot copy all of the files at the same moment
    without filesystem support, you must prevent the data files from changing while
    you are making the copy. This can be accomplished with a command called `fsyncLock`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This command locks the database against any further writes and then flushes
    all dirty data to disk (`fsync`), ensuring that the files in the data directory
    have the latest consistent information and are not changing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Once this command has been run, *mongod* will enqueue all incoming writes. It
    will not process any further writes until it has been unlocked. Note that this
    command stops writes to *all* databases (not just the one *db* is connected to).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `fsyncLock` command returns, copy all of the files in your data directory
    to a backup location. On Linux, this can be done with a command such as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Make sure that you copy absolutely every file and folder from the data directory
    to the backup location. Excluding files or directories may make the backup unusable
    or corrupt.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have finished copying the data, unlock the database to allow it to
    take writes again:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Your database will begin handling writes again normally.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are some locking issues with authentication and `fsyncLock`.
    If you are using authentication, do not close the shell between calling `fsyncLock`
    and `fsyncUnlock`. If you disconnect, you may be unable to reconnect and have
    to restart `mongod`. The `fsyncLock` setting does not persist between restarts;
    `mongod` will always start up unlocked.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to `fsyncLock`, you can instead shut down *mongod*, copy the
    files, and then start *mongod* back up again. Shutting down *mongod* effectively
    flushes all changes to disk and prevents new writes from occurring during the
    backup.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore from the copy of the data directory, ensure that *mongod* is not
    running and that the data directory you want to restore into is empty. Copy the
    backed-up data files to the data directory, and then start *mongod*. For example,
    the following command would restore the files backed up with the command shown
    earlier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Despite the warnings about partial data directory copies, you can use this method
    to back up individual databases if you know what to copy and where they are using
    the `--directoryperdb` option. To back up an individual database (called, say,
    *myDB*), which is only available if you are using the `--directoryperdb` option,
    copy the entire *myDB* directory. Partial data directory copies are only possible
    with the `--directoryperdb` option.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'You can restore specific databases by copying just the files with the correct
    database name into your data directory. You must be starting from a clean shutdown
    to restore piecemeal like this. If you had a crash or a hard shutdown, do not
    attempt to restore a single database from the backup: replace the entire directory
    and start the *mongod* to allow the journal files to be replayed.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Never use `fsyncLock` in conjunction with *mongodump* (described next). Depending
    on what else your database is doing, *mongodump* may hang forever if the database
    is locked.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Using mongodump
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The final way of making a single-server backup is to use *mongodump*. *mongodump*
    is mentioned last because it has some downsides. It is slower (both to get the
    backup and to restore from it) and it has some issues with replica sets, which
    are discussed in [“Specific Considerations for Replica Sets”](#sect1-rs-backup).
    However, it also has some benefits: it is a good way to back up individual databases,
    collections, and even subsets of collections.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '*mongodump* has a variety of options that you can see by running `mongodump
    --help`. Here, we will focus on the most useful ones to use for backing up.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'To back up all databases, simply run *mongodump*. If you are running *mongodump*
    on the same machine as the *mongod*, you can simply specify the port *mongod*
    is running on:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*mongodump* will create a *dump* directory in the current directory, which
    contains a dump of all your data. This *dump* directory is organized by database
    and by collection into folders and subfolders. The actual data is stored in *.bson*
    files, which merely contain every document in a collection in BSON, concatenated
    together. You can examine *.bson* files using the *bsondump* tool, which comes
    with MongoDB.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'You do not even need to have a server running to use *mongodump*. You can use
    the `--dbpath` option to specify your data directory, and *mongodump* will use
    the data files to copy data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You should not use `--dbpath` if *mongod* is running.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'One issue with *mongodump* is that it is not an instantaneous backup: the system
    may be taking writes while the backup occurs. Thus, you might end up with a situation
    where user A begins a backup that causes *mongodump* to dump the database *A*,
    but while this is happening user B drops *A*. However, *mongodump* has already
    dumped it, so you’ll end up with a snapshot of the data that is inconsistent with
    the state on the original server.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, if you are running *mongod* with `--replSet`, you can use *mongodump*’s
    `--oplog` option. This will keep track of all operations that occur on the server
    while the dump is taking place, so these operations can be replayed when the backup
    is restored. This gives you a consistent point-in-time snapshot of data from the
    source server.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: If you pass *mongodump* a replica set connection string (e.g., ``"*`setName`*/*`seed1`*,*`seed2`*,*`seed3`*"``),
    it will automatically select the primary to dump from. If you want to use a secondary,
    you can specify a `read preference`. The `read preference` can be specified by
    `--uri connection string`, by the `uri readPreferenceTags` option, or by the `--readPreference`
    command-line option. For more details on the various settings and options, please
    see [the *mongodump* MongoDB documentation page](https://oreil.ly/GH3-O).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore from a *mongodump* backup, use the *mongorestore* tool:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you used the `--oplog` option to dump the database, you must use the `--oplogReplay`
    option with *mongorestore* to get the point-in-time snapshot.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: If you are replacing data on a running server, you may (or may not) wish to
    use the `--drop` option, which drops a collection before restoring it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The behavior of *mongodump* and *mongorestore* has changed over time. To prevent
    compatibility issues, try to use the same version of both utilities (you can see
    their versions by running `mongodump --version` and `mongorestore --version`).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From MongoDB version 4.2 and up, you cannot use either *mongodump* or *mongorestore*
    as a strategy for backing up a sharded cluster. These tools do not maintain the
    atomicity guarantees of transactions across shards.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Moving collections and databases with mongodump and mongorestore
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can restore into an entirely different database and collection than you
    dumped from. This can be useful if different environments use different database
    names (say, *dev* and *prod*) but the same collection names.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore a *.bson* file into a specific database and collection, specify
    the targets on the command line:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It is also possible to use these tools with SSH to perform data migration without
    any disk I/O using the archive feature of these tools. This simplifies three stages
    into one operation, when previously you had to back up to disk, then copy those
    backup files to a target server, and then run *mongorestore* on that server to
    restore the backups:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Compression can be combined with the archive feature of these tools to further
    reduce the size of the information sent while performing a data migration. Here
    is the same SSH data migration example using both the archive and compression
    features of these tools:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Administrative complications with unique indexes
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have a unique index (other than `"_id"`) on any of your collections,
    you should consider using a different type of backup than *mongodump*/*mongorestore*.
    Unique indexes require that the data does not change in ways that would violate
    the unique index constraint during the copy. The safest way to ensure this is
    to choose a method that “freezes” the data, then make a backup as described in
    either of the previous two sections.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: If you are determined to use *mongodump*/*mongorestore*, you may need to preprocess
    your data when you restore from a backup.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Specific Considerations for Replica Sets
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main additional consideration when backing up a replica set is that as well
    as the data, you must also capture the state of the replica set to ensure an accurate
    point-in-time snapshot of your deployment is made.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, you should make backups from a secondary: this keeps load off of
    the primary, and you can lock a secondary without affecting your application (so
    long as your application isn’t sending it read requests). You can use any of the
    three methods outlined previously to back up a replica set member, but a filesystem
    snapshot or data file copy is recommended. Either of these techniques can be applied
    to replica set secondaries with no modification.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '*mongodump* is not quite as simple to use when replication is enabled. First,
    if you are using *mongodump*, you must take your backups using the `--oplog` option
    to get a point-in-time snapshot; otherwise the backup’s state won’t match the
    state of any other members in the cluster. You must also create an oplog when
    you restore from a *mongodump* backup, or the restored member will not know where
    it was synced to.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore a replica set member from a *mongodump* backup, start the target
    replica set member as a standalone server with an empty data directory and run
    *mongorestore* on it (as described in the previous section) with the `--oplogReplay`
    option. Now it should have a complete copy of the data, but it still needs an
    oplog. Create an oplog using the `createCollection` command:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Specify the size of the collection in bytes. See [“Resizing the Oplog”](ch13.xhtml#sect1-resize-oplog)
    for advice on oplog sizing.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you need to populate the oplog. The easiest way to do this is to restore
    the *oplog.bson* backup file from the dump into the *local.oplog.rs* collection:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that this is not a dump of the oplog itself (*dump/local/oplog.rs.bson*),
    but rather of the oplog operations that occurred during the dump. Once this *mongorestore*
    is complete, you can restart this server as a replica set member.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Specific Considerations for Sharded Clusters
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main additional consideration when backing up a sharded cluster using the
    approaches in this chapter is that you can only back up the pieces when they are
    active, and sharded clusters are impossible to “perfectly” back up while active:
    you can’t get a snapshot of the entire state of the cluster at a point in time.
    However, this limitation is generally sidestepped by the fact that as your cluster
    gets bigger, it becomes less and less likely that you’d ever have to restore the
    whole thing from a backup. Thus, when dealing with a sharded cluster, we focus
    on backing up pieces: the config servers and the replica sets individually. If
    you need the ability to back up the whole cluster to a particular point in time
    or would prefer an automated solution, you can avail yourself of MongoDB’s Cloud
    Manager or Atlas backup feature.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Turn off the balancer before performing any of these operations on a sharded
    cluster (either backup or restore). You cannot get a consistent snapshot of the
    world with chunks flying around. See [“Balancing Data”](ch17.xhtml#sect1-balancer)
    for instructions on turning the balancer on and off.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Backing Up and Restoring an Entire Cluster
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a cluster is very small or in development, you may want to actually dump
    and restore the entire thing. You can accomplish this by turning off the balancer
    and then running *mongodump* through the *mongos*. This creates a backup of all
    of the shards on whatever machine *mongodump* is running on.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: To restore from this type of backup, run *mongorestore* connected to a *mongos*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, after turning off the balancer you can take filesystem or data
    directory backups of each shard and the config servers. However, you will inevitably
    get copies from each at slightly different times, which may or may not be a problem.
    Also, as soon as you turn on the balancer and a migrate occurs, some of the data
    you backed up from one shard will no longer be there.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Backing Up and Restoring a Single Shard
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most often, you’ll only need to restore a single shard in a cluster. If you
    are not too picky, you can restore from a backup of that shard using one of the
    single-server methods just described.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，您只需要恢复集群中的一个分片。如果您不是太挑剔的话，可以使用刚才描述的单服务器方法之一从该分片的备份中恢复。
- en: There is one important issue to be aware of, however. Suppose you make a backup
    of your cluster on Monday. On Thursday, your disk melts down and you have to restore
    from the backup. In the intervening days, new chunks may have moved to this shard.
    Your backup of the shard from Monday will not contain these new chunks. You may
    be able to use a config server backup to figure out where the disappearing chunks
    lived on Monday, but it is a lot more difficult than simply restoring the shard.
    In most cases, restoring the shard and losing the data in those chunks is the
    preferable route.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个重要问题需要注意。假设您在星期一备份了集群。到了星期四，您的硬盘损坏了，您需要从备份中恢复。在这几天中，新的数据块可能已经移到了该分片。您星期一备份的分片不包含这些新的数据块。您可能可以使用配置服务器的备份来找出星期一消失的数据块在哪里，但这比简单地恢复分片要困难得多。在大多数情况下，恢复分片并丢失这些数据块中的数据是更可取的路径。
- en: You can connect directly to a shard to restore from a backup (instead of going
    through *mongos*).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接连接到一个分片来从备份中恢复（而不通过*mongos*）。
