- en: Chapter 5\. Kubernetes Networking Abstractions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we covered a swath of networking fundamentals and how traffic in
    Kubernetes gets from A to B. In this chapter, we will discuss networking abstractions
    in Kubernetes, primarily service discovery and load balancing. Most notably, this
    is the chapter on services and ingresses. Both resources are notoriously complex,
    due to the large number of options, as they attempt to solve numerous use cases.
    They are the most visible part of the Kubernetes network stack, as they define
    basic network characteristics of workloads on Kubernetes. This is where developers
    interact with the networking stack for their applications deployed on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover fundamental examples of Kubernetes networking abstractions
    and the details on\f how they work. To follow along, you will need the following
    tools:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KIND
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linkerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need to be familiar with the `kubectl exec` and `Docker exec` commands.
    If you are not, our code repo will have any and all the commands we discuss, so
    don’t worry too much. We will also make use of `ip` and `netns` from Chapters
    [2](ch02.xhtml#linux_networking) and [3](ch03.xhtml#container_networking_basics).
    Note that most of these tools are for debugging and showing implementation details;
    you will not necessarily need them during normal operations.
  prefs: []
  type: TYPE_NORMAL
- en: Docker, KIND, and Linkerd installs are available on their respective sites,
    and we’ve provided more information in the book’s code repository as well.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`kubectl` is a key tool in this chapter’s examples, and it’s the standard for
    operators to interact with clusters and their networks. You should be familiar
    with the `kubectl create`, `apply`, `get`, `delete`, and `exec` commands. Learn
    more in the [Kubernetes documentation](https://oreil.ly/H8bTU) or run `kubectl
    [command] --help`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will explore these Kubernetes networking abstractions:'
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Endpoint slices
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NodePort
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Headless
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: External
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LoadBalancer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress controller
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress rules
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Service meshes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linkerd
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To explore these abstractions, we will deploy the examples to our Kubernetes
    cluster with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy a KIND cluster with ingress enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore StatefulSets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy Kubernetes services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy an ingress controller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy a Linkerd service mesh.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These abstractions are at the heart of what the Kubernetes API provides to developers
    and administrators to programmatically control the flow of communications into
    and out of the cluster. Understanding and mastering how to deploy these abstractions
    is crucial for the success of any workload inside a cluster. After working through
    these examples, you will understand which abstractions to use in certain situations
    for your applications.
  prefs: []
  type: TYPE_NORMAL
- en: With the KIND cluster configuration YAML, we can use KIND to create that cluster
    with the command in the next section. If this is the first time running it, it
    will take some time to download all the Docker images for the working and control
    plane Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following examples assume that you still have the local KIND cluster running
    from the previous chapter, along with the Golang web server and the `dnsutils`
    images for testing.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'StatefulSets are a workload abstraction in Kubernetes to manage pods like you
    would a deployment. Unlike a deployment, StatefulSets add the following features
    for applications that require them:'
  prefs: []
  type: TYPE_NORMAL
- en: Stable, unique network identifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stable, persistent storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ordered, graceful deployment and scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ordered, automated rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment resource is better suited for applications that do not have these
    requirements (for example, a service that stores data in an external database).
  prefs: []
  type: TYPE_NORMAL
- en: Our database for the Golang minimal web server uses a StatefulSet. The database
    has a service, a ConfigMap for the Postgres username, a password, a test database
    name, and a StatefulSet for the containers running Postgres.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy it now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s examine the DNS and network ramifications of using a StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test DNS inside the cluster, we can use the `dnsutils` image; this image
    is `gcr .io/kubernetes-e2e-test-images/dnsutils:1.3` and is used for Kubernetes
    testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With the replica configured with two pods, we see the StatefulSet deploy `postgres-0`
    and `postgres-1`, in that order, a feature of StatefulSets with IP address 10.244.1.3
    and 10.244.2.3, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the name of our headless service, Postgres, that the client can use
    for queries to return the endpoint IP addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Using our `dnsutils` image, we can see that the DNS names for the StatefulSets
    will return those IP addresses along with the cluster IP address of the Postgres
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: StatefulSets attempt to mimic a fixed group of persistent machines. As a generic
    solution for stateful workloads, specific behavior may be frustrating in specific
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common problem that users encounter is an update requiring manual intervention
    to fix when using `.spec .updateStrategy.type: RollingUpdate`, and `.spec.podManagementPolicy:
    OrderedReady`, both of which are default settings. With these settings, a user
    must manually intervene if an updated pod never becomes ready.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, StatefulSets require a service, preferably headless, to be responsible
    for the network identity of the pods, and end users are responsible for creating
    this service.
  prefs: []
  type: TYPE_NORMAL
- en: Statefulsets have many configuration options, and many third-party alternatives
    exist (both generic stateful workload controllers and software-specific workload
    controllers).
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets offer functionality for a specific use case in Kubernetes. They
    should not be used for everyday application deployments. Later in this section,
    we will discuss more appropriate networking abstractions for run-of-the-mill deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we will explore endpoints and endpoint slices, the backbone
    of Kubernetes services.
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Endpoints help identify what pods are running for the service it powers. Endpoints
    are created and managed by services. We will discuss services on their own later,
    to avoid covering too many new things at once. For now, let’s just say that a
    service contains a standard label selector (introduced in [Chapter 4](ch04.xhtml#kubernetes_networking_introduction)),
    which defines which pods are in the endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-1](#img-endpoints), we can see traffic being directed to an endpoint
    on node 2, pod 5.
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes Endpoints](Images/neku_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Endpoints in a service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s discuss how this endpoint is created and maintained in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each endpoint contains a list of ports (which apply to all pods) and two lists
    of addresses: ready and unready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Addresses are listed in `.addresses` if they are passing pod readiness checks.
    Addresses are listed in `.notReadyAddresses` if they are not. This makes endpoints
    a *service discovery* tool, where you can watch an `Endpoints` object to see the
    health and addresses of all pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get a better view of all the addresses with `kubectl describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s remove the app label and see how Kubernetes responds. In a separate terminal,
    run this command. This will allow us to see changes to the pods in real time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In another separate terminal, let’s do the same thing with endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to get a pod name to remove from the `Endpoints` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With `kubectl label`, we can alter the pod’s `app-5586fc9d77-7frts` `app=app`
    label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Both watch commands on endpoints and pods will see some changes for the same
    reason: removal of the label on the pod. The endpoint controller will notice a
    change to the pods with the label `app=app` and so did the deployment controller.
    So Kubernetes did what Kubernetes does: it made the real state reflect the desired
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The deployment has four pods, but our relabeled pod still exists: `app-5586fc9d77-7frts`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The pod `app-5586fc9d77-6dcg2` now is part of the deployment and endpoint object
    with IP address `10.244.1.6`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, we can see the full picture of details with `kubectl describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For large deployments, that endpoint object can become very large, so much so
    that it can actually slow down changes in the cluster. To solve that issue, the
    Kubernetes maintainers have come up with endpoint slices.
  prefs: []
  type: TYPE_NORMAL
- en: Endpoint Slices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be asking, how are they different from endpoints? This is where we *really*
    start to get into the weeds of Kubernetes networking.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical cluster, Kubernetes runs `kube-proxy` on every node. `kube-proxy`
    is responsible for the per-node portions of making services work, by handling
    routing and *outbound* load balancing to all the pods in a service. To do that,
    `kube-proxy` watches all endpoints in the cluster so it knows all applicable pods
    that all services should route to.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine we have a *big* cluster, with thousands of nodes, and tens of thousands
    of pods. That means thousands of kube-proxies are watching endpoints. When an
    address changes in an `Endpoints` object (say, from a rolling update, scale up,
    eviction, health-check failure, or any number of reasons), the updated `Endpoints`
    object is pushed to all listening kube-proxies. It is made worse by the number
    of pods, since more pods means larger `Endpoints` objects, and more frequent changes.
    This eventually becomes a strain on `etcd`, the Kubernetes API server, and the
    network itself. Kubernetes scaling limits are complex and depend on specific criteria,
    but endpoints watching is a common problem in clusters that have thousands of
    nodes. Anecdotally, many Kubernetes users consider endpoint watches to be the
    ultimate bottleneck of cluster size.
  prefs: []
  type: TYPE_NORMAL
- en: This problem is a function of `kube-proxy`’s design and the expectation that
    any pod should be immediately able to route to any service with no notice. Endpoint
    slices are an approach that allows `kube-proxy`’s fundamental design to continue,
    while drastically reducing the watch bottleneck in large clusters where large
    services are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Endpoint slices have similar contents to `Endpoints` objects but also include
    an array of endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The meaningful difference between endpoints and endpoint slices is not the schema,
    but how Kubernetes treats them. With “regular” endpoints, a Kubernetes service
    creates one endpoint for all pods in the service. A service creates *multiple*
    endpoint slices, each containing a *subset* of pods; [Figure 5-2](#img-endpointslice)
    depicts this subset. The union of all endpoint slices for a service contains all
    pods in the service. This way, an IP address change (due to a new pod, a deleted
    pod, or a pod’s health changing) will result in a much smaller data transfer to
    watchers. Because Kubernetes doesn’t have a transactional API, the same address
    may appear temporarily in multiple slices. Any code consuming endpoint slices
    (such as `kube-proxy`) must be able to account for this.
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of addresses in an endpoint slice is set using the `--max-endpoints-per-slice`
    `kube-controller-manager` flag. The current default is 100, and the maximum is
    1000. The endpoint slice controller attempts to fill existing endpoint slices
    before creating new ones, but does not rebalance endpoint slice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The endpoint slice controller mirrors endpoints to endpoint slice, to allow
    systems to continue writing endpoints while treating endpoint slice as the source
    of truth. The exact future of this behavior, and endpoints in general, has not
    been finalized (however, as a v1 resource, endpoints would be sunset with substantial
    notice). There are four exceptions that will prevent mirroring:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no corresponding service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The corresponding service resource selects pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Endpoints` object has the label `endpointslice.kubernetes.io/skip-mirror:
    true`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Endpoints` object has the annotation `control-⁠⁠plane.alpha.kubernetes​​.io/leader`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![EndpointsVSliice](Images/neku_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. `Endpoints` versus `EndpointSlice` objects
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can fetch all endpoint slices for a specific service by fetching endpoint
    slices filtered to the desired name in `.metadata.labels."kubernetes.io/service-name"`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Endpoint slices have been in beta state since Kubernetes 1.17. This is still
    the case in Kubernetes 1.20, the current version at the time of writing. Beta
    resources typically don’t see major changes, and eventually graduate to stable
    APIs, but that is not guaranteed. If you directly use endpoint slices, be aware
    that a future Kubernetes release may make a breaking change without much warning,
    or the behaviors described here may change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see some endpoints running in the cluster with `kubectl get endpointslice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want more detail about the endpoint slices `clusterip-service-l2n9q`,
    we can use `kubectl describe` on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the output, we see the pod powering the endpoint slice from `TargetRef`.
    The `Topology` information gives us the hostname of the worker node that the pod
    is deployed to. Most importantly, the `Addresses` returns the IP address of the
    endpoint object.
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints and endpoint slices are important to understand because they identify
    the pods responsible for the services, no matter the type deployed. Later in the
    chapter, we’ll review how to use endpoints and labels for troubleshooting. Next,
    we will investigate all the Kubernetes service types.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A service in Kubernetes is a load balancing abstraction within a cluster. There
    are four types of services, specified by the `.spec.Type` field. Each type offers
    a different form of load balancing or discovery, which we will cover individually.
    The four types are: ClusterIP, NodePort, LoadBalancer, and ExternalName.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Services use a standard pod selector to match pods. The service includes all
    matching pods. Services create an endpoint (or endpoint slice) to handle pod discovery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We will use the Golang minimal web server for all the service examples. We have
    added functionality to the application to display the host and pod IP addresses
    in the REST request.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-3](#img-pod-connection) outlines our pod networking status as a single
    pod in a cluster. The networking objects we are about to explore will expose our
    app pods outside the cluster in some instances and in others allow us to scale
    our application to meet demand. Recall from Chapters [3](ch03.xhtml#container_networking_basics)
    and [4](ch04.xhtml#kubernetes_networking_introduction) that containers running
    inside pods share a network namespace. In addition, there is also a pause container
    that is created for each pod. The pause container manages the namespaces for the
    pod.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The pause container is the parent container for all running containers in the
    pod. It holds and shares all the namespaces for the pod. You can read more about
    the pause container in Ian Lewis’ [blog post](https://oreil.ly/n51eq).
  prefs: []
  type: TYPE_NORMAL
- en: '![Pod on Host](Images/neku_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Pod on host
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Before we deploy the services, we must first deploy the web server that the
    services will be routing traffic to, if we have not already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at each type of service starting with NodePort.
  prefs: []
  type: TYPE_NORMAL
- en: NodePort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A NodePort service provides a simple way for external software, such as a load
    balancer, to route traffic to the pods. The software only needs to be aware of
    node IP addresses, and the service’s port(s). A NodePort service exposes a fixed
    port on all nodes, which routes to applicable pods. A NodePort service uses the
    `.spec.ports.[].nodePort` field to specify the port to open on all nodes, for
    the corresponding port on pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `nodePort` field can be left blank, in which case Kubernetes automatically
    selects a unique port. The `--service-node-port-range` flag in `kube-controller-manager`
    sets the valid range for ports, 30000–32767. Manually specified ports must be
    within this range.
  prefs: []
  type: TYPE_NORMAL
- en: Using a NodePort service, external users can connect to the nodeport on any
    node and be routed to a pod on a node that has a pod backing that service; [Figure 5-4](#img-node-port-traffic)
    demonstrates this. The service directs traffic to node 3, and `iptables` rules
    forward the traffic to node 2 hosting the pod. This is a bit inefficient, as a
    typical connection will be routed to a pod on another node.
  prefs: []
  type: TYPE_NORMAL
- en: '![Node Port Traffic Flow](Images/neku_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. NodePort traffic flow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 5-4](#img-node-port-traffic) requires us to discuss an attribute of
    services, externalTrafficPolicy. ExternalTrafficPolicy indicates how a service
    will route external traffic to either node-local or cluster-wide endpoints. `Local`
    preserves the client source IP and avoids a second hop for LoadBalancer and NodePort
    type services but risks potentially imbalanced traffic spreading. Cluster obscures
    the client source IP and may cause a second hop to another node but should have
    good overall load-spreading. A `Cluster` value means that for each worker node,
    the `kube-proxy iptable` rules are set up to route the traffic to the pods backing
    the service anywhere in the cluster, just like we have shown in [Figure 5-4](#img-node-port-traffic).'
  prefs: []
  type: TYPE_NORMAL
- en: A `Local` value means the `kube-proxy iptable` rules are set up only on the
    worker nodes with relevant pods running to route the traffic local to the worker
    node. Using `Local` also allows application developers to preserve the source
    IP of the user request. If you set externalTrafficPolicy to the value `Local`,
    `kube-proxy` will proxy requests only to node-local endpoints and will not forward
    traffic to other nodes. If there are no local endpoints, packets sent to the node
    are dropped.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s scale up the deployment of our web app for some more testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With four pods running, we will have one pod at every node in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s deploy our NodePort service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the NodePort service, we must retrieve the IP address of a worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Communication external to the cluster will use a `NodePort` value of 30040 opened
    on each worker and the node worker’s IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that our pods are reachable on each host in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to consider the limitations as well. A NodePort deployment will
    fail if it cannot allocate the requested port. Also, ports must be tracked across
    all applications using a NodePort service. Using manually selected ports raises
    the issue of port collisions (especially when applying a workload to multiple
    clusters, which may not have the same NodePorts free).
  prefs: []
  type: TYPE_NORMAL
- en: Another downside of using the NodePort service type is that the load balancer
    or client software must be aware of the node IP addresses. A static configuration
    (e.g., an operator manually copying node IP addresses) may become too outdated
    over time (especially on a cloud provider) as IP addresses change or nodes are
    replaced. A reliable system automatically populates node IP addresses, either
    by watching which machines have been allocated to the cluster or by listing nodes
    from the Kubernetes API itself.
  prefs: []
  type: TYPE_NORMAL
- en: NodePorts are the earliest form of services. We will see that other service
    types use NodePorts as a base structure in their architecture. NodePorts should
    not be used by themselves, as clients would need to know the IP addresses of hosts
    and the node for connection requests. We will see how NodePorts are used to enable
    load balancers later in the chapter when we discuss cloud networks.
  prefs: []
  type: TYPE_NORMAL
- en: Next up is the default type for services, ClusterIP.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IP addresses of pods share the life cycle of the pod and thus are not reliable
    for clients to use for requests. Services help overcome this pod networking design.
    A ClusterIP service provides an internal load balancer with a single IP address
    that maps to all matching (and ready) pods.
  prefs: []
  type: TYPE_NORMAL
- en: The service’s IP address must be within the CIDR set in `service-cluster-ip-range`,
    in the API server. You can specify a valid IP address manually, or leave `.spec.clusterIP`
    unset to have one assigned automatically. The ClusterIP service address is a virtual
    IP address that is routable only internally.
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-proxy` is responsible for making the ClusterIP service address route
    to all applicable pods. In “normal” configurations, `kube-proxy` performs L4 load
    balancing, which may not be sufficient. For example, older pods may see more load,
    due to accumulating more long-lived connections from clients. Or, a few clients
    making many requests may cause the load to be distributed unevenly.'
  prefs: []
  type: TYPE_NORMAL
- en: A particular use case example for ClusterIP is when a workload requires a load
    balancer within the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-5](#img-clusterip), we can see a ClusterIP service deployed. The
    service name is App with a selector, or App=App1. There are two pods powering
    this service. Pod 1 and Pod 5 match the selector for the service.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster IP](Images/neku_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Cluster IP example service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s dig into an example on the command line with our KIND cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will deploy a ClusterIP service for use with our Golang web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The ClusterIP service name is resolvable in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can reach the host API endpoint with the Cluster IP address `10.98.252.195`,
    with the service name `clusterip-service`; or directly with the pod IP address
    `10.244.1.4` and port 8080:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The ClusterIP service is the default type for services. With that default status,
    it is warranted that we should explore what the ClusterIP service abstracted for
    us. If you recall from Chapters [2](ch02.xhtml#linux_networking) and [3](ch03.xhtml#container_networking_basics),
    this list is similar to what is set up with the Docker network, but we now also
    have `iptables` for the service across all nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: View the VETH pair and match with the pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: View the network namespace and match with the pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the PIDs on the node and match the pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Match services with `iptables` rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To explore this, we need to know what worker node the pod is deployed to, and
    that is `kind-worker2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The container IDs and names will be different for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are using KIND, we can use `docker ps` and `docker exec` to get information
    out of the running worker node `kind-worker-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kind-worker2` container ID is `df6df0736958`; KIND was *kind* enough to
    label each container with names, so we can reference each worker node with its
    name `kind-worker2`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the IP address and route table information of our pod, `app-9cc7d9df8-ffsm6`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Our pod’s IP address is `10.244.1.4` running on interface `eth0@if5` with `10.244.1.1`
    as its default route. That matches interface 5 on the pod `veth45d1f3e8@if5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the network namespace as well, from the `node ip a` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`netns list` confirms that the network namespaces match our pods, interface
    to the host interface, `cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what processes run inside that network namespace. For that we will
    use `docker exec` to run commands inside the node `kind-worker2` hosting the pod
    and its network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can `grep` for each process ID and inspect what they are doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`4737` is the process ID of our web server container running on `kind-worker2`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`4687` is our pause container holding onto all our namespaces.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see what will happen to the `iptables` on the worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: That is a lot of tables being managed by Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can dive a little deeper to examine the `iptables` responsible for the services
    we deployed. Let’s retrieve the IP address of the `clusterip-service` deployed.
    We need this to find the matching `iptables` rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now use the clusterIP of the service, `10.98.252.195`, to find our `iptables`
    rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'List all the rules on the chain `KUBE-SVC-V7R3EVKW3DT43QQM`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The `KUBE-SEP-` will contain the endpoints for the services, `KUBE-SEP-THJR2P3Q4C2QAEPT`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can see what the rules for this chain are in `iptables`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`10.244.1.4:8080` is one of the service endpoints, aka a pod backing the service,
    which is confirmed with the output of `kubectl get ep clusterip-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s explore the limitations of the ClusterIP service. The ClusterIP service
    is for internal traffic to the cluster, and it suffers the same issues as endpoints
    do. As the service size grows, updates to it will slow. In [Chapter 2](ch02.xhtml#linux_networking),
    we discussed how to mitigate that by using IPVS over `iptables` as the proxy mode
    for `kube-proxy`. We will discuss later in this chapter how to get traffic into
    the cluster using ingress and the other service type LoadBalancer.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP is the default type of service, but there are several other specific
    types of services such as headless and ExternalName. ExternalName is a specific
    type of services that helps with reaching services outside the cluster. We briefly
    touched on headless services with StatefulSets, but let’s review those services
    in depth now.
  prefs: []
  type: TYPE_NORMAL
- en: Headless
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A headless service isn’t a formal type of service (i.e., there is no `.spec.type:
    Headless`). A headless service is a service with `.spec.clusterIP: "None"`. This
    is distinct from merely *not setting* a cluster IP address, which makes Kubernetes
    automatically assign a cluster IP address.'
  prefs: []
  type: TYPE_NORMAL
- en: When ClusterIP is set to None, the service does not support any load balancing
    functionality. Instead, it only provisions an `Endpoints` object and points the
    service DNS record at all pods that are selected and ready.
  prefs: []
  type: TYPE_NORMAL
- en: A headless service provides a generic way to watch endpoints, without needing
    to interact with the Kubernetes API. Fetching DNS records is much simpler than
    integrating with the Kubernetes API, and it may not be possible with third-party
    software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Headless services allow developers to deploy multiple copies of a pod in a
    deployment. Instead of a single IP address returned, like with the ClusterIP service,
    all the IP addresses of the endpoint are returned in the query. It then is up
    to the client to pick which one to use. To see this in action, let’s scale up
    the deployment of our web app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s deploy the headless service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The DNS query will return all four of the pod IP addresses. Using our `dnsutils`
    image, we can verify that is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The IP addresses returned from the query also match the endpoints for the service.
    Using `kubectl describe` for the endpoint confirms that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Headless has a specific use case and is not typically used for deployments.
    As we mentioned in [“StatefulSets”](#statefulsets), if developers need to let
    the client decide which endpoint to use, headless is the appropriate type of service
    to deploy. Two examples of headless services are clustered databases and applications
    that have client-side load-balancing logic built into the code.
  prefs: []
  type: TYPE_NORMAL
- en: Our next example is ExternalName, which aids in migrations of services external
    to the cluster. It also offers other DNS advantages inside cluster DNS.
  prefs: []
  type: TYPE_NORMAL
- en: ExternalName Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ExternalName is a special type of service that does not have selectors and uses
    DNS names instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking up the host `ext-service.default.svc.cluster.local`, the cluster
    DNS service returns a CNAME record of `database.mycompany.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: If developers are migrating an application into Kubernetes but its dependencies
    are staying external to the cluster, ExternalName service allows them to define
    a DNS record internal to the cluster no matter where the service actually runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'DNS will try the search as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As an example, the ExternalName service allows developers to map a service to
    a DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if we deploy the external service like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The A record for github.com is returned from the `external-service` query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The CNAME for the external service returns github.com:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Sending traffic to a headless service via a DNS record is possible but inadvisable.
    DNS is a notoriously poor way to load balance, as software takes very different
    (and often simple or unintuitive) approaches to A or AAAA DNS records that return
    multiple IP addresses. For example, it is common for software to always choose
    the first IP address in the response and/or cache and reuse the same IP address
    indefinitely. If you need to be able to send traffic to the service’s DNS address,
    consider a (standard) ClusterIP or LoadBalancer service.
  prefs: []
  type: TYPE_NORMAL
- en: The “correct” way to use a headless service is to query the service’s A/AAAA
    DNS record and use that data in a server-side or client-side load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the services we have been discussing are for internal traffic management
    for the cluster network. In our next sections, will be reviewing how to route
    requests into the cluster with service type LoadBalancer and ingress.
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LoadBalancer service exposes services external to the cluster network. They
    combine the NodePort service behavior with an external integration, such as a
    cloud provider’s load balancer. Notably, LoadBalancer services handle L4 traffic
    (unlike ingress, which handles L7 traffic), so they will work for any TCP or UDP
    service, provided the load balancer selected supports L4 traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration and load balancer options are extremely dependent on the cloud
    provider. For example, some will support `.spec.loadBalancerIP` (with varying
    setup required), and some will ignore it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Once the load balancer has been provisioned, its IP address will be written
    to `.status.loadBalancer.ingress.ip`.
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer services are useful for exposing TCP or UDP services to the outside
    world. Traffic will come into the load balancer on its public IP address and TCP
    port 80, defined by `spec.ports[*].port` and routed to the cluster IP address,
    `10.0.5.1`, and then to container target port 8080, `spec.ports[*].targetPort`.
    Not shown in the example is the `.spec.ports[*].nodePort`; if not specified, Kubernetes
    will pick one for the service.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The service’s `spec.ports[*].targetPort` must match your pod’s container applications
    `spec.container[*].ports.containerPort`, along with the protocol. It’s like missing
    a semicolon in Kubernetes networking otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-6](#loadbalancer), we can see how a LoadBalancer type builds on
    the other service types. The cloud load balancer will determine how to distribute
    traffic; we will discuss that in depth in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![LoadBalancer service](Images/neku_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. LoadBalancer service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s continue to extend our Golang web server example with a LoadBalancer service.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are running on our local machine and not in a service provider like
    AWS, GCP, or Azure, we can use MetalLB as an example for our LoadBalancer service.
    The MetalLB project aims to allow users to deploy bare-metal load balancers for
    their clusters.
  prefs: []
  type: TYPE_NORMAL
- en: This example has been modified from the [KIND example deployment](https://oreil.ly/h8xIt).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step is to deploy a separate namespace for MetalLB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'MetalLB members also require a secret for joining the LoadBalancer cluster;
    let’s deploy one now for them to use in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Now we can deploy MetalLB!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it deploys many objects, and now we wait for the deployment
    to finish. We can monitor the deployment of resources with the `--watch` option
    in the `metallb-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'To complete the configuration, we need to provide MetalLB with a range of IP
    addresses it controls. This range has to be on the Docker KIND network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '`172.18.0.0/16` is our Docker network running locally.'
  prefs: []
  type: TYPE_NORMAL
- en: We want our LoadBalancer IP range to come from this subclass. We can configure
    MetalLB, for instance, to use `172.18.255.200` to `172.18.255.250` by creating
    the ConfigMap.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ConfigMap would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s deploy it so we can use MetalLB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we deploy a load balancer for our web app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'For fun let’s scale the web app deployment to 10, if you have the resources
    for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Now we can test the provisioned load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'With more replicas deployed for our app behind the load balancer, we need the
    external IP of the load balancer, `172.18.255.200`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Since Docker for Mac or Windows does not expose the KIND network to the host,
    we cannot directly reach the `172.18.255.200` LoadBalancer IP on the Docker private
    network.
  prefs: []
  type: TYPE_NORMAL
- en: We can simulate it by attaching a Docker container to the KIND network and cURLing
    the load balancer as a workaround.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you would like to read more about this issue, there is a great [blog post](https://oreil.ly/6rTKJ).
  prefs: []
  type: TYPE_NORMAL
- en: We will use another great networking Docker image called `nicolaka/netshoot`
    to run locally, attach to the KIND Docker network, and send requests to our MetalLB
    load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run it several times, we can see the load balancer is doing its job of
    routing traffic to different pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'With each new request, the metalLB service is sending requests to different
    pods. LoadBalancer, like other services, uses selectors and labels for the pods,
    and we can see that in the `kubectl describe endpoints loadbalancer-service`.
    The pod IP addresses match our results from the cURL commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: It is important to remember that LoadBalancer services require specific integrations
    and will not work without cloud provider support, or manually installed software
    such as MetalLB.
  prefs: []
  type: TYPE_NORMAL
- en: They are not (normally) L7 load balancers, and therefore cannot intelligently
    handle HTTP(S) requests. There is a one-to-one mapping of load balancer to workload,
    which means that all requests sent to that load balancer must be handled by the
    same workload.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While it’s not a network service, it is important to mention the Horizontal
    Pod Autoscaler service, which that will scale pods in a replication controller,
    deployment, ReplicaSet, or StatefulSet based on CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: We can scale our application to the demands of the users, with no need for configuration
    changes on anyone’s part. Kubernetes and the LoadBalancer service take care of
    all of that for developers, systems, and network administrators.
  prefs: []
  type: TYPE_NORMAL
- en: We will see in the next chapter how we can take that even further using cloud
    services for autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: Services Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some troubleshooting tips if issues arise with the endpoints or services:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing the label on the pod allows it to continue to run while also updating
    the endpoint and service. The endpoint controller will remove that unlabeled pod
    from the endpoint objects, and the deployment will deploy another pod; this will
    allow you to troubleshoot issues with that specific unlabeled pod but not adversely
    affect the service to end customers. I’ve used this one countless times during
    development, and we did so in the previous section’s examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two probes that communicate the pod’s health to the Kubelet and the
    rest of the Kubernetes environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also easy to mess up the YAML manifest, so make sure to compare ports
    on the service and pods and make sure they match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed network policies in [Chapter 3](ch03.xhtml#container_networking_basics),
    which can also stop pods from communicating with each other and services. If your
    cluster network is using network policies, ensure that they are set up appropriately
    for application traffic flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also remember to use diagnostic tools like the `dnsutils` pod; the `netshoot`
    pods on the cluster network are helpful debugging tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If endpoints are taking too long to come up in the cluster, there are several
    options that can be configured on the Kubelet to control how fast it responds
    to change in the Kubernetes environment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--kube-api-qps`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sets the query-per-second rate the Kubelet will use when communicating with
    the Kubernetes API server; the default is 5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--kube-api-burst`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Temporarily allows API queries to burst to this number; the default is 10.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--iptables-sync-period`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the maximum interval of how often `iptables` rules are refreshed (e.g.,
    5s, 1m, 2h22m). This must be greater than 0; the default is 30s.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--ipvs-sync-period duration`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the maximum interval of how often IPVS rules are refreshed. This must
    be greater than 0; the efault is 30s.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Increasing these options for larger clusters is recommended, but also remember
    this increases the resources on both the Kubelet and the API server, so keep that
    in mind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tips can help alleviate issues and are good to be aware of as the number
    of services and pods grow in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The various types of services exemplify how powerful the network abstractions
    are in Kubernetes. We have dug deep into how these work for each layer of the
    tool chain. Developers looking to deploy applications to Kubernetes now have the
    knowledge to pick and choose which services are right for their use cases. No
    longer will network administrators have to manually update load balancers with
    IP addresses, with Kubernetes managing that for them.
  prefs: []
  type: TYPE_NORMAL
- en: We have just scratched the surface of what is possible with services. With each
    new version of Kubernetes, there are options to tune and configurations to run
    services. Test each service for your use cases and ensure you are using the appropriate
    services to optimize your applications on the Kubernetes network.
  prefs: []
  type: TYPE_NORMAL
- en: The LoadBalancer service type is the only one that allows for traffic into the
    cluster, exposing HTTP(S) services behind a load balancer for external users to
    connect to. Ingresses support path-based routing, which allows different HTTP
    paths to be served by different services. The next section will discuss ingress
    and how it is an alternative to managing connectivity into the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ingress is a Kubernetes-specific L7 (HTTP) load balancer, which is accessible
    externally, contrasting with L4 ClusterIP service, which is internal to the cluster.
    This is the typical choice for exposing an HTTP(S) workload to external users.
    An ingress can be a single entry point into an API or a microservice-based architecture.
    Traffic can be routed to services based on HTTP information in the request. Ingress
    is a configuration spec (with multiple implementations) for routing HTTP traffic
    to Kubernetes services. [Figure 5-7](#img-ingress) outlines the ingress components.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ingress](Images/neku_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Ingress architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To manage traffic in a cluster with ingress, there are two components required:
    the controller and rules. The controller manages ingress pods, and the rules deployed
    define how the traffic is routed.'
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Controllers and Rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We call ingress implementations ingress *controllers*. In Kubernetes, a controller
    is software that is responsible for managing a typical resource type and making
    reality match the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two general kinds of controllers: external load balancer controllers
    and internal load balancer controllers. External load balancer controllers create
    a load balancer that exists “outside” the cluster, such as a cloud provider product.
    Internal load balancer controllers deploy a load balancer that runs within the
    cluster and do not directly solve the problem of routing consumers to the load
    balancer. There are a myriad of ways that cluster administrators run internal
    load balancers, such as running the load balancer on a subset of special nodes,
    and routing traffic somehow to those nodes. The primary motivation for choosing
    an internal load balancer is cost reduction. An internal load balancer for ingress
    can route traffic for multiple ingress objects, whereas an external load balancer
    controller typically needs one load balancer per ingress. As most cloud providers
    charge by load balancer, it is cheaper to support a single cloud load balancer
    that does fan-out within the cluster, than many cloud load balancers. Note that
    this incurs operational overhead and increased latency and compute costs, so be
    sure the money you’re saving is worth it. Many companies have a bad habit of optimizing
    on inconsequential cloud spend line items.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the spec for an ingress controller. Like LoadBalancer services,
    most of the spec is universal, but various ingress controllers have different
    features and accept different configs. We’ll start with the basics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous example is representative of a typical ingress. It sends traffic
    to `/demo` to one service and all other traffic to another. Ingresses have a “default
    backend” where requests are routed if no rule matches. This can be configured
    in many ingress controllers in the controller configuration itself (e.g., a generic
    404 page), and many support the `.spec.defaultBackend` field. Ingresses support
    multiple ways to specify a path. There are currently three:'
  prefs: []
  type: TYPE_NORMAL
- en: Exact
  prefs: []
  type: TYPE_NORMAL
- en: Matches the specific path and only the given path (including trailing `/` or
    lack thereof).
  prefs: []
  type: TYPE_NORMAL
- en: Prefix
  prefs: []
  type: TYPE_NORMAL
- en: Matches all paths that start with the given path.
  prefs: []
  type: TYPE_NORMAL
- en: ImplementationSpecific
  prefs: []
  type: TYPE_NORMAL
- en: Allows for custom semantics from the current ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: When a request matches multiple paths, the most specific match is chosen. For
    example, if there are rules for `/first` and `/first/second`, any request starting
    with `/first/second` will go to the backend for `/first/second`. If a path matches
    an exact path and a prefix path, the request will go to the backend for the exact
    rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingresses can also use hostnames in rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we serve traffic to `a.example.com` from one service and traffic
    to `b.example.com` from another. This is comparable to virtual hosts in web servers.
    You may want to use host rules to use a single load balancer and IP to serve multiple
    unique domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingresses have basic TLS support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The TLS config references a Kubernetes secret by name, in `.spec.tls.[*].secretName`.
    Ingress controllers expect the TLS certificate and key to be provided in `.data."tls.crt"`
    and `.data."tls.key"` respectively, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you don’t need to manage traditionally issued certificates by hand, you can
    use [cert-manager](https://oreil.ly/qkN0h) to automatically fetch and update certs.
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned earlier that ingress is simply a spec, and drastically different
    implementations exist. It’s possible to use multiple ingress controllers in a
    single cluster, using `IngressClass` settings. An ingress class represents an
    ingress controller, and therefore a specific ingress implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Annotations in Kubernetes must be strings. Because `true` and `false` have distinct
    nonstring meanings, you cannot set an annotation to `true` or `false` without
    quotes. `"true"` and `"false"` are both valid. This is a [long-running bug](https://oreil.ly/76uSI),
    which is often encountered when setting a default priority class.
  prefs: []
  type: TYPE_NORMAL
- en: '`IngressClass` was introduced in Kubernetes 1.18. Prior to 1.18, annotating
    ingresses with `kubernetes.io/ingress.class` was a common convention but relied
    on all installed ingress controllers to support it. Ingresses can pick an ingress
    class by setting the class’s name in `.spec.ingressClassName`.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If more than one ingress class is set as default, Kubernetes will not allow
    you to create an ingress with no ingress class or remove the ingress class from
    an existing ingress. You can use admission control to prevent multiple ingress
    classes from being marked as default.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress only supports HTTP(S) requests, which is insufficient if your service
    uses a different protocol (e.g., most databases use their own protocols). Some
    ingress controllers, such as the NGINX ingress controller, do support TCP and
    UDP, but this is not the norm.
  prefs: []
  type: TYPE_NORMAL
- en: Now on to deploying an ingress controller so we can add ingress rules to our
    Golang web server example.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we deployed our KIND cluster, we had to add several options to allow us
    to deploy an ingress controller:'
  prefs: []
  type: TYPE_NORMAL
- en: extraPortMappings allow the local host to make requests to the ingress controller
    over ports 80/443.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node-labels only allow the ingress controller to run on a specific node(s) matching
    the label selector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many options to choose from with ingress controllers. The Kubernetes
    system does not start or have a default controller like it does with other pieces.
    The Kubernetes community does support AWS, GCE, and Nginx ingress controllers.
    [Table 5-1](#brief_list_of_ingress_controller_options) outlines several options
    for ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Brief list of ingress controller options
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Commercial support | Engine | Protocol support | SSL termination |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ambassador ingress controller | Yes | Envoy | gRPC, HTTP/2, WebSockets |
    Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Community ingress Nginx | No | NGINX | gRPC, HTTP/2, WebSockets | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| NGINX Inc. ingress | Yes | NGINX | HTTP, Websocket, gRPC | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| HAProxy ingress | Yes | HAProxy | gRPC, HTTP/2, WebSockets | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Istio Ingress | No | Envoy | HTTP, HTTPS, gRPC, HTTP/2 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Kong ingress controller for Kubernetes | Yes | Lua on top of Nginx | gRPC,
    HTTP/2 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Traefik Kubernetes ingress | Yes | Traefik | HTTP/2, gRPC, and WebSockets
    | Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Some things to consider when deciding on the ingress for your clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Protocol support: Do you need more than TCP/UDP, for example gRPC integration
    or WebSocket?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Commercial support: Do you need commercial support?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advanced features: Are JWT/oAuth2 authentication or circuit breakers requirements
    for your applications?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'API gateway features: Do you need some API gateway functionalities such as
    rate-limiting?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Traffic distribution: Does your application require support for specialized
    traffic distribution like canary A/B testing or mirroring?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our example, we have chosen to use the Community version of the NGINX ingress
    controller.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For more ingress controllers to choose from, [kubernetes.io](https://oreil.ly/Lzn5q)
    maintains a list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy the NGINX ingress controller into our KIND cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'As with all deployments, we must wait for the controller to be ready before
    we can use it. With the following command, we can verify if our ingress controller
    is ready for use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The controller is deployed to the cluster, and now we’re ready to write ingress
    rules for our application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy ingress rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our YAML manifest defines several ingress rules to use with our Golang web
    server example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'With `describe` we can see all the backends that map to the ClusterIP service
    and the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Our ingress rule is only for the `/host` route and will route requests to our
    `clusterip-service:8080` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test that with cURL to http://localhost/host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Now we can see how powerful ingresses are; let’s deploy a second deployment
    and ClusterIP service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new deployment and service will be used to answer the requests for `/data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now both the `/host` and `/data` work but are going to separate services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Since ingress works on layer 7, there are many more options to route traffic
    with, such as host header and URI path.
  prefs: []
  type: TYPE_NORMAL
- en: For more advanced traffic routing and release patterns, a service mesh is required
    to be deployed in the cluster network. Let’s dig into that next.
  prefs: []
  type: TYPE_NORMAL
- en: Service Meshes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A new cluster with the default options has some limitations. So, let’s get an
    understanding for what those limitations are and how a service mesh can resolve
    some of those limitations. A *service mesh* is an API-driven infrastructure layer
    for handling service-to-service communication.
  prefs: []
  type: TYPE_NORMAL
- en: From a security point of view, all traffic inside the cluster is unencrypted
    between pods, and each application team that runs a service must configure monitoring
    separately for each service. We have discussed the service types, but we have
    not discussed how to update deployments of pods for them. Service meshes support
    more than the basic deployment type; they support rolling updates and re-creations,
    like Canary does. From a developer’s perspective, injecting faults into the network
    is useful, but also not directly supported in default Kubernetes network deployments.
    With service meshes, developers can add fault testing, and instead of just killing
    pods, you can use service meshes to inject delays—again, each application would
    have to build in fault testing or circuit breaking.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several pieces of functionality that a service mesh enhances or provides
    in a default Kubernetes cluster network:'
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery
  prefs: []
  type: TYPE_NORMAL
- en: Instead of relying on DNS for service discovery, the service mesh manages service
    discovery, and removes the need for it to be implemented in each individual application.
  prefs: []
  type: TYPE_NORMAL
- en: Load Balancing
  prefs: []
  type: TYPE_NORMAL
- en: The service mesh adds more advanced load balancing algorithms such as least
    request, consistent hashing, and zone aware.
  prefs: []
  type: TYPE_NORMAL
- en: Communication Resiliency
  prefs: []
  type: TYPE_NORMAL
- en: The service mesh can increase communication resilience for applications by not
    having to implement retries, timeouts, circuit breaking, or rate limiting in application
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs: []
  type: TYPE_NORMAL
- en: 'A service mesh can provide the folllowing: * End-to-end encryption with mTLS
    between services * Authorization policies, which authorize what services can communicate
    with each other, not just at the layer 3 and 4 levels like in Kubernetes network
    polices.'
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs: []
  type: TYPE_NORMAL
- en: Service meshes add in observability by enriching the layer 7 metrics and adding
    tracing and alerting.
  prefs: []
  type: TYPE_NORMAL
- en: Routing Control
  prefs: []
  type: TYPE_NORMAL
- en: Traffic shifting and mirroring in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: API
  prefs: []
  type: TYPE_NORMAL
- en: All of this can be controlled via an API provided by the service mesh implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through several components of a service mesh in [Figure 5-8](#img-service-mesh).
  prefs: []
  type: TYPE_NORMAL
- en: '![Service mesh Components](Images/neku_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Service mesh components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Traffic is handled differently depending on the component or destination of
    traffic. Traffic into and out of the cluster is managed by the gateways. Traffic
    between the frontend, backend, and user service is all encrypted with Mutual TLS
    (mTLS) and is handled by the service mesh. All the traffic to the frontend, backend,
    and user pods in the service mesh is proxied by the sidecar proxy deployed within
    the pods. Even if the control plane is down and updates cannot be made to the
    mesh, the service and application traffic are not affected.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several options to use when deploying a service mesh; here are highlights
    of just a few:'
  prefs: []
  type: TYPE_NORMAL
- en: Istio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses a Go control plane with an Envoy proxy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a Kubernetes-native solution that was initially released by Lyft.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consul
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses HashiCorp Consul as the control plane.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consul Connect uses an agent installed on every node as a DaemonSet, which communicates
    with the Envoy sidecar proxies that handle routing and forwarding of traffic.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS App Mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is an AWS-managed solution that implements its own control plane.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not have mTLS or traffic policy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the Envoy proxy for the data plane.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Linkerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also uses Go for the control plane with the Linkerd proxy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No traffic shifting and no distributed tracing.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a Kubernetes-only solution, which results in fewer moving pieces and means
    that Linkerd has less complexity overall.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is our opinion that the best use case for a service mesh is mTLS between
    services. Other higher-level use cases for developers include circuit breaking
    and fault testing for APIs. For network administrators, advanced routing policies
    and algorithms can be deployed with service meshes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a service mesh example. The first thing you need to do if you
    haven’t already is [install the Linkerd CLI](https://oreil.ly/jVaPm).
  prefs: []
  type: TYPE_NORMAL
- en: 'Your choices are cURL, bash, or brew if you’re on a Mac:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'This preflight checklist will verify that our cluster can run Linkerd:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The Linkerd CLI tool can install Linkerd for us onto our KIND cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: As with the ingress controller and MetalLB, we can see that a lot of components
    are installed in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Linkerd can validate the installation with the `linkerd check` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'It will validate a plethora of checks for the Linkerd install, included but
    not limited to the Kubernetes API version, controllers, pods, and configs to run
    Linkerd, as well as all the services, versions, and APIs needed to run Linkerd:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that everything looks good with our install of Linkerd, we can add our
    application to the service mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Let’s pull up the Linkerd console to investigate what we have just deployed.
    We can start the console with `linkerd dashboard &`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will proxy the console to our local machine available at `http://localhost:50750`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re having issues with reaching the dashboard, you can run `linkerd viz
    check` and find more help in the Linkerd [documentation](https://oreil.ly/MqgAp).
  prefs: []
  type: TYPE_NORMAL
- en: We can see all our deployed objects from the previous exercises in [Figure 5-9](#linkerd-dashboards).
  prefs: []
  type: TYPE_NORMAL
- en: Our ClusterIP service is not part of the Linkerd service mesh. We will need
    to use the proxy injector to add our service to the mesh. It accomplishes this
    by watching for a specific annotation that can be added either with Linkerd `inject`
    or by hand to the pod’s spec.
  prefs: []
  type: TYPE_NORMAL
- en: '![Linkderd Dashboard](Images/neku_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. Linkerd dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s remove some older exercises’ resources for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: We can use the Linkerd CLI to inject the proper annotations into our deployment
    spec, so that will become part of the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to get our application manifest, `cat web.yaml`, and use Linkerd
    to inject the annotations, `linkerd inject -`, then apply them back to the Kubernetes
    API with `kubectl apply -f -`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'If we describe our app deployment, we can see that Linkerd has injected new
    annotations for us, `Annotations: linkerd.io/inject: enabled`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: If we navigate to the app in the dashboard, we can see that our deployment is
    part of the Linkerd service mesh now, as shown in [Figure 5-10](#app-dashboards).
  prefs: []
  type: TYPE_NORMAL
- en: '![App Linkderd Dashboard](Images/neku_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. Web app deployment linkerd dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The CLI can also display our stats for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, let’s scale up our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: In [Figure 5-11](#app-stats), we navigate to the web browser and open [this
    link](https://oreil.ly/qQx9T) so we can watch the stats in real time. Select the
    default namespaces, and in Resources select our deployment/app. Then click “start
    for the web” to start displaying the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a separate terminal let’s use the `netshoot` image, but this time running
    inside our KIND cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '![App Stats Dashboard](Images/neku_0511.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. Web app dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s send a few hundred queries and see the stats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: In our terminal we can see all the liveness and readiness probes as well as
    our `/host` requests.
  prefs: []
  type: TYPE_NORMAL
- en: '`tmp-shell` is our `netshoot` bash terminal with our `for` loop running.'
  prefs: []
  type: TYPE_NORMAL
- en: '`10.244.2.1`, `10.244.3.1`, and `10.244.2.1` are the Kubelets of the hosts
    running our probes for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Our example showed the observability functionality for a service mesh only.
    Linkerd, Istio, and the like have many more options available for developers and
    network administrators to control, monitor, and troubleshoot services running
    inside their cluster network. As with the ingress controller, there are many options
    and features available. It is up to you and your teams to decide what functionality
    and features are important for your networks.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes networking world is feature rich with many options for teams
    to deploy, test, and manage with their Kubernetes cluster. Each new addition will
    add complexity and overhead to the cluster operations. We have given developers,
    network administrators, and system administrators a view into the abstractions
    that Kubernetes offers.
  prefs: []
  type: TYPE_NORMAL
- en: From internal traffic to external traffic to the cluster, teams must choose
    what abstractions work best for their workloads. This is no small task, and now
    you are armed with the knowledge to begin those discussions.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we take our Kubernetes services and network learnings to
    the cloud! We will explore the network services offered by each cloud provider
    and how they are integrated into their Kubernetes managed service offering.
  prefs: []
  type: TYPE_NORMAL
