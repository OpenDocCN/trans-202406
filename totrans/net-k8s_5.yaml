- en: Chapter 5\. Kubernetes Networking Abstractions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章 Kubernetes 网络抽象
- en: Previously, we covered a swath of networking fundamentals and how traffic in
    Kubernetes gets from A to B. In this chapter, we will discuss networking abstractions
    in Kubernetes, primarily service discovery and load balancing. Most notably, this
    is the chapter on services and ingresses. Both resources are notoriously complex,
    due to the large number of options, as they attempt to solve numerous use cases.
    They are the most visible part of the Kubernetes network stack, as they define
    basic network characteristics of workloads on Kubernetes. This is where developers
    interact with the networking stack for their applications deployed on Kubernetes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们涵盖了大量的网络基础知识，以及 Kubernetes 中如何将流量从 A 点传输到 B 点。在本章中，我们将讨论 Kubernetes 中的网络抽象，主要是服务发现和负载均衡。特别值得注意的是，这是关于服务和入口的章节。由于它们试图解决多种用例，这两个资源都因选项繁多而复杂。它们是
    Kubernetes 网络栈中最显著的部分，因为它们定义了在 Kubernetes 上部署的应用程序的基本网络特性。这是开发人员与其应用程序的网络堆栈交互的地方。
- en: 'This chapter will cover fundamental examples of Kubernetes networking abstractions
    and the details on\f how they work. To follow along, you will need the following
    tools:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖 Kubernetes 网络抽象的基本示例及其工作原理的详细信息。为了跟上进度，您需要以下工具：
- en: Docker
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: KIND
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KIND
- en: Linkerd
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: You will need to be familiar with the `kubectl exec` and `Docker exec` commands.
    If you are not, our code repo will have any and all the commands we discuss, so
    don’t worry too much. We will also make use of `ip` and `netns` from Chapters
    [2](ch02.xhtml#linux_networking) and [3](ch03.xhtml#container_networking_basics).
    Note that most of these tools are for debugging and showing implementation details;
    you will not necessarily need them during normal operations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要熟悉`kubectl exec`和`Docker exec`命令。如果你不熟悉，我们的代码仓库会包含我们讨论的所有命令，所以不要太担心。我们还将使用来自第
    [2](ch02.xhtml#linux_networking) 章和第 [3](ch03.xhtml#container_networking_basics)
    章的`ip`和`netns`。请注意，这些工具大多用于调试和显示实现细节；在正常操作期间，您不一定需要它们。
- en: Docker, KIND, and Linkerd installs are available on their respective sites,
    and we’ve provided more information in the book’s code repository as well.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Docker、KIND 和 Linkerd 的安装可以在它们各自的网站上找到，我们还在书籍的代码仓库中提供了更多信息。
- en: Tip
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: '`kubectl` is a key tool in this chapter’s examples, and it’s the standard for
    operators to interact with clusters and their networks. You should be familiar
    with the `kubectl create`, `apply`, `get`, `delete`, and `exec` commands. Learn
    more in the [Kubernetes documentation](https://oreil.ly/H8bTU) or run `kubectl
    [command] --help`.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl` 是本章示例中的一个关键工具，也是操作员与集群及其网络进行交互的标准工具。您应该熟悉`kubectl create`、`apply`、`get`、`delete`和`exec`命令。在[Kubernetes
    文档](https://oreil.ly/H8bTU)中了解更多信息或运行`kubectl [command] --help`。'
- en: 'This chapter will explore these Kubernetes networking abstractions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨这些 Kubernetes 网络抽象：
- en: StatefulSets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态副本集
- en: Endpoints
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终结点
- en: Endpoint slices
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Endpoint slices
- en: Services
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务
- en: NodePort
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodePort
- en: Cluster
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群
- en: Headless
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无头
- en: External
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部
- en: LoadBalancer
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoadBalancer
- en: Ingress
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口
- en: Ingress controller
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口控制器
- en: Ingress rules
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口规则
- en: Service meshes
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务网格
- en: Linkerd
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: 'To explore these abstractions, we will deploy the examples to our Kubernetes
    cluster with the following steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索这些抽象，我们将按照以下步骤将示例部署到我们的 Kubernetes 集群中：
- en: Deploy a KIND cluster with ingress enabled.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用启用了入口的 KIND 集群进行部署。
- en: Explore StatefulSets.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索有状态副本集。
- en: Deploy Kubernetes services.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 Kubernetes 服务。
- en: Deploy an ingress controller.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署一个入口控制器。
- en: Deploy a Linkerd service mesh.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 Linkerd 服务网格。
- en: These abstractions are at the heart of what the Kubernetes API provides to developers
    and administrators to programmatically control the flow of communications into
    and out of the cluster. Understanding and mastering how to deploy these abstractions
    is crucial for the success of any workload inside a cluster. After working through
    these examples, you will understand which abstractions to use in certain situations
    for your applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些抽象是 Kubernetes API 为开发人员和管理员提供的核心内容，用于在集群内程序控制通信流动。理解和掌握如何部署这些抽象对于集群内任何工作负载的成功至关重要。通过这些示例，您将了解在特定情况下为您的应用程序使用哪些抽象。
- en: With the KIND cluster configuration YAML, we can use KIND to create that cluster
    with the command in the next section. If this is the first time running it, it
    will take some time to download all the Docker images for the working and control
    plane Docker images.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KIND集群配置YAML，我们可以使用下一节中的命令使用KIND创建该集群。如果这是第一次运行它，下载工作和控制平面Docker镜像将需要一些时间。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The following examples assume that you still have the local KIND cluster running
    from the previous chapter, along with the Golang web server and the `dnsutils`
    images for testing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例假设您仍然从上一章节运行本地的KIND集群，以及Golang Web服务器和用于测试的`dnsutils`镜像。
- en: StatefulSets
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StatefulSets
- en: 'StatefulSets are a workload abstraction in Kubernetes to manage pods like you
    would a deployment. Unlike a deployment, StatefulSets add the following features
    for applications that require them:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets是Kubernetes中的工作负载抽象，用于像管理部署一样管理Pod。与部署不同，StatefulSets为需要这些功能的应用程序添加以下功能：
- en: Stable, unique network identifiers
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定且唯一的网络标识符
- en: Stable, persistent storage
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定的持久存储
- en: Ordered, graceful deployment and scaling
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序、优雅的部署和扩展
- en: Ordered, automated rolling updates
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序、自动化的滚动更新
- en: The deployment resource is better suited for applications that do not have these
    requirements (for example, a service that stores data in an external database).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 部署资源更适合于不具备这些要求的应用程序（例如，将数据存储在外部数据库的服务）。
- en: Our database for the Golang minimal web server uses a StatefulSet. The database
    has a service, a ConfigMap for the Postgres username, a password, a test database
    name, and a StatefulSet for the containers running Postgres.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Golang最小Web服务器的数据库使用了StatefulSet。数据库具有服务、用于Postgres用户名、密码、测试数据库名称的ConfigMap，以及运行Postgres的容器的StatefulSet。
- en: 'Let’s deploy it now:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署它：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s examine the DNS and network ramifications of using a StatefulSet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析使用StatefulSet时DNS和网络带来的影响。
- en: 'To test DNS inside the cluster, we can use the `dnsutils` image; this image
    is `gcr .io/kubernetes-e2e-test-images/dnsutils:1.3` and is used for Kubernetes
    testing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试集群内的DNS，我们可以使用`dnsutils`镜像；该镜像是`gcr.io/kubernetes-e2e-test-images/dnsutils:1.3`，用于Kubernetes测试：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the replica configured with two pods, we see the StatefulSet deploy `postgres-0`
    and `postgres-1`, in that order, a feature of StatefulSets with IP address 10.244.1.3
    and 10.244.2.3, respectively:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 配置为两个Pod的副本，我们看到StatefulSet依次部署`postgres-0`和`postgres-1`，具有分别为10.244.1.3和10.244.2.3的IP地址：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the name of our headless service, Postgres, that the client can use
    for queries to return the endpoint IP addresses:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的无头服务Postgres的名称，客户端可以用来查询返回端点IP地址：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using our `dnsutils` image, we can see that the DNS names for the StatefulSets
    will return those IP addresses along with the cluster IP address of the Postgres
    service:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的`dnsutils`镜像，我们可以看到StatefulSets的DNS名称将返回这些IP地址以及Postgres服务的集群IP地址：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: StatefulSets attempt to mimic a fixed group of persistent machines. As a generic
    solution for stateful workloads, specific behavior may be frustrating in specific
    use cases.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets试图模拟一组固定的持久化机器。作为有状态工作负载的通用解决方案，特定行为可能在特定用例中令人沮丧。
- en: 'A common problem that users encounter is an update requiring manual intervention
    to fix when using `.spec .updateStrategy.type: RollingUpdate`, and `.spec.podManagementPolicy:
    OrderedReady`, both of which are default settings. With these settings, a user
    must manually intervene if an updated pod never becomes ready.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '用户常遇到的一个问题是，在使用`.spec.updateStrategy.type: RollingUpdate`和`.spec.podManagementPolicy:
    OrderedReady`时，更新需要手动干预。使用这些设置，如果更新后的Pod永远无法准备就绪，用户必须手动干预。'
- en: Also, StatefulSets require a service, preferably headless, to be responsible
    for the network identity of the pods, and end users are responsible for creating
    this service.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，StatefulSets需要一个服务（最好是无头服务），负责管理Pod的网络标识，并且最终用户负责创建此服务。
- en: Statefulsets have many configuration options, and many third-party alternatives
    exist (both generic stateful workload controllers and software-specific workload
    controllers).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets有许多配置选项，并且存在许多第三方替代方案（既通用有状态工作负载控制器，也特定软件的工作负载控制器）。
- en: StatefulSets offer functionality for a specific use case in Kubernetes. They
    should not be used for everyday application deployments. Later in this section,
    we will discuss more appropriate networking abstractions for run-of-the-mill deployments.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we will explore endpoints and endpoint slices, the backbone
    of Kubernetes services.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Endpoints help identify what pods are running for the service it powers. Endpoints
    are created and managed by services. We will discuss services on their own later,
    to avoid covering too many new things at once. For now, let’s just say that a
    service contains a standard label selector (introduced in [Chapter 4](ch04.xhtml#kubernetes_networking_introduction)),
    which defines which pods are in the endpoints.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-1](#img-endpoints), we can see traffic being directed to an endpoint
    on node 2, pod 5.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes Endpoints](Images/neku_0501.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Endpoints in a service
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s discuss how this endpoint is created and maintained in the cluster.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Each endpoint contains a list of ports (which apply to all pods) and two lists
    of addresses: ready and unready:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Addresses are listed in `.addresses` if they are passing pod readiness checks.
    Addresses are listed in `.notReadyAddresses` if they are not. This makes endpoints
    a *service discovery* tool, where you can watch an `Endpoints` object to see the
    health and addresses of all pods:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can get a better view of all the addresses with `kubectl describe`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s remove the app label and see how Kubernetes responds. In a separate terminal,
    run this command. This will allow us to see changes to the pods in real time:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In another separate terminal, let’s do the same thing with endpoints:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now need to get a pod name to remove from the `Endpoints` object:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With `kubectl label`, we can alter the pod’s `app-5586fc9d77-7frts` `app=app`
    label:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Both watch commands on endpoints and pods will see some changes for the same
    reason: removal of the label on the pod. The endpoint controller will notice a
    change to the pods with the label `app=app` and so did the deployment controller.
    So Kubernetes did what Kubernetes does: it made the real state reflect the desired
    state:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The deployment has four pods, but our relabeled pod still exists: `app-5586fc9d77-7frts`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The pod `app-5586fc9d77-6dcg2` now is part of the deployment and endpoint object
    with IP address `10.244.1.6`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As always, we can see the full picture of details with `kubectl describe`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For large deployments, that endpoint object can become very large, so much so
    that it can actually slow down changes in the cluster. To solve that issue, the
    Kubernetes maintainers have come up with endpoint slices.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Endpoint Slices
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be asking, how are they different from endpoints? This is where we *really*
    start to get into the weeds of Kubernetes networking.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: In a typical cluster, Kubernetes runs `kube-proxy` on every node. `kube-proxy`
    is responsible for the per-node portions of making services work, by handling
    routing and *outbound* load balancing to all the pods in a service. To do that,
    `kube-proxy` watches all endpoints in the cluster so it knows all applicable pods
    that all services should route to.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine we have a *big* cluster, with thousands of nodes, and tens of thousands
    of pods. That means thousands of kube-proxies are watching endpoints. When an
    address changes in an `Endpoints` object (say, from a rolling update, scale up,
    eviction, health-check failure, or any number of reasons), the updated `Endpoints`
    object is pushed to all listening kube-proxies. It is made worse by the number
    of pods, since more pods means larger `Endpoints` objects, and more frequent changes.
    This eventually becomes a strain on `etcd`, the Kubernetes API server, and the
    network itself. Kubernetes scaling limits are complex and depend on specific criteria,
    but endpoints watching is a common problem in clusters that have thousands of
    nodes. Anecdotally, many Kubernetes users consider endpoint watches to be the
    ultimate bottleneck of cluster size.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: This problem is a function of `kube-proxy`’s design and the expectation that
    any pod should be immediately able to route to any service with no notice. Endpoint
    slices are an approach that allows `kube-proxy`’s fundamental design to continue,
    while drastically reducing the watch bottleneck in large clusters where large
    services are used.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Endpoint slices have similar contents to `Endpoints` objects but also include
    an array of endpoints:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The meaningful difference between endpoints and endpoint slices is not the schema,
    but how Kubernetes treats them. With “regular” endpoints, a Kubernetes service
    creates one endpoint for all pods in the service. A service creates *multiple*
    endpoint slices, each containing a *subset* of pods; [Figure 5-2](#img-endpointslice)
    depicts this subset. The union of all endpoint slices for a service contains all
    pods in the service. This way, an IP address change (due to a new pod, a deleted
    pod, or a pod’s health changing) will result in a much smaller data transfer to
    watchers. Because Kubernetes doesn’t have a transactional API, the same address
    may appear temporarily in multiple slices. Any code consuming endpoint slices
    (such as `kube-proxy`) must be able to account for this.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of addresses in an endpoint slice is set using the `--max-endpoints-per-slice`
    `kube-controller-manager` flag. The current default is 100, and the maximum is
    1000. The endpoint slice controller attempts to fill existing endpoint slices
    before creating new ones, but does not rebalance endpoint slice.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'The endpoint slice controller mirrors endpoints to endpoint slice, to allow
    systems to continue writing endpoints while treating endpoint slice as the source
    of truth. The exact future of this behavior, and endpoints in general, has not
    been finalized (however, as a v1 resource, endpoints would be sunset with substantial
    notice). There are four exceptions that will prevent mirroring:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 端点片段控制器将端点镜像到端点片段，以允许系统在将端点视为事实来源的同时继续编写端点。这种行为的确切未来以及端点总体的未来尚未最终确定（但作为v1资源，端点将在大幅通知后被废弃）。有四个例外情况会阻止镜像：
- en: There is no corresponding service.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有对应的服务。
- en: The corresponding service resource selects pods.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应的服务资源选择Pod。
- en: 'The `Endpoints` object has the label `endpointslice.kubernetes.io/skip-mirror:
    true`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Endpoints`对象具有标签`endpointslice.kubernetes.io/skip-mirror: true`。'
- en: The `Endpoints` object has the annotation `control-⁠⁠plane.alpha.kubernetes​​.io/leader`.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Endpoints`对象具有注释`control-⁠⁠plane.alpha.kubernetes​​.io/leader`。'
- en: '![EndpointsVSliice](Images/neku_0502.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![EndpointsVSliice](Images/neku_0502.png)'
- en: Figure 5-2\. `Endpoints` versus `EndpointSlice` objects
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. `Endpoints`与`EndpointSlice`对象
- en: You can fetch all endpoint slices for a specific service by fetching endpoint
    slices filtered to the desired name in `.metadata.labels."kubernetes.io/service-name"`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过获取以`.metadata.labels."kubernetes.io/service-name"`中的所需名称为过滤条件的端点片段来获取特定服务的所有端点片段。
- en: Warning
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Endpoint slices have been in beta state since Kubernetes 1.17. This is still
    the case in Kubernetes 1.20, the current version at the time of writing. Beta
    resources typically don’t see major changes, and eventually graduate to stable
    APIs, but that is not guaranteed. If you directly use endpoint slices, be aware
    that a future Kubernetes release may make a breaking change without much warning,
    or the behaviors described here may change.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 自Kubernetes 1.17以来，端点片段一直处于Beta状态。在撰写本文时，Kubernetes 1.20仍然如此。Beta资源通常不会有重大更改，并最终升级为稳定的API，但这并不保证。如果直接使用端点片段，请注意未来的Kubernetes版本可能会进行重大更改而没有提前警告，或者这里描述的行为可能会发生变化。
- en: 'Let’s see some endpoints running in the cluster with `kubectl get endpointslice`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中运行的一些端点可以通过`kubectl get endpointslice`来查看：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we want more detail about the endpoint slices `clusterip-service-l2n9q`,
    we can use `kubectl describe` on it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要关于端点片段`clusterip-service-l2n9q`的更多细节，可以使用`kubectl describe`来查看它：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the output, we see the pod powering the endpoint slice from `TargetRef`.
    The `Topology` information gives us the hostname of the worker node that the pod
    is deployed to. Most importantly, the `Addresses` returns the IP address of the
    endpoint object.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们可以看到通过`TargetRef`支持端点片段的Pod。`Topology`信息为我们提供了Pod部署到的工作节点的主机名。最重要的是，`Addresses`返回端点对象的IP地址。
- en: Endpoints and endpoint slices are important to understand because they identify
    the pods responsible for the services, no matter the type deployed. Later in the
    chapter, we’ll review how to use endpoints and labels for troubleshooting. Next,
    we will investigate all the Kubernetes service types.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 理解端点和端点片段很重要，因为它们标识了负责服务的Pod，无论部署的类型如何。在本章后面，我们将详细讨论如何使用端点和标签进行故障排除。接下来，我们将调查所有的Kubernetes服务类型。
- en: Kubernetes Services
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes服务
- en: 'A service in Kubernetes is a load balancing abstraction within a cluster. There
    are four types of services, specified by the `.spec.Type` field. Each type offers
    a different form of load balancing or discovery, which we will cover individually.
    The four types are: ClusterIP, NodePort, LoadBalancer, and ExternalName.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的服务是集群内的负载均衡抽象。由`.spec.Type`字段指定四种类型的服务。每种类型提供不同形式的负载均衡或发现，我们将分别介绍这四种类型。这四种类型是：ClusterIP、NodePort、LoadBalancer和ExternalName。
- en: 'Services use a standard pod selector to match pods. The service includes all
    matching pods. Services create an endpoint (or endpoint slice) to handle pod discovery:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 服务使用标准的Pod选择器来匹配Pod。服务包含所有匹配的Pod。服务创建一个端点（或端点片段）来处理Pod的发现：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will use the Golang minimal web server for all the service examples. We have
    added functionality to the application to display the host and pod IP addresses
    in the REST request.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有的服务示例，我们将使用Golang最小化Web服务器。我们已经为应用程序添加了功能，在REST请求中显示主机和Pod IP地址。
- en: '[Figure 5-3](#img-pod-connection) outlines our pod networking status as a single
    pod in a cluster. The networking objects we are about to explore will expose our
    app pods outside the cluster in some instances and in others allow us to scale
    our application to meet demand. Recall from Chapters [3](ch03.xhtml#container_networking_basics)
    and [4](ch04.xhtml#kubernetes_networking_introduction) that containers running
    inside pods share a network namespace. In addition, there is also a pause container
    that is created for each pod. The pause container manages the namespaces for the
    pod.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 5-3](#img-pod-connection)概述了我们作为集群中单个 Pod 的网络状态。我们即将探讨的网络对象将在某些情况下将我们的应用
    Pod 暴露到集群外部，在其他情况下，允许我们扩展应用程序以满足需求。回顾第 [3](ch03.xhtml#container_networking_basics)
    和第 [4](ch04.xhtml#kubernetes_networking_introduction) 章节中的内容，运行在 Pod 内部的容器共享网络命名空间。此外，每个
    Pod 都会创建一个暂停容器来管理命名空间。'
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The pause container is the parent container for all running containers in the
    pod. It holds and shares all the namespaces for the pod. You can read more about
    the pause container in Ian Lewis’ [blog post](https://oreil.ly/n51eq).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 暂停容器是 Pod 内所有运行容器的父容器。它持有并共享 Pod 的所有命名空间。您可以在 Ian Lewis 的[博文](https://oreil.ly/n51eq)中详细了解有关暂停容器的信息。
- en: '![Pod on Host](Images/neku_0503.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Pod on Host](Images/neku_0503.png)'
- en: Figure 5-3\. Pod on host
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 5-3 图。主机上的 Pod
- en: 'Before we deploy the services, we must first deploy the web server that the
    services will be routing traffic to, if we have not already:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署服务之前，我们必须首先部署 Web 服务器，这些服务将路由流量到它，如果我们尚未部署：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s look at each type of service starting with NodePort.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 NodePort 开始看每种类型的服务。
- en: NodePort
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodePort
- en: 'A NodePort service provides a simple way for external software, such as a load
    balancer, to route traffic to the pods. The software only needs to be aware of
    node IP addresses, and the service’s port(s). A NodePort service exposes a fixed
    port on all nodes, which routes to applicable pods. A NodePort service uses the
    `.spec.ports.[].nodePort` field to specify the port to open on all nodes, for
    the corresponding port on pods:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 服务为外部软件（如负载均衡器）提供了一种简单的方式来将流量路由到 Pod。该软件只需知道节点 IP 地址和服务的端口。NodePort
    服务在所有节点上公开一个固定端口，该端口将流量路由到适用的 Pod。NodePort 服务使用`.spec.ports.[].nodePort`字段指定要在所有节点上打开的端口，用于对应
    Pod 上的端口：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `nodePort` field can be left blank, in which case Kubernetes automatically
    selects a unique port. The `--service-node-port-range` flag in `kube-controller-manager`
    sets the valid range for ports, 30000–32767. Manually specified ports must be
    within this range.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `nodePort` 字段为空，则 Kubernetes 会自动选择一个唯一的端口。`kube-controller-manager` 中的 `--service-node-port-range`
    标志设置端口的有效范围为 30000–32767。手动指定的端口必须在此范围内。
- en: Using a NodePort service, external users can connect to the nodeport on any
    node and be routed to a pod on a node that has a pod backing that service; [Figure 5-4](#img-node-port-traffic)
    demonstrates this. The service directs traffic to node 3, and `iptables` rules
    forward the traffic to node 2 hosting the pod. This is a bit inefficient, as a
    typical connection will be routed to a pod on another node.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NodePort 服务，外部用户可以连接到任何节点上的 nodeport，并被路由到托管该服务的 Pod 的节点；[Figure 5-4](#img-node-port-traffic)展示了这一点。服务将流量定向到节点
    3，`iptables` 规则将流量转发到托管 Pod 的节点 2。这有点低效，因为典型的连接将被路由到另一个节点上的 Pod。
- en: '![Node Port Traffic Flow](Images/neku_0504.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Node Port Traffic Flow](Images/neku_0504.png)'
- en: Figure 5-4\. NodePort traffic flow
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 5-4 图。NodePort 流量流向
- en: '[Figure 5-4](#img-node-port-traffic) requires us to discuss an attribute of
    services, externalTrafficPolicy. ExternalTrafficPolicy indicates how a service
    will route external traffic to either node-local or cluster-wide endpoints. `Local`
    preserves the client source IP and avoids a second hop for LoadBalancer and NodePort
    type services but risks potentially imbalanced traffic spreading. Cluster obscures
    the client source IP and may cause a second hop to another node but should have
    good overall load-spreading. A `Cluster` value means that for each worker node,
    the `kube-proxy iptable` rules are set up to route the traffic to the pods backing
    the service anywhere in the cluster, just like we have shown in [Figure 5-4](#img-node-port-traffic).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: A `Local` value means the `kube-proxy iptable` rules are set up only on the
    worker nodes with relevant pods running to route the traffic local to the worker
    node. Using `Local` also allows application developers to preserve the source
    IP of the user request. If you set externalTrafficPolicy to the value `Local`,
    `kube-proxy` will proxy requests only to node-local endpoints and will not forward
    traffic to other nodes. If there are no local endpoints, packets sent to the node
    are dropped.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s scale up the deployment of our web app for some more testing:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With four pods running, we will have one pod at every node in the cluster:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now let’s deploy our NodePort service:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To test the NodePort service, we must retrieve the IP address of a worker node:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Communication external to the cluster will use a `NodePort` value of 30040 opened
    on each worker and the node worker’s IP address.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that our pods are reachable on each host in the cluster:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It’s important to consider the limitations as well. A NodePort deployment will
    fail if it cannot allocate the requested port. Also, ports must be tracked across
    all applications using a NodePort service. Using manually selected ports raises
    the issue of port collisions (especially when applying a workload to multiple
    clusters, which may not have the same NodePorts free).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Another downside of using the NodePort service type is that the load balancer
    or client software must be aware of the node IP addresses. A static configuration
    (e.g., an operator manually copying node IP addresses) may become too outdated
    over time (especially on a cloud provider) as IP addresses change or nodes are
    replaced. A reliable system automatically populates node IP addresses, either
    by watching which machines have been allocated to the cluster or by listing nodes
    from the Kubernetes API itself.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: NodePorts are the earliest form of services. We will see that other service
    types use NodePorts as a base structure in their architecture. NodePorts should
    not be used by themselves, as clients would need to know the IP addresses of hosts
    and the node for connection requests. We will see how NodePorts are used to enable
    load balancers later in the chapter when we discuss cloud networks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Next up is the default type for services, ClusterIP.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IP addresses of pods share the life cycle of the pod and thus are not reliable
    for clients to use for requests. Services help overcome this pod networking design.
    A ClusterIP service provides an internal load balancer with a single IP address
    that maps to all matching (and ready) pods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The service’s IP address must be within the CIDR set in `service-cluster-ip-range`,
    in the API server. You can specify a valid IP address manually, or leave `.spec.clusterIP`
    unset to have one assigned automatically. The ClusterIP service address is a virtual
    IP address that is routable only internally.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-proxy` is responsible for making the ClusterIP service address route
    to all applicable pods. In “normal” configurations, `kube-proxy` performs L4 load
    balancing, which may not be sufficient. For example, older pods may see more load,
    due to accumulating more long-lived connections from clients. Or, a few clients
    making many requests may cause the load to be distributed unevenly.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: A particular use case example for ClusterIP is when a workload requires a load
    balancer within the same cluster.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-5](#img-clusterip), we can see a ClusterIP service deployed. The
    service name is App with a selector, or App=App1. There are two pods powering
    this service. Pod 1 and Pod 5 match the selector for the service.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster IP](Images/neku_0505.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Cluster IP example service
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s dig into an example on the command line with our KIND cluster.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'We will deploy a ClusterIP service for use with our Golang web server:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The ClusterIP service name is resolvable in the network:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we can reach the host API endpoint with the Cluster IP address `10.98.252.195`,
    with the service name `clusterip-service`; or directly with the pod IP address
    `10.244.1.4` and port 8080:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The ClusterIP service is the default type for services. With that default status,
    it is warranted that we should explore what the ClusterIP service abstracted for
    us. If you recall from Chapters [2](ch02.xhtml#linux_networking) and [3](ch03.xhtml#container_networking_basics),
    this list is similar to what is set up with the Docker network, but we now also
    have `iptables` for the service across all nodes:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: View the VETH pair and match with the pod.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: View the network namespace and match with the pod.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the PIDs on the node and match the pods.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Match services with `iptables` rules.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To explore this, we need to know what worker node the pod is deployed to, and
    that is `kind-worker2`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The container IDs and names will be different for you.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are using KIND, we can use `docker ps` and `docker exec` to get information
    out of the running worker node `kind-worker-2`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `kind-worker2` container ID is `df6df0736958`; KIND was *kind* enough to
    label each container with names, so we can reference each worker node with its
    name `kind-worker2`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the IP address and route table information of our pod, `app-9cc7d9df8-ffsm6`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Our pod’s IP address is `10.244.1.4` running on interface `eth0@if5` with `10.244.1.1`
    as its default route. That matches interface 5 on the pod `veth45d1f3e8@if5`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s check the network namespace as well, from the `node ip a` output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`netns list` confirms that the network namespaces match our pods, interface
    to the host interface, `cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s see what processes run inside that network namespace. For that we will
    use `docker exec` to run commands inside the node `kind-worker2` hosting the pod
    and its network namespace:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we can `grep` for each process ID and inspect what they are doing:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`4737` is the process ID of our web server container running on `kind-worker2`.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '`4687` is our pause container holding onto all our namespaces.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see what will happen to the `iptables` on the worker node:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: That is a lot of tables being managed by Kubernetes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'We can dive a little deeper to examine the `iptables` responsible for the services
    we deployed. Let’s retrieve the IP address of the `clusterip-service` deployed.
    We need this to find the matching `iptables` rules:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now use the clusterIP of the service, `10.98.252.195`, to find our `iptables`
    rule:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'List all the rules on the chain `KUBE-SVC-V7R3EVKW3DT43QQM`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `KUBE-SEP-` will contain the endpoints for the services, `KUBE-SEP-THJR2P3Q4C2QAEPT`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can see what the rules for this chain are in `iptables`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`10.244.1.4:8080` is one of the service endpoints, aka a pod backing the service,
    which is confirmed with the output of `kubectl get ep clusterip-service`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now, let’s explore the limitations of the ClusterIP service. The ClusterIP service
    is for internal traffic to the cluster, and it suffers the same issues as endpoints
    do. As the service size grows, updates to it will slow. In [Chapter 2](ch02.xhtml#linux_networking),
    we discussed how to mitigate that by using IPVS over `iptables` as the proxy mode
    for `kube-proxy`. We will discuss later in this chapter how to get traffic into
    the cluster using ingress and the other service type LoadBalancer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP is the default type of service, but there are several other specific
    types of services such as headless and ExternalName. ExternalName is a specific
    type of services that helps with reaching services outside the cluster. We briefly
    touched on headless services with StatefulSets, but let’s review those services
    in depth now.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Headless
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A headless service isn’t a formal type of service (i.e., there is no `.spec.type:
    Headless`). A headless service is a service with `.spec.clusterIP: "None"`. This
    is distinct from merely *not setting* a cluster IP address, which makes Kubernetes
    automatically assign a cluster IP address.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: When ClusterIP is set to None, the service does not support any load balancing
    functionality. Instead, it only provisions an `Endpoints` object and points the
    service DNS record at all pods that are selected and ready.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: A headless service provides a generic way to watch endpoints, without needing
    to interact with the Kubernetes API. Fetching DNS records is much simpler than
    integrating with the Kubernetes API, and it may not be possible with third-party
    software.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务提供了一种通用的方式来监视端点，无需与Kubernetes API交互。获取DNS记录比与Kubernetes API集成简单得多，对于第三方软件可能无法实现。
- en: 'Headless services allow developers to deploy multiple copies of a pod in a
    deployment. Instead of a single IP address returned, like with the ClusterIP service,
    all the IP addresses of the endpoint are returned in the query. It then is up
    to the client to pick which one to use. To see this in action, let’s scale up
    the deployment of our web app:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务允许开发人员在部署中部署多个Pod的副本。与ClusterIP服务返回单个IP地址不同，查询返回所有端点的IP地址。然后由客户端选择使用哪个。为了看到这一点，请扩展我们Web应用程序的部署：
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now let’s deploy the headless service:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署无头服务：
- en: '[PRE45]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The DNS query will return all four of the pod IP addresses. Using our `dnsutils`
    image, we can verify that is the case:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: DNS查询将返回所有四个Pod IP地址。使用我们的`dnsutils`镜像，我们可以验证这一点：
- en: '[PRE46]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The IP addresses returned from the query also match the endpoints for the service.
    Using `kubectl describe` for the endpoint confirms that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 查询返回的IP地址也与服务的端点匹配。使用`kubectl describe`确认了端点的情况：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Headless has a specific use case and is not typically used for deployments.
    As we mentioned in [“StatefulSets”](#statefulsets), if developers need to let
    the client decide which endpoint to use, headless is the appropriate type of service
    to deploy. Two examples of headless services are clustered databases and applications
    that have client-side load-balancing logic built into the code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务有一个特定的用例，通常不用于部署。正如我们在[“StatefulSets”](#statefulsets)中提到的，如果开发人员需要让客户端决定使用哪个端点，则无头是部署的适当服务类型。无头服务的两个示例是集群数据库和在代码中构建了客户端负载均衡逻辑的应用程序。
- en: Our next example is ExternalName, which aids in migrations of services external
    to the cluster. It also offers other DNS advantages inside cluster DNS.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个示例是ExternalName，它有助于迁移到集群外的服务。它还在集群DNS内部提供其他DNS优势。
- en: ExternalName Service
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ExternalName服务
- en: ExternalName is a special type of service that does not have selectors and uses
    DNS names instead.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName是一种特殊类型的服务，它没有选择器，而是使用DNS名称。
- en: 'When looking up the host `ext-service.default.svc.cluster.local`, the cluster
    DNS service returns a CNAME record of `database.mycompany.com`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当查找主机`ext-service.default.svc.cluster.local`时，集群DNS服务返回了`database.mycompany.com`的CNAME记录：
- en: '[PRE48]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: If developers are migrating an application into Kubernetes but its dependencies
    are staying external to the cluster, ExternalName service allows them to define
    a DNS record internal to the cluster no matter where the service actually runs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果开发人员将应用程序迁移到Kubernetes，但其依赖项保留在集群外部，ExternalName服务允许他们定义一个内部集群的DNS记录，无论服务实际运行在哪里。
- en: 'DNS will try the search as shown in the following example:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: DNS将尝试如下示例中显示的搜索：
- en: '[PRE49]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As an example, the ExternalName service allows developers to map a service to
    a DNS name.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ExternalName服务允许开发人员将一个服务映射到一个DNS名称。
- en: 'Now if we deploy the external service like so:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们这样部署外部服务：
- en: '[PRE50]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The A record for github.com is returned from the `external-service` query:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: github.com的A记录从`external-service`查询返回：
- en: '[PRE51]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The CNAME for the external service returns github.com:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 外部服务的CNAME返回github.com：
- en: '[PRE52]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Sending traffic to a headless service via a DNS record is possible but inadvisable.
    DNS is a notoriously poor way to load balance, as software takes very different
    (and often simple or unintuitive) approaches to A or AAAA DNS records that return
    multiple IP addresses. For example, it is common for software to always choose
    the first IP address in the response and/or cache and reuse the same IP address
    indefinitely. If you need to be able to send traffic to the service’s DNS address,
    consider a (standard) ClusterIP or LoadBalancer service.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过DNS记录发送流量到无头服务是可能的，但不建议这样做。DNS作为负载均衡的方式并不理想，因为软件在处理返回多个IP地址的A或AAAA DNS记录时采取的方法很不同（通常是简单或不直观的方法），例如，软件通常会选择响应中的第一个IP地址并/或者缓存并重复使用同一个IP地址。如果需要能够发送流量到服务的DNS地址，请考虑（标准的）ClusterIP或LoadBalancer服务。
- en: The “correct” way to use a headless service is to query the service’s A/AAAA
    DNS record and use that data in a server-side or client-side load balancer.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无头服务的“正确”方法是查询服务的A/AAAA DNS记录，并在服务器端或客户端负载均衡器中使用该数据。
- en: Most of the services we have been discussing are for internal traffic management
    for the cluster network. In our next sections, will be reviewing how to route
    requests into the cluster with service type LoadBalancer and ingress.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LoadBalancer service exposes services external to the cluster network. They
    combine the NodePort service behavior with an external integration, such as a
    cloud provider’s load balancer. Notably, LoadBalancer services handle L4 traffic
    (unlike ingress, which handles L7 traffic), so they will work for any TCP or UDP
    service, provided the load balancer selected supports L4 traffic.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration and load balancer options are extremely dependent on the cloud
    provider. For example, some will support `.spec.loadBalancerIP` (with varying
    setup required), and some will ignore it:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Once the load balancer has been provisioned, its IP address will be written
    to `.status.loadBalancer.ingress.ip`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer services are useful for exposing TCP or UDP services to the outside
    world. Traffic will come into the load balancer on its public IP address and TCP
    port 80, defined by `spec.ports[*].port` and routed to the cluster IP address,
    `10.0.5.1`, and then to container target port 8080, `spec.ports[*].targetPort`.
    Not shown in the example is the `.spec.ports[*].nodePort`; if not specified, Kubernetes
    will pick one for the service.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The service’s `spec.ports[*].targetPort` must match your pod’s container applications
    `spec.container[*].ports.containerPort`, along with the protocol. It’s like missing
    a semicolon in Kubernetes networking otherwise.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-6](#loadbalancer), we can see how a LoadBalancer type builds on
    the other service types. The cloud load balancer will determine how to distribute
    traffic; we will discuss that in depth in the next chapter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![LoadBalancer service](Images/neku_0506.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. LoadBalancer service
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s continue to extend our Golang web server example with a LoadBalancer service.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Since we are running on our local machine and not in a service provider like
    AWS, GCP, or Azure, we can use MetalLB as an example for our LoadBalancer service.
    The MetalLB project aims to allow users to deploy bare-metal load balancers for
    their clusters.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: This example has been modified from the [KIND example deployment](https://oreil.ly/h8xIt).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step is to deploy a separate namespace for MetalLB:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'MetalLB members also require a secret for joining the LoadBalancer cluster;
    let’s deploy one now for them to use in our cluster:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now we can deploy MetalLB!
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'As you can see, it deploys many objects, and now we wait for the deployment
    to finish. We can monitor the deployment of resources with the `--watch` option
    in the `metallb-system` namespace:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To complete the configuration, we need to provide MetalLB with a range of IP
    addresses it controls. This range has to be on the Docker KIND network:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`172.18.0.0/16` is our Docker network running locally.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: We want our LoadBalancer IP range to come from this subclass. We can configure
    MetalLB, for instance, to use `172.18.255.200` to `172.18.255.250` by creating
    the ConfigMap.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The ConfigMap would look like this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let’s deploy it so we can use MetalLB:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now we deploy a load balancer for our web app:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'For fun let’s scale the web app deployment to 10, if you have the resources
    for it:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now we can test the provisioned load balancer.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'With more replicas deployed for our app behind the load balancer, we need the
    external IP of the load balancer, `172.18.255.200`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Since Docker for Mac or Windows does not expose the KIND network to the host,
    we cannot directly reach the `172.18.255.200` LoadBalancer IP on the Docker private
    network.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: We can simulate it by attaching a Docker container to the KIND network and cURLing
    the load balancer as a workaround.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you would like to read more about this issue, there is a great [blog post](https://oreil.ly/6rTKJ).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: We will use another great networking Docker image called `nicolaka/netshoot`
    to run locally, attach to the KIND Docker network, and send requests to our MetalLB
    load balancer.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run it several times, we can see the load balancer is doing its job of
    routing traffic to different pods:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'With each new request, the metalLB service is sending requests to different
    pods. LoadBalancer, like other services, uses selectors and labels for the pods,
    and we can see that in the `kubectl describe endpoints loadbalancer-service`.
    The pod IP addresses match our results from the cURL commands:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: It is important to remember that LoadBalancer services require specific integrations
    and will not work without cloud provider support, or manually installed software
    such as MetalLB.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: They are not (normally) L7 load balancers, and therefore cannot intelligently
    handle HTTP(S) requests. There is a one-to-one mapping of load balancer to workload,
    which means that all requests sent to that load balancer must be handled by the
    same workload.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While it’s not a network service, it is important to mention the Horizontal
    Pod Autoscaler service, which that will scale pods in a replication controller,
    deployment, ReplicaSet, or StatefulSet based on CPU utilization.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: We can scale our application to the demands of the users, with no need for configuration
    changes on anyone’s part. Kubernetes and the LoadBalancer service take care of
    all of that for developers, systems, and network administrators.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: We will see in the next chapter how we can take that even further using cloud
    services for autoscaling.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Services Conclusion
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some troubleshooting tips if issues arise with the endpoints or services:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Removing the label on the pod allows it to continue to run while also updating
    the endpoint and service. The endpoint controller will remove that unlabeled pod
    from the endpoint objects, and the deployment will deploy another pod; this will
    allow you to troubleshoot issues with that specific unlabeled pod but not adversely
    affect the service to end customers. I’ve used this one countless times during
    development, and we did so in the previous section’s examples.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two probes that communicate the pod’s health to the Kubelet and the
    rest of the Kubernetes environment.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also easy to mess up the YAML manifest, so make sure to compare ports
    on the service and pods and make sure they match.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed network policies in [Chapter 3](ch03.xhtml#container_networking_basics),
    which can also stop pods from communicating with each other and services. If your
    cluster network is using network policies, ensure that they are set up appropriately
    for application traffic flow.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also remember to use diagnostic tools like the `dnsutils` pod; the `netshoot`
    pods on the cluster network are helpful debugging tools.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If endpoints are taking too long to come up in the cluster, there are several
    options that can be configured on the Kubelet to control how fast it responds
    to change in the Kubernetes environment:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--kube-api-qps`'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sets the query-per-second rate the Kubelet will use when communicating with
    the Kubernetes API server; the default is 5.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--kube-api-burst`'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Temporarily allows API queries to burst to this number; the default is 10.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--iptables-sync-period`'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the maximum interval of how often `iptables` rules are refreshed (e.g.,
    5s, 1m, 2h22m). This must be greater than 0; the default is 30s.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--ipvs-sync-period duration`'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the maximum interval of how often IPVS rules are refreshed. This must
    be greater than 0; the efault is 30s.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Increasing these options for larger clusters is recommended, but also remember
    this increases the resources on both the Kubelet and the API server, so keep that
    in mind.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tips can help alleviate issues and are good to be aware of as the number
    of services and pods grow in the cluster.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: The various types of services exemplify how powerful the network abstractions
    are in Kubernetes. We have dug deep into how these work for each layer of the
    tool chain. Developers looking to deploy applications to Kubernetes now have the
    knowledge to pick and choose which services are right for their use cases. No
    longer will network administrators have to manually update load balancers with
    IP addresses, with Kubernetes managing that for them.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: We have just scratched the surface of what is possible with services. With each
    new version of Kubernetes, there are options to tune and configurations to run
    services. Test each service for your use cases and ensure you are using the appropriate
    services to optimize your applications on the Kubernetes network.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The LoadBalancer service type is the only one that allows for traffic into the
    cluster, exposing HTTP(S) services behind a load balancer for external users to
    connect to. Ingresses support path-based routing, which allows different HTTP
    paths to be served by different services. The next section will discuss ingress
    and how it is an alternative to managing connectivity into the cluster resources.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ingress is a Kubernetes-specific L7 (HTTP) load balancer, which is accessible
    externally, contrasting with L4 ClusterIP service, which is internal to the cluster.
    This is the typical choice for exposing an HTTP(S) workload to external users.
    An ingress can be a single entry point into an API or a microservice-based architecture.
    Traffic can be routed to services based on HTTP information in the request. Ingress
    is a configuration spec (with multiple implementations) for routing HTTP traffic
    to Kubernetes services. [Figure 5-7](#img-ingress) outlines the ingress components.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![Ingress](Images/neku_0507.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Ingress architecture
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To manage traffic in a cluster with ingress, there are two components required:
    the controller and rules. The controller manages ingress pods, and the rules deployed
    define how the traffic is routed.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Controllers and Rules
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We call ingress implementations ingress *controllers*. In Kubernetes, a controller
    is software that is responsible for managing a typical resource type and making
    reality match the desired state.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two general kinds of controllers: external load balancer controllers
    and internal load balancer controllers. External load balancer controllers create
    a load balancer that exists “outside” the cluster, such as a cloud provider product.
    Internal load balancer controllers deploy a load balancer that runs within the
    cluster and do not directly solve the problem of routing consumers to the load
    balancer. There are a myriad of ways that cluster administrators run internal
    load balancers, such as running the load balancer on a subset of special nodes,
    and routing traffic somehow to those nodes. The primary motivation for choosing
    an internal load balancer is cost reduction. An internal load balancer for ingress
    can route traffic for multiple ingress objects, whereas an external load balancer
    controller typically needs one load balancer per ingress. As most cloud providers
    charge by load balancer, it is cheaper to support a single cloud load balancer
    that does fan-out within the cluster, than many cloud load balancers. Note that
    this incurs operational overhead and increased latency and compute costs, so be
    sure the money you’re saving is worth it. Many companies have a bad habit of optimizing
    on inconsequential cloud spend line items.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the spec for an ingress controller. Like LoadBalancer services,
    most of the spec is universal, but various ingress controllers have different
    features and accept different configs. We’ll start with the basics:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The previous example is representative of a typical ingress. It sends traffic
    to `/demo` to one service and all other traffic to another. Ingresses have a “default
    backend” where requests are routed if no rule matches. This can be configured
    in many ingress controllers in the controller configuration itself (e.g., a generic
    404 page), and many support the `.spec.defaultBackend` field. Ingresses support
    multiple ways to specify a path. There are currently three:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Exact
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Matches the specific path and only the given path (including trailing `/` or
    lack thereof).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Prefix
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Matches all paths that start with the given path.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ImplementationSpecific
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Allows for custom semantics from the current ingress controller.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: When a request matches multiple paths, the most specific match is chosen. For
    example, if there are rules for `/first` and `/first/second`, any request starting
    with `/first/second` will go to the backend for `/first/second`. If a path matches
    an exact path and a prefix path, the request will go to the backend for the exact
    rule.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingresses can also use hostnames in rules:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In this example, we serve traffic to `a.example.com` from one service and traffic
    to `b.example.com` from another. This is comparable to virtual hosts in web servers.
    You may want to use host rules to use a single load balancer and IP to serve multiple
    unique domains.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingresses have basic TLS support:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The TLS config references a Kubernetes secret by name, in `.spec.tls.[*].secretName`.
    Ingress controllers expect the TLS certificate and key to be provided in `.data."tls.crt"`
    and `.data."tls.key"` respectively, as shown here:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Tip
  id: totrans-337
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you don’t need to manage traditionally issued certificates by hand, you can
    use [cert-manager](https://oreil.ly/qkN0h) to automatically fetch and update certs.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned earlier that ingress is simply a spec, and drastically different
    implementations exist. It’s possible to use multiple ingress controllers in a
    single cluster, using `IngressClass` settings. An ingress class represents an
    ingress controller, and therefore a specific ingress implementation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-340
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Annotations in Kubernetes must be strings. Because `true` and `false` have distinct
    nonstring meanings, you cannot set an annotation to `true` or `false` without
    quotes. `"true"` and `"false"` are both valid. This is a [long-running bug](https://oreil.ly/76uSI),
    which is often encountered when setting a default priority class.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '`IngressClass` was introduced in Kubernetes 1.18. Prior to 1.18, annotating
    ingresses with `kubernetes.io/ingress.class` was a common convention but relied
    on all installed ingress controllers to support it. Ingresses can pick an ingress
    class by setting the class’s name in `.spec.ingressClassName`.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If more than one ingress class is set as default, Kubernetes will not allow
    you to create an ingress with no ingress class or remove the ingress class from
    an existing ingress. You can use admission control to prevent multiple ingress
    classes from being marked as default.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Ingress only supports HTTP(S) requests, which is insufficient if your service
    uses a different protocol (e.g., most databases use their own protocols). Some
    ingress controllers, such as the NGINX ingress controller, do support TCP and
    UDP, but this is not the norm.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Now on to deploying an ingress controller so we can add ingress rules to our
    Golang web server example.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'When we deployed our KIND cluster, we had to add several options to allow us
    to deploy an ingress controller:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: extraPortMappings allow the local host to make requests to the ingress controller
    over ports 80/443.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node-labels only allow the ingress controller to run on a specific node(s) matching
    the label selector.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many options to choose from with ingress controllers. The Kubernetes
    system does not start or have a default controller like it does with other pieces.
    The Kubernetes community does support AWS, GCE, and Nginx ingress controllers.
    [Table 5-1](#brief_list_of_ingress_controller_options) outlines several options
    for ingress.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Brief list of ingress controller options
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Commercial support | Engine | Protocol support | SSL termination |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| Ambassador ingress controller | Yes | Envoy | gRPC, HTTP/2, WebSockets |
    Yes |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| Community ingress Nginx | No | NGINX | gRPC, HTTP/2, WebSockets | Yes |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| NGINX Inc. ingress | Yes | NGINX | HTTP, Websocket, gRPC | Yes |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| HAProxy ingress | Yes | HAProxy | gRPC, HTTP/2, WebSockets | Yes |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Istio Ingress | No | Envoy | HTTP, HTTPS, gRPC, HTTP/2 | Yes |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| Kong ingress controller for Kubernetes | Yes | Lua on top of Nginx | gRPC,
    HTTP/2 | Yes |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| Traefik Kubernetes ingress | Yes | Traefik | HTTP/2, gRPC, and WebSockets
    | Yes |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: 'Some things to consider when deciding on the ingress for your clusters:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Protocol support: Do you need more than TCP/UDP, for example gRPC integration
    or WebSocket?'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Commercial support: Do you need commercial support?'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advanced features: Are JWT/oAuth2 authentication or circuit breakers requirements
    for your applications?'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'API gateway features: Do you need some API gateway functionalities such as
    rate-limiting?'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Traffic distribution: Does your application require support for specialized
    traffic distribution like canary A/B testing or mirroring?'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our example, we have chosen to use the Community version of the NGINX ingress
    controller.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-368
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For more ingress controllers to choose from, [kubernetes.io](https://oreil.ly/Lzn5q)
    maintains a list.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deploy the NGINX ingress controller into our KIND cluster:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'As with all deployments, we must wait for the controller to be ready before
    we can use it. With the following command, we can verify if our ingress controller
    is ready for use:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The controller is deployed to the cluster, and now we’re ready to write ingress
    rules for our application.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器已部署到集群中，现在我们准备为我们的应用编写入口规则。
- en: Deploy ingress rules
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署入口规则
- en: 'Our YAML manifest defines several ingress rules to use with our Golang web
    server example:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的YAML清单定义了几个入口规则，用于我们的Golang Web服务器示例：
- en: '[PRE72]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'With `describe` we can see all the backends that map to the ClusterIP service
    and the pods:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `describe` 我们可以看到映射到 ClusterIP 服务和 pod 的所有后端：
- en: '[PRE73]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Our ingress rule is only for the `/host` route and will route requests to our
    `clusterip-service:8080` service.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的入口规则仅适用于 `/host` 路由，并将请求路由到我们的 `clusterip-service:8080` 服务。
- en: 'We can test that with cURL to http://localhost/host:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 cURL 来测试 http://localhost/host：
- en: '[PRE74]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Now we can see how powerful ingresses are; let’s deploy a second deployment
    and ClusterIP service.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到入口规则有多么强大；让我们部署第二个部署和ClusterIP服务。
- en: 'Our new deployment and service will be used to answer the requests for `/data`:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新部署和服务将用于响应 `/data` 的请求：
- en: '[PRE75]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now both the `/host` and `/data` work but are going to separate services:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `/host` 和 `/data` 都可以工作，但将会路由到不同的服务：
- en: '[PRE76]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Since ingress works on layer 7, there are many more options to route traffic
    with, such as host header and URI path.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 由于入口在第7层工作，有许多其他选项可以用来路由流量，例如主机头和URI路径。
- en: For more advanced traffic routing and release patterns, a service mesh is required
    to be deployed in the cluster network. Let’s dig into that next.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更高级的流量路由和发布模式，需要在集群网络中部署服务网格。让我们接下来深入探讨这一点。
- en: Service Meshes
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格
- en: A new cluster with the default options has some limitations. So, let’s get an
    understanding for what those limitations are and how a service mesh can resolve
    some of those limitations. A *service mesh* is an API-driven infrastructure layer
    for handling service-to-service communication.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认选项的新群集有一些限制。所以，让我们了解这些限制是什么，以及服务网格如何解决其中的一些限制。*服务网格* 是一个API驱动的基础设施层，用于处理服务间的通信。
- en: From a security point of view, all traffic inside the cluster is unencrypted
    between pods, and each application team that runs a service must configure monitoring
    separately for each service. We have discussed the service types, but we have
    not discussed how to update deployments of pods for them. Service meshes support
    more than the basic deployment type; they support rolling updates and re-creations,
    like Canary does. From a developer’s perspective, injecting faults into the network
    is useful, but also not directly supported in default Kubernetes network deployments.
    With service meshes, developers can add fault testing, and instead of just killing
    pods, you can use service meshes to inject delays—again, each application would
    have to build in fault testing or circuit breaking.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 从安全角度来看，集群内部所有流量在pod之间都是未加密的，运行服务的每个应用团队必须单独为每个服务配置监控。我们已经讨论了服务类型，但我们还没有讨论如何更新它们的pod部署。服务网格支持的不仅仅是基本的部署类型；它们支持滚动更新和重建，就像Canary一样。从开发者的角度来看，将故障注入网络是有用的，但默认的Kubernetes网络部署不直接支持。通过服务网格，开发者可以添加故障测试，而不仅仅是杀死pod，还可以使用服务网格来注入延迟——同样，每个应用程序都必须构建故障测试或断路器。
- en: 'There are several pieces of functionality that a service mesh enhances or provides
    in a default Kubernetes cluster network:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认的Kubernetes集群网络中，服务网格增强或提供了几个功能：
- en: Service Discovery
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现
- en: Instead of relying on DNS for service discovery, the service mesh manages service
    discovery, and removes the need for it to be implemented in each individual application.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格管理服务发现，不再依赖DNS，消除了在每个单独应用程序中实现服务发现的需要。
- en: Load Balancing
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡
- en: The service mesh adds more advanced load balancing algorithms such as least
    request, consistent hashing, and zone aware.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格增加了更先进的负载均衡算法，如最小请求、一致性哈希和区域感知。
- en: Communication Resiliency
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 通信韧性
- en: The service mesh can increase communication resilience for applications by not
    having to implement retries, timeouts, circuit breaking, or rate limiting in application
    code.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格可以通过在应用程序中不必实现重试、超时、断路或速率限制来增加应用程序的通信韧性。
- en: Security
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性
- en: 'A service mesh can provide the folllowing: * End-to-end encryption with mTLS
    between services * Authorization policies, which authorize what services can communicate
    with each other, not just at the layer 3 and 4 levels like in Kubernetes network
    polices.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格可以提供以下功能：*服务之间的端到端加密通过mTLS* *授权策略，授权哪些服务可以与其他服务通信，不仅限于Kubernetes网络策略的第3和第4层。
- en: Observability
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性
- en: Service meshes add in observability by enriching the layer 7 metrics and adding
    tracing and alerting.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格通过丰富的第7层指标、添加跟踪和警报来增强可观测性。
- en: Routing Control
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 路由控制
- en: Traffic shifting and mirroring in the cluster.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的流量转移和镜像。
- en: API
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: API
- en: All of this can be controlled via an API provided by the service mesh implementation.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可以通过服务网格实现提供的API进行控制。
- en: Let’s walk through several components of a service mesh in [Figure 5-8](#img-service-mesh).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下[图5-8](#img-service-mesh)中服务网格的几个组件。
- en: '![Service mesh Components](Images/neku_0508.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![服务网格组件](Images/neku_0508.png)'
- en: Figure 5-8\. Service mesh components
  id: totrans-410
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 服务网格组件
- en: Traffic is handled differently depending on the component or destination of
    traffic. Traffic into and out of the cluster is managed by the gateways. Traffic
    between the frontend, backend, and user service is all encrypted with Mutual TLS
    (mTLS) and is handled by the service mesh. All the traffic to the frontend, backend,
    and user pods in the service mesh is proxied by the sidecar proxy deployed within
    the pods. Even if the control plane is down and updates cannot be made to the
    mesh, the service and application traffic are not affected.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 流量根据组件或流量的目的地处理不同。进出集群的流量由网关管理。前端、后端和用户服务之间的流量都使用双向TLS（mTLS）进行加密，并由服务网格处理。即使控制平面关闭且无法更新网格，服务和应用程序流量也不会受到影响。
- en: 'There are several options to use when deploying a service mesh; here are highlights
    of just a few:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署服务网格时有几个选项可供选择；以下是其中几个要点：
- en: Istio
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio
- en: Uses a Go control plane with an Envoy proxy.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用带有Envoy代理的Go控制平面。
- en: This is a Kubernetes-native solution that was initially released by Lyft.
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个由Lyft最初发布的基于Kubernetes的本地解决方案。
- en: Consul
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul
- en: Uses HashiCorp Consul as the control plane.
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HashiCorp Consul作为控制平面。
- en: Consul Connect uses an agent installed on every node as a DaemonSet, which communicates
    with the Envoy sidecar proxies that handle routing and forwarding of traffic.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul Connect在每个节点上安装一个代理作为DaemonSet，它与处理流量路由和转发的Envoy Sidecar代理通信。
- en: AWS App Mesh
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS App Mesh
- en: Is an AWS-managed solution that implements its own control plane.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是AWS管理的解决方案，实现了自己的控制平面。
- en: Does not have mTLS or traffic policy.
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有mTLS或流量策略。
- en: Uses the Envoy proxy for the data plane.
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Envoy代理作为数据平面。
- en: Linkerd
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: Also uses Go for the control plane with the Linkerd proxy.
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd代理也使用Go控制平面。
- en: No traffic shifting and no distributed tracing.
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有流量转移和分布式跟踪。
- en: Is a Kubernetes-only solution, which results in fewer moving pieces and means
    that Linkerd has less complexity overall.
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是一个仅限于Kubernetes的解决方案，这导致移动部件较少，意味着Linkerd总体上复杂性较低。
- en: It is our opinion that the best use case for a service mesh is mTLS between
    services. Other higher-level use cases for developers include circuit breaking
    and fault testing for APIs. For network administrators, advanced routing policies
    and algorithms can be deployed with service meshes.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为服务网格的最佳用例是服务之间的mTLS。开发人员的其他高级用例包括断路器和API的故障测试。对于网络管理员来说，可以使用服务网格部署高级路由策略和算法。
- en: Let’s look at a service mesh example. The first thing you need to do if you
    haven’t already is [install the Linkerd CLI](https://oreil.ly/jVaPm).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个服务网格的例子。如果您还没有安装，请首先[安装Linkerd CLI](https://oreil.ly/jVaPm)。
- en: 'Your choices are cURL, bash, or brew if you’re on a Mac:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Mac，您可以选择cURL、bash或brew：
- en: '[PRE77]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'This preflight checklist will verify that our cluster can run Linkerd:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预检查列表将验证我们的集群是否能运行Linkerd：
- en: '[PRE78]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The Linkerd CLI tool can install Linkerd for us onto our KIND cluster:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd CLI工具可以将Linkerd安装到我们的KIND集群中：
- en: '[PRE79]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: As with the ingress controller and MetalLB, we can see that a lot of components
    are installed in our cluster.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 与入口控制器和MetalLB一样，我们可以看到许多组件安装在我们的集群中。
- en: Linkerd can validate the installation with the `linkerd check` command.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd可以使用`linkerd check`命令验证安装。
- en: 'It will validate a plethora of checks for the Linkerd install, included but
    not limited to the Kubernetes API version, controllers, pods, and configs to run
    Linkerd, as well as all the services, versions, and APIs needed to run Linkerd:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 它将验证Linkerd安装的大量检查，包括但不限于Kubernetes API版本、控制器、Pod和配置以运行Linkerd，以及运行Linkerd所需的所有服务、版本和API：
- en: '[PRE80]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now that everything looks good with our install of Linkerd, we can add our
    application to the service mesh:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的Linkerd安装看起来一切正常，我们可以将我们的应用程序添加到服务网格中：
- en: '[PRE81]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Let’s pull up the Linkerd console to investigate what we have just deployed.
    We can start the console with `linkerd dashboard &`.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'This will proxy the console to our local machine available at `http://localhost:50750`:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Tip
  id: totrans-444
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re having issues with reaching the dashboard, you can run `linkerd viz
    check` and find more help in the Linkerd [documentation](https://oreil.ly/MqgAp).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: We can see all our deployed objects from the previous exercises in [Figure 5-9](#linkerd-dashboards).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Our ClusterIP service is not part of the Linkerd service mesh. We will need
    to use the proxy injector to add our service to the mesh. It accomplishes this
    by watching for a specific annotation that can be added either with Linkerd `inject`
    or by hand to the pod’s spec.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '![Linkderd Dashboard](Images/neku_0509.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. Linkerd dashboard
  id: totrans-449
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s remove some older exercises’ resources for clarity:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We can use the Linkerd CLI to inject the proper annotations into our deployment
    spec, so that will become part of the mesh.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to get our application manifest, `cat web.yaml`, and use Linkerd
    to inject the annotations, `linkerd inject -`, then apply them back to the Kubernetes
    API with `kubectl apply -f -`:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'If we describe our app deployment, we can see that Linkerd has injected new
    annotations for us, `Annotations: linkerd.io/inject: enabled`:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: If we navigate to the app in the dashboard, we can see that our deployment is
    part of the Linkerd service mesh now, as shown in [Figure 5-10](#app-dashboards).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '![App Linkderd Dashboard](Images/neku_0510.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. Web app deployment linkerd dashboard
  id: totrans-459
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The CLI can also display our stats for us:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Again, let’s scale up our deployment:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: In [Figure 5-11](#app-stats), we navigate to the web browser and open [this
    link](https://oreil.ly/qQx9T) so we can watch the stats in real time. Select the
    default namespaces, and in Resources select our deployment/app. Then click “start
    for the web” to start displaying the metrics.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 'In a separate terminal let’s use the `netshoot` image, but this time running
    inside our KIND cluster:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '![App Stats Dashboard](Images/neku_0511.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. Web app dashboard
  id: totrans-468
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s send a few hundred queries and see the stats:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: In our terminal we can see all the liveness and readiness probes as well as
    our `/host` requests.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '`tmp-shell` is our `netshoot` bash terminal with our `for` loop running.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '`10.244.2.1`, `10.244.3.1`, and `10.244.2.1` are the Kubelets of the hosts
    running our probes for us:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Our example showed the observability functionality for a service mesh only.
    Linkerd, Istio, and the like have many more options available for developers and
    network administrators to control, monitor, and troubleshoot services running
    inside their cluster network. As with the ingress controller, there are many options
    and features available. It is up to you and your teams to decide what functionality
    and features are important for your networks.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-476
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes networking world is feature rich with many options for teams
    to deploy, test, and manage with their Kubernetes cluster. Each new addition will
    add complexity and overhead to the cluster operations. We have given developers,
    network administrators, and system administrators a view into the abstractions
    that Kubernetes offers.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网络世界功能丰富，团队可以通过多种选项部署、测试和管理其 Kubernetes 集群。每个新的添加都会给集群操作增加复杂性和开销。我们已经为开发人员、网络管理员和系统管理员提供了
    Kubernetes 提供的抽象视图。
- en: From internal traffic to external traffic to the cluster, teams must choose
    what abstractions work best for their workloads. This is no small task, and now
    you are armed with the knowledge to begin those discussions.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 从内部流量到集群外部流量，团队必须选择最适合其工作负载的抽象方法。这并不是一件小事，现在你已经掌握了开始这些讨论的知识。
- en: In our next chapter, we take our Kubernetes services and network learnings to
    the cloud! We will explore the network services offered by each cloud provider
    and how they are integrated into their Kubernetes managed service offering.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一章中，我们将把我们的 Kubernetes 服务和网络学习带到云端！我们将探索每个云提供商提供的网络服务，并了解它们如何集成到其 Kubernetes
    管理服务中。
