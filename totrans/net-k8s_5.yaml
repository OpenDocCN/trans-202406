- en: Chapter 5\. Kubernetes Networking Abstractions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章 Kubernetes 网络抽象
- en: Previously, we covered a swath of networking fundamentals and how traffic in
    Kubernetes gets from A to B. In this chapter, we will discuss networking abstractions
    in Kubernetes, primarily service discovery and load balancing. Most notably, this
    is the chapter on services and ingresses. Both resources are notoriously complex,
    due to the large number of options, as they attempt to solve numerous use cases.
    They are the most visible part of the Kubernetes network stack, as they define
    basic network characteristics of workloads on Kubernetes. This is where developers
    interact with the networking stack for their applications deployed on Kubernetes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们涵盖了大量的网络基础知识，以及 Kubernetes 中如何将流量从 A 点传输到 B 点。在本章中，我们将讨论 Kubernetes 中的网络抽象，主要是服务发现和负载均衡。特别值得注意的是，这是关于服务和入口的章节。由于它们试图解决多种用例，这两个资源都因选项繁多而复杂。它们是
    Kubernetes 网络栈中最显著的部分，因为它们定义了在 Kubernetes 上部署的应用程序的基本网络特性。这是开发人员与其应用程序的网络堆栈交互的地方。
- en: 'This chapter will cover fundamental examples of Kubernetes networking abstractions
    and the details on\f how they work. To follow along, you will need the following
    tools:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖 Kubernetes 网络抽象的基本示例及其工作原理的详细信息。为了跟上进度，您需要以下工具：
- en: Docker
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: KIND
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KIND
- en: Linkerd
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: You will need to be familiar with the `kubectl exec` and `Docker exec` commands.
    If you are not, our code repo will have any and all the commands we discuss, so
    don’t worry too much. We will also make use of `ip` and `netns` from Chapters
    [2](ch02.xhtml#linux_networking) and [3](ch03.xhtml#container_networking_basics).
    Note that most of these tools are for debugging and showing implementation details;
    you will not necessarily need them during normal operations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要熟悉`kubectl exec`和`Docker exec`命令。如果你不熟悉，我们的代码仓库会包含我们讨论的所有命令，所以不要太担心。我们还将使用来自第
    [2](ch02.xhtml#linux_networking) 章和第 [3](ch03.xhtml#container_networking_basics)
    章的`ip`和`netns`。请注意，这些工具大多用于调试和显示实现细节；在正常操作期间，您不一定需要它们。
- en: Docker, KIND, and Linkerd installs are available on their respective sites,
    and we’ve provided more information in the book’s code repository as well.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Docker、KIND 和 Linkerd 的安装可以在它们各自的网站上找到，我们还在书籍的代码仓库中提供了更多信息。
- en: Tip
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: '`kubectl` is a key tool in this chapter’s examples, and it’s the standard for
    operators to interact with clusters and their networks. You should be familiar
    with the `kubectl create`, `apply`, `get`, `delete`, and `exec` commands. Learn
    more in the [Kubernetes documentation](https://oreil.ly/H8bTU) or run `kubectl
    [command] --help`.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl` 是本章示例中的一个关键工具，也是操作员与集群及其网络进行交互的标准工具。您应该熟悉`kubectl create`、`apply`、`get`、`delete`和`exec`命令。在[Kubernetes
    文档](https://oreil.ly/H8bTU)中了解更多信息或运行`kubectl [command] --help`。'
- en: 'This chapter will explore these Kubernetes networking abstractions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨这些 Kubernetes 网络抽象：
- en: StatefulSets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态副本集
- en: Endpoints
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终结点
- en: Endpoint slices
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Endpoint slices
- en: Services
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务
- en: NodePort
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodePort
- en: Cluster
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群
- en: Headless
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无头
- en: External
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部
- en: LoadBalancer
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoadBalancer
- en: Ingress
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口
- en: Ingress controller
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口控制器
- en: Ingress rules
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口规则
- en: Service meshes
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务网格
- en: Linkerd
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: 'To explore these abstractions, we will deploy the examples to our Kubernetes
    cluster with the following steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索这些抽象，我们将按照以下步骤将示例部署到我们的 Kubernetes 集群中：
- en: Deploy a KIND cluster with ingress enabled.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用启用了入口的 KIND 集群进行部署。
- en: Explore StatefulSets.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索有状态副本集。
- en: Deploy Kubernetes services.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 Kubernetes 服务。
- en: Deploy an ingress controller.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署一个入口控制器。
- en: Deploy a Linkerd service mesh.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 Linkerd 服务网格。
- en: These abstractions are at the heart of what the Kubernetes API provides to developers
    and administrators to programmatically control the flow of communications into
    and out of the cluster. Understanding and mastering how to deploy these abstractions
    is crucial for the success of any workload inside a cluster. After working through
    these examples, you will understand which abstractions to use in certain situations
    for your applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些抽象是 Kubernetes API 为开发人员和管理员提供的核心内容，用于在集群内程序控制通信流动。理解和掌握如何部署这些抽象对于集群内任何工作负载的成功至关重要。通过这些示例，您将了解在特定情况下为您的应用程序使用哪些抽象。
- en: With the KIND cluster configuration YAML, we can use KIND to create that cluster
    with the command in the next section. If this is the first time running it, it
    will take some time to download all the Docker images for the working and control
    plane Docker images.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KIND集群配置YAML，我们可以使用下一节中的命令使用KIND创建该集群。如果这是第一次运行它，下载工作和控制平面Docker镜像将需要一些时间。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The following examples assume that you still have the local KIND cluster running
    from the previous chapter, along with the Golang web server and the `dnsutils`
    images for testing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例假设您仍然从上一章节运行本地的KIND集群，以及Golang Web服务器和用于测试的`dnsutils`镜像。
- en: StatefulSets
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StatefulSets
- en: 'StatefulSets are a workload abstraction in Kubernetes to manage pods like you
    would a deployment. Unlike a deployment, StatefulSets add the following features
    for applications that require them:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets是Kubernetes中的工作负载抽象，用于像管理部署一样管理Pod。与部署不同，StatefulSets为需要这些功能的应用程序添加以下功能：
- en: Stable, unique network identifiers
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定且唯一的网络标识符
- en: Stable, persistent storage
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定的持久存储
- en: Ordered, graceful deployment and scaling
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序、优雅的部署和扩展
- en: Ordered, automated rolling updates
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序、自动化的滚动更新
- en: The deployment resource is better suited for applications that do not have these
    requirements (for example, a service that stores data in an external database).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 部署资源更适合于不具备这些要求的应用程序（例如，将数据存储在外部数据库的服务）。
- en: Our database for the Golang minimal web server uses a StatefulSet. The database
    has a service, a ConfigMap for the Postgres username, a password, a test database
    name, and a StatefulSet for the containers running Postgres.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Golang最小Web服务器的数据库使用了StatefulSet。数据库具有服务、用于Postgres用户名、密码、测试数据库名称的ConfigMap，以及运行Postgres的容器的StatefulSet。
- en: 'Let’s deploy it now:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署它：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s examine the DNS and network ramifications of using a StatefulSet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析使用StatefulSet时DNS和网络带来的影响。
- en: 'To test DNS inside the cluster, we can use the `dnsutils` image; this image
    is `gcr .io/kubernetes-e2e-test-images/dnsutils:1.3` and is used for Kubernetes
    testing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试集群内的DNS，我们可以使用`dnsutils`镜像；该镜像是`gcr.io/kubernetes-e2e-test-images/dnsutils:1.3`，用于Kubernetes测试：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the replica configured with two pods, we see the StatefulSet deploy `postgres-0`
    and `postgres-1`, in that order, a feature of StatefulSets with IP address 10.244.1.3
    and 10.244.2.3, respectively:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 配置为两个Pod的副本，我们看到StatefulSet依次部署`postgres-0`和`postgres-1`，具有分别为10.244.1.3和10.244.2.3的IP地址：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the name of our headless service, Postgres, that the client can use
    for queries to return the endpoint IP addresses:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的无头服务Postgres的名称，客户端可以用来查询返回端点IP地址：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using our `dnsutils` image, we can see that the DNS names for the StatefulSets
    will return those IP addresses along with the cluster IP address of the Postgres
    service:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的`dnsutils`镜像，我们可以看到StatefulSets的DNS名称将返回这些IP地址以及Postgres服务的集群IP地址：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: StatefulSets attempt to mimic a fixed group of persistent machines. As a generic
    solution for stateful workloads, specific behavior may be frustrating in specific
    use cases.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets试图模拟一组固定的持久化机器。作为有状态工作负载的通用解决方案，特定行为可能在特定用例中令人沮丧。
- en: 'A common problem that users encounter is an update requiring manual intervention
    to fix when using `.spec .updateStrategy.type: RollingUpdate`, and `.spec.podManagementPolicy:
    OrderedReady`, both of which are default settings. With these settings, a user
    must manually intervene if an updated pod never becomes ready.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '用户常遇到的一个问题是，在使用`.spec.updateStrategy.type: RollingUpdate`和`.spec.podManagementPolicy:
    OrderedReady`时，更新需要手动干预。使用这些设置，如果更新后的Pod永远无法准备就绪，用户必须手动干预。'
- en: Also, StatefulSets require a service, preferably headless, to be responsible
    for the network identity of the pods, and end users are responsible for creating
    this service.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，StatefulSets需要一个服务（最好是无头服务），负责管理Pod的网络标识，并且最终用户负责创建此服务。
- en: Statefulsets have many configuration options, and many third-party alternatives
    exist (both generic stateful workload controllers and software-specific workload
    controllers).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets有许多配置选项，并且存在许多第三方替代方案（既通用有状态工作负载控制器，也特定软件的工作负载控制器）。
- en: StatefulSets offer functionality for a specific use case in Kubernetes. They
    should not be used for everyday application deployments. Later in this section,
    we will discuss more appropriate networking abstractions for run-of-the-mill deployments.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets 在 Kubernetes 中针对特定用例提供功能。它们不应用于日常应用程序部署。在本节后面，我们将讨论更适合普通部署的网络抽象。
- en: In our next section, we will explore endpoints and endpoint slices, the backbone
    of Kubernetes services.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨 endpoints 和 endpoint slices，这是 Kubernetes 服务的支柱。
- en: Endpoints
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Endpoints
- en: Endpoints help identify what pods are running for the service it powers. Endpoints
    are created and managed by services. We will discuss services on their own later,
    to avoid covering too many new things at once. For now, let’s just say that a
    service contains a standard label selector (introduced in [Chapter 4](ch04.xhtml#kubernetes_networking_introduction)),
    which defines which pods are in the endpoints.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoints 帮助识别为其提供服务的 pods 正在运行的内容。Endpoints 是由 services 创建和管理的。我们稍后将单独讨论 services，以避免一次性涵盖太多新内容。现在，让我们只说服务包含标准标签选择器（在
    [第 4 章](ch04.xhtml#kubernetes_networking_introduction) 中介绍），定义 endpoints 中的 pods。
- en: In [Figure 5-1](#img-endpoints), we can see traffic being directed to an endpoint
    on node 2, pod 5.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-1](#img-endpoints) 中，我们可以看到流量被定向到节点 2 上的 endpoint，pod 5。
- en: '![Kubernetes Endpoints](Images/neku_0501.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes Endpoints](Images/neku_0501.png)'
- en: Figure 5-1\. Endpoints in a service
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 服务中的 Endpoints
- en: Let’s discuss how this endpoint is created and maintained in the cluster.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何在集群中创建和维护此 endpoint 的更好视图。
- en: 'Each endpoint contains a list of ports (which apply to all pods) and two lists
    of addresses: ready and unready:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 endpoint 包含一个端口列表（适用于所有 pods）和两个地址列表：ready 和 unready：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Addresses are listed in `.addresses` if they are passing pod readiness checks.
    Addresses are listed in `.notReadyAddresses` if they are not. This makes endpoints
    a *service discovery* tool, where you can watch an `Endpoints` object to see the
    health and addresses of all pods:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过了 pod 的就绪检查，地址将列在 `.addresses` 中。如果没有通过，地址将列在 `.notReadyAddresses` 中。这使得
    endpoints 成为一个 *服务发现* 工具，您可以观察 `Endpoints` 对象以查看所有 pods 的健康状态和地址：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can get a better view of all the addresses with `kubectl describe`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `kubectl describe` 查看所有地址的更好视图：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s remove the app label and see how Kubernetes responds. In a separate terminal,
    run this command. This will allow us to see changes to the pods in real time:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们移除 app 标签，看看 Kubernetes 的反应。在另一个终端中，运行此命令。这将允许我们实时查看 pods 的变化：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In another separate terminal, let’s do the same thing with endpoints:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个单独的终端中，让我们用 endpoints 做同样的事情：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now need to get a pod name to remove from the `Endpoints` object:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要获取要从 `Endpoints` 对象中删除的 pod 名称：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With `kubectl label`, we can alter the pod’s `app-5586fc9d77-7frts` `app=app`
    label:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl label`，我们可以修改 pod 的 `app-5586fc9d77-7frts` 的 `app=app` 标签：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Both watch commands on endpoints and pods will see some changes for the same
    reason: removal of the label on the pod. The endpoint controller will notice a
    change to the pods with the label `app=app` and so did the deployment controller.
    So Kubernetes did what Kubernetes does: it made the real state reflect the desired
    state:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相同的原因，endpoints 和 pods 的观察命令都会看到一些变化：移除 pod 上的标签 `app=app`。endpoint 控制器将注意到
    pod 的变化，部署控制器也会。所以 Kubernetes 做了 Kubernetes 擅长的事情：使实际状态反映所需状态：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The deployment has four pods, but our relabeled pod still exists: `app-5586fc9d77-7frts`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 部署有四个 pods，但我们重新标记的 pod 仍然存在：`app-5586fc9d77-7frts`：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The pod `app-5586fc9d77-6dcg2` now is part of the deployment and endpoint object
    with IP address `10.244.1.6`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Pod `app-5586fc9d77-6dcg2` 现在已成为部署和 endpoint 对象的一部分，带有 IP 地址 `10.244.1.6`：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As always, we can see the full picture of details with `kubectl describe`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们可以使用 `kubectl describe` 查看详细的完整图片：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For large deployments, that endpoint object can become very large, so much so
    that it can actually slow down changes in the cluster. To solve that issue, the
    Kubernetes maintainers have come up with endpoint slices.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型部署，那个 endpoint 对象可能会变得非常庞大，以至于实际上会减慢集群中的变更速度。为了解决这个问题，Kubernetes 的维护者提出了
    endpoint slices。
- en: Endpoint Slices
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Endpoint Slices
- en: You may be asking, how are they different from endpoints? This is where we *really*
    start to get into the weeds of Kubernetes networking.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，它们与 endpoints 有何不同？这正是我们开始深入研究 Kubernetes 网络的地方。
- en: In a typical cluster, Kubernetes runs `kube-proxy` on every node. `kube-proxy`
    is responsible for the per-node portions of making services work, by handling
    routing and *outbound* load balancing to all the pods in a service. To do that,
    `kube-proxy` watches all endpoints in the cluster so it knows all applicable pods
    that all services should route to.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型集群中，Kubernetes 在每个节点上运行 `kube-proxy`。`kube-proxy` 负责使服务正常工作的每个节点部分，通过处理路由和
    *出站* 负载均衡到服务中的所有 pod。为了做到这一点，`kube-proxy` 监视集群中的所有 endpoints，以便了解所有服务应路由到的适用 pod。
- en: Now, imagine we have a *big* cluster, with thousands of nodes, and tens of thousands
    of pods. That means thousands of kube-proxies are watching endpoints. When an
    address changes in an `Endpoints` object (say, from a rolling update, scale up,
    eviction, health-check failure, or any number of reasons), the updated `Endpoints`
    object is pushed to all listening kube-proxies. It is made worse by the number
    of pods, since more pods means larger `Endpoints` objects, and more frequent changes.
    This eventually becomes a strain on `etcd`, the Kubernetes API server, and the
    network itself. Kubernetes scaling limits are complex and depend on specific criteria,
    but endpoints watching is a common problem in clusters that have thousands of
    nodes. Anecdotally, many Kubernetes users consider endpoint watches to be the
    ultimate bottleneck of cluster size.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下我们有一个 *大* 的集群，有数千个节点和成千上万个 pod。这意味着成千上万个 kube-proxy 在监视 endpoints。当 `Endpoints`
    对象中的地址发生变化（比如滚动更新、扩展、驱逐、健康检查失败或其他任何原因），更新后的 `Endpoints` 对象会推送到所有正在监听的 kube-proxy。由于
    pod 的数量更多，意味着 `Endpoints` 对象更大且变化更频繁，这加剧了问题。这最终会对 `etcd`、Kubernetes API 服务器和网络本身造成压力。Kubernetes
    的扩展限制是复杂的，并且取决于具体的标准，但 endpoint 监视是具有数千节点的集群中常见的问题。据传闻，许多 Kubernetes 用户认为 endpoint
    监视是集群大小的最终瓶颈。
- en: This problem is a function of `kube-proxy`’s design and the expectation that
    any pod should be immediately able to route to any service with no notice. Endpoint
    slices are an approach that allows `kube-proxy`’s fundamental design to continue,
    while drastically reducing the watch bottleneck in large clusters where large
    services are used.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题是 `kube-proxy` 设计的一个功能，期望任何 pod 都能立即路由到任何服务而无需通知。Endpoint 切片是一种方法，允许 `kube-proxy`
    的基本设计继续存在，同时大大减少在使用大型服务的大集群中的监视瓶颈。
- en: 'Endpoint slices have similar contents to `Endpoints` objects but also include
    an array of endpoints:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoint 切片与 `Endpoints` 对象有类似的内容，但也包括一个 endpoints 数组：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The meaningful difference between endpoints and endpoint slices is not the schema,
    but how Kubernetes treats them. With “regular” endpoints, a Kubernetes service
    creates one endpoint for all pods in the service. A service creates *multiple*
    endpoint slices, each containing a *subset* of pods; [Figure 5-2](#img-endpointslice)
    depicts this subset. The union of all endpoint slices for a service contains all
    pods in the service. This way, an IP address change (due to a new pod, a deleted
    pod, or a pod’s health changing) will result in a much smaller data transfer to
    watchers. Because Kubernetes doesn’t have a transactional API, the same address
    may appear temporarily in multiple slices. Any code consuming endpoint slices
    (such as `kube-proxy`) must be able to account for this.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: endpoints 和 endpoint 切片之间的显著区别不在于架构，而在于 Kubernetes 如何处理它们。对于“常规” endpoints，Kubernetes
    服务为服务中的所有 pod 创建一个 endpoint。一个服务创建 *多个* endpoint 切片，每个切片包含一部分 pod；[图 5-2](#img-endpointslice)
    描述了这个子集。服务的所有 endpoint 切片的并集包含服务中的所有 pod。这样，IP 地址的变化（由于新的 pod、已删除的 pod 或 pod 的健康状态变化）将导致向观察者传输的数据量大大减少。由于
    Kubernetes 没有事务 API，同一个地址可能会暂时出现在多个切片中。任何消费 endpoint 切片的代码（如 `kube-proxy`）必须能够处理这种情况。
- en: The maximum number of addresses in an endpoint slice is set using the `--max-endpoints-per-slice`
    `kube-controller-manager` flag. The current default is 100, and the maximum is
    1000. The endpoint slice controller attempts to fill existing endpoint slices
    before creating new ones, but does not rebalance endpoint slice.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `--max-endpoints-per-slice` `kube-controller-manager` 标志设置 endpoint 切片中地址的最大数目。当前默认值为
    100，最大值为 1000。Endpoint 切片控制器在创建新的切片之前会尝试填充现有的 endpoint 切片，但不会重新平衡 endpoint 切片。
- en: 'The endpoint slice controller mirrors endpoints to endpoint slice, to allow
    systems to continue writing endpoints while treating endpoint slice as the source
    of truth. The exact future of this behavior, and endpoints in general, has not
    been finalized (however, as a v1 resource, endpoints would be sunset with substantial
    notice). There are four exceptions that will prevent mirroring:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 端点片段控制器将端点镜像到端点片段，以允许系统在将端点视为事实来源的同时继续编写端点。这种行为的确切未来以及端点总体的未来尚未最终确定（但作为v1资源，端点将在大幅通知后被废弃）。有四个例外情况会阻止镜像：
- en: There is no corresponding service.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有对应的服务。
- en: The corresponding service resource selects pods.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应的服务资源选择Pod。
- en: 'The `Endpoints` object has the label `endpointslice.kubernetes.io/skip-mirror:
    true`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Endpoints`对象具有标签`endpointslice.kubernetes.io/skip-mirror: true`。'
- en: The `Endpoints` object has the annotation `control-⁠⁠plane.alpha.kubernetes​​.io/leader`.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Endpoints`对象具有注释`control-⁠⁠plane.alpha.kubernetes​​.io/leader`。'
- en: '![EndpointsVSliice](Images/neku_0502.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![EndpointsVSliice](Images/neku_0502.png)'
- en: Figure 5-2\. `Endpoints` versus `EndpointSlice` objects
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. `Endpoints`与`EndpointSlice`对象
- en: You can fetch all endpoint slices for a specific service by fetching endpoint
    slices filtered to the desired name in `.metadata.labels."kubernetes.io/service-name"`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过获取以`.metadata.labels."kubernetes.io/service-name"`中的所需名称为过滤条件的端点片段来获取特定服务的所有端点片段。
- en: Warning
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Endpoint slices have been in beta state since Kubernetes 1.17. This is still
    the case in Kubernetes 1.20, the current version at the time of writing. Beta
    resources typically don’t see major changes, and eventually graduate to stable
    APIs, but that is not guaranteed. If you directly use endpoint slices, be aware
    that a future Kubernetes release may make a breaking change without much warning,
    or the behaviors described here may change.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 自Kubernetes 1.17以来，端点片段一直处于Beta状态。在撰写本文时，Kubernetes 1.20仍然如此。Beta资源通常不会有重大更改，并最终升级为稳定的API，但这并不保证。如果直接使用端点片段，请注意未来的Kubernetes版本可能会进行重大更改而没有提前警告，或者这里描述的行为可能会发生变化。
- en: 'Let’s see some endpoints running in the cluster with `kubectl get endpointslice`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中运行的一些端点可以通过`kubectl get endpointslice`来查看：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we want more detail about the endpoint slices `clusterip-service-l2n9q`,
    we can use `kubectl describe` on it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要关于端点片段`clusterip-service-l2n9q`的更多细节，可以使用`kubectl describe`来查看它：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the output, we see the pod powering the endpoint slice from `TargetRef`.
    The `Topology` information gives us the hostname of the worker node that the pod
    is deployed to. Most importantly, the `Addresses` returns the IP address of the
    endpoint object.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们可以看到通过`TargetRef`支持端点片段的Pod。`Topology`信息为我们提供了Pod部署到的工作节点的主机名。最重要的是，`Addresses`返回端点对象的IP地址。
- en: Endpoints and endpoint slices are important to understand because they identify
    the pods responsible for the services, no matter the type deployed. Later in the
    chapter, we’ll review how to use endpoints and labels for troubleshooting. Next,
    we will investigate all the Kubernetes service types.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 理解端点和端点片段很重要，因为它们标识了负责服务的Pod，无论部署的类型如何。在本章后面，我们将详细讨论如何使用端点和标签进行故障排除。接下来，我们将调查所有的Kubernetes服务类型。
- en: Kubernetes Services
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes服务
- en: 'A service in Kubernetes is a load balancing abstraction within a cluster. There
    are four types of services, specified by the `.spec.Type` field. Each type offers
    a different form of load balancing or discovery, which we will cover individually.
    The four types are: ClusterIP, NodePort, LoadBalancer, and ExternalName.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的服务是集群内的负载均衡抽象。由`.spec.Type`字段指定四种类型的服务。每种类型提供不同形式的负载均衡或发现，我们将分别介绍这四种类型。这四种类型是：ClusterIP、NodePort、LoadBalancer和ExternalName。
- en: 'Services use a standard pod selector to match pods. The service includes all
    matching pods. Services create an endpoint (or endpoint slice) to handle pod discovery:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 服务使用标准的Pod选择器来匹配Pod。服务包含所有匹配的Pod。服务创建一个端点（或端点片段）来处理Pod的发现：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will use the Golang minimal web server for all the service examples. We have
    added functionality to the application to display the host and pod IP addresses
    in the REST request.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有的服务示例，我们将使用Golang最小化Web服务器。我们已经为应用程序添加了功能，在REST请求中显示主机和Pod IP地址。
- en: '[Figure 5-3](#img-pod-connection) outlines our pod networking status as a single
    pod in a cluster. The networking objects we are about to explore will expose our
    app pods outside the cluster in some instances and in others allow us to scale
    our application to meet demand. Recall from Chapters [3](ch03.xhtml#container_networking_basics)
    and [4](ch04.xhtml#kubernetes_networking_introduction) that containers running
    inside pods share a network namespace. In addition, there is also a pause container
    that is created for each pod. The pause container manages the namespaces for the
    pod.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 5-3](#img-pod-connection)概述了我们作为集群中单个 Pod 的网络状态。我们即将探讨的网络对象将在某些情况下将我们的应用
    Pod 暴露到集群外部，在其他情况下，允许我们扩展应用程序以满足需求。回顾第 [3](ch03.xhtml#container_networking_basics)
    和第 [4](ch04.xhtml#kubernetes_networking_introduction) 章节中的内容，运行在 Pod 内部的容器共享网络命名空间。此外，每个
    Pod 都会创建一个暂停容器来管理命名空间。'
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The pause container is the parent container for all running containers in the
    pod. It holds and shares all the namespaces for the pod. You can read more about
    the pause container in Ian Lewis’ [blog post](https://oreil.ly/n51eq).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 暂停容器是 Pod 内所有运行容器的父容器。它持有并共享 Pod 的所有命名空间。您可以在 Ian Lewis 的[博文](https://oreil.ly/n51eq)中详细了解有关暂停容器的信息。
- en: '![Pod on Host](Images/neku_0503.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Pod on Host](Images/neku_0503.png)'
- en: Figure 5-3\. Pod on host
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 5-3 图。主机上的 Pod
- en: 'Before we deploy the services, we must first deploy the web server that the
    services will be routing traffic to, if we have not already:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署服务之前，我们必须首先部署 Web 服务器，这些服务将路由流量到它，如果我们尚未部署：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s look at each type of service starting with NodePort.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 NodePort 开始看每种类型的服务。
- en: NodePort
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodePort
- en: 'A NodePort service provides a simple way for external software, such as a load
    balancer, to route traffic to the pods. The software only needs to be aware of
    node IP addresses, and the service’s port(s). A NodePort service exposes a fixed
    port on all nodes, which routes to applicable pods. A NodePort service uses the
    `.spec.ports.[].nodePort` field to specify the port to open on all nodes, for
    the corresponding port on pods:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 服务为外部软件（如负载均衡器）提供了一种简单的方式来将流量路由到 Pod。该软件只需知道节点 IP 地址和服务的端口。NodePort
    服务在所有节点上公开一个固定端口，该端口将流量路由到适用的 Pod。NodePort 服务使用`.spec.ports.[].nodePort`字段指定要在所有节点上打开的端口，用于对应
    Pod 上的端口：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `nodePort` field can be left blank, in which case Kubernetes automatically
    selects a unique port. The `--service-node-port-range` flag in `kube-controller-manager`
    sets the valid range for ports, 30000–32767. Manually specified ports must be
    within this range.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `nodePort` 字段为空，则 Kubernetes 会自动选择一个唯一的端口。`kube-controller-manager` 中的 `--service-node-port-range`
    标志设置端口的有效范围为 30000–32767。手动指定的端口必须在此范围内。
- en: Using a NodePort service, external users can connect to the nodeport on any
    node and be routed to a pod on a node that has a pod backing that service; [Figure 5-4](#img-node-port-traffic)
    demonstrates this. The service directs traffic to node 3, and `iptables` rules
    forward the traffic to node 2 hosting the pod. This is a bit inefficient, as a
    typical connection will be routed to a pod on another node.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NodePort 服务，外部用户可以连接到任何节点上的 nodeport，并被路由到托管该服务的 Pod 的节点；[Figure 5-4](#img-node-port-traffic)展示了这一点。服务将流量定向到节点
    3，`iptables` 规则将流量转发到托管 Pod 的节点 2。这有点低效，因为典型的连接将被路由到另一个节点上的 Pod。
- en: '![Node Port Traffic Flow](Images/neku_0504.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Node Port Traffic Flow](Images/neku_0504.png)'
- en: Figure 5-4\. NodePort traffic flow
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 5-4 图。NodePort 流量流向
- en: '[Figure 5-4](#img-node-port-traffic) requires us to discuss an attribute of
    services, externalTrafficPolicy. ExternalTrafficPolicy indicates how a service
    will route external traffic to either node-local or cluster-wide endpoints. `Local`
    preserves the client source IP and avoids a second hop for LoadBalancer and NodePort
    type services but risks potentially imbalanced traffic spreading. Cluster obscures
    the client source IP and may cause a second hop to another node but should have
    good overall load-spreading. A `Cluster` value means that for each worker node,
    the `kube-proxy iptable` rules are set up to route the traffic to the pods backing
    the service anywhere in the cluster, just like we have shown in [Figure 5-4](#img-node-port-traffic).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](#img-node-port-traffic) 需要我们讨论服务的一个属性，即 externalTrafficPolicy。ExternalTrafficPolicy
    表示服务将如何将外部流量路由到节点本地或集群范围的端点。`Local` 会保留客户端源 IP，避免 LoadBalancer 和 NodePort 类型服务的第二跳，但可能导致流量分布不均衡。Cluster
    会隐藏客户端源 IP，并可能导致第二跳到另一个节点，但应具有良好的整体负载平衡。`Cluster` 值意味着对于每个工作节点，`kube-proxy iptable`
    规则都设置为将流量路由到集群中任何位置支持服务的 pod，就像我们在 [图 5-4](#img-node-port-traffic) 中展示的那样。'
- en: A `Local` value means the `kube-proxy iptable` rules are set up only on the
    worker nodes with relevant pods running to route the traffic local to the worker
    node. Using `Local` also allows application developers to preserve the source
    IP of the user request. If you set externalTrafficPolicy to the value `Local`,
    `kube-proxy` will proxy requests only to node-local endpoints and will not forward
    traffic to other nodes. If there are no local endpoints, packets sent to the node
    are dropped.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`Local` 值意味着 `kube-proxy iptable` 规则仅在运行相关 pod 的工作节点上设置，以将流量路由到工作节点本地。使用 `Local`
    还允许应用开发人员保留用户请求的源 IP。如果将 externalTrafficPolicy 设置为值 `Local`，`kube-proxy` 将仅代理请求到节点本地端点，并且不会将流量转发到其他节点。如果没有本地端点，则发送到节点的数据包将被丢弃。'
- en: 'Let’s scale up the deployment of our web app for some more testing:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展我们的 web 应用程序的部署以进行更多测试：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With four pods running, we will have one pod at every node in the cluster:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当有四个 pod 运行时，集群中每个节点将有一个 pod：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now let’s deploy our NodePort service:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署我们的 NodePort 服务：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To test the NodePort service, we must retrieve the IP address of a worker node:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试 NodePort 服务，我们必须获取一个工作节点的 IP 地址：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Communication external to the cluster will use a `NodePort` value of 30040 opened
    on each worker and the node worker’s IP address.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 集群外部的通信将在每个工作节点和节点工作的 IP 地址上开放一个值为 `NodePort` 的端口 30040。
- en: 'We can see that our pods are reachable on each host in the cluster:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的 pod 在集群中每个主机上都是可访问的：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It’s important to consider the limitations as well. A NodePort deployment will
    fail if it cannot allocate the requested port. Also, ports must be tracked across
    all applications using a NodePort service. Using manually selected ports raises
    the issue of port collisions (especially when applying a workload to multiple
    clusters, which may not have the same NodePorts free).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是考虑其限制。如果 NodePort 部署无法分配所请求的端口，则会失败。此外，端口必须在使用 NodePort 服务的所有应用程序之间进行跟踪。手动选择端口可能会引发端口冲突的问题（特别是在将工作负载应用于多个可能没有相同空闲
    NodePorts 的集群时）。
- en: Another downside of using the NodePort service type is that the load balancer
    or client software must be aware of the node IP addresses. A static configuration
    (e.g., an operator manually copying node IP addresses) may become too outdated
    over time (especially on a cloud provider) as IP addresses change or nodes are
    replaced. A reliable system automatically populates node IP addresses, either
    by watching which machines have been allocated to the cluster or by listing nodes
    from the Kubernetes API itself.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NodePort 服务类型的另一个缺点是负载均衡器或客户端软件必须知道节点 IP 地址。静态配置（例如运营人员手动复制节点 IP 地址）可能会随时间变化而过时（特别是在云提供商上），因为
    IP 地址会发生变化或节点被替换。可靠的系统会自动填充节点 IP 地址，可以通过观察已分配给集群的机器或从 Kubernetes API 中列出节点来实现。
- en: NodePorts are the earliest form of services. We will see that other service
    types use NodePorts as a base structure in their architecture. NodePorts should
    not be used by themselves, as clients would need to know the IP addresses of hosts
    and the node for connection requests. We will see how NodePorts are used to enable
    load balancers later in the chapter when we discuss cloud networks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 是服务的最早形式。我们将看到其他服务类型在其架构中使用 NodePort 作为基础结构。不应仅使用 NodePort，因为客户端需要知道主机和节点的
    IP 地址以进行连接请求。我们将在本章后面讨论云网络时看到如何使用 NodePort 来启用负载均衡器。
- en: Next up is the default type for services, ClusterIP.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是服务的默认类型，ClusterIP。
- en: ClusterIP
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClusterIP
- en: The IP addresses of pods share the life cycle of the pod and thus are not reliable
    for clients to use for requests. Services help overcome this pod networking design.
    A ClusterIP service provides an internal load balancer with a single IP address
    that maps to all matching (and ready) pods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: pod 的 IP 地址与 pod 的生命周期共享，因此不适合客户端用于请求。服务有助于克服这种 pod 网络设计。ClusterIP 服务提供了一个内部负载均衡器，具有一个单一的
    IP 地址，映射到所有匹配的（并且就绪的）pod。
- en: The service’s IP address must be within the CIDR set in `service-cluster-ip-range`,
    in the API server. You can specify a valid IP address manually, or leave `.spec.clusterIP`
    unset to have one assigned automatically. The ClusterIP service address is a virtual
    IP address that is routable only internally.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的 IP 地址必须在 API 服务器中 `service-cluster-ip-range` 中设置的 CIDR 范围内。您可以手动指定一个有效的
    IP 地址，或者将 `.spec.clusterIP` 置空以自动分配一个。ClusterIP 服务地址是一个虚拟 IP 地址，仅在内部可路由。
- en: '`kube-proxy` is responsible for making the ClusterIP service address route
    to all applicable pods. In “normal” configurations, `kube-proxy` performs L4 load
    balancing, which may not be sufficient. For example, older pods may see more load,
    due to accumulating more long-lived connections from clients. Or, a few clients
    making many requests may cause the load to be distributed unevenly.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-proxy` 负责将 ClusterIP 服务地址路由到所有适用的 pod。在“正常”配置中，`kube-proxy` 执行 L4 负载平衡，可能不足以满足需求。例如，旧的
    pod 可能由于客户端积累了更多的长连接而看到更多的负载。或者，少数客户端的大量请求可能导致负载不均匀分布。'
- en: A particular use case example for ClusterIP is when a workload requires a load
    balancer within the same cluster.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterIP 的一个特定用例示例是当工作负载需要同一集群内的负载均衡器时。
- en: In [Figure 5-5](#img-clusterip), we can see a ClusterIP service deployed. The
    service name is App with a selector, or App=App1. There are two pods powering
    this service. Pod 1 and Pod 5 match the selector for the service.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-5](#img-clusterip) 中，我们可以看到部署了一个 ClusterIP 服务。服务名称为 App，具有选择器，或者 App=App1。有两个支持该服务的
    pod。Pod 1 和 Pod 5 匹配服务的选择器。
- en: '![Cluster IP](Images/neku_0505.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![Cluster IP](Images/neku_0505.png)'
- en: Figure 5-5\. Cluster IP example service
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. Cluster IP 示例服务
- en: Let’s dig into an example on the command line with our KIND cluster.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入到使用我们的 KIND 集群的命令行中的示例中。
- en: 'We will deploy a ClusterIP service for use with our Golang web server:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为我们的 Golang web 服务器部署一个 ClusterIP 服务以供使用：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The ClusterIP service name is resolvable in the network:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterIP 服务名称在网络中是可解析的：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we can reach the host API endpoint with the Cluster IP address `10.98.252.195`,
    with the service name `clusterip-service`; or directly with the pod IP address
    `10.244.1.4` and port 8080:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 Cluster IP 地址 `10.98.252.195` 和服务名称 `clusterip-service` 或直接使用 pod IP
    地址 `10.244.1.4` 和端口 8080 来访问主机 API 端点：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The ClusterIP service is the default type for services. With that default status,
    it is warranted that we should explore what the ClusterIP service abstracted for
    us. If you recall from Chapters [2](ch02.xhtml#linux_networking) and [3](ch03.xhtml#container_networking_basics),
    this list is similar to what is set up with the Docker network, but we now also
    have `iptables` for the service across all nodes:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterIP 服务是服务的默认类型。在默认状态下，我们应该探索 ClusterIP 服务为我们抽象出了什么。如果您回忆起第 [2](ch02.xhtml#linux_networking)
    章和第 [3](ch03.xhtml#container_networking_basics) 章，此列表类似于在 Docker 网络中设置的内容，但现在我们还有
    `iptables` 用于所有节点的服务：
- en: View the VETH pair and match with the pod.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 VETH 对并与 pod 匹配。
- en: View the network namespace and match with the pod.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看网络命名空间并与 pod 匹配。
- en: Verify the PIDs on the node and match the pods.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证节点上的 PID 并与 pod 匹配。
- en: Match services with `iptables` rules.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将服务与 `iptables` 规则匹配。
- en: 'To explore this, we need to know what worker node the pod is deployed to, and
    that is `kind-worker2`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这一点，我们需要知道 pod 部署到哪个工作节点，即 `kind-worker2`：
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The container IDs and names will be different for you.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您的容器 ID 和名称将不同。
- en: 'Since we are using KIND, we can use `docker ps` and `docker exec` to get information
    out of the running worker node `kind-worker-2`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在使用 KIND，我们可以使用 `docker ps` 和 `docker exec` 从运行的工作节点 `kind-worker-2` 中获取信息：
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `kind-worker2` container ID is `df6df0736958`; KIND was *kind* enough to
    label each container with names, so we can reference each worker node with its
    name `kind-worker2`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`kind-worker2` 容器 ID 是 `df6df0736958`；KIND 很“kind”地用名称标记了每个容器，所以我们可以通过其名称 `kind-worker2`
    引用每个工作节点：'
- en: 'Let’s see the IP address and route table information of our pod, `app-9cc7d9df8-ffsm6`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看我们的 pod `app-9cc7d9df8-ffsm6` 的 IP 地址和路由表信息：
- en: '[PRE32]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Our pod’s IP address is `10.244.1.4` running on interface `eth0@if5` with `10.244.1.1`
    as its default route. That matches interface 5 on the pod `veth45d1f3e8@if5`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们pod的IP地址是`10.244.1.4`，运行在`eth0@if5`接口上，其默认路由为`10.244.1.1`。这与pod `veth45d1f3e8@if5`上的接口5匹配：
- en: '[PRE33]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s check the network namespace as well, from the `node ip a` output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也从`node ip a`输出中检查网络命名空间：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`netns list` confirms that the network namespaces match our pods, interface
    to the host interface, `cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`netns list`确认网络命名空间与我们的pod匹配，接口与主机接口，`cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4`：'
- en: '[PRE35]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s see what processes run inside that network namespace. For that we will
    use `docker exec` to run commands inside the node `kind-worker2` hosting the pod
    and its network namespace:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看那个网络命名空间内运行的进程。为此，我们将使用`docker exec`在托管该pod及其网络命名空间的节点`kind-worker2`上运行命令：
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we can `grep` for each process ID and inspect what they are doing:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以`grep`每个进程ID并检查它们在做什么：
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`4737` is the process ID of our web server container running on `kind-worker2`.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`4737`是运行在`kind-worker2`上的我们的Web服务器容器的进程ID。'
- en: '`4687` is our pause container holding onto all our namespaces.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`4687`是我们的暂停容器，持有所有我们的命名空间。'
- en: 'Now let’s see what will happen to the `iptables` on the worker node:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看工作节点上的`iptables`会发生什么变化：
- en: '[PRE38]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: That is a lot of tables being managed by Kubernetes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes正在管理大量的表格。
- en: 'We can dive a little deeper to examine the `iptables` responsible for the services
    we deployed. Let’s retrieve the IP address of the `clusterip-service` deployed.
    We need this to find the matching `iptables` rules:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以深入了解我们部署的服务所负责的`iptables`。让我们检索已部署的`clusterip-service`的IP地址。我们需要这个来找到匹配的`iptables`规则：
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now use the clusterIP of the service, `10.98.252.195`, to find our `iptables`
    rule:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用服务的`clusterIP`，`10.98.252.195`，来查找我们的`iptables`规则：
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'List all the rules on the chain `KUBE-SVC-V7R3EVKW3DT43QQM`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列出链`KUBE-SVC-V7R3EVKW3DT43QQM`上的所有规则：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `KUBE-SEP-` will contain the endpoints for the services, `KUBE-SEP-THJR2P3Q4C2QAEPT`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`KUBE-SEP-`将包含服务的端点，`KUBE-SEP-THJR2P3Q4C2QAEPT`。'
- en: 'Now we can see what the rules for this chain are in `iptables`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到`iptables`中这个链的规则是什么：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`10.244.1.4:8080` is one of the service endpoints, aka a pod backing the service,
    which is confirmed with the output of `kubectl get ep clusterip-service`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`10.244.1.4:8080`是服务端点之一，也就是支持服务的pod，这与`kubectl get ep clusterip-service`的输出确认一致：'
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now, let’s explore the limitations of the ClusterIP service. The ClusterIP service
    is for internal traffic to the cluster, and it suffers the same issues as endpoints
    do. As the service size grows, updates to it will slow. In [Chapter 2](ch02.xhtml#linux_networking),
    we discussed how to mitigate that by using IPVS over `iptables` as the proxy mode
    for `kube-proxy`. We will discuss later in this chapter how to get traffic into
    the cluster using ingress and the other service type LoadBalancer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索ClusterIP服务的限制。ClusterIP服务用于集群内部流量，并且面临与端点相同的问题。随着服务规模的增长，对其的更新将变慢。在[第2章](ch02.xhtml#linux_networking)中，我们讨论了通过将IPVS作为`kube-proxy`的代理模式来减轻这一问题。我们将在本章后面讨论如何通过Ingress和其他服务类型LoadBalancer将流量引入集群。
- en: ClusterIP is the default type of service, but there are several other specific
    types of services such as headless and ExternalName. ExternalName is a specific
    type of services that helps with reaching services outside the cluster. We briefly
    touched on headless services with StatefulSets, but let’s review those services
    in depth now.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterIP是默认的服务类型，但还有几种其他特定类型的服务，如无头服务和ExternalName。ExternalName是一种帮助访问集群外服务的特定类型服务。我们简要介绍了StatefulSets中的无头服务，现在让我们深入了解这些服务。
- en: Headless
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无头
- en: 'A headless service isn’t a formal type of service (i.e., there is no `.spec.type:
    Headless`). A headless service is a service with `.spec.clusterIP: "None"`. This
    is distinct from merely *not setting* a cluster IP address, which makes Kubernetes
    automatically assign a cluster IP address.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '无头服务不是服务的正式类型（即没有`.spec.type: Headless`）。无头服务是一种带有`.spec.clusterIP: "None"`的服务。这与仅*不设置*集群IP地址是不同的，后者使Kubernetes自动分配集群IP地址。'
- en: When ClusterIP is set to None, the service does not support any load balancing
    functionality. Instead, it only provisions an `Endpoints` object and points the
    service DNS record at all pods that are selected and ready.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当ClusterIP设置为None时，该服务不支持任何负载平衡功能。相反，它只会为所选和就绪的所有pod分配一个`Endpoints`对象，并将服务DNS记录指向它们。
- en: A headless service provides a generic way to watch endpoints, without needing
    to interact with the Kubernetes API. Fetching DNS records is much simpler than
    integrating with the Kubernetes API, and it may not be possible with third-party
    software.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务提供了一种通用的方式来监视端点，无需与Kubernetes API交互。获取DNS记录比与Kubernetes API集成简单得多，对于第三方软件可能无法实现。
- en: 'Headless services allow developers to deploy multiple copies of a pod in a
    deployment. Instead of a single IP address returned, like with the ClusterIP service,
    all the IP addresses of the endpoint are returned in the query. It then is up
    to the client to pick which one to use. To see this in action, let’s scale up
    the deployment of our web app:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务允许开发人员在部署中部署多个Pod的副本。与ClusterIP服务返回单个IP地址不同，查询返回所有端点的IP地址。然后由客户端选择使用哪个。为了看到这一点，请扩展我们Web应用程序的部署：
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now let’s deploy the headless service:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署无头服务：
- en: '[PRE45]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The DNS query will return all four of the pod IP addresses. Using our `dnsutils`
    image, we can verify that is the case:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: DNS查询将返回所有四个Pod IP地址。使用我们的`dnsutils`镜像，我们可以验证这一点：
- en: '[PRE46]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The IP addresses returned from the query also match the endpoints for the service.
    Using `kubectl describe` for the endpoint confirms that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 查询返回的IP地址也与服务的端点匹配。使用`kubectl describe`确认了端点的情况：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Headless has a specific use case and is not typically used for deployments.
    As we mentioned in [“StatefulSets”](#statefulsets), if developers need to let
    the client decide which endpoint to use, headless is the appropriate type of service
    to deploy. Two examples of headless services are clustered databases and applications
    that have client-side load-balancing logic built into the code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务有一个特定的用例，通常不用于部署。正如我们在[“StatefulSets”](#statefulsets)中提到的，如果开发人员需要让客户端决定使用哪个端点，则无头是部署的适当服务类型。无头服务的两个示例是集群数据库和在代码中构建了客户端负载均衡逻辑的应用程序。
- en: Our next example is ExternalName, which aids in migrations of services external
    to the cluster. It also offers other DNS advantages inside cluster DNS.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个示例是ExternalName，它有助于迁移到集群外的服务。它还在集群DNS内部提供其他DNS优势。
- en: ExternalName Service
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ExternalName服务
- en: ExternalName is a special type of service that does not have selectors and uses
    DNS names instead.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName是一种特殊类型的服务，它没有选择器，而是使用DNS名称。
- en: 'When looking up the host `ext-service.default.svc.cluster.local`, the cluster
    DNS service returns a CNAME record of `database.mycompany.com`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当查找主机`ext-service.default.svc.cluster.local`时，集群DNS服务返回了`database.mycompany.com`的CNAME记录：
- en: '[PRE48]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: If developers are migrating an application into Kubernetes but its dependencies
    are staying external to the cluster, ExternalName service allows them to define
    a DNS record internal to the cluster no matter where the service actually runs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果开发人员将应用程序迁移到Kubernetes，但其依赖项保留在集群外部，ExternalName服务允许他们定义一个内部集群的DNS记录，无论服务实际运行在哪里。
- en: 'DNS will try the search as shown in the following example:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: DNS将尝试如下示例中显示的搜索：
- en: '[PRE49]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As an example, the ExternalName service allows developers to map a service to
    a DNS name.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ExternalName服务允许开发人员将一个服务映射到一个DNS名称。
- en: 'Now if we deploy the external service like so:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们这样部署外部服务：
- en: '[PRE50]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The A record for github.com is returned from the `external-service` query:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: github.com的A记录从`external-service`查询返回：
- en: '[PRE51]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The CNAME for the external service returns github.com:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 外部服务的CNAME返回github.com：
- en: '[PRE52]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Sending traffic to a headless service via a DNS record is possible but inadvisable.
    DNS is a notoriously poor way to load balance, as software takes very different
    (and often simple or unintuitive) approaches to A or AAAA DNS records that return
    multiple IP addresses. For example, it is common for software to always choose
    the first IP address in the response and/or cache and reuse the same IP address
    indefinitely. If you need to be able to send traffic to the service’s DNS address,
    consider a (standard) ClusterIP or LoadBalancer service.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过DNS记录发送流量到无头服务是可能的，但不建议这样做。DNS作为负载均衡的方式并不理想，因为软件在处理返回多个IP地址的A或AAAA DNS记录时采取的方法很不同（通常是简单或不直观的方法），例如，软件通常会选择响应中的第一个IP地址并/或者缓存并重复使用同一个IP地址。如果需要能够发送流量到服务的DNS地址，请考虑（标准的）ClusterIP或LoadBalancer服务。
- en: The “correct” way to use a headless service is to query the service’s A/AAAA
    DNS record and use that data in a server-side or client-side load balancer.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无头服务的“正确”方法是查询服务的A/AAAA DNS记录，并在服务器端或客户端负载均衡器中使用该数据。
- en: Most of the services we have been discussing are for internal traffic management
    for the cluster network. In our next sections, will be reviewing how to route
    requests into the cluster with service type LoadBalancer and ingress.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的大多数服务都是用于集群网络的内部流量管理。在接下来的章节中，我们将会审查如何通过LoadBalancer和Ingress类型服务将请求路由到集群中。
- en: LoadBalancer
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: LoadBalancer service exposes services external to the cluster network. They
    combine the NodePort service behavior with an external integration, such as a
    cloud provider’s load balancer. Notably, LoadBalancer services handle L4 traffic
    (unlike ingress, which handles L7 traffic), so they will work for any TCP or UDP
    service, provided the load balancer selected supports L4 traffic.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器服务将服务暴露给集群网络之外的外部。它们结合了NodePort服务的行为和外部集成，如云服务提供商的负载均衡器。值得注意的是，负载均衡器服务处理L4流量（不像Ingress处理L7流量），因此它们适用于任何TCP或UDP服务，只要所选的负载均衡器支持L4流量。
- en: 'Configuration and load balancer options are extremely dependent on the cloud
    provider. For example, some will support `.spec.loadBalancerIP` (with varying
    setup required), and some will ignore it:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 配置和负载均衡选项极大地依赖于云服务提供商。例如，有些支持`.spec.loadBalancerIP`（需要不同的设置），而有些则会忽略它：
- en: '[PRE53]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Once the load balancer has been provisioned, its IP address will be written
    to `.status.loadBalancer.ingress.ip`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦负载均衡器被配置完成，其IP地址将被写入`.status.loadBalancer.ingress.ip`。
- en: LoadBalancer services are useful for exposing TCP or UDP services to the outside
    world. Traffic will come into the load balancer on its public IP address and TCP
    port 80, defined by `spec.ports[*].port` and routed to the cluster IP address,
    `10.0.5.1`, and then to container target port 8080, `spec.ports[*].targetPort`.
    Not shown in the example is the `.spec.ports[*].nodePort`; if not specified, Kubernetes
    will pick one for the service.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器服务非常适用于将TCP或UDP服务暴露给外部。流量将通过其公共IP地址和TCP端口80进入负载均衡器，由`spec.ports[*].port`定义，并路由到集群IP地址`10.0.5.1`，然后进入容器目标端口8080，`spec.ports[*].targetPort`。示例中未显示的是`.spec.ports[*].nodePort`；如果未指定，Kubernetes会为服务选择一个。
- en: Tip
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The service’s `spec.ports[*].targetPort` must match your pod’s container applications
    `spec.container[*].ports.containerPort`, along with the protocol. It’s like missing
    a semicolon in Kubernetes networking otherwise.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的`spec.ports[*].targetPort`必须与您的Pod容器应用程序的`spec.container[*].ports.containerPort`及其协议匹配。否则，在Kubernetes网络中会像缺少分号一样。
- en: In [Figure 5-6](#loadbalancer), we can see how a LoadBalancer type builds on
    the other service types. The cloud load balancer will determine how to distribute
    traffic; we will discuss that in depth in the next chapter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 5-6](#loadbalancer)中，我们可以看到负载均衡器类型是如何在其他服务类型基础上构建的。云负载均衡器将决定如何分发流量；我们将在下一章节深入讨论这一点。
- en: '![LoadBalancer service](Images/neku_0506.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![负载均衡器服务](Images/neku_0506.png)'
- en: Figure 5-6\. LoadBalancer service
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 负载均衡器服务
- en: Let’s continue to extend our Golang web server example with a LoadBalancer service.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续通过LoadBalancer服务扩展我们的Golang Web服务器示例。
- en: Since we are running on our local machine and not in a service provider like
    AWS, GCP, or Azure, we can use MetalLB as an example for our LoadBalancer service.
    The MetalLB project aims to allow users to deploy bare-metal load balancers for
    their clusters.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们是在本地机器上运行，而不是在像AWS、GCP或Azure这样的服务提供商上，所以我们可以使用MetalLB作为负载均衡器服务的示例。MetalLB项目旨在允许用户为其集群部署裸金属负载均衡器。
- en: This example has been modified from the [KIND example deployment](https://oreil.ly/h8xIt).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例已从[KIND示例部署](https://oreil.ly/h8xIt)修改而来。
- en: 'Our first step is to deploy a separate namespace for MetalLB:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是为MetalLB部署一个单独的命名空间：
- en: '[PRE54]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'MetalLB members also require a secret for joining the LoadBalancer cluster;
    let’s deploy one now for them to use in our cluster:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB成员还需要一个用于加入负载均衡器集群的秘钥；让我们现在为他们在我们的集群中部署一个：
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now we can deploy MetalLB!
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以部署MetalLB了！
- en: '[PRE56]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'As you can see, it deploys many objects, and now we wait for the deployment
    to finish. We can monitor the deployment of resources with the `--watch` option
    in the `metallb-system` namespace:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它部署了许多对象，现在我们等待资源的部署完成。我们可以在`metallb-system`命名空间中使用`--watch`选项监视资源的部署：
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To complete the configuration, we need to provide MetalLB with a range of IP
    addresses it controls. This range has to be on the Docker KIND network:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成配置，我们需要为MetalLB提供一个它控制的IP地址范围。此范围必须在Docker KIND网络上：
- en: '[PRE58]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`172.18.0.0/16` is our Docker network running locally.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`172.18.0.0/16`是我们在本地运行的Docker网络。'
- en: We want our LoadBalancer IP range to come from this subclass. We can configure
    MetalLB, for instance, to use `172.18.255.200` to `172.18.255.250` by creating
    the ConfigMap.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的 LoadBalancer IP 范围来自这个子类。例如，我们可以通过创建 ConfigMap 配置 MetalLB，使用 `172.18.255.200`
    到 `172.18.255.250`：
- en: 'The ConfigMap would look like this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMap 将如下所示：
- en: '[PRE59]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let’s deploy it so we can use MetalLB:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署它，这样我们就可以使用 MetalLB：
- en: '[PRE60]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now we deploy a load balancer for our web app:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为我们的 web 应用程序部署一个负载均衡器：
- en: '[PRE61]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'For fun let’s scale the web app deployment to 10, if you have the resources
    for it:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，让我们将 web 应用程序部署扩展到 10 个，如果你有资源的话：
- en: '[PRE62]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now we can test the provisioned load balancer.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以测试配置的负载均衡器。
- en: 'With more replicas deployed for our app behind the load balancer, we need the
    external IP of the load balancer, `172.18.255.200`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们应用程序在负载均衡器后面部署更多副本，我们需要负载均衡器的外部 IP，`172.18.255.200`：
- en: '[PRE63]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Since Docker for Mac or Windows does not expose the KIND network to the host,
    we cannot directly reach the `172.18.255.200` LoadBalancer IP on the Docker private
    network.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Docker for Mac 或 Windows 不会将 KIND 网络暴露给主机，所以无法直接访问 Docker 私有网络上的 `172.18.255.200`
    LoadBalancer IP。
- en: We can simulate it by attaching a Docker container to the KIND network and cURLing
    the load balancer as a workaround.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将 Docker 容器附加到 KIND 网络并模拟 cURL 负载均衡器来模拟它。
- en: Tip
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you would like to read more about this issue, there is a great [blog post](https://oreil.ly/6rTKJ).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步了解这个问题，这里有一篇很棒的 [博客文章](https://oreil.ly/6rTKJ)。
- en: We will use another great networking Docker image called `nicolaka/netshoot`
    to run locally, attach to the KIND Docker network, and send requests to our MetalLB
    load balancer.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用另一个称为 `nicolaka/netshoot` 的出色的网络 Docker 镜像在本地运行，附加到 KIND Docker 网络，并发送请求到我们的
    MetalLB 负载均衡器。
- en: 'If we run it several times, we can see the load balancer is doing its job of
    routing traffic to different pods:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行多次，我们可以看到负载均衡器正在有效地将流量路由到不同的 pod：
- en: '[PRE64]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'With each new request, the metalLB service is sending requests to different
    pods. LoadBalancer, like other services, uses selectors and labels for the pods,
    and we can see that in the `kubectl describe endpoints loadbalancer-service`.
    The pod IP addresses match our results from the cURL commands:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新请求时，metalLB 服务都会将请求发送到不同的 pod。LoadBalancer 与其他服务一样，使用选择器和标签来选择 pod，我们可以在
    `kubectl describe endpoints loadbalancer-service` 中看到这一点。Pod IP 地址与我们从 cURL 命令的结果匹配：
- en: '[PRE65]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: It is important to remember that LoadBalancer services require specific integrations
    and will not work without cloud provider support, or manually installed software
    such as MetalLB.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，LoadBalancer 服务需要特定的集成，并且如果没有云提供商支持或手动安装软件如 MetalLB，将无法工作。
- en: They are not (normally) L7 load balancers, and therefore cannot intelligently
    handle HTTP(S) requests. There is a one-to-one mapping of load balancer to workload,
    which means that all requests sent to that load balancer must be handled by the
    same workload.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通常不是 L7 负载均衡器，因此无法智能处理 HTTP(S) 请求。负载均衡器与工作负载是一对一映射，这意味着发送到该负载均衡器的所有请求必须由相同的工作负载处理。
- en: Tip
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: While it’s not a network service, it is important to mention the Horizontal
    Pod Autoscaler service, which that will scale pods in a replication controller,
    deployment, ReplicaSet, or StatefulSet based on CPU utilization.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它不是网络服务，但重要的是提到 Horizontal Pod Autoscaler 服务，它将根据 CPU 利用率扩展副本控制器、部署、ReplicaSet
    或 StatefulSet 中的 pod。
- en: We can scale our application to the demands of the users, with no need for configuration
    changes on anyone’s part. Kubernetes and the LoadBalancer service take care of
    all of that for developers, systems, and network administrators.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据用户的需求扩展我们的应用程序，而无需任何人员的配置更改。Kubernetes 和 LoadBalancer 服务会为开发人员、系统和网络管理员处理所有这些。
- en: We will see in the next chapter how we can take that even further using cloud
    services for autoscaling.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章节看到如何通过云服务进一步扩展这一点。
- en: Services Conclusion
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务总结
- en: 'Here are some troubleshooting tips if issues arise with the endpoints or services:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现端点或服务问题，这里有一些故障排除提示：
- en: Removing the label on the pod allows it to continue to run while also updating
    the endpoint and service. The endpoint controller will remove that unlabeled pod
    from the endpoint objects, and the deployment will deploy another pod; this will
    allow you to troubleshoot issues with that specific unlabeled pod but not adversely
    affect the service to end customers. I’ve used this one countless times during
    development, and we did so in the previous section’s examples.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除 pod 上的标签允许其继续运行，并更新端点和服务。端点控制器将从端点对象中删除未标记的 pod，并重新部署另一个 pod；这将允许您调试特定未标记的
    pod 的问题，但不会对最终客户的服务产生不利影响。在开发过程中，我经常使用这个方法，我们在上一节的示例中也这样做了。
- en: There are two probes that communicate the pod’s health to the Kubelet and the
    rest of the Kubernetes environment.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有两个探测器将 pod 的健康状态传达给 Kubelet 和 Kubernetes 环境的其余部分。
- en: It is also easy to mess up the YAML manifest, so make sure to compare ports
    on the service and pods and make sure they match.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YAML 配置很容易弄乱，因此请确保比较服务和 pod 上的端口，并确保它们匹配。
- en: We discussed network policies in [Chapter 3](ch03.xhtml#container_networking_basics),
    which can also stop pods from communicating with each other and services. If your
    cluster network is using network policies, ensure that they are set up appropriately
    for application traffic flow.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在[第三章](ch03.xhtml#container_networking_basics)讨论了网络策略，这也可以阻止 pod 之间及服务之间的通信。如果您的集群网络正在使用网络策略，请确保为应用程序流量正确设置它们。
- en: Also remember to use diagnostic tools like the `dnsutils` pod; the `netshoot`
    pods on the cluster network are helpful debugging tools.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还要记得使用诊断工具，如 `dnsutils` pod；集群网络上的 `netshoot` pod 是有用的调试工具。
- en: 'If endpoints are taking too long to come up in the cluster, there are several
    options that can be configured on the Kubelet to control how fast it responds
    to change in the Kubernetes environment:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果端点在集群中启动时间过长，可以在 Kubelet 上配置几个选项来控制其对 Kubernetes 环境中变化的响应速度：
- en: '`--kube-api-qps`'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`--kube-api-qps`'
- en: Sets the query-per-second rate the Kubelet will use when communicating with
    the Kubernetes API server; the default is 5.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置 Kubelet 在与 Kubernetes API 服务器通信时使用的每秒查询速率；默认为 5。
- en: '`--kube-api-burst`'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`--kube-api-burst`'
- en: Temporarily allows API queries to burst to this number; the default is 10.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 临时允许 API 查询突发到这个数字；默认值为 10。
- en: '`--iptables-sync-period`'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`--iptables-sync-period`'
- en: This is the maximum interval of how often `iptables` rules are refreshed (e.g.,
    5s, 1m, 2h22m). This must be greater than 0; the default is 30s.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是刷新 `iptables` 规则的最大间隔时间（例如 5 秒，1 分钟，2 小时22分钟）。必须大于 0；默认为 30 秒。
- en: '`--ipvs-sync-period duration`'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`--ipvs-sync-period duration`'
- en: This is the maximum interval of how often IPVS rules are refreshed. This must
    be greater than 0; the efault is 30s.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是刷新 IPVS 规则的最大间隔时间。必须大于 0；默认为 30 秒。
- en: Increasing these options for larger clusters is recommended, but also remember
    this increases the resources on both the Kubelet and the API server, so keep that
    in mind.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议针对较大的集群增加这些选项，但也请记住，这会增加 Kubelet 和 API 服务器的资源使用量，所以请注意。
- en: These tips can help alleviate issues and are good to be aware of as the number
    of services and pods grow in the cluster.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这些提示可以帮助减轻问题，并且随着集群中服务和 pod 的数量增长，了解这些是很好的。
- en: The various types of services exemplify how powerful the network abstractions
    are in Kubernetes. We have dug deep into how these work for each layer of the
    tool chain. Developers looking to deploy applications to Kubernetes now have the
    knowledge to pick and choose which services are right for their use cases. No
    longer will network administrators have to manually update load balancers with
    IP addresses, with Kubernetes managing that for them.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 各种类型的服务展示了 Kubernetes 中网络抽象的强大。我们深入研究了如何为工具链的每一层配置这些服务。希望要将应用程序部署到 Kubernetes
    的开发人员现在具备了选择合适服务的知识。不再需要网络管理员手动更新负载均衡器的 IP 地址，Kubernetes 会为他们管理这些。
- en: We have just scratched the surface of what is possible with services. With each
    new version of Kubernetes, there are options to tune and configurations to run
    services. Test each service for your use cases and ensure you are using the appropriate
    services to optimize your applications on the Kubernetes network.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚触及了服务可能性的表面。随着 Kubernetes 的每个新版本，都有调整选项和运行服务的配置。为您的用例测试每个服务，并确保您使用适当的服务来优化
    Kubernetes 网络上的应用程序。
- en: The LoadBalancer service type is the only one that allows for traffic into the
    cluster, exposing HTTP(S) services behind a load balancer for external users to
    connect to. Ingresses support path-based routing, which allows different HTTP
    paths to be served by different services. The next section will discuss ingress
    and how it is an alternative to managing connectivity into the cluster resources.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer 服务类型是唯一允许流量进入集群的类型，通过负载均衡器公开 HTTP(S) 服务，供外部用户连接使用。Ingress 支持基于路径的路由，允许不同的
    HTTP 路径由不同的服务提供。接下来的部分将讨论 Ingress 及其作为管理集群资源连接的替代方法。
- en: Ingress
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress
- en: Ingress is a Kubernetes-specific L7 (HTTP) load balancer, which is accessible
    externally, contrasting with L4 ClusterIP service, which is internal to the cluster.
    This is the typical choice for exposing an HTTP(S) workload to external users.
    An ingress can be a single entry point into an API or a microservice-based architecture.
    Traffic can be routed to services based on HTTP information in the request. Ingress
    is a configuration spec (with multiple implementations) for routing HTTP traffic
    to Kubernetes services. [Figure 5-7](#img-ingress) outlines the ingress components.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 是 Kubernetes 特有的 L7（HTTP）负载均衡器，可以从外部访问，与集群内部的 L4 ClusterIP 服务形成对比。这通常是暴露
    HTTP(S) 工作负载给外部用户的典型选择。Ingress 可以是 API 或基于微服务架构的单一入口点。流量可以根据请求中的 HTTP 信息路由到服务。Ingress
    是一个配置规范（具有多种实现），用于将 HTTP 流量路由到 Kubernetes 服务。[图 5-7](#img-ingress) 概述了 Ingress
    的组件。
- en: '![Ingress](Images/neku_0507.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![Ingress](Images/neku_0507.png)'
- en: Figure 5-7\. Ingress architecture
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. Ingress 架构
- en: 'To manage traffic in a cluster with ingress, there are two components required:
    the controller and rules. The controller manages ingress pods, and the rules deployed
    define how the traffic is routed.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ingress 管理集群中的流量需要两个必要组件：控制器和规则。控制器管理 Ingress pods，而部署的规则定义了流量的路由方式。
- en: Ingress Controllers and Rules
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress 控制器和规则
- en: We call ingress implementations ingress *controllers*. In Kubernetes, a controller
    is software that is responsible for managing a typical resource type and making
    reality match the desired state.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称 Ingress 实现为 Ingress *控制器*。在 Kubernetes 中，控制器是负责管理典型资源类型并使实际状态与期望状态匹配的软件。
- en: 'There are two general kinds of controllers: external load balancer controllers
    and internal load balancer controllers. External load balancer controllers create
    a load balancer that exists “outside” the cluster, such as a cloud provider product.
    Internal load balancer controllers deploy a load balancer that runs within the
    cluster and do not directly solve the problem of routing consumers to the load
    balancer. There are a myriad of ways that cluster administrators run internal
    load balancers, such as running the load balancer on a subset of special nodes,
    and routing traffic somehow to those nodes. The primary motivation for choosing
    an internal load balancer is cost reduction. An internal load balancer for ingress
    can route traffic for multiple ingress objects, whereas an external load balancer
    controller typically needs one load balancer per ingress. As most cloud providers
    charge by load balancer, it is cheaper to support a single cloud load balancer
    that does fan-out within the cluster, than many cloud load balancers. Note that
    this incurs operational overhead and increased latency and compute costs, so be
    sure the money you’re saving is worth it. Many companies have a bad habit of optimizing
    on inconsequential cloud spend line items.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见类型的控制器：外部负载均衡器控制器和内部负载均衡器控制器。外部负载均衡器控制器创建一个存在于集群“外部”的负载均衡器，如云提供商的产品。内部负载均衡器控制器部署一个运行在集群内部的负载均衡器，并不直接解决将消费者路由到负载均衡器的问题。集群管理员运行内部负载均衡器的方式有多种，例如在特定节点子集上运行负载均衡器，并以某种方式将流量路由到这些节点。选择内部负载均衡器的主要动机是降低成本。Ingress
    的内部负载均衡器可以为多个 Ingress 对象路由流量，而外部负载均衡器控制器通常每个 Ingress 需要一个负载均衡器。由于大多数云提供商按负载均衡器收费，支持集群内部的单个云负载均衡器比多个云负载均衡器更便宜。但要注意，这会增加操作开销、延迟和计算成本，所以务必确保节省的资金是值得的。许多公司有优化无关紧要的云支出项目的不良习惯。
- en: 'Let’s look at the spec for an ingress controller. Like LoadBalancer services,
    most of the spec is universal, but various ingress controllers have different
    features and accept different configs. We’ll start with the basics:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下 Ingress 控制器的规范。与 LoadBalancer 服务一样，大多数规范是通用的，但不同的 Ingress 控制器具有不同的功能并接受不同的配置。我们将从基础知识开始：
- en: '[PRE66]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The previous example is representative of a typical ingress. It sends traffic
    to `/demo` to one service and all other traffic to another. Ingresses have a “default
    backend” where requests are routed if no rule matches. This can be configured
    in many ingress controllers in the controller configuration itself (e.g., a generic
    404 page), and many support the `.spec.defaultBackend` field. Ingresses support
    multiple ways to specify a path. There are currently three:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子代表了一个典型的 Ingress。它将流量发送到 `/demo` 的一个服务，将所有其他流量发送到另一个服务。Ingress 有一个“默认后端”，如果没有匹配的规则，请求将路由到它。这可以在许多
    Ingress 控制器中的控制器配置本身中配置（例如通用的 404 页面），并且许多支持 `.spec.defaultBackend` 字段。Ingress
    支持多种指定路径的方法。目前有三种：
- en: Exact
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 精确
- en: Matches the specific path and only the given path (including trailing `/` or
    lack thereof).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 仅匹配特定路径及该路径（包括结尾的 `/` 或缺失的 `/`）。
- en: Prefix
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀
- en: Matches all paths that start with the given path.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配所有以给定路径开头的路径。
- en: ImplementationSpecific
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ImplementationSpecific
- en: Allows for custom semantics from the current ingress controller.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 允许从当前 Ingress 控制器获得自定义语义。
- en: When a request matches multiple paths, the most specific match is chosen. For
    example, if there are rules for `/first` and `/first/second`, any request starting
    with `/first/second` will go to the backend for `/first/second`. If a path matches
    an exact path and a prefix path, the request will go to the backend for the exact
    rule.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当请求匹配多个路径时，将选择最具体的匹配。例如，如果有 `/first` 和 `/first/second` 的规则，任何以 `/first/second`
    开头的请求将转发到 `/first/second` 的后端。如果路径匹配精确路径和前缀路径，请求将转发到精确规则的后端。
- en: 'Ingresses can also use hostnames in rules:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 还可以在规则中使用主机名：
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In this example, we serve traffic to `a.example.com` from one service and traffic
    to `b.example.com` from another. This is comparable to virtual hosts in web servers.
    You may want to use host rules to use a single load balancer and IP to serve multiple
    unique domains.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们从一个服务向 `a.example.com` 提供流量，从另一个服务向 `b.example.com` 提供流量。这类似于 Web 服务器中的虚拟主机。您可能希望使用主机规则来使用单个负载均衡器和
    IP 来为多个唯一域名提供服务。
- en: 'Ingresses have basic TLS support:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 具有基本的 TLS 支持：
- en: '[PRE68]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The TLS config references a Kubernetes secret by name, in `.spec.tls.[*].secretName`.
    Ingress controllers expect the TLS certificate and key to be provided in `.data."tls.crt"`
    and `.data."tls.key"` respectively, as shown here:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: TLS 配置引用一个 Kubernetes 密钥，位于 `.spec.tls.[*].secretName`。Ingress 控制器期望在 `.data."tls.crt"`
    和 `.data."tls.key"` 中提供 TLS 证书和密钥，如下所示：
- en: '[PRE69]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Tip
  id: totrans-337
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you don’t need to manage traditionally issued certificates by hand, you can
    use [cert-manager](https://oreil.ly/qkN0h) to automatically fetch and update certs.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不需要手动管理传统颁发的证书，可以使用 [cert-manager](https://oreil.ly/qkN0h) 自动获取和更新证书。
- en: We mentioned earlier that ingress is simply a spec, and drastically different
    implementations exist. It’s possible to use multiple ingress controllers in a
    single cluster, using `IngressClass` settings. An ingress class represents an
    ingress controller, and therefore a specific ingress implementation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们前面提到，Ingress 只是一个规范，存在极大的不同实现。可以在单个集群中使用多个 Ingress 控制器，使用 `IngressClass` 设置。Ingress
    类表示一个 Ingress 控制器，因此表示特定的 Ingress 实现。
- en: Warning
  id: totrans-340
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Annotations in Kubernetes must be strings. Because `true` and `false` have distinct
    nonstring meanings, you cannot set an annotation to `true` or `false` without
    quotes. `"true"` and `"false"` are both valid. This is a [long-running bug](https://oreil.ly/76uSI),
    which is often encountered when setting a default priority class.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的注解必须是字符串。因为 `true` 和 `false` 有不同的非字符串含义，所以不能在不加引号的情况下设置注解为 `true`
    或 `false`。`"true"` 和 `"false"` 都是有效的。这是一个[长期存在的问题](https://oreil.ly/76uSI)，在设置默认优先级类时经常会遇到。
- en: '`IngressClass` was introduced in Kubernetes 1.18. Prior to 1.18, annotating
    ingresses with `kubernetes.io/ingress.class` was a common convention but relied
    on all installed ingress controllers to support it. Ingresses can pick an ingress
    class by setting the class’s name in `.spec.ingressClassName`.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '`IngressClass` 在 Kubernetes 1.18 中引入。在 1.18 之前，使用 `kubernetes.io/ingress.class`
    注解 Ingress 是一种常见的约定，但依赖于所有已安装的 Ingress 控制器是否支持它。Ingress 可以通过在 `.spec.ingressClassName`
    中设置类名来选择一个 Ingress 类。'
- en: Warning
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If more than one ingress class is set as default, Kubernetes will not allow
    you to create an ingress with no ingress class or remove the ingress class from
    an existing ingress. You can use admission control to prevent multiple ingress
    classes from being marked as default.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置了多个默认的入口类别，默认情况下，Kubernetes 将不允许您创建没有入口类别或从现有入口中移除入口类别。您可以使用准入控制来防止将多个入口类别标记为默认。
- en: Ingress only supports HTTP(S) requests, which is insufficient if your service
    uses a different protocol (e.g., most databases use their own protocols). Some
    ingress controllers, such as the NGINX ingress controller, do support TCP and
    UDP, but this is not the norm.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 入口仅支持 HTTP(S) 请求，如果您的服务使用不同的协议（例如，大多数数据库使用其自己的协议），这是不够的。某些入口控制器，如 NGINX 入口控制器，支持
    TCP 和 UDP，但这不是标准。
- en: Now on to deploying an ingress controller so we can add ingress rules to our
    Golang web server example.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来部署一个入口控制器，以便我们可以向我们的 Golang Web 服务器示例添加入口规则。
- en: 'When we deployed our KIND cluster, we had to add several options to allow us
    to deploy an ingress controller:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署 KIND 群集时，我们必须添加几个选项，以便我们可以部署入口控制器：
- en: extraPortMappings allow the local host to make requests to the ingress controller
    over ports 80/443.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: extraPortMappings 允许本地主机通过端口 80/443 请求入口控制器。
- en: Node-labels only allow the ingress controller to run on a specific node(s) matching
    the label selector.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Node-labels 仅允许入口控制器在匹配标签选择器的特定节点上运行。
- en: There are many options to choose from with ingress controllers. The Kubernetes
    system does not start or have a default controller like it does with other pieces.
    The Kubernetes community does support AWS, GCE, and Nginx ingress controllers.
    [Table 5-1](#brief_list_of_ingress_controller_options) outlines several options
    for ingress.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 具有入口控制器的选择有很多。与其他组件不同，Kubernetes 系统没有默认控制器或启动控制器。Kubernetes 社区支持 AWS、GCE 和 Nginx
    入口控制器。[Table 5-1](#brief_list_of_ingress_controller_options) 概述了几个入口的选项。
- en: Table 5-1\. Brief list of ingress controller options
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-1\. 入口控制器选项简要列表
- en: '| Name | Commercial support | Engine | Protocol support | SSL termination |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 商业支持 | 引擎 | 协议支持 | SSL 终止 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Ambassador ingress controller | Yes | Envoy | gRPC, HTTP/2, WebSockets |
    Yes |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Ambassador 入口控制器 | 是 | Envoy | gRPC, HTTP/2, WebSockets | 是 |'
- en: '| Community ingress Nginx | No | NGINX | gRPC, HTTP/2, WebSockets | Yes |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 社区版入口 Nginx | 否 | NGINX | gRPC, HTTP/2, WebSockets | 是 |'
- en: '| NGINX Inc. ingress | Yes | NGINX | HTTP, Websocket, gRPC | Yes |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| NGINX Inc. 入口 | 是 | NGINX | HTTP, Websocket, gRPC | 是 |'
- en: '| HAProxy ingress | Yes | HAProxy | gRPC, HTTP/2, WebSockets | Yes |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| HAProxy 入口 | 是 | HAProxy | gRPC, HTTP/2, WebSockets | 是 |'
- en: '| Istio Ingress | No | Envoy | HTTP, HTTPS, gRPC, HTTP/2 | Yes |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Istio 入口 | 否 | Envoy | HTTP, HTTPS, gRPC, HTTP/2 | 是 |'
- en: '| Kong ingress controller for Kubernetes | Yes | Lua on top of Nginx | gRPC,
    HTTP/2 | Yes |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 用于 Kubernetes 的 Kong 入口控制器 | 是 | Lua 在 Nginx 之上 | gRPC, HTTP/2 | 是 |'
- en: '| Traefik Kubernetes ingress | Yes | Traefik | HTTP/2, gRPC, and WebSockets
    | Yes |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Traefik Kubernetes 入口 | 是 | Traefik | HTTP/2, gRPC 和 WebSockets | 是 |'
- en: 'Some things to consider when deciding on the ingress for your clusters:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择群集的入口时需要考虑一些事项：
- en: 'Protocol support: Do you need more than TCP/UDP, for example gRPC integration
    or WebSocket?'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协议支持：您是否需要超过 TCP/UDP 的更多支持，例如 gRPC 集成或 WebSocket？
- en: 'Commercial support: Do you need commercial support?'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业支持：您是否需要商业支持？
- en: 'Advanced features: Are JWT/oAuth2 authentication or circuit breakers requirements
    for your applications?'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级功能：您的应用程序是否需要 JWT/oAuth2 认证或断路器功能？
- en: 'API gateway features: Do you need some API gateway functionalities such as
    rate-limiting?'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 网关功能：您是否需要一些 API 网关功能，例如速率限制？
- en: 'Traffic distribution: Does your application require support for specialized
    traffic distribution like canary A/B testing or mirroring?'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量分发：您的应用程序是否需要支持特殊的流量分发，例如金丝雀 A/B 测试或镜像？
- en: For our example, we have chosen to use the Community version of the NGINX ingress
    controller.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们选择使用 NGINX 入口控制器的社区版。
- en: Tip
  id: totrans-368
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For more ingress controllers to choose from, [kubernetes.io](https://oreil.ly/Lzn5q)
    maintains a list.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择更多入口控制器，请访问 [kubernetes.io](https://oreil.ly/Lzn5q) 维护的列表。
- en: 'Let’s deploy the NGINX ingress controller into our KIND cluster:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 NGINX 入口控制器部署到我们的 KIND 群集中：
- en: '[PRE70]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'As with all deployments, we must wait for the controller to be ready before
    we can use it. With the following command, we can verify if our ingress controller
    is ready for use:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有部署一样，我们必须等待控制器准备就绪，然后才能使用它。使用以下命令，我们可以验证我们的入口控制器是否准备好供使用：
- en: '[PRE71]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The controller is deployed to the cluster, and now we’re ready to write ingress
    rules for our application.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器已部署到集群中，现在我们准备为我们的应用编写入口规则。
- en: Deploy ingress rules
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署入口规则
- en: 'Our YAML manifest defines several ingress rules to use with our Golang web
    server example:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的YAML清单定义了几个入口规则，用于我们的Golang Web服务器示例：
- en: '[PRE72]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'With `describe` we can see all the backends that map to the ClusterIP service
    and the pods:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `describe` 我们可以看到映射到 ClusterIP 服务和 pod 的所有后端：
- en: '[PRE73]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Our ingress rule is only for the `/host` route and will route requests to our
    `clusterip-service:8080` service.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的入口规则仅适用于 `/host` 路由，并将请求路由到我们的 `clusterip-service:8080` 服务。
- en: 'We can test that with cURL to http://localhost/host:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 cURL 来测试 http://localhost/host：
- en: '[PRE74]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Now we can see how powerful ingresses are; let’s deploy a second deployment
    and ClusterIP service.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到入口规则有多么强大；让我们部署第二个部署和ClusterIP服务。
- en: 'Our new deployment and service will be used to answer the requests for `/data`:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新部署和服务将用于响应 `/data` 的请求：
- en: '[PRE75]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now both the `/host` and `/data` work but are going to separate services:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `/host` 和 `/data` 都可以工作，但将会路由到不同的服务：
- en: '[PRE76]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Since ingress works on layer 7, there are many more options to route traffic
    with, such as host header and URI path.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 由于入口在第7层工作，有许多其他选项可以用来路由流量，例如主机头和URI路径。
- en: For more advanced traffic routing and release patterns, a service mesh is required
    to be deployed in the cluster network. Let’s dig into that next.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更高级的流量路由和发布模式，需要在集群网络中部署服务网格。让我们接下来深入探讨这一点。
- en: Service Meshes
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格
- en: A new cluster with the default options has some limitations. So, let’s get an
    understanding for what those limitations are and how a service mesh can resolve
    some of those limitations. A *service mesh* is an API-driven infrastructure layer
    for handling service-to-service communication.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认选项的新群集有一些限制。所以，让我们了解这些限制是什么，以及服务网格如何解决其中的一些限制。*服务网格* 是一个API驱动的基础设施层，用于处理服务间的通信。
- en: From a security point of view, all traffic inside the cluster is unencrypted
    between pods, and each application team that runs a service must configure monitoring
    separately for each service. We have discussed the service types, but we have
    not discussed how to update deployments of pods for them. Service meshes support
    more than the basic deployment type; they support rolling updates and re-creations,
    like Canary does. From a developer’s perspective, injecting faults into the network
    is useful, but also not directly supported in default Kubernetes network deployments.
    With service meshes, developers can add fault testing, and instead of just killing
    pods, you can use service meshes to inject delays—again, each application would
    have to build in fault testing or circuit breaking.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 从安全角度来看，集群内部所有流量在pod之间都是未加密的，运行服务的每个应用团队必须单独为每个服务配置监控。我们已经讨论了服务类型，但我们还没有讨论如何更新它们的pod部署。服务网格支持的不仅仅是基本的部署类型；它们支持滚动更新和重建，就像Canary一样。从开发者的角度来看，将故障注入网络是有用的，但默认的Kubernetes网络部署不直接支持。通过服务网格，开发者可以添加故障测试，而不仅仅是杀死pod，还可以使用服务网格来注入延迟——同样，每个应用程序都必须构建故障测试或断路器。
- en: 'There are several pieces of functionality that a service mesh enhances or provides
    in a default Kubernetes cluster network:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认的Kubernetes集群网络中，服务网格增强或提供了几个功能：
- en: Service Discovery
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现
- en: Instead of relying on DNS for service discovery, the service mesh manages service
    discovery, and removes the need for it to be implemented in each individual application.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格管理服务发现，不再依赖DNS，消除了在每个单独应用程序中实现服务发现的需要。
- en: Load Balancing
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡
- en: The service mesh adds more advanced load balancing algorithms such as least
    request, consistent hashing, and zone aware.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格增加了更先进的负载均衡算法，如最小请求、一致性哈希和区域感知。
- en: Communication Resiliency
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 通信韧性
- en: The service mesh can increase communication resilience for applications by not
    having to implement retries, timeouts, circuit breaking, or rate limiting in application
    code.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格可以通过在应用程序中不必实现重试、超时、断路或速率限制来增加应用程序的通信韧性。
- en: Security
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性
- en: 'A service mesh can provide the folllowing: * End-to-end encryption with mTLS
    between services * Authorization policies, which authorize what services can communicate
    with each other, not just at the layer 3 and 4 levels like in Kubernetes network
    polices.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格可以提供以下功能：*服务之间的端到端加密通过mTLS* *授权策略，授权哪些服务可以与其他服务通信，不仅限于Kubernetes网络策略的第3和第4层。
- en: Observability
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性
- en: Service meshes add in observability by enriching the layer 7 metrics and adding
    tracing and alerting.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格通过丰富的第7层指标、添加跟踪和警报来增强可观测性。
- en: Routing Control
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 路由控制
- en: Traffic shifting and mirroring in the cluster.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的流量转移和镜像。
- en: API
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: API
- en: All of this can be controlled via an API provided by the service mesh implementation.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可以通过服务网格实现提供的API进行控制。
- en: Let’s walk through several components of a service mesh in [Figure 5-8](#img-service-mesh).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下[图5-8](#img-service-mesh)中服务网格的几个组件。
- en: '![Service mesh Components](Images/neku_0508.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![服务网格组件](Images/neku_0508.png)'
- en: Figure 5-8\. Service mesh components
  id: totrans-410
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 服务网格组件
- en: Traffic is handled differently depending on the component or destination of
    traffic. Traffic into and out of the cluster is managed by the gateways. Traffic
    between the frontend, backend, and user service is all encrypted with Mutual TLS
    (mTLS) and is handled by the service mesh. All the traffic to the frontend, backend,
    and user pods in the service mesh is proxied by the sidecar proxy deployed within
    the pods. Even if the control plane is down and updates cannot be made to the
    mesh, the service and application traffic are not affected.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 流量根据组件或流量的目的地处理不同。进出集群的流量由网关管理。前端、后端和用户服务之间的流量都使用双向TLS（mTLS）进行加密，并由服务网格处理。即使控制平面关闭且无法更新网格，服务和应用程序流量也不会受到影响。
- en: 'There are several options to use when deploying a service mesh; here are highlights
    of just a few:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署服务网格时有几个选项可供选择；以下是其中几个要点：
- en: Istio
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio
- en: Uses a Go control plane with an Envoy proxy.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用带有Envoy代理的Go控制平面。
- en: This is a Kubernetes-native solution that was initially released by Lyft.
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个由Lyft最初发布的基于Kubernetes的本地解决方案。
- en: Consul
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul
- en: Uses HashiCorp Consul as the control plane.
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HashiCorp Consul作为控制平面。
- en: Consul Connect uses an agent installed on every node as a DaemonSet, which communicates
    with the Envoy sidecar proxies that handle routing and forwarding of traffic.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul Connect在每个节点上安装一个代理作为DaemonSet，它与处理流量路由和转发的Envoy Sidecar代理通信。
- en: AWS App Mesh
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS App Mesh
- en: Is an AWS-managed solution that implements its own control plane.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是AWS管理的解决方案，实现了自己的控制平面。
- en: Does not have mTLS or traffic policy.
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有mTLS或流量策略。
- en: Uses the Envoy proxy for the data plane.
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Envoy代理作为数据平面。
- en: Linkerd
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: Also uses Go for the control plane with the Linkerd proxy.
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd代理也使用Go控制平面。
- en: No traffic shifting and no distributed tracing.
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有流量转移和分布式跟踪。
- en: Is a Kubernetes-only solution, which results in fewer moving pieces and means
    that Linkerd has less complexity overall.
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是一个仅限于Kubernetes的解决方案，这导致移动部件较少，意味着Linkerd总体上复杂性较低。
- en: It is our opinion that the best use case for a service mesh is mTLS between
    services. Other higher-level use cases for developers include circuit breaking
    and fault testing for APIs. For network administrators, advanced routing policies
    and algorithms can be deployed with service meshes.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为服务网格的最佳用例是服务之间的mTLS。开发人员的其他高级用例包括断路器和API的故障测试。对于网络管理员来说，可以使用服务网格部署高级路由策略和算法。
- en: Let’s look at a service mesh example. The first thing you need to do if you
    haven’t already is [install the Linkerd CLI](https://oreil.ly/jVaPm).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个服务网格的例子。如果您还没有安装，请首先[安装Linkerd CLI](https://oreil.ly/jVaPm)。
- en: 'Your choices are cURL, bash, or brew if you’re on a Mac:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Mac，您可以选择cURL、bash或brew：
- en: '[PRE77]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'This preflight checklist will verify that our cluster can run Linkerd:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预检查列表将验证我们的集群是否能运行Linkerd：
- en: '[PRE78]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The Linkerd CLI tool can install Linkerd for us onto our KIND cluster:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd CLI工具可以将Linkerd安装到我们的KIND集群中：
- en: '[PRE79]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: As with the ingress controller and MetalLB, we can see that a lot of components
    are installed in our cluster.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 与入口控制器和MetalLB一样，我们可以看到许多组件安装在我们的集群中。
- en: Linkerd can validate the installation with the `linkerd check` command.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd可以使用`linkerd check`命令验证安装。
- en: 'It will validate a plethora of checks for the Linkerd install, included but
    not limited to the Kubernetes API version, controllers, pods, and configs to run
    Linkerd, as well as all the services, versions, and APIs needed to run Linkerd:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 它将验证Linkerd安装的大量检查，包括但不限于Kubernetes API版本、控制器、Pod和配置以运行Linkerd，以及运行Linkerd所需的所有服务、版本和API：
- en: '[PRE80]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now that everything looks good with our install of Linkerd, we can add our
    application to the service mesh:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的Linkerd安装看起来一切正常，我们可以将我们的应用程序添加到服务网格中：
- en: '[PRE81]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Let’s pull up the Linkerd console to investigate what we have just deployed.
    We can start the console with `linkerd dashboard &`.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开 Linkerd 控制台，调查我们刚刚部署的内容。我们可以使用 `linkerd dashboard &` 启动控制台。
- en: 'This will proxy the console to our local machine available at `http://localhost:50750`:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把控制台代理到我们的本地机器上，位于 `http://localhost:50750` 可用：
- en: '[PRE82]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Tip
  id: totrans-444
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re having issues with reaching the dashboard, you can run `linkerd viz
    check` and find more help in the Linkerd [documentation](https://oreil.ly/MqgAp).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在访问仪表板时遇到问题，可以运行 `linkerd viz check` 并在 Linkerd [文档](https://oreil.ly/MqgAp)
    中找到更多帮助。
- en: We can see all our deployed objects from the previous exercises in [Figure 5-9](#linkerd-dashboards).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 [图 5-9](#linkerd-dashboards) 中看到之前练习中部署的所有对象。
- en: Our ClusterIP service is not part of the Linkerd service mesh. We will need
    to use the proxy injector to add our service to the mesh. It accomplishes this
    by watching for a specific annotation that can be added either with Linkerd `inject`
    or by hand to the pod’s spec.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 ClusterIP 服务不是 Linkerd 服务网格的一部分。我们需要使用代理注入器将我们的服务添加到网格中。它通过观察可以添加到 pod 规范中的特定注释来实现这一点，可以通过
    Linkerd 的 `inject` 或手动方式添加。
- en: '![Linkderd Dashboard](Images/neku_0509.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![Linkerd 仪表板](Images/neku_0509.png)'
- en: Figure 5-9\. Linkerd dashboard
  id: totrans-449
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. Linkerd 仪表板
- en: 'Let’s remove some older exercises’ resources for clarity:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们清理一些旧练习资源，以便更清晰：
- en: '[PRE83]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We can use the Linkerd CLI to inject the proper annotations into our deployment
    spec, so that will become part of the mesh.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Linkerd CLI 将适当的注释注入到我们的部署规范中，以便它成为网格的一部分。
- en: 'We first need to get our application manifest, `cat web.yaml`, and use Linkerd
    to inject the annotations, `linkerd inject -`, then apply them back to the Kubernetes
    API with `kubectl apply -f -`:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要获取我们的应用程序清单，`cat web.yaml`，然后使用 Linkerd 将注释注入，`linkerd inject -`，最后将它们应用回
    Kubernetes API，`kubectl apply -f -`：
- en: '[PRE84]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'If we describe our app deployment, we can see that Linkerd has injected new
    annotations for us, `Annotations: linkerd.io/inject: enabled`:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们描述我们的应用部署，我们可以看到 Linkerd 为我们注入了新的注释，`注释: linkerd.io/inject: enabled`：'
- en: '[PRE85]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: If we navigate to the app in the dashboard, we can see that our deployment is
    part of the Linkerd service mesh now, as shown in [Figure 5-10](#app-dashboards).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在仪表板中导航到应用程序，我们可以看到我们的部署现在是 Linkerd 服务网格的一部分，如 [图 5-10](#app-dashboards)
    所示。
- en: '![App Linkderd Dashboard](Images/neku_0510.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![应用 Linkerd 仪表板](Images/neku_0510.png)'
- en: Figure 5-10\. Web app deployment linkerd dashboard
  id: totrans-459
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. Web 应用部署 Linkerd 仪表板
- en: 'The CLI can also display our stats for us:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: CLI 还可以为我们显示统计信息：
- en: '[PRE86]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Again, let’s scale up our deployment:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们扩展我们的部署：
- en: '[PRE87]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: In [Figure 5-11](#app-stats), we navigate to the web browser and open [this
    link](https://oreil.ly/qQx9T) so we can watch the stats in real time. Select the
    default namespaces, and in Resources select our deployment/app. Then click “start
    for the web” to start displaying the metrics.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-11](#app-stats) 中，我们导航到 Web 浏览器并打开 [此链接](https://oreil.ly/qQx9T)，这样我们就可以实时查看统计数据。选择默认的命名空间，在资源中选择我们的部署/应用。然后点击“为
    Web 启动”以开始显示指标。
- en: 'In a separate terminal let’s use the `netshoot` image, but this time running
    inside our KIND cluster:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在单独的终端中，让我们使用 `netshoot` 镜像，但这次在我们的 KIND 集群内运行：
- en: '[PRE88]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '![App Stats Dashboard](Images/neku_0511.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![应用统计仪表板](Images/neku_0511.png)'
- en: Figure 5-11\. Web app dashboard
  id: totrans-468
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-11\. Web 应用仪表板
- en: 'Let’s send a few hundred queries and see the stats:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们发送几百个查询，查看统计数据：
- en: '[PRE89]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: In our terminal we can see all the liveness and readiness probes as well as
    our `/host` requests.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的终端中，我们可以看到所有的存活探针、就绪探针以及我们的 `/host` 请求。
- en: '`tmp-shell` is our `netshoot` bash terminal with our `for` loop running.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '`tmp-shell` 是我们的 `netshoot` bash 终端，其中运行着我们的 `for` 循环。'
- en: '`10.244.2.1`, `10.244.3.1`, and `10.244.2.1` are the Kubelets of the hosts
    running our probes for us:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '`10.244.2.1`、`10.244.3.1` 和 `10.244.2.1` 是运行我们探测的主机的 Kubelet：'
- en: '[PRE90]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Our example showed the observability functionality for a service mesh only.
    Linkerd, Istio, and the like have many more options available for developers and
    network administrators to control, monitor, and troubleshoot services running
    inside their cluster network. As with the ingress controller, there are many options
    and features available. It is up to you and your teams to decide what functionality
    and features are important for your networks.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例仅展示了服务网格的可观察功能。Linkerd、Istio 等等还有许多可用于开发人员和网络管理员控制、监控和故障排除集群网络内运行服务的选项。与入口控制器一样，提供了许多选项和功能。由您和您的团队决定哪些功能和特性对您的网络重要。
- en: Conclusion
  id: totrans-476
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The Kubernetes networking world is feature rich with many options for teams
    to deploy, test, and manage with their Kubernetes cluster. Each new addition will
    add complexity and overhead to the cluster operations. We have given developers,
    network administrators, and system administrators a view into the abstractions
    that Kubernetes offers.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网络世界功能丰富，团队可以通过多种选项部署、测试和管理其 Kubernetes 集群。每个新的添加都会给集群操作增加复杂性和开销。我们已经为开发人员、网络管理员和系统管理员提供了
    Kubernetes 提供的抽象视图。
- en: From internal traffic to external traffic to the cluster, teams must choose
    what abstractions work best for their workloads. This is no small task, and now
    you are armed with the knowledge to begin those discussions.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 从内部流量到集群外部流量，团队必须选择最适合其工作负载的抽象方法。这并不是一件小事，现在你已经掌握了开始这些讨论的知识。
- en: In our next chapter, we take our Kubernetes services and network learnings to
    the cloud! We will explore the network services offered by each cloud provider
    and how they are integrated into their Kubernetes managed service offering.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一章中，我们将把我们的 Kubernetes 服务和网络学习带到云端！我们将探索每个云提供商提供的网络服务，并了解它们如何集成到其 Kubernetes
    管理服务中。
