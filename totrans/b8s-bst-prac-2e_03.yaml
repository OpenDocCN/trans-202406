- en: Chapter 3\. Monitoring and Logging in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discuss best practices for monitoring and logging in Kubernetes.
    We’ll dive into the details of different monitoring patterns, important metrics
    to collect, and building dashboards from these raw metrics. We then wrap up with
    examples of implementing monitoring for your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Versus Logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You first need to understand the difference between log collection and metrics
    collection. They are complementary but serve different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs: []
  type: TYPE_NORMAL
- en: A series of numbers measured over a period of time.
  prefs: []
  type: TYPE_NORMAL
- en: Logs
  prefs: []
  type: TYPE_NORMAL
- en: Logs keep track of what happens while a program is running, including any errors,
    warnings, or notable events that occur.
  prefs: []
  type: TYPE_NORMAL
- en: A example of where you would need to use both metrics and logging is when an
    application is performing poorly. Our first indication of the issue might be an
    alert of high latency on the pods hosting the application, but the metrics might
    not give a good indication of the issue. We then can look into our logs to investigate
    errors that are being emitted from the application.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Closed-box monitoring focuses on monitoring from the outside of an application
    and is what’s been used traditionally when monitoring systems for components like
    CPU, memory, storage, and so on. Closed-box monitoring can still be useful for
    monitoring at the infrastructure level, but it lacks insights and context into
    how the application is operating. For example, to test whether a cluster is healthy,
    we might schedule a pod, and if it’s successful, we know that the scheduler and
    service discovery are healthy within our cluster, so we can assume the cluster
    components are healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Open-box monitoring focuses on the details in the context of the application
    state, such as total HTTP requests, number of 500 errors, latency of requests,
    and so on. With open-box monitoring, we can begin to understand the *why* of our
    system state. It allows us to ask, “Why did the disk fill up?” and not just state,
    “The disk filled up.”
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might look at monitoring and say, “How difficult can this be? We’ve always
    monitored our systems.” The concept of monitoring isn’t new, and we have many
    tools at our disposal to help us understand how our systems are performing. But
    platforms like Kubernetes are much more dynamic and transient, so you’ll need
    to change your thinking about how to monitor these environments. For example,
    when monitoring a virtual machine (VM) you expect that VM to be up 24/7 and all
    its state preserved. In Kubernetes, pods can be very dynamic and short-lived,
    so you need to have monitoring in place that can handle this dynamic and transient
    nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two monitoring patterns to focus on when monitoring distributed systems.
    The *USE* method, popularized by Brendan Gregg, focuses on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: U—Utilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S—Saturation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E—Errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method is focused on infrastructure monitoring because there are limitations
    on using it for application-level monitoring. The USE method is described as “For
    every resource, check utilization, saturation, and error rates.” This method lets
    you quickly identify resource constraints and error rates of your systems. For
    example, to check the health of the network for your nodes in the cluster, you
    will want to monitor the utilization, saturation, and error rate to be able to
    easily identify any network bottlenecks or errors in the network stack. The USE
    method is a tool in a larger toolbox and is not the only method you will utilize
    to monitor your systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another monitoring approach, called the *RED* method, was popularized by Tom
    Wilkie. The RED method approach is focused on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: R—Rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E—Errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D—Duration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The philosophy was taken from Google’s *Four Golden Signals*:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs: []
  type: TYPE_NORMAL
- en: How long it takes to serve a request
  prefs: []
  type: TYPE_NORMAL
- en: Traffic
  prefs: []
  type: TYPE_NORMAL
- en: How much demand is placed on your system
  prefs: []
  type: TYPE_NORMAL
- en: Errors
  prefs: []
  type: TYPE_NORMAL
- en: The rate of requests that are failing
  prefs: []
  type: TYPE_NORMAL
- en: Saturation
  prefs: []
  type: TYPE_NORMAL
- en: How utilized your service is
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, you could use this method to monitor a frontend service running
    in Kubernetes to calculate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How many requests is my frontend service processing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many 500 errors are users of the service receiving?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the service overutilized by requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see from the previous example, this method is more focused on the
    users’ experience with the service.
  prefs: []
  type: TYPE_NORMAL
- en: The USE and RED methods are complementary given that the USE method focuses
    on the infrastructure components and the RED method focuses on monitoring the
    end-user experience for the application.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Metrics Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know the different monitoring techniques and patterns, let’s look
    at what components you should be monitoring in your Kubernetes cluster. A Kubernetes
    cluster consists of control-plane components and node components. The control-plane
    components consist of the API server, etcd, scheduler, and controller manager.
    The nodes consist of the kubelet, container runtime, kube-proxy, kube-dns, and
    pods. You need to monitor all these components to ensure a healthy cluster and
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes exposes these metrics in a variety of ways, so let’s look at different
    components that you can use to collect metrics within your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Container Advisor, or cAdvisor, is an open source project that collects resources
    and metrics for containers running on a node. cAdvisor is built into the Kubernetes
    kubelet, which runs on every node in the cluster. It collects memory and CPU metrics
    through the Linux control group (cgroup) tree. If you are not familiar with cgroups,
    it’s a Linux kernel feature that allows isolation of resources for CPU, disk I/O,
    or network I/O. cAdvisor will also collect disk metrics through statfs, which
    is built into the Linux kernel. These are implementation details you don’t really
    need to worry about, but you should understand how these metrics are exposed and
    the type of information you can collect. You should consider cAdvisor as the source
    of truth for all container metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kubernetes metrics server and Metrics Server API replace the deprecated
    Heapster. Heapster had some architectural disadvantages with how it implemented
    the data sink, which caused a lot of vendored solutions in the core Heapster code
    base. This issue was solved by implementing a resource and Custom Metrics API
    as an aggregated API in Kubernetes. This allows implementations to be switched
    out without changing the API.
  prefs: []
  type: TYPE_NORMAL
- en: There are two aspects to understand in the Metrics Server API and metrics server.
  prefs: []
  type: TYPE_NORMAL
- en: First, the canonical implementation of the Resource Metrics API is the metrics
    server. The metrics server gathers resource metrics such as CPU and memory. It
    gathers these metrics from the kubelet’s API and then stores them in memory. Kubernetes
    uses these resource metrics in the scheduler, Horizontal Pod Autoscaler (HPA),
    and Vertical Pod Autoscaler (VPA).
  prefs: []
  type: TYPE_NORMAL
- en: Second, the Custom Metrics API allows monitoring systems to collect arbitrary
    metrics. This allows monitoring solutions to build custom adapters that will allow
    for extending outside the core resource metrics. For example, Prometheus built
    one of the first custom metrics adapters, which allows you to use the HPA based
    on a custom metric. This opens up better scaling based on your use case because
    now you can bring in metrics like queue size and scale based on a metric that
    might be external to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that there is a standardized Metrics API, this opens up many possibilities
    to scale outside the plain old CPU and memory metrics.
  prefs: []
  type: TYPE_NORMAL
- en: kube-state-metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: kube-state-metrics is a Kubernetes add-on that monitors the object stored in
    Kubernetes. Where cAdvisor and Metrics Server are used to provide detailed metrics
    on resource usage, kube-state-metrics is focused on identifying conditions on
    Kubernetes objects deployed to your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some questions that kube-state-metrics can answer for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many pods are deployed to the cluster?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How many pods are in a pending state?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there enough resources to serve a pods request?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many pods are in a running state versus a desired state?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How many replicas are available?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What deployments have been updated?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s the status of my nodes?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the allottable CPU cores in my cluster?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any nodes that are unschedulable?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When did a job start?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When did a job complete?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How many jobs failed?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As of this writing, kube-state-metrics tracks many object types. These are always
    expanding, and you can find the documentation in the [GitHub repository](https://oreil.ly/bdTp2).
  prefs: []
  type: TYPE_NORMAL
- en: What Metrics Do I Monitor?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easy answer is “everything,” but if you try to monitor too much, you can
    create noise that filters out the real signals into which you need to have insight.
    When we think about monitoring in Kubernetes, we want a layered approach that
    takes into account the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Physical or virtual nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster add-ons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-user applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this layered approach to monitoring allows you to more easily identify
    the correct signals in your monitoring system. It allows you to approach issues
    in a more targeted way. For example, if you have pods going into a pending state,
    you can start with resource utilization of the nodes, and if all is OK, you can
    target cluster-level components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are metrics you would want to target in your system:'
  prefs: []
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU utilization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory utilization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Network utilization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk utilization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etcd latency
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster add-ons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster Autoscaler
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress controller
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container memory utilization and saturation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Container CPU utilization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Container network utilization and error rate
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Application framework–specific metrics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many monitoring tools can integrate with Kubernetes, and more arrive every
    day, building on their feature set to better integrate with Kubernetes. Following
    are a few popular tools that integrate with Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is an open source systems monitoring and alerting toolkit originally
    built at SoundCloud. Since its inception in 2012, many companies and organizations
    have adopted Prometheus, and the project has a very active developer and user
    community. It is now a standalone open source project and maintained independent
    of any company. To emphasize this, and to clarify the project’s governance structure,
    Prometheus joined the Cloud Native Computing Foundation (CNCF) in 2016 as the
    second hosted project, after Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB is a time-series database designed to handle high write and query loads.
    It is an integral component of the TICK (Telegraf, InfluxDB, Chronograf, and Kapacitor)
    stack. InfluxDB is meant to be used as a backing store for any use case involving
    large amounts of timestamped data, including DevOps monitoring, application metrics,
    IoT sensor data, and real-time analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Datadog
  prefs: []
  type: TYPE_NORMAL
- en: Datadog provides a monitoring service for cloud-scale applications, providing
    monitoring of servers, databases, tools, and services through a SaaS-based data
    analytics platform.
  prefs: []
  type: TYPE_NORMAL
- en: Sysdig
  prefs: []
  type: TYPE_NORMAL
- en: Sysdig Monitor is a commercial tool that provides Docker monitoring and Kubernetes
    monitoring for container-native apps. Sysdig also allows you to collect, correlate,
    and query Prometheus metrics with direct Kubernetes integration.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud provider tools
  prefs: []
  type: TYPE_NORMAL
- en: 'All major cloud providers provide monitoring tools for their different solutions.
    These tools are typically integrated into the cloud provider’s ecosystem and provide
    a good starting point for monitoring your Kubernetes cluster. Following are some
    examples of cloud provider tools:'
  prefs: []
  type: TYPE_NORMAL
- en: GCP Stackdriver
  prefs: []
  type: TYPE_NORMAL
- en: Stackdriver Kubernetes Engine Monitoring is designed to monitor Google Kubernetes
    Engine (GKE) clusters. It manages monitoring and logging services together and
    its interface provides a dashboard customized for GKE clusters. Stackdriver Monitoring
    provides visibility into the performance, uptime, and overall health of cloud-powered
    applications. It collects metrics, events, and metadata from Google Cloud Platform
    (GCP), Amazon Web Services (AWS), hosted uptime probes, and application instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure Monitor for containers
  prefs: []
  type: TYPE_NORMAL
- en: Azure Monitor for containers is a feature designed to monitor the performance
    of container workloads deployed to either Azure Container Instances or managed
    Kubernetes clusters hosted on Azure Kubernetes Service. Monitoring your containers
    is critical, especially when you’re running a production cluster, at scale, with
    multiple applications. Azure Monitor for containers gives you performance visibility
    by collecting memory and processor metrics from controllers, nodes, and containers
    that are available in Kubernetes through the Metrics API. Container logs are also
    collected. After you enable monitoring from Kubernetes clusters, metrics and logs
    are automatically collected for you through a containerized version of the Log
    Analytics agent for Linux.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Container Insights
  prefs: []
  type: TYPE_NORMAL
- en: If you use Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes
    Service, or other Kubernetes platforms on Amazon EC2, you can use CloudWatch Container
    Insights to collect, aggregate, and summarize metrics and logs from your containerized
    applications and microservices. The metrics include utilization for resources
    such as CPU, memory, disk, and network. Container Insights also provides diagnostic
    information, such as container restart failures, to help you isolate issues and
    resolve them quickly.
  prefs: []
  type: TYPE_NORMAL
- en: One important aspect when looking at implementing a tool to monitor metrics
    is to look at how the metrics are stored. Tools that provide a time-series database
    with key/value pairs will give you a higher degree of attributes for the metric.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Always evaluate monitoring tools you already have, because taking on a new monitoring
    tool has a learning curve and a cost due to the operational implementation of
    the tool. Many of the monitoring tools now have integration into Kubernetes, so
    evaluate which ones you have today and whether they will meet your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Kubernetes Using Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we focus on monitoring metrics with Prometheus, which provides
    good integrations with Kubernetes labeling, service discovery, and metadata. The
    high-level concepts we implement throughout the chapter will also apply to other
    monitoring systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prometheus is an open source project hosted by the CNCF. It was originally
    developed at SoundCloud, and a lot of its concepts are based on Google’s internal
    monitoring system, Borgmon. It implements a multidimensional data model with keypairs
    that work much like how the Kubernetes labeling system works. Prometheus exposes
    metrics in a human-readable format, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To collect metrics, Prometheus uses a pull model in which it scrapes a metrics
    endpoint to collect and ingest the metrics into the Prometheus server. Systems
    like Kubernetes already expose their metrics in a Prometheus format, making it
    simple to collect metrics. Many other Kubernetes ecosystem projects (NGINX, Traefik,
    Istio, Linkerd, etc.) also expose their metrics in a Prometheus format. Prometheus
    also can use exporters, which allow you to take emitted metrics from your service
    and translate them to Prometheus-formatted metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus has a very simplified architecture, as depicted in [Figure 3-1](#prometheus_architecture).
  prefs: []
  type: TYPE_NORMAL
- en: '![Prometheus architecture](assets/kbp2_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Prometheus architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can install Prometheus within the cluster or outside the cluster. It’s a
    good practice to monitor your cluster from a “utility cluster” to avoid a production
    issue also affecting your monitoring system. Tools like [Thanos](https://oreil.ly/7e6Wf)
    provide high availability for Prometheus and allow you to export metrics into
    an external storage system.
  prefs: []
  type: TYPE_NORMAL
- en: 'A deep dive into the Prometheus architecture is beyond the scope of this book,
    and you should refer to one of the dedicated books on this topic. [*Prometheus:
    Up & Running*](https://oreil.ly/NewNE) (O’Reilly) is a good in-depth book to get
    you started.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s dive in and get Prometheus set up on our Kubernetes cluster. There
    are many different ways to deploy Prometheus, and the deployment will depend on
    your specific implementation. We will install the Prometheus Operator with Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus server
  prefs: []
  type: TYPE_NORMAL
- en: Pulls and stores metrics being collected from systems.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus Operator
  prefs: []
  type: TYPE_NORMAL
- en: Makes the Prometheus configuration Kubernetes native, and manages and operates
    Prometheus and Alertmanager clusters. Allows you to create, destroy, and configure
    Prometheus resources through native Kubernetes resource definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Node Exporter
  prefs: []
  type: TYPE_NORMAL
- en: Exports host metrics from Kubernetes nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: kube-state-metrics
  prefs: []
  type: TYPE_NORMAL
- en: Collects Kubernetes-specific metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Alertmanager
  prefs: []
  type: TYPE_NORMAL
- en: Allows you to configure and forward alerts to external systems.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs: []
  type: TYPE_NORMAL
- en: Provides visualization on dashboard capabilities for Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll start by getting minikube setup to deploy Prometheus to. We are
    using Macs so we’ll use `brew` to install minikube. You can also install minikube
    from the [minikube website](https://oreil.ly/BgFFL).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we’ll install kube-prometheus-stack (formerly Prometheus Operator) and prepare
    our cluster to start monitoring the Kubernetes API server for changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace for monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the prometheus-community Helm chart repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the Helm Stable chart repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the chart repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the kube-prometheus-stack chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check to ensure that all the pods are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If installed correctly you should see the following pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we’ll create a tunnel to the Grafana instance that is included with kube-prometheus-stack.
    This will allow us to connect to Grafana from our local machine.
  prefs: []
  type: TYPE_NORMAL
- en: This creates a tunnel to our localhost on port 3000\. Now we can open a web
    browser and connect to Grafana on [*http://127.0.0.1:3000*](http://127.0.0.1:3000).
  prefs: []
  type: TYPE_NORMAL
- en: We talked earlier in the chapter about employing the USE method, so let’s gather
    some node metrics on CPU utilization and saturation. Kube-prometheus-stack provides
    visualizations for these common USE method metrics we want to track. The great
    thing about the kube-prometheus-stack you installed is that it comes with prebuilt
    Grafana dashboards you can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll create a tunnel to the Grafana instance that is included with kube-prometheus-stack.
    This will allow us to connect to Grafana from our local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Point your web browser at [*http://localhost:3000*](http://localhost:3000)
    and log in using the following credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Username: admin'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Password: prom-operator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the Grafana dashboards you’ll find a dashboard called Kubernetes / USE
    Method / Cluster. This dashboard gives you a good overview of the utilization
    and saturation of the Kubernetes cluster, which is at the heart of the USE method.
    [Figure 3-2](#grafana_dashboard) presents an example of the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Grafana dashboard](assets/kbp2_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A Grafana dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Go ahead and take some time to explore the different dashboards and metrics
    that you can visualize in Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Avoid creating too many dashboards (aka “The Wall of Graphs”) because this can
    be difficult for engineers to reason with in troubleshooting situations. You might
    think having more information in a dashboard means better monitoring, but the
    majority of the time it causes more confusion for a user looking at the dashboard.
    Focus your dashboard design on outcomes and time to resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Logging Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to this point, we have discussed a lot about metrics and Kubernetes, but
    to get the full picture of your environment, you also need to collect and centralize
    logs from the Kubernetes cluster and the applications deployed to your cluster.
    With logging, it might be easy to say, “Let’s just log everything,” but this can
    cause two issues:'
  prefs: []
  type: TYPE_NORMAL
- en: There is too much noise to find issues quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs can consume a lot of resources and come with a high cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no clear-cut answer to what exactly you should log because debug logs
    become a necessary evil. Over time you’ll start to understand your environment
    better and learn what noise you can tune out from the logging system. Also, to
    address the ever-increasing number of logs stored, you will need to implement
    a retention and archival policy. From an end-user experience, having somewhere
    between 30 and 45 days’ worth of historical logs is a good fit. This allows for
    investigation of problems that manifest over a longer period of time but also
    reduces the amount of resources needed to store logs. If you require longer-term
    storage for compliance reasons, you’ll want to archive the logs to more cost-effective
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Kubernetes cluster, there are multiple components to log. Following is
    a list of components from which you should be collecting metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Node logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes control-plane logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API server
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Controller manager
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes audit logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application container logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With node logs, you want to collect events that happen to essential node services.
    For example, you will want to collect logs from the Docker daemon running on the
    nodes. A healthy Docker daemon is essential for running containers on the node.
    Collecting these logs will help you diagnose any issues that you might run into
    with the Docker daemon, and it will give you information into any underlying issues
    with the daemon. There are also other essential services that you will want to
    log from the underlying node.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes control plane consists of several components from which you’ll
    need to collect logs to give you more insight into underlying issues within it.
    The Kubernetes control plane is core to a healthy cluster, and you’ll want to
    aggregate the logs that it stores on the host in */var/log/kube-APIserver.log*,
    */var/log/kube-scheduler.log*, and */var/log/kube-controller-manager.log*. The
    controller manager is responsible for creating objects defined by the end user.
    As an example, as a user you create a Kubernetes service with type LoadBalancer
    and it just sits in a pending state; the Kubernetes events might not give all
    the details to diagnose the issue. If you collect the logs in a centralized system,
    it will give you more detail into the underlying issue and a quicker way to investigate
    it.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of Kubernetes audit logs as security monitoring because they give
    you insight into who did what within the system. These logs can be very noisy,
    so you’ll want to tune them for your environment. In many instances these logs
    can cause a huge spike in your logging system when first initialized, so make
    sure that you follow the Kubernetes documentation guidance on audit log monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Application container logs give you insight into the actual logs your application
    is emitting. You can forward these logs to a central repository in multiple ways.
    The first and recommended way is to send all application logs to STDOUT because
    this gives you a uniform way of application logging, and a monitoring daemon set
    can gather the logs directly from the Docker daemon. The other way is to use a
    *sidecar* pattern and run a log-forwarding container next to the application container
    in a Kubernetes pod. You might need to use this pattern if your application logs
    to the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many options and configurations for managing Kubernetes audit logs.
    These audit logs can be very noisy and it can be expensive to log all actions.
    You should consider looking at the [audit logging documentation](https://oreil.ly/L84dM)
    so that you can fine-tune these logs for your environment.
  prefs: []
  type: TYPE_NORMAL
- en: Tools for Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with collecting metrics, there are many tools to collect logs from Kubernetes
    and applications running in the cluster. You might already have tooling for this,
    but be aware of how the tool implements logging. The tool should have the capability
    to run as a Kubernetes DaemonSet, and have a solution to run as a sidecar for
    applications that don’t send logs to STDOUT. An advantage of using an existing
    tool is that you will already have operational knowledge of the tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the more popular tools with Kubernetes integration are:'
  prefs: []
  type: TYPE_NORMAL
- en: Loki
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic Stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datadog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sumo Logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sysdig
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud provider services (GCP Stackdriver, Azure Monitor for containers, and
    Amazon CloudWatch)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When looking for a tool to centralize logs, hosted solutions can provide a lot
    of value because they offload a lot of the operational cost. Hosting your own
    logging solution seems great on day *N*, but as the environment grows, it can
    be very time consuming to maintain the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Logging by Using a Loki-Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the purposes of this book, we use a Loki-Stack with prom-tail for logging
    for our cluster. Implementing a Loki-Stack can be a good way to get started, but
    at some point you’ll probably ask yourself, “Is it really worth managing my own
    logging platform?” Typically, it’s not worth the effort because self-hosted logging
    solutions are great at first, but become overly complex with time. Self-hosted
    logging solutions become more operationally complex as your environment scales.
    There is no one correct answer, so evaluate whether your business requirements
    need you to host your own solution. There is also a hosted Loki solution (provided
    by Grafana), so you can always move pretty easily if you choose not to host it
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following for the logging stack:'
  prefs: []
  type: TYPE_NORMAL
- en: Loki
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prom-tail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy Loki-Stack with Helm to your Kubernetes cluster with the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add Loki-Stack Helm repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Update Helm repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This deploys Loki with prom-tail, which will allow us to forward logs to Loki
    and visualize the logs using Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following pods deployed to your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After all pods are “Running,” go ahead and connect to Grafana through port
    forwarding to our localhost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, point your web browser at [*http://localhost:3000*](http://localhost:3000)
    and log in using the following credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Username: admin'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Password: prom-operator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the Grafana configuration you’ll find data sources, as shown in [Figure 3-3](#grafana_datasource).
    We’ll then add Loki as a `Data Source`.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Grafana datasource](assets/kbp2_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. The Grafana data source
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will then add a new data source and add Loki as the data source (see [Figure 3-4](#loki_datasource)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Loki datasource](assets/kbp2_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Loki datasource
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the Loki settings page ([Figure 3-5](#loki_configuration)), fill in the URL
    with http://loki:3100, then click the Save & Test button.
  prefs: []
  type: TYPE_NORMAL
- en: '![Loki configuration](assets/kbp2_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Loki configuration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Grafana, you can perform ad hoc queries on the logs, and you can build out
    dashboards to give you an overview of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: To explore the logs that the Loki-Stack has collected we can use the *Explore*
    function in Grafana, as shown in [Figure 3-6](#loki_explore). This will allow
    us to run a query against the logs that have been collected.
  prefs: []
  type: TYPE_NORMAL
- en: '![Explore Loki logs](assets/kbp2_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Explore Loki logs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For the label filter you will need the following filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Go ahead and take some time to explore the different logs that you can visualize
    from Loki and Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alerting is a double-edged sword, and you need to strike a balance between what
    you alert on versus what should just be monitored. Alerting on too much causes
    alert fatigue, and important events will be lost in all the noise. An example
    would be generating an alert any time a pod fails. You might be asking, “Why wouldn’t
    I want to monitor for a pod failure?” Well, the beauty of Kubernetes is that it
    provides features to automatically check the health of a container and restart
    the container automatically. You really want to focus alerting on events that
    affect your Service-Level Objectives (SLOs). SLOs are specific measurable characteristics
    such as availability, throughput, frequency, and response time that you agree
    upon with the end user of your service. Setting SLOs sets expectations with your
    end users and provides clarity on how the system should behave. Without an SLO,
    users can form their opinion, which might be an unrealistic expectation of the
    service. Alerting in a system like Kubernetes needs an entirely new approach from
    what we are typically accustomed to and needs to focus on how the end user is
    experiencing the service. For example, if your SLO for a frontend service is a
    20-ms response time and you are seeing higher latency than average, you want to
    be alerted on the problem.
  prefs: []
  type: TYPE_NORMAL
- en: You need to decide what alerts are good and require intervention. In typical
    monitoring, you might be accustomed to alerting on high CPU usage, memory usage,
    or processes not responding. These might seem like good alerts, but they probably
    don’t indicate an issue that someone needs to take immediate action on and that
    requires notifying an on-call engineer. An alert to an on-call engineer should
    be an issue that needs immediate human attention and is affecting the UX of the
    application. If you have ever experienced a “that issue resolved itself” scenario,
    then that is a good indication that the alert did not need to contact an on-call
    engineer.
  prefs: []
  type: TYPE_NORMAL
- en: One way to handle alerts that don’t need immediate action is to focus on automating
    the remediation of the cause. For example, when a disk fills up, you could automate
    the deletion of logs to free up space on the disk. Also, utilizing Kubernetes
    *liveness probes* in your app deployment can help auto-remediate issues with a
    process that is not responding in the application.
  prefs: []
  type: TYPE_NORMAL
- en: When building alerts, you also need to consider *alert thresholds*; if you set
    thresholds too short, then you can get a lot of false positives with your alerts.
    It’s generally recommended to set a threshold of at least five minutes to help
    eliminate false positives. Coming up with standard thresholds can help define
    a standard and avoid micromanaging many different thresholds. For example, you
    might want to follow a specific pattern of 5 minutes, 10 minutes, 30 minutes,
    1 hour, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: When building notifications for alerts, you want to ensure that you provide
    relevant information in the notification. For example, you might provide a link
    to a “playbook” that gives troubleshooting or other helpful information on resolving
    the issue. You should also include information on the datacenter, region, app
    owner, and affected system in notifications. Providing all this information will
    allow engineers to quickly formulate a theory around the issue.
  prefs: []
  type: TYPE_NORMAL
- en: You also need to build notification channels to route alerts that are fired.
    When thinking about “Who do I notify when an alert is triggered?” you should ensure
    that notifications are not just sent to a distribution list or team emails. What
    tends to happen if alerts are sent to larger groups is that they end up getting
    filtered out because users see these as noise. You should route notifications
    to the user who is going to take responsibility for the issue.
  prefs: []
  type: TYPE_NORMAL
- en: With alerting, you’ll never get it perfect on day one, and we could argue it
    might never be perfect. You just want to make sure that you incrementally improve
    on alerting to preclude alert fatigue, which can cause many issues with staff
    burnout and your systems.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For further insight on how to approach alerting on and managing systems, read
    [“My Philosophy on Alerting”](https://oreil.ly/YPxju) by Rob Ewaschuk, which is
    based on Rob’s observations as a site reliability engineer (SRE) at Google.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Monitoring, Logging, and Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following are the best practices that you should adopt regarding monitoring,
    logging, and alerting.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitor nodes and all Kubernetes components for utilization, saturation, and
    error rates, and monitor applications for rate, errors, and duration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use closed-box monitoring to monitor for symptoms and not predictive health
    of a system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use open-box monitoring to inspect the system and its internals with instrumentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement time-series-based metrics to gain high-precision metrics that also
    allow you to have insight into the behavior of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize monitoring systems like Prometheus that provide key labeling for high
    dimensionality; this will give a better signal to symptoms of an impacting issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use average metrics to visualize subtotals and metrics based on factual data.
    Utilize sum metrics to visualize the distribution across a specific metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should use logging in combination with metrics monitoring to get the full
    picture of how your environment is operating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be cautious of storing logs for more than 30 to 45 days; if needed, use cheaper
    resources for long-term archiving.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limit usage of log forwarders in a sidecar pattern, as they will utilize a lot
    more resources. Opt for using a DaemonSet for the log forwarder and sending logs
    to STDOUT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Be cautious of alert fatigue because it can lead to bad behaviors in people
    and processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always look at incrementally improving on alerting and accept that it will not
    always be perfect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert for symptoms that affect your SLOs and customers and not for transient
    issues that don’t need immediate human attention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we discussed the patterns, techniques, and tools that can be
    used for monitoring our systems with metrics and log collection. The most important
    piece to take away from this chapter is that you need to rethink how you perform
    monitoring and do it from the outset. Too many times we see this implemented after
    the fact, and it can get you into a very bad place in understanding your system.
    Monitoring is all about having better insight into a system and being able to
    provide better resiliency, which in turn provides a better end-user experience
    for your application. Monitoring distributed applications and distributed systems
    like Kubernetes requires a lot of work, so you must be ready for it at the beginning
    of your journey.
  prefs: []
  type: TYPE_NORMAL
