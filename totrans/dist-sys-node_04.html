<html><head></head><body><section data-pdf-bookmark="Chapter 3. Scaling" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch_scaling">&#13;
<h1><span class="label">Chapter 3. </span>Scaling</h1>&#13;
&#13;
&#13;
<p>Running redundant copies of a service is important for at least two reasons.</p>&#13;
&#13;
<p>The first reason is <a data-primary="high availability" data-type="indexterm" id="idm46291192455928"/><a data-primary="scaling" data-secondary="high availability and" data-type="indexterm" id="idm46291192455192"/>to achieve <em>high availability</em>. Consider that processes, and entire machines, occasionally crash. If only a single instance of a producer is running and that instance crashes, then consumers are unable to function until the crashed producer has been relaunched. With two or more running producer instances, a single downed instance won’t necessarily prevent a consumer from functioning.</p>&#13;
&#13;
<p>Another reason is that there’s only <a data-primary="throughput limitations" data-type="indexterm" id="idm46291192452920"/>so much throughput that a given Node.js instance can handle. For example, depending on the hardware, the most basic Node.js “Hello World” service might have a throughput of around 40,000 requests per second (r/s). Once an application begins serializing and deserializing payloads or doing other CPU-intensive work, that throughput is going to drop by orders of magnitude. &#13;
<span class="keep-together">Offloading</span> work to additional processes helps prevent a single process from getting &#13;
<span class="keep-together">overwhelmed.</span></p>&#13;
&#13;
<p>There are a few tools available for splitting up work. <a data-type="xref" href="#ch_scaling_sec_clustering">“The Cluster Module”</a> looks at a built-in module that makes it easy to run redundant copies of application code on the same server. <a data-type="xref" href="#ch_scaling_sec_rp">“Reverse Proxies with HAProxy”</a> runs multiple redundant copies of a service using an external tool—allowing them to run on different machines. Finally, <a data-type="xref" href="#ch_scaling_sec_bm">“SLA and Load Testing”</a> looks at how to understand the load that a service can handle by examining benchmarks, which can be used to determine the number of instances it should scale to.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Cluster Module" data-type="sect1"><div class="sect1" id="ch_scaling_sec_clustering">&#13;
<h1>The Cluster Module</h1>&#13;
&#13;
<p>Node.js provides the <code>cluster</code> module to <a data-primary="cluster module" data-type="indexterm" id="idm46291192444536"/><a data-primary="modules" data-secondary="cluster" data-type="indexterm" id="idm46291192443800"/><a data-primary="scaling" data-secondary="cluster module" data-type="indexterm" id="clust_mod"/>allow running multiple copies of a Node.js application on the same machine, dispatching incoming network messages to the copies. This module is similar to the <code>child_process</code> module, which provides a <code>fork()</code> method<sup><a data-type="noteref" href="ch03.html#idm46291192440536" id="idm46291192440536-marker">1</a></sup> for spawning Node.js sub processes; the main difference is the added mechanism for routing incoming requests.</p>&#13;
&#13;
<p>The <code>cluster</code> module provides a simple API and is immediately accessible to any Node.js program. Because of this it’s often the knee-jerk solution when an application needs to scale to multiple instances. It’s become somewhat ubiquitous, with many open source Node.js application depending on it. Unfortunately, it’s also a bit of an antipattern, and is almost never the best tool to scale a process. Due to this ubiquity it’s necessary to understand how it works, even though you should avoid it more often than not.</p>&#13;
&#13;
<p>The <a class="orm:hideurl" href="https://nodejs.org/api/cluster.html">documentation for <code>cluster</code></a> includes a single Node.js file that loads the <code>http</code> and <code>cluster</code> modules and has an <code>if</code> statement to see if the script is being run as the master, forking off some worker processes if true. Otherwise, if it’s not the master, it creates an HTTP service and begins listening. This example code is both a little dangerous and a little misleading.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="A Simple Example" data-type="sect2"><div class="sect2" id="idm46291192434424">&#13;
<h2>A Simple Example</h2>&#13;
&#13;
<p>The reason the documentation code sample is dangerous is that it promotes loading a lot of potentially heavy and complicated modules within the parent process. The reason it’s misleading is that the example doesn’t make it obvious that multiple separate instances of the application are running and that things like global variables cannot be shared. For these reasons you’ll consider the modified example shown in <a data-type="xref" href="#ex_cluster_master">Example 3-1</a>.</p>&#13;
<div data-type="example" id="ex_cluster_master">&#13;
<h5><span class="label">Example 3-1. </span><em>recipe-api/producer-http-basic-master.js</em></h5>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting"><code class="c-Hashbang">#!/usr/bin/env node&#13;
</code><code class="kr">const</code><code> </code><code class="nx">cluster</code><code> </code><code class="o">=</code><code> </code><code class="nx">require</code><code class="p">(</code><code class="s1">'cluster'</code><code class="p">)</code><code class="p">;</code><code> </code><a class="co" href="#callout_scaling_CO1-1" id="co_scaling_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="sb">`</code><code class="sb">master pid=</code><code class="si">${</code><code class="nx">process</code><code class="p">.</code><code class="nx">pid</code><code class="si">}</code><code class="sb">`</code><code class="p">)</code><code class="p">;</code><code>&#13;
</code><code class="nx">cluster</code><code class="p">.</code><code class="nx">setupMaster</code><code class="p">(</code><code class="p">{</code><code>&#13;
  </code><code class="nx">exec</code><code class="o">:</code><code> </code><code class="nx">__dirname</code><code class="o">+</code><code class="s1">'/producer-http-basic.js'</code><code> </code><a class="co" href="#callout_scaling_CO1-2" id="co_scaling_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code class="p">}</code><code class="p">)</code><code class="p">;</code><code>&#13;
</code><code class="nx">cluster</code><code class="p">.</code><code class="nx">fork</code><code class="p">(</code><code class="p">)</code><code class="p">;</code><code> </code><a class="co" href="#callout_scaling_CO1-3" id="co_scaling_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code class="nx">cluster</code><code class="p">.</code><code class="nx">fork</code><code class="p">(</code><code class="p">)</code><code class="p">;</code><code>&#13;
&#13;
</code><code class="nx">cluster</code><code>&#13;
  </code><code class="p">.</code><code class="nx">on</code><code class="p">(</code><code class="s1">'disconnect'</code><code class="p">,</code><code> </code><code class="p">(</code><code class="nx">worker</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="p">{</code><code> </code><a class="co" href="#callout_scaling_CO1-4" id="co_scaling_CO1-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
    </code><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s1">'disconnect'</code><code class="p">,</code><code> </code><code class="nx">worker</code><code class="p">.</code><code class="nx">id</code><code class="p">)</code><code class="p">;</code><code>&#13;
  </code><code class="p">}</code><code class="p">)</code><code>&#13;
  </code><code class="p">.</code><code class="nx">on</code><code class="p">(</code><code class="s1">'exit'</code><code class="p">,</code><code> </code><code class="p">(</code><code class="nx">worker</code><code class="p">,</code><code> </code><code class="nx">code</code><code class="p">,</code><code> </code><code class="nx">signal</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="p">{</code><code>&#13;
    </code><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s1">'exit'</code><code class="p">,</code><code> </code><code class="nx">worker</code><code class="p">.</code><code class="nx">id</code><code class="p">,</code><code> </code><code class="nx">code</code><code class="p">,</code><code> </code><code class="nx">signal</code><code class="p">)</code><code class="p">;</code><code>&#13;
    </code><code class="c1">// cluster.fork(); </code><a class="co" href="#callout_scaling_CO1-5" id="co_scaling_CO1-5"><img alt="5" src="assets/5.png"/></a><code class="c1">&#13;
</code><code>  </code><code class="p">}</code><code class="p">)</code><code>&#13;
  </code><code class="p">.</code><code class="nx">on</code><code class="p">(</code><code class="s1">'listening'</code><code class="p">,</code><code> </code><code class="p">(</code><code class="nx">worker</code><code class="p">,</code><code> </code><code class="p">{</code><code class="nx">address</code><code class="p">,</code><code> </code><code class="nx">port</code><code class="p">}</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="p">{</code><code>&#13;
    </code><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s1">'listening'</code><code class="p">,</code><code> </code><code class="nx">worker</code><code class="p">.</code><code class="nx">id</code><code class="p">,</code><code> </code><code class="sb">`</code><code class="si">${</code><code class="nx">address</code><code class="si">}</code><code class="sb">:</code><code class="si">${</code><code class="nx">port</code><code class="si">}</code><code class="sb">`</code><code class="p">)</code><code class="p">;</code><code>&#13;
  </code><code class="p">}</code><code class="p">)</code><code class="p">;</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO1-1" id="callout_scaling_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>cluster</code> module is needed in the parent process.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO1-2" id="callout_scaling_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Override the default application entry point of <code>__filename</code>.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO1-3" id="callout_scaling_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p><code>cluster.fork()</code> is called once for each time a worker needs to be created. This code produces two workers.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO1-4" id="callout_scaling_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Several events that <code>cluster</code> emits are listened to and logged.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO1-5" id="callout_scaling_CO1-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Uncomment this to make workers difficult to kill.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The way <code>cluster</code> works is that the <a data-primary="cluster module" data-secondary="master/worker relationship" data-type="indexterm" id="idm46291192187912"/><a data-primary="modules" data-secondary="cluster" data-tertiary="master/worker relationship" data-type="indexterm" id="idm46291192187064"/>master process spawns worker processes in a special mode where a few things can happen. In this mode, when a worker attempts to listen on a port, it sends a message to the master. It’s actually the master that listens on the port. Incoming requests are then routed to the different worker processes. If any workers attempt to listen on the special port <code>0</code> (used for picking a random port), the master will listen once and each individual worker will receive requests from that same random port. A visualization of this master and worker relationship is provided in <a data-type="xref" href="#fig_cluster">Figure 3-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig_cluster">&#13;
<img alt="A Master Node.js process and two Worker Node.js processes" src="assets/dsnj_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>Master-worker relationships with <code>cluster</code></h6>&#13;
</div></figure>&#13;
&#13;
<p>No changes need to be made to basic stateless applications that serve as the worker—the <em>recipe-api/producer-http-basic.js</em> code will work just fine.<sup><a data-type="noteref" href="ch03.html#idm46291192181256" id="idm46291192181256-marker">2</a></sup> Now it’s time to make a few requests to the server. This time, execute the <em>recipe-api/producer-http-basic-master.js</em> file instead of the <em>recipe-api/producer-http-basic.js</em> file. In the output you should see some messages resembling the following:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">master pid=7649&#13;
Producer running at http://127.0.0.1:4000&#13;
Producer running at http://127.0.0.1:4000&#13;
listening 1 127.0.0.1:4000&#13;
listening 2 127.0.0.1:4000</pre>&#13;
&#13;
<p>Now there are three running processes. This can be confirmed by running the following command, where <code>&lt;PID&gt;</code> is replaced with the process ID of the master process, in my case <em>7649</em>:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>brew install pstree <code class="c"># if using macOS</code>&#13;
<code class="nv">$ </code>pstree &lt;PID&gt; -p -a</pre>&#13;
&#13;
<p>A truncated version of the output from this command when run on my Linux machine looks like this:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">node,7649 ./master.js&#13;
  ├─node,7656 server.js&#13;
  │   ├─{node},15233&#13;
  │   ├─{node},15234&#13;
  │   ├─{node},15235&#13;
  │   ├─{node},15236&#13;
  │   ├─{node},15237&#13;
  │   └─{node},15243&#13;
  ├─node,7657 server.js&#13;
  │   ├─ ... Six total children like above ...&#13;
  │   └─{node},15244&#13;
  ├─ ... Six total children like above ...&#13;
  └─{node},15230</pre>&#13;
&#13;
<p>This provides a visualization of the <a data-primary="cluster module" data-secondary="parent processes" data-type="indexterm" id="idm46291192150616"/><a data-primary="modules" data-secondary="cluster" data-tertiary="parent processes" data-type="indexterm" id="idm46291192149704"/><a data-primary="cluster module" data-secondary="child processes" data-type="indexterm" id="idm46291192131032"/><a data-primary="modules" data-secondary="cluster" data-tertiary="child processes" data-type="indexterm" id="idm46291192130088"/>parent process, displayed as <code>./master.js</code>, as well as the two child processes, displayed as <code>server.js</code>. It also displays some other interesting information if run on a Linux machine. Note that each of the three processes shows six additional child entries below them, each labelled as <code>{node}</code>, as well as their unique process IDs. These entries suggest multithreading in the underlying libuv layer. Note that if you run this on macOS, you will only see the three Node.js <a data-primary="scaling" data-secondary="cluster module" data-startref="clust_mod" data-type="indexterm" id="idm46291192174072"/>&#13;
<span class="keep-together">processes</span> listed.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Request Dispatching" data-type="sect2"><div class="sect2" id="idm46291192433864">&#13;
<h2>Request Dispatching</h2>&#13;
&#13;
<p>On macOS and Linux <a data-primary="request dispatching" data-type="indexterm" id="idm46291192170216"/><a data-primary="scaling" data-secondary="request dispatching" data-type="indexterm" id="req_disp"/><a data-primary="cluster module" data-secondary="request dispatching" data-type="indexterm" id="idm46291192110808"/>machines, the requests will be dispatched round-robin to the workers by default. On Windows, requests will be dispatched depending on which worker is perceived to be the least busy. You can make three successive requests directly <a data-primary="recipe-api" data-secondary="requests" data-type="indexterm" id="idm46291192109496"/><a data-primary="web-api service" data-secondary="requests" data-type="indexterm" id="idm46291192108552"/>to the <em>recipe-api</em> service and see this happening for yourself. With this example, requests are made directly to the <em>recipe-api</em>, since these changes won’t affect &#13;
<span class="keep-together">the <em>web-api</em> service.</span> Run the following command three times in another terminal &#13;
<span class="keep-together">window:</span></p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>curl http://localhost:4000/recipes/42 <code class="c"># run three times</code></pre>&#13;
&#13;
<p>In the output you should see that the requests have been cycled between the two running worker instances:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">worker request pid=7656&#13;
worker request pid=7657&#13;
worker request pid=7656</pre>&#13;
&#13;
<p>As you may recall from <a data-type="xref" href="#ex_cluster_master">Example 3-1</a>, some <a data-primary="cluster module" data-secondary="listening event" data-type="indexterm" id="idm46291192051016"/><a data-primary="event listeners, cluster module" data-type="indexterm" id="idm46291192050040"/>event listeners were created in the <em>recipe-api/master.js</em> file. So far the <code>listening</code> event has been triggered. This next step triggers the other two events. When you made the three HTTP requests, the PID values of the worker processes were displayed in the console. Go ahead and kill one of the processes to see what happens. Choose one of the PIDs and run the following &#13;
<span class="keep-together">command:</span></p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code><code class="nb">kill</code> &lt;pid&gt;</pre>&#13;
&#13;
<p>In my case I ran <code>kill 7656</code>. The master process <a data-primary="events" data-secondary="disconnect" data-type="indexterm" id="idm46291192074648"/><a data-primary="events" data-secondary="exit" data-type="indexterm" id="idm46291192073640"/><a data-primary="disconnect event" data-type="indexterm" id="idm46291192072696"/><a data-primary="exit event" data-type="indexterm" id="idm46291192072024"/>then has both the <code>disconnect</code> and the <code>exit</code> events fire, in that order. You should see output similar to the following:</p>&#13;
&#13;
<pre data-code-language="text" data-type="programlisting">disconnect 1&#13;
exit 1 null SIGTERM</pre>&#13;
&#13;
<p>Now go ahead and repeat the same three HTTP requests:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>curl http://localhost:4000/recipes/42 <code class="c"># run three times</code></pre>&#13;
&#13;
<p>This time, each of the responses is coming from the same remaining worker process. If you then run the <code>kill</code> command with the remaining worker process, you’ll see that the <code>disconnect</code> and <code>exit</code> events are called and that the master process then quits.</p>&#13;
&#13;
<p>Notice that there’s a commented call to <code>cluster.fork()</code> inside of the <code>exit</code> event handler. Uncomment that line, start the master process again, and make some requests to get the PID values of the workers. Then, run the <code>kill</code> command <a data-primary="kill command" data-type="indexterm" id="idm46291192025928"/><a data-primary="commands" data-secondary="kill" data-type="indexterm" id="idm46291191982648"/>to stop one of the workers. Notice that the worker process is then immediately started again by the master. In this case, the only way to permanently kill the children is to kill the master.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Shortcomings" data-type="sect2"><div class="sect2" id="idm46291192171608">&#13;
<h2>Cluster Shortcomings</h2>&#13;
&#13;
<p>The <code>cluster</code> module isn’t a <a data-primary="cluster module" data-secondary="limitations" data-type="indexterm" id="clust_limit"/>magic bullet. In fact, it is often more of an antipattern. More often than not, another tool should be used to manage multiple copies of a Node.js process. Doing so usually helps with visibility into process crashes and allows you to easily scale instances. Sure, you could build in application support for scaling the number of workers up and down, but that’s better left to an outside tool. &#13;
<span class="keep-together"><a data-type="xref" href="ch07.html#ch_kubernetes">Chapter 7</a></span> looks into doing just that.</p>&#13;
&#13;
<p>This module is mostly useful in situations where an application is bound by the CPU, not by I/O. This is in part due to JavaScript being single threaded, and also because libuv is so efficient at <a data-primary="events" data-secondary="asynchronous, libuv and" data-type="indexterm" id="idm46291192004120"/><a data-primary="asynchronous events" data-type="indexterm" id="idm46291192003176"/>handling asynchronous events. It’s also fairly fast due to the way it passes incoming requests to a child process. In theory, this is faster than using a reverse proxy.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Node.js applications can get complex. Processes often end up with dozens, if not hundreds, of modules that make outside connections, consume memory, or read configuration. Each one of these operations can expose another weakness in an application that can cause it to crash.</p>&#13;
&#13;
<p>For this reason it’s better to keep the master process as simple as possible. <a data-type="xref" href="#ex_cluster_master">Example 3-1</a> proves that there’s no reason for a master to load an HTTP framework or consume another database connection. Logic <em>could</em> be built into the master to restart failed workers, but the master itself can’t be restarted as easily.</p>&#13;
</div>&#13;
&#13;
<p>Another caveat of the <code>cluster</code> module is <a data-primary="cluster module" data-secondary="TCP/UDP level (Layer 4)" data-type="indexterm" id="idm46291192083720"/>that it essentially operates at Layer 4, at the TCP/UDP level, and isn’t necessarily aware of Layer 7 protocols. Why might this matter? Well, with an incoming HTTP request being sent to a master and two workers, assuming the TCP connection closes after the request finishes, each subsequent request then gets dispatched to a different backend service. However, with gRPC over HTTP/2, those connections are intentionally left open for much longer. In these situations, future gRPC calls will not get dispatched to separate worker processes—they’ll be stuck with just one. When this happens, you’ll often see that one worker is doing most of the work and the whole purpose of clustering has been defeated.</p>&#13;
&#13;
<p>This issue with <a data-primary="sticky connections, cluster module" data-type="indexterm" id="idm46291192081256"/><a data-primary="cluster module" data-secondary="sticky connections" data-type="indexterm" id="idm46291192080552"/>sticky connections can be proved by adapting it to the code written previously in <a data-type="xref" href="ch02.html#ch_protocols_sec_grpc">“RPC with gRPC”</a>. By leaving the producer and consumer code exactly the same, and by introducing the generic cluster master from <a data-type="xref" href="#ex_cluster_master">Example 3-1</a>, the issue surfaces. Run the producer master and the consumer, and make several HTTP requests to the consumer, and the returned <code>producer_data.pid</code> value will always be the same. Then, stop and restart the consumer. This will cause the HTTP/2 connection to stop and start again. The round-robin <a data-primary="cluster module" data-secondary="round robin routing" data-type="indexterm" id="idm46291192077064"/><a data-primary="round robin routing" data-secondary="cluster module" data-type="indexterm" id="idm46291191962152"/>routing of <code>cluster</code> will then route the consumer to the other worker. Make several HTTP requests to the consumer again, and the <code>producer_data.pid</code> values will now all point to the second worker.</p>&#13;
&#13;
<p>Another reason you shouldn’t always reach for the <code>cluster</code> module is that it won’t always make an <a data-primary="cluster module" data-secondary="application speed" data-type="indexterm" id="idm46291191959640"/><a data-primary="applications" data-secondary="speed, cluster module" data-type="indexterm" id="idm46291191958792"/>application faster. In some situations it can simply consume more resources and have either no effect or a negative effect on the performance of the application. Consider, for example, an environment where a process is limited to a single CPU core. This can happen if you’re running on a <a data-primary="VPS (Virtual Private Server)" data-type="indexterm" id="idm46291191957512"/>VPS (Virtual Private Server, a fancy name for a dedicated virtual machine) such as a <code>t3.small</code> machine offered on AWS EC2. It can also happen if a process is running inside of a container with CPU constraints, which can be configured when running an application within Docker.</p>&#13;
&#13;
<p>The reason for a slowdown is this: when running a cluster with two workers, there are three single-threaded instances of JavaScript running. However, there is a single CPU core available to run each instance one at a time. This means the operating system has to do more work deciding which of the three processes runs at any given time. True, the master instance is mostly asleep, but the two workers will fight with each other for CPU cycles.</p>&#13;
&#13;
<p>Time to switch from theory to practice. First, create a new file for simulating a service that performs CPU-intensive work, making it a candidate to use with <code>cluster</code>. This service will simply calculate Fibonacci values based on an input number. <a data-type="xref" href="#ex_fibonacci">Example 3-2</a> is an illustration of such a service.</p>&#13;
<div data-type="example" id="ex_fibonacci">&#13;
<h5><span class="label">Example 3-2. </span><em>cluster-fibonacci.js</em></h5>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting"><code class="c-Hashbang">#!/usr/bin/env node&#13;
</code><code>&#13;
</code><code class="c1">// npm install fastify@3.2&#13;
</code><code class="kr">const</code><code> </code><code class="nx">server</code><code> </code><code class="o">=</code><code> </code><code class="nx">require</code><code class="p">(</code><code class="s1">'fastify'</code><code class="p">)</code><code class="p">(</code><code class="p">)</code><code class="p">;</code><code>&#13;
</code><code class="kr">const</code><code> </code><code class="nx">HOST</code><code> </code><code class="o">=</code><code> </code><code class="nx">process</code><code class="p">.</code><code class="nx">env</code><code class="p">.</code><code class="nx">HOST</code><code> </code><code class="o">||</code><code> </code><code class="s1">'127.0.0.1'</code><code class="p">;</code><code>&#13;
</code><code class="kr">const</code><code> </code><code class="nx">PORT</code><code> </code><code class="o">=</code><code> </code><code class="nx">process</code><code class="p">.</code><code class="nx">env</code><code class="p">.</code><code class="nx">PORT</code><code> </code><code class="o">||</code><code> </code><code class="mi">4000</code><code class="p">;</code><code>&#13;
&#13;
</code><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="sb">`</code><code class="sb">worker pid=</code><code class="si">${</code><code class="nx">process</code><code class="p">.</code><code class="nx">pid</code><code class="si">}</code><code class="sb">`</code><code class="p">)</code><code class="p">;</code><code>&#13;
&#13;
</code><code class="nx">server</code><code class="p">.</code><code class="nx">get</code><code class="p">(</code><code class="s1">'/:limit'</code><code class="p">,</code><code> </code><code class="nx">async</code><code> </code><code class="p">(</code><code class="nx">req</code><code class="p">,</code><code> </code><code class="nx">reply</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="p">{</code><code> </code><a class="co" href="#callout_scaling_CO2-1" id="co_scaling_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
  </code><code class="k">return</code><code> </code><code class="nb">String</code><code class="p">(</code><code class="nx">fibonacci</code><code class="p">(</code><code class="nb">Number</code><code class="p">(</code><code class="nx">req</code><code class="p">.</code><code class="nx">params</code><code class="p">.</code><code class="nx">limit</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code class="p">;</code><code>&#13;
</code><code class="p">}</code><code class="p">)</code><code class="p">;</code><code>&#13;
&#13;
</code><code class="nx">server</code><code class="p">.</code><code class="nx">listen</code><code class="p">(</code><code class="nx">PORT</code><code class="p">,</code><code> </code><code class="nx">HOST</code><code class="p">,</code><code> </code><code class="p">(</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="p">{</code><code>&#13;
  </code><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="sb">`</code><code class="sb">Producer running at http://</code><code class="si">${</code><code class="nx">HOST</code><code class="si">}</code><code class="sb">:</code><code class="si">${</code><code class="nx">PORT</code><code class="si">}</code><code class="sb">`</code><code class="p">)</code><code class="p">;</code><code>&#13;
</code><code class="p">}</code><code class="p">)</code><code class="p">;</code><code>&#13;
&#13;
</code><code class="kd">function</code><code> </code><code class="nx">fibonacci</code><code class="p">(</code><code class="nx">limit</code><code class="p">)</code><code> </code><code class="p">{</code><code> </code><a class="co" href="#callout_scaling_CO2-2" id="co_scaling_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
  </code><code class="kd">let</code><code> </code><code class="nx">prev</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code class="nx">n</code><code class="p">,</code><code> </code><code class="nx">next</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code class="nx">n</code><code class="p">,</code><code> </code><code class="nx">swap</code><code class="p">;</code><code>&#13;
  </code><code class="k">while</code><code> </code><code class="p">(</code><code class="nx">limit</code><code class="p">)</code><code> </code><code class="p">{</code><code>&#13;
    </code><code class="nx">swap</code><code> </code><code class="o">=</code><code> </code><code class="nx">prev</code><code class="p">;</code><code>&#13;
    </code><code class="nx">prev</code><code> </code><code class="o">=</code><code> </code><code class="nx">prev</code><code> </code><code class="o">+</code><code> </code><code class="nx">next</code><code class="p">;</code><code>&#13;
    </code><code class="nx">next</code><code> </code><code class="o">=</code><code> </code><code class="nx">swap</code><code class="p">;</code><code>&#13;
    </code><code class="nx">limit</code><code class="o">--</code><code class="p">;</code><code>&#13;
  </code><code class="p">}</code><code>&#13;
  </code><code class="k">return</code><code> </code><code class="nx">next</code><code class="p">;</code><code>&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO2-1" id="callout_scaling_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The service has a single route, <code>/&lt;limit&gt;</code>, where <code>limit</code> is the number of iterations to count.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO2-2" id="callout_scaling_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>fibonacci()</code> method does a <a data-primary="event loops" data-secondary="fibonacci() method and" data-type="indexterm" id="idm46291191740392"/><a data-primary="fibonnaci() method" data-type="indexterm" id="idm46291191739544"/>lot of CPU-intensive math and blocks the event loop.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The same <a data-type="xref" href="#ex_cluster_master">Example 3-1</a> code can be used for acting as the cluster master. Re-create the content from the cluster master example and place it in a <em>master-fibonacci.js</em> file next to <em>cluster-fibonacci.js</em>. Then, update it so that it’s loading <em>cluster-fibonacci.js</em>, instead of <em>producer-http-basic.js</em>.</p>&#13;
&#13;
<p>The first thing you’ll do is run a benchmark against a cluster of Fibonacci services. Execute the <em>master-fibonacci.js</em> file and then run a benchmarking command:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>npm install -g autocannon@6                  <code class="c"># terminal 1</code>&#13;
<code class="nv">$ </code>node master-fibonacci.js                     <code class="c"># terminal 1</code>&#13;
<code class="nv">$ </code>autocannon -c <code class="m">2</code> http://127.0.0.1:4000/100000 <code class="c"># terminal 2</code></pre>&#13;
&#13;
<p>This will run the <em>Autocannon</em> benchmarking <a data-primary="Autocannon" data-type="indexterm" id="idm46291191699416"/><a data-primary="benchmarking" data-secondary="Autocannon" data-type="indexterm" id="idm46291191698712"/>tool (covered in more detail in <a data-type="xref" href="#ch_scaling_sec_bm_subsec_autocannon">“Introduction to Autocannon”</a>) against the application. It will run over two connections, as fast as it can, for 10 seconds. Once the operation is complete you’ll get a table of statistics in response. For now you’ll only consider two values, and the values I received have been re-created in <a data-type="xref" href="#table_fibonacci_cluster">Table 3-1</a>.</p>&#13;
<table id="table_fibonacci_cluster">&#13;
<caption><span class="label">Table 3-1. </span>Fibonacci cluster with multiple cores</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Statistic</th>&#13;
<th>Result</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Avg latency</p></td>&#13;
<td><p>147.05ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Avg req/sec</p></td>&#13;
<td><p>13.46 r/s</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Next, kill the <em>master-fibonacci.js</em> cluster master, then run just the <em>cluster-fibonacci.js</em> file directly. Then, run the exact <a data-primary="autocannon command" data-type="indexterm" id="idm46291191683720"/><a data-primary="commands" data-secondary="autocannon" data-type="indexterm" id="idm46291191683016"/>same <code>autocannon</code> command that you ran before. Again, you’ll get some more results, and mine happen to look like <a data-type="xref" href="#table_fibonacci_single">Table 3-2</a>.</p>&#13;
<table id="table_fibonacci_single">&#13;
<caption><span class="label">Table 3-2. </span>Fibonacci single process</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Statistic</th>&#13;
<th>Result</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Avg latency</p></td>&#13;
<td><p>239.61ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Avg req/sec</p></td>&#13;
<td><p>8.2 r/s</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>In this situation, on my machine with multiple CPU cores, I can see that by running two instances of the CPU-intensive Fibonacci service, I’m able to increase throughput by about 40%. You should see something similar.</p>&#13;
&#13;
<p>Next, assuming you have access <a data-primary="CPU, single instance" data-type="indexterm" id="idm46291191656440"/><a data-primary="taskset command" data-type="indexterm" id="idm46291191655944"/><a data-primary="commands" data-secondary="taskset" data-type="indexterm" id="idm46291191655336"/>to a Linux machine, you’ll simulate an environment that only has a single CPU instance available. This is done by using the <code>taskset</code> command to force processes to use a specific CPU core. This command doesn’t exist on macOS, but you can get the gist of it by reading along.</p>&#13;
&#13;
<p>Run the <em>master-fibonacci.js</em> cluster <a data-primary="PID values" data-type="indexterm" id="idm46291191652904"/>master file again. Note that the output of the service includes the PID value of the master, as well as the two workers. Take note of these PID values, and in another terminal, run the following command:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="c"># Linux-only command:</code>&#13;
<code class="nv">$ </code>taskset -cp <code class="m">0</code> &lt;pid&gt; <code class="c"># run for master, worker 1, worker 2</code></pre>&#13;
&#13;
<p>Finally, run the same <code>autocannon</code> command <a data-primary="commands" data-secondary="autocannon" data-type="indexterm" id="idm46291191651192"/><a data-primary="autocannon command" data-type="indexterm" id="idm46291191650248"/>used throughout this section. Once it completes, more information will be provided to you. In my case, I received the results shown in <a data-type="xref" href="#table_fibonacci_cluster_restricted">Table 3-3</a>.</p>&#13;
<table id="table_fibonacci_cluster_restricted">&#13;
<caption><span class="label">Table 3-3. </span>Fibonacci cluster with single core</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Statistic</th>&#13;
<th>Result</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Avg latency</p></td>&#13;
<td><p>252.09ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Avg req/sec</p></td>&#13;
<td><p>7.8 r/s</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>In this case, I can see that using the <code>cluster</code> module, while having more worker threads than I have CPU cores, results in an application that runs slower than if I had only run a single instance of the process on my machine.</p>&#13;
&#13;
<p>The greatest shortcoming of <code>cluster</code> is that it only dispatches incoming requests to processes running on the same machine. The next section looks at a tool that works when application code runs <a data-primary="cluster module" data-secondary="limitations" data-startref="clust_limit" data-type="indexterm" id="idm46291191637416"/>on multiple machines.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reverse Proxies with HAProxy" data-type="sect1"><div class="sect1" id="ch_scaling_sec_rp">&#13;
<h1>Reverse Proxies with HAProxy</h1>&#13;
&#13;
<p>A reverse proxy is a tool <a data-primary="reverse proxy" data-type="indexterm" id="revprox"/><a data-primary="HAProxy" data-type="indexterm" id="idm46291191633576"/><a data-primary="proxies" data-seealso="reverse proxies" data-type="indexterm" id="idm46291191632904"/><a data-primary="scaling" data-secondary="HAProxy and" data-type="indexterm" id="scalHAP"/>that accepts a request from a client, forwards it to a server, takes the response from the server, and sends it back to the client. At first glance it may sound like such a tool merely adds an unnecessary network hop and increases network latency, but as you’ll see, it actually provides many useful features to a service stack. Reverse proxies often operate at either Layer 4, such as TCP, or Layer 7, &#13;
<span class="keep-together">via HTTP.</span></p>&#13;
&#13;
<p>One of the features it <a data-primary="load balancing" data-secondary="reverse proxies" data-type="indexterm" id="idm46291191629032"/><a data-primary="reverse proxy" data-secondary="load balancing" data-type="indexterm" id="idm46291191628024"/>provides is that of load balancing. A reverse proxy can accept an incoming request and forward it to one of several servers before replying with the response to the client. Again, this may sound like an additional hop for no reason, as a client could maintain a list of upstream servers and directly communicate with a specific server. However, consider the situation where an organization may have &#13;
<span class="keep-together">several</span> different API servers running. An organization wouldn’t want to put the onus of choosing which API instance to use on a third-party consumer, like by exposing <code>api1.example.org</code> through <code>api9.example.org</code>. Instead, consumers should be able to use &#13;
<span class="keep-together"><code>api.example.org</code></span> and their requests should automatically get routed to an appropriate service. A diagram of this concept is shown in <a data-type="xref" href="#fig_reverse_proxy">Figure 3-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig_reverse_proxy">&#13;
<img alt="A request comes from the internet, passes through a reverse proxy, and gets sent to a Node.js application" src="assets/dsnj_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Reverse proxies intercept incoming network traffic</h6>&#13;
</div></figure>&#13;
&#13;
<p>There are several different approaches a reverse proxy can take when choosing which backend service to route an incoming request to. Just like with the <code>cluster</code> module, the round-robin <a data-primary="round robin routing" data-secondary="reverse proxies" data-type="indexterm" id="idm46291191600200"/><a data-primary="reverse proxy" data-secondary="round robin routing" data-type="indexterm" id="idm46291191599352"/>is usually the default behavior. Requests can also be dispatched based on which backend service is currently servicing the fewest requests. They can be dispatched randomly, or they can even be dispatched based on content of the initial request, such as a session ID stored in an HTTP URL or cookie (also known as a sticky session). And, perhaps most importantly, a reverse proxy can poll <a data-primary="reverse proxy" data-secondary="backend services" data-type="indexterm" id="idm46291191597976"/>backend services to see which ones are healthy, refusing to dispatch requests to services that aren’t healthy.</p>&#13;
&#13;
<p>Other beneficial features include <a data-primary="reverse proxy" data-secondary="HTTP requests" data-type="indexterm" id="idm46291191596488"/>cleaning up or rejecting malformed HTTP requests (which can prevent bugs in the Node.js HTTP parser from being exploited), logging requests so that application code doesn’t have to, adding request timeouts, and performing gzip <a data-primary="compression" data-secondary="gzip" data-type="indexterm" id="idm46291191595272"/>compression and TLS encryption. The benefits of a reverse proxy usually far outweigh the losses for all but the most performance-critical applications. Because of this you should almost always use some form of reverse proxy between <a data-primary="reverse proxy" data-startref="rev_prox" data-type="indexterm" id="idm46291191594040"/>your Node.js applications and the internet.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Introduction to HAProxy" data-type="sect2"><div class="sect2" id="idm46291191592936">&#13;
<h2>Introduction to HAProxy</h2>&#13;
&#13;
<p>HAProxy is a very performant open <a data-primary="HAProxy" data-type="indexterm" id="hap_prox"/>source reverse proxy that works with both Layer 4 and Layer 7 protocols. It’s written in C and is designed to be stable and use minimal resources, offloading as much processing as possible to the kernel. Like JavaScript, HAProxy is <a data-primary="HAProxy" data-secondary="events and" data-type="indexterm" id="idm46291191590136"/><a data-primary="events" data-secondary="HAProxy" data-type="indexterm" id="idm46291191589288"/>event driven and single threaded.</p>&#13;
&#13;
<p>HAProxy is quite simple to setup. It can be deployed by shipping a single binary executable weighing in at about a dozen megabytes. Configuration can be done entirely using a single text file.</p>&#13;
&#13;
<p>Before you start running HAProxy, you’ll <a data-primary="HAProxy" data-secondary="installing" data-type="indexterm" id="idm46291191587336"/>first need to have it installed. A few suggestions for doing so are provided in <a data-type="xref" href="app01.html#appendix_install_haproxy">Appendix A</a>. Otherwise, feel free to use your preferred software installation method to get a copy of HAProxy (at least v2) installed on your development machine.</p>&#13;
&#13;
<p>HAProxy provides an <a data-primary="HAProxy" data-secondary="web dashboard" data-type="indexterm" id="idm46291191585080"/>optional web dashboard that displays statistics for a running HAProxy instance. Create an HAProxy configuration file, one that doesn’t yet perform any <a data-primary="HAProxy" data-secondary="configuration file" data-type="indexterm" id="idm46291191583928"/>actual reverse proxying but instead just exposes the dashboard. Create a file named <em>haproxy/stats.cfg</em> in your project folder and add the content shown in <a data-type="xref" href="#ex_haproxy_stats">Example 3-3</a>.</p>&#13;
<div data-type="example" id="ex_haproxy_stats">&#13;
<h5><span class="label">Example 3-3. </span><em>haproxy/stats.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">frontend inbound <a class="co" href="#callout_scaling_CO3-1" id="co_scaling_CO3-1"><img alt="1" src="assets/1.png"/></a>&#13;
  mode http <a class="co" href="#callout_scaling_CO3-2" id="co_scaling_CO3-2"><img alt="2" src="assets/2.png"/></a>&#13;
  bind localhost:8000&#13;
  stats enable <a class="co" href="#callout_scaling_CO3-3" id="co_scaling_CO3-3"><img alt="3" src="assets/3.png"/></a>&#13;
  stats uri /admin?stats</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO3-1" id="callout_scaling_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Create a <code>frontend</code> called <code>inbound</code>.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO3-2" id="callout_scaling_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Listen for HTTP traffic on port <code>:8000</code>.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO3-3" id="callout_scaling_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Enable the stats interface.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>With that file created, you’re now ready to execute HAProxy. Run the following command in a terminal window:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>haproxy -f haproxy/stats.cfg</pre>&#13;
&#13;
<p>You’ll get a few warnings printed in the console since the config file is a little too simple. These warnings will be fixed soon, but HAProxy will otherwise run just fine. Next, in a web browser, open the following URL:</p>&#13;
&#13;
<pre data-type="programlisting">http://localhost:8000/admin?stats</pre>&#13;
&#13;
<p>At this point you’ll be able to see some stats about the HAProxy instance. Of course, there isn’t anything interesting in there just yet. The only statistics displayed are for the single frontend. At this point you can refresh the page, and the bytes transferred count will increase because the dashboard also measures requests to itself.</p>&#13;
&#13;
<p>HAProxy works by creating both <em>frontends</em>—ports that <a data-primary="HAProxy" data-secondary="frontends ports" data-type="indexterm" id="idm46291191561720"/><a data-primary="HAProxy" data-secondary="backends ports" data-type="indexterm" id="idm46291191560152"/>it listens on for incoming requests—and <em>backends</em>—upstream backend services identified by hosts and ports that it will forward requests to. The next section actually creates a backend to route incoming requests to.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46291191558392">&#13;
<h5>Alternatives to HAProxy</h5>&#13;
<p>There are plenty of alternative <a data-primary="HAProxy" data-secondary="alternatives" data-type="indexterm" id="idm46291191557064"/><a data-primary="reverse proxy" data-secondary="Nginx" data-type="indexterm" id="idm46291191556088"/><a data-primary="Nginx" data-type="indexterm" id="idm46291191555144"/>reverse proxies to consider. One of the most popular is <a class="orm:hideurl" href="https://nginx.org">Nginx</a>. Much like HAProxy, it’s an open source tool distributed as a binary that can be easily run with a single configuration file. Nginx is able to perform load balancing, compression, TLS termination, and many other features that HAProxy supports. It is notably different in that it is classified as a web server—it’s able to map requests to files on disk, a feature intentionally absent in HAProxy. Nginx is also able to cache responses.</p>&#13;
&#13;
<p>When running applications on AWS, the <a data-primary="reverse proxy" data-secondary="ELB (Elastic Load Balancing)" data-type="indexterm" id="idm46291191551656"/><a data-primary="ELB (Elastic Load Balancing)" data-type="indexterm" id="idm46291191550616"/><a data-primary="load balancing" data-secondary="ELB (elastic load balancing)" data-type="indexterm" id="idm46291191549928"/>preferred tool for performing load balancing and TLS termination is going to be ELB (Elastic Load Balancing). Other functionality of HAProxy, like the ability to route requests to backend services based on the content, can be performed by API Gateway.</p>&#13;
&#13;
<p>If you’re just looking for <a data-primary="Traefik" data-type="indexterm" id="idm46291191547960"/><a data-primary="Kong Gateway" data-type="indexterm" id="idm46291191547256"/><a data-primary="load balancing" data-secondary="Traefik" data-type="indexterm" id="idm46291191546584"/><a data-primary="load balancing" data-secondary="Kong Gateway" data-type="indexterm" id="idm46291191545640"/>an open source solution for performing more robust routing than what HAProxy offers, <a data-primary="HAProxy" data-startref="hap_prox" data-type="indexterm" id="idm46291191544568"/><a data-primary="scaling" data-secondary="HAProxy" data-startref="scalHAP" data-type="indexterm" id="idm46291191543624"/>consider <a class="orm:hideurl" href="https://traefik.io/traefik/">Traefik</a> and <a class="orm:hideurl" href="https://konghq.com">Kong Gateway</a>.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Load Balancing and Health Checks" data-type="sect2"><div class="sect2" id="ch_scaling_sec_rp_subsec_health">&#13;
<h2>Load Balancing and Health Checks</h2>&#13;
&#13;
<p>This section enables the load balancing features <a data-primary="HAProxy" data-secondary="load balancing" data-type="indexterm" id="idm46291191521592"/><a data-primary="load balancing" data-secondary="HAProxy" data-type="indexterm" id="idm46291191520744"/><a data-primary="scaling" data-secondary="load balancing" data-type="indexterm" id="scalLOAD"/><a data-primary="scaling" data-secondary="health check" data-type="indexterm" id="scalHEAL"/>of HAProxy and also gets rid of those warnings in the <a data-type="xref" href="#ex_haproxy_stats">Example 3-3</a> configuration. Earlier you looked at the reasons why an organization should use a reverse proxy to intercept incoming traffic. In this section, you’ll configure HAProxy to do just that; it will act as a load balancer between external traffic and the <em>web-api</em> service, <a data-primary="load balancing" data-secondary="web-api service" data-type="indexterm" id="idm46291191516200"/>exposing a single host/port combination but ultimately serving up traffic from two service instances. <a data-type="xref" href="#fig_haproxy_loadbalance">Figure 3-3</a> provides a visual representation of this.</p>&#13;
&#13;
<p>Technically, no application changes need to be made to allow for load balancing with HAProxy. However, to better show off the capabilities of HAProxy, a feature called a <em>health check</em> will be <a data-primary="HAProxy" data-secondary="health check" data-type="indexterm" id="HAPcheck"/><a data-primary="health check (HAProxy)" data-type="indexterm" id="HAPhealth"/>added. A simple endpoint that responds with a 200 status code will suffice for now. To do this, duplicate the <em>web-api/consumer-http-basic.js</em> file and add a new endpoint, as shown in <a data-type="xref" href="#ex_health_endpoint">Example 3-4</a>. <a data-type="xref" href="ch04.html#ch_monitoring_sec_health">“Health Checks”</a> will look at building out a more accurate health check endpoint.</p>&#13;
&#13;
<figure><div class="figure" id="fig_haproxy_loadbalance">&#13;
<img alt="HAProxy load balancing requests to two web-api instances" src="assets/dsnj_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>Load balancing with HAProxy</h6>&#13;
</div></figure>&#13;
<div data-type="example" id="ex_health_endpoint">&#13;
<h5><span class="label">Example 3-4. </span><em>web-api/consumer-http-healthendpoint.js</em> (truncated)</h5>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting"><code class="nx">server</code><code class="p">.</code><code class="nx">get</code><code class="p">(</code><code class="s1">'/health'</code><code class="p">,</code> <code class="nx">async</code> <code class="p">()</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s1">'health check'</code><code class="p">);</code>&#13;
  <code class="k">return</code> <code class="s1">'OK'</code><code class="p">;</code>&#13;
<code class="p">});</code></pre></div>&#13;
&#13;
<p>You’ll also need a new configuration file for HAProxy. Create a file named <em>haproxy/load-balance.cfg</em> and add the content from <a data-type="xref" href="#ex_haproxy_loadbalance">Example 3-5</a> to it.</p>&#13;
<div data-type="example" id="ex_haproxy_loadbalance">&#13;
<h5><span class="label">Example 3-5. </span><em>haproxy/load-balance.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">defaults <a class="co" href="#callout_scaling_CO4-1" id="co_scaling_CO4-1"><img alt="1" src="assets/1.png"/></a>&#13;
  mode http&#13;
  timeout connect 5000ms <a class="co" href="#callout_scaling_CO4-2" id="co_scaling_CO4-2"><img alt="2" src="assets/2.png"/></a>&#13;
  timeout client 50000ms&#13;
  timeout server 50000ms&#13;
&#13;
frontend inbound&#13;
  bind localhost:3000&#13;
  default_backend web-api <a class="co" href="#callout_scaling_CO4-3" id="co_scaling_CO4-3"><img alt="3" src="assets/3.png"/></a>&#13;
  stats enable&#13;
  stats uri /admin?stats&#13;
&#13;
backend web-api <a class="co" href="#callout_scaling_CO4-4" id="co_scaling_CO4-4"><img alt="4" src="assets/4.png"/></a>&#13;
  option httpchk GET /health <a class="co" href="#callout_scaling_CO4-5" id="co_scaling_CO4-5"><img alt="5" src="assets/5.png"/></a>&#13;
  server web-api-1 localhost:3001 check <a class="co" href="#callout_scaling_CO4-6" id="co_scaling_CO4-6"><img alt="6" src="assets/6.png"/></a>&#13;
  server web-api-2 localhost:3002 check</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO4-1" id="callout_scaling_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>defaults</code> section configures multiple frontends.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO4-2" id="callout_scaling_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Timeout values have been added, eliminating the HAProxy warnings.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO4-3" id="callout_scaling_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>A frontend can route to multiple backends. In this case, only the <em>web-api</em> backend <a data-primary="web-api service" data-secondary="backend" data-type="indexterm" id="idm46291191457784"/>should be routed to.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO4-4" id="callout_scaling_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The first backend, <em>web-api</em>, has been configured.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO4-5" id="callout_scaling_CO4-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Health checks for this backend make a <code>GET /health</code> HTTP request.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO4-6" id="callout_scaling_CO4-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>The <em>web-api</em> routes requests to two backends, and the <code>check</code> parameter enables health checking.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>This configuration file <a data-primary="HAProxy" data-secondary="web-api service" data-type="indexterm" id="idm46291191430088"/><a data-primary="web-api service" data-secondary="HAProxy" data-type="indexterm" id="idm46291191429240"/>instructs HAProxy to look for two <em>web-api</em> instances running on the current machine. To avoid a port collision, the application instances have been instructed to listen on ports <code>:3001</code> and <code>:3002</code>. The <em>inbound</em> frontend is configured to listen on port <code>:3000</code>, essentially allowing HAProxy to be a swap-in replacement for a regular running <em>web-api</em> instance.</p>&#13;
&#13;
<p>Much like with the <code>cluster</code> module <a data-primary="HAProxy" data-secondary="round robin routing" data-type="indexterm" id="idm46291191424936"/><a data-primary="round robin routing" data-secondary="HAProxy" data-type="indexterm" id="idm46291191424088"/>in <a data-type="xref" href="#ch_scaling_sec_clustering">“The Cluster Module”</a>, requests are routed round-robin<sup><a data-type="noteref" href="ch03.html#idm46291191422376" id="idm46291191422376-marker">3</a></sup> between two separate Node.js processes. But now there is one fewer running Node.js process to maintain. As implied by the <code>host:port</code> combination, these processes don’t need to run on localhost for HAProxy to forward the requests.</p>&#13;
&#13;
<p>Now that you’ve created the config file and have a new endpoint, it’s time to run some processes. For this example, you’ll need to open five different terminal windows. Run the following four commands in four different terminal window, and run the fifth command several times in a fifth window:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>node recipe-api/producer-http-basic.js&#13;
<code class="nv">$ PORT</code><code class="o">=</code><code class="m">3001</code> node web-api/consumer-http-healthendpoint.js&#13;
<code class="nv">$ PORT</code><code class="o">=</code><code class="m">3002</code> node web-api/consumer-http-healthendpoint.js&#13;
<code class="nv">$ </code>haproxy -f ./haproxy/load-balance.cfg&#13;
&#13;
<code class="nv">$ </code>curl http://localhost:3000/ <code class="c"># run several times</code></pre>&#13;
&#13;
<p>Notice that in the output <a data-primary="curl command" data-type="indexterm" id="idm46291191410584"/><a data-primary="commands" data-secondary="curl" data-type="indexterm" id="idm46291191409976"/><a data-primary="web-api service" data-secondary="round robin routing" data-type="indexterm" id="idm46291191409096"/>for the <code>curl</code> command, <code>consumer_pid</code> cycles between two values as HAProxy routes requests round-robin between the two <em>web-api</em> instances. Also, notice that the <code>producer_pid</code> value stays the same since only a single <em>recipe-api</em> instance is running.</p>&#13;
&#13;
<p>This command order runs the dependent programs first. In this case the <em>recipe-api</em> instance is run first, then two <em>web-api</em> instances, followed by HAProxy. Once the HAProxy instance is running, you should notice something interesting in the <em>web-api</em> terminals: the <em>health check</em> message is being printed over and over, once every two seconds. This is because HAProxy has started performing health checks.</p>&#13;
&#13;
<p>Open up the HAProxy statistics <a data-primary="HAProxy" data-secondary="statistics page" data-type="indexterm" id="idm46291191396936"/>page again<sup><a data-type="noteref" href="ch03.html#idm46291191395832" id="idm46291191395832-marker">4</a></sup> by visiting <a href="http://localhost:3000/admin?stats"><em class="hyperlink">http://localhost:3000/admin?stats</em></a>. You should now see two sections in the output: one for the <em>inbound</em> frontend and one for the new <em>web-api</em> backend. In the <em>web-api</em> section, you should see the two different server instances listed. Both of them should have green backgrounds, signaling that their health checks are passing. A truncated version of the results I get is shown in <a data-type="xref" href="#table_haproxy_stats">Table 3-4</a>.</p>&#13;
<table id="table_haproxy_stats">&#13;
<caption><span class="label">Table 3-4. </span>Truncated HAProxy stats</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>Sessions total</th>&#13;
<th>Bytes out</th>&#13;
<th>LastChk</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>web-api-1</p></td>&#13;
<td><p>6</p></td>&#13;
<td><p>2,262</p></td>&#13;
<td><p>L7OK/200 in 1ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>web-api-2</p></td>&#13;
<td><p>5</p></td>&#13;
<td><p>1,885</p></td>&#13;
<td><p>L7OK/200 in 0ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Backend</p></td>&#13;
<td><p>11</p></td>&#13;
<td><p>4,147</p></td>&#13;
<td/>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The final line, <em>Backend</em>, represents the totals for the columns above it. In this output, you can see that the requests are distributed essentially equally between the two instances. You can also see that the health checks are passing by examining the <em>LastChk</em> column. In this case both servers are passing the L7 health check (HTTP) by returning a 200 status within 1ms.</p>&#13;
&#13;
<p>Now it’s time to have a little fun with this setup. First, switch to one of the terminals running a copy of <em>web-api</em>. Stop the process by pressing Ctrl + C. Then, switch back to the statistics webpage and refresh a few times. Depending on how quick you are, you should see one of the lines in the <em>web-api</em> section change from green to yellow to red. This is because HAProxy has determined the service is down since it’s no longer responding to health checks.</p>&#13;
&#13;
<p>Now that HAProxy has determined the service to be down, switch back to the fifth terminal screen and run a few <a data-primary="curl command" data-type="indexterm" id="idm46291191358264"/><a data-primary="commands" data-secondary="curl" data-type="indexterm" id="idm46291191357560"/>more <code>curl</code> commands. Notice that you continuously get responses, albeit from the same <em>web-api</em> PID. Since HAProxy knows one of the services is down, it’s only going to route requests to the healthy instance.</p>&#13;
&#13;
<p>Switch back to the terminal where you killed the <em>web-api</em> instance, start it again, and switch back to the stats page. Refresh a few times and notice how the status turns from red to yellow to green. Switch back to the <code>curl</code> terminal, run the command a few more times, and notice that HAProxy is now dispatching commands between both instances again.</p>&#13;
&#13;
<p>At first glance, this setup seems to work pretty smoothly. You killed a service, and it stopped receiving traffic. Then, you brought it back, and the traffic resumed. But can you guess what the problem is?</p>&#13;
&#13;
<p>Earlier, in the console output from the running <em>web-api</em> instances, the health checks could be seen firing every two seconds. This means that there is a length of time for which a server can be down, but HAProxy isn’t aware of it yet. This means that there are periods of time that requests can still fail. To illustrate this, first restart the <a data-primary="web-api service" data-secondary="restarting" data-type="indexterm" id="idm46291191351832"/>dead <em>web-api</em> instance, then pick one of the <code>consumer_pid</code> values from the output and replace the <code>CONSUMER_PID</code> in the following command:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code><code class="nb">kill</code> &lt;CONSUMER_PID&gt; <code class="se">\</code>&#13;
  <code class="o">&amp;&amp;</code> curl http://localhost:3000/ <code class="se">\</code>&#13;
  <code class="o">&amp;&amp;</code> curl http://localhost:3000/</pre>&#13;
&#13;
<p>What this command does is kill a <em>web-api</em> process and then make two HTTP requests, all so quickly that HAProxy shouldn’t have enough time to know that something bad has happened. In the output, you should see that one of the commands has failed and that the other has succeeded.</p>&#13;
&#13;
<p>The health checks can be configured a little more than what’s been shown so far. Additional <code>flag value</code> pairs can be specified after the <code>check</code> flag present at the end of the <code>server</code> lines. For example, such a configuration might look like this: <code>server ... check inter 10s fall 4</code>. <a data-type="xref" href="#table_haproxy_health">Table 3-5</a> describes these flags and how they may be &#13;
<span class="keep-together">configured.</span></p>&#13;
<table id="table_haproxy_health">&#13;
<caption><span class="label">Table 3-5. </span>HAProxy health check flags</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Flag</th>&#13;
<th>Type</th>&#13;
<th>Default</th>&#13;
<th>Description</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p><code>inter</code></p></td>&#13;
<td><p>interval</p></td>&#13;
<td><p>2s</p></td>&#13;
<td><p>Interval between checks</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>fastinter</code></p></td>&#13;
<td><p>interval</p></td>&#13;
<td><p><code>inter</code></p></td>&#13;
<td><p>Interval when transitioning states</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>downinter</code></p></td>&#13;
<td><p>interval</p></td>&#13;
<td><p><code>inter</code></p></td>&#13;
<td><p>Interval between checks when down</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>fall</code></p></td>&#13;
<td><p>int</p></td>&#13;
<td><p>3</p></td>&#13;
<td><p>Consecutive healthy checks before being UP</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>rise</code></p></td>&#13;
<td><p>int</p></td>&#13;
<td><p>2</p></td>&#13;
<td><p>Consecutive unhealthy checks before being DOWN</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Even though the health checks can be configured to run very aggressively, there still isn’t a perfect solution to the problem of detecting when a service is down; with this approach there is always a risk that requests will be sent to an unhealthy service. <a data-type="xref" href="ch08.html#ch_resilience_sec_messaging">“Idempotency and Messaging Resilience”</a> looks at a solution to this problem <a data-primary="HAProxy" data-secondary="health check" data-startref="HAPcheck" data-type="indexterm" id="idm46291191296904"/><a data-primary="health check (HAProxy)" data-startref="HAPhealth" data-type="indexterm" id="idm46291191295688"/><a data-primary="scaling" data-secondary="health check" data-startref="scalHEAL" data-type="indexterm" id="idm46291191294744"/>where clients are configured to retry failed requests.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Compression" data-type="sect2"><div class="sect2" id="idm46291191522856">&#13;
<h2>Compression</h2>&#13;
&#13;
<p>Compression can be <a data-primary="compression" data-secondary="HAProxy" data-type="indexterm" id="idm46291191292248"/><a data-primary="HAProxy" data-secondary="compression" data-type="indexterm" id="idm46291191291240"/>configured easily with HAProxy by setting additional configuration flags on the particular backend containing content that HAProxy should compress. See <a data-type="xref" href="#ex_haproxy_compression">Example 3-6</a> for a demonstration of how to do this.</p>&#13;
<div data-type="example" id="ex_haproxy_compression">&#13;
<h5><span class="label">Example 3-6. </span><em>haproxy/compression.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">defaults&#13;
  mode http&#13;
  timeout connect 5000ms&#13;
  timeout client 50000ms&#13;
  timeout server 50000ms&#13;
&#13;
frontend inbound&#13;
  bind localhost:3000&#13;
  default_backend web-api&#13;
&#13;
backend web-api&#13;
  compression offload <a class="co" href="#callout_scaling_CO5-1" id="co_scaling_CO5-1"><img alt="1" src="assets/1.png"/></a>&#13;
  compression algo gzip <a class="co" href="#callout_scaling_CO5-2" id="co_scaling_CO5-2"><img alt="2" src="assets/2.png"/></a>&#13;
  compression type application/json text/plain <a class="co" href="#callout_scaling_CO5-3" id="co_scaling_CO5-3"><img alt="3" src="assets/3.png"/></a>&#13;
  server web-api-1 localhost:3001</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO5-1" id="callout_scaling_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Prevent HAProxy from forwarding the <code>Accept-Encoding</code> header to the backend service.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO5-2" id="callout_scaling_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This enables gzip compression; other algorithms are also available.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO5-3" id="callout_scaling_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Compression is enabled depending on the <code>Content-Type</code> header.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>This example specifically states that compression should only be enabled on responses that have a <code>Content-Type</code> header value of <code>application/json</code>, which is what the two services have been using, or <code>text/plain</code>, which can sometimes sneak through if an endpoint hasn’t been properly configured.</p>&#13;
&#13;
<p>Much like in <a data-type="xref" href="ch02.html#ex_node_gzip">Example 2-4</a>, where <a data-primary="compression" data-secondary="gzip" data-type="indexterm" id="idm46291191269176"/>gzip compression was performed entirely in Node.js, HAProxy is also going to perform compression only when it knows the client supports it by checking the <code>Accept-Encoding</code> header. To confirm that HAProxy is compressing the responses, run the following commands in separate terminal windows (in this case you only need a single <em>web-api</em> running):</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>node recipe-api/producer-http-basic.js&#13;
<code class="nv">$ PORT</code><code class="o">=</code><code class="m">3001</code> node web-api/consumer-http-basic.js&#13;
<code class="nv">$ </code>haproxy -f haproxy/compression.cfg&#13;
<code class="nv">$ </code>curl http://localhost:3000/&#13;
<code class="nv">$ </code>curl -H <code class="s1">'Accept-Encoding: gzip'</code> http://localhost:3000/ <code class="p">|</code> gunzip</pre>&#13;
&#13;
<p>Performing gzip compression <a data-primary="gzip compression, HAProxy and" data-type="indexterm" id="idm46291191258456"/><a data-primary="HAProxy" data-secondary="compression" data-tertiary="gzip compression" data-type="indexterm" id="idm46291191257800"/><a data-primary="compression" data-secondary="HAProxy" data-type="indexterm" id="idm46291191256680"/>using HAProxy will be more performant than doing it within the Node.js process. <a data-type="xref" href="#ch_scaling_sec_bm_subsec_gzip">“HTTP compression”</a> will test the performance of this.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="TLS Termination" data-type="sect2"><div class="sect2" id="idm46291191254664">&#13;
<h2>TLS Termination</h2>&#13;
&#13;
<p>Performing TLS termination <a data-primary="TLS (Transport Layer Security)" data-secondary="termination, HAProxy" data-type="indexterm" id="TLS_term"/><a data-primary="HAProxy" data-secondary="TLS termination" data-type="indexterm" id="TLSterm"/>in a centralized location is convenient for many reasons. A big reason is that additional logic doesn’t need to be added to applications for updating certificates. Hunting down which instances have outdated certificates can also be avoided. A single team within an organization can handle all of the certificate generation. Applications also don’t have to incur additional CPU overhead.</p>&#13;
&#13;
<p>That said, HAProxy will direct traffic to a single service in this example. The architecture for this looks like <a data-type="xref" href="#fig_haproxy_tls">Figure 3-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig_haproxy_tls">&#13;
<img alt="HAProxy performs TLS Termination, sending unencrypted HTTP traffic to backend services" src="assets/dsnj_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>HAProxy TLS termination</h6>&#13;
</div></figure>&#13;
&#13;
<p>TLS termination is rather straight-forward with HAProxy, and many of the same rules covered in <a data-type="xref" href="ch02.html#ch_protocols_sec_http_subsec_tls">“HTTPS / TLS”</a> still apply. For example, all the certificate generation and chain of trust concepts still apply, and these cert files adhere to well-understood standards. One difference is that in this section a <em>.pem</em> file is used, which is a file containing both the content of the <em>.cert</em> file and the <em>.key</em> files. <a data-type="xref" href="#ex_generate_pem">Example 3-7</a> is a modified version of a previous command. It generates the individual files and concatenates them together.</p>&#13;
<div data-type="example" id="ex_generate_pem">&#13;
<h5><span class="label">Example 3-7. </span>Generating a <em>.pem</em> file</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>openssl req -nodes -new -x509 <code class="se">\</code>&#13;
  -keyout haproxy/private.key <code class="se">\</code>&#13;
  -out haproxy/certificate.cert&#13;
<code class="nv">$ </code>cat haproxy/certificate.cert haproxy/private.key <code class="se">\</code>&#13;
  &gt; haproxy/combined.pem</pre></div>&#13;
&#13;
<p>Another HAProxy configuration script is now needed. <a data-type="xref" href="#ex_haproxy_tls">Example 3-8</a> modifies the <em>inbound</em> frontend to listen via HTTPS and to load the <em>combined.pem</em> file.</p>&#13;
<div data-type="example" id="ex_haproxy_tls">&#13;
<h5><span class="label">Example 3-8. </span><em>haproxy/tls.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">defaults&#13;
  mode http&#13;
  timeout connect 5000ms&#13;
  timeout client 50000ms&#13;
  timeout server 50000ms&#13;
&#13;
global <a class="co" href="#callout_scaling_CO6-1" id="co_scaling_CO6-1"><img alt="1" src="assets/1.png"/></a>&#13;
  tune.ssl.default-dh-param 2048&#13;
&#13;
frontend inbound&#13;
  bind localhost:3000 ssl crt haproxy/combined.pem <a class="co" href="#callout_scaling_CO6-2" id="co_scaling_CO6-2"><img alt="2" src="assets/2.png"/></a>&#13;
  default_backend web-api&#13;
&#13;
backend web-api&#13;
  server web-api-1 localhost:3001</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO6-1" id="callout_scaling_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>global</code> section configures global HAProxy settings.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO6-2" id="callout_scaling_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>ssl</code> flag specifies that the frontend uses TLS, and the <code>crt</code> flag points to the <em>.pem</em> file.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The <code>global</code> section allows for global HAProxy configuration. In this case it sets the Diffie-Hellman key size parameter used by clients and prevents an HAProxy warning.</p>&#13;
&#13;
<p>Now that you’ve configured HAProxy, go ahead and run it with this new configuration file and then send it some requests. Run the following commands in four &#13;
<span class="keep-together">separate</span> terminal windows:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>node recipe-api/producer-http-basic.js        <code class="c"># terminal 1</code>&#13;
<code class="nv">$ PORT</code><code class="o">=</code><code class="m">3001</code> node web-api/consumer-http-basic.js <code class="c"># terminal 2</code>&#13;
<code class="nv">$ </code>haproxy -f haproxy/tls.cfg                    <code class="c"># terminal 3</code>&#13;
<code class="nv">$ </code>curl --insecure https://localhost:3000/       <code class="c"># terminal 4</code></pre>&#13;
&#13;
<p>Since HAProxy is using a <a data-primary="self-signed certificates, HAProxy" data-type="indexterm" id="idm46291191162856"/><a data-primary="HAProxy" data-secondary="self-signed certificates" data-type="indexterm" id="idm46291191162248"/><a data-primary="curl command" data-type="indexterm" id="idm46291191161400"/><a data-primary="commands" data-secondary="curl" data-type="indexterm" id="idm46291191160760"/>self-signed certificate, the <code>curl</code> command requires the &#13;
<span class="keep-together"><code>--insecure</code></span> flag again. With a real-world example, since the HTTPS traffic is public facing, you’d want to use a real certificate authority like <em>Let’s Encrypt</em> to generate certificates for you. Let’s Encrypt comes with a tool called <em>certbot</em>, which can be configured to automatically renew certificates before they expire, as well as reconfigure HAProxy on the fly to make use of the updated certificates. Configuring certbot is beyond the scope of this book, and there exists literature on how to do this.</p>&#13;
&#13;
<p>There are many other options that can be configured regarding TLS in HAProxy. It allows for specifying which <a data-primary="SNI (Server Name Indication)" data-type="indexterm" id="idm46291191153032"/>cipher suites to use, TLS session cache sizes, and SNI (Server Name Indication). A single frontend can specify a port for both standard HTTP and HTTPS. HAProxy can redirect a user agent making an HTTP request to the equivalent HTTPS path.</p>&#13;
&#13;
<p>Performing TLS termination using HAProxy may be more performant than doing it within the Node.js process. <a data-type="xref" href="#ch_scaling_sec_bm_subsec_tls">“TLS termination”</a> will <a data-primary="TLS (Transport Layer Security)" data-secondary="termination, HAProxy" data-startref="TLS_term" data-type="indexterm" id="idm46291191150616"/><a data-primary="HAProxy" data-secondary="TLS termination" data-startref="TLSterm" data-type="indexterm" id="idm46291191149352"/>test this claim.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Rate Limiting and Back Pressure" data-type="sect2"><div class="sect2" id="idm46291191248488">&#13;
<h2>Rate Limiting and Back Pressure</h2>&#13;
&#13;
<p><a data-type="xref" href="#ch_scaling_sec_bm">“SLA and Load Testing”</a> looks at ways to determine how much load a Node.js service can handle. This section looks at ways of enforcing such a limit.</p>&#13;
&#13;
<p>A Node.js process, by <a data-primary="rate limiting" data-type="indexterm" id="ratelimit"/><a data-primary="HAProxy" data-secondary="rate limiting" data-type="indexterm" id="HAPlimit"/><a data-primary="scaling" data-secondary="back pressure" data-type="indexterm" id="scalback"/><a data-primary="scaling" data-secondary="rate limiting" data-type="indexterm" id="scalrate"/>default, will “handle” as many requests as it receives. For example, when creating a basic HTTP server with a callback when a request is received, those callbacks will keep getting scheduled by the <a data-primary="event loops" data-secondary="callbacks" data-type="indexterm" id="idm46291191139784"/><a data-primary="callbacks" data-secondary="event loops" data-type="indexterm" id="idm46291191138840"/>event loop and called whenever possible. Sometimes, though, this can overwhelm a process. If the callback is doing a lot of blocking work, having too many of them scheduled will result in the process locking up. A bigger issue is memory consumption; every single queued callback comes with a new function context containing variables and references to the incoming request. Sometimes the best solution is to reduce the amount of concurrent connections being handled by a Node.js process at a given time.</p>&#13;
&#13;
<p>One way to do this is to <a data-primary="maxConnections property" data-type="indexterm" id="idm46291191136872"/>set the <code>maxConnections</code> property of an <code>http.Server</code> instance. By setting this value, the Node.js process will automatically drop any incoming connections that would increase the connection count to be greater than this limit.</p>&#13;
&#13;
<p>Every popular Node.js HTTP framework on npm will either expose the <code>http.Server</code> instance it uses or provide a method for overriding the value. However, in this example, a basic HTTP server using the built-in <code>http</code> module is constructed.</p>&#13;
&#13;
<p>Create a new file and add the contents of <a data-type="xref" href="#ex_node_maxconn">Example 3-9</a> to it.</p>&#13;
<div data-type="example" id="ex_node_maxconn">&#13;
<h5><span class="label">Example 3-9. </span><em>low-connections.js</em></h5>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting"><code class="c-Hashbang">#!/usr/bin/env node&#13;
</code><code>&#13;
</code><code class="kr">const</code><code> </code><code class="nx">http</code><code> </code><code class="o">=</code><code> </code><code class="nx">require</code><code class="p">(</code><code class="s1">'http'</code><code class="p">)</code><code class="p">;</code><code>&#13;
&#13;
</code><code class="kr">const</code><code> </code><code class="nx">server</code><code> </code><code class="o">=</code><code> </code><code class="nx">http</code><code class="p">.</code><code class="nx">createServer</code><code class="p">(</code><code class="p">(</code><code class="nx">req</code><code class="p">,</code><code> </code><code class="nx">res</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="p">{</code><code>&#13;
  </code><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s1">'current conn'</code><code class="p">,</code><code> </code><code class="nx">server</code><code class="p">.</code><code class="nx">_connections</code><code class="p">)</code><code class="p">;</code><code>&#13;
  </code><code class="nx">setTimeout</code><code class="p">(</code><code class="p">(</code><code class="p">)</code><code> </code><code class="o">=&gt;</code><code> </code><code class="nx">res</code><code class="p">.</code><code class="nx">end</code><code class="p">(</code><code class="s1">'OK'</code><code class="p">)</code><code class="p">,</code><code> </code><code class="mi">10</code><code class="nx">_000</code><code class="p">)</code><code class="p">;</code><code> </code><a class="co" href="#callout_scaling_CO7-1" id="co_scaling_CO7-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="p">}</code><code class="p">)</code><code class="p">;</code><code>&#13;
&#13;
</code><code class="nx">server</code><code class="p">.</code><code class="nx">maxConnections</code><code> </code><code class="o">=</code><code> </code><code class="mi">2</code><code class="p">;</code><code> </code><a class="co" href="#callout_scaling_CO7-2" id="co_scaling_CO7-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code class="nx">server</code><code class="p">.</code><code class="nx">listen</code><code class="p">(</code><code class="mi">3020</code><code class="p">,</code><code> </code><code class="s1">'localhost'</code><code class="p">)</code><code class="p">;</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO7-1" id="callout_scaling_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This <code>setTimeout()</code> simulates slow asynchronous activity, like a database &#13;
<span class="keep-together">operation.</span></p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO7-2" id="callout_scaling_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The maximum number of incoming connections is set to 2.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>This server simulates a slow application. Each incoming request takes 10 seconds to run before the response is received. This won’t simulate a process with heavy CPU usage, but it does simulate a request that is slow enough to possibly overwhelm Node.js.</p>&#13;
&#13;
<p>Next, open four terminal windows. In the first one, run the <em>low-connections.js</em> service. In the other three, make the same HTTP request by using the <code>curl</code> command. You’ll need to run the <code>curl</code> commands within 10 seconds, so you might want to first paste the command three times and then execute them:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>node low-connections.js     <code class="c"># terminal 1</code>&#13;
<code class="nv">$ </code>curl http://localhost:3020/ <code class="c"># terminals 2-4</code></pre>&#13;
&#13;
<p>Assuming you ran the commands quick enough, the first two <code>curl</code> calls should run, albeit slowly, pausing for 10 seconds before finally writing the message <code>OK</code> to the terminal window. The third time it ran, however, the command should have written an error and would have closed immediately. On my machine, the <code>curl</code> command prints <code>curl: (56) Recv failure: Connection reset by peer</code>. Likewise, the server terminal window should <em>not</em> have written a message about the current number of &#13;
<span class="keep-together">connections.</span></p>&#13;
&#13;
<p>The <code>server.maxConnections</code> value sets a hard <a data-primary="Node.js" data-secondary="maximum connections" data-type="indexterm" id="idm46291191004824"/>limit to the number of requests for this particular server instance, and Node.js will drop any connections above that limit.</p>&#13;
&#13;
<p>This might sound a bit harsh! As a client consuming a service, a more ideal situation might instead be to have the server queue up the request. Luckily, HAProxy can be configured to do this on behalf of the application. Create a new HAProxy configuration file with the content from <a data-type="xref" href="#ex_haproxy_maxconn">Example 3-10</a>.</p>&#13;
<div data-type="example" id="ex_haproxy_maxconn">&#13;
<h5><span class="label">Example 3-10. </span><em>haproxy/backpressure.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">defaults&#13;
  maxconn 8 <a class="co" href="#callout_scaling_CO8-1" id="co_scaling_CO8-1"><img alt="1" src="assets/1.png"/></a>&#13;
  mode http&#13;
&#13;
frontend inbound&#13;
  bind localhost:3010&#13;
  default_backend web-api&#13;
&#13;
backend web-api&#13;
  option httpclose <a class="co" href="#callout_scaling_CO8-2" id="co_scaling_CO8-2"><img alt="2" src="assets/2.png"/></a>&#13;
  server web-api-1 localhost:3020 maxconn 2 <a class="co" href="#callout_scaling_CO8-3" id="co_scaling_CO8-3"><img alt="3" src="assets/3.png"/></a></pre>&#13;
<div style="page-break-after: always;"/>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_scaling_CO8-1" id="callout_scaling_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Max connections can be configured globally. This includes incoming frontend and outgoing backend connections.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO8-2" id="callout_scaling_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Force HAProxy to close HTTP connections to the backend.</p></dd>&#13;
<dt><a class="co" href="#co_scaling_CO8-3" id="callout_scaling_CO8-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Max connections can be specified per backend-service instance.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>This example sets a global flag of <code>maxconn 8</code>. This means that between all frontends and backends combined, only eight connections can be running at the same time, including any calls to the admin interface. Usually you’ll want to set this to a conservative value, if you use it at all. More interestingly, however, is the <code>maxconn 2</code> flag attached to the specific backend instance. This will be the real limiting factor with this configuration file.</p>&#13;
&#13;
<p>Also, note that <code>option httpclose</code> is set on the backend. This is to cause HAProxy &#13;
<span class="keep-together">to immediately</span> close connections to the service. Having these connections remain &#13;
<span class="keep-together">open won’t</span> necessarily slow down the service, but it’s required since the &#13;
<span class="keep-together"><code>server.maxConnections</code></span> value is still set to 2 in the application; with the connections left open, the server will drop new connections, even though the callbacks have finished firing with previous requests.</p>&#13;
&#13;
<p>Now, with the new configuration file, go ahead and run the same Node.js service, an instance of HAProxy using the configuration, and again, run multiple copies of the <code>curl</code> requests in parallel:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>node low-connections.js             <code class="c"># terminal 1</code>&#13;
<code class="nv">$ </code>haproxy -f haproxy/backpressure.cfg <code class="c"># terminal 2</code>&#13;
<code class="nv">$ </code>curl http://localhost:3010/         <code class="c"># terminals 3-5</code></pre>&#13;
&#13;
<p>Again, you should see the first two <code>curl</code> commands successfully kicking off a log message on the server. However, this <a data-primary="curl command" data-type="indexterm" id="idm46291190935160"/><a data-primary="commands" data-secondary="curl" data-type="indexterm" id="idm46291190934488"/>time the third <code>curl</code> command doesn’t immediately close. Instead, it’ll wait until one of the previous commands finishes and the connection closes. Once that happens, HAProxy becomes aware that it’s now free to send an additional request along, and the third request is sent through, causing the server to log another message about having two concurrent requests:</p>&#13;
&#13;
<pre data-type="programlisting">current conn 1&#13;
current conn 2&#13;
current conn 2</pre>&#13;
&#13;
<p><em>Back pressure</em> results when a <a data-primary="back pressure" data-type="indexterm" id="idm46291190929576"/><a data-primary="HAProxy" data-secondary="back pressure" data-type="indexterm" id="idm46291190928840"/>consuming service has its requests queued up, like what is now happening here. If the consumer fires requests serially, back pressure created on the producer’s side will cause the consumer to slow down.</p>&#13;
&#13;
<p>Usually it’s fine to only enforce limits within the reverse proxy without having to also enforce limits in the application itself. However, depending on how your architecture is implemented, it could be that sources other than a single HAProxy instance are able to send requests to your services. In those cases it might make sense to set a higher limit within the Node.js process and then set a more conservative limit within the reverse proxy. For example, if you know your service will come to a standstill with 100 concurrent requests, perhaps set <code>server.maxConnections</code> to 90 and set <code>maxconn</code> to 80, adjusting margins depending on how dangerous you’re feeling.</p>&#13;
&#13;
<p>Now that you know how to configure the maximum number of connections, it’s time to look at methods for <a data-primary="rate limiting" data-startref="ratelimit" data-type="indexterm" id="idm46291190922280"/><a data-primary="HAProxy" data-secondary="rate limiting" data-startref="HAPlimit" data-type="indexterm" id="idm46291190921304"/><a data-primary="scaling" data-secondary="back pressure" data-startref="scalback" data-type="indexterm" id="idm46291190920088"/><a data-primary="scaling" data-secondary="rate limiting" data-type="indexterm" id="scalrate1"/>determining how many connections a service can actually handle.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="SLA and Load Testing" data-type="sect1"><div class="sect1" id="ch_scaling_sec_bm">&#13;
<h1>SLA and Load Testing</h1>&#13;
&#13;
<p>Software as a service (SaaS) <a data-primary="SLAs (Service Level Agreements)" data-type="indexterm" id="idm46291190916168"/><a data-primary="load testing" data-type="indexterm" id="idm46291190915496"/><a data-primary="scaling" data-secondary="SLAs" data-type="indexterm" id="scalSLA"/><a data-primary="scaling" data-secondary="load testing" data-type="indexterm" id="scalload"/>companies provide an online service to their users. The expectation of the modern user is that such services are available 24/7. Just imagine how weird it would be if Facebook weren’t available on Fridays from 2 P.M. to 3 P.M. Business-to-business (B2B) companies typically have even stricter requirements, often paired with contractual obligation. When an organization sells access to an API, there are often contractual provisions stating that the organization won’t make backwards-breaking changes without ample notice to upgrade, that a service will &#13;
<span class="keep-together">be available</span> around the clock, and that requests will be served within a specified &#13;
<span class="keep-together">timespan.</span></p>&#13;
&#13;
<p>Such contractual requirements are called a <em>Service Level Agreement (SLA)</em>. Sometimes companies make them available online, such as the <a href="https://oreil.ly/ZYoE5">Amazon Compute Service Level Agreement</a> page. Sometimes they’re negotiated on a per-client basis. Sadly, often they do not exist at all, performance isn’t prioritized, and engineers don’t get to tackle such concerns until a customer complaint ticket arrives.</p>&#13;
&#13;
<p>An SLA may contain more <a data-primary="SLOs (Service Level Objectives)" data-type="indexterm" id="idm46291190887624"/>than one <em>Service Level Objective (SLO)</em>. These are individual promises in the SLA that an organization makes to a customer. They can include things like uptime requirements, API request latency, and failure rates. When it comes to measuring the real values that a service is achieving, those are <a data-primary="SLIs (Service Level Indicators)" data-type="indexterm" id="idm46291190886072"/>called <em>Service Level Indicators (SLI)</em>. I like to think of the SLO as a numerator and the SLI as a denominator. An SLO might be that an API should respond in 100ms, and an SLI might be that the API does respond in 83ms.</p>&#13;
&#13;
<p>This section looks at the importance of determining SLOs, not only for an organization but for individual services as well. It looks at ways to define an SLO and ways to measure a service’s performance by running one-off load tests (sometimes called a benchmark). Later, <a data-type="xref" href="ch04.html#ch_monitoring_sec_metrics">“Metrics with Graphite, StatsD, and Grafana”</a> looks at how to constantly monitor performance.</p>&#13;
&#13;
<p>Before defining what an SLA should look like, you’ll first look at some performance characteristics and how they can be measured. To do this, you’ll load test some of the services you built previously. This should get you familiar with load testing tools and with what sort of throughput to expect in situations without business logic. Once you have that familiarity, measuring your own applications should be <a data-primary="scaling" data-secondary="SLAs" data-startref="scalSLA" data-type="indexterm" id="idm46291190882152"/><a data-primary="scaling" data-secondary="load testing" data-startref="scalload" data-type="indexterm" id="idm46291190880904"/>easier.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Introduction to Autocannon" data-type="sect2"><div class="sect2" id="ch_scaling_sec_bm_subsec_autocannon">&#13;
<h2>Introduction to Autocannon</h2>&#13;
&#13;
<p>These load tests use <em>Autocannon</em>. There are <a data-primary="load testing" data-secondary="Autocannon" data-type="indexterm" id="load_auto"/><a data-primary="Autocannon" data-type="indexterm" id="autocannon"/><a data-primary="scaling" data-secondary="Autocannon" data-type="indexterm" id="idm46291190875000"/>plenty of alternatives, but this one is both easy to install (it’s a one-line npm command) and displays detailed statistics.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Feel free to use whatever load-testing tool you’re most comfortable with. However, never compare the results of one tool with the results from another, as the results for the same service can &#13;
<span class="keep-together">vary greatly.</span> Try to standardize on the same tool throughout your &#13;
<span class="keep-together">organization</span> so that teams can consistently communicate about &#13;
<span class="keep-together">performance.</span></p>&#13;
</div>&#13;
&#13;
<p>Autocannon is available as an npm <a data-primary="npm packages" data-secondary="Autocannon" data-type="indexterm" id="idm46291190869944"/>package and it happens to provide a histogram of request statistics, which is a very important tool when measuring performance. Install it by running the following command (note that you might need to prefix it with <code>sudo</code> if you get permission errors):</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>npm install -g autocannon@6</pre>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46291190864904">&#13;
<h5>Alternatives to Autocannon</h5>&#13;
<p>There are many command line tools for running HTTP load tests. Because Autocannon requires Node.js and npm to be installed, it might be difficult for a polyglot organization to standardize on, since other tools are available as native binaries and can be easier to install.</p>&#13;
&#13;
<p>Some of the more popular <a data-primary="ab (Apache Bench)" data-type="indexterm" id="idm46291190863096"/><a data-primary="wrk, load testing" data-type="indexterm" id="idm46291190862040"/><a data-primary="Siege, load testing" data-type="indexterm" id="idm46291190861368"/>tools include Apache Bench (ab), wrk, and Siege. These are usually available via operating system package manager.</p>&#13;
&#13;
<p>Gil Tene has a <a data-primary="Tene, Gil" data-type="indexterm" id="idm46291190860056"/>presentation, <a href="https://oreil.ly/cbH36">“How NOT to Measure Latency”</a>, in which he discusses common shortcomings of most load-testing tools. His wrk2 tool is an attempt to solve such issues and provides highly accurate load-testing results. Autocannon was inspired by wrk2.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Running a Baseline Load Test" data-type="sect2"><div class="sect2" id="idm46291190858024">&#13;
<h2>Running a Baseline Load Test</h2>&#13;
&#13;
<p>These load tests will mostly <a data-primary="load testing" data-secondary="baseline" data-type="indexterm" id="load_base"/><a data-primary="scaling" data-secondary="baseline load tests" data-type="indexterm" id="scale_base"/>run the applications that you’ve already created in the <em>examples/</em> folder. But first, you’ll get familiar with the Autocannon command and establish a baseline by load testing some very simple services. The first will be a vanilla Node.js HTTP server, and the next will be using a framework. In both, a simple string will be used as the reply.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Be sure to disable any <code>console.log()</code> statements that run <em>within</em> a request handler. Although these statements provide an insignificant amount of delay in a production application doing real work, they significantly slow down many of the load tests in this section.</p>&#13;
</div>&#13;
&#13;
<p>For this first example, create a new directory called <em>benchmark/</em> and create a file within it with the contents from <a data-type="xref" href="#ex_bm_vanilla">Example 3-11</a>. This vanilla HTTP server will function as the most basic of load tests.</p>&#13;
<div data-type="example" id="ex_bm_vanilla">&#13;
<h5><span class="label">Example 3-11. </span><em>benchmark/native-http.js</em></h5>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting"><code class="c-Hashbang">#!/usr/bin/env node</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">HOST</code> <code class="o">=</code> <code class="nx">process</code><code class="p">.</code><code class="nx">env</code><code class="p">.</code><code class="nx">HOST</code> <code class="o">||</code> <code class="s1">'127.0.0.1'</code><code class="p">;</code>&#13;
<code class="kr">const</code> <code class="nx">PORT</code> <code class="o">=</code> <code class="nx">process</code><code class="p">.</code><code class="nx">env</code><code class="p">.</code><code class="nx">PORT</code> <code class="o">||</code> <code class="mi">4000</code><code class="p">;</code>&#13;
&#13;
<code class="nx">require</code><code class="p">(</code><code class="s2">"http"</code><code class="p">).</code><code class="nx">createServer</code><code class="p">((</code><code class="nx">req</code><code class="p">,</code> <code class="nx">res</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="nx">res</code><code class="p">.</code><code class="nx">end</code><code class="p">(</code><code class="s1">'ok'</code><code class="p">);</code>&#13;
<code class="p">}).</code><code class="nx">listen</code><code class="p">(</code><code class="nx">PORT</code><code class="p">,</code> <code class="p">()</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="sb">`Producer running at http://</code><code class="si">${</code><code class="nx">HOST</code><code class="si">}</code><code class="sb">:</code><code class="si">${</code><code class="nx">PORT</code><code class="si">}</code><code class="sb">`</code><code class="p">);</code>&#13;
<code class="p">});</code></pre></div>&#13;
&#13;
<p>Ideally, all of these tests would be run on an unused server with the same capabilities as a production server, but for the sake of learning, running it on your local development laptop is fine. Do keep in mind that the numbers you get locally will not reflect the numbers you would get in production!</p>&#13;
&#13;
<p>Run the service and, in another terminal window, run Autocannon to start the load test:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>node benchmark/native-http.js&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> -l http://localhost:4000/</pre>&#13;
&#13;
<p>This command uses three different flags. The <code>-d</code> flag stands for <em>duration</em>, and in this case it’s configured to run for 60 seconds. The <code>-c</code> flag represents the number of concurrent <em>connections</em>, and here it’s configured to use 10 connections. The <code>-l</code> flag tells Autocannon to display a detailed <em>latency</em> histogram. The URL to be tested is the final argument to the command. In this case Autocannon simply sends <code>GET</code> requests, but it can be configured to make <code>POST</code> requests and provide request bodies.</p>&#13;
&#13;
<p>Tables <a data-type="xref" data-xrefstyle="select:labelnumber" href="#table_bm_latency">3-6</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#table_bm_latency_detailed">3-8</a> contain my results.</p>&#13;
<table id="table_bm_latency">&#13;
<caption><span class="label">Table 3-6. </span>Autocannon request latency</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Stat</th>&#13;
<th>2.5%</th>&#13;
<th>50%</th>&#13;
<th>97.5%</th>&#13;
<th>99%</th>&#13;
<th>Avg</th>&#13;
<th>Stdev</th>&#13;
<th>Max</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Latency</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>0.01ms</p></td>&#13;
<td><p>0.08ms</p></td>&#13;
<td><p>9.45ms</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The first table contains <a data-primary="latency" data-secondary="load testing" data-type="indexterm" id="idm46291190739608"/>information about the latency, or how much time it takes to receive a response after a request has been sent. As you can see, Autocannon groups latency into four buckets. The <em>2.5%</em> bucket represents rather speedy requests, <em>50%</em> is the median, <em>97.5%</em> are the slower results, and <em>99%</em> are some of the slowest, with the <em>Max</em> column representing the slowest request. In this table, lower results are faster. The numbers so far are all so small that a decision can’t yet be made.</p>&#13;
<table id="table_bm_throughput">&#13;
<caption><span class="label">Table 3-7. </span>Autocannon request volume</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Stat</th>&#13;
<th>1%</th>&#13;
<th>2.5%</th>&#13;
<th>50%</th>&#13;
<th>97.5%</th>&#13;
<th>Avg</th>&#13;
<th>Stdev</th>&#13;
<th>Min</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Req/Sec</p></td>&#13;
<td><p>29,487</p></td>&#13;
<td><p>36,703</p></td>&#13;
<td><p>39,039</p></td>&#13;
<td><p>42,751</p></td>&#13;
<td><p>38,884.14</p></td>&#13;
<td><p>1,748.17</p></td>&#13;
<td><p>29,477</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Bytes/Sec</p></td>&#13;
<td><p>3.66 MB</p></td>&#13;
<td><p>4.55 MB</p></td>&#13;
<td><p>4.84 MB</p></td>&#13;
<td><p>5.3 MB</p></td>&#13;
<td><p>4.82 MB</p></td>&#13;
<td><p>217 kB</p></td>&#13;
<td><p>3.66 MB</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The second table provides some different information, namely the requests per second that were sent to the server. In this table, higher numbers are better. The headings in this table correlate to their opposites in the previous table; the <em>1%</em> column correlates to the <em>99%</em> column, for example.</p>&#13;
&#13;
<p>The numbers in this table are much more interesting. What they describe is that, on average, the server is able to handle 38,884 requests per second. But the average isn’t too useful, and is it not a number that engineers should rely on.</p>&#13;
&#13;
<p>Consider that it’s often the case that one request from a user can result in several requests being sent to a given service. For example, if a user opens a web page that lists which ingredients they should stock up on based on their top 10 recipes, that one request might then generate 10 requests to the recipe service. The slowness of the overall user request is then compounded by the slowness of the backend service requests. For this reason, it’s important to pick a higher percentile, like 95% or 99%, when reporting service speed. This is referred to as being the <em>top percentile</em> and is abbreviated as <em>TP95</em> or <em>TP99</em> when communicating throughput.</p>&#13;
&#13;
<p>In the case of these results, one can say the TP99 has a latency of 0ms, or a throughput of 29,487 requests per second.</p>&#13;
&#13;
<p>The third table is the result of providing the <code>-l</code> flag, and contains more granular latency information.</p>&#13;
<div style="page-break-after: always;"/>&#13;
<table id="table_bm_latency_detailed">&#13;
<caption><span class="label">Table 3-8. </span>Autocannon detailed latency results</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Percentile</th>&#13;
<th>Latency</th>&#13;
<th>Percentile</th>&#13;
<th>Latency</th>&#13;
<th>Percentile</th>&#13;
<th>Latency</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>0.001%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>10%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>97.5%</p></td>&#13;
<td><p>0ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>0.01%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>25%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>99%</p></td>&#13;
<td><p>0ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>0.1%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>50%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>99.9%</p></td>&#13;
<td><p>1ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>75%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>99.99%</p></td>&#13;
<td><p>2ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2.5%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>90%</p></td>&#13;
<td><p>0ms</p></td>&#13;
<td><p>99.999%</p></td>&#13;
<td><p>3ms</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The second-to-last row explains that 99.99% of requests (four nines) will get a response within at least 2ms. The final row explains that 99.999% of requests will get a response within 3ms.</p>&#13;
&#13;
<p>This information can then be graphed to better convey what’s going on, as shown in <a data-type="xref" href="#fig_bm_latency_graph">Figure 3-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig_bm_latency_graph">&#13;
<img alt="Autocannon Latency Results Graph" src="assets/dsnj_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>Autocannon latency results graph</h6>&#13;
</div></figure>&#13;
&#13;
<p>Again, with these low numbers, the results aren’t that interesting yet.</p>&#13;
&#13;
<p>Based on my results, I can determine that, assuming TP99, the absolute best throughput I can get from a Node.js service using this specific version of Node.js and this specific hardware is roughly 25,000 r/s (after some conservative rounding). It would then be silly to attempt to achieve anything higher than that value.</p>&#13;
&#13;
<p>As it turns, out 25,000 r/s is actually pretty high, and you’ll very likely never end up in a situation where achieving such a throughput from a single application instance is a requirement. If your use-case does demand higher throughput, you’ll likely need to consider other <a data-primary="load testing" data-secondary="baseline" data-startref="load_base" data-type="indexterm" id="idm46291190628232"/><a data-primary="scaling" data-secondary="baseline load tests" data-type="indexterm" id="scale_base2"/>languages like Rust or C++.</p>&#13;
<div style="page-break-after: always;"/>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space" data-pdf-bookmark="Reverse Proxy Concerns" data-type="sect2"><div class="sect2" id="ch_scaling_sec_bm_subsec_rp">&#13;
<h2>Reverse Proxy Concerns</h2>&#13;
&#13;
<p>Previously I claimed that <a data-primary="load testing" data-secondary="reverse proxies and" data-type="indexterm" id="idm46291190623512"/><a data-primary="reverse proxy" data-secondary="load testing and" data-type="indexterm" id="idm46291190622536"/><a data-primary="scaling" data-secondary="reverse proxies" data-type="indexterm" id="scale_reverse"/>performing certain actions, specifically gzip compression and TLS termination, within a reverse proxy is usually faster than performing them within a running Node.js process. Load tests can be used to see if these claims &#13;
<span class="keep-together">are true.</span></p>&#13;
&#13;
<p>These tests run the client and the server on the same machine. To accurately load test your production application, you’ll need to test in a production setting. The intention here is to measure CPU impact, as the network traffic generated by Node.js and HAProxy should be equivalent.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Establishing a baseline" data-type="sect3"><div class="sect3" id="idm46291190618632">&#13;
<h3>Establishing a baseline</h3>&#13;
&#13;
<p>But first, another baseline needs to <a data-primary="baseline, establishing" data-type="indexterm" id="idm46291190617128"/><a data-primary="load testing" data-secondary="baseline" data-tertiary="establishing" data-type="indexterm" id="idm46291190616424"/>be established, and an inevitable truth must be faced: introducing a reverse proxy must increase latency by at least a little bit. To prove this, use the same <em>benchmark/native-http.js</em> file from before. However, this time you’ll put minimally configured HAProxy in front of it. Create a configuration file with the content from <a data-type="xref" href="#ex_haproxy_benchmark">Example 3-12</a>.</p>&#13;
<div data-type="example" id="ex_haproxy_benchmark">&#13;
<h5><span class="label">Example 3-12. </span><em>haproxy/benchmark-basic.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">defaults&#13;
  mode http&#13;
&#13;
frontend inbound&#13;
  bind localhost:4001&#13;
  default_backend native-http&#13;
&#13;
backend native-http&#13;
  server native-http-1 localhost:4000</pre></div>&#13;
&#13;
<p>Run the service in one terminal window and HAProxy in a second terminal window, and then run the same Autocannon load test in a third terminal window:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>node benchmark/native-http.js&#13;
<code class="nv">$ </code>haproxy -f haproxy/benchmark-basic.cfg&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> -l http://localhost:4001</pre>&#13;
&#13;
<p>The results I get look like those in <a data-type="xref" href="#fig_bm_latency_haproxy_basic">Figure 3-6</a>. The TP99 throughput is 19,967 r/s, a decrease of 32%, and the max request took 28.6ms.</p>&#13;
&#13;
<p>These results may seem high when compared to the previous results, but again, remember that the application isn’t doing much work. The TP99 latency for a request, both before and after adding HAProxy, is still less than 1ms. If a real service takes 100ms to respond, the addition of HAProxy has increased the response time by less than 1%.</p>&#13;
&#13;
<figure><div class="figure" id="fig_bm_latency_haproxy_basic">&#13;
<img alt="HAProxy Latency" src="assets/dsnj_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>HAProxy latency</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="HTTP compression" data-type="sect3"><div class="sect3" id="ch_scaling_sec_bm_subsec_gzip">&#13;
<h3>HTTP compression</h3>&#13;
&#13;
<p>A simple pass-through <a data-primary="HAProxy" data-secondary="compression" data-tertiary="HTTP compression" data-type="indexterm" id="hap_http"/><a data-primary="HTTP (Hypertext Transfer Protocol)" data-secondary="compression" data-type="indexterm" id="httpHAPcomp"/><a data-primary="scaling" data-secondary="HTTP compression" data-type="indexterm" id="scale_comp"/>configuration file is required for the next two tests. This configuration will have HAProxy simply forward requests from the client to the server. The config file has a <code>mode tcp</code> line, which means HAProxy will essentially act as an L4 proxy and not inspect the HTTP requests.</p>&#13;
&#13;
<p>Having HAProxy ensures the benchmarks will test the effects of offloading processing from Node.js to HAProxy, not the effects of an additional network hop. Create an <em>haproxy/passthru.cfg</em> file with the contents from <a data-type="xref" href="#ex_haproxy_passthru">Example 3-13</a>.</p>&#13;
<div data-type="example" id="ex_haproxy_passthru">&#13;
<h5><span class="label">Example 3-13. </span><em>haproxy/passthru.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">defaults&#13;
  mode tcp&#13;
  timeout connect 5000ms&#13;
  timeout client 50000ms&#13;
  timeout server 50000ms&#13;
&#13;
frontend inbound&#13;
  bind localhost:3000&#13;
  default_backend server-api&#13;
&#13;
backend server-api&#13;
  server server-api-1 localhost:3001</pre></div>&#13;
&#13;
<p>Now you can measure the cost of performing gzip compression. Compression versus no compression won’t be compared here. (If that were the goal, the tests would absolutely need to be on separate machines, since the gain is in reduced bandwidth.) Instead, the performance of performing compression in HAProxy versus Node.js is compared.</p>&#13;
&#13;
<p>Use the same <em>server-gzip.js</em> file that was created in <a data-type="xref" href="ch02.html#ex_node_gzip">Example 2-4</a>, though you’ll want to comment out the <code>console.log</code> calls. The same <em>haproxy/compression.cfg</em> file created in <a data-type="xref" href="#ex_haproxy_compression">Example 3-6</a> will also be used, as well as the <em>haproxy/passthru.cfg</em> file you just created from <a data-type="xref" href="#ex_haproxy_passthru">Example 3-13</a>. For this test, you’ll need to stop HAProxy and restart it with a different configuration file:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ </code>rm index.html <code class="p">;</code> curl -o index.html https://thomashunter.name&#13;
<code class="nv">$ PORT</code><code class="o">=</code><code class="m">3001</code> node server-gzip.js&#13;
<code class="nv">$ </code>haproxy -f haproxy/passthru.cfg&#13;
<code class="nv">$ </code>autocannon -H <code class="s2">"Accept-Encoding: gzip"</code> <code class="se">\</code>&#13;
  -d <code class="m">60</code> -c <code class="m">10</code> -l http://localhost:3000/ <code class="c"># Node.js</code>&#13;
<code class="c"># Kill the previous haproxy process</code>&#13;
<code class="nv">$ </code>haproxy -f haproxy/compression.cfg&#13;
<code class="nv">$ </code>autocannon -H <code class="s2">"Accept-Encoding: gzip"</code> <code class="se">\</code>&#13;
  -d <code class="m">60</code> -c <code class="m">10</code> -l http://localhost:3000/ <code class="c"># HAProxy</code></pre>&#13;
&#13;
<p>Here are the results when I ran the tests on my machine. <a data-type="xref" href="#fig_bm_latency_gzip_node">Figure 3-7</a> shows the results of running gzip with Node.js, and <a data-type="xref" href="#fig_bm_latency_gzip_haproxy">Figure 3-8</a> contains the results for HAProxy.</p>&#13;
&#13;
<figure><div class="figure" id="fig_bm_latency_gzip_node">&#13;
<img alt="Node.js gzip Compression Latency" src="assets/dsnj_0307.png"/>&#13;
<h6><span class="label">Figure 3-7. </span>Node.js gzip compression latency</h6>&#13;
</div></figure>&#13;
&#13;
<p>This test shows that requests are <a data-primary="HAProxy" data-secondary="compression" data-startref="hap_http" data-tertiary="HTTP compression" data-type="indexterm" id="idm46291190542360"/><a data-primary="HTTP (Hypertext Transfer Protocol)" data-secondary="compression" data-startref="httpHAPcomp" data-type="indexterm" id="idm46291190540840"/>served a bit faster using HAProxy for performing gzip compression <a data-primary="scaling" data-secondary="HTTP compression" data-startref="scale_comp" data-type="indexterm" id="idm46291190539432"/>than when using Node.js.</p>&#13;
&#13;
<figure><div class="figure" id="fig_bm_latency_gzip_haproxy">&#13;
<img alt="HAProxy gzip Compression Latency" src="assets/dsnj_0308.png"/>&#13;
<h6><span class="label">Figure 3-8. </span>HAProxy gzip compression latency</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="TLS termination" data-type="sect3"><div class="sect3" id="ch_scaling_sec_bm_subsec_tls">&#13;
<h3>TLS termination</h3>&#13;
&#13;
<p>TLS absolutely has a <a data-primary="HAProxy" data-secondary="TLS termination" data-type="indexterm" id="hapTLSterm"/><a data-primary="TLS (Transport Layer Security)" data-secondary="termination, HAProxy" data-type="indexterm" id="tlstermHAP"/><a data-primary="scaling" data-secondary="TLS termination" data-type="indexterm" id="scale_TLS"/>negative impact on application performance<sup><a data-type="noteref" href="ch03.html#idm46291190530232" id="idm46291190530232-marker">5</a></sup> (in an HTTP versus HTTPS sense). These tests just compare the performance impact of performing TLS termination within HAProxy instead of Node.js, not HTTP compared to HTTPS. The throughput numbers have been reproduced in the following since the tests run so fast that the latency listing graphs mostly contains zeros.</p>&#13;
&#13;
<p>First, performing TLS termination within the Node.js process is tested. For this test use the same <em>recipe-api/producer-https-basic.js</em> file that you created in <a data-type="xref" href="ch02.html#ex_node_server_https">Example 2-7</a>, commenting out any <code>console.log</code> statements from the request handler:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ PORT</code><code class="o">=</code><code class="m">3001</code> node recipe-api/producer-https-basic.js&#13;
<code class="nv">$ </code>haproxy -f haproxy/passthru.cfg&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> https://localhost:3000/recipes/42</pre>&#13;
&#13;
<p><a data-type="xref" href="#table_bm_tls_node">Table 3-9</a> contains the results of running this load test on my machine.</p>&#13;
<table id="table_bm_tls_node">&#13;
<caption><span class="label">Table 3-9. </span>Native Node.js TLS termination throughput</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Stat</th>&#13;
<th>1%</th>&#13;
<th>2.5%</th>&#13;
<th>50%</th>&#13;
<th>97.5%</th>&#13;
<th>Avg</th>&#13;
<th>Stdev</th>&#13;
<th>Min</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Req/Sec</p></td>&#13;
<td><p>7,263</p></td>&#13;
<td><p>11,991</p></td>&#13;
<td><p>13,231</p></td>&#13;
<td><p>18,655</p></td>&#13;
<td><p>13,580.7</p></td>&#13;
<td><p>1,833.58</p></td>&#13;
<td><p>7,263</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Bytes/Sec</p></td>&#13;
<td><p>2.75 MB</p></td>&#13;
<td><p>4.53 MB</p></td>&#13;
<td><p>5 MB</p></td>&#13;
<td><p>7.05 MB</p></td>&#13;
<td><p>5.13 MB</p></td>&#13;
<td><p>693 kB</p></td>&#13;
<td><p>2.75 MB</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<div style="page-break-after: always;"/>&#13;
&#13;
<p>Next, to test HAProxy, make use of the <em>recipe-api/producer-http-basic.js</em> file created back in <a data-type="xref" href="ch01.html#ex_producer">Example 1-6</a> (again, comment out the <code>console.log</code> calls), as well as the &#13;
<span class="keep-together"><em>haproxy/tls.cfg</em></span> file from <a data-type="xref" href="#ex_haproxy_tls">Example 3-8</a>:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ PORT</code><code class="o">=</code><code class="m">3001</code> node recipe-api/producer-http-basic.js&#13;
<code class="nv">$ </code>haproxy -f haproxy/tls.cfg&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> https://localhost:3000/recipes/42</pre>&#13;
&#13;
<p><a data-type="xref" href="#table_bm_tls_haproxy">Table 3-10</a> contains the results of running this load test on my machine.</p>&#13;
<table id="table_bm_tls_haproxy">&#13;
<caption><span class="label">Table 3-10. </span>HAProxy TLS termination throughput</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Stat</th>&#13;
<th>1%</th>&#13;
<th>2.5%</th>&#13;
<th>50%</th>&#13;
<th>97.5%</th>&#13;
<th>Avg</th>&#13;
<th>Stdev</th>&#13;
<th>Min</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Req/Sec</p></td>&#13;
<td><p>960</p></td>&#13;
<td><p>1,108</p></td>&#13;
<td><p>1,207</p></td>&#13;
<td><p>1,269</p></td>&#13;
<td><p>1,202.32</p></td>&#13;
<td><p>41.29</p></td>&#13;
<td><p>960</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Bytes/Sec</p></td>&#13;
<td><p>216 kB</p></td>&#13;
<td><p>249 kB</p></td>&#13;
<td><p>272 kB</p></td>&#13;
<td><p>286 kB</p></td>&#13;
<td><p>271 kB</p></td>&#13;
<td><p>9.29 kB</p></td>&#13;
<td><p>216 kB</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>In this case, a massive penalty happens when having HAProxy perform the TLS termination instead of Node.js! However, take this with a grain of salt. The JSON payload being used so far is about 200 bytes long. With a larger payload, like those in excess of 20kb, HAProxy usually outperforms Node.js when doing TLS termination.</p>&#13;
&#13;
<p>As with all benchmarks, it’s important to test your application in your environment. The services used in this book are quite simple; a “real” application, doing CPU-intensive work like template rendering, and sending documents with varying payload sizes will behave completely <a data-primary="HAProxy" data-secondary="TLS termination" data-startref="hapTLSterm" data-type="indexterm" id="idm46291190420488"/><a data-primary="TLS (Transport Layer Security)" data-secondary="termination, HAProxy" data-startref="tlstermHAP" data-type="indexterm" id="idm46291190419240"/><a data-primary="scaling" data-secondary="TLS termination" data-startref="scale_TLS" data-type="indexterm" id="idm46291190418056"/>differently.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Protocol Concerns" data-type="sect2"><div class="sect2" id="ch_scaling_sec_bm_subsec_protocol">&#13;
<h2>Protocol Concerns</h2>&#13;
&#13;
<p>Now you’ll load test some of the <a data-primary="protocols" data-type="indexterm" id="idm46291190393928"/><a data-primary="scaling" data-secondary="protocols" data-type="indexterm" id="scale_proto"/>previously covered protocols, namely JSON over HTTP, GraphQL, and gRPC. Since these approaches do change the payload contents, measuring their transmission over a network will be more important than in <a data-type="xref" href="#ch_scaling_sec_bm_subsec_rp">“Reverse Proxy Concerns”</a>. Also, recall that protocols like gRPC are more likely to be used for cross-service traffic than for external traffic. For that reason, &#13;
<span class="keep-together">I’ll run</span> these load tests on two different machines within the same cloud provider &#13;
<span class="keep-together">data center.</span></p>&#13;
&#13;
<p>For these tests, your approach is going to be to cheat a little bit. Ideally, you’d build a client from scratch, one that would natively speak the protocol being tested and would measure the throughput. But since you already built the <em>web-api</em> clients that accept HTTP requests, you’ll simply point Autocannon at those so that you don’t need to build three new applications. This is visualized in <a data-type="xref" href="#fig_benchmark_cloud">Figure 3-9</a>.</p>&#13;
&#13;
<p>Since there’s an additional network hop, this approach can’t accurately measure performance, like X is Y% faster than Z, but it can rank their performance—as implemented in Node.js using these particular libraries—from fastest to slowest.</p>&#13;
&#13;
<figure><div class="figure" id="fig_benchmark_cloud">&#13;
<img alt="Autocannon and web-api run on one VPS, while recipe-api runs on the other" src="assets/dsnj_0309.png"/>&#13;
<h6><span class="label">Figure 3-9. </span>Benchmarking in the cloud</h6>&#13;
</div></figure>&#13;
&#13;
<p>If you have access to a cloud provider and a few dollars to spare, feel free to spin up two new VPS instances and copy the <em>examples/</em> directory that you have so far to them. You should use machines with at least two CPU cores. This is particularly important on the client where Autocannon <a data-primary="web-api service" data-secondary="Autocannon and" data-type="indexterm" id="idm46291190384040"/>and <em>web-api</em> might compete for CPU access with a single core. Otherwise, you can also run the examples on your development machine, at which point you can omit the <code>TARGET</code> environment variable.</p>&#13;
&#13;
<p>Be sure to replace <code>&lt;RECIPE_API_IP&gt;</code> with the IP address or hostname of the <em>recipe-api</em> service in each of the following examples.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JSON over HTTP benchmarks" data-type="sect3"><div class="sect3" id="ch_scaling_sec_bm_subsec_protocol_json">&#13;
<h3>JSON over HTTP benchmarks</h3>&#13;
&#13;
<p>This first load test will <a data-primary="protocols" data-secondary="JSON over HTTP" data-tertiary="benchmarks" data-type="indexterm" id="idm46291190378328"/><a data-primary="JSON" data-secondary="over HTTP" data-tertiary="benchmarks" data-type="indexterm" id="idm46291190377080"/><a data-primary="benchmarking" data-secondary="JSON over HTTP" data-type="indexterm" id="idm46291190375864"/>benchmark the <em>recipe-api/producer-http-basic.js</em> service created in <a data-type="xref" href="ch01.html#ex_producer">Example 1-6</a> by sending requests through the <em>web-api/consumer-http-basic.js</em> service created in <a data-type="xref" href="ch01.html#ex_consumer">Example 1-7</a>:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="c"># Server VPS</code>&#13;
<code class="nv">$ HOST</code><code class="o">=</code>0.0.0.0 node recipe-api/producer-http-basic.js&#13;
<code class="c"># Client VPS</code>&#13;
<code class="nv">$ TARGET</code><code class="o">=</code>&lt;RECIPE_API_IP&gt;:4000 node web-api/consumer-http-basic.js&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> -l http://localhost:3000</pre>&#13;
&#13;
<p>My results for this benchmark appear in <a data-type="xref" href="#fig_benchmark_jsonhttp">Figure 3-10</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig_benchmark_jsonhttp">&#13;
<img alt="Histogram of JSON over HTTP Results" src="assets/dsnj_0310.png"/>&#13;
<h6><span class="label">Figure 3-10. </span>Benchmarking JSON over HTTP</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GraphQL benchmarks" data-type="sect3"><div class="sect3" id="ch_scaling_sec_bm_subsec_protocol_graphql">&#13;
<h3>GraphQL benchmarks</h3>&#13;
&#13;
<p>This next load test will <a data-primary="protocols" data-secondary="GraphQL" data-tertiary="benchmarks" data-type="indexterm" id="idm46291190352664"/><a data-primary="GraphQL" data-secondary="benchmarks" data-type="indexterm" id="idm46291190351416"/><a data-primary="benchmarking" data-secondary="GraphQL" data-type="indexterm" id="idm46291190350472"/>use the <em>recipe-api/producer-graphql.js</em> service created in <a data-type="xref" href="ch02.html#ex_graphql_producer">Example 2-11</a> by sending requests through the <em>web-api/consumer-graphql.js</em> service created in <a data-type="xref" href="ch02.html#ex_graphql_consumer">Example 2-12</a>:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="c"># Server VPS</code>&#13;
<code class="nv">$ HOST</code><code class="o">=</code>0.0.0.0 node recipe-api/producer-graphql.js&#13;
<code class="c"># Client VPS</code>&#13;
<code class="nv">$ TARGET</code><code class="o">=</code>&lt;RECIPE_API_IP&gt;:4000 node web-api/consumer-graphql.js&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> -l http://localhost:3000</pre>&#13;
&#13;
<p>My results for this load test appear in <a data-type="xref" href="#fig_benchmark_graphql">Figure 3-11</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig_benchmark_graphql">&#13;
<img alt="Histogram of GraphQL Results" src="assets/dsnj_0311.png"/>&#13;
<h6><span class="label">Figure 3-11. </span>Benchmarking GraphQL</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="gRPC benchmarks" data-type="sect3"><div class="sect3" id="ch_scaling_sec_bm_subsec_protocol_grpc">&#13;
<h3>gRPC benchmarks</h3>&#13;
&#13;
<p>This final load test will <a data-primary="protocols" data-secondary="gRPC" data-type="indexterm" id="idm46291190336904"/><a data-primary="gRPC" data-secondary="benchmarks" data-type="indexterm" id="idm46291190335928"/><a data-primary="benchmarking" data-secondary="gRPC" data-type="indexterm" id="idm46291190308152"/>test the <em>recipe-api/producer-grpc.js</em> service created in <a data-type="xref" href="ch02.html#ex_grpc_producer">Example 2-14</a> by sending requests through the <em>web-api/consumer-grpc.js</em> service &#13;
<span class="keep-together">created</span> in <a data-type="xref" href="ch02.html#ex_grpc_consumer">Example 2-15</a>:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="c"># Server VPS</code>&#13;
<code class="nv">$ HOST</code><code class="o">=</code>0.0.0.0 node recipe-api/producer-grpc.js&#13;
<code class="c"># Client VPS</code>&#13;
<code class="nv">$ TARGET</code><code class="o">=</code>&lt;RECIPE_API_IP&gt;:4000 node web-api/consumer-grpc.js&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> -l http://localhost:3000</pre>&#13;
&#13;
<p>My results for this load test appear in <a data-type="xref" href="#fig_benchmark_grpc">Figure 3-12</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig_benchmark_grpc">&#13;
<img alt="Histogram of gRPC Results" src="assets/dsnj_0312.png"/>&#13;
<h6><span class="label">Figure 3-12. </span>Benchmarking gRPC</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect3"><div class="sect3" id="idm46291190295080">&#13;
<h3>Conclusion</h3>&#13;
&#13;
<p>According to these results, JSON over HTTP is typically the fastest, with GraphQL being the second fastest and gRPC being the third fastest. Again, these results will change for real-world applications, especially when dealing with more complex payloads or when servers are farther apart.</p>&#13;
&#13;
<p>The reason for this is that <code>JSON.stringify()</code> is <a data-primary="JSON.stringify() method" data-type="indexterm" id="idm46291190292584"/><a data-primary="methods" data-secondary="JSON.stringify()" data-type="indexterm" id="idm46291190266200"/>extremely optimized in V8, so any other serializer is going to have a hard time keeping up. GraphQL has its own parser for parsing query strings, which will add some additional latency versus a query represented purely using JSON. gRPC needs to do a bunch of <code>Buffer</code> work to serialize and deserialize objects into binary. This means gRPC should be faster in more static, compiled <a data-primary="scaling" data-secondary="protocols" data-startref="scale_proto" data-type="indexterm" id="idm46291190264344"/>languages like C++ than in JavaScript.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Coming Up with SLOs" data-type="sect2"><div class="sect2" id="idm46291190395144">&#13;
<h2>Coming Up with SLOs</h2>&#13;
&#13;
<p>An SLO can cover many different aspects of a service. Some of these are business-related requirements, like the service will never double charge a customer for a single purchase. Other more generic SLOs are the topic of this section, like the service will have a TP99 latency of 200ms and will have an uptime of 99.9%.</p>&#13;
&#13;
<p>Coming up with an SLO for latency <a data-primary="SLOs (Service Level Objectives)" data-secondary="latancy" data-type="indexterm" id="SLOlate"/><a data-primary="latency" data-secondary="SLOs" data-type="indexterm" id="lateSLOs"/><a data-primary="scaling" data-secondary="SLOs" data-type="indexterm" id="scale_SLO"/>can be tricky. For one thing, the time it will take for your application to serve a response might depend on the time it takes an upstream service to return its response. If you’re adopting the concept of an SLO for the first time, you’ll need upstream services to <em>also</em> come up with SLOs of their own. Otherwise, when their service latency jumps from 20ms to 40ms, who’s to know if they’re actually doing something wrong?</p>&#13;
&#13;
<p>Another thing to keep in mind is that your service will very likely receive more traffic during certain times of the day and certain days of the week, especially if traffic is governed by the interactions of people. For example, a backend service used by an online retailer will get more traffic on Mondays, in the evenings, and near holidays, whereas a service receiving periodic sensor data will always handle data at the same rate. Whatever SLOs you do decide on will need to hold true during times of peak traffic.</p>&#13;
&#13;
<p>Something that can make measuring performance difficult is the concept of the <em>noisy neighbor</em>. This is a <a data-primary="noisy neighbor" data-type="indexterm" id="idm46291190253672"/>problem that occurs when a service is running on a machine with other services and those other services end up consuming too many resources, such as CPU or bandwidth. This can cause your service to take more time to respond.</p>&#13;
&#13;
<p>When first starting with an SLO, it’s useful to perform a load test on your service as a starting point. For example, <a data-type="xref" href="#fig_benchmark_radar">Figure 3-13</a> is the result of benchmarking a production application that I built. With this service, the TP99 has a latency of 57ms. To get it any faster would require performance work.</p>&#13;
&#13;
<p>Be sure to completely mimic production situations when load testing your service. For example, if a real consumer makes a request through a reverse proxy, then make sure your load tests also go through the same reverse proxy, instead of connecting directly to the service.</p>&#13;
&#13;
<figure><div class="figure" id="fig_benchmark_radar">&#13;
<img alt="Benchmark of a Production Application" src="assets/dsnj_0313.png"/>&#13;
<h6><span class="label">Figure 3-13. </span>Benchmarking a production application</h6>&#13;
</div></figure>&#13;
&#13;
<p>Another thing to consider is what the consumers of your service are expecting. For example, if your service provides suggestions for an autocomplete form when a user types a query, having a response time of less than 100ms is vital. On the other hand, if your service triggers the creation of a bank loan, having a response time of 60s might also be acceptable.</p>&#13;
&#13;
<p>If a downstream service has a hard response time requirement and you’re not currently satisfying it, you’ll have to find a way to make your service more performant. You can try throwing more servers at the problem, but often you’ll need to get into the code and make things faster. Consider adding a performance test when code is being considered for merging. <a data-type="xref" href="ch06.html#ch_deployments_sec_testing">“Automated Testing”</a> discusses automated tests in further detail.</p>&#13;
&#13;
<p>When you do determine a latency SLO, you’ll want to determine how many service instances to run. For example, you might have an SLO where the TP99 response time is 100ms. Perhaps a single server is able to perform at this level when handling 500 requests per minute. However, when the traffic increases to 1,000 requests per minute, the TP99 drops to 150ms. In this situation, you’ll need to add a second service. Experiment with adding more services, and testing load at different rates, to understand how many services it takes to&#13;
increase your traffic by two, three, or even ten times the amount.</p>&#13;
&#13;
<p>Autocannon has the <code>-R</code> flag for <a data-primary="Autocannon" data-secondary="requsts, exact number" data-type="indexterm" id="idm46291190227384"/>specifying an exact number of requests per second. Use this to throw an exact rate of requests at your service. Once you do that, you can measure your application at different request rates and find out where it stops performing at the intended latency. Once that happens, add another service instance and test again. Using this method, you’ll know how many service instances are needed in order to satisfy the TP99 SLO based on different overall throughputs.</p>&#13;
&#13;
<p>Using the <em>cluster-fibonacci.js</em> application created in <a data-type="xref" href="#ex_fibonacci">Example 3-2</a> as a guide, you’ll now attempt to measure just this. This application, with a Fibonacci limit of 10,000, is an attempt to simulate a real service. The TP99 value you’ll want to maintain is 20ms. Create another HAProxy configuration file <em>haproxy/fibonacci.cfg</em> based on the content in <a data-type="xref" href="#ex_haproxy_fibonacci">Example 3-14</a>. You’ll iterate on this file as you add new service instances.</p>&#13;
<div data-type="example" id="ex_haproxy_fibonacci">&#13;
<h5><span class="label">Example 3-14. </span><em>haproxy/fibonacci.cfg</em></h5>&#13;
&#13;
<pre data-type="programlisting">defaults&#13;
  mode http&#13;
&#13;
frontend inbound&#13;
  bind localhost:5000&#13;
  default_backend fibonacci&#13;
&#13;
backend fibonacci&#13;
  server fibonacci-1 localhost:5001&#13;
# server fibonacci-2 localhost:5002&#13;
# server fibonacci-3 localhost:5003</pre></div>&#13;
&#13;
<p>This application is a little too CPU heavy. Add a sleep statement to simulate a slow database connection, which should keep the event loop a little busier.&#13;
Introduce a <code>sleep()</code> function <a data-primary="sleep() function" data-type="indexterm" id="idm46291190219272"/><a data-primary="functions" data-secondary="sleep()" data-type="indexterm" id="idm46291190218536"/>like this one, causing requests to take at least 10ms longer:</p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting"><code class="c1">// Add this line inside the server.get async handler</code>&#13;
<code class="nx">await</code> <code class="nx">sleep</code><code class="p">(</code><code class="mi">10</code><code class="p">);</code>&#13;
&#13;
<code class="c1">// Add this function to the end of the file</code>&#13;
<code class="kd">function</code> <code class="nx">sleep</code><code class="p">(</code><code class="nx">ms</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="k">return</code> <code class="k">new</code> <code class="nb">Promise</code><code class="p">(</code><code class="nx">resolve</code> <code class="o">=&gt;</code> <code class="nx">setTimeout</code><code class="p">(</code><code class="nx">resolve</code><code class="p">,</code> <code class="nx">ms</code><code class="p">));</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>Next, run a single instance of <em>cluster-fibonacci.js</em>, as well as HAProxy, using the following commands:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nv">$ PORT</code><code class="o">=</code><code class="m">5001</code> node cluster-fibonacci.js <code class="c"># later run with 5002 &amp; 5003</code>&#13;
<code class="nv">$ </code>haproxy -f haproxy/fibonacci.cfg&#13;
<code class="nv">$ </code>autocannon -d <code class="m">60</code> -c <code class="m">10</code> -R <code class="m">10</code> http://localhost:5000/10000</pre>&#13;
&#13;
<p>My TP99 value is 18ms, which is below the 20ms SLO, so I know that one instance can handle traffic of at least 10 r/s. So, now double that value! Run the Autocannon command again by setting the <code>-R</code> flag to 20. On my machine the value is now 24ms, which is too high. Of course, your results will be different. Keep tweaking the requests per second value until you reach the 20ms TP99 SLO threshold. At this point you’ve discovered how many requests per second a single instance of your service can handle! Write that number down.</p>&#13;
&#13;
<p>Next, uncomment the second-to-last line of the <em>haproxy/fibonacci.cfg</em> file. Also, run another instance of <em>cluster-fibonacci.js</em>, setting the <code>PORT</code> value to <code>5002</code>. Restart HAProxy to reload the modified config file. Then, run the Autocannon command again with increased traffic. Increase the requests per second until you reach the threshold again, and write down the value. Do it a third and final time. <a data-type="xref" href="#table_fibonacci_sla">Table 3-11</a> contains my results.</p>&#13;
<table id="table_fibonacci_sla">&#13;
<caption><span class="label">Table 3-11. </span>Fibonacci SLO</caption>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Instance count</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>2</p></td>&#13;
<td><p>3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Max r/s</p></td>&#13;
<td><p>12</p></td>&#13;
<td><p>23</p></td>&#13;
<td><p>32</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>With this information I can deduce that if my service needs to run with 10 requests per second, then a single instance will allow me to honor my 20ms SLO for my consumers. If, however, the holiday season is coming and I know consumers are going to want to calculate the 5,000th Fibonacci sequence at a rate of 25 requests per second, then I’m going to need to run three instances.</p>&#13;
&#13;
<p>If you work in an organization that doesn’t currently make any performance promises, I encourage you to measure your service’s performance and come up with an SLO using current performance as a starting point. Add that SLO to your project’s <em>README</em> and strive to improve it each quarter.</p>&#13;
&#13;
<p>Benchmark results are useful for <a data-primary="SLOs (Service Level Objectives)" data-secondary="latancy" data-startref="SLOlate" data-type="indexterm" id="idm46291190170936"/><a data-primary="latency" data-secondary="SLOs" data-startref="lateSLOs" data-type="indexterm" id="idm46291190169624"/><a data-primary="scaling" data-secondary="SLOs" data-startref="scale_SLO" data-type="indexterm" id="idm46291190168408"/>coming up with initial SLO values. To know whether or not your application actually achieves an SLO in production requires observing real production SLIs. The next chapter covers application observability, which can be used to measure SLIs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm46291192440536"><sup><a href="ch03.html#idm46291192440536-marker">1</a></sup> The <code>fork()</code> method name is inspired by the fork system call, though the two are technically unrelated.</p><p data-type="footnote" id="idm46291192181256"><sup><a href="ch03.html#idm46291192181256-marker">2</a></sup> More advanced applications might have some race-conditions unearthed when running multiple copies.</p><p data-type="footnote" id="idm46291191422376"><sup><a href="ch03.html#idm46291191422376-marker">3</a></sup> This backend has a <code>balance &lt;algorithm&gt;</code> directive implicitely set to <code>roundrobin</code>. It can be set to <code>leastconn</code> to route requests to the instance with the fewest connections, <code>source</code> to consistently route a client by IP to an instance, and several other algorithm options are also available.</p><p data-type="footnote" id="idm46291191395832"><sup><a href="ch03.html#idm46291191395832-marker">4</a></sup> You’ll need to manually refresh it any time you want to see updated statistics; the page only displays a static snapshot.</p><p data-type="footnote" id="idm46291190530232"><sup><a href="ch03.html#idm46291190530232-marker">5</a></sup> Regardless of performance, it’s necessary that services exposed to the internet are encrypted.</p></div></div></section></body></html>