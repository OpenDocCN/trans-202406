<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Observability"><div class="chapter" id="observability-id000002">
<h1><span class="label">Chapter 5. </span>Observability</h1>
<p>In this chapter we will discuss the difference between monitoring and observability in the context of Kubernetes deployments. We will explain best practices and tools for implementing observability in your Kubernetes cluster. In the next chapter we will cover how you can use observability to secure your cluster.</p>
<p>Observability has been a topic of discussion recently in the Kubernetes community and has garnered a lot of interest. We begin by understanding the difference between monitoring and observability. We then look at why observability is critical to security in a distributed application like Kubernetes, and review tools and reference implementations for observability. While observability is a broad topic and applies to several areas, we will keep the discussion focused on Kubernetes in this chapter. Let’s start by looking at monitoring and observability and how they are different.</p>
<section data-type="sect1" data-pdf-bookmark="Monitoring"><div class="sect1" id="monitoring">
<h1>Monitoring</h1>
<p>Monitoring is a known set of measurements in a system that are used<a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="about" id="idm45326831415360"/> to alert for deviations from a normal range. The following are examples of types of data you can monitor in Kubernetes:</p>
<ul>
<li><p>Pod logs</p></li>
<li><p>Network flow logs</p></li>
<li><p>Application flow logs</p></li>
<li><p>Audit logs</p></li>
</ul>
<p class="pagebreak-before">Examples of metrics you can monitor include the following:</p>
<ul>
<li><p>Connections per second</p></li>
<li><p>Packets per second, bytes per second</p></li>
<li><p>Application (API) requests per second</p></li>
<li><p>CPU and memory utilization</p></li>
</ul>
<p>These logs and metrics can help you identify known failures and provide more information about the symptom to help you remediate the issue.</p>
<p>In order to monitor your Kubernetes cluster, you use techniques like polling and uptime checks depending on the SLAs you need to maintain for their cluster. The following are examples of metrics you could monitor for SLAs:</p>
<ul>
<li><p>Polling of application/API endpoints</p></li>
<li><p>Application response codes (e.g., HTTP or database error codes)</p></li>
<li><p>Application response time (e.g., HTTP duration, database transaction time)</p></li>
<li><p>Node availability for scale-out use cases</p></li>
<li><p>Memory/CPU/disk/IO resources on a node</p></li>
</ul>
<p>The other important part of monitoring is alerting. <a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="alerting" data-tertiary="tools for" id="idm45326831402640"/><a contenteditable="false" data-type="indexterm" data-primary="alerting" data-secondary="tools for" id="idm45326831400992"/>You need an alerting system as part of your monitoring solution that generates alerts for any metric that violates the specified threshold. Tools like Grafana, Prometheus, OpenMetrics, OpenTelemetry, and Fluentd are used as monitoring tools to collect logs and metrics, and generate reports, dashboards, and alerts for Kubernetes clusters. Kubernetes offers several integrations to tools like Opsgenie, PagerDuty, Slack, and JIRA for alert forwarding and management.</p>
<p>Monitoring your production Kubernetes cluster has the following issues:<a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="challenges of" id="idm45326831398592"/></p>
<dl>
<dt>Amount of log data</dt>
<dd>In a system like Kubernetes, a node has several pods that run on the host,<a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="data challenges" id="idm45326831395808"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="logging challenges" id="idm45326831394432"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="monitoring" id="idm45326831393056"/><a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="monitoring" id="idm45326831391680"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="monitoring" data-see="monitoring" id="idm45326831390304"/> and each pod comes with its own logs, its own network identity, and its own resources. This means you have logs from the application operation, network flow logs, Kubernetes activity (audit) logs, and application flow logs for each pod. In a non-Kubernetes environment, you typically had an application running on a node and so it would be just one set of logs as opposed to one set of logs per pod running on the node. This multiplies the amount of log data that needs to be collected/inspected. In addition to the per-pod logs, you also need to collect cluster logs from Kubernetes. Typically these are also known as audit logs that provide visibility into Kubernetes cluster activity. The number of logs in the system will make monitoring very resource-intensive and expensive to maintain. Your log collection cluster should not be more expensive to operate than the cluster running your applications!</dd>
<dt>Monitoring distributed applications</dt>
<dd>In a Kubernetes cluster, applications are distributed across the Kubernetes<a contenteditable="false" data-type="indexterm" data-primary="distributed application monitoring" id="idm45326831386768"/> cluster network. An application that needs more than one pod (e.g., a deployment set or a service) will have logs for each pod that need to be examined in addition to  the context of the set of pods (e.g., scale out, error handling, etc.). We have multiple pods that need to be considered as a group before we generate an alert for the application. Please note the goal is to monitor the application and generate alerts for the application, and generating alerts for pods that are a part of the application independently does not provide an accurate representation of the state of the application. <a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="monitoring" data-tertiary="microservices" id="idm45326831384912"/><a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="monitoring" id="idm45326831383264"/>There is also the case of the microservices application, where a single application is deployed as a set of services known as <em>microservices</em>, and each microservice is responsible for a part of the functionality of the application. In this case, you need to monitor each microservice as an entity (note a microservice is a set of one or more pods) and then understand which microservices impact any given application transaction. Only then can you report an alert for the <span class="keep-together">application.</span></dd>
<dt>Declarative nature of Kubernetes</dt>
<dd>As we have covered, Kubernetes is declarative and allows you to specify exactly how you want pods to be created and run in the cluster. Kubernetes allows you to specify resource limits for memory, CPU, storage, etc., and you can also create custom resources and specify limits for these resources. The scheduler will find a node that has the required resources and schedule a pod on the node. Kubernetes also monitors usage for pods and will terminate pods that consume more resources than those allocated to them. In addition, <a contenteditable="false" data-type="indexterm" data-primary="Prometheus" id="idm45326831378688"/><a contenteditable="false" data-type="indexterm" data-primary="Horizontal Pod Autoscaler" id="idm45326831377584"/><a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="Prometheus and Horizontal Pod Autoscaler" id="idm45326831376464"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="declarative nature" data-tertiary="monitoring and" id="idm45326831375056"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="Prometheus" id="idm45326831373408"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="Prometheus" id="idm45326831371744"/>Kubernetes provides detailed metrics that can be used to monitor pods and cluster state. For example, you can use a tool like <a href="https://oreil.ly/zzjVG">Prometheus</a> that can monitor pods and cluster state and use the metrics, and you can automatically scale pods or other cluster resources with a <a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="documentation online" data-tertiary="Horizontal Pod Autoscaler" id="idm45326831368992"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="Kubernetes documentation" data-tertiary="Horizontal Pod Autoscaler" id="idm45326831367328"/>mechanism known as the <a href="https://oreil.ly/luM5u">Horizontal Pod Autoscaler</a>. What this means is that Kubernetes as a part of its operation is monitoring and making changes to the cluster to maintain operations as per the configured specification. In this scenario, an alert from monitoring a single metric can be a result of Kubernetes making changes to adapt to the load in the cluster, or it could be a real issue. You need to be able to distinguish between the two scenarios to be able to accurately monitor your application.</dd>
</dl>
<p>Now that we understand monitoring and how it can be implemented and the challenges with using monitoring for a Kubernetes cluster, let’s look at observability and how it can help overcome these challenges.</p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Observability"><div class="sect1" id="observability-id000003">
<h1>Observability</h1>
<p>Observability is defined as the ability to understand the<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="about" id="idm45326831362048"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="about" data-tertiary="Observability Engineering book" id="idm45326831360672"/><a contenteditable="false" data-type="indexterm" data-primary="Observability Engineering (Majors et al.)" id="idm45326831359056"/><a contenteditable="false" data-type="indexterm" data-primary="Majors, Charity" id="idm45326831357984"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="Observability Engineering book link" id="idm45326831356880"/> internal state of a system by only looking at external outputs of the system. <a class="orm:hideurl" href="https://oreil.ly/3hPEr"><em>Observability Engineering</em> by Charity Majors et al. (O’Reilly)</a> is an excellent resource to learn more about observability. The book’s second chapter discusses monitoring and observability and is very relevant to this discussion.</p>
<p>Observability builds on monitoring and enables you to gain insights <a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="observability component" id="idm45326831353536"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="monitoring and" id="idm45326831352112"/><a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="observability" data-tertiary="monitoring and" id="idm45326831350736"/>about the internal state of your application. For example, in a Kubernetes cluster an unexpected pod restart event may have limited to no impact on services as other instances of the pod may be adequate to handle the load at the time of the restart. A monitoring system will generate an alert that an unexpected pod restart occurred, and an observability system will generate a medium-priority event with the context that an unexpected pod restart occurred but had no impact on the system if there is no other event like application errors at the time of the pod restart. <a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="monitoring" data-tertiary="observability and" id="idm45326831348368"/>Another example is when an event is generated at the application layer (e.g., duration for HTTP request is larger than the norm). In this scenario, the observability system will provide context for the reason of degradation in application response time (e.g., network layer issue, retransmits, application pod restarts due to resource or other application issues, a Kubernetes infrastructure issue like DNS latency or API server load). As explained previously, an observability system can look at multiple events that impact application state and report application status after considering all of them. Now let’s look at how you can implement observability in a Kubernetes system.</p>
<section data-type="sect2" data-pdf-bookmark="How Observability Works for Kubernetes"><div class="sect2" id="how_observability_works_for_kubernetes">
<h2>How Observability Works for Kubernetes</h2>
<p>The declarative nature of Kubernetes helps a lot in implementing<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Kubernetes and" id="ch05-kuo"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="observability and" data-seealso="observability" id="ch05-kuo2"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="declarative nature" data-tertiary="observability and" id="idm45326831340080"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="observability and" data-seealso="observability" id="idm45326831338432"/> an observability system. We recommend that you build a system that is native to Kubernetes and is able to understand operations in a cluster. For example, a system that understands Kubernetes will monitor a pod (e.g., restarts, out of memory, network activity, etc.) but also understand if a pod is a standalone instance or part of a deployment, replica set, or service. It will also know how critical the pod is to the service or deployment (e.g., how the service is configured for scalability and high availability). So when it reports any event related to the pod, it will provide all this context and help you easily make a decision about how you need to respond to the event.</p>
<p>Another thing to remember is that in Kubernetes you can deploy applications as pods that are a part of higher-level constructs like a deployment or a service. In order to appreciate the complexity in implementing observability for these constructs, we will use an example to explain them. When you configure a service, Kubernetes manages all pods associated with the service and ensures that traffic is delivered to available pods that are a part of the service. <a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="observability and" data-tertiary="service definition" id="idm45326831335056"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Kubernetes and" data-tertiary="service definition" id="idm45326831333408"/><a contenteditable="false" data-type="indexterm" data-primary="service definition" id="idm45326831331760"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="documentation online" data-tertiary="service definition" id="idm45326831330656"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="Kubernetes documentation" data-tertiary="service definition" id="idm45326831329008"/><a contenteditable="false" data-type="indexterm" data-primary="service definition" data-secondary="documentation online" id="idm45326831327296"/>Let’s take a look at an example of <a href="https://oreil.ly/ijVz5">service definition from the Kubernetes documentation</a>:</p>
<pre data-code-language="yaml"><strong><code class="nt">apiVersion</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>
</code><strong><code class="nt">kind</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Service</code><code>
</code><strong><code class="nt">metadata</code></strong><code class="p">:</code><code>
</code><code>  </code><strong><code class="nt">name</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">my-service</code><code>
</code><strong><code class="nt">spec</code></strong><code class="p">:</code><code>
</code><code>  </code><strong><code class="nt">selector</code></strong><code class="p">:</code><code>
</code><code>    </code><strong><code class="nt">app</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">MyApp</code><code>
</code><code>  </code><strong><code class="nt">ports</code></strong><code class="p">:</code><code>
</code><code>    </code><code class="p-Indicator">-</code><code> </code><strong><code class="nt">protocol</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TCP</code><code>
</code><code>      </code><strong><code class="nt">port</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code><code>
</code><code>      </code><strong><code class="nt">targetPort</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">9376</code></pre>
<p>In this example all pods that have the label MyApp and listen on TCP port 9376 become part of the service, and all traffic destined to the service is redirected to these pods. We cover this concept in detail in <a data-type="xref" href="ch08.xhtml#managing_trust_across_teams">Chapter 8</a>. So in this scenario, the observability solution should also work to provide insights at the service level. Monitoring a pod in this case is not sufficient. What is needed is that the observability aggregates metrics across all pods in a service and uses the aggregated information for more analytics and alerts.</p>
<p>Now let’s look at an example of <a href="https://oreil.ly/23Eam">deployments</a> in Kubernetes. <a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Kubernetes and" data-tertiary="deployments" id="idm45326831279600"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="observability and" data-tertiary="deployments" id="idm45326831277920"/><a contenteditable="false" data-type="indexterm" data-primary="deploying a workload" data-secondary="observability" id="idm45326831276272"/><a contenteditable="false" data-type="indexterm" data-primary="workloads" data-secondary="deploying" data-tertiary="observability" id="idm45326831274896"/><a contenteditable="false" data-type="indexterm" data-primary="deploying a workload" data-secondary="securing each stage" data-tertiary="observability" id="idm45326831273248"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="documentation online" data-tertiary="deployments" id="idm45326831246560"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="Kubernetes documentation" data-tertiary="deployments" id="idm45326831244912"/><a contenteditable="false" data-type="indexterm" data-primary="deploying a workload" data-secondary="documentation online" id="idm45326831243200"/><a contenteditable="false" data-type="indexterm" data-primary="workloads" data-secondary="deploying" data-tertiary="documentation online" id="idm45326831241824"/>Deployments allow you to manage pods and replica sets (replicas of a pod, typically used for scaling and high availability). The following is an example configuration for a deployment in <span class="keep-together">Kubernetes:</span></p>
<pre data-code-language="yaml"><strong><code class="nt">apiVersion</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">apps/v1</code><code>
</code><strong><code class="nt">kind</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Deployment</code><code>
</code><strong><code class="nt">metadata</code></strong><code class="p">:</code><code>
</code><code>  </code><strong><code class="nt">name</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx-deployment</code><code>
</code><code>  </code><strong><code class="nt">labels</code></strong><code class="p">:</code><code>
</code><code>    </code><strong><code class="nt">app</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><strong><code class="nt">spec</code></strong><code class="p">:</code><code>
</code><code>  </code><strong><code class="nt">replicas</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">3</code><code>
</code><code>  </code><strong><code class="nt">selector</code></strong><code class="p">:</code><code>
</code><code>    </code><strong><code class="nt">matchLabels</code></strong><code class="p">:</code><code>
</code><code>      </code><strong><code class="nt">app</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>  </code><strong><code class="nt">template</code></strong><code class="p">:</code><code>
</code><code>    </code><strong><code class="nt">metadata</code></strong><code class="p">:</code><code>
</code><code>      </code><strong><code class="nt">labels</code></strong><code class="p">:</code><code>
</code><code>        </code><strong><code class="nt">app</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>    </code><strong><code class="nt">spec</code></strong><code class="p">:</code><code>
</code><code>      </code><strong><code class="nt">containers</code></strong><code class="p">:</code><code>
</code><code>      </code><code class="p-Indicator">-</code><code> </code><strong><code class="nt">name</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>        </code><strong><code class="nt">image</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx:1.14.2</code><code>
</code><code>        </code><strong><code class="nt">ports</code></strong><code class="p">:</code><code>
</code><code>        </code><code class="p-Indicator">-</code><code> </code><strong><code class="nt">containerPort</code></strong><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code></pre>
<p>This configuration will create a deployment for nginx with three replica pods with the configured metadata and specification. Kubernetes has a deployment controller to ensure that all pods and replicas that are a part of the deployment are available. There are several other benefits, like rolling updates, autoscaling, etc., that can be achieved by using the deployment resource in Kubernetes. In such a scenario for observability, the tool you use should look at the activity of all pods (replicas) in a deployment as an aggregate (e.g., all traffic to/from pods in a deployment, pod restarts and their effect on a deployment, etc.). Monitoring and alerting for each pod will not be sufficient to understand how the deployment is operating.</p>
<p>In both these examples it is clear that the collection of metrics needs to be in the context of Kubernetes. Instead of collecting all data and metrics at a pod-level granularity, the collection engine should collect data at a deployment- or service-level granularity when applicable to deliver an accurate representation of the state of the deployment or service. <a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="abstracting pod-level details" id="idm45326831140336"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="logging challenges" data-tertiary="aggregation at higher level" id="idm45326831157440"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="data challenges" data-tertiary="aggregation at higher level" id="idm45326831155824"/>Remember, Kubernetes abstracts pod-level details, and so we need to focus on measuring and alerting at a higher level than pods. Aggregation of data at a deployment and service level will reduce the number of logs you need to collect all the time and address the concern of the costs associated with a large number of logs. Please note the tool needs to have the ability to drill down and capture pod-level details when the operator needs to analyze an issue. We will cover this later in this chapter when we discuss data collection.</p>
<p>Now that we understand how we can leverage the declarative nature of Kubernetes to help with observability and reduce the amount of log data we need to collect and generate relevant alerts, let’s explore the distributed nature of Kubernetes and its impact on observability.</p>
<p>In a microservices-based application deployment, a single application<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Kubernetes and" data-tertiary="microservices" id="idm45326831152192"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="observability and" data-tertiary="microservices" id="idm45326831150544"/><a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="observability" id="idm45326831118432"/><a contenteditable="false" data-type="indexterm" data-primary="distributed application monitoring" data-secondary="microservices observability" id="idm45326831117056"/> comprises several microservices that are deployed in a Kubernetes cluster. This means that in order to service a single transaction from the user, one or more services need to interact with each other, resulting in one or more subtransactions. <a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="demo application online" id="idm45326831115360"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="microservices demo application" id="idm45326831113984"/><a contenteditable="false" data-type="indexterm" data-primary="Google online boutique microservices demo" id="idm45326831112640"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Google microservices demo" id="idm45326831111568"/><a contenteditable="false" data-type="indexterm" data-primary="distributed application monitoring" data-secondary="microservices demo application online" id="idm45326831110176"/>A great example of a <span class="keep-together">sample</span> microservices application is the <a href="https://oreil.ly/wx7bj">Google online boutique demo microservices application</a>. <a data-type="xref" href="#architecture_of_the_google_microservice">Figure 5-1</a> shows the architecture for this <span class="keep-together">application.</span></p>

<p><a data-type="xref" href="#architecture_of_the_google_microservice">Figure 5-1</a> shows how an online boutique application can be deployed as microservices in Kubernetes. There are 11 microservices, each responsible for some aspect of the application. We encourage you to review this application as we will use it to demonstrate how you can implement observability later in the chapter. If you look at the checkout transaction, a user makes a request to the frontend service, which then makes a request to the checkout service. The checkout service needs to interact with several services (e.g., PaymentService, Shipping Service, CurrencyService, EmailService, ProductCatalog Service, CartService) to complete the transaction. So in this scenario if we see our HTTP application log indicate a larger-than-expected duration for the checkout process API response time, we will need to review each of the subtransactions and see if there is an issue with each one and what the issue is (an application issue, network issue, etc.). Another thing that makes this complicated is the fact that each subtransaction is asynchronous and each microservice is serving several <span class="keep-together">independent</span> transactions simultaneously. <a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="observability" data-tertiary="distributed tracing" id="idm45326831102880"/><a contenteditable="false" data-type="indexterm" data-primary="distributed application monitoring" data-secondary="microservices observability" data-tertiary="distributed tracing" id="idm45326831101232"/><a contenteditable="false" data-type="indexterm" data-primary="distributed tracing" data-secondary="microservices observability" id="idm45326831099504"/>In such a scenario you need to use a technique known as <em>distributed tracing</em> to trace the flow of a single transaction across a set of microservices. Distributed tracing can happen by instrumenting the application or instrumenting the kernel. We will cover distributed tracing later in the chapter.</p>

<figure><div id="architecture_of_the_google_microservice" class="figure">
<img src="Images/ksao_0501.png" alt="" width="1457" height="795"/>
<h6><span class="label">Figure 5-1. </span>Architecture of the Google microservices demo application</h6>
</div></figure>
<p>Now that we understand observability and how you should think about it for a Kubernetes cluster, let’s look at the components for an observability tool for Kubernetes. <a data-type="xref" href="#components_of_an_observability_tool_for">Figure 5-2</a> shows a block diagram of the various components of an observability tool for Kuebrnetes.</p>
<figure><div id="components_of_an_observability_tool_for" class="figure">
<img src="Images/ksao_0502.png" alt="" width="1471" height="541"/>
<h6><span class="label">Figure 5-2. </span>Components of an observability tool for Kubernetes</h6>
</div></figure>
<p class="pagebreak-before"><a data-type="xref" href="#components_of_an_observability_tool_for">Figure 5-2</a> shows that you need the following components for your observability implementation:<a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="observability and" data-tertiary="components needed" id="idm45326831090256"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Kubernetes and" data-tertiary="components needed" id="idm45326831088640"/></p>
<dl>
<dt>Telemetry collection</dt>
<dd>As mentioned, your observability solution needs to collect telemetry data<a contenteditable="false" data-type="indexterm" data-primary="telemetry collection" id="idm45326831085584"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="telemetry collection" id="idm45326831084480"/> from various sensors in your cluster. It needs to be distributed and Kubernetes-native. It must support sensors across all layers, from L3 to L7. It also needs to collect information about Kubernetes infrastructure (e.g., DNS and API server logs) and Kubernetes activity (these are known as audit logs). As described, this information must be collected in the context of deployments and services.</dd>
<dt>Analytics and visibility</dt>
<dd>In this layer, the system must provide visualizations that are specific to <a contenteditable="false" data-type="indexterm" data-primary="visualizations" data-secondary="observability components" id="idm45326831081488"/>Kubernetes operations (e.g., service graph, Kubernetes platform view, application views). We will cover some common visualizations that are native to Kubernetes. We recommend you pick a solution that leverages machine learning techniques for baselining and reporting anomalies. <a contenteditable="false" data-type="indexterm" data-primary="pod-to-pod packet capture" id="idm45326831079632"/><a contenteditable="false" data-type="indexterm" data-primary="packet capture pod-to-pod" id="idm45326831078512"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="packet capture pod-to-pod" id="idm45326831077392"/>Finally, the system needs to support the ability for operators to enable pod-to-pod packet capture (note that this is not the same as enabling packet capture on the host interface, as the pod-level visibility is lost). We will cover this in the next section.</dd>
<dt>Security and troubleshooting applications</dt>
<dd>The observability system you implement must support distributed<a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="observability and security" id="idm45326831074768"/><a contenteditable="false" data-type="indexterm" data-primary="distributed tracing" data-secondary="observability component" id="idm45326831073376"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="security strategy via" id="idm45326831072000"/> tracing as described in the previous section to help troubleshoot applications. We also recommend the use of advanced machine learning techniques to understand Kubernetes cluster behavior and predict performance or security concerns. Please note that this is a new area and there is ongoing innovation in it.</dd>
</dl>
<p>Now that we have covered what is needed to implement observability in a Kubernetes cluster, let’s review each of the components in detail.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch05-kuo" id="idm45326831069600"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch05-kuo2" id="idm45326831068224"/></p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Implementing Observability for Kubernetes"><div class="sect2" id="implementing_observability_for_kubernet">
<h2>Implementing Observability for Kubernetes</h2>
<p>In this section we will review each component needed to build an effective observability system in Kubernetes.<a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="observability and" data-tertiary="implementing" id="ch05-imp"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Kubernetes and" data-tertiary="implementing" id="ch05-imp2"/></p>

<p>You should think of log collection as a set of sensors that are distributed<a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" id="idm45326831060720"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="logs" id="idm45326831058992"/> in your cluster. You need to ensure that the sensors are efficient and do not interfere with system operation (e.g., adding latency). We will cover methods of collection later in this section that will show how you can efficiently collect metrics. You should consider deploying sensors (or collecting information) across all the layers of the stack, as shown in <a data-type="xref" href="#components_of_an_observability_tool_for">Figure 5-2</a>. <a contenteditable="false" data-type="indexterm" data-primary="audit logs" data-secondary="observability component" id="idm45326831055936"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="audit logs" data-tertiary="observability component" id="idm45326831054560"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="audit logs" id="idm45326831052912"/>Kubernetes audit logs are an excellent source of information to understand the complete life cycle of various Kubernetes resources. In addition to audit logs, <a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="observability component" id="idm45326831050944"/><a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="documentation online" id="idm45326831049568"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="documentation online" data-tertiary="monitoring" id="idm45326831048192"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="Kubernetes documentation" data-tertiary="monitoring" id="idm45326831046544"/>Kubernetes provides a variety of options for <a href="https://oreil.ly/FSwUs">monitoring</a>. <a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="network traffic logs" id="idm45326831044064"/><a contenteditable="false" data-type="indexterm" data-primary="networking" data-secondary="observability" data-tertiary="network traffic logs" id="idm45326831042368"/><a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="observability" data-tertiary="traffic observability" id="idm45326831040720"/>Next you need to focus on traffic flow logs (Layer 3/Layer 4) to understand the operation of the Kubernetes cluster network. Given the declarative nature of Kubernetes, it is important to collect logs related to application flows (e.g., HTTP or MySQL), where the logs provide visibility into the application behavior (e.g., response time, availability, etc.) as seen by the user. In order to help with troubleshooting, you should also collect logs related to Kubernetes cluster infrastructure (e.g., API server, DNS). Some advanced troubleshooting systems also collect information from the Linux kernel that is a result of activity by a pod (e.g., process information, socket stats for a flow initiated by a pod) and provide a way to enable packet capture (raw packets) for pod-to-pod traffic. The following describes what you should collect for each:</p>
<dl>
<dt>Kubernetes audit logs</dt>
<dd>Kubernetes provides the ability to collect and monitor activity. <a contenteditable="false" data-type="indexterm" data-primary="audit logs" data-secondary="documentation online" id="idm45326831036800"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="audit logs" data-tertiary="documentation online" id="idm45326831035424"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="documentation online" data-tertiary="audit logs" id="idm45326831033776"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="Kubernetes documentation" data-tertiary="audit logs" id="idm45326831032128"/>Here is an excellent <a href="https://oreil.ly/aKmCE">guide</a> that explains how you can control what to collect and also mechanisms for logging and alerting. We suggest you review what you need to collect and set the audit policy carefully—we recommend against just collecting everything. For example, you should log API requests, usernames, RBAC, decisions, request verbs, the client (user-agent) that made the request, and response codes for API requests. We will show a sample Kubernetes activity dashboard in the visualization section.</dd>
<dt>Network flow logs</dt>
<dd>Network flow logs (Layer 3/Layer 4) are key to understanding the Kubernetes cluster network operation. Typically these include the five-tuple (source and destination IP addresses/ports and port). It is also important to collect Kubernetes metadata associated with pods (source and destination namespaces, pod names, labels associated with pods, host on which the pods were running) and aggregate bytes/packets for each flow. Note this can result in a large amount of flow data, as there could be a large number of pods on a node. We will address this in the following section about aggregation at collection time.</dd>
<dt>DNS flow logs</dt>
<dd>Along with the API server, the DNS server is a critical part of the Kubernetes cluster<a contenteditable="false" data-type="indexterm" data-primary="networking" data-secondary="observability" data-tertiary="DNS flow logs" id="idm45326831026816"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="DNS flow logs" id="idm45326831025168"/><a contenteditable="false" data-type="indexterm" data-primary="DNS activity logs" data-secondary="DNS flow logs" id="idm45326831023552"/> and is used by applications to resolve domain names in order to connect other services/pods as a part of normal operation. An issue with the DNS server can impact several applications in the cluster. It is important to collect information from the client’s perspective. You should log DNS requests by pods that capture request count, latency, which DNS server was used to resolve the request, the DNS response code, and the response. This should be collected with Kubernetes metadata (e.g., namespace, pod name, labels, etc.), as this will help associate the DNS issue with a service and facilitate further troubleshooting.</dd>
<dt>Application logs</dt>
<dd>As explained, the collection of application logs<a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="application logs" id="idm45326831020544"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="application flow logs" id="idm45326831019168"/> (HTTP, MySQL) is very important in a declarative system like Kubernetes. These logs provide a view into the user experience (e.g., response time or availability). The logs will be application-specific information but must include response codes (status), response time, and other application-specific context. For example, for HTTP requests, you should log domains (part of the URL), user agent, number of requests, HTTP response codes, and in some cases complete URL paths. Again, logs should include Kubernetes metadata (e.g., namespace, service, labels, pod names, etc.).</dd>
<dt>Process information and socket stats</dt>
<dd>As mentioned, these stats are not part of typical observability implementations,<a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="process information and socket stats" id="idm45326831015952"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="observability and" data-tertiary="process information and socket stats" id="idm45326831014224"/><a contenteditable="false" data-type="indexterm" data-primary="process monitoring" data-secondary="process information and socket stats" id="idm45326831012560"/><a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="process monitoring" data-tertiary="process information and socket stats" id="idm45326831011168"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="process information and socket stats" id="idm45326831009504"/> but we recommend that you consider collecting these stats as they provide a more comprehensive view of the Kubernetes cluster operation. For example, if you can get information about processes (that run in a pod), this can be an excellent way to correlate with application performance data (e.g., co-relating memory usage, or garbage collection events in a Java-based application to response time and network activity initiated by the process). Socket stats are details of a TCP flow between two endpoints (e.g., network round-trip time, TCP congestion windows, TCP retransmits, etc.). These stats when associated with pods can provide a view into the impact of the underlying network on pod-to-pod communication.</dd>
</dl>
<p>Now that we have covered what you need to collect for a complete <a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" id="idm45326831006992"/>observability solution, let’s look at the tools and techniques available to implement collection. <a data-type="xref" href="#reference_implementation_for_collection">Figure 5-3</a> is an example reference implementation to show how you can implement collection on a node in your Kubernetes cluster.</p>

<p><a data-type="xref" href="#reference_implementation_for_collection">Figure 5-3</a> shows a node in your Kubernetes cluster that has applications deployed as services, deployments, and pods in namespaces as you would see in a typical Kubernetes cluster. In order to facilitate collection, a few components are added as shown in the observability components section, and it shows a few additions to the Linux kernel to facilitate collection. Let’s explore the functions of each of these components.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch05-imp" id="idm45326831002704"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch05-imp2" id="idm45326831001360"/></p>
<figure><div id="reference_implementation_for_collection" class="figure">
<img src="Images/ksao_0503.png" alt="" width="1138" height="863"/>
<h6><span class="label">Figure 5-3. </span>Reference implementation for collection on a node</h6>
</div></figure>

</div></section>
<section data-type="sect2" data-pdf-bookmark="Linux Kernel Tools"><div class="sect2" id="linux_kernel_tools">
<h2>Linux Kernel Tools</h2>
<p>The Linux kernel offers several options that you can use to help with data collection.<a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="observability tools" id="idm45326830996080"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="Linux kernel tools" id="idm45326830994608"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="Linux kernel tools" id="idm45326830992960"/> It is very important that the tool you use leverages these tools instead of focusing on processing raw logs that are generated by other tools:</p>
<dl>
<dt>eBPF programs and kprobes</dt>
<dd>eBPF stands for extended Berkley Packet Filter. It is an exciting <a contenteditable="false" data-type="indexterm" data-primary="distributed tracing" data-secondary="eBPF and kprobes" id="idm45326830989680"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="kprobes for kernel observability" id="idm45326830988304"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="observability tools" data-tertiary="eBPF programs and kprobes" id="idm45326830986912"/><a contenteditable="false" data-type="indexterm" data-primary="eBPF (extended Berkley Packet Filter)" data-secondary="kprobes and" id="idm45326830985248"/><a contenteditable="false" data-type="indexterm" data-primary="kprobes for eBPF programs" id="idm45326830983856"/>technology that can be used for collection and observability. It was originally designed for packet filtering, but was then extended to allow adding programs to various hooks in the kernel to be used as trace points. In case you are using an eBPF-based dataplane, the eBPF programs that are managing the packet path will also provide packet and flow information. <a contenteditable="false" data-type="indexterm" data-primary="eBPF (extended Berkley Packet Filter)" data-secondary="documentation online" id="idm45326830982224"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="eBPF documentation" id="idm45326830980832"/>We recommend reading <a href="https://oreil.ly/s54kG">Brendan Gregg’s blog post “Linux Extended BPF (eBPF) Tracing Tools”</a> to understand how to use eBPF for performance and tracing. In the context of this discussion, you can attach an eBPF program to a kernel probe (kprobe), which is essentially a trace point that is triggered and executes the program whenever the code executes the function for which the kprobe is registered. <a contenteditable="false" data-type="indexterm" data-primary="kprobes for eBPF programs" data-secondary="documentation online" id="idm45326830978288"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="kprobe documentation" id="idm45326830976832"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="kprobes for kernel observability" data-tertiary="documentation online" id="idm45326830975456"/>The kernel documentation for <a href="https://oreil.ly/hLziH">kprobes</a> provides more details. This is a great way to get information from the Linux kernel for observability.</dd>
<dt class="pagebreak-before">NFLOG and conntrack</dt>
<dd>If you are using the standard Linux networking dataplane (iptables-based), <a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="observability tools" data-tertiary="NFLOG" id="idm45326830971856"/><a contenteditable="false" data-type="indexterm" data-primary="NFLOG packet logger" id="idm45326830970016"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="NFLOG packet logger" id="idm45326830968912"/><a contenteditable="false" data-type="indexterm" data-primary="networking" data-secondary="observability" data-tertiary="NFLOG packet logger" id="idm45326830967536"/>there are tools available to track packets and network flows. We recommend using NFLOG, which is a mechanism to be used in conjunction with iptables to log packets. You can review the details in the iptables documentation; at a high level NFLOG can be set as a target for an iptables rule, and it will log packets via a netlink socket on a multicast address that a user space process can subscribe to and collect packets from. <a contenteditable="false" data-type="indexterm" data-primary="networking" data-secondary="observability" data-tertiary="conntrack" id="idm45326830965312"/><a contenteditable="false" data-type="indexterm" data-primary="conntrack" id="idm45326830963664"/>Conntrack is another module used in conjunction with iptables to query the connection state of a packet or a flow, and it can be used to update statistics for a flow. </dd>
</dl>
<p>We recommend you review options (e.g., Net Filter) that the Linux kernel provides and leverage them in sensors that are used to collect information. This is very important as it will be an efficient way to collect data, since these options provided by the Linux kernel are highly optimized.</p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Observability Components"><div class="sect1" id="observability_components">
<h1>Observability Components</h1>
<p>Now that we understand how to collect data from the Linux kernel, let’s look at how this data needs to be processed in user space to ensure we have an effective observability solution:</p>
<dl>
<dt>Log collector</dt>
<dd>This is a very important component in the system. The goal of this component<a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="logs collector" id="idm45326830957952"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="log collector" id="idm45326830956336"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="log collector" id="idm45326830954688"/> is to add context from the Kubernetes cluster to the data collected from other sensors—for example, to add pod metadata (name, namespace, label, etc.)  to source and destination IP addresses, respectively, from a network flow. This is how you can add Kubernetes context to raw network flow logs. Likewise, any data you collect from kernel probes can also be enriched by adding relevant Kubernetes metadata. This way you can have log data that associates activity in the kernel to objects in your Kubernetes cluster (e.g., pods, services, deployments). It is critical for you to be able derive insights about your Kubernetes cluster operation. Please note that this component is something you need to implement, or you must ensure that the tool you choose for observability has this functionality. It is a critical part of your observability implementation.</dd>
<dt>Envoy (proxy)</dt>
<dd>We discussed the importance of having a collection of application-specific data,<a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="Envoy proxy" id="idm45326830951152"/><a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="observability" data-tertiary="Envoy proxy" id="idm45326830949392"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="Envoy proxy" data-seealso="Envoy proxy" id="idm45326830947744"/><a contenteditable="false" data-type="indexterm" data-primary="Envoy proxy" id="idm45326830945824"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="observability and" data-tertiary="Envoy proxy" id="idm45326830944720"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="application flow logs" id="idm45326830943072"/> and for this we recommend that you use <a href="https://oreil.ly/0niF8">Envoy</a>, a well-known proxy that is used to analyze application protocols and log application transaction flows (e.g., HTTP transactions on a single HTTP connection). Please note that Envoy can be used as a sidecar pattern where it attaches to every pod as a sidecar and tracks packets to/from the pod. It can also be deployed as a daemonset (a transparent proxy) where you can use the dataplane to redirect traffic to pass through an envoy instance running on the host. We strongly recommend using Envoy with this latter configuration, as using the sidecar pattern has security concerns and can be disruptive to applications. In the context of this discussion, the Envoy daemonset will be the source of application flow logs to the log collector. The log collector can now use the pod metadata (name, namespace, labels, deployments, services, IP addresses)  to correlate this data with the data received from the kernel and further enrich it with application data. </dd>
<dt>Fluentd</dt>
<dd>Note that the data collection discussed so far is processed by the log collector<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="Fluentd" id="idm45326830938688"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="Fluentd" id="idm45326830936944"/><a contenteditable="false" data-type="indexterm" data-primary="Fluentd" id="idm45326830935280"/> on every node in the cluster. You need to implement a mechanism to send the data from all nodes to a datastore or security information and event management (SIEM), where it can be picked up by analytics and visualization tools. <a href="https://oreil.ly/11s22">Fluentd</a> is an excellent option to send collected data to the datastore of your choice. It offers excellent integrations and is a tool that is Kubernetes native. There are other options available, but we recommend you use Fluentd for shipping collected log data to a data store.</dd>
<dt>Prometheus</dt>
<dd>We’ve discussed how you collect flow logs; now we need a component<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="Prometheus" id="idm45326830931968"/><a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="Prometheus" id="idm45326830930320"/><a contenteditable="false" data-type="indexterm" data-primary="Prometheus" id="idm45326830928656"/><a contenteditable="false" data-type="indexterm" data-primary="alerting" data-secondary="tools for" id="idm45326830927552"/> for the collection of metrics and alerting. Prometheus, a Kubernetes-native tool, is a great choice for metrics collection and alerting. It’s deployed as endpoints that scrape metrics and send them to a time-series database that’s a part of the Prometheus server for analysis and query. You can also define alerts for data sent to the Prometheus server. It’s a widely used option and has integrations to dashboards and alerting tools. We recommend you consider it as an option for your cluster.</dd>
</dl>
<p>We hope that this discussion has given you an idea of how you can implement data collection for your Kubernetes cluster. Now let’s look at aggregation and correlation.</p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Aggregation and Correlation"><div class="sect1" id="aggregation_and_correlation">
<h1>Aggregation and Correlation</h1>
<p>In the previous section we covered data collection and discussed<a contenteditable="false" data-type="indexterm" data-primary="data collection" data-secondary="observability components" data-tertiary="aggregation and correlation" id="idm45326830923024"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="aggregation and correlation of data" id="idm45326830921344"/><a contenteditable="false" data-type="indexterm" data-primary="aggregation of observability data" id="idm45326830919952"/><a contenteditable="false" data-type="indexterm" data-primary="correlation of observability data" id="idm45326830918832"/> how you can collect data from various sources in your cluster (API server, network flows, kernel probes, application flows). This is very useful, but we still need to address the concern of data volume if we keep the collection at pod-level granularity. Another thing to note is that the data volume concern multiples if we keep data from various sources separate and then associate it at query time. You can say that it’s better to keep as much raw data as possible, and there are efficient tools to query and aggregate data after collection (offline), so why not use that approach? Yes, that is a valid point, but there are a couple of things to think about. The large volume of data would mean aggregation and query time joins of data will be resource-intensive (it can very well be more expensive to operate your data collection system than your Kubernetes cluster!). Also, given the ephemeral nature of Kubernetes (pod life cycles can be very short), the latency in analyzing data offline prevents any kind of reasonable response to data collected to mitigate the issue reported by the data. In some cases, if correlation is not done at collection time, it will not be possible to associate two different collections. For example, you cannot collect a list of policies and a list of flows and then associate the policy with a flow offline without rerunning the policy evaluation.</p>
<p>We also discussed the declarative nature of Kubernetes and how a deployment<a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="declarative nature" data-tertiary="observability and" id="idm45326830915760"/> and a service are higher-level constructs than a pod. In such a scenario, we recommend that you consider the aggregation of data at a deployment or a service level. This means data from all pods for a service is aggregated; you would collect data between deployments and data to services as a default option. This will give you the right level of granularity. You can provide an option to reduce the aggregation to collect pod-level data as a drill-down action or in response to an event. This way you can address the concern about large amounts of log data collected and the associated processing cost. Also, the data collection is more Kubernetes-native as Kubernetes monitors deployments/services as a unit and makes adjustments to ensure the deployment/service is operating as per the specification.</p>
<p>In the data collection section we discussed the log collector component<a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="observability components" data-tertiary="log collector" id="idm45326830912656"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="log collector" id="idm45326830910992"/> that receives data from various sources. It could be used as a source to correlate data at collection time, so you don’t have to do any additional correlation after data collection, and you also benefit from not having to collect redundant data for each source. For example, if the kprobe in the kernel collects socket data for five-tuple (IP addresses, ports, protocol), and the NFLOG provides other information like bytes and packets for the same five-tuple, the log collector can create a single log with the five-tuple, the Kubernetes metadata, the network flow data, and the socket statistics. This will provide logs with very high context and low occupancy for collection and processing.</p>
<p>Now let’s go back to the Google online boutique example and see a sample<a contenteditable="false" data-type="indexterm" data-primary="distributed application monitoring" data-secondary="microservices demo application online" data-tertiary="aggregation and correlation of data" id="idm45326830907856"/><a contenteditable="false" data-type="indexterm" data-primary="Google online boutique microservices demo" data-secondary="aggregation and correlation of data" id="idm45326830906208"/><a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="demo application online" data-tertiary="aggregation and correlation of data" id="idm45326830904784"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="aggregation and correlation of data" data-tertiary="Google microservices demo" id="idm45326830903120"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Google microservices demo" data-tertiary="aggregation and correlation of data" id="idm45326830901440"/> of what a log will look like with aggregation and correlation of kernel and network flow data. The sample log is generated using the collection and aggregation concepts described previously for a transaction between the frontend service and the currencyservice of the application. It is a gRPC-based transaction:<a contenteditable="false" data-type="indexterm" data-primary="flow log from Calico Enterprise" id="idm45326830899296"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="flow log from Calico Enterprise" id="idm45326830898176"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="Calico Enterprise flow log" data-seealso="Calico Enterprise" id="idm45326830896784"/><a contenteditable="false" data-type="indexterm" data-primary="Calico Enterprise" data-secondary="flow log" id="idm45326830894848"/></p>
<pre data-code-language="yaml"><code class="p-Indicator">{</code>
   <code class="s">"_id"</code><code class="p-Indicator">:</code> <code class="s">"YTBT5HkBf0waR4u9Z0U3"</code><code class="p-Indicator">,</code>
   <code class="s">"_score"</code><code class="p-Indicator">:</code> <code class="nv">3</code><code class="p-Indicator">,</code>
   <code class="s">"_type"</code><code class="p-Indicator">:</code> <code class="s">"_doc"</code><code class="p-Indicator">,</code>
   <code class="s">"start_time"</code><code class="p-Indicator">:</code> <code class="nv">1623033303</code><code class="p-Indicator">,</code>
   <code class="s">"end_time"</code><code class="p-Indicator">:</code> <code class="nv">1623033334</code><code class="p-Indicator">,</code>
   <code class="s">"source_ip"</code><code class="p-Indicator">:</code> <code class="s">"10.57.209.32"</code><code class="p-Indicator">,</code>
   <code class="s">"source_name"</code><code class="p-Indicator">:</code> <code class="s">"frontend-6f794fbff7-58qrq"</code><code class="p-Indicator">,</code>
   <code class="s">"source_name_aggr"</code><code class="p-Indicator">:</code> <code class="s">"frontend-6f794fbff7-*"</code><code class="p-Indicator">,</code>
   <code class="s">"source_namespace"</code><code class="p-Indicator">:</code> <code class="s">"onlinebotique"</code><code class="p-Indicator">,</code>
   <code class="s">"source_port"</code><code class="p-Indicator">:</code> <code class="nv">null</code><code class="p-Indicator">,</code>
   <code class="s">"source_type"</code><code class="p-Indicator">:</code> <code class="s">"wep"</code><code class="p-Indicator">,</code>
   <code class="s">"source_labels"</code><code class="p-Indicator">:</code> <code class="p-Indicator">[</code>
       <code class="s">"app=frontend"</code><code class="p-Indicator">,</code>
       <code class="s">"pod-template-hash=6f794fbff7"</code>
   <code class="p-Indicator">],</code>
   <code class="s">"dest_ip"</code><code class="p-Indicator">:</code> <code class="s">"10.57.209.29"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_name"</code><code class="p-Indicator">:</code> <code class="s">"currencyservice-7fd6c64-t2zvl"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_name_aggr"</code><code class="p-Indicator">:</code> <code class="s">"currencyservice-7fd6c64-*"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_namespace"</code><code class="p-Indicator">:</code> <code class="s">"onlinebotique"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_service_namespace"</code><code class="p-Indicator">:</code> <code class="s">"onlinebotique"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_service_name"</code><code class="p-Indicator">:</code> <code class="s">"currencyservice"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_service_port"</code><code class="p-Indicator">:</code> <code class="s">"grpc"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_port"</code><code class="p-Indicator">:</code> <code class="nv">7000</code><code class="p-Indicator">,</code>
   <code class="s">"dest_type"</code><code class="p-Indicator">:</code> <code class="s">"wep"</code><code class="p-Indicator">,</code>
   <code class="s">"dest_labels"</code><code class="p-Indicator">:</code> <code class="p-Indicator">[</code>
       <code class="s">"app=currencyservice"</code><code class="p-Indicator">,</code>
       <code class="s">"pod-template-hash=7fd6c64"</code>
   <code class="p-Indicator">],</code>
   <code class="s">"proto"</code><code class="p-Indicator">:</code> <code class="s">"tcp"</code><code class="p-Indicator">,</code>
   <code class="s">"action"</code><code class="p-Indicator">:</code> <code class="s">"allow"</code><code class="p-Indicator">,</code>
   <code class="s">"reporter"</code><code class="p-Indicator">:</code> <code class="s">"src"</code><code class="p-Indicator">,</code>
   <code class="s">"policies"</code><code class="p-Indicator">:</code> <code class="p-Indicator">[</code>
       <code class="s">"1|platform|platform.allow-kube-dns|pass"</code><code class="p-Indicator">,</code>
       <code class="s">"2|__PROFILE__|__PROFILE__.kns.hipstershop|allow"</code><code class="p-Indicator">,</code>
       <code class="s">"0|security|security.pass|pass"</code>
   <code class="p-Indicator">],</code>
   <code class="s">"bytes_in"</code><code class="p-Indicator">:</code> <code class="nv">68437</code><code class="p-Indicator">,</code>
   <code class="s">"bytes_out"</code><code class="p-Indicator">:</code> <code class="nv">81760</code><code class="p-Indicator">,</code>
   <code class="s">"num_flows"</code><code class="p-Indicator">:</code> <code class="nv">1</code><code class="p-Indicator">,</code>
   <code class="s">"num_flows_started"</code><code class="p-Indicator">:</code> <code class="nv">0</code><code class="p-Indicator">,</code>
   <code class="s">"num_flows_completed"</code><code class="p-Indicator">:</code> <code class="nv">0</code><code class="p-Indicator">,</code>
   <code class="s">"packets_in"</code><code class="p-Indicator">:</code> <code class="nv">656</code><code class="p-Indicator">,</code>
   <code class="s">"packets_out"</code><code class="p-Indicator">:</code> <code class="nv">861</code><code class="p-Indicator">,</code>
   <code class="s">"http_requests_allowed_in"</code><code class="p-Indicator">:</code> <code class="nv">0</code><code class="p-Indicator">,</code>
   <code class="s">"http_requests_denied_in"</code><code class="p-Indicator">:</code> <code class="nv">0</code><code class="p-Indicator">,</code>
   <code class="s">"process_name"</code><code class="p-Indicator">:</code> <code class="s">"wrk:worker_0"</code><code class="p-Indicator">,</code>
   <code class="s">"num_process_names"</code><code class="p-Indicator">:</code> <code class="nv">1</code><code class="p-Indicator">,</code>
   <code class="s">"process_id"</code><code class="p-Indicator">:</code> <code class="s">"26446"</code><code class="p-Indicator">,</code>
   <code class="s">"num_process_ids"</code><code class="p-Indicator">:</code> <code class="nv">1</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_mean_send_congestion_window"</code><code class="p-Indicator">:</code> <code class="nv">10</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_min_send_congestion_window"</code><code class="p-Indicator">:</code> <code class="nv">10</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_mean_smooth_rtt"</code><code class="p-Indicator">:</code> <code class="nv">9303</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_max_smooth_rtt"</code><code class="p-Indicator">:</code> <code class="nv">13537</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_mean_min_rtt"</code><code class="p-Indicator">:</code> <code class="nv">107</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_max_min_rtt"</code><code class="p-Indicator">:</code> <code class="nv">107</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_mean_mss"</code><code class="p-Indicator">:</code> <code class="nv">1408</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_min_mss"</code><code class="p-Indicator">:</code> <code class="nv">1408</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_total_retransmissions"</code><code class="p-Indicator">:</code> <code class="nv">0</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_lost_packets"</code><code class="p-Indicator">:</code> <code class="nv">0</code><code class="p-Indicator">,</code>
   <code class="s">"tcp_unrecovered_to"</code><code class="p-Indicator">:</code> <code class="nv">0</code><code class="p-Indicator">,</code>
   <code class="s">"host"</code><code class="p-Indicator">:</code> <code class="s">"gke-v2y0ly8k-logging-default-pool-e0c7499d-76z8"</code><code class="p-Indicator">,</code>
   <code class="s">"@timestamp"</code><code class="p-Indicator">:</code> <code class="nv">1623033334000</code>
<code class="p-Indicator">}</code></pre>
<p>This is an example of a flow log from Calico Enterprise. There are a few things <span class="keep-together">to note</span> about the log: It aggregates data from all pods backing the frontend service (<code>frontend-6f794fbff7-*</code>) and all pods belonging to the currencyservice (<code>currencyservice-7fd6c64-*</code>). The data from the kprobe and socket statistics are aggregated as mean, min, and max for each metric for the data between services. The process ID and the process name received from the kernel are correlated with the other data, and we also see the network policy action and the network policies impacting the flow correlated with other data. This is an example of what you want to achieve for data collection in your Kubernetes cluster!</p>
<p>Now that we have covered how to collect, aggregate, and correlate data in a Kubernetes-native manner, let’s explore visualization of data.</p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Visualization"><div class="sect1" id="visualization">
<h1>Visualization</h1>
<p>There are some great tools that support the visualization of the data collected.<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="tools for" data-tertiary="visualization of data" id="ch05-vis"/><a contenteditable="false" data-type="indexterm" data-primary="visualizations" data-secondary="observability data" id="ch05-vis2"/><a contenteditable="false" data-type="indexterm" data-primary="visualizations" data-secondary="tools that support" id="idm45326826640528"/><a contenteditable="false" data-type="indexterm" data-primary="Calico Enterprise" data-secondary="visualization of data" id="idm45326826639152"/><a contenteditable="false" data-type="indexterm" data-primary="Datadog" data-secondary="visualization of data" id="idm45326826637776"/> For example, Prometheus offers an integration with Grafana that provides very good dashboards to visualize data. There are also some commercial tools like Datadog, New Relic, and Calico Enterprise that support the collection and visualization of data. We will cover a few common visualizations that are useful for Kubernetes clusters.</p>
<section data-type="sect2" data-pdf-bookmark="Service Graph"><div class="sect2" id="service_graph">
<h2>Service Graph</h2>
<p>This is a representation of your Kubernetes cluster as a graph showing<a contenteditable="false" data-type="indexterm" data-primary="service graph" id="idm45326826634016"/><a contenteditable="false" data-type="indexterm" data-primary="Google online boutique microservices demo" data-secondary="service graph" id="idm45326826632912"/><a contenteditable="false" data-type="indexterm" data-primary="distributed application monitoring" data-secondary="microservices demo application online" data-tertiary="service graph" id="idm45326826631440"/><a contenteditable="false" data-type="indexterm" data-primary="microservices" data-secondary="demo application online" data-tertiary="service graph" id="idm45326826629712"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="Google microservices demo" data-tertiary="service graph" id="idm45326826628064"/> services in a Kubernetes cluster and interactions between them. If we go back to the Google microservices online boutique example, <a data-type="xref" href="#service_graph_representation_of_the_onl">Figure 5-4</a> shows the online boutique application implemented and represented as a service graph.</p>

<p><a data-type="xref" href="#service_graph_representation_of_the_onl">Figure 5-4</a> is a visualization of the online boutique namespace as a service graph, with the nodes representing services and pods backing a service or a group of pods either standalone or as a part of a deployment. The edges show network activity and policy action. The graph is interactive and allows you to pick a service (e.g., frontend service) and allows the viewing of detailed logs collected for the service. <a data-type="xref" href="#detailed_view_of_the_frontend_microserv">Figure 5-5</a> shows a summarized view of all collected data for the service selected (frontend).</p>

<p><a data-type="xref" href="#detailed_view_of_the_frontend_microserv">Figure 5-5</a> shows a detailed view of the frontend service as a drill-down—it shows information from all sources in one view, so it’s very easy to analyze the operation of the service.</p>
<p>Service graph is a very common pattern to represent Kubernetes cluster topology. There are several tools that provide this view, such as Kiali, Datadog, and Calico Enterprise.</p>

<figure><div id="service_graph_representation_of_the_onl" class="figure">
<img src="Images/ksao_0504.png" alt="" width="2553" height="1148"/>
<h6><span class="label">Figure 5-4. </span>Service graph representation of the online boutique application</h6>
</div></figure>

<figure><div id="detailed_view_of_the_frontend_microserv" class="figure">
<img src="Images/ksao_0505.png" alt="" width="2520" height="1293"/>
<h6><span class="label">Figure 5-5. </span>Detailed view of the frontend microservice</h6>
</div></figure>


</div></section>
<section data-type="sect2" data-pdf-bookmark="Visualization of Network Flows"><div class="sect2" id="visualization_of_network_flows">
<h2>Visualization of Network Flows</h2>
<p><a data-type="xref" href="#network_flow_visualization">Figure 5-6</a> shows a common pattern<a contenteditable="false" data-type="indexterm" data-primary="networking" data-secondary="observability" data-tertiary="visualization of network flows" id="idm45326826613440"/> used to visualize flows. This is ring-based visualization, where each ring represents an aggregation level. In the example shown in <a data-type="xref" href="#detailed_view_of_the_frontend_microserv">Figure 5-5</a>, the outermost ring represents a namespace and all flows within the namespaces. Selecting a ring in the middle shows all flows for a service, and selecting the innermost ring shows all flows for pods backing the service. The panel on the right is a selector to enable more-granular views using filtering and details like flows and policy action for the selection. This is an excellent way to visualize network flows in your cluster.</p>
<figure><div id="network_flow_visualization" class="figure">
<img src="Images/ksao_0506.png" alt="" width="1890" height="1053"/>
<h6><span class="label">Figure 5-6. </span>Network flow visualization</h6>
</div></figure>
<p>In this section we have covered some common visualization patterns and tried to show how they can be applied to Kubernetes. Please note that there are several visualizations that can be applied to Kubernetes; these are examples to show how you can represent data collected in a Kubernetes cluster.</p>
<p>Now that we have covered data collection, aggregation, correlation, and visualization, let’s explore some advanced topics to utilize the data collected to derive insights into the operation of the Kubernetes cluster.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch05-vis" id="idm45326826606608"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch05-vis2" id="idm45326826605232"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Analytics and Troubleshooting "><div class="sect1" id="analytics_and_troubleshooting">
<h1>Analytics and Troubleshooting </h1>
<p>In this section we will explore analytics applications that leverage the collection, aggregation, and correlation components to help provide additional insights. Note that there are many applications that can be built to leverage the context-rich data in a Kubernetes cluster. We cover some applications as examples.</p>
<section data-type="sect2" data-pdf-bookmark="Distributed Tracing"><div class="sect2" id="distributed_tracing">
<h2>Distributed Tracing</h2>
<p>We explained distributed tracing before and discussed its importance<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="analytics" data-tertiary="distributed tracing" id="idm45326826599776"/><a contenteditable="false" data-type="indexterm" data-primary="distributed tracing " id="idm45326826598128"/> in a microservices-based architecture, where it is critical to trace a single user request across multiple transactions that need to happen between various microservices. There are two well-known approaches to implementing distributed tracing,</p>
<dl>
<dt>Instrument transaction request headers</dt>
<dd>In this method the HTTP headers are instrumented with a<a contenteditable="false" data-type="indexterm" data-primary="distributed tracing" data-secondary="instrument transaction request headers" id="idm45326826595520"/><a contenteditable="false" data-type="indexterm" data-primary="instrument transaction request headers" id="idm45326826593984"/><a contenteditable="false" data-type="indexterm" data-primary="Envoy proxy" data-secondary="distributed tracing" id="idm45326826592864"/> request ID, and the request ID is preserved in headers across calls to various other services. <a href="https://oreil.ly/jprHR">Envoy</a> is a very popular tool used to implement distributed tracing. It supports integrations with other well-known application tracers like Lightstep and AWS X-Ray. We recommend that you use Envoy if you are fine with instrumenting applications to add and preserve the request ID across calls between microservices.</dd>
<dt>eBPF and kprobes</dt>
<dd>In the method described for using Envoy, there is a change required<a contenteditable="false" data-type="indexterm" data-primary="distributed tracing" data-secondary="eBPF and kprobes" id="idm45326826589376"/><a contenteditable="false" data-type="indexterm" data-primary="eBPF (extended Berkley Packet Filter)" data-secondary="kprobes and" data-tertiary="distributed tracing" id="idm45326826587920"/><a contenteditable="false" data-type="indexterm" data-primary="kprobes for eBPF programs" id="idm45326826586256"/> to the application traffic. It is possible to implement distributed tracing for service-to-service calls using eBPF and Linux kernel probes. You can attach eBPF programs to kprobes/uprobes and other trace points in the kernel and build a distributed tracing application. Note the detailed implementation of such an application is beyond the scope of this book, but we wanted to mention this as an option for distributed tracing in case you are wary of altering application traffic.</dd>
</dl>
<p>Now that we have covered distributed tracing, let’s look at how you can implement packet capture in your Kubernetes cluster.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Packet Capture"><div class="sect2" id="packet_capture">
<h2>Packet Capture</h2>
<p>In your Kubernetes cluster we recommend that you implement<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="analytics" data-tertiary="packet capture" id="idm45326826581872"/><a contenteditable="false" data-type="indexterm" data-primary="packet capture pod-to-pod" id="idm45326826580144"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="packet capture pod-to-pod" id="idm45326826579072"/><a contenteditable="false" data-type="indexterm" data-primary="pod-to-pod packet capture" id="idm45326826577728"/> or pick a tool that supports raw packet captures between pods. The tool should support a selector-based packet capture (e.g., pod labels) and role-based access control to enable and view packet captures. This is a simple yet very effective feature that can be used as a response action to an event (e.g., increased application latency) to analyze raw packet flows to understand the issue and find the root cause. <a contenteditable="false" data-type="indexterm" data-primary="pod-to-pod packet capture" data-secondary="libpcap" id="idm45326826576048"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="packet capture pod-to-pod" data-tertiary="libpcap" id="idm45326826574656"/><a contenteditable="false" data-type="indexterm" data-primary="packet capture pod-to-pod" data-secondary="libpcap" id="idm45326826572992"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="libpcap packet capture tool" id="idm45326826571600"/><a contenteditable="false" data-type="indexterm" data-primary="libpcap packet capture tool" id="idm45326826570208"/>In order to implement raw packet captures, we recommend using <a href="https://oreil.ly/c2UFJ">libpcap</a>, which supports the ability to capture packets on an interface on Linux systems.</p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="conclusion-id000010">
<h1>Conclusion</h1>
<p>In this chapter we covered what observability is and how to implement it for your Kubernetes cluster. The following are the highlights of this chapter:</p>
<ul>
<li><p>Monitoring needs to be a part of your observability strategy; monitoring alone is not sufficient.</p></li>
<li><p>It is important to leverage the declarative nature of Kubernetes when you implement an observability solution.</p></li>
<li><p>The key components for implementing observability for your Kubernetes cluster are log collection, log aggregation and correlation, visualization, distributed tracing, and analytics.</p></li>
<li><p>You must implement your observability using a tool that is native to Kubernetes.</p></li>
<li><p>You should use tools available in the Linux kernel to drive efficient collection and aggregation of data (e.g., NFLOG, eBPF-based probes).</p></li>
</ul>
</div></section>
</div></section></div></body></html>