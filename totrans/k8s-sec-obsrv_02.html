<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Infrastructure Security"><div class="chapter" id="infrastructure_security">
    <h1><span class="label">Chapter 2. </span>Infrastructure Security</h1>
    <p>Many Kubernetes configurations are insecure by default. In this <a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="about" id="idm45326833147872"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="infrastructure security" data-seealso="infrastructure security" id="idm45326833146416"/>chapter we will explore how to secure Kubernetes at the
        infrastructure level. It can be made more secure through the combination of host hardening to make the servers
        or VMs Kubernetes is hosted on more secure, cluster hardening to secure the Kubernetes control plane components,
        and network security to integrate the cluster with the surrounding infrastructure. Please note that the concepts
        discussed in this chapter apply to self-hosted Kubernetes clusters as well as managed Kubernetes clusters.</p>
    <dl>
        <dt>Host hardening</dt>
        <dd>This covers the choice of operating system, avoiding running nonessential processes on the hosts, and
            host-based firewalling.</dd>
        <dt>Cluster hardening</dt>
        <dd>This covers a range of configuration and policy settings needed to harden the control plane, including
            configuring TLS certificates, locking down the Kubernetes datastore, encrypting secrets at rest, credential
            rotation, and user authentication and access control.</dd>
        <dt>Network security</dt>
        <dd>This covers securely integrating the cluster with the surrounding infrastructure, and in particular which
            network interactions between the cluster and the surrounding infrastructure are allowed, for control plane,
            host, and workload traffic.</dd>
    </dl>
    <p>Let’s look at the details for each of these aspects and explore what is needed to build a secure infrastructure
        for your Kubernetes cluster.</p>
    <section data-type="sect1" data-pdf-bookmark="Host Hardening"><div class="sect1" id="host_hardening">
        <h1>Host Hardening</h1>
        <p>A secure host is an important building block for a secure Kubernetes cluster.<a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="host hardening" id="ch02-hh"/><a contenteditable="false" data-type="indexterm" data-primary="host hardening" data-secondary="infrastructure security" id="ch02-hh2"/> When you think of a host, it is in the
            context of workloads that make up your Kubernetes cluster. We will now explore techniques to ensure a strong
            security posture for the host.</p>
        <section data-type="sect2" data-pdf-bookmark="Choice of Operating System"><div class="sect2" id="choice_of_operating_system">
            <h2>Choice of Operating System</h2>
            <p>Many enterprises standardize on a single operating system across<a contenteditable="false" data-type="indexterm" data-primary="operating system selection" data-secondary="host hardening" data-tertiary="infrastructure security" id="idm45326833132112"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="infrastructure security" data-tertiary="host hardening" id="idm45326833130368"/> all of their infrastructure, which means the choice may
                have already been made for you. However, if there is flexibility to choose an operating system, then it
                is worth considering a <a contenteditable="false" data-type="indexterm" data-primary="immutability" data-secondary="Linux distributions" id="idm45326833128352"/>modern immutable Linux distribution specifically
                designed for containers. These distributions are advantageous for the following reasons:</p>
            <ul>
                <li>
                    <p>They often have newer kernels, which include the latest vulnerability fixes as well as up-to-date
                        implementations of <a contenteditable="false" data-type="indexterm" data-primary="eBPF (extended Berkley Packet Filter)" data-secondary="Linux distributions" id="idm45326833125456"/>newer technologies such as eBPF, which can be
                        leveraged by Kubernetes networking and security monitoring tools.</p>
                </li>
                <li>
                    <p>They are designed to be immutable, which brings additional benefits for security.<a contenteditable="false" data-type="indexterm" data-primary="immutability" id="idm45326833122832"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="immutability" id="idm45326833121632"/><a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="operating system" data-tertiary="immutability of" id="idm45326833120256"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="infrastructure security" data-tertiary="immutability" id="idm45326833118608"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="immutability" data-tertiary="infrastructure security" id="idm45326833116960"/> Immutability in this context means that
                        the root filesystem is locked and cannot be changed by applications. Applications can only be
                        installed using containers. This isolates applications from the root filesystem and
                        significantly reduces the ability for malicious applications to compromise the host.</p>
                </li>
                <li>
                    <p>They often include the ability to self-update to newer versions, with the upstream versions being
                        geared up for rapid releases to address security vulnerabilities.</p>
                </li>
            </ul>
            <p>Two popular examples of modern immutable Linux distributions designed for containers <a contenteditable="false" data-type="indexterm" data-primary="Flatcar Container Linux immutability" id="idm45326833113056"/><a contenteditable="false" data-type="indexterm" data-primary="Bottlerocket Linux immutability" id="idm45326833111936"/>are Flatcar Container
                Linux (which was originally based on CoreOS Container Linux) and Bottlerocket (originally created and
                maintained by Amazon).</p>
            <p>Whichever operating system you choose, it is good practice to monitor upstream security announcements so
                you know when new security vulnerabilities are identified and disclosed and to make sure you have
                processes in place to update your cluster to a newer version to address critical security
                vulnerabilities. Based on your assessment of these vulnerabilities, you will want to make a decision on
                whether to upgrade your cluster to a new version of the operating system. When you consider the choice
                of the operating system, you must also take into account shared libraries from the host operating system
                and understand their impact on containers that will be deployed on the host.</p>
            <p>Another security best practice is to ensure that application developers<a contenteditable="false" data-type="indexterm" data-primary="operating system selection" data-secondary="applications not version specific" id="idm45326833108864"/><a contenteditable="false" data-type="indexterm" data-primary="application layer" data-secondary="operating system" data-tertiary="version independence" id="idm45326833107376"/> do not depend on a specific version of the operating
                system or kernel, as this will not allow you to update the host operating system as needed.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Nonessential Processes"><div class="sect2" id="nonessential_processes">
            <h2>Nonessential Processes</h2>
            <p>Each running host process is a potential attack vector for hackers. From a security perspective, it is
                best to remove any nonessential processes that may be running by default. If a process isn’t needed for
                the successful running of Kubernetes, management of your host, or security of your host, then it is best
                not to run the process. How you disable the process will depend on your particular setup (e.g., systemd
                configuration change or removing the initialization script from /etc/init.d/).</p>
            <p>If you are using an immutable Linux distribution optimized for containers,<a contenteditable="false" data-type="indexterm" data-primary="immutability" data-secondary="Linux distributions" data-tertiary="nonessential processes eliminated" id="idm45326833102816"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="immutability" data-tertiary="nonessential processes eliminated" id="idm45326833101056"/> then nonessential processes will have
                already been eliminated and you can only run additional processes/applications as containers.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Host-Based Firewalling"><div class="sect2" id="host_based_firewalling">
            <h2>Host-Based Firewalling</h2>
            <p>To further lock down the servers or VMs Kubernetes is hosted on,<a contenteditable="false" data-type="indexterm" data-primary="host-based firewalls" id="idm45326833097072"/><a contenteditable="false" data-type="indexterm" data-primary="firewalls" data-secondary="host-based" id="idm45326833095888"/> the host
                itself can be configured with local firewall rules to restrict which IP address ranges and ports are
                allowed to interact with the host.</p>
            <p>Depending on your operating system, this can be done with traditional Linux admin tools such as iptables
                rules or firewalld configuration. <a contenteditable="false" data-type="indexterm" data-primary="firewalls" data-secondary="rule compatibility" id="idm45326830747264"/><a contenteditable="false" data-type="indexterm" data-primary="plug-ins" data-secondary="network firewall policies" id="idm45326830745888"/>It is important to make sure any such rules are
                compatible with both the Kubernetes control plane and whichever Kubernetes network plug-in you plan to
                use so they do not block the Kubernetes control plane, pod networking, or the pod network control
                plane. Getting these rules right, and keeping them up to date over time, can be a time-consuming
                process. <a contenteditable="false" data-type="indexterm" data-primary="immutability" data-secondary="Linux distributions" data-tertiary="firewall tools and" id="idm45326830743936"/><a contenteditable="false" data-type="indexterm" data-primary="Linux distributions" data-secondary="immutability" data-tertiary="firewall tools and" id="idm45326830742288"/>In addition, if using an
                immutable Linux distribution, you may not easily be able to directly use these tools.</p>
            <p>Fortunately, some Kubernetes network plug-ins can help solve this problem for you. For example, several
                Kubernetes network plug-ins, like Weave Net, Kube-router, and Calico, include the ability to apply
                network policies. You should review these plug-ins and pick one that also supports applying network
                policies to the hosts themselves (rather than just to Kubernetes pods). This makes securing the hosts in
                the cluster significantly simpler and is largely operating system independent, including working with
                immutable Linux distributions.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Always Research the Latest Best Practices"><div class="sect2" id="always_research_the_latest_best_practic">
            <h2>Always Research the Latest Best Practices</h2>
            <p>As new vulnerabilities or attack vectors are identified by the security research community, security best
                practices evolve over time. Many of these are well-documented online and are available for free.</p>
            <p>For example, the Center for Internet Security maintains free PDF guides<a contenteditable="false" data-type="indexterm" data-primary="host hardening" data-secondary="CIS Benchmarks" id="idm45326830736768"/><a contenteditable="false" data-type="indexterm" data-primary="best practices" data-secondary="Center for Internet Security guides" id="idm45326830735312"/><a contenteditable="false" data-type="indexterm" data-primary="resources online" data-secondary="Center for Internet Security guides" id="idm45326830733968"/><a contenteditable="false" data-type="indexterm" data-primary="online resources" data-see="resources online" id="idm45326830732576"/><a contenteditable="false" data-type="indexterm" data-primary="CIS Benchmarks (Center for Internet Security)" id="idm45326830731200"/><a contenteditable="false" data-type="indexterm" data-primary="CIS Benchmarks (Center for Internet Security)" data-secondary="host hardening" id="idm45326830730128"/> with comprehensive configuration guidance to secure many
                of the most common operating systems. Known as CIS Benchmarks, they are an excellent resource for making
                sure you are covering the many important actions required to secure your host operating system. You can
                find an <a href="https://oreil.ly/dpUnC">up-to-date list of CIS Benchmarks on their website</a>.
                Please note there are Kubernetes-specific benchmarks, and we will discuss them later in this chapter.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch02-hh" id="idm45326830727440"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch02-hh2" id="idm45326830726096"/>
            </p>
        </div></section>
    </div></section>
    <section data-type="sect1" data-pdf-bookmark="Cluster Hardening"><div class="sect1" id="cluster_hardening">
        <h1>Cluster Hardening</h1>
        <p>Kubernetes is insecure by default. So in addition to hardening the hosts that make up a cluster, it is
            important to harden the cluster itself. This can be done through a combination of Kubernetes component and
            configuration management, authentication and role-based access control (RBAC), and keeping the cluster
            updated with the latest versions of Kubernetes to ensure the cluster has the latest vulnerability fixes.</p>
        <section data-type="sect2" data-pdf-bookmark="Secure the Kubernetes Datastore"><div class="sect2" id="secure_the_kubernetes_datastore">
            <h2>Secure the Kubernetes Datastore</h2>
            <p>Kubernetes uses etcd as its main datastore. This is where all cluster<a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="datastore security" id="idm45326830720064"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="datastore security" id="idm45326830718336"/><a contenteditable="false" data-type="indexterm" data-primary="datastore security" id="idm45326830716688"/><a contenteditable="false" data-type="indexterm" data-primary="etcd datastore" data-secondary="cluster hardening" id="idm45326830715584"/> configuration and
                desired state is stored. Having access to the etcd datastore is essentially equivalent to having root
                login on all your nodes. Almost any other security measures you have put in place within the cluster
                become moot if a malicious actor gains access to the etcd datastore. They will have complete control
                over your cluster at that point, including the ability to run arbitrary containers with elevated
                privileges on any node.</p>
            <p>The main way to secure etcd is to use the security features provided by etcd itself. <a contenteditable="false" data-type="indexterm" data-primary="Public Key Infrastructure (PKI)" data-secondary="etcd security" id="idm45326830713120"/>These are based around x509 Public Key Infrastructure
                (PKI), using a combination of keys and certificates. They ensure that all data in transit is encrypted
                with TLS and all access is restricted with strong credentials. It is best to configure etcd with one set
                of credentials (key pairs and certificates) for peer communications between the different etcd
                instances, and another set of credentials for client communications from the Kubernetes API. As part of
                this configuration, <a contenteditable="false" data-type="indexterm" data-primary="certificate authority (CA)" data-secondary="client credentials" id="idm45326830711024"/>etcd must
                also be configured with the details of certificate authority (CA) used to generate the client
                credentials.</p>
            <p>Once etcd is configured correctly, only clients with valid certificates can access it. <a contenteditable="false" data-type="indexterm" data-primary="API server cluster hardening" data-secondary="etcd security" id="idm45326830708928"/>You must then configure the Kubernetes API server with the
                client certificate, key, and certificate authority so it can access etcd.</p>
            <p>You can also use network-level firewall rules to <a contenteditable="false" data-type="indexterm" data-primary="firewalls" data-secondary="etcd security" id="idm45326830706736"/>restrict etcd access so it can
                only be accessed from Kubernetes control nodes (hosting the Kubernetes API server). Depending on your
                environment you can use a traditional firewall, virtual cloud firewall, or rules on the etcd hosts
                themselves (for example, a networking policy implementation that supports host endpoint protection) to
                block traffic. This is best done in addition to using etcd’s own security features as part of an
                in-depth defense strategy, since limiting access with firewall rules does not address the security need
                for Kubernetes’ sensitive data to be encrypted in transit.</p>
            <p class="pagebreak-before">In addition to securing etcd access for Kubernetes, it is recommended to not use the Kubernetes etcd
                datastore for anything other than Kubernetes. In other words, do not store non-Kubernetes data within
                the datastore and do not give other components access to the etcd cluster. If you are running
                applications or infrastructure (within the cluster or external to the cluster) that uses etcd as a
                datastore, the best practice is to set up a separate etcd cluster for that. The arguable exception would
                be if the application or infrastructure was sufficiently privileged that a compromise to its datastore
                would also result in a complete compromise of Kubernetes. It is also very important to maintain backups
                of etcd and secure the backups so that it is possible to recover from failures like a failed upgrade or
                a security incident.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Secure the Kubernetes API Server"><div class="sect2" id="secure_the_kubernetes_api_server">
            <h2>Secure the Kubernetes API Server</h2>
            <p>One layer up from the etcd datastore, the next set of crown jewels to be <a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="API server security" id="idm45326830701264"/><a contenteditable="false" data-type="indexterm" data-primary="API server cluster hardening" id="idm45326830699424"/><a contenteditable="false" data-type="indexterm" data-primary="Public Key Infrastructure (PKI)" data-secondary="API server security" id="idm45326830698256"/><a contenteditable="false" data-type="indexterm" data-primary="PKI" data-see="Public Key Infrastructure" id="idm45326830696864"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="API server security" id="idm45326830695472"/>secured is the Kubernetes API server. As with etcd,
                this can be done using x509 PKI and TLS. The details of how to bootstrap a cluster in this way vary
                depending on the Kubernetes installation method you are using, but most methods include steps that
                create the required keys and certificates and distribute them to the other Kubernetes cluster
                components. It’s worth noting that some installation methods may enable insecure local ports for some
                components, so it is important to familiarize yourself with the settings of each component to identify
                potential unsecured traffic so you can take appropriate action to secure them.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Encrypt Kubernetes Secrets at Rest"><div class="sect2" id="encrypt_kubernetes_secrets_at_rest">
            <h2>Encrypt Kubernetes Secrets at Rest</h2>
            <p>Kubernetes can be configured to encrypt sensitive data it stores in etcd, <a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="secrets encrypted at rest" id="idm45326830690720"/><a contenteditable="false" data-type="indexterm" data-primary="etcd datastore" data-secondary="secrets" id="idm45326830689008"/><a contenteditable="false" data-type="indexterm" data-primary="secrets" data-secondary="etcd datastore storage" id="idm45326830687632"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="secrets encrypted at rest" id="idm45326830686256"/><a contenteditable="false" data-type="indexterm" data-primary="encryption" data-secondary="secrets at rest" id="idm45326830684592"/><a contenteditable="false" data-type="indexterm" data-primary="data encryption" data-secondary="secrets at rest" id="idm45326830683216"/>such as Kubernetes
                secrets. This keeps the secrets safe from any attacker that may gain access to etcd or to an offline
                copy of etcd such as offline backup.</p>
            <p>By default, Kubernetes does not encrypt secrets at rest, and when<a contenteditable="false" data-type="indexterm" data-primary="secrets" data-secondary="management best practices" data-tertiary="encryption" id="idm45326830681088"/><a contenteditable="false" data-type="indexterm" data-primary="secrets" data-secondary="etcd datastore storage" data-tertiary="encryption at rest" id="idm45326830679344"/> encryption is enabled, it only encrypts when a secret
                is written to etcd. Therefore, when enabling encryption at rest, it is important to rewrite all secrets
                (through standard kubectl apply or update commands) to trigger their encryption within etcd.</p>
            <p>Kubernetes supports a variety of encryption providers. It is important to <a contenteditable="false" data-type="indexterm" data-primary="best practices" data-secondary="encryption" id="idm45326830676848"/><a contenteditable="false" data-type="indexterm" data-primary="encryption" data-secondary="best practices" id="idm45326830675376"/><a contenteditable="false" data-type="indexterm" data-primary="AES-CBC with PKCS #7–based encryption" id="idm45326830674000"/><a contenteditable="false" data-type="indexterm" data-primary="data encryption" data-secondary="best practices" id="idm45326830672880"/>pick
                the recommended encryption based on encryption best practices. The mainline recommended choice is
                AES-CBC with PKCS #7–based encryption. This provides very strong encryption using 32-byte keys and is
                relatively fast. There are two different providers that support this encryption:</p>
            <ul class="less_space pagebreak-before">
                <li>
                    <p>The local provider that runs entirely with Kubernetes and uses locally configured keys</p>
                </li>
                <li>
                    <p>The KMS provider that uses an external key management service (KMS) to manage the keys<a contenteditable="false" data-type="indexterm" data-primary="key management services (KMS)" data-secondary="encryption" id="idm45326830668624"/></p>
                </li>
            </ul>
            <p>The local provider stores its keys on the API server’s local disk. This therefore has the limitation that
                if the API server host is compromised, then all of your secrets become compromised. Depending on your
                security posture, this may be acceptable.</p>
            <p>The KMS provider uses a technique called <em>envelope encryption</em>.<a contenteditable="false" data-type="indexterm" data-primary="data encryption" data-secondary="data encryption key" id="idm45326830665008"/><a contenteditable="false" data-type="indexterm" data-primary="encryption" data-secondary="data encryption key" id="idm45326830663600"/><a contenteditable="false" data-type="indexterm" data-primary="encryption" data-secondary="envelope encryption" id="idm45326830662224"/><a contenteditable="false" data-type="indexterm" data-primary="envelope encryption" id="idm45326830660848"/><a contenteditable="false" data-type="indexterm" data-primary="data encryption key (DEK)" id="idm45326830659744"/><a contenteditable="false" data-type="indexterm" data-primary="key encryption key (KEK)" id="idm45326830658672"/><a contenteditable="false" data-type="indexterm" data-primary="data encryption" data-secondary="envelope encryption" id="idm45326830657552"/> With envelope encryption, each secret is encrypted
                with a dynamically generated data encryption key (DEK). The DEK is then encrypted using a key encryption
                key (KEK) provided by the KMS, and the encrypted DEK is stored alongside the encrypted secret in etcd.
                The KEK is always hosted by the KMS as the central root of trust and is never stored within the
                cluster. Most large public cloud providers offer a cloud-based KMS service that can be used as the KMS
                provider in Kubernetes. For on-prem clusters there are third-party solutions, such as HashiCorp’s Vault,
                which can act as the KMS provider for the cluster. Because the detailed implementations vary, it is
                important to evaluate the mechanism through which the KMS authenticates the API server and whether a
                compromise to the API server host could in turn compromise your secrets and therefore offer only
                limited benefits compared with a local encryption provider.</p>
            <p>If exceptionally high volumes of encrypted storage read/writes are anticipated, <a contenteditable="false" data-type="indexterm" data-primary="encryption" data-secondary="secretbox encryption" id="idm45326830654672"/><a contenteditable="false" data-type="indexterm" data-primary="secretbox encryption" id="idm45326830653296"/><a contenteditable="false" data-type="indexterm" data-primary="data encryption" data-secondary="secretbox encryption" id="idm45326830652192"/>then using the
                secretbox encryption provider could potentially be faster. However, secretbox is a newer standard and at
                the time of writing has had less review than other encryption algorithms. It therefore may not be
                considered acceptable in environments that require high levels of review. In addition, secretbox is not
                yet supported by the KMS provider and must use a local provider, which stores the keys on the API
                server.</p>
            <p>Encrypting Kubernetes secrets is the most common must-have encryption at-rest requirement, but note that
                you can also configure Kubernetes to encrypt storage of other Kubernetes resources if desired.</p>
            <p>It’s also worth noting there are third-party secret management solutions that can be used if you have
                requirements beyond the capabilities of Kubernetes secrets. One such solution, already mentioned as a
                potential KMS provider for envelope encryption in Kubernetes, is HashiCorp’s Vault. In addition to
                providing secure secrets management for Kubernetes, Vault can be used beyond the scope of Kubernetes to
                manage secrets more broadly across the enterprise if desired. Vault was also a very popular choice for
                plugging the major gap in earlier Kubernetes versions, which did not support the encryption of secrets
                at rest.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Rotate Credentials Frequently"><div class="sect2" id="rotate_credentials_frequently">
            <h2>Rotate Credentials Frequently</h2>
            <p>Rotating credentials frequently makes it harder for attackers to make<a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="credentials rotated" id="idm45326830646624"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="credentials rotated" id="idm45326830644896"/><a contenteditable="false" data-type="indexterm" data-primary="credential rotation" id="idm45326830643248"/><a contenteditable="false" data-type="indexterm" data-primary="rotation of credentials" id="idm45326830642144"/> use of any compromised credential they may obtain.
                It is therefore a best practice to set short lifetimes on any TLS certificates or other credentials and
                automate their rotation. Most authentication providers can control how long issued certificates or
                service tokens are valid for, and it is best to use short lifetimes whenever possible, for example
                rotating keys every day or more frequently if they are particularly sensitive. This needs to include any
                service tokens used in external integrations or as part of the cluster bootstrap process.</p>
            <p>Fully automating the rotation of credentials may require custom DevOps development work, but normally
                represents a good investment compared with attempting to manually rotate credentials on an ongoing
                basis.</p>
            <p>When rotating keys for Kubernetes secrets stored at rest<a contenteditable="false" data-type="indexterm" data-primary="secrets" data-secondary="management best practices" data-tertiary="rotating secrets" id="idm45326830639184"/> (as discussed in the previous section), local providers
                support multiple keys. The first key is always used to encrypt any new secret writes. For decryption,
                the keys are tried in order until the secret is successfully decrypted. As keys are encrypted only on
                writes, it is important to rewrite all secrets (through standard kubectl apply or update commands) to
                trigger their encryption with the latest keys. If the rotation of secrets is fully automated, then the
                write will happen as part of this process without requiring a separate step.</p>
            <p>When using a KMS provider (rather than a local provider), the KEK <a contenteditable="false" data-type="indexterm" data-primary="key management services (KMS)" data-secondary="rotating keys" id="idm45326830636256"/>can be rotated without requiring reencryption of all the
                secrets, which can reduce the performance impact of reencrypting all secrets if you have a large number
                of sizable secrets.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Authentication and RBAC"><div class="sect2" id="authentication_and_rbac">
            <h2>Authentication and RBAC</h2>
            <p>In the previous sections we primarily focused on securing programmatic/code <a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="authentication and RBAC" id="idm45326830632816"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="authentication and RBAC" id="idm45326830631072"/><a contenteditable="false" data-type="indexterm" data-primary="authentication" data-secondary="cluster hardening" id="idm45326830629424"/><a contenteditable="false" data-type="indexterm" data-primary="role-based access control (RBAC)" data-secondary="cluster hardening" id="idm45326830628048"/>access within the cluster. Equally important is to
                follow best practices for securing user interactions with the cluster. This includes creating separate
                user accounts for each user and using Kubernetes RBAC to grant users the minimal access they need to
                perform their role, following the principle of least privilege access. Usually it is better to do this
                using groups and roles, rather than assigning RBAC permissions to individual users. This makes it easier
                to manage privileges over time, both in terms of adjusting privileges for different groups of users when
                requirements change and for reducing the effort required to periodically review/audit the user
                privileges across the cluster to verify they are correct and up to date.</p>
            <p>Kubernetes has limited built-in authentication capabilities for users, but can be integrated with most
                external enterprise authentication providers, such as public cloud provider IAM systems or on-prem
                authentication services, either directly or through third-party projects such as Dex (originally created
                by CoreOS). It is generally recommended to integrate with one of these external authentication providers
                rather than using Kubernetes basic auth or service account tokens, since external authentication
                providers typically have more user-friendly support for rotation of credentials, including the ability
                to specify password strength and rotation frequency timeframes.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Restricting Cloud Metadata API Access"><div class="sect2" id="restricting_cloud_metadata_api_access">
            <h2>Restricting Cloud Metadata API Access</h2>
            <p>Most public clouds provide a metadata API that is accessible locally <a contenteditable="false" data-type="indexterm" data-primary="cloud environments" data-secondary="metadata API access" id="idm45326830622448"/><a contenteditable="false" data-type="indexterm" data-primary="public clouds" data-see="cloud environments" id="idm45326830620992"/><a contenteditable="false" data-type="indexterm" data-primary="metadata API access" id="idm45326830619616"/><a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="cloud metadata API access" id="idm45326830618512"/>from
                each host/VM instance. The APIs provide access to the instance’s cloud credentials, IAM permissions, and
                other potentially sensitive information about the instance. By default, these APIs are accessible by the
                Kubernetes pods running on an instance. Any compromised pod can use these credentials to elevate its
                intended privilege level within the cluster or to other cloud provider–hosted services the instance may
                have privileges to access.</p>
            <p>To address this security issue, the best practice is to:</p>
            <ul>
                <li>
                    <p>Provide any required pod IAM credentials following<a contenteditable="false" data-type="indexterm" data-primary="pods" data-secondary="IAM credentials" id="idm45326830614784"/><a contenteditable="false" data-type="indexterm" data-primary="IAM credentials for pods" id="idm45326830613312"/><a contenteditable="false" data-type="indexterm" data-primary="managed identities" id="idm45326830612192"/> the cloud provider’s
                        recommended mechanisms. For example, Amazon EKS allows you to assign a unique IAM role to a
                        service account, Microsoft Azure’s AKS allows you to assign a managed identity to a pod, and
                        Google Cloud’s GKE allows you to assign IAM permissions via Workload Identity.</p>
                </li>
                <li>
                    <p>Limit the cloud privileges of each instance to the minimum required to reduce the impact of any
                        compromised access to the metadata API from the instance.</p>
                </li>
                <li>
                    <p>Use network policies to block pod access to the metadata API. This can be done with per-namespace
                        Kubernetes network policies, or <a contenteditable="false" data-type="indexterm" data-primary="Calico Enterprise" data-secondary="network policies" id="idm45326830608480"/>preferably with
                        extensions to Kubernetes network policies such as those offered by Calico, which enable a single
                        network policy to apply across the whole of the cluster (without the need to create a new
                        Kubernetes network policy each time a new namespace is added to the cluster). This topic is
                        covered in more depth in the Default Deny and Default App Policy section of <a data-type="xref" href="ch07.xhtml#network_policy">Chapter 7</a>.</p>
                </li>
            </ul>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Enable Auditing"><div class="sect2" id="enable_auditing">
            <h2>Enable Auditing</h2>
            <p>Kubernetes auditing provides a log of all actions within the cluster<a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="auditing enabled" id="idm45326830603120"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="auditing enabled" id="idm45326830601392"/><a contenteditable="false" data-type="indexterm" data-primary="audit logs" data-secondary="cluster hardening" id="idm45326830599744"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="audit logs" data-tertiary="cluster hardening" id="idm45326830598368"/> with configurable scope and levels of detail. Enabling
                Kubernetes audit logging and archiving audit logs on a secure service is recommended as an important
                source of forensic details in the event of needing to analyze a security breach.</p>
            <p>The forensic review of the audit log can help answer questions such as:</p>
            <ul>
                <li>
                    <p>What happened, when, and where in the cluster?</p>
                </li>
                <li>
                    <p>Who or what initiated it and from where?</p>
                </li>
            </ul>
            <p>In addition, Kubernetes audit logs can be actively monitored to alert on suspicious activity using your
                own custom tooling or third-party solutions. There are many enterprise products that you can use to
                monitor Kubernetes audit logs and generate alerts based on configurable match criteria.</p>
            <p>The details of what events are captured in Kubernetes audit logs are controlled using policy. The policy
                determines which events are recorded, for which resources, and with what level of detail.</p>
            <p>Each action being performed can generate a series of events, defined as stages:</p>
            <dl>
                <dt>RequestReceived</dt>
                <dd>Generated as soon as the audit handler receives the request</dd>
                <dt>ResponseStarted</dt>
                <dd>Generated when the response headers are sent, but before the response body is sent (generated only
                    for long-running requests such as watches)</dd>
                <dt>ResponseComplete</dt>
                <dd>Generated when the response body has been completed</dd>
                <dt>Panic</dt>
                <dd>Generated when a panic occurs</dd>
            </dl>
            <p>The level of detail recorded for each event can be one of the following:</p>
            <dl>
                <dt>None</dt>
                <dd>Does not log the event at all</dd>
                <dt>Metadata</dt>
                <dd>Logs the request metadata (user, timestamp, resource, verb, etc.) but not the request details or
                    response body</dd>
                <dt>Request</dt>
                <dd>Logs event metadata and the request body but not the response body</dd>
                <dt>RequestResponse</dt>
                <dd>Logs the full details of the event, including the metadata and request and response bodies</dd>
            </dl>
            <p>Kubernetes’ audit policy is very flexible and well documented in the main Kubernetes documentation.
                Included here are just a couple of simple examples to illustrate.</p>
            <p>To log all requests at the metadata level:</p>
            <pre data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">audit.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Policy</code>
<code class="nt">rules</code><code class="p">:</code>
<code class="p-Indicator">-</code> <code class="nt">level</code><code class="p">:</code> <code class="l-Scalar-Plain">Metadata</code></pre>
            <p>To omit the <code>RequestReceived</code> stage and to log pod changes at the <code>RequestResponse</code>
                level and configmap and secrets changes at the metadata level:</p>
            <pre data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">audit.k8s.io/v1</code> <code class="c1"># This is required.</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Policy</code>
<code class="nt">omitStages</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="s">"RequestReceived"</code>
<code class="nt">rules</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">level</code><code class="p">:</code> <code class="l-Scalar-Plain">RequestResponse</code>
    <code class="nt">resources</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">group</code><code class="p">:</code> <code class="s">""</code>
      <code class="nt">resources</code><code class="p">:</code> <code class="p-Indicator">[</code><code class="s">"pods"</code><code class="p-Indicator">]</code>
  <code class="p-Indicator">-</code> <code class="nt">level</code><code class="p">:</code> <code class="l-Scalar-Plain">Metadata</code>
    <code class="nt">resources</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">group</code><code class="p">:</code> <code class="s">""</code> <code class="c1"># core API group</code>
      <code class="nt">resources</code><code class="p">:</code> <code class="p-Indicator">[</code><code class="s">"secrets"</code><code class="p-Indicator">,</code> <code class="s">"configmaps"</code><code class="p-Indicator">]</code></pre>
            <p>This second example illustrates the important consideration of sensitive data<a contenteditable="false" data-type="indexterm" data-primary="audit logs" data-secondary="sensitive data in" id="idm45326833545088"/><a contenteditable="false" data-type="indexterm" data-primary="logs" data-secondary="audit logs" data-tertiary="sensitive data in" id="idm45326833356096"/><a contenteditable="false" data-type="indexterm" data-primary="secrets" data-secondary="management best practices" data-tertiary="audit logs" id="idm45326833354480"/><a contenteditable="false" data-type="indexterm" data-primary="sensitive data in audit logs" id="idm45326833352864"/> in audit logs. Depending on the level of
                security around access to the audit logs, it may be essential to ensure the audit policy does not log
                details of secrets or other sensitive data. Just like the practice of encrypting secrets at rest, it is
                generally a good practice to always exclude sensitive details from your audit log.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Restrict Access to Alpha or Beta Features"><div class="sect2" id="restrict_access_to_alpha_or_beta_featur">
            <h2>Restrict Access to Alpha or Beta Features</h2>
            <p>Each Kubernetes release includes alpha and beta features. Whether <a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="alpha and beta features" id="idm45326833349472"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="alpha and beta features" id="idm45326833347824"/><a contenteditable="false" data-type="indexterm" data-primary="alpha feature assessment" id="idm45326833346176"/><a contenteditable="false" data-type="indexterm" data-primary="beta feature assessment" id="idm45326833345056"/>these are enabled can be controlled by specifying
                feature gate flags for the individual Kubernetes components. As these features are in development, they
                can have limitations or bugs that result in security vulnerabilities. So it is a good practice to make
                sure that all alpha and beta features you do not intend to use are disabled.</p>
            <p>Alpha features are normally (but not always) disabled by default. They might be buggy, and the support
                for the feature could radically change without backward compatibility, or be dropped in future releases.
                They are generally recommended for testing clusters only, not production clusters.</p>
            <p>Beta features are normally enabled by default. They can still change in non-backward-compatible ways
                between releases, but they are usually considered reasonably well tested and safe to enable. But as with
                any new feature, they are inherently more likely to have vulnerabilities because they have been used
                less and had less review.</p>
            <p>Always assess the value an alpha or beta feature may provide against the security risk it represents, as
                well as the potential ops risk of non-backward-compatible changes of the features between releases.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Upgrade Kubernetes Frequently"><div class="sect2" id="upgrade_kubernetes_frequently">
            <h2>Upgrade Kubernetes Frequently</h2>
            <p>It is inevitable that new vulnerabilities will be discovered over time<a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="cluster hardening" data-tertiary="upgrading Kubernetes" id="idm45326833339360"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="upgrading Kubernetes" id="idm45326833337712"/><a contenteditable="false" data-type="indexterm" data-primary="upgrades" data-secondary="Kubernetes cluster hardening" id="idm45326833336064"/><a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="upgrades for cluster hardening" id="idm45326833334624"/> in any large software project. The
                Kubernetes developer community has a good track record of responding to newly discovered vulnerabilities
                in a timely manner. Severe vulnerabilities are normally fixed under embargo, meaning that the knowledge
                of the vulnerability is not made public until the developers have had time to produce a fix. Over time,
                the number of publicly known vulnerabilities for older Kubernetes versions grows, which can put older
                clusters at greater security risk.</p>
            <p>To reduce the risk of your clusters being compromised, it is important to regularly upgrade your
                clusters, and to have in place the ability to urgently upgrade if a severe vulnerability is discovered.
            </p>
            <p>All Kubernetes security updates and vulnerabilities are reported (once any embargo ends) via the public
                and free-to-join <em>kubernetes-announce</em> email group. Joining this group is highly recommended for
                anyone wanting to keep track of known vulnerabilities so they can minimize their security exposure.</p>
        </div></section>
    </div></section>
    <section data-type="sect1" data-pdf-bookmark="Use a Managed Kubernetes Service"><div class="sect1" id="use_a_managed_kubernetes_service">
        <h1>Use a Managed Kubernetes Service</h1>
        <p>One way to reduce the effort required to act on all of the advice<a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="managed Kubernetes services" id="idm45326833328944"/><a contenteditable="false" data-type="indexterm" data-primary="managed Kubernetes services" data-secondary="infrastructure security" id="idm45326833327552"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="managed Kubernetes services" id="idm45326833326160"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="shared responsibility model" id="idm45326833324768"/><a contenteditable="false" data-type="indexterm" data-primary="shared responsibility model for security" id="idm45326833323376"/><a contenteditable="false" data-type="indexterm" data-primary="cloud environments" data-secondary="managed Kubernetes services" data-tertiary="infrastructure security" id="idm45326833322240"/> in this chapter is to use one of the major public
            cloud managed Kubernetes services, such as EKS, AKS, GKE, or IKS. Using one of these services moves security
            from being 100% your own responsibility to being a shared responsibility model. Shared responsibility means
            there are many elements of security that the service includes by default, or can be easily configured to
            support, but that there are elements you still have to take responsibility for yourself in order to make the
            whole truly secure.</p>
        <p>The details vary depending on which public cloud service you are using, but there’s no doubt that all of them
            do significant heavy lifting, which reduces the effort required to secure the cluster compared to if you are
            installing and managing the cluster yourself. In addition, there are plenty of resources available from the
            public cloud providers and from third parties that detail what you need to do as part of the shared
            responsibility model to fully secure your cluster. For example, one of these resources is CIS Benchmarks, as
            discussed next.</p>
        <section data-type="sect2" data-pdf-bookmark="CIS Benchmarks"><div class="sect2" id="cis_benchmarks">
            <h2>CIS Benchmarks</h2>
            <p>As discussed earlier in this chapter, CIS <a contenteditable="false" data-type="indexterm" data-primary="host hardening" data-secondary="CIS Benchmarks" id="idm45326833316928"/><a contenteditable="false" data-type="indexterm" data-primary="CIS Benchmarks (Center for Internet Security)" data-secondary="managed Kubernetes services" id="idm45326833315552"/><a contenteditable="false" data-type="indexterm" data-primary="CIS Benchmarks (Center for Internet Security)" data-secondary="host hardening" id="idm45326833314064"/><a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="CIS Benchmarks" id="idm45326833312656"/>maintains free PDF
                guides with comprehensive configuration guidance to secure many of the most common operating systems.
                These guides, known as CIS Benchmarks, can be an invaluable resource to help you with host hardening.
            </p>
            <p>In addition to helping with host hardening, there are also CIS Benchmarks for Kubernetes itself,
                including configuration guidance for many of the popular managed Kubernetes services, which can help you
                implement much of the guidance in this chapter. For example, the GKE CIS Benchmark includes guidance on
                ensuring the cluster is configured with autoupgrade of nodes, and for using managing Kubernetes
                authentication and RBAC with Google Groups. These guides are highly recommended resources to keep up to
                date with the latest practical advice on the steps required to secure Kubernetes clusters.</p>
            <p>In addition to the guides themselves, there are third-party tools available that can assess the status of
                a running cluster against many of these benchmarks. One popular tool is kube-bench, an open source
                project originally created by the team at Aqua Security. Or if you prefer a more packaged solution, then
                many enterprise products have CIS Benchmark and other security compliance tooling and alerting built
                into their cluster management dashboards. Having these kinds of tools in place, ideally running
                automatically at regular intervals, can be valuable for verifying the security posture of a cluster and
                ensuring that careful security measures that might have been put in place at cluster creation time are
                not accidentally lost or compromised as the cluster is managed and updated over time.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Network Security"><div class="sect2" id="network_security">
            <h2>Network Security</h2>
            <p>When securely integrating the cluster with the surrounding infrastructure<a contenteditable="false" data-type="indexterm" data-primary="infrastructure security" data-secondary="network security" id="ch02-net"/><a contenteditable="false" data-type="indexterm" data-primary="networking" data-secondary="infrastructure security" id="ch02-net2"/><a contenteditable="false" data-type="indexterm" data-primary="clusters" data-secondary="cluster hardening" data-tertiary="network security" id="ch02-net3"/> outside the scope of the cluster, network
                security is the primary consideration. There are two aspects to consider: how to protect the cluster
                from attack from outside the cluster and how to protect the infrastructure outside of the cluster from
                any compromised element inside the cluster. This applies at both the cluster workload level (i.e.,
                Kubernetes pods) and the cluster infrastructure level (i.e., the Kubernetes control plane and the hosts
                on which the Kubernetes cluster is running).</p>
            <p>The first thing to consider is whether or not the cluster needs to be accessible from the public
                internet, either directly (e.g., one or more nodes have public IP addresses) or indirectly (e.g., via a
                load balancer or similar that is reachable from the internet). If the cluster is accessible from the
                internet, then the number of attacks or probes from hackers is massively increased. So if there is not a
                strong requirement for the cluster to be accessible from the internet, then it is highly recommended to
                not allow any access at a routability level (i.e., ensuring there is no way of packets getting from the
                internet to the cluster). In an enterprise on-prem environment, this may equate to the choice of IP
                address ranges to use for the cluster and their routability within the enterprise network. If using a
                public cloud managed Kubernetes service, you may find these settings can be set only at cluster
                creation. For example, in GKE, whether the Kubernetes control plane is accessible from the internet can
                be set at cluster <span class="keep-together">creation.</span></p>
            <p>Network policy within the cluster is the next line of defense. Network policy can be used to restrict
                both workload and host communications to/from the cluster and the infrastructure outside of the cluster.
                It has the strong advantage of being workload aware (i.e., the ability to limit communication of groups
                of pods that make up an individual microservices) and being platform agnostic (i.e., the same techniques
                and policy language can be used in any environment, whether on-prem within the enterprise or in public
                cloud). Network policy is discussed in depth later in a dedicated chapter.</p>
            <p>Finally, it is highly recommended to use perimeter firewalls, or their cloud equivalents<a contenteditable="false" data-type="indexterm" data-primary="firewalls" data-secondary="cluster hardening" id="idm45326833296688"/> such as security groups, to restrict traffic to/from
                the cluster. In most cases these are not Kubernetes workload aware, so they don’t understand individual
                pods and are therefore usually limited in granularity to treating the whole of the cluster as a single
                entity. Even with this limitation they add value as part of a defense strategy, though on their own they
                are unlikely to be sufficient for any security-conscious enterprise.</p>
            <p>If stronger perimeter defense is desired, there are strategies and third-party tools that can make perimeter firewalls or their cloud equivalents more effective:</p>
            <ul>
                <li>
                    <p>One approach is to designate a small number of specific nodes<a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="firewall rules" id="idm45326833293024"/>
                        in the cluster as having a particular level of access to the rest of the network, which is not
                        granted to the rest of the nodes in the cluster. Kubernetes taints can then be used to ensure
                        that only workloads that need that special level of access are scheduled to those nodes. This
                        way perimeter firewall rules can be set based on the IP addresses of the specific nodes to allow
                        desired access, and all other nodes in the cluster are denied access outside the cluster.</p>
                </li>
                <li>
                    <p>In an on-prem environment, some Kubernetes network plug-ins <a contenteditable="false" data-type="indexterm" data-primary="plug-ins" data-secondary="IP addresses in firewall rules" id="idm45326833290048"/>allow you to use routable pod IP
                        addresses (nonoverlay networks) and control the IP address ranges that the group of pods backing
                        a particular microservice use. This allows perimeter firewalls to act on IP address ranges in a
                        similar way as they do with traditional non-Kubernetes workloads. For example, you need to pick
                        a network plug-in that supports nonoverlay networks on-prem that are routable across the broader
                        enterprise networks and that has flexible IP address management capabilities to facilitate such
                        an approach.</p>
                </li>
                <li>
                    <p>A variation of the previous is useful in any environment where it is not practical to make pod IP
                        addresses routable outside of the cluster (e.g., when using an overlay network). In this
                        scenario, the network traffic from pods appears to come from the<a contenteditable="false" data-type="indexterm" data-primary="IP addresses" data-secondary="source network address translation" id="idm45326833286480"/><a contenteditable="false" data-type="indexterm" data-primary="source network address translation (SNAT)" data-secondary="firewall IP address rules" id="idm45326833285136"/><a contenteditable="false" data-type="indexterm" data-primary="egress gateways" id="idm45326833283776"/> IP address of the node as it
                        uses source network address translation (SNAT). In order to address this, you can use a
                        Kubernetes network plug-in that supports fine-grained control of egress NAT gateways. The egress
                        NAT gateway feature supported by some Kubernetes network plug-ins allows this behavior to be
                        changed so that the egress traffic for a set of pods is routed via specific gateways within the
                        cluster that perform the SNAT, so the traffic appears to be <span class="keep-together">coming</span> from the gateway, rather than
                        from the node hosting the pod. Depending on the network plug-in being used, the gateways can be
                        allocated to specific IP address ranges or to specific nodes, which in turn allows perimeter
                        firewall rules to act more selectively than treating the whole of the cluster as a single
                        entity. There are a few options that support this functionality: Red Hat’s OpenShift SDN,
                        Nirmata, and Calico all support egress gateways.<a contenteditable="false" data-type="indexterm" data-primary="egress gateways" data-secondary="tools that support" id="idm45326833280656"/><a contenteditable="false" data-type="indexterm" data-primary="Red Hat OpenShift SDN egress gateways" id="idm45326833093280"/><a contenteditable="false" data-type="indexterm" data-primary="Nirmata egress gateways" id="idm45326833092160"/><a contenteditable="false" data-type="indexterm" data-primary="Calico Enterprise" data-secondary="egress gateways" id="idm45326833091056"/></p>
                </li>
                <li><p>Finally, some firewalls support some plug-ins or third-party tools that allow the firewall to be
                        more aware of Kubernetes workloads, for example, by automatically populating IP address lists
                        within the firewall with pod IP addresses (or node IP addresses of the nodes hosting particular
                        pods). Or in a cloud environment, there are automatic programming rules that allow security
                        groups to selectively act on traffic to/from Kubernetes pods, rather than operating only at the
                        node level. This integration is very important for your cluster to help complement the security
                        provided by firewalls. There are several tools in the market that allow this type of
                        integration. It is important to choose a tool that supports these integrations with major
                        firewall vendors and is native to Kubernetes.</p>
                </li>
            </ul>
            <p>In the previous section we discussed the importance of network security and how you can use network
                security to secure access to your Kubernetes cluster from traffic originating outside the cluster and
                also how to control access for traffic originating from within the cluster destined to hosts outside the
                cluster.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch02-net" id="idm45326833087056"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch02-net2" id="idm45326833085680"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch02-net3" id="idm45326833084304"/></p>
        </div></section>
    </div></section>
    <section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="conclusion-id000007">
        <h1>Conclusion</h1>
        <p>In this chapter we discussed the following key concepts, which you should use to ensure you have a secure
            infrastructure for your Kubernetes cluster.</p>
        <ul>
            <li>
                <p>You need to ensure that the host is running an operating system that is secure and free from critical
                    vulnerabilities.</p>
            </li>
            <li>
                <p>You need to deploy access controls on the host to control access to the host operating system and
                    deploy controls for network traffic to and from the host.</p>
            </li>
            <li>
                <p>You need to ensure a secure configuration for your Kubernetes cluster; securing the datastore and API
                    server are key to ensuring you have a secure cluster <span class="keep-together">configuration.</span></p>
            </li>
            <li>
                <p>Finally, you need to deploy network security to control network traffic that originates from pods in
                    the cluster and is destined to pods in the cluster.</p>
            </li>
        </ul>
    </div></section>
</div></section></div></body></html>