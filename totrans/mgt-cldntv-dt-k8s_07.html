<html><head></head><body><section data-pdf-bookmark="Chapter 6. Integrating Data Infrastructure in a Kubernetes Stack" data-type="chapter" epub:type="chapter"><div class="chapter" id="integrating_data_infrastructure_in_a_ku">&#13;
<h1><span class="label">Chapter 6. </span>Integrating Data Infrastructure <span class="keep-together">in a Kubernetes Stack</span></h1>&#13;
<p><a contenteditable="false" data-primary="data" data-secondary="infrastructure of" data-type="indexterm" id="data_ch"/><a contenteditable="false" data-primary="Kubernetes stacks, integrating data infrastructure in" data-type="indexterm" id="ks_ch"/>In this book, we are illuminating a future of modern, cloud native applications that run on Kubernetes. Up until this point, we’ve noted that historically, data has been one of the hardest parts of making this a reality. In previous chapters, we’ve introduced the primitives Kubernetes provides for managing compute, network, and storage (<a data-type="xref" href="ch02.html#managing_data_storage_on_kubernetes">Chapter 2</a>) resources, and considered how databases (<a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a>) can be deployed on Kubernetes using these resources. We’ve also examined the automation of infrastructure using controllers and the operator pattern (<a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a>).</p>&#13;
<p>Now let’s expand our focus to consider how data infrastructure fits into your <span class="keep-together">overall application</span> architecture in Kubernetes. In this chapter, we’ll explore how to assemble the building blocks discussed in previous chapters into integrated <span class="keep-together">data infrastructure</span> stacks that are easy to deploy and tailor to the unique needs of each application. These stacks represent a step toward the vision of the virtual datacenter we <span class="keep-together">introduced</span> in <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>. To learn the considerations involved in building and using these larger assemblies, let’s take an in-depth look at <a href="https://k8ssandra.io">K8ssandra</a>. This open source project provides an integrated data stack based on Apache Cassandra, a database we first discussed in <a data-type="xref" href="ch03.html#running_apache_cassandra_on_kubernetes">“Running Apache Cassandra on Kubernetes”</a>.</p>&#13;
<section data-pdf-bookmark="K8ssandra: Production-Ready Cassandra on Kubernetes" data-type="sect1"><div class="sect1" id="keightssandra_production_ready_cassandr">&#13;
<h1>K8ssandra: Production-Ready Cassandra on Kubernetes</h1>&#13;
<p><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="production-ready" data-type="indexterm" id="ac_pr"/><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="about" data-type="indexterm" id="idm46183196868080"/>To set the context, let’s consider some of the practical challenges of moving <span class="keep-together">application</span> workloads into Kubernetes. As organizations have begun to migrate existing applications to Kubernetes and create new cloud native applications <span class="keep-together">in Kubernetes,</span> modernizing the data tier is a step that is often deferred. Whatever the causes <span class="keep-together">of these</span> delays—the belief that Kubernetes is not ready for stateful workloads, a lack of <span class="keep-together">development</span> resources, or other factors—the result has been mismatched <span class="keep-together">architectures</span> in which applications are running in Kubernetes with databases and other data <span class="keep-together">infrastructure</span> running externally. This leads to a division of focus for developers and SREs that can limit productivity. It’s also common to see distinct toolsets for monitoring applications and database infrastructure, which increases cloud computing costs.</p>&#13;
<p>This adoption challenge became evident in the Cassandra community. Despite the growing collaboration and consensus around building a single Cassandra operator as discussed in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>, developers were still confronted with key questions about how the database and operator would fit in the larger application context:</p>&#13;
<ul>&#13;
<li><p>How can you have an integrated view of the health of your entire stack, including both applications and data?</p></li>&#13;
<li><p>How can you tailor the automation of installation, upgrades, and other operational tasks in a Kubernetes native way that fits the way we manage your <span class="keep-together">Datacenters?</span></p></li>&#13;
</ul>&#13;
<p>To help address these questions, John Sanda and a team of engineers at DataStax launched an open source project called K8ssandra with the goal of providing <span class="keep-together">a production-ready</span> deployment of Cassandra that embodies best practices for <span class="keep-together">running</span> Cassandra in Kubernetes. K8ssandra provides custom resources that help manage tasks including cluster deployment, upgrades, scaling up and down, data backup and restore, and more. You can read more about the motivations for the project in Jeff Carpenter’s blog post <a href="https://oreil.ly/dB6mJ">“Why K8ssandra?”</a>.</p>&#13;
<section data-pdf-bookmark="K8ssandra Architecture" data-type="sect2"><div class="sect2" id="keightssandra_architectur">&#13;
<h2>K8ssandra Architecture</h2>&#13;
<p><a contenteditable="false" data-primary="architecture" data-secondary="K8ssandra" data-type="indexterm" id="idm46183196931776"/><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="architecture" data-type="indexterm" id="idm46183197195296"/>K8ssandra is deployed in units known as <em>clusters</em>, which is similar terminology to that used by Kubernetes and Cassandra. A K8ssandra cluster includes a Cassandra cluster along with additional components depicted in <a data-type="xref" href="#keightssandra_architecture">Figure 6-1</a> to provide a full data management ecosystem. Let’s consider these in roughly clockwise order starting from the top center:</p>&#13;
<dl>&#13;
<dt>Cass Operator</dt>&#13;
<dd><a contenteditable="false" data-primary="Cass Operator" data-secondary="about" data-type="indexterm" id="idm46183196958480"/>A Kubernetes operator first introduced in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>. It manages the lifecycle of Cassandra nodes on Kubernetes, including provisioning new nodes and storage, and scaling up and down.</dd>&#13;
<dt>Cassandra Reaper</dt>&#13;
<dd><a contenteditable="false" data-primary="Cassandra Reaper" data-type="indexterm" id="idm46183196862496"/>This manages the details of repairing Cassandra nodes in order to maintain high data consistency.</dd>&#13;
</dl>&#13;
&#13;
<dl class="pagebreak-before">&#13;
<dt class="less_space">Cassandra Medusa</dt>&#13;
<dd><a contenteditable="false" data-primary="Cassandra Medusa" data-secondary="about" data-type="indexterm" id="idm46183196845488"/>Provides backup and restore for data stored in Cassandra.</dd>&#13;
<dt>Prometheus and Grafana</dt>&#13;
<dd><a contenteditable="false" data-primary="Prometheus" data-type="indexterm" id="idm46183196844336"/><a contenteditable="false" data-primary="Grafana" data-type="indexterm" id="idm46183196842768"/>Used for the collection and visualization of metrics.</dd>&#13;
<dt>Stargate</dt>&#13;
<dd><a contenteditable="false" data-primary="Stargate" data-type="indexterm" id="idm46183196840640"/>A data gateway that provides API access to client applications as an alternative to CQL.</dd>&#13;
<dt>K8ssandra Operator</dt>&#13;
<dd><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="about" data-type="indexterm" id="idm46183196839248"/>Orchestrates all of the other components, including multicluster support for managing Cassandra clusters that span multiple Kubernetes clusters.</dd>&#13;
</dl>&#13;
<figure><div class="figure" id="keightssandra_architecture">&#13;
<img alt="K8ssandra architecture" src="assets/mcdk_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>K8ssandra architecture</h6>&#13;
</div></figure>&#13;
<p>In the following sections, we’ll take a look at each component of the K8ssandra project to understand the role that it plays within the architecture and its relationship to other components.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Installing the K8ssandra Operator" data-type="sect2"><div class="sect2" id="installing_the_keightssandra_operator">&#13;
<h2>Installing the K8ssandra Operator</h2>&#13;
<p><a contenteditable="false" data-primary="installing" data-secondary="K8ssandra Operator" data-type="indexterm" id="ins_kas"/><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="installing" data-type="indexterm" id="kas_ins"/>Let’s dive in with some hands-on experience of installing K8ssandra. To get a basic installation of K8ssandra running that fully demonstrates the power of the operator, you’ll need a Kubernetes cluster with several Worker Nodes.</p> &#13;
&#13;
<p class="pagebreak-before">To make the deployment simpler, the K8ssandra team has provided scripts to <span class="keep-together">automate</span> the process of creating Kubernetes clusters and then deploying the operator to these clusters. These scripts use <a href="https://kind.sigs.k8s.io">kind clusters</a> for simplicity, so you’ll want to make sure you have this installed before starting.</p> &#13;
&#13;
<p>Instructions for installing on various clouds are also available on the K8ssandra website. The instructions we provide here are based on an <a href="https://oreil.ly/4z2oH">installation guide</a> in the K8ssandra Operator <a href="https://oreil.ly/kgeWY">repository</a>.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>K8ssandra 2.0 Release Status</h1>&#13;
<p>This chapter focuses on the K8ssandra 2.0 release, including the K8ssandra Operator. At the time of writing, K8ssandra 2.0 is still <span class="keep-together">in beta</span> status. As K8ssandra 2.0 moves toward a full GA release, <span class="keep-together">the instructions</span> on the <a href="https://oreil.ly/nT1n5">“Get Started” section of the K8ssandra website</a> will be updated to reference the new version.</p>&#13;
</div>&#13;
<p>First, start by cloning the K8ssandra operator repository from GitHub:</p>&#13;
<pre data-type="programlisting"><strong>git clone https://github.com/k8ssandra/k8ssandra-operator.git</strong></pre>&#13;
<p>Next, you’ll want to use the provided Makefile to create a Kubernetes cluster and deploy the K8ssandra Operator into it (this assumes you have <code>make</code> installed):</p>&#13;
<pre data-type="programlisting"><strong>cd k8ssandra-operator</strong>&#13;
<strong>make single-up</strong></pre>&#13;
<p>If you examine the Makefile, you’ll notice the operator is installed using Kustomize, which we discussed in <a data-type="xref" href="ch04.html#additional_deployment_tools_kustomize_a">“Additional Deployment Tools: Kustomize and Skaffold”</a>. The target you just executed creates a kind cluster with four Worker Nodes and changes your current context to point to that cluster, as you can see by running the following:</p>&#13;
<pre data-type="programlisting"><strong>% kubectl config current-context</strong>&#13;
kind-k8ssandra-0&#13;
<strong>% kubectl get nodes</strong>&#13;
NAME                        STATUS   ROLES                  AGE     VERSION&#13;
k8ssandra-0-control-plane   Ready    control-plane,master   6m45s   v1.22.4&#13;
k8ssandra-0-worker          Ready    &lt;none&gt;                 6m13s   v1.22.4&#13;
k8ssandra-0-worker2         Ready    &lt;none&gt;                 6m13s   v1.22.4&#13;
k8ssandra-0-worker3         Ready    &lt;none&gt;                 6m13s   v1.22.4&#13;
k8ssandra-0-worker4         Ready    &lt;none&gt;                 6m13s   v1.22.4</pre>&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="CRD (Custom Resource Definition)" data-type="indexterm" id="idm46183196817296"/>Now examine the list of CRDs that have been created:</p>&#13;
<pre data-type="programlisting"><strong>% kubectl get crd</strong>&#13;
NAME                                          CREATED AT&#13;
cassandrabackups.medusa.k8ssandra.io          2022-02-05T17:31:35Z&#13;
cassandradatacenters.cassandra.datastax.com   2022-02-05T17:31:35Z&#13;
cassandrarestores.medusa.k8ssandra.io         2022-02-05T17:31:35Z&#13;
cassandratasks.control.k8ssandra.io           2022-02-05T17:31:36Z&#13;
certificaterequests.cert-manager.io           2022-02-05T17:31:16Z&#13;
certificates.cert-manager.io                  2022-02-05T17:31:16Z&#13;
challenges.acme.cert-manager.io               2022-02-05T17:31:16Z&#13;
clientconfigs.config.k8ssandra.io             2022-02-05T17:31:36Z&#13;
clusterissuers.cert-manager.io                2022-02-05T17:31:17Z&#13;
issuers.cert-manager.io                       2022-02-05T17:31:17Z&#13;
k8ssandraclusters.k8ssandra.io                2022-02-05T17:31:36Z&#13;
orders.acme.cert-manager.io                   2022-02-05T17:31:17Z&#13;
reapers.reaper.k8ssandra.io                   2022-02-05T17:31:36Z&#13;
replicatedsecrets.replication.k8ssandra.io    2022-02-05T17:31:36Z&#13;
stargates.stargate.k8ssandra.io               2022-02-05T17:31:36Z</pre>&#13;
<p><a contenteditable="false" data-primary="kubectl apt-resources command" data-type="indexterm" id="idm46183196813712"/>As you can see, several CRDs are associated with the cert-manager and K8ssandra. There is also the CassandraDatacenter CRD used by Cass Operator. The K8ssandra and Cass Operator CRDs are all Namespaced, which you can verify using the <code>kubectl api-resources</code> command, meaning that resources created according to these definitions are assigned to a specific Namespace. That command will also show you the acceptable abbreviations for each resource type (for example, <code>k8c</code> for <code>k8ssandracluster</code>).</p>&#13;
<p>Next, you can examine the contents that have been installed within the kind cluster. If you list the Namespaces using <code>kubectl get ns</code>, you’ll note two new Namespaces: <code>cert-manager</code> and <code>k8ssandra-operator</code>. As you may suspect, K8ssandra is using the same cert-manager project as Pulsar, as described in  <a data-type="xref" href="ch08.html#securing_communications_by_default_with">“Securing Communications by Default with cert-manager”</a>. Let’s examine the contents of the <code>k8ssandra-operator</code> Namespace, which are summarized in <a data-type="xref" href="#keightssandra_operator_architecture">Figure 6-2</a> along with related K8ssandra CRDs.</p>&#13;
&#13;
	&#13;
&#13;
<p>Examine the workloads and you’ll notice that two Deployments have been created: one for the K8ssandra Operator and one for Cass Operator. Take a look at the K8ssandra Operator source code, and you’ll see that it contains multiple controllers, while the Cass Operator consists of a single controller. This packaging reflects the fact that Cass Operator is an independent project which can be used by itself without having to adopt the entire K8ssandra framework—otherwise, it could have been included as a controller within the K8ssandra Operator.</p>&#13;
&#13;
<figure><div class="figure" id="keightssandra_operator_architecture">&#13;
<img alt="K8ssandra Operator architecture" src="assets/mcdk_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>K8ssandra Operator architecture</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
<p><a data-type="xref" href="#mapping_keightssandra_crds_to_controlle">Table 6-1</a> describes the mapping of these various controllers to the key resources with which they interact<a contenteditable="false" data-primary="Cassandra Medusa" data-secondary="about" data-type="indexterm" id="idm46183196803584"/><a contenteditable="false" data-primary="Cassandra Reaper" data-type="indexterm" id="idm46183196790384"/><a contenteditable="false" data-primary="replication controller" data-type="indexterm" id="idm46183196789616"/><a contenteditable="false" data-primary="Stargate" data-type="indexterm" id="idm46183196788848"/><a contenteditable="false" data-primary="Cass Operator" data-secondary="about" data-type="indexterm" id="idm46183196788080"/>.</p>&#13;
<table class="border" id="mapping_keightssandra_crds_to_controlle">&#13;
<caption><span class="label">Table 6-1. </span>Mapping K8ssandra CRDs to controllers</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Operator</th>&#13;
<th>Controller</th>&#13;
<th>Key custom resources</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td rowspan="5">K8ssandra Operator</td>&#13;
<td>K8ssandra controller</td>&#13;
<td><code>K8ssandraCluster</code>, <code>CassandraDatacenter</code></td>&#13;
</tr>&#13;
<tr>&#13;
<td>Medusa controller</td>&#13;
<td><code>CassandraBackup</code>, <code>CassandraRestore</code></td>&#13;
</tr>&#13;
<tr>&#13;
<td>Reaper controller</td>&#13;
<td><code>Reaper</code></td>&#13;
</tr>&#13;
<tr>&#13;
<td>Replication controller</td>&#13;
<td><code>ClientConfig</code>, <code>ReplicatedSecret</code></td>&#13;
</tr>&#13;
<tr>&#13;
<td>Stargate controller</td>&#13;
<td><code>Stargate</code></td>&#13;
</tr>&#13;
<tr>&#13;
<td>Cass Operator</td>&#13;
<td>Cass Operator controller manager</td>&#13;
<td><code>CassandraDatacenter</code></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p><a contenteditable="false" data-primary="" data-startref="ins_kas" data-type="indexterm" id="idm46183196771040"/><a contenteditable="false" data-primary="" data-startref="kas_ins" data-type="indexterm" id="idm46183196769600"/>We’ll introduce each K8ssandra and Cass Operator CRD in more detail in the followiing sections:</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Creating a K8ssandraCluster" data-type="sect2"><div class="sect2" id="creating_a_keightssandracluster">&#13;
<h2>Creating a K8ssandraCluster</h2>&#13;
<p><a contenteditable="false" data-primary="K8ssandraCluster" data-primary-sortas="kassandra" data-type="indexterm" id="idm46183196767968"/>Once you’ve installed the K8ssandra Operator, the next step is to create a K8ssandraCluster. The source code used in this section is available in the <a href="https://oreil.ly/1n3k7">“Vitess Operator Example” section of the book’s repository</a>, based on samples available in the <a href="https://oreil.ly/5WxRO">K8ssandra Operator GitHub repo</a>. First, have a look at the <em>k8ssandra-cluster.yaml</em> file:</p>&#13;
<pre data-type="programlisting">apiVersion: k8ssandra.io/v1alpha1&#13;
kind: K8ssandraCluster&#13;
metadata:&#13;
  name: demo&#13;
spec:&#13;
  cassandra:&#13;
    cluster: demo&#13;
    serverVersion: "4.0.1"&#13;
    datacenters:&#13;
      - metadata:&#13;
          name: dc1&#13;
        size: 3&#13;
        storageConfig:&#13;
          cassandraDataVolumeClaimSpec:&#13;
            storageClassName: standard&#13;
            accessModes:&#13;
              - ReadWriteOnce&#13;
            resources:&#13;
              requests:&#13;
                storage: 1Gi&#13;
        config:&#13;
          jvmOptions:&#13;
            heapSize: 512M&#13;
        stargate:&#13;
          size: 1&#13;
          heapSize: 256M</pre>&#13;
<p>This code specifies a K8ssandraCluster resource consisting of a single Datacenter <code>dc1</code> running three nodes of Cassandra 4.0.1, where the Pod specification for each Cassandra node requests 1 GB of storage using a PersistentVolumeClaim that references the <code>standard</code> StorageClass. This configuration also includes a single Stargate node to provide API access to the Cassandra cluster. This is a minimal configuration that accepts the chart defaults for most of the other components. Create the <code>demo</code> K8ssandraCluster in the <code>k8ssandra-operator</code> Namespace with this command:</p>&#13;
<pre data-type="programlisting"><strong>% kubectl apply -f k8ssandra-cluster.yaml -n k8ssandra-operator</strong>&#13;
k8ssandracluster.k8ssandra.io/demo created</pre>&#13;
<p>Once the command completes, you can check on the installation of the K8ssandraCluster using commands such as <code>kubectl get k8ssandraclusters</code> (or <code>kubectl get k8c</code> for short). <a data-type="xref" href="#a_simple_keightssandracluster">Figure 6-3</a> depicts some of the key compute, network, and storage resources that the operator built on your behalf when you created the <code>demo</code> K8ssandraCluster.</p>&#13;
<figure><div class="figure" id="a_simple_keightssandracluster">&#13;
<img alt="A simple K8ssandraCluster" src="assets/mcdk_0603.png"/>&#13;
<h6><span class="label">Figure 6-3. </span>A simple K8ssandraCluster</h6>&#13;
</div></figure>&#13;
<p>Here are some key items to note:</p>&#13;
<ul>&#13;
<li><p>A single StatefulSet has been created to represent the Cassandra Datacenter <code>dc1</code>, with three Pods containing the replicas you specified. As you’ll learn in the next section, K8ssandra uses a CassandraDatacenter CRD to manage this StatefulSet via the Cass Operator.</p></li>&#13;
<li><p>While the figure shows a single Service <code>demo-dc1-service</code> exposing access to the Cassandra cluster as a single endpoint, this is a simplification. You will find multiple Services configured to provide access for various clients.</p></li>&#13;
<li><p>There is a Deployment managing a single Stargate Pod, as well as Services that provide client endpoints to the various API services provided by Stargate. This is another simplification, and we’ll explore this part of configuration in more detail in <a data-type="xref" href="#enabling_developer_productivity_with_st">“Enabling Developer Productivity with Stargate APIs”</a>.</p></li>&#13;
<li><p>Similar to examples of infrastructure we’ve shown in previous chapters, the K8ssandra Operator also creates additional supporting security resources such as ServiceAccounts, Roles, and RoleBindings.</p></li>&#13;
</ul>&#13;
<p>Once you have a K8ssandraCluster created, you can point client applications at the Cassandra interfaces and Stargate APIs, and perform cluster maintenance operations. You can remove a K8ssandraCluster just by deleting its resource, but you won’t want to do that yet as we have a lot more to explore! We’ll describe several of these interactions as we examine each of the K8ssandra components in more detail. Along the way, we’ll make sure to note some of the interesting design choices made by contributors to K8ssandra and related projects in terms of how they use Kubernetes resources and how they adapt data infrastructure that predates Kubernetes into the Kubernetes way of doing things.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>StackGres: An Integrated Kubernetes Stack for Postgres</h1>&#13;
<p><a contenteditable="false" data-primary="StackGres" data-type="indexterm" id="idm46183196879072"/><a contenteditable="false" data-primary="OnGres" data-type="indexterm" id="idm46183196879424"/><a contenteditable="false" data-primary="Patroni" data-type="indexterm" id="idm46183197472560"/><a contenteditable="false" data-primary="Prometheus" data-type="indexterm" id="idm46183197026944"/><a contenteditable="false" data-primary="Grafana" data-type="indexterm" id="idm46183197025840"/><a contenteditable="false" data-primary="" data-startref="ac_pr" data-type="indexterm" id="idm46183196746752"/>The K8ssandra project is not the only instance of an integrated data stack that runs on Kubernetes. Another example can be found in <a href="https://stackgres.io">StackGres</a>, a project managed by OnGres. StackGres uses <a href="https://github.com/zalando/patroni">Patroni</a> to support clustered, highly available Postgres deployments and adds automated backup functionality. StackGres supports integration with Prometheus and Grafana for metrics aggregation and visualization, along with an optional Envoy proxy for getting more fine-grained metrics at the protocol level. StackGres is composed of open source components and uses the <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">AGPLv3 License</a> for its community edition.</p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Managing Cassandra in Kubernetes with Cass Operator" data-type="sect1"><div class="sect1" id="managing_cassandra_in_kubernetes_with_c">&#13;
<h1>Managing Cassandra in Kubernetes with Cass Operator</h1>&#13;
<p><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="managing with Cass Operator" data-type="indexterm" id="ac_cass"/><a contenteditable="false" data-primary="Cass Operator" data-secondary="managing Apache Cassandra with" data-type="indexterm" id="cass_man"/><a contenteditable="false" data-primary="DataStax" data-type="indexterm" id="idm46183196737376"/><em>Cass Operator</em> is the shorthand name for the DataStax Kubernetes Operator for Apache Cassandra. This open source project available on <a href="https://oreil.ly/xWjZr">GitHub</a> was brought under the umbrella of the K8ssandra project in 2021, replacing its previous home under the <a href="https://oreil.ly/JAF3Y">DataStax GitHub organization</a>.</p>&#13;
<p>Cass Operator is a key part of K8ssandra, since a Cassandra cluster is the basic data infrastructure around which all the other infrastructure elements and tools are added. However, Cass Operator was developed before K8ssandra and will continue to exist as a separately deployable project. This is helpful since not every capability of Cass Operator is exposed via K8ssandra, especially more advanced Cassandra configuration options. Cass Operator is listed as its own project in <a href="https://oreil.ly/gPtl3">Operator Hub</a> and can be installed via Kustomize.</p>&#13;
<p>Cass Operator provides a mapping of Cassandra’s topology concepts including clusters, Datacenters, racks, and nodes onto Kubernetes resources. The key construct is the CassandraDatacenter CRD, which represents a Datacenter within the topology of a Cassandra cluster. (Reference <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a> if you need a refresher on Cassandra topology.)</p>&#13;
<p>When you created a K8ssandraCluster resource in the previous section, the K8ssandra Operator created a single CassandraDatacenter resource, which would have looked something like this:</p>&#13;
<pre data-type="programlisting">apiVersion: cassandra.datastax.com/v1beta1&#13;
kind: CassandraDatacenter&#13;
metadata:&#13;
  name: dc1&#13;
spec:&#13;
  clusterName: demo&#13;
  serverType: cassandra&#13;
  serverVersion: 4.0.1&#13;
  size: 3&#13;
  racks:&#13;
  - name: default</pre>&#13;
<p>Since you didn’t specify a rack in the K8ssandraCluster definition, K8ssandra interprets this as a single rack named <code>default</code>. By creating the CassandraDatacenter, K8ssandra Operator is delegating the operation of the Cassandra nodes in this Datacenter to Cass Operator.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Cass Operator and Multiple Datacenters</h1>&#13;
<p><a contenteditable="false" data-primary="datacenters" data-type="indexterm" id="idm46183196821840"/><a contenteditable="false" data-primary="multiclusters" data-secondary="topologies for" data-type="indexterm" id="idm46183197892672"/>You may be wondering why Cass Operator does not define a CRD representing a Cassandra cluster. From the perspective of the Cass Operator, the Cassandra cluster is basically just a piece of metadata—the CassandraDatacenter’s <code>clusterName</code>—rather than an actual resource. This reflects the convention that Cassandra clusters used in production systems are typically deployed across multiple physical datacenters, which is beyond the scope of a Kubernetes cluster.</p>&#13;
<p>While you can certainly create multiple CassandraDatacenters and link them together using the same <code>clusterName</code>, they must be in the same Kuberneters cluster for Cass Operator to be able to manage them. It’s also recommended to use a separate Namespace to install a dedicated instance of Cass Operator to manage each cluster. You’ll see how K8ssandra supports the ability to create Cassandra clusters that span multiple physical datacenters (and Kubernetes clusters) in multicluster topologies.</p>&#13;
</div>&#13;
<p>When Cass Operator is notified by the API server of the creation of the CassandraDatacenter resource, it creates resources used to implement the datacenter, including a StatefulSet to manage the nodes in each rack, as well as various Services and security-related resources. The StatefulSet will start the requested number of Pods in parallel. This brings up a situation in which Cass Operator provides logic to adapt between how Cassandra and Kubernetes operate.</p>&#13;
<p>If you have worked with Cassandra previously, you may be aware that the best practice for adding nodes to a cluster is to do so one one at a time, to simplify the process of a node joining the cluster. This process, called <em>bootstrapping</em>, includes the step of negotiating which data the node will be responsible for, and may include streaming data from other nodes to the new node. However, since the StatefulSet is not aware of these constraints, how can adding multiple nodes to a new or existing cluster one at a time be accomplished?</p>&#13;
<p>The answer lies in the composition of the Pod specification that Cass Operator passes to the StatefulSet, which is then used to create each Cassandra node, as shown in <a data-type="xref" href="#cass_operator_interactions_with_cassand">Figure 6-4</a>.</p>&#13;
<figure><div class="figure" id="cass_operator_interactions_with_cassand">&#13;
<img alt="Cass Operator interactions with Cassandra Pods" src="assets/mcdk_0604.png"/>&#13;
<h6><span class="label">Figure 6-4. </span>Cass Operator interactions with Cassandra Pods</h6>&#13;
</div></figure>&#13;
<p>Cass Operator deploys a custom image of Cassandra in each Cassandra Pod that it manages. The Pod specification includes at least two containers: an init container called <code>server-config-init</code> and a Cassandra container called <code>cassandra</code>.</p>&#13;
<p>As an init container, <code>server-config-init</code> is started before the Cassandra container. It’s responsible for generating the <em>cassandra.yaml</em> configuration file based on the selected configuration options for the CassandraDatacenter. You can specify additional configuration values using the <code>config</code> section of the CassandraDatacenter resource, as described in the <a href="https://oreil.ly/SlN0F">K8ssandra documentation</a>.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Additional Sidecar Containers in Cassandra Pods</h1>&#13;
<p><a contenteditable="false" data-primary="containers" data-secondary="sidecar" data-type="indexterm" id="idm46183196718832"/><a contenteditable="false" data-primary="sidecar containers" data-type="indexterm" id="idm46183196717536"/>As you’ll learn in the following sections, the Cassandra Pod may have additional sidecar containers when deployed in a K8ssandraCluster, depending on which of the additional K8ssandra components you have enabled. For right now, though, we are focusing on the most basic installation.</p>&#13;
</div>&#13;
<p>The <code>cassandra</code> container actually contains two separate processes: the daemon that runs the Cassandra instance and a Management API. This goes somewhat against the traditional best practice of running a single process per container, but there is a good reason for this exception.</p>&#13;
<p>Cassandra’s management interface is exposed via the Java Management Extensions (JMX). While this was a legitimate design choice for a Java-based application like Cassandra when the project was just starting out, JMX has fallen out of favor because of its complexity and security issues. While there has been some progress toward an alternate management interface for Cassandra, the work is not yet complete, so the developers of Cass Operator decided to integrate another open source project, the <a href="https://oreil.ly/1XIPi">Management API for Apache Cassandra</a>.</p>&#13;
<p><a contenteditable="false" data-primary="JMX (Java Management Extensions)" data-type="indexterm" id="idm46183197310096"/>The Management API project provides a RESTful API that translates HTTP-based invocations into calls on Cassandra’s legacy JMX interface. By running the Management API inside the Cassandra container, we avoid having to expose the JMX port outside of the Cassandra containers. This is an instance of a pattern frequently used in cloud native architectures to adapt custom protocols into HTTP-based interfaces, for which there is much better support for routing and security in ingress controllers.</p>&#13;
<p>Cass Operator discovers and connects to the Management API on each Cassandra Pod in order to perform management operations that are not related to Kubernetes. When adding new nodes, this involves the simple action of using the Management API to verify that the node is up and running successfully and updating the CassandraDatacenter’s status accordingly. This sequence is described in more detail in the <a href="https://oreil.ly/T7io1">K8ssandra documentation</a>.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Customizing the Cassandra Image Used by Cass Operator</h1>&#13;
<p><a contenteditable="false" data-primary="Docker Hub" data-type="indexterm" id="idm46183197771088"/><a contenteditable="false" data-primary="Cass Operator" data-secondary="customizing Cassandra images used by" data-type="indexterm" id="idm46183197772592"/>The Management API project provides images for recent Cassandra versions in the 3.<em>x</em> and 4.<em>x</em> series, which are available on <a href="https://oreil.ly/FQa3q">Docker Hub</a>. While it is possible to override the Cassandra image that Cass Operator uses with one of your own, Cass Operator does require that the Management API is available on each Cassandra Pod. If you need to build your own custom image including the Management API, you could use the Dockerfiles and supporting scripts from the <a href="https://oreil.ly/GKRP1">GitHub repository</a> as a starting point.</p>&#13;
</div>&#13;
<p><a contenteditable="false" data-primary="Cass Operator" data-secondary="features for" data-type="indexterm" id="idm46183197003344"/>While this section focused largely on the startup and scaling of Cassandra clusters just described, Cass Operator provides several features for deploying and managing Cassandra clusters:</p>&#13;
<dl>&#13;
<dt>Topology management</dt>&#13;
<dd><a contenteditable="false" data-primary="topology management, as a feature of Cass Operator" data-type="indexterm" id="idm46183196696816"/>Cass Operator uses Kubernetes affinity principles to manage the placement of Cassandra nodes (Pods) across Kubernetes Worker Nodes to maximize availability of your data.</dd>&#13;
<dt>Scaling down</dt>&#13;
<dd><a contenteditable="false" data-primary="scalability" data-secondary="as a feature of Cass Operator" data-type="indexterm" id="idm46183196723280"/>Just as nodes are added one at a time to scale up, Cass Operator manages scaling down one node at a time.</dd>&#13;
<dt>Replacing nodes</dt>&#13;
<dd><a contenteditable="false" data-primary="node replacement, as a feature of Cass Operator" data-type="indexterm" id="idm46183196691872"/>If a Cassandra node is lost because it crashes or the Worker Node on which it is running goes down, Cass Operator relies on the StatefulSet to replace the node and bind the new node to the appropriate PersistentVolumeClaim.</dd>&#13;
<dt>Upgrading images</dt>&#13;
<dd><a contenteditable="false" data-primary="image upgrades, as a feature of Cass Operator" data-type="indexterm" id="idm46183197051344"/>Cass Operator also leverages the capabilities of StatefulSet to perform rolling upgrades of the images used by the Cassandra Pods.</dd>&#13;
<dt>Managing seed nodes</dt>&#13;
<dd><a contenteditable="false" data-primary="seed nodes, as a feature of Cass Operator" data-type="indexterm" id="idm46183196735344"/>Cass Operator creates Kubernetes Services to expose the seed nodes in each Datacenter according to Cassandra’s recommended conventions of one seed node per rack, for a minimum of three per Datacenter.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="" data-startref="ac_cass" data-type="indexterm" id="idm46183197475024"/><a contenteditable="false" data-primary="" data-startref="cass-man" data-type="indexterm" id="idm46183197346592"/>You can read about these and other features in the <a href="https://oreil.ly/UafkF">Cass Operator documentation</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Enabling Developer Productivity with Stargate APIs" data-type="sect1"><div class="sect1" id="enabling_developer_productivity_with_st">&#13;
<h1>Enabling Developer Productivity with Stargate APIs</h1>&#13;
<p><a contenteditable="false" data-primary="APIs (application programming interfaces)" data-type="indexterm" id="api_abo"/><a contenteditable="false" data-primary="developers" data-type="indexterm" id="dev_ab"/><a contenteditable="false" data-primary="Stargate" data-type="indexterm" id="star_ab"/>Our focus so far in this book has been primarily on deployment of data infrastructure such as databases in Kubernetes, more than on the way that infrastructure is used in cloud native applications. The usage of <a href="https://stargate.io">Stargate</a> in K8ssandra gives us a good opportunity to have that discussion.</p>&#13;
<p>In many organizations, there is an ongoing conversation about the pros and cons of direct application access to databases versus abstracting the details of database interactions. This debate occurs especially frequently in larger organizations that divide responsibilities between application development teams and teams that manage platforms including data infrastructure. However, it can also be observed in organizations that employ modern practices including DevOps and microservice architectures, where each microservice may have a different data store behind it.</p>&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="DAOs (data access objects)" data-type="indexterm" id="idm46183197249632"/>The idea of providing abstractions over direct database access has taken many forms over the years. Even in the days of monolithic client-server applications, it was common to use stored procedures or isolate data access and complex query logic behind object-relational mapping tools such as Hibernate, or to use patterns like data access objects (DAOs).</p>&#13;
<p><a contenteditable="false" data-primary="SOA (service oriented architecture)" data-type="indexterm" id="idm46183196674576"/><a contenteditable="false" data-primary="Carpenter, Jeff" data-secondary="“Data Services for the Masses”" data-secondary-sortas="data" data-type="indexterm" id="idm46183197274304"/><a contenteditable="false" data-primary="“Data Services for the Masses” (Carpenter)" data-primary-sortas="data" data-type="indexterm" id="idm46183197492672"/><a contenteditable="false" data-primary="CRUD (create, read, update, and delete) operations" data-type="indexterm" id="idm46183196669088"/>More recently, as the software industry has moved toward service oriented architecture (SOA) and microservices, similar patterns for abstracting data access have appeared. As described in Jeff’s article <a href="https://oreil.ly/u6K58">“Data Services for the Masses”</a>, many teams have found themselves creating a layer of microservices in their architecture dedicated to data access, providing create, read, update, and delete (CRUD) operations on specific data types or entities. These services abstract the details of interacting with a specific database backend, and if well executed and maintained, can help increase developer productivity and facilitate migration to a different database when needed.</p>&#13;
<p><a contenteditable="false" data-primary="data API gateway" data-type="indexterm" id="idm46183196680416"/>The Stargate project was born out of the realization that multiple teams were building very similar abstraction layers to provide data access via APIs. The goal of the Stargate project is to provide an open source <em>data API gateway—</em>a common set of APIs for data access to help eliminate the need for teams to develop and maintain their own custom API layers. While the initial implementation of Stargate is based on Cassandra, the goal of the project is to support multiple database backends, and even other types of data infrastructure such as caches and streaming.</p>&#13;
<p><a contenteditable="false" data-primary="API layer" data-type="indexterm" id="idm46183196667760"/><a contenteditable="false" data-primary="REST API" data-type="indexterm" id="idm46183196668112"/><a contenteditable="false" data-primary="Document API" data-type="indexterm" id="idm46183196904064"/><a contenteditable="false" data-primary="GraphQL API" data-type="indexterm" id="idm46183196660688"/><a contenteditable="false" data-primary="gRPC API" data-type="indexterm" id="idm46183196659712"/><a contenteditable="false" data-primary="routing layer" data-type="indexterm" id="idm46183196658640"/><a contenteditable="false" data-primary="coordination layer" data-type="indexterm" id="idm46183196657536"/><a contenteditable="false" data-primary="storage layer" data-type="indexterm" id="idm46183196656432"/>With Cassandra used as the backend data store, the Stargate architecture can be described as having three layers, as shown in <a data-type="xref" href="#stargate_conceptual_architecture_with_c">Figure 6-5</a>.</p> &#13;
&#13;
<figure><div class="figure" id="stargate_conceptual_architecture_with_c">&#13;
<img alt="Stargate conceptual architecture with Cassandra" src="assets/mcdk_0605.png"/>&#13;
<h6><span class="label">Figure 6-5. </span>Stargate conceptual architecture with Cassandra</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
<p class="pagebreak-before">The <em>API layer</em> is the outermost layer, consisting of services that implement various APIs on top of the underlying Cassandra cluster. Available APIs include a <a href="https://oreil.ly/qTEY6">REST API</a>, a <a href="https://oreil.ly/ekhlV">Document API</a> that provides access to JSON documents over HTTP, a <a href="https://oreil.ly/BescX">GraphQL API</a>, and a <a href="https://oreil.ly/k2fNY">gRPC API</a>. The <em>routing layer</em> (or <em>coordination layer</em>) consists of a set of nodes that act as Cassandra nodes, but perform only routing of queries, not data storage. The <em>storage layer</em> consists of a traditional Cassandra cluster, which can currently be Cassandra 3.11, Cassandra 4.0, or DataStax Enterprise 6.8.</p>&#13;
&#13;
<p>One of the key benefits of this architecture is that it recognizes the separation of concerns for managing usage of compute and storage resources and provides the ability to scale this usage independently based on the needs of client applications:</p>&#13;
<ul>&#13;
<li><p>The number of storage nodes can be scaled up or down to provide the storage capacity required by the application.</p></li>&#13;
<li><p>The number of coordinator nodes and API instances can be scaled up or down to match the application’s read and write load and optimize throughput.</p></li>&#13;
<li><p>APIs that are not used by the application can be scaled to zero (disabled) to reduce resource consumption.</p></li>&#13;
</ul>&#13;
<p>K8ssandra supports the provision of Stargate on top of an underlying Cassandra cluster via the Stargate CRD. The CassandraDatacenter deployed by Cass Operator serves as the storage layer, and the Stargate CRD specifies the configuration of the routing and API layers. An example configuration is shown in <a data-type="xref" href="#stargate_deployment_on_kubernetes">Figure 6-6</a>.</p>&#13;
<figure><div class="figure" id="stargate_deployment_on_kubernetes">&#13;
<img alt="Stargate deployment on Kubernetes" src="assets/mcdk_0606.png"/>&#13;
<h6><span class="label">Figure 6-6. </span>Stargate deployment on Kubernetes</h6>&#13;
</div></figure>&#13;
<p>The installation includes a Deployment to manage the coordinator nodes, and a Service to provide access to the Bridge API, a private gRPC interface exposed on the coordinator nodes that can be used to create new API implementations. See the <a href="https://oreil.ly/6ct5m">Stargate v2 design</a> for more details on the Bridge API. There is also a Deployment for each of the APIs that is enabled in the installation, along with a Service to provide access to client applications.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="api_abo" data-type="indexterm" id="idm46183196654704"/><a contenteditable="false" data-primary="" data-startref="dev_ab" data-type="indexterm" id="idm46183196697664"/><a contenteditable="false" data-primary="" data-startref="star_ab" data-type="indexterm" id="idm46183196634800"/>As you can see, the Stargate project provides a promising framework for extending your data infrastructure with developer-friendly APIs that can scale along with the underlying database.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Unified Monitoring Infrastructure with Prometheus and Grafana" data-type="sect1"><div class="sect1" id="unified_monitoring_infrastructure_with">&#13;
<h1>Unified Monitoring Infrastructure with <span class="keep-together">Prometheus and Grafana</span></h1>&#13;
<p><a contenteditable="false" data-primary="Grafana" data-type="indexterm" id="graf_ab"/><a contenteditable="false" data-primary="Prometheus" data-type="indexterm" id="prom_ab"/>Now that we’ve considered the addition of infrastructure that makes life easier for application developers, let’s look at some of the more operations-focused aspects of integrating data infrastructure in a Kubernetes stack. We’ll start with monitoring.</p>&#13;
<p>Observability is a key attribute of any application deployed on Kubernetes, since it has implications for your awareness of its availability, performance, and cost. Your goal should be to have an integrated view across both your application and the infrastructure it depends on. Observability is often described as consisting of three types of data: metrics, logs, and tracing. Kubernetes itself provides capabilities for logging as well as associating events with resources, and you’ve already learned how the Cass Operator facilitates the collection of logs from Cassandra nodes.</p>&#13;
<p><a contenteditable="false" data-primary="PromQL (Prometheus Query Language)" data-type="indexterm" id="idm46183196833328"/><a contenteditable="false" data-primary="Alertmanager" data-type="indexterm" id="idm46183196623152"/>In this section, we’ll focus on how K8ssandra incorporates the Prometheus/Grafana stack, which provides metrics. <em>Prometheus</em> is a popular open source monitoring platform. It supports a variety of interfaces for collecting data from applications and services and stores them in a time series database which can be queried efficiently using the Prometheus Query Language (PromQL). It also includes an Alertmanager which generates alerts and other notifications based on metric thresholds.</p>&#13;
<p>While previous releases of K8ssandra in the 1.<em>x</em> series incorporated the Prometheus stack as part of a K8ssandra, K8ssandra 2.<em>x</em> provides the capability to integrate with an existing Prometheus installation.</p>&#13;
<p><a contenteditable="false" data-primary="kube-prometheus" data-type="indexterm" id="idm46183196742096"/><a contenteditable="false" data-primary="Prometheus Operator" data-type="indexterm" id="idm46183196741040"/>One easy way to install the Prometheus Operator is to use <a href="https://oreil.ly/tzDmJ">kube-prometheus</a>, a repository provided as part of the Prometheus Operator project. Kube-prometheus is intended as a comprehensive monitoring stack for Kubernetes including the control plane and applications. You can clone this repository and use the library of manifests (YAML files) that it contains to install the integrated stack of components shown in <a data-type="xref" href="#components_of_the_kube_prometheus_stack">Figure 6-7</a>.</p>&#13;
<figure class="width-95"><div class="figure" id="components_of_the_kube_prometheus_stack">&#13;
<img alt="Components of the kube-prometheus stack" src="assets/mcdk_0607.png"/>&#13;
<h6><span class="label">Figure 6-7. </span>Components of the kube-prometheus stack</h6>&#13;
</div></figure>&#13;
<p>These components include the following:</p>&#13;
<dl>&#13;
<dt>Prometheus Operator</dt>&#13;
<dd>The operator, which is set apart in the figure, manages the other components.</dd>&#13;
<dt>Prometheus</dt>&#13;
<dd>The metrics database is run in a high-availability configuration managed via a StatefulSet. Prometheus stores data using a time series database with a backing PersistentVolume.</dd>&#13;
<dt>Node exporter</dt>&#13;
<dd><a contenteditable="false" data-primary="node exporter" data-type="indexterm" id="idm46183196695008"/>The <a href="https://oreil.ly/Yt0YO">node exporter</a> runs on each Kubernetes Worker Node, allowing Prometheus to pull operating system metrics via HTTP.</dd>&#13;
<dt>Client library</dt>&#13;
<dd><a contenteditable="false" data-primary="client library" data-type="indexterm" id="idm46183196602224"/>Applications can embed a Prometheus client library, which allows Prometheus to pull metrics via HTTP.</dd>&#13;
<dt>Alert manager</dt>&#13;
<dd>This can be configured to generate alerts based on thresholds for specific metrics for delivery via email or third-party tools such as PagerDuty. The kube-prometheus stack comes with built-in alerts for the Kubernetes cluster; application-specific alerts can also be added.</dd>&#13;
<dt>Grafana</dt>&#13;
<dd>This is deployed to provide charts that are used to display metrics to human operators. Grafana uses PromQL to access metrics from Prometheus, and this interface is available to other clients as well.</dd>&#13;
</dl>&#13;
<p>While not shown in the figure, the stack also includes the <a href="https://oreil.ly/g033n">Prometheus Adapter for Kubernetes Metrics APIs</a>, an optional component that exposes metrics collected by Prometheus to the Kubernetes control plane so that they can be used to auto-scale applications.</p>&#13;
<p>Connecting K8ssandra with Prometheus can be accomplished in a few quick steps. The <a href="https://oreil.ly/UOt4t">instructions</a> in the K8ssandra documentation walk you through installing the Prometheus Operator using kube-prometheus if you do not have it already. Since kube-prometheus installs Prometheus Operator in its own Namespace, you’ll want to make sure the operator has permissions to manage resources in other Namespaces.</p>&#13;
<p>To integrate K8ssandra with Prometheus, you set attributes on your K8ssandraCluster resource to enable monitoring on Cassandra and Stargate nodes. For example, you could do something like the following to enable monitoring for nodes in all Datacenters in the cluster:</p>&#13;
<pre data-type="programlisting">apiVersion: k8ssandra.io/v1alpha1&#13;
kind: K8ssandraCluster&#13;
metadata:&#13;
  name: demo&#13;
spec:&#13;
  cassandra:&#13;
    datacenters:&#13;
      ...&#13;
    telemetry:&#13;
      prometheus:&#13;
        enabled: true&#13;
  stargate:&#13;
    telemetry:&#13;
      prometheus:&#13;
        enabled: true</pre>&#13;
<p>It’s also possible to selectively enable monitoring on individual datacenters.</p>&#13;
<p><a contenteditable="false" data-primary="MCAC (Metrics Collector for Apache Cassandra)" data-type="indexterm" id="idm46183196597168"/>Let’s take a look at how the integration works. First, let’s consider how the Cassandra nodes expose metrics. As discussed in <a data-type="xref" href="#managing_cassandra_in_kubernetes_with_c">“Managing Cassandra in Kubernetes with Cass Operator”</a>, Cassandra exposes management capabilities via JMX, and this includes metrics reporting. The <a href="https://oreil.ly/CHNMQ">Metric Collector for Apache Cassandra (MCAC)</a> is an open source project that exposes metrics so that they can be accessed by Prometheus or other backends that use the Prometheus protocol via HTTP. K8ssandra and Cass Operator use a Cassandra Docker image that includes MCAC as well as the Management API as additional processes that run in the Cassandra container. This configuration is shown on the left side of <a data-type="xref" href="#monitoring_cassandra_with_kube_promethe">Figure 6-8</a>.</p>&#13;
<figure><div class="figure" id="monitoring_cassandra_with_kube_promethe">&#13;
<img alt="Monitoring Cassandra with kube-prometheus stack" src="assets/mcdk_0608.png"/>&#13;
<h6><span class="label">Figure 6-8. </span>Monitoring Cassandra with the kube-prometheus stack</h6>&#13;
</div></figure>&#13;
<p>The right side of <a data-type="xref" href="#monitoring_cassandra_with_kube_promethe">Figure 6-8</a> shows how Prometheus and Grafana are configured to consume and expose the Cassandra metrics. The K8ssandra Operator creates <span class="keep-together">ServiceMonitor</span> resources for each CassandraDatacenter for which monitoring has been enabled. The ServiceMonitor, a CRD defined by the Prometheus Operator, contains configuration details describing how to collect metrics from a set of Pods, including the following:</p>&#13;
<ul>&#13;
<li><p>A <code>selector</code> referencing the name of a label which identifies the Pods</p></li>&#13;
<li><p>Connection information such as the <code>scheme</code> (protocol), <code>port</code>, and <code>path</code> to use to gather metrics from each Pod</p></li>&#13;
<li><p>The <code>interval</code> at which metrics should be pulled</p></li>&#13;
<li><p>Optional <code>metricRelabelings</code>, which are instructions that indicate any desired renaming of metrics, or even indicate metrics that should be dropped and not ingested by Prometheus</p></li>&#13;
</ul>&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="ServiceMonitor" data-type="indexterm" id="idm46183196811808"/>K8ssandra creates separate ServiceMonitor instances for Cassandra and Stargate nodes, since the metrics exposed are slightly different. To observe the ServiceMonitors deployed in your cluster, you can execute a command such as <code>kubectl get servicemonitors -n monitoring</code>.</p>&#13;
<p>Prometheus provides access to its metrics to Grafana and other tools via a PromQL endpoint exposed as a Kubernetes service. The kube-prometheus installation configures an instance of Grafana to connect to Prometheus using an instance of the Grafana Datasource CRD. Grafana accepts dashboards defined using YAML files, which you can provide as ConfigMaps. See the K8ssandra <a href="https://oreil.ly/vCfmR">documentation</a> for guidance on loading dashboard definitions that display Cassandra and Stargate metrics. You may also wish to create dashboards that display your application metrics alongside the data tier metrics provided by K8ssandra for an integrated view of application performance.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="graf_ab" data-type="indexterm" id="idm46183196585632"/><a contenteditable="false" data-primary="" data-startref="prom_ab" data-type="indexterm" id="idm46183196640480"/>As you can see, kube-prometheus provides a comprehensive and extensible monitoring stack for Kubernetes clusters, much as K8ssandra provides a stack for data management. The integration of K8ssandra with kube-prometheus is a great example of how you can assemble integrated stacks of Kubernetes resources to form even more powerful applications.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Performing Repairs with Cassandra Reaper" data-type="sect1"><div class="sect1" id="performing_repairs_with_cassandra_reape">&#13;
<h1>Performing Repairs with Cassandra Reaper</h1>&#13;
<p><a contenteditable="false" data-primary="repairs" data-type="indexterm" id="idm46183196676352"/><a contenteditable="false" data-primary="CAP theorem" data-type="indexterm" id="idm46183196565712"/><a contenteditable="false" data-primary="Cassandra Reaper" data-type="indexterm" id="idm46183196564608"/><a contenteditable="false" data-primary="eventual consistency" data-type="indexterm" id="idm46183196563504"/>As a NoSQL database, Cassandra emphasizes high performance (especially for writes) and high availability by default. If you’re familiar with the CAP theorem, you’ll understand that this means that sometimes Cassandra will temporarily sacrifice consistency of data across nodes in order to deliver this high performance and high availability at scale, an approach known as <em>eventual consistency</em>. Cassandra does provide the ability to tune the amount of consistency to your needs via options for specifying replication strategies and the consistency level required per query. Users and administrators should be aware of these options and their behavior in order to use Cassandra effectively.</p>&#13;
<p><a contenteditable="false" data-primary="anti-entropy mechanisms" data-type="indexterm" id="idm46183196896240"/>Cassandra has multiple built-in “anti-entropy” mechanisms such as hinted handoff and repair that help maintain consistency of data between nodes over time. Repair is a background process by which a node compares a portion of the data it owns with the latest contents of other nodes that are also responsible for that data. While these checks can be somewhat optimized through the use of checksums, repair can still be a performance-intensive process and is best performed when a cluster is under reduced or off-peak load. Combined with the fact that multiple options are available, including full and incremental repairs, executing repairs has traditionally required some tailoring for each cluster. It also has tended to be a manual process that was unfortunately frequently neglected by some Cassandra cluster administrators.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>More Detail on Repairs in Cassandra</h1>&#13;
<p>For a deeper treatment of repair, see <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/cassandra-the-definitive/9781492097136"><em>Cassandra: The</em> <span class="keep-together"><em>Definitive</em></span> <em>Guide</em></a>, where repair concepts and the available options are described in Chapters 6 and 12, respectively.</p>&#13;
</div>&#13;
<p><a contenteditable="false" data-primary="Spotify" data-type="indexterm" id="idm46183196538656"/><a href="http://cassandra-reaper.io">Cassandra Reaper</a> was created to take the difficulty out of executing repairs on Cassandra clusters and optimize repair performance to <span class="keep-together">minimize</span> the impact of running repairs on heavily used clusters. Reaper was created by Spotify and enhanced by The Last Pickle, which currently manages the project on <a href="https://oreil.ly/2MttB">GitHub</a>. Reaper exposes a RESTful API for configuring repair schedules for one or more Cassandra clusters, and also provides a command-line tool and web interface which guides administrators through the process of creating schedules.</p>&#13;
<p><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="about" data-type="indexterm" id="idm46183196606144"/>K8ssandra provides the option to incorporate an instance of Cassandra Reaper as part of a K8ssandraCluster. The K8ssandra Operator includes a Reaper controller that is responsible for managing the local Cassandra Reaper manager process through its associated Reaper CRD. By default, enabling Reaper in a K8ssandraCluster will cause an instance of Reaper to be installed in each Kubernetes cluster represented in the installation, but you can also use a single instance of Reaper to manage repairs across multiple Datacenters, or even across multiple Cassandra clusters, provided they are accessible via the network.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="how_important_is_it_to_be_kubernetes_na">&#13;
<h5>How Important Is It to Be Kubernetes Native?</h5>&#13;
<p>K8ssandra’s usage of Reaper is an example of the trade-offs involved in building more complex stacks of data infrastructure. For example, a more Kubernetes native design for the Reaper manager might involve factoring out each repair task into a Kubernetes CronJob that could be scheduled alongside the associated CassandraDatacenter, thus making more use of Kubernetes built-in resources. For now, the K8ssandra project has made the choice to integrate Reaper as is.</p>&#13;
<p>We saw another example of this “wrap versus rewrite” type of decision in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>, where the Vitess Operator reuses the Vitess control daemon <code>vtctld</code> and its <code>vtctlclient</code> as is. In both of these examples, the project developers have made pragmatic choices to do initial deployments that do “just enough” to port existing infrastructure to run in Kubernetes, while leaving room for more Kubernetes native approaches in the future. In <a data-type="xref" href="ch07.html#the_kubernetes_native_database">Chapter 7</a>, we’ll examine what it looks like to start with a Kubernetes native approach from scratch on new infrastructure projects.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Backing Up and Restoring Data with Cassandra Medusa" data-type="sect1"><div class="sect1" id="backing_up_and_restoring_data_with_cass">&#13;
<h1>Backing Up and Restoring Data with Cassandra Medusa</h1>&#13;
<p><a contenteditable="false" data-primary="backing up data, with Cassandra Medusa" data-type="indexterm" id="idm46183196584912"/><a contenteditable="false" data-primary="Cassandra Medusa" data-secondary="backing up data with" data-type="indexterm" id="idm46183196583776"/><a contenteditable="false" data-primary="data" data-secondary="backing up with Cassandra Medusa" data-type="indexterm" id="idm46183196900368"/>Managing backups is an important part of maintaining high availability and disaster recovery planning for any system that stores data. Cassandra supports both full and differential backups by creating hard links to the SSTable files it uses for data persistence. Cassandra itself does not take responsibility for copying the SSTable files to backup storage. Instead, this is left to the user. Similarly, recovering from backup involves copying the SSTable files to the Cassandra node where the data is to be reloaded; then Cassandra can be pointed to the local files to restore their contents.</p>&#13;
<p>Cassandra’s backup and restore operations are traditionally executed on individual nodes using nodetool, a command-line tool that leverages Cassandra’s JMX interface. <a href="https://oreil.ly/tmP91">Cassandra Medusa</a> is an open source command-line tool created by Spotify and The Last Pickle that executes nodetool commands to perform backups, including synchronization of backups across multiple nodes. Medusa supports Amazon S3, Google Cloud Storage (GCS), Azure Storage, and S3-compatible storage such as MinIO and Ceph Object Gateway, and can be extended to support other storage providers via the Apache Libcloud project.</p>&#13;
<p>Medusa can restore either individual nodes to support fast replacement of a downed node, or entire clusters in a disaster recovery scenario. Restoring to a cluster can either be to the original cluster or to a new cluster. Medusa is able to restore data to a cluster with a different size or topology than the original cluster, which has traditionally been a challenge to figure out manually.</p>&#13;
<p>K8ssandra has incorporated Medusa in order to provide backup and restore capabilities for Cassandra clusters running in Kubernetes. To configure the use of Medusa in a K8ssandraCluster, you’ll want to configure the <code>medusa</code> properties:</p>&#13;
<pre data-type="programlisting">apiVersion: k8ssandra.io/v1alpha1&#13;
kind: K8ssandraCluster&#13;
metadata:&#13;
  name: demo&#13;
spec:&#13;
  cassandra:&#13;
    ...&#13;
  medusa:&#13;
    storageProperties:&#13;
      storageProvider: google_storage&#13;
      storageSecretRef:&#13;
        name: medusa-bucket-key&#13;
      bucketName: k8ssandra-medusa&#13;
      prefix: test&#13;
      ...</pre>&#13;
<p>The options shown here include the storage provider, the bucket to use for backups, an optional prefix to add to directory names used to organize backup files, and the name of a Kubernetes Secret containing login credentials for the bucket. See the <a href="https://oreil.ly/ujZYw">documentation</a> for details on the contents of the Secret. Other available options include enabling SSL on the bucket connection, and setting the policies for purging old backups such as a maximum age or number of backups.</p>&#13;
<section data-pdf-bookmark="Creating a Backup" data-type="sect2"><div class="sect2" id="creating_a_backup">&#13;
<h2>Creating a Backup</h2>&#13;
<p>Once the K8ssandraCluster has been started, you can create backups using the CassandraBackup CRD. For example, you could initiate a backup of the CassandraDatacenter <code>dc1</code> using a command like this:</p>&#13;
<pre data-type="programlisting"><strong>cat &lt;&lt;EOF | kubectl apply -f -n k8ssandra-operator -&#13;
apiVersion: medusa.k8ssandra.io/v1alpha1&#13;
kind: CassandraBackup&#13;
metadata:&#13;
  name: medusa-backup1&#13;
spec:&#13;
  cassandraDatacenter: dc1&#13;
  name: medusa-backup1&#13;
EOF</strong></pre>&#13;
<p>The steps in processing of this resource are shown in <a data-type="xref" href="#performing_a_datacenter_backup_using_me">Figure 6-9</a>.</p> &#13;
<figure><div class="figure" id="performing_a_datacenter_backup_using_me">&#13;
<img alt="Performing a Datacenter backup using Medusa" src="assets/mcdk_0609.png"/>&#13;
<h6><span class="label">Figure 6-9. </span>Performing a Datacenter backup using Medusa</h6>&#13;
</div></figure>&#13;
<p>When you apply the resource definition (1), <code>kubectl</code> registers the resource with the API Server (2). The API server notifies the Medusa Controller running as part of the K8ssandra Operator (3).</p> &#13;
&#13;
<p>The Medusa Controller contacts a sidecar container (4), which K8ssandra has injected into the Cassandra Pod because you chose to enable Medusa on the K8ssandraCluster. The Medusa sidecar container uses nodetool commands to a backup on the Cassandra node via JMX (5) (the JMX interface is exposed only within the Pod).</p> &#13;
&#13;
<p>Cassandra performs a backup (6), marking the SSTable files on the PersistentVolume that mark the current snapshot.  The Medusa sidecar copies the snapshot files from the PV to the bucket (7). Steps 4–7 are repeated for each Cassandra Pod in the CassandraDatacenter.</p>&#13;
&#13;
<p>You can monitor the progress of the backup by checking the status of the resource:</p>&#13;
<pre data-type="programlisting"><strong>kubectl get cassandrabackup/medusa-backup1 -n k8ssandra-operator -o yaml</strong>&#13;
kind: CassandraBackup&#13;
metadata:&#13;
    name: medusa-backup1&#13;
spec:&#13;
  backupType: differential&#13;
  cassandraDatacenter: dc1&#13;
  name: medusa-backup1&#13;
status:&#13;
  ...&#13;
  ...&#13;
  finishTime: "2022-02-26T09:21:38Z"&#13;
  finished:&#13;
  - demo-dc1-default-sts-0&#13;
  - demo-dc1-default-sts-1&#13;
  - demo-dc1-default-sts-2&#13;
  startTime: "2022-02-26T09:21:35Z"</pre>&#13;
<p>You’ll know the backup is complete when the <code>finishTime</code> attribute is populated. The Pods that have been backed up are listed under the <code>finished</code> attribute.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Restoring from Backup" data-type="sect2"><div class="sect2" id="restoring_from_backup">&#13;
<h2>Restoring from Backup</h2>&#13;
<p><a contenteditable="false" data-primary="Cassandra Medusa" data-secondary="restoring data with" data-type="indexterm" id="idm46183196550176"/><a contenteditable="false" data-primary="data" data-secondary="restoring with Cassandra Medusa" data-type="indexterm" id="idm46183196548736"/><a contenteditable="false" data-primary="restoring data, with Cassandra Medusa" data-type="indexterm" id="idm46183196504416"/>The process of restoring data from a backup is similar. To restore an entire Datacenter from backed-up data, you could create a CassandraRestore resource like this:</p>&#13;
<pre data-type="programlisting"><strong>cat &lt;&lt;EOF | kubectl apply -f -n k8ssandra-operator -&#13;
apiVersion: medusa.k8ssandra.io/v1alpha1&#13;
kind: CassandraRestore&#13;
metadata:&#13;
  name: restore-backup1&#13;
spec:&#13;
  cassandraDatacenter:&#13;
    name: dc1&#13;
    clusterName: demo&#13;
  backup: medusa-backup1&#13;
  inPlace: true&#13;
  shutdown: true&#13;
EOF</strong></pre>&#13;
<p>When the Medusa Controller is notified of the new resource, it locates the CassandraDatacenter and updates the Pod spec template within the StatefulSet that is managing the Cassandra Pods. The updates consist of adding a new init container called <code>medusa-restore</code> and setting environment variables that <code>medusa-restore</code> will use to locate the datafiles that are to be restored. The update to the Pod spec template causes the StatefulSet controller to perform a rolling update of the Cassandra Pods in the StatefulSet. As each Pod restarts, <code>medusa-restore</code> copies the files from object storage onto the PersistentVolume for the node, and then the Cassandra container starts as usual. You can monitor the progress of the restore by checking the status of the CassandraRestore resource.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>A Common Language for Data Recovery?</h1>&#13;
<p><a contenteditable="false" data-primary="data" data-secondary="language for recovery of" data-type="indexterm" id="idm46183196637600"/><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="data recovery and" data-type="indexterm" id="idm46183196496928"/><a contenteditable="false" data-primary="language, for data recovery" data-type="indexterm" id="idm46183196495344"/>It is interesting to note the similarities and differences between the ways backup and restore operations are supported by the K8ssandra Operator we’ve discussed in this chapter and the Vitess Operator discussed in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>.</p>&#13;
<p>In K8ssandra, the CassandraBackup and CassandraRestore resources function in a manner similar to Kubernetes Jobs—they represent a task that you would like to have performed as well as the results of the task. In contrast, the VitessBackup resource represents a record of a backup that the Vitess Operator has performed based on the configuration of a VitessCluster resource. There is no equivalent resource to the CassandraRestore operator in Vitess.</p>&#13;
<p>Although K8ssandra and Vitess differ significantly in their approach to managing backups, both represent each backup task as a resource. Perhaps this common ground could be the starting point toward the development of common resource definitions for backup and restore operations, helping fulfill the vision introduced in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>.</p>&#13;
</div>&#13;
<p>Similar to the behavior of Cassandra Reaper, a single instance of Medusa can be configured to manage backup and restore operations across multiple Datacenters or Cassandra clusters. See the K8ssandra <a href="https://oreil.ly/Y2EkE">documentation</a> for more details on performing backup and restore operations with Medusa.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deploying Multicluster Applications in Kubernetes" data-type="sect1"><div class="sect1" id="deploying_multicluster_applications_in">&#13;
<h1>Deploying Multicluster Applications in Kubernetes</h1>&#13;
<p><a contenteditable="false" data-primary="applications" data-secondary="multicluster" data-type="indexterm" id="app_mul"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="deploying multicluster applications in" data-type="indexterm" id="kub_mul"/><a contenteditable="false" data-primary="multiclusters" data-secondary="deploying applications in Kubernetes" data-type="indexterm" id="mul_kub"/>One of the main selling points of a distributed database like Cassandra is its ability to support deployments across multiple Datacenters. Many users take advantage of this in order to promote high availability across geographically distributed Datacenters, to provide lower-latency reads and writes for applications and their users.</p>&#13;
<p>However, Kubernetes itself was not originally designed to support applications that span multiple Kubernetes clusters. This has traditionally meant that creating such multiregion applications leaves a lot of work to development teams.</p> &#13;
&#13;
<p>This work takes two main forms: creating the network infrastructure to connect the Kubernetes clusters, and coordinating interactions between resources in those clusters. Let’s examine these requirements and the implications for an application like Cassandra:</p>&#13;
<dl>&#13;
<dt>Multicluster networking requirements</dt>&#13;
<dd><p><a contenteditable="false" data-primary="VPC (virtual private cloud)" data-type="indexterm" id="idm46183196531024"/><a contenteditable="false" data-primary="networks" data-secondary="multicluster requirements" data-type="indexterm" id="idm46183196499376"/>From a networking perspective, the key is to have secure, reliable networking between Datacenters. If you’re using a single cloud provider for your application, this may be relatively simple to achieve using VPC capabilities offered by the major cloud vendors.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="DNS (domain name resolution)" data-type="indexterm" id="idm46183196469312"/>If you’re using multiple clouds, you’ll need a third-party solution. For the most part, Cassandra requires routable IPs between its nodes and does not rely on name resolution, but it is helpful to have DNS in place as well to simplify the process of managing Cassandra’s seed nodes.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="Carpenter, Jeff" data-secondary="“Deploy a Multi-Data Center Cluster in Kubernetes” blog post" data-type="indexterm" id="idm46183196483552"/><a contenteditable="false" data-primary="“Deploy a Multi-Data Center Cluster in Kubernetes” blog post (Carpenter)" data-primary-sortas="deploy" data-type="indexterm" id="idm46183196488464"/><a contenteditable="false" data-primary="“Multi-Region Cassandra on EKS with K8ssandra and Kubefed” blog post (Srinivas)" data-primary-sortas="multiregion" data-type="indexterm" id="idm46183196486576"/><a contenteditable="false" data-primary="Srinivas, Raghavan, “Multi-Region Cassandra on EKS with K8ssandra and Kubefed” blog post" data-type="indexterm" id="idm46183196714816"/><a contenteditable="false" data-primary="EKS (Elastic Kubernetes Service)" data-type="indexterm" id="idm46183196466752"/>Jeff’s blog post <a href="https://oreil.ly/HpCYX">“Deploy a Multi-Data Center Cassandra Cluster in Kubernetes”</a> describes an example configuration in Google Cloud Platform (GCP) using the CloudDNS service, while Raghavan Srinivas’s blog post <a href="https://oreil.ly/9byYo">“Multi-Region Cassandra on EKS with K8ssandra and Kubefed”</a> describes a similar configuration on Amazon EKS.</p>&#13;
</dd>&#13;
&#13;
<dt>Multicluster resource coordination requirements</dt>&#13;
<dd><p><a contenteditable="false" data-primary="resources" data-secondary="multicluster coordination requirements" data-type="indexterm" id="idm46183196463168"/>Managing an application that spans multiple Kubernetes clusters means that there are distinct resources in each cluster which have no relationship to resources in other clusters that the Kubernetes control plane is aware of. To manage the lifecycle of an application including deployment, upgrade, scaling up and down, and teardown, you need to coordinate resources across multiple Datacenters.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="KubeFed (Kubernetes Cluster Federation)" data-type="indexterm" id="idm46183196594064"/>The Kubernetes Cluster Federation project (<a href="https://oreil.ly/yvUCm">KubeFed</a> for short) provides one approach to providing a set of APIs for managing resources across clusters that can be leveraged to build multicluster applications. This includes mechanisms that represent Kubernetes clusters themselves as resources. While KubeFed is still in beta, the K8ssandra Operator uses a similar design approach for managing resources across clusters. We’ll examine this in more detail in <a data-type="xref" href="#kubernetes_cluster_federation">“Kubernetes Cluster Federation”</a>.</p>&#13;
</dd>&#13;
</dl>&#13;
<p>To achieve a multicluster Kubernetes deployment of Cassandra, you’ll need to establish networking between Datacenters according to your specific situation. Given that foundation, the K8ssandra Operator provides the facilities to manage the lifecycle of resources across the Kubernetes clusters. For a simple example of deploying a multiregion K8ssandraCluster, use the <a href="https://oreil.ly/bmcil">instructions</a> found in the K8ssandra documentation, again using the Makefile:</p>&#13;
<pre data-type="programlisting"><strong>make multi-up</strong></pre>&#13;
<p>This builds two kind clusters, deploys the K8ssandra Operator in each of them, and creates a multicluster K8ssandraCluster. One advantage of using kind for a simple demonstration is that Docker provides the networking between clusters. We’ll walk through some of the key steps in this process in order to describe how the K8ssandra Operator accomplishes this work.</p>&#13;
<p>The K8ssandra Operator supports two modes of installation: control plane (the default) and data plane. For a multicluster deployment, one Kubernetes cluster must be designated as the control plane cluster, and the others as data plane clusters. The control plane cluster can optionally include a CassandraDatacenter, as in the configuration shown in <a data-type="xref" href="#keightssandra_multicluster_architecture">Figure 6-10</a>.</p>&#13;
<figure><div class="figure" id="keightssandra_multicluster_architecture">&#13;
<img alt="K8ssandra multicluster architecture" src="assets/mcdk_0610.png"/>&#13;
<h6><span class="label">Figure 6-10. </span>K8ssandra multicluster architecture</h6>&#13;
</div></figure>&#13;
<p>When installed in control plane mode, the K8ssandra Operator uses two additional CRDs to manage multicluster deployments: ReplicatedSecret and ClientConfig. You can see evidence of the ClientConfig in the K8ssandraCluster configuration that was used, which looks something like the following:</p>&#13;
<pre data-type="programlisting">apiVersion: k8ssandra.io/v1alpha1&#13;
kind: K8ssandraCluster&#13;
metadata:&#13;
  name: demo&#13;
spec:&#13;
  cassandra:&#13;
    serverVersion: "4.0.1"&#13;
    ...&#13;
    networking:&#13;
      hostNetwork: true   &#13;
    datacenters:&#13;
      - metadata:&#13;
          name: dc1&#13;
        size: 3&#13;
        stargate:&#13;
          size: 1&#13;
      - metadata:&#13;
          name: dc2&#13;
        k8sContext: kind-k8ssandra-1&#13;
        size: 3&#13;
        stargate:&#13;
          size: 1</pre>&#13;
<p>This configuration specifies a K8ssandraCluster <code>demo</code> consisting of two CassandraDatacenters, <code>dc1</code> and <code>dc2</code>. Each Datacenter has its own configuration so that you can select a different number of Cassandra and Stargate nodes, or different resource allocations for the Pods. In the <code>demo</code> configuration, <code>dc1</code> is running in the control plane cluster <code>kind-k8ssandra-0</code>, and <code>dc2</code> is running in the data plane cluster <code>kind-k8ssandra-1</code>.</p>&#13;
<p>Notice the <code>k8sContext: kind-k8ssandra-1</code> line in the configuration. This is a reference to a ClientConfig resource that was created by the <code>make</code> command. A ClientConfig is a resource that represents the information needed to connect to the API server of another cluster, similar to the way <code>kubectl</code> stores information about different clusters on your local machine. The ClientConfig resource references a Secret that is used to store access credentials securely. The K8ssandra Operator repo includes a <a href="https://oreil.ly/wPINU">convenience script</a> that can be used to create ClientConfig resources for Kubernetes clusters.</p>&#13;
<p>When you create a K8ssandraCluster in the control plane cluster, it uses the ClientConfigs to connect to each remote Kubernetes cluster in order to create the specified resources. For the preceding configuration, this includes CassandraDatacenter and Stargate resources, but can also include other resources such as Medusa and Prometheus ServiceMonitor.</p>&#13;
<p><a contenteditable="false" data-primary="ReplicatedSecret" data-type="indexterm" id="idm46183196427840"/>The ReplicatedSecret is another resource involved in sharing access credentials. The control plane K8ssandra Operator uses this resource to keep track of Secrets that it creates in each remote cluster. These Secrets are used by the various K8ssandra components to securely communicate information such as the default Cassandra administrator credentials with each other. The K8ssandra Operator creates and manages ReplicatedSecret resources itself; you don’t need to interact with them.</p>&#13;
<p><a contenteditable="false" data-primary="K8ssandraCluster" data-primary-sortas="kassandra" data-type="indexterm" id="idm46183196447504"/><a contenteditable="false" data-primary="ClientConfig" data-type="indexterm" id="idm46183196732144"/>The K8ssandraCluster, ClientConfig, and ReplicatedSecret resources exist only in the control plane cluster, and when the K8ssandra Operator is deployed in data plane mode, it does not even run the controllers associated with those resource types.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>More Detail on the K8ssandra Operator</h1>&#13;
<p><a contenteditable="false" data-primary="Sanda, John" data-type="indexterm" id="idm46183196609952"/>This is a quick summary of a complex design for a multicluster operator. For more details on the approach, see the K8ssandra Operator <a href="https://oreil.ly/ACAD2">architecture overview</a> and John Sanda’s <a href="https://oreil.ly/RMK3E">presentation</a> at the Data on Kubernetes Community (DoKC) meetup.</p>&#13;
</div>&#13;
<p>Now let’s consider a more general approach to building multicluster applications that we can compare and contrast with K8ssandra’s approach.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="kubernetes_cluster_federation">&#13;
<h5>Kubernetes Cluster Federation</h5>&#13;
<p><em>With Irfan Ur Rehman, Senior Engineer, Turbonomic (an IBM company)</em></p>&#13;
<p><a contenteditable="false" data-primary="KubeFed (Kubernetes Cluster Federation)" data-type="indexterm" id="idm46183196415424"/><a contenteditable="false" data-primary="Rehman, Irfan Ur" data-type="indexterm" id="idm46183196415776"/>KubeFed is a project for building multicluster applications, managed by the <a href="https://oreil.ly/lfWvB">Kubernetes Multicluster SIG</a>. The project was initially called <em>Federation</em>, but was renamed <em>Kubernetes Cluster Federation (KubeFed)</em>, to distinguish it from the term <em>federation</em> being used in projects outside of Kubernetes.</p>&#13;
<p>KubeFed defines <em>federation</em> as joining a set of clusters into a pool, which then provides a unified API to the user to distribute applications into those clusters. To use KubeFed, you create federated resources in a base cluster. A federated resource contains templates for Kubernetes built-in or custom resources. KubeFed acts as a resource reconciler, using the templates you provide to push resources to the member clusters.</p>&#13;
<p>You might want the templates to be applied in slightly different ways in each member cluster, so KubeFed supports concepts called placements and overrides. A <em>placement</em> defines where applications and their resources are deployed. For example, you could use a placement to push resources in cluster 1 but not cluster 2 or to indicate you want more replicas in one cluster than another cluster. <em>Overrides</em> allow you to provide different values for resource attributes for a specific cluster.</p>&#13;
<p><a contenteditable="false" data-primary="ReplicaScheduler" data-type="indexterm" id="idm46183196492832"/>KubeFed also provides resources to support higher-order things you might want to do. The ReplicaScheduler is a resource that manages Deployments and ReplicaSets. This allows you to deploy your application by specifying the total number of replicas desired across clusters, without worrying about which clusters they go to. You can do something similar for StatefulSets.</p>&#13;
<p>Cluster federation, placements, and overrides are three key concepts defined by KubeFed, along with others defined on the <a href="https://oreil.ly/xByUc">concepts page</a>. These terms have gained wide popularity and are used across other projects as well. For example, Argo CD is a GitOps toolset for Kubernetes which employs similar concepts such as placement rules and overrides.</p>&#13;
<p>Other multicluster projects in the Kubernetes ecosystem have similar goals but differ in implementation and scope:</p>&#13;
<dl>&#13;
<dt><a href="https://oreil.ly/yLA6g">Kubernetes Armada (Karmada)</a></dt>&#13;
<dd><a contenteditable="false" data-primary="Karmada (Kubernetes Armada)" data-type="indexterm" id="idm46183196429216"/>This project, sponsored by Huawei, is similar to KubeFed but takes a different API approach. Karmada reuses existing Kubernetes resources but extends them with additional attributes in order to provide the appearance of a single Kubernetes cluster.</dd>&#13;
<dt><a href="https://crossplane.io">Crossplane</a></dt>&#13;
<dd><a contenteditable="false" data-primary="Crossplane" data-type="indexterm" id="idm46183196468480"/><a contenteditable="false" data-primary="DBaaS (database as a service)" data-type="indexterm" id="idm46183196467344"/>This CNCF incubating project aims to provide a single API surface for you to distribute resources and consume services from multiple clouds. Crossplane uses the same declarative approach as Kubernetes but goes beyond just Kubernetes resources, allowing you to incorporate offerings from the major cloud providers such as database as a service (DBaaS) or network as a service (NaaS).</dd>&#13;
<dt><a href="https://oreil.ly/v6nV5">Open Cluster Management (OCM)</a></dt>&#13;
<dd><p><a contenteditable="false" data-primary="OCM (Open Cluster Management)" data-type="indexterm" id="idm46183196558768"/><a contenteditable="false" data-primary="Red Hat" data-type="indexterm" id="idm46183196397120"/>This project, sponsored by Red Hat, provides an ecosystem of components for working across multiple Kubernetes clusters.</p></dd>&#13;
</dl>&#13;
<p>Each of these projects takes a similar approach at a high level but has its own opinionated APIs and nuances which might be more suitable to different users.</p>&#13;
<p><a contenteditable="false" data-primary="Submariner" data-type="indexterm" id="idm46183196440416"/><a contenteditable="false" data-primary="Cilium" data-type="indexterm" id="idm46183196460240"/>KubeFed and these similar projects are primarily concerned with resource replication. To have multicluster applications, you also need networking solutions, which can get a little more complex. One approach is to create cross-cluster network overlays using open source projects like <a href="https://oreil.ly/ZalRQ">Submariner</a> or <a href="https://oreil.ly/hy7ck">Cilium</a>.</p>&#13;
<p><a contenteditable="false" data-primary="Multi-Cluster Services API" data-type="indexterm" id="idm46183196436720"/><a contenteditable="false" data-primary="alpha implementation" data-type="indexterm" id="idm46183196387520"/>Even with the network in place, you still have the problem of discovering applications and resources across clusters and connecting them securely. The <a href="https://oreil.ly/lMVf4">Multi-Cluster Services API</a> is a proposal in the Kubernetes Multicluster SIG for providing this discovery. It is based on <em>endpoint slices</em>, which allow a cluster to discover services from another cluster. An <a href="https://oreil.ly/kDReX">alpha implementation</a> is available.</p>&#13;
<p>Although KubeFed is still in Beta status, it is in a mature state, and some organizations are already using it in production. The core functionality of reconciling resources across clusters is something that just works. The main item in the KubeFed roadmap is a GA release, which should lead to further adoption.</p>&#13;
<p>Adoption can be a chicken-and-egg problem, because organizations often prefer to back established projects. Throughout its history, KubeFed has had support from RedHat/IBM, Huawei, D2iQ, and others, and backing by larger organizations is important for driving adoption by the larger community.</p>&#13;
<p>Coming up with a single standard is challenging. Major cloud providers have a <a href="https://oreil.ly/1Hc43">lack of incentive</a> to contribute to these efforts as opposed to supporting tooling centered on their own platforms, so it is up to us in the open source community to invest in this area.</p>&#13;
&#13;
</div></aside>&#13;
<p>As you can see, there is a lot of potential for growth in the area of Kubernetes federation and the ability to manage resources across Kubernetes cluster boundaries. For example, as a database whose primary superpower is running across multiple Datacenters, Cassandra seems like a great match for a multicluster solution like KubeFed.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="data_ch" data-type="indexterm" id="idm46183196432080"/><a contenteditable="false" data-primary="" data-startref="ks_ch" data-type="indexterm" id="idm46183196474880"/>The K8ssandra Operator and KubeFed have taken similar architectural approaches, where custom “federated” resources provide templates used to define resources in other clusters. This commonality points to the possibility for future collaboration across these projects and others based on similar design principles. Perhaps in the future, CRDs like K8ssandra’s ClientConfig and ReplicatedSecret can be replaced by equivalent functionality provided by KubeFed<a contenteditable="false" data-primary="" data-startref="app_mul" data-type="indexterm" id="idm46183196472640"/><a contenteditable="false" data-primary="" data-startref="kub_mul" data-type="indexterm" id="idm46183196521744"/><a contenteditable="false" data-primary="" data-startref="mul_kub" data-type="indexterm" id="idm46183196473568"/>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000005">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, you’ve learned how data infrastructure can be composed with other infrastructure to build reusable stacks on Kubernetes. Using the K8ssandra project as an example, you’ve learned about aspects including integrating data infrastructure with API gateways and monitoring solutions to provide more full-featured solutions.</p>&#13;
<p>You’ve also learned some of the opportunities and challenges with adapting existing technologies onto Kubernetes and creating multicluster data infrastructure deployments. In the next chapter, we’ll explore how to design new cloud native data infrastructure that takes advantage of everything that Kubernetes provides without requiring adaptation and discover what new possibilities that opens up.</p>&#13;
</div></section>&#13;
</div></section></body></html>