<html><head></head><body><section data-pdf-bookmark="Chapter 10. Singleton Service" data-type="chapter" epub:type="chapter"><div class="chapter" id="SingletonService">&#13;
<h1><span class="label">Chapter 10. </span>Singleton Service</h1>&#13;
&#13;
&#13;
<p>The <em>Singleton Service</em> pattern<a data-primary="Singleton Service" data-type="indexterm" id="singserv10"/> ensures that only one instance o an application is active at a time and yet is highly available. This pattern can be implemented from within the application or delegated fully to Kubernetes.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect1"><div class="sect1" id="idm45902101160384">&#13;
<h1>Problem</h1>&#13;
&#13;
<p>One<a data-primary="problems" data-secondary="active instances, controlling" data-type="indexterm" id="idm45902101158864"/> of the main capabilities provided by Kubernetes is the ability to easily and transparently scale applications. Pods<a data-primary="Pods" data-secondary="creating and managing" data-type="indexterm" id="idm45902101157632"/> can scale imperatively with a single command such as<a data-primary="kubectl" data-secondary="Singleton Service" data-type="indexterm" id="idm45902101156560"/> <code>kubectl scale</code>, or declaratively through a controller definition such as ReplicaSet, or even dynamically based on the application load, as we describe in<a data-primary="Elastic Scale" data-type="indexterm" id="idm45902101154912"/><a data-primary="Elastic Scale" data-secondary="Singleton Service" data-type="indexterm" id="idm45902101154208"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch29.html#ElasticScale">Chapter 29, “Elastic Scale”</a>. By running multiple instances of the same service (not a Kubernetes Service but a component of a distributed application represented by a Pod), the system usually increases throughput and availability. The availability increases because if one instance of a service becomes unhealthy, the request dispatcher forwards future requests to other healthy instances. In Kubernetes, multiple instances are the replicas of a Pod, and the Service resource is responsible for the request distribution and load balancing.</p>&#13;
&#13;
<p>However, in some cases, only one instance of a service is allowed to run at a time. For example, if there is a periodically executed task in a service and multiple instances of the same service, every instance will trigger the task at the scheduled intervals, leading to duplicates rather than having only one task fired as expected. Another example is a service that performs polling on specific resources (a filesystem or database) and we want to ensure that only a single instance and maybe even a single thread performs the polling and processing. A third case occurs when we have to consume messages from a messages broker in an order-preserving manner with a single-threaded consumer that is also a singleton service.</p>&#13;
&#13;
<p>In all these and similar situations, we need some control over how many instances \ of a service are active at a time (usually only one is required), while still ensuring high availability, regardless of how many instances have been started and kept running.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect1"><div class="sect1" id="idm45902101150944">&#13;
<h1>Solution</h1>&#13;
&#13;
<p>Running multiple replicas of the same Pod creates an<a data-primary="active-active topology" data-type="indexterm" id="idm45902101149376"/> <em>active-active</em> topology, where all instances of a service are active. What we need is an<a data-primary="active-passive topology" data-type="indexterm" id="idm45902101148128"/> <em>active-passive</em> topology, where only one instance is active and all the other instances are passive. Fundamentally, this can be achieved at two possible levels: out-of-application and in-application locking.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Out-of-Application Locking" data-type="sect2"><div class="sect2" id="singleton-locking-out">&#13;
<h2>Out-of-Application Locking</h2>&#13;
&#13;
<p>As<a data-primary="out-of-application locking" data-type="indexterm" id="idm45902101144304"/><a data-primary="locking" data-secondary="out-of-application" data-type="indexterm" id="idm45902101143552"/> the name suggests, this mechanism relies on a managing process that is outside of the application to ensure that only a single instance of the application is running. The application implementation itself is not aware of this constraint and is run as a singleton instance. From this perspective, it is similar to having a Java class that is instantiated only once by the managing runtime (such<a data-primary="Spring Framework" data-type="indexterm" id="idm45902101142480"/> as the Spring Framework). The class implementation is not aware that it is run as a singleton, nor that it contains any code constructs to prevent instantiating multiple instances.</p>&#13;
&#13;
<p><a data-type="xref" href="#img-out-of-app-locking">Figure 10-1</a> shows how to implement out-of-application locking with the help of a StatefulSet or ReplicaSet controller with one replica.</p>&#13;
&#13;
<figure class="width-50"><div class="figure" id="img-out-of-app-locking">&#13;
<img alt="Out-of-application locking mechanism" src="assets/kup2_1001.png"/>&#13;
<h6><span class="label">Figure 10-1. </span>Out-of-application locking mechanism</h6>&#13;
</div></figure>&#13;
&#13;
<p>The way to achieve this in Kubernetes is to start a single Pod. This activity alone does not ensure the singleton Pod is highly available. What we have to do is also back the Pod with a controller such as a<a data-primary="ReplicaSet" data-secondary="out-of-application locking" data-type="indexterm" id="idm45902101137952"/> ReplicaSet that turns the singleton Pod into a highly available singleton. This topology is not exactly <em>active-passive</em> (there is no passive instance), but it has the same effect, as Kubernetes ensures that one instance of the Pod is running at all times. In addition, the single Pod instance is highly available, thanks to the controller performing health checks as described in<a data-primary="Health Probe" data-type="indexterm" id="idm45902101136272"/><a data-primary="Health Probe" data-secondary="Singleton Service" data-type="indexterm" id="idm45902101135568"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch04.html#HealthProbe">Chapter 4, “Health Probe”</a>, and healing the Pod in case of failures.</p>&#13;
&#13;
<p>The main thing to keep an eye on with this approach is the replica count, which should not be changed accidentally. In this section, you will see how we can voluntarily decrease the replica count through PodDisruptionBudget, but there is no platform-level mechanism to prevent an increase of the replica count.</p>&#13;
&#13;
<p>It’s not entirely true that only one instance is running at all times, especially when things go wrong. Kubernetes<a data-primary="ReplicaSet" data-secondary="benefits of" data-type="indexterm" id="idm45902101132240"/> primitives such as ReplicaSet favor availability over consistency—a deliberate decision for achieving highly available and scalable distributed systems. That means a ReplicaSet applies “at least” rather than “at most” semantics for its replicas. If we configure a ReplicaSet to be a singleton with <code>replicas: 1</code>, the controller makes sure at least one instance is always running, but occasionally it can be more instances.</p>&#13;
&#13;
<p>The most popular corner case here occurs when a node with a controller-managed Pod becomes unhealthy and disconnects from the rest of the Kubernetes cluster. In this scenario, a ReplicaSet controller starts another Pod instance on a healthy node (assuming there is enough capacity), without ensuring the Pod on the disconnected node is shut down. Similarly, when changing the number of replicas or relocating Pods to different nodes, the number of Pods can temporarily go above the desired number. That temporary increase is done with the intention of ensuring high availability and avoiding disruption, as needed for stateless and scalable applications.</p>&#13;
&#13;
<p>Singletons can be resilient and recover, but by definition, they are not highly available. Singletons typically favor consistency over availability. The Kubernetes resource that also favors consistency over availability and provides the desired strict singleton guarantees is the StatefulSet. If ReplicaSets do not provide the desired guarantees for your application, and you have strict singleton requirements, StatefulSets might be the answer. StatefulSets are intended for stateful applications and offer many features, including stronger singleton guarantees, but they come with increased complexity as well. We discuss concerns around singletons and cover StatefulSets in more detail in<a data-primary="Stateful Service" data-secondary="Singleton Service" data-type="indexterm" id="idm45902101129792"/><a data-primary="Stateful Service" data-type="indexterm" id="idm45902101128816"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch12.html#StatefulService">Chapter 12, “Stateful Service”</a>.</p>&#13;
&#13;
<p>Typically, singleton applications running in Pods on Kubernetes open outgoing connections to message brokers, relational databases, file servers, or other systems running on other Pods or external systems. However, occasionally, your singleton Pod may need to accept incoming connections, and the way to enable that on Kubernetes is through the Service resource.</p>&#13;
&#13;
<p>We<a data-primary="Service Discovery" data-type="indexterm" id="idm45902101125696"/><a data-primary="Service Discovery" data-secondary="Singleton Service" data-type="indexterm" id="idm45902101124960"/> cover Kubernetes Services in depth in <a data-type="xref" data-xrefstyle="chap-num-title" href="ch13.html#ServiceDiscovery">Chapter 13, “Service Discovery”</a>, but let’s discuss briefly the part that applies to singletons here. A regular Service (with <code>type: ClusterIP</code>) creates a virtual IP and performs load balancing among all the Pod instances that its selector matches. However, a singleton Pod managed through a StatefulSet has only one Pod and a stable network identity. In such a case, it is better to create a <em>headless Service</em> (by setting both <code>type: ClusterIP</code> and <code>clusterIP: None</code>). It is called <em>headless</em> because such a Service doesn’t have a virtual IP address, kube-proxy doesn’t handle these Services, and the platform performs no proxying.</p>&#13;
&#13;
<p>However, such a Service is still useful because a headless Service with selectors creates endpoint records in the API Server and generates DNS A records for the matching Pod(s). With that, a DNS lookup for the Service does not return its virtual IP but instead the IP address(es) of the backing Pod(s). That enables direct access to the singleton Pod via the Service DNS record, and without going through the Service virtual IP. For example, if we create a headless Service with the name <code>my-singleton</code>, we can use it as <code>my-singleton.default.svc.cluster.local</code> to access the Pod’s IP address directly.</p>&#13;
&#13;
<p>To sum up, for nonstrict singletons with at least one instance requirement, defining a ReplicaSet with one replica would suffice. This configuration favors availability and ensures there is at least one available instance, and possibly more in some corner cases. For a strict singleton with an<a data-primary="At-Most-One" data-type="indexterm" id="idm45902103640752"/> At-Most-One requirement and better performant service discovery, a StatefulSet and a headless Service would be preferred. Using StatefulSet will favor consistency and ensure there is an At-Most-One instance and occasionally none in some corner cases. You can find a complete example of this in<a data-primary="Stateful Service" data-secondary="Singleton Service" data-type="indexterm" id="idm45902103639616"/><a data-primary="Stateful Service" data-type="indexterm" id="idm45902103638672"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch12.html#StatefulService">Chapter 12, “Stateful Service”</a>, where you have to change the number of replicas to one to make it a singleton.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="In-Application Locking" data-type="sect2"><div class="sect2" id="singleton-locking-in">&#13;
<h2>In-Application Locking</h2>&#13;
&#13;
<p>In<a data-primary="in-application locking" data-type="indexterm" id="idm45902103634672"/><a data-primary="locking" data-secondary="in-application" data-type="indexterm" id="idm45902103633936"/> a distributed environment, one way to control the service instance count is through a distributed lock, as shown in <a data-type="xref" href="#img-in-app-locking">Figure 10-2</a>. Whenever a service instance or a component inside the instance is activated, it can try to acquire a lock, and if it succeeds, the service becomes active. Any subsequent service instance that fails to acquire the lock waits and continuously tries to get the lock in case the currently active service releases it.</p>&#13;
&#13;
<p>Many existing distributed frameworks use this mechanism for achieving high availability and resiliency. For example, the message broker<a data-primary="Apache ActiveMQ" data-type="indexterm" id="idm45902103631552"/><a data-primary="ActiveMQ" data-type="indexterm" id="idm45902103630848"/> Apache ActiveMQ can run in a highly available<a data-primary="active-passive topology" data-type="indexterm" id="idm45902103630048"/> <em>active-passive</em> topology, where the data source provides the shared lock. The first broker instance that starts up acquires the lock and becomes active, and any other subsequently started instances become passive and wait for the lock to be released. This strategy ensures there is a single active broker instance that is also resilient to failures.</p>&#13;
&#13;
<figure><div class="figure" id="img-in-app-locking">&#13;
<img alt="In-application locking mechanism" src="assets/kup2_1002.png"/>&#13;
<h6><span class="label">Figure 10-2. </span>In-application locking mechanism</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can compare this strategy to a classic Singleton, as it is known in the object-oriented world: a <em>Singleton</em> is an object instance stored in a static class variable. In this instance, the class is aware of being a singleton, and it is written in a way that does not allow instantiation of multiple instances for the same process. In distributed systems, this would mean the containerized application itself has to be written in a way that does not allow more than one active instance at a time, regardless of the number of Pod instances that are started. To achieve this in a distributed environment, first we need a distributed lock implementation such as the one provided by Apache ZooKeeper, HashiCorp’s Consul, Redis, or etcd.</p>&#13;
&#13;
<p>The typical implementation with<a data-primary="Zookeeper" data-type="indexterm" id="zook10"/> ZooKeeper uses ephemeral nodes, which exist as long as there is a client session and are deleted as soon as the session ends. The first service instance that starts up initiates a session in the ZooKeeper server and creates an ephemeral node to become active. All other service instances from the same cluster become passive and have to wait for the ephemeral node to be released. This is how a ZooKeeper-based implementation makes sure there is only one active service instance in the whole cluster, ensuring an active-passive failover behavior.</p>&#13;
&#13;
<p>In the Kubernetes world, instead of managing a ZooKeeper cluster only for the locking feature, a better option would be to use etcd capabilities exposed through the Kubernetes API and running on the main nodes. etcd is a distributed key-value store that uses the Raft protocol to maintain its replicated state and provides the necessary building blocks for implementing leader election. For example, Kubernetes offers the<a data-primary="Lease objects" data-type="indexterm" id="idm45902103623952"/> Lease object, which is used for node heartbeats and component-level leader election. For every node, there is a Lease object with a matching name, and the<a data-primary="Kubelet" data-secondary="Lease objects" data-type="indexterm" id="idm45902103623120"/> Kubelet on every node keeps running a heart beat by updating the Lease object’s <code>renewTime</code> field. This information is used by the Kubernetes control plane to determine the availability<a data-primary="nodes" data-secondary="determining availability of" data-type="indexterm" id="idm45902103621552"/> of the nodes. Kubernetes Leases are also used in highly available cluster deployment scenarios for ensuring only single control plane components such as kube-controller-manager and kube-scheduler are active at a time and other instances remain on standby.</p>&#13;
&#13;
<p>Another example is in<a data-primary="Apache Camel" data-type="indexterm" id="idm45902103620000"/><a data-primary="Camel" data-type="indexterm" id="idm45902103619264"/> Apache Camel, which has a Kubernetes connector that also provides leader election and singleton capabilities. This connector goes a step further, and rather than accessing the etcd API directly, it uses Kubernetes APIs to leverage ConfigMaps as a distributed lock. It relies on Kubernetes optimistic locking guarantees for editing resources such as ConfigMaps, where only one Pod can update a ConfigMap at a time. The Camel implementation uses this guarantee to ensure only one Camel route instance is active, and any other instance has to wait and acquire the lock before activating. It is a custom implementation of a lock but achieves the same goal: when there are multiple Pods with the same Camel application, only one of them becomes the active singleton, and the others wait in passive mode.</p>&#13;
&#13;
<p>A<a data-primary="Singleton Service" data-secondary="Dapr's Distributed Lock" data-type="indexterm" id="idm45902103617728"/><a data-primary="Dapr's Distributed Lock" data-type="indexterm" id="idm45902103616720"/><a data-primary="Singleton Service" data-type="indexterm" id="idm45902103616048"/><a data-primary="Distributed Lock (Dapr)" data-type="indexterm" id="idm45902103615376"/> more generic implementation of the <em>Singleton Service</em> pattern is provided by the Dapr project. Dapr’s Distributed Lock building block provides APIs (HTTP and gRPC) with swappable implementations for mutually exclusive access to shared resources. The idea is that each application determines the resources the lock grants access to. Then, multiple instances of the same application use a named lock to exclusively access the shared resource. At any given moment, only one instance of an application can hold a named lock. All other instances of the application are unable to acquire the lock and therefore are not allowed to access the shared resource until the lock is released through unlock or the lock times out. Thanks<a data-primary="lease-based locking" data-type="indexterm" id="idm45902103614192"/><a data-primary="locking" data-secondary="lease-based" data-type="indexterm" id="idm45902103613488"/> to its lease-based locking mechanism, if an application acquires a lock, encounters an exception, and cannot free the lock, the lock is automatically released after a period of time using a lease. This prevents resource deadlocks in the event of application failures. Behind this generic distributed lock API, Dapr will be configured to use some kind of storage and lock implementation. This API can be used by applications to implement access to shared resources or in-application singletons.</p>&#13;
&#13;
<p>An implementation with Dapr, ZooKeeper, etcd, or any other distributed lock implementation would be similar to the one described: only one instance of the application becomes the leader and activates itself, and other instances are passive and wait for the lock. This ensures that even if multiple Pod replicas are started and all are healthy, up, and running, only one service is active and performs the business functionality as a singleton, and other instances wait to acquire the lock in case the leader fails or shuts down.<a data-primary="" data-startref="zook10" data-type="indexterm" id="idm45902103612160"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Disruption Budget" data-type="sect2"><div class="sect2" id="idm45902103610928">&#13;
<h2>Pod Disruption Budget</h2>&#13;
&#13;
<p>While singleton service and leader election try to limit the maximum number of instances a service is running at a time, the<a data-primary="PodDisruptionBudget" data-type="indexterm" id="idm45902103609392"/> PodDisruptionBudget functionality of Kubernetes provides a complementary and somewhat opposite functionality—limiting the number of instances that are simultaneously down for maintenance.</p>&#13;
&#13;
<p>At its core, PodDisruptionBudget ensures a certain number or percentage of Pods will not voluntarily be evicted from a node at any one point in time. <em>Voluntarily</em> here means an<a data-primary="Pods" data-secondary="preventing voluntary eviction" data-type="indexterm" id="idm45902103607392"/> eviction that can be delayed for a particular time—for example, when it is triggered by draining a node for maintenance or upgrade<a data-primary="kubectl" data-secondary="PodDisruptionBudget" data-type="indexterm" id="idm45902103606240"/> (<code>kubectl drain</code>), or a cluster scaling down, rather than a node becoming unhealthy, which cannot be predicted or controlled.</p>&#13;
&#13;
<p>The PodDisruptionBudget in <a data-type="xref" href="#ex-pod-disruption-budget">Example 10-1</a> applies to Pods that match its selector and ensures two Pods must be available all the time.</p>&#13;
<div data-type="example" id="ex-pod-disruption-budget">&#13;
<h5><span class="label">Example 10-1. </span>PodDisruptionBudget</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">policy/v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PodDisruptionBudget</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator-pdb</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w">             </code><a class="co" href="#callout_singleton_service_CO1-1" id="co_singleton_service_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">minAvailable</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2</code><code class="w">            </code><a class="co" href="#callout_singleton_service_CO1-2" id="co_singleton_service_CO1-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_singleton_service_CO1-1" id="callout_singleton_service_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Selector to count available Pods.</p></dd>&#13;
<dt><a class="co" href="#co_singleton_service_CO1-2" id="callout_singleton_service_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>At least two Pods have to be available. You can also specify a percentage, like 80%, to configure that only 20% of the matching Pods might be evicted.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>In addition to<a data-primary="minAvailable" data-type="indexterm" id="idm45902103552480"/> <code>.spec.minAvailable</code>, there is also the option to use <code>.spec.maxUnavailable</code>, which specifies the number of Pods from that set that can be unavailable after the eviction. Similar to <code>.spec.minAvailable</code>, it can be either an absolute number or a percentage, but it has a few additional limitations. You can specify only either <code>.spec.minAvailable</code> or <code>.spec.maxUnavailable</code> in a single PodDisruptionBudget, and then it can be used only to control the eviction of Pods that have an associated controller such as ReplicaSet or StatefulSet. For Pods not managed by a controller<a data-primary="naked Pods" data-type="indexterm" id="idm45902103549024"/><a data-primary="Pods" data-secondary="naked" data-type="indexterm" id="idm45902103514320"/><a data-primary="Pods" data-secondary="bare" data-type="indexterm" id="idm45902103513376"/><a data-primary="bare Pods" data-type="indexterm" id="idm45902103512432"/> (also referred to as <em>bare</em> or <em>naked</em> Pods), other limitations around PodDisruptionBudget should be considered.</p>&#13;
&#13;
<p class="pagebreak-before">PodDisruptionBudget is useful for<a data-primary="quorum-based applications" data-type="indexterm" id="idm45902103510320"/> quorum-based applications that require a minimum number of replicas running at all times to ensure a quorum. Or maybe when an application is serving critical traffic that should never go below a certain percentage of the total number of instances.</p>&#13;
&#13;
<p>PodDisruptionBudget is useful in the context of singletons too. For example, setting <code>maxUnavailable</code> to <code>0</code> or setting <code>minAvailable</code> to <code>100%</code> will prevent any<a data-primary="Pods" data-secondary="preventing voluntary eviction" data-type="indexterm" id="idm45902103507056"/> voluntary eviction. Setting voluntary eviction to zero for a workload will turn it into an unevictable Pod and will prevent draining the node forever. This can be used as a step in the process where a cluster operator has to contact the singleton workload owner for downtime before accidentally evicting a not highly available Pod. StatefulSet, combined with PodDisruptionBudget, and headless Service are Kubernetes primitives that control and help with the instance count at runtime and are worth mentioning in this chapter.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect1"><div class="sect1" id="idm45902103505952">&#13;
<h1>Discussion</h1>&#13;
&#13;
<p>If<a data-primary="ReplicaSet" data-secondary="out-of-application locking" data-type="indexterm" id="idm45902103504656"/> your use case requires strong singleton guarantees, you cannot rely on the out-of-application locking mechanisms of ReplicaSets. Kubernetes ReplicaSets are designed to preserve the availability of their Pods rather than to ensure At-Most-One<a data-primary="At-Most-One" data-type="indexterm" id="idm45902103503552"/> semantics for Pods. As a consequence, there are many failure scenarios that have two copies of a Pod running concurrently for a short period (efor example, when a node that runs the singleton Pod is partitioned from the rest of the cluster—such as when replacing a deleted Pod instance with a new one). If that is not acceptable, use StatefulSets or investigate the in-application locking options that provide you more control over the leader election process with stronger guarantees. The latter also mitigates the risk of accidentally scaling Pods by changing the number of replicas. You can combine this with PodDisruptionBudget and prevent voluntary eviction and disruption of your singleton workloads.</p>&#13;
&#13;
<p>In other scenarios, only a part of a containerized application should be a singleton. For example, there might be a containerized application that provides an HTTP endpoint that is safe to scale to multiple instances, but also a polling component that must be a singleton. Using the out-of-application locking approach would prevent scaling the whole service. In such a situation, we either have to split the singleton component in its deployment unit to keep it a singleton (good in theory but not always practical or worth the overhead) or use the in-application locking mechanism and lock only the component that has to be a singleton. This would allow us to scale the whole application transparently, have HTTP endpoints scaled, and have other parts as <em>active-passive</em> singletons.<a data-primary="" data-startref="singserv10" data-type="indexterm" id="idm45902103501952"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="More Information" data-type="sect1"><div class="sect1" id="idm45902103500688">&#13;
<h1>More Information</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/aGoPv">Singleton Service Example</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/tb9aX">Leases</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/W1ABD">Specifying a Disruption Budget for Your Application</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/NU1aN">Leader Election in Go Client</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/ES8Ve">Dapr: Distributed Lock Overview</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/K8zI1">Creating Clustered Singleton Services on Kubernetes</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/tho5T">Akka: Kubernetes Lease</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section></body></html>