<html><head></head><body><section data-pdf-bookmark="Chapter 16. Platform Abstractions" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter16">&#13;
<h1><span class="label">Chapter 16. </span>Platform Abstractions</h1>&#13;
&#13;
&#13;
<p>Many times we have seen organizations adopt a <em>build it and they will come</em> approach to designing and building a Kubernetes platform.<a data-primary="platform abstractions" data-type="indexterm" id="ix_pltabs"/> However, this philosophy is usually fraught with risk as it often fails to deliver on the key requirements of the many teams (e.g., development, information security, networking, etc.) that will be interacting with the platform, resulting in rework and additional effort. It’s important to bring the other groups along on the journey and ensure that the platform constructed is fit for purpose.<a data-primary="abstractions" data-seealso="platform abstractions" data-type="indexterm" id="idm45611970902968"/></p>&#13;
&#13;
<p>In this chapter we’ll cover some of the angles you should consider when designing the onboarding and usage experience for other teams (specifically developers) to your Kubernetes platform. First we’ll look at some of the more philosophical aspects and ask the question, How much should developers know about Kubernetes? Then we’ll move on to discuss how we can build a smooth onramp for developers to get started deploying to Kubernetes and deploying clusters themselves. Finally, we’ll revisit the complexity spectrum we mentioned in <a data-type="xref" href="ch01.html#chapter1">Chapter 1</a> and look at some of the levels of abstractions we can put in place. Our objective is to strike a good balance between complexity and flexibility when offering a Kubernetes platform to development teams that have varying degrees of knowledge and desired engagement with the underlying implementation.</p>&#13;
&#13;
<p>Many of the areas in this chapter are covered elsewhere in the book, which we’ll call out where appropriate. Here we aim to cover aspects from the specific stance of increasing team collaboration and building a platform that serves the needs of everyone in the organization. Although on the surface this might seem a light topic, the issues discussed within are often some of the hardest hurdles for many companies to overcome and can make or break the successful adoption of a Kubernetes-based application platform.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Platform Exposure" data-type="sect1"><div class="sect1" id="idm45611970898888">&#13;
<h1>Platform Exposure</h1>&#13;
&#13;
<p>We’ve talked many times in this book about the need to evaluate your individual requirements and ask questions in different areas when designing and implementing a Kubernetes platform.<a data-primary="platform abstractions" data-secondary="platform exposure" data-type="indexterm" id="idm45611970897112"/> One major question that will inform many choices is deciding how much exposure you want development teams to have to the underlying Kubernetes systems and resources. There are several factors that will impact this decision.</p>&#13;
&#13;
<p>Kubernetes is a relatively new technology. In some cases the drive to adopt Kubernetes will come from the infrastructure side of the house, to simplify infrastructure usage and efficiency, or standardize workloads. In other cases the drive may be from development teams that are keen to implement a new technology that they feel can accommodate and accelerate their development and deployment of cloud native applications. Wherever the drive is coming from, impact will be felt in the other teams, whether it be adapting to a new paradigm, learning new tools, or just a change in user experience when interacting with a new platform.</p>&#13;
&#13;
<p>In some organizations, there is a strong requirement that development teams should not be exposed to the underlying platform. The driver for this is the belief that developers should focus on delivering business value and not be distracted by the implementation details of the platform being developed. There is some value to this approach, but in our experience, we don’t always completely agree with it. For example, it’s necessary for developers to have at least <em>some</em> understanding of the base platform in order to effectively develop applications that target it. This doesn’t mean increasing the coupling of the application and the platform, but purely understanding how to maximize the platform capabilities. <a data-type="xref" href="ch14.html#application_considerations_chapter">Chapter 14</a> covers this application and platform relationship in more detail.</p>&#13;
&#13;
<p>For the approach of <em>no developer exposure</em> to be successful, there must be sufficient capacity in the platform team. Firstly because they will be solely responsible for maintaining and supporting the environments, and secondly, that team will also be responsible for building the necessary abstractions required for developers to seamlessly interact with the platform. This point is important, as even when developers are not directly exposed to Kubernetes, they will still need ways of analyzing application performance, debugging issues, and troubleshooting. If giving developers <code>kubectl</code> access to a cluster exposes too much underlying detail, there needs to be an intermediate layer that allows developers to <em>own</em> the application into production while simultaneously not overwhelming them with implementation details. In <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a> we cover many of the main ways to expose debugging tools to development teams in an effective way.</p>&#13;
&#13;
<p>Simply streamlining the troubleshooting experience for developers may not be enough in some organizations. Deploying applications to Kubernetes can also be complex, potentially requiring many components. Maybe an application needs a StatefulSet, PersistentVolumeClaim, Service, and ConfigMap to be successfully deployed. When exposing these concepts to developers is not desirable, platform teams may go a step further and create abstractions in this area. This could be achieved through self-service pipelines or building custom resources (this is covered in <a data-type="xref" href="ch11.html#building_platform_services">Chapter 11</a>) to more simply encapsulate the required pieces. We’ll cover both of these approaches later in this chapter.</p>&#13;
&#13;
<p>A limiting factor when deciding how much of the platform to expose is the skillset and experience of the teams creating the abstractions. For example, platform teams will need to have some development skills and knowledge of Kubernetes API best practices if they want to go down the custom resource route. If they don’t, you may be limited in what abstractions you can build and as such have to expose more of the platform internals to development teams.</p>&#13;
&#13;
<p>In the next section we’ll look at some of the ways we’ve seen teams offer a self-service model to developers (and other end users) in order to streamline and standardize the deployment of both clusters and applications.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Self-Service Onboarding" data-type="sect1"><div class="sect1" id="idm45611970885336">&#13;
<h1>Self-Service Onboarding</h1>&#13;
&#13;
<p>Early on in an organization’s Kubernetes journey it’s likely that platform teams will<a data-primary="onboarding, self-service" data-type="indexterm" id="ix_onb"/> be responsible for the provisioning and configuration of clusters for all the teams that need them. They’ll probably also be responsible for at least helping with the deployment of applications to those clusters.<a data-primary="clusters" data-secondary="provisioning and configuration for teams" data-type="indexterm" id="idm45611970882328"/><a data-primary="tenancy models" data-type="indexterm" id="idm45611970881352"/> Depending on the tenancy model adopted (read more about workload tenancy in <a data-type="xref" href="ch12.html#multi_tenancy_chapter">Chapter 12</a>), there will be different requirements in this setup process. In a single-tenant model, cluster provisioning and configuration may be more straightforward, with a set of common permissions, core services (logging, monitoring, ingress, etc.), and access (e.g., Single Sign-On) setup. However, in a multitenant cluster we may need to create multiple additional components (e.g., Namespaces, Network Policies, Quotas, etc.) for each team and application onboarded.</p>&#13;
&#13;
<p>However, as the organization begins to scale, provisioning and configuring manually is not sustainable. It represents repetitive toil for the platform team and blocks the development teams waiting for manual tasks to complete. Once a base level of maturity has been reached, we usually see teams start to offer some kind of self-service offering to their internal users.<a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="providing self-service onboarding" data-type="indexterm" id="idm45611970878504"/> An effective way of providing this is through an existing CI/CD tool or process like Jenkins or GitLab. Both tools allow the easy creation of pipelines and provide the capability for additional customized inputs at execution time.</p>&#13;
&#13;
<p>The maturity of tools like <code>kubeadm</code> and Cluster API make the automation of cluster creation relatively straightforward and consistent<a data-primary="kubeadm" data-type="indexterm" id="idm45611970876104"/>.<a data-primary="Cluster API" data-type="indexterm" id="idm45611970875272"/> Teams can expose tweakable parameters like cluster name and sizing, for example, and the pipeline can invoke those tools to provision clusters with sensible defaults before outputting appropriate credentials or access to the requesting team. Like many things, this automation can be as sophisticated as you choose to make it. We have seen pipelines that create load balancers and DNS automatically based on the requesting user’s LDAP information, including automatically tagging the underlying infrastructure with the relevant cost centers. Sizing can be open to the user but constrained with certain ranges depending on the team, environment, or project. We can even choose whether to provision in the private or public cloud depending on the classification or security profile of the application. There is a wide array of possibilities for platform teams to create a flexible yet powerful automated provisioning process for development teams.</p>&#13;
&#13;
<p>For multitenant scenarios we’ll<a data-primary="multitenancy" data-type="indexterm" id="idm45611970873048"/> not be creating clusters but rather creating Namespaces and all of the associated objects that allow us to provide a soft-isolation environment for the new application. Again, we can use a similiar pipeline approach, but this time allow development teams to choose the cluster (or clusters) where their application will be deployed to. At a base level we’ll want to generate the following:</p>&#13;
<dl>&#13;
<dt>Namespace</dt>&#13;
<dd>&#13;
<p>For the application to reside in and to provide the logical isolation for our other components to build on.<a data-primary="Namespaces" data-type="indexterm" id="idm45611970869816"/></p>&#13;
</dd>&#13;
<dt>RBAC</dt>&#13;
<dd>&#13;
<p>To ensure that only the appropriate authorized groups can access resources in their own application’s Namespace.<a data-primary="RBAC (role-based access control)" data-type="indexterm" id="idm45611970867704"/></p>&#13;
</dd>&#13;
<dt>Network policies</dt>&#13;
<dd>&#13;
<p>To ensure that the<a data-primary="network policies" data-type="indexterm" id="idm45611970865704"/> application is only allowed to communicate with itself or other shared cluster services, but <em>not</em> other applications on the same cluster (if required).</p>&#13;
</dd>&#13;
<dt>Quotas</dt>&#13;
<dd>&#13;
<p>To limit the amount of r<a data-primary="resources" data-secondary="quotas on" data-type="indexterm" id="idm45611970862936"/>esources that one Namespace or application may consume in the cluster, reducing the potential for a <em>noisy neighbor</em> situation to arise.</p>&#13;
</dd>&#13;
<dt>Limit ranges</dt>&#13;
<dd>&#13;
<p>To set sensible defaults for specific objects created in the Namespace.</p>&#13;
</dd>&#13;
<dt>Pod Security Policies</dt>&#13;
<dd>&#13;
<p>To ensure workloads conform to sensible default security settings, such as not running as a root user.<a data-primary="Pod Security Policies (PSPs)" data-type="indexterm" id="idm45611970858568"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Not all of these are required or necessary in every scenario, although combined they allow cluster administrators and platform teams to create a seamless onboarding experience and deployment environment for new development teams without requiring manual intervention.</p>&#13;
&#13;
<p>As organizations mature in their usage and knowledge of Kubernetes, these pipelines can be implemented in a Kubernetes native way using operators.<a data-primary="operators (Kubernetes)" data-type="indexterm" id="idm45611970856552"/> For example, we might define a <code>Team</code> resource in the following structure:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">examples.namespace-operator.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Team</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">team-a</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">owner</code><code class="p">:</code> <code class="l-Scalar-Plain">Alice</code>&#13;
  <code class="nt">resourceQuotas</code><code class="p">:</code>&#13;
    <code class="nt">pods</code><code class="p">:</code> <code class="s">"50"</code>&#13;
    <code class="nt">storage</code><code class="p">:</code> <code class="s">"300Gi"</code></pre>&#13;
&#13;
<p>In this example we might define a specific <code>Team</code> that we want to be onboarded with an owner (user) and some resource quotas. Our controller that lives in the cluster would be responsible for reading this object and creating the relevant Namespace, RBAC resources, and quotas and tying them together. This approach can be powerful because it allows us to tie in closely with the Kubernetes API and expose a native way of managing and reconciling resources. For instance, if a role were to be accidentally deleted or a quota were to get modified, the controller would be able to automatically remediate the situation. These higher-level types of resources (like a <code>Team</code> or <code>Application</code>) can be great for bootstrapping a cluster also, but just adding several team objects and our controller we’re able to automate all of the relevant configuration ready for use.</p>&#13;
&#13;
<p>We can definitely dig deeper down this rabbit hole to produce a sophisticated automation setup. For instance, let’s think about some of the observability tooling that might need to be configured for new applications.<a data-primary="observability" data-secondary="tooling for new applications" data-type="indexterm" id="idm45611970773080"/> Perhaps we could have our team controller generate and submit customized dashboards for a new team or application and have Grafana automatically reload them. We might dynamically add new alert targets in Alertmanager for new teams or Namespaces. We can create very powerful functionality behind these simpler, more user-friendly onboarding abstractions.<a data-primary="onboarding, self-service" data-startref="ix_onb" data-type="indexterm" id="idm45611970771752"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Spectrum of Abstraction" data-type="sect1"><div class="sect1" id="idm45611970884712">&#13;
<h1>The Spectrum of Abstraction</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch01.html#chapter1">Chapter 1</a> we introduced the idea of a <em>spectrum of abstraction</em>.<a data-primary="abstractions" data-secondary="spectrum of" data-type="indexterm" id="idm45611970768040"/><a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-type="indexterm" id="idm45611970767032"/> In <a data-type="xref" href="#spectrum_of_abstraction">Figure 16-1</a> we have expanded on that original concept and added some concrete levels of abstraction along the spectrum.</p>&#13;
&#13;
<p>In the preceding sections we talked about some of the philosophical decisions and organization constraints that might influence where on this spectrum you might land. In this section we’ll walk through this more detailed spectrum from left (no abstractions) to right (fully abstracted platform) and discuss some of the options and trade-offs as we go.</p>&#13;
&#13;
<figure><div class="figure" id="spectrum_of_abstraction">&#13;
<img alt="prku 1601" src="assets/prku_1601.png"/>&#13;
<h6><span class="label">Figure 16-1. </span>Spectrum of abstraction.</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Command-Line Tooling" data-type="sect2"><div class="sect2" id="idm45611970762072">&#13;
<h2>Command-Line Tooling</h2>&#13;
&#13;
<p>By exposing the Kubernetes API through native command-line tooling we are at the far-left end of the spectrum with no abstractions in place.<a data-primary="command-line tooling" data-type="indexterm" id="idm45611970760344"/><a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-tertiary="command-line tooling" data-type="indexterm" id="idm45611970759640"/><a data-primary="kubectl" data-type="indexterm" id="idm45611970758360"/> In some organizations <code>kubectl</code> will be the primary point of entry to Kubernetes for developers. This might be due to constraints (lack of available support from the platform teams) or choice (familiarity with and desire to work directly on Kubernetes from the developers). There may still be some automation or guardrails in place on the cluster, but developers will interact with it using native tooling.</p>&#13;
&#13;
<p>There are some downsides to this approach (even if your development teams <em>are</em> a little familiar with Kubernetes):</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The manual overhead of having to set up and configure the authentication methods for potentially multiple clusters can be cumbersome. This includes switching contexts between multiple clusters and ensuring that individuals are always targeting the intended cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>The output format of <code>kubectl</code> commands can be cumbersome to view and work with. By default we are given tabular output, but this can be marshaled into different formats and piped to external tooling like <code>jq</code> to more concisely display the information. However, this requires that developers know about <code>kubectl</code>’s options and how to use them (in addition to the external tooling).</p>&#13;
</li>&#13;
<li>&#13;
<p>Raw <code>kubectl</code> opens all the tweaks and knobs of Kubernetes to the user without abstraction/mediation. As such we need to ensure not only that there are appropriate RBAC rules in place to restrict unauthorized access, but also that there is a layer of admission control vetting all the requests coming into the API server.<a data-primary="API server" data-secondary="admission control and" data-type="indexterm" id="idm45611970750504"/></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Various tools can enhance this experience.<a data-primary="kubectl" data-secondary="plug-ins" data-type="indexterm" id="idm45611970748888"/> There are many <code>kubectl</code> plug-ins that can provide a better user experience in the local shell, such as <code>kubens</code> and <code>kubectx</code>, that provide better usability and visibility into Namespaces and contexts, respectively. There are plug-ins that will aggregate logs from multiple Pods, or provide a terminal UI for application health. While not advanced tools, they can remove common pain points and eliminate the need for developers to know the intricacies of some of the underlying implementation details. These additional tools are useful helpers, but we’re still exposing the Kubernetes API directly with barely any abstraction on top.</p>&#13;
&#13;
<p>There are also plug-ins that tie into external auth systems to streamline the authentication flow to abstract the complexity of kubeconfigs, certificates, tokens, etc., away from the user. This is an area where we regularly see some augmentation of the vanilla tooling, as enabling developers to have secure access to multiple clusters (especially those that may come and go dynamically) can be challenging. In noncritical environments, access may be based on key pairs (which must be generated, managed, and distributed), whereas in more stable environments access is likely to be linked to a Single Sign-On system. We have developed command-line utilities for several clients to pull credentials from a central cluster registry based on a local user’s login credentials.</p>&#13;
&#13;
<p>Additionally, you may decide to go down the route of Airbnb. In a <a href="https://oreil.ly/OxTSc">recent QCon talk</a>, Melanie Cebula shared Airbnb’s approach of building out more advanced toolsets both as standalone binaries and <code>kubectl</code> plug-ins to interact with its clusters, hook into image building, deployment, and more.</p>&#13;
&#13;
<p>There is an additional class of tooling that allows developers a graphical interface to interact with the cluster. Recent popular choices here are <a href="https://octant.dev">Octant</a> and <a href="https://k8slens.dev">Lens</a>. Rather than sitting in the cluster as the Kubernetes dashboard does, these tools run locally on a workstation and utilize a <code>kubeconfig</code> to access the cluster. These tools can be a great onramp for developers newer to the platform who want to see a visual representation of the cluster and their applications. Enhancing the client-side experience is the first step that organizations can take to simplify developer interactions with Kubernetes.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Abstraction Through Templating" data-type="sect2"><div class="sect2" id="idm45611970711368">&#13;
<h2>Abstraction Through Templating</h2>&#13;
&#13;
<p>Deploying a single application to Kubernetes can require the creation of multiple Kubernetes objects.<a data-primary="templating, abstraction through" data-type="indexterm" id="ix_tmpl"/><a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-tertiary="templating" data-type="indexterm" id="ix_pltabstmpl"/> For example, a <em>simple</em> Wordpress application may need the &#13;
<span class="keep-together">following</span>:</p>&#13;
<dl>&#13;
<dt>Deployment</dt>&#13;
<dd>&#13;
<p>For describing the image, commands, and properties of the Wordpress instance.</p>&#13;
</dd>&#13;
<dt>StatefulSet</dt>&#13;
<dd>&#13;
<p>For deploying MySQL as a datastore for Wordpress.</p>&#13;
</dd>&#13;
<dt>Services</dt>&#13;
<dd>&#13;
<p>To provide discovery and load balancing for both Wordpress and MySQL.</p>&#13;
</dd>&#13;
<dt>PVC</dt>&#13;
<dd>&#13;
<p>To dynamically create a Volume for the Wordpress data.</p>&#13;
</dd>&#13;
<dt>ConfigMaps</dt>&#13;
<dd>&#13;
<p>To hold configuration for both Wordpress and MySQL.</p>&#13;
</dd>&#13;
<dt>Secrets</dt>&#13;
<dd>&#13;
<p>To hold admin credentials for both Wordpress and MySQL.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In this list we have nearly 10 different objects all to support an extremely small application. Not only that, but there are nuances and expert knowledge needed to configure them. For example, when using a StatefulSet we need to create a special <em>headless</em> Service to front it. We want our developers to be able to deploy their application to the cluster <em>without</em> having to know how to create and configure all of these different Kubernetes object types themselves.</p>&#13;
&#13;
<p>One of the ways we can ease the user experience when deploying these applications is by only exposing a small set of inputs and generating the rest of the boilerplate behind the scenes. This approach doesn’t require developers to know about all the fields in all the objects, but it still exposes some of the underlying objects and uses tools that are a level up from pure kubectl. The tools that have some maturity in this area are templating tools like Helm and Kustomize.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Helm" data-type="sect3"><div class="sect3" id="idm45611970694632">&#13;
<h3>Helm</h3>&#13;
&#13;
<p>Helm has become a popular tool in the Kubernetes ecosystem over the past couple of years.<a data-primary="Helm" data-type="indexterm" id="idm45611970692856"/> We realize that it has capabilities over and above just templating, but in our experience have found the templating use case (followed by editing and application of manifests) more compelling over some of its life cycle management features.</p>&#13;
&#13;
<p>Following is a snippet from the Wordpress Helm chart (package describing an application) describing a Service:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">ports</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">http</code>&#13;
    <code class="nt">port</code><code class="p">:</code> <code class="p-Indicator">{{</code> <code class="nv">.Values.service.port</code> <code class="p-Indicator">}}</code>&#13;
    <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">http</code></pre>&#13;
&#13;
<p>This template is not directly exposed to developers but is a template that will use an injected or defined value from elsewhere. In the case of Helm, this can be passed on the command line or more commonly through a values file:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="c1">## Kubernetes configuration</code>&#13;
<code class="c1">## For minikube, set this to NodePort, elsewhere use LoadBalancer or ClusterIP</code>&#13;
<code class="c1">##</code>&#13;
<code class="nt">service</code><code class="p">:</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">LoadBalancer</code>&#13;
  <code class="c1">## HTTP Port</code>&#13;
  <code class="c1">##</code>&#13;
  <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
&#13;
<p>Charts contain a default <em>Values.yaml</em> file with sensible settings, but developers can provide an override where they modify only the settings they need. This allows powerful customization via the templating without needing in-depth knowledge. Rather than purely templating values, Helm also contains functionality for basic logic operations, allowing a single tweak in the values file to generate or modify large sections in the underlying templates.</p>&#13;
&#13;
<p>For instance, in the example values file just shown, there is a <code>type: Loadbalancer</code> declaration. This is injected directly into the template in several places, but it is also responsible for triggering more complex templating through the use of conditionals and built-in functions, as shown in the following code snippet:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="p-Indicator">{{</code> <code class="nv">.Values.service.type</code> <code class="p-Indicator">}}</code>&#13;
  <code class="p-Indicator">{{</code><code class="nv">- if (or (eq .Values.service.type "LoadBalancer")</code>&#13;
    <code class="nv">(eq .Values.service.type "NodePort"))</code> <code class="p-Indicator">}}</code>&#13;
  <code class="nt">externalTrafficPolicy</code><code class="p">:</code> <code class="p-Indicator">{{</code> <code class="nv">.Values.service.externalTrafficPolicy | quote</code> <code class="p-Indicator">}}</code>&#13;
  <code class="p-Indicator">{{</code><code class="nv">- end</code> <code class="p-Indicator">}}</code>&#13;
  <code class="p-Indicator">{{</code><code class="nv">- if (and (eq .Values.service.type "LoadBalancer")</code>&#13;
    <code class="nv">.Values.service.loadBalancerSourceRanges)</code> <code class="p-Indicator">}}</code>&#13;
  <code class="nt">loadBalancerSourceRanges</code><code class="p">:</code>&#13;
  <code class="p-Indicator">{{</code><code class="nv">- with .Values.service.loadBalancerSourceRanges</code> <code class="p-Indicator">}}</code>&#13;
<code class="p-Indicator">{{</code> <code class="nv">toYaml . | indent 4</code> <code class="p-Indicator">}}</code>&#13;
  <code class="p-Indicator">{{</code><code class="nv">- end</code> <code class="p-Indicator">}}</code>&#13;
  <code class="p-Indicator">{{</code><code class="nv">- end</code> <code class="p-Indicator">}}</code></pre>&#13;
&#13;
<p>This inline logic may look complex and certainly has its detractors. However, the complexity is owned by the <em>creators</em> of the charts, and not the end-user development teams. The construction of the complex YAML structures in the template is keyed from a single <code>type</code> key in the values file, which is the interface for the developer to modify the configuration. The values file can be specified at runtime so different files (with different configurations) can be used for different clusters, teams, or &#13;
<span class="keep-together">environments</span>.</p>&#13;
&#13;
<p>Implementing Helm for configuring and deploying both third-party and internal applications can be an effective first step to abstracting some of the underlying platform from developers and allowing them to focus more closely on only the options they need. However, there are still some disadvantages. The interface (<em>Values.yaml</em>) is still YAML and can be an unfriendly user experience if developers need to explore the templates to understand the impact of a change (although good documentation can mitigate this).</p>&#13;
&#13;
<p>For those who want to go a step further, we’ve seen tools developed that will abstract these tweakable items to a user interface. This allows a more native approach under the hood, but the user experience can be customized depending on the requirements of the audience.<a data-primary="workflows" data-type="indexterm" id="idm45611970554584"/> For example, workflows can be built into an existing deployment tool (like Jenkins) or a ticketing type of service, but the underlying output can still be Kubernetes manifests that are then applied to a cluster. While powerful, these models can get complex to maintain, and the abstractions can eventually leak through to the user.</p>&#13;
&#13;
<p>An interesting take on this model that has recently appeared is the <a href="https://app.getambassador.io/initializer">K8s Initializer by Ambassador Labs</a>. Using a browser-based UI workflow, the user is asked multiple questions about the type of service they want to deploy and the target platform. The site then outputs a downloadable package for the user to apply to the cluster with all the customizations applied.</p>&#13;
&#13;
<p>All the templating approaches have many of the same strengths and weaknesses. We are still dealing with Kubernetes native objects that are applied to the cluster. For example, when outputting our Helm files with completed values we’re still exposed to Services, StatefulSets, and more. This isn’t a complete abstraction of the platform, so developers are still required to have <em>some level</em> of underlying knowledge. However, on the flip side, that’s also an <em>advantage</em> of this approach (either with Helm, or the more abstracted approach from K8s Initializer). If upstream Helm charts or the Initializer do not output exactly what we need, we still have the full flexibility to modify the results before applying to the cluster.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kustomize" data-type="sect3"><div class="sect3" id="idm45611970694008">&#13;
<h3>Kustomize</h3>&#13;
&#13;
<p>Kustomize is a flexible tool that can be used standalone or as part of <code>kubectl</code> to apply abitrary additions, deletions, and modifications to fields in any Kubernetes YAML objects. <a data-primary="Kustomize" data-type="indexterm" id="idm45611970548248"/>It is not a templating tool but is useful when leveraged on a set of manifests that have been templated by Helm as a method of modifying fields that Helm does not otherwise expose.</p>&#13;
&#13;
<p>For the reasons previously discussed, we have seen Helm as a templating tool piped into something like Kustomize for additional customizations as a very powerful abstraction that also allows full flexibility. This approach sits somewhere in the middle of the spectrum and is often a sweet spot for organizations. In the next section we’ll move further to the right on the abstraction spectrum and see how we can start encapsulating underlying objects with custom resources tailored specifically to each organization/use case.<a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-startref="ix_pltabstmpl" data-tertiary="templating" data-type="indexterm" id="idm45611970546296"/><a data-primary="templating, abstraction through" data-startref="ix_tmpl" data-type="indexterm" id="idm45611970544808"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Abstracting Kubernetes Primitives" data-type="sect2"><div class="sect2" id="idm45611970543640">&#13;
<h2>Abstracting Kubernetes Primitives</h2>&#13;
&#13;
<p>As we’ve spoken about many times in this book, Kubernetes provides a set of primitive objects and API patterns.<a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-tertiary="abstracting Kubernetes primitives" data-type="indexterm" id="ix_pltabsKupr"/> In combination these allow us to build higher-level abstractions and custom resources to capture types and ideas that are not built in. In late 2019 the social media company Pinterest published an interesting blog post describing how it had created CRDs (and associated controllers) to model its internal workloads as a way of abstracting Kubernetes native building blocks away from its development teams. Pinterest summarized its rationale for this approach as such:</p>&#13;
<blockquote>&#13;
<p>On the other hand, the Kubernetes native workload model, such as deployment, jobs and daemonsets, are not enough for modeling our own workloads. Usability issues are huge blockers on the way to adopt Kubernetes. For example, we’ve heard service developers complaining about missing or misconfigured Ingress messing up their endpoints. We’ve also seen batch job users using template tools to generate hundreds of copies of the same job specification and ending up with a debugging nightmare.</p>&#13;
<p data-type="attribution">Lida Li, June Liu, Rodrigo Menezes, Suli Xu, Harry Zhang, and Roberto Rodriguez Alcala; <a href="https://oreil.ly/Ovmgh">“Building a Kubernetes platform at Pinterest”</a></p>&#13;
</blockquote>&#13;
&#13;
<p>In the following code snippet, <code>PinterestService</code> is an example of Pinterest’s custom internal resources. The 25-line object creates multiple Kubernetes native objects that would equate to more than 350 lines if created directly:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">pinterest.com/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">PinterestService</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">exampleservice</code>&#13;
  <code class="nt">project</code><code class="p">:</code> <code class="l-Scalar-Plain">exampleproject</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">iamrole</code><code class="p">:</code> <code class="l-Scalar-Plain">role1</code>&#13;
  <code class="nt">loadbalancer</code><code class="p">:</code>&#13;
    <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>&#13;
  <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
  <code class="nt">sidecarconfig</code><code class="p">:</code>&#13;
    <code class="nt">sidecar1</code><code class="p">:</code>&#13;
      <code class="nt">deps</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">example.dep</code>&#13;
    <code class="nt">sidecar2</code><code class="p">:</code>&#13;
      <code class="nt">log_level</code><code class="p">:</code> <code class="l-Scalar-Plain">info</code>&#13;
  <code class="nt">template</code><code class="p">:</code>&#13;
    <code class="nt">spec</code><code class="p">:</code>&#13;
      <code class="nt">initcontainers</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">init</code>&#13;
        <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">gcr.io/kuar-demo/kuard-amd64:1</code>&#13;
      <code class="nt">containers</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">init</code>&#13;
        <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">gcr.io/kuar-demo/kuard-amd64:1</code></pre>&#13;
&#13;
<p>This is an extension of the templating model we saw in the previous section where only certain inputs are exposed to the end user. However, in this case we can construct an input object that makes sense in the context of the application (rather than a relatively unstructured <em>Values.yaml</em> file) and be more intuitively understood by the developers. While it’s still possible for leaky abstractions to occur with this approach, it’s less likely as the platform team (creating the CRDs/operator) have full control of how to create and modify the underlying resources rather than having to work within the constraints of the existing objects as with the Helm approach. They also have the ability (with the controller) to craft much more sophisticated logic through a general-purpose programming language instead of being limited by Helm’s built-in functions. However, as we discussed earlier, this comes with the trade-off that platform teams must now have programming expertise. For more depth on creating platform services and operators take a look at <a data-type="xref" href="ch11.html#building_platform_services">Chapter 11</a>.</p>&#13;
&#13;
<p>By utilizing an operator, we can also call out to external APIs to integrate richer functionality into our abstracted object types. For example, one client had an internal DNS system that all applications needed to be registered with to work correctly and be exposed to external clients. The incumbent flow would have developers visit a web portal and manually add the location of their service and the ports they needed forwarded from their assigned DNS name. We have a couple of options to enhance the developer experience.</p>&#13;
&#13;
<p>If we’re utilizing native Kubernetes objects (like Ingress in this case), we can create an operator that will read a special annotation on the applied Ingress and automatically register the application with the DNS service. This might look like the following:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
  <code class="nt">annotations</code><code class="p">:</code>&#13;
    <code class="nt">company.ingress.required</code><code class="p">:</code> <code class="l-Scalar-Plain">true</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">rules</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="s">"my-app"</code>&#13;
    <code class="nt">http</code><code class="p">:</code>&#13;
      <code class="nt">paths</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>&#13;
        <code class="nt">backend</code><code class="p">:</code>&#13;
          <code class="nt">service</code><code class="p">:</code>&#13;
            <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
            <code class="nt">port</code><code class="p">:</code>&#13;
              <code class="nt">number</code><code class="p">:</code> <code class="l-Scalar-Plain">8000</code></pre>&#13;
&#13;
<p>Our controller would read the <code>company.ingress.required: true</code> annotation, and depending on the name of the application, the Namespace or some other metadata could go and register the appropriate DNS records, as well as potentially modifying the host field depending on certain rules. While it reduces a lot of the manual work (of creating the records) required by the developer, it still requires some knowledge/creation of Kubernetes objects (in this case, the Ingress). In that way, it is more in line with the level of abstraction described in the previous section.</p>&#13;
&#13;
<p>Another option is to use a custom resource like the <code>PinterestService</code>. We have all the information we need encapsulated there, and we can create the Ingress via our operator, as well as configuring external services like the DNS system. We haven’t leaked any of the underlying abstraction through to the developer and have full flexibility with our implementation.<a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-startref="ix_pltabsKupr" data-tertiary="abstracting Kubernetes primitives" data-type="indexterm" id="idm45611970320552"/></p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611970318984">&#13;
<h5>Supporting Differing Levels of Abstraction</h5>&#13;
<p>When deciding on the right level of abstractions to offer with your organization, it’s important to think about how you’ll document and support them.<a data-primary="platform abstractions" data-secondary="supporting different levels of abstraction" data-type="indexterm" id="idm45611970399352"/> For instance, when using something like Helm (or other templating tools) or an approach that distills down to raw Kubernetes objects (Pods, PVCs, ConfigMaps, etc.) you are able to leverage a huge amount of community resources when troubleshooting.</p>&#13;
&#13;
<p>An example of this can be a status condition on a PVC describing why storage couldn’t be bound, or perhaps an error message coming from Helm when trying to install a Chart with a misconfigured template. Both of these occurrences can be easily searched online due to the mature and extensive documentation and shared community experience with those commonly utilized tools and objects.</p>&#13;
&#13;
<p>When fully abstracting away those objects (either behind pipelines where developers don’t have raw debugging acccess or behind custom resource types like <code>PinterestService</code>), it can be a lot more difficult for development teams to tap into those shared community sources if the scope of usage is very narrow (potentially just internal to your organization). In these cases good documentation is essential, but also providing a <em>window</em> to those underlying internals can be useful in break-glass situations.</p>&#13;
&#13;
<p>The break-glass approach is especially powerful as it allows end users to experience a default high level of abstraction, minimizing day-to-day friction, but be able to dive in and choose to consume a different level of abstraction if required or desirable (based on individual skillsets or knowledge). In our experience this model should be the ideal model when designing platform abstractions.</p>&#13;
&#13;
<p>Another relevant aspect here is the <em>transferability</em> of skills.<a data-primary="transferability of skills" data-type="indexterm" id="idm45611970393768"/> As upstream Kubernetes skills become more commonplace, it will be easier for those people to interact with and troubleshoot the platform if the underlying internals are accessible or exposed. It also may be more advantageous to have a more vanilla platform (or at least one where the layers are accessible) to attract talent who may not want to get deeply ingrained in any specific downstream platform/distribution for fear of narrowing their skillset.</p>&#13;
</div></aside>&#13;
&#13;
<p>Even with the custom resource and operator approach we’ve discussed in this section, we are still exposing some of the core mechanics of the platform to development teams. We need to specify valid Kubernetes metadata, API versions, and resource types. We also expose YAML (unless we’re also providing a pipeline, wrapper, or UI to build it) and the quirks associated with it. In the next (and final) section we’ll move fully to the right on our abstraction spectrum and talk about some of the options that allow developers to go directly from their application code to the platform and not even necessarily know about Kubernetes at all.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Making Kubernetes Invisible" data-type="sect2"><div class="sect2" id="idm45611970543048">&#13;
<h2>Making Kubernetes Invisible</h2>&#13;
&#13;
<p>In previous sections we’ve been moving from left to right on the spectrum of abstraction, starting with the least abstracted (raw <code>kubectl</code> access) and now ending with a fully abstracted platform.<a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-tertiary="making Kubernetes invisible" data-type="indexterm" id="ix_pltabsKuinv"/> In this section we’ll talk about cases and tooling where developers don’t even know they’re using Kubernetes and whose only interface to the platform (more or less) is committing/pushing code, allowing them to retain a fairly narrow (and deep) focus without being exposed to platform nuances.</p>&#13;
&#13;
<p>SaaS providers like Heroku and tools like Cloud Foundry popularized the developer-focused <em>push</em> experience over 10 years ago.<a data-primary="Heroku" data-type="indexterm" id="idm45611970386248"/><a data-primary="Cloud Foundry" data-type="indexterm" id="idm45611970385544"/> The concept is that the tooling (once configured) would provide a platform as a service (PaaS, now a nebulous term) that contained all of the necessary complementary components for an application to function (observability stacks, some form of routing/traffic management, software catalogs, etc.) and would allow developers to <em>simply</em> push code to a source repository. Specialized components within the platform would set appropriate resource limits, provision (if necessary) environments for the code to run, and plumb together the standard PaaS components to enable a streamlined end-user experience.</p>&#13;
&#13;
<p>You might be thinking that there is some crossover with Kubernetes here, which also provides us some primitives to enable some similar functionality. When the original PaaS platforms were built, Docker and Kubernetes didn’t exist and the prevalence of more rudimentary containerized workloads was very limited. Hence, these tools were built from the ground up for a virtual machine–based environment. We are now increasingly seeing these tools (and other new ones) be ported to or rewritten for Kubernetes for exactly the reason we identified earlier. Kubernetes provides very strong mechanical foundations, API conventions, and raw primitives to <em>build</em> these higher-level platforms atop of it.</p>&#13;
&#13;
<p>One of the criticisms that is often leveled at Kubernetes is that it introduces a non-trivial amount of additional complexity into an environment, both for operations and development teams (on top of the paradigm shift to containers that must also be negotiated). However, this perspective misses one of the primary aims of Kubernetes which (as cofounder Joe Beda has articulated many times) is to be a platform <em>for building platforms</em>. Complexity will always exist somewhere, but through its architectural decisions and primitives, Kubernetes allows us to abstract the complexity to platform developers, vendors, and the open source community for them to build seamless development and deployment experiences <em>upon</em> Kubernetes.</p>&#13;
&#13;
<p>We already mentioned Cloud Foundry, which is probably the most popular and successful open source PaaS (now ported to Kubernetes), and there are other fairly mature options like Google App Engine (and some other serverless technologies) and parts of RedHat OpenShift. In addition to these we’re seeing more platforms appear as the space matures. One such popular platform is <a href="https://backstage.io">Backstage</a>, originally created by Spotify. Now a CNCF Sandbox project, it is a platform for &#13;
<span class="keep-together">building</span> portals that &#13;
<span class="keep-together">provide</span> tailored abstractions for developers to deploy and manage applications. Even as we write this chapter, HashiCorp (developer of many cloud native OSS tools like Vault and Consul) has just announced Project Waypoint, a new tool to separate end users from the underlying deployment platforms and provide a high-level abstraction for development teams. In their announcement blog post they wrote:</p>&#13;
<blockquote>&#13;
<p>We built Waypoint for one simple reason: developers just want to deploy.</p>&#13;
<p data-type="attribution">Mitchell Hashimoto, <a href="https://oreil.ly/ZhTJ4">“Announcing HashiCorp Waypoint”</a></p>&#13;
</blockquote>&#13;
&#13;
<p>Waypoint aims to encapsulate the build, deploy, and release stages of software development. With Waypoint the developers still have to create (or have help creating) a configuration file that describes their process, akin to a Dockerfile except it describes the full set of stages in a minimal way, soliciting only the essential inputs. An example of this configuration is as follows:</p>&#13;
&#13;
<pre data-type="programlisting">project = "example-nodejs"&#13;
&#13;
app "example-nodejs" {&#13;
  labels = {&#13;
      "service" = "example-nodejs",&#13;
      "env" = "dev"&#13;
  }&#13;
&#13;
  build {&#13;
    use "pack" {}&#13;
    registry {&#13;
        use "docker" {&#13;
          image = "example-nodejs"&#13;
          tag = "1"&#13;
          local = true&#13;
        }&#13;
    }&#13;
 }&#13;
&#13;
  deploy {&#13;
    use "kubernetes" {&#13;
    probe_path = "/"&#13;
    }&#13;
  }&#13;
&#13;
  release {&#13;
    use "kubernetes" {&#13;
    }&#13;
  }&#13;
}</pre>&#13;
&#13;
<p>Note that Waypoint’s approach is still to push <em>some</em> complexity onto the developer (writing this file); however, they have abstracted a huge number of decisions away. Abstracting the platform doesn’t always mean that all complexity is removed from the process, or that no one has to learn anything new. Instead, as in this case, we can introduce a new, simplified interface <em>at the right level</em> of abstraction that hits the sweet spot of speed and flexibility. In Waypoint’s case even the underlying platform can be switched out in the deploy and release stages to use something like Hashicorp’s own Nomad, or some other orchestration engine. All of the underlying details and logic are abstracted by the platform. As Kubernetes and these other platforms evolve and become more stable and <em>boring</em> (some would argue we are nearly there), then the real innovation will continue in the development of higher-level platforms to better enable development teams to deliver more rapid business value.<a data-primary="platform abstractions" data-secondary="spectrum of abstractions" data-startref="ix_pltabsKuinv" data-tertiary="making Kubernetes invisible" data-type="indexterm" id="idm45611970370184"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611970390744">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter we’ve discussed the different layers of abstraction that platform teams can offer to their users (usually development teams) and the common tools and patterns we’ve seen used to implement them. Probably more than any other area, this is where organizational culture, history, tooling, skillsets, and more will all inform any decisions and trade-offs that you choose, and almost every client we’ve worked with has chosen to solve the issues described in this chapter in slightly different ways.</p>&#13;
&#13;
<p>It’s also important to note that while we often have espoused the value of having development teams not having to concern themselves too much with the underlying deployment platform, that is <em>not</em> to say that this is always the right choice or that developers should never understand where and how their applications are running. This information is key to being able to leverage specific features of the platform, for instance, or being able to debug issues with their software. As always, maintaining a strong balance is often the most successful way forward.<a data-primary="platform abstractions" data-startref="ix_pltabs" data-type="indexterm" id="idm45611970365336"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>