<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 22. Organizing Your Application" data-type="chapter" epub:type="chapter"><div class="chapter" id="organizing_your_application">
<h1><span class="label">Chapter 22. </span>Organizing Your Application</h1>
<p>Throughout this book we have described various components of an application built on top of Kubernetes.<a data-primary="applications" data-secondary="organizing Kubernetes applications" data-type="indexterm" id="ix_apporg"/> We have described how to wrap programs up as containers, place those containers in Pods, replicate those Pods with ReplicaSets, and roll them out with Deployments. We have even described how to deploy stateful and real-world applications that collect these objects into a single distributed system. But we have not covered how to actually work with such an application in a practical way. How can you lay out, share, manage, and update the various configurations that make up your application? That is the topic of this chapter.<a data-primary="organizing your application" data-see="applications" data-type="indexterm" id="idm45664067237872"/></p>
<section data-pdf-bookmark="Principles to Guide Us" data-type="sect1"><div class="sect1" id="idm45664067236784">
<h1>Principles to Guide Us</h1>
<p>Before digging into the concrete details of how to structure your application, it’s worth considering the goals that drive this structure.<a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="guiding principles" data-type="indexterm" id="idm45664067235152"/> Obviously, reliability and agility are the general goals of developing a cloud native application in Kubernetes, but how does this relate to how you design your application’s maintenance and deployment? The following sections describe three principles that can guide you in designing a structure that best suits these goals. The principles are:</p>
<ul>
<li>
<p>Treat filesystems as the source of truth</p>
</li>
<li>
<p>Conduct code review to ensure the quality of changes</p>
</li>
<li>
<p>Use feature flags to stage rollouts and rollbacks</p>
</li>
</ul>
<section data-pdf-bookmark="Filesystems as the Source of Truth" data-type="sect2"><div class="sect2" id="idm45664067230496">
<h2>Filesystems as the Source of Truth</h2>
<p>When you first begin to explore Kubernetes, as we did in the beginning of this book, you generally interact with it imperatively.<a data-primary="filesystems" data-secondary="filesystems as source of truth" data-type="indexterm" id="idm45664067228880"/><a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="filesystems as source of truth" data-type="indexterm" id="idm45664067227840"/> You run commands like <code>kubectl run</code> or <code>kubectl edit</code> to create and modify Pods or other objects running in your cluster. Even when we started exploring how to write and use YAML files, this was presented in an ad-hoc manner, as if the file itself is just a way station on the road to modifying the state of the cluster. In reality, in a true productionized application the opposite should be true.</p>
<p>Rather than viewing the state of the cluster—the data in <code>etcd</code>—as the source of truth, it is optimal to view the filesystem of YAML objects as the source of truth for your application.<a data-primary="state" data-secondary="viewing cluster state as source of truth" data-type="indexterm" id="idm45664067224256"/> The API objects deployed into your Kubernetes cluster(s) are then a reflection of the truth stored in the filesystem.</p>
<p>There are numerous reasons why this is the right point of view. The first and foremost is that it largely enables you to treat your cluster as if it is immutable infrastructure. As we have moved into cloud native architectures, we have become increasingly comfortable with the notion that our applications and their containers are immutable infrastructure, but treating a cluster as such is less common. And yet, the same reasons for moving our applications to immutable infrastructure apply to our clusters. If your cluster is a snowflake you made by applying random YAML files downloaded from the internet ad hoc, it is as dangerous as a virtual machine built from imperative bash scripts.</p>
<p>Additionally, managing the cluster state via the filesystem makes it very easy to collaborate with multiple team members.<a data-primary="state" data-secondary="managing cluster state via the filesystem" data-type="indexterm" id="idm45664067221984"/> Source-control systems are well understood and can easily enable multiple people to edit the state of the cluster simultaneously, while making conflicts (and the resolution of those conflicts) clear to <span class="keep-together">everyone</span>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It is absolutely a first principle that <em>all applications deployed to Kubernetes should first be described in files stored in a filesystem</em>. The actual API objects are then just a projection of this filesystem into a particular cluster.</p>
</div>
</div></section>
<section data-pdf-bookmark="The Role of Code Review" data-type="sect2"><div class="sect2" id="idm45664067218032">
<h2>The Role of Code Review</h2>
<p>It wasn’t long ago that code review for application source code was a novel idea. <a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="role of code review" data-type="indexterm" id="idm45664067216768"/><a data-primary="code review" data-type="indexterm" id="idm45664067215504"/>But it is clear now that multiple people looking at a piece of code before it is committed to an application is a best practice for producing high-quality, reliable code.</p>
<p>It is therefore surprising that the same is somewhat less true for the configurations used to deploy those applications. All of the same reasons for reviewing code apply directly to application configurations.<a data-primary="configurations" data-secondary="application, code review for" data-type="indexterm" id="idm45664067213904"/> But when you think about it, it is also obvious that code review of these configurations is critical to the reliable deployment of services. In our experience, most service outages are self-inflicted via unexpected consequences, typos, or other simple mistakes. Ensuring that at least two people look at any configuration change significantly decreases the probability of such errors.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The second principle of our application layout is that it must facilitate the review of every change merged into the set of files that represents the source of truth for our cluster.</p>
</div>
</div></section>
<section data-pdf-bookmark="Feature Gates" data-type="sect2"><div class="sect2" id="idm45664067211232">
<h2>Feature Gates</h2>
<p>Once your application source code and your deployment configuration files are in source control, one of the most common questions is how these repositories relate to one another.<a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="feature gates" data-type="indexterm" id="idm45664067209536"/><a data-primary="feature gates" data-type="indexterm" id="idm45664067208272"/> Should you use the same repository for application source code and configuration? This can work for small projects, but in larger projects it often makes sense to separate the two. Even if the same people are responsible for both building and deploying the application, the perspectives of the builder versus those of the deployer are different enough that this separation of concerns makes sense.</p>
<p>If that is the case, then how do you bridge the development of new features in source control with the deployment of those features into a production environment? This is where feature gates play an important role.</p>
<p>The idea is that when some new feature is developed, that development takes place entirely behind a feature flag or <em>gate</em>. This gate looks something like:</p>
<pre data-type="programlisting">if (featureFlags.myFlag) {
    // Feature implementation goes here
}</pre>
<p>There are a variety of benefits to this approach. First, it lets the team commit to the production branch long before the feature is ready to ship. This enables feature development to stay much more closely aligned with the <code>HEAD</code> of a repository, and thus you avoid the horrendous merge conflicts of a long-lived branch.</p>
<p>Working behind a feature flag also means that enabling a feature simply involves making a configuration change to activate the flag. This makes it very clear what changed in the production environment, and very simple to roll back the feature activation if it causes problems.</p>
<p>Using feature flags thus both simplifies debugging and ensures that disabling a feature doesn’t require a binary rollback to an older version of the code that would remove all of the bug fixes and other improvements made by the newer version.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The third principle of application layout is that code lands in source control, by default off, behind a feature flag, and is only activated through a code-reviewed change to configuration files.</p>
</div>
</div></section>
</div></section>
<section data-pdf-bookmark="Managing Your Application in Source Control" data-type="sect1"><div class="sect1" id="idm45664067201920">
<h1>Managing Your Application in Source Control</h1>
<p>Now that we have determined that the filesystem should represent the source of truth for your cluster, the next important question is how to actually lay out the files in the filesystem.<a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="managing application in source control" data-type="indexterm" id="ix_apporgsrcctrl"/><a data-primary="source control" data-secondary="managing your application in" data-type="indexterm" id="ix_srcctrl"/> Obviously, filesystems contain hierarchical directories, and a source-control system adds concepts like tags and branches, so this section describes how to put these together to represent and manage your application.</p>
<section data-pdf-bookmark="Filesystem Layout" data-type="sect2"><div class="sect2" id="idm45664067197568">
<h2>Filesystem Layout</h2>
<p>This section describes how to lay out an instance of your application for a single cluster.<a data-primary="source control" data-secondary="managing your application in" data-tertiary="filesystem layout" data-type="indexterm" id="idm45664067195984"/><a data-primary="filesystems" data-secondary="filesystem layout" data-type="indexterm" id="idm45664067194720"/> In later sections, we will describe how to parameterize this layout for multiple instances. It’s worth getting this organization right when you begin. Much like modifying the layout of packages in source control, modifying your deployment configurations after the fact is a complicated and expensive refactor that you’ll probably never get around to.</p>
<p>The first cardinality on which you want to organize your application is the semantic component or layer (for instance, <em>frontend</em> or <em>batch work queue</em>). Though early on this might seem like overkill, since a single team manages all of these components, it sets the stage for team scaling—eventually, different teams (or subteams) may be responsible for each of these components.</p>
<p>Thus, for an application with a frontend that uses two services, the filesystem might look like this:</p>
<pre data-type="programlisting">frontend/
service-1/
service-2/</pre>
<p>Within each of these directories, the configurations for each application are stored. These are the YAML files that directly represent the current state of the cluster. It’s generally useful to include both the service name and the object type within the same file.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>While Kubernetes allows you to create YAML files with multiple objects in the same file, this is generally an antipattern. The only good reason to group several objects in the same file is if they are conceptually identical. When deciding what to include in a single YAML file, consider design principles similar to those for defining a class or struct. If grouping the objects together doesn’t form a single concept, they probably shouldn’t be in a single file.</p>
</div>
<p>Thus, extending our previous example, the filesystem might look like:</p>
<pre data-type="programlisting">frontend/
   frontend-deployment.yaml
   frontend-service.yaml
   frontend-ingress.yaml
service-1/
   service-1-deployment.yaml
   service-1-service.yaml
   service-1-configmap.yaml
...</pre>
</div></section>
<section data-pdf-bookmark="Managing Periodic Versions" data-type="sect2"><div class="sect2" id="idm45664067187904">
<h2>Managing Periodic Versions</h2>
<p>What about managing releases? It is very useful to be able to look back and see what your application deployment previously looked like.<a data-primary="source control" data-secondary="managing your application in" data-tertiary="periodic versions" data-type="indexterm" id="idm45664067186272"/><a data-primary="versioning" data-secondary="managing periodic versions" data-type="indexterm" id="idm45664067184960"/> Similarly, it is very useful to be able to iterate a configuration forward while still deploying a stable release configuration.</p>
<p>Consequently, it’s handy to be able to simultaneously store and maintain multiple revisions of your configuration. There are two different approaches that you can use with the file and version control systems we’ve outlined here. The first is to use tags, branches, and source-control features. This is convenient because it maps to the way people manage revisions in source control, and leads to a more simplified directory structure. The other option is to clone the configuration within the filesystem and use directories for different revisions. This makes viewing the configurations simultaneously very straightforward.</p>
<p>These approaches have the same capabilities in terms of managing different release versions, so it is ultimately an aesthetic
choice between the two. We will discuss both approaches and let you or your team
decide which you prefer.</p>
<section data-pdf-bookmark="Versioning with branches and tags" data-type="sect3"><div class="sect3" id="idm45664067182800">
<h3>Versioning with branches and tags</h3>
<p>When you use branches and tags to manage configuration revisions, the directory
structure does not change from the example in the previous section.<a data-primary="versioning" data-secondary="managing periodic versions" data-tertiary="versioning with branches and tags" data-type="indexterm" id="idm45664067181168"/><a data-primary="tags" data-secondary="source control" data-type="indexterm" id="idm45664067179840"/> When you are ready for a release, you place a source-control tag (such as <code>git tag v1.0</code>) in the configuration source-control system. The tag represents the configuration used for that version, and the <code>HEAD</code> of source control continues to iterate forward.</p>
<p>Updating the release configuration is somewhat more complicated, but the approach models what you would do in source control.<a data-primary="branches, versioning with" data-type="indexterm" id="idm45664067176896"/> First, you commit the change to the <code>HEAD</code> of the repository. Then you create a new branch named <code>v1</code> at the <code>v1.0</code> tag. You cherry-pick the desired change onto the release branch (<code>git cherry-pick <em>&lt;edit&gt;</em></code>), and finally, you tag this branch with the <code>v1.1</code> tag to indicate a new point release. This approach is illustrated in <a data-type="xref" href="#fig2201">Figure 22-1</a>.</p>
<figure><div class="figure" id="fig2201">
<img alt="kur3 2201" height="681" src="assets/kur3_2201.png" width="1352"/>
<h6><span class="label">Figure 22-1. </span>Cherry-pick workflow</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>One common error when cherry-picking fixes into a release branch is to only pick the change into the latest release. It’s a good idea to cherry-pick it into all active releases, in case you need to roll back versions but the fix is still needed.</p>
</div>
</div></section>
<section data-pdf-bookmark="Versioning with directories" data-type="sect3"><div class="sect3" id="idm45664067169376">
<h3>Versioning with directories</h3>
<p>An alternative to using source-control features is to use filesystem features.<a data-primary="directories, versioning with" data-type="indexterm" id="idm45664067167904"/><a data-primary="versioning" data-secondary="managing periodic versions" data-tertiary="versioning with directories" data-type="indexterm" id="idm45664067167184"/> In this approach, each versioned deployment exists within its own directory. For example, the filesystem for your application might look like this:</p>
<pre data-type="programlisting">frontend/
  v1/
    frontend-deployment.yaml
    frontend-service.yaml
  current/
    frontend-deployment.yaml
    frontend-service.yaml
service-1/
  v1/
     service-1-deployment.yaml
     service-1-service.yaml
  v2/
     service-1-deployment.yaml
     service-1-service.yaml
  current/
     service-1-deployment.yaml
     service-1-service.yaml
...</pre>
<p>Thus, each revision exists in a parallel directory structure within a directory associated with the release. All deployments occur from <code>HEAD</code> instead of from specific revisions or tags. You would add a new configuration to the files in the <em>current</em> directory.</p>
<p>When creating a new release, you copy the <em>current</em> directory to create a new directory associated with the new release.</p>
<p>When you’re performing a bug-fix change to a release, your pull request must modify the YAML file in all the relevant release directories. This is a slightly better experience than the cherry-picking approach described earlier, since it is clear in a single change request that all of the relevant versions are being updated with the same change, instead of requiring a cherry-pick per version.<a data-primary="applications" data-secondary="organizing Kubernetes applications" data-startref="ix_apporgsrcctrl" data-tertiary="managing application in source control" data-type="indexterm" id="idm45664067161584"/><a data-primary="source control" data-secondary="managing your application in" data-startref="ix_srcctrl" data-type="indexterm" id="idm45664067160032"/></p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Structuring Your Application for Development, &#10;Testing, and Deployment" data-type="sect1"><div class="sect1" id="idm45664067158544">
<h1>Structuring Your Application for Development, 
<span class="keep-together">Testing, and Deployment</span></h1>
<p>In addition to structuring your application for a periodic release cadence, you also want to structure your application to enable Agile development, quality testing, and safe deployment. <a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="structuring for development, testing, and deployment" data-type="indexterm" id="idm45664067156688"/>This allows developers to make and test changes to the distributed application rapidly and roll those changes out to customers safely.</p>
<section data-pdf-bookmark="Goals" data-type="sect2"><div class="sect2" id="idm45664067154976">
<h2>Goals</h2>
<p>There are two goals for your application with regard to development and testing.<a data-primary="goals for your application in development and testing" data-type="indexterm" id="idm45664067153680"/> The first is that each developer should be able to easily develop new features for the application. In most cases, the developer is only working on a single component, yet that component is interconnected to all of the other microservices within the cluster. Thus, to facilitate development, it is essential that developers be able to work in their own environment with all
services available.</p>
<p>The other goal is to structure your application for easy and accurate testing prior to deployment. This is essential for rolling out features quickly while maintaining high reliability.</p>
</div></section>
<section data-pdf-bookmark="Progression of a Release" data-type="sect2"><div class="sect2" id="idm45664067152080">
<h2>Progression of a Release</h2>
<p>To achieve both of these goals, it is important to relate the stages of development to the
release versions described earlier.<a data-primary="progression of a release" data-type="indexterm" id="idm45664067150464"/><a data-primary="release, stages of" data-type="indexterm" id="idm45664067149696"/> The stages of a release are:</p>
<dl>
<dt><code>HEAD</code></dt>
<dd>
<p>The bleeding edge of the configuration; the latest changes.</p>
</dd>
<dt>Development</dt>
<dd>
<p>Largely stable, but not ready for deployment. Suitable for developers to use for
building features.</p>
</dd>
<dt>Staging</dt>
<dd>
<p>The beginnings of testing, unlikely to change unless problems are found.</p>
</dd>
<dt>Canary</dt>
<dd>
<p>The first real release to users, used to test for problems with real-world traffic
and likewise give users a chance to test what is coming next.</p>
</dd>
<dt>Release</dt>
<dd>
<p>The current production release.</p>
</dd>
</dl>
<section data-pdf-bookmark="Introducing a development tag" data-type="sect3"><div class="sect3" id="idm45664067148176">
<h3>Introducing a development tag</h3>
<p>Regardless of whether you structure releases using the filesystem or version control, the right way to model the development stage<a data-primary="development tag, introducing" data-type="indexterm" id="idm45664067139664"/><a data-primary="tags" data-secondary="introducing a development tag" data-type="indexterm" id="idm45664067138896"/> is via a source-control tag. This is because development is necessarily fast moving as it tracks stability only slightly behind <code>HEAD</code>.</p>
<p>To introduce a development stage, you add a new <code>development</code> tag to the source-control system and use an automated process to move this tag forward. On a periodic cadence, you’ll test <code>HEAD</code> via automated integration testing. If these tests pass, you move the <code>development</code> tag forward to <code>HEAD</code>. Thus, developers can track reasonably close to the latest changes when deploying their own environments, but also be assured that the deployed configurations have at least passed a limited smoke test. This approach is illustrated in <a data-type="xref" href="#fig2202">Figure 22-2</a>.</p>
<figure><div class="figure" id="fig2202">
<img alt="kur3 2202" height="715" src="assets/kur3_2202.png" width="1298"/>
<h6><span class="label">Figure 22-2. </span>Development tag workflow</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Mapping stages to revisions" data-type="sect3"><div class="sect3" id="idm45664067131408">
<h3>Mapping stages to revisions</h3>
<p>It might be tempting to introduce a new set of configurations for each of these stages, but in reality, every combination of versions and stages would create a mess that would be very difficult to reason about.<a data-primary="revisions, mapping stages to" data-type="indexterm" id="idm45664067129712"/><a data-primary="versioning" data-secondary="mapping stages to revisions" data-type="indexterm" id="idm45664067128992"/> Instead, the right practice is to introduce a mapping between revisions and stages.</p>
<p>Regardless of whether you are using the filesystem or source-control revisions to represent different configuration versions, it is easy to implement a map from stage to revision. In the filesystem case, you can use symbolic links to map a stage name to a revision:</p>
<pre data-type="programlisting">frontend/
   canary/ -&gt; v2/
   release/ -&gt; v1/
   v1/
     frontend-deployment.yaml
...</pre>
<p>For version control, it is simply an additional tag at the same revision as the
appropriate version.</p>
<p>In either case, versioning proceeds using the processes described previously, and the stages are moved forward to new versions separately as appropriate. In effect, this means that there are two simultaneous processes: the first for cutting new release versions and the second for qualifying a release version for a particular stage in the application life cycle.</p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Parameterizing Your Application with Templates" data-type="sect1"><div class="sect1" id="idm45664067125248">
<h1>Parameterizing Your Application with Templates</h1>
<p>Once you have a Cartesian product of environments and stages, it becomes
impractical or impossible to keep them all entirely identical.<a data-primary="parameterizing your application with templates" data-type="indexterm" id="idm45664067123520"/><a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="parameterizing application with templates" data-type="indexterm" id="idm45664067122720"/><a data-primary="templates, parameterizing your application with" data-type="indexterm" id="ix_tmpl"/> And yet, it is important to strive for the environments to be as identical as possible. Variance and drift between different environments produces snowflakes and systems that are hard to reason about. If your staging environment is different than your release environment, can you really trust the load tests that you ran in the staging environment to qualify a release? To ensure that your environments stay as similar as possible, it is useful to use parameterized environments. Parameterized environments use <em>templates</em> for the bulk of their configuration, but they mix in a limited set of <em>parameters</em> to produce the final configuration.<a data-primary="parameters" data-type="indexterm" id="idm45664067118992"/> In this way, most of the configuration is contained within a shared template, while the parameterization is limited in scope and maintained in a small parameters file for easy visualization of differences between environments.</p>
<section data-pdf-bookmark="Parameterizing with Helm and Templates" data-type="sect2"><div class="sect2" id="idm45664067118160">
<h2>Parameterizing with Helm and Templates</h2>
<p>There are a variety of different languages for creating parameterized configurations.<a data-primary="parameterizing your application with templates" data-secondary="parameterizing with Helm and templates" data-type="indexterm" id="idm45664067116560"/><a data-primary="configurations" data-secondary="parameterizing with templates and Helm" data-type="indexterm" id="idm45664067115488"/> In general they all divide the files into a <em>template</em> file, which contains the bulk of the configuration, and a <em>parameters</em> file, which can be combined with the template to produce a complete configuration. In addition to parameters, most templating languages allow parameters to have default values if no value is specified.</p>
<p>The following gives examples of how to parameterize configurations using <a href="https://helm.sh">Helm</a>, a package manager for Kubernetes.<a data-primary="Helm tool" data-secondary="parameterizing configurations with" data-type="indexterm" id="idm45664067112000"/> Despite what devotees of various languages may say, all parameterization languages are largely equivalent, and as with programming languages, which one you prefer is largely a matter of personal or team style. Thus, the patterns described here for Helm apply regardless of the templating language you choose.</p>
<p>The Helm template language uses “mustache” syntax:</p>
<pre data-type="programlisting">metadata:
  name: {{ .Release.Name }}-deployment</pre>
<p>This indicates that <code>Release.Name</code> should be substituted with the name of a 
<span class="keep-together">deployment</span>.</p>
<p>To pass a parameter for this value, you use a <em>values.yaml</em> file with contents like:</p>
<pre data-type="programlisting">Release:
  Name: my-release</pre>
<p>After parameter substitution, this results in:</p>
<pre data-type="programlisting">metadata:
  name: my-release-deployment</pre>
</div></section>
<section data-pdf-bookmark="Filesystem Layout for Parameterization" data-type="sect2"><div class="sect2" id="idm45664067117568">
<h2>Filesystem Layout for Parameterization</h2>
<p>Now that you understand how to parameterize your configurations, how do you apply that to the filesystem layouts?<a data-primary="filesystems" data-secondary="filesystem layout for parameterization" data-type="indexterm" id="idm45664067103552"/><a data-primary="parameterizing your application with templates" data-secondary="filesystem layout for parameterization" data-type="indexterm" id="idm45664067102512"/>  Instead of treating each deployment life cycle stage as a pointer to a version, think of each deployment life cycle as the combination of a parameters file and a pointer to a specific version. For example, in a directory-based layout, it might look like this:</p>
<pre data-type="programlisting">frontend/
  staging/
    templates -&gt; ../v2
    staging-parameters.yaml
  production/
    templates -&gt; ../v1
    production-parameters.yaml
  v1/
    frontend-deployment.yaml
    frontend-service.yaml
  v2/
    frontend-deployment.yaml
    frontend-service.yaml
...</pre>
<p>Doing this with version control looks similar, except that the parameters for each life cycle stage are kept at the root of the configuration directory tree:</p>
<pre data-type="programlisting">frontend/
  staging-parameters.yaml
  templates/
    frontend-deployment.YAML
...</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="Deploying Your Application Around the World" data-type="sect1"><div class="sect1" id="idm45664067098896">
<h1>Deploying Your Application Around the World</h1>
<p>Now that you have multiple versions of your application moving through multiple stages of deployment, the final step in structuring your configurations is to deploy your application around the world.<a data-primary="templates, parameterizing your application with" data-startref="ix_tmpl" data-type="indexterm" id="idm45664067097216"/><a data-primary="applications" data-secondary="organizing Kubernetes applications" data-tertiary="deploying your application around the world" data-type="indexterm" id="ix_apporgdepwrld"/><a data-primary="deployments" data-secondary="deploying your application around the world" data-type="indexterm" id="ix_depworld"/> But don’t think that these approaches are only for large-scale applications. You can use them to scale from two different regions to tens or hundreds around the world. In the cloud, where an entire region can fail, deploying to multiple regions (and managing that deployment) is the only way to achieve sufficient uptime for demanding users.</p>
<section data-pdf-bookmark="Architectures for Worldwide Deployment" data-type="sect2"><div class="sect2" id="idm45664067092736">
<h2>Architectures for Worldwide Deployment</h2>
<p>Generally speaking, each Kubernetes cluster is intended to live in a single region and to contain a single, complete deployment of your application.
Consequently, worldwide deployment of an application consists of multiple different Kubernetes clusters, each with its own application configuration. Describing how to actually build a worldwide application, especially with complex subjects like data replication, is beyond the scope of this chapter, but we will describe how to arrange the application configurations in the filesystem.</p>
<p>A particular region’s configuration is conceptually the same as a stage in the deployment life cycle. Thus, adding multiple regions to your configuration is identical to adding new life cycle stages. For example, instead of:</p>
<ul>
<li>
<p>Development</p>
</li>
<li>
<p>Staging</p>
</li>
<li>
<p>Canary</p>
</li>
<li>
<p>Production</p>
</li>
</ul>
<p>You might have:</p>
<ul>
<li>
<p>Development</p>
</li>
<li>
<p>Staging</p>
</li>
<li>
<p>Canary</p>
</li>
<li>
<p>EastUS</p>
</li>
<li>
<p>WestUS</p>
</li>
<li>
<p>Europe</p>
</li>
<li>
<p>Asia</p>
</li>
</ul>
<p>Modeling this in the filesystem for configuration looks like:</p>
<pre data-type="programlisting">frontend/
  staging/
    templates -&gt; ../v3/
    parameters.yaml
  eastus/
    templates -&gt; ../v1/
    parameters.yaml
  westus/
    templates -&gt; ../v2/
    parameters.yaml
  ...</pre>
<p>If you instead are using version control and tags, the filesystem would look like:</p>
<pre data-type="programlisting">frontend/
  staging-parameters.yaml
  eastus-parameters.yaml
  westus-parameters.yaml
  templates/
    frontend-deployment.yaml
...</pre>
<p>Using this structure, you would introduce a new tag for each region and use the file contents at that tag to deploy to that region.</p>
</div></section>
<section data-pdf-bookmark="Implementing Worldwide Deployment" data-type="sect2"><div class="sect2" id="idm45664067092144">
<h2>Implementing Worldwide Deployment</h2>
<p>Now that you have configurations for each region around the world, the question becomes how to update those various regions.<a data-primary="deployments" data-secondary="deploying your application around the world" data-tertiary="implementing worldwide deployment" data-type="indexterm" id="idm45664067074352"/> One of the primary goals of using multiple regions is to ensure very high reliability and uptime. While it would be tempting to assume that cloud and datacenter outages are the primary causes of downtime, the truth is that outages are generally caused by new versions of software rolling out. Because of this, the key to a highly available system is limiting the effect, or “blast radius,” of any change that you might make. Thus, as you roll out a version across a variety of regions, it makes sense to move carefully from region to region, and to validate and gain confidence in one region before moving on to the next.</p>
<p>Rolling out software across the world generally looks more like a workflow than a single declarative update: you begin by updating the version in staging to the latest version and then proceed through all regions until it is rolled out everywhere. But how should you structure the various regions, and how long should you wait to validate between regions?</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can use tools such as <a href="https://oreil.ly/BhWxi">GitHub Actions</a> to automate the deployment workflow.<a data-primary="GitHub Actions" data-type="indexterm" id="idm45664067070336"/> They provide a declarative syntax to define your workflow and are also stored in source 
<span class="keep-together">control</span>.</p>
</div>
<p>To determine the length of time between rollouts to regions, consider the “mean
time to smoke” for your software. This is the time it takes on average after a new release is rolled out to a region for a problem (if it exists) to be discovered. Obviously, each problem is unique and can take a varying amount of time to make itself known, and that is why you want to understand the <em>average</em> time. Managing software at scale is a business of probability, not certainty, so you want to wait for a time that makes the probability of an error low enough that you are comfortable moving on to the next region. Something like two to three times the mean time to smoke is probably a reasonable place to start, but it is highly variable depending on your application.</p>
<p>To determine the order of regions, it is important to consider the characteristics of various regions. For example, you are likely to have high-traffic regions and low-traffic regions. Depending on your application, you may have features that are more popular in one geographic area than another. All of these characteristics should be considered when putting together a release schedule. You likely want to begin by rolling out to a low-traffic region. This ensures that any early problems you catch are limited to an area of little impact. Though it is not a hard-and-fast rule, early problems are often the most severe, since they manifest quickly enough to be caught in the first region you roll out to. Thus, minimizing the impact of such problems on your customers makes sense. Next, roll out to a high-traffic region. Once you have successfully validated that your release works correctly via the low-traffic region, validate that it works correctly at scale. The only way to do this is to roll it out to a single high-traffic region. When you have successfully rolled out to both a low- and a high-traffic region, you may have confidence that your application can safely roll out everywhere. However, if there are regional variations, you may want to also test slowly across a variety of geographies before pushing your release more broadly.</p>
<p>When you put your release schedule together, it is important to follow it completely for every release, no matter how big or how small. Many outages have been caused by people accelerating releases, either to fix some other problem or because they believed it to be “safe.”</p>
</div></section>
<section data-pdf-bookmark="Dashboards and Monitoring for Worldwide Deployments" data-type="sect2"><div class="sect2" id="idm45664067064752">
<h2>Dashboards and Monitoring for Worldwide Deployments</h2>
<p>It may seem an odd <a data-primary="deployments" data-secondary="deploying your application around the world" data-tertiary="dashboards and monitoring" data-type="indexterm" id="idm45664067063456"/><a data-primary="dashboards for worldwide application deployment" data-type="indexterm" id="idm45664067062064"/><a data-primary="monitoring worldwide application deployments" data-type="indexterm" id="idm45664067061360"/>concept when you are developing at a small scale, but one significant problem that you will likely run into at a medium or large scale is having different versions of your application deployed to different regions. This can happen for a variety of reasons (such as, because a release has failed, been aborted, or had problems in a particular region), and if you don’t track things carefully you can rapidly end up with an unmanageable snowflake of different versions deployed around the world. Furthermore, as customers inquire about fixes to bugs they are experiencing, a common question will become: “Is it deployed yet?”</p>
<p>Thus, it is essential to develop dashboards, which can tell you at a glance which version is running in which region, as well as alerting, which will fire when too many versions of your application are deployed. A best practice is to limit the number of active versions to no more than three: one testing, one rolling out, and one being replaced by the rollout. Any more active versions than this is asking for trouble.<a data-primary="applications" data-secondary="organizing Kubernetes applications" data-startref="ix_apporgdepwrld" data-tertiary="deploying your application around the world" data-type="indexterm" id="idm45664067059712"/><a data-primary="deployments" data-secondary="deploying your application around the world" data-startref="ix_depworld" data-type="indexterm" id="idm45664067058144"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45664067056640">
<h1>Summary</h1>
<p>This chapter provides guidance on how to manage a Kubernetes application through software versions, deployment stages, and regions around the world. It highlights the principles at the foundation of organizing your application: relying on the filesystem for organization, using code review to ensure quality changes, and relying on feature flags, or gates, to make it easy to incrementally add and remove functionality.</p>
<p>As with everything, the recipes in this chapter should be taken as inspiration, rather than absolute truth. Read the guidance, and find the mix of approaches that works best for the particular circumstances of your application. But keep in mind that in laying out your application for deployment, you are setting a process that you will likely have to live with for years.<a data-primary="applications" data-secondary="organizing Kubernetes applications" data-startref="ix_apporg" data-type="indexterm" id="idm45664067054144"/></p>
</div></section>
</div></section></div></body></html>