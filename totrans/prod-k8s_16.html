<html><head></head><body><section data-pdf-bookmark="Chapter 15. Software Supply Chain" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter15">&#13;
<h1><span class="label">Chapter 15. </span>Software Supply Chain</h1>&#13;
&#13;
&#13;
<p>Implementing a Kubernetes platform should never be the goal of your team or company (assuming you are not a vendor or consultant!).<a data-primary="software supply chain" data-type="indexterm" id="ix_softSC"/> This might seem a strange claim for a book exclusively devoted to Kubernetes to make, but let’s step back a moment. All companies are in the business of delivering their <em>core-competency</em>. This might be an ecommerce platform, a SaaS monitoring system, or an insurance website. Platforms like Kubernetes (and almost any other tooling) exist to <em>enable</em> the delivery of core business value, a truth that is often forgotten by teams when designing and implementing IT solutions.</p>&#13;
&#13;
<p>With that sentiment in mind, this chapter will focus on the actual process of getting code from developers to production on Kubernetes. To best cover each stage that we think is relevant, we’ll follow the model of a pipeline that many are familiar with.</p>&#13;
&#13;
<p>First we’ll look at some of the considerations when building container images (our deployed assets) from source code. If you’re already utilizing Kubernetes or other container platforms you’ll probably be familiar with some of the concepts in this section, but hopefully we’ll cover some questions that you may not have considered. If you’re <em>new</em> to containers this will be a paradigm shift from the way that you currently build software (WAR files, Go binaries, etc.) to thinking about the container image and the nuances involved with building and maintaining them.</p>&#13;
&#13;
<p>Once we have built our assets we need somewhere to store them. We’ll discuss container registries (e.g., DockerHub, Harbor, Quay) and the functionality that we think is important when choosing one. Many of the attributes of container registries are related to security, and we’ll discuss options like image scanning, updates, and &#13;
<span class="keep-together">signing</span>.</p>&#13;
&#13;
<p class="pagebreak-before">Finally, we’ll dedicate some time to reviewing continuous delivery and how those practices and associated tooling intersect with Kubernetes. We’ll look at emerging ideas like GitOps (deployments through syncing cluster state from git repositories) and more traditional imperative pipeline approaches.</p>&#13;
&#13;
<p>Even if you are not yet running Kubernetes, you will likely have considered and/or solved for all of the high-level areas just mentioned (build, asset storage, deployment). It’s reasonable that everyone has investments and expertise in existing tooling and approaches, and we very rarely encounter a situation where an organization wants to start afresh with its entire software supply chain. One of the things we’ll try to emphasize in this chapter is that there are clean handoff points in the pipeline and we can pick and choose the most effective approaches for each phase. As with many of the topics covered in this book, it is entirely possible (and recommended) to enact <em>incremental</em> positive change while remaining focused on delivering business value.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Building Container Images" data-type="sect1"><div class="sect1" id="idm45611971571624">&#13;
<h1>Building Container Images</h1>&#13;
&#13;
<p>Before containers we would package applications as a binary, compressed asset, or raw source code for it to be deployed onto a server.<a data-primary="software supply chain" data-secondary="building container images" data-type="indexterm" id="ix_softSCCI"/><a data-primary="container images" data-secondary="building" data-type="indexterm" id="ix_cntrimgbld"/> This would either run standalone or inside of an application server. Alongside the application itself we’d need to ensure the environment contained the correct dependencies and configuration available for it to run successfully in the environment we were deploying to.</p>&#13;
&#13;
<p>In a container-based environment, the container image is the deployable asset. It contains not only the application binary itself but also the execution environment and any associated dependencies. The image itself is a compressed set of <em>filesystem layers</em> alongside some metadata, which together conform to the Open Container Initiative (OCI) Image Specification.<a data-primary="OCI (Open Container Initiative)" data-secondary="image specification" data-type="indexterm" id="idm45611971566168"/> This is an agreed standard within the cloud native community to ensure that image building can be implemented in many different ways (we’ll see some of these in the following sections) while still producing an artifact that is runnable by all the different container runtimes (more information on this can be found in <a data-type="xref" href="ch03.html#container_runtime_chapter">Chapter 3</a>).</p>&#13;
&#13;
<p>Typically, building a container image involves creating a Dockerfile that describes the image and using Docker Engine to execute the Dockerfile.<a data-primary="Dockerfiles" data-type="indexterm" id="idm45611971563352"/> With that said, there is an ecosystem of tools (each with their own approaches) that you can use to create container images in different scenarios. To borrow a concept from BuildKit (one such tool, built by Docker) we can think about building in terms of <em>frontend</em> and <em>backend</em>. The <em>frontend</em> is the method for defining the high-level process that should be used to build the image, e.g., a Dockerfile or Buildpack (more on these later in this chapter). The <em>backend</em> is the actual build engine that takes the definition generated by the <em>frontend</em> and executes commands on the filesystem to construct the image.</p>&#13;
&#13;
<p>In many cases the <em>backend</em> is the Docker daemon, which may not be suitable for all cases. For example, if we want to run builds in Kubernetes we need either to run a Docker daemon inside a container (Docker in Docker) or mount the Docker Unix socket from the host machine into the build container. Both of these approaches have drawbacks, and in the latter case exposes potential security issues. In response to these issues, other build backends like Kaniko have emerged. Kaniko uses the same <em>frontend</em> (a Dockerfile) but utilizes different techniques to create the image under the hood, making it a solid choice for running in a Kubernetes Pod. When deciding how you want to build images, you should answer the following questions:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Can we run our builder as root?</p>&#13;
</li>&#13;
<li>&#13;
<p>Are we OK mounting the Docker socket?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do we care about running a daemon?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do we want to containerize builds?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do we want to run them among workloads in Kubernetes?</p>&#13;
</li>&#13;
<li>&#13;
<p>How much do we intend to leverage layer caching?</p>&#13;
</li>&#13;
<li>&#13;
<p>How will our tooling choice affect distributing builds?</p>&#13;
</li>&#13;
<li>&#13;
<p>What <em>frontends</em> or image definition mechanisms do we want to use? What is &#13;
<span class="keep-together">supported</span>?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In this section, we will first cover some of the patterns and antipatterns we’ve seen when building container images (Cloud Native Buildpacks) that will hopefully help you on your journey to build better container images. Then, we will review an alternative method for building container images, and how all of these techniques can be integrated into a pipeline.</p>&#13;
&#13;
<p>One question that often comes up early on within organizations is, Who should be responsible for building images? Early on as Docker was becoming popular it was largely embraced as a developer-focused tool. In our experience, smaller organizations still have developers responsible for writing Dockerfiles and defining the build process for their application images. However, as organizations look to adopt containers (and Kubernetes) at scale, having individual developers or development teams all creating their own Dockerfiles becomes unsustainable. Firstly it creates extra work for developers, which pulls them away from their core responsibility, and secondly, it results in a huge variance in produced images with little to no standardization.</p>&#13;
&#13;
<p>As a result, we are seeing a move toward abstracting the build process from development teams and instead moving the responsibility toward operations and platform teams to implement <em>source to image</em> patterns and tooling that receive a code repository as an input and are capable of producing a container image ready to move through the pipeline. We’ll discuss this pattern more in <a data-type="xref" href="#cloud_native_buildpacks">“Cloud Native Buildpacks”</a>. In the interim we have also commonly seen a pattern of platform teams running workshops and/or assisting development teams with Dockerfile and image creation. As organizations scale this can be an effective first step but is usually not sustainable given the ratio of development teams to platform personnel.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Golden Base Images Antipattern" data-type="sect2"><div class="sect2" id="idm45611971546600">&#13;
<h2>The Golden Base Images Antipattern</h2>&#13;
&#13;
<p>In the field we have encountered several antipatterns that are usually the result of teams not adjusting their thinking to embrace patterns that have emerged in the container and cloud native landscape.<a data-primary="golden base images antipattern" data-type="indexterm" id="idm45611971545000"/><a data-primary="container images" data-secondary="building" data-tertiary="golden base image antipattern" data-type="indexterm" id="idm45611971544392"/><a data-primary="software supply chain" data-secondary="building container images" data-tertiary="golden base image antipattern" data-type="indexterm" id="idm45611971543304"/><a data-primary="base images" data-secondary="golden base image antipattern" data-type="indexterm" id="idm45611971542216"/> Maybe the most common of these is the concept of pre-ordained, <em>gold</em> images. The scenario is that in a pre-container environment specific base images (for example, a preconfigured CentOS base) would be approved for use within an organization, and all applications going into production would have to be based on that image. This approach is usually adopted for <em>security</em> reasons, as the tools and libraries in the image have been well vetted. However, when moving to containers, teams found themselves consigned to reinventing the wheel by pulling useful upstream images from third parties and vendors and rebasing their applications and configurations onto them.</p>&#13;
&#13;
<p>This introduces a few related issues. Firstly, there is the additional work involved with the initial conversion from the upstream image to an internal customized version. Secondly, there is now an onus of maintenance placed on the internal platform team to store and maintain these internal images. Because this situation can sprawl (given how many images are in use in a typical environment), this approach usually ends up resulting in a <em>worse</em> security posture as updates are performed infrequently (if at all) given the extra work involved.</p>&#13;
&#13;
<p>Our recommendation in this area is usually to partner with security teams and identify what specific requirements the gold images are serving. Usually several of the following will apply:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Ensure specific agents/software is installed</p>&#13;
</li>&#13;
<li>&#13;
<p>Ensure no vulnerable libraries are present</p>&#13;
</li>&#13;
<li>&#13;
<p>Ensure user accounts have the correct permissions</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>By understanding the reasoning behind the restrictions, we can instead codify these requirements into tooling that will sit in the pipeline and reject and/or alert on non-compliant images and maintain the desired security posture, while still broadly allowing teams to reuse images (and the work that has gone into crafting them) from the upstream community. We’ll take a deeper look at one example workflow in <a data-type="xref" href="#image_registries">“Image Registries”</a>.</p>&#13;
&#13;
<p>One of the more compelling reasons to specify a base OS is to ensure that operational knowledge exists in the organization should troubleshooting be required. However, when digging a little deeper this is not as useful as it may seem. Very rarely should it be necessary to <code>exec</code> into containers to troubleshoot specific issues, and even then the differences between Linux-based operating systems are fairly trivial for the kinds of support required. Additionally, more and more applications are being packaged in ultra-lightweight scratch or distroless images to reduce the overhead inside the &#13;
<span class="keep-together">container</span>.</p>&#13;
&#13;
<p>Attempting to refactor all <em>upstream</em>/vendor images onto your own base(s) should be avoided for the reasons described in this section. However, we’re not asserting that maintaining an internal set of curated base images is a bad idea. These can be great to use as a foundation for your own applications, and we talk about some of the considerations when building these internal bases in the next section.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Choosing a Base Image" data-type="sect2"><div class="sect2" id="idm45611971529864">&#13;
<h2>Choosing a Base Image</h2>&#13;
&#13;
<p>The base image of the container determines the bottom layers on which the application’s container image is to be built. <a data-primary="base images" data-secondary="choosing an image" data-type="indexterm" id="idm45611971528216"/><a data-primary="container images" data-secondary="building" data-tertiary="choosing a base image" data-type="indexterm" id="idm45611971527240"/><a data-primary="software supply chain" data-secondary="building container images" data-tertiary="choosing a base image" data-type="indexterm" id="idm45611971526024"/>The base image is critical as it usually contains operating system libraries and tools that will be part of your application container image. If you are not mindful when choosing a base image, it can be the source of unnecessary libraries and tools that not only bloat your container image but also can become security vulnerabilities.</p>&#13;
&#13;
<p>Depending on your organization’s maturity and security posture, you might not have a choice when it comes to base images. We have worked with many organizations that have a dedicated team responsible for curating and maintaining a set of approved base images that must be used across the organization. With that said, if you do have a choice or you are part of the team that is vetting base images, consider the following guidelines when evaluating base images:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Ensure the images are published by a reputable vendor. You don’t want to use a base image from a random DockerHub user. After all, these images will be the foundation for most, if not all, your applications.</p>&#13;
</li>&#13;
<li>&#13;
<p>Understand the update cycle and prefer images that are updated continuously. As mentioned earlier, the base image typically contains libraries and tools that must be patched whenever new vulnerabilities are discovered.</p>&#13;
</li>&#13;
<li>&#13;
<p>Prefer images that have an open source build process or specification. This is typically a Dockerfile that you can inspect to understand how the image is built.</p>&#13;
</li>&#13;
<li>&#13;
<p>Avoid images that have unnecessary tools or libraries. Prefer minimal images that provide a small footprint that your developers can build upon, when necessary.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Most of the time if you are building your own images we’ve seen scratch or distroless to be a solid choice as both embody the preceding principles. The scratch image contains absolutely nothing, so with a simpler static binary scratch can be the leanest possible image.<a data-primary="certificates" data-secondary="root CA certificates" data-type="indexterm" id="idm45611971518072"/> However, you may encounter issues if you need root CA certificates or some other assets. These can be copied in, but are something to think about. The distroless base is what we’d recommend in most cases as they contain some sensible users precreated (<code>nonroot</code>, <code>nobody</code>, etc.) and a set of minimal required libraries that vary depending on the flavor of the base image chosen. Distroless has several language-specific base variants for you to choose from.</p>&#13;
&#13;
<p>In the next few sections we’ll continue to talk about best-practice patterns, starting with the importance of specifying an appropriate user for your application to run under.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Runtime User" data-type="sect2"><div class="sect2" id="idm45611971514920">&#13;
<h2>Runtime User</h2>&#13;
&#13;
<p>Because of the container isolation model (mainly the fact that containers share the underlying Linux kernel), the runtime user of a container has important implications that some developers don’t think about.<a data-primary="runtime user of a container" data-type="indexterm" id="idm45611971513192"/><a data-primary="software supply chain" data-secondary="building container images" data-tertiary="runtime user" data-type="indexterm" id="idm45611971512520"/><a data-primary="container images" data-secondary="building" data-tertiary="runtime user" data-type="indexterm" id="idm45611971511336"/> In most cases, when the container’s runtime user is left unspecified, the process runs as the root user. This is problematic because it increases the attack surface of the container. For example, if an attacker were to compromise the application and escape the container, they could gain root access on the underlying host.</p>&#13;
&#13;
<p>When building your container image, it is critical for you to consider the runtime user of the container. Does the application need to run as root? Does the application depend on the contents of <em>/etc/passwd</em>? Do you need to add a nonroot user to the container image? As you answer these questions, ensure that you specify the runtime user in the container image’s configuration. If you are using a Dockerfile to build your image, you can use the <code>USER</code> directive to specify the runtime user, as in the following example, which runs the <code>my-app</code> binary with the user and group ID <code>nonroot</code> (which is configured by default as part of the distroless set of images):</p>&#13;
&#13;
<pre data-type="programlisting">FROM gcr.io/distroless/base&#13;
USER nonroot:nonroot&#13;
COPY ./my-app /my-app&#13;
CMD ["./my-app", "serve"]</pre>&#13;
&#13;
<p>Even though you can specify the runtime user in your Kubernetes deployment manifests, defining it as part of the container image specification is valuable as it results in a self-documenting container image. It also ensures that developers use the same user and group ID as they work with the container in their local or development &#13;
<span class="keep-together">environments</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pinning Package Versions" data-type="sect2"><div class="sect2" id="idm45611971504760">&#13;
<h2>Pinning Package Versions</h2>&#13;
&#13;
<p>If your application leverages external packages you will most likely install them using a package manager such as <code>apt</code>, <code>yum</code>, or <code>apk</code>. <a data-primary="container images" data-secondary="building" data-tertiary="pinning package versions" data-type="indexterm" id="idm45611971501800"/><a data-primary="package versions, pinning" data-type="indexterm" id="idm45611971500552"/><a data-primary="software supply chain" data-secondary="building container images" data-tertiary="pinning package versions" data-type="indexterm" id="idm45611971499912"/>As you build your container image, it is important that you pin or specify the version of these packages. For example, the following example shows an application that depends on imagemagick. The <code>apk</code> &#13;
<span class="keep-together">instruction</span> in the Dockerfile pins imagemagick to the version that is compatible with the application:</p>&#13;
&#13;
<pre data-type="programlisting">FROM alpine:3.12&#13;
&lt;...snip...&gt;&#13;
RUN ["apk", "add", "imagemagick=7.0.10.25-r0"]&#13;
&lt;...snip...&gt;</pre>&#13;
&#13;
<p>If you leave package versions unspecified, you risk getting different packages that might break your application. Thus, always specify the versions of the packages you install in your container image. By doing so, you ensure that your container image builds are repeatable and produce container images with compatible package &#13;
<span class="keep-together">versions</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Build Versus Runtime Image" data-type="sect2"><div class="sect2" id="idm45611971495160">&#13;
<h2>Build Versus Runtime Image</h2>&#13;
&#13;
<p>In addition to packaging applications for deployment, development teams can also leverage containers to build their applications.<a data-primary="container images" data-secondary="building" data-tertiary="build versus runtime image" data-type="indexterm" id="idm45611971493640"/><a data-primary="software supply chain" data-secondary="building container images" data-tertiary="build versus runtime image" data-type="indexterm" id="idm45611971492552"/> Containers can provide a well-defined build environment that can be codified into a Dockerfile, for example. This is useful as developers are not required to install the build tooling in their systems. More importantly, containers can provide a standardized build environment across the entire development team and their continuous integration (CI) systems.<a data-primary="build container image versus  runtime image" data-type="indexterm" id="idm45611971490968"/></p>&#13;
&#13;
<p>While using containers to build applications can be useful, it is important to distinguish between the build container image and the runtime image. The build image contains all the tooling and libraries that are necessary to compile the application, whereas the runtime image contains the application to be deployed. For example in a Java application we might have a build image that contains the JDK, Gradle/Maven, and all of our compilation and testing tooling. Then our runtime image can contain only the Java runtime and our application.</p>&#13;
&#13;
<p>Given that the application typically does not need the build tooling at runtime, the runtime image should not contain these tools. This results in a leaner container image that is faster to distribute and has a tighter attack surface. If you are using docker to build images, you can leverage its multistage build feature to separate the build from the runtime image. The following snippet shows a Dockerfile for a Go application. The build stage uses the <code>golang</code> image, which includes the Go toolchain, while the runtime stage uses the scratch base image and contains nothing more than the application binary:</p>&#13;
&#13;
<pre data-type="programlisting"># Build stage&#13;
FROM golang:1.12.7 as build <a class="co" href="#callout_software_supply_chain_CO1-1" id="co_software_supply_chain_CO1-1"><img alt="1" src="assets/1.png"/></a>&#13;
&#13;
WORKDIR /my-app&#13;
&#13;
COPY go.mod . <a class="co" href="#callout_software_supply_chain_CO1-2" id="co_software_supply_chain_CO1-2"><img alt="2" src="assets/2.png"/></a>&#13;
RUN go mod download&#13;
&#13;
COPY main.go .&#13;
ENV CGO_ENABLED=0&#13;
RUN go build -o my-app&#13;
&#13;
# Deploy stage&#13;
FROM gcr.io/distroless/base <a class="co" href="#callout_software_supply_chain_CO1-3" id="co_software_supply_chain_CO1-3"><img alt="3" src="assets/3.png"/></a>&#13;
USER nonroot:nonroot <a class="co" href="#callout_software_supply_chain_CO1-4" id="co_software_supply_chain_CO1-4"><img alt="4" src="assets/4.png"/></a>&#13;
COPY --from=build --chown=nonroot:nonroot /my-app/my-app /my-app <a class="co" href="#callout_software_supply_chain_CO1-5" id="co_software_supply_chain_CO1-5"><img alt="5" src="assets/5.png"/></a>&#13;
CMD ["/my-app"]</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_software_supply_chain_CO1-1" id="callout_software_supply_chain_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The main <code>golang</code> image contains all the Go build tools, which are not required at runtime.</p></dd>&#13;
<dt><a class="co" href="#co_software_supply_chain_CO1-2" id="callout_software_supply_chain_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>We copy the <em>go.mod</em> file first and download so that we can cache this step if the code changes but the dependencies don’t.</p></dd>&#13;
<dt><a class="co" href="#co_software_supply_chain_CO1-3" id="callout_software_supply_chain_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We can use <code>distroless</code> as a runtime image to take advantage of a minimal base but with no unnecessary extra dependencies.</p></dd>&#13;
<dt><a class="co" href="#co_software_supply_chain_CO1-4" id="callout_software_supply_chain_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>We want to run our apps as a nonroot user if possible.</p></dd>&#13;
<dt><a class="co" href="#co_software_supply_chain_CO1-5" id="callout_software_supply_chain_CO1-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Only the compiled file (<em>my-app</em>) is being copied from the build stage to the deploy stage.</p></dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Containers run a single process and usually have no supervisor or init system. For this reason you need to ensure that signals are handled correctly and orphaned processes are correctly reparented and reaped. There are several minimal init scripts capable of fulfilling these requirements and acting as a bootstrap for your application instance.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cloud Native Buildpacks" data-type="sect2"><div class="sect2" id="cloud_native_buildpacks">&#13;
<h2>Cloud Native Buildpacks</h2>&#13;
&#13;
<p>An additional method of building container images involves tooling that analyze the application’s source code and automatically produce a container image.<a data-primary="software supply chain" data-secondary="building container images" data-tertiary="Cloud Native Buildpacks" data-type="indexterm" id="idm45611971459432"/><a data-primary="container images" data-secondary="building" data-tertiary="Cloud Native Buildpacks" data-type="indexterm" id="idm45611971458168"/> Similar to application-specific build tools, this approach greatly simplifies the developer experience as developers don’t have to create and maintain Dockerfiles.<a data-primary="Cloud Native Buildpacks (CNB)" data-type="indexterm" id="idm45611971456648"/> Cloud Native Buildpacks is an implementation of such an approach, and the high-level flow is shown in <a data-type="xref" href="#buildpack_flow">Figure 15-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="buildpack_flow">&#13;
<img alt="prku 1501" src="assets/prku_1501.png"/>&#13;
<h6><span class="label">Figure 15-1. </span>Buildpack flow.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Cloud Native Buildpacks (CNB) is a container-focused implementation of Buildpacks, a technology that Heroku and Cloud Foundry have used for years to package applications for those platforms.<a data-primary="Buildpacks" data-type="indexterm" id="idm45611971452296"/> In the case of CNB, it packages applications into OCI container images, ready to run on Kubernetes. To build the image, CNB analyzes the application source code and executes the buildpacks accordingly. For example, the Go buildpack is executed if there are Go files present in your source code. Similarly, the Maven (Java) buildpack is executed if CNB finds a <em>pom.xml</em> file. This all happens behind the scenes, and developers can initiate this process using a CLI tool called <code>pack</code>. The great thing about this approach is that the buildpacks are tightly scoped, which enables building high-quality images that follow best practices.</p>&#13;
&#13;
<p>In addition to improving the developer experience and lowering the barrier to platform adoption, platform teams can leverage custom buildpacks to enforce policy, ensure compliance, and standardize the container images running in their platform.</p>&#13;
&#13;
<p>Overall, providing a solution that builds container images from source code can be a worthy endeavor. Furthermore, we find that the value of such a solution increases with the size of the organization. At the end of the day, development teams want to focus on building value in the application and not on how to containerize it.<a data-primary="software supply chain" data-secondary="building container images" data-startref="ix_softSCCI" data-type="indexterm" id="idm45611971448552"/><a data-primary="container images" data-secondary="building" data-startref="ix_cntrimgbld" data-type="indexterm" id="idm45611971447288"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Image Registries" data-type="sect1"><div class="sect1" id="image_registries">&#13;
<h1>Image Registries</h1>&#13;
&#13;
<p>If you are already using containers, then you’ll likely have a registry you prefer. <a data-primary="software supply chain" data-secondary="image registries" data-type="indexterm" id="ix_softSCimgr"/><a data-primary="image registries" data-type="indexterm" id="ix_imgreg"/>It’s one of the core requirements for utilizing Docker and Kubernetes because we need somewhere to store the images we build on one machine and want to run on many others (either standalone or in a cluster). As is the case for images, the OCI also defines a standard spec for registry operations (to ensure interoperability), and there are many proprietary and open source solutions available, most of which share a common set of core features. Most image registries are comprised of three major components: a server (for the user interface and API logic), a blob store (for the images themselves), and a database (for user and image metadata). Usually, the storage backends are configurable, and this can impact how you design your registry architecture. We’ll talk more about this in a minute.</p>&#13;
&#13;
<p>In this section we’ll look at some of the most important features offered by registries and some of the patterns for integrating them into your pipeline. We’re not going to look deeply at any <em>specific</em> registry implementations, as functionality is generally similar; however, there are scenarios where you may want to lean in a certain direction based on your existing setup or requirements.</p>&#13;
&#13;
<p>If you are already leveraging an artifact store like Artifactory or Nexus you may want to take advantage of their image hosting capabilities for ease of management.<a data-primary="cloud computing" data-secondary="cloud provider registries" data-type="indexterm" id="idm45611971438984"/><a data-primary="image registries" data-secondary="cloud provider registries" data-type="indexterm" id="idm45611971438040"/> Similarly, if your environments are heavily cloud-based there may be cost benefits to utilizing cloud provider registries like AWS Elastic Container Registry (ECR), Google Container Registry (GCR), or Azure Container Registry (ACR).</p>&#13;
&#13;
<p>Another key factor to consider when choosing a registry is the topology, architecture, and failure domains of your environments and clusters. You may choose to place registries in each failure domain to ensure high availability. When doing this you’ll need to decide whether you want a centralized blob store or whether you want blob stores in each region and set up image replication between the registries. Replication is a feature of most registries that allows you to push an image to one of a set of registries and have that image automatically pushed to the others in the set. Even if this is not directly supported in your registry of choice, it is fairly trivial to set up basic replication by using a pipeline tool (e.g., Jenkins) and webhooks that trigger on each image push.</p>&#13;
&#13;
<p>The decision of one versus many registries is also impacted by how much throughput you need to support. In organizations with many thousands of developers triggering code and image builds on every code commit, the number of concurrent operations (pulls and pushes) can be significant. It is important to therefore understand that an image registry, while playing only a limited role in the pipeline, is in the critical path for not only production deployments but also development activities. It is a core component that must be monitored and maintained in the same way as other critical components to achieve a high level of service availability.</p>&#13;
&#13;
<p>Many registries are built with the intention of being easily run <em>inside</em> a cluster or containerized environment. This approach (which we’ll cover again in <a data-type="xref" href="#continuous_delivery">“Continuous Delivery”</a>) has many advantages. Primarily, we are able to leverage all of the primitives and conventions inside Kubernetes to keep the services running, &#13;
<span class="keep-together">discoverable</span>, and easily configurable. The obvious downside here is that we now have a reliance on a service <em>inside</em> the cluster to provide images to start new services inside that cluster. It’s more common to see registries run on a shared services cluster and have a failover system to a backup cluster to ensure that <em>some</em> instance of the registry will always be able to service requests.</p>&#13;
&#13;
<p>We’ve also commonly seen registries run <em>outside</em> of Kubernetes and treated as more of a standalone <em>bootstrap</em> component that is required by all clusters. This is usually the case where an organization is already using an existing instance of Artifactory or another registry and repurposes it for image hosting. Utilizing cloud registries is also a common pattern here, although you also need to be aware of their availability guarantees (as the same topology questions apply) and potential extra latency.</p>&#13;
&#13;
<p>In the following subsections we’ll look at some of the most common concerns when choosing and working with a registry. These concerns are all security related as securing our software supply chain revolves around our deployed artifacts (images). First we’ll look at vulnerability scanning and how to ensure that our images don’t contain known security flaws. Then we’ll describe a commonly used quarantine flow that can be effective at bringing external/vendor images into our environments. Finally, we’ll discuss image trust and signing. This is an area that many organizations are interested in but where the upstream tooling and approaches are still maturing.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vulnerability Scanning" data-type="sect2"><div class="sect2" id="idm45611971427416">&#13;
<h2>Vulnerability Scanning</h2>&#13;
&#13;
<p>Scanning images for known vulnerabilities is a key competency of most image registries.<a data-primary="security" data-secondary="vulnerability scanning of images" data-type="indexterm" id="idm45611971425912"/><a data-primary="software supply chain" data-secondary="image registries" data-tertiary="vulnerability scanning" data-type="indexterm" id="idm45611971424968"/><a data-primary="vulnerability scanning by image registries" data-type="indexterm" id="idm45611971423752"/><a data-primary="image registries" data-secondary="vulnerability scanning" data-type="indexterm" id="idm45611971422984"/> Usually the scanning itself along with the database of common vulnerabilities and exposures (CVEs) are delegated to a third-party component.<a data-primary="common vulnerabilities and exposures (CVEs)" data-type="indexterm" id="idm45611971421752"/> Clair is a popular open source choice and, in many cases, is pluggable should you have specific &#13;
<span class="keep-together">requirements</span>.</p>&#13;
&#13;
<p>Every organization will have its own requirements about what constitutes an acceptable risk when considering CVE scores. Registries will commonly expose controls that allow you to disable the pulling of images that contain CVEs over a defined score threshold. Additionally, the ability to add CVEs to an allowlist can be useful to bypass issues that are flagged but not relevant in your environment, or for those CVEs that are deemed acceptable risk and/or have no fixes released and available.</p>&#13;
&#13;
<p>This static scanning at initial pull time can be useful to begin with, but what happens if vulnerabilities are discovered over time in the images we’re already using in the environment?<a data-primary="static vulnerability scanning of images" data-type="indexterm" id="idm45611971418472"/> Scans can be scheduled to detect these changes, but then we need to have a plan for updating and replacing the images. It can be tempting to automatically remediate (patch) and push out updated images, and there are solutions that will always try to keep images up-to-date. However, this can be problematic as image updates can introduce changes that may be incompatible and/or break the running application. These automated image update systems may also work outside of your designated deployment change process and could be hard to audit in the environment. Even the blocking of image pulls (described previously) can cause issues. If a core application’s image has a new CVE discovered and pulls are suddenly prohibited, this could cause availability issues in the application if those workloads are scheduled to new nodes and images are unavailable for pulling. As we’ve discussed many times in this book, it’s imperative to understand the trade-offs that you encounter when implementing each solution (in this case, security versus availability) and make informed, well-documented decisions.</p>&#13;
&#13;
<p>A more common model than the automatic remediation described briefly is to alert and/or monitor on image vulnerability scans and bubble these up to operations and security teams. The implementation of this alerting may differ depending on the capabilities offered by your choice of registry. Some registries can be configured to trigger a webhook call on completion of a scan with the payload including details of the vulnerable images and discovered CVEs.<a data-primary="metrics" data-secondary="image and CVE details" data-type="indexterm" id="idm45611971415688"/> Others may expose a scrapeable set of metrics with image and CVE details that can be alerted on using standard tooling (take a look at <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a> for more details on metrics and alerting tools). While this method requires more manual intervention, it allows good visibility into the security state of images in your environment while also affording more control over how and when they are patched.</p>&#13;
&#13;
<p>Once we have CVE information for an image, then decisions about whether or not to patch the image (and when) can be made based on the impact of the vulnerability. If we need to patch and update the image we can trigger the update, testing, and deployment via our regular deployment pipelines. This ensures that we have full transparency and auditability and that those changes all go through our regular processes. We will discuss CI/CD and deployment models in more detail later in this chapter.</p>&#13;
&#13;
<p>While the static image vulnerability scanning covered in this subsection is a commonly implemented part of an organization’s software supply chain, it is only one layer of what should be a <em>defense in depth</em> strategy to container security. Images may download malicious content post-deployment or containerized applications may be compromised/hijacked at runtime. It’s essential therefore to implement some type of runtime scanning. In a more naive form, this could take the form of periodic filesystem scanning on running containers to ensure that no vulnerable binaries and/or libraries are introduced post-deployment. However, for more robust protection it’s necessary to limit the <em>actions</em> and <em>behaviors</em> that a container is capable of performing. This eliminates the inevitable game of whack-a-mole that can occur with CVEs being discovered and patched and instead focuses on the capabilities a containerized application should possess.<a data-primary="runtime image vulnerability scanning" data-type="indexterm" id="idm45611971409736"/> Runtime scanning is a larger topic that we don’t have the space to fully cover here, but you should look into tools like <a href="https://falco.org">Falco</a> and the <a href="https://github.com/aquasecurity">Aqua Security suite</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Quarantine Workflow" data-type="sect2"><div class="sect2" id="idm45611971407400">&#13;
<h2>Quarantine Workflow</h2>&#13;
&#13;
<p>As mentioned, most registries provide a mechanism to scan images for known vulnerabilities and restrict image pulls.<a data-primary="image registries" data-secondary="quarantine workflow" data-type="indexterm" id="idm45611971405768"/><a data-primary="software supply chain" data-secondary="image registries" data-tertiary="quarantine workflow" data-type="indexterm" id="idm45611971404792"/> However, there may be additional requirements that images must satisfy before they can be used. We have also encountered the scenario where developers are unable to directly pull images from the public internet and must use an internal registry. Both of these use cases can be solved by using a multiregistry setup with a quarantine workflow pipeline described next.</p>&#13;
&#13;
<p>First, we can provide developers with a self-service portal to request images. Something like ServiceNow or a Jenkins job works fine here, and we’ve seen this many times. Chatbots can also offer a more seamless integration for developers and are gaining popularity. Once an image is requested, it is automatically pulled to a <em>quarantine</em> registry where checks can be run on the image and the pipeline can spin up environments to pull and verify the image meets specific criteria.</p>&#13;
&#13;
<p>Once the checks pass, the image can be signed (this is optional, see <a data-type="xref" href="#image_signing">“Image Signing”</a> for more information) and pushed to an approved registry. The developer can also be notified (either via the chatbot, or an updated ticket/job, etc.) that the image has been approved (or rejected, and the reasoning). The whole flow can be seen in <a data-type="xref" href="#quarantine_flow">Figure 15-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="quarantine_flow">&#13;
<img alt="prku 1502" src="assets/prku_1502.png"/>&#13;
<h6><span class="label">Figure 15-2. </span>Quarantine flow.</h6>&#13;
</div></figure>&#13;
&#13;
<p>This flow can be combined with admission controllers to ensure that only images that are signed, or those that come from a specific registry, are allowed to be run on a cluster.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Image Signing" data-type="sect2"><div class="sect2" id="image_signing">&#13;
<h2>Image Signing</h2>&#13;
&#13;
<p>The issue of supply chain security is becoming more prevalent as applications come to rely on an increasing number of external dependencies, be those code libraries or container images.<a data-primary="image registries" data-secondary="image signing" data-type="indexterm" id="idm45611971394376"/><a data-primary="software supply chain" data-secondary="image registries" data-tertiary="image signing" data-type="indexterm" id="idm45611971393400"/></p>&#13;
&#13;
<p>One of the security features often mentioned when discussing images is the notion of signing. Very simply, the concept with signing is that an image publisher can cryptographically sign an image by generating a hash of the image and associating their identity with it before pushing it to a registry. Users are then able to verify the authenticity of an image by validating the signed hash against the publisher’s public key.</p>&#13;
&#13;
<p>This workflow is attractive because it means we can create an image at the start of our software supply chain and sign it after each stage of the pipeline. Perhaps we can sign it after testing has been completed, and again after it has been approved for deployment by a release management team. Then at deploy time we are able to gate the deployment of the image into production based on whether it has been signed by the various parties that we specify. Not only do we ensure that it has passed those approvals, but we ensure that it is exactly the same image that is now being promoted to a production environment. This high-level flow is shown in <a data-type="xref" href="#signing_flow">Figure 15-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="signing_flow">&#13;
<img alt="prku 1503" src="assets/prku_1503.png"/>&#13;
<h6><span class="label">Figure 15-3. </span>Signing flow.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The primary project in this area is Notary, which was originally developed by Docker and is built upon The Update Framework (TUF), a system designed to facilitate the secure distribution of software updates.<a data-primary="Notary" data-type="indexterm" id="idm45611971386792"/></p>&#13;
&#13;
<p>Despite its benefits, we have not encountered much adoption of image signing in the field for a couple of reasons. First,  Notary has several components including a server and multiple databases. These are additional components that need to be installed, configured, and maintained. Not only that, but because the ability to sign and verify images is usually in the critical path for software deployment, the Notary system must be configured for high availability and resilience.</p>&#13;
&#13;
<p>Secondly, Notary requires each image be identified with a Globally Unique Name (GUN), which includes the registry URL as part of the name. This makes signing more problematic if you have multiple registries (e.g., caches, edge locations) as the signatures are tied to the registry and cannot be moved/copied.</p>&#13;
&#13;
<p>Finally, Notary and TUF require different sets of key pairs to be used across the signing process. Each of the keys have different security requirements and can be challenging to rotate in the case of a security breach. While it provides an academically well-designed solution, the current Notary/TUF implementation posed too high of a barrier to entry for many organizations that were only just getting comfortable with some of the base technologies they were using. Thus, many weren’t ready to trade more convenience and knowledge for the additional security benefits that the signing workflow provided.</p>&#13;
&#13;
<p>At the time of writing, there are efforts underway to develop and release a second version of Notary. This updated version should improve the user experience by solving many of the issues just discussed, like reducing the complexity of key management and eliminating the constraint that signatures are not transferable by bundling them with the OCI images themselves.</p>&#13;
&#13;
<p>There are already several existing projects that implement an admission webhook that will check images to ensure they have been signed before allowing them to be run in a Kubernetes cluster. Once the issues are addressed, we anticipate signing becoming a more oft-implemented property of the software supply chain and these signing admission webhooks to mature even further.<a data-primary="image registries" data-startref="ix_imgreg" data-type="indexterm" id="idm45611971381832"/><a data-primary="software supply chain" data-secondary="image registries" data-startref="ix_softSCimgr" data-type="indexterm" id="idm45611971380856"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Continuous Delivery" data-type="sect1"><div class="sect1" id="continuous_delivery">&#13;
<h1>Continuous Delivery</h1>&#13;
&#13;
<p>In the previous sections we’ve discussed in detail the process of converting source code into a container image.<a data-primary="software supply chain" data-secondary="continuous delivery" data-type="indexterm" id="ix_softSCCD"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-type="indexterm" id="ix_CICDCD"/> We also looked at where images are stored and the architectural and procedural decisions we need to make around choosing and deploying image registries. In this final section we’ll turn to examining the entire pipeline that ties these early steps up with the actual deployment of the image to potentially multiple Kubernetes clusters across many environments (test, staging, production).</p>&#13;
&#13;
<p>We’ll cover how to integrate the build process into the automated pipeline before looking at imperative, push-driven pipelines that many folks will already be familiar with. Lastly, we’ll take a look at some of the principles and tooling emerging in the field of GitOps, a relatively new approach to deployments that leverages version control repositories as the source of truth for the assets that should be deployed to our environments.</p>&#13;
&#13;
<p>It’s worth noting that continuous delivery is a huge area and is the sole subject of many books. In this section we’re assuming some knowledge of CD principles, and we’ll focus on how to implement those principles within Kubernetes and associated tooling.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Integrating Builds into a Pipeline" data-type="sect2"><div class="sect2" id="idm45611971373224">&#13;
<h2>Integrating Builds into a Pipeline</h2>&#13;
&#13;
<p>For local development and testing phases, developers may build images with Docker locally<a data-primary="software supply chain" data-secondary="continuous delivery" data-tertiary="integrating builds into a pipeline" data-type="indexterm" id="ix_softSCCDpl"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-tertiary="integrating builds into a pipeline" data-type="indexterm" id="ix_CICDCDpl"/>. However, anything beyond those early phases and organizations will want to have builds performed as part of an automated pipeline triggered by the committing of code into a central version control repository. We’ll talk later in this chapter about more advanced patterns around the actual <em>deployment</em> of images into an environment, but in this section want to focus purely on how the build phases can also be run in cluster using cloud native pipeline automated tooling.</p>&#13;
&#13;
<p>We typically want new image builds to be triggered with a code commit. Some pipeline tools will intermittently poll a set of configured repositories and trigger a task run when changes are detected. In other cases it might be possible to trigger a process to begin by firing a webhook from the version control system. We’ll use a few examples from Tekton, a popular open source pipeline tool that is designed to run on Kubernetes to illustrate some concepts in this section. <a data-primary="Tekton" data-type="indexterm" id="idm45611971366744"/>Tekton (and many other Kubernetes native tools) utilize CRDs to describe components in the pipeline. In the following code, we can see a (heavily edited) instance of a <code>Task</code> CRD that can be reused across multiple pipelines.<a data-primary="Task CRD" data-type="indexterm" id="idm45611971365336"/> Tekton maintains a catalog of common actions (such as cloning a git repository, as shown in the following snippet) that can be utilized in your own pipelines:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">tekton.dev/v1beta1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Task</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">git-clone</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">workspaces</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">output</code>&#13;
      <code class="nt">description</code><code class="p">:</code> <code class="s">"The</code><code class="nv"> </code><code class="s">git</code><code class="nv"> </code><code class="s">repo</code><code class="nv"> </code><code class="s">will</code><code class="nv"> </code><code class="s">be</code><code class="nv"> </code><code class="s">cloned</code><code class="nv"> </code><code class="s">onto</code><code class="nv"> </code><code class="s">the</code><code class="nv"> </code><code class="s">\</code>&#13;
      <code class="s">volume</code><code class="nv"> </code><code class="s">backing</code><code class="nv"> </code><code class="s">this</code><code class="nv"> </code><code class="s">workspace"</code>&#13;
  <code class="nt">params</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">url</code>&#13;
      <code class="nt">description</code><code class="p">:</code> <code class="l-Scalar-Plain">git url to clone</code>&#13;
      <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">string</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">revision</code>&#13;
      <code class="nt">description</code><code class="p">:</code> <code class="l-Scalar-Plain">git revision to checkout (branch, tag, sha, ref…)</code>&#13;
      <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">string</code>&#13;
      <code class="nt">default</code><code class="p">:</code> <code class="l-Scalar-Plain">master</code>&#13;
    <code class="l-Scalar-Plain">&lt;...snip...&gt;</code>&#13;
  <code class="nt">results</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">commit</code>&#13;
      <code class="nt">description</code><code class="p">:</code> <code class="l-Scalar-Plain">The precise commit SHA that was fetched by this Task</code>&#13;
  <code class="nt">steps</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">clone</code>&#13;
      <code class="nt">image</code><code class="p">:</code> <code class="s">"gcr.io/tekton-releases/github.com/tektoncd/\</code>&#13;
      <code class="s">pipeline/cmd/git-init:v0.12.1"</code>&#13;
      <code class="nt">script</code><code class="p">:</code> <code class="p-Indicator">|</code>&#13;
        <code class="no">CHECKOUT_DIR="$(workspaces.output.path)/$(params.subdirectory)"</code>&#13;
        <code class="no">&lt;...snip...&gt;</code>&#13;
        <code class="no">/ko-app/git-init \</code>&#13;
          <code class="no">-url "$(params.url)" \</code>&#13;
          <code class="no">-revision "$(params.revision)" \</code>&#13;
          <code class="no">-refspec "$(params.refspec)" \</code>&#13;
          <code class="no">-path "$CHECKOUT_DIR" \</code>&#13;
          <code class="no">-sslVerify="$(params.sslVerify)" \</code>&#13;
          <code class="no">-submodules="$(params.submodules)" \</code>&#13;
          <code class="no">-depth "$(params.depth)"</code>&#13;
        <code class="no">cd "$CHECKOUT_DIR"</code>&#13;
        <code class="no">RESULT_SHA="$(git rev-parse HEAD | tr -d '\n')"</code>&#13;
        <code class="no">EXIT_CODE="$?"</code>&#13;
        <code class="no">if [ "$EXIT_CODE" != 0 ]</code>&#13;
        <code class="no">then</code>&#13;
          <code class="no">exit $EXIT_CODE</code>&#13;
        <code class="no">fi</code>&#13;
        <code class="no"># Make sure we don't add a trailing newline to the result!</code>&#13;
        <code class="no">echo -n "$RESULT_SHA" &gt; $(results.commit.path)</code></pre>&#13;
&#13;
<p>As mentioned in previous sections, there are many different ways of building OCI images. Some of these require a Dockerfile, and some do not. You may also need to perform additional actions as part of a build. Almost all pipeline tools expose the notion of stages, steps, or tasks that allow users to configure discrete pieces of functionality that can be chained together.<a data-primary="Cloud Native Buildpacks (CNB)" data-type="indexterm" id="idm45611971355352"/><a data-primary="Task CRD" data-type="indexterm" id="idm45611971354696"/> The following code snippet shows an example <code>Task</code> definition that uses Cloud Native Buildpacks to build an image:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">tekton.dev/v1beta1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Task</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">buildpacks-phases</code>&#13;
  <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app.kubernetes.io/version</code><code class="p">:</code> <code class="s">"0.1"</code>&#13;
  <code class="nt">annotations</code><code class="p">:</code>&#13;
    <code class="nt">tekton.dev/pipelines.minVersion</code><code class="p">:</code> <code class="s">"0.12.1"</code>&#13;
    <code class="nt">tekton.dev/tags</code><code class="p">:</code> <code class="l-Scalar-Plain">image-build</code>&#13;
    <code class="nt">tekton.dev/displayName</code><code class="p">:</code> <code class="s">"buildpacks-phases"</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">params</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">BUILDER_IMAGE</code>&#13;
      <code class="nt">description</code><code class="p">:</code> <code class="s">"The</code><code class="nv"> </code><code class="s">image</code><code class="nv"> </code><code class="s">on</code><code class="nv"> </code><code class="s">which</code><code class="nv"> </code><code class="s">builds</code><code class="nv"> </code><code class="s">will</code><code class="nv"> </code><code class="s">run</code><code class="nv"> </code><code class="s">\</code>&#13;
      <code class="s">(must</code><code class="nv"> </code><code class="s">include</code><code class="nv"> </code><code class="s">lifecycle</code><code class="nv"> </code><code class="s">and</code><code class="nv"> </code><code class="s">compatible</code><code class="nv"> </code><code class="s">buildpacks)."</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">PLATFORM_DIR</code>&#13;
      <code class="nt">description</code><code class="p">:</code> <code class="l-Scalar-Plain">The name of the platform directory.</code>&#13;
      <code class="nt">default</code><code class="p">:</code> <code class="l-Scalar-Plain">empty-dir</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">SOURCE_SUBPATH</code>&#13;
      <code class="nt">description</code><code class="p">:</code> <code class="s">"A</code><code class="nv"> </code><code class="s">subpath</code><code class="nv"> </code><code class="s">within</code><code class="nv"> </code><code class="s">the</code><code class="nv"> </code><code class="s">`source`</code><code class="nv"> </code><code class="s">input</code><code class="nv"> </code><code class="s">\</code>&#13;
      <code class="s">where</code><code class="nv"> </code><code class="s">the</code><code class="nv"> </code><code class="s">source</code><code class="nv"> </code><code class="s">to</code><code class="nv"> </code><code class="s">build</code><code class="nv"> </code><code class="s">is</code><code class="nv"> </code><code class="s">located."</code>&#13;
      <code class="nt">default</code><code class="p">:</code> <code class="s">""</code>&#13;
&#13;
  <code class="nt">resources</code><code class="p">:</code>&#13;
    <code class="nt">outputs</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">image</code>&#13;
        <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">image</code>&#13;
&#13;
  <code class="nt">workspaces</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">source</code>&#13;
&#13;
  <code class="nt">steps</code><code class="p">:</code>&#13;
    <code class="l-Scalar-Plain">&lt;...snip...&gt;</code>&#13;
    <code class="l-Scalar-Plain">- name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">build</code>&#13;
      <code class="l-Scalar-Plain">image</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">$(params.BUILDER_IMAGE)</code>&#13;
      <code class="l-Scalar-Plain">imagePullPolicy</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Always</code>&#13;
      <code class="l-Scalar-Plain">command</code><code class="p-Indicator">:</code> <code class="p-Indicator">[</code><code class="s">"/cnb/lifecycle/builder"</code><code class="p-Indicator">]</code>&#13;
      <code class="nt">args</code><code class="p">:</code>&#13;
        <code class="p-Indicator">-</code> <code class="s">"-app=$(workspaces.source.path)/$(params.SOURCE_SUBPATH)"</code>&#13;
        <code class="p-Indicator">-</code> <code class="s">"-layers=/layers"</code>&#13;
        <code class="p-Indicator">-</code> <code class="s">"-group=/layers/group.toml"</code>&#13;
        <code class="p-Indicator">-</code> <code class="s">"-plan=/layers/plan.toml"</code>&#13;
      <code class="nt">volumeMounts</code><code class="p">:</code>&#13;
        <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">layers-dir</code>&#13;
          <code class="nt">mountPath</code><code class="p">:</code> <code class="l-Scalar-Plain">/layers</code>&#13;
        <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">$(params.PLATFORM_DIR)</code>&#13;
          <code class="nt">mountPath</code><code class="p">:</code> <code class="l-Scalar-Plain">/platform</code>&#13;
        <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">empty-dir</code>&#13;
          <code class="nt">mountPath</code><code class="p">:</code> <code class="l-Scalar-Plain">/tekton/home</code>&#13;
    <code class="l-Scalar-Plain">&lt;...snip...&gt;</code></pre>&#13;
&#13;
<p>We can then tie together this task (and others) with our input repository as part of a <code>Pipeline</code> (not shown here). This involves mapping the workspace we cloned our git repository into earlier with the workspace that our buildpack builder will use as a source. We can also specify that the image be pushed to a registry at the end of the process.</p>&#13;
&#13;
<p>The flexibility of this approach (configurable task blocks) means that pipelines become very powerful tools for defining a build flow on Kubernetes. We could add testing and/or linting stages to the build, or some kind of static code analysis. We could also easily add a signing step to our image (as described in <a data-type="xref" href="#image_signing">“Image Signing”</a>) if desired. We can also define our own tasks to run other build tools such as Kaniko or BuildKit (if not utilizing buildpacks as in this example).<a data-primary="software supply chain" data-secondary="continuous delivery" data-startref="ix_softSCCDpl" data-tertiary="integrating builds into a pipeline" data-type="indexterm" id="idm45611970994344"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-startref="ix_CICDCDpl" data-tertiary="integrating builds into a pipeline" data-type="indexterm" id="idm45611970992840"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Push-Based Deployments" data-type="sect2"><div class="sect2" id="idm45611970991240">&#13;
<h2>Push-Based Deployments</h2>&#13;
&#13;
<p>In the previous section we saw how to automate builds in a pipeline.<a data-primary="deployments" data-secondary="push-based" data-type="indexterm" id="ix_deppsh"/><a data-primary="software supply chain" data-secondary="continuous delivery" data-tertiary="push-based deployments" data-type="indexterm" id="ix_softSCCDdep"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-tertiary="push-based deployments" data-type="indexterm" id="ix_CICDCDdep"/> In this section we’ll see how this can be extended to actually perform the deployment to a cluster and some of the patterns you’ll want to implement to make these types of automated delivery pipelines easier.</p>&#13;
&#13;
<p>Because of the flexibility of the task/step-based approach that we saw previously (which is present in almost every tool), it is trivial to create a step at the end of the pipeline that reads the tag of the newly created (and pushed) image and updates the image for the Deployment. This <em>could</em> be achieved by updating the Deployment directly in the cluster using <code>kubectl set image</code>, and several articles/tutorials still demonstrate this approach. A better alternative is to have our pipeline write the image tag change back into the YAML file describing the Deployment and then committing this change <em>back into version control</em>. We can then trigger a <code>kubectl apply</code> over the new version of the repository to enact the change. The latter approach is &#13;
<span class="keep-together">preferred</span> as we can keep our YAML as the approximate source of truth for our cluster in that case (we’ll discuss more on this in <a data-type="xref" href="#gitops">“GitOps”</a>) but the former is an acceptable iterative step when migrating to this type of Kubernetes-native automated pipeline.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611970980296">&#13;
<h5>Image Tagging and Metadata</h5>&#13;
<p>The naming and/or versioning of images is a topic that comes up periodically with clients, and we have seen <a data-primary="image tagging and metadata" data-type="indexterm" id="idm45611970978904"/>some common patterns that work in most cases. When building images out of development, using the git hash as a tag works well. These git hashes are not human-readable, but with automation pipelines in place that’s not usually a blocker and the hashes provide a nice trail back to the commit.</p>&#13;
&#13;
<p>You will want to add more descriptive versioning tags (e.g., SemVer) as the pipeline progresses so it becomes easier to see at a glance what versions are deployed in your environments. Also consider using immutable tags (which most registries support) to avoid writing over existing tags and ensure some consistency when pulling images with the same tag to an environment.</p>&#13;
&#13;
<p>It can also <a data-primary="metadata" data-secondary="image" data-type="indexterm" id="idm45611970976520"/>be really useful to add metadata (labels) to your images with contact data for the image owner and perhaps information about the build. This metadata can be reported on later on or utilized in other tooling, e.g., a policy tool that might allow some image access based on specific labels.</p>&#13;
</div></aside>&#13;
&#13;
<p class="pagebreak-before">When deploying applications to Kubernetes we have two distinct types of artifacts to consider: the code and configuration required for the application and how to <em>build</em> it, and the configuration for how to <em>deploy</em> it. We are often asked to weigh in on how best to organize these artifacts, with some folks preferring to keep <em>everything</em> related to an application together in a single tree, and others preferring to keep them &#13;
<span class="keep-together">separate</span>.</p>&#13;
&#13;
<p>Our advice is usually to choose the latter route for the following reasons:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Each concern is usually the responsibility of a separate domain or team in the organization. While developers should be aware of how their applications will be deployed and will have input into the process, the configuration around sizing, environments, the injection of secrets, etc. mostly sit as responsibilities of the platform or operations team.</p>&#13;
</li>&#13;
<li>&#13;
<p>Security permissions and audit requirements are likely different for code repositories versus those containing deployment pipeline artifacts, secrets, and environment configurations.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Once we have our deployment configurations in a separate repository, it’s easy to follow how the deployment pipeline might first checkout this repository, then run an update of the image tag (using <code>sed</code> or something similar), and finally <em>check the change back in to git</em> to ensure that is our source of truth. We can then run <code>kubectl apply -f</code> over the changed manifests. This imperative (or <em>push-based</em>) model provides great auditability as we can leverage the built-in reporting and logging capabilities provided by our version control system and easily see the changes flow through our pipeline, as shown in <a data-type="xref" href="#push_based_deployments">Figure 15-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="push_based_deployments">&#13;
<img alt="prku 1504" src="assets/prku_1504.png"/>&#13;
<h6><span class="label">Figure 15-4. </span>Push-based deployments.</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Depending on the level of automation within your organization, you may want to have promotions between environments handled by your pipelines, and even deployments executed against different Kubernetes clusters. There are certainly ways to achieve this with most tools, and some will have better native support for this than others. However, this is an area where the imperative pipeline model described in this subsection can be more challenging to implement because we have to keep an inventory (and credentials) for each of the clusters we want to use as a target.</p>&#13;
&#13;
<p>Another challenge of this imperative (where we have a centralized tool pushing changes out to our environments) is that if the pipeline is interrupted for some reason, we need to ensure that it is restarted or reconciled back to a healthy state. We also need to maintain monitoring and alerting on our deployment pipelines (however they are implemented) to make sure that we’re aware of deployment issues if they arise.<a data-primary="deployments" data-secondary="push-based" data-startref="ix_deppsh" data-type="indexterm" id="idm45611970961080"/><a data-primary="software supply chain" data-secondary="continuous delivery" data-startref="ix_softSCCDdep" data-tertiary="push-based deployments" data-type="indexterm" id="idm45611970959832"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-startref="ix_CICDCDdep" data-tertiary="push-based deployments" data-type="indexterm" id="idm45611970958344"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Rollout Patterns" data-type="sect2"><div class="sect2" id="idm45611970990648">&#13;
<h2>Rollout Patterns</h2>&#13;
&#13;
<p>We mentioned briefly at the end of the previous section the need to monitor pipelines to ensure that they successfully complete.<a data-primary="software supply chain" data-secondary="continuous delivery" data-tertiary="rollout patterns" data-type="indexterm" id="ix_softSCCDroll"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-tertiary="rollout patterns" data-type="indexterm" id="ix_CICDCDroll"/><a data-primary="rollout patterns" data-type="indexterm" id="ix_roll"/> However, when deploying new versions of applications we also need a way to monitor their health and decide whether we need to resolve issues or roll back to a previous working state.</p>&#13;
&#13;
<p>There are several patterns that organizations may want to implement. There are entire books dedicated to these patterns, but we’ll cover them briefly here to show how you might implement them in Kubernetes:</p>&#13;
<dl>&#13;
<dt>Canary</dt>&#13;
<dd>&#13;
<p>Canary releases are<a data-primary="canary releases" data-type="indexterm" id="idm45611970948568"/> where a new version of an application is deployed to the cluster and a small subset of traffic (based on metadata, users, or some other attribute) is directed to the new version. This can be monitored closely to ensure that the new version (Canary) behaves the same way as the previous version, or at least does not result in an error scenario. The percentage of traffic can slowly be increased over time as confidence increases.</p>&#13;
</dd>&#13;
<dt>Blue/green</dt>&#13;
<dd>&#13;
<p>This approach is similar to the Canary but involves more of a big bang cutover of traffic.<a data-primary="blue/green rollout pattern" data-type="indexterm" id="idm45611970945864"/> This could be achieved over multiple clusters (one on the old version, <em>blue</em>, and one of the new version, <em>green</em>) or could be achieved in the same cluster. The idea here is that we can test that deployment of the service works as intended and perform some tests on the environment which is not user facing before cutting traffic across to the new version. If we see elevated errors, we can cut the traffic back. There is additional nuance in this approach, of course, as your applications may need to gracefully handle state, sessions, and other concerns.</p>&#13;
</dd>&#13;
<dt>A/B testing</dt>&#13;
<dd>&#13;
<p>Again similar to the Canary, we can <a data-primary="A/B testing" data-type="indexterm" id="idm45611970942344"/>roll out a version of the application that may contain some different behavior that targets a subset of consumers. We can gather metrics and analysis on the usage patterns of the new version to make decisions about whether to roll back or forward, or expand the experiment.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These patterns move us toward the desired state of being able to decouple the <em>deployment</em> of our applications from their <em>release</em> to consumers by giving us control about when features and/or new versions are enabled.<a data-primary="deployments" data-secondary="separating from releases" data-type="indexterm" id="idm45611970939880"/> These practices are great at de-risking the deployment of changes into our environments.</p>&#13;
&#13;
<p>Most of these patterns are implemented via some kind of network traffic shifting. In Kubernetes, we have some very rich networking primitives and capabilities that make the implementation of these patterns possible. One open source tool that enables these patterns (on top of various service mesh solutions) is Flagger. Flagger runs as a controller within the Kubernetes cluster and watches for changes to the image field of Deployment resources. It exposes many tweakable options to enable the preceding patterns by programmatically configuring an underlying service mesh to appropriately shift traffic. It also adds the ability to monitor the health of newly rolled-out versions and either continue or halt and revert the rollout process.</p>&#13;
&#13;
<p>We definitely see the value of looking into Flagger and other solutions in this space. However, we see these approaches more typically broached as part of a second or third phase of an organization’s Kubernetes journey, given the additional complexity they both depend on (service mesh is required to enable most patterns) and &#13;
<span class="keep-together">introduce</span>.<a data-primary="software supply chain" data-secondary="continuous delivery" data-startref="ix_softSCCDroll" data-tertiary="rollout patterns" data-type="indexterm" id="idm45611970935912"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-startref="ix_CICDCDroll" data-tertiary="rollout patterns" data-type="indexterm" id="idm45611970934360"/><a data-primary="rollout patterns" data-startref="ix_roll" data-type="indexterm" id="idm45611970932840"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GitOps" data-type="sect2"><div class="sect2" id="gitops">&#13;
<h2>GitOps</h2>&#13;
&#13;
<p>So far we have looked at adding a push-based deployment stage into your delivery pipelines for Kubernetes.<a data-primary="software supply chain" data-secondary="continuous delivery" data-tertiary="GitOps" data-type="indexterm" id="idm45611970929672"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-tertiary="GitOps" data-type="indexterm" id="idm45611970928424"/><a data-primary="GitOps" data-type="indexterm" id="idm45611970927240"/> An emerging alternative model in the deployment space is GitOps. Rather than imperatively push out changes to the cluster, the GitOps model features a controller that constantly reconciles the contents of a git repository with resources running in the cluster, as shown in <a data-type="xref" href="#gitops_flow">Figure 15-5</a>. This model brings it closely in line with the control-loop reonciliation experience that we get with Kubernetes itself. The two primary tools in the GitOps space are ArgoCD and Flux, and both teams are working together on a common engine to underpin their respective tools.</p>&#13;
&#13;
<figure><div class="figure" id="gitops_flow">&#13;
<img alt="prku 1505" src="assets/prku_1505.png"/>&#13;
<h6><span class="label">Figure 15-5. </span>GitOps flow.</h6>&#13;
</div></figure>&#13;
&#13;
<p>There are a couple of major benefits to this model:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It is declarative in nature, so that any issues with the deployment itself (tooling going down, etc.) or deployments being deleted ad hoc will result in (attempted) reconciliation to a good state.</p>&#13;
</li>&#13;
<li>&#13;
<p>Git becomes our single source of truth, and we can leverage existing expertise and familiarity with the tooling, in addition to getting a strong audit log of changes by default. We can use a pull request workflow as a gate for changes to our clusters and integrate with external tooling as required through the extension points that most version control systems expose (webhooks, workflows, actions, etc.).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>However, this model is not without downsides. For those organizations that truly want to use git as their <em>single</em> source of truth, this means keeping secret data in version control. Several projects have emerged over recent years to solve this problem, with the most well-known of these being Bitnami’s Sealed Secrets. That project allows encrypted versions of Secrets to be committed to a repository and then decrypted once applied to the cluster (so as to be available to applications). We discuss this approach more in <a data-type="xref" href="ch05.html#chapter5">Chapter 5</a>.</p>&#13;
&#13;
<p>We also need to ensure that we are monitoring the current health of state synchronization. If the pipeline was push-based and fails, we’ll see a failure in the pipeline. However, because the GitOps approach is declarative, we need to make sure we get alerted if the observed state (in cluster) and the declared state (in git) remain diverged for an extended period of time.</p>&#13;
&#13;
<p class="pagebreak-before">Increased embracing of GitOps is a trend we see in the field, although it’s definitely a paradigm shift from more traditional push-based models. Not all applications deploy cleanly as-is with a flat application of YAML resources, and it may be necessary to build in ordering and some scripting initially as organizations transition to this approach.</p>&#13;
&#13;
<p>It’s also important to be cognizant of tools that may create, modify, or delete resources as part of their life cycle as these sometimes require some massaging to fit into the GitOps model. An example of this would be a controller that runs in the cluster and watches for a specific CRD and then creates multiple other resources directly via the Kubernetes API. If running in <em>strict</em> mode, GitOps tooling may delete those dynamically created resources as they are not in the single source of truth (the git repo). This deletion of <em>unknown</em> resources may, of course, be desirable in most cases and is one of the <em>positive</em> attributes of GitOps. However, you should absolutely be mindful of situations where changes may intentionally occur out of band from the git repository that may break the model and need to be worked around.<a data-primary="software supply chain" data-secondary="continuous delivery" data-startref="ix_softSCCD" data-type="indexterm" id="idm45611970913256"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="continuous delivery" data-startref="ix_CICDCD" data-type="indexterm" id="idm45611970912008"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611970931336">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter we looked at the process of getting source code into a container and deployed onto a Kubernetes cluster. Many of the stages and principles that you will already be familiar with (build/test, CI, CD, etc.) still apply in a container/Kubernetes environment but with different tooling. At the same time some concepts (like GitOps) will likely be new, building on concepts that are present in Kubernetes itself to enhance reliability and security within existing deployment patterns.</p>&#13;
&#13;
<p>There are many tools in this space that can enable many different flows and patterns. However, one of the key takeaways from this chapter should be the importance of deciding how much (or little) to expose each part of this pipeline to different groups in the organization. Maybe development teams are engaged with Kubernetes and confident enough to write build and deployment artifacts (or at least have significant input). Or maybe the desire is toward abstracting all the underlying details away from development teams to ease scaling and standardization concerns, at the expense of additional burden on the platform teams to put relevant foundations and automation in place.<a data-primary="software supply chain" data-startref="ix_softSC" data-type="indexterm" id="idm45611970907784"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>