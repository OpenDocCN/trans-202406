<html><head></head><body><section data-pdf-bookmark="Chapter 11. Migrating Data Workloads to Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="migrating_data_workloads_to_kubernetes">&#13;
<h1><span class="label">Chapter 11. </span>Migrating Data Workloads to Kubernetes</h1>&#13;
<p><a contenteditable="false" data-primary="data workloads, migrating to Kubernetes" data-type="indexterm" id="dw_mig"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="migrating data workloads to" data-type="indexterm" id="kub_mig"/><a contenteditable="false" data-primary="application-aware platforms" data-type="indexterm" id="idm46183195517520"/><a contenteditable="false" data-primary="McLuckie, Craig" data-type="indexterm" id="idm46183195516448"/>In the first chapter, we presented a vision for combining all of the infrastructure needed for your cloud native applications into one place: Kubernetes. Our argument was simple: if you’re excluding data and its supporting infrastructure from your Kubernetes deployments, you haven’t fully embraced cloud native principles. We’ve covered a lot of ground since then, examining how various types of data infrastructure work on Kubernetes and demonstrating the art of the possible.</p>&#13;
<p>So, where do you go from here? What are the steps to fully realize this vision? At this point, you may already have some parts of your applications in Kubernetes. More than likely, you also have several previous generations of infrastructure such as containers, VMs, or bare-metal servers, whether running in your own datacenters or in the cloud. In this final chapter, we’ll leverage everything you’ve learned so far to help you create a plan to fully manage your cloud native data in Kubernetes.</p>&#13;
<section data-pdf-bookmark="The Vision: Application-Aware Platforms" data-type="sect1"><div class="sect1" id="the_vision_application_aware_platforms">&#13;
<h1>The Vision: Application-Aware Platforms</h1>&#13;
<p>Throughout the book, we’ve heard a diverse range of voices in the community present their wisdom about data in Kubernetes and practical advice for this monumental undertaking. No matter where you are in the process, whether you’re a Kubernetes beginner or a seasoned multiyear operator, we all have things to learn from their expertise. Now it’s time to zoom out and consider how the move to Kubernetes intersects with other trends in the software industry. Craig McLuckie was part of the team that created Kubernetes at Google and eventually shepherded it into open source. He’s been very active in the cloud native infrastructure community and shares some possibilities and challenges as we move toward data on Kubernetes.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="what_the_kubernetes_duty_cycle_means_fo">&#13;
<h5>What the Kubernetes Duty Cycle Means for Data</h5>&#13;
<p><em>With Craig McLuckie, Kubernetes OSS cocreator</em></p>&#13;
<p><a contenteditable="false" data-primary="duty cycle" data-type="indexterm" id="idm46183194922608"/>In electronics, the term <em>duty cycle</em> describes the time period when a signal is active in a system. In the IT domain, there are duty cycles associated with systems and technologies, and in some cases these cycles can be extremely long. For example, many people are surprised to learn that IBM continues to sell mainframes as a huge part of its business. In recognition of the evolving landscape of cloud native, we should anticipate a long duty cycle for Kubernetes as well. It’s projected that by 2024, more than half of workloads running in public clouds will be hosted in Kubernetes, representing a growth rate of 24% year over year.</p>&#13;
<p>As an early proponent of not running stateful workloads in Kubernetes, I was always cautious to say it wasn’t ready for data…yet. Now, things are beginning to change. Kubernetes began as a relatively simple way to orchestrate containers, but Kubernetes represents key concepts that help build higher layers beyond just infrastructure. For example, Kubernetes began as an ideal logical infrastructure abstraction and a way to frame systems thinking. It provides a set of logical primitives decoupled from the infrastructure that you can use to construct distributed systems. Now we’re seeing applications like data infrastructure being built specifically for Kubernetes, which opens a more interesting future.</p>&#13;
<p><a contenteditable="false" data-primary="Cluster API" data-type="indexterm" id="idm46183194914816"/><a contenteditable="false" data-primary="CRD (Custom Resource Definition)" data-type="indexterm" id="idm46183194933616"/>Another key concept being adopted outside the Kubernetes core is the controller-reconciler pattern. The <a href="https://oreil.ly/6kmTt">Cluster API project</a> uses Kubernetes-style APIs and the controller-reconciler pattern to automate lifecycle management of Kubernetes clusters. Starting with a CRD, a well-formed Kubernetes cluster can be assembled from infrastructure based on a declarative expression. This is a great example of the power of the controller-reconciler pattern applied to custom resources that can be followed for other infrastructure.</p>&#13;
<p><a contenteditable="false" data-primary="PaaS (platform as a service)" data-type="indexterm" id="idm46183194939568"/>Following the convention of expressing as much as possible in a declarative context can help us address the problems associated with data management. We can then apply the pattern of choreography to handle fleet management of distributed data infrastructure, one of the historical challenges of running data at scale. The use of custom resources will be as disruptive as Kubernetes itself. There will be tremendous utility and value in having controls and capabilities built on Kubernetes expressed as an API. Bringing this platform-as-a-service (PaaS) mindset into enterprise organizations will enable developers to access data services in much the same way as they interact with public cloud services.</p>&#13;
<p><a contenteditable="false" data-primary="IoC (Inversion of control) pattern" data-type="indexterm" id="idm46183195413488"/><a contenteditable="false" data-primary="duck typing" data-type="indexterm" id="idm46183194913664"/>The inversion of control (IoC) pattern provides us with a powerful technique to build this type of infrastructure. Starting with a declarative description of the desired service, a controller implementing the description will read the manifest, and render and connect the components. Now combine that with the work done in Knative with <a href="https://oreil.ly/ANlHt">duck typing</a>, which allows a resource to declare a dependency on another resource using a well-defined syntax. An intelligent system can examine each resource as it is provisioned to see if it matches any unbound dependencies and perform dependency injection and binding. If we do our job right, we will start to build application-aware platforms, a game-changing switch. In a world where applications can express their data infrastructure needs declaratively, we can move away from building infrastructure-coupled applications.</p>&#13;
<p>This level of automation doesn’t mean that infrastructure will be completely hands-off, with nothing for SREs to do. Data infrastructure providers should strive to provide high-level abstractions first, but also provide lower-level access. When constructing systems, situations inevitably arise requiring users to be able to break glass and look at the number of Pods configured, or tune resource allocations based on insights into application behavior.</p>&#13;
<p><a contenteditable="false" data-primary="national clouds" data-type="indexterm" id="idm46183194959712"/><a contenteditable="false" data-primary="sovereign clouds" data-type="indexterm" id="idm46183194963920"/>The world isn’t getting any less complicated for people building applications. Data sovereignty laws are becoming more and more prevalent, and we are starting to think in terms of “sovereign clouds” and even “national clouds.” When you layer on things like edge computing, it gets even more complicated as you want to run compute workloads closer to your users. Of the three main infrastructure resources—compute, network, and storage—networking will become more and more important as highly distributed use cases emerge.</p>&#13;
<p>Kubernetes offers a solution to these looming challenges—creating a unique opportunity to deliver an “as a service” experience into infrastructure that you control. The real potency will come via highly optimized “as a service” experiences delivered via a connected control plane into any infrastructure destination. Whoever can figure this out will create a tremendous amount of usability and, ultimately, win the game.</p>&#13;
</div></aside>&#13;
<p>Craig offers an inspiring vision for a future where infrastructure conforms to the application instead of the infrastructure-coupled applications we have today. As you’ve seen in the technologies we’ve explored in this book, the idea of declarative infrastructure that reconciles via the Kubernetes control plane is everywhere. Now we can begin to flip the script by building applications from the top down instead of from the bottom up. This is an opportunity to change the way your organization leverages data technology. Are you ready to start? It’s time to map out your journey.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Charting Your Path to Success" data-type="sect1"><div class="sect1" id="charting_your_path_to_success">&#13;
<h1>Charting Your Path to Success</h1>&#13;
<p><a contenteditable="false" data-primary="PPT (people, process, and technology)" data-type="indexterm" id="ppt_ab"/>In preparing to migrate your stateful workloads to Kubernetes, you’ll probably have a few questions in mind, like “What technologies should we use?” and “How will we roll out the changes?” and “How do we make sure our team is ready?” Most of these questions will map nicely to the classic IT framework of people, process, and technology (PPT). Since every organization’s journey will be different, we’ll provide recommendations in each category instead of a detailed roadmap. An important part of your exercise is choosing what migrates into Kubernetes and what doesn’t. Every migration should have a strong case.</p>&#13;
<p>You will likely have some of these recommendations in place already, so the actual work needed is to ensure that your efforts in all three areas work together toward your desired outcome. One word of warning: this is not the time to “run fast and break things.” You’ll have plenty of time to do that after you have the core elements in place. With a strong foundation, you will achieve levels of agility and speed you haven’t seen before.</p>&#13;
<section data-pdf-bookmark="People" data-type="sect2"><div class="sect2" id="people">&#13;
<h2>People</h2>&#13;
<p>The core of any IT organization is its people. Migrating any workload to Kubernetes represents a massive shift in mindset for your organization and requires proper training and preparation. You will need people who understand the technology already or are willing to learn. This requirement is even more true in preparing to migrate to stateful workloads. Beyond the apparent tasks of training up on Kubernetes and reading books like this one, we’d like to draw your attention to two areas: specific job roles that successful organizations execute well and leveraging open source communities as a force multiplier for your teams.</p>&#13;
<section data-pdf-bookmark="Critical people roles for cloud native data" data-type="sect3"><div class="sect3" id="critical_people_roles_for_cloud_native">&#13;
<h3>Critical people roles for cloud native data</h3>&#13;
<p><a contenteditable="false" data-primary="cloud native data" data-secondary="critical people roles for" data-type="indexterm" id="idm46183195048048"/>We could list many roles that are key to a successful migration, but we’ll highlight three that are central to managing cloud native data and discuss how they relate:</p>&#13;
<dl>&#13;
<dt>Cloud architects</dt>&#13;
<dd><a contenteditable="false" data-primary="cloud architects" data-type="indexterm" id="idm46183195260640"/>Architects provide technical direction to the development of cloud applications, influencing everything from the clouds and regions where you’ll deploy your applications, to the data infrastructure you’ll use. This includes when to rely on self-managed open source projects versus managed services. An effective cloud architect carefully selects technology to meet current business needs while leaving room for future extensibility.</dd>&#13;
<dt>Site reliability engineers</dt>&#13;
<dd><a contenteditable="false" data-primary="SREs (site reliability engineers)" data-type="indexterm" id="idm46183194851024"/>In <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>, we talked about adopting an SRE mindset. While this mindset is something that every engineer in your organization should be working toward, DBAs have an incredibly strategic opportunity to make the transition into an SRE role. Instead of just deploying a database and walking away, a data-focused SRE takes a holistic view of the data infrastructure and how it supports the system’s overall goals, with an eye toward the best performance for the cost.</dd>&#13;
<dt>Data engineers</dt>&#13;
<dd><a contenteditable="false" data-primary="data engineers/engineering" data-type="indexterm" id="idm46183197145392"/>Whereas data scientists are concerned about extracting the value from data, data engineers are responsible for operationalizing data. They build data processes, assemble systems, and think about the end-user consumption of data products. Data engineers should be versed not only in Kubernetes-based technology but also in what cloud services can be used in concert for an optimized outcome. Data engineers will play a significant role in selecting and deploying technology that supports the AI/ML workloads we discussed in <a data-type="xref" href="ch10.html#machine_learning_and_other_emerging_use">Chapter 10</a>, composing multiple components to create flows that deliver real-time insights into your applications.</dd>&#13;
</dl>&#13;
<p>To think about how these roles work together in an organization, consider the analogy of a farming operation:</p>&#13;
<ul>&#13;
<li><p>The architect is like the planner who determines what crops to grow, and in what quantities in each season.</p></li>&#13;
<li><p>The SRE is like the farmer who plants and cultivates the crops to ensure they are healthy and productive.</p></li>&#13;
<li><p>The data engineer is like a distributor who harvests the crops and ensures they reach their proper destination.</p></li>&#13;
</ul>&#13;
<p>If you don’t already have these roles defined within your organization, don’t worry. In many cases, it is possible to retrain engineers in your organization who are currently in a different role.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Communities to fast-track your innovation" data-type="sect3"><div class="sect3" id="communities_to_fast_track_your_innovati">&#13;
<h3>Communities to fast-track your innovation</h3>&#13;
<p><a contenteditable="false" data-primary="cloud native data" data-secondary="communities in" data-type="indexterm" id="cnd_com"/><a contenteditable="false" data-primary="communities, in cloud native data" data-type="indexterm" id="com_cnd"/>To paraphrase the sword-wielding old man in <em>The Legend of Zelda</em>, it’s dangerous to go alone. Bring friends. Communities are a core part of working in technology, and we work together, learn together, and share successes and failures. When embarking on a new technology journey, look for the communities that form around that technology. The following are a few notable communities in cloud native data. You can seek them out for information, join the conversation, and hopefully contribute:</p>&#13;
<dl>&#13;
<dt>Cloud Native Computing Foundation</dt>&#13;
<dd><p><a contenteditable="false" data-primary="CNCF (Cloud Native Computing Foundation)" data-type="indexterm" id="idm46183194830592"/>Also known as the <a href="https://www.cncf.io">CNCF</a>, this organization is a part of the more extensive <a href="https://www.linuxfoundation.org">Linux Foundation</a>, a nonprofit organization devoted to open source advocacy. The CNCF is the home for Kubernetes and many projects that run in Kubernetes, including several featured in this book. You can see the amount of energy put into Kubernetes native projects from the graduated and incubating projects list. Members of CNCF pay a fee that goes to support the advocacy and administration of the foundation and its projects.</p>&#13;
<p><a contenteditable="false" data-primary="TOC (Technical Oversight Committee)" data-type="indexterm" id="idm46183194814352"/><a contenteditable="false" data-primary="TAGs (Technical Advisory Groups)" data-type="indexterm" id="idm46183195287616"/>The Technical Oversight Committee (TOC) approves and maintains the technical vision for CNCF projects. With so many projects to maintain, <a href="https://oreil.ly/KSxmL">Technical Advisory Groups (TAGs)</a> have been formed to handle cross-project concerns. Each TAG maintains its autonomy within an initial charter to create a place for similarly grouped projects to maintain interoperability standards. Each maintains its own Slack workspace and mailing lists for community discussions.</p>&#13;
<p>All development activity for a project is centered around its GitHub repository. To get involved in contributing code, search for the <a href="https://oreil.ly/xt2QR">“good first issue”</a> tag in GitHub Issues for each project. If you have broader interests, you might consider joining the conversation happening in TAGs to help shape future direction. Twice a year, the <a href="https://oreil.ly/Ijlki">KubeCon + CloudNativeCon user conferences</a> are held by the CNCF in North America, China, and Europe, with an enormous session list. Some of the best sessions are the user stories about deploying specific cloud native technologies.</p></dd>&#13;
<dt>Apache Software Foundation</dt>&#13;
<dd><p><a contenteditable="false" data-primary="ASF (Apache Software Foundation)" data-type="indexterm" id="idm46183194834624"/><a contenteditable="false" data-primary="PMC (project management committee)" data-type="indexterm" id="idm46183194805504"/>The <a href="https://www.apache.org">ASF</a> is a nonprofit organization for software conservancy. ASF members provide governance, services, and support for accepted projects. After going through an incubation process, projects graduate to become top-level projects where they earn the Apache name (e.g., Apache Cassandra, Apache Spark, and Apache Pulsar). Each project is run independently by a project management committee (PMC), and users with the right to make project changes are known as <em>committers</em>.</p>&#13;
<p>It’s important to note the distinction between the project and user communities around Apache projects. The project community is concerned with building the project, and the user community is downstream and primarily focuses on using the project in their applications. This separation of concerns is evident in the two mailing lists available for most projects: <em>dev@&lt;project name&gt;.apache.org</em> and <em>user@&lt;project name&gt;.apache.org</em>.</p>&#13;
<p><a contenteditable="false" data-primary="Stack Overflow" data-type="indexterm" id="idm46183195231520"/><a contenteditable="false" data-primary="Jira" data-type="indexterm" id="idm46183194803616"/>If you are interested in contributing code, jumping right in is the best way to start. Apache projects use <a href="https://oreil.ly/Odauf">Jira</a> to track changes and bugs. Look for “low hanging fruit” or “good first project” tags on the Jira issues. In the user community, participating in the mailing list or Stack Overflow is a great way to start contributing by helping others. Giving presentations about Apache projects is the lifeblood of awareness for each project and one of the best contributions.</p></dd>&#13;
<dt>Data on Kubernetes Community (DoKC)</dt>&#13;
<dd><p><a contenteditable="false" data-primary="DoKC (Data on Kubernetes Community)" data-type="indexterm" id="idm46183194799008"/>A different kind of organization than the CNCF and ASF, DoKC is a knowledge community composed of industry vendors and end users. DoKC isn’t a place for hosting software projects but a central gathering place for people in a growing field within infrastructure. Technology vendors sponsor the community, but the charter is to remain vendor neutral in all activities. Those activities include <span class="keep-together">in-person</span> and online meetups, blogs on the <a href="https://dok.community"><em>dok.community</em> website</a>, and a companion event to KubeCon, DoK Day.</p>&#13;
<p>In addition to gathering the community, DoKC also produces useful resources to guide users as they make decisions about data technology on Kubernetes:</p>&#13;
<ul>&#13;
<li><p><a contenteditable="false" data-primary="DoK Landscape" data-type="indexterm" id="idm46183195060320"/>Given the number of data technologies available, the <a href="https://oreil.ly/HgYlL">DoK Landscape</a> has been created to help compare and evaluate the various options. You can search by attributes such as open source versus commercial licensing, or whether an operator or Helm chart is available.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="DoK survey" data-type="indexterm" id="idm46183194790288"/>An annual <a href="https://oreil.ly/ZmaQu">DoK survey</a> is also conducted to gauge industry opinions and provide guidance on common problems. The report is free and can be used in your presentations.</p></li>&#13;
</ul>&#13;
<p>As a knowledge community, the best way to participate in the DoKC is sharing knowledge. When the community was being formed, the amount of information about end users running stateful workloads in Kubernetes was scarce. Creating a space to focus on data topics has led to a growing set of common interests and concepts. Most of the interviews in this book came from people we met in the DoKC.</p></dd>&#13;
</dl>&#13;
<p>Throughout the book, we’ve seen the benefits of contributions from each of these communities toward making data technologies run effectively on Kubernetes:</p>&#13;
<ul>&#13;
<li><p>The PersistentVolume subsystem we discussed in <a data-type="xref" href="ch02.html#managing_data_storage_on_kubernetes">Chapter 2</a> has provided a solid foundation for a wide variety of open source and commercial storage solutions on Kubernetes.</p></li>&#13;
<li><p>Operator frameworks we discussed in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>, including Operator SDK, Kubebuilder, and KUDO have proven to be a great enabler toward developing operators for a variety of data infrastructure from the ASF and other open source projects.</p></li>&#13;
<li><p>Kubernetes StatefulSets (first introduced in <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a>) are an interesting case. While they have proven quite valuable for managing distributed databases, the community has also identified some opportunities for improvement that we look forward to seeing addressed in the future.</p></li>&#13;
<li><p>Similarly, Spark and other projects in the analytics community have identified challenges with the Kubernetes default scheduler, as you learned in <a data-type="xref" href="ch09.html#data_analytics_on_kubernetes">Chapter 9</a>. Thankfully, Kubernetes provides APIs for extending the scheduler that projects like Apache YuniKorn and Volcano can leverage.</p></li>&#13;
</ul>&#13;
<p><a contenteditable="false" data-primary="" data-startref="cnd_com" data-type="indexterm" id="idm46183195525776"/><a contenteditable="false" data-primary="" data-startref="com_cnd" data-type="indexterm" id="idm46183194776896"/>As you can see, plenty of work remains to be done in this ecosystem of interconnected communities, and it will take contributions from all corners of the cloud native world to get us to the next stage of maturity as an industry. Remember, community participation isn’t limited to providing code to a project. One of the most important contributions to any community is sharing your story. Think about your experiences of learning new technologies, and you’ll likely recall good documentation, great examples, and the most valuable of all: “how we built this” stories. Please consider sharing your story any way you can. Your community needs you!</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Technology" data-type="sect2"><div class="sect2" id="technology">&#13;
<h2>Technology</h2>&#13;
<p><a contenteditable="false" data-primary="technology" data-secondary="about" data-type="indexterm" id="idm46183194775264"/>For many of you, this is the most exciting part. Cool toys! As you consider your journey to cloud native data, you’ll have important decisions in terms of the technologies you choose to use and the way you integrate them into your applications. You’ll recall from <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a> the critical guiding principles for deploying cloud native data in Kubernetes:</p>&#13;
<ul>&#13;
<li><p>Principle 1: Leverage compute, network, and storage as commodity APIs.</p></li>&#13;
<li><p>Principle 2: Separate the control and data planes.</p></li>&#13;
<li><p>Principle 3: Make observability easy.</p></li>&#13;
<li><p>Principle 4: Make the default configuration secure.</p></li>&#13;
<li><p>Principle 5: Prefer declarative configuration.</p></li>&#13;
</ul>&#13;
<p>As it turns out, these principles are useful for technology selection and integration, which you’ll see next.</p>&#13;
<section data-pdf-bookmark="Selecting cloud native data projects" data-type="sect3"><div class="sect3" id="selecting_cloud_native_data_projects">&#13;
<h3>Selecting cloud native data projects</h3>&#13;
<p><a contenteditable="false" data-primary="technology" data-secondary="selecting cloud native data projects" data-type="indexterm" id="idm46183195531824"/><a contenteditable="false" data-primary="cloud native data" data-secondary="selecting projects" data-type="indexterm" id="idm46183194768112"/>The years of building massive scale infrastructure, especially in data, have yielded an enormous supply of tooling to pick from, provided by various vendors and open source communities. For our examination here, we’ve made a deliberate choice to reason in terms of selecting projects instead of selecting technologies. Projects encapsulate the needed technology while integrating with the processes we need, created by the people who will drive the success. You’re here because you believe Kubernetes is one of these enabling projects, but how do you make your next set of choices? Here are some principles we recommend:</p>&#13;
<dl>&#13;
<dt>Ready for Kubernetes</dt>&#13;
<dd><p><a data-type="xref" href="ch07.html#the_kubernetes_native_database">Chapter 7</a>, outlined requirements for a Kubernetes native database, including:</p>&#13;
<ul>&#13;
<li><p>Maximum leverage of Kubernetes APIs</p></li>&#13;
<li><p>Automated, declarative management via operators</p></li>&#13;
<li><p>Observable through standard APIs (such as Prometheus)</p></li>&#13;
<li><p>Secure by default</p></li></ul></dd>&#13;
<dd>While not every project you use has to be Kubernetes native, the criterion for being Kubernetes-ready is a bit broader. At a minimum, projects you use should have an operator or Helm chart. The next level is a step toward the Kubernetes native idea of built-in awareness of Kubernetes for deeper integration. An <span class="keep-together">example</span> is Apache Spark, with the Kubernetes cluster deployment option that uses specialized containers. The highest level of maturity is populated by fully realized cloud native projects that can run only in Kubernetes because they depend on components in a Kubernetes cluster. An example of this type of project is KServe, which has no way of running outside of Kubernetes.</dd>&#13;
<dt>Open source</dt>&#13;
<dd><a contenteditable="false" data-primary="open source projects" data-type="indexterm" id="idm46183195606992"/>Using an open source project in the age of cloud native is about choice. You can deploy what you need, where you need it. If you choose to use a managed service based on an open source project, it should be completely compatible with the open source version, with no restrictions in moving back to a self-managed solution. Choosing the right license gives you the confidence to use a project and maintain your choice. We recommend projects with the Apache License 2.0 (APLv2). All ASF and CNCF projects use this license, so projects from either source guarantee you a permissive license. <a href="https://oreil.ly/61pjy">Many other licenses</a> offer differing levels of permissiveness and restrictions, and you should carefully consider how they will affect your deployment and requirements.</dd>&#13;
</dl>&#13;
<p>Of course, project selections aren’t something you can do in isolation. Upstream decisions influence each subsequent decision, and in turn can constrain what choices are available. This is why, in many cases, it makes sense to look at combinations of projects that work well together, either by deliberate design or by standard interfaces.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="New architectures for cloud native data" data-type="sect3"><div class="sect3" id="new_architectures_for_cloud_native_data">&#13;
<h3>New architectures for cloud native data</h3>&#13;
<p><a contenteditable="false" data-primary="architecture" data-secondary="for cloud native data" data-type="indexterm" id="arc_cnd"/><a contenteditable="false" data-primary="cloud native data" data-secondary="architectures for" data-type="indexterm" id="cnd_arc"/><a contenteditable="false" data-primary="technology" data-secondary="architectures for cloud native data" data-type="indexterm" id="tec_cnd"/>The future of cloud native data should focus less on new projects and more on new architectures. This means using the projects we have today in combinations that make the best use of each. As we’ve discussed previously, the software industry has a history of leveraging ideas from prior generations lasting a decade or more to innovate from a new point of view. In the cloud native world, the past decade has been spent building scale infrastructure, and the next 10 years will likely be about how we can combine these projects for our needs.</p>&#13;
<p>The infrastructure community has historically demonstrated a fondness for integrated infrastructure stacks that solve a common set of problems. One example is the LAMP stack popularized for web applications in the early 2000s, consisting of the Linux operating system, the Apache HTTP Server, MySQL, and either PHP, Perl, or Python, depending on who you asked. The 2010s brought us the SMACK stack for big data applications, with the Spark engine, Mesos as the resource manager, Akka, Cassandra, and Kafka.</p>&#13;
<p>While it’s tempting to describe such a stack for cloud native data, the reality is that the variety of use cases and available projects are simply too large to come up with a one-size-fits-all stack. Instead, let’s consider a candidate solution architecture for a simple weather application case, as shown in <a data-type="xref" href="#sample_architecture_for_a_weather_appli">Figure 11-1</a>. This architecture demonstrates the principles and recommendations discussed throughout the book, leveraging our data infrastructure categories of persistence, streaming, and analytics. This is a conceptual vision that we can discuss, critique, and improve as a community. Each choice we’ve made here has alternatives and should be considered a starting point for the sake of discussion.</p>&#13;
<figure><div class="figure" id="sample_architecture_for_a_weather_appli">&#13;
<img alt="Sample architecture for a weather application" src="assets/mcdk_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>Sample architecture for a weather application</h6>&#13;
</div></figure>&#13;
<p>Let’s walk through the flow of data to understand how this architecture satisfies the needs of a weather application with multiple data requirements. We’ll assume that the entire server-side infrastructure stack is contained in a single Kubernetes cluster. More advanced forms of this architecture could include multicluster deployments or inclusion of networking capabilities such as load balancing or Ingress. For now, this will serve to illustrate the data architecture.</p>&#13;
<p>Weather data is collected from weather stations and posted to a waiting API with an Ingress port into your running Kubernetes cluster. The business logic and server-side application code are containerized and run as microservices in the <code>application</code> Namespace. Client-side web and mobile applications also use the microservices via API calls, so all external data communications pass through the microservices layer.</p>&#13;
<p><a contenteditable="false" data-primary="CDC (change data capture)" data-type="indexterm" id="idm46183195514176"/>Real-time data is sent to Cassandra for immediate use in the <code>persistence</code> Namespace. Once the data is committed at the desired consistency level, change data capture (CDC) emits the fully committed data to a Pulsar topic in the <code>streaming</code> Namespace. A Pulsar sink exports the raw data into a Parquet file put in object storage. At the same time, a Flink consumer subscribed to the topic analyzes new data for user-defined limits such as high or low temperatures. If a boundary condition is triggered, the temperature and station data is sent back to the microservices, which will send push alerts to the user application.</p>&#13;
<p>In the <code>analytics</code> Namespace, two separate processes will use the Parquet data in object storage. Spark Jobs are used to group temperature averages across geographic data. This application code needs a wide view of the data stored for multiple locations and times. Ray applies analysis code written in Python to accomplish the predictive analysis of weather forecasting. The following five-day forecast is built daily by looking at recent data and applying against models built over historical trends. Both the Spark and Ray jobs populate new tables of fast transactional data in Cassandra.</p>&#13;
<p>This candidate architecture also demonstrates some recommendations that aren’t specific to a weather application that you should consider for all your deployments:</p>&#13;
<dl>&#13;
<dt>Use Namespaces to separate domains within applications</dt>&#13;
<dd><a contenteditable="false" data-primary="applications" data-secondary="separating domains within" data-type="indexterm" id="idm46183195275440"/><a contenteditable="false" data-primary="Namespaces" data-secondary="separating domains within applications using" data-type="indexterm" id="idm46183194895008"/>Deploying hundreds of Pods into a Kubernetes cluster can create organizational issues you won’t encounter with a small cluster on your laptop. Our recommendation here is simple: use Namespaces liberally to create order in your complex deployments. In the weather application example, we used simple Namespaces for each functional area of infrastructure: <code>application</code>, <code>persistence</code>, <code>streaming</code>, <code>analytics</code>, <code>security</code>, and <code>observability</code>. This approach will provide clear boundaries and naming when addressing services or managing Pods.</dd>&#13;
<dt>Automate certificate management</dt>&#13;
<dd><a contenteditable="false" data-primary="certificate management, automating" data-type="indexterm" id="idm46183194967600"/><a contenteditable="false" data-primary="automation" data-secondary="of certificate management" data-type="indexterm" id="idm46183194726704"/>In <a data-type="xref" href="ch08.html#streaming_data_on_kubernetes">Chapter 8</a>, we asserted that the best security solutions are the ones you don’t have to think about. Automating your certificate management with cert-manager is an excellent example of a solution that makes that a possibility. Use TLS for all inter-service communication. For Ingress routes, ensure all traffic is HTTPS. Both cases use ACME plug-ins to rotate and assign certificates and never suffer another outage due to an expired certificate. When a security audit comes around, you can check the box that says you enforce all policies and guidelines and that all network communication is adequately encrypted. Just do it.</dd>&#13;
<dt>Prefer object storage</dt>&#13;
<dd><a contenteditable="false" data-primary="object storage" data-type="indexterm" id="idm46183194724432"/>When choosing storage for the stateful services in your Kubernetes cluster, you should prefer object storage where possible. As discussed in <a data-type="xref" href="ch07.html#the_kubernetes_native_database">Chapter 7</a>, several reasons behind this recommendation will put you in a better place for deploying cloud native data. The primary one is the impact of immutability on separating storage from running processes. Block storage is generally tightly aligned with compute infrastructure and has a higher level of complexity. The tight coupling between compute and storage must be broken to build truly serverless data infrastructure. Object storage has proven to be a key enabler. You can choose to implement your own object storage inside Kubernetes or via a cloud service.</dd>&#13;
<dt>Standardize on Prometheus APIs for metrics</dt>&#13;
<dd><a contenteditable="false" data-primary="Prometheus API" data-type="indexterm" id="idm46183194758624"/>Observability is mandatory for the complex infrastructure being built and run in Kubernetes, and the Prometheus API is the most widely adopted for metrics. Ensure that all services expose metrics in Prometheus format and that you collect them in a single place. The Prometheus API is implemented on various backends such as VictoriaMetrics and InfluxDB, giving you options for managing your own Prometheus deployment or connecting to a cloud service. Finally, collecting metrics is only one part of the challenge, and using those metrics to build dashboards and alerting completes the package.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="" data-startref="arc_cnd" data-type="indexterm" id="idm46183195814464"/><a contenteditable="false" data-primary="" data-startref="cnd_arc" data-type="indexterm" id="idm46183195265712"/><a contenteditable="false" data-primary="" data-startref="tec_cnd" data-type="indexterm" id="idm46183194750448"/>As this architecture demonstrates, you can now deploy all of the infrastructure needed to support a complex application in a single deployment in Kubernetes. It’s a flexible architecture in which new components can be tried and rejected or replaced as your requirements change: are you using the database that best fits your application needs? Should data be analyzed in the stream or after it is at rest? Architecture represents a series of choices based on capabilities, limits, knowledge, and philosophy.</p>&#13;
<p>We look forward to future conversations and conference talks sharing the patterns that work and the antipatterns to avoid.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deploy services, not servers" data-type="sect3"><div class="sect3" id="deploy_servicescomma_not_servers">&#13;
<h3>Deploy services, not servers</h3>&#13;
<p><a contenteditable="false" data-primary="Services" data-secondary="deploying" data-type="indexterm" id="serv_dep"/><a contenteditable="false" data-primary="technology" data-secondary="deploying services" data-type="indexterm" id="tech_serv"/>One pattern we recommend is to start delivering capabilities at a higher level of abstraction: as services instead of servers. To help frame this discussion, think about the architectural design of a building. An architect must understand a structure’s requirements and then apply knowledge of materials and style to create a plan for builders to implement. When an architect considers where to place a door, it must be in a useful location, but one that will not weaken the overall structure. At no point do they specify minute details such as whether the door has to have brass hinges.</p>&#13;
<p>In the software industry, we’ve historically required a lot of minute details about individual compute, network, and storage resources well before we get to the deployment stage. For example, in the days of bare-metal infrastructure, the idea of installing a server was a significant event. Each server represented a physical device with a network connection that needed a whole bill of software, including operating system and applications to fill its role in the system. Procuring, configuring, and deploying a web server or database server was a process that could take months.</p>&#13;
<p>Along with the migration to cloud computing came the aphorism that we should treat servers as “cattle, not pets.” Despite this helpful emphasis, the care and feeding of individual servers persist in plenty of cases. Where a network server accepts requests and then responds with data, it still requires people installing these systems to get much further into the details needed for today’s cloud native applications. These details create friction.</p>&#13;
<p>Kubernetes has encouraged a lot of progress in this area, emphasizing managing fleets of both stateless and stateful services with Deployments and StatefulSets, instead of focusing on individual Pods. It’s time to take this kind of thinking to the next level, and Kubernetes gives us the tools to make it happen.</p>&#13;
<p>Consider how the architecture in the previous section can be described as a vertically integrated service—a weather service—consisting of an assembly of microservices and data infrastructure built from Kubernetes primitives for compute, network, and storage. Recalling the “virtual datacenter” concept from <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>, <a data-type="xref" href="#vertically_integrated_service">Figure 11-2</a> depicts the contents of a vertically integrated service that exposes a simple API. While such a service could encompass a wide range of business logic and infrastructure, that complexity is hidden behind a simple API.</p>&#13;
<figure><div class="figure" id="vertically_integrated_service">&#13;
<img alt="Vertically integrated Service" src="assets/mcdk_1102.png"/>&#13;
<h6><span class="label">Figure 11-2. </span>Vertically integrated Service</h6>&#13;
</div></figure>&#13;
<p>Returning to the example of the preceding weather Service, let’s examine how this represents a lot of power behind a deceptively simple API. When you zoom out, the collection of deployed infrastructure looks like a function machine. Rather than a simple microservice that merely gets and puts data records, this function machine takes multiple inputs and produces multiple outputs, as shown in <a data-type="xref" href="#weather_service_as_a_function_machine">Figure 11-3</a>.</p>&#13;
<figure><div class="figure" id="weather_service_as_a_function_machine">&#13;
<img alt="Weather Service as a function machine" src="assets/mcdk_1103.png"/>&#13;
<h6><span class="label">Figure 11-3. </span>Weather Service as a function machine</h6>&#13;
</div></figure>&#13;
<p>As a function machine, the weather Service takes a stream of temperature measurements and produces multiple outputs. Beyond the ability to retrieve the individual records originally inserted, it produces value-added information like statistics, alerts, and forecasts that help users make sense of the data and how it relates to them personally.</p>&#13;
<p>A single traditional server won’t service the variety required, which is why modern data infrastructure and architectures exist. It takes architectural work to assemble the right parts, connect them, and create new data from the single input value.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="serv_dep" data-type="indexterm" id="idm46183195653280"/><a contenteditable="false" data-primary="" data-startref="tech_serv" data-type="indexterm" id="idm46183194904096"/>Users and other applications expect service endpoints that respond to the data they need. What happens inside the function machine is left to the implementation meeting the API contract. When thinking shifts to outcomes, it’s clear how deploying services replaces the focus on deploying individual servers. Data services that can operate at various scales, built with resilience, using automation to keep us from worrying about minute details of the tools deployed. Using Kubernetes, you can specify what the function machine will do by using compute, network, and storage the same way you use any other consumable resource.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Process" data-type="sect2"><div class="sect2" id="process">&#13;
<h2>Process</h2>&#13;
<p><a contenteditable="false" data-primary="process" data-secondary="about" data-type="indexterm" id="idm46183194723536"/>Now that we have discussed the people and technology aspects of moving stateful workloads to Kubernetes, let’s look at the practical process steps required to successfully execute this transition. To be clear, <em>process</em> doesn’t mean more meetings or people involved in decision making. The dictionary defines a process as “a series of actions or steps to achieve a particular end.” For a cloud native deployment process, let’s append the word “automated,” and that’s the right spirit. The goal is to define and codify an automated process that enables you to deploy constantly with confidence. You’ll know you’ve succeeded when you have not only a complete cloud native application stack managed in Kubernetes, but also a repeatable set of steps to reproduce that stack.</p>&#13;
<p>Where are you in your cloud native journey? You can be at the starting line or somewhere further along. For either starting point, we recommend the stages shown in <a data-type="xref" href="#stages_of_moving_data_workloads_to_kube">Figure 11-4</a> for adopting cloud native data in Kubernetes.</p>&#13;
<figure><div class="figure" id="stages_of_moving_data_workloads_to_kube">&#13;
<img alt="Stages of moving data workloads to Kubernetes" src="assets/mcdk_1104.png"/>&#13;
<h6><span class="label">Figure 11-4. </span>Stages of moving data workloads to Kubernetes</h6>&#13;
</div></figure>&#13;
<p>Each stage contains core competencies developed by organizations that successfully made this transition. You’ll want to adopt and stabilize these competencies before moving on. Take your time and use the many resources available to become proficient in each stage. We’ll explore each stage in greater detail next.</p>&#13;
<section data-pdf-bookmark="DevOps practices" data-type="sect3"><div class="sect3" id="devops_practices">&#13;
<h3>DevOps practices</h3>&#13;
<p><a contenteditable="false" data-primary="DevOps" data-type="indexterm" id="idm46183196084048"/><a contenteditable="false" data-primary="process" data-secondary="DevOps practices" data-type="indexterm" id="idm46183195169504"/>Before you even begin the adoption of Kubernetes, you should completely embrace two areas of managing cloud native infrastructure:</p>&#13;
<dl>&#13;
<dt>Continuous integration/continuous delivery (CI/CD)</dt>&#13;
<dd><a contenteditable="false" data-primary="CI/CD" data-secondary="about" data-type="indexterm" id="idm46183194691616"/>DevOps teams have already widely used CI/CD for years. Correctly implemented, the outcome is a system that gives you the agility to make changes multiple times in a day with high confidence. For cloud native infrastructure, this has also been described as <a href="https://oreil.ly/p20Gt">GitOps</a>. Using source control as the starting point for infrastructure changes that a system like <a href="https://oreil.ly/fdj2s">Argo CD</a> will use to automate deployments. Made a mistake? Roll it back.</dd>&#13;
<dt>Observability</dt>&#13;
<dd><a contenteditable="false" data-primary="observability" data-secondary="DevOps and" data-type="indexterm" id="idm46183194687648"/>You may have heard the phrase “Trust but verify,” and nowhere is that more important than a highly complex cloud native deployment. You need to see what’s happening and make adjustments, especially when using CI/CD. In the process of building services that perform to meet SLAs, every step must be observed. This builds confidence that the changes you make are working; if not, you can roll back and try again. Every step is being watched and recorded.</dd>&#13;
</dl>&#13;
<p>While specific implementation details of your CI/CD and observability practices will inevitably change as you begin to adopt Kubernetes, having a firm foundation in these areas will set you up nicely for success.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Basic Kubernetes maturity" data-type="sect3"><div class="sect3" id="basic_kubernetes_maturity">&#13;
<h3>Basic Kubernetes maturity</h3>&#13;
<p><a contenteditable="false" data-primary="process" data-secondary="basic maturity" data-type="indexterm" id="idm46183194773680"/>If you are just starting with Kubernetes, this is a vital stage. Setting up a basic Kubernetes deployment on your laptop or cloud is an excellent way to learn, but your first production Kubernetes projects will stress the capability of your operations. It’s realistic to take several months in this phase to fully understand all of the potential issues and solutions:</p>&#13;
<dl>&#13;
<dt>Deploying and managing clusters</dt>&#13;
<dd><a contenteditable="false" data-primary="clusters, deploying and managing" data-type="indexterm" id="idm46183195062352"/>This is the most fundamentally important experience you can have. While there is great learning in building your own Kubernetes clusters and installing your own databases, as we noted in <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a>, you can make progress toward production capability more quickly by using a managed Kubernetes service or tools like Terraform that can help automate Kubernetes cluster deployments. You’ll get the most value from learning how to deploy and connect services both inside and outside the cluster, tasks that many new users find surprisingly tricky. You’ll also want to understand the metrics collected for various elements of a cluster and what they can tell you about performance and capacity.</dd>&#13;
<dt>Moving stateless workloads</dt>&#13;
<dd><a contenteditable="false" data-primary="stateless workloads, moving" data-type="indexterm" id="idm46183194883248"/>Once you are proficient at working with Kubernetes and understand some of the complexities, you can begin moving stateless workloads. The resource requirements for these workloads tend to be more straightforward to understand, and the body of prior art of deploying stateless workloads is deep. You’ll likely need to manage external networking to stateful workloads and data infrastructure that you aren’t yet moving during this stage. After a few successful migrations, you should feel comfortable with managing production workloads in Kubernetes and begin to see improvements in your operational tempo.</dd>&#13;
</dl>&#13;
<p>Here are a few competencies we recommend building as you start to move stateless workloads:</p>&#13;
<dl>&#13;
<dt>Leverage continuous delivery</dt>&#13;
<dd><a contenteditable="false" data-primary="CI/CD" data-secondary="leveraging" data-type="indexterm" id="idm46183195404624"/>Using <code>kubectl</code> on the command line is great for learning, but terrible for daily operations. Get used to managing groups of resources as services instead of individual Pods and let the Kubernetes Operators do the work of maintaining your systems.</dd>&#13;
<dt>Network routing and Ingress</dt>&#13;
<dd><a contenteditable="false" data-primary="Ingress" data-type="indexterm" id="idm46183194954848"/><a contenteditable="false" data-primary="networks" data-secondary="routing" data-type="indexterm" id="idm46183194928064"/>Bad things happen when you fight the way Kubernetes works, and one place that people new to Kubernetes fail is with network communications. You should prefer service names over IP addresses and understand how the LoadBalancer and Ingress APIs work.</dd>&#13;
<dt>Default security and observability</dt>&#13;
<dd><a contenteditable="false" data-primary="security, default" data-type="indexterm" id="idm46183194689792"/><a contenteditable="false" data-primary="observability" data-secondary="default" data-type="indexterm" id="idm46183194688624"/>Deployed services should default to a secure state and expose observability interfaces such as metrics endpoints with no manual configuration required. Ensure that every new service is deployed with network-level encryption. To manage systems effectively, SREs must have the metrics available to diagnose problems without gaps in coverage.</dd>&#13;
</dl>&#13;
<p>These competencies will serve you well as you move into the following stages.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deploy stateful workloads" data-type="sect3"><div class="sect3" id="deploy_stateful_workloads">&#13;
<h3>Deploy stateful workloads</h3>&#13;
<p><a contenteditable="false" data-primary="process" data-secondary="deploying stateful workloads" data-type="indexterm" id="idm46183194839936"/><a contenteditable="false" data-primary="stateful workloads, deploying" data-type="indexterm" id="idm46183194651472"/>The next stage is to migrate stateful workloads to Kubernetes, including their supporting data infrastructure. In this case, we recommend a phased approach in roughly the following order:</p>&#13;
<dl>&#13;
<dt>Persistence</dt>&#13;
<dd><a contenteditable="false" data-primary="persistence" data-type="indexterm" id="idm46183194751664"/>We recommend migrating databases as your first stateful workloads. Databases have been running in Kubernetes for far longer than other stateful workloads, with a higher level of maturity and documentation. Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#automating_database_deployment_on_kuber">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#automating_database_management_on_kuber">5</a> provide guidance on deploying with Helm and operators, respectively. Start with your development environment and parallel the same production traffic loads outside Kubernetes. Get proficient at backups and restore operations. Make sure your test cases include the loss of database compute and storage resources and move into staging and production when you feel your recovery response is sufficient.</dd>&#13;
<dt>Streaming</dt>&#13;
<dd><a contenteditable="false" data-primary="streaming" data-secondary="stateful workloads and" data-type="indexterm" id="idm46183194644416"/>The Kubernetes readiness for streaming workloads is becoming much more mature, but we still recommend you migrate these workloads after persistence workloads. As we discussed in <a data-type="xref" href="ch08.html#streaming_data_on_kubernetes">Chapter 8</a>, streaming workloads have some unique properties that can make them easier for migrations: most use cases don’t need long-term message storage, so switching from one streaming service to the other typically doesn’t require data migration. Since streaming is network intensive, proficiency with Kubernetes networking is a must.</dd>&#13;
<dt>Analytics</dt>&#13;
<dd><a contenteditable="false" data-primary="analytics" data-secondary="stateful workloads and" data-type="indexterm" id="idm46183195166912"/>The complex nature of analytic workloads makes them the next logical choice for migration into Kubernetes after persistence and streaming are in place. A good starting approach is to deploy analytic workloads into a dedicated Kubernetes cluster so that you can learn the Kubernetes deployment modes and special considerations for job management and data access. Ultimately you should consider using a different scheduler to support batch workloads such as YuniKorn or Volcano, as we discussed in <a data-type="xref" href="ch09.html#data_analytics_on_kubernetes">Chapter 9</a>.</dd>&#13;
<dt>AI/ML workloads</dt>&#13;
<dd><a contenteditable="false" data-primary="AI/ML (artificial intelligence/machine learning)" data-secondary="stateful workloads and" data-type="indexterm" id="idm46183194669872"/>You may consider your AI/ML workloads for extra migration bonus points. As we discussed in <a data-type="xref" href="ch10.html#machine_learning_and_other_emerging_use">Chapter 10</a>, this is one of the least mature areas organizations have concerning data infrastructure. Projects like KServe and Feast are well suited for Kubernetes, so this isn’t the concern. The real question is whether your organization is proficient in MLOps and data engineering. You may be ready, but as a general recommendation for most organizations, this is an area you should address after other analytic workloads.</dd>&#13;
</dl>&#13;
<p>The details of your specific adoption plan will vary according to your Kubernetes readiness, the maturity of each workload, and the underlying data infrastructure on which it is built. The Kubernetes native definitions in <a data-type="xref" href="ch07.html#the_kubernetes_native_database">Chapter 7</a> provide a valuable way of assessing the readiness of your infrastructure and where you may encounter additional work to properly deploy and manage it on Kubernetes.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Continually optimize your deployments" data-type="sect3"><div class="sect3" id="continually_optimize_your_deployments">&#13;
<h3>Continually optimize your deployments</h3>&#13;
<p><a contenteditable="false" data-primary="process" data-secondary="continually optimizing deployments" data-type="indexterm" id="idm46183194720752"/><a contenteditable="false" data-primary="cost, optimizing" data-type="indexterm" id="idm46183194630048"/><a contenteditable="false" data-primary="COGS (cost of goods sold)" data-type="indexterm" id="idm46183194629072"/>In the early days of the internet explosion, known as the “dot-com” years, startups were vying for venture capital and presenting plans. Almost every pitch deck would include a slide showing the planned datacenter build-out. It was there for a good reason: datacenters were a significant capital cost, and when asking for money, that had to be in the budget.</p>&#13;
<p>Things are different today. Startups now rent what they need from a cloud provider, and larger enterprises that still manage datacenters are reducing their footprints quickly. In this cloud native world, we have a lot more flexibility over the infrastructure we use, which gives greater opportunity for managing elements like cost, quality, and the trade-offs between them:</p>&#13;
<dl>&#13;
<dt>Optimizing cost</dt>&#13;
<dd><p>In any business, you have things that add to the ledger and subtract. People in finance call that the <em>cost of goods sold</em> (<em>COGS</em>). If you are building cars, COGS may account for costs like steel, the factory, and labor. Selling cars covers <span class="keep-together">COGS and</span> brings profit to the company. Controlling costs and making them predictable is a way to create a sustainable business.</p>&#13;
<p>In application software technology today, there are four main components of COGS: human labor, compute, network, and storage. These metrics have been tracked for a long time, and a lot of progress has been made to reduce costs and make things more predictable. DevOps has reduced the amount of human interaction needed, and cloud has normalized infrastructure costs. As mentioned in <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>, Kubernetes wasn’t a revolution. It was an evolution and a place to converge to help solve the problem of COGS with application software technology, a solution that doesn’t compromise on quality and creates predictability.</p>&#13;
<p><a contenteditable="false" data-primary="HorizontalPodAutoscaler" data-type="indexterm" id="idm46183194985024"/>Elasticity is one aspect of cloud native data that can lead to significant cost savings. If services are initially deployed with fixed capacity, optimize your deployments with the ability to not only scale up but also scale down when needed. When possible, use automation such as <a href="https://oreil.ly/AoiRm">HorizontalPodAutoscaler</a> for hands-off scaling with the added benefit of scaling under load to maintain performance. Choosing projects that can support elastic workload management is the most crucial way to be sure you are getting the best performance for the cost.</p></dd>&#13;
<dt>Optimizing quality (availability and performance)</dt>&#13;
<dd><p><a contenteditable="false" data-primary="quality, optimizing" data-type="indexterm" id="idm46183194875632"/>Reducing human toil reduces the number of people needed to run your Kubernetes deployments. Automated deployments and sane defaults go a long way to reducing labor, but self-healing infrastructure will reduce the number of people that need to be on hand for when things go bad. Optimize your self-healing deployments by testing the recovery of services by injecting failures into your cluster. Kill a Pod or a StatefulSet. What happens to the surrounding services? If that scenario makes you nervous, you need to optimize your Deployment until you are comfortable with failures.</p>&#13;
<p>Reducing costs should never be optimized by sacrificing quality. Continuously optimize for price and performance. As you constantly look to optimize your Kubernetes deployments, you should ask yourself these questions:</p>&#13;
<ul>&#13;
<li><p>Are you maintaining SLAs?</p></li>&#13;
<li><p>Is the need for human interaction reduced?</p></li>&#13;
<li><p>Can you scale to zero with no traffic?</p></li>&#13;
</ul></dd>&#13;
</dl>&#13;
<p>Given the current trends in operations, <em>AIOps</em> is a term that will soon enter your vocabulary, if it hasn’t already. AIOps doesn’t mean operations for AI/ML workloads; it refers to the use of AI/ML to manage infrastructure intelligently. With a strong baseline of observability, the metrics and other information you’re collecting can be analyzed and used to generate recommended adjustments to your infrastructure. Automated scaling up and scaling down of Deployments and StatefulSets is just the beginning. We hope to soon see advanced AIOps capabilities, for example:</p>&#13;
<ul>&#13;
<li><p>A system that detects increased usage of a vertically integrated service in a given region and responds by deploying microservices and supporting infrastructure into that region, and proactively replicating data to optimize latency for client applications.</p></li>&#13;
<li><p>A multitenant system that detects when a particular tenant is demonstrating increased usage and migrates traffic for that tenant to dedicated infrastructure.</p></li>&#13;
</ul>&#13;
<p><a contenteditable="false" data-primary="" data-startref="ppt_ab" data-type="indexterm" id="idm46183194653680"/>These are just a couple of examples of what we might be able to achieve. We already have the foundations in the controller-reconciler pattern implemented by the Kubernetes control plane. Today’s operators are heavily procedural, but what kind of decision flexibility could we build into future operators in order to achieve a desired quality of service? Stay tuned, because the cloud native world is constantly evolving.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Future of Cloud Native Data" data-type="sect1"><div class="sect1" id="the_future_of_cloud_native_data">&#13;
<h1>The Future of Cloud Native Data</h1>&#13;
<p><a contenteditable="false" data-primary="cloud native data" data-secondary="future of" data-type="indexterm" id="cnd_fut"/>Over the course of a career in information technology, you’re likely to see several generational shifts. A subtle evolution occurs over five- or ten-year periods as changes slowly build on a previous generation of technology, until the day you realize that the way you work is fundamentally new. Perhaps you’ve spent part of your career installing operating systems on physical servers. In a more recent generation, we’ve started using scripts to provision cloud instances with operating system images ready for software to be installed. Kubernetes represents the latest generation, where engineers define everything they need in a text file, and the control plane converges the state while performing all of the tasks every previous generation of engineers had to do manually.</p>&#13;
<p>What kind of progress will we continue to see from generation to generation? The following is a fictional story about a very possible near future. This story provides an example of where we could go as a community of data infrastructure engineers. The changes will be subtle but profound.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="a_vision_of_a_not_too_distant_future">&#13;
<h5>A Vision of a Not-Too-Distant Future</h5>&#13;
<dl>&#13;
<dt>The kickoff meeting</dt>&#13;
<dd><p>I arrive Monday morning in the office and see an email invitation to a meeting from one of our product managers. Our company has been growing at a fast pace, and to stay ahead of our competition, we constantly release new products. The meeting will be a kickoff with everyone from user experience to the backend infrastructure. Somebody dreamed up a new app, so here we go.</p>&#13;
<p>Product management shares a one-pager with an ambitious, groundbreaking idea requiring everyone to make it work. The room is divided into groups of people you see in any product build: user experience and interface design, <span class="keep-together">microservice</span> developers, data engineers, and my team, responsible for the backend infrastructure. We collaborate closely across teams to move quickly and keep our customers happy.</p>&#13;
<p>In the meeting, it’s clear that nobody is sure of how big of an idea this is and thus we have no idea of the needed capacity. While we used to expend a lot of energy on infrastructure planning and wasted a lot of money by provisioning too early, we don’t have to do this anymore because our systems will adapt to what’s needed. Things happen, and plans change. If we’re lucky, an idea will take off like a rocket and then we’ll scale to a global user base. Our job is to build the right product with quality and be ready for whatever comes.</p></dd>&#13;
<dt>Confidence booster</dt>&#13;
<dd><p>One aspect of my job I love is getting requirements and designing the right infrastructure to do the job. I’m inspired by the building architect I. M. Pei, who worked with clients to translate their vision into a functional reality, and one of his famous quotes: “Success is a collection of problems solved.” He maintained the beauty and elegance of the original requirements and built things with a purpose, like the Grand Louvre and Kennedy Library. I’m not saying that I’m the I. M. Pei of data infrastructure, but his example is inspiring.</p>&#13;
<p>The first task I have is reasoning through each piece of the data infrastructure like a puzzle. It reminds me of the LEGO projects I created as a kid. I’ll need a square piece, a rectangular piece, a specialty piece with a hinge, and that round dot that always seems to get lost. I help the development teams identify the data infrastructure components that will support this particular application: there’s a mix of transactional data managed by various microservices, and we put together the right combination of databases, object storage, transformations, and streams to store, enhance and move data where it’s needed. The data engineers highlight the data sets they will need to analyze business results and produce intelligent recommendations that will make the application work better for our users.</p>&#13;
<p>The initial architecture decisions aren’t final; in most cases, the infrastructure we’re building will automatically evolve. The basic parameters are defined by the application’s needs according to its API contract. I set boundaries around costs and location and let Kubernetes and my operators figure out the rest intelligently. The system makes adjustments by analyzing usage to lower latency while remaining at or below budget. The solution I want is the one that gives me the lowest cost while still meeting our SLAs.</p></dd>&#13;
<dt>No plans survive contact with the enemy</dt>&#13;
<dd><p>We decide to roll out the new application with an initial pilot in North America. We deploy a Kubernetes cluster and install the application and supporting data infrastructure using automated scripting we’ve built up in previous projects and then run a few automated performance and compliance tests. The CI/CD pipeline reduces the time to deploy fixes for a couple of minor defects we found in testing to just a few minutes. Since we’ve automated the data collection and observability tools, it takes only a couple of hours for the quality and security teams to sign off on the application, and we are live!</p>&#13;
<p>At the product launch, things are looking great. From the first meeting to the final product approval, our time span was a few days, and everything is working as designed. The user experience and development engineers weren’t waiting on us to provision infrastructure, so they could spend their time focusing on what made the product a delight for our customers. The application is working as designed, and we are continuing to refine and improve. I’m sure we’ll get the go-ahead to start expanding into new regions soon.</p>&#13;
<p>On my way to work, I see on the news that storms threaten to shut down large parts of the eastern United States. I check my alerts and see the notifications from our service providers: “Due to the worsening storm conditions, the following locations will be flagged for evacuations…” They aren’t just talking about human evacuations; these are infrastructure evacuations. Since our applications can route around datacenter failures, our providers don’t have to put people in harm’s way. Heroic efforts to fulfill fuel contracts and keep the generators running aren’t needed now.</p>&#13;
<p>Years ago, this might have caused a Tier 1 emergency, but now our systems are resilient against both natural and human-error disasters. When providers post an evacuation warning, our intelligent operators ingest the new parameters and start reconciling a solution: finding new capacity, negotiating the price, and shutting down the affected areas. Even without a warning, like when human error is involved, our application will still be online. Intelligent geographical redundancy insulates us from localized problems, like somebody accidentally cutting a network or power cable. I feel bad for the people who are stuck maintaining legacy applications. Those systems have no way of managing an emergency, and it’s left to the humans to figure it out. Somebody is going to have a late night, and I’m thankful it isn’t me.</p></dd>&#13;
<dt>It’s a hit!</dt>&#13;
<dd><p>A few days later, I get an email about how our new application has taken off in Europe. Our management has questions about our infrastructure capacity and potential effects on our SLA. We want to provide a good user experience, no matter how users find us or where they are. I reply with confidence that we have things covered because our infrastructure has already detected the new usage pattern and anticipated the needed changes. There’s not much for me to do other than verify that our SLAs are being met and watch it work.</p>&#13;
<p>When I examine the updated deployment, I see adjustments to my initial architecture. In addition to the expansion in storage capacity and compute processing I expected to see, I notice that the faster and more expensive streaming pipelines I started with have been changed out for slower but more cost-effective rollups in batch analytics. This is the result of continuous analysis of the traffic patterns by our intelligent operators in response to observing user interaction. I can see that the application’s response rate will be improved without the need to have the streaming pipeline results immediately. This provides a better user experience for a smaller cost, with no application code changes required.</p>&#13;
<p>Shortly after, our data engineering team reaches out. After analyzing the application usage patterns, they’ve identified some improvements to the recommendation engine the application uses. Working together, I add a new stream to push more operational data to an analytic store while they add analytic jobs to generate new feature data. The microservice developers do some quick A/B testing with the new recommendation data to verify that customers can make decisions more quickly, so we roll the changes out to the entire fleet. Our PR team shares an unsolicited article about how people are starting to notice the application. Is this our viral moment? We’re not sure yet, but the management team is definitely excited.</p></dd>&#13;
<dt>No worries</dt>&#13;
<dd><p>Weeks later, I meet with the product manager to discuss changes for the next product rollout. They are delighted that things worked so seamlessly. I remind them that this is how people build cloud native applications these days. We don’t deploy infrastructure; we declare it. My job isn’t dominated by editing configuration files and spending time in a terminal. I spend my time listening to teams and working with them on what they need. Like I. M. Pei, I enjoy the creative process of defining elegant architectures. My focus is to design something that makes it easy for developers to be productive on the first day while giving our end users an amazing experience. With a modern Kubernetes native approach, I define what I need and worry a lot less about the how. Because most of what’s deployed is open source, I can even take time to fix little things and contribute back to a project.</p>&#13;
<p>The area I never worry about is having to deploy something. I never have to ask if it will work or if there are a lot of trade-offs. I define what the application needs, and it will emerge. Having development timelines of days or hours instead of months is a great place to be.</p></dd>&#13;
</dl>&#13;
</div></aside>&#13;
<p><a contenteditable="false" data-primary="" data-startref="dw_mig" data-type="indexterm" id="idm46183195196832"/><a contenteditable="false" data-primary="" data-startref="kub_mig" data-type="indexterm" id="idm46183195888512"/><a contenteditable="false" data-primary="" data-startref="cnd_fut" data-type="indexterm" id="idm46183194862576"/>This story aims to help you look beyond the drudgery of configuration files and the shiny distraction of hot new projects and focus on how embracing cloud native data opens the door for a more fantastic tomorrow. When the toil of infrastructure is reduced or even removed, think of the new abilities we have and how this could translate into tangible daily outcomes. This isn’t science fiction, and you don’t have to wait for the next generational breakthrough. All of this is feasible today with the correct application of existing technology.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000010">&#13;
<h1>Summary</h1>&#13;
<p>You made it! We have taken quite a journey together and covered a lot of ground, not only in this chapter but in the entire book. At the outset, we presented an ambitious goal of putting stateful workloads on Kubernetes. As we learned from Craig McLuckie, this is very much in line with the original goals of the Kubernetes project. Ultimately, we will reverse the trend of infrastructure-aware applications and have application-aware platforms and building applications with speed, efficiency, and confidence.</p>&#13;
<p>Hopefully, we’ve convinced you that this is achievable technically and extremely compelling from a cost and quality standpoint. In this chapter, we’ve focused on helping you chart the course to make this transition by focusing on the people, process, and technology changes you’ll need to make to be successful:</p>&#13;
<ul>&#13;
<li><p>Help people in your organization skill up on Kubernetes and data technologies, including those we’ve covered here. If you are in leadership, help place people in roles that give them direct responsibility and accountability for infrastructure choices. Empower them to interact and contribute in open source communities and be the catalyst for change in your organization.</p></li>&#13;
<li><p>Select data infrastructure technologies that embody cloud native and Kubernetes native principles. Use Kubernetes custom resources and operators to raise the level of abstraction in your architecture to begin thinking about managing services that implement well-defined APIs instead of managing individual servers.</p></li>&#13;
<li><p>Update your processes to automate “all the things”—from integration and delivery (CI/CD) to observability and management (AIOps). Leverage these mature processes as you strategically migrate stateful workloads to Kubernetes. Carefully balance the trade-offs between cost and quality to sustainably deliver the best experiences for your end users.</p></li>&#13;
</ul>&#13;
<p>Now, the narrative of this journey shifts to you and where you choose to take us next. While this book has provided a broad overview of the world of data infrastructure on Kubernetes, each chapter could easily fill a book on its own. We encourage you to continue learning where your specific interests take you and share what you learn to continue to fill the gaps in our collective knowledge. As you successfully manage your cloud native data on Kubernetes, we hope to hear your story.</p>&#13;
</div></section>&#13;
</div></section></body></html>