<html><head></head><body><section data-pdf-bookmark="Chapter 5. Continuous Delivery Across Clusters" data-type="chapter" epub:type="chapter"><div class="chapter" id="continuous_delivery_across_clusters">&#13;
<h1><span class="label">Chapter 5. </span>Continuous Delivery Across Clusters</h1>&#13;
<p>Cloud native applications have the potential to disrupt entire industries. A key <a contenteditable="false" data-primary="cloud native applications" data-secondary="continuous delivery support by" data-type="indexterm" id="idm45358200245320"/><a contenteditable="false" data-primary="applications" data-secondary="continuous delivery of cloud native" data-type="indexterm" id="idm45358200243864"/><a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="about" data-type="indexterm" id="idm45358200242472"/><a contenteditable="false" data-primary="container images" data-secondary="about continuous delivery" data-type="indexterm" id="idm45358200241080"/><a contenteditable="false" data-primary="production" data-secondary="continuous delivery" data-seealso="continuous delivery" data-type="indexterm" id="idm45358200239688"/><a contenteditable="false" data-primary="deployments" data-secondary="continuous delivery" data-seealso="continuous delivery" data-type="indexterm" id="idm45358200238040"/>reason for this is their ability to support continuous delivery. When the market environment changes and applications need to be updated to address real-world constraints that pop up quickly, continuous delivery enables applications to quickly adapt to meet these newly encountered issues. Here is a brief overview of how container images, Kubernetes, and OpenShift support DevOps principles to facilitate continuous <span class="keep-together">delivery</span>:</p>&#13;
<dl>&#13;
<dt>Small batch changes</dt>&#13;
<dd>All changes should be incremental and finite. When failures occur, small batch <a contenteditable="false" data-primary="failure" data-secondary="small batch changes and" data-type="indexterm" id="idm45358200233544"/>changes are typically easier to recover than large, disruptive changes.</dd>&#13;
<dt>Source control all the things</dt>&#13;
<dd>A history of all changes is helpful to understand the changes that have been made and to identify the causes of regressions in the code base or configuration.</dd>&#13;
<dt>Production-like environments</dt>&#13;
<dd>Developers should have access to environments and tools that are <a contenteditable="false" data-primary="production" data-secondary="development reflecting production environments" data-type="indexterm" id="idm45358200230024"/><a contenteditable="false" data-primary="development reflecting production environments" data-type="indexterm" id="idm45358200228536"/><a contenteditable="false" data-primary="quality assurance (QA)" data-secondary="production environments reflected" data-type="indexterm" id="idm45358200227400"/>representative of production. Production environments typically operate at larger scales than development or quality assurance (QA) and with more complex configuration. The variance can mean that features that work fine in the early stages do not work correctly in production, which is the only place they matter.</dd>&#13;
<dt>Shift-left of operational practices</dt>&#13;
<dd>We should expose behaviors for health management, log collection, change management, and so on earlier in the development process.</dd>&#13;
<dt>Continuous integration of changes</dt>&#13;
<dd>All changes should be built and deployed together on an ongoing basis to identify when the intermingling of various changes lead to an unforeseen issue or API incompatibility.</dd>&#13;
<dt>Highly automated testing with continuous feedback</dt>&#13;
<dd>To manage velocity, we have to automate our testing and validation work<a contenteditable="false" data-primary="always be testing (ABT)" data-type="indexterm" id="idm45358200222616"/> so that we can always be testing (ABT).</dd>&#13;
</dl>&#13;
<p>In this chapter, we provide an overview of a variety of tools and methodologies for supporting continuous delivery in production across clusters. We begin with a discussion of Helm, which is a popular packaging tool for Kubernetes applications. Next we introduce Kustomize, which provides the ability to repurpose your existing Kubernetes YAML files for multiple purposes and configurations while avoiding the use of templates. We then describe several popular approaches for supporting continuous delivery pipelines, including GitOps, Razee, and Tekton. Finally, we conclude this chapter with an extensive discussion of OpenShift pipelines and the Open Cluster Management project for deploying applications across multiple clusters.</p>&#13;
<section data-pdf-bookmark="Helm" data-type="sect1"><div class="sect1" id="helm">&#13;
<h1>Helm</h1>&#13;
<p>Helm is the Kubernetes package manager. It enables you to create a <a contenteditable="false" data-primary="Helm package manager" data-type="indexterm" id="idm45358200218104"/><a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="Helm package manager" data-type="indexterm" id="idm45358200216920"/><a contenteditable="false" data-primary="package management by Helm" data-type="indexterm" id="idm45358200215528"/><a contenteditable="false" data-primary="applications" data-secondary="Helm package manager" data-type="indexterm" id="idm45358200214408"/>package containing multiple templates for all of the resources that make up your application. Common Kubernetes resources you would expect to find included in a Helm package are <code>ConfigMaps</code>, deployments, <code>PersistentVolumeClaims</code>, services, and many others.</p>&#13;
<p>Helm provides its own <a href="https://oreil.ly/9S6Ne">CLI</a> that<a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Helm CLI" data-type="indexterm" id="idm45358200210712"/><a contenteditable="false" data-primary="templates" data-secondary="Helm" data-type="indexterm" id="idm45358200209240"/> will generate a collection of template files, and it also supports passing variable values into these template files. <a contenteditable="false" data-primary="Helm package manager" data-secondary="Helm charts" data-type="indexterm" id="idm45358200207608"/>The collection of template files that Helm generates is called a <em>Helm chart</em>. The CLI will then create complete YAML files from the template files, package up all the related files, and deploy the chart.</p>&#13;
<p>The <code>helm create</code> command is used to create a new package<a contenteditable="false" data-primary="Helm package manager" data-secondary="helm create command" data-type="indexterm" id="idm45358200204600"/><a contenteditable="false" data-primary="YAML files" data-secondary="Helm package manager" data-type="indexterm" id="idm45358200203224"/> by passing in the name of the chart. So to create a new chart called <code>firstchart</code> we do the following:</p>&#13;
<pre data-type="programlisting">$ <strong>helm create firstchart</strong></pre>&#13;
<p>This command creates the following files and subfolders:</p>&#13;
<ul>&#13;
<li><p><em>Chart.yaml</em></p></li>&#13;
<li><p><em>charts/</em></p></li>&#13;
<li><p><em>templates/</em></p></li>&#13;
<li><p><em>values.yaml</em></p></li>&#13;
</ul>&#13;
<p>The <em>templates</em> folder contains some generated templates for representing key Kubernetes abstractions like deployments and service accounts. These template files are:</p>&#13;
<ul>&#13;
<li><p><em>NOTES.txt</em></p></li>&#13;
<li><p><em>_helpers.tpl</em></p></li>&#13;
<li><p><em>deployment.yaml</em></p></li>&#13;
<li><p><em>hpa.yaml</em></p></li>&#13;
<li><p><em>ingress.yaml</em></p></li>&#13;
<li><p><em>Service.yaml</em></p></li>&#13;
<li><p><em>serviceaccount.yaml</em></p></li>&#13;
<li><p><em>tests/</em></p></li>&#13;
</ul>&#13;
<p>The <em>_helpers.tpl</em> file contains partial templates that are used to generate values such as Kubernetes selector labels and the name to use for the service account. Template files such as <em>deployment.yaml</em> and <em>serviceaccount.yaml</em> then use the partial templates to obtain these values. The template files also pull values from the <em>values.yaml</em> file. This file contains values such as <code>replicaCount</code> that are variables the user can change. These variables are also passed into template files such as <em>deployment.yaml</em> and <em>serviceaccount.yaml</em> and used to set desired values.</p>&#13;
<p>Once you have the proper set of template files and proper variables set for your application, it’s time to package up all the contents associated with this Helm chart. <a contenteditable="false" data-primary="Helm package manager" data-secondary="helm package command" data-type="indexterm" id="idm45358200184424"/>This is accomplished by using the <code>helm package</code> command. In our example, we would call the <code>helm package</code> as follows:</p>&#13;
<pre data-type="programlisting">$ <strong>helm package firstchart</strong>&#13;
 &#13;
Successfully packaged chart and saved it to:&#13;
/Users/bradtopol/go/k8s.io/helmexample/firstchart-0.1.0.tgz</pre>&#13;
<p>As shown in the example, running the <code>helm package</code> command generates an archive file containing all the content associated with the Helm chart. Now, if we want to install the Helm chart, we just need to run the <code>helm install</code> command as follows:</p>&#13;
<pre data-type="programlisting">$ <strong>helm install firstchart_release1 firstchart-0.1.0.tgz</strong></pre>&#13;
<p>The <code>helm install</code> command in its most basic form takes two<a contenteditable="false" data-primary="Helm package manager" data-secondary="helm install command" data-type="indexterm" id="idm45358200176984"/> arguments, the name of the release and the archive file. The <code>helm install</code> command is quite flexible and can install a Helm chart and its content in a variety of different ways. <a contenteditable="false" data-primary="Helm package manager" data-secondary="helm update command" data-type="indexterm" id="idm45358200174856"/>In addition, the Helm release that has been installed can be easily updated using the <code>helm update</code> command. <a contenteditable="false" data-primary="Helm package manager" data-secondary="documentation online" data-type="indexterm" id="idm45358200172936"/>For more information on the use of Helm and its capabilities, please see the <a href="https://helm.sh/docs">Helm documentation</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Kustomize" data-type="sect1"><div class="sect1" id="kustomize">&#13;
<h1>Kustomize</h1>&#13;
<p>When running Kubernetes in production, you will invariably have lots of YAML files <a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="Kustomize for configurations" data-type="indexterm" id="ch05-Kust"/><a contenteditable="false" data-primary="Kustomize for configurations" data-type="indexterm" id="ch05-Kust2"/><a contenteditable="false" data-primary="configuration customization with Kustomize" data-type="indexterm" id="ch05-Kust3"/><a contenteditable="false" data-primary="YAML files" data-secondary="Kustomize for configuration files" data-type="indexterm" id="ch05-Kust4"/><a contenteditable="false" data-primary="kubectl command-line tool" data-secondary="Kustomize for configurations" data-type="indexterm" id="ch05-Kust5"/>used for deploying and configuring applications. Moreover, you would typically store these configuration files in some form of source control to enable continuous delivery. Experience with running in production has shown that in many cases the configuration YAML files will need customizations for a variety of reasons. For example, there may be configuration changes for differences between development and production environments. Or custom prefix names may be needed to distinguish similar applications. To address these needs, the Kubernetes <code>kubectl</code> CLI has incorporated a recently developed tool called <a href="https://kustomize.io">Kustomize</a>. The Kustomize tool provides a template-free approach to customizing Kubernetes configurations.<sup><a data-type="noteref" href="ch05.html#ch01fn33" id="ch01fn33-marker">1</a></sup> The Kustomize tool has several key features that both reduce the complexity of managing a large number of Kubernetes configurations for production deployments and facilitate the use of continuous deployment methodologies. The primary capabilities provided by Kustomize include generators, composition, patches, and overlays.</p>&#13;
<section data-pdf-bookmark="Generators" data-type="sect2"><div class="sect2" id="generators">&#13;
<h2>Generators</h2>&#13;
<p>Best practices for Kubernetes deployment recommend that configuration<a contenteditable="false" data-primary="Kustomize for configurations" data-secondary="generators" data-type="indexterm" id="idm45358200154424"/><a contenteditable="false" data-primary="generators for secrets and ConfigMaps" data-type="indexterm" id="idm45358200153000"/><a contenteditable="false" data-primary="secrets" data-secondary="generators for" data-type="indexterm" id="idm45358200151928"/><a contenteditable="false" data-primary="ConfigMap generators" data-type="indexterm" id="idm45358200150552"/><a contenteditable="false" data-primary="YAML files" data-secondary="Kustomize for configuration files" data-tertiary="generators" data-type="indexterm" id="idm45358200149448"/> and secret data should not be hardcoded into an application. Instead, this information should be stored in Kubernetes resources like <code>ConfigMaps</code> and secrets. Kustomize provides the notion of <em>generators</em> that can create <code>ConfigMaps</code> and secrets to simplify this aspect of deployment. Kustomize uses YAML files to describe what should be generated. The following Kustomize YAML is used to generate a Kubernetes secret:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">secretGenerator</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">sample-secret</code>&#13;
<code class="-Error"> </code><code class="nt">literals</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">username=admin</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">password=letbradin</code></pre>&#13;
<p>To run the previous example, save the file as <em>kustomization.yaml</em> in a new folder called <em>generateSecret</em>. You can now run it by doing the following:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl kustomize ./generateSecret</strong></pre>&#13;
<p>After you run the above command, Kustomize will generate a Kubernetes secret resource as shown:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
data:&#13;
 password: bGV0YnJhZGlu&#13;
 username: YWRtaW4=&#13;
kind: Secret&#13;
metadata:&#13;
 name: sample-secret-dh2bm897df&#13;
type: Opaque</pre>&#13;
<p>As an alternative, Kustomize can generate a secret where the username and password are stored in a separate file. For example, create a file called <em>password.txt</em> and store the following key value pairs in the file:</p>&#13;
<pre data-type="programlisting">username=admin&#13;
password=letbradin</pre>&#13;
<p>With these values now in <em>password.txt</em>, you can create a Kustomization file that generates a secret and pulls the username and password from <em>password.txt</em> as shown:</p>&#13;
<pre data-type="programlisting">secretGenerator:&#13;
- name: sample-secret2&#13;
 files:&#13;
 - password.txt</pre>&#13;
<p>To run the previous example, save the file as <em>kustomization.yaml</em> in a new folder called <em>generateSecret2</em>. You can now run Kustomize using this file by doing the <span class="keep-together">following</span>:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl kustomize ./generateSecret2</strong></pre>&#13;
<p>After you run this command, Kustomize will generate a Kubernetes secret resource similar to the one shown:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
data:&#13;
 password.txt: CnVzZXJuYW1lPWFkbWluCnBhc3N3b3JkPWxldGJyYWRpbgo=&#13;
kind: Secret&#13;
metadata:&#13;
 name: sample-secret2-77bf8kf96f&#13;
type: Opaque</pre>&#13;
<p>In the next section, we discuss another key feature of Kustomize: the ability to aggregate individual Kubernetes resource files into a single deployment YAML file.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Composition" data-type="sect2"><div class="sect2" id="composition">&#13;
<h2>Composition</h2>&#13;
<p>Kubernetes deployments are typically created from a large set of YAML files.<a contenteditable="false" data-primary="Kustomize for configurations" data-secondary="composition to aggregate YAML files" data-type="indexterm" id="idm45358200110984"/><a contenteditable="false" data-primary="YAML files" data-secondary="Kustomize for configuration files" data-tertiary="composition to aggregate YAML files" data-type="indexterm" id="idm45358200109576"/><a contenteditable="false" data-primary="YAML files" data-secondary="aggregating via Kustomize" data-type="indexterm" id="idm45358200107896"/> The YAML files themselves may be shared for multiple purposes. Kustomize supports the notion of <em>composition</em>, which enables individual YAML files to be selected and aggregated into a single deployment YAML file. The composition functionality also reduces the need to copy YAML files and then make slight modifications to those duplicate files for different purposes.</p>&#13;
<p class="pagebreak-before less_space">To demonstrate the capabilities of Kustomize composition, let’s create some Kubernetes YAML resource files that we want to aggregate. First, create a new folder called <em>composition</em> and store the following content in a file called <em>deploymentset.yaml</em> in the <em>composition</em> folder:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">annotations</code><code class="p">:</code>&#13;
  <code class="nt">deployment.kubernetes.io/revision</code><code class="p">:</code> <code class="s">"1"</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">matchLabels</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">strategy</code><code class="p">:</code>&#13;
  <code class="nt">rollingUpdate</code><code class="p">:</code>&#13;
   <code class="nt">maxSurge</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
   <code class="nt">maxUnavailable</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">RollingUpdate</code>&#13;
 <code class="nt">template</code><code class="p">:</code>&#13;
  <code class="nt">metadata</code><code class="p">:</code>&#13;
   <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">spec</code><code class="p">:</code>&#13;
   <code class="nt">containers</code><code class="p">:</code>&#13;
   <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
     <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx:1.7.9</code>&#13;
     <code class="nt">ports</code><code class="p">:</code>&#13;
     <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
<p>Next, store the following service resource YAML in a file called <em>service.yaml</em> in the <em>composition</em> folder:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">ports</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>&#13;
   <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code></pre>&#13;
<p>You can now create a customization file that is capable of composing <em>deploymentset.yaml</em> and <em>service.yaml</em> by doing the following:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">resources</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">deploymentset.yaml</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">service.yaml</code></pre>&#13;
<p>To run this example, save the file as <em>kustomization.yaml</em> in the <em>composition</em> folder you previously created. You can now run Kustomize using this file by doing the <span class="keep-together">following</span>:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl kustomize ./composition</strong></pre>&#13;
<p>After you run this command, Kustomize will generate the aggregated deployment YAML as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">ports</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>&#13;
   <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
<code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">annotations</code><code class="p">:</code>&#13;
  <code class="nt">deployment.kubernetes.io/revision</code><code class="p">:</code> <code class="s">"1"</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">matchLabels</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">strategy</code><code class="p">:</code>&#13;
  <code class="nt">rollingUpdate</code><code class="p">:</code>&#13;
   <code class="nt">maxSurge</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
   <code class="nt">maxUnavailable</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">RollingUpdate</code>&#13;
 <code class="nt">template</code><code class="p">:</code>&#13;
  <code class="nt">metadata</code><code class="p">:</code>&#13;
   <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">spec</code><code class="p">:</code>&#13;
   <code class="nt">containers</code><code class="p">:</code>&#13;
   <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx:1.7.9</code>&#13;
     <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
     <code class="nt">ports</code><code class="p">:</code>&#13;
     <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
<p>In the next section, we cover Kustomize’s patch capability, which enables us to avoid duplicating a full Kubernetes YAML file when all we need is a small change to an existing YAML file.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Patches" data-type="sect2"><div class="sect2" id="patches">&#13;
<h2>Patches</h2>&#13;
<p>In many cases, the YAML file needed for a deployment may be very similar <a contenteditable="false" data-primary="YAML files" data-secondary="patching via Kustomize" data-type="indexterm" id="idm45358199983480"/><a contenteditable="false" data-primary="Kustomize for configurations" data-secondary="patches" data-type="indexterm" id="idm45358199740344"/><a contenteditable="false" data-primary="YAML files" data-secondary="Kustomize for configuration files" data-tertiary="patches" data-type="indexterm" id="idm45358199738904"/><a contenteditable="false" data-primary="patches to YAML files via Kustomize" data-type="indexterm" id="idm45358199737240"/>to an existing YAML file and only needs a small change to one of the resources. As an example, perhaps the only thing in a YAML file that needs to be changed is the number of replicas to create. For this type of situation, Kustomize provides the notion of <em>patches</em>, which are capable of making small modifications to resources described in YAML files. This is a much better approach than having to make a copy of the complete YAML file and then making the small modification to the duplicate file.</p>&#13;
<p>To demonstrate the capabilities of patches, let’s create a Kubernetes YAML resource file that is close to what we want but needs small changes. First, create a new folder called <em>patch</em> and store the following content in a file called <em>deploymentset.yaml</em> in the <em>patch</em> folder:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">annotations</code><code class="p">:</code>&#13;
  <code class="nt">deployment.kubernetes.io/revision</code><code class="p">:</code> <code class="s">"1"</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">matchLabels</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">strategy</code><code class="p">:</code>&#13;
  <code class="nt">rollingUpdate</code><code class="p">:</code>&#13;
   <code class="nt">maxSurge</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
   <code class="nt">maxUnavailable</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">RollingUpdate</code>&#13;
 <code class="nt">template</code><code class="p">:</code>&#13;
  <code class="nt">metadata</code><code class="p">:</code>&#13;
   <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">spec</code><code class="p">:</code>&#13;
   <code class="nt">containers</code><code class="p">:</code>&#13;
   <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
     <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx:1.7.9</code>&#13;
     <code class="nt">ports</code><code class="p">:</code>&#13;
     <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
<p>The deployment YAML file shown above is close to what we desire, but we now need a new version of this file that needs to provide six replicas instead of three. Rather than duplicating the whole <em>deploymentset.yaml</em> resource file and then changing the value of the <code>replicas</code> field, we can create a patch file that is much smaller and easier to maintain.</p>&#13;
<p>Here is a simple patch file that can be used to create a duplicate <em>deploymentset.yaml</em> with a modified <code>replicas</code> field value:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">6</code></pre>&#13;
<p>Note that this small file identifies the kind of resource, the name of the resource, and the attribute field we wish to modify. This small file gives Kustomize enough information to identify the field that needs to be patched. To use this patch file, save it in a new file called <em>update_replicas.yaml</em> in the <em>patch</em> folder you just created.</p>&#13;
<p>With the patch file created, we can now create a <em>kustomization.yaml</em> file that identifies the <em>deploymentset.yaml</em> file we wish to modify and also identifies the <em>update_replicas.yaml</em> file as the file that contains information on which fields to modify:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">resources</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">deploymentset.yaml</code>&#13;
<code class="nt">patchesStrategicMerge</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">update_replicas.yaml</code></pre>&#13;
<p>To run the previous example, save the file as <em>kustomization.yaml</em> in the <em>patch</em> folder you previously created. You can now run Kustomize using this file by doing the <span class="keep-together">following</span>:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl kustomize ./patch</strong></pre>&#13;
<p>After you run this command, Kustomize will generate the patched deployment YAML as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">annotations</code><code class="p">:</code>&#13;
  <code class="nt">deployment.kubernetes.io/revision</code><code class="p">:</code> <code class="s">"1"</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">6</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">matchLabels</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
 <code class="nt">strategy</code><code class="p">:</code>&#13;
  <code class="nt">rollingUpdate</code><code class="p">:</code>&#13;
   <code class="nt">maxSurge</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
   <code class="nt">maxUnavailable</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">RollingUpdate</code>&#13;
 <code class="nt">template</code><code class="p">:</code>&#13;
  <code class="nt">metadata</code><code class="p">:</code>&#13;
   <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">spec</code><code class="p">:</code>&#13;
   <code class="nt">containers</code><code class="p">:</code>&#13;
   <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx:1.7.9</code>&#13;
     <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
     <code class="nt">ports</code><code class="p">:</code>&#13;
     <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
<p>In the next section, we describe the overlays capability, which is one of the most powerful features available in Kustomize.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Overlays" data-type="sect2"><div class="sect2" id="overlays">&#13;
<h2>Overlays</h2>&#13;
<p>Building on the features described in the previous sections, <a contenteditable="false" data-primary="Kustomize for configurations" data-secondary="overlays" data-type="indexterm" id="ch05-ovl"/><a contenteditable="false" data-primary="YAML files" data-secondary="Kustomize for configuration files" data-tertiary="overlays" data-type="indexterm" id="ch05-ovl2"/><a contenteditable="false" data-primary="overlays on YAML files via Kustomize" data-type="indexterm" id="ch05-ovl3"/>the <em>overlays</em> functionality is where users see the most significant capabilities available from Kustomize. Overlays provide an approach where the user starts with a base folder that contains a set of Kubernetes deployment YAML files and builds on the base folder using overlay folders that enhance and customize the deployment YAML files contained in the base folder. In this section, we demonstrate how Kustomize overlays can be used to manage the differences that occur in deployment YAML files that exist when using the deployment YAML files across development, staging, and production Kubernetes environments.</p>&#13;
<p>To demonstrate how Kustomize can be used in this fashion, let’s first create a new folder called <em>base</em> and copy into this folder the <em>deploymentset.yaml</em> and <em>service.yaml</em> files that we created in <a data-type="xref" href="#composition">“Composition”</a>. With these files in the <em>base</em> folder, we can now create a <em>kustomization.yaml</em> in the <em>base</em> folder that references the deployment YAML files as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">resources</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">deploymentset.yaml</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">service.yaml</code></pre>&#13;
<p>Next, we are going to create an overlay folder that tailors the base deployment files for a development environment. We begin by creating a new folder called <em>development</em>. Inside this folder, we create a new <em>kustomization.yaml</em> as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">commonLabels</code><code class="p">:</code>&#13;
 <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">development</code>&#13;
<code class="nt">bases</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">../base</code>&#13;
<code class="nt">namePrefix</code><code class="p">:</code> <code class="l-Scalar-Plain">dev-</code>&#13;
<code class="nt">patchesStrategicMerge</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">update_replicas.yaml</code></pre>&#13;
<p>As shown in <em>kustomization.yaml</em>, the Kustomization file starts by defining a new label called <code>env</code> that contains the value <code>development</code>. Since this label is defined as part of the <code>commonLabels</code> field, the effect is that this label will be set on all resources that are included in <em>kustomization.yaml</em>. The Kustomization then declares that its base will be the deployment YAML files that are located in the <em>base</em> folder. Next, the <span class="keep-together">Kustomization</span> defines a <code>namePrefix</code> with the value <code>dev-</code><em>.</em> The effect of this declaration is to add a prefix to all resource names and references. In the final portion of <em>kustomization.yaml</em>, a patch is declared that will contain modifications to the deployment YAML files found in the <em>base</em> directory. The patch is contained in the file <em>update_replicas.yaml</em>. The contents of <em>update_replicas.yaml</em> are shown here:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">2</code></pre>&#13;
<p>The <em>update_replicas.yaml</em> file contains a patch that modifies the number of replicas to now be <code>2</code> for a development environment. To run this example, make sure to save the patch in the <em>development</em> folder with the name <em>update_replicas.yaml</em>. You can now run Kustomize using the <em>kustomization.yaml</em> and <em>update_replicas.yaml</em> files by doing the following:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="l-Scalar-Plain">$</code><code class="l-Scalar-Plain"> </code><strong><code class="l-Scalar-Plain">kubectl</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">kustomize</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">./development</code></strong></pre>&#13;
<p>After you run this command, Kustomize will generate the patched deployment YAML as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">development</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">dev-nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">ports</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>&#13;
   <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">development</code>&#13;
<code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">annotations</code><code class="p">:</code>&#13;
  <code class="nt">deployment.kubernetes.io/revision</code><code class="p">:</code> <code class="s">"1"</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">development</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">dev-nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">2</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">matchLabels</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
   <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">development</code>&#13;
 <code class="nt">strategy</code><code class="p">:</code>&#13;
  <code class="nt">rollingUpdate</code><code class="p">:</code>&#13;
   <code class="nt">maxSurge</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
   <code class="nt">maxUnavailable</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">RollingUpdate</code>&#13;
 <code class="nt">template</code><code class="p">:</code>&#13;
  <code class="nt">metadata</code><code class="p">:</code>&#13;
   <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
    <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">development</code>&#13;
  <code class="nt">spec</code><code class="p">:</code>&#13;
   <code class="nt">containers</code><code class="p">:</code>&#13;
   <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx:1.7.9</code>&#13;
     <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
     <code class="nt">ports</code><code class="p">:</code>&#13;
     <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
<p>Upon reviewing the deployment file that was generated, we see several changes that resulted from the use of <em>kustomization.yaml</em> and <em>update_replicas.yaml</em> files. First, notice that a new <code>env</code> label with the value of <code>development</code> now exists in both the service and deployment resources, as well as in the <code>template</code> section used to create the container replicas. Also note that all names listed in the deployment YAML have a prefix of <code>dev-</code><em>.</em> Finally, the number of replicas for the development deployment YAML has been modified to denote that only two replicas should be created.</p>&#13;
<p>Next, we are going to create <em>kustomize.yaml</em> and <em>update_replicas.yaml</em> files that are used to tailor the base deployment YAMLs for a staging environment. In our situation, the staging environment should have a label of <code>env</code> with the value of <code>staging</code> on all resources, the names of all resources should have a prefix of <code>staging-</code><em>,</em> and the number of replicas that should be used for the staging environment is five. Similar to the previous example for the development environment customizations, we begin by creating a new folder called <em>staging</em>. Inside this folder, we create a new <em>kustomization.yaml</em> as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">commonLabels</code><code class="p">:</code>&#13;
 <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">staging</code>&#13;
<code class="nt">bases</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">../base</code>&#13;
<code class="nt">namePrefix</code><code class="p">:</code> <code class="l-Scalar-Plain">staging-</code>&#13;
<code class="nt">patchesStrategicMerge</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">update_replicas.yaml</code></pre>&#13;
<p class="pagebreak-after">In this example, we defined <code>env</code> as a <code>commonLabel</code> and gave it the value of <code>staging</code>. We created a new <code>namePrefix</code> with the value of <code>staging-</code><em>.</em> We also are once again creating an <em>update_replicas.yaml</em> that will be used as a patch file to modify the number of replicas when the base deployments YAML files are used in a staging environment. The contents of <em>update_replicas.yaml</em> are shown here:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code></pre>&#13;
<p>Once again, the <em>update_replicas.yaml</em> file contains a patch that modifies the number of replicas. For staging, we have chosen to use five replicas. To run this example, make sure to save the patch in the <em>staging</em> folder with the name <em>update_replicas.yaml</em>. You can now run Kustomize using the <em>kustomization.yaml</em> and <em>update_replicas.yaml</em> files by doing the following:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl kustomize ./staging</strong></pre>&#13;
<p>After you run this command, Kustomize will generate the patched deployment YAML as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">staging</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">staging-nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">ports</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>&#13;
   <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">staging</code>&#13;
<code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">annotations</code><code class="p">:</code>&#13;
  <code class="nt">deployment.kubernetes.io/revision</code><code class="p">:</code> <code class="s">"1"</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">staging</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">staging-nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">matchLabels</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
   <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">staging</code>&#13;
  <code class="nt">strategy</code><code class="p">:</code>&#13;
   <code class="nt">rollingUpdate</code><code class="p">:</code>&#13;
    <code class="nt">maxSurge</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
    <code class="nt">maxUnavailable</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
   <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">RollingUpdate</code>&#13;
  <code class="nt">template</code><code class="p">:</code>&#13;
   <code class="nt">metadata</code><code class="p">:</code>&#13;
    <code class="nt">labels</code><code class="p">:</code>&#13;
     <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
     <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">staging</code>&#13;
   <code class="nt">spec</code><code class="p">:</code>&#13;
    <code class="nt">containers</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx:1.7.9</code>&#13;
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
      <code class="nt">ports</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
<p>Once more, we see several changes that resulted from the use of our <em>kustomization.yaml</em> and <em>update_replicas.yaml</em> files. First, notice that a new <code>env</code> label with the value of <code>staging</code> now exists in both the service and deployment resources, as well as in the <code>template</code> section used to create the container replicas. Also note that all names listed in the deployment YAML have a prefix of <code>staging-</code><em>.</em> Finally, the number of replicas for the development deployment YAML has been modified to denote that five replicas should be created.</p>&#13;
<p>For our final example, we will use the same techniques to modify the base deployment YAML files for a production environment. In our production environment, we should have a label of <code>env</code> with the value of <code>production</code> on all resources, the names of all resources should have a prefix of <code>prod-</code><em>,</em> and the number of replicas that should be used for the production environment is 20. We begin by creating a new folder called <em>production</em>. Inside this folder, we create a new <em>kustomization.yaml</em> as shown:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">commonLabels</code><code class="p">:</code>&#13;
 <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">production</code>&#13;
<code class="nt">bases</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">../base</code>&#13;
<code class="nt">namePrefix</code><code class="p">:</code> <code class="l-Scalar-Plain">prod-</code>&#13;
<code class="nt">patchesStrategicMerge</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="l-Scalar-Plain">update_replicas.yaml</code></pre>&#13;
<p>We again create an <em>update_replicas.yaml</em> file that will be used as a patch file to modify the number of replicas when the base deployments YAML files are used in the production environment. The contents of <em>update_replicas.yaml</em> are as follows:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">20</code></pre>&#13;
<p>To run this example, make sure to save the patch in the <em>production</em> folder with the name <em>update_replicas.yaml</em>. You can now run Kustomize using the <em>kustomization.yaml</em> and <em>update_replicas.yaml</em> files by doing the following:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl kustomize ./production</strong></pre>&#13;
<p class="pagebreak-before less_space">After you run this command, Kustomize will generate the patched deployment YAML with the proper labels, prefixes, and replica count value as shown here:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">production</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">prod-nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">ports</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>&#13;
   <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">production</code>&#13;
<code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">annotations</code><code class="p">:</code>&#13;
  <code class="nt">deployment.kubernetes.io/revision</code><code class="p">:</code> <code class="s">"1"</code>&#13;
 <code class="nt">labels</code><code class="p">:</code>&#13;
  <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
  <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">production</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">prod-nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">20</code>&#13;
 <code class="nt">selector</code><code class="p">:</code>&#13;
  <code class="nt">matchLabels</code><code class="p">:</code>&#13;
   <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
   <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">production</code>&#13;
 <code class="nt">strategy</code><code class="p">:</code>&#13;
  <code class="nt">rollingUpdate</code><code class="p">:</code>&#13;
   <code class="nt">maxSurge</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
   <code class="nt">maxUnavailable</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">RollingUpdate</code>&#13;
 <code class="nt">template</code><code class="p">:</code>&#13;
  <code class="nt">metadata</code><code class="p">:</code>&#13;
   <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">webserver</code>&#13;
    <code class="nt">env</code><code class="p">:</code> <code class="l-Scalar-Plain">production</code>&#13;
  <code class="nt">spec</code><code class="p">:</code>&#13;
   <code class="nt">containers</code><code class="p">:</code>&#13;
   <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx:1.7.9</code>&#13;
     <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
     <code class="nt">ports</code><code class="p">:</code>&#13;
     <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
<p>As we have demonstrated with this comprehensive example, Kustomize was able to manage deployment YAML files for a development environment, a staging environment, and a production environment, and it reduced the number of deployment YAML files we needed to maintain for all of these environments. We avoided a large amount of copy and paste of files and ended up with a much more manageable solution. In the next section, we show how Kustomize can be used to directly deploy its generated deployment files.<a contenteditable="false" data-primary="" data-startref="ch05-ovl" data-type="indexterm" id="idm45358191812024"/><a contenteditable="false" data-primary="" data-startref="ch05-ovl2" data-type="indexterm" id="idm45358191810808"/><a contenteditable="false" data-primary="" data-startref="ch05-ovl3" data-type="indexterm" id="idm45358191653432"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Direct Deploy of Kustomize-Generated Resource Files" data-type="sect2"><div class="sect2" id="direct_deploy_of_kustomize_generated_res">&#13;
<h2>Direct Deploy of Kustomize-Generated Resource Files</h2>&#13;
<p>In all of the previous Kustomize examples, we generated the modified deployment<a contenteditable="false" data-primary="Kustomize for configurations" data-secondary="direct deploy of Kustomize-generated resource files" data-type="indexterm" id="idm45358191650600"/><a contenteditable="false" data-primary="YAML files" data-secondary="Kustomize for configuration files" data-tertiary="direct deploy of Kustomize-generated resource files" data-type="indexterm" id="idm45358191649160"/><a contenteditable="false" data-primary="deployments" data-secondary="Kustomize-generated resource files" data-type="indexterm" id="idm45358191647512"/> YAMLs and outputted them to make it clear what Kustomize was generating. Kustomize provides another option in which the generated deployment YAML files can be automatically submitted to Kubernetes for processing. This avoids the step of first generating all the deployment YAML files and instead enables us to use Kustomize to directly deploy what it generates. To use Kustomize to directly deploy to Kubernetes, we use the <code>kubectl apply -k</code> option and pass in the desired customization directory. For example, if we wanted to directly deploy the production example we just covered, we would do the following:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl apply -k ./production/</strong></pre>&#13;
<p>In addition, Kustomize provides commands for viewing and deleting the Kubernetes resources objects that it generates and deploys. More details on these capabilities can be found in the <a href="https://oreil.ly/4kCu4">Kustomization documentation</a>. In the next section, we introduce the concept of GitOps, which is a popular cloud native methodology for automated continuous delivery that is driven directly from Git repositories.<a contenteditable="false" data-primary="" data-startref="ch05-Kust" data-type="indexterm" id="idm45358191642456"/><a contenteditable="false" data-primary="" data-startref="ch05-Kust2" data-type="indexterm" id="idm45358191641112"/><a contenteditable="false" data-primary="" data-startref="ch05-Kust3" data-type="indexterm" id="idm45358191639736"/><a contenteditable="false" data-primary="" data-startref="ch05-Kust4" data-type="indexterm" id="idm45358191638360"/><a contenteditable="false" data-primary="" data-startref="ch05-Kust5" data-type="indexterm" id="idm45358191636984"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="GitOps" data-type="sect1"><div class="sect1" id="gitops">&#13;
<h1>GitOps</h1>&#13;
<p><em>GitOps</em> is the idea of using Git to manage your infrastructure<a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="GitOps for infrastructure management" data-type="indexterm" id="idm45358191633080"/><a contenteditable="false" data-primary="GitOps for infrastructure management" data-type="indexterm" id="idm45358191631768"/><a contenteditable="false" data-primary="infrastructure" data-secondary="GitOps for management" data-type="indexterm" id="idm45358191630648"/><a contenteditable="false" data-primary="Git repositories" data-secondary="GitOps for infrastructure management" data-type="indexterm" id="idm45358191629272"/><a contenteditable="false" data-primary="GitOps for infrastructure management" data-secondary="documentation online" data-type="indexterm" id="idm45358191627880"/><a contenteditable="false" data-primary="GitHub" data-secondary="GitOps for infrastructure management" data-type="indexterm" id="idm45358191626488"/> as code. The idea was first popularized by Weaveworks,<sup><a data-type="noteref" href="ch05.html#ch01fn34" id="ch01fn34-marker">2</a></sup> and with the proliferation of more containerized approaches to software delivery, GitOps has become a popular topic in the industry. The basic GitOps flow is always the same: an update to your source code triggers automation and ultimately validates a potential new change or delivers a new change into a running environment.</p>&#13;
<p>Applying GitOps involves the following steps:</p>&#13;
<ol>&#13;
<li><p>Containerized applications are represented with declarative infrastructure (easily done with Kubernetes API represented as YAML) and stored in Git.</p></li>&#13;
<li><p>All changes originate from your source control revision system, typically Git-based systems like GitHub, GitLab, Bitbucket, and so on.</p></li>&#13;
<li><p>Code-review techniques like pull requests or merge requests allow you to apply a review process and even automated validation to ensure that changes work correctly before wide rollouts.</p></li>&#13;
<li><p>Kubernetes (or more generally a software agent) is then used to apply and reconcile the desired state whenever changes are merged into the appropriate branch in your repository.</p></li>&#13;
</ol>&#13;
<p>The first half of this flow is really about your team and organizational culture. Is your organization disciplined enough with the appropriate skill set to make all changes indirectly via Git rather than directly manipulating the target infrastructure? Are you at a scale in terms of the number of applications, number of delivery stages, and number of actual clusters that you can manage the proliferation of repositories and branches? As with any cultural change, look for teams in your organization that adapt well to new ideas and can become good representative examples for the broader organization. Focus on a specific application or set of related applications applying this approach from end to end before attempting a broader reorganization of your team’s delivery practices.</p>&#13;
<p>The second half of the flow is where you have various tools that have evolved to simplify the management of adopting changes out of Git, applying the changes to your running environments, and validating the changes. We will review a few projects that can help you adopt your changes from Git and apply them to your Kubernetes environment in the following sections.</p>&#13;
<p>Pull requests/merge requests (PR/MR) allow you to leverage feature branches<a contenteditable="false" data-primary="pull requests/merge requests (PR/MR)" data-type="indexterm" id="idm45358191616536"/><a contenteditable="false" data-primary="Git repositories" data-secondary="pull requests/merge requests" data-type="indexterm" id="idm45358191615416"/><a contenteditable="false" data-primary="GitHub" data-secondary="pull requests/merge requests" data-type="indexterm" id="idm45358191614024"/><a contenteditable="false" data-primary="GitOps for infrastructure management" data-secondary="pull requests/merge requests" data-type="indexterm" id="idm45358191612632"/> to complete enhancements and fixes against your code base and validate those changes before integration with the main delivery branches. These PR/MR branches generally apply automated validation to virtually all dimensions of code quality, including security scans, linting or best practices required by your team or organization, automated unit testing, automated functional testing, automated integration testing, and review by other humans on your team to ensure that the change is consistent with the rest of the system’s goals and use cases.</p>&#13;
<p>GitOps can work well in systems with fewer independent applications and fewer active environments under management (e.g., software as a service, or SaaS). When your delivery output includes software that must be packaged and versioned, GitOps can still add value but likely becomes part of a larger story.</p>&#13;
<p>Consider a single application managed with source under management in a Git repository. For each stage of your release process, you will create a distinct Git branch. In the most basic version, you keep a single “main” branch deployed continuously within your end-user accessible production applications. In other cases, your release process may require additional lower environments and thus additional corresponding branches like “main” (for code under active development), “candidate” (for code undergoing user acceptance testing), and “stable” (for code running in production). For example, you might have branches “main” and “stable” where the “main” branch accumulates changes for development and “stable” tracks the version of code and configuration deployed in production. Many organizations likely have additional layers of validation, including quality engineering, user acceptance testing, performance and scale testing, and so on. Each distinct phase that you want control over means potentially another branch in your Git repository.</p>&#13;
<p>In the next section, we introduce Razee, which is the first of several production-quality continuous delivery systems for Kubernetes that we survey in this chapter.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Razee" data-type="sect1"><div class="sect1" id="razee">&#13;
<h1>Razee</h1>&#13;
<p><a href="https://razee.io">Razee</a> is an open source continuous deployment<a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="Razee deployment system" data-type="indexterm" id="idm45358191605464"/><a contenteditable="false" data-primary="deployments" data-secondary="Razee pull-based system" data-type="indexterm" id="idm45358191604120"/><a contenteditable="false" data-primary="Razee pull-based deployment system" data-type="indexterm" id="idm45358191602744"/> system originally developed by IBM to address issues that arise when supporting extreme-scale environments with tens of thousands of Kubernetes clusters that need to be managed. Razee automates the deployment of Kubernetes resources across a large number of clusters and environments and provides several key features to address issues that arise when managing a large number of clusters.</p>&#13;
<p>In contrast to other deployment systems, Razee has a pull-based deployment approach that enables Kubernetes clusters to be self-updating. <a contenteditable="false" data-primary="Razee pull-based deployment system" data-secondary="dynamic inventory creation" data-type="indexterm" id="idm45358191600568"/><a contenteditable="false" data-primary="inventorying clusters via Razee" data-type="indexterm" id="idm45358191599160"/><a contenteditable="false" data-primary="clusters" data-secondary="inventorying via Razee" data-type="indexterm" id="idm45358191598040"/>Also, Razee has a dynamic inventory-creation feature that allows it to ascertain what is running in all the Kubernetes clusters that it manages. With this feature, operators can gain insights into what applications and versions run in their clusters. Razee keeps a history of this inventory data and provides alerts to help troubleshoot issues in clusters.</p>&#13;
<p>Razee provides a lot of rule-based support for the grouping of clusters that helps to simplify managing large groups of clusters. Razee also uses a rule-based approach to orchestrate its pull-based update deployment model. The combination of all of these features enables Razee to support automated deployment and management of tens of thousands of clusters across multiple availability zones without manual intervention. <a contenteditable="false" data-primary="Razee pull-based deployment system" data-secondary="documentation online" data-type="indexterm" id="idm45358191595320"/>For more details on Razee’s approach to scalability and the key features it provides, see the <a href="https://oreil.ly/1N0ql">Razee documentation</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Argo CD" data-type="sect1"><div class="sect1" id="argo_cd">&#13;
<h1>Argo CD</h1>&#13;
<p><a href="https://oreil.ly/H2UiL">Argo CD</a> is a declarative<a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="Argo CD declarative system" data-type="indexterm" id="idm45358191590440"/><a contenteditable="false" data-primary="Argo CD declarative system" data-type="indexterm" id="idm45358191589048"/><a contenteditable="false" data-primary="Git repositories" data-secondary="Argo CD declarative system" data-type="indexterm" id="idm45358191587928"/><a contenteditable="false" data-primary="GitHub" data-secondary="Argo CD declarative system" data-type="indexterm" id="idm45358191586536"/> continuous delivery system that will continuously reconcile the contents of a Git repository with a Kubernetes cluster. Argo CD has a number of flexible access control models ranging from a deployment per namespace to per cluster, or you can provide credentials for multiple clusters via Kubernetes secrets to manage multiple clusters.</p>&#13;
<p>Argo CD adapts well to Kubernetes applications using Helm charts, <a href="https://oreil.ly/TOE56">Kustomize</a>, or <a href="https://oreil.ly/9apwT">ksonnet</a> <a contenteditable="false" data-primary="Helm package manager" data-secondary="Helm charts" data-tertiary="Argo CD declarative system" data-type="indexterm" id="idm45358191582840"/><a contenteditable="false" data-primary="Kustomize for configurations" data-secondary="Argo CD declarative system" data-type="indexterm" id="idm45358191581144"/><a contenteditable="false" data-primary="ksonnet and Argo CD" data-type="indexterm" id="idm45358191579736"/><a contenteditable="false" data-primary="templates" data-secondary="Argo CD declarative system" data-type="indexterm" id="idm45358191578632"/>for templating changes to your resources for different clusters. Once deployed and configured, Argo CD will respond to postcommit hooks or ongoing polling of changes to the repository and apply those changes to the cluster. In many ways, it resembles a Puppet- or Chef-style convergence model. If changes are made out of band to the cluster, the desired state will eventually be restored.</p>&#13;
<p>As of the time of this writing, Argo CD has begun looking at additional API kinds, including <a href="https://oreil.ly/5nbsB">ApplicationSet</a>, <a contenteditable="false" data-primary="Argo CD declarative system" data-secondary="ApplicationSet multicluster support" data-type="indexterm" id="idm45358191575496"/>that allow the deployment of resources to more than one cluster. <a contenteditable="false" data-primary="GitOps for infrastructure management" data-secondary="ArgoCD auto-configuring" data-type="indexterm" id="idm45358191573928"/>One proposed path would enable Argo CD to leverage the inventory of clusters available from Open Cluster Management to automatically configure GitOps for members of the fleet.</p>&#13;
<p>For more information on Argo CD, we recommend <a contenteditable="false" data-primary="Argo CD declarative system" data-secondary="documentation online" data-type="indexterm" id="idm45358191571800"/>the <a href="https://oreil.ly/VJQk5">Getting Started</a> materials from the project website.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Tekton" data-type="sect1"><div class="sect1" id="tekton">&#13;
<h1>Tekton</h1>&#13;
<p>Tekton is a <em>continuous integration and continuous delivery</em> (CI/CD) system<a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="Tekton CI/CD system" data-type="indexterm" id="ch05-tek"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-type="indexterm" id="ch05-tek2"/><a contenteditable="false" data-primary="continuous integration and continuous delivery (CI/CD) system Tekton" data-type="indexterm" id="ch05-tek3"/><a contenteditable="false" data-primary="cloud native applications" data-secondary="continuous delivery support by" data-tertiary="Tekton CI/CD system" data-type="indexterm" id="idm45358191562760"/> that runs as a Kubernetes-based cloud native application. Since Tekton runs as a Kubernetes-based application, it is built to run as a scalable cloud native application. This is a significant advantage over more legacy CI/CD systems, which are not cloud native based and thus are more susceptible to failures or performance bottlenecks.</p>&#13;
<p>Tekton was originally the build system for the Knative serverless workload platform. Because it provided value as a general-purpose CI/CD platform, it was converted to a standalone project and donated to the Continuous Delivery Foundation in March 2019.<sup><a data-type="noteref" href="ch05.html#ch01fn35" id="ch01fn35-marker">3</a></sup></p>&#13;
<p>Tekton provides a CLI and a dashboard user interface for managing its CI/CD workloads.<a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-type="indexterm" id="idm45358191557208"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-type="indexterm" id="idm45358191555816"/> It also has event trigger and webhooks support. There are several great <a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="tutorials online" data-type="indexterm" id="idm45358191554312"/>tutorials for getting started with Tekton available at the <a href="https://oreil.ly/0yZDx">Tekton Pipeline GitHub site</a> and the <a href="https://oreil.ly/FSE9g">IBM Developer Tekton Tutorial page</a>. In the next few sections, we introduce the fundamental concepts of Tekton: tasks and pipelines.</p>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Tasks" data-type="sect2"><div class="sect2" id="tasks">&#13;
<h2 class="less_space">Tasks</h2>&#13;
<p>A <em>task</em> represents a collection of commands or tools that <a contenteditable="false" data-primary="tasks" data-secondary="Tekton CI/CD system" data-type="indexterm" id="idm45358191547960"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="tasks" data-type="indexterm" id="idm45358191546584"/><a contenteditable="false" data-primary="tasks" data-type="indexterm" id="idm45358191545208"/><a contenteditable="false" data-primary="steps in tasks" data-type="indexterm" id="idm45358191544104"/><a contenteditable="false" data-primary="YAML files" data-secondary="tasks with steps" data-type="indexterm" id="idm45358191543000"/>should be executed in a specific order. Each command or tool is represented as a <em>step,</em> which defines the command or tool to be executed and the container image that contains the command or tool. The following example illustrates a simple task with two discrete steps that each run an echo command:</p>&#13;
<pre data-type="programlisting">apiVersion: tekton.dev/v1beta1&#13;
kind: Task&#13;
metadata:&#13;
 name: simple-task-example&#13;
spec:&#13;
 steps:&#13;
 - name: echo&#13;
   image: ubuntu&#13;
   command:&#13;
   - echo&#13;
   args:&#13;
   - "This is step one of our first simple task example"&#13;
 - name: echo2&#13;
   image: ubuntu&#13;
   command:&#13;
   - echo&#13;
   args:&#13;
   - "This is step two of our first simple task example"</pre>&#13;
<p>To run this task, first save the previous example in a file called <em>simpleexample.yaml</em>. <a contenteditable="false" data-primary="tasks" data-secondary="TaskRun resources" data-type="indexterm" id="idm45358191539112"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="tasks" data-tertiary="TaskRun resources" data-type="indexterm" id="idm45358191537704"/>Next, you need to create a <code>TaskRun</code> resource that will be used to run the task in a standalone fashion on a Kubernetes cluster. Here is the YAML for the <code>TaskRun</code> resource that we will use:</p>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">tekton.dev/v1beta1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">TaskRun</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">simple-task-example-run</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
 <code class="nt">taskRef</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">simple-task-example</code></pre>&#13;
<p>As shown in our sample <code>TaskRun</code> YAML, we create a <code>TaskRun</code> resource and give it a name. Then in the <code>taskRef</code> field, we provide the name of the task that we want to run on a Kubernetes cluster. To deploy the <code>TaskRun</code> resource, save the previous example as <em>simpleexamplerun.yaml</em>.</p>&#13;
<p>On a Kubernetes cluster that has Tekton installed, run the following commands:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl apply -f simpleexample.yaml</strong>&#13;
$ <strong>kubectl apply -f simpleexamplerun.yaml</strong></pre>&#13;
<p>After running these commands, you should see output like the following:</p>&#13;
<pre data-type="programlisting">task.tekton.dev/simple-task-example created&#13;
taskrun.tekton.dev/simple-task-example-run created</pre>&#13;
<p>To confirm that the task ran properly, we can use the <code>tkn taskrun logs --last -f</code> <a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="task logs" data-type="indexterm" id="idm45358191484872"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="task logs" data-type="indexterm" id="idm45358191483256"/>Tekton command to view the logs from the <code>simple-task-example</code> that we just ran:</p>&#13;
<pre data-type="programlisting">$ <strong>tkn taskrun logs --last -f</strong>&#13;
[echo] This is step one of our first simple task example&#13;
 &#13;
[echo2] This is step two of our first simple task example</pre>&#13;
<p>If you need more details on the execution of your task, you can use the <code>tkn taskrun describe</code> command to get a much larger list of information:</p>&#13;
<pre data-type="programlisting">$ <strong>tkn taskrun describe simple-task-example-run</strong>&#13;
Name: simple-task-example-run&#13;
Namespace: default&#13;
Task Ref: simple-task-example&#13;
Timeout: 1h0m0s&#13;
Labels:&#13;
 app.kubernetes.io/managed-by=tekton-pipelines&#13;
 tekton.dev/task=simple-task-example&#13;
 &#13;
Status&#13;
 &#13;
STARTED      DURATION   STATUS&#13;
1 minute ago 16 seconds Succeeded&#13;
 Input Resources&#13;
No input resources&#13;
 &#13;
Output Resources&#13;
 No output resources&#13;
 Params&#13;
 &#13;
 No params&#13;
 &#13;
 Steps&#13;
 &#13;
 NAME    STATUS&#13;
 ∙ echo  Completed&#13;
 ∙ echo2 Completed&#13;
 &#13;
Sidecars&#13;
 &#13;
No sidecars</pre>&#13;
<p>While not shown in the previous example, Tekton has a large number of built-in capabilities for common integration steps like pulling in content from Git repositories and building container images. In addition, Tekton gives tasks a new feature called <em>workspaces,</em> which is a shared persistent volume. The workspace provides an area of shared storage that tasks can use to share files with other tasks that are working with it cooperatively. For more details on workspaces, see the <a href="https://oreil.ly/XUZdv">Tekton Workspaces Documentation</a>.</p>&#13;
<p>In the next section, we describe pipelines, which is Tekton’s construct for enabling multiple tasks to work together on common build integration and deployment <span class="keep-together">activities</span>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Pipelines" data-type="sect2"><div class="sect2" id="pipelines">&#13;
<h2>Pipelines</h2>&#13;
<p>Tekton provides the notion of <em>pipelines</em> as its mechanism for <a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="pipelines" data-type="indexterm" id="ch05-pip"/><a contenteditable="false" data-primary="pipelines" data-type="indexterm" id="ch05-pip2"/><a contenteditable="false" data-primary="tasks" data-secondary="pipelines" data-type="indexterm" id="idm45358191467976"/>creating a collection of tasks and ordering the execution of the group of tasks. In some cases, tasks have dependencies on other tasks and must declare that they execute after the tasks they are dependent on. In other cases, tasks may not have dependencies on one another, and those tasks can run concurrently. The pipelines construct manages its collection of tasks and the order in which they execute, as well as the common shared workspaces that each task is entitled to use. <a contenteditable="false" data-primary="pipelines" data-secondary="Tekton CI/CD system" data-type="indexterm" id="idm45358191465976"/><a contenteditable="false" data-primary="YAML files" data-secondary="tasks with steps" data-tertiary="pipelines" data-type="indexterm" id="ch05-pip3"/>To better understand how pipelines work, we need at least two tasks. In the previous section, we defined and deployed the <code>simple-task-example</code> task. We will now create a second task called <code>simple-task-example2</code>. This task is shown here:</p>&#13;
<pre data-type="programlisting">apiVersion: tekton.dev/v1beta1&#13;
kind: Task&#13;
metadata:&#13;
 name: simple-task-example2&#13;
spec:&#13;
 steps:&#13;
 - name: echo&#13;
 image: ubuntu&#13;
 command:&#13;
 - echo&#13;
 args:&#13;
 - "This is step one of our second simple task example"&#13;
 - name: echo2&#13;
 image: ubuntu&#13;
 command:&#13;
 - echo&#13;
 args:&#13;
 - "This is step two of our second simple task example"</pre>&#13;
<p>To deploy this task, save the previous example in a file called <em>simpleexample2.yaml</em>. Deploy this task by running the following command:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl apply -f simpleexample2.yaml</strong></pre>&#13;
<p>You may notice that with our second task example, we did not provide a corresponding <code>TaskRun</code> for running it. The reason we don’t have a second <code>TaskRun</code> is because we are going to group both tasks into a pipeline, and the pipeline will be responsible for creating any <code>TaskRun</code> objects it needs to run its tasks.</p>&#13;
<p>The pipeline that we are using for this example is named <code>simple-pipeline</code>, and it declares that it manages two tasks, which it references as <code>simple-task-example</code> and <code>simple-task-example2</code> respectively:</p>&#13;
<pre data-type="programlisting">apiVersion: tekton.dev/v1beta1&#13;
kind: Pipeline&#13;
metadata:&#13;
 name: simple-pipeline&#13;
spec:&#13;
 tasks:&#13;
 - name: simple-task&#13;
   taskRef:&#13;
    name: simple-task-example&#13;
 - name: simple-task2&#13;
   taskRef:&#13;
    name: simple-task-example2</pre>&#13;
<p>To run this pipeline, first save the previous example in a file called <em>simplepipeline.yaml</em>. Deploy this pipeline using the following command:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl apply -f simplepipeline.yaml</strong></pre>&#13;
<p>Next, we will create a <code>PipelineRun</code> object that is responsible for running this pipeline on a Kubernetes cluster:</p>&#13;
<pre data-type="programlisting">apiVersion: tekton.dev/v1beta1&#13;
kind: PipelineRun&#13;
metadata:&#13;
 generateName: run-simple-tasks&#13;
spec:&#13;
 pipelineRef:&#13;
  name: simple-pipeline</pre>&#13;
<p>Save the previous file as <em>simplepipelinerun.yaml</em>. You can then run the <code>simple-pipeline</code> example on the cluster by running the following command:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl create -f simplepipelinerun.yaml</strong></pre>&#13;
<p>To confirm that the pipeline ran properly, use the <code>tkn pipelinerun logs --last -f</code> <a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="pipeline logs" data-type="indexterm" id="idm45358191423672"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="pipeline logs" data-type="indexterm" id="idm45358191421960"/>Tekton command and view the logs from the <code>simple-pipeline</code> example that you just ran:</p>&#13;
<pre data-type="programlisting">$ <strong>tkn pipelinerun logs --last -f</strong>&#13;
[simple-task2 : echo] This is step one of our second simple task example&#13;
[simple-task : echo] This is step one of our first simple task example&#13;
 &#13;
[simple-task2 : echo2] This is step two of our second simple task example&#13;
 &#13;
 &#13;
[simple-task : echo2] This is step two of our first simple task example</pre>&#13;
<p>Upon reviewing the log output, we notice that the two tasks ran concurrently. This is consistent with how we defined our tasks. If the two tasks had a dependency and we needed the second task to defer execution until after the first task completed, the Tekton pipeline supports this by adding a <code>runAfter</code> declaration to the second task that states it should be run after the first task completes. This is shown in the following example:</p>&#13;
<pre data-type="programlisting">apiVersion: tekton.dev/v1beta1&#13;
kind: Pipeline&#13;
metadata:&#13;
 name: simple-pipeline&#13;
spec:&#13;
 tasks:&#13;
 - name: simple-task&#13;
   taskRef:&#13;
    name: simple-task-example&#13;
 - name: simple-task2&#13;
   runAfter: &#13;
   - simple-task&#13;
  taskRef:&#13;
   name: simple-task-example2</pre>&#13;
<p>Note that the <code>runAfter</code> field explicitly references the task named <code>simple-task</code> as the task that must be completed before <code>simple-task2</code> is permitted to execute.</p>&#13;
<p>Tekton has a large number of useful features that are beyond the scope of this book. <a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="documentation online" data-type="indexterm" id="idm45358191413704"/><a contenteditable="false" data-primary="pipelines" data-secondary="Tekton CI/CD system" data-tertiary="documentation online" data-type="indexterm" id="idm45358191412232"/>For more information on Tekton and its capabilities, see the <a href="https://oreil.ly/nw5yD">Tekton Pipeline documentation</a>. In addition, Tekton pipelines serve as the foundation for OpenShift Pipelines, an offering available from Red Hat.<a contenteditable="false" data-primary="" data-startref="ch05-tek" data-type="indexterm" id="idm45358191409592"/><a contenteditable="false" data-primary="" data-startref="ch05-tek2" data-type="indexterm" id="idm45358191408248"/><a contenteditable="false" data-primary="" data-startref="ch05-pip" data-type="indexterm" id="idm45358191406872"/><a contenteditable="false" data-primary="" data-startref="ch05-tek3" data-type="indexterm" id="idm45358191405496"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="OpenShift Pipelines" data-type="sect1"><div class="sect1" id="openshift_pipelines">&#13;
<h1>OpenShift Pipelines</h1>&#13;
<p><a href="https://oreil.ly/VU8Gr">OpenShift Pipelines</a> <a contenteditable="false" data-primary="pipelines" data-secondary="OpenShift Pipelines installation" data-type="indexterm" id="ch05-pip5"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="Pipelines" data-tertiary="installation" data-type="indexterm" id="ch05-pip4"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="pipelines" data-tertiary="OpenShift Pipelines installation" data-type="indexterm" id="ch05-pip7"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="installation" data-type="indexterm" id="ch05-tekin"/>is a fully supported software offering from Red Hat based on Tekton. Using the operator paradigm simplifies the configuration of Tekton and helps you get to value from the ecosystem faster.</p>&#13;
<p>Let’s look at an end-to-end example using OpenShift Pipelines, which is <a contenteditable="false" data-primary="Git repositories" data-secondary="OpenShift Pipelines" data-type="indexterm" id="ch05-git"/><a contenteditable="false" data-primary="GitHub" data-secondary="OpenShift Pipelines" data-type="indexterm" id="ch05-git2"/><a contenteditable="false" data-primary="PAC-MAN via OpenShift Pipelines" data-type="indexterm" id="ch05-pip6"/>available from the <a href="https://oreil.ly/X749O">GitHub organization associated with this book</a>. In this example, you’ll see the Tekton concepts of tasks and pipelines put to work to build a simple app that lets you play PAC-MAN in your browser.</p>&#13;
<p>Configure OpenShift Pipelines by installing the operator from the <a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-tertiary="PAC-MAN via OpenShift Pipelines" data-type="indexterm" id="idm45358191386840"/><a contenteditable="false" data-primary="OperatorHub" data-secondary="PAC-MAN via OpenShift Pipelines" data-type="indexterm" id="idm45358191385096"/><a contenteditable="false" data-primary="operators" data-secondary="custom resource creation" data-tertiary="PAC-MAN via OpenShift Pipelines" data-type="indexterm" id="idm45358191383704"/>OperatorHub catalog available within your OpenShift Container Platform web console (4.4 or greater):</p>&#13;
<ol>&#13;
<li><p>Navigate to Operators &gt; OperatorHub and search for “OpenShift Pipelines,” as shown in <a data-type="xref" href="#operatorhub_catalog_filtered_by_the_quer">Figure 5-1</a>.</p>&#13;
<figure><div class="figure" id="operatorhub_catalog_filtered_by_the_quer">&#13;
<img src="assets/hcok_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>OperatorHub catalog filtered by the query “OpenShift Pipelines”</h6>&#13;
</div></figure></li>&#13;
<li><p>Click on the tile to display information about the operator. Scroll to the <a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="downloading" data-type="indexterm" id="idm45358191376984"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="downloading" data-type="indexterm" id="idm45358191375272"/>bottom of the page to download the appropriate command-line version for your platform (see <a data-type="xref" href="#download_the_cli_for_tekton_left_parenth">Figure 5-2</a>).</p>&#13;
<figure class="width-75"><div class="figure" id="download_the_cli_for_tekton_left_parenth">&#13;
<img src="assets/hcok_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>Download the CLI for Tekton (<code>tkn</code>) for your platform</h6>&#13;
</div></figure></li>&#13;
<li><p>Click Install.</p></li>&#13;
<li><p>Select the appropriate update channel for your version of OpenShift. For example, if you’re running OCP 4.4.x, use ocp-4.4.</p></li>&#13;
<li><p>Click Subscribe.</p></li>&#13;
</ol>&#13;
<p>You can confirm that the installation was successful by navigating to Operators &gt; Installed Operator and filtering the project to “openshift-operators,” as shown in <a data-type="xref" href="#confirm_successful_installation_of_the_o">Figure 5-3</a>.</p>&#13;
<figure><div class="figure" id="confirm_successful_installation_of_the_o">&#13;
<img src="assets/hcok_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>Confirm successful installation of the OpenShift Pipelines Operator</h6>&#13;
</div></figure>&#13;
<p>When assembling your continuous delivery solution with Tekton Tasks, you have access to a broad community library of existing tasks:<a contenteditable="false" data-primary="" data-startref="ch05-tekin" data-type="indexterm" id="idm45358191363912"/></p>&#13;
<ul>&#13;
<li><p><a href="https://oreil.ly/INXgA">Tekton task catalog</a></p></li>&#13;
<li><p><a href="https://oreil.ly/z3KwI">OpenShift Pipelines task catalog</a></p></li>&#13;
</ul>&#13;
<p>Let’s reuse some of the tasks in the public catalogs to put together our <a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="cluster tasks" data-type="indexterm" id="idm45358191359720"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="cluster tasks" data-type="indexterm" id="idm45358191358040"/>working example. You will also have a set of tasks available after installing the operator:</p>&#13;
<pre data-type="programlisting">$ <strong>tkn clustertasks ls</strong>&#13;
NAME DESCRIPTION AGE&#13;
buildah 25 minutes ago&#13;
buildah-v0-11-3 25 minutes ago&#13;
git-clone 25 minutes ago&#13;
jib-maven 25 minutes ago&#13;
kn 25 minutes ago&#13;
maven 25 minutes ago&#13;
openshift-client 25 minutes ago&#13;
openshift-client-v0-11-3 25 minutes ago&#13;
s2i 25 minutes ago&#13;
s2i-dotnet-3 25 minutes ago&#13;
s2i-dotnet-3-v0-11-3 25 minutes ago&#13;
s2i-go 25 minutes ago&#13;
s2i-go-v0-11-3 25 minutes ago&#13;
s2i-java-11 25 minutes ago&#13;
s2i-java-11-v0-11-3 25 minutes ago&#13;
s2i-java-8 25 minutes ago&#13;
s2i-java-8-v0-11-3 25 minutes ago&#13;
s2i-nodejs 25 minutes ago&#13;
s2i-nodejs-v0-11-3 25 minutes ago&#13;
s2i-perl 25 minutes ago&#13;
s2i-perl-v0-11-3 25 minutes ago&#13;
s2i-php 25 minutes ago&#13;
s2i-php-v0-11-3 25 minutes ago&#13;
s2i-python-3 25 minutes ago&#13;
s2i-python-3-v0-11-3 25 minutes ago&#13;
s2i-ruby 25 minutes ago&#13;
s2i-ruby-v0-11-3 25 minutes ago&#13;
s2i-v0-11-3 25 minutes ago&#13;
tkn 25 minutes ago</pre>&#13;
<p>We will now create a pipeline to build our image and publish to the in-cluster registry. First, create a project in your OpenShift cluster to hold the resources we are about to create:</p>&#13;
<pre data-type="programlisting">$ <strong>oc new-project pipelines-tutorial</strong></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>All of the following examples assume that you are working in this same <a contenteditable="false" data-primary="namespaces" data-secondary="KUBECONFIG context via oc" data-type="indexterm" id="idm45358191351336"/><a contenteditable="false" data-primary="KUBECONFIG context via oc" data-type="indexterm" id="idm45358191349864"/><a contenteditable="false" data-primary="oc command-line tool" data-secondary="KUBECONFIG context" data-type="indexterm" id="idm45358191348744"/>namespace, <code>pipelines-tutorial</code>. If for some reason your <code>KUBECONFIG</code> is referencing a different namespace, then you can use the <code>oc project</code> command to update your context:</p>&#13;
<pre data-type="programlisting">$ <strong>oc project pipelines-tutorial</strong></pre>&#13;
<p>Alternatively, you can add the <code>-n pipelines-tutorial</code> flag to the example commands after <code>oc apply</code>, <code>oc create</code>, <code>oc get</code>, and so on. For example:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get pipelines -n pipelines-tutorial</strong></pre>&#13;
</div>&#13;
<p>To build and publish an image, the default service account <code>pipeline</code> must have the <a contenteditable="false" data-primary="registries" data-see="image registries" data-type="indexterm" id="idm45358191340408"/><a contenteditable="false" data-primary="container images" data-secondary="image registries" data-see="image registries" data-type="indexterm" id="idm45358191339000"/><a contenteditable="false" data-primary="image registries" data-secondary="Quay.io" data-type="indexterm" id="idm45358191337352"/><a contenteditable="false" data-primary="service accounts" data-secondary="authorization via registries" data-type="indexterm" id="idm45358191335976"/><a contenteditable="false" data-primary="Quay.io registry" data-type="indexterm" id="idm45358191334584"/>authorization to push an image into your destination registry. For this example, the <a href="https://quay.io">Quay.io</a> registry is used, but any registry will work fine.</p>&#13;
<p>To enable the authorization for the <code>pipeline</code> service account,<a contenteditable="false" data-primary="secrets" data-secondary="service account authorization" data-type="indexterm" id="idm45358191331960"/> you must create a <code>docker-registry</code> secret and update the <code>pipeline</code> service account. These steps are not related to Tekton specifically but are relevant to our example:</p>&#13;
<ol class="pagebreak-before less_space">&#13;
<li><p>Create the Quay.io image repository named <code>quay.io/<em>&lt;username&gt;</em>/pacman</code> for your username by following the <a href="https://oreil.ly/N3Ng3">Quay.io documentation</a>.</p></li>&#13;
<li><p>Download the Kubernetes secret from Quay.io. You can access the secret from the Settings page under “Generate Encrypted Password.”</p></li>&#13;
<li><p>Apply the secret (and be sure to update the default name of <code><em>&lt;username&gt;</em>-pull-secret to quay-registry-secret</code>) or use the <code>kubectl</code> or <code>oc</code> command line to create the secret:<a contenteditable="false" data-primary="kubectl command-line tool" data-secondary="secret creation" data-type="indexterm" id="idm45358191323304"/><a contenteditable="false" data-primary="oc command-line tool" data-secondary="secret creation" data-type="indexterm" id="idm45358191321864"/></p>&#13;
<pre data-type="programlisting">$ <strong>kubectl create secret docker-registry \</strong>&#13;
 --docker-server="quay.io" \&#13;
 --docker-username="YOUR_USERNAME" \&#13;
 --docker-password="YOUR_PASSWORD" \&#13;
 --docker-email="YOUR_EMAIL" \&#13;
 quay-registry-secret</pre></li>&#13;
<li><p>Patch the <code>quay-registry-secret</code> into the <code>pipeline</code> service account. <a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="tasks" data-tertiary="TaskRun resources authorization" data-type="indexterm" id="idm45358191317544"/><a contenteditable="false" data-primary="tasks" data-secondary="TaskRun resources" data-tertiary="authorization" data-type="indexterm" id="idm45358191315848"/>The OpenShift pipelines operator automatically creates the <code>pipeline</code> service account in every namespace of your cluster. By updating the service account in the <code>pipelines-tutorial</code> namespace, you will allow any Tekton <code>TaskRun</code>s to leverage this authorization for pushing images:<a contenteditable="false" data-primary="" data-startref="ch05-pip4" data-type="indexterm" id="idm45358191312568"/><a contenteditable="false" data-primary="" data-startref="ch05-pip7" data-type="indexterm" id="idm45358191311192"/></p>&#13;
<pre data-type="programlisting">$ <strong>oc patch sa pipeline -p '{"secrets":[{"name":"quay-registry-secret"}]}'</strong></pre></li>&#13;
</ol>&#13;
<p>We will start creating a pipeline that will build an image and push it into the image repository that you just defined:<a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="Pipelines" data-tertiary="creating and running" data-type="indexterm" id="ch05-crn"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="pipelines" data-tertiary="OpenShift Pipelines" data-type="indexterm" id="ch05-crn2"/></p>&#13;
<pre data-type="programlisting">pipelines/01-pipeline-build-pacman.yaml &#13;
 &#13;
apiVersion: tekton.dev/v1beta1&#13;
kind: Pipeline&#13;
metadata:&#13;
 name: build-pacman&#13;
spec:&#13;
 workspaces:&#13;
 - name: shared-workspace&#13;
 resources:&#13;
 - name: source-repo&#13;
 type: git&#13;
 - name: image&#13;
 type: image&#13;
 params:&#13;
 - name: dockerfile-path&#13;
 type: string&#13;
 description: The path to your Dockerfile&#13;
 default: "Dockerfile"&#13;
 tasks:&#13;
 - name: build-image&#13;
 taskRef:&#13;
 name: buildah&#13;
 kind: ClusterTask&#13;
 resources:&#13;
 inputs:&#13;
 - name: source&#13;
 resource: source-repo&#13;
 outputs:&#13;
 - name: image&#13;
 resource: image&#13;
 params:&#13;
 - name: TLSVERIFY&#13;
 value: "false"&#13;
 - name: DOCKERFILE&#13;
 value: "$(params.dockerfile-path)"</pre>&#13;
<p>The <code>build-pacman</code> Pipeline defines a single task that uses the <code>buildah ClusterTask</code>. The input requires a Git repository with the <code>Dockerfile</code> and associated source files that are required and the details of the image to build.</p>&#13;
<p>We create the pipeline using the <code>oc</code> command-line tool:</p>&#13;
<pre data-type="programlisting">$ <strong>oc apply -f 01-pipeline-build-pacman.yaml</strong> &#13;
pipeline.tekton.dev/build-and-deploy-pacman created</pre>&#13;
<p>After applying the pipeline definition, we can verify it exists:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get pipelines</strong>&#13;
NAME                    AGE&#13;
build-and-deploy-pacman 8s</pre>&#13;
<p>The <code>tkn</code> command-line tool offers a specific set of actions for Tekton resources.<a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="resource commands" data-type="indexterm" id="idm45358191296600"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="resource commands" data-type="indexterm" id="idm45358191294888"/> In addition to equivalents for commands like <code>get</code> or <code>describe</code>, there are direct commands to view task logs and other Tekton-specific behaviors:</p>&#13;
<pre data-type="programlisting">$ <strong>tkn pipelines ls</strong>&#13;
NAME                    AGE            LAST RUN STARTED DURATION STATUS&#13;
build-and-deploy-pacman 21 seconds ago ---      ---     ---      ---</pre>&#13;
<p>Tekton pipelines define parameters that drive their behavior. To simplify the management of parameters, Tekton also defines <code>PipelineResource</code>s that represent different kinds of objects that occur frequently in desired pipeline behavior.</p>&#13;
<p>The following are the defined <a href="https://oreil.ly/KrZlb"><code>PipelineResource</code> types</a>:</p>&#13;
<dl>&#13;
<dt><code>git</code></dt>&#13;
<dd>GitHub repository</dd>&#13;
<dt><code>storage</code></dt>&#13;
<dd>Storage blob</dd>&#13;
<dt><code>image</code></dt>&#13;
<dd>Container image metadata</dd>&#13;
<dt><code>cluster</code></dt>&#13;
<dd>Kubernetes cluster description with access credentials</dd>&#13;
<dt><code>pullRequest</code></dt>&#13;
<dd>A GitHub pull request</dd>&#13;
<dt><code>cloudEvent</code></dt>&#13;
<dd>Cloud Event</dd>&#13;
<dt><code>gcs</code></dt>&#13;
<dd>GCSResource backed by a GCS blob/directory</dd>&#13;
<dt><code>build-gcs</code></dt>&#13;
<dd>BuildGCSResources added to be compatible with Knative build</dd>&#13;
</dl>&#13;
<p>We will create <code>PipelineResource</code>s that will become inputs to the pipeline and fulfill the required values for the input parameters:</p>&#13;
<pre data-type="programlisting">pipelines/02-resource-git-repo-pacman.yaml&#13;
 &#13;
apiVersion: tekton.dev/v1alpha1&#13;
kind: PipelineResource&#13;
metadata:&#13;
 name: pacman-git&#13;
spec:&#13;
 type: git&#13;
 params:&#13;
 - name: revision&#13;
 value: master&#13;
 - name: url&#13;
 value: https://github.com/hybrid-cloud-apps-openshift-k8s-book/k8s-example-apps/&#13;
 &#13;
pipelines/03-resource-pacman-image.yaml&#13;
 &#13;
apiVersion: tekton.dev/v1alpha1&#13;
kind: PipelineResource&#13;
metadata:&#13;
 name: pacman-image&#13;
spec:&#13;
 type: image&#13;
 params:&#13;
 - name: url&#13;
 value: quay.io/mdelder/pacman</pre>&#13;
<p>We will apply these resources and then reference them in our <code>PipelineRun</code>:</p>&#13;
<pre data-type="programlisting">$ <strong>oc apply -f 02-resource-git-repo-pacman.yaml \</strong>&#13;
 <strong>-f 03-resource-pacman-image.yaml</strong></pre>&#13;
<p>Now we have a pipeline and we have inputs (our Git repo and our desired image to build). Let’s trigger the pipeline by creating a <code>PipelineRun</code>:</p>&#13;
<pre data-type="programlisting">pipelines/04-pipelinerun-build-pacman-01.yaml&#13;
 &#13;
apiVersion: tekton.dev/v1beta1&#13;
kind: PipelineRun&#13;
metadata:&#13;
 generateName: pipelinerun-build-pacman-&#13;
spec:&#13;
 serviceAccountName: pipeline&#13;
 pipelineRef:&#13;
 name: build-pacman&#13;
 resources:&#13;
 - name: source-repo&#13;
 resourceRef:&#13;
 name: pacman-git&#13;
 - name: image&#13;
 resourceRef:&#13;
 name: pacman-image&#13;
 workspaces:&#13;
 - name: shared-workspace&#13;
 emptyDir: {}&#13;
 params:&#13;
 - name: dockerfile-path&#13;
 value: "pacman-nodejs-app/docker/Dockerfile"</pre>&#13;
<p>The <code>PipelineRun</code> will carry out two actions with a single <code>buildah</code> task: clone the Git repository, and then build the image and publish it to the Quay.io registry that you previously created.</p>&#13;
<p>To run the pipeline, use <code>oc create</code>:</p>&#13;
<pre data-type="programlisting">$ <strong>oc create -f pipelines/04-pipelinerun-build-pacman-01.yaml</strong></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>We are using <code>oc create</code> here instead of <code>oc apply</code> because the <span class="keep-together"><code>PipelineRun</code></span> uses a <code>generateName</code> instead of a <code>name</code> attribute. The <code>oc apply</code> command requires the <code>name</code> attribute, whereas <code>oc create</code> supports additional behavior to generate a suffix for the name automatically.</p>&#13;
</div>&#13;
<p>You can see the running pipelines with the <code>tkn</code> command-line tool:<a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="pipelines run" data-type="indexterm" id="idm45358191263160"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="pipelines run" data-type="indexterm" id="idm45358191261512"/></p>&#13;
<pre data-type="programlisting">$ <strong>tkn pipelinerun ls</strong>&#13;
NAME                           STARTED        DURATION STATUS&#13;
pipelinerun-build-pacman-qk5lw 23 seconds ago ---      Running</pre>&#13;
<p>You can follow along with the <code>PipelineRun</code> using the <code>tkn</code> command line:</p>&#13;
<pre data-type="programlisting">$ <strong>tkn pipelinerun logs -f</strong></pre>&#13;
<p>The output should resemble the following:</p>&#13;
<pre data-type="programlisting">[build-image : git-source-pacman-git-s2mxf]&#13;
{"level":"info","ts":1598817082.1290805,"caller":"git/git.go:105","msg":&#13;
"Successfully cloned https://github.com/hybrid-cloud-apps-openshift-k8s-book/&#13;
k8s-example-apps/ @ master in path /workspace/source"}&#13;
...&#13;
 &#13;
[build-image : build] STEP 1: FROM node:boron&#13;
[build-image : build] Getting image source signatures&#13;
[build-image : build] Copying blob &#13;
sha256:3b7ca19181b24b87e24423c01b490633bc1e47d2fcdc1987bf2e37949d6789b5&#13;
 &#13;
...&#13;
 &#13;
[build-image : push] Getting image source signatures&#13;
[build-image : push] Copying blob &#13;
sha256:ec62f19bb3aa1dcfacc9864be06f0af635c18021893d42598da1564beed97448&#13;
 &#13;
...&#13;
 &#13;
[build-image : push] Writing manifest to image destination&#13;
[build-image : push] Copying config &#13;
sha256:854daaf20193c74d16a68ba8c1301efa4d02e133d383f04fedc9532ae34e8929&#13;
[build-image : push] Writing manifest to image destination&#13;
[build-image : push] Storing signatures&#13;
 &#13;
...</pre>&#13;
<p>In this example, we built the container image. Let’s take it further and apply the change to the cluster. In this step, we will create a pipeline with three distinct stages:<a contenteditable="false" data-primary="Kustomize for configurations" data-secondary="pipelines" data-type="indexterm" id="idm45358191253560"/></p>&#13;
<ol>&#13;
<li><p>Build the application image.</p></li>&#13;
<li><p>Fetch the Git repository that contains our deployment manifests (using <span class="keep-together">Kustomize</span>).</p></li>&#13;
<li><p>Apply Kustomization deployment manifests.</p></li>&#13;
</ol>&#13;
<p>The first step works exactly the same way as before. The additional steps introduce a few new ideas:</p>&#13;
<ul>&#13;
<li><p>We will create a <code>PersistentVolumeClaim</code> to provide available <a contenteditable="false" data-primary="storage volumes" data-secondary="PersistentVolumeClaim" data-type="indexterm" id="idm45358191247112"/><a contenteditable="false" data-primary="PersistentVolumeClaim" data-type="indexterm" id="idm45358191245704"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="storage volume" data-type="indexterm" id="idm45358191244600"/>storage to host the contents of our Git repository. Otherwise, the files retrieved from Git in Step 2 will not be available for use in Step 3.</p></li>&#13;
<li><p>We will require additional permissions for our <code>pipeline</code> service account to allow the deployment manifests to be applied to the application namespace on this cluster.</p></li>&#13;
</ul>&#13;
<p>Let’s create the <code>PersistentVolumeClaim</code>. The <code>PersistentVolumeClaim</code> should request enough capacity for all persistent file system storage required by all tasks in the pipeline. If the <code>PersistentVolume</code> is reclaimed or recycled in between tasks, you may lose important state and the pipeline run will likely fail. On the other hand, if the same <code>PersistentVolume</code> is reused across many pipeline runs, it may eventually exhaust all available space. If the same <code>PersistentVolume</code> is expected to support multiple pipeline runs in parallel, be sure to set the <code>accessMode</code> to <code>ReadWriteMany</code>:</p>&#13;
<pre data-type="programlisting">pipelines/00-pvc-shared-workspace.yaml&#13;
 &#13;
apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
 name: shared-workspace&#13;
spec:&#13;
 accessModes:&#13;
 - ReadWriteOnce&#13;
 resources:&#13;
 requests:&#13;
 storage: 1Gi&#13;
 &#13;
$ <strong>oc apply -f 00-pvc-shared-workspace.yaml</strong> &#13;
persistentvolumeclaim/shared-workspace created</pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>State management of workspaces could become an issue over time. <a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="state management of workspaces" data-type="indexterm" id="idm45358191235224"/>Tekton 0.12 introduces a <code>volumeClaimTemplate</code> that offers to simplify this process. Otherwise, you may always be creating <span class="keep-together"><code>PersistentVolumeClaim</code>s</span> and <code>PersistentVolume</code>s for each <code>PipelineRun</code>. For any resource that you are creating via automation, be sure to define your reclamation strategy to destroy or allow any unnecessary resources to age out as appropriate.</p>&#13;
</div>&#13;
<p>In our first pipeline, we updated the <code>system:serviceaccounts:pipelines-tutorial:pipeline</code> service account to allow the use of an additional secret that authorized our service account to push images into our Quay.io image registry. In our second pipeline, our service account will apply deployment manifests to the same cluster running the pipeline and will require authorization to the application <span class="keep-together">namespace</span>:</p>&#13;
<pre data-type="programlisting">$ <strong>oc adm policy add-role-to-user edit --namespace pacman \</strong>&#13;
 <strong>system:serviceaccount:pipelines-tutorial:pipeline</strong></pre>&#13;
<p>With the <code>edit ClusterRoleBinding</code> to the <code>pacman</code> namespace, the service account will be able to create, modify, and view most of the Kubernetes API objects, including deployments, services, and OpenShift routes. Our chosen example application creates each of these as part of its deployment manifests.</p>&#13;
<p>To verify that you have applied the permission correctly, you can use the <code>can-i</code> command, which will print a simple “yes” or “no” answer:</p>&#13;
<pre data-type="programlisting">$ <strong>oc auth can-i get deployments \</strong>&#13;
 <strong>--namespace pacman \</strong>&#13;
 <strong>--as system:serviceaccount:pipelines-tutorial:pipeline</strong></pre>&#13;
<p>Now we will create our new pipeline:</p>&#13;
<pre data-type="programlisting">pipelines/05-pipeline-deploy-pacman.yaml&#13;
 &#13;
apiVersion: tekton.dev/v1beta1&#13;
kind: Pipeline&#13;
metadata:&#13;
 name: build-and-deploy-pacman&#13;
spec:&#13;
 workspaces:&#13;
 - name: shared-workspace&#13;
 resources:&#13;
 - name: source-repo&#13;
 type: git&#13;
 - name: image&#13;
 type: image&#13;
 params:&#13;
 - name: kustomization-path&#13;
 type: string&#13;
 default: kustomization&#13;
 - name: kustomization-git-repo-url&#13;
 type: string&#13;
 - name: kustomization-git-revision&#13;
 type: string&#13;
 default: master&#13;
 - name: dockerfile-path&#13;
 type: string&#13;
 description: The path to your Dockerfile&#13;
 default: "Dockerfile"&#13;
 tasks:&#13;
 - name: build-image&#13;
 taskRef:&#13;
 name: buildah&#13;
 kind: ClusterTask&#13;
 resources:&#13;
 inputs:&#13;
 - name: source&#13;
 resource: source-repo&#13;
 outputs:&#13;
 - name: image&#13;
 resource: image&#13;
 params:&#13;
 - name: TLSVERIFY&#13;
 value: "false"&#13;
 - name: DOCKERFILE&#13;
 value: "$(params.dockerfile-path)"&#13;
 - name: fetch-repository&#13;
 taskRef:&#13;
 name: git-clone&#13;
 kind: ClusterTask&#13;
 workspaces:&#13;
 - name: output&#13;
 workspace: shared-workspace&#13;
 params:&#13;
 - name: url&#13;
 value: "$(params.kustomization-git-repo-url)"&#13;
 - name: subdirectory&#13;
 value: ""&#13;
 - name: deleteExisting&#13;
 value: "true"&#13;
 - name: revision&#13;
 value: "$(params.kustomization-git-revision)"&#13;
 runAfter:&#13;
 - build-image&#13;
 - name: apply-config&#13;
 params:&#13;
 - name: kustomization-path&#13;
 value: "$(params.kustomization-path)"&#13;
 workspaces:&#13;
 - name: source&#13;
 workspace: shared-workspace&#13;
 taskSpec:&#13;
 params:&#13;
 - name: kustomization-path&#13;
 default: "kustomization"&#13;
 workspaces:&#13;
 - name: source&#13;
 steps:&#13;
 - name: apply-kustomization&#13;
 image: quay.io/openshift/origin-cli:latest&#13;
 workingDir: /workspace/source&#13;
 command: ['/bin/bash', '-c']&#13;
 args:&#13;
 - |-&#13;
 echo "Applying kustomization in DIR \"$(params.kustomization-path)\""&#13;
 oc apply -k $(params.kustomization-path)&#13;
 runAfter:&#13;
 - fetch-repository&#13;
 &#13;
$ <strong>oc apply -f 05-pipeline-deploy-pacman.yaml</strong></pre>&#13;
<p>We do not need any additional <code>PipelineResource</code>s to run this pipeline. <a contenteditable="false" data-primary="" data-startref="ch05-pip3" data-type="indexterm" id="idm45358191218680"/>In fact, you may notice that the details about the two related Git repositories are managed differently in this pipeline. As you consume different tasks or define your own, you may find slight inconsistencies in how you assemble tasks to accomplish your goals. Specifically, the community <code>git-clone</code> task does not use the <code>git</code> type <code>Pipeline​Res⁠ource</code> but rather accepts the component parts needed to identify the repository URL and revision.<a contenteditable="false" data-primary="" data-startref="ch05-git" data-type="indexterm" id="idm45358191215448"/><a contenteditable="false" data-primary="" data-startref="ch05-git2" data-type="indexterm" id="idm45358191214072"/></p>&#13;
<p>Just as before, we will create a <span class="keep-together"><code>PipelineRun</code></span> and monitor its progress:</p>&#13;
<pre data-type="programlisting">$ <strong>oc create -f 06-pipelinerun-build-and-deploy-pacman-01.yaml</strong> &#13;
pipelinerun.tekton.dev/pipelinerun-build-and-deploy-pacman-cjc7b created</pre>&#13;
<p>Again, you can use the <code>tkn</code> command-line tool to view all <code>PipelineRun</code>s:<a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="pipeline logs" data-type="indexterm" id="idm45358191209032"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="pipelines run" data-type="indexterm" id="idm45358191207288"/><a contenteditable="false" data-primary="Tekton CI/CD system" data-secondary="command-line interface" data-tertiary="pipeline logs" data-type="indexterm" id="idm45358191205640"/><a contenteditable="false" data-primary="command-line interfaces (CLIs)" data-secondary="Tekton CI/CD system" data-tertiary="pipelines run" data-type="indexterm" id="idm45358191203992"/></p>&#13;
<pre data-type="programlisting">$ <strong>tkn pipelinerun ls</strong>&#13;
NAME                                      STARTED        DURATION  STATUS&#13;
pipelinerun-build-and-deploy-pacman-cjc7b 3 minutes ago  2 minutes Succeeded&#13;
pipelinerun-build-pacman-qk5lw            57 minutes ago 2 minutes Succeeded</pre>&#13;
<p>You can review or follow the logs as well. Note that if you run this after the <span class="keep-together"><code>PipelineRun</code></span> has completed, the log order will be reversed:</p>&#13;
<pre data-type="programlisting">$ <strong>tkn pipelinerun logs -f pipelinerun-build-and-deploy-pacman-cjc7b</strong></pre>&#13;
<p>Defining and troubleshooting tasks can be a little error prone at first. Use the API reference and don’t be afraid to delete or recreate the initial pipelines and pipeline runs to resolve reference issues.</p>&#13;
<p class="pagebreak-before less_space">Now we can confirm whether <code>pacman</code> was successfully deployed by opening the route with the web browser (<a data-type="xref" href="#successful_deployment_of_the_pac_man_app">Figure 5-4</a>):<a contenteditable="false" data-primary="" data-startref="ch05-pip2" data-type="indexterm" id="idm45358191195336"/><a contenteditable="false" data-primary="" data-startref="ch05-pip5" data-type="indexterm" id="idm45358191193960"/><a contenteditable="false" data-primary="" data-startref="ch05-pip6" data-type="indexterm" id="idm45358191192584"/><a contenteditable="false" data-primary="" data-startref="ch05-crn" data-type="indexterm" id="idm45358191191208"/><a contenteditable="false" data-primary="" data-startref="ch05-crn2" data-type="indexterm" id="idm45358191189832"/></p>&#13;
<pre data-type="programlisting">$ <strong>oc get route pacman --namespace pacman \</strong>&#13;
 <strong>-ojsonpath="{.status.ingress[0].host}"</strong></pre>&#13;
<figure class="width-90"><div class="figure" id="successful_deployment_of_the_pac_man_app">&#13;
<img src="assets/hcok_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>Successful deployment of the PAC-MAN application</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Open Cluster Management Apps" data-type="sect1"><div class="sect1" id="open_cluster_management_apps">&#13;
<h1 class="less_space">Open Cluster Management Apps</h1>&#13;
<p>The <a href="https://oreil.ly/QbBRN">Open Cluster Management</a> project<a contenteditable="false" data-primary="continuous delivery (CD)" data-secondary="Open Cluster Management apps" data-type="indexterm" id="ch05-ocmp"/><a contenteditable="false" data-primary="Open Cluster Management project" data-type="indexterm" id="ch05-ocmp2"/><a contenteditable="false" data-primary="clusters" data-secondary="Open Cluster Management project" data-type="indexterm" id="ch05-ocmp3"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="Open Cluster Management project" data-type="indexterm" id="ch05-ocmp4"/><a contenteditable="false" data-primary="GitOps for infrastructure management" data-secondary="Open Cluster Management project" data-type="indexterm" id="idm45358191175528"/><a contenteditable="false" data-primary="PAC-MAN via OpenShift Pipelines" data-secondary="Open Cluster Management project" data-type="indexterm" id="ch05-pacm"/> is a new approach to managing applications across one or more OpenShift clusters. The approach applies a native GitOps approach to attaching a Kubernetes object to a Git repository. Let’s take a simple example based on the open source PAC-MAN app.</p>&#13;
<p>The Open Cluster Management project is focused on several aspects of managing Kubernetes clusters, including creating and upgrading clusters, distributed delivery and management of applications, syndicating cluster configuration, and maintaining visibility on the compliance and governance of managed clusters. <a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="hub cluster" data-type="indexterm" id="idm45358191171304"/><a contenteditable="false" data-primary="hub cluster" data-type="indexterm" id="idm45358191169912"/><a contenteditable="false" data-primary="health checking" data-secondary="Open Cluster Management project" data-type="indexterm" id="idm45358191168808"/><a contenteditable="false" data-primary="control planes" data-secondary="hub cluster running" data-type="indexterm" id="idm45358191167416"/>The <em>hub</em> cluster runs the multicluster control plane, and a lightweight agent that runs as a set of pods on the <em>managed clusters</em> applies the desired state to all clusters under management and provides a feedback loop for health, search index, and compliance. We will focus only on the application management concepts for the next example.</p>&#13;
<p>The Open Cluster Management app model relies on the following concepts:<a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="concept definitions" data-type="indexterm" id="idm45358191164280"/></p>&#13;
<dl>&#13;
<dt><code>Application</code></dt>&#13;
<dd>A grouping of related resources required to provide a logical service to a <span class="keep-together">consumer</span>.<a contenteditable="false" data-primary="applications" data-secondary="Open Cluster Management" data-type="indexterm" id="idm45358191160488"/></dd>&#13;
<dt><code>Channel</code></dt>&#13;
<dd>A source of application parts required for deployment. Current supported channels include Git repositories, object store buckets, and Helm repositories.<a contenteditable="false" data-primary="channel per Open Cluster Management" data-type="indexterm" id="idm45358191157912"/></dd>&#13;
<dt><code>Subscription</code></dt>&#13;
<dd>Connects parts of an application from a channel to one or more clusters. Subscriptions consume a range of versions from a release branch (e.g., “latest,” “stable,” “production,” etc.).<a contenteditable="false" data-primary="subscription per Open Cluster Management" data-type="indexterm" id="idm45358191155640"/></dd>&#13;
<dt><code>PlacementRule</code></dt>&#13;
<dd>Links a subscription to one or more clusters.<a contenteditable="false" data-primary="PlacementRules" data-type="indexterm" id="idm45358191153448"/></dd>&#13;
</dl>&#13;
<p><a href="https://oreil.ly/6x2Ba"><em>Red Hat Advanced Cluster Management</em> (RHACM)</a> is<a contenteditable="false" data-primary="Red Hat Advanced Cluster Management (RHACM)" data-secondary="about" data-type="indexterm" id="idm45358191150984"/><a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="Red Hat Advanced Cluster Management" data-tertiary="about" data-type="indexterm" id="idm45358191149512"/><a contenteditable="false" data-primary="RHACM" data-see="Red Hat Advanced Cluster Management" data-type="indexterm" id="idm45358191147832"/> a fully supported software offering that is based on the Open Cluster Management project. Similar to how OpenShift Pipelines simplifies the setup and life cycle of adopting Tekton and other projects, RHACM for Kubernetes simplifies the adoption of the Open Cluster Management project (<a data-type="xref" href="#installation_of_advanced_cluster_managem">Figure 5-5</a>).</p>&#13;
<figure><div class="figure" id="installation_of_advanced_cluster_managem">&#13;
<img src="assets/hcok_0505.png"/>&#13;
<h6><span class="label">Figure 5-5. </span>Installation of Advanced Cluster Management for Kubernetes in OpenShift</h6>&#13;
</div></figure>&#13;
<p>To install RHACM for Kubernetes, follow these steps:<a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="Red Hat Advanced Cluster Management" data-tertiary="installation" data-type="indexterm" id="idm45358191142728"/><a contenteditable="false" data-primary="Red Hat Advanced Cluster Management (RHACM)" data-secondary="installation" data-type="indexterm" id="idm45358191141048"/></p>&#13;
<ol>&#13;
<li><p>Search for the operator by name and click Install.</p></li>&#13;
<li><p>Once the operator is installed, create an instance of the <code>MultiClusterHub</code> API:</p>&#13;
<pre data-type="programlisting">$ <strong>oc new-project open-cluster-management</strong>&#13;
 &#13;
$ <strong>oc create -f - &lt;&lt;EOF</strong>&#13;
<strong>apiVersion: operator.open-cluster-management.io/v1</strong>&#13;
<strong>kind: MultiClusterHub</strong>&#13;
<strong>metadata:</strong>&#13;
<strong> namespace: open-cluster-management</strong>&#13;
<strong> name: multiclusterhub</strong>&#13;
<strong>spec: {}</strong>&#13;
<strong>EOF</strong></pre></li>&#13;
<li><p>From the Applications list in the OpenShift Container Platform web console, click the new item to open the RHACM web console, as shown in <a data-type="xref" href="#opening_the_rhacm_web_console_in_openshi">Figure 5-6</a>. You may need to refresh your web browser for this new link to appear.</p>&#13;
<figure><div class="figure" id="opening_the_rhacm_web_console_in_openshi">&#13;
<img src="assets/hcok_0506.png"/>&#13;
<h6><span class="label">Figure 5-6. </span>Opening the RHACM web console in OpenShift</h6>&#13;
</div></figure></li>&#13;
</ol>&#13;
<p>The example assumes that you have created or imported two clusters in Advanced Cluster Management with the following labels:<a contenteditable="false" data-primary="Red Hat Advanced Cluster Management (RHACM)" data-secondary="example" data-type="indexterm" id="ch05-exm"/><a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="Red Hat Advanced Cluster Management" data-tertiary="example" data-type="indexterm" id="ch05-exm2"/></p>&#13;
<pre data-type="programlisting">Cluster 1:&#13;
 apps/pacman: deployed&#13;
 environment: dev&#13;
 region: us-east&#13;
 &#13;
Cluster 2:&#13;
 apps/pacman: deployed&#13;
 environment: dev&#13;
 region: europe-west3</pre>&#13;
<p>For reference, we will assume the following two managed clusters: “cluster1-aws-east” and “cluster3-gcp-europe-west3.” Notice that one cluster (“cluster1-aws-east”) is provisioned on Amazon in North America, while the second (“cluster3-gcp-europe-west3”) is provisioned on Google in Europe (see <a data-type="xref" href="#using_the_rhacm_web_console_to_manage_bo">Figure 5-7</a>). So for this example, we’re deploying our app to a multicluster and multicloud platform backed by <span class="keep-together">OpenShift</span>!</p>&#13;
<figure><div class="figure" id="using_the_rhacm_web_console_to_manage_bo">&#13;
<img src="assets/hcok_0507.png"/>&#13;
<h6><span class="label">Figure 5-7. </span>Using the RHACM web console to manage both Amazon- and Google-provisioned clusters</h6>&#13;
</div></figure>&#13;
<p>We can display these managed clusters from the command line as well:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get managedclusters -o yaml</strong>&#13;
apiVersion: v1&#13;
items:&#13;
- apiVersion: cluster.open-cluster-management.io/v1&#13;
 kind: ManagedCluster&#13;
 metadata:&#13;
 labels:&#13;
 apps/pacman: deployed&#13;
 cloud: Amazon&#13;
 clusterID: 7de6ab45-58ac-47f7-897d-b742b7197653&#13;
 environment: dev&#13;
 name: cluster1-aws-east&#13;
 region: us-east&#13;
 vendor: OpenShift&#13;
 name: cluster1-aws-east&#13;
 spec:&#13;
 hubAcceptsClient: true&#13;
 leaseDurationSeconds: 60&#13;
 status:&#13;
 ...&#13;
 version:&#13;
 kubernetes: v1.18.3+b0068a8&#13;
- apiVersion: cluster.open-cluster-management.io/v1&#13;
 kind: ManagedCluster&#13;
 metadata:&#13;
 labels:&#13;
 apps/pacman: deployed&#13;
 cloud: Google&#13;
 clusterID: 9e170dd8-a463-44c7-a59f-39b7459964ec&#13;
 environment: dev&#13;
 name: cluster3-gcp-europe-west3&#13;
 region: europe-west3&#13;
 vendor: OpenShift&#13;
 name: cluster3-gcp-europe-west3&#13;
 spec:&#13;
 hubAcceptsClient: true&#13;
 leaseDurationSeconds: 60&#13;
 status:&#13;
 ...&#13;
 version:&#13;
 kubernetes: v1.18.3+b0068a8&#13;
kind: List&#13;
metadata:&#13;
 resourceVersion: ""&#13;
 selfLink: ""</pre>&#13;
<p>We start by defining our application and referencing the<a contenteditable="false" data-primary="application per Open Cluster Management" data-type="indexterm" id="idm45358191116808"/><a contenteditable="false" data-primary="subscription per Open Cluster Management" data-type="indexterm" id="idm45358191115560"/> <code>Subscription</code> kind that will make up the application:</p>&#13;
<pre data-type="programlisting">apiVersion: app.k8s.io/v1beta1&#13;
kind: Application&#13;
metadata:&#13;
 name: pacman-app&#13;
 namespace: pacman-app&#13;
spec:&#13;
 componentKinds:&#13;
 - group: apps.open-cluster-management.io&#13;
 kind: Subscription&#13;
 descriptor: {}&#13;
 selector:&#13;
 matchExpressions:&#13;
 - key: app.kubernetes.io/name&#13;
 operator: In&#13;
 values:&#13;
 - pacman</pre>&#13;
<p>The application provides a way to group a set of related parts into a logical unit for management. As of the current project readiness, the application is used to understand the delivery of parts to different managed clusters. Work is underway to also use the application to aggregate health information and summarize the readiness of the complete application for all supporting clusters where the application or its parts are deployed.</p>&#13;
<p>Now let’s define the channel and subscription to attach our application<a contenteditable="false" data-primary="channel per Open Cluster Management" data-type="indexterm" id="idm45358191111576"/> to one or more clusters. The channel simply references the Git repository for our application:</p>&#13;
<pre data-type="programlisting">apiVersion: apps.open-cluster-management.io/v1&#13;
kind: Channel&#13;
metadata:&#13;
 name: pacman-app-latest&#13;
 namespace: pacman-app&#13;
 annotations:&#13;
 apps.open-cluster-management.io/github-path: kustomization&#13;
spec:&#13;
 type: GitHub&#13;
 pathname: https://github.com/hybrid-cloud-apps-openshift-k8s-book/&#13;
openshift-pipeline-example-pacman.git&#13;
 # secretRef:&#13;
 # name: github-credentials</pre>&#13;
<p>The subscription then references the channel, includes details about the requested branch for application changes, and isolates the relevant directory structure within the Git repository. Subscriptions can further restrict when deployments are allowed by specifying <code>timeWindows</code> that either explicitly allow or block changes to the cluster that are recognized in the source repository.</p>&#13;
<p>Here we see the subscription for the <code>pacman-app</code> with references to the channel defined previously:</p>&#13;
<pre data-type="programlisting">apiVersion: apps.open-cluster-management.io/v1&#13;
kind: Subscription&#13;
metadata:&#13;
 annotations:&#13;
 apps.open-cluster-management.io/git-branch: main&#13;
 apps.open-cluster-management.io/github-path: kustomization&#13;
 name: pacman-app&#13;
 namespace: pacman-app&#13;
 labels:&#13;
 app.kubernetes.io/name: pacman&#13;
spec:&#13;
 channel: pacman-app/pacman-app-latest&#13;
 placement:&#13;
 placementRef:&#13;
 kind: PlacementRule&#13;
 name: pacman-dev-clusters&#13;
 # timewindow:&#13;
 # windowtype: blocked&#13;
 # location: America/Toronto&#13;
 # weekdays: ["Monday","Tuesday","Wednesday","Thursday","Friday"]&#13;
 # hours:&#13;
 # - start: "06:00AM"&#13;
 # end: "05:00PM"</pre>&#13;
<p>The subscription also provides the ability to supply information to the deployment via <code>packageOverrides</code> for Kustomization projects or Helm charts.</p>&#13;
<p>The subscription is then matched to a managed cluster by a<a contenteditable="false" data-primary="PlacementRules" data-type="indexterm" id="idm45358191104296"/> <code>PlacementRule</code>. The <span class="keep-together"><code>PlacementRule</code></span> uses match selectors to identify target clusters that are under management that should host the application.</p>&#13;
<p class="pagebreak-before less_space">In the following example, a <code>PlacementRule</code> defines a selection clause to select at most two clusters that have a region value of <code>us-east</code>, <code>us-west</code>, or <code>europe-west3</code> and include the labels <code>environment=dev</code> and <code>apps/pacman=deployed</code>:</p>&#13;
<pre data-type="programlisting">apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: pacman-dev-clusters&#13;
 namespace: pacman-app&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
 type: ManagedClusterConditionAvailable&#13;
 clusterReplicas: 2&#13;
 clusterSelector:&#13;
 &#13;
 matchExpressions:&#13;
 - key: region&#13;
 operator: In&#13;
 values:&#13;
 - us-east&#13;
 - us-west&#13;
 - europe-west3&#13;
 matchLabels:&#13;
 environment: dev&#13;
 apps/pacman: deployed</pre>&#13;
<p>We can apply all of these API resources from our example project:</p>&#13;
<pre data-type="programlisting">$ <strong>git clone git@github.com:hybrid-cloud-apps-openshift-k8s-book/</strong>&#13;
<strong>openshift-pipeline-example-pacman.git</strong>&#13;
$ <strong>cd openshift-pipeline-example-pacman</strong>&#13;
$ <strong>oc new-project pacman-app</strong>&#13;
$ <strong>oc apply -f deploy/pacman-app.yaml</strong></pre>&#13;
<p>Now let’s see what this would look like if we had two clusters under management. Our first cluster is an OpenShift cluster running on Amazon Elastic Compute Cloud (EC2) in the us-east region. Our second cluster is an OpenShift cluster running on Google Compute Platform in the europe-west3 region. We can inspect any managed clusters in RHACM with the following command:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get managedclusters --show-labels</strong>&#13;
NAME HUB ACCEPTED MANAGED CLUSTER URLS JOINED AVAILABLE AGE LABELS&#13;
local-cluster true True True 55m &#13;
  cloud=Amazon,clusterID=65333a32-ba14-4711-98db-28c2aa0153d6,&#13;
  installer.name=multiclusterhub,installer.&#13;
  namespace=open-&#13;
cluster-management,local-cluster=true,vendor=OpenShift&#13;
&#13;
cluster1-aws-east true True True 52m &#13;
  apps/pacman=deployed,cloud=Amazon,&#13;
  clusterID=7de6ab45-58ac-47f7-897d-&#13;
b742b7197653,environment=dev,&#13;
  name=cluster1-aws-east,region=us-east,vendor=OpenShift&#13;
&#13;
cluster3-gcp-europe-west3 true True True 52m &#13;
  apps/pacman=deployed,cloud=Google,&#13;
  clusterID=9e170dd8-a463-44c7-a59f-&#13;
39b7459964ec,environment=dev,name=cluster3-gcp-europe-west3,&#13;
  region=europe-west3,vendor=OpenShift</pre>&#13;
<p>Our <code>PlacementRule</code> will have identified these two eligible clusters based on the <span class="keep-together"><code>matchLabels</code></span> and <code>matchExpressions</code> that we defined previously:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get placementrule -n pacman-app pacman-dev-clusters -oyamlapiVersion: </strong>&#13;
<strong>apps.open-cluster-management.io/v1</strong>&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: pacman-dev-clusters&#13;
 namespace: pacman-app&#13;
 resourceVersion: "3954663"&#13;
 selfLink: /apis/apps.open-cluster-management.io/v1/namespaces/pacman-&#13;
app/placementrules/pacman-dev-clusters&#13;
 uid: 4baae9ee-520c-407e-9cbd-645465e122ea&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
 type: ManagedClusterConditionAvailable&#13;
 clusterSelector:&#13;
 clusterReplicas: 2&#13;
 matchExpressions:&#13;
 - key: region&#13;
 operator: In&#13;
 values:&#13;
 - us-east&#13;
 - us-west&#13;
 - europe-west3&#13;
 matchLabels:&#13;
 apps/pacman: deployed&#13;
 environment: dev&#13;
status:&#13;
 decisions:&#13;
 - clusterName: cluster1-aws-east&#13;
 clusterNamespace: cluster1-aws-east&#13;
 - clusterName: cluster3-gcp-europe-west3&#13;
 clusterNamespace: cluster3-gcp-europe-west3</pre>&#13;
<p>We can view our application in the Advanced Cluster Management topology view and its relevant parts (described by the subscription) that were deployed to our two managed clusters (identified by the <code>PlacementRule</code>) that originated from our Git repository (identified by the channel). In <a data-type="xref" href="#visualization_of_the_application_in_the">Figure 5-8</a>, we can see the application has exactly one subscription (it could have multiple) that is placed on two clusters.</p>&#13;
<p>We can select elements in the topology to view more information, as shown in <a data-type="xref" href="#displaying_the_details_of_multiple_clust">Figure 5-9</a>.</p>&#13;
<figure><div class="figure" id="visualization_of_the_application_in_the">&#13;
<img src="assets/hcok_0508.png"/>&#13;
<h6><span class="label">Figure 5-8. </span>Visualization of the application in the RHACM topology view</h6>&#13;
</div></figure>&#13;
<figure><div class="figure" id="displaying_the_details_of_multiple_clust">&#13;
<img src="assets/hcok_0509.png"/>&#13;
<h6><span class="label">Figure 5-9. </span>Displaying the details of multiple clusters in the RHACM topology view</h6>&#13;
</div></figure>&#13;
<p>The cluster icon depicted in <a data-type="xref" href="#displaying_the_details_of_a_selected_clu">Figure 5-10</a> shows us the clusters that were selected.</p>&#13;
<figure class="width-90"><div class="figure" id="displaying_the_details_of_a_selected_clu">&#13;
<img src="assets/hcok_0510.png"/>&#13;
<h6><span class="label">Figure 5-10. </span>Displaying the details of a selected cluster in the RHACM topology view</h6>&#13;
</div></figure>&#13;
<p>The deployment icons show us how our deployment is doing and whether it was successfully deployed and is currently healthy on our managed clusters.</p>&#13;
<p>Clicking “Launch resource in Search” will show us details about the <code>pacman</code> deployment across all managed clusters (see <a data-type="xref" href="#display_of_the_pacman_deployment_in_the">Figure 5-11</a>).</p>&#13;
<p>Here we can see our <code>pacman</code> deployment is running on two clusters: cluster3-gcp-europe-west3 and cluster1-aws-east. From here we could further inspect related objects, including the related pods, services, and secrets used by the deployment.</p>&#13;
<p>The powerful search capability allows you to understand your application with a holistic view. At a minimum, you are able to validate that parts of the application are deployed as expected. If a problem arises, these views help you isolate what may be the root cause of an observed failure. Making the information available in a more consumable form helps SREs and developers be more effective in dealing with the complexity of a multicluster or multicloud architecture.<a contenteditable="false" data-primary="" data-startref="ch05-ocmp" data-type="indexterm" id="idm45358191070520"/><a contenteditable="false" data-primary="" data-startref="ch05-ocmp2" data-type="indexterm" id="idm45358191069144"/><a contenteditable="false" data-primary="" data-startref="ch05-ocmp3" data-type="indexterm" id="idm45358191067768"/><a contenteditable="false" data-primary="" data-startref="ch05-ocmp4" data-type="indexterm" id="idm45358191066392"/><a contenteditable="false" data-primary="" data-startref="ch05-exm" data-type="indexterm" id="idm45358191065016"/><a contenteditable="false" data-primary="" data-startref="ch05-exm2" data-type="indexterm" id="idm45358191063640"/><a contenteditable="false" data-primary="" data-startref="ch05-pacm" data-type="indexterm" id="idm45358191062264"/></p>&#13;
<figure><div class="figure" id="display_of_the_pacman_deployment_in_the">&#13;
<img src="assets/hcok_0511.png"/>&#13;
<h6><span class="label">Figure 5-11. </span>Display of the <code>pacman</code> deployment in the RHACM web console</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00010">&#13;
<h1>Summary</h1>&#13;
<p>This chapter provided an overview of several popular tools and methodologies for supporting continuous delivery in production across traditional Kubernetes and OpenShift clusters. We first introduced Helm, which is a popular packaging tool for Kubernetes applications. Next, we described Kustomize, which provides the ability to use your existing Kubernetes YAML files for multiple purposes and configurations while avoiding the use of templates. We then described several popular approaches for supporting continuous delivery pipelines, including GitOps, Razee, Tekton, and Argo CD. Finally, we concluded with an extensive discussion of OpenShift Pipelines and Open Cluster Management tools for deploying and managing OpenShift applications across multiple clusters. With the techniques learned in this chapter, you now have a solid understanding of the most popular and proven continuous delivery options available to you and some hands-on experience managing applications across multiple clusters. In the next chapter, we provide an in-depth examination of the crucial operations of provisioning and upgrading in multicluster environments.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch01fn33"><sup><a href="ch05.html#ch01fn33-marker">1</a></sup> Jeff Regan and Phil Wittrock, “Introducing Kustomize; Template-free Configuration Customization for Kubernetes,” Kubernetes Blog (May 29, 2018), <a href="https://oreil.ly/fli5E"><em class="hyperlink">https://oreil.ly/fli5E</em></a>.</p><p data-type="footnote" id="ch01fn34"><sup><a href="ch05.html#ch01fn34-marker">2</a></sup> Weaveworks, “Guide to GitOps,” <a href="https://oreil.ly/QIQ24"><em class="hyperlink">https://oreil.ly/QIQ24</em></a>.</p><p data-type="footnote" id="ch01fn35"><sup><a href="ch05.html#ch01fn35-marker">3</a></sup> “Introducing the Continuous Delivery Foundation, the New Home for Tekton, Jenkins, Jenkins X, and Spinnaker,” Google Open Source Blog (March 12, 2019), <a href="https://oreil.ly/FvwF1"><em class="hyperlink">https://oreil.ly/FvwF1</em></a>.</p></div></div></section></body></html>