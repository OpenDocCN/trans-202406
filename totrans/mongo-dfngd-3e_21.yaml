- en: Chapter 17\. Sharding Administration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with replica sets, you have a number of options for administering sharded
    clusters. Manual administration is one option. These days it is becoming increasingly
    common to use tools such as Ops Manager and Cloud Manager and the Atlas Database-as-a-Service
    (DBaaS) offering for all cluster administration. In this chapter, we will demonstrate
    how to administer a sharded cluster manually, including:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspecting the cluster’s state: who its members are, where data is held, and
    what connections are open'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding, removing, and changing members of a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Administering data movement and manually moving data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing the Current State
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several helpers available to find out what data is where, what the
    shards are, and what the cluster is doing.
  prefs: []
  type: TYPE_NORMAL
- en: Getting a Summary with sh.status()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`sh.status()` gives you an overview of your shards, databases, and sharded
    collections. If you have a small number of chunks, it will print a breakdown of
    which chunks are where as well. Otherwise it will simply give the collection’s
    shard key and report how many chunks each shard has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once there are more than a few chunks, `sh.status()` will summarize the chunk
    stats instead of printing each chunk. To see all chunks, run `sh.status(true)`
    (the `true` tells `sh.status()` to be verbose).
  prefs: []
  type: TYPE_NORMAL
- en: All the information `sh.status()` shows is gathered from your *config* database.
  prefs: []
  type: TYPE_NORMAL
- en: Seeing Configuration Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the configuration information about your cluster is kept in collections
    in the *config* database on the config servers. The shell has several helpers
    for exposing this information in a more readable way. However, you can always
    directly query the *config* database for metadata about your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Never connect directly to your config servers, as you do not want to take the
    chance of accidentally changing or removing config server data. Instead, connect
    to the *mongos* process and use the *config* database to see its data, as you
    would for any other database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you manipulate config data through *mongos* (instead of connecting directly
    to the config servers), *mongos* will ensure that all of your config servers stay
    in sync and prevent various dangerous actions like accidentally dropping the *config*
    database.
  prefs: []
  type: TYPE_NORMAL
- en: In general, you should not directly change any data in the *config* database
    (exceptions are noted in the following sections). If you change anything, you
    will generally have to restart all of your *mongos* servers to see its effect.
  prefs: []
  type: TYPE_NORMAL
- en: There are several collections in the *config* database. This section covers
    what each one contains and how it can be used.
  prefs: []
  type: TYPE_NORMAL
- en: config.shards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *shards* collection keeps track of all the shards in the cluster. A typical
    document in the *shards* collection might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The shard’s `"_id"` is picked up from the replica set name, so each replica
    set in your cluster must have a unique name.
  prefs: []
  type: TYPE_NORMAL
- en: When you update your replica set configuration (e.g., adding or removing members),
    the `"host"` field will be updated automatically.
  prefs: []
  type: TYPE_NORMAL
- en: config.databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *databases* collection keeps track of all of the databases, sharded and
    not, that the cluster knows about:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If `enableSharding` has been run on a database, `"partitioned"` will be `true`.
    The `"primary"` is the database’s “home base.” By default, all new collections
    in that database will be created on the database’s primary shard.
  prefs: []
  type: TYPE_NORMAL
- en: config.collections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *collections* collection keeps track of all sharded collections (nonsharded
    collections are not shown). A typical document looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The important fields are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"_id"`'
  prefs: []
  type: TYPE_NORMAL
- en: The namespace of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: '`"key"`'
  prefs: []
  type: TYPE_NORMAL
- en: The shard key. In this case, it is a hashed shard key on `"imdbId"`.
  prefs: []
  type: TYPE_NORMAL
- en: '`"unique"`'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates that the shard key is not a unique index. By default, the shard key
    is not unique.
  prefs: []
  type: TYPE_NORMAL
- en: config.chunks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *chunks* collection keeps a record of each chunk in all the collections.
    A typical document in the *chunks* collection looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The most useful fields are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"_id"`'
  prefs: []
  type: TYPE_NORMAL
- en: The unique identifier for the chunk. Generally this is the namespace, shard
    key, and lower chunk boundary.
  prefs: []
  type: TYPE_NORMAL
- en: '`"ns"`'
  prefs: []
  type: TYPE_NORMAL
- en: The collection that this chunk is from.
  prefs: []
  type: TYPE_NORMAL
- en: '`"min"`'
  prefs: []
  type: TYPE_NORMAL
- en: The smallest value in the chunk’s range (inclusive).
  prefs: []
  type: TYPE_NORMAL
- en: '`"max"`'
  prefs: []
  type: TYPE_NORMAL
- en: All values in the chunk are smaller than this value.
  prefs: []
  type: TYPE_NORMAL
- en: '`"shard"`'
  prefs: []
  type: TYPE_NORMAL
- en: Which shard the chunk resides on.
  prefs: []
  type: TYPE_NORMAL
- en: The `"lastmod"` field tracks chunk versioning. For example, if the chunk `"video.movies-imdbId_MinKey"`
    were split into two chunks, we’d want a way of distinguishing the new, smaller
    `"video.movies-imdbId_MinKey"` chunks from their previous incarnation as a single
    chunk. Thus, the first component of the `Timestamp` value reflects the number
    of times a chunk has been migrated to a new shard. The second component of this
    value reflects the number of splits. The `"lastmodEpoch"` field specifies the
    collection’s creation epoch. It is used to differentiate requests for the same
    collection name in the cases where the collection was dropped and immediately
    recreated.
  prefs: []
  type: TYPE_NORMAL
- en: '`sh.status()` uses the *config.chunks* collection to gather most of its information.'
  prefs: []
  type: TYPE_NORMAL
- en: config.changelog
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *changelog* collection is useful for keeping track of what a cluster is
    doing, since it records all of the splits and migrations that have occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Splits are recorded in a document that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `"details"` field gives information about what the original document looked
    like and what it was split into.
  prefs: []
  type: TYPE_NORMAL
- en: This output shows what the first chunk split of a collection looks like. Note
    that the second component of `"lastmod"` for each new chunk was updated so that
    the values are `Timestamp(9, 2)` and `Timestamp(9, 3)`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Migrations are a bit more complicated and actually create four separate changelog
    documents: one noting the start of the migrate, one for the “from” shard, one
    for the “to” shard, and one for the commit that occurs when the migration is finalized.
    The middle two documents are of interest because these give a breakdown of how
    long each step in the process took. This can give you an idea of whether it’s
    the disk, network, or something else that is causing a bottleneck on migrates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the document created by the “from” shard looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Each of the steps listed in `"details"` is timed and the ``"step*`N`* of *`N`*"``
    messages show how long each step took, in milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the “from” shard receives a `moveChunk` command from the *mongos*, it:'
  prefs: []
  type: TYPE_NORMAL
- en: Checks the command parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Confirms with the config servers that it can acquire a distributed lock for
    the migrate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tries to contact the “to” shard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copies the data. This is referred to and logged as “the critical section.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Coordinates with the “to” shard and config servers to confirm the migration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that the “to” and “from” shards must be in close communication starting
    at `"step4 of 6"`: the shards directly talk to one another and the config server
    to perform the migration. If the “from” server has flaky network connectivity
    during the final steps, it may end up in a state where it cannot undo the migration
    and cannot move forward with it. In this case, the *mongod* will shut down.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “to” shard’s changelog document is similar to the “from” shard’s, but the
    steps are a bit different. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When the “to” shard receives a command from the “from” shard, it:'
  prefs: []
  type: TYPE_NORMAL
- en: Migrates indexes. If this shard has never held chunks from the migrated collection
    before, it needs to know what fields are indexed. If this isn’t the first time
    a chunk from this collection is being moved to this shard, then this should be
    a no-op.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deletes any existing data in the chunk range. There might be data left over
    from a failed migration or restore procedure that we wouldn’t want to interfere
    with the current data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copies all documents in the chunk to the “to” shard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replays any operations that happened to these documents during the copy (on
    the “to” shard).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Waits for the “to” shard to have replicated the newly migrated data to a majority
    of servers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Commits the migrate by changing the chunk’s metadata to say that it lives on
    the “to” shard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: config.settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This collection contains documents representing the current balancer settings
    and chunk size. By changing the documents in this collection, you can turn the
    balancer on or off or change the chunk size. Note that you should always connect
    to *mongos*, not the config servers directly, to change values in this collection.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking Network Connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a lot of connections between the components of a cluster. This section
    covers some sharding-specific information (see [Chapter 24](ch24.xhtml#chapter-ops)
    for more information on networking).
  prefs: []
  type: TYPE_NORMAL
- en: Getting Connection Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The command `connPoolStats` returns information regarding the open outgoing
    connections from the current database instance to other members of the sharded
    cluster or replica set.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid interference with any running operations, `connPoolStats` does not
    take any locks. As such, the counts may change slightly as `connPoolStats` gathers
    information, resulting in slight differences between the hosts and pools connection
    counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"totalAvailable"` shows the total number of available outgoing connections
    from the current *mongod*/*mongos* instance to other members of the sharded cluster
    or replica set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"totalCreated"` reports the total number of outgoing connections ever created
    by the current *mongod*/*mongos* instance to other members of the sharded cluster
    or replica set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"totalInUse"` provides the total number of outgoing connections from the current
    *mongod*/*mongos* instance to other members of the sharded cluster or replica
    set that are currently in use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"totalRefreshing"` displays the total number of outgoing connections from
    the current *mongod*/*mongos* instance to other members of the sharded cluster
    or replica set that are currently being refreshed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"numClientConnections"` identifies the number of active and stored outgoing
    synchronous connections from the current *mongod*/*mongos* instance to other members
    of the sharded cluster or replica set. These represent a subset of the connections
    reported by `"totalAvailable"`, `"totalCreated"`, and `"totalInUse"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"numAScopedConnection"` reports the number of active and stored outgoing scoped
    synchronous connections from the current *mongod*/*mongos* instance to other members
    of the sharded cluster or replica set. These represent a subset of the connections
    reported by `"totalAvailable"`, `"totalCreated"`, and `"totalInUse"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"pools"` shows connection statistics (in use/available/created/refreshing)
    grouped by the connection pools. A *mongod* or *mongos* has two distinct families
    of outgoing connection pools:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBClient-based pools (the “write path,” identified by the field name `"global"`
    in the `"pools"` document)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NetworkInterfaceTL-based pools (the “read path”)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"hosts"` shows connection statistics (in use/available/created/refreshing)
    grouped by the hosts. It reports on connections between the current *mongod*/*mongos*
    instance and each member of the sharded cluster or replica set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might see connections to other shards in the output of `connPoolStats`.
    These indicate that shards are connecting to other shards to migrate data. The
    primary of one shard will connect directly to the primary of another shard and
    “suck” its data.
  prefs: []
  type: TYPE_NORMAL
- en: When a migrate occurs, a shard sets up a `ReplicaSetMonitor` (a process that
    monitors replica set health) to track the health of the shard on the other side
    of the migrate. *mongod* never destroys this monitor, so you may see messages
    in one replica set’s log about the members of another replica set. This is totally
    normal and should have no effect on your application.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the Number of Connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a client connects to a *mongos*, the *mongos* creates a connection to at
    least one shard to pass along the client’s request. Thus, every client connection
    into a *mongos* yields at least one outgoing connection from *mongos* to the shards.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have many *mongos* processes, they may create more connections than
    your shards can handle: by default a *mongos* will accept up to 65,536 connections
    (the same as *mongod*), so if you have 5 *mongos* processes with 10,000 client
    connections each, they may be attempting to create 50,000 connections to a shard!'
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent this, you can use the `--maxConns` option to your command-line configuration
    for *mongos* to limit the number of connections it can create. The following formula
    can be used to calculate the maximum number of connections a shard can handle
    from a single *mongos*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*maxConns* = *maxConnsPrimary* − (*numMembersPerReplicaSet* × 3) −'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (*other* x 3) / *numMongosProcesses*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breaking down the pieces of this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: maxConnsPrimary
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of connections on the Primary, typically set to 20,000 to
    avoid overwhelming the shard with connections from the *mongos*.
  prefs: []
  type: TYPE_NORMAL
- en: (numMembersPerReplicaSet × 3)
  prefs: []
  type: TYPE_NORMAL
- en: The primary creates a connection to each secondary and each secondary creates
    two connections to the primary, for a total of three connections.
  prefs: []
  type: TYPE_NORMAL
- en: (other x 3)
  prefs: []
  type: TYPE_NORMAL
- en: Other is the number of miscellaneous processes that may connect to your *mongod*s,
    such as monitoring or backup agents, direct shell connections (for administration),
    or connections to other shards for migrations.
  prefs: []
  type: TYPE_NORMAL
- en: numMongosProcesses
  prefs: []
  type: TYPE_NORMAL
- en: The total number of *mongos* in the sharded cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that `--maxConns` only prevents *mongos* from creating more than this
    many connections. It doesn’t do anything particularly helpful when this limit
    is reached: it will simply block requests, waiting for connections to be “freed.”
    Thus, you must prevent your application from using this many connections, especially
    as your number of *mongos* processes grows.'
  prefs: []
  type: TYPE_NORMAL
- en: When a MongoDB instance exits cleanly it closes all connections before stopping.
    The members that were connected to it will immediately get socket errors on those
    connections and be able to refresh them. However, if a MongoDB instance suddenly
    goes offline due to a power loss, crash, or network problems, it probably won’t
    cleanly close all of its sockets. In this case, other servers in the cluster may
    be under the impression that their connection is healthy until they try to perform
    an operation on it. At that point, they will get an error and refresh the connection
    (if the member is up again at that point).
  prefs: []
  type: TYPE_NORMAL
- en: This is a quick process when there are only a few connections. However, when
    there are thousands of connections that must be refreshed one by one you can get
    a lot of errors because each connection to the downed member must be tried, determined
    to be bad, and reestablished. There isn’t a particularly good way of preventing
    this, aside from restarting processes that get bogged down in a reconnection storm.
  prefs: []
  type: TYPE_NORMAL
- en: Server Administration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As your cluster grows, you’ll need to add capacity or change configurations.
    This section covers how to add and remove servers in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Servers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can add new *mongos* processes at any time. Make sure their `--configdb`
    option specifies the correct set of config servers and they should be immediately
    available for clients to connect to.
  prefs: []
  type: TYPE_NORMAL
- en: To add new shards, use the `addShard` command as shown in [Chapter 15](ch15.xhtml#chapter-shard-config).
  prefs: []
  type: TYPE_NORMAL
- en: Changing Servers in a Shard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you use your sharded cluster, you may want to change the servers in individual
    shards. To change a shard’s membership, connect directly to the shard’s primary
    (not through the *mongos*) and issue a replica set reconfig. The cluster configuration
    will pick up the change and update *config.shards* automatically. Do not modify
    *config.shards* by hand.
  prefs: []
  type: TYPE_NORMAL
- en: The only exception to this is if you started your cluster with standalone servers
    as shards, not replica sets.
  prefs: []
  type: TYPE_NORMAL
- en: Changing a shard from a standalone server to a replica set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The easiest way to do this is to add a new, empty replica set shard and then
    remove the standalone server shard (as discussed in the next section). Migrations
    will take care of moving your data to the new shard.
  prefs: []
  type: TYPE_NORMAL
- en: Removing a Shard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, shards should not be removed from a cluster. If you are regularly
    adding and removing shards, you are putting a lot more stress on the system than
    necessary. If you add too many shards it is better to let your system grow into
    them, not remove them and add them back later. However, if necessary, you can
    remove shards.
  prefs: []
  type: TYPE_NORMAL
- en: 'First make sure that the balancer is on. The balancer will be tasked with moving
    all the data on the shard you want to remove to other shards in a process called
    *draining*. To start draining, run the `removeShard` command. `removeShard` takes
    the shard’s name and drains all the chunks on that shard to the other shards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Draining can take a long time if there are a lot of chunks or large chunks to
    move. If you have jumbo chunks (see [“Jumbo Chunks”](#sect2-jumbo)), you may have
    to temporarily increase the chunk size to allow draining to move them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to keep tabs on how much has been moved, run `removeShard` again
    to give you the current status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can run `removeShard` as many times as you want.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunks may have to be split to be moved, so you may see the number of chunks
    increase in the system during the drain. For example, suppose we have a five-shard
    cluster with the following chunk distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This cluster has a total of 52 chunks. If we remove *test-rs3*, we might end
    up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The cluster now has 60 chunks, 18 of which came from shard *test-rs3* (11 were
    there to start and 7 were created from draining splits).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the chunks have been moved, if there are still databases that have
    the removed shard as their primary, you’ll need to remove them before the shard
    can be removed. Each database in a sharded cluster has a primary shard. If the
    shard you want to remove is also the primary of one of the cluster’s databases,
    `removeShard` lists the database in the `"dbsToMove"` field. To finish removing
    the shard, you must either move the database to a new shard after migrating all
    data from the shard or drop the database, deleting the associated data files.
    The output of `removeShard` will be something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To finish the remove, move the listed databases with the `movePrimary` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have done this, run `removeShard` one more time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is not strictly necessary, but it confirms that you have completed the
    process. If there are no databases that have this shard as their primary, you
    will get this response as soon as all chunks have been migrated off the shard.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you have started a shard draining, there is no built-in way to stop it.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, MongoDB automatically takes care of balancing data. This section
    covers how to enable and disable this automatic balancing as well as how to intervene
    in the balancing process.
  prefs: []
  type: TYPE_NORMAL
- en: The Balancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Turning off the balancer is a prerequisite to nearly any administrative activity.
    There is a shell helper to make this easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'With the balancer off a new balancing round will not begin, but turning it
    off will not force an ongoing balancing round to stop immediately—migrations generally
    cannot stop on a dime. Thus, you should check the *config.locks* collection to
    see whether or not a balancing round is still in progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`0` means the balancer is off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Balancing puts load on your system: the destination shard must query the source
    shard for all the documents in a chunk and insert them, and then the source shard
    must delete them. There are two circumstances in particular where migrations can
    cause performance problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a hotspot shard key will force constant migrations (as all new chunks
    will be created on the hotspot). Your system must have the capacity to handle
    the flow of data coming off of your hotspot shard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding a new shard will trigger a stream of migrations as the balancer attempts
    to populate it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you find that migrations are affecting your application’s performance, you
    can schedule a window for balancing in the *config.settings* collection. Run the
    following update to only allow balancing between 1 p.m. and 4 p.m. First make
    sure the balancer is on, then schedule the window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you set a balancing window, monitor it closely to ensure that *mongos* can
    actually keep your cluster balanced in the time that you have allotted it.
  prefs: []
  type: TYPE_NORMAL
- en: You must be careful if you plan to combine manual balancing with the automatic
    balancer, since the automatic balancer always determines what to move based on
    the current state of the set and does not take into account the set’s history.
    For example, suppose you have *shardA* and *shardB*, each holding 500 chunks.
    *shardA* is getting a lot of writes, so you turn off the balancer and move 30
    of the most active chunks to *shardB*. If you turn the balancer back on at this
    point, it will immediately swoop in and move 30 chunks (possibly a different 30)
    back from *shardB* to *shardA* to balance the chunk counts.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent this, move 30 quiescent chunks from *shardB* to *shardA* before starting
    the balancer. That way there will be no imbalance between the shards and the balancer
    will be happy to leave things as they are. Alternatively, you could perform 30
    splits on *shardA*’s chunks to even out the chunk counts.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the balancer only uses number of chunks as a metric, not size of data.
    Moving a chunk is called a migration and is how MongoDB balances data across your
    cluster. Thus, a shard with a few large chunks may end up as the target of a migration
    from a shard with many small chunks (but a smaller data size).
  prefs: []
  type: TYPE_NORMAL
- en: Changing Chunk Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There can be anywhere from zero to millions of documents in a chunk. Generally,
    the larger a chunk is, the longer it takes to migrate to another shard. In [Chapter 14](ch14.xhtml#chapter_d1e10482),
    we used a chunk size of 1 MB, so that we could see chunk movement easily and quickly.
    This is generally impractical in a live system; MongoDB would be doing a lot of
    unnecessary work to keep shards within a few megabytes of each other in size.
    By default chunks are 64 MB, which generally provides a good balance between ease
    of migration and migratory churn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes you may find that migrations are taking too long with 64 MB chunks.
    To speed them up, you can decrease your chunk size. To do this, connect to *mongos*
    through the shell and update the *config.settings* collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The previous update would change your chunk size to 32 MB. Existing chunks would
    not be changed immediately, however; automatic splitting only occurs on insert
    or update. Thus, if you lower the chunk size, it may take time for all chunks
    to split to the new size.
  prefs: []
  type: TYPE_NORMAL
- en: Splits cannot be undone. If you increase the chunk size, existing chunks grow
    only through insertion or updates until they reach the new size. The allowed range
    of the chunk size is between 1 and 1,024 MB, inclusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this is a cluster-wide setting: it affects all collections and databases.
    Thus, if you need a small chunk size for one collection and a large chunk size
    for another, you may have to compromise with a chunk size in between the two ideals
    (or put the collections in different clusters).'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If MongoDB is doing too many migrations or your documents are large, you may
    want to increase your chunk size.
  prefs: []
  type: TYPE_NORMAL
- en: Moving Chunks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, all the data in a chunk lives on a certain shard. If that
    shard ends up with more chunks than the other shards, MongoDB will move some chunks
    off it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can manually move chunks using the `moveChunk` shell helper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This would move the chunk containing the document with an `"imdbId"` of `500000`
    to the shard named *shard02*. You must use the shard key (`"imdbId"`, in this
    case) to find which chunk to move. Generally, the easiest way to specify a chunk
    is by its lower bound, although any value in the chunk will work (the upper bound
    will not, as it is not actually in the chunk). This command will move the chunk
    before returning, so it may take a while to run. The logs are the best place to
    see what it is doing if it takes a long time.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a chunk is larger than the max chunk size, *mongos* will refuse to move
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, you must manually split the chunk before moving it, using the
    `splitAt` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Once the chunk has been split into smaller pieces, it should be movable. Alternatively,
    you can raise the max chunk size and then move it, but you should break up large
    chunks whenever possible. Sometimes, though, chunks cannot be broken up—we’ll
    look at this situation next.^([1](ch17.xhtml#idm45882339279480))
  prefs: []
  type: TYPE_NORMAL
- en: Jumbo Chunks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you choose the `"date"` field as your shard key. The `"date"` field
    in this collection is a string that looks like ``"*`year`*/*`month`*/*`day`*"``,
    which means that *mongos* can create at most one chunk per day. This works fine
    for a while, until your application suddenly goes viral and gets a thousand times
    its typical traffic for one day.
  prefs: []
  type: TYPE_NORMAL
- en: This day’s chunk is going to be much larger than any other day’s, but it is
    also completely unsplittable because every document has the same value for the
    shard key.
  prefs: []
  type: TYPE_NORMAL
- en: Once a chunk is larger than the max chunk size set in *config.settings*, the
    balancer will not be allowed to move the chunk. These unsplittable, unmovable
    chunks are called *jumbo chunks* and they are inconvenient to deal with.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take an example. Suppose you have three shards, *shard1*, *shard2*, and
    *shard3*. If you use the hotspot shard key pattern described in [“Ascending Shard
    Keys”](ch16.xhtml#sect2-hotspot), all your writes will be going to one shard—say,
    *shard1*. The shard primary *mongod* will request that the *balancer* move each
    new top chunk evenly between the other shards, but the only chunks that the balancer
    can move are the nonjumbo chunks, so it will migrate all the small chunks off
    the hot shard.
  prefs: []
  type: TYPE_NORMAL
- en: Now all the shards will have roughly the same number of chunks, but all of *shard2*
    and *shard3*’s chunks will be less than 64 MB in size. And if jumbo chunks are
    being created, more and more of *shard1*’s chunks will be more than 64 MB in size.
    Thus, *shard1* will fill up a lot faster than the other two shards, even though
    the number of chunks is perfectly balanced between the three.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, one of the indicators that you have jumbo chunk problems is that one
    shard’s size is growing much faster than the others. You can also look at the
    output of `sh.status()` to see if you have jumbo chunks—they will be marked with
    the `jumbo` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the `dataSize` command to check chunk sizes. First, use the *config.chunks*
    collection to find the chunk ranges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then use these chunk ranges to find possible jumbo chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Be careful, though—the `dataSize` command does have to scan the chunk’s data
    to figure out how big it is. If you can, narrow down your search by using your
    knowledge of your data: were jumbo chunks created on a certain date? For example,
    if July 1 was a really busy day, look for chunks with that day in their shard
    key range.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re using GridFS and sharding by `"files_id"`, you can look at the *fs.files*
    collection to find a file’s size.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing jumbo chunks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To fix a cluster thrown off-balance by jumbo chunks, you must evenly distribute
    them among the shards.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a complex manual process, but should not cause any downtime (it may
    cause slowness, as you’ll be migrating a lot of data). In the following description,
    the shard with the jumbo chunks is referred to as the “from” shard. The shards
    that the jumbo chunks are migrated to are called the “to” shards. Note that you
    may have multiple “from” shards that you wish to move chunks off of. Repeat these
    steps for each:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Turn off the balancer. You don’t want the balancer trying to “help” during
    this process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'MongoDB will not allow you to move chunks larger than the max chunk size, so
    temporarily increase the chunk size. Make a note of what your original chunk size
    is and then change it to something large, like `10000`. Chunk size is specified
    in megabytes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use the `moveChunk` command to move jumbo chunks off the “from” shard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `splitChunk` on the remaining chunks on the “from” shard until it has roughly
    the same number of chunks as the “to” shards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the chunk size back to its original value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Turn on the balancer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When the balancer is turned on again, it will once again be unable to move the
    jumbo chunks; they are essentially held in place by their size.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing jumbo chunks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the amount of data you are storing grows, the manual process described in
    the previous section becomes unsustainable. Thus, if you’re having problems with
    jumbo chunks, you should make it a priority to prevent them from forming.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent jumbo chunks, modify your shard key to have more granularity. You
    want almost every document to have a unique value for the shard key, or at least
    to never have more than the chunk size’s worth of data with a single shard key
    value.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you were using the year/month/day key described earlier, it
    could quickly be made more fine-grained by adding hours, minutes, and seconds.
    Similarly, if you’re sharding on something coarse-grained like log level, you
    can add to your shard key a second field with a lot of granularity, such as an
    MD5 hash or UUID. Then you can always split a chunk, even if the first field is
    the same for many documents.
  prefs: []
  type: TYPE_NORMAL
- en: Refreshing Configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a final tip, sometimes *mongos* will not update its configuration correctly
    from the config servers. If you ever get a configuration that you don’t expect
    or a *mongos* seems to be out of date or cannot find data that you know is there,
    use the `flushRouterConfig` command to manually clear all caches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If `flushRouterConfig` does not work, restarting all your *mongos* or *mongod*
    processes clears any cached data.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch17.xhtml#idm45882339279480-marker)) MongoDB 4.4 is planning to add a
    new parameter (`forceJumbo`) in the `moveChunk` function, as well as a new balancer
    configuration setting `attemptToBalanceJumboChunks` to address jumbo chunks. The
    details are in [this JIRA ticket describing the work](https://jira.mongodb.org/browse/SERVER-42273).
  prefs: []
  type: TYPE_NORMAL
