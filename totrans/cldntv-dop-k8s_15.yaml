- en: Chapter 13\. Development Workflow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Surfing is such an amazing concept. You’re taking on Nature with a little stick
    and saying, *I’m gonna ride you!* And a lot of times Nature says, *No you’re not!*
    and crashes you to the bottom.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jolene Blalock
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we’ll expand on the discussion in [Chapter 12](ch12.html#deploying),
    turning our attention to the whole application life cycle, from local development
    to deploying updates to a Kubernetes cluster, including the tricky topic of database
    migrations. We’ll cover some tools that help you develop, test, and deploy your
    applications, including Skaffold and Telepresence. We will also introduce Knative
    and OpenFaaS, two options for running “serverless” architectures on your clusters.
    We’ll also look at more complex deployments, and ordering application rollouts
    using Helm hooks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Development Tools
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 12](ch12.html#deploying), we looked at some tools to help you write,
    build, and deploy your Kubernetes resource manifests. That’s fine as far as it
    goes, but often when you’re developing an application that runs in Kubernetes,
    you want to be able to try things out and see changes instantly, without going
    through a full build-push-deploy-update loop.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Skaffold
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Skaffold](https://oreil.ly/UTaRj) is an open source tool from Google designed
    to provide a fast local development workflow. It automatically rebuilds your containers
    as you develop locally, and deploys those changes to either a local or remote
    cluster.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: You define your desired workflow in a *skaffold.yaml* file in your repository
    and run the `skaffold` command-line tool to start the pipeline. As you make changes
    to files in your local directory, Skaffold wakes up, builds a new container with
    the changes, and then deploys it for you automatically, saving you a round trip
    to the container.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a Skaffold example in the [demo repo](https://oreil.ly/LAI8f) to show
    how it works. Follow the Skaffold [installation instructions](https://oreil.ly/heDih)
    for your operating system, point your `kubectl` at your local development Kubernetes
    cluster, and switch to the *hello-skaffold* directory to see how it works:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Skaffold automatically built and deployed the demo application using Helm and
    is now watching for changes to the code or the chart. It uses a randomly generated
    SHA as a temporary placeholder container tag as you are doing development. If
    you `curl localhost:8888` you should see `Hello, skaffold!`, because that is what
    is set in the `main.go` for this example. Leave Skaffold running and go change
    that *main.go* file to say something else, like `Hola, skaffold!`. As soon as
    you save your changes, Skaffold will automatically rebuild and redeploy the updated
    copy of the app:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now if you `curl localhost:8888`, you’ll see your modified code running. Skaffold
    built a new copy of the demo container and released a new version to your cluster
    using Helm. Keep making changes with Skaffold running, and it will continue updating
    the application automatically. When you’ve had enough fun, you can stop Skaffold
    with `Ctrl+C` and it will automatically uninstall the demo app:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您`curl localhost:8888`，您将看到您修改后的代码正在运行。Skaffold构建了演示容器的新副本，并使用Helm发布了一个新版本到您的集群。继续在运行Skaffold时进行更改，它将继续自动更新应用程序。当您玩够了，可以使用`Ctrl+C`停止Skaffold，并自动卸载演示应用程序。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can use Skaffold for other types of builds, such as Bazel, or using your
    own custom-build scripts. It supports using [Google Cloud Build](https://cloud.google.com/build)
    as a remote image builder, rather than using your local machine. You can also
    incorporate your test suite into the workflow and automatically run through your
    tests as part of the build and deploy process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Skaffold进行其他类型的构建，例如Bazel，或使用您自己的自定义构建脚本。它支持使用[Google Cloud Build](https://cloud.google.com/build)作为远程镜像构建工具，而不是使用您的本地计算机。您还可以将测试套件整合到工作流程中，并在构建和部署过程中自动运行测试。
- en: Telepresence
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Telepresence
- en: '[Telepresence](https://www.telepresence.io) takes a slightly different approach
    from Skaffold. You don’t need a local Kubernetes cluster; a Telepresence Pod runs
    in your real cluster as a placeholder for your application. Traffic for the application
    Pod is then intercepted and routed to the container running on your local machine.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[Telepresence](https://www.telepresence.io)与Skaffold采取了稍微不同的方法。您不需要本地Kubernetes集群；Telepresence
    Pod在您的实际集群中作为应用程序的占位符运行。然后拦截和路由应用程序Pod的流量，将其转发到在您本地计算机上运行的容器。'
- en: This enables the developer to make their local machine participate in the remote
    cluster. As they make changes to application code, they will be reflected in the
    live cluster, without a new container having to be deployed to it. If your application
    has a particularly long build time, this may be an appealing tool to try.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这使开发者能够使其本地计算机参与远程集群。随着他们对应用程序代码的更改，这些更改将在实时集群中反映出来，而无需部署新的容器。如果您的应用程序构建时间特别长，这可能是一个值得尝试的吸引人工具。
- en: One common complaint about developing applications in containers is that a new
    container needs to be built containing the new code changes, and this can be a
    frustrating experience if the rebuild is slow. Telepresence aims to ease this
    frustration and speed up the local development process while still using containers
    and Kubernetes so that the runtime environment between local development and what
    is deployed in production does not need to be entirely different.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 开发应用程序时，常见的抱怨之一是需要构建一个新的容器来包含新的代码更改，如果重新构建速度慢的话，这可能会让人沮丧。Telepresence旨在减轻这种沮丧，并加快本地开发过程，同时仍然使用容器和Kubernetes，这样本地开发和生产部署之间的运行环境不需要完全不同。
- en: Waypoint
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Waypoint
- en: HashiCorp, the developers behind popular tools like Vault, Terraform, and Consul,
    have built a new tool focused on simplifying the end-to-end application development
    experience. [Waypoint](https://www.waypointproject.io) introduces a declarative
    *waypoint.hcl* file that allows you to define the build, deploy, config, and release
    steps for your application regardless of the runtime or platform it uses. Using
    a standard wrapper like Waypoint means that your CI/CD tooling and processes can
    be standardized, even if your applications use different build and deploy steps.
    Developers can check out an application repository, run `waypoint build`—while
    not having to consider if this particular application uses a Dockerfile or something
    like a [CloudNative Buildpack](https://buildpacks.io)—and expect to end up with
    the correct artifact for running the application. Similarly, `waypoint deploy`
    abstracts away whether this particular application uses Kubernetes, Helm, or maybe
    it runs as an AWS Lambda function. Having a standard CLI experience for developers
    and the CI/CD tooling can greatly simplify how the build and deployment process
    works across different types of applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Knative
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the other tools we’ve looked at are focused on speeding up the local
    development loop, [Knative](https://oreil.ly/SXohX) is more ambitious. It aims
    to provide a standard mechanism for deploying all kinds of workloads to Kubernetes:
    not just containerized applications, but *serverless*-style functions too.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Knative integrates with both Kubernetes and Istio (see [“Istio”](ch09.html#istio))
    to provide a complete application/function deployment platform, including setting
    up a build process, automated deployment, and an *eventing* mechanism that standardizes
    the way applications use messaging and queueing systems (for example, Pub/Sub,
    Kafka, or RabbitMQ).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: OpenFaaS
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[OpenFaas](https://www.openfaas.com) is another project that makes it possible
    to run serverless functions on Kubernetes. It includes a CLI tool for building
    and deploying functions to your clusters, and can trigger on HTTP events, a cron
    schedule, MQTT, Kafka, and RabbitMQ messages, and other popular event-driven systems
    and message queues. These serverless types of architectures work well with the
    microservice mindset and can lead to more efficient usage of your servers.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: An interesting side project from the folks working on OpenFaaS is called [*arkade*](https://oreil.ly/XBbiD),
    which works sort of like an App Store or marketplace for installing popular cloud
    native tools (like OpenFaaS itself) with one unified tool. With one `arkade` command
    you can install OpenFaaS on your Kubernetes cluster and start deploying your serverless
    workloads.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Crossplane
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Crossplane](https://crossplane.io) is a tool that aims to bring the standardization
    of Kubernetes to non-Kubernetes cloud resources. Many applications require pieces
    of infrastructure that live outside of Kubernetes, such as an AWS S3 bucket, an
    RDS database instance, or an Azure Cosmos DB instance. Often this means that developers
    end up using tools like [Terraform](https://www.terraform.io) to deploy these
    external dependencies first, and then switch to deploying the application using
    Dockerfiles, Kubernetes manifests, and Helm charts.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Crossplane uses CRDs (see [“Operators and Custom Resource Definitions (CRDs)”](ch09.html#crds))
    to define these types of infrastructure that live outside of Kubernetes as if
    they were native Kubernetes objects. It uses the APIs of the cloud providers to
    actually create and deploy these cloud resources, much like you would using the
    Terraform CLI or your cloud provider’s CLI and SDK tools. Imagine that you could
    declare a PostgreSQL RDS instance that your application requires to run like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now you can bundle this manifest for your application’s RDS instance alongside
    of your other Kubernetes manifests and deploy them together. The Kubernetes reconciliation
    loop will then ensure that this RDS instance is provisioned, just as it does for
    your application Pods. Another benefit here is when the infrastructure is managed
    and deployed together with the application, you can easily pass configuration
    between them. For example, the Crossplane CRD can automatically create a Kubernetes
    Secret for the password when it creates the database, and pass it to your application
    to use to connect. We expect that as Kubernetes continues to gain widespread adoption
    that this sort of extending the use of Kubernetes for managing other types of
    infrastructure will become popular.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Deployment Strategies
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you were upgrading a running application by hand, without Kubernetes, one
    way to do it would be to just shut down the application, install the new version,
    and restart it. But that means downtime.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'If you had multiple instances of your application spread across several servers,
    a better way would be to upgrade each one in turn so that there would be no interruption
    in service: a so-called *zero-downtime deployment*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Not all applications need zero downtime. Internal services that consume message
    queues, for example, are idempotent, so they can be upgraded all at once. This
    means the upgrade happens faster, but for user-facing applications, we are usually
    more concerned with avoiding downtime than achieving quick upgrades.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, you can choose whichever of these strategies is most appropriate.
    `RollingUpdate` is the zero-downtime, Pod-by-Pod option, while `Recreate` is the
    fast, all-Pods-at-once option. There are also some fields that you can tweak to
    get the exact behavior that you need for your application.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes, an application’s deployment strategy is defined in the Deployment
    manifest. The default is `RollingUpdate`, so if you don’t specify a strategy,
    this is what you’ll get. To change the strategy to `Recreate`, set it like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now let’s look more closely at these deployment strategies and see how they
    work.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Rolling Updates
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a rolling update, Pods are upgraded one at a time, until all replicas have
    been replaced with the new version.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine we’re running an application with three replicas, each
    running `v1`. The developer starts an upgrade to `v2` with a `kubectl apply...`
    or `helm upgrade...` command. What happens?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: First, one of the three `v1` Pods will be terminated. Kubernetes will flag it
    as unready, and stop sending it traffic. A new `v2` Pod will be spun up to take
    its place. Meanwhile, the remaining `v1` Pods continue receiving incoming requests.
    While we’re waiting for the first `v2` Pod to become ready, we’re down to two
    Pods, but we’re still serving users.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: When the `v2` Pod becomes ready, Kubernetes will start sending it user traffic,
    alongside the other two `v1` Pods. Now we’re back to our full complement of three
    Pods.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: This process will continue, Pod by Pod, until all of the `v1` Pods have been
    replaced with `v2` Pods. While there are times when fewer Pods than usual are
    available to handle traffic, the application as a whole is never actually down.
    That’s zero-downtime deployment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: During a rolling update, both old and new versions of your application will
    be in service at the same time. While this usually isn’t a problem, you may need
    to take steps to ensure that this is safe; if your changes involve a database
    migration, for example (see [“Handling Migrations with Helm”](#helm-db-migrate)),
    a normal rolling update won’t be possible.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: If your Pods can sometimes crash or fail after a short time in a ready state,
    use the `minReadySeconds` field to have the rollout wait until each Pod is stable
    (see [“minReadySeconds”](ch05.html#minreadyseconds)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Recreate
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `Recreate` mode, all running replicas are terminated at once, and then new
    ones are created.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: For applications that do not directly handle requests, this should be acceptable.
    One advantage of `Recreate` is that it avoids the situation where two different
    versions of the application are running simultaneously (see [“Rolling Updates”](#rollingupdate)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: maxSurge and maxUnavailable
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a rolling update progresses, sometimes you will have more Pods running than
    the nominal `replicas` value, and sometimes less. Two important settings govern
    this behavior: `maxSurge` and `maxUnavailable`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '`maxSurge` sets the maximum number of excess Pods. For example, if you have
    10 replicas and you set `maxSurge` to 30%, then no more than 13 Pods will be allowed
    to run at any one time.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxUnavailable` sets the maximum number of unavailable Pods. With a nominal
    10 replicas and `maxUnavailable` of 20%, Kubernetes will never let the number
    of available Pods drop below 8.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These can be set as either a whole number, or as a percentage:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Usually, the default value of `25%` for both settings is fine, and you probably
    won’t need to adjust these, but depending on your load requirements you may need
    to tweak them to make sure your application can maintain acceptable capacity during
    an upgrade. At very large scale, you may find that running at 75% availability
    during a deployment is not sufficient to handle your incoming traffic, and you’ll
    need to reduce `maxUnavailable` slightly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The bigger the value of `maxSurge`, the faster your rollout but the more load
    it places on your cluster’s resources. Large values of `maxUnavailable` also speed
    up rollouts, at the expense of your application’s capacity.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, small values of `maxSurge` and `maxUnavailable` reduce the
    impact on your cluster and your users, but can make your rollouts take much longer.
    Only you can decide what the right trade-off is for your application.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Blue/Green Deployments
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a *blue/green* deployment, rather than killing and replacing Pods one at
    a time, a whole new Deployment is created, and a new separate stack of Pods running
    `v2` is launched beside the existing `v1` Deployment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of this is that you don’t have to deal with both old and new versions
    of the application processing requests simultaneously. On the other hand, your
    cluster will need to be big enough to run twice the number of replicas required
    for your application, which can be expensive, and means a lot of unused capacity
    sitting around most of the time (unless you scale the cluster up and down as needed).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: We learned in [“Service Resources”](ch04.html#services) that Kubernetes uses
    labels to decide which Pods should receive traffic from a Service. One way to
    implement a blue/green deployment is to set different labels on your old and new
    Pods (see [“Labels”](ch09.html#labels)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'With a small tweak to the Service definition for our example application, you
    can send traffic to only Pods labeled `deployment: blue`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When deploying a new version, you can label it `deployment: green`. It won’t
    receive any traffic, even when it’s fully up and running, because the Service
    only sends traffic to `blue` Pods. You can test it and make sure it’s ready before
    making the cutover.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'To switch over to the new `Deployment`, edit the `Service` to change the selector
    to `deployment: green`. Now the new `green` Pods will start receiving traffic,
    and once all the old `blue` Pods are idle, you can shut them down.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Rainbow Deployments
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some rare cases, particularly when Pods have very long-lived connections
    (websockets, for example), just blue and green may not be enough. You may need
    to maintain three or more versions of your application in flight at the same time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: This is sometimes referred to as a *rainbow deployment*. Every time you deploy
    an update, you get a new color set of Pods. As connections finally drain from
    your oldest set of Pods, they can be shut down.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Brandon Dimcheff describes a [rainbow deployment example](https://oreil.ly/fuFoi)
    in detail.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Canary Deployments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advantage of blue/green (or rainbow) deployments is that if you decide you
    don’t like the new version, or it isn’t behaving correctly, you can simply switch
    back to the old version, which is still running. However, it is expensive because
    you need the cluster capacity to run both versions simultaneously, which means
    you may need to run more nodes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: One alternative approach that avoids this problem is a *canary deployment*.
    Like a canary in a coal mine, a small handful of new Pods are exposed to the dangerous
    world of production to see what happens to them. If they survive, the rollout
    can continue to completion. If there *is* a problem, the blast radius is strictly
    limited.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Just as with blue/green deployments, you can do this using labels (see [“Labels”](ch09.html#labels)).
    There is a detailed example of running a canary deployment in the Kubernetes [documentation](https://oreil.ly/PqUe0).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated way to do this is to use a service mesh such as Istio,
    Linkerd, or one of the other tools we mention in [“Service Mesh”](ch09.html#service-mesh),
    which allows you to randomly route a variable proportion of traffic to one or
    more service versions. This also makes it easy to do things like A/B testing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Handling Migrations with Helm
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stateless applications are easy to deploy and upgrade, but when a database is
    involved the situation can be more complicated. Changes to the schema of a database
    usually require a *migration* task to be run before the new version of the application
    is running. For example, with Rails apps, you need to run `rails db:migrate` before
    starting the new application Pods.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: On Kubernetes, you can use a Job resource to do this (see [“Jobs”](ch09.html#jobs)).
    Another option would be to use an `initContainer` (see [“Init Containers”](ch08.html#init-containers)).
    You could also script this using `kubectl` commands as part of your deploy process,
    or if you are using Helm, then you can use a built-in feature called *hooks*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Helm Hooks
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helm hooks allow you to control the order in which things happen during a deployment.
    They also let you bail out of an upgrade if things go wrong.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a database migration Job for a Rails application deployed
    with Helm:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `helm.sh/hook` properties are defined in the `annotations` section:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `pre-upgrade` setting tells Helm to apply this Job manifest before doing
    an upgrade. The Job will run the standard Rails migration command.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The `"helm.sh/hook-delete-policy": hook-succeeded` tells Helm to delete the
    Job if it completes successfully (that is, exits with status 0).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Handling Failed Hooks
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the Job returns a nonzero exit code, this is a sign that there was an error
    and the migration did not complete successfully. Helm will leave the Job in place
    in its failed state so that you can debug what went wrong.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: If this happens, the release process will stop, and the application will not
    be upgraded. Running `kubectl get pods` will show you the failed Pod, allowing
    you to inspect the logs and see what happened.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Once the issue has been resolved, you can delete the failed job (`kubectl delete
    job <job-name>`) and then try the upgrade again. Another option for automatically
    cleaning up a failed job would be to add the `ttlSecondsAfterFinished` field that
    we cover in [“Cleaning up completed Jobs”](ch05.html#job-ttl).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Other Hooks
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hooks have phases other than just `pre-upgrade`. You can use a hook at any
    of the following stages of a release:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '`pre-install` executes after templates are rendered, but before any resources
    are created.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post-install` executes after all resources are loaded.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre-delete` executes on a deletion request before any resources are deleted.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post-delete` executes on a deletion request after all of the release’s resources
    have been deleted.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre-upgrade` executes on an upgrade request after templates are rendered,
    but before any resources are loaded (for example, before a `kubectl apply` operation).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post-upgrade` executes on an upgrade after all resources have been upgraded.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre-rollback` executes on a rollback request after templates are rendered,
    but before any resources have been rolled back.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post-rollback` executes on a rollback request after all resources have been
    modified.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaining Hooks
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Helm hooks also come with the ability to chain them together in a specific
    order, using the `helm.sh/hook-weight` property. The hooks will be run in order
    from lowest to highest, so a Job with a `hook-weight` of 0 will run before one
    with a `hook-weight` of `1`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can find everything you need to know about hooks in the Helm [documentation](https://oreil.ly/NHKvD).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Another way to implement more complex deployment situations is by using Kubernetes
    `Operators`. These allow you to build your own custom set of tasks tailored to
    your use-cases, including any special logic or custom steps that need to happen
    as part of deploying new versions of your applications. A common example where
    using an `Operator` makes good sense is for upgrading databases, which often require
    additional maintenance or preparation tasks as part of upgrading to a new version.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing Kubernetes applications can be tedious if you have to build, push,
    and deploy a container image to test every little code change. Tools like Skaffold
    and Telepresence make this loop much faster, speeding up development.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, rolling out changes to production is far easier with Kubernetes
    than with traditional servers, provided you understand the basic concepts, and
    how you can customize them to suit your application:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The default `RollingUpdate` deployment strategy in Kubernetes upgrades a few
    Pods at a time, waiting for each replacement Pod to become ready before shutting
    down the old one.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling updates avoid downtime at the expense of making the rollout take longer.
    It also means that both old and new versions of your application will be running
    simultaneously during the rollout period.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新避免了停机时间，但代价是使部署时间变长。这也意味着在部署期间您的应用的新旧版本将同时运行。
- en: You can adjust the `maxSurge` and `maxUnavailable` fields to fine tune rolling
    updates. Depending on the versions of the Kubernetes API you are using, the defaults
    may or may not be appropriate for your situation.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以调整`maxSurge`和`maxUnavailable`字段来优化滚动更新。取决于您使用的Kubernetes API版本，预设值可能适合也可能不适合您的情况。
- en: The `Recreate` strategy just blows away all the old Pods and starts up new ones
    all at once. This is fast, but results in downtime, so it’s not suitable for user-facing
    applications.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recreate`策略会直接清除所有旧的Pod并一次性启动新的Pod。这很快，但会导致停机时间，因此不适合面向用户的应用程序。'
- en: In a blue/green deployment, all the new Pods are started up and made ready without
    receiving any user traffic. Then all traffic is switched over to the new Pods
    in one go, before retiring the old Pods.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在蓝/绿部署中，所有新的Pod都会启动并准备就绪，但不接收任何用户流量。然后在一次性切换所有流量到新的Pod之前，将所有流量转移到新的Pod上，然后淘汰旧的Pod。
- en: Rainbow deployments are similar to blue/green deployments, but with more than
    two versions in service simultaneously.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彩虹部署类似于蓝/绿部署，但同时有多个版本在服务中。
- en: You can implement blue/green and rainbow deployments in Kubernetes by adjusting
    the labels on your Pods and changing the selector on the frontend Service to direct
    traffic to the appropriate set of Pods. Service mesh tools also add the ability
    to split traffic to different running versions of your application.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过调整Pod的标签并更改前端Service上的选择器来在Kubernetes中实现蓝/绿和彩虹部署，以将流量引导到适当的Pod集合。服务网格工具还增加了将流量分配到应用的不同运行版本的能力。
- en: Helm hooks provide a way to apply certain Kubernetes resources (usually Jobs)
    at a particular stage of a deployment, for example, to run a database migration.
    Hooks can define the order in which resources should be applied during a deployment,
    and cause the deployment to halt if something does not succeed.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helm钩子提供了一种在部署的特定阶段应用某些Kubernetes资源（通常是作业）的方法，例如运行数据库迁移。钩子可以定义部署期间应用资源的顺序，并在某些操作失败时导致部署停止。
- en: Knative and OpenFaaS allow you to run “serverless” functions on your clusters,
    making it easy to deploy event-driven architectures wherever you can run Kubernetes
    (pretty much anywhere!).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knative和OpenFaaS允许您在集群上运行“无服务器”函数，使得在可以运行Kubernetes的任何地方（几乎任何地方！）部署事件驱动架构变得更加容易。
