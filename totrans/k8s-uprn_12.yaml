- en: Chapter 12\. Jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have focused on long-running processes, such as databases and web
    applications. These types of workloads run until they are either upgraded or the
    service is no longer needed. While long-running processes make up the large majority
    of workloads that run on a Kubernetes cluster, there is often a need to run short-lived,
    one-off tasks. The Job object is made for handling these types of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A Job creates Pods that run until successful termination (for instance, exit
    with 0). In contrast, a regular Pod will continually restart regardless of its
    exit code. Jobs are useful for things you only want to do once, such as database
    migrations or batch jobs. If run as a regular Pod, your database migration task
    would run in a loop, continually repopulating the database after every exit.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore the most common job patterns Kubernetes affords.
    We will also show you how to leverage these patterns in real-life scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The Job Object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Job object is responsible for creating and managing Pods defined in a template
    in the job specification. These Pods generally run until successful completion.
    The Job object coordinates running a number of Pods in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: If the Pod fails before a successful termination, the job controller will create
    a new Pod based on the Pod template in the job specification. Given that Pods
    have to be scheduled, there is a chance that your job will not execute if the
    scheduler does not find the required resources. Also, due to the nature of distributed
    systems, there is a small chance that duplicate Pods will be created for a specific
    task during certain failure scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Job Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jobs are designed to manage batch-like workloads where work items are processed
    by one or more Pods. By default, each job runs a single Pod once until successful
    termination. This job pattern is defined by two primary attributes of a job: the
    number of job completions and the number of Pods to run in parallel. In the case
    of the “run once until completion” pattern, the `completions` and `parallelism`
    parameters are set to `1`. [Table 12-1](#Job-pattern-table) highlights job patterns
    based on the combination of `completions` and `parallelism` for a job configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 12-1\. Job patterns
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Use case | Behavior | `completions` | `parallelism` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| One shot | Database migrations | A single Pod running once until successful
    termination | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Parallel fixed completions | Multiple Pods processing a set of work in parallel
    | One or more Pods running one or more times until reaching a fixed completion
    count | 1+ | 1+ |'
  prefs: []
  type: TYPE_TB
- en: '| Work queue: parallel jobs | Multiple Pods processing from a centralized work
    queue | One or more Pods running once until successful termination | 1 | 2+ |'
  prefs: []
  type: TYPE_TB
- en: One Shot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-shot jobs provide a way to run a single Pod once until successful termination.
    While this may sound like an easy task, there is some work involved in pulling
    this off. First, a Pod must be created and submitted to the Kubernetes API. This
    is done using a Pod template defined in the job configuration. Once a job is up
    and running, the Pod backing the job must be monitored for successful termination.
    A job can fail for any number of reasons, including an application error, an uncaught
    exception during runtime, or a node failure before the job has a chance to complete.
    In all cases, the job controller is responsible for re-creating the Pod until
    a successful termination occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to create a one-shot job in Kubernetes. The easiest
    is to use the `kubectl` command-line tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some things to note here:'
  prefs: []
  type: TYPE_NORMAL
- en: The `-i` option to `kubectl` indicates that this is an interactive command.
    `kubectl` will wait until the job is running and then show the log output from
    the first (and in this case only) Pod in the job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--restart=OnFailure` is the option that tells `kubectl` to create a Job object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the options after `--` are command-line arguments to the container image.
    These instruct our test server (`kuard`) to generate ten 4,096-bit SSH keys and
    then exit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your output may not match this exactly. `kubectl` often misses the first couple
    of lines of output with the `-i` option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After the job has completed, the Job object and related Pod are retained so
    that you can inspect the log output. Note that this job won’t show up in `kubectl
    get jobs` unless you pass the `-a` flag. Without this flag, `kubectl` hides completed
    jobs. Delete the job before continuing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The other option for creating a one-shot job is using a configuration file,
    as shown in [Example 12-1](#oneshot-yaml-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-1\. job-oneshot.yaml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Submit the job using the `kubectl apply` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then `describe` the `oneshot` job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view the results of the job by looking at the logs of the Pod that
    was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations, your job has run successfully!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that we didn’t specify any labels when creating the Job object. Like
    with other controllers (such as DaemonSets, ReplicaSets, and Deployments) that
    use labels to identify a set of Pods, unexpected behaviors can occur if a Pod
    is reused across objects.
  prefs: []
  type: TYPE_NORMAL
- en: Because jobs have a finite beginning and ending, users often create many of
    them. This makes picking unique labels more difficult and more critical. For this
    reason, the Job object will automatically pick a unique label and use it to identify
    the Pods it creates. In advanced scenarios (such as swapping out a running job
    without killing the Pods it is managing), users can choose to turn off this automatic
    behavior and manually specify labels and selectors.
  prefs: []
  type: TYPE_NORMAL
- en: We just saw how a job can complete successfully. But what happens if something
    fails? Let’s try that out and see what happens. Modify the arguments to `kuard`
    in our configuration file to cause it to fail with a nonzero exit code after generating
    three keys, as shown in [Example 12-2](#oneshot-failure-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-2\. job-oneshot-failure1.yaml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now launch this with `kubectl apply -f job-oneshot-failure1.yaml`. Let it run
    for a bit and then look at the Pod status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here we see that the same Pod has restarted four times. Kubernetes is in `CrashLoopBackOff`
    for this Pod. It is not uncommon to have a bug someplace that causes a program
    to crash as soon as it starts. In that case, Kubernetes will wait a bit before
    restarting the Pod to avoid a crash loop that would eat resources on the node.
    This is all handled local to the node by the `kubelet` without the job being involved
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: Kill the job (`kubectl delete jobs oneshot`), and let’s try something else.
    Modify the config file again and change the `restartPolicy` from `OnFailure` to
    `Never`. Launch this with `kubectl apply -f jobs-oneshot-failure2.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we let this run for a bit and then look at related Pods, we’ll find something
    interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'What we see is that we have multiple Pods here that have errored out. By setting
    `restartPolicy: Never`, we are telling the `kubelet` not to restart the Pod on
    failure, but rather just declare the Pod as failed. The Job object then notices
    and creates a replacement Pod. If you aren’t careful, this’ll create a lot of
    “junk” in your cluster. For this reason, we suggest you use `restartPolicy: OnFailure`
    so failed Pods are rerun in place. Clean this up with `kubectl delete jobs oneshot`.'
  prefs: []
  type: TYPE_NORMAL
- en: So far we’ve seen a program fail by exiting with a nonzero exit code. But workers
    can fail in other ways. Specifically, they can get stuck and not make any forward
    progress. To help cover this case, you can use liveness probes with jobs. If the
    liveness probe policy determines that a Pod is dead, it’ll be restarted or replaced
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating keys can be slow. Let’s start a bunch of workers together to make
    key generation faster. We’re going to use a combination of the `completions` and
    `parallelism` parameters. Our goal is to generate 100 keys by having 10 runs of
    `kuard`, with each run generating 10 keys. But we don’t want to swamp our cluster,
    so we’ll limit ourselves to only five Pods at a time.
  prefs: []
  type: TYPE_NORMAL
- en: This translates to setting `completions` to `10` and `parallelism` to `5`. The
    config is shown in [Example 12-3](#job-para-yaml).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-3\. job-parallel.yaml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Start it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now watch as the Pods come up, do their thing, and exit. New Pods are created
    until 10 have completed altogether. Here we use the `--watch` flag to have `kubectl`
    stay around and list changes as they happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to study the completed jobs and check out their logs to see the fingerprints
    of the keys they generated. Clean up by deleting the finished Job object with
    `kubectl delete job parallel`.
  prefs: []
  type: TYPE_NORMAL
- en: Work Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common use case for jobs is to process work from a work queue. In this scenario,
    some task creates a number of work items and publishes them to a work queue. A
    worker job can be run to process each work item until the work queue is empty
    ([Figure 12-1](#fig1001)).
  prefs: []
  type: TYPE_NORMAL
- en: '![kur3 1201](assets/kur3_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Parallel jobs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Starting a work queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We start by launching a centralized work queue service. `kuard` has a simple
    memory-based work queue system built in. We will start an instance of `kuard`
    to act as a coordinator for all the work.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a simple ReplicaSet to manage a singleton work queue daemon.
    We are using a ReplicaSet to ensure that a new Pod will get created in the face
    of machine failure, as shown in [Example 12-4](#rs.queue.yaml-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-4\. rs-queue.yaml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the work queue with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the work queue daemon should be up and running. Let’s use port-forwarding
    to connect to it. Leave this command running in a terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can open your browser to *http://localhost:8080* and see the `kuard` interface.
    Switch to the “MemQ Server” tab to keep an eye on what is going on.
  prefs: []
  type: TYPE_NORMAL
- en: With the work queue server in place, the next step is to expose it using a service.
    This will make it easy for producers and consumers to locate the work queue via
    DNS, as [Example 12-5](#serviceyaml_EX) shows.
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-5\. service-queue.yaml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the queue service with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Loading up the queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are now ready to put a bunch of work items in the queue. For the sake of
    simplicity, we’ll just use `curl` to drive the API for the work queue server and
    insert a bunch of work items. `curl` will communicate to the work queue through
    the `kubectl port-forward` we set up earlier, as shown in [Example 12-6](#load_ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-6\. load-queue.sh
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Run these commands, and you should see 100 JSON objects output to your terminal
    with a unique message identifier for each work item. You can confirm the status
    of the queue by looking at the “MemQ Server” tab in the UI, or you can ask the
    work queue API directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to kick off a job to consume the work queue until it’s empty.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the consumer job
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is where things get interesting! `kuard` can also act in consumer mode.
    We can set it up to draw work items from the work queue, create a key, and then
    exit once the queue is empty, as shown in [Example 12-7](#job-con-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-7\. job-consumers.yaml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are telling the job to start up five Pods in parallel. As the `completions`
    parameter is unset, we put the job into worker-pool mode. Once the first Pod exits
    with a zero exit code, the job will start winding down and will not start any
    new Pods. This means that none of the workers should exit until the work is done
    and they are all in the process of finishing up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create the `consumers` job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can view the Pods backing the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note there are five Pods running in parallel. These Pods will continue to run
    until the work queue is empty. You can watch as it happens in the UI on the work
    queue server. As the queue empties, the consumer Pods will exit cleanly and the
    `consumers` job will be considered complete.
  prefs: []
  type: TYPE_NORMAL
- en: Cleanup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using labels, we can clean up all of the stuff we created in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: CronJobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes you want to schedule a job to be run at a certain interval. To achieve
    this, you can declare a CronJob in Kubernetes, which is responsible for creating
    a new Job object at a particular interval. [Example 12-8](#job-cronjob-ex) is
    an example CronJob declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-8\. job-cronjob.yaml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note the `spec.schedule` field, which contains the interval for the CronJob
    in standard `cron` format.
  prefs: []
  type: TYPE_NORMAL
- en: You can save this file as *job-cronjob.yaml*, and create the CronJob with `kubectl
    create -f cron-job.yaml`. If you are interested in the current state of a CronJob,
    you can use `kubectl describe *<cron-job>*` to get the details.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On a single cluster, Kubernetes can handle both long-running workloads such
    as web applications and short-lived workloads such as batch jobs. The job abstraction
    allows you to model batch job patterns ranging from simple, one-time tasks to
    parallel jobs that process many items until the work has been exhausted.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs are a low-level primitive and can be used directly for simple workloads.
    However, Kubernetes is built from the ground up to be extensible by higher-level
    objects. Jobs are no exception; higher-level orchestration systems can easily
    use them to take on more complex tasks.
  prefs: []
  type: TYPE_NORMAL
