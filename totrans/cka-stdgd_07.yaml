- en: Chapter 7\. Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Establishing a Kubernetes cluster is one thing. Making sure that the cluster
    stays operational is another. As a Kubernetes administrator, you are continuously
    confronted with making sure that the cluster stays functional. Therefore, your
    troubleshooting skills must be sharp so that you can come up with strategies for
    identifying the root cause of an issue and fixing it.
  prefs: []
  type: TYPE_NORMAL
- en: Of all the domains covered by the exam, the section “Troubleshooting” has the
    highest weight for the overall score, so it’s important to understand failure
    scenarios and learn how to fix them. This chapter will address how to monitor
    and troubleshoot applications in different constellations. Furthermore, we’ll
    discuss failures that may arise for cluster components due to misconfiguration
    or error conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, this chapter covers the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating logging options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing container logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting application failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting cluster failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating Cluster and Node Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A real-world Kubernetes cluster manages hundreds or even thousands of Pods.
    For every Pod, you have at least a single container running a process. Each process
    can produce log output to the standard output or standard error streams. It’s
    imperative to capture the log output to proficiently determine the root cause
    of an application error. Moreover, cluster components produce logs for diagnostic
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Kubernetes’ logging mechanism is crucial for tracking down errors
    and monitoring cluster components and applications. Kubernetes can be configured
    to log on the cluster or the node level. The implementation approaches and their
    potential trade-offs may differ from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes doesn’t provide a native solution for cluster-level logging, but
    you can choose from the following three options to fulfill the requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating a node-level logging agent that runs on each of the cluster nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring a sidecar container responsible for handling the application logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing the logs directly to a logging backend from the application logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following discussion explains the benefits and drawbacks for each approach.
    For a detailed discussion, see the [Kubernetes documentation](https://oreil.ly/rJjfV).
  prefs: []
  type: TYPE_NORMAL
- en: Using a node logging agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The logging agent is a dedicated tool that publishes the logs to a backend.
    A backend can be an external logging service outside of the cluster. [Figure 7-1](#cluster-logging-agent)
    visualizes the logging architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0701](Images/ckas_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Cluster-level logging with an agent
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The benefit of this approach is that the application doesn’t require any changes
    to the code or the Pod configuration to support collecting logs. Agents should
    be run as a DaemonSet.
  prefs: []
  type: TYPE_NORMAL
- en: Using a sidecar container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Pod can be configured to run another sidecar container alongside the main
    application container. The sidecar container streams standard output and error
    produced by the application and redirects the streams to a different location
    (e.g., a logging backend or a volume mounted to the container). [Figure 7-2](#cluster-logging-sidecar)
    shows the logging setup of a Pod that incorporates a streaming sidecar.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0702](Images/ckas_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Cluster-level logging with a sidecar container
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This approach has the benefit of being able to easily separate different streams
    (e.g., separating error from info log entries).
  prefs: []
  type: TYPE_NORMAL
- en: Pushing directly to logging backend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This approach pushes the responsibility onto the application without adding
    a middleman. [Figure 7-3](#cluster-logging-direct) shows the logging setup.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0703](Images/ckas_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Cluster-level logging by directly pushing to the backend
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While architecturally less complex, any change to the logging backend will require
    a change to the application code and therefore a new deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Node Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Node logging comes with the implication that the log files will be stored on
    the cluster node. The container runtime (e.g., Docker Engine) redirects standard
    output and error streams to the storage of the node with the help of the configured
    logging driver.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid filling up the node storage with logging content, log rotation should
    be implemented. Log rotation is an automated process for compressing, moving,
    deleting, and/or archiving log data that grow beyond a certain threshold. The
    Linux tool [logrotate](https://oreil.ly/DE7XB) is one way to configure log rotation
    for a Kubernetes cluster. [Figure 7-4](#node-logging) visualizes the node-level
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0704](Images/ckas_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Node-level logging
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you run `kubectl logs`, the kubelet receives the request, reads directly
    from the log file on the node, and returns the content to the client. The `kubectl
    logs` command returns only the latest log content, not the log entries that have
    already been archived.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster components kube-scheduler and kube-proxy run in a container. Therefore,
    the log handling is the same as for any other application container. For system
    components that do not run in the container (e.g., the kubelet and the container
    runtime), logs will be written to journald if systemd is available. If systemd
    is not available, system components write their log files to the directory `/var/log`
    with the file extension `.log`.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Cluster Components and Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying software to a Kubernetes cluster is only the start of operating an
    application long-term. Developers and administrators alike need to understand
    resource consumption patterns and behaviors of their applications with the goal
    of providing a scalable and reliable service.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Kubernetes world, monitoring tools like Prometheus and Datadog help
    with collecting, processing, and visualizing the information over time. The exam
    does not expect you to be familiar with commercial monitoring, logging, tracing,
    and aggregation tools; however, it is helpful to gain a rough understanding of
    the underlying Kubernetes infrastructure responsible for collecting usage metrics.
    The following list shows examples of typical metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of nodes in the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health status of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node performance metrics such as CPU, memory, disk space, network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod-level performance metrics such as CPU and memory consumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This responsibility falls into the hands of the [metrics server](https://oreil.ly/OycST),
    a cluster-wide aggregator of resource usage data. As shown in [Figure 7-5](#metrics-server),
    kubelets running on nodes collect the metrics and send them to the metrics server.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0705](Images/ckas_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Data collection for the metrics server
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The metrics server stores data in memory and does not persist data over time.
    If you are looking for a solution that keeps historical data, then you need to
    look into commercial options. Refer to the documentation for more information
    on its installation process. If you’re using Minikube as your practice environment,
    [enabling the metrics-server add-on](https://oreil.ly/NanXK) is straightforward
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now query for metrics of cluster nodes and Pods with the `top` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Troubleshooting Application Failures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When operating an application in a production Kubernetes cluster, it’s almost
    inevitable that you’ll come across failure situations. It’s your responsibility
    as an administrator (potentially working closely with the application developer)
    to troubleshoot issues with deployed Kubernetes objects.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’re going to take a look at debugging strategies that can
    help with identifying the root cause of an issue so that you can take action and
    correct the failure appropriately. For more information, reference the [Kubernetes
    documentation](https://oreil.ly/4pxVS).
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, creating a Pod is no issue. You simply emit the `run`, `create`,
    or `apply` commands to instantiate the Pod. If the YAML manifest is formed properly,
    Kubernetes accepts your request, so the assumption is that everything works as
    expected. To verify the correct behavior, the first thing you’ll want to do is
    to check the high-level runtime information of the Pod. The operation could involve
    other Kubernetes objects like a Deployment responsible for rolling out multiple
    replicas of a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving high-level information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To retrieve the information, run either the `kubectl get pods` command for just
    the Pods running in the namespace or the `kubectl get all` command to retrieve
    the most prominent object types in the namespace (which includes Deployments).
    You will want to take a look at the columns `READY`, `STATUS`, and `RESTARTS`.
    In the optimal case, the number of ready containers matches the number of containers
    expected to be created by the Pod. For a single-container Pod, the `READY` column
    would say 1/1\. The status should say `Running` to indicate that the Pod entered
    the proper lifecycle state. Be aware that it’s totally possible that a Pod renders
    a `Running` state, but the application isn’t actually working properly. If the
    number of restarts is greater than 0, then you might want to check the logic of
    the liveness probe (if defined) and identify the reason why a restart was necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Pod observes the status `ErrImagePull` and makes 0/1 containers
    available to incoming traffic. In short, this Pod has a problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After working with Kubernetes for a while, you’ll automatically recognize common
    error conditions. [Table 7-1](#common_pod_error_statuses) lists some of those
    error statuses and explains how to fix them.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Common Pod error statuses
  prefs: []
  type: TYPE_NORMAL
- en: '| Status | Root cause | Potential fix |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `ImagePullBackOff` or `ErrImagePull` | Image could not be pulled from registry.
    | Check correct image name, check that image name exists in registry, verify network
    access from node to registry, ensure proper authentication. |'
  prefs: []
  type: TYPE_TB
- en: '| `CrashLoopBackOff` | Application or command run in container crashes. | Check
    command executed in container, ensure that image can properly execute (e.g., by
    creating a container with Docker). |'
  prefs: []
  type: TYPE_TB
- en: '| `CreateContainerConfigError` | ConfigMap or Secret referenced by container
    cannot be found. | Check correct name of the configuration object, verify the
    existence of the configuration object in the namespace. |'
  prefs: []
  type: TYPE_TB
- en: Inspecting events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s totally possible that you’ll not encounter any of those error statuses.
    But there’s still a chance of the Pod having a configuration issue. You can retrieve
    detailed information about the Pod and its events using the `kubectl describe
    pod` command to inspect its events. The following output belongs to a Pod that
    tries to mount a Secret that doesn’t exist. Instead of rendering a specific error
    message, the Pod gets stuck with the status `ContainerCreating`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Another helpful command is `kubectl get events`. The output of the command
    lists the events across all Pods for a given namespace. You can use additional
    command-line options to further filter and sort events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When debugging a Pod, the next level of details can be retrieved by downloading
    and inspecting its logs. You may or may not find additional information that points
    to the root cause of a misbehaving Pod. It’s definitely worth a look. The YAML
    manifest shown in [Example 7-1](#pod_failing_command) defines a Pod running a
    shell command.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-1\. A Pod running a failing shell command
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the object, the Pod fails with the status `CrashLoopBackOff`.
    Running the `logs` command reveals that the command run in the container has an
    issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `logs` command provides two helpful options I’d like to mention here. The
    option `-f` streams the logs, meaning you’ll see new log entries as they’re being
    produced in real time. The option `--previous` gets the logs from the previous
    instantiation of a container, which is helpful if the container has been restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Opening an Interactive Shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If any of the previous commands do not point you to the root cause of the failing
    Pod, it’s time to open an interactive shell to a container. As an application
    developer, you’ll probably know best what behavior to expect from the application
    at runtime. Ensure that the correct configuration has been created and inspect
    the running processes by using the Unix or Windows utility tools, depending on
    the image run in the container.
  prefs: []
  type: TYPE_NORMAL
- en: Say you encounter a situation where a Pod seems to work properly on the surface,
    as shown in [Example 7-2](#pod_current_date).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-2\. A Pod periodically writing the current date to a file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the Pod, you check the status. It says `Running`; however, when
    making a request to the application, the endpoint reports an error. Next, you
    check the logs. The log output renders an error message that points to a nonexistent
    directory. Apparently, the directory hasn’t been set up correctly but is needed
    by the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `exec` command opens an interactive shell to further investigate the issue.
    Below, we’re using the Unix tools `mkdir`, `cd`, and `ls` inside of the running
    container to fix the problem. Obviously, the better mitigation strategy is to
    create the directory from the application or provide an instruction in the Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Troubleshooting Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Service provides a unified network interface for Pods. For full coverage on
    networking aspects in Kubernetes, see [Chapter 5](ch05.xhtml#services_networking).
    Here, I want to point out troubleshooting techniques for this primitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you can’t reach the Pods that should map to the Service, start by ensuring
    that the label selector matches with the assigned labels of the Pods. You can
    query the information by describing the Service and then render the labels of
    the available Pods with the option `--show-labels`. The following example does
    not have matching labels and therefore wouldn’t apply to any of the Pods running
    in the namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can also query the endpoints of the Service instance. Say
    you expected three Pods to be selected by a matching label but only two have been
    exposed by the Service. You’ll want to look at the label selection criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A common source of confusion is the type of a Service. By default, the Service
    type is `ClusterIP`, which means that a Pod can be reached through the Service
    only if queried from the same node inside of the cluster. First, check the Service
    type. If you think that `ClusterIP` is the proper type you wanted to assign, open
    an interactive shell from a temporary Pod inside the cluster and run a `curl`
    or `wget` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, check if the port mapping from the target port of the Service to the
    container port of the Pod is configured correctly. Both ports need to match or
    the network traffic wouldn’t be routed properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Troubleshooting Cluster Failures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many influencing factors that can render a Kubernetes cluster faulty
    on the component level. It’s a good idea to list the nodes available in the cluster
    to identify potential issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output will give you a lay of the land. You can easily identify the responsibility
    of each node from the `ROLES` column, the Kubernetes version used, and the current
    health status.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of things to look out for when identifying issues at a high
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the health status for the node anything other than “Ready”?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the version of a node deviate from the version of other nodes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections you can find individual sections on troubleshooting
    control plane nodes versus worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Control Plane Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Control plane nodes are the critical components for keeping a cluster operational.
    As described in [“Managing a Highly Available Cluster”](ch02.xhtml#managing_ha_cluster),
    a cluster can consist of more than one control plane node to ensure a high degree
    of uptime. Detecting that one of the control plane nodes is faulty should be treated
    with extreme urgency to avoid compromising high-availability characteristics.
    For more information on troubleshooting techniques and root-cause analysis, reference
    the [Kubernetes documentation](https://oreil.ly/KWeTt).
  prefs: []
  type: TYPE_NORMAL
- en: Rendering cluster information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further diagnose issues on the control plane node, run the command `kubectl
    cluster-info`. As you can see in the following output, the command renders the
    addresses of the control plane and other cluster services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For a detailed view of the cluster logs, append the `dump` subcommand. Due
    to the pages and pages of log messages, we won’t render the output in this book.
    Parse through the message to see if you can find any errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting control plane components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Among those [components available on the control plane node](https://oreil.ly/IZR8Z)
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kube-apiserver: Exposes the Kubernetes API used by clients like `kubectl` for
    managing objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'etcd: A key-value store for storing the cluster data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'kube-scheduler: Selects nodes for Pods that have been scheduled but not created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'kube-controller-manager: Runs controller processes (e.g., the job controller
    responsible for Job object execution).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'cloud-controller-manager: Links cloud provider–specific API to the Kubernetes
    cluster. This controller is not available in on-premise cluster installations
    of Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To discover those components and their status, list the Pods available in the
    namespace `kube-system`. Here, you can find the list of control-plane components
    on Minikube:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Any status that does not show “Running” should be inspected further. You can
    retrieve the logs for control-plane component Pods in the same fashion you do
    for any other Pod, using the `logs` command. The following command downloads the
    logs for the kube-apiserver component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Troubleshooting Worker Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Worker nodes are responsible for managing the workload. Make sure you have a
    sufficient number of worker nodes available to distribute the load. For a deeper
    discussion on how to join worker nodes to a cluster, see [Chapter 2](ch02.xhtml#cluster_architecture_installation_configuration).
  prefs: []
  type: TYPE_NORMAL
- en: 'Any of the nodes available in a cluster can transition into an error state.
    It’s your job as a Kubernetes administrator to identify those situations and fix
    them in a timely manner. When listing the nodes of a cluster, you may see that
    a worker node is not in the “Ready” state, which is a good indicator that it’s
    not available to handle the workload. In the output of the `get nodes` command,
    you can see that the node named `worker-1` is in the “NotReady” state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The “NotReady” state means that the node is unused and will accumulate operational
    costs without actually scheduling workload. There might be a variety of reasons
    why the node entered this state. The following list shows the most common reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Insufficient resources: The node may be low on memory or disk space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Issues with the kubelet process: The process may have crashed or stopped on
    the node. Therefore, it cannot communicate with the API server running on any
    of the control plane nodes anymore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Issues with kube-proxy: The Pod running kube-proxy is responsible for network
    communication from within the cluster and from the outside. The Pod transitioned
    into a nonfunctional state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSH into the relevant worker node(s) and start your investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Checking available resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A good way to identify the root cause of an unavailable worker node is to look
    at its details. The `describe node` command renders the section labeled “Conditions”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The table contains information about the resources available to the node, as
    well as an indication of other services like networking. See if any of the resource
    types render the status `True` or `Unknown`, which means that there’s an issue
    with the particular resource. You can further troubleshoot unavailable resources
    with a system-level command.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check on memory and the number of processes running, use the `top` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To check on the available disk space, use the command `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Checking the kubelet process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some conditions rendered by the `describe node` command mention the kubelet
    process. If you look at the `Message` column, you might get an idea if the kubelet
    process is running properly. To troubleshoot a misbehaving kubelet process, run
    the following `systemctl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important information in the output is the value of the `Active` attribute.
    If it says something other than “active (running),” then you will need to dig
    deeper. Use `journalctl` to take a look at the log files of the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You will want to restart the process once you have identified the issue in
    the logs and fixed it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Checking the certificate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, the certificate used by the kubelet can expire. Make sure that the
    values for the attributes `Issuer` and `Not After` are correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Checking the kube-proxy Pod
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The kube-proxy components run in a set of dedicated Pods in the namespace `kube-system`.
    You can clearly identify the Pods by their naming prefix `kube-proxy` and the
    appended hash. Verify if any of the Pods states a different status than “Running.”
    Each of the kube-proxy Pods runs on a dedicated worker node. You can add the `-o
    wide` option to render the node the Pod is running on in a new column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the event log for kube-proxy Pods that seem to have an issue.
    The following command describes the Pod named `kube-proxy-csrww`. In addition,
    you might find more information in the event log of the corresponding DaemonSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The logs may come in handy as well. You will be able to check the logs only
    for the kube-proxy Pod that runs on the specific worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a Kubernetes administrator, you need to be capable of identifying and fixing
    application and cluster component issues.
  prefs: []
  type: TYPE_NORMAL
- en: Logging is essential for tracing application flows and capturing potential error
    messages. You can configure logging on a cluster level and a node level, each
    of which comes with its own benefits and potential drawbacks. Depending on the
    log capturing approach, aspects such as log rotation and logging backend service
    may be incorporated. On the Pod level, you can directly ask for the application
    logs using the `kubectl logs` command. Use the command-line option `-c` to target
    a specific container in a multi-container setup.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes’ native monitoring service, the metrics server, can be installed
    on the cluster to collect and aggregate Pod and node resource utilization data.
    Nodes send metrics via the kubelet to the centralized metrics server. End users
    can use the `kubectl top` command to render those metrics as a means to identify
    excessive resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: Misconfiguration of workload and networking objects can lead to applications
    mishaps. You need to be familiar with the strategies relevant for diagnosing root
    causes and how fix them. A cluster may also develop error states leading to a
    variety of operational issues. You need to know which cluster components and processes
    run on control plane nodes and worker nodes. We discussed strategies for tackling
    different failure situations.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understand logging configuration on a theoretical level
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes logging is not a built-in capability. You have to configure logging
    proactively on the cluster or the node level. Compare the different approaches
    and their potential trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Make accessing container logs your daily bread and butter
  prefs: []
  type: TYPE_NORMAL
- en: Accessing container logs is straightforward. Simply use the `logs` command.
    Practice the use of all relevant command-line options. The option `-c` targets
    a specific container. The option does not have to be used explicitly for single-container
    Pods. The option `-f` tails the log entries if you want to see live processing
    in an application. The `-p` option can be used for accessing logs if the container
    needed to be restarted, but you still want to take a look at the previous container
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: Install and use the metrics server
  prefs: []
  type: TYPE_NORMAL
- en: The metrics server isn’t installed on a Kubernetes cluster by default. Go through
    the motions of installing the service. For the exam, you can assume that the metrics
    server is already available. Use the `top` command for Pods and nodes to identify
    resource consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Know how to troubleshoot applications
  prefs: []
  type: TYPE_NORMAL
- en: Applications running in a Pod can easily break due to misconfiguration. Think
    of possible scenarios that can occur and try to model them proactively to represent
    a failure situation. Then using the commands `get`, `logs`, and `exec`, get to
    the bottom of the issue and fix it. Try to dream up obscure scenarios to become
    more comfortable with finding and fixing application issues for different resource
    types.
  prefs: []
  type: TYPE_NORMAL
- en: Know how to troubleshoot clusters
  prefs: []
  type: TYPE_NORMAL
- en: Control-plane and worker nodes can become unresponsive or dysfunctional for
    a variety of reasons. Administrators need to take care of their cluster’s health
    in order to keep it operational and scalable. Try to emulate error scenarios that
    may occur and apply the discussed troubleshooting techniques to identify and fix
    the underlying root cause.
  prefs: []
  type: TYPE_NORMAL
- en: Sample Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solutions to these exercises are available in the [Appendix](app01.xhtml#appendix-a).
  prefs: []
  type: TYPE_NORMAL
- en: You are supposed to implement cluster-level logging with a sidecar container.
    Create a multi-container Pod named `multi`. The main application container named
    `nginx` should use the image `nginx:1.21.6`. The sidecar container named `streaming`
    uses the image `busybox:1.35.0` and the arguments `/bin/sh`, `-c`, and `'tail
    -n+1 -f /var/log/nginx/access.log'`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a volume of type `emptyDir` for the Pod and mount it to the path `/var/log/nginx`
    for both containers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access the endpoint of the nginx service a couple of times using a `wget` or
    `curl` command. Inspect the logs of the sidecar container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create two Pods named `stress-1` and `stress-2`. Define a container that uses
    the image `polinux/stress:1.0.4` with the command `stress` and the arguments `/bin/sh`,
    `-c`, and `'stress --vm 1 --vm-bytes $(shuf -i 20-200 -n 1)M --vm-hang 1'`. Set
    the container memory resource limits and requests to 250Mi.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the data available through the metrics server to identify which of the Pods,
    `stress-1` or `stress-2`, consumes the most memory. Write the name of the Pod
    to the file `max-memory.txt`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the directory `app-a/ch07/troubleshooting-pod` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file [`instructions.md`](https://oreil.ly/j9lTr) for troubleshooting
    a faulty Pod setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the directory `app-a/ch07/troubleshooting-deployment` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file `instructions.md` for troubleshooting a faulty Deployment
    setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the directory `app-a/ch07/troubleshooting-service` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file [`instructions.md`](https://oreil.ly/Z9kou) for troubleshooting
    a faulty Service setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the directory `app-a/ch07/troubleshooting-control-plane-node` of
    the checked-out GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8).
    Follow the instructions in the file [`instructions.md`](https://oreil.ly/lGygu)
    for troubleshooting a faulty control plane node setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prerequisite:* This exercise requires the installation of the tools [Vagrant](https://oreil.ly/sasln)
    and [VirtualBox](https://oreil.ly/9Cvg9).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the directory `app-a/ch07/troubleshooting-worker-node` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file [`instructions.md`](https://oreil.ly/Kyh58) for troubleshooting
    a faulty worker node setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prerequisite:* This exercise requires the installation of the tools [Vagrant](https://oreil.ly/sasln)
    and [VirtualBox](https://oreil.ly/9Cvg9).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
