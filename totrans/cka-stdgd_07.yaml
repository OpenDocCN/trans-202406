- en: Chapter 7\. Troubleshooting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 故障排除
- en: Establishing a Kubernetes cluster is one thing. Making sure that the cluster
    stays operational is another. As a Kubernetes administrator, you are continuously
    confronted with making sure that the cluster stays functional. Therefore, your
    troubleshooting skills must be sharp so that you can come up with strategies for
    identifying the root cause of an issue and fixing it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 建立Kubernetes集群是一回事。确保集群保持运行是另一回事。作为Kubernetes管理员，您不断面临确保集群保持功能性的挑战。因此，您的故障排除技能必须精湛，以便能够提出识别问题根本原因和修复问题的策略。
- en: Of all the domains covered by the exam, the section “Troubleshooting” has the
    highest weight for the overall score, so it’s important to understand failure
    scenarios and learn how to fix them. This chapter will address how to monitor
    and troubleshoot applications in different constellations. Furthermore, we’ll
    discuss failures that may arise for cluster components due to misconfiguration
    or error conditions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在考试涵盖的所有领域中，“故障排除”部分对总体分数的影响最大，因此理解故障场景并学会如何解决它们至关重要。本章将讨论如何在不同情况下监视和故障排除应用程序。此外，我们将讨论可能由于错误配置或错误条件而导致集群组件失败的情况。
- en: 'At a high level, this chapter covers the following concepts:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，本章涵盖了以下概念：
- en: Evaluating logging options
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估日志记录选项
- en: Monitoring applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控应用程序
- en: Accessing container logs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问容器日志
- en: Troubleshooting application failures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除应用程序故障
- en: Troubleshooting cluster failures
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除集群故障
- en: Evaluating Cluster and Node Logging
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估集群和节点日志记录
- en: A real-world Kubernetes cluster manages hundreds or even thousands of Pods.
    For every Pod, you have at least a single container running a process. Each process
    can produce log output to the standard output or standard error streams. It’s
    imperative to capture the log output to proficiently determine the root cause
    of an application error. Moreover, cluster components produce logs for diagnostic
    purposes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的Kubernetes集群管理着数百甚至数千个Pod。对于每个Pod，至少有一个运行进程的容器。每个进程可以将日志输出到标准输出或标准错误流。捕获日志输出以有效地确定应用程序错误的根本原因至关重要。此外，集群组件生成用于诊断目的的日志。
- en: As you can see, Kubernetes’ logging mechanism is crucial for tracking down errors
    and monitoring cluster components and applications. Kubernetes can be configured
    to log on the cluster or the node level. The implementation approaches and their
    potential trade-offs may differ from one another.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Kubernetes的日志记录机制对于追踪错误和监视集群组件和应用程序至关重要。可以配置Kubernetes在集群或节点级别上记录日志。实施方法及其潜在的权衡可能各不相同。
- en: Cluster Logging
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群日志记录
- en: 'Kubernetes doesn’t provide a native solution for cluster-level logging, but
    you can choose from the following three options to fulfill the requirements:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes并没有为集群级别的日志记录提供原生解决方案，但您可以从以下三个选项中选择以满足需求：
- en: Instantiating a node-level logging agent that runs on each of the cluster nodes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化运行在每个集群节点上的节点级别日志代理
- en: Configuring a sidecar container responsible for handling the application logs
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置负责处理应用程序日志的边车容器
- en: Pushing the logs directly to a logging backend from the application logic
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接从应用程序逻辑推送日志到日志后端
- en: The following discussion explains the benefits and drawbacks for each approach.
    For a detailed discussion, see the [Kubernetes documentation](https://oreil.ly/rJjfV).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的讨论解释了每种方法的优缺点。要详细讨论，请参阅[Kubernetes文档](https://oreil.ly/rJjfV)。
- en: Using a node logging agent
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用节点日志代理
- en: The logging agent is a dedicated tool that publishes the logs to a backend.
    A backend can be an external logging service outside of the cluster. [Figure 7-1](#cluster-logging-agent)
    visualizes the logging architecture.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 日志代理是一个专用工具，将日志发布到后端。后端可以是集群外的外部日志服务。[图 7-1](#cluster-logging-agent)展示了日志架构。
- en: '![ckas 0701](Images/ckas_0701.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![ckas 0701](Images/ckas_0701.png)'
- en: Figure 7-1\. Cluster-level logging with an agent
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1. 使用代理进行集群级别日志记录
- en: The benefit of this approach is that the application doesn’t require any changes
    to the code or the Pod configuration to support collecting logs. Agents should
    be run as a DaemonSet.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的好处在于，应用程序不需要更改代码或Pod配置来支持收集日志。代理应作为DaemonSet运行。
- en: Using a sidecar container
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用边车容器
- en: A Pod can be configured to run another sidecar container alongside the main
    application container. The sidecar container streams standard output and error
    produced by the application and redirects the streams to a different location
    (e.g., a logging backend or a volume mounted to the container). [Figure 7-2](#cluster-logging-sidecar)
    shows the logging setup of a Pod that incorporates a streaming sidecar.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可以配置 Pod 来运行另一个侧车容器，与主应用程序容器并行运行。侧车容器会流式传输应用程序生成的标准输出和错误，并将流重定向到不同的位置（例如日志后端或挂载到容器的卷）。[图
    7-2](#cluster-logging-sidecar) 展示了集成流式侧车的 Pod 的日志设置。
- en: '![ckas 0702](Images/ckas_0702.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![ckas 0702](Images/ckas_0702.png)'
- en: Figure 7-2\. Cluster-level logging with a sidecar container
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 使用侧车容器进行集群级别的日志记录
- en: This approach has the benefit of being able to easily separate different streams
    (e.g., separating error from info log entries).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的好处在于可以轻松地分离不同的流（例如，将错误日志与信息日志分开）。
- en: Pushing directly to logging backend
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接推送到日志后端
- en: This approach pushes the responsibility onto the application without adding
    a middleman. [Figure 7-3](#cluster-logging-direct) shows the logging setup.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将责任推给应用程序而没有添加中间人。[图 7-3](#cluster-logging-direct) 展示了直接推送到后端的日志设置。
- en: '![ckas 0703](Images/ckas_0703.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![ckas 0703](Images/ckas_0703.png)'
- en: Figure 7-3\. Cluster-level logging by directly pushing to the backend
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 通过直接推送到后端的集群级别的日志记录
- en: While architecturally less complex, any change to the logging backend will require
    a change to the application code and therefore a new deployment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在架构上较少复杂，但任何对日志后端的更改都将要求修改应用程序代码，并因此需要新的部署。
- en: Node Logging
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点日志
- en: Node logging comes with the implication that the log files will be stored on
    the cluster node. The container runtime (e.g., Docker Engine) redirects standard
    output and error streams to the storage of the node with the help of the configured
    logging driver.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 节点日志意味着日志文件将存储在集群节点上。容器运行时（例如 Docker Engine）通过配置的日志驱动程序将标准输出和错误流重定向到节点的存储中。
- en: To avoid filling up the node storage with logging content, log rotation should
    be implemented. Log rotation is an automated process for compressing, moving,
    deleting, and/or archiving log data that grow beyond a certain threshold. The
    Linux tool [logrotate](https://oreil.ly/DE7XB) is one way to configure log rotation
    for a Kubernetes cluster. [Figure 7-4](#node-logging) visualizes the node-level
    architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为避免填满节点存储的日志内容，应实施日志轮换。日志轮换是一种自动化过程，用于压缩、移动、删除和/或归档超过某一阈值的日志数据。Linux 工具 [logrotate](https://oreil.ly/DE7XB)
    是配置 Kubernetes 集群日志轮换的一种方式。[图 7-4](#node-logging) 可视化了节点级别的架构。
- en: '![ckas 0704](Images/ckas_0704.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![ckas 0704](Images/ckas_0704.png)'
- en: Figure 7-4\. Node-level logging
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 节点级别的日志记录
- en: When you run `kubectl logs`, the kubelet receives the request, reads directly
    from the log file on the node, and returns the content to the client. The `kubectl
    logs` command returns only the latest log content, not the log entries that have
    already been archived.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行 `kubectl logs` 命令时，kubelet 收到请求后直接从节点上的日志文件读取，并将内容返回给客户端。`kubectl logs`
    命令仅返回最新的日志内容，而不是已经归档的日志条目。
- en: The cluster components kube-scheduler and kube-proxy run in a container. Therefore,
    the log handling is the same as for any other application container. For system
    components that do not run in the container (e.g., the kubelet and the container
    runtime), logs will be written to journald if systemd is available. If systemd
    is not available, system components write their log files to the directory `/var/log`
    with the file extension `.log`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 集群组件 kube-scheduler 和 kube-proxy 运行在容器中。因此，日志处理与任何其他应用程序容器相同。对于不在容器中运行的系统组件（例如
    kubelet 和容器运行时），如果存在 systemd，则将日志写入 journald。如果没有 systemd，则系统组件将其日志文件写入目录 `/var/log`，文件扩展名为
    `.log`。
- en: Monitoring Cluster Components and Applications
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控集群组件和应用程序
- en: Deploying software to a Kubernetes cluster is only the start of operating an
    application long-term. Developers and administrators alike need to understand
    resource consumption patterns and behaviors of their applications with the goal
    of providing a scalable and reliable service.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将软件部署到 Kubernetes 集群只是长期运行应用程序的开始。开发人员和管理员都需要了解其应用程序的资源消耗模式和行为，以提供可扩展和可靠的服务。
- en: 'In the Kubernetes world, monitoring tools like Prometheus and Datadog help
    with collecting, processing, and visualizing the information over time. The exam
    does not expect you to be familiar with commercial monitoring, logging, tracing,
    and aggregation tools; however, it is helpful to gain a rough understanding of
    the underlying Kubernetes infrastructure responsible for collecting usage metrics.
    The following list shows examples of typical metrics:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 的世界中，像 Prometheus 和 Datadog 这样的监控工具帮助收集、处理和可视化信息。考试不要求您熟悉商业监控、日志记录、跟踪和聚合工具；但是，了解负责收集使用情况指标的底层
    Kubernetes 基础设施的基本情况会有所帮助。以下列表显示了典型指标的示例：
- en: Number of nodes in the cluster
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的节点数
- en: Health status of nodes
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的健康状态
- en: Node performance metrics such as CPU, memory, disk space, network
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点性能指标如 CPU、内存、磁盘空间、网络
- en: Pod-level performance metrics such as CPU and memory consumption
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 级别的性能指标如 CPU 和内存消耗
- en: This responsibility falls into the hands of the [metrics server](https://oreil.ly/OycST),
    a cluster-wide aggregator of resource usage data. As shown in [Figure 7-5](#metrics-server),
    kubelets running on nodes collect the metrics and send them to the metrics server.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这一责任落在[度量服务器](https://oreil.ly/OycST)手中，它是整个集群中资源使用数据的聚合器。如[图 7-5](#metrics-server)所示，运行在节点上的
    kubelet 收集指标并将其发送到度量服务器。
- en: '![ckas 0705](Images/ckas_0705.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![ckas 0705](Images/ckas_0705.png)'
- en: Figure 7-5\. Data collection for the metrics server
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 用于度量服务器的数据收集
- en: 'The metrics server stores data in memory and does not persist data over time.
    If you are looking for a solution that keeps historical data, then you need to
    look into commercial options. Refer to the documentation for more information
    on its installation process. If you’re using Minikube as your practice environment,
    [enabling the metrics-server add-on](https://oreil.ly/NanXK) is straightforward
    using the following command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 度量服务器将数据存储在内存中，不会持久化数据。如果您正在寻找保留历史数据的解决方案，那么您需要考虑商业选项。有关其安装过程的更多信息，请参阅文档。如果您在使用
    Minikube 作为练习环境，则可以使用以下命令简单地[启用度量服务器插件](https://oreil.ly/NanXK)：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can now query for metrics of cluster nodes and Pods with the `top` command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用 `top` 命令查询集群节点和 Pod 的指标：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Troubleshooting Application Failures
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除应用程序故障
- en: When operating an application in a production Kubernetes cluster, it’s almost
    inevitable that you’ll come across failure situations. It’s your responsibility
    as an administrator (potentially working closely with the application developer)
    to troubleshoot issues with deployed Kubernetes objects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产 Kubernetes 集群中运行应用时，几乎不可避免地会遇到故障情况。作为管理员，您有责任（可能与应用程序开发人员密切合作）调试已部署的 Kubernetes
    对象的问题。
- en: In this section, we’re going to take a look at debugging strategies that can
    help with identifying the root cause of an issue so that you can take action and
    correct the failure appropriately. For more information, reference the [Kubernetes
    documentation](https://oreil.ly/4pxVS).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨可以帮助识别问题根本原因的调试策略，以便您可以采取行动并适当地修正故障。有关更多信息，请参考[Kubernetes 文档](https://oreil.ly/4pxVS)。
- en: Troubleshooting Pods
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障排除 Pods
- en: In most cases, creating a Pod is no issue. You simply emit the `run`, `create`,
    or `apply` commands to instantiate the Pod. If the YAML manifest is formed properly,
    Kubernetes accepts your request, so the assumption is that everything works as
    expected. To verify the correct behavior, the first thing you’ll want to do is
    to check the high-level runtime information of the Pod. The operation could involve
    other Kubernetes objects like a Deployment responsible for rolling out multiple
    replicas of a Pod.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，创建 Pod 不会出现问题。您只需使用 `run`、`create` 或 `apply` 命令来实例化 Pod。如果 YAML 文件清单格式正确，Kubernetes
    将接受您的请求，因此假设一切正常运行。为了验证正确行为，您首先需要检查 Pod 的高级运行时信息。此操作可能涉及其他 Kubernetes 对象，如负责部署多个
    Pod 副本的 Deployment。
- en: Retrieving high-level information
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索高级信息
- en: To retrieve the information, run either the `kubectl get pods` command for just
    the Pods running in the namespace or the `kubectl get all` command to retrieve
    the most prominent object types in the namespace (which includes Deployments).
    You will want to take a look at the columns `READY`, `STATUS`, and `RESTARTS`.
    In the optimal case, the number of ready containers matches the number of containers
    expected to be created by the Pod. For a single-container Pod, the `READY` column
    would say 1/1\. The status should say `Running` to indicate that the Pod entered
    the proper lifecycle state. Be aware that it’s totally possible that a Pod renders
    a `Running` state, but the application isn’t actually working properly. If the
    number of restarts is greater than 0, then you might want to check the logic of
    the liveness probe (if defined) and identify the reason why a restart was necessary.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 若要检索信息，请运行 `kubectl get pods` 命令来获取命名空间中正在运行的 Pods 的信息，或者运行 `kubectl get all`
    命令来检索命名空间中最重要的对象类型（包括 Deployments）。你需要查看 `READY`、`STATUS` 和 `RESTARTS` 列。在最佳情况下，准备好的容器数量应该与
    Pod 预期创建的容器数量匹配。对于单容器 Pod，`READY` 列应为 1/1。状态应为 `Running`，表示 Pod 进入了正确的生命周期状态。请注意，可能存在一个
    Pod 处于 `Running` 状态，但实际上应用程序未能正常工作。如果重启次数大于 0，则可能需要检查存活探测器（如果定义了）的逻辑，并确定为何需要重启。
- en: 'The following Pod observes the status `ErrImagePull` and makes 0/1 containers
    available to incoming traffic. In short, this Pod has a problem:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Pod 观察到 `ErrImagePull` 状态，并且没有可用于入站流量的容器（0/1）。简言之，这个 Pod 存在问题：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After working with Kubernetes for a while, you’ll automatically recognize common
    error conditions. [Table 7-1](#common_pod_error_statuses) lists some of those
    error statuses and explains how to fix them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Kubernetes 一段时间后，您将自动识别常见的错误条件。[表 7-1](#common_pod_error_statuses) 列出了一些常见的错误状态，并解释了如何修复它们。
- en: Table 7-1\. Common Pod error statuses
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 常见 Pod 错误状态
- en: '| Status | Root cause | Potential fix |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 根本原因 | 潜在修复方法 |'
- en: '| --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `ImagePullBackOff` or `ErrImagePull` | Image could not be pulled from registry.
    | Check correct image name, check that image name exists in registry, verify network
    access from node to registry, ensure proper authentication. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `ImagePullBackOff` 或 `ErrImagePull` | 无法从注册表拉取图像。 | 检查正确的图像名称，检查图像名称在注册表中是否存在，验证节点到注册表的网络访问，确保适当的身份验证。
    |'
- en: '| `CrashLoopBackOff` | Application or command run in container crashes. | Check
    command executed in container, ensure that image can properly execute (e.g., by
    creating a container with Docker). |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `CrashLoopBackOff` | 容器中运行的应用程序或命令崩溃。 | 检查容器中执行的命令，确保图像可以正确执行（例如，通过使用 Docker
    创建容器）。 |'
- en: '| `CreateContainerConfigError` | ConfigMap or Secret referenced by container
    cannot be found. | Check correct name of the configuration object, verify the
    existence of the configuration object in the namespace. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `CreateContainerConfigError` | 容器引用的 ConfigMap 或 Secret 未找到。 | 检查配置对象的正确名称，在命名空间中验证配置对象的存在性。
    |'
- en: Inspecting events
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查事件
- en: 'It’s totally possible that you’ll not encounter any of those error statuses.
    But there’s still a chance of the Pod having a configuration issue. You can retrieve
    detailed information about the Pod and its events using the `kubectl describe
    pod` command to inspect its events. The following output belongs to a Pod that
    tries to mount a Secret that doesn’t exist. Instead of rendering a specific error
    message, the Pod gets stuck with the status `ContainerCreating`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 完全可能您不会遇到这些错误状态中的任何一个。但 Pod 仍可能存在配置问题的可能性。您可以使用 `kubectl describe pod` 命令检索有关
    Pod 及其事件的详细信息。以下输出属于尝试挂载不存在的 Secret 的 Pod。而不是显示具体的错误消息，该 Pod 卡在 `ContainerCreating`
    状态：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Another helpful command is `kubectl get events`. The output of the command
    lists the events across all Pods for a given namespace. You can use additional
    command-line options to further filter and sort events:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的命令是 `kubectl get events`。该命令的输出列出了给定命名空间中所有 Pods 的事件。您可以使用额外的命令行选项进一步过滤和排序事件：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Inspecting logs
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查日志
- en: When debugging a Pod, the next level of details can be retrieved by downloading
    and inspecting its logs. You may or may not find additional information that points
    to the root cause of a misbehaving Pod. It’s definitely worth a look. The YAML
    manifest shown in [Example 7-1](#pod_failing_command) defines a Pod running a
    shell command.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试 Pod 时，可以通过下载和检查其日志来获取下一级别的详细信息。您可能会找到指向行为不端 Pod 根本原因的额外信息，也可能找不到。但值得一试。在
    [示例 7-1](#pod_failing_command) 中显示的 YAML 清单定义了运行 shell 命令的 Pod。
- en: Example 7-1\. A Pod running a failing shell command
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-1\. 运行失败的 shell 命令的 Pod
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After creating the object, the Pod fails with the status `CrashLoopBackOff`.
    Running the `logs` command reveals that the command run in the container has an
    issue:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 创建对象后，Pod 的状态为 `CrashLoopBackOff`。运行 `logs` 命令显示容器中运行的命令存在问题：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `logs` command provides two helpful options I’d like to mention here. The
    option `-f` streams the logs, meaning you’ll see new log entries as they’re being
    produced in real time. The option `--previous` gets the logs from the previous
    instantiation of a container, which is helpful if the container has been restarted.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 命令 `logs` 提供了两个有用的选项，在此我想提一下。选项 `-f` 可以实时流式传输日志，意味着您会看到新的日志条目随着其实时生成而出现。选项 `--previous`
    获取容器上一次实例化的日志，如果容器已重新启动，则此选项非常有用。
- en: Opening an Interactive Shell
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打开交互式 Shell
- en: If any of the previous commands do not point you to the root cause of the failing
    Pod, it’s time to open an interactive shell to a container. As an application
    developer, you’ll probably know best what behavior to expect from the application
    at runtime. Ensure that the correct configuration has been created and inspect
    the running processes by using the Unix or Windows utility tools, depending on
    the image run in the container.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果之前的任何命令未指向故障 Pod 的根本原因，那么现在是打开容器交互式 shell 的时候了。作为应用程序开发者，您可能最了解运行时应用程序的预期行为。确保已创建正确的配置，并使用运行在容器中的
    Unix 或 Windows 实用工具检查正在运行的进程。
- en: Say you encounter a situation where a Pod seems to work properly on the surface,
    as shown in [Example 7-2](#pod_current_date).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您遇到一个情况，一个 Pod 表面上似乎正常工作，如 [示例 7-2](#pod_current_date) 所示。
- en: Example 7-2\. A Pod periodically writing the current date to a file
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-2\. 一个周期性地将当前日期写入文件的 Pod
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After creating the Pod, you check the status. It says `Running`; however, when
    making a request to the application, the endpoint reports an error. Next, you
    check the logs. The log output renders an error message that points to a nonexistent
    directory. Apparently, the directory hasn’t been set up correctly but is needed
    by the application:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Pod 后，检查其状态。状态显示为 `Running`；但是，在向应用程序发出请求时，端点报告错误。接下来，您检查日志。日志输出显示了指向不存在目录的错误消息。显然，目录尚未正确设置，但应用程序需要它：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `exec` command opens an interactive shell to further investigate the issue.
    Below, we’re using the Unix tools `mkdir`, `cd`, and `ls` inside of the running
    container to fix the problem. Obviously, the better mitigation strategy is to
    create the directory from the application or provide an instruction in the Dockerfile:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 命令 `exec` 打开一个交互式 shell 以进一步调查问题。下面，我们在运行中的容器内使用 Unix 工具 `mkdir`、`cd` 和 `ls`
    来解决问题。显然，更好的缓解策略是从应用程序中创建目录或在 Dockerfile 中提供指令：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Troubleshooting Services
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障排除服务
- en: A Service provides a unified network interface for Pods. For full coverage on
    networking aspects in Kubernetes, see [Chapter 5](ch05.xhtml#services_networking).
    Here, I want to point out troubleshooting techniques for this primitive.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 服务为 Pod 提供统一的网络接口。有关 Kubernetes 网络方面的全面覆盖，请参阅 [第 5 章](ch05.xhtml#services_networking)。在此，我想指出这个原始的故障排除技术。
- en: 'In case you can’t reach the Pods that should map to the Service, start by ensuring
    that the label selector matches with the assigned labels of the Pods. You can
    query the information by describing the Service and then render the labels of
    the available Pods with the option `--show-labels`. The following example does
    not have matching labels and therefore wouldn’t apply to any of the Pods running
    in the namespace:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法访问应该映射到服务的 Pod，请先确保标签选择器与 Pod 的分配标签匹配。您可以通过描述服务查询信息，然后使用选项 `--show-labels`
    渲染可用 Pod 的标签。以下示例没有匹配的标签，因此不适用于命名空间中运行的任何 Pod：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Alternatively, you can also query the endpoints of the Service instance. Say
    you expected three Pods to be selected by a matching label but only two have been
    exposed by the Service. You’ll want to look at the label selection criteria:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您还可以查询 Service 实例的端点。假设您期望有三个 Pod 被匹配的标签选择，但只暴露了两个。您将需要查看标签选择条件：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A common source of confusion is the type of a Service. By default, the Service
    type is `ClusterIP`, which means that a Pod can be reached through the Service
    only if queried from the same node inside of the cluster. First, check the Service
    type. If you think that `ClusterIP` is the proper type you wanted to assign, open
    an interactive shell from a temporary Pod inside the cluster and run a `curl`
    or `wget` command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的困惑来源是 Service 的类型。默认情况下，Service 类型是`ClusterIP`，这意味着只有从集群内的同一节点查询时，才能通过
    Service 访问 Pod。首先检查 Service 的类型。如果您认为`ClusterIP`是您想要分配的正确类型，请从集群内的临时 Pod 打开交互式
    shell，并运行`curl`或`wget`命令：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, check if the port mapping from the target port of the Service to the
    container port of the Pod is configured correctly. Both ports need to match or
    the network traffic wouldn’t be routed properly:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，检查从 Service 的目标端口到 Pod 的容器端口的端口映射是否正确配置。这两个端口需要匹配，否则网络流量将无法正确路由：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Troubleshooting Cluster Failures
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除集群故障
- en: 'There are many influencing factors that can render a Kubernetes cluster faulty
    on the component level. It’s a good idea to list the nodes available in the cluster
    to identify potential issues:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 许多影响因素可以在组件级别上导致 Kubernetes 集群出现故障。列出集群中可用的节点是一个好主意，以识别潜在问题：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output will give you a lay of the land. You can easily identify the responsibility
    of each node from the `ROLES` column, the Kubernetes version used, and the current
    health status.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将为您提供一览。您可以从`ROLES`列、使用的 Kubernetes 版本以及当前的健康状态轻松地识别每个节点的责任。
- en: 'There are a couple of things to look out for when identifying issues at a high
    level:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在高级别识别问题时需要注意几点：
- en: Is the health status for the node anything other than “Ready”?
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的健康状态是否除“Ready”之外的其他状态？
- en: Does the version of a node deviate from the version of other nodes?
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的版本是否与其他节点的版本不一致？
- en: In the following sections you can find individual sections on troubleshooting
    control plane nodes versus worker nodes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您可以找到关于故障排除控制平面节点与工作节点的各自部分。
- en: Troubleshooting Control Plane Nodes
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障排除控制平面节点
- en: Control plane nodes are the critical components for keeping a cluster operational.
    As described in [“Managing a Highly Available Cluster”](ch02.xhtml#managing_ha_cluster),
    a cluster can consist of more than one control plane node to ensure a high degree
    of uptime. Detecting that one of the control plane nodes is faulty should be treated
    with extreme urgency to avoid compromising high-availability characteristics.
    For more information on troubleshooting techniques and root-cause analysis, reference
    the [Kubernetes documentation](https://oreil.ly/KWeTt).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面节点是保持集群运行的关键组件。如[“管理高可用集群”](ch02.xhtml#managing_ha_cluster)中所述，集群可以由多个控制平面节点组成，以确保高度的可用性。检测到其中一个控制平面节点故障应被视为极端紧急，以避免损害高可用特性。有关故障排除技术和根本原因分析的更多信息，请参考[Kubernetes
    文档](https://oreil.ly/KWeTt)。
- en: Rendering cluster information
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渲染集群信息
- en: 'To further diagnose issues on the control plane node, run the command `kubectl
    cluster-info`. As you can see in the following output, the command renders the
    addresses of the control plane and other cluster services:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步诊断控制平面节点上的问题，请运行命令`kubectl cluster-info`。如下输出所示，该命令呈现了控制平面和其他集群服务的地址：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For a detailed view of the cluster logs, append the `dump` subcommand. Due
    to the pages and pages of log messages, we won’t render the output in this book.
    Parse through the message to see if you can find any errors:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细查看集群日志，请添加`dump`子命令。由于页面和页面的日志消息，我们不会在本书中渲染输出。解析消息以查找是否存在任何错误：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Inspecting control plane components
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查控制平面组件
- en: 'Among those [components available on the control plane node](https://oreil.ly/IZR8Z)
    are the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制平面节点上[可用的组件](https://oreil.ly/IZR8Z)包括以下内容：
- en: 'kube-apiserver: Exposes the Kubernetes API used by clients like `kubectl` for
    managing objects.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kube-apiserver：为像`kubectl`这样的客户端公开 Kubernetes API，用于管理对象。
- en: 'etcd: A key-value store for storing the cluster data.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd：用于存储集群数据的键值存储。
- en: 'kube-scheduler: Selects nodes for Pods that have been scheduled but not created.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kube-scheduler：为已计划但未创建的Pod选择节点。
- en: 'kube-controller-manager: Runs controller processes (e.g., the job controller
    responsible for Job object execution).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kube-controller-manager：运行控制器进程（例如，负责Job对象执行的作业控制器）。
- en: 'cloud-controller-manager: Links cloud provider–specific API to the Kubernetes
    cluster. This controller is not available in on-premise cluster installations
    of Kubernetes.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cloud-controller-manager：将特定于云提供商的API链接到Kubernetes集群。在Kubernetes的本地集群安装中，此控制器不可用。
- en: 'To discover those components and their status, list the Pods available in the
    namespace `kube-system`. Here, you can find the list of control-plane components
    on Minikube:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找这些组件及其状态，请列出命名空间`kube-system`中可用的Pods。在这里，你可以找到Minikube上控制平面组件的列表：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Any status that does not show “Running” should be inspected further. You can
    retrieve the logs for control-plane component Pods in the same fashion you do
    for any other Pod, using the `logs` command. The following command downloads the
    logs for the kube-apiserver component:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 任何状态不显示“Running”的情况都应进一步检查。你可以像检索其他Pod的日志一样，使用`logs`命令检索控制平面组件Pod的日志。以下命令下载kube-apiserver组件的日志：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Troubleshooting Worker Nodes
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障排除工作节点
- en: Worker nodes are responsible for managing the workload. Make sure you have a
    sufficient number of worker nodes available to distribute the load. For a deeper
    discussion on how to join worker nodes to a cluster, see [Chapter 2](ch02.xhtml#cluster_architecture_installation_configuration).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点负责管理工作负载。确保有足够数量的工作节点可用以分配负载。有关如何加入工作节点到集群的更深入讨论，请参阅[第2章](ch02.xhtml#cluster_architecture_installation_configuration)。
- en: 'Any of the nodes available in a cluster can transition into an error state.
    It’s your job as a Kubernetes administrator to identify those situations and fix
    them in a timely manner. When listing the nodes of a cluster, you may see that
    a worker node is not in the “Ready” state, which is a good indicator that it’s
    not available to handle the workload. In the output of the `get nodes` command,
    you can see that the node named `worker-1` is in the “NotReady” state:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的任何节点都可以转入错误状态。作为Kubernetes管理员，你的工作是及时识别这些情况并加以修复。当列出集群节点时，你可能会看到一个工作节点不处于“Ready”状态，这表明它无法处理工作负载。在`get
    nodes`命令的输出中，你可以看到名为`worker-1`的节点处于“NotReady”状态：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The “NotReady” state means that the node is unused and will accumulate operational
    costs without actually scheduling workload. There might be a variety of reasons
    why the node entered this state. The following list shows the most common reasons:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: “NotReady”状态意味着节点未使用，并将积累操作成本而未实际安排工作负载。节点进入此状态可能有各种原因。以下列表显示了最常见的原因：
- en: 'Insufficient resources: The node may be low on memory or disk space.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源不足：节点可能内存或磁盘空间不足。
- en: 'Issues with the kubelet process: The process may have crashed or stopped on
    the node. Therefore, it cannot communicate with the API server running on any
    of the control plane nodes anymore.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet进程存在问题：该进程可能已崩溃或在节点上停止运行。因此，它无法再与任何控制平面节点上运行的API服务器进行通信。
- en: 'Issues with kube-proxy: The Pod running kube-proxy is responsible for network
    communication from within the cluster and from the outside. The Pod transitioned
    into a nonfunctional state.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kube-proxy存在问题：运行kube-proxy的Pod负责集群内外的网络通信。该Pod已转入非功能状态。
- en: SSH into the relevant worker node(s) and start your investigation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: SSH登录相关工作节点并开始调查。
- en: Checking available resources
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查可用资源
- en: 'A good way to identify the root cause of an unavailable worker node is to look
    at its details. The `describe node` command renders the section labeled “Conditions”:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 确定不可用工作节点根本原因的一种好方法是查看其详细信息。`describe node`命令呈现标记为“Conditions”的部分：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The table contains information about the resources available to the node, as
    well as an indication of other services like networking. See if any of the resource
    types render the status `True` or `Unknown`, which means that there’s an issue
    with the particular resource. You can further troubleshoot unavailable resources
    with a system-level command.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表格包含有关节点可用资源的信息，以及类似网络的其他服务的指示。查看是否有任何资源类型显示状态`True`或`Unknown`，这意味着特定资源存在问题。你可以进一步使用系统级命令排查不可用资源。
- en: 'To check on memory and the number of processes running, use the `top` command:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查内存使用情况和运行进程的数量，请使用`top`命令：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To check on the available disk space, use the command `df`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查可用磁盘空间，请使用 `df` 命令：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Checking the kubelet process
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查 kubelet 进程
- en: 'Some conditions rendered by the `describe node` command mention the kubelet
    process. If you look at the `Message` column, you might get an idea if the kubelet
    process is running properly. To troubleshoot a misbehaving kubelet process, run
    the following `systemctl` command:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由 `describe node` 命令生成的某些条件提到了 kubelet 进程。如果查看 `Message` 列，您可能会对 kubelet 进程是否正常运行有所了解。若要排除
    kubelet 进程的问题，请运行以下 `systemctl` 命令：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The most important information in the output is the value of the `Active` attribute.
    If it says something other than “active (running),” then you will need to dig
    deeper. Use `journalctl` to take a look at the log files of the process:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中最重要的信息是 `Active` 属性的值。如果除了“active (running)”之外还有其他内容，则需要深入挖掘。使用 `journalctl`
    查看进程的日志文件：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You will want to restart the process once you have identified the issue in
    the logs and fixed it:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在日志中确定了问题并进行了修复，您将需要重新启动进程：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Checking the certificate
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查证书
- en: 'Sometimes, the certificate used by the kubelet can expire. Make sure that the
    values for the attributes `Issuer` and `Not After` are correct:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，kubelet 使用的证书可能会过期。确保 `Issuer` 和 `Not After` 属性的值是正确的：
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Checking the kube-proxy Pod
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查 kube-proxy Pod
- en: 'The kube-proxy components run in a set of dedicated Pods in the namespace `kube-system`.
    You can clearly identify the Pods by their naming prefix `kube-proxy` and the
    appended hash. Verify if any of the Pods states a different status than “Running.”
    Each of the kube-proxy Pods runs on a dedicated worker node. You can add the `-o
    wide` option to render the node the Pod is running on in a new column:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy 组件在 `kube-system` 命名空间中的一组专用 Pods 中运行。您可以通过它们的命名前缀 `kube-proxy` 和附加的哈希来清楚地识别这些
    Pods。验证任何一个 kube-proxy Pod 的状态是否与“Running”不同。每个 kube-proxy Pod 都运行在专用的工作节点上。您可以添加
    `-o wide` 选项，在新列中渲染 Pod 所在的节点：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Take a look at the event log for kube-proxy Pods that seem to have an issue.
    The following command describes the Pod named `kube-proxy-csrww`. In addition,
    you might find more information in the event log of the corresponding DaemonSet:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 查看似乎存在问题的 kube-proxy Pods 的事件日志。以下命令描述了名为 `kube-proxy-csrww` 的 Pod。此外，您可能会在相应
    DaemonSet 的事件日志中找到更多信息：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The logs may come in handy as well. You will be able to check the logs only
    for the kube-proxy Pod that runs on the specific worker node:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 日志也可能很有用。您只能检查在特定工作节点上运行的 kube-proxy Pod 的日志：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As a Kubernetes administrator, you need to be capable of identifying and fixing
    application and cluster component issues.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Kubernetes 管理员，您需要能够识别和修复应用程序和集群组件的问题。
- en: Logging is essential for tracing application flows and capturing potential error
    messages. You can configure logging on a cluster level and a node level, each
    of which comes with its own benefits and potential drawbacks. Depending on the
    log capturing approach, aspects such as log rotation and logging backend service
    may be incorporated. On the Pod level, you can directly ask for the application
    logs using the `kubectl logs` command. Use the command-line option `-c` to target
    a specific container in a multi-container setup.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录对于追踪应用流程和捕获潜在错误消息至关重要。您可以在集群级别和节点级别配置日志记录，每种方法都有其各自的优势和潜在缺陷。根据日志捕获的方法，可能会涉及诸如日志轮换和日志后端服务等方面。在
    Pod 级别，您可以直接使用 `kubectl logs` 命令请求应用程序日志。使用命令行选项 `-c` 来针对多容器设置中的特定容器。
- en: Kubernetes’ native monitoring service, the metrics server, can be installed
    on the cluster to collect and aggregate Pod and node resource utilization data.
    Nodes send metrics via the kubelet to the centralized metrics server. End users
    can use the `kubectl top` command to render those metrics as a means to identify
    excessive resource usage.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的原生监控服务，指标服务器，可以安装在集群上，用于收集和聚合 Pod 和节点资源利用率数据。节点通过 kubelet 向集中的指标服务器发送指标。最终用户可以使用
    `kubectl top` 命令，将这些指标呈现出来，以识别资源使用过度的情况。
- en: Misconfiguration of workload and networking objects can lead to applications
    mishaps. You need to be familiar with the strategies relevant for diagnosing root
    causes and how fix them. A cluster may also develop error states leading to a
    variety of operational issues. You need to know which cluster components and processes
    run on control plane nodes and worker nodes. We discussed strategies for tackling
    different failure situations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载和网络对象的错误配置可能导致应用程序问题。您需要熟悉诊断根本原因的相关策略以及如何修复它们。集群也可能出现错误状态，导致各种运营问题。您需要了解控制平面节点和工作节点上运行的集群组件和进程。我们讨论了应对不同故障情况的策略。
- en: Exam Essentials
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考试要点
- en: Understand logging configuration on a theoretical level
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 理解日志配置的理论层面。
- en: Kubernetes logging is not a built-in capability. You have to configure logging
    proactively on the cluster or the node level. Compare the different approaches
    and their potential trade-offs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 日志记录不是内置功能。您需要在集群或节点级别积极配置日志记录。比较不同的方法及其潜在的权衡。
- en: Make accessing container logs your daily bread and butter
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使访问容器日志成为您的日常工作重点。
- en: Accessing container logs is straightforward. Simply use the `logs` command.
    Practice the use of all relevant command-line options. The option `-c` targets
    a specific container. The option does not have to be used explicitly for single-container
    Pods. The option `-f` tails the log entries if you want to see live processing
    in an application. The `-p` option can be used for accessing logs if the container
    needed to be restarted, but you still want to take a look at the previous container
    logs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 访问容器日志非常直接。只需使用`logs`命令即可。练习所有相关的命令行选项的使用。选项`-c`用于指定特定容器。对于单容器 Pod，不必显式使用该选项。选项`-f`可用于跟踪应用程序中的日志条目。如果需要查看先前容器的日志，即使容器已重启，也可以使用选项`-p`来访问日志。
- en: Install and use the metrics server
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 安装和使用度量服务器
- en: The metrics server isn’t installed on a Kubernetes cluster by default. Go through
    the motions of installing the service. For the exam, you can assume that the metrics
    server is already available. Use the `top` command for Pods and nodes to identify
    resource consumption.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群默认未安装度量服务器。按照安装服务的步骤操作。在考试中，可以假设度量服务器已经可用。使用`top`命令查看 Pods 和节点的资源消耗情况。
- en: Know how to troubleshoot applications
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 知道如何排查应用程序问题。
- en: Applications running in a Pod can easily break due to misconfiguration. Think
    of possible scenarios that can occur and try to model them proactively to represent
    a failure situation. Then using the commands `get`, `logs`, and `exec`, get to
    the bottom of the issue and fix it. Try to dream up obscure scenarios to become
    more comfortable with finding and fixing application issues for different resource
    types.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在 Pod 中的应用程序可能因配置错误而容易出现故障。考虑可能发生的情景，并试图积极地对其进行建模，以表示故障情况。然后使用命令`get`、`logs`和`exec`，深入了解问题并修复它。尝试构想不常见的场景，以便更轻松地找到和修复不同资源类型的应用程序问题。
- en: Know how to troubleshoot clusters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 知道如何排查集群问题。
- en: Control-plane and worker nodes can become unresponsive or dysfunctional for
    a variety of reasons. Administrators need to take care of their cluster’s health
    in order to keep it operational and scalable. Try to emulate error scenarios that
    may occur and apply the discussed troubleshooting techniques to identify and fix
    the underlying root cause.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面和工作节点可能因多种原因而变得无响应或无功能。管理员需要关注其集群的健康状况，以保持其可操作性和可扩展性。尝试模拟可能发生的错误情景，并应用讨论的故障排除技术来识别和修复潜在的根本原因。
- en: Sample Exercises
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样例练习
- en: Solutions to these exercises are available in the [Appendix](app01.xhtml#appendix-a).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在[附录](app01.xhtml#appendix-a)中找到。
- en: You are supposed to implement cluster-level logging with a sidecar container.
    Create a multi-container Pod named `multi`. The main application container named
    `nginx` should use the image `nginx:1.21.6`. The sidecar container named `streaming`
    uses the image `busybox:1.35.0` and the arguments `/bin/sh`, `-c`, and `'tail
    -n+1 -f /var/log/nginx/access.log'`.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该使用一个带有边车容器的集群级别日志记录。创建一个名为`multi`的多容器 Pod。主应用程序容器名为`nginx`，使用镜像`nginx:1.21.6`。边车容器名为`streaming`，使用镜像`busybox:1.35.0`和参数`/bin/sh`,
    `-c`, `'tail -n+1 -f /var/log/nginx/access.log'`。
- en: Define a volume of type `emptyDir` for the Pod and mount it to the path `/var/log/nginx`
    for both containers.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 Pod 定义一个`emptyDir`类型的卷，并将其挂载到路径`/var/log/nginx`上，供两个容器使用。
- en: Access the endpoint of the nginx service a couple of times using a `wget` or
    `curl` command. Inspect the logs of the sidecar container.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `wget` 或 `curl` 命令多次访问 nginx 服务的端点。检查 sidecar 容器的日志。
- en: Create two Pods named `stress-1` and `stress-2`. Define a container that uses
    the image `polinux/stress:1.0.4` with the command `stress` and the arguments `/bin/sh`,
    `-c`, and `'stress --vm 1 --vm-bytes $(shuf -i 20-200 -n 1)M --vm-hang 1'`. Set
    the container memory resource limits and requests to 250Mi.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个名为 `stress-1` 和 `stress-2` 的 Pod。定义一个使用镜像 `polinux/stress:1.0.4`，命令为 `stress`，参数为
    `/bin/sh`, `-c` 和 `'stress --vm 1 --vm-bytes $(shuf -i 20-200 -n 1)M --vm-hang
    1'` 的容器。设置容器的内存资源限制和请求为 250Mi。
- en: Use the data available through the metrics server to identify which of the Pods,
    `stress-1` or `stress-2`, consumes the most memory. Write the name of the Pod
    to the file `max-memory.txt`.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用指标服务器提供的数据，确定 `stress-1` 或 `stress-2` 中哪个 Pod 消耗了最多的内存。将 Pod 的名称写入文件 `max-memory.txt`。
- en: Navigate to the directory `app-a/ch07/troubleshooting-pod` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file [`instructions.md`](https://oreil.ly/j9lTr) for troubleshooting
    a faulty Pod setup.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入检出的 GitHub 仓库 [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8) 的目录 `app-a/ch07/troubleshooting-pod`。按照文件
    [`instructions.md`](https://oreil.ly/j9lTr) 中的说明来排除 Pod 安装的故障。
- en: Navigate to the directory `app-a/ch07/troubleshooting-deployment` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file `instructions.md` for troubleshooting a faulty Deployment
    setup.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入检出的 GitHub 仓库 [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8) 的目录 `app-a/ch07/troubleshooting-deployment`。按照文件
    `instructions.md` 中的说明来排除部署安装的故障。
- en: Navigate to the directory `app-a/ch07/troubleshooting-service` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file [`instructions.md`](https://oreil.ly/Z9kou) for troubleshooting
    a faulty Service setup.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入检出的 GitHub 仓库 [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8) 的目录 `app-a/ch07/troubleshooting-service`。按照文件
    [`instructions.md`](https://oreil.ly/Z9kou) 中的说明来排除服务安装的故障。
- en: Navigate to the directory `app-a/ch07/troubleshooting-control-plane-node` of
    the checked-out GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8).
    Follow the instructions in the file [`instructions.md`](https://oreil.ly/lGygu)
    for troubleshooting a faulty control plane node setup.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入检出的 GitHub 仓库 [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8) 的目录 `app-a/ch07/troubleshooting-control-plane-node`。按照文件
    [`instructions.md`](https://oreil.ly/lGygu) 中的说明来排除控制平面节点安装的故障。
- en: '*Prerequisite:* This exercise requires the installation of the tools [Vagrant](https://oreil.ly/sasln)
    and [VirtualBox](https://oreil.ly/9Cvg9).'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*先决条件:* 此练习需要安装工具 [Vagrant](https://oreil.ly/sasln) 和 [VirtualBox](https://oreil.ly/9Cvg9)。'
- en: Navigate to the directory `app-a/ch07/troubleshooting-worker-node` of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Follow
    the instructions in the file [`instructions.md`](https://oreil.ly/Kyh58) for troubleshooting
    a faulty worker node setup.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入检出的 GitHub 仓库 [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8) 的目录 `app-a/ch07/troubleshooting-worker-node`。按照文件
    [`instructions.md`](https://oreil.ly/Kyh58) 中的说明来排除工作节点安装的故障。
- en: '*Prerequisite:* This exercise requires the installation of the tools [Vagrant](https://oreil.ly/sasln)
    and [VirtualBox](https://oreil.ly/9Cvg9).'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*先决条件:* 此练习需要安装工具 [Vagrant](https://oreil.ly/sasln) 和 [VirtualBox](https://oreil.ly/9Cvg9)。'
