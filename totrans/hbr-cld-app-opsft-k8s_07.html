<html><head></head><body><section data-pdf-bookmark="Chapter 6. Multicluster Fleets: Provision and Upgrade Life Cycles" data-type="chapter" epub:type="chapter"><div class="chapter" id="multicluster_fleets_provision_and_upgrad">&#13;
<h1><span class="label">Chapter 6. </span>Multicluster Fleets: Provision <span class="keep-together">and Upgrade Life Cycles</span></h1>&#13;
<p>The terms <em>multicluster</em> and <em>multicloud</em> have become common in today’s landscape. For the purposes of this discussion, we will define these terms as follows:</p>&#13;
<dl>&#13;
<dt>Multicluster</dt>&#13;
<dd>Refers to scenarios where more than a single cluster is under management or an application is made up of parts that are hosted on more than one cluster<a contenteditable="false" data-primary="multicluster management" data-secondary="definition of multicluster" data-type="indexterm" id="idm45358191051896"/></dd>&#13;
<dt>Multicloud</dt>&#13;
<dd>Refers to scenarios where the multiple clusters in use also span infrastructure substrates, which might include a private datacenter and a single public cloud provider or multiple public cloud providers<a contenteditable="false" data-primary="multicloud management" data-secondary="definition of multicloud" data-type="indexterm" id="idm45358191049272"/></dd>&#13;
</dl>&#13;
<p>The differences here are more academic; the reality is that you are more likely than not to have to manage many clusters just as your organization has had to manage multiple VMware ESXi hosts that run VMs.</p>&#13;
<p>The differences will matter when your container orchestration platform has variation across infrastructure substrates. We’ll talk about some of the places where this currently comes up and may affect some of your management techniques or application architectures.</p>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Why Multicluster?" data-type="sect1"><div class="sect1" id="why_multiclusterquestion_mark">&#13;
<h1 class="less_space">Why Multicluster?</h1>&#13;
<p>Let’s discuss the use cases that lead to multiple clusters under management.</p>&#13;
<section data-pdf-bookmark="Use Case: Using Multiple Clusters to Provide Regional Availability for Your Applications" data-type="sect2"><div class="sect2" id="use_case_using_multiple_clusters_to_prov">&#13;
<h2>Use Case: Using Multiple Clusters to Provide Regional Availability for Your Applications</h2>&#13;
<p>As discussed in <a data-type="xref" href="ch04.html#single_cluster_availability">Chapter 4</a>, a<a contenteditable="false" data-primary="multicluster management" data-secondary="about multicluster design" data-type="indexterm" id="ch06-ymul"/><a contenteditable="false" data-primary="availability zones" data-type="indexterm" id="idm45358191039288"/><a contenteditable="false" data-primary="geographic distribution" data-secondary="single versus multiple clusters" data-type="indexterm" id="idm45358191038184"/><a contenteditable="false" data-primary="regional availability" data-see="geographic distribution" data-type="indexterm" id="idm45358191036792"/><a contenteditable="false" data-primary="geographic distribution" data-secondary="multicluster availability zones" data-type="indexterm" id="idm45358191035416"/> single cluster can span multiple availability zones. Each availability zone has independent failure characteristics. A failure in the power supply, network provider, and even physical space (e.g., the datacenter building) should be isolated to one availability zone. Typically, the network links across availability zones still provide for significant throughput and low latency, allowing the etcd cluster for the Kubernetes API server to span hosts running in different availability zones. However, your application may need to tolerate an outage that affects more than two availability zones within a region or tolerate an outage of the entire region.</p>&#13;
<p>So perhaps one of the most easily understood use cases is to create more than one multiavailability zone cluster in two or more regions. You will commonly find applications that are federated across two “swim lanes,” sometimes referred to as a <a href="https://oreil.ly/82hDU"><em>blue-green architecture</em></a>.<a contenteditable="false" data-primary="blue-green architecture" data-type="indexterm" id="idm45358191031592"/> The “blue-green” pairing pattern can often be found within the same region, with alternate blue-green pairs in other regions. You may choose to bring that same architecture to OpenShift where you run two separate clusters that host the same set of components for the application, effectively running two complete end-to-end environments, either of which can support most of the load of your users. Additional issues concerning load balancing and data management arise around architectural patterns required to support cross-region deployments and will be covered in <a data-type="xref" href="ch08.html#working_example_of_multicluster_applicat">Chapter 8</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Use Case: Using Multiple Clusters for Multiple Tenants" data-type="sect2"><div class="sect2" id="use_case_using_multiple_clusters_for_mul">&#13;
<h2>Use Case: Using Multiple Clusters for Multiple Tenants</h2>&#13;
<p>The Kubernetes community boundary for tenancy is a single cluster. <a contenteditable="false" data-primary="role-based access control (RBAC)" data-secondary="namespaces in Kubernetes" data-type="indexterm" id="idm45358191026456"/><a contenteditable="false" data-primary="access control" data-secondary="role-based" data-tertiary="namespaces in Kubernetes" data-type="indexterm" id="idm45358191025016"/><a contenteditable="false" data-primary="project capability of OpenShift" data-secondary="project synonymous with namespace" data-tertiary="access permissions for projects" data-type="indexterm" id="idm45358191023352"/><a contenteditable="false" data-primary="ClusterRole object" data-type="indexterm" id="idm45358191021656"/><a contenteditable="false" data-primary="security" data-secondary="role-based access control" data-type="indexterm" id="idm45358191020552"/><a contenteditable="false" data-primary="namespaces" data-secondary="project synonymous with namespace" data-tertiary="access permissions for projects" data-type="indexterm" id="idm45358191019160"/><a contenteditable="false" data-primary="tenancy in clusters" data-type="indexterm" id="idm45358191017480"/>In general, the API constructs within Kubernetes focus on dividing the compute resources of the cluster into namespaces (also called <em>projects</em> in OpenShift). Users are then assigned roles or <code>ClusterRole</code>s to access their namespaces. However, cluster-scoped resources like <code>ClusterRole</code>s, CRDs, namespaces/projects, webhook configurations, and so on really cannot be managed by independent parties. Each API resource must have a unique name within the collection of the same kind of API resources. If there were true multitenancy within a cluster, then some API concept (like a tenant) would group things like <code>ClusterRole</code>s, CRDs, and webhook configurations and prevent collisions (in naming or behavior) across each tenant, much like projects do for applications (e.g., deployments, services, and <code>PersistentVolumeClaim</code>s can duplicate names or behavior across different namespaces/projects).</p>&#13;
<p>So Kubernetes is easiest to consume when you can assign a cluster to a tenant. A tenant might be a line of business or a functional team within your organization (e.g., quality engineering or performance and scale test). Then, a set of cluster-admins or similarly elevated <code>ClusterRole</code>s can be assigned to the owners of the cluster.</p>&#13;
<p>Hence, an emerging pattern is that platform teams that manage OpenShift clusters will define a process where a consumer may request a cluster for their purposes. As a result, multiple clusters now require consistent governance and policy management.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Use Case: Supporting Far-Edge Use Cases Where Clusters Do Not Run in Traditional Datacenters or Clouds" data-type="sect2"><div class="sect2" id="use_case_supporting_far_edge_use_cases_w">&#13;
<h2>Use Case: Supporting Far-Edge Use Cases Where Clusters Do Not Run in Traditional Datacenters or Clouds</h2>&#13;
<p>There are some great examples of <a contenteditable="false" data-primary="edge computing clusters" data-type="indexterm" id="idm45358191009528"/>how technology is being applied to a variety of use cases where computing power is coupled with sensor data from cameras, audio sensors, or environmental sensors and machine learning or AI to improve efficiency, provide greater safety, or create novel consumer interactions.<sup><a data-type="noteref" href="ch06.html#ch01fn36" id="ch01fn36-marker">1</a></sup> These use cases are often referred to generically as <em>edge computing</em> because the computing power is outside the datacenter and closer to the “edge” of the consumer experience.</p>&#13;
<p>The introduction of high-bandwidth capabilities with 5G also creates scenarios where an edge-computing solution can use a localized 5G network within a space like a manufacturing plant and where edge-computing applications help track product assembly, automate safety controls for employees, or protect sensitive machinery.</p>&#13;
<p>Just as containers provide a discrete package for enterprise web-based applications, there are significant benefits of using containers in edge-based applications. Similarly, the automated recovery of services by your container orchestration is also beneficial, even more so when the computing source is not easily accessible within your <span class="keep-together">datacenter</span>.<a contenteditable="false" data-primary="" data-startref="ch06-ymul" data-type="indexterm" id="idm45358191001816"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Architectural Characteristics" data-type="sect2"><div class="sect2" id="architectural_characteristics">&#13;
<h2>Architectural Characteristics</h2>&#13;
<p>Now that we have seen some of the reasons why you may use multiple clusters or clouds to support your needs, let’s take a look at some of the<a contenteditable="false" data-primary="multicluster management" data-secondary="architecture" data-type="indexterm" id="ch06-arcm"/><a contenteditable="false" data-primary="architecture" data-secondary="multicluster design" data-type="indexterm" id="ch06-arcm2"/> architectural benefits and challenges of such an approach.</p>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Region availability versus availability zones" data-type="sect3"><div class="sect3" id="region_availability_versus_availability">&#13;
<h3 class="less_space">Region availability versus availability zones</h3>&#13;
<p>With multiple clusters hosting the application, you can spread instances <a contenteditable="false" data-primary="availability zones" data-type="indexterm" id="idm45358190992408"/><a contenteditable="false" data-primary="geographic distribution" data-secondary="multicluster availability zones" data-type="indexterm" id="idm45358190991304"/>of the application across multiple cloud regions. Each cluster within a region will still spread compute capacity across multiple availability zones. See <a data-type="xref" href="#cluster_region_availability_allows_multi">Figure 6-1</a> for a visual representation of this topology.</p>&#13;
<figure><div class="figure" id="cluster_region_availability_allows_multi">&#13;
<img src="assets/hcok_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>Cluster region availability allows multiple clusters to run across independent cloud regions</h6>&#13;
</div></figure>&#13;
<p>Under this style of architecture, each cluster can tolerate the total loss of any one<a contenteditable="false" data-primary="failure" data-secondary="multicluster availability zones" data-type="indexterm" id="idm45358190986200"/><a contenteditable="false" data-primary="availability zones" data-secondary="failure in multicluster architecture" data-type="indexterm" id="idm45358190984808"/><a contenteditable="false" data-primary="control plane failure" data-secondary="multicluster across independent cloud regions" data-type="indexterm" id="idm45358190983416"/> availability zone (AZ1, AZ2, or AZ3 could become unavailable but not more than one), and the workload will continue to run and serve requests. As a result of two availability zone failures, the etcd cluster would lose quorum and the control plane would become unavailable.</p>&#13;
<p>The reason that a Kubernetes control plane becomes inoperable with more than a <a contenteditable="false" data-primary="etcd" data-secondary="failure of" data-tertiary="quorum of etcd cluster" data-type="indexterm" id="idm45358190981176"/>single availability zone failure is because of quorum requirements for etcd. Typically, etcd will have three replicas that are maintained in the control plane, each replica supported by exactly one availability zone. If a single availability zone is lost, there are still two out of three replicas present and distributed writes can still be sure that the write transaction is accepted. If two availability zones fail, then write attempts will be rejected. Pods running on worker nodes in the cluster may still be able to serve traffic, but no updates related to the Kubernetes API will be accepted or take place. However, the independent cluster running in one of the other regions could continue to respond to user requests. See <a data-type="xref" href="ch04.html#single_cluster_availability">Chapter 4</a> for a deeper analysis of how this process works.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Mitigating latency for users based on geography" data-type="sect3"><div class="sect3" id="mitigating_latency_for_users_based_on_ge">&#13;
<h3>Mitigating latency for users based on geography</h3>&#13;
<p>If you have users in different locations, using more than one cloud region can<a contenteditable="false" data-primary="geographic distribution" data-secondary="latency mitigation" data-type="indexterm" id="idm45358190975624"/><a contenteditable="false" data-primary="latency mitigation in geographic distribution" data-type="indexterm" id="idm45358190974152"/> also improve response times for your users. When a user attempts to access the web user experience of your application or an API exposed by your workload, their request can be routed to the nearest available instance of your application. <a contenteditable="false" data-primary="GSLB (Global Server Load Balancer)" data-type="indexterm" id="idm45358190972568"/><a contenteditable="false" data-primary="Global Server Load Balancer (GSLB)" data-type="indexterm" id="idm45358190971448"/>Typically, a <em>Global Server Load Balancer</em> (GSLB) is used to efficiently route traffic in these scenarios. When the user attempts to access the service, a DNS lookup will be delegated to the nameservers hosted by your GSLB. Then, based on a heuristic of where the request originated, the nameserver will respond with the IP address of the nearest hosted instance of your application. You can see a visual representation of this in <a data-type="xref" href="#requests_to_resolve_the_address_of_a_glo">Figure 6-2</a>.</p>&#13;
<figure><div class="figure" id="requests_to_resolve_the_address_of_a_glo">&#13;
<img src="assets/hcok_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>Requests to resolve the address of a global service using a GSLB will return the closest instance based on proximity to the request originator</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Consistency of your platform (managed Kubernetes versus OpenShift plus cloud identity providers)" data-type="sect3"><div class="sect3" id="consistency_of_your_platform_left_parent">&#13;
<h3>Consistency of your platform (managed Kubernetes versus OpenShift plus cloud identity providers)</h3>&#13;
<p>One of the major benefits of the OpenShift Container Platform is that it <a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="enhancements provided by" data-tertiary="cloud providers and substrates" data-type="indexterm" id="idm45358190964632"/>deploys and runs consistently across all cloud providers and substrates like VMware and bare metal. When you consider whether to consume a managed Kubernetes provider or OpenShift, be aware that each distribution of Kubernetes makes various architectural decisions that can require greater awareness of your application to ensure cross-provider portability.<a contenteditable="false" data-primary="" data-startref="ch06-arcm" data-type="indexterm" id="idm45358190962216"/><a contenteditable="false" data-primary="" data-startref="ch06-arcm2" data-type="indexterm" id="idm45358190960840"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Provisioning Across Clouds" data-type="sect1"><div class="sect1" id="provisioning_across_clouds">&#13;
<h1>Provisioning Across Clouds</h1>&#13;
<p>Choosing a Kubernetes strategy affords a great way to simplify how <a contenteditable="false" data-primary="multicluster management" data-secondary="provisioning across clouds" data-see="provisioning across clouds" data-type="indexterm" id="idm45358190957496"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="about" data-type="indexterm" id="idm45358190955736"/><a contenteditable="false" data-primary="multicloud management" data-secondary="provisioning across clouds" data-see="provisioning across clouds" data-type="indexterm" id="idm45358190954344"/><a contenteditable="false" data-primary="applications" data-secondary="across clouds" data-see="provisioning across clouds" data-type="indexterm" id="ch06-apac"/>applications consume elastic cloud-based infrastructure. To some extent, the problem of how you consume a cloud’s resources shifts from solving the details for every application to one platform—namely, how your organization will adopt and manage Kubernetes across infrastructure substrates.</p>&#13;
<p>There are several ways to provision Kubernetes from community-supported projects. For the purposes of this section, we’ll focus on how to provision Kubernetes using the Red Hat OpenShift Container Platform. Then we’ll discuss how you could alternatively consume managed OpenShift or managed Kubernetes services as part of your provisioning life cycle.</p>&#13;
<section data-pdf-bookmark="User-Managed OpenShift" data-type="sect2"><div class="sect2" id="user_managed_openshift">&#13;
<h2>User-Managed OpenShift</h2>&#13;
<p>When you provision an OpenShift Container Platform 4.x cluster, you <a contenteditable="false" data-primary="multicluster management" data-secondary="user-managed OpenShift" data-type="indexterm" id="ch06-umos"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="user-managed OpenShift" data-type="indexterm" id="ch06-umos2"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="OpenShift installers for web platforms" data-type="indexterm" id="ch06-wbinst"/><a contenteditable="false" data-primary="infrastructure" data-secondary="user- versus installer-provisioned" data-type="indexterm" id="idm45358190941224"/><a contenteditable="false" data-primary="user-provisioned infrastructure (UPI)" data-type="indexterm" id="idm45358190939832"/>have two options for how infrastructure resources are created. <em>User-provisioned infrastructure</em> (UPI) allows you more control to spin up VMs, network resources, and storage and then give these details to the install process and allow them to be bootstrapped into a running cluster. <a contenteditable="false" data-primary="installer-provisioned infrastructure (IPI)" data-type="indexterm" id="idm45358190937880"/>Alternatively, you can rely on the more automated approach of <em>installer-provisioned infrastructure</em> (IPI). Using IPI, the installer accepts cloud credentials with the appropriate privileges to create the required infrastructure resources. <a contenteditable="false" data-primary="virtual private cloud (VPC)" data-type="indexterm" id="idm45358190935944"/>The IPI process will typically define a <em>virtual private cloud</em> (VPC). Note that you can specify the VPC as an input parameter if your organization has its own conventions for how these resources are created and managed. Within the VPC, resources, including network load balancers, object store buckets, virtual computing resources, elastic IP addresses, and so forth, are all created and managed by the install process.</p>&#13;
<p>Let’s take a look at provisioning an OpenShift cluster across three cloud providers:<a contenteditable="false" data-primary="Azure (Microsoft) provisioning" data-type="indexterm" id="ch06-webprov2"/><a contenteditable="false" data-primary="Microsoft Azure provisioning" data-type="indexterm" id="ch06-webprov"/><a contenteditable="false" data-primary="Amazon Web Services (AWS) provisioning" data-type="indexterm" id="ch06-webprov3"/><a contenteditable="false" data-primary="AWS (Amazon Web Services) provisioning" data-type="indexterm" id="ch06-webprov4"/><a contenteditable="false" data-primary="Google" data-secondary="Cloud Platform provisioning" data-type="indexterm" id="ch06-webprov5"/><a contenteditable="false" data-primary="cloud provider provisioning" data-see="provisioning across clouds" data-type="indexterm" id="idm45358190926248"/><a contenteditable="false" data-primary="cluster version operator (CVO)" data-type="indexterm" id="idm45358190924840"/><a contenteditable="false" data-primary="CVO (cluster version operator)" data-type="indexterm" id="idm45358190923720"/> AWS, Microsoft Azure, and Google Cloud Platform. For this discussion, we will review how the install process makes use of declarative configuration (just as Kubernetes does in general) and how this relates to the <code>ClusterVersionOperator</code> (CVO), which manages the life cycle of the OpenShift cluster itself.</p>&#13;
<p>First, you will need to download the openshift-installer binary for your appropriate version. Visit <a href="https://cloud.redhat.com">Red Hat</a>, create an account, and follow the steps to Create Cluster and download the binary for local use. Specific details about the options available for installation are available in the <a href="https://oreil.ly/HerIm">product documentation</a>.</p>&#13;
<p>Let’s demonstrate how this approach works by looking at a few example configuration files for the openshift-installer binary. The full breadth of options for installing and configuring OpenShift is beyond the scope of this book. <a contenteditable="false" data-primary="provisioning across clouds" data-secondary="documentation online" data-type="indexterm" id="idm45358190919096"/>See the <a href="https://oreil.ly/wOJ3P">OpenShift Container Platform documentation</a> for a thorough reference of all supported options. The following examples will highlight how the declarative nature of the OpenShift 4.x install methodology simplifies provisioning clusters across multiple substrates. Further, a walk-through example of the <code>MachineSet</code> API will demonstrate how operators continue to manage the life cycle and health of the cluster after <span class="keep-together">provisioning</span>.</p>&#13;
<p><a data-type="xref" href="#an_example_install_configdotyaml_to_pro">Example 6-1</a> defines a set of options for provisioning an OpenShift cluster on AWS. <a data-type="xref" href="#an_example_install_configdotyaml_to_pr">Example 6-2</a> defines how to provision an OpenShift cluster on Microsoft Azure, while <a data-type="xref" href="#an_example_install_configdotyaml_to_p">Example 6-3</a> defines the equivalent configuration for Google Cloud Platform. <a data-type="xref" href="#an_example_install_configdotyaml_to">Example 6-4</a>—you guessed it!—provides an example configuration for VMware vSphere. With the exception of the VMware vSphere example (which is more sensitive to your own environment), you can use these examples to provision your own clusters with minimal updates. Refer to the OpenShift Container Platform product documentation for a full examination of install methods.<a contenteditable="false" data-primary="provisioning across clouds" data-secondary="OpenShift installers for web platforms" data-tertiary="AWS install-config.yaml" data-type="indexterm" id="idm45358190910424"/><a contenteditable="false" data-primary="YAML files" data-secondary="provisioning across clouds" data-tertiary="AWS install-config.yaml" data-type="indexterm" id="idm45358190908728"/></p>&#13;
<div data-type="example" id="an_example_install_configdotyaml_to_pro">&#13;
<h5><span class="label">Example 6-1. </span>An example <em>install-config.yaml</em> to provision an OpenShift cluster on AWS</h5>&#13;
</div>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">clusterName</code>&#13;
<code class="nt">baseDomain</code><code class="p">:</code> <code class="l-Scalar-Plain">yourcompany.domain.com</code>&#13;
<code class="nt">controlPlane</code><code class="p">:</code>&#13;
 <code class="nt">hyperthreading</code><code class="p">:</code> <code class="l-Scalar-Plain">Enabled</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">master</code>&#13;
 <code class="nt">platform</code><code class="p">:</code> <code class="p-Indicator">{}</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
<code class="nt">compute</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">hyperthreading</code><code class="p">:</code> <code class="l-Scalar-Plain">Enabled</code>&#13;
<code class="-Error"> </code><code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">worker</code>&#13;
 <code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">aws</code><code class="p">:</code>&#13;
 <code class="nt">zones</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">us-east-1b</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">us-east-1c</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">us-east-1d</code>&#13;
 <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">m5.xlarge</code>&#13;
 <code class="nt">rootVolume</code><code class="p">:</code>&#13;
 <code class="nt">iops</code><code class="p">:</code> <code class="l-Scalar-Plain">4000</code>&#13;
 <code class="nt">size</code><code class="p">:</code> <code class="l-Scalar-Plain">250</code>&#13;
 <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">io1</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
<code class="nt">networking</code><code class="p">:</code>&#13;
 <code class="nt">clusterNetwork</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">cidr</code><code class="p">:</code> <code class="l-Scalar-Plain">10.128.0.0/14</code>&#13;
 <code class="nt">hostPrefix</code><code class="p">:</code> <code class="l-Scalar-Plain">23</code>&#13;
 <code class="nt">machineCIDR</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.0.0/16</code>&#13;
 <code class="nt">networkType</code><code class="p">:</code> <code class="l-Scalar-Plain">OpenShiftSDN</code>&#13;
 <code class="nt">serviceNetwork</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">172.30.0.0/16</code>&#13;
<code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">aws</code><code class="p">:</code>&#13;
 <code class="nt">region</code><code class="p">:</code> <code class="l-Scalar-Plain">us-east-1</code>&#13;
 <code class="nt">userTags</code><code class="p">:</code>&#13;
 <code class="nt">owner</code><code class="p">:</code> <code class="l-Scalar-Plain">user@email.domain</code>&#13;
<code class="nt">publish</code><code class="p">:</code> <code class="l-Scalar-Plain">External</code>&#13;
<code class="nt">pullSecret</code><code class="p">:</code> <code class="s">'REDACTED'</code>&#13;
<code class="nt">sshKey</code><code class="p">:</code> <code class="p-Indicator">|</code>&#13;
 <code class="no">REDACTED</code></pre>&#13;
<p/>&#13;
<div data-type="example" id="an_example_install_configdotyaml_to_pr">&#13;
<h5><span class="label">Example 6-2. </span>An example <em>install-config.yaml</em> to provision an OpenShift cluster on Microsoft Azure<a contenteditable="false" data-primary="provisioning across clouds" data-secondary="OpenShift installers for web platforms" data-tertiary="Microsoft Azure install-config.yaml" data-type="indexterm" id="idm45358190900424"/><a contenteditable="false" data-primary="YAML files" data-secondary="provisioning across clouds" data-tertiary="Microsoft Azure install-config.yaml" data-type="indexterm" id="idm45358190791336"/></h5>&#13;
</div>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
 <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">clusterName</code>&#13;
<code class="nt">baseDomain</code><code class="p">:</code> <code class="l-Scalar-Plain">yourcompany.domain.com</code>&#13;
<code class="nt">controlPlane</code><code class="p">:</code>&#13;
 <code class="nt">hyperthreading</code><code class="p">:</code> <code class="l-Scalar-Plain">Enabled</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">master</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
 <code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">azure</code><code class="p">:</code>&#13;
 <code class="nt">osDisk</code><code class="p">:</code>&#13;
 <code class="nt">diskSizeGB</code><code class="p">:</code> <code class="l-Scalar-Plain">128</code>&#13;
 <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">Standard_D4s_v3</code>&#13;
<code class="nt">compute</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">hyperthreading</code><code class="p">:</code> <code class="l-Scalar-Plain">Enabled</code>&#13;
<code class="-Error"> </code><code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">worker</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
 <code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">azure</code><code class="p">:</code>&#13;
 <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">Standard_D2s_v3</code>&#13;
 <code class="nt">osDisk</code><code class="p">:</code>&#13;
 <code class="nt">diskSizeGB</code><code class="p">:</code> <code class="l-Scalar-Plain">128</code>&#13;
 <code class="nt">zones</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="s">"1"</code>&#13;
 <code class="p-Indicator">-</code> <code class="s">"2"</code>&#13;
 <code class="p-Indicator">-</code> <code class="s">"3"</code>&#13;
<code class="nt">networking</code><code class="p">:</code>&#13;
 <code class="nt">clusterNetwork</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">cidr</code><code class="p">:</code> <code class="l-Scalar-Plain">10.128.0.0/14</code>&#13;
 <code class="nt">hostPrefix</code><code class="p">:</code> <code class="l-Scalar-Plain">23</code>&#13;
 <code class="nt">machineCIDR</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.0.0/16</code>&#13;
 <code class="nt">networkType</code><code class="p">:</code> <code class="l-Scalar-Plain">OpenShiftSDN</code>&#13;
 <code class="nt">serviceNetwork</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">172.30.0.0/16</code>&#13;
<code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">azure</code><code class="p">:</code>&#13;
 <code class="nt">baseDomainResourceGroupName</code><code class="p">:</code> <code class="l-Scalar-Plain">resourceGroupName</code>&#13;
 <code class="nt">region</code><code class="p">:</code> <code class="l-Scalar-Plain">centralus</code>&#13;
<code class="nt">pullSecret</code><code class="p">:</code> <code class="s">'REDACTED'</code></pre>&#13;
<p/>&#13;
<div data-type="example" id="an_example_install_configdotyaml_to_p">&#13;
<h5><span class="label">Example 6-3. </span>An example <em>install-config.yaml</em> to provision an OpenShift cluster on Google Cloud Platform<a contenteditable="false" data-primary="provisioning across clouds" data-secondary="OpenShift installers for web platforms" data-tertiary="Google Cloud Platform install-config.yaml" data-type="indexterm" id="idm45358190785272"/><a contenteditable="false" data-primary="YAML files" data-secondary="provisioning across clouds" data-tertiary="Google Cloud Platform install-config.yaml" data-type="indexterm" id="idm45358190620808"/></h5>&#13;
</div>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">clusterName</code>&#13;
<code class="nt">baseDomain</code><code class="p">:</code> <code class="l-Scalar-Plain">yourcompany.domain.com</code>&#13;
<code class="nt">controlPlane</code><code class="p">:</code>&#13;
 <code class="nt">hyperthreading</code><code class="p">:</code> <code class="l-Scalar-Plain">Enabled</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">master</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
 <code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">gcp</code><code class="p">:</code>&#13;
 <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">n1-standard-4</code>&#13;
<code class="nt">compute</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">hyperthreading</code><code class="p">:</code> <code class="l-Scalar-Plain">Enabled</code>&#13;
<code class="-Error"> </code><code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">worker</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
 <code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">gcp</code><code class="p">:</code>&#13;
 <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">n1-standard-4</code>&#13;
<code class="nt">networking</code><code class="p">:</code>&#13;
 <code class="nt">clusterNetwork</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="nt">cidr</code><code class="p">:</code> <code class="l-Scalar-Plain">10.128.0.0/14</code>&#13;
 <code class="nt">hostPrefix</code><code class="p">:</code> <code class="l-Scalar-Plain">23</code>&#13;
 <code class="nt">machineCIDR</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.0.0/16</code>&#13;
 <code class="nt">networkType</code><code class="p">:</code> <code class="l-Scalar-Plain">OpenShiftSDN</code>&#13;
 <code class="nt">serviceNetwork</code><code class="p">:</code>&#13;
 <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">172.30.0.0/16</code>&#13;
<code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">gcp</code><code class="p">:</code>&#13;
 <code class="nt">projectID</code><code class="p">:</code> <code class="l-Scalar-Plain">gcpProjectId</code>&#13;
 <code class="nt">region</code><code class="p">:</code> <code class="l-Scalar-Plain">us-east1</code>&#13;
 <code class="nt">userTags</code><code class="p">:</code>&#13;
 <code class="nt">owner</code><code class="p">:</code> <code class="l-Scalar-Plain">user@email.com</code>&#13;
<code class="nt">pullSecret</code><code class="p">:</code> <code class="s">'REDACTED'</code>&#13;
<code class="nt">sshKey</code><code class="p">:</code> <code class="p-Indicator">|-</code>&#13;
 <code class="no">REDACTED</code></pre>&#13;
<p/>&#13;
<div class="pagebreak-before" data-type="example" id="an_example_install_configdotyaml_to">&#13;
<h5 class="less_space"><span class="label">Example 6-4. </span>An example <em>install-config.yaml</em> to provision an OpenShift cluster on VMware vSphere<a contenteditable="false" data-primary="provisioning across clouds" data-secondary="OpenShift installers for web platforms" data-tertiary="VMware vSphere install-config.yaml" data-type="indexterm" id="idm45358190469352"/><a contenteditable="false" data-primary="YAML files" data-secondary="provisioning across clouds" data-tertiary="VMware vSphere install-config.yaml" data-type="indexterm" id="idm45358190467800"/></h5>&#13;
</div>&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">example</code>&#13;
<code class="nt">baseDomain</code><code class="p">:</code> <code class="l-Scalar-Plain">demo.red-chesterfield.com</code>&#13;
<code class="nt">compute</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">hyperthreading</code><code class="p">:</code> <code class="l-Scalar-Plain">Enabled</code>&#13;
<code class="-Error"> </code><code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">worker</code>&#13;
 <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">3</code>&#13;
<code class="nt">platform</code><code class="p">:</code>&#13;
 <code class="nt">vsphere</code><code class="p">:</code>&#13;
 <code class="nt">vCenter</code><code class="p">:</code> <code class="l-Scalar-Plain">example.vCenterServer.io</code>&#13;
 <code class="nt">username</code><code class="p">:</code> <code class="l-Scalar-Plain">admin</code>&#13;
 <code class="nt">password</code><code class="p">:</code> <code class="l-Scalar-Plain">admin</code>&#13;
 <code class="nt">datacenter</code><code class="p">:</code> <code class="l-Scalar-Plain">exampleDatacenter</code>&#13;
 <code class="nt">defaultDatastore</code><code class="p">:</code> <code class="l-Scalar-Plain">exampleDatastore</code>&#13;
 <code class="nt">cluster</code><code class="p">:</code> <code class="l-Scalar-Plain">exampleClusterName</code>&#13;
 <code class="nt">apiVIP</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.0.200</code>&#13;
 <code class="nt">ingressVIP</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.0.201</code>&#13;
 <code class="nt">network</code><code class="p">:</code> <code class="l-Scalar-Plain">Public Network</code>&#13;
<code class="nt">pullSecret</code><code class="p">:</code> <code class="s">'REDACTED'</code>&#13;
<code class="nt">sshKey</code><code class="p">:</code> <code class="p-Indicator">|-</code>&#13;
 <code class="no">REDACTED</code></pre>&#13;
<p>Any one of these <em>install-config.yaml</em> files can be used to provision your cluster using the following command:<a contenteditable="false" data-primary="YAML files" data-secondary="provisioning across clouds" data-type="indexterm" id="idm45358190462824"/></p>&#13;
<pre data-type="programlisting">$ <strong>mkdir hubcluster</strong>&#13;
$ <strong># copy install-config.yaml template from above</strong> &#13;
$ <strong># or customize your own into the “hubcluster” dir</strong>&#13;
$ <strong>openshift-installer create cluster --dir=hubcluster</strong></pre>&#13;
<p>Note how each example shares some of the same options, notably the <code>clusterName</code> and <code>baseDomain</code> that will be used to derive the default network address of the cluster (applications will be hosted by default at <code>https://*.apps.clusterName.baseDomain</code> and the API endpoint for OpenShift will be available at <code>https://api.clusterName.baseDomain:6443</code>). When the openshift-installer runs, DNS entries on the cloud provider (e.g., Route 53 in the case of AWS) will be created and linked to the appropriate network load balancers (also created by the install process), that in turn resolve to IP addresses running within the VPC.</p>&#13;
<p>Each example defines sections for the <code>controlPlane</code> and <code>compute</code> that correspond to <code>MachineSet</code>s that will be created and managed. We’ll talk about how these relate to operators within the cluster shortly. More than one <code>MachinePool</code> within the <code>compute</code> section can be specified. Both the <code>controlPlane</code> and <code>compute</code> sections provide configurability for the compute hosts and can customize which availability zones are used. Settings including the type (or size) for each host and the options for what kind of storage is attached to the hosts are also available, but reasonable defaults will be chosen if omitted.</p>&#13;
<p>Now, if we compare where the <em>install-config.yaml</em> properties vary for each substrate, we will find cloud-specific options within the <code>platform</code> sections. There is a global <code>platform</code> to specify which region the cluster should be created within, as well as <span class="keep-together"><code>platform</code></span> sections under each of the <code>controlPlane</code> and <code>compute</code> sections to override settings for each provisioned host.<a contenteditable="false" data-primary="" data-startref="ch06-wbinst" data-type="indexterm" id="idm45358190336360"/></p>&#13;
<p>As introduced in <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>, the <a href="https://oreil.ly/D1UvC">Open Cluster Management</a> project is a new approach to managing the multicluster challenges that most cluster maintainers encounter. <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a> discussed how applications can be distributed easily across clusters. Now let’s look at how the cluster provisioning, upgrade, and decommissioning process can be driven using Open Cluster Management.</p>&#13;
<p>In the following example, we will walk through creating a new cluster on a cloud provider.<a contenteditable="false" data-primary="provisioning across clouds" data-secondary="creating new cluster on cloud provider" data-type="indexterm" id="idm45358190330904"/> The underlying behavior is leveraging the same openshift-install process that we just discussed. <a contenteditable="false" data-primary="klusterlet agent" data-type="indexterm" id="idm45358190329256"/><a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="provisioning across clouds" data-type="indexterm" id="idm45358190328152"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="creating new cluster on cloud provider" data-tertiary="klusterlet agent" data-type="indexterm" id="idm45358190326744"/>Once provisioned, the Open Cluster Management framework will install an agent that runs as a set of pods on the new cluster. We refer to this agent as a <code>klusterlet</code>, mimicking the naming of the <code>kubelet</code> process that runs on nodes that are part of a Kubernetes cluster.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The following assumes the user has already set up the Open Cluster Management project or RHACM for Kubernetes <a contenteditable="false" data-primary="Red Hat Advanced Cluster Management (RHACM)" data-secondary="provisioning across clouds" data-type="indexterm" id="idm45358190322920"/>as described in <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>.</p>&#13;
</div>&#13;
<p>From the RHACM for Kubernetes web console, open the Automate Infrastructure &gt; Clusters page and click on the action to “Create cluster” as shown in <a data-type="xref" href="#the_cluster_overview_page_allows_you_to">Figure 6-3</a>.</p>&#13;
<figure><div class="figure" id="the_cluster_overview_page_allows_you_to">&#13;
<img src="assets/hcok_0603.png"/>&#13;
<h6><span class="label">Figure 6-3. </span>The cluster overview page allows you to provision new clusters from the <span class="keep-together">console</span></h6>&#13;
</div></figure>&#13;
<p>The “Create cluster” action shown in <a data-type="xref" href="#cluster_creation_form_via_rhacm_for_kube">Figure 6-4</a> opens to a form where you provide a name and select one of the available cloud providers.</p>&#13;
<figure><div class="figure" id="cluster_creation_form_via_rhacm_for_kube">&#13;
<img src="assets/hcok_0604.png"/>&#13;
<h6><span class="label">Figure 6-4. </span>Cluster creation form via RHACM for Kubernetes</h6>&#13;
</div></figure>&#13;
<p class="pagebreak-after less_space">Next, select a version of OpenShift to provision. The available list maps directly to the <code>ClusterImageSet</code>s available on the hub cluster. You can introspect these images with the following command:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get clusterimagesets</strong>&#13;
NAME             RELEASE&#13;
img4.3.40-x86-64 quay.io/openshift-release-dev/ocp-release:4.3.40-x86_64&#13;
img4.4.27-x86-64 quay.io/openshift-release-dev/ocp-release:4.4.27-x86_64&#13;
img4.5.15-x86-64 quay.io/openshift-release-dev/ocp-release:4.5.15-x86_64&#13;
img4.6.1-x86-64  quay.io/openshift-release-dev/ocp-release:4.6.1-x86_64</pre>&#13;
<p>Further down the page, as shown in <a data-type="xref" href="#select_your_release_image_left_parenthes">Figure 6-5</a>, you will also need to specify a provider connection. In the case of AWS, you will need to provide the Access ID and Secret Key to allow API access by the installation process with your AWS account.</p>&#13;
<figure><div class="figure" id="select_your_release_image_left_parenthes">&#13;
<img src="assets/hcok_0605.png"/>&#13;
<h6><span class="label">Figure 6-5. </span>Select your release image (the version to provision) and your provider <span class="keep-together">connection</span></h6>&#13;
</div></figure>&#13;
<p>At this point, you can simply click Create and the cluster will be provisioned. However, let’s walk through how the <code>MachinePool</code> operator allows you to manage <code>MachineSet</code>s within the cluster.</p>&#13;
<p>Customize the “Worker pool1” <code>NodePool</code> for your desired region and availability zones. See Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#customizing_the_region_and_zones_for_the">6-6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#customizing_the_availability_zone">6-7</a> for an example of what this will look like in the form. You can amend these options after the cluster is provisioned as well.</p>&#13;
<figure><div class="figure" id="customizing_the_region_and_zones_for_the">&#13;
<img src="assets/hcok_0606.png"/>&#13;
<h6><span class="label">Figure 6-6. </span>Customizing the region and zones for the cluster workers</h6>&#13;
</div></figure>&#13;
<figure><div class="figure" id="customizing_the_availability_zone">&#13;
<img src="assets/hcok_0607.png"/>&#13;
<h6><span class="label">Figure 6-7. </span>Customize the availability zones within the region that are valid to host workers</h6>&#13;
</div></figure>&#13;
<p>A final summary of your confirmed choices is presented for review in <a data-type="xref" href="#confirmed_options_in_form">Figure 6-8</a>.</p>&#13;
<figure><div class="figure" id="confirmed_options_in_form">&#13;
<img src="assets/hcok_0608.png"/>&#13;
<h6><span class="label">Figure 6-8. </span>Confirmed options selected in the form</h6>&#13;
</div></figure>&#13;
<p>Once you have made your final customizations, click Create to begin the provisioning process as shown in <a data-type="xref" href="#rhacm_web_console_view_that_includes_lin">Figure 6-9</a>. <a contenteditable="false" data-primary="debugging" data-secondary="provisioning logs for" data-type="indexterm" id="idm45358190292632"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="creating new cluster on cloud provider" data-tertiary="provisioning logs" data-type="indexterm" id="idm45358190291256"/>The web console provides a view that includes links to the provisioning logs for the cluster. If the cluster fails to provision (e.g., due to a quota restriction in your cloud account), the provisioning logs provide clues on what to troubleshoot.</p>&#13;
<figure><div class="figure" id="rhacm_web_console_view_that_includes_lin">&#13;
<img src="assets/hcok_0609.png"/>&#13;
<h6><span class="label">Figure 6-9. </span>RHACM web console view that includes links to the provisioning logs for the cluster</h6>&#13;
</div></figure>&#13;
<p>Behind the form editor, a number of Kubernetes API objects are created.<a contenteditable="false" data-primary="clusters" data-secondary="cluster-scoped API objects" data-type="indexterm" id="idm45358190286712"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="API objects for provisioning across clouds" data-type="indexterm" id="idm45358190285288"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="Kubernetes API objects created" data-type="indexterm" id="idm45358190283880"/><a contenteditable="false" data-primary="scope" data-secondary="provisioning across clouds" data-type="indexterm" id="idm45358190282472"/><a contenteditable="false" data-primary="ManagedCluster API object" data-type="indexterm" id="idm45358190281080"/><a contenteditable="false" data-primary="namespaces" data-secondary="provisioning across clouds" data-type="indexterm" id="idm45358190279960"/> A small number of these API objects are cluster scoped (<code>ManagedCluster</code> in particular). The <code>ManagedCluster</code> controller will ensure that a project (namespace) exists that maps to the cluster name. Other controllers, including the controller that begins the provisioning process, will use the <code>cluster</code> project (namespace) to store resources that provide an API control surface for provisioning. Let’s take a look at a subset of these that you should become familiar with.</p>&#13;
<section data-pdf-bookmark="ManagedCluster" data-type="sect3"><div class="sect3" id="managedcluster">&#13;
<h3>ManagedCluster</h3>&#13;
<p><code>ManagedCluster</code> (API group: cluster.open-cluster-management.io/v1; cluster scoped) recognizes that a remote cluster is under the control of the hub cluster. The agent that runs on the remote cluster will attempt to create <code>ManagedCluster</code> if it does not exist on the hub, and it must be accepted by a user identity on the hub with appropriate permissions. You can see the example created in <a data-type="xref" href="#example_of_the_managedcluster_api_object">Example 6-5</a>. Note that labels for this object will drive placement decisions later in this chapter.</p>&#13;
<div data-type="example" id="example_of_the_managedcluster_api_object">&#13;
<h5><span class="label">Example 6-5. </span>Example of the <span class="plain"><code>ManagedCluster</code></span> API object</h5>&#13;
</div>&#13;
<pre data-type="programlisting">apiVersion: cluster.open-cluster-management.io/v1&#13;
kind: ManagedCluster&#13;
metadata:&#13;
 ...&#13;
 labels:&#13;
 cloud: Amazon&#13;
 clusterID: f2c2853e-e003-4a99-a4f7-2e231f9b36d9&#13;
 name: mycluster&#13;
 region: us-east-1&#13;
 vendor: OpenShift&#13;
 name: mycluster&#13;
spec:&#13;
 hubAcceptsClient: true&#13;
 leaseDurationSeconds: 60&#13;
status:&#13;
 allocatable:&#13;
 cpu: "21"&#13;
 memory: 87518Mi&#13;
 capacity:&#13;
 cpu: "24"&#13;
 memory: 94262Mi&#13;
 conditions:&#13;
 ...&#13;
 version:&#13;
 kubernetes: v1.18.3+2fbd7c7</pre>&#13;
</div></section>&#13;
<section data-pdf-bookmark="ClusterDeployment" data-type="sect3"><div class="sect3" id="clusterdeployment">&#13;
<h3>ClusterDeployment</h3>&#13;
<p><code>ClusterDeployment</code> (API group: hive.openshift.io/v1; namespace scoped) <a contenteditable="false" data-primary="ClusterDeployment API object" data-type="indexterm" id="idm45358190239896"/>controls the provisioning and decommissioning phases of the cluster. A controller on the hub takes care of running the openshift-installer on your behalf. If the cluster creation process fails for any reason (e.g., you encounter a quota limit within your cloud account), the cloud resources will be destroyed and another attempt will be made after a waiting period to reattempt successful creation of the cluster. Unlike traditional automation methods that “try once” and require user intervention upon failure, the Kubernetes reconcile loop for this API kind will continue to attempt to create the cluster (with appropriate waiting periods in between) as shown in <a data-type="xref" href="#example_clusterdeployment_created_by_the">Example 6-6</a>. You can also create these resources directly through the <code>oc</code> or <code>kubectl</code> like any Kubernetes native resource.</p>&#13;
<div data-type="example" id="example_clusterdeployment_created_by_the">&#13;
<h5><span class="label">Example 6-6. </span>Example <code>ClusterDeployment</code> created by the form</h5>&#13;
</div>&#13;
<pre data-type="programlisting">apiVersion: hive.openshift.io/v1&#13;
kind: ClusterDeployment&#13;
metadata:&#13;
 ...&#13;
 name: mycluster&#13;
 namespace: mycluster&#13;
spec:&#13;
 baseDomain: demo.red-chesterfield.com&#13;
 clusterMetadata:&#13;
 adminKubeconfigSecretRef:&#13;
 name: mycluster-0-cqpz4-admin-kubeconfig&#13;
 adminPasswordSecretRef:&#13;
 name: mycluster-0-cqpz4-admin-password&#13;
 clusterID: f2c2853e-e003-4a99-a4f7-2e231f9b36d9&#13;
 infraID: mycluster-9bn6s&#13;
 clusterName: mycluster&#13;
 controlPlaneConfig:&#13;
 servingCertificates: {}&#13;
 installed: true&#13;
 platform:&#13;
 aws:&#13;
 credentialsSecretRef:&#13;
 name: mycluster-aws-creds&#13;
 region: us-east-1&#13;
 provisioning:&#13;
 imageSetRef:&#13;
 name: img4.5.15-x86-64&#13;
 installConfigSecretRef:&#13;
 name: mycluster-install-config&#13;
 sshPrivateKeySecretRef:&#13;
 name: mycluster-ssh-private-key&#13;
 pullSecretRef:&#13;
 name: mycluster-pull-secret&#13;
status:&#13;
 apiURL: https://api.mycluster.REDACTED:6443&#13;
 cliImage: quay.io/openshift-release-dev/ocp-v4.0-art-&#13;
dev@sha256:8b8e08e498c61ccec5c446d6ab50c96792799c992c78cfce7bbb8481f04a64cb&#13;
 conditions:&#13;
 ...&#13;
 installerImage: quay.io/openshift-release-dev/ocp-v4.0-art-&#13;
dev@sha256:a3ed2bf438dfa5a114aa94cb923103432cd457cac51d1c4814ae0ef7e6e9853b&#13;
 provisionRef:&#13;
 name: mycluster-0-cqpz4&#13;
 webConsoleURL: https://console-openshift-console.apps.mycluster.REDACTED</pre>&#13;
</div></section>&#13;
<section data-pdf-bookmark="KlusterletAddonConfig" data-type="sect3"><div class="sect3" id="klusterletaddonconfig">&#13;
<h3>KlusterletAddonConfig</h3>&#13;
<p><code>KlusterletAddonConfig</code> (API group: agent.open-cluster-management.io/v1; namespace scoped) <a contenteditable="false" data-primary="KlusterletAddonConfig API object" data-type="indexterm" id="idm45358190230200"/>represents the capabilities that should be provided on the remote agent that manages the cluster. In <a data-type="xref" href="#an_example_of_the_klusterletaddonconfig">Example 6-7</a>, the Open Cluster Management project refers to the remote agent as a <code>klusterlet</code>, mirroring the language of <code>kubelet</code>.</p>&#13;
<div data-type="example" id="an_example_of_the_klusterletaddonconfig">&#13;
<h5><span class="label">Example 6-7. </span>An example of the <code>KlusterletAddonConfig</code> API object</h5>&#13;
</div>&#13;
<pre data-type="programlisting">apiVersion: agent.open-cluster-management.io/v1&#13;
kind: KlusterletAddonConfig&#13;
metadata:&#13;
 ..&#13;
 name: mycluster&#13;
 namespace: mycluster&#13;
spec:&#13;
 applicationManager:&#13;
 enabled: true&#13;
 certPolicyController:&#13;
 enabled: true&#13;
 clusterLabels:&#13;
 cloud: Amazon&#13;
 vendor: OpenShift&#13;
 clusterName: mycluster&#13;
 clusterNamespace: mycluster&#13;
 iamPolicyController:&#13;
 enabled: true&#13;
 policyController:&#13;
 enabled: true&#13;
 searchCollector:&#13;
 enabled: true&#13;
 version: 2.1.0</pre>&#13;
</div></section>&#13;
<section data-pdf-bookmark="MachinePool" data-type="sect3"><div class="sect3" id="machinepool">&#13;
<h3>MachinePool</h3>&#13;
<p><code>MachinePool</code> (API group: hive.openshift.io/v1; namespace -scoped) <a contenteditable="false" data-primary="MachinePool API object" data-type="indexterm" id="idm45358190220520"/><a contenteditable="false" data-primary="tenancy in clusters" data-secondary="MachinePool to group" data-type="indexterm" id="idm45358190219416"/><a contenteditable="false" data-primary="ManagedCluster API object" data-secondary="MachineSets" data-type="indexterm" id="idm45358190218040"/><a contenteditable="false" data-primary="MachineSets" data-type="indexterm" id="idm45358190216696"/>allows you to create a collection of hosts that work together and share characteristics. You might use a <code>MachinePool</code> to group a set of compute capacity that supports a specific team or line of business. As we will see in the next section, <code>MachinePool</code> also allows you to dynamically size your cluster. Finally, the status provides a view into the <code>MachineSet</code>s that are available on <code>ManagedCluster</code>. See <a data-type="xref" href="#the_machinepool_api_object_provides_a_co">Example 6-8</a> for the example <code>MachinePool</code> created earlier, which provides a control surface to scale the number of <a contenteditable="false" data-primary="MachinePool API object" data-secondary="replica scaling" data-type="indexterm" id="idm45358190211912"/><a contenteditable="false" data-primary="replicas" data-secondary="MachinePool API object scaling" data-type="indexterm" id="idm45358190210536"/>replicas up or down within the pool and status about the <code>MachineSet</code>s under management on the remote cluster.</p>&#13;
<div data-type="example" id="the_machinepool_api_object_provides_a_co">&#13;
<h5><span class="label">Example 6-8. </span>The example <code>MachinePool</code> API object</h5>&#13;
</div>&#13;
<pre data-type="programlisting">apiVersion: hive.openshift.io/v1&#13;
kind: MachinePool&#13;
metadata:&#13;
 ...&#13;
 name: mycluster-worker&#13;
 namespace: mycluster&#13;
spec:&#13;
 clusterDeploymentRef:&#13;
 name: mycluster&#13;
 name: worker&#13;
 platform:&#13;
 aws:&#13;
 rootVolume:&#13;
 iops: 100&#13;
 size: 100&#13;
 type: gp2&#13;
 type: m5.xlarge&#13;
 replicas: 3&#13;
status:&#13;
 machineSets:&#13;
 - maxReplicas: 1&#13;
 minReplicas: 1&#13;
 name: mycluster-9bn6s-worker-us-east-1a&#13;
 replicas: 1&#13;
 - maxReplicas: 1&#13;
 minReplicas: 1&#13;
 name: mycluster-9bn6s-worker-us-east-1b&#13;
 replicas: 1&#13;
 - maxReplicas: 1&#13;
 minReplicas: 1&#13;
 name: mycluster-9bn6s-worker-us-east-1c&#13;
 replicas: 1&#13;
 - maxReplicas: 0&#13;
 minReplicas: 0&#13;
 name: mycluster-9bn6s-worker-us-east-1d&#13;
 replicas: 0&#13;
 - maxReplicas: 0&#13;
 minReplicas: 0&#13;
 name: mycluster-9bn6s-worker-us-east-1e&#13;
 replicas: 0&#13;
 - maxReplicas: 0&#13;
 minReplicas: 0&#13;
 name: mycluster-9bn6s-worker-us-east-1f&#13;
 replicas: 0&#13;
 replicas: 3</pre>&#13;
<p>Once provisioned, the address for the Kubernetes API server and <a contenteditable="false" data-primary="provisioning across clouds" data-secondary="kubeconfig certificates for command-line cluster access" data-type="indexterm" id="idm45358190204792"/><a contenteditable="false" data-primary="kubeconfig certificates for command-line cluster access" data-type="indexterm" id="idm45358190203288"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="creating new cluster on cloud provider" data-tertiary="command-line cluster access" data-type="indexterm" id="idm45358190202152"/>OpenShift web console will be available from the cluster details page. You can use these coordinates to open your web browser and authenticate with the new cluster as the kubeadmin user. You can also access the <code>KUBECONFIG</code> certificates that allow you command-line access to the cluster.</p>&#13;
<p>You can download the <code>KUBECONFIG</code> authorization as shown in<a contenteditable="false" data-primary="provisioning across clouds" data-secondary="kubeconfig certificates for command-line cluster access" data-tertiary="downloading authorization" data-type="indexterm" id="idm45358190198552"/><a contenteditable="false" data-primary="kubeconfig certificates for command-line cluster access" data-secondary="downloading authorization" data-type="indexterm" id="idm45358190196808"/><a contenteditable="false" data-primary="Red Hat Advanced Cluster Management (RHACM)" data-secondary="provisioning across clouds" data-tertiary="kubeconfig authorization" data-type="indexterm" id="idm45358190195384"/> <a data-type="xref" href="#downloading_the_kubeconfig_authorization">Figure 6-10</a> for the new cluster from the RHACM web console under the cluster overview page or access it from the command line. From the web console, click on the cluster name in the list of clusters to view an overview of that cluster. Once the provisioning process has completed, you will be able to download the <code>KUBECONFIG</code> file that will allow you command-line access to the cluster.</p>&#13;
<figure><div class="figure" id="downloading_the_kubeconfig_authorization">&#13;
<img src="assets/hcok_0610.png"/>&#13;
<h6><span class="label">Figure 6-10. </span>Downloading the kubeconfig authorization from the RHACM web console</h6>&#13;
</div></figure>&#13;
<p>From the command line, you can retrieve the information stored in a secret under the cluster project (namespace) as in <a data-type="xref" href="#output_of_the_cluster_kubeconfig_filedot">Example 6-9</a>. Save the file contents and<a contenteditable="false" data-primary="KUBECONFIG context via oc" data-type="indexterm" id="idm45358190188232"/><a contenteditable="false" data-primary="oc command-line tool" data-secondary="KUBECONFIG context" data-type="indexterm" id="idm45358190187192"/><a contenteditable="false" data-primary="namespaces" data-secondary="KUBECONFIG context via oc" data-type="indexterm" id="idm45358190185816"/> configure your <code>KUBECONFIG</code> environment variable to point to the location of the file. Then <code>oc</code> will be able to run commands against the remote cluster.</p>&#13;
<div data-type="example" id="output_of_the_cluster_kubeconfig_filedot">&#13;
<h5><span class="label">Example 6-9. </span>Output of the cluster <code>KUBECONFIG</code> file</h5>&#13;
</div>&#13;
<pre data-type="programlisting">$ <strong>CLUSTER_NAME=mycluster ; oc get secret -n $CLUSTER_NAME \</strong>&#13;
<strong> -l hive.openshift.io/cluster-deployment-name=$CLUSTER_NAME \</strong>&#13;
<strong> -l hive.openshift.io/secret-type=kubeconfig \</strong>&#13;
<strong> -ogo-template="{{range .items}}{{.data.kubeconfig|base64decode}}{{end}}"</strong></pre>&#13;
<p>Now that our cluster is up and running, let’s walk through how we can <a contenteditable="false" data-primary="provisioning across clouds" data-secondary="creating new cluster on cloud provider" data-tertiary="scaling new cluster" data-type="indexterm" id="idm45358190178616"/><a contenteditable="false" data-primary="scaling" data-secondary="new cluster on another cloud" data-type="indexterm" id="idm45358190176888"/>scale the cluster. We will review this concept from the context of the oc CLI.</p>&#13;
<p>First, open two terminals and configure the <code>KUBECONFIG</code> or context for each of the hub clusters and our newly minted <code>mycluster</code>. See Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#example_six_onezero_an_example_of_what_t">6-10</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#an_example_of_how_terminal_two_will_look">6-11</a> for examples of what each of the two separate terminals will look like after you run these commands. Note the tip to override your PS1 shell prompt temporarily to avoid confusion when running commands on each cluster.</p>&#13;
<div data-type="example" id="example_six_onezero_an_example_of_what_t">&#13;
<h5><span class="label">Example 6-10. </span>An example of what Terminal 1 will look like</h5>&#13;
</div>&#13;
<pre data-type="programlisting">$ <strong>export PS1="hubcluster $ "</strong>&#13;
hubcluster $ <strong>export KUBECONFIG=hubcluster/auth/kubeconfig</strong> &#13;
hubcluster $ <strong>oc cluster-info</strong>&#13;
Kubernetes master is running at https://api.hubcluster.&lt;baseDomain&gt;:6443&#13;
 &#13;
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.<a contenteditable="false" data-primary="debugging" data-secondary="cluster information dump" data-type="indexterm" id="idm45358190167096"/><a contenteditable="false" data-primary="clusters" data-secondary="information dump" data-type="indexterm" id="idm45358190165656"/><a contenteditable="false" data-primary="kubectl command-line tool" data-secondary="cluster information dump" data-type="indexterm" id="idm45358190164280"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="creating new cluster on cloud provider" data-tertiary="debugging with cluster info dump" data-type="indexterm" id="idm45358190162872"/></pre>&#13;
<p/>&#13;
<div data-type="example" id="an_example_of_how_terminal_two_will_look">&#13;
<h5><span class="label">Example 6-11. </span>An example of how Terminal 2 will look</h5>&#13;
</div>&#13;
<pre data-type="programlisting">$ <strong>export PS1="mycluster $ "</strong>&#13;
mycluster $ <strong>CLUSTER_NAME=mycluster ; \</strong>&#13;
<strong> oc get secret -n $CLUSTER_NAME \</strong>&#13;
<strong> -l hive.openshift.io/cluster-deployment-name=$CLUSTER_NAME \</strong>&#13;
<strong> -l hive.openshift.io/secret-type=kubeconfig \</strong>&#13;
<strong> -ogo-template="{{range .items}}{{.data.kubeconfig|base64decode}}{{end}}" \</strong>&#13;
<strong> &gt; mycluster-kubeconfig&#13;
</strong>mycluster $ <strong>export KUBECONFIG=mycluster-kubeconfig</strong>&#13;
mycluster $ <strong>oc cluster-info</strong>&#13;
Kubernetes master is running at https://api.mycluster.&lt;baseDomain&gt;:6443&#13;
 &#13;
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</pre>&#13;
<p>Now you should have Terminal 1 with a prompt including <code>hubcluster</code> and Terminal 2 with a prompt including <code>mycluster</code>. We will refer to these terminals by the appropriate names through the rest of the example.</p>&#13;
<p>In the following walk-through, we will review the <code>MachineSet</code> API, which <a contenteditable="false" data-primary="MachineSets" data-secondary="MachineSet API" data-type="indexterm" id="idm45358190152072"/><a contenteditable="false" data-primary="replicas" data-secondary="MachineSet API" data-type="indexterm" id="idm45358190150664"/><a contenteditable="false" data-primary="MachineSet API" data-type="indexterm" id="idm45358190149288"/>underpins how an OpenShift cluster understands compute capacity. We will then scale the size of our managed cluster from the hub using the <code>MachinePool</code> API that we saw earlier.</p>&#13;
<p>In the <code>mycluster</code> terminal, review the <code>MachineSet</code>s for your cluster:</p>&#13;
<pre data-type="programlisting">mycluster $ <strong>oc get machinesets -n openshift-machine-api</strong>&#13;
NAME                              DESIRED CURRENT READY AVAILABLE AGE&#13;
mycluster-9bn6s-worker-us-east-1a 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1b 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1c 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1d 0                     0         1d&#13;
mycluster-9bn6s-worker-us-east-1e 0                     0         1d&#13;
mycluster-9bn6s-worker-us-east-1f 0                     0         1d</pre>&#13;
<p>Each <code>MachineSet</code> will have a name following the pattern:<a contenteditable="false" data-primary="MachineSets" data-secondary="names" data-type="indexterm" id="idm45358190143160"/> <code>&lt;clusterName&gt;-&lt;five-character identifier&gt;-&lt;machinePoolName&gt;-&lt;availabilityZone&gt;</code>. In your cluster, you should see counts for the desired number of machines per <code>MachineSet</code>, the current number of machines that are available, and the current number that are considered <code>Ready</code> to be integrated as nodes into the OpenShift cluster. Note that these three counts should generally be equivalent and should only vary when the cluster is in a transition state (adding or removing machines) or when an underlying availability problem in the cluster causes one or more machines to be considered unhealthy. For example, when you edit a <code>MachineSet</code> to increase the desired replicas, you will see the <code>Desired</code> count increment by one for that <code>MachineSet</code>. As the machine is provisioned and proceeds to boot and configure the <code>kubelet</code>, the <code>Current</code> count will increment by one. <a contenteditable="false" data-primary="kubelet" data-secondary="registering with Kubernetes API control plane" data-type="indexterm" id="idm45358190137528"/>Finally, as the <code>kubelet</code> registers with the Kubernetes API control plane and marks the node as <code>Ready</code>, the <code>Ready</code> count will increment by one. If at any point the machine becomes unhealthy, the <code>Ready</code> count may decrease. Similarly, if you reduced the <code>Desired</code> count by one, you would see the same staggered reduction in counts as the machine proceeds through various life-cycle states until it is removed.</p>&#13;
<p>Next, in the hub terminal, review the <code>worker MachinePool</code> defined for the managed cluster:</p>&#13;
<pre data-type="programlisting">hubcluster $ <strong>oc get machinepool -n mycluster</strong>&#13;
NAME             POOLNAME CLUSTERDEPLOYMENT REPLICAS&#13;
mycluster-worker worker   mycluster         3</pre>&#13;
<p>We will increase the size of the managed cluster <code>mycluster</code> by one node:</p>&#13;
<pre data-type="programlisting">hubcluster $ <strong>oc patch machinepool -n mycluster mycluster-worker \</strong>&#13;
<strong> -p '{"spec":{"replicas":4}}' --type=merge</strong>&#13;
<strong>machinepool.hive.openshift.io/mycluster-worker patched</strong></pre>&#13;
<p>The size of the worker node will be determined by the values set in the <code>MachinePool mycluster-worker</code>. The availability zone of the new node will be determined by the <code>MachinePool</code> controller, where nodes are distributed across availability zones as evenly as possible.</p>&#13;
<p>Immediately after you have patched the <code>MachinePool</code> to increase the number of desired replicas, rerun the command to view the <code>MachineSet</code> on your managed <span class="keep-together">cluster</span>:</p>&#13;
<pre data-type="programlisting">mycluster $ <strong>oc get machinesets -n openshift-machine-api</strong>&#13;
NAME                              DESIRED CURRENT READY AVAILABLE AGE&#13;
mycluster-9bn6s-worker-us-east-1a 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1b 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1c 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1d 1                     1         1d&#13;
mycluster-9bn6s-worker-us-east-1e 0                     0         1d&#13;
mycluster-9bn6s-worker-us-east-1f 0                     0         1d</pre>&#13;
<p>Over the course of a few minutes, you should see the new node on the managed cluster transition from <code>Desired</code> to <code>Current</code> to <code>Ready</code>, with a final result that looks like the following output:</p>&#13;
<pre data-type="programlisting">mycluster $ <strong>oc get machinesets -n openshift-machine-api</strong>&#13;
NAME                              DESIRED CURRENT READY AVAILABLE AGE&#13;
mycluster-9bn6s-worker-us-east-1a 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1b 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1c 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1d 1       1       1     1         1d&#13;
mycluster-9bn6s-worker-us-east-1e 0                     0         1d&#13;
mycluster-9bn6s-worker-us-east-1f 0                     0         1d</pre>&#13;
<p>Let’s recap what we’ve just seen. First, we used a declarative method (<em>install-config.yaml</em>) to provision our first cluster, called a hub. Next, we used the hub to provision the first managed cluster in our fleet. That managed cluster was created under the covers using the same IPI method but with the aid of Kubernetes API and <span class="keep-together">continuous</span> reconcilers that ensure that the running cluster matches the <code>Desired</code> state. One of the APIs that governs the <code>Desired</code> state is the <code>MachinePool</code> API on the hub cluster. Because our first fleet member, <code>mycluster</code>, was created from the hub cluster, we can use the <code>MachinePool API</code> to govern how <code>mycluster</code> adds or removes nodes. Or indeed, we can create additional <code>MachinePool</code>s that add capacity to the cluster.</p>&#13;
<p>Throughout the process, the underlying infrastructure substrate was completely managed through operators. The <code>MachineSet</code> operator on the managed cluster was given updated instructions by the <code>MachinePool</code> operator on the hub to grow the number of machines available in one of the <code>MachineSet</code>s supporting <code>mycluster</code>.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>We will use the term <em>infrastructure substrate</em> as a catchall <a contenteditable="false" data-primary="infrastructure" data-secondary="infrastructure substrate" data-type="indexterm" id="idm45358190110312"/>term to refer to the compute, network, and storage resources provided by bare metal virtualization within your datacenter or virtualization offered by a public cloud provider.</p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Upgrading Your Clusters to the Latest Version of Kubernetes" data-type="sect2"><div class="sect2" id="upgrading_your_clusters_to_the_latest_ve">&#13;
<h2>Upgrading Your Clusters to the Latest Version of Kubernetes</h2>&#13;
<p>As we saw with <code>MachinePool</code>s and <code>MachineSet</code>s, operators provide a <a contenteditable="false" data-primary="multicluster management" data-secondary="operating system currency for nodes" data-tertiary="upgrading Kubernetes version" data-type="indexterm" id="ch06-upK"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="upgrading multicluster fleet" data-type="indexterm" id="ch06-upK2"/><a contenteditable="false" data-primary="upgrades" data-secondary="Kubernetes upgrade for multicluster fleet" data-type="indexterm" id="ch06-upK3"/><a contenteditable="false" data-primary="ClusterVersionOperator" data-type="indexterm" id="idm45358190101512"/><a contenteditable="false" data-primary="version upgrade in multicluster Kubernetes" data-type="indexterm" id="ch06-upK4"/><a contenteditable="false" data-primary="operators" data-secondary="operator construct" data-tertiary="operator of operators ClusterVersionOperator" data-type="indexterm" id="idm45358190099000"/><a contenteditable="false" data-primary="operators" data-secondary="upgrading multicluster Kubernetes" data-type="indexterm" id="ch06-upK5"/>powerful way to abstract the differences across infrastructure substrates, allowing an administrator to declaratively specify the desired outcome. An OpenShift cluster is managed by the CVO, which acts as an “operator of operators” pattern to manage operators for each dimension of the cluster’s configuration (authentication, networking, machine creation, bootstrapping, and removal, and so on). <a contenteditable="false" data-primary="ClusterVersion API object" data-type="indexterm" id="idm45358190095112"/>Every cluster will have a <code>Cluster​Ver⁠sion</code> API object named <code>version</code>. You can retrieve the details for this object with the command:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get clusterversion version -o yaml</strong></pre>&#13;
<p><code>ClusterVersion</code> specifies a “channel” to seek available<a contenteditable="false" data-primary="channel per Open Cluster Management" data-secondary="upgrading multicluster Kubernetes" data-type="indexterm" id="idm45358190091368"/> versions for the cluster and a desired version from that channel. Think of a channel as an ongoing list of available versions (e.g., 4.5.1, 4.5.2, 4.5.7, and so on). There are channels for “fast” adoption of new versions, as well as “stable” versions. The fast channels produce new versions quickly. Coupled with connected telemetry data from a broad source of OpenShift clusters running across infrastructure substrates and industries, fast channels allow the delivery and validation of new releases very quickly (on order of weeks or days). As releases in fast channels have enough supporting evidence that they are broadly acceptable across the global fleet of OpenShift clusters, versions are promoted to stable channels. Hence, the list of versions within a channel is not always consecutive. <a contenteditable="false" data-primary="ClusterDeployment API object" data-secondary="example code" data-type="indexterm" id="idm45358190088968"/>An example <code>ClusterVersion</code> API object is represented in <a data-type="xref" href="#an_example_clusterversion_api_object_tha">Example 6-12</a>.</p>&#13;
<div data-type="example" id="an_example_clusterversion_api_object_tha">&#13;
<h5><span class="label">Example 6-12. </span>An example <code>ClusterVersion</code> API object that records the version history for the cluster and the desired version—changing the desired version will cause the operator to begin applying updates to achieve the goal</h5>&#13;
</div>&#13;
<pre data-type="programlisting">apiVersion: config.openshift.io/v1&#13;
kind: ClusterVersion&#13;
metadata:&#13;
 name: version&#13;
spec:&#13;
 channel: stable-4.5&#13;
 clusterID: f2c2853e-e003-4a99-a4f7-2e231f9b36d9&#13;
 desiredUpdate:&#13;
 force: false&#13;
 image: quay.io/openshift-release-dev/ocp-&#13;
release@sha256:38d0bcb5443666b93a0c117f41ce5d5d8b3602b411c574f4e164054c43408a01&#13;
 version: 4.5.22&#13;
 upstream: https://api.openshift.com/api/upgrades_info/v1/graph&#13;
status:&#13;
 availableUpdates: null&#13;
 conditions:&#13;
 - lastTransitionTime: "2020-12-02T23:08:32Z"&#13;
 message: Done applying 4.5.22&#13;
 status: "True"&#13;
 type: Available&#13;
 - lastTransitionTime: "2020-12-11T17:05:00Z"&#13;
 status: "False"&#13;
 type: Failing&#13;
 - lastTransitionTime: "2020-12-11T17:09:32Z"&#13;
 message: Cluster version is 4.5.22&#13;
 status: "False"&#13;
 type: Progressing&#13;
 - lastTransitionTime: "2020-12-02T22:46:39Z"&#13;
 status: "True"&#13;
 type: RetrievedUpdates&#13;
 desired:&#13;
 force: false&#13;
 image: quay.io/openshift-release-dev/ocp-&#13;
release@sha256:38d0bcb5443666b93a0c117f41ce5d5d8b3602b411c574f4e164054c43408a01&#13;
 version: 4.5.22&#13;
 history:&#13;
 - completionTime: "2020-12-11T17:09:32Z"&#13;
 image: quay.io/openshift-release-dev/ocp-&#13;
release@sha256:38d0bcb5443666b93a0c117f41ce5d5d8b3602b411c574f4e164054c43408a01&#13;
 startedTime: "2020-12-11T16:39:05Z"&#13;
 state: Completed&#13;
 verified: false&#13;
 version: 4.5.22&#13;
 - completionTime: "2020-12-02T23:08:32Z"&#13;
 image: quay.io/openshift-release-dev/ocp-&#13;
release@sha256:1df294ebe5b84f0eeceaa85b2162862c390143f5e84cda5acc22cc4529273c4c&#13;
 startedTime: "2020-12-02T22:46:39Z"&#13;
 state: Completed&#13;
 verified: false&#13;
 version: 4.5.15&#13;
 observedGeneration: 2&#13;
 versionHash: m0fIO00kMu8=</pre>&#13;
<p>Upgrading a version of Kubernetes along with all other supporting APIs and infrastructure around it can be a daunting task. <a contenteditable="false" data-primary="operators" data-secondary="upgrading multicluster Kubernetes" data-tertiary="OpenShift Update Service" data-type="indexterm" id="idm45358190081000"/><a contenteditable="false" data-primary="OpenShift Update Service (OSUS)" data-type="indexterm" id="idm45358190079400"/><a contenteditable="false" data-primary="OSUS (OpenShift Update Service)" data-type="indexterm" id="idm45358190078280"/><a contenteditable="false" data-primary="Cincinnati (OpenShift Update Service)" data-type="indexterm" id="idm45358190077160"/>The operator that controls the life cycle of all of the container images is known informally as <a href="https://oreil.ly/6HIZd">“Cincinnati”</a> and formally as the <em>OpenShift Update Service</em> (OSUS). OSUS (or Cincinnati) maintains a connected graph of versions and tracks which “walks” or “routes” within the graph are known as good upgrade paths. For example, an issue may be detected in early release channels that indicates that the upgrade from 4.4.23 to 4.5.0 to 4.5.18 may be associated with a specific problem. A fix can be released to create a new release 4.4.24 that then allows a successful and predictable upgrade from 4.4.23 to 4.4.24 to 4.5.0 to 4.5.18. The graph records the successive nodes that must be walked to ensure success.</p>&#13;
<p>However, the OSUS operator removes the guesswork, <a contenteditable="false" data-primary="channel per Open Cluster Management" data-secondary="upgrading multicluster Kubernetes" data-type="indexterm" id="idm45358190073640"/><a contenteditable="false" data-primary="ClusterVersionOperator" data-type="indexterm" id="idm45358190072184"/>allowing the cluster administrator to specify the desired version from the channel. From there, the CVO will carry out the following tasks:<sup><a data-type="noteref" href="ch06.html#ch01fn37" id="ch01fn37-marker">2</a></sup></p>&#13;
<ol>&#13;
<li><p>Upgrade the Kubernetes and OpenShift control plane pods, including etcd.</p></li>&#13;
<li><p>Upgrade the operating system for the nodes running the control plane pods.</p></li>&#13;
<li><p>Upgrade the cluster operators controlling aspects like authentication, networking, storage, and so on.</p></li>&#13;
<li><p>For nodes managed by the <code>MachineConfigOperator</code>, upgrade the operating system for the nodes running the data plane pods (user workload).</p></li>&#13;
</ol>&#13;
<p>The upgrade takes place in a rolling fashion, avoiding bursting the size<a contenteditable="false" data-primary="Kubernetes" data-secondary="about" data-tertiary="rolling upgrades" data-type="indexterm" id="idm45358190064648"/><a contenteditable="false" data-primary="rolling upgrades" data-secondary="multicluster Kubernetes upgrade" data-type="indexterm" id="idm45358190063000"/><a contenteditable="false" data-primary="control plane failure" data-secondary="quorum of etcd cluster" data-tertiary="rolling multicluster upgrades" data-type="indexterm" id="idm45358190061560"/><a contenteditable="false" data-primary="control planes" data-secondary="rolling multicluster upgrades" data-type="indexterm" id="idm45358190059896"/><a contenteditable="false" data-primary="control planes" data-secondary="failure" data-see="control plane failure" data-type="indexterm" id="idm45358190058504"/> of the cluster or taking out too much capacity at the same time. Because the control plane is spread across three machines, as each machine undergoes an operating system update and reboot, the other two nodes maintain the availability of the Kubernetes control plane, including the datastore (etcd), the scheduler, the controller, the Kubernetes API server, and the network ingress controller.</p>&#13;
<p>When the data plane is upgraded, the upgrade process will respect <code>PodDisruptionBudget</code>s and look for feedback about the health of OpenShift and user workloads running on each node by means of liveness and readiness probes.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Sometimes the group of clusters under management is referred to as a <em>fleet</em>.<a contenteditable="false" data-primary="multicluster management" data-secondary="fleets" data-type="indexterm" id="idm45358190053736"/><a contenteditable="false" data-primary="fleets of cluster groups" data-type="indexterm" id="idm45358190052328"/><a contenteditable="false" data-primary="fleets of cluster groups" data-secondary="fleet members" data-type="indexterm" id="idm45358190051208"/><a contenteditable="false" data-primary="multicluster management" data-secondary="fleets" data-tertiary="fleet members" data-type="indexterm" id="idm45358190049816"/> Individual clusters under management may be referred to as <em>fleet members</em>, primarily to distinguish them from the hub cluster that is responsible for management.</p>&#13;
</div>&#13;
<p>From the RHACM web console, you can manipulate the desired version of a managed cluster for a single fleet member or the entire fleet. <a contenteditable="false" data-primary="Red Hat Advanced Cluster Management (RHACM)" data-secondary="provisioning across clouds" data-tertiary="upgrading Kubernetes version" data-type="indexterm" id="idm45358190046744"/>From the console, choose the “Upgrade cluster” action for any cluster that shows “Upgrade available.” Recall from the discussion around channels that not every channel may have an upgrade currently available. Additionally, the list of versions may not be consecutive. Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#actions_permitted_on_a_cluster_allow_a_f">6-11</a>, <a data-type="xref" data-xrefstyle="select:labelnumber" href="#the_list_of_available_versions_is_provid">6-12</a>, and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#multiple_clusters_may_be_selected_for_up">6-13</a> provide examples of what this actually looks like for a specific cluster or for multiple clusters.</p>&#13;
<figure><div class="figure" id="actions_permitted_on_a_cluster_allow_a_f">&#13;
<img src="assets/hcok_0611.png"/>&#13;
<h6><span class="label">Figure 6-11. </span>Actions permitted on a cluster allow a fleet manager to upgrade the desired version of a cluster</h6>&#13;
</div></figure>&#13;
<figure><div class="figure" id="the_list_of_available_versions_is_provid">&#13;
<img src="assets/hcok_0612.png"/>&#13;
<h6><span class="label">Figure 6-12. </span>The list of available versions is provided for user selection</h6>&#13;
</div></figure>&#13;
<figure><div class="figure" id="multiple_clusters_may_be_selected_for_up">&#13;
<img src="assets/hcok_0613.png"/>&#13;
<h6><span class="label">Figure 6-13. </span>Multiple clusters may be selected for upgrade, and the versions available will vary based on the cluster’s attached channel configuration in the <code>ClusterVersion</code> object</h6>&#13;
</div></figure>&#13;
<p>A core topic for this book is how to manage your clusters as a fleet, and for that, we will rely on policies. The preceding discussion should provide a foundation for you to understand the moving parts and see that you can explicitly trigger upgrade behavior across the fleet. In <a data-type="xref" href="ch07.html#multicluster_policy_configuration">Chapter 7</a>, we will discuss how we can control upgrade behavior across the fleet by policy.<a contenteditable="false" data-primary="" data-startref="ch06-upK" data-type="indexterm" id="idm45358190032792"/><a contenteditable="false" data-primary="" data-startref="ch06-upK2" data-type="indexterm" id="idm45358190031448"/><a contenteditable="false" data-primary="" data-startref="ch06-upK3" data-type="indexterm" id="idm45358190030072"/><a contenteditable="false" data-primary="" data-startref="ch06-upK4" data-type="indexterm" id="idm45358190028696"/><a contenteditable="false" data-primary="" data-startref="ch06-upK5" data-type="indexterm" id="idm45358190027320"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary of Multicloud Cluster Provisioning" data-type="sect2"><div class="sect2" id="summary_of_multicloud_cluster_provisioni">&#13;
<h2>Summary of Multicloud Cluster Provisioning</h2>&#13;
<p>Throughout our example, the specific infrastructure substrate showed up in a <a contenteditable="false" data-primary="provisioning across clouds" data-secondary="about" data-tertiary="summary" data-type="indexterm" id="idm45358190024008"/><a contenteditable="false" data-primary="infrastructure" data-secondary="infrastructure substrate" data-tertiary="provisioning across clouds" data-type="indexterm" id="idm45358190022344"/><a contenteditable="false" data-primary="YAML files" data-secondary="provisioning across clouds" data-tertiary="summary" data-type="indexterm" id="idm45358190020664"/><a contenteditable="false" data-primary="hub cluster" data-secondary="infrastructure substrate" data-type="indexterm" id="idm45358190019000"/><a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="hub cluster" data-tertiary="infrastructure substrate" data-type="indexterm" id="idm45358190017608"/><a contenteditable="false" data-primary="ClusterDeployment API object" data-secondary="infrastructure substrate" data-type="indexterm" id="idm45358190015928"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="OpenShift installers for web platforms" data-tertiary="infrastructure substrate specified" data-type="indexterm" id="idm45358190014520"/>few declarative APIs, specifically represented by the <em>install-config.yaml</em> for the hub cluster and as part of the secrets referenced by the <code>ClusterDeployment</code> API object for the managed cluster. However, the action of provisioning a new cluster and adding or removing nodes for that fleet member was completely driven through Kubernetes API objects.</p>&#13;
<p>In addition, the upgrade life cycle managed through <a contenteditable="false" data-primary="ClusterVersionOperator" data-secondary="provisioning across clouds summary" data-type="indexterm" id="idm45358190011192"/>the CVO is consistent across supported infrastructure substrates. Hence, regardless if you provision an OpenShift cluster on a public cloud service or in your datacenter, you can still declaratively manage the upgrade process. The powerful realization you should now understand is that managing the infrastructure substrate for OpenShift clusters in multicloud scenarios can be completely abstracted away from many basic cluster-provisioning life-cycle operations.</p>&#13;
<p>Beyond controlling the capacity of your fleet from the hub, <a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="policies assigned via" data-type="indexterm" id="idm45358190008696"/>you can assign policies with Open Cluster Management and drive behavior like fleet upgrades. We will see an example of fleet upgrade by policy in <a data-type="xref" href="ch07.html#multicluster_policy_configuration">Chapter 7</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="OpenShift as a Service" data-type="sect2"><div class="sect2" id="openshift_as_a_service">&#13;
<h2>OpenShift as a Service</h2>&#13;
<p>The previous section described how you can abstract the provisioning and<a contenteditable="false" data-primary="clusters" data-secondary="multicloud management by OpenShift" data-type="indexterm" id="idm45358190004056"/><a contenteditable="false" data-primary="multicluster management" data-secondary="OpenShift as a Service" data-type="indexterm" id="idm45358190002568"/><a contenteditable="false" data-primary="OpenShift as a Service" data-type="indexterm" id="idm45358190001192"/><a contenteditable="false" data-primary="Kubernetes as a Service" data-type="indexterm" id="idm45358190000088"/><a contenteditable="false" data-primary="vendor-provided Kubernetes/OpenShift as a Service" data-type="indexterm" id="idm45358189998984"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="managed service for OpenShift" data-type="indexterm" id="idm45358189997784"/><a contenteditable="false" data-primary="multicluster management" data-secondary="managed service for OpenShift" data-type="indexterm" id="idm45358189996376"/> life cycle of OpenShift across multiple infrastructure substrates. Under the model we’ve outlined, you are responsible for the availability of your clusters. For budget or organizational reasons, you may choose to consider a managed service for OpenShift or Kubernetes. Using a vendor-provided “OpenShift as a Service” or “Kubernetes as a Service” can change how you interact with some dimensions, including cluster creation or decommissioning. However, your applications will run consistently regardless of whether the vendor manages the underlying infrastructure or you manage it.</p>&#13;
<section data-pdf-bookmark="Azure Red Hat OpenShift" data-type="sect3"><div class="sect3" id="azure_red_hat_openshift">&#13;
<h3>Azure Red Hat OpenShift</h3>&#13;
<p><a href="https://oreil.ly/0OWW7">Azure Red Hat OpenShift</a> <a contenteditable="false" data-primary="Microsoft Azure provisioning" data-secondary="Azure Red Hat OpenShift" data-type="indexterm" id="idm45358189991864"/><a contenteditable="false" data-primary="Azure (Microsoft) provisioning" data-secondary="Azure Red Hat OpenShift" data-type="indexterm" id="idm45358189990440"/>is integrated into the Microsoft Azure ecosystem, including Azure billing. Other aspects, including single sign-on, are automatically configured with Azure Active Directory, simplifying how you expose capabilities to your organization, particularly if you are already consuming other services on Azure. The underlying service is maintained by a partnership between Microsoft and Red Hat.</p>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Red Hat OpenShift on AWS" data-type="sect3"><div class="sect3" id="red_hat_openshift_on_aws">&#13;
<h3 class="less_space">Red Hat OpenShift on AWS</h3>&#13;
<p><a href="https://oreil.ly/fX0a2">Red Hat OpenShift on AWS</a> <a contenteditable="false" data-primary="Amazon Web Services (AWS) provisioning" data-secondary="Red Hat OpenShift" data-type="indexterm" id="idm45358189985528"/><a contenteditable="false" data-primary="AWS (Amazon Web Services) provisioning" data-secondary="Red Hat OpenShift" data-type="indexterm" id="idm45358189984088"/>was announced at the end of 2020 with planned availability in 2021. It integrates OpenShift into the Amazon ecosystem, allowing for access and creation through the Amazon cloud console and consistent billing with the rest of your Amazon account. The underlying service is maintained by a partnership between Amazon and Red Hat.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Red Hat OpenShift on IBM Cloud" data-type="sect3"><div class="sect3" id="red_hat_openshift_on_ibm_cloud">&#13;
<h3>Red Hat OpenShift on IBM Cloud</h3>&#13;
<p><a href="https://oreil.ly/gZBHU">Red Hat OpenShift on IBM Cloud</a> <a contenteditable="false" data-primary="IBM Cloud" data-secondary="Red Hat OpenShift" data-type="indexterm" id="idm45358189979832"/>integrates OpenShift consumption into the IBM Cloud ecosystem, including integration with IBM Cloud single sign-on and billing. In addition, IBM Cloud APIs are provided to manage cluster provisioning, worker nodes, and the upgrade process. These APIs allow separate access controls via IBM Cloud Identity and Access Management for management of the cluster versus the access controls used for managing resources in the cluster. The underlying service is maintained by IBM.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="OpenShift Dedicated" data-type="sect3"><div class="sect3" id="openshift_dedicated">&#13;
<h3>OpenShift Dedicated</h3>&#13;
<p><a href="https://cloud.redhat.com">OpenShift Dedicated</a> is a managed <a contenteditable="false" data-primary="OpenShift Dedicated (Red Hat)" data-type="indexterm" id="idm45358189975352"/>OpenShift as a Service offering provided by Red Hat. OpenShift clusters can be created across a variety of clouds from this service, in some cases under your own preexisting cloud account. Availability and maintenance of the cluster are handled by the Red Hat SRE team. The underlying service is maintained by Red Hat with options to bring your own cloud accounts on some supported infrastructure providers like AWS.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Kubernetes as a Service" data-type="sect2"><div class="sect2" id="kubernetes_as_a_service">&#13;
<h2>Kubernetes as a Service</h2>&#13;
<p>In addition to vendor-managed OpenShift as a Service, many vendors offer <a contenteditable="false" data-primary="Kubernetes as a Service" data-type="indexterm" id="idm45358189971224"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="managed service for Kubernetes" data-type="indexterm" id="idm45358189970120"/><a contenteditable="false" data-primary="multicluster management" data-secondary="managed service for Kubernetes" data-type="indexterm" id="idm45358189968808"/>managed Kubernetes as a Service distributions. These are typically where the vendor adopts Kubernetes and integrates it into its ecosystem. The following are some examples of these services:<a contenteditable="false" data-primary="Kubernetes as a Service" data-secondary="providers" data-type="indexterm" id="idm45358189967080"/><a contenteditable="false" data-primary="IBM Cloud" data-secondary="Kubernetes as a Service" data-type="indexterm" id="idm45358189965704"/><a contenteditable="false" data-primary="Amazon Elastic Compute Cloud (EC2) Kubernetes as a Service" data-type="indexterm" id="idm45358189964328"/><a contenteditable="false" data-primary="Azure (Microsoft) provisioning" data-secondary="Kubernetes as a Service" data-type="indexterm" id="idm45358189963176"/><a contenteditable="false" data-primary="Microsoft Azure provisioning" data-secondary="Kubernetes as a Service" data-type="indexterm" id="idm45358189961784"/><a contenteditable="false" data-primary="Google" data-secondary="Kubernetes as a Service" data-type="indexterm" id="idm45358189960392"/></p>&#13;
<ul>&#13;
<li><p>Amazon Elastic Kubernetes Service</p></li>&#13;
<li><p>Azure Kubernetes Service</p></li>&#13;
<li><p>Google Kubernetes Engine</p></li>&#13;
<li><p>IBM Cloud Kubernetes Service</p></li>&#13;
</ul>&#13;
<p>Because the Kubernetes community leaves some decisions up to vendors<a contenteditable="false" data-primary="Kubernetes as a Service" data-secondary="vendor distribution variations" data-type="indexterm" id="idm45358189955912"/> or users who assemble their own distributions, each of these managed services can introduce some variations that you should be aware of when adopting them as part of a larger multicloud strategy. In particular, several specific areas in Kubernetes have been evolving quickly:</p>&#13;
<ul>&#13;
<li><p>Cluster creation</p></li>&#13;
<li><p>User identity and access management</p></li>&#13;
<li><p>Network routing</p></li>&#13;
<li><p>Pod security management</p></li>&#13;
<li><p>Role-based access control</p></li>&#13;
<li><p>Value-added admission controllers</p></li>&#13;
<li><p>Operating system management of the worker nodes</p></li>&#13;
<li><p>Different security apparatus to manage compliance</p></li>&#13;
</ul>&#13;
<p>Across each dimension, a vendor providing a managed Kubernetes service must decide how to best integrate that aspect of Kubernetes into the cloud provider’s ecosystem. The core API should respond consistently by way of the <a href="https://oreil.ly/sWAXA">CNCF Kubernetes Certification process</a>.<a contenteditable="false" data-primary="Cloud Native Computing Foundation (CNCF)" data-secondary="Kubernetes Certification process" data-type="indexterm" id="idm45358189947464"/> In practice, differences tend to arise where Kubernetes is integrated into a particular cloud ecosystem.</p>&#13;
<p>For instance, in some cases a managed Kubernetes service will come with Kubernetes RBAC deployed and configured out of the box. Other vendors may leave it to the cluster creator to configure RBAC for Kubernetes. Across vendors that automatically configure the Kubernetes with RBAC, the set of out-of-the-box <code>ClusterRole</code>s and roles can vary.</p>&#13;
<p>In other cases, the network ingress for a Kubernetes cluster can vary from cloud-specific extensions to use of the community network ingress controller. Hence, your application may need to provide alternative network ingress behavior based on the cloud providers that you choose to provide Kubernetes. When using Ingress (API group: networking.k8s.io/v1) on a particular cloud vendor–managed Kubernetes, the set of respected annotations can vary across providers, requiring additional validation for apps that must tolerate different managed Kubernetes services. With an OpenShift cluster (managed by a vendor or by you), all applications define the standard Ingress API with a fixed set of annotations or Route (API group: route.openshift.io/v1) API, which will be correctly exposed into the specific infrastructure substrate.</p>&#13;
<p>The variation that you must address in your application architectures and multicloud management strategies is not insurmountable. However, be aware of these aspects as you plan your adoption strategy. Whether you adopt an OpenShift as a Service provider or run OpenShift within your own cloud accounts, all of the API-facing applications, including RBAC and networking, will behave the same.<a contenteditable="false" data-primary="" data-startref="ch06-apac" data-type="indexterm" id="idm45358189942408"/><a contenteditable="false" data-primary="" data-startref="ch06-webprov" data-type="indexterm" id="idm45358189941032"/><a contenteditable="false" data-primary="" data-startref="ch06-webprov2" data-type="indexterm" id="idm45358189939656"/><a contenteditable="false" data-primary="" data-startref="ch06-webprov3" data-type="indexterm" id="idm45358189938280"/><a contenteditable="false" data-primary="" data-startref="ch06-webprov4" data-type="indexterm" id="idm45358189936904"/><a contenteditable="false" data-primary="" data-startref="ch06-webprov5" data-type="indexterm" id="idm45358189935528"/><a contenteditable="false" data-primary="" data-startref="ch06-umos" data-type="indexterm" id="idm45358189934152"/><a contenteditable="false" data-primary="" data-startref="ch06-umos2" data-type="indexterm" id="idm45358189932776"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Operating System Currency for Nodes" data-type="sect1"><div class="sect1" id="operating_system_currency_for_nodes">&#13;
<h1>Operating System Currency for Nodes</h1>&#13;
<p>As your consumption of an OpenShift cluster grows, practical concerns around <a contenteditable="false" data-primary="multicluster management" data-secondary="operating system currency for nodes" data-type="indexterm" id="idm45358189929688"/><a contenteditable="false" data-primary="upgrades" data-secondary="operating system currency for nodes" data-type="indexterm" id="idm45358189928344"/><a contenteditable="false" data-primary="control planes" data-secondary="operating system updates" data-type="indexterm" id="idm45358189926952"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="operating system currency for nodes" data-type="indexterm" id="idm45358189925560"/><a contenteditable="false" data-primary="clusters" data-secondary="cluster version management by OpenShift" data-type="indexterm" id="idm45358189924152"/>security and operating system currency must be addressed. With an OpenShift 4.x cluster, the control plane hosts are configured with Red Hat CoreOS as the operating system. When upgrades occur for the cluster, the operating system of the control plane nodes are also upgraded. The CoreOS package manager uses a novel approach to applying updates: updates are packaged into containers and applied transactionally. Either the entire update succeeds or fails. When managing the update of an OpenShift control plane, the result of this approach limits the potential for partially completed or failed installs from the interaction of unknown or untested configurations within the operating system. By default, the operating system provisioned for workers will also use Red Hat CoreOS, allowing the data plane of your cluster the same transactional update benefits.</p>&#13;
<p>It is possible to add workers to an OpenShift cluster configured with RHEL. The process to add a RHEL worker node is covered in the product documentation and is beyond the scope of this book.</p>&#13;
<p>If you integrate a managed Kubernetes service into your multicluster strategy, <a contenteditable="false" data-primary="multicluster management" data-secondary="managed service division of responsibility" data-type="indexterm" id="idm45358189920712"/><a contenteditable="false" data-primary="provisioning across clouds" data-secondary="managed service division of responsibility" data-type="indexterm" id="idm45358189919208"/><a contenteditable="false" data-primary="Kubernetes as a Service" data-secondary="division of responsibility" data-type="indexterm" id="idm45358189917784"/><a contenteditable="false" data-primary="OpenShift as a Service" data-secondary="division of responsibility" data-type="indexterm" id="idm45358189916392"/><a contenteditable="false" data-primary="vendor-provided Kubernetes/OpenShift as a Service" data-secondary="division of responsibility" data-type="indexterm" id="idm45358189915000"/>pay attention to the division of responsibilities between your vendor and your teams: who owns the currency/compliance status of the worker nodes in the cluster? Virtually all of the managed Kubernetes service providers manage the operating system of the control plane nodes. However, there is variance across the major cloud vendors on who is responsible for the operating system of the worker nodes.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00012">&#13;
<h1>Summary</h1>&#13;
<p>We have covered quite a bit in this chapter. By now, you should understand how IPI provides a consistent abstraction of many infrastructure substrates. Once provisioned, operators within OpenShift manage the life-cycle operations for key functions of the cluster, including machine management, authentication, networking, and storage. We can also use the API exposed by these operators to request and drive upgrade operations against the control plane of the cluster and the operating system of the supporting nodes.</p>&#13;
<p>We also introduced cluster life-cycle management from <a href="https://oreil.ly/3J1SW">Open Cluster Management</a> using a supporting offering, Red Hat Advanced Cluster Management. Using RHACM, we saw how to trigger the upgrade behavior for user-managed clusters on any infrastructure substrate.</p>&#13;
<p>In the next chapter, we will continue to leverage cluster operators to configure and drive cluster behavior by defining Open Cluster Management policies that we can apply to one or more clusters under management.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch01fn36"><sup><a href="ch06.html#ch01fn36-marker">1</a></sup> Ted Dunning, “Using Data Fabric and Kubernetes in Edge Computing,” The Newstack (May 21, 2020), <a href="https://oreil.ly/W3J7f"><em class="hyperlink">https://oreil.ly/W3J7f</em></a>; “Edge Computing at Chick-fil-A,” Chick-fil-A Tech Blog (July 30, 2018), <a href="https://oreil.ly/HcJqZ"><em class="hyperlink">https://oreil.ly/HcJqZ</em></a>.</p><p data-type="footnote" id="ch01fn37"><sup><a href="ch06.html#ch01fn37-marker">2</a></sup> Rob Szumski, “The Ultimate Guide to OpenShift Release and Upgrade Process for Cluster Administrators,” Red Hat OpenShift Blog (November 9, 2020), <a href="https://oreil.ly/hKCex"><em class="hyperlink">https://oreil.ly/hKCex</em></a>.</p></div></div></section></body></html>