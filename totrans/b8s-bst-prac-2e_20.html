<html><head></head><body><section data-pdf-bookmark="Chapter 20. Chaos Testing, Load Testing, &#10;and Experiments" data-type="chapter" epub:type="chapter"><div class="chapter" id="chaos_testing_load_testing_experiments">&#13;
<h1><span class="label">Chapter 20. </span>Chaos Testing, Load Testing, &#13;
<span class="keep-together">and Experiments</span></h1>&#13;
&#13;
&#13;
<p>This chapter covers three different methods of testing applications in your Kubernetes&#13;
cluster: chaos testing, load testing, and experiments. All these tools can be used to help you build more useful, more resilient, and more performant&#13;
applications. They can also provide insight into your application and help you better understand your users and anticipate the impact of changes before you&#13;
roll them out broadly. This insight enables you to make better decisions and&#13;
identify areas for future improvements. The following sections will describe&#13;
the details of each type of test, their goals, and the prerequisites&#13;
necessary before starting each test.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Chaos Testing" data-type="sect1"><div class="sect1" id="id260">&#13;
<h1>Chaos Testing</h1>&#13;
&#13;
<p>Chaos testing, as<a data-primary="testing" data-secondary="chaos testing" data-tertiary="purpose of" data-type="indexterm" id="id1124"/><a data-primary="chaos testing" data-secondary="purpose of" data-type="indexterm" id="id1125"/> its name indicates, is testing your application’s ability&#13;
to respond to chaos in the world, But what exactly does chaos mean? Broadly&#13;
speaking, for an application chaos means introducing unusual, but not&#13;
wholly unexpected, edge conditions to your application and seeing how it&#13;
responds. This enables you to understand if your application is resilient&#13;
to these edge conditions that may not have previously occurred during&#13;
development of the application but may occur at some point during&#13;
the operation of your application. Often our application development&#13;
occurs during idealized conditions. Unfortunately, when exposed to the&#13;
real world for long enough, these idealized conditions are challenged by&#13;
errors and failures that were not present during initial development.&#13;
These errors can include communication errors, network disconnections, storage&#13;
problems, and application crashes and failures. Chaos testing is the art&#13;
of artificially introducing these errors into your test environments and&#13;
observing how well your application copes with them.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Goals for Chaos Testing" data-type="sect2"><div class="sect2" id="id261">&#13;
<h2>Goals for Chaos Testing</h2>&#13;
&#13;
<p>The goals <a data-primary="testing" data-secondary="chaos testing" data-tertiary="goals of" data-type="indexterm" id="id1126"/><a data-primary="chaos testing" data-secondary="goals of" data-type="indexterm" id="id1127"/>for chaos testing are to introduce extreme conditions into your application’s environment and to observe how your application&#13;
behaves in these conditions, especially, how it fails. It may seem unusual to test in such a way that failures are expected and desirable. While application failures in general are something that&#13;
we try to avoid, it is far better to observe those failures in a test&#13;
environment where customers or users are not impacted.&#13;
We hope to observe failures when chaos testing because they offer an opportunity to fix those problems before they affect our users or&#13;
customers.</p>&#13;
&#13;
<p>Of course the goal is to introduce a <em>realistic</em> level of error into our applications to see how they behave. Introducing a level of error that is not expected to ever occur in practice,&#13;
while interesting, isn’t a great use of time or resources.&#13;
Excessive levels of error can help us harden our applications for&#13;
extreme environments, but if such extremes never occur, the effort&#13;
to harden the application is wasted. Of course each application has&#13;
a different level of both variability and resilience that is&#13;
desired. The level of resiliency expected of a mobile game is&#13;
dramatically less than the level of resilience expected of an aircraft&#13;
or automobile. Understanding both the resilience requirements and&#13;
expected environment for your application is a critical prerequisite&#13;
for high-quality chaos testing.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prerequisites for Chaos Testing" data-type="sect2"><div class="sect2" id="id262">&#13;
<h2>Prerequisites for Chaos Testing</h2>&#13;
&#13;
<p>To build <a data-primary="testing" data-secondary="chaos testing" data-tertiary="prerequisites" data-type="indexterm" id="test-chaos-prereq"/><a data-primary="chaos testing" data-secondary="prerequisites" data-type="indexterm" id="chaos-prereq"/><a data-primary="prerequisites" data-secondary="chaos testing" data-type="indexterm" id="prereq-chaos"/>a useful chaos test it is critical to understand the&#13;
environmental conditions that your application may encounter. This&#13;
includes both the expected frequency of errors and also the types&#13;
of errors that may occur. For example, is your storage already resilient? If you are building a stateless application that uses cloud-backed storage as a service, you may not need to test your application&#13;
for disk failures, but you will likely want to introduce chaos in&#13;
the communication with the cloud storage solution.</p>&#13;
&#13;
<p>Before beginning&#13;
chaos testing think about the risks in your application, and identify&#13;
places where you want to introduce error and at what frequency.&#13;
When thinking about frequency, remember that we’re not trying to&#13;
test for the average case. The average case is already well&#13;
represented in your existing integration tests. Instead we are looking&#13;
to simulate the kind of environment that may occur only once a year&#13;
or once in a decade. You need to understand your application well&#13;
enough to describe what is plausible.</p>&#13;
&#13;
<p>In terms of understanding your application, the other important prerequisite for chaos testing is high-quality monitoring for the correctness and behavior of your application. It is one thing to introduce chaos into your environment, but to make this chaos useful you also need to be able to observe the operation of your application with sufficient detail to determine the impact of the chaos and to identify the areas where your application needs hardening to be able to deal with the chaos. In general, this monitoring is necessary for any production application. In addition to its&#13;
core contributions around resiliency, chaos testing can also be a good test to see if your monitoring and logging are sufficient to handle&#13;
a real<a data-primary="testing" data-secondary="chaos testing" data-startref="test-chaos-prereq" data-tertiary="prerequisites" data-type="indexterm" id="id1128"/><a data-primary="chaos testing" data-secondary="prerequisites" data-startref="chaos-prereq" data-type="indexterm" id="id1129"/><a data-primary="prerequisites" data-secondary="chaos testing" data-startref="prereq-chaos" data-type="indexterm" id="id1130"/> outage.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Chaos Testing Your Application’s Communication" data-type="sect2"><div class="sect2" id="id168">&#13;
<h2>Chaos Testing Your Application’s Communication</h2>&#13;
&#13;
<p>One of the <a data-primary="testing" data-secondary="chaos testing" data-tertiary="communication" data-type="indexterm" id="id1131"/><a data-primary="chaos testing" data-secondary="communication" data-type="indexterm" id="id1132"/><a data-primary="communication, chaos testing of" data-type="indexterm" id="id1133"/>easiest ways to inject chaos into your application’s&#13;
communication is to place a proxy between each client and your&#13;
service. This proxy handles all the network traffic between&#13;
your client and the server and injects random faults like&#13;
extra latency, disconnects, or other errors. There are several&#13;
different open source options for such a proxy, but one of <a data-primary="ToxiProxy" data-type="indexterm" id="id1134"/>the most&#13;
popular is <a href="https://oreil.ly/N8QNF">ToxiProxy</a>, which was&#13;
created by Shopify. The easiest&#13;
way to add ToxiProxy to your system is to effectively run a&#13;
ToxiProxy layer in front of each actual service in your cluster.</p>&#13;
&#13;
<p>To achieve this, you first need to rename each service to which&#13;
you want to add chaos. To see this in more detail, suppose you&#13;
have a service named <code>backend</code> that serves traffic on port 8080.&#13;
You can update a Kubernetes Service named <code>backend</code> to be called&#13;
<code>backend-real</code>. Then you can create new Deployment of&#13;
ToxiProxy Pods that are configured using the ToxiProxy command-line tool as follows:</p>&#13;
&#13;
<pre data-type="programlisting">toxiproxy-cli create -l 0.0.0.0:8080 -u backend-real:8080 backend</pre>&#13;
&#13;
<p>When you build the Pod definition for this Deployment of ToxiProxy,&#13;
you can run this command as a PostStart life-cycle hook. This&#13;
command configures ToxiProxy to listen on port 8080 within the&#13;
pod and then forward traffic to your actual backend service,&#13;
which has the DNS name <code>backend-real</code>.</p>&#13;
&#13;
<p>Next you create a new service named <code>backend</code> to replace the one&#13;
that you renamed, and you point this service at the Deployment&#13;
of ToxiProxy Pods that you just created. In this way, any client&#13;
in your application that communicates with <code>backend</code> will&#13;
automatically start communicating with the chaos proxy instead.</p>&#13;
&#13;
<p>Finally, you can start adding chaos to your application using the&#13;
ToxiProxy command-line tool by issuing commands like:</p>&#13;
&#13;
<pre data-type="programlisting">kubectl exec $SomeToxiProxyPod -- toxiproxy-cli toxic add -t latency&#13;
  -a latency=2000 backend</pre>&#13;
&#13;
<p>This will add 2,000 milliseconds of latency to all traffic through this&#13;
proxy. If you create multiple pods in your proxy Deployment, you will&#13;
need to run this command for each pod, or automate it using scripts&#13;
or code.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Chaos Testing Your Application’s Operation" data-type="sect2"><div class="sect2" id="id263">&#13;
<h2>Chaos Testing Your Application’s Operation</h2>&#13;
&#13;
<p>In addition<a data-primary="testing" data-secondary="chaos testing" data-tertiary="application operation" data-type="indexterm" id="test-chaos-app-operate"/><a data-primary="chaos testing" data-secondary="application operation" data-type="indexterm" id="chaos-app-operate"/> to testing the operation of your application when communication&#13;
is flaky, it is also a good idea to test your application in situations&#13;
where the infrastructure it is running on is flaky or overloaded.</p>&#13;
&#13;
<p>The easiest way to start with infrastructure failures is to simply delete&#13;
pods. Starting with a single Deployment, you can delete random pods within&#13;
the Deployment based on its label selector using a simple bash script:</p>&#13;
&#13;
<pre data-type="programlisting">NAMESPACE="some-namespace"&#13;
LABEL=k8s-app=my-app&#13;
PODS=$(kubectl get pods --selector=${LABEL} -n ${NAMESPACE} --no-headers | awk&#13;
    '{print $1}')&#13;
for x in $PODS; do&#13;
    if [ $[ $RANDOM % 10 ] == 0 ]; then&#13;
        kubectl delete pods -n $NAMESPACE $x;&#13;
    fi;&#13;
done</pre>&#13;
&#13;
<p>Of course if you’d rather have something more complete you can write code using&#13;
the various <a href="https://oreil.ly/Ib1kp">Kubernetes clients</a> out there or even an existing open source tool like <a href="https://chaos-mesh.org">Chaos Mesh</a>.</p>&#13;
&#13;
<p>Once you have moved through all the microservice Deployments in your&#13;
application, you can move on to deleting pods within the different&#13;
services at once. This simulates a more broad outage. You can&#13;
extend the previous script to randomly delete pods within a particular&#13;
namespace as follows:</p>&#13;
&#13;
<pre data-type="programlisting">NAMESPACE="some-namespace"&#13;
PODS=$(kubectl get pods -n ${NAMESPACE} --no-headers | awk '{print $1}')&#13;
for x in $PODS; do&#13;
    if [ $[ $RANDOM % 10 ] == 0 ]; then&#13;
        kubectl delete pods -n $NAMESPACE $x;&#13;
    fi;&#13;
done</pre>&#13;
&#13;
<p>Finally, you can simulate complete failures in your infrastructure by&#13;
causing entire nodes in your cluster to fail. There are a variety of ways&#13;
to accomplish this. If you are running in a cloud-based Kubernetes, you&#13;
can use cloud VM APIs to shut down or reboot a machine in your cluster.&#13;
If you are running on physical infrastructure, you can literally pull&#13;
the power plug on a particular machine, or reboot it by logging in and&#13;
running commands. On both physical and virtual hardware you can also&#13;
cause your kernel to panic by running <code>sudo sh -c 'echo c &gt; /proc/sysrq-trigger'</code>.</p>&#13;
&#13;
<p>Here is a simple script that will randomly panic approximately 10% of the machines in a Kubernetes cluster:</p>&#13;
&#13;
<pre data-type="programlisting">NODES=$(kubectl get nodes -o jsonpath='{.items[*].status.addresses[0].address}')&#13;
for x in $NODES; do&#13;
  if [ $[ $RANDOM % 10 ] == 0 ]; then&#13;
    ssh $x sudo sh -c 'echo c &gt; /proc/sysrq-trigger'&#13;
  fi&#13;
done</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Fuzz Testing Your Application for Security and Resiliency" data-type="sect2"><div class="sect2" id="id169">&#13;
<h2>Fuzz Testing Your Application for Security and Resiliency</h2>&#13;
&#13;
<p>One final type <a data-primary="testing" data-secondary="fuzz testing" data-type="indexterm" id="id1135"/><a data-primary="fuzz testing" data-type="indexterm" id="id1136"/>of testing in the same spirit as chaos testing is fuzz testing. Fuzz testing is like chaos testing in that it introduces randomness and chaos into your application, but instead of introducing failures, fuzz testing focuses on introducing inputs that are technically legal but extreme in one way or another. For example, you might send an endpoint a legal JSON request but include duplicate fields or data that is especially long or contains random values. The goal of fuzz testing is to test the resiliency of your application to random extreme or malicious inputs. Fuzz testing is most often used in the context of security testing because random inputs can cause unexpected code paths to be executed and to introduce&#13;
vulnerabilities or crashes. Fuzz testing can help you ensure that your application is resilient to chaos from malicious or erroneous input in addition&#13;
to failures in the environment. Fuzz testing can be added at both the cluster service level as well as the unit test level.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect2"><div class="sect2" id="id390">&#13;
<h2>Summary</h2>&#13;
&#13;
<p>Chaos testing is the art of introducing unexpected but not impossible&#13;
conditions into the runtime of your application and observing what happens.&#13;
Introducing potential errors and failures into an environment before&#13;
any failures can impact actual usage of your application helps you identify&#13;
problem areas before they become critical.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Load Testing" data-type="sect1"><div class="sect1" id="id264">&#13;
<h1>Load Testing</h1>&#13;
&#13;
<p>Load testing is<a data-primary="testing" data-secondary="load testing" data-tertiary="purpose of" data-type="indexterm" id="id1137"/><a data-primary="load testing" data-secondary="purpose of" data-type="indexterm" id="id1138"/> used to determine how your application behaves under load.&#13;
A load-testing tool is used to generate realistic application traffic&#13;
that is equivalent to real production usage of your application. This&#13;
traffic can either be artificially generated or recorded traffic from&#13;
actual production traffic that is replayed. Load testing can be used to&#13;
either identify areas that may become problems in the future or to ensure&#13;
that new code and features do not cause regressions.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Goals for Load Testing" data-type="sect2"><div class="sect2" id="id170">&#13;
<h2>Goals for Load Testing</h2>&#13;
&#13;
<p>The core goal <a data-primary="testing" data-secondary="load testing" data-tertiary="goals of" data-type="indexterm" id="test-load-goals"/><a data-primary="load testing" data-secondary="goals of" data-type="indexterm" id="load-goals"/>for load testing is to understand how your application behaves&#13;
under load. When you are building an application, it is generally exposed to&#13;
occasional traffic from only a few users. This traffic is sufficient for understanding&#13;
the correctness of the application, but it doesn’t help us understand how the&#13;
application behaves under realistic load. Thus, to understand how your application&#13;
works when deployed in production, load testing is necessary.</p>&#13;
&#13;
<p>Two fundamental uses of load testing are estimating current capacity&#13;
and regression prevention. Regression prevention is<a data-primary="regression prevention" data-type="indexterm" id="id1139"/> the use of load testing&#13;
to ensure that&#13;
a new release of software can sustain the same load as the previous version&#13;
of the software. Whenever we roll out a new version of our software there&#13;
is new code and configuration in the release (if there wasn’t, then what is&#13;
the point of the release?). While these code changes introduce&#13;
new features and fix bugs, they can also introduce performance regressions: the new version cannot serve the same&#13;
level of load as the previous version. Of course sometimes these performance&#13;
regressions are known and expected; for example, a new feature may have made a computation&#13;
more complex and thus slower, but even in such cases, load testing is necessary&#13;
to determine how the infrastructure (e.g., the number of pods, the resources&#13;
they require) needs to be scaled up to sustain production traffic.</p>&#13;
&#13;
<p>In contrast to regression prevention, which is used to catch problems newly&#13;
introduced into your application, predictive load testing<a data-primary="predictive load testing" data-type="indexterm" id="id1140"/> is used to anticipate problems before they occur. For many services, there&#13;
is a steady growth in the use of the service. Each month there are more&#13;
users and more requests to your service. In general this is a good thing,&#13;
but keeping those users happy means continuing to improve your&#13;
infrastructure to keep up with the new load. Predictive load testing takes&#13;
the historical growth trends from your application and uses them to test&#13;
your application as if it were operating in the future. For example, if&#13;
your application’s traffic is growing 10% each month, you might run a&#13;
predictive load test at 110% of the current peak traffic to simulate how&#13;
your application will work in the next month. While scaling up&#13;
your application can be as easy as adding more replicas and more resources,&#13;
often fundamental bottlenecks in your application require&#13;
rearchitecting. Predictive load testing allows you to anticipate the future&#13;
and perform these changes without the emergency of a user-facing outage&#13;
due to increased load.</p>&#13;
&#13;
<p>Predictive load testing can also be used to&#13;
anticipate how an application will behave prior to launch. Rather than&#13;
using historical information, you can use your predictions about&#13;
usage at launch to ensure that such a launch is successful and not a<a data-primary="testing" data-secondary="load testing" data-startref="test-load-goals" data-tertiary="goals of" data-type="indexterm" id="id1141"/><a data-primary="load testing" data-secondary="goals of" data-startref="load-goals" data-type="indexterm" id="id1142"/>&#13;
disaster.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prerequisites for Load Testing" data-type="sect2"><div class="sect2" id="id265">&#13;
<h2>Prerequisites for Load Testing</h2>&#13;
&#13;
<p>Load testing<a data-primary="testing" data-secondary="load testing" data-tertiary="prerequisites" data-type="indexterm" id="test-load-prereq"/><a data-primary="load testing" data-secondary="prerequisites" data-type="indexterm" id="load-prereq"/><a data-primary="prerequisites" data-secondary="load testing" data-type="indexterm" id="prereq-load"/> is used to ensure that your application can perform while&#13;
operating under significant load. Further, like chaos testing, load testing&#13;
can also introduce failure conditions in your application due to that load.&#13;
Consequently, load testing shares the same prerequisites as chaos testing&#13;
around application observability. To successfully use a load test,&#13;
you need to be able to verify that your application is operating&#13;
correctly and have enough information to gain insight into where and&#13;
why failures occur if they do.</p>&#13;
&#13;
<p>In addition to the core observability of your application, another&#13;
critical prerequisite for load testing is the ability to generate realistic&#13;
load for your test. If your load test doesn’t closely mimic real-world&#13;
user behaviors, then it is of little use. As a concrete example, imagine&#13;
if your load test continuously makes repeated requests for a single user. In many&#13;
applications, such traffic will produce an unrealistic cache hit rate, and&#13;
your load test will seem to show an ability to handle large amounts of load&#13;
that is not possible under more realistic <a data-primary="testing" data-secondary="load testing" data-startref="test-load-prereq" data-tertiary="prerequisites" data-type="indexterm" id="id1143"/><a data-primary="load testing" data-secondary="prerequisites" data-startref="load-prereq" data-type="indexterm" id="id1144"/><a data-primary="prerequisites" data-secondary="load testing" data-startref="prereq-load" data-type="indexterm" id="id1145"/>traffic.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Generating Realistic Traffic" data-type="sect2"><div class="sect2" id="id171">&#13;
<h2>Generating Realistic Traffic</h2>&#13;
&#13;
<p>Methods for<a data-primary="testing" data-secondary="load testing" data-tertiary="traffic generation" data-type="indexterm" id="id1146"/><a data-primary="load testing" data-secondary="traffic generation" data-type="indexterm" id="id1147"/><a data-primary="traffic generation for load testing" data-type="indexterm" id="id1148"/> generating real-world traffic patterns for your application vary&#13;
depending on your application. For certain types of more read-only sites, for example, a news site, it may be sufficient to repeatedly access&#13;
each of the different pages using some sort of probability distribution.&#13;
But for many applications, especially those that involve both reading and&#13;
writing operations, the only way to generate a realistic load test is to&#13;
record real-world traffic and play it back. One of the easiest ways to&#13;
do this is to write the complete details of each HTTP&#13;
request to a file, and then resend those requests back to the server at&#13;
a later time.</p>&#13;
&#13;
<p>Unfortunately, such an approach can have complications. The first and&#13;
foremost consequence of recording all the requests to your application is&#13;
user privacy and security. In many cases requests to an application contain&#13;
both private information as well as security tokens. If you record all&#13;
this information to a file for playback, you must be very, very careful&#13;
in handling these files to ensure that user privacy and security&#13;
are respected.</p>&#13;
&#13;
<p>Another challenge with recording and playing back actual user requests has&#13;
to do with the timeliness of the requests themselves. If there is a time&#13;
component to the requests, for example, search queries about the latest news&#13;
events, these requests will have a very different behavior several weeks&#13;
(or months) after those events have occurred. There will be many fewer&#13;
messages related to old news. Timeliness also affects the correct behavior&#13;
of your application. Requests often contain security tokens and if&#13;
you are doing security properly, those tokens are short lived. This means&#13;
that recorded tokens will likely not work correctly when verified.</p>&#13;
&#13;
<p>Finally, when requests write data to backend storage systems, replaying requests&#13;
that modify storage must be performed in a copy or snapshot of the production&#13;
storage infrastructure. If you are not careful about how you set this up, you can&#13;
cause significant problems with customer data.</p>&#13;
&#13;
<p>For all these reasons, simply recording and playing back requests, though&#13;
easy, is not a best practice. Instead the more useful way to use requests&#13;
is to build up a model of the ways in which your service is used. How many&#13;
read requests? For what resources? How many writes? Using this model you&#13;
can generate synthetic load that has realistic characteristics.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Load Testing Your Application" data-type="sect2"><div class="sect2" id="id172">&#13;
<h2>Load Testing Your Application</h2>&#13;
&#13;
<p>Once you <a data-primary="testing" data-secondary="load testing" data-type="indexterm" id="id1149"/><a data-primary="load testing" data-type="indexterm" id="id1150"/>have generated the requests to power your load test, it is simply a&#13;
matter of applying that load to your service. Unfortunately, it is rarely that simple. In most real-world applications there are databases&#13;
and other storage systems involved. To correctly simulate your application&#13;
under load, you also need to write into storage systems, but not to&#13;
the production data store since this is artificial load. Thus, to correctly&#13;
load test your application, you need to be able to turn up a true copy&#13;
of your application with all its dependencies.</p>&#13;
&#13;
<p>Once your application clone is up and running, it is a matter of sending&#13;
all the requests. It turns out large-scale load testing is also a&#13;
distributed systems problem. You will want to use a large number of&#13;
different pods to send load onto your application. This is to ensure&#13;
an even distribution of requests through the load balancers and to&#13;
make it feasible to send more load than a single pod’s network can support.&#13;
One of the choices you will need to make is whether to run these load&#13;
testing pods within the same cluster as your application or in a separate&#13;
cluster. Running the pods within the same cluster maximizes the load that&#13;
you can send to your application, but it does exercise the edge load&#13;
balancers that bring traffic from the internet onto your application.&#13;
Depending on which parts of your application you wish to test, you may&#13;
want to run the load within the cluster, outside of the cluster, or both.</p>&#13;
&#13;
<p>Two popular tools for running distributed load tests in Kubernetes are <a href="https://oreil.ly/MXBgj">JMeter</a> and&#13;
<a href="https://locust.io">Locust</a>. Both provide ways to describe&#13;
the load that you want to send to your service and allow you to deploy&#13;
distributed load test bots to Kubernetes.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tuning Your Application Using Load Tests" data-type="sect2"><div class="sect2" id="id173">&#13;
<h2>Tuning Your Application Using Load Tests</h2>&#13;
&#13;
<p>In addition<a data-primary="testing" data-secondary="load testing" data-tertiary="application tuning with" data-type="indexterm" id="test-load-tune"/><a data-primary="load testing" data-secondary="application tuning with" data-type="indexterm" id="load-tune"/><a data-primary="tuning with load testing" data-type="indexterm" id="tune-load-test"/> to using load tests to prevent performance regressions and&#13;
to anticipate future performance problems, load testing can also be used&#13;
to optimize the resource utilization of your application. For any given&#13;
service multiple variables can be tuned and can&#13;
impact system performance. For the purposes of this discussion we consider&#13;
three: number of pods, number of cores, and memory.</p>&#13;
&#13;
<p>At first it might&#13;
seem that an application would perform the same given the same number of&#13;
replicas times cores. That is, an application with five pods, each with three cores,&#13;
would perform the same as an application with three pods, each with five cores.&#13;
In some cases this is true, but in many cases it is not; the specific&#13;
details of the service and location of its bottlenecks often cause&#13;
differences in behavior that are hard to anticipate. For example, an application&#13;
built in a language like Java, dotnet, or Go that provides garbage collection: with one or two cores, the application is going to tune the garbage&#13;
collector significantly differently than if it has many cores.</p>&#13;
&#13;
<p>The same&#13;
thing is true of memory. More memory means that more things can be kept&#13;
in cache, and this often leads to more performance, but this benefit has&#13;
an asymptotic limit. You cannot simply throw more memory at a service and&#13;
expect it to continue to improve in performance.</p>&#13;
&#13;
<p>Often times the only way to understand how your application will behave&#13;
under different configurations is to actually do the experimentation.&#13;
To do this properly you can set up an experimental set of configurations&#13;
with different values for pods, cores, and memory and run each configuration&#13;
through a load test. Using the data from these experiments you often can&#13;
identify patterns of behavior that can drive insight into the particular&#13;
details of your system’s performance, and you can use the results&#13;
to select the most efficient configuration for your<a data-primary="testing" data-secondary="load testing" data-startref="test-load-tune" data-tertiary="application tuning with" data-type="indexterm" id="id1151"/><a data-primary="load testing" data-secondary="application tuning with" data-startref="load-tune" data-type="indexterm" id="id1152"/><a data-primary="tuning with load testing" data-startref="tune-load-test" data-type="indexterm" id="id1153"/> service.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect2"><div class="sect2" id="id391">&#13;
<h2>Summary</h2>&#13;
&#13;
<p>Performance is a critical part of building an application that delights&#13;
users. Load testing ensures that you do not introduce regressions that&#13;
impact performance and lead to poor user experiences. Load testing can also&#13;
serve as a time machine, enabling you to imagine your application’s behavior&#13;
in the future and make changes to your architecture to support additional&#13;
growth. Load testing can also help you understand and optimize your resource&#13;
usage, lowering costs and improving efficiency.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Experiments" data-type="sect1"><div class="sect1" id="id266">&#13;
<h1>Experiments</h1>&#13;
&#13;
<p>In contrast <a data-primary="testing" data-secondary="experiments" data-tertiary="purpose of" data-type="indexterm" id="id1154"/><a data-primary="experiments" data-secondary="purpose of" data-type="indexterm" id="id1155"/>to chaos testing and load testing, experiments are used not to&#13;
discover problems in your service’s architecture and operation but to identify ways to improve how your users use your service.&#13;
An experiment is a long-running change to your service, generally in the&#13;
user experience, in which a small percentage of users (for example, 1% of&#13;
all traffic) receive a slightly different experience. From examining the&#13;
difference between the control (the group with no changes) and the experiment&#13;
(the group that had a different experience) you can understand the impact&#13;
of the changes and decide whether to continue to experiment or to roll out&#13;
the changes more broadly.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Goals for Experiments" data-type="sect2"><div class="sect2" id="id267">&#13;
<h2>Goals for Experiments</h2>&#13;
&#13;
<p>When we build<a data-primary="testing" data-secondary="experiments" data-tertiary="goals of" data-type="indexterm" id="test-experiment-goals"/><a data-primary="experiments" data-secondary="goals of" data-type="indexterm" id="experiment-goals"/> a service, we build it with a goal in mind. That goal more often&#13;
than not is to provide something that is useful, easy to use, and pleasing&#13;
to our customers or users. But how can we know if we have achieved that goal?&#13;
It’s relatively easy to see that our site breaks in the presence of chaos&#13;
or that it can only handle a small amount of load before failing, but&#13;
understanding how a user experiences our services can be tricky to determine.</p>&#13;
&#13;
<p>Several traditional methods for understanding user experience include surveys, in which you ask users how they feel about the current service. While this can be useful&#13;
in understanding the current performance of our service, it is much harder&#13;
to use surveys to predict the impact of future changes. Much like performance&#13;
regressions, it is far better to know the impact <em>before</em> the change is&#13;
rolled out everywhere. That is the main goal of any experiment: to learn&#13;
with minimal impact on our users’ <a data-primary="testing" data-secondary="experiments" data-startref="test-experiment-goals" data-tertiary="goals of" data-type="indexterm" id="id1156"/><a data-primary="experiments" data-secondary="goals of" data-startref="experiment-goals" data-type="indexterm" id="id1157"/>experience.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prerequisites for an Experiment" data-type="sect2"><div class="sect2" id="id268">&#13;
<h2>Prerequisites for an Experiment</h2>&#13;
&#13;
<p>Just like<a data-primary="testing" data-secondary="experiments" data-tertiary="prerequisites" data-type="indexterm" id="id1158"/><a data-primary="experiments" data-secondary="prerequisites" data-type="indexterm" id="id1159"/><a data-primary="prerequisites" data-secondary="experiments" data-type="indexterm" id="id1160"/> when we were kids in a science fair, every good experiment starts&#13;
with a good hypothesis, and that is a natural prerequisite for our service&#13;
experiments also. There is some change that we are thinking about making,&#13;
and we need to have a guess as to what impact it will have on user experience.</p>&#13;
&#13;
<p>Of course to understand the impact on user experience, we also need to be able&#13;
to measure the user experience. This data can come in the form of the surveys&#13;
mentioned previously, through which you can gather metrics like satisfaction&#13;
(“please rate us one through five”) or net promoter score (“how likely are you&#13;
to recommend this to a friend?”). Or it can come from passive metrics associated&#13;
with user behavior (“how long did they spend on our site?” or “how many pages did they click on?” etc.).</p>&#13;
&#13;
<p>Once you have a hypothesis and a way to measure user experience, you’re ready&#13;
to begin the experiment.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Setting Up an Experiment" data-type="sect2"><div class="sect2" id="id174">&#13;
<h2>Setting Up an Experiment</h2>&#13;
&#13;
<p>There are two<a data-primary="testing" data-secondary="experiments" data-tertiary="setting up" data-type="indexterm" id="test-experiment-setup"/><a data-primary="experiments" data-secondary="setting up" data-type="indexterm" id="experiment-setup"/> different ways to set up an experiment. The approach you take&#13;
depends on the specific things being tested. You can include multiple&#13;
possible experiences in a single service, or you can deploy two copies of&#13;
your service and use a service mesh to direct traffic between them.</p>&#13;
&#13;
<p>The first approach is to check&#13;
both versions of the code into your release binary and switch between the&#13;
experiment and control using some property of the requests that your service&#13;
is receiving. You can use HTTP headers, cookies, or query parameters to enable&#13;
users to explicitly opt in to the experiment. Alternatively you can use&#13;
characteristics of the requests, such as the source IP, to randomly select users for your experiments. For example, you could choose&#13;
people for the experiments whose IP addresses ended in one.</p>&#13;
&#13;
<p>A common way&#13;
to implement experiments is to use explicit <a data-primary="feature flags" data-type="indexterm" id="id1161"/>feature flagging where a user&#13;
decides to opt in to an experiment by supplying the query parameter or cookie&#13;
that turns on the experiment. This is a good way to allow specific customers&#13;
to try new functionality or to demonstrate a new feature without releasing&#13;
it broadly. Feature flags can also be used to rapidly turn features on or&#13;
off in the case of instability. Numerous open source projects, for&#13;
example <a href="https://flagger.app">Flagger</a>, can be used to implement feature&#13;
flagging.</p>&#13;
&#13;
<p>The benefit of placing the experiment in the same binary as your control&#13;
code is that it is simplest to roll it out into production, but this simplicity&#13;
also leads to two drawbacks. The first is that if the experimental code&#13;
is unstable and crashes, it can also impact your production traffic. The other&#13;
is that because any changes are tied to a complete release of your service&#13;
it is much slower to make changes to update the experiment or to roll out&#13;
new experiments.</p>&#13;
&#13;
<p>The second approach to experiments is to deploy two (or more) different versions&#13;
of your service. In this approach, you have the control production service&#13;
that receives the bulk of the traffic and a separate experimental deployment&#13;
of your service that receives only a fraction of the traffic. You can use&#13;
a service mesh (described in <a data-type="xref" href="ch09.html#networking_network_security_and_service_mesh">Chapter 9</a>) to route a small percentage of traffic&#13;
to this <a data-primary="service meshes" data-type="indexterm" id="id1162"/>experimental deployment instead of the production deployment. Though&#13;
this approach is more complex to implement, it is significantly more agile&#13;
and robust than including experimental code in your production binary.&#13;
Because it requires a completely new deployment of code the upfront cost&#13;
of setting up an experiment is increased, but because it has no impact on&#13;
anything except the experimental traffic, you can easily deploy new versions&#13;
of the experiment (or even multiple versions of the experiment) at&#13;
any time without impacting the bulk of your traffic.</p>&#13;
&#13;
<p>Additionally, because&#13;
the service mesh can measure whether requests are successful, if the&#13;
experimental code starts failing it can quickly be removed from use and user&#13;
impact is minimized. Of course detecting these failures can be a challenge.&#13;
You need to make sure that the experimental infrastructure is monitored&#13;
independently from the standard production monitoring; otherwise the&#13;
experimental failures may be lost in successful requests that are processed&#13;
by the current production infrastructure. Ideally the name of the pod or&#13;
the deployment provides sufficient context to determine if the monitoring&#13;
signals are from production or an experiment.</p>&#13;
&#13;
<p>In general, using separate deployments and some sort of traffic router like&#13;
a service mesh is the best practice for experiments, but it is a lot of&#13;
infrastructure to set up. For your initial experiments, or if you are a small&#13;
team that is already fairly agile, it may be that checking in experimental&#13;
code is the easiest path to experimentation and<a data-primary="testing" data-secondary="experiments" data-startref="test-experiment-setup" data-tertiary="setting up" data-type="indexterm" id="id1163"/><a data-primary="experiments" data-secondary="setting up" data-startref="experiment-setup" data-type="indexterm" id="id1164"/> iteration.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect2"><div class="sect2" id="id392">&#13;
<h2>Summary</h2>&#13;
&#13;
<p>Experiments enable you to understand the impact of changes on your users’&#13;
experience before those changes are rolled out to the broad user base.&#13;
Experiments play a critical role in helping us quickly understand what changes&#13;
are possible and how we can update our services to better serve our users.&#13;
Experiments make the improvement of our services easier, quicker, and safer.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Chaos Testing, Load Testing, and Experiments Summary" data-type="sect1"><div class="sect1" id="id393">&#13;
<h1>Chaos Testing, Load Testing, and Experiments Summary</h1>&#13;
&#13;
<p>In this chapter we’ve covered a variety of different ways to learn more about&#13;
your service to make it more resilient, more performant, and more&#13;
useful. Just as testing your code with unit tests is a&#13;
critical part of the software development process, testing your service with&#13;
chaos, load, and experiments is a critical part of service design and operation.</p>&#13;
</div></section>&#13;
</div></section></body></html>