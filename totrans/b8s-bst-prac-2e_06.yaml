- en: Chapter 6\. Versioning, Releases, and Rollouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main complaints of traditional monolithic applications is that over
    time they begin to grow too large and unwieldy to properly upgrade, version, or
    modify at the speed the business requires. Many can argue that this is one of
    the critical factors that led to more Agile development practices and the advent
    of microservice architectures. Being able to quickly iterate on new code, solve
    new problems, or fix hidden problems before they become major issues, as well
    as the promise of zero-downtime upgrades, are all goals that development teams
    strive for. Practically, these issues can be solved with proper processes and
    procedures in place, no matter the type of system, but this usually comes at a
    much higher cost of both technology and human capital to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: When designing systems, isolation and composability are important variables.
    The adoption of containers as the runtime for application code allows for this
    but still requires a high level of human automation or system management to maintain
    at a dependable level for large systems. Over time, the system grew, more brittleness
    was introduced, and systems engineers began to build complex automation processes
    to deliver on complex release, upgrade, and failure detection mechanisms. Service
    orchestrators such as Apache Mesos, HashiCorp Nomad, and even specialized container-based
    orchestrators such as Kubernetes and Docker Swarm have evolved these processes
    into more primitive components directly into their runtimes. Now, systems engineers
    can solve more complex system problems as the table stakes have been elevated
    to include the versioning, release, and deployment of applications into the system.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is not meant to be a primer on software versioning and the history
    behind it; there are countless articles and computer science course books on the
    subject. The main thing is to pick a pattern and stick with it. The majority of
    software companies and developers have agreed that some form of *semantic versioning*
    is the most useful, especially in a microservice architecture in which a team
    that writes a certain microservice will depend on the API compatibility of other
    microservices that make up the system.
  prefs: []
  type: TYPE_NORMAL
- en: For those new to semantic versioning, the basics are that it follows a three-part
    version number in a pattern of *major version*, *minor version*, and *patch*,
    usually expressed in a *dot notation* such as 1(major).2(minor).3(patch). The
    patch signifies an incremental release that includes a bug fix or very minor change
    that has no API changes. The minor version signifies updates that might have new
    API changes but it is backward compatible with the previous version. This is a
    key attribute for developers working with other microservices they might not be
    involved in developing. Knowing that I have my service written to communicate
    with version 1.4.7 of another microservice that has been recently upgraded to
    1.5.7 should signify that I might not need to change my code unless I want to
    take advantage of any new API features. The major version is a breaking change
    increment to the code. In most cases, the API is no longer compatible between
    major versions of the same code. There are many slight modifications to this process,
    including a “4” version to indicate the stage of the software in its development
    life cycle, such as 1.4.7.0 for alpha code and 1.4.7.3 for release. The most important
    thing is that there is consistency across the system.
  prefs: []
  type: TYPE_NORMAL
- en: Releases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In truth, Kubernetes does not really have a release controller, so there is
    no native concept of a release. This is usually added to a Deployment `metadata.labels`
    specification and/or in the `pod.spec.template.metadata.label` specification.
    When to include either is very important, and based on how CD is used to update
    changes to deployments, it can have varied effects. When Helm for Kubernetes was
    introduced, one of its main concepts was the notion of a release to differentiate
    the running instance of the same Helm chart in a cluster. This concept is easily
    reproducible without Helm; however, Helm natively keeps track of releases and
    their history, so many CD tools integrate Helm into their pipelines to be the
    actual release service. Again, the key here is consistency in how versioning is
    used and where it is surfaced in the system state of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Release names can be quite useful if there is institutional agreement as to
    the definition of certain names. Often, labels such as `stable` or `canary` are
    used, which helps to give some operational control when tools such as service
    meshes are added to make fine-grained routing decisions. Large organizations that
    drive numerous changes for different audiences will also adopt a ring architecture
    that can be denoted as `ring-0`, `ring-1`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This topic requires a little side trip into the specifics of labels in the Kubernetes
    declarative model. Labels themselves are very much free form and can be any key/value
    pair that follows the syntactical rules of the API. The key is not really the
    content but how each controller handles labels, changes to labels, and selector
    matching of labels. Jobs, Deployments, ReplicaSets, and DaemonSets support selector-based
    matching of pods via labels through direct mapping or set-based expressions. It
    is important to understand that label selectors are immutable after they are created,
    which means if you add a new selector and the pod’s labels have a corresponding
    match, a new ReplicaSet is made, not an upgrade to an existing ReplicaSet. This
    becomes very important to understand when dealing with rollouts, discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Rollouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to the Deployment controller being introduced in Kubernetes, the only
    mechanism that existed to control how applications were rolled out by the Kubernetes
    controller process was using the command-line interface (CLI) command `kubectl
    rolling-update` on the specific `replicaController` that was to be updated. This
    was very difficult for declarative CD models because this was not part of the
    state of the original manifest. One had to carefully ensure that manifests were
    updated correctly, versioned properly so as to not accidentally roll the system
    back, and archived when no longer needed. The Deployment controller added the
    ability to automate this update process using a specific strategy and then allowing
    the system to read the declarative new state based on changes to the `spec.template`
    of the Deployment. This last fact is often misunderstood by new users of Kubernetes
    and causes frustration when they change a label in the Deployment metadata fields,
    reapply a manifest, and no update has been triggered. The Deployment controller
    is able to determine changes to the specification and will take action to update
    the Deployment based on a strategy that is defined by the specification. Kubernetes
    Deployments support two strategies, `rollingUpdate` and `recreate`, the former
    being the default.
  prefs: []
  type: TYPE_NORMAL
- en: If a rolling update is specified, the Deployment will create a new ReplicaSet
    to scale to the number of required replicas, and the old ReplicaSet will scale
    down to zero based on specific values for `maxUnavailble` and `maxSurge`. In essence,
    those two values will prevent Kubernetes from removing older pods until a sufficient
    number of newer pods have come online, and Kubernetes will not create new pods
    until a certain number of old pods have been removed. The nice thing is that the
    Deployment controller will keep a history of the updates, and through the CLI,
    you can roll back Deployments to previous versions.
  prefs: []
  type: TYPE_NORMAL
- en: The `recreate` strategy is a valid strategy for certain workloads that can handle
    a complete outage of the pods in a ReplicaSet with little to no degradation of
    service. In this strategy the Deployment controller will create a new ReplicaSet
    with the new configuration and will delete the prior ReplicaSet before bringing
    the new pods online. Services that sit behind queue-based systems are an example
    of a service that could handle this type of disruption, because messages will
    queue while waiting for the new pods to come online, and message processing will
    resume as soon as the new pods come online.
  prefs: []
  type: TYPE_NORMAL
- en: Putting It All Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Within a single service Deployment, a few key areas are affected by versioning,
    release, and rollout management. Let’s examine an example Deployment and then
    break down the specific areas of interest as they relate to best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Upon first inspection, things might look a little off. How can a Deployment
    have a version tag and the container image the Deployment uses have a different
    version tag? What will happen if one changes and the other does not? What does
    release mean in this example, and what will be the effect on the system if it
    changes? If a certain label is changed, when will it trigger an update to my Deployment?
    We can find the answers to these questions by looking at some of the best practices
    for versioning, releases, and rollouts.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Versioning, Releases, and Rollouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Effective CI/CD and the ability to offer reduced- or zero-downtime deployments
    depend on using consistent practices for versioning and release management. The
    following best practices can help to define consistent parameters that can assist
    DevOps teams in delivering smooth software deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: Use semantic versioning for the application that differs from the version of
    the containers and the version of the pods Deployment that make up the entire
    application. This allows for independent life cycles of the containers that make
    up the application and the application as a whole. This can be quite confusing
    at first, but if a principled hierarchical approach is taken for when one changes
    the other, you can easily track it. In the previous example, the container itself
    is currently on `v1.5.5`; however, the pod specification is `1.5.8`, which could
    mean that changes were made to the pod specification, such as new ConfigMaps,
    additional secrets, or updated replica values, but the specific container used
    has not changed its version. The application itself, the entire guestbook application,
    and all its services, is at `1.6.9`, which could mean that operations made changes
    along the way that were beyond just this specific service, such as other services
    that make up the entire application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a release and release version/number label in your deployment metadata to
    track releases from CI/CD pipelines. The release name and release number should
    coordinate with the actual release in the CI/CD tool records. This allows for
    both traceability through the CI/CD process into the cluster and easier rollback
    identification. In the previous example, the release number comes directly from
    the release ID of the CD pipeline that created the manifest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Helm is being used to package services for deployment into Kubernetes, take
    special care to bundle together those services that need to be rolled back or
    upgraded together into the same Helm chart. Helm allows for easy rollback of all
    components of the application to bring the state back to what it was before the
    upgrade. Because Helm actually processes the templates and all the Helm directives
    before passing a flattened YAML configuration, the use of life-cycle hooks allows
    for proper ordering of the application of specific templates. Operators can use
    proper Helm life-cycle hooks to ensure that upgrades and rollback will happen
    correctly. The previous example for the `Job` specification uses Helm life-cycle
    hooks to ensure that the template runs a backup of the database before a rollback,
    upgrade, or delete of the Helm release. It also ensures that the `Job` is deleted
    after the job is run successfully, which, until the TTL Controller comes out of
    alpha in Kubernetes, would require manual cleanup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agree on a release nomenclature that makes sense for the operational tempo of
    the organization. Simple `stable`, `canary`, and `alpha` states are quite adequate
    for most situations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes has allowed for more complex Agile development processes to be adopted
    within companies large and small. The ability to automate many of the complex
    processes that would usually require large amounts of human and technical capital
    has now been democratized so that even startups can take advantage of this cloud
    pattern with relative ease. The true declarative nature of Kubernetes really shines
    when planning the proper use of labels and using native Kubernetes controller
    capabilities. By properly identifying operational and development states within
    the declarative properties of the applications deployed into Kubernetes, organizations
    can tie in tooling and automation to more easily manage the complex processes
    of upgrades, rollouts, and rollbacks of capabilities.
  prefs: []
  type: TYPE_NORMAL
