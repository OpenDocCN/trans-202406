<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Networking"><div class="chapter" id="ch-networking">
<h1><span class="label">Chapter 5. </span>Networking</h1>


<p>In this chapter we will focus on networking aspects of your workloads. We will
first review the defaults that Kubernetes proper comes equipped with and
what else is readily available due to integrations. We cover
networking topics including East-West and North-South traffic—that is,
intra-pod and inter-pod communication, communication with the worker node
(hosts), cluster-external communication, workload identity, and encryption on the wire.</p>

<p>In the second part of this chapter we have a look at two more recent additions
to the Kubernetes networking toolbox: service meshes and the Linux kernel extension
mechanism eBPF. We try to give you a rough idea if, how, and where you can,
going forward, benefit from both.</p>

<p>As you can see in <a data-type="xref" href="#fig-networking-layer-model">Figure 5-1</a>, there <a data-type="indexterm" data-primary="networks" data-secondary="layer model" id="idm45302814410176"/>are many moving parts
in the networking space.</p>

<figure><div id="fig-networking-layer-model" class="figure">
<img src="Images/haku_0501.png" alt="Network layer model" width="864" height="654"/>
<h6><span class="label">Figure 5-1. </span>Network layer model</h6>
</div></figure>

<p>The good news is that most if not all of the protocols should be familiar to you, since Kubernetes uses the standard Internet Engineering Task Force (IETF) suite of <a data-type="indexterm" data-primary="protocols" data-secondary="networking" id="idm45302814406432"/><a data-type="indexterm" data-primary="networking protocols" id="idm45302814405456"/>networking protocols, from the <a href="https://oreil.ly/xp1b4">Internet Protocol</a> to the Domain Name System (<a href="https://oreil.ly/nzOlg">DNS</a>). What changes, really, is the scope
and generally the assumptions about how the protocols are used. For example, when deployed on a worldwide scale, it makes sense to make the time-to-live (TTL) of a DNS record months or longer.</p>

<p>In the context of a container that may run for hours or days at best, this assumption doesn’t hold anymore. Clever adversaries can exploit such assumptions and as you should know by now, that’s exactly what the Captain would do.</p>

<p>In this chapter we will focus on the protocols most often used in Kubernetes—and their weak points with respect to workloads. As Captain Hashjack likes to say, “loose lips sink ships,” so we’ll first explore
for permissive networking defaults, then show how to attack them as well as discuss the controls you can implement to detect and mitigate these attacks.</p>






<section data-type="sect1" data-pdf-bookmark="Defaults"><div class="sect1" id="idm45302814401424">
<h1>Defaults</h1>

<p>With defaults we mean the default values of configurations of components that you get when you use Kubernetes from source, in an unmodified manner. From a networking perspective, workloads<a data-type="indexterm" data-primary="workloads" data-secondary="default configuration" id="idm45302814399552"/><a data-type="indexterm" data-primary="configuration" data-secondary="workloads, defaults" id="idm45302814398576"/><a data-type="indexterm" data-primary="networks" data-secondary="workloads, default configuration" id="idm45302814397632"/> in Kubernetes have the following setup:</p>

<ul>
<li>
<p>Flat topology. Every pod can see and talk to every other pod in the cluster.</p>
</li>
<li>
<p>No securityContext. Workloads can escalate to host network interface controller (NIC).</p>
</li>
<li>
<p>No environmental restrictions. Workloads can query their host and cloud 
<span class="keep-together">metadata.</span></p>
</li>
<li>
<p>No identity for workloads.</p>
</li>
<li>
<p>No encryption on the wire (between pods and cluster-external traffic).</p>
</li>
</ul>

<p>While the preceding list might look scary, a different way to look at it might make it easier to assess the risks present. As depicted in <a data-type="xref" href="#fig-networking-overview">Figure 5-2</a>, the <a data-type="indexterm" data-primary="communication paths" id="idm45302814389200"/><a data-type="indexterm" data-primary="traffic" data-secondary="communication paths" id="idm45302814388496"/><a data-type="indexterm" data-primary="networks" data-secondary="communication paths" id="idm45302814387552"/>main communication paths in
Kubernetes are as follows:</p>

<ul>
<li>
<p>Intra-pod traffic: containers within a pod communicating (see the next section)</p>
</li>
<li>
<p>Inter-pod traffic: pods in the same cluster communicating (see <a data-type="xref" href="#workload-networking-inter-pod">“Inter-Pod Traffic”</a>)</p>
</li>
<li>
<p>Pod-to-worker node traffic (see <a data-type="xref" href="#workload-networking-env">“Pod-to-Worker Node Traffic”</a>)</p>
</li>
<li>
<p>Cluster-external traffic: communication of pods with the outside world (see <a data-type="xref" href="#workload-networking-external">“Cluster-External Traffic”</a>)</p>
</li>
</ul>

<figure><div id="fig-networking-overview" class="figure">
<img src="Images/haku_0502.png" alt="Kubernetes networking overview" width="1039" height="736"/>
<h6><span class="label">Figure 5-2. </span>Kubernetes networking overview</h6>
</div></figure>

<p>Let’s now have a closer look at the communication paths and other networking-relevant defaults in Kubernetes. Among other things, we’ll discuss <a data-type="xref" href="#workload-networking-arp">“The State of the ARP”</a>,
<a data-type="xref" href="#workload-networking-sec-ctx">“No securityContext”</a>, <a data-type="xref" href="#workload-networking-identity">“No Workload Identity”</a>, and
<a data-type="xref" href="#workload-networking-encryption">“No Encryption on the Wire”</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are some aspects of the networking space that depend heavily on the
environment in which Kubernetes is used. For example, when using hosted Kubernetes
from one of the cloud providers, the control plane and/or data plane may<a data-type="indexterm" data-primary="cloud providers" data-secondary="control and data planes, public availability of" id="idm45302814372432"/>
or may not be publicly available. If you are interested in learning
more how the big three handle this, have a look at:</p>

<ul>
<li>
<p>Amazon <a href="https://oreil.ly/Q8VkK">EKS private clusters</a></p>
</li>
<li>
<p>Azure <a href="https://oreil.ly/7XD3l">AKS private clusters</a></p>
</li>
<li>
<p>Google <a href="https://oreil.ly/lNCG3">GKE private clusters</a></p>
</li>
</ul>

<p>Since this is not an intrinsic property of Kubernetes and many combinations are possible, we decided to exclude this topic from our discussion
in this chapter.</p>
</div>

<p>So, are you ready to learn about the Kubernetes networking defaults?</p>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Intra-Pod Networking"><div class="sect2" id="workload-networking-intra-pod">
<h2>Intra-Pod Networking</h2>

<p>The way intra-pod networking<a data-type="indexterm" data-primary="intra-pod networking" id="idm45302814362464"/><a data-type="indexterm" data-primary="networks" data-secondary="intra-pod" id="idm45302814361760"/> in Kubernetes works is as follows. An implicit
so-called <a href="https://oreil.ly/VRjJQ">pause</a> container
in<a data-type="indexterm" data-primary="pause containers" id="idm45302814359952"/><a data-type="indexterm" data-primary="pods" data-secondary="pause containers" id="idm45302814359216"/><a data-type="indexterm" data-primary="containers" data-secondary="pause" id="idm45302814358272"/> a pod (<code>cp</code> in <a data-type="xref" href="#fig-networking-pod">Figure 5-3</a>) spans a Linux <a href="https://oreil.ly/rr1RM">network namespace</a>.</p>

<figure><div id="fig-networking-pod" class="figure">
<img src="Images/haku_0503.png" alt="Internals of a Kubernetes pod" width="376" height="333"/>
<h6><span class="label">Figure 5-3. </span>Internals of a Kubernetes pod</h6>
</div></figure>

<p>Other containers in the pod, such as init containers (like <code>ci1</code> and <code>ci2</code>), and the main
application container and sidecars, such as proxies or logging containers  (for example, <code>c1</code> to <code>c3</code>), then join the pause container’s network and IPC namespace.</p>

<p>The pause container has the network bridge mode<a data-type="indexterm" data-primary="network bridge mode, containers" id="idm45302814350496"/><a data-type="indexterm" data-primary="containers" data-secondary="network bridge mode" id="idm45302814349776"/> enabled and all the other
containers in the pod are sharing their namespace via container mode.</p>

<p>As discussed in <a data-type="xref" href="ch02.xhtml#ch-pod-level-resources">Chapter 2</a>, pods were designed to make it easy
to lift and shift existing applications into Kubernetes, which has sobering security implications. Ideally, you rewrite the application so that the tight coupling of containers
in a pod are not necessary or deploy traditional tooling in the context of a pod.</p>

<p>While the latter seems like a good idea initially, do remember that this is a stopgap
measure at best. Once the boundaries are clear and effectively every
microservice is deployed in its own pod, you can go ahead and use the techniques
discussed in the next sections.</p>

<p>In addition, no matter if you’re looking at <a data-type="indexterm" data-primary="defense in depth" data-secondary="container security and" id="idm45302814345888"/>defense in depth in the context
of a pod or cluster-wide, you can employ a range of dedicated container security open source and commercial offerings. See also respective section <a data-type="xref" href="app02.xhtml#apx-networking">“Networking”</a> in <a data-type="xref" href="app02.xhtml#appendix-resources">Appendix B</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Inter-Pod Traffic"><div class="sect2" id="workload-networking-inter-pod">
<h2>Inter-Pod Traffic</h2>

<p>In a Kubernetes cluster, by <a data-type="indexterm" data-primary="inter-pod traffic" id="idm45302814341296"/><a data-type="indexterm" data-primary="networks" data-secondary="inter-pod" id="idm45302814340688"/>default every pod can see and talk to every other pod. This default is a nightmare from a security perspective (or a free ride, depending on which side you’re on) and we can not emphasize enough how dangerous this fact is.</p>

<p>No matter what your <a data-type="indexterm" data-primary="threat models" data-secondary="pod communication, default behavior" id="idm45302814339008"/>threat model is, this “all traffic is allowed” policy
for both inter-pod and external traffic represents one giant attack
vector. In other words, you should never rely on the Kubernetes defaults in the networking space. You should never, ever run a Kubernetes cluster without restricting network traffic in some form or shape. For a practical example on how you can go about this, have a look at <a data-type="xref" href="#workload-network-policies">“Traffic Flow Control”</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pod-to-Worker Node Traffic"><div class="sect2" id="workload-networking-env">
<h2>Pod-to-Worker Node Traffic</h2>

<p>If not disabled, workloads<a data-type="indexterm" data-primary="pod-to-worker-node traffic" id="idm45302814334576"/><a data-type="indexterm" data-primary="networks" data-secondary="pod-to-worker-node traffic" id="idm45302814333808"/> can query the worker node (host) they
are running on as well as the (cloud) environments they are deployed into.</p>

<p>No default protection exists for worker nodes, routable from the CNI. Further,
the worker nodes may be able to access cloud resources, datastores, and
API servers. Some cloud providers, notably Google, offer some solutions for this
issue; see, for example, <a href="https://oreil.ly/RykAM">shielded GKE Nodes</a>.</p>

<p>For cloud environments<a data-type="indexterm" data-primary="cloud" data-secondary="environments, security practices" id="idm45302814330800"/> in general, good practices exist. For example,
Amazon EKS recommends to <a href="https://oreil.ly/ycvSd">restrict access to instance metadata</a> and equally GKE documents how to
<a href="https://oreil.ly/pMDaj">protect cluster metadata</a>.</p>

<p>Further, commercial offerings like Nirmata’s <a href="https://oreil.ly/LSElg">Virtual Clusters and Workload Policies</a>
can be used in this context.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Cluster-External Traffic"><div class="sect2" id="workload-networking-external">
<h2>Cluster-External Traffic</h2>

<p>To allow pods to communicate with<a data-type="indexterm" data-primary="cluster-external traffic" id="clustext_trafic"/><a data-type="indexterm" data-primary="networks" data-secondary="cluster-external traffic" id="netwrk_clustextraf"/> cluster-external endpoints, Kubernetes has added a number of mechanisms over time. The most recent and widely used is called an
<a href="https://oreil.ly/F17YB">Ingress</a>.
This allows for layer 7<a data-type="indexterm" data-primary="Ingress" id="idm45302814321792"/> routing (HTTP), whereas for other use cases such as layer 3/4 routing you would need to use older, less convenient methods. See also <a href="https://oreil.ly/pzlGE">Publishing Services (ServiceTypes)</a> in the docs.</p>

<p>In order for you to use the Ingress resource, you will need to pick an<a data-type="indexterm" data-primary="ingress controllers" id="idm45302814319696"/> ingress controller. You have many many choices, oftentimes open source-based, which include:</p>

<ul>
<li>
<p><a href="https://oreil.ly/N8v57">Emissary-ingress</a></p>
</li>
<li>
<p><a href="https://oreil.ly/T9nsX">Contour</a></p>
</li>
<li>
<p><a href="https://oreil.ly/ZAm5g">HAProxy Ingress</a></p>
</li>
<li>
<p><a href="https://oreil.ly/G6yS3">NGINX Ingress Controller</a></p>
</li>
<li>
<p><a href="https://oreil.ly/h0Tpr">Traefik</a></p>
</li>
</ul>

<p>In addition, cloud providers usually provide their own solutions, integrated with
their managed load-balancing services.</p>

<p>Encryption on the wire (TLS) is almost the default nowadays, and most Ingress
solutions support it out of the box. Alternatively, you can use a
service mesh for securing your North-South traffic
(see <a data-type="xref" href="#workload-networking-service-meshes">“Service Meshes”</a>).</p>

<p>Last but not least, on the application level you might want to consider
using a web application firewall (WAF) such as offered by most cloud providers,
or you can use standalone offering such as <a href="https://oreil.ly/iTXQu">Wallarm</a>.</p>

<p>More and more practitioners are sharing their experiences in this space, so keep
an eye out for blog posts and CNCF webinars covering <a data-type="indexterm" data-primary="cluster-external traffic" data-startref="clustext_trafic" id="idm45302814307664"/><a data-type="indexterm" data-primary="networks" data-secondary="cluster-external traffic" data-startref="netwrk_clustextraf" id="idm45302814306672"/>this topic. See, for example,
<a href="https://oreil.ly/Q8Twj">“Shaping Chick-fil-A One Traffic in a Multi-Region Active-Active Architecture”</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The State of the ARP"><div class="sect2" id="workload-networking-arp">
<h2>The State of the ARP</h2>

<p><a href="https://oreil.ly/8egtf">Address Resolution Protocol</a>
(ARP) is a link layer protocol used <a data-type="indexterm" data-primary="ARP (Address Resolution Protocol)" data-secondary="attacks" id="idm45302814302144"/><a data-type="indexterm" data-primary="Address Resolution Protocol (ARP)" data-see="ARP (Address Resolution Protocol)" id="idm45302814301136"/><a data-type="indexterm" data-primary="attacks" data-secondary="ARP-based" id="idm45302814300160"/>by the Internet Protocol (IP) to map IP network addresses
to the hardware (MAC) addresses. Liz Rice showed in her KubeCon NA 2019 talk,
<a href="https://oreil.ly/wnQKi">“CAP_NET_RAW and ARP Spoofing in Your Cluster: It’s Going Downhill From Here”</a>,
how defaults allow us to open raw network sockets and how this can lead to issues.</p>

<p>It involves using ARP and DNS to fool a victim pod to visit a fake URL, which is possible due to the way Kubernetes <a href="https://oreil.ly/NAPnT">handles local FQDNs</a>, and it requires that <code>CAP_NET_RAW</code> is available to a pod.</p>

<p>For more details, see the Aqua Security blog post
<a href="https://oreil.ly/g6Zu3">“DNS Spoofing on Kubernetes Clusters”</a>.</p>

<p>The good news is, there are defenses available to mitigate the ARP-based attacks
and spoil the Captain’s mood:</p>

<ul>
<li>
<p>Using Pod Security Policies (PSP)<a data-type="indexterm" data-primary="Pod Security Policies (PSPs)" id="idm45302814293536"/><a data-type="indexterm" data-primary="PSPs (Pod Security Policies)" id="idm45302814292768"/><a data-type="indexterm" data-primary="policies" data-secondary="ARP-based attacks and" id="idm45302814292080"/> as discussed in <a data-type="xref" href="ch08.xhtml#policy-runtime-policies">“Runtime Policies”</a>, to drop <code>CAP_NET_RAW</code>.</p>
</li>
<li>
<p>Using generic policy engines as described in <a data-type="xref" href="ch08.xhtml#generic-policy-engines">“Generic Policy Engines”</a> such as
<a href="https://oreil.ly/VpDrN">Open Policy Agent/Gatekeeper</a>,
or by using <a href="https://oreil.ly/5Xn2T">Kyverno to convert PSPs</a> (also see <a href="https://oreil.ly/ZWp1X">this video</a>).</p>
</li>
<li>
<p>Using, for example, <a href="https://oreil.ly/jQchh">Calico</a> or <a href="https://oreil.ly/ZWrKF">Cilium</a> on layer 3.</p>
</li>
</ul>

<p>How can you tell if you’re affected? Use
<a href="https://oreil.ly/uDFka">kube-hunter</a>, for example.</p>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="No securityContext"><div class="sect2" id="workload-networking-sec-ctx">
<h2>No securityContext</h2>

<p>By default, workloads <a data-type="indexterm" data-primary="workloads" data-secondary="NIC, escalating to" id="idm45302814279712"/>can escalate to the NIC of the worker node they are running on. For example, when running privileged containers, one can escape from the container <a href="https://oreil.ly/uRrgr">using kernel modules</a>. Further, as the Microsoft Azure team pointed out in its <a href="https://oreil.ly/1ohm8">“Threat matrix for Kubernetes” blog post</a>:</p>
<blockquote>
<p>Attackers with network access to the host (for example, via running code on a compromised container)
can send API requests to the Kubelet API. Specifically querying <code>https://[NODE IP]:10255/pods/</code>
retrieves the running pods on the node. <code>https://[NODE IP]:10255/spec/</code> retrieves information
about the node itself, such as CPU and memory consumption.</p></blockquote>

<p>Naturally, one wants to avoid these scenarios and one way to go about this is
to apply PSPs,<a data-type="indexterm" data-primary="policies" data-secondary="pod security" id="idm45302814274608"/> as discussed in <a data-type="xref" href="ch08.xhtml#policy-runtime-policies">“Runtime Policies”</a>.</p>

<p>For example, the <a href="https://oreil.ly/MD6BD">Baseline/default policy</a>
has the following defined:</p>
<dl>
<dt>Sharing the host namespaces must be disallowed</dt>
<dd>

<ul>
<li>
<p><code>spec.hostNetwork</code></p>
</li>
<li>
<p><code>spec.hostPID</code></p>
</li>
<li>
<p><code>spec.hostIPC</code></p>
</li>
</ul>
</dd>
<dt>Privileged pods disable most security mechanisms and must be disallowed</dt>
<dd>

<ul>
<li>
<p><code>spec.containers[*].securityContext.privileged</code></p>
</li>
<li>
<p><code>spec.initContainers[*].securityContext.privileged</code></p>
</li>
</ul>
</dd>
<dt><code>HostPorts</code> should be disallowed or at minimum restricted to a known list</dt>
<dd>

<ul>
<li>
<p><code>spec.containers[*].ports[*].hostPort</code></p>
</li>
<li>
<p><code>spec.initContainers[*].ports[*].hostPort</code></p>
</li>
</ul>
</dd>
</dl>

<p>In addition, there are a number of commercial offerings, such as Palo Alto
Networks <a href="https://oreil.ly/3kNmb">Prisma Cloud</a> (formerly Twistlock), that you can use to harden <a data-type="indexterm" data-primary="worker nodes" data-secondary="hardening" id="idm45302814257920"/>your worker nodes in this context.</p>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="No Workload Identity"><div class="sect2" id="workload-networking-identity">
<h2>No Workload Identity</h2>

<p>By default, Kubernetes does not assign<a data-type="indexterm" data-primary="workloads" data-secondary="identifying" id="idm45302814254608"/> an identity to services. SPIFFE/SPIRE can
be used to manage workload identities and enable mTLS. <a href="https://spiffe.io">SPIFFE</a> (Secure Production Identity Framework for Everyone)<a data-type="indexterm" data-primary="SPIFFE (Secure Production Identity Framework for Everyone)" id="idm45302814252672"/><a data-type="indexterm" data-primary="Secure Production Identity Framework for Everyone (SPIFFE)" data-see="SPIFFE (Secure Production Identity Framework for Everyone)" id="idm45302814251920"/>
is a collection of specifications for securely identifying workloads. It provides a framework <a data-type="indexterm" data-primary="services, assigning identities to" id="idm45302814250640"/>enabling you to dynamically issue an identity to a
service across environments by defining short-lived cryptographic identity
documents—called <a data-type="indexterm" data-primary="SPIFFE Verifiable Identity Documents (SVIDs)" data-see="SVIDs (SPIFFE Verifiable Identity Documents)" id="idm45302814249664"/><a data-type="indexterm" data-primary="SVIDs (SPIFFE Verifiable Identity Documents)" id="idm45302814248592"/>SPIFFE Verifiable Identity Documents 
<span class="keep-together">(SVIDs)—via</span> an API.
Your workloads in turn can use these SVIDs when authenticating to other workloads.
For example, an SVID can be used to establish an TLS connection or to verify a JWT token.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="No Encryption on the Wire"><div class="sect2" id="workload-networking-encryption">
<h2>No Encryption on the Wire</h2>

<p>For workloads in regulated industries, that is, any kind of app that is
required to 
<span class="keep-together">conform</span> to a (government issued) regulation,<a data-type="indexterm" data-primary="encryption" data-secondary="on the wire" data-secondary-sortas="wire" id="idm45302814244112"/> encryption
on the wire—or encryption in transit, <a data-type="indexterm" data-primary="encryption" data-secondary="in transit" data-secondary-sortas="transit" id="idm45302814242736"/>as it’s sometimes called—is typically
one of the requirements. For example, if you have a
<a href="https://oreil.ly/Pr7oP">Payment Card Industry Data Security Standard</a>
(PCI DSS)–compliant app as a bank, or a <a href="https://oreil.ly/uLjNn">Health Insurance Portability and Accountability Act</a>

<span class="keep-together">(HIPAA)–compliant</span> app as a health care provider, you will want to make sure that
the 
<span class="keep-together">communication</span> between your containerized microservices<a data-type="indexterm" data-primary="containerized microservices, protecting" id="idm45302814238256"/><a data-type="indexterm" data-primary="microservices, protecting" id="idm45302814237488"/> is protected against
sniffing and person-in-the-middle attacks.</p>

<p>These days, the <a data-type="indexterm" data-primary="Transport Layer Security (TLS)" data-see="TLS (Transport Layer Security)" id="idm45302814236336"/><a data-type="indexterm" data-primary="TLS (Transport Layer Security)" id="idm45302814235328"/><a data-type="indexterm" data-primary="protocols" data-secondary="TLS" id="idm45302814234656"/>Transport Layer Security (TLS) protocol as defined in
<a href="https://oreil.ly/VeMXC">RFC 8446</a> and older IETF paperwork is usually
used to encrypt traffic on the wire. It uses 
<span class="keep-together">asymmetric</span> encryption to agree on
a shared secret negotiated at the beginning of the session (“handshake”) and in
turn symmetric encryption to encrypt the workload data. This setup is a nice
performance versus security trade-off.</p>

<p>While control plane components such as the API server, <code>etcd</code>, or a <code>kubelet</code>
can rely on an PKI infra out-of-the-box, providing APIs and good practices for
<a href="https://oreil.ly/PA3NQ">certificates</a>, the
same is sadly not true for your workloads.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can see the API server’s hostname, and any IPs encoded into its TLS certificate,
with <code>openssl</code>. You can find example code for this in <a data-type="xref" href="ch07.xhtml#control_plane">“Control Plane”</a>.</p>
</div>

<p>By default, the traffic between pods and to the outside world is not encrypted. To mitigate, enable <a data-type="indexterm" data-primary="workloads" data-secondary="encryption, enabling" id="idm45302814226336"/><a data-type="indexterm" data-primary="encryption" data-secondary="workloads, enabling" id="idm45302814225360"/>workload encryption on the wire, for example with
<a href="https://oreil.ly/jwHQU">Calico</a>,
with <a href="https://oreil.ly/ZxGoa">Wireguard</a> VPN, or with Cilium, which supports
both <a href="https://oreil.ly/6bYn9">Wireguard and IPsec</a>. Another option to provide not only this sort of encryption but also workload
identity, as discussed in <a data-type="xref" href="#workload-networking-identity">“No Workload Identity”</a>, are <a data-type="indexterm" data-primary="service meshes" id="idm45302814221168"/>service meshes. With the defaults out of the way, let’s move on to the threat modeling
for the networking space.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Threat Model"><div class="sect1" id="idm45302814400800">
<h1>Threat Model</h1>

<p>The threat model <a data-type="indexterm" data-primary="threat modeling" data-secondary="network space" id="idm45302814218928"/><a data-type="indexterm" data-primary="networks" data-secondary="threat modeling" id="idm45302814217920"/>in the networking space (see <a data-type="xref" href="ch01.xhtml#threat-models-def">“Starting to Threat Model”</a>); that is, the collection of identified networking vulnerabilities according to the risk they pose—is what we’re focusing on in this section.</p>

<p>So, what is the threat model we consider in the networking space, with respect to workloads? What are our assumptions about what attackers could do to our precious workloads and beyond to the infrastructure?</p>

<p>The following observations should give you an idea about potential threat models. We illustrate these scenarios with some examples of past attacks, covering the 2018–2020 time frame:</p>

<ul>
<li>
<p>Using the front door, for example via an ingress controller or a load balancer,
and then either pivoting or performing a denial-of-service attack, such as observed in
<a href="https://oreil.ly/m4aKx">CVE-2020-15127</a></p>
</li>
<li>
<p>Using developer access paths like <code>kubectl cp</code> (<a href="https://oreil.ly/U7ogf">CVE-2019-11249</a>)
or developer environments such as Minikube, witnessed in
<a href="https://oreil.ly/lkf3P">CVE-2018-1002103</a></p>
</li>
<li>
<p>Launching a pod with access to host networking or unnecessary capabilities, as
we will further discuss in <a data-type="xref" href="#workload-networking-arp">“The State of the ARP”</a></p>
</li>
<li>
<p>Leveraging a compromised workload to connect to another workload</p>
</li>
<li>
<p>Port scanning of all CNI plug-ins and further use this information to identify
vulnerabilities; for example, <a href="https://oreil.ly/3kVUY">CVE-2019-9946</a></p>
</li>
<li>
<p>Attacking a control plane component such as the API server and <code>etcd</code> or a <code>kubelet</code>
 or <code>kube-proxy</code> on the worker; for example, <a href="https://oreil.ly/tauhi">CVE-2020-8558</a>,
<a href="https://oreil.ly/Tybuu">CVE-2019-11248</a>,
<a href="https://oreil.ly/60und">CVE-2019-11247</a>, and
<a href="https://oreil.ly/KouIk">CVE-2018-1002105</a></p>
</li>
<li>
<p>Performing server-side request forgery (SSRF); for example, concerning the hosting environment,
like a cloud provider’s VMs</p>
</li>
<li>
<p>Performing person-in-the-middle attacks, such as seen in the context of
<a href="https://oreil.ly/hGXIM">IPv6 routing</a>;
see also <a href="https://oreil.ly/CHK6q">CVE-2020-10749</a></p>
</li>
</ul>

<p>Now that we have a basic idea of the potential threat model, let’s go through and
see how the defaults can be exploited and defended against, in turn.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Traffic Flow Control"><div class="sect1" id="workload-network-policies">
<h1>Traffic Flow Control</h1>

<p>We’ve seen the networking defaults and what kind of communication paths
are present in Kubernetes. In this section, we walk you through an end-to-end setup and show you how to secure the <a data-type="indexterm" data-primary="traffic" data-secondary="external, securing" id="traf_ext_sec"/><a data-type="indexterm" data-primary="networks" data-secondary="external traffic, securing" id="net_extraf_sec"/><a data-type="indexterm" data-primary="external traffic, securing" id="extraf_sec"/>external traffic using
network policies.</p>








<section data-type="sect2" data-pdf-bookmark="The Setup"><div class="sect2" id="idm45302814191056">
<h2>The Setup</h2>

<p>To demonstrate the networking defaults in action,
let’s use <a href="https://oreil.ly/9KldR">kind</a>, a tool<a data-type="indexterm" data-primary="kind clusters, configuring" id="idm45302814188336"/><a data-type="indexterm" data-primary="configuration" data-secondary="kind clusters" id="idm45302814187584"/> for running local Kubernetes
clusters using Docker containers.</p>

<p>Let’s create a <code>kind</code> cluster with networking prepared for Calico and enable Ingress (also see the <a href="https://oreil.ly/H7dPf">documentation</a>).
We are using the following 
<span class="keep-together">config:</span></p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Cluster</code><code>
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">kind.x-k8s.io/v1alpha4</code><code>
</code><code class="nt">nodes</code><code class="p">:</code><code>
</code><code class="p-Indicator">-</code><code> </code><code class="nt">role</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">control-plane</code><code>
</code><code>  </code><code class="nt">kubeadmConfigPatches</code><code class="p">:</code><code>
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="p-Indicator">|</code><code>
</code><code>    </code><code class="no">kind: InitConfiguration</code><code>
</code><code>    </code><code class="no">nodeRegistration:</code><code>
</code><code>      </code><code class="no">kubeletExtraArgs:</code><code>
</code><code>        </code><code class="no">node-labels: "ingress-ready=true" </code><a class="co" id="co_networking_CO1-1" href="#callout_networking_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>  </code><code class="nt">extraPortMappings</code><code class="p">:</code><code>
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">containerPort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code><code>
</code><code>    </code><code class="nt">hostPort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code><code>
</code><code>    </code><code class="nt">protocol</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TCP</code><code>
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">containerPort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">443</code><code>
</code><code>    </code><code class="nt">hostPort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">443</code><code>
</code><code>    </code><code class="nt">protocol</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TCP</code><code>
</code><code class="p-Indicator">-</code><code> </code><code class="nt">role</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">worker</code><code>
</code><code class="nt">networking</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">disableDefaultCNI</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">true</code><code> </code><a class="co" id="co_networking_CO1-2" href="#callout_networking_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>  </code><code class="nt">podSubnet</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">192.168.0.0/16</code><code> </code><a class="co" id="co_networking_CO1-3" href="#callout_networking_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_networking_CO1-1" href="#co_networking_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Enable Ingress for cluster.</p></dd>
<dt><a class="co" id="callout_networking_CO1-2" href="#co_networking_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Disable the native <code>kindnet</code>.</p></dd>
<dt><a class="co" id="callout_networking_CO1-3" href="#co_networking_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>In preparation to install Calico, set to its default subnet.</p></dd>
</dl>

<p>Assuming the preceding YAML snippet is stored in a file called <em>cluster-config.yaml</em>,
you can now create the <code>kind</code> cluster as follows:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kind create cluster --name cnnp <code class="se">\</code>
  --config cluster-config.yaml

Creating cluster <code class="s2">"cnnp"</code> ...</pre>

<p class="pagebreak-before">Note that if you do this the first time, the preceding output might look different
and it can take several minutes to pull the respective container images.</p>

<p>Next we install and<a data-type="indexterm" data-primary="Calico" data-secondary="patching" id="idm45302814050416"/> patch Calico to make it work with <code>kind</code>. Kudos to Alex
Brand for putting together the necessary
<a href="https://oreil.ly/qudSv">patch instructions</a>:</p>
<pre data-type="programlisting" data-code-language="bash" class="small no-indent">
<code class="nv">$ </code>kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
...
serviceaccount/calico-kube-controllers created

<code class="nv">$ </code>kubectl -n kube-system <code class="nb">set </code>env daemonset/calico-node <code class="nv">FELIX_IGNORELOOSERPF</code><code class="o">=</code><code class="nb">true</code>
daemonset.apps/calico-node env updated
</pre>

<p>And to verify if everything is up and running as expected:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl -n kube-system get pods <code class="p">|</code> grep calico-node
calico-node-2j2wd     0/1     Running     <code class="m">0</code>     18s
calico-node-4hx46     0/1     Running     <code class="m">0</code>     18s
calico-node-qnvs6     0/1     Running     <code class="m">0</code>     18s</pre>

<p>Before we can deploy our app, we need one last bit of infrastructure in place,
a <a data-type="indexterm" data-primary="load balancers" data-secondary="external traffic and" id="idm45302814010848"/>load balancer, making the pods available to the outside world (your machine).</p>

<p>For this we use <a href="https://oreil.ly/fv4K2">Ambassador</a> as an <a data-type="indexterm" data-primary="Ambassador" id="idm45302813984096"/><a data-type="indexterm" data-primary="ingress controllers" data-secondary="Ambassador" id="idm45302813983360"/>ingress controller:</p>
<pre data-type="programlisting" data-code-language="bash" class="small no-indent">
<code class="nv">$ </code>kubectl apply -f https://github.com/datawire/ambassador-operator/releases/latest
/download/ambassador-operator-crds.yaml <code class="o">&amp;&amp;</code> <code class="se">\</code>
  kubectl apply -n ambassador -f https://github.com/datawire/ambassador-operator/releases
/latest/download/ambassador-operator-kind.yaml <code class="o">&amp;&amp;</code> <code class="se">\</code>
  kubectl <code class="nb">wait</code> --timeout<code class="o">=</code>180s <code class="se">\</code>
-n ambassador --
<code class="k">for</code><code class="o">=</code><code class="nv">condition</code><code class="o">=</code>deployed <code class="se">\</code>
ambassadorinstallations/ambassador
customresourcedefinition.apiextensions.k8s.io/ambassadorinstallations.getambassador.io created
namespace/ambassador created
configmap/static-helm-values created
serviceaccount/ambassador-operator created
clusterrole.rbac.authorization.k8s.io/ambassador-operator-cluster created
clusterrolebinding.rbac.authorization.k8s.io/ambassador-operator-cluster created
role.rbac.authorization.k8s.io/ambassador-operator created
rolebinding.rbac.authorization.k8s.io/ambassador-operator created
deployment.apps/ambassador-operator created
ambassadorinstallation.getambassador.io/ambassador created
ambassadorinstallation.getambassador.io/ambassador condition met
</pre>

<p>Now we can launch the application, a <a data-type="indexterm" data-primary="web servers, creating namespaces" id="idm45302813956160"/><a data-type="indexterm" data-primary="namespaces" data-secondary="creating, web servers" id="idm45302813955456"/>web server. First off, we want to do all of the
following in a dedicated namespace called <code>npdemo</code>, so let’s create one:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl create ns npdemo
namespace/npdemo created</pre>

<p class="pagebreak-before">Next, create a YAML file called <em>workload.yaml</em> that defines a deployment, a
service, and an ingress resource, in total representing our<a data-type="indexterm" data-primary="workloads" data-secondary="YAML files" id="idm45302813944768"/><a data-type="indexterm" data-primary="YAML Ain’t Markup Language" data-secondary="workload applications" id="idm45302813953440"/> workload
application:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Deployment</code><code>
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">apps/v1</code><code>
</code><code class="nt">metadata</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">labels</code><code class="p">:</code><code>
</code><code>    </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code class="nt">spec</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">replicas</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">1</code><code>
</code><code>  </code><code class="nt">selector</code><code class="p">:</code><code>
</code><code>    </code><code class="nt">matchLabels</code><code class="p">:</code><code>
</code><code>      </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>  </code><code class="nt">template</code><code class="p">:</code><code>
</code><code>    </code><code class="nt">metadata</code><code class="p">:</code><code>
</code><code>      </code><code class="nt">labels</code><code class="p">:</code><code>
</code><code>        </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>    </code><code class="nt">spec</code><code class="p">:</code><code>
</code><code>      </code><code class="nt">containers</code><code class="p">:</code><code>
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">image</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx:alpine</code><code>
</code><code>        </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">main</code><code>
</code><code>        </code><code class="nt">ports</code><code class="p">:</code><code>
</code><code>        </code><code class="p-Indicator">-</code><code> </code><code class="nt">containerPort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code><code>
</code><code class="nn">---</code><code>
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Service</code><code>
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>
</code><code class="nt">metadata</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code class="nt">spec</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">selector</code><code class="p">:</code><code>
</code><code>    </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>  </code><code class="nt">ports</code><code class="p">:</code><code>
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">port</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code><code>
</code><code class="nn">---</code><code>
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Ingress</code><code> </code><a class="co" id="co_networking_CO2-1" href="#callout_networking_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">extensions/v1beta1</code><code>
</code><code class="nt">metadata</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">mainig</code><code>
</code><code>  </code><code class="nt">annotations</code><code class="p">:</code><code>
</code><code>    </code><code class="nt">kubernetes.io/ingress.class</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ambassador</code><code>
</code><code class="nt">spec</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">rules</code><code class="p">:</code><code>
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">http</code><code class="p">:</code><code>
</code><code>      </code><code class="nt">paths</code><code class="p">:</code><code>
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">path</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">/api</code><code>
</code><code>        </code><code class="nt">backend</code><code class="p">:</code><code>
</code><code>          </code><code class="nt">serviceName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>
</code><code>          </code><code class="nt">servicePort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_networking_CO2-1" href="#co_networking_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>We configure the ingress in a way that if we hit the <em>/api</em> URL path we
expect it to route traffic to our <code>nginx</code> service.</p></dd>
</dl>

<p class="pagebreak-before">Next, you want to create the <a data-type="indexterm" data-primary="resources" data-secondary="workload applications, creating" id="idm45302813601728"/><a data-type="indexterm" data-primary="workloads" data-secondary="resources, creating" id="idm45302813600688"/>resources defined in <em>workload.yaml</em> by using:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl -n npdemo apply -f workload.yaml
deployment.apps/nginx created
service/nginx created
ingress.extensions/mainig created</pre>

<p>When you now try to access the app as exposed in the Ingress resource you should be able to do the following (note that we’re only counting the lines
returned to verify we get something back):</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>curl -s 127.0.0.1/api <code class="p">|</code> wc -l

  25</pre>

<p>Wait. What just happened? We put an <a data-type="indexterm" data-primary="workloads" data-secondary="ingress, testing" id="idm45302813592416"/><a data-type="indexterm" data-primary="testing" data-secondary="ingress, NGINX service" id="idm45302813591680"/><a data-type="indexterm" data-primary="NGINX service, testing ingress" id="idm45302813590768"/>Ingress in front of the NGINX service and
it happily receives traffic from outside? That can’t be good.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Network Policies to the Rescue!"><div class="sect2" id="idm45302814190272">
<h2>Network Policies to the Rescue!</h2>

<p>So, how can we keep the Captain and their <a data-type="indexterm" data-primary="network policies" data-secondary="traffic" data-tertiary="securing" id="netpol_traf_sec"/><a data-type="indexterm" data-primary="traffic" data-secondary="securing, network policies" id="traf_sec_netpol"/>crew from getting their dirty paws on
our cluster?
<a href="https://oreil.ly/lIqM8">Network policies</a> are coming to our rescue. While we will cover policies in a
dedicated chapter (see <a data-type="xref" href="ch08.xhtml#ch-policy">Chapter 8</a>), we point out network policies and their
usage here since they are so useful and, given the “by default all traffic is
allowed” attitude of Kubernetes, one can argue almost necessary.</p>

<p>While Kubernetes allows you to define and apply network policies out-of-the-box,
you need something that <em>enforces</em> the policies you define and that’s the job of a
<a href="https://oreil.ly/4uK2d">provider</a>.</p>

<p>For example, in the following walkthrough, we will be using
<a href="https://oreil.ly/zoAbv">Calico</a>, however there are many more options available, such as the eBPF-based solutions<a data-type="indexterm" data-primary="Calico" data-secondary="network traffic, securing" id="idm45302813888704"/> discussed in <a data-type="xref" href="#workload-networking-bpf">“eBPF”</a>.</p>

<p>We shut down all traffic with the following Kubernetes network policy in
a file called, fittingly, <em>np-deny-all.yaml</em>:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">NetworkPolicy</code><code>
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code>
</code><code class="nt">metadata</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">deny-all</code><code>
</code><code class="nt">spec</code><code class="p">:</code><code>
</code><code>  </code><code class="nt">podSelector</code><code class="p">:</code><code> </code><code class="p-Indicator">{</code><code class="p-Indicator">}</code><code> </code><a class="co" id="co_networking_CO3-1" href="#callout_networking_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>  </code><code class="nt">policyTypes</code><code class="p">:</code><code>
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">Ingress</code><code> </code><a class="co" id="co_networking_CO3-2" href="#callout_networking_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_networking_CO3-1" href="#co_networking_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Selects the pods in the same namespace, in our case all.</p></dd>
<dt><a class="co" id="callout_networking_CO3-2" href="#co_networking_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Disallow any Ingress traffic.</p></dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>Network policies are notoriously difficult to get right, so in this context,
you may want to check out the following:</p>

<ul>
<li>
<p>To help you edit and visualize network pollicies, check out the tool available from <a href="https://oreil.ly/6bTIb">networkpolicy.io</a>.</p>
</li>
<li>
<p>To debug network policy you can use <a href="https://oreil.ly/2bk4K">krew-net-forward</a>.</p>
</li>
<li>
<p>To test policies, have a look at <a href="https://oreil.ly/xvabI">netassert</a>.</p>
</li>
</ul>
</div>

<p>So let’s apply the preceding network policy and see if we can still access the app
from outside of the cluster:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl -n npdemo apply -f np-deny-all.yaml
networkpolicy.networking.k8s.io/deny-all created

<code class="nv">$ </code>kubectl -n npdemo describe netpol deny-all
Name:         deny-all
Namespace:    npdemo
Created on:   2020-09-22 10:39:27 +0100 IST
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
PodSelector:  &lt;none&gt; <code class="o">(</code>Allowing the specific traffic to all pods in this namespace<code class="o">)</code>
  Allowing ingress traffic:
    &lt;none&gt; <code class="o">(</code>Selected pods are isolated <code class="k">for</code> ingress connectivity<code class="o">)</code>
  Not affecting egress traffic
  Policy Types: Ingress</pre>

<p>And this should fail now, based on our network policy (giving it a 3-second time
out, just to be sure):</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>curl --max-time <code class="m">3</code> 127.0.0.1/api
curl: <code class="o">(</code>28<code class="o">)</code> Operation timed out after <code class="m">3005</code> milliseconds with <code class="m">0</code> bytes received</pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you only have <code>kubectl</code> available, you can still make raw network requests,
as <a href="https://oreil.ly/TDxaW">Rory McCune</a> pointed
out:</p>

<pre data-type="programlisting">kubectl --insecure-skip-tls-verify -s bbc.co.uk get --raw /</pre>

<p>Of course, it shouldn’t be in your container image in the first place!</p>
</div>

<p>We hope by now you get an idea how dangerous the defaults are—all network traffic
to and from pods is allowed—and how you can defend against it.</p>

<p>Learn more about network policies, including recipes, tips, and tricks,
via the resources we put together in <a data-type="xref" href="app02.xhtml#appendix-resources">Appendix B</a>.</p>
<div data-type="tip" class="less_space pagebreak-before"><h6>Tip</h6>
<p>In addition to network policies, some cloud providers offer other native
mechanisms to restrict traffic from/to pods; for example, <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="security groups for pods" id="idm45302813557040"/><a data-type="indexterm" data-primary="security groups for pods (AWS)" id="idm45302813556032"/>see AWS
<a href="https://oreil.ly/HWxpu">security groups for pods</a>.</p>
</div>

<p>Finally, don’t forget to clean up your Kubernetes <a data-type="indexterm" data-primary="clusters" data-secondary="cleanup" id="idm45302813553984"/>cluster using
<code>kind delete cluster --name cnnp</code>, once you’re done exploring
the topic of network policies.</p>

<p>Now that we’ve seen a concrete networking setup in action, let’s move on to
a different topic: service meshes. This  relatively recent technology can help
you in addressing some of the not-so-secure defaults discussed earlier,
including workload identity and encryption <a data-type="indexterm" data-primary="traffic" data-secondary="external, securing" data-startref="traf_ext_sec" id="idm45302813551744"/><a data-type="indexterm" data-primary="networks" data-secondary="external traffic, securing" data-startref="net_extraf_sec" id="idm45302813550496"/><a data-type="indexterm" data-primary="external traffic, securing" data-startref="extraf_sec" id="idm45302813549216"/><a data-type="indexterm" data-primary="network policies" data-secondary="traffic" data-tertiary="securing" data-startref="netpol_traf_sec" id="idm45302813548256"/><a data-type="indexterm" data-primary="traffic" data-secondary="securing, network policies" data-startref="traf_sec_netpol" id="idm45302813496576"/>on the wire.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Service Meshes"><div class="sect1" id="workload-networking-service-meshes">
<h1>Service Meshes</h1>

<p>A somewhat advanced topic, a service<a data-type="indexterm" data-primary="service meshes" data-secondary="workload issues and" id="idm45302813493680"/><a data-type="indexterm" data-primary="workloads" data-secondary="service meshes" id="idm45302813492704"/> mesh is in a sense complementary to Kubernetes
and can be beneficial in a number of
<a href="https://oreil.ly/0GmzF">use cases</a>. Let’s have a
look at how the most important workload-level networking issues can be addressed
using a service mesh.</p>








<section data-type="sect2" data-pdf-bookmark="Concept"><div class="sect2" id="idm45302813490720">
<h2>Concept</h2>

<p>A service mesh <a data-type="indexterm" data-primary="service meshes" data-secondary="overview" id="idm45302813489392"/><a data-type="indexterm" data-primary="workloads" data-secondary="service meshes" data-tertiary="overview" id="idm45302813488384"/>as conceptually shown in <a data-type="xref" href="#fig-networking-service-mesh-concept">Figure 5-4</a>
is, as per its <a href="https://oreil.ly/Zo8Xk">creators</a>, a collection of userspace
proxies in front of your apps along with a management process to configure said proxies.</p>

<figure><div id="fig-networking-service-mesh-concept" class="figure">
<img src="Images/haku_0504.png" alt="Service mesh concept" width="1013" height="523"/>
<h6><span class="label">Figure 5-4. </span>Service mesh concept</h6>
</div></figure>

<p class="pagebreak-before">The proxies are <a data-type="indexterm" data-primary="proxies" data-secondary="service meshes" id="idm45302813482704"/>referred to as the service mesh’s <em>data plane</em>, and the
management process as its <em>control plane</em>. The proxies intercept calls between
services and do something interesting with or to these calls, such as
disallowing a certain communication path or collecting metrics from the call.
The control plane, on the other hand, coordinates the behavior of the proxies and
provides the administrator an API.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Options and Uptake"><div class="sect2" id="idm45302813480144">
<h2>Options and Uptake</h2>

<p>At the time of <a data-type="indexterm" data-primary="service meshes" data-secondary="adoption" id="idm45302813478640"/><a data-type="indexterm" data-primary="workloads" data-secondary="service meshes" data-tertiary="adoption" id="idm45302813477632"/>writing, a number of <a href="https://oreil.ly/DeWKV">service meshes</a> exist as well as proposed quasi-standards for interoperability, such as the CNCF project
<a href="https://oreil.ly/2s1H4">Service Mesh Interface</a> or work of the Envoy-based <a href="https://oreil.ly/3xQ3m">Universal Data Plane API Working Group</a> (UDPA-WG).</p>

<p>While it is early days, we witness certain uptake, especially out of security considerations (see  <a data-type="xref" href="#fig-networking-service-mesh-survey">Figure 5-5</a>). For example, The New Stack (TNS) reports in its 2020 <a href="https://oreil.ly/m0Kk7">Service Mesh survey</a>:</p>
<blockquote>
<p>A third of respondents’ organizations are using service meshes to control
communications traffic between microservices in production Kubernetes environments.
Another 34% use service mesh technology in a test environment, or are piloting
or actively evaluating solutions.</p></blockquote>

<figure><div id="fig-networking-service-mesh-survey" class="figure">
<img src="Images/haku_0505.png" alt="TNS 2020 service mesh survey excerpt" width="1256" height="636"/>
<h6><span class="label">Figure 5-5. </span>TNS 2020 service mesh survey excerpt</h6>
</div></figure>

<p>Going forward, many exciting application areas and nifty defense mechanisms
based on service meshes are possible—for example,
<a href="https://oreil.ly/WxRpf">Identity Federation for Multi-Cluster Kubernetes and Service Mesh</a>
or using <a href="https://oreil.ly/2KacR">OPA in Istio</a>. That said, many
end users are not yet ready to go all in and/or are in a holding pattern,
waiting for cloud and platform providers to make the data plane of the service
mesh part of the underlying infrastructure. Alternatively, the data plane may
be implemented on the operating system level, for example, using
<a href="https://ebpf.io">eBPF</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Case Study: mTLS with Linkerd"><div class="sect2" id="idm45302813465824">
<h2>Case Study: mTLS with Linkerd</h2>

<p>Linkerd<a data-type="indexterm" data-primary="Linkerd" data-secondary="installing" id="idm45302813463808"/><a data-type="indexterm" data-primary="service meshes" data-secondary="mTLS with Linkerd" id="servmesh_Linkerd"/><a data-type="indexterm" data-primary="workloads" data-secondary="service meshes" data-tertiary="adoption" id="idm45302813461584"/> is a graduated CNCF project, originally created by Buoyant.</p>

<p>Linkerd <a href="https://oreil.ly/vGiXq">automatically enables</a>
mutual Transport Layer Security (mTLS)<a data-type="indexterm" data-primary="mutual Transport Layer Security (mTLS)" data-see="mTLS (mutual Transport Layer Security)" id="idm45302813459088"/><a data-type="indexterm" data-primary="mTLS (mutual Transport Layer Security)" id="idm45302813458032"/> for most HTTP-based communication
between meshed pods. Let’s see that in action.</p>

<p>To follow along, <a href="https://oreil.ly/crGez">install Linkerd</a>
in a test cluster. We’re using <code>kind</code> in the following example and assume you have
both the Kubernetes cluster set up and configured as well as the Linkerd CLI:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>linkerd check --pre
kubernetes-api
...
Status check results are √</pre>

<p>Now that we know that we’re in a position to install Linkerd, let’s go ahead
 and do it:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>linkerd install <code class="p">|</code> kubectl apply -f -
namespace/linkerd created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-identity created
...
deployment.apps/linkerd-grafana created</pre>

<p>And finally verify the install:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>linkerd check
kubernetes-api
...
Status check results are √</pre>

<p>Great! All up and running. You could have a quick look at the Linkerd dashboard using
<code>linkerd dashboard &amp;</code>, which should show something similar to what’s depicted in <a data-type="xref" href="#fig-networking-linkerd-dashboard">Figure 5-6</a>.</p>

<p>OK, back<a data-type="indexterm" data-primary="Linkerd" data-secondary="dashboard" id="idm45302813389648"/> to mTLS: once we have enabled the mesh in the respective namespaces
it should be impossible for us, even from within the cluster, to directly talk
to a service using, say <code>curl</code>, and doing an HTTP query. Let’s see how that works.</p>

<p>In the following example, we’re reusing the setup from <a data-type="xref" href="#workload-networking-inter-pod">“Inter-Pod Traffic”</a>
but you can really use any workload that exposes an HTTP service within the
cluster.</p>

<figure><div id="fig-networking-linkerd-dashboard" class="figure">
<img src="Images/haku_0506.png" alt="Linkerd dashboard showing example traffic stats" width="1227" height="1024"/>
<h6><span class="label">Figure 5-6. </span>Linkerd dashboard showing example traffic stats</h6>
</div></figure>

<p>First, we need to enable <a data-type="indexterm" data-primary="Linkerd" data-secondary="meshes, enabling" id="idm45302813417072"/>the mesh, or meshify, as the good folks from Buoyant
call it:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl get -n npdemo deploy -o yaml <code class="p">|</code> <code class="se">\</code>
          linkerd inject - <code class="p">|</code> kubectl apply -f -


<code class="nv">$ </code>kubectl get -n ambassador deploy -o yaml <code class="p">|</code> <code class="se">\</code>
          linkerd inject - <code class="p">|</code> kubectl apply -f -</pre>

<p>Now we can <a href="https://oreil.ly/RsUhF">validate</a> our mTLS
setup <a data-type="indexterm" data-primary="Linkerd" data-secondary="mTLS, validating" id="idm45302813356000"/><a data-type="indexterm" data-primary="mTLS (mutual Transport Layer Security)" data-secondary="validating" id="idm45302813355056"/>using <a href="https://oreil.ly/0JAvk">tshark</a> as follows:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>curl -sL https://run.linkerd.io/emojivoto.yml <code class="p">|</code>
  linkerd inject --enable-debug-sidecar - <code class="p">|</code>
  kubectl apply -f -
namespace <code class="s2">"emojivoto"</code> injected
...
deployment.apps/web created</pre>

<p>Once the sample app is up and running we can use an <a data-type="indexterm" data-primary="Linkerd" data-secondary="remote shells" id="idm45302813369056"/><a data-type="indexterm" data-primary="remote shells, Linkerd" id="idm45302813368208"/>remote shell into the attached
debug container that Linkerd kindly put there for us:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code><code>kubectl</code><code> </code><code>-n</code><code> </code><code>emojivoto</code><code> </code><code class="nb">exec</code><code> </code><code>-it</code><code> </code><code class="se">\ </code><a class="co" id="co_networking_CO4-1" href="#callout_networking_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
  </code><code class="k">$(</code><code>kubectl</code><code> </code><code>-n</code><code> </code><code>emojivoto</code><code> </code><code>get</code><code> </code><code>po</code><code> </code><code>-o</code><code> </code><code>name</code><code> </code><code class="p">|</code><code> </code><code>grep</code><code> </code><code>voting</code><code class="k">)</code><code> </code><code class="se">\ </code><a class="co" id="co_networking_CO4-2" href="#callout_networking_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
  </code><code>-c</code><code> </code><code>linkerd-debug</code><code> </code><code>--</code><code> </code><code>/bin/bash</code><code> </code><a class="co" id="co_networking_CO4-3" href="#callout_networking_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_networking_CO4-1" href="#co_networking_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Connect to pod for interactive (terminal) use.</p></dd>
<dt><a class="co" id="callout_networking_CO4-2" href="#co_networking_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Provide pod name for the <code>exec</code> command.</p></dd>
<dt><a class="co" id="callout_networking_CO4-3" href="#co_networking_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Target the <code>linkerd-debug</code> container in the pod.</p></dd>
</dl>

<p>Now, from within the debug container we use <code>tshark</code> to inspect <a data-type="indexterm" data-primary="packets, inspecting" id="idm45302813256000"/><a data-type="indexterm" data-primary="TLS (Transport Layer Security)" data-secondary="packets, inspecting" id="idm45302813255504"/>the packets on the
NIC and expect to see TLS traffic (output edited to fit):</p>

<pre data-type="programlisting" data-code-language="bash"><code>root@voting-57bc56-s4l:/#</code><code> </code><code>tshark</code><code> </code><code>-i</code><code> </code><code>any</code><code> </code><code class="se">\ </code><a class="co" id="co_networking_CO5-1" href="#callout_networking_CO5-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
                                 </code><code>-d</code><code> </code><code>tcp.port</code><code class="o">=</code><code class="o">=</code><code>8080,ssl</code><code> </code><code class="p">|</code><code> </code><a class="co" id="co_networking_CO5-2" href="#callout_networking_CO5-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
                          </code><code>grep</code><code> </code><code>-v</code><code> </code><code>127.0.0.1</code><code> </code><a class="co" id="co_networking_CO5-3" href="#callout_networking_CO5-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>

</code><code>Running</code><code> </code><code>as</code><code> </code><code>user</code><code> </code><code class="s2">"root"</code><code> </code><code>and</code><code> </code><code>group</code><code> </code><code class="s2">"root."</code><code> </code><code>This</code><code> </code><code>could</code><code> </code><code>be</code><code> </code><code>dangerous.</code><code>
</code><code>Capturing</code><code> </code><code>on</code><code> </code><code class="s1">'any'</code><code>

 </code><code class="m">1</code><code> </code><code>0.000000000</code><code> </code><code>192.168.49.192</code><code> </code><code>→</code><code> </code><code>192.168.49.231</code><code> </code><code>TCP</code><code> </code><code class="m">76</code><code> </code><code class="m">41704</code><code> </code><code>→</code><code> </code><code class="m">4191</code><code> </code><code class="o">[</code><code>SYN</code><code class="o">]</code><code> </code><code class="nv">Seq</code><code class="o">=</code><code>0...</code><code>
 </code><code class="m">2</code><code> </code><code>0.000023419</code><code> </code><code>192.168.49.231</code><code> </code><code>→</code><code> </code><code>192.168.49.192</code><code> </code><code>TCP</code><code> </code><code class="m">76</code><code> </code><code class="m">4191</code><code> </code><code>→</code><code> </code><code class="m">41704</code><code> </code><code class="o">[</code><code>SYN,</code><code> </code><code>ACK</code><code class="o">]</code><code>...</code><code>
 </code><code class="m">3</code><code> </code><code>0.000041904</code><code> </code><code>192.168.49.192</code><code> </code><code>→</code><code> </code><code>192.168.49.231</code><code> </code><code>TCP</code><code> </code><code class="m">68</code><code> </code><code class="m">41704</code><code> </code><code>→</code><code> </code><code class="m">4191</code><code> </code><code class="o">[</code><code>ACK</code><code class="o">]</code><code> </code><code class="nv">Seq</code><code class="o">=</code><code>1...</code><code>
 </code><code class="m">4</code><code> </code><code>0.000356637</code><code> </code><code>192.168.49.192</code><code> </code><code>→</code><code> </code><code>192.168.49.231</code><code> </code><code>HTTP</code><code> </code><code class="m">189</code><code> </code><code>GET</code><code> </code><code>/ready</code><code> </code><code>HTTP/1.1</code><code>
 </code><code class="m">5</code><code> </code><code>0.000397207</code><code> </code><code>192.168.49.231</code><code> </code><code>→</code><code> </code><code>192.168.49.192</code><code> </code><code>TCP</code><code> </code><code class="m">68</code><code> </code><code class="m">4191</code><code> </code><code>→</code><code> </code><code class="m">41704</code><code> </code><code class="o">[</code><code>ACK</code><code class="o">]</code><code> </code><code class="nv">Seq</code><code class="o">=</code><code>1...</code><code>
 </code><code class="m">6</code><code> </code><code>0.000483689</code><code> </code><code>192.168.49.231</code><code> </code><code>→</code><code> </code><code>192.168.49.192</code><code> </code><code>HTTP</code><code> </code><code class="m">149</code><code> </code><code>HTTP/1.1</code><code> </code><code class="m">200</code><code> </code><code>OK</code><code>
 </code><code>...</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_networking_CO5-1" href="#co_networking_CO5-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Listen on all available network interfaces for live packet capture.</p></dd>
<dt><a class="co" id="callout_networking_CO5-2" href="#co_networking_CO5-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Decode any traffic running over port <code>8080</code> as TLS.</p></dd>
<dt><a class="co" id="callout_networking_CO5-3" href="#co_networking_CO5-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Ignoring <code>127.0.0.1</code> (localhost) as this traffic will always be unencrypted.</p></dd>
</dl>

<p>Yay, it works, <a data-type="indexterm" data-primary="encryption" data-secondary="mTLS" id="idm45302813032768"/>encryption on the wire for free! And with this we’ve completed
the mTLS case study.</p>

<p>If you want to learn more about how to use service meshes to secure
your East-West communication, we have put together some suggested further
reading in <a data-type="xref" href="app02.xhtml#apx-networking">“Networking”</a> in <a data-type="xref" href="app02.xhtml#appendix-resources">Appendix B</a>.</p>

<p>While service meshes certainly can help you with networking-related security
challenges, fending off the Captain and their crew, you should be aware
of weaknesses. For example, from Envoy-based systems, if you run a container
with UID 1337, it bypasses the Istio/Envoy sidecar or, by default, the Envoy
admin dashboard is accessible from within the container because it shares
a network. For more background on this topic, check out the in-depth
<a href="https://oreil.ly/1LKUg">Istio Security Assessment</a>.</p>

<p>Now it’s time to move on to the last part of the workload networking topic:
what happens on a single worker<a data-type="indexterm" data-primary="service meshes" data-secondary="mTLS with Linkerd" data-startref="servmesh_Linkerd" id="idm45302813027296"/> node.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="eBPF"><div class="sect1" id="workload-networking-bpf">
<h1>eBPF</h1>

<p>After the service mesh <a data-type="indexterm" data-primary="eBPF" data-secondary="overview" id="idm45302813024448"/>adventure, we focus our attention now on a topic
that is on the one hand entirely of opposite character and on the other
hand can also be viewed and understood to be used in the service mesh data plane.
We have a look at eBPF, a modern and powerful way to extend the Linux kernel,
and with it you can address a number of networking-related security challenges.</p>








<section data-type="sect2" data-pdf-bookmark="Concept"><div class="sect2" id="idm45302813022800">
<h2>Concept</h2>

<p>Originally, this piece of Linux kernel technology was known under the name
Berkeley Packet Filter (BPF). Then it experienced a number of enhancements,
mainly dirven by Google, Facebook, and Netflix, and to distinguish it from the
original implementation it was called <a href="https://oreil.ly/xIhBB">eBPF</a>.
Nowadays, the kernel project and technology is commonly known as eBPF, which
is a term in itself and does not stand for anything per se; that is, it’s not considered an acronym any longer.</p>

<p>Technically, eBPF is a feature of the Linux kernel and you’ll need the<a data-type="indexterm" data-primary="Linux" data-secondary="kernel, extending" id="Lin_kern_ext"/><a data-type="indexterm" data-primary="kernel (Linux), extending" id="kern_ext"/> Linux kernel
version 3.18 or above to benefit from it. It enables you to safely and efficiently extend
the Linux kernel functions by using the <code>bpf(2)</code> syscall (see also the
<a href="https://oreil.ly/YurzV">man pages</a> for details). eBPF
is implemented as an in-kernel virtual machine using a custom 64-bit RISC instruction set.</p>

<p>In <a data-type="xref" href="#fig-bpf-overview">Figure 5-7</a> you see a high-level overview taken from Brendan Gregg’s
<a href="https://oreil.ly/n7jqx"><em>Linux Extended BPF (eBPF) Tracing Tools</em> (Addison-Wesley)</a> .</p>

<figure><div id="fig-bpf-overview" class="figure">
<img src="Images/haku_0507.png" alt="eBPF overview in the Linux kernel" width="1261" height="429"/>
<h6><span class="label">Figure 5-7. </span>eBPF overview in the Linux kernel</h6>
</div></figure>

<p>This all looks promising, but is eBPF already used in the wild, and also, which
options are available? Let’s take a look.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Options and Uptake"><div class="sect2" id="idm45302813011088">
<h2>Options and Uptake</h2>

<p>In 2021, eBPF is already used <a data-type="indexterm" data-primary="eBPF" data-secondary="adoption" id="idm45302813009632"/>in a number of places and for use cases such as:</p>

<ul>
<li>
<p>In Kubernetes, as a CNI plug-in to enable, for example, pod networking in <a href="https://oreil.ly/mjYG3">Cilium</a> and Project Calico, as well as for service scalability (in the context of <code>kube-proxy</code>)</p>
</li>
<li>
<p>For observability, like for Linux kernel tracing such as with <a href="https://oreil.ly/Qk7G5">iovisor/bpftrace</a> as well as in a clustered setup with <a href="https://oreil.ly/Pqzra">Hubble</a></p>
</li>
<li>
<p>As a security control, for example to perform container runtime scanning as you can use with projects such as <a href="https://falco.org">CNCF Falco</a>, but also for enforcing network policies in Kubernetes (via Cilium, Calico, etc.) as discussed in <a data-type="xref" href="#workload-network-policies">“Traffic Flow Control”</a></p>
</li>
<li>
<p>Network load balancing like Facebook’s L4 <a href="https://oreil.ly/csPLr">katran</a> library</p>
</li>
<li>
<p>Low-level intrusion detection systems (IDS) for Kubernetes (see <a data-type="xref" href="ch09.xhtml#ch-intrusion-detection">Chapter 9</a> for details)</p>
</li>
</ul>

<p>We see an increasing number of players entering the eBPF field and leading the
charge is <a href="https://isovalent.com">Isovalent</a>. While it’s still early days from an adoption perspective,
eBPF has a huge potential. Coming back to the service mesh data plane: it is
perfectly doable and thinkable to implement the Envoy APIs as a set of eBPF
programs and push the handling from user space sidecar proxy into the kernel.</p>

<p>Extending the kernel with userspace programs sounds interesting, but how does
that look, in practice?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Case Study: Attaching a Probe to a Go Program"><div class="sect2" id="idm45302812995920">
<h2>Case Study: Attaching a Probe to a Go Program</h2>

<p>Let’s have a look at an example from the <a href="https://oreil.ly/MPvHJ">Cilium</a>
project. The following is a Go<a data-type="indexterm" data-primary="Go (programming language)" data-secondary="eBPM example" id="Go_eBPMex"/> program available in
<a href="https://oreil.ly/I0L8S">main.go</a> and
demonstrates how you can attach an eBPF program (written in C) to a kernel symbol.
The overall result of the exercise is that whenever the <code>sys_execve</code> syscall is
invoked, a kernel counter is increased, which the Go program then reads and
prints out the number of times the probed symbol has been called per second.</p>

<p>The following line in <em>main.go</em> (edited to fit the page; should all be
on the same line) instructs the Go toolchain to include the compiled C program that
contains our eBPF code:</p>

<pre data-type="programlisting" data-code-language="go"><code class="c1">//go:generate go run github.com/cilium/ebpf/cmd/bpf2go</code>
  <code class="o">-</code><code class="nx">cc</code> <code class="nx">clang</code><code class="o">-</code><code class="mi">11</code> <code class="nx">KProbeExample</code> <code class="p">.</code><code class="o">/</code><code class="nx">bpf</code><code class="o">/</code><code class="nx">kprobe_example</code><code class="p">.</code><code class="nx">c</code> <code class="o">--</code> <code class="o">-</code><code class="nx">I</code><code class="p">..</code><code class="o">/</code><code class="nx">headers</code></pre>

<p>In
<a href="https://oreil.ly/BmGIb">kprobe_example.c</a>
we find the eBPF program itself:</p>

<pre data-type="programlisting" data-code-language="c"><code class="cp">#</code><code class="cp">include "common.h"</code><code class="cp">
</code><code class="cp">#</code><code class="cp">include "bpf_helpers.h"</code><code class="cp">
</code><code>
</code><code class="kt">char</code><code> </code><code class="n">__license</code><code class="p">[</code><code class="p">]</code><code> </code><code class="n">SEC</code><code class="p">(</code><code class="s">"</code><code class="s">license</code><code class="s">"</code><code class="p">)</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Dual MIT/GPL</code><code class="s">"</code><code class="p">;</code><code> </code><a class="co" id="co_networking_CO6-1" href="#callout_networking_CO6-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>

</code><code class="k">struct</code><code> </code><code class="n">bpf_map_def</code><code> </code><code class="n">SEC</code><code class="p">(</code><code class="s">"</code><code class="s">maps</code><code class="s">"</code><code class="p">)</code><code> </code><code class="n">kprobe_map</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code> </code><a class="co" id="co_networking_CO6-2" href="#callout_networking_CO6-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
    </code><code class="p">.</code><code class="n">type</code><code> </code><code class="o">=</code><code> </code><code class="n">BPF_MAP_TYPE_ARRAY</code><code class="p">,</code><code>
</code><code>    </code><code class="p">.</code><code class="n">key_size</code><code> </code><code class="o">=</code><code> </code><code class="k">sizeof</code><code class="p">(</code><code class="n">u32</code><code class="p">)</code><code class="p">,</code><code>
</code><code>    </code><code class="p">.</code><code class="n">value_size</code><code> </code><code class="o">=</code><code> </code><code class="k">sizeof</code><code class="p">(</code><code class="n">u64</code><code class="p">)</code><code class="p">,</code><code>
</code><code>    </code><code class="p">.</code><code class="n">max_entries</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code class="p">,</code><code>
</code><code class="p">}</code><code class="p">;</code><code>
</code><code>
</code><code class="n">SEC</code><code class="p">(</code><code class="s">"</code><code class="s">kprobe/sys_execve</code><code class="s">"</code><code class="p">)</code><code>
</code><code class="kt">int</code><code> </code><code class="n">kprobe_execve</code><code class="p">(</code><code class="p">)</code><code> </code><code class="p">{</code><code> </code><a class="co" id="co_networking_CO6-3" href="#callout_networking_CO6-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
    </code><code class="n">u32</code><code> </code><code class="n">key</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code class="p">;</code><code>
</code><code>    </code><code class="n">u64</code><code> </code><code class="n">initval</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="o">*</code><code class="n">valp</code><code class="p">;</code><code>
</code><code>
</code><code>    </code><code class="n">valp</code><code> </code><code class="o">=</code><code> </code><code class="n">bpf_map_lookup_elem</code><code class="p">(</code><code class="o">&amp;</code><code class="n">kprobe_map</code><code class="p">,</code><code> </code><code class="o">&amp;</code><code class="n">key</code><code class="p">)</code><code class="p">;</code><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="p">(</code><code class="o">!</code><code class="n">valp</code><code class="p">)</code><code> </code><code class="p">{</code><code>
</code><code>        </code><code class="n">bpf_map_update_elem</code><code class="p">(</code><code class="o">&amp;</code><code class="n">kprobe_map</code><code class="p">,</code><code> </code><code class="o">&amp;</code><code class="n">key</code><code class="p">,</code><code> </code><code class="o">&amp;</code><code class="n">initval</code><code class="p">,</code><code> </code><code class="n">BPF_ANY</code><code class="p">)</code><code class="p">;</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="mi">0</code><code class="p">;</code><code>
</code><code>    </code><code class="p">}</code><code>
</code><code>    </code><code class="n">__sync_fetch_and_add</code><code class="p">(</code><code class="n">valp</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">)</code><code class="p">;</code><code>
</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="mi">0</code><code class="p">;</code><code>
</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_networking_CO6-1" href="#co_networking_CO6-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>You must define a license.</p></dd>
<dt><a class="co" id="callout_networking_CO6-2" href="#co_networking_CO6-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Enables exchange of data between kernel and userspace.</p></dd>
<dt><a class="co" id="callout_networking_CO6-3" href="#co_networking_CO6-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>The entry point of our eBPF probe (program).</p></dd>
</dl>

<p>As you can guess, writing eBPF by hand is not fun. Luckily there are a number
of great tools and environments available that take care of the low-level stuff
for you.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Just as we were wrapping up the book writing, the Linux Foundation announced
that Facebook, Google, Isovalent, Microsoft, and Netflix joined together
to <a href="https://oreil.ly/9JkNz">create the eBPF Foundation</a>,
and with it giving the eBPF project <a data-type="indexterm" data-primary="eBPF" data-secondary="eBPF Foundation" id="idm45302812772400"/>a vendor-neutral home. Stay tuned!</p>
</div>

<p>To dive deeper into the eBPF topic we suggest you read
<a href="https://oreil.ly/jbERf"><em>Linux Observability with BPF</em></a> by David Calavera and Lorenzo Fontana (O’Reilly).
If you’re looking for a quick overview, Matt Oswalt has a nice
<a href="https://oreil.ly/kROpT">Introduction to eBPF</a>.</p>

<p>To stay on top of things, have a look at <a href="https://ebpf.io">ebpf.io</a> and check out
what the community publishes on the <a href="https://oreil.ly/wfFnN">YouTube channel</a> for
this topic.</p>

<p>Further, have a look at <a href="https://px.dev">Pixie</a>, an open source, eBPF-based observability tool with an
active community and broad industry support (see <a data-type="xref" href="#fig-pixie-example">Figure 5-8</a>).</p>

<figure><div id="fig-pixie-example" class="figure">
<img src="Images/haku_0508.png" alt="Pixie in action" width="2670" height="2398"/>
<h6><span class="label">Figure 5-8. </span>Pixie in action</h6>
</div></figure>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45302812995296">
<h1>Conclusion</h1>

<p>Summing up, <a data-type="indexterm" data-primary="Linux" data-secondary="kernel" data-tertiary="extending" data-startref="Lin_kern_ext" id="idm45302812725456"/><a data-type="indexterm" data-primary="kernel (Linux), extending" data-startref="kern_ext" id="idm45302812724128"/><a data-type="indexterm" data-primary="Go (programming language)" data-secondary="eBPM example" data-startref="Go_eBPMex" id="idm45302812723280"/>there are a number of defaults in the Kubernetes networking space
you want to be aware of. As a baseline, you can apply the good practices you
know from a noncontainerized environment in combination with intrusion
detection tooling as shown in <a data-type="xref" href="ch09.xhtml#ch-intrusion-detection">Chapter 9</a>. In addition, you
want to use native resources such as network policies potentially in combination with other
CNCF projects such as SPIFFE for workload identity to strengthen your security posture.</p>

<p>Service meshes, while still in the early days, are another promising option to enforce policies
and gain insights into what is going on. Last but not least, eBPF is the up and coming
star in the networking arena, enabling a number of security-related use cases.</p>

<p>Now that we have the networking secured, we are ready for the Captain to move
on to more “solid” grounds: storage.</p>
</div></section>







</div></section></div></body></html>