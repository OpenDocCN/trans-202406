- en: Chapter 14\. Introduction to Sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers how to scale with MongoDB. We’ll look at:'
  prefs: []
  type: TYPE_NORMAL
- en: What sharding is and the components of a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to configure sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of how sharding interacts with your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Is Sharding?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Sharding* refers to the process of splitting data up across machines; the
    term *partitioning* is also sometimes used to describe this concept. By putting
    a subset of data on each machine, it becomes possible to store more data and handle
    more load without requiring larger or more powerful machines—just a larger quantity
    of less-powerful machines. Sharding may be used for other purposes as well, including
    placing more frequently accessed data on more performant hardware or splitting
    a dataset based on geography to locate a subset of documents in a collection (e.g.,
    for users based in a particular locale) close to the application servers from
    which they are most commonly accessed.'
  prefs: []
  type: TYPE_NORMAL
- en: Manual sharding can be done with almost any database software. With this approach,
    an application maintains connections to several different database servers, each
    of which are completely independent. The application manages storing different
    data on different servers and querying against the appropriate server to get data
    back. This setup can work well but becomes difficult to maintain when adding or
    removing nodes from the cluster or in the face of changing data distributions
    or load patterns.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB supports autosharding, which tries to both abstract the architecture
    away from the application and simplify the administration of such a system. MongoDB
    allows your application to ignore the fact that it isn’t talking to a standalone
    MongoDB server, to some extent. On the operations side, MongoDB automates balancing
    data across shards and makes it easier to add and remove capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding is the most complex way of configuring MongoDB, both from a development
    and an operational point of view. There are many components to configure and monitor,
    and data moves around the cluster automatically. You should be comfortable with
    standalone servers and replica sets before attempting to deploy or use a sharded
    cluster. Also, as with replica sets, the recommended means of configuring and
    deploying sharded clusters is through either MongoDB Ops Manager or MongoDB Atlas.
    Ops Manager is recommended if you need to maintain control of your computing infrastructure.
    MongoDB Atlas is recommended if you can leave the infrastructure management to
    MongoDB (you have the option of running in Amazon AWS, Microsoft Azure, or Google
    Compute Cloud).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Components of a Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MongoDB’s sharding allows you to create a cluster of many machines (shards)
    and break up a collection across them, putting a subset of data on each shard.
    This allows your application to grow beyond the resource limits of a standalone
    server or replica set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many people are confused about the difference between replication and sharding.
    Remember that replication creates an exact copy of your data on multiple servers,
    so every server is a mirror image of every other server. Conversely, every shard
    contains a different subset of data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the goals of sharding is to make a cluster of 2, 3, 10, or even hundreds
    of shards look like a single machine to your application. To hide these details
    from the application, we run one or more routing processes called a *mongos* in
    front of the shards. A *mongos* keeps a “table of contents” that tells it which
    shard contains which data. Applications can connect to this router and issue requests
    normally, as shown in [Figure 14-1](#sharded_client_connection). The router, knowing
    what data is on which shard, is able to forward the requests to the appropriate
    shard(s). If there are responses to a request the router collects them and, if
    necessary, merges them, and sends them back to the application. As far as the
    application knows, it’s connected to a standalone *mongod*, as illustrated in
    [Figure 14-2](#nonsharded_client_connection).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Sharded client connection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. Nonsharded client connection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sharding on a Single-Machine Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll start by setting up a quick cluster on a single machine. First, start
    a *mongo* shell with the `--nodb` and `--norc` options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a cluster, use the `ShardingTest` class. Run the following in the
    *mongo* shell you just launched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `chunksize` option is covered in [Chapter 17](ch17.xhtml#chapter-sharding-admin).
    For now, simply set it to `1`. As for the other options passed to `ShardingTest`
    here, `name` simply provides a label for our sharded cluster, `shards` specifies
    that our cluster will be composed of two shards (we do this to keep the resource
    requirements low for this example), and `rs` defines each shard as a three-node
    replica set with an `oplogSize` of 10 MiB (again, to keep resource shard as a
    three-node replica set with an `oplogSize` of 10 MiB (again, to keep resource
    utilization low). Though it is possible to run one standalone *mongod* for each
    shard, it paints a clearer picture of the typical architecture of a sharded cluster
    if we create each shard as a replica set. In the last option specified, we are
    instructing `ShardingTest` to enable the balancer once the cluster is spun up.
    This will ensure that data is evenly distributed across both shards.
  prefs: []
  type: TYPE_NORMAL
- en: '`ShardingTest` is a class designed for internal use by MongoDB Engineering
    and is therefore undocumented externally. However, because it ships with the MongoDB
    server, it provides the most straightforward means of experimenting with a sharded
    cluster. `ShardingTest` was originally designed to support server test suites
    and is still used for this purpose. By default it provides a number of conveniences
    that help in keeping resource utilization as small as possible and in setting
    up the relatively complex architecture of a sharded cluster. It makes an assumption
    about the presence of a `/data /db` directory on your machine; if `ShardingTest`
    fails to run then create this directory and rerun the command again.'
  prefs: []
  type: TYPE_NORMAL
- en: When you run this command, `ShardingTest` will do a lot for you automatically.
    It will create a new cluster with two shards, each of which is a replica set.
    It will configure the replica sets and launch each node with the necessary options
    to establish replication protocols. It will launch a *mongos* to manage requests
    across the shards so that clients can interact with the cluster as if communicating
    with a standalone *mongod*, to some extent. Finally, it will launch an additional
    replica set for the config servers that maintain the routing table information
    necessary to ensure queries are directed to the correct shard. Remember that the
    primary use cases for sharding are to split a dataset to address hardware and
    cost constraints or to provide better performance to applications (e.g., geographical
    partitioning). MongoDB sharding provides these capabilities in a way that is seamless
    to the application in many respects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once `ShardingTest` has finished setting up your cluster, you will have 10
    processes up and running to which you can connect: two replica sets of three nodes
    each, one config server replica set of three nodes, and one *mongos*. By default,
    these processes should begin at port 20000\. The *mongos* should be running at
    port 20009\. Other processes you have running on your local machine and previous
    calls to `ShardingTest` can have an effect on which ports `ShardingTest` uses,
    but you should not have too much difficulty determining the ports on which your
    cluster processes are running.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you’ll connect to the *mongos* to play around with the cluster. Your
    entire cluster will be dumping its logs to your current shell, so open up a second
    terminal window and launch another *mongo* shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Use this shell to connect to your cluster’s *mongos*. Again, your *mongos*
    should be running on port 20009:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the prompt in your *mongo* shell should change to reflect that you
    are connected to a *mongos*. Now you are in the situation shown earlier, in [Figure 14-1](#sharded_client_connection):
    the shell is the client and is connected to a *mongos*. You can start passing
    requests to the *mongos* and it’ll route them to the shards. You don’t really
    have to know anything about the shards, like how many there are or what their
    addresses are. So long as there are some shards out there, you can pass the requests
    to the *mongos* and allow it to forward them appropriately.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by inserting some data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, interacting with *mongos* works the same way as interacting
    with a standalone server does.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get an overall view of your cluster by running `sh.status()`. It will
    give you a summary of your shards, databases, and collections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`sh` is similar to `rs`, but for sharding: it is a global variable that defines
    a number of sharding helper functions, which you can see by running `sh.help()`.
    As you can see from the `sh.status()` output, you have two shards and two databases
    (*config* is created automatically).'
  prefs: []
  type: TYPE_NORMAL
- en: Your *accounts* database may have a different primary shard than the one shown
    here. A primary shard is a “home base” shard that is randomly chosen for each
    database. All of your data will be on this primary shard. MongoDB cannot automatically
    distribute your data yet because it doesn’t know how (or if) you want it to be
    distributed. You have to tell it, per collection, how you want it to distribute
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A primary shard is different from a replica set primary. A primary shard refers
    to the entire replica set composing a shard. A primary in a replica set is the
    single server in the set that can take writes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To shard a particular collection, first enable sharding on the collection’s
    database. To do so, run the `enableSharding` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now sharding is enabled on the *accounts* database, which allows you to shard
    collections within the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you shard a collection, you choose a shard key. This is a field or two
    that MongoDB uses to break up data. For example, if you chose to shard on `"username"`,
    MongoDB would break up the data into ranges of usernames: `"a1-steak-sauce"` through
    `"defcon"`, `"defcon1"` through `"howie1998"`, and so on. Choosing a shard key
    can be thought of as choosing an ordering for the data in the collection. This
    is a similar concept to indexing, and for good reason: the shard key becomes the
    most important index on your collection as it gets bigger. To even create a shard
    key, the field(s) must be indexed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, before enabling sharding, you have to create an index on the key you want
    to shard by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can shard the collection by `"username"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Although we are choosing a shard key without much thought here, it is an important
    decision that should be carefully considered in a real system. See [Chapter 16](ch16.xhtml#chapter-shardkey)
    for more advice on choosing a shard key.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wait a few minutes and run `sh.status()` again, you’ll see that there’s
    a lot more information displayed than there was before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The collection has been split up into 13 chunks, where each chunk is a subset
    of your data. These are listed by shard key range (the ``{"username" : *`minValue`*}
    -->> {"username" : *`maxValue`*}`` denotes the range of each chunk). Looking at
    the ``"on" : *`shard`*`` part of the output, you can see that these chunks have
    been evenly distributed between the shards.'
  prefs: []
  type: TYPE_NORMAL
- en: This process of a collection being split into chunks is shown graphically in
    Figures [14-3](#figure-unsplit-coll) through [14-5](#figure-dist-coll). Before
    sharding, the collection is essentially a single chunk. Sharding splits it into
    smaller chunks based on the shard key, as shown in [Figure 14-4](#figure-split-coll).
    These chunks can then be distributed across the cluster, as [Figure 14-5](#figure-dist-coll)
    shows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-3\. Before a collection is sharded, it can be thought of as a single
    chunk from the smallest value of the shard key to the largest
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-4\. Sharding splits the collection into many chunks based on shard
    key ranges
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-5\. Chunks are evenly distributed across the available shards
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice the keys at the beginning and end of the chunk list: `$minKey` and `$maxKey`.
    `$minKey` can be thought of as “negative infinity.” It is smaller than any other
    value in MongoDB. Similarly, `$maxKey` is like “positive infinity.” It is greater
    than any other value. Thus, you’ll always see these as the “caps” on your chunk
    ranges. The values for your shard key will always be between `$minKey` and `$maxKey`.
    These values are actually BSON types and should not be used in your application;
    they are mainly for internal use. If you wish to refer to them in the shell, use
    the `MinKey` and `MaxKey` constants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the data is distributed across multiple shards, let’s try doing some
    queries. First, try a query on a specific username:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, querying works normally. However, let’s run an `explain` to
    see what MongoDB is doing under the covers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the `"winningPlan"` field in the `explain` output, we can see that our
    cluster satisfied this query using a single shard, *one-min-shards-rs0*. Based
    on the output of `sh.status()` shown earlier, we can see that *user12345* does
    fall within the key range for the first chunk listed for that shard in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because `"username"` is the shard key, *mongos* was able to route the query
    directly to the correct shard. Contrast that with the results for querying for
    all of the users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this `explain`, this query has to visit both shards to find
    all the data. In general, if we are not using the shard key in the query, *mongos*
    will have to send the query to every shard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Queries that contain the shard key and can be sent to a single shard or a subset
    of shards are called *targeted queries*. Queries that must be sent to all shards
    are called *scatter-gather* (broadcast) queries: *mongos* scatters the query to
    all the shards and then gathers up the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are finished experimenting, shut down the set. Switch back to your
    original shell and hit Enter a few times to get back to the command line, then
    run `st.stop()` to cleanly shut down all of the servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you are ever unsure of what an operation will do, it can be helpful to use
    `ShardingTest` to spin up a quick local cluster and try it out.
  prefs: []
  type: TYPE_NORMAL
