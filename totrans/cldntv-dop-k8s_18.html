<html><head></head><body><section data-pdf-bookmark="Chapter 16. Metrics in Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="metrics">&#13;
<h1><span class="label">Chapter 16. </span>Metrics in Kubernetes</h1>&#13;
&#13;
<blockquote class="epigraph">&#13;
<p>It is possible to know so much about a subject that you become totally ignorant.</p>&#13;
<p data-type="attribution">Frank Herbert, <cite><em>Chapterhouse: Dune</em></cite></p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="metrics" data-type="indexterm" id="ix_16-metrics-adoc0"/>In this chapter, we’ll take the concept of metrics that we introduced in <a data-type="xref" href="ch15.html#observability">Chapter 15</a> and dive into the details for Kubernetes: what kind of metrics are there, which ones are important for cloud native services, how do you choose which metrics to focus on, how do you analyze metrics data to get actionable information, and how do you turn raw metrics data into useful dashboards and alerts? Finally, we’ll outline some of the options for metrics tools and platforms.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Are Metrics, Really?" data-type="sect1"><div class="sect1" id="idm45979374298624">&#13;
<h1>What Are Metrics, Really?</h1>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="uses" data-type="indexterm" id="idm45979374297312"/>As we saw in <a data-type="xref" href="ch15.html#metrics-intro">“Introducing Metrics”</a>, metrics are numerical measures of specific things. A familiar example from the world of traditional servers is the memory usage of a particular machine. If only 10% of physical memory is currently allocated to user processes, the machine has spare capacity. But if 90% of the memory is in use, the machine is probably pretty busy.</p>&#13;
&#13;
<p>So one valuable kind of information that metrics can give us is a snapshot of what’s going on at a particular instant. But we can do more. Memory usage goes up and down all the time as workloads start and stop, but sometimes what we’re interested in is the <em>change</em> in memory usage over time.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Time-Series Data" data-type="sect2"><div class="sect2" id="idm45979374293824">&#13;
<h2>Time-Series Data</h2>&#13;
&#13;
<p><a data-primary="time series" data-type="indexterm" id="idm45979374292624"/>If you sample memory usage regularly, you can create a <em>time series</em> of that data. <a data-type="xref" href="#img-timeseries">Figure 16-1</a> shows a graph of the time-series data for memory usage on a Google Kubernetes Engine node, over one week. This gives a much more intelligible picture of what’s happening than a handful of instantaneous values.</p>&#13;
&#13;
<figure><div class="figure" id="img-timeseries">&#13;
<img alt="Graph showing fluctuating memory usage" src="assets/cnd2_1601.png"/>&#13;
<h6><span class="label">Figure 16-1. </span>Time-series graph of memory usage for a GKE node</h6>&#13;
</div></figure>&#13;
&#13;
<p>Most metrics that we’re interested in for cloud native observability purposes are expressed as time series. They are also all numeric. Unlike log data, for example, metrics are values that you can do math and statistics on.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Counters and Gauges" data-type="sect2"><div class="sect2" id="idm45979374287328">&#13;
<h2>Counters and Gauges</h2>&#13;
&#13;
<p><a data-primary="counters" data-type="indexterm" id="idm45979374285952"/><a data-primary="gauges" data-type="indexterm" id="idm45979374285248"/>What kind of numbers are they? While some quantities can be represented by integers (the number of physical CPUs in a machine, for example), most require a decimal part, and to save having to handle two different types of numbers, metrics are almost always represented as floating-point decimal values.</p>&#13;
&#13;
<p>Given that, there are two main types of metric values: <em>counters</em> and <em>gauges</em>. Counters can only go up (or reset to zero); they’re suitable for measuring things like number of requests served and number of errors received. Gauges, on the other hand, can vary up and down; they’re useful for continuously varying quantities like memory usage, or for expressing ratios of other quantities.</p>&#13;
&#13;
<p>The answers to some questions are just yes or no: whether a particular endpoint is responding to HTTP connections, for example. In this case, the appropriate metric will be a gauge with a limited range of values: 0 and 1, perhaps.</p>&#13;
&#13;
<p>For example, an HTTP check of an endpoint might be named something like <code>http.can_connect</code>, and its value might be 1 when the endpoint is responding, and 0 otherwise.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Can Metrics Tell Us?" data-type="sect2"><div class="sect2" id="idm45979374281200">&#13;
<h2>What Can Metrics Tell Us?</h2>&#13;
&#13;
<p>What use are metrics? Well, as we’ve seen earlier in this chapter, metrics can tell you when things are broken. For example, if your error rate suddenly goes up (or requests to your support page suddenly spike), that may indicate a problem. You can generate alerts automatically for certain metrics based on a threshold.</p>&#13;
&#13;
<p>But metrics can also tell you how well things are working, for example, how many simultaneous users your application is currently supporting. Long-term trends in these numbers can be useful for operations decision-making and for business <span class="keep-together">intelligence</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Choosing Good Metrics" data-type="sect1"><div class="sect1" id="idm45979374278096">&#13;
<h1>Choosing Good Metrics</h1>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="choosing" data-type="indexterm" id="idm45979374276224"/>At first, you might think “If metrics are good, then lots of metrics must be even better!” But it doesn’t work that way. You can’t monitor everything. Google Cloud’s Operations suite, for example, captures data for literally hundreds of built-in metrics about your cloud resources, including:</p>&#13;
<dl>&#13;
<dt><code>instance/network/sent_packets_count</code></dt>&#13;
<dd>&#13;
<p>The number of network packets sent by each compute instance</p>&#13;
</dd>&#13;
<dt><code>storage/object_count</code></dt>&#13;
<dd>&#13;
<p>The total number of objects in each storage bucket</p>&#13;
</dd>&#13;
<dt><code>container/cpu/utilization</code></dt>&#13;
<dd>&#13;
<p>The percentage of its CPU allocation that a container is currently using</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The list <a href="https://oreil.ly/QBgM0">goes on</a> (and on, and on). Even if you could display graphs of all these metrics at once, which would need a monitor screen the size of a house, you’d never be able to take in all that information and deduce anything useful from it. To do that, we need to <em>focus</em> on the subset of metrics that we care about.</p>&#13;
&#13;
<p>So what should you focus on when observing your own applications? Only you can answer that, but we have a few suggestions that may be helpful. In the rest of this section, we’ll outline some common metrics patterns for observability, aimed at different audiences and designed to meet different requirements.</p>&#13;
&#13;
<p>It’s worth saying that this is a perfect opportunity for some DevOps collaboration, and you should start thinking and talking about what metrics you’ll need at the beginning of development, not at the end (see <a data-type="xref" href="ch01.html#learningtogether">“Learning Together”</a>).</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Services: The RED Pattern" data-type="sect2"><div class="sect2" id="redpattern">&#13;
<h2>Services: The RED Pattern</h2>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="RED pattern" data-type="indexterm" id="idm45979374263552"/><a data-primary="RED (Requests-Errors-Duration) pattern" data-type="indexterm" id="idm45979374262576"/><a data-primary="Requests-Errors-Duration (RED) pattern" data-type="indexterm" id="idm45979374261840"/>Most people using Kubernetes are running some kind of web service: users make requests, and the application sends responses. The <em>users</em> could be programs or other services; in a distributed system based on microservices, each service makes requests to other services or to a central API gateway server and uses the results to serve information back to yet more services. Either way, it’s a request-driven system.</p>&#13;
&#13;
<p>What’s useful to know about a request-driven system?</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>One obvious thing is the number of <em>requests</em> you’re getting.</p>&#13;
</li>&#13;
<li>&#13;
<p>Another is the number of requests that failed in various ways; that is to say, the number of <em>errors</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>A third useful metric is the <em>duration</em> of each request. This gives you an idea how well your service is performing and how unhappy your users might be getting.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The <em>Requests-Errors-Duration</em> (RED) pattern is a classic observability tool that goes back to the earliest days of online services. <a href="https://oreil.ly/zD72v">Google’s <em>Site Reliability Engineering</em> book</a> talks about the Four Golden Signals, which are essentially requests, errors, duration, and <em>saturation</em> (we’ll talk about saturation in a moment).</p>&#13;
&#13;
<p>Engineer Tom Wilkie, who coined the <em>RED</em> acronym, has outlined the rationale behind this pattern in a blog post:</p>&#13;
<blockquote>&#13;
<p>Why should you measure the same metrics for every service? Surely each service is special? The benefits of treating each service the same, from a monitoring perspective, is scalability in your operations teams. By making every service look, feel and taste the same, this reduces the cognitive load on those responding to an incident. As an aside, if you treat all your services the same, many repetitive tasks become automatable.</p>&#13;
<p data-type="attribution">Tom Wilkie</p>&#13;
</blockquote>&#13;
&#13;
<p>So how exactly do we measure these numbers? Since the total number of requests only ever goes up, it’s more useful to look at request <em>rate</em>: the number of requests per second, for example. This gives us a meaningful idea of how much traffic the system is handling over a given time interval.</p>&#13;
&#13;
<p>Because error rate is related to request rate, it’s a good idea to measure errors as a percentage of requests. So, for example, a typical service dashboard might show:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Requests received per second</p>&#13;
</li>&#13;
<li>&#13;
<p>Percentage of requests that returned an error</p>&#13;
</li>&#13;
<li>&#13;
<p>Duration of requests (also known as <em>latency</em>)</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resources: The USE Pattern" data-type="sect2"><div class="sect2" id="usepattern">&#13;
<h2>Resources: The USE Pattern</h2>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="USE pattern" data-type="indexterm" id="idm45979374242576"/><a data-primary="USE (Utilization, Saturation, Errors) pattern" data-type="indexterm" id="idm45979374241600"/>You’ve seen that the RED pattern gives you useful information about how your services are performing, and how your users are experiencing them. You could think of this as a top-down way of looking at observability data.</p>&#13;
&#13;
<p>On the other hand, the <a href="https://oreil.ly/PpB0f">USE pattern</a>, developed by Netflix performance engineer Brendan Gregg, is a bottom-up approach that is intended to help analyze performance issues and find bottlenecks. USE stands for Utilization, Saturation, and Errors.</p>&#13;
&#13;
<p>Rather than services, with USE we’re interested in <em>resources</em>: lower-level infrastructure server components such as CPU and disks, or network interfaces and links. Any of these could be a bottleneck in system performance, and the USE metrics will help us find out which:</p>&#13;
<dl>&#13;
<dt>Utilization</dt>&#13;
<dd>&#13;
<p>The average time that the resource was busy serving requests, or the amount of resource capacity that’s currently in use. For example, a disk that is 90% full would have a utilization of 90%.</p>&#13;
</dd>&#13;
<dt>Saturation</dt>&#13;
<dd>&#13;
<p>The extent to which the resource is overloaded, or the length of the queue of requests waiting for this resource to become available. For example, if there are 10 processes waiting to run on a CPU, it has a saturation value of 10.</p>&#13;
</dd>&#13;
<dt>Errors</dt>&#13;
<dd>&#13;
<p>The number of times an operation on that resource failed. For example, a disk with some bad sectors might have an error count of 25 failed reads.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Measuring this data for the key resources in your system is a good way to spot bottlenecks and potential upcoming problems. Resources with low utilization, no saturation, and no errors are probably fine. Anything that deviates from this is worth looking into. For example, if one of your network links is saturated, or has a high number of errors, it may be contributing to overall performance problems:</p>&#13;
<blockquote>&#13;
<p>The USE Method is a simple strategy you can use to perform a complete check of system health, identifying common bottlenecks and errors. It can be deployed early in the investigation and quickly identify problem areas, which then can be studied in more detail other methodologies, if need be.</p>&#13;
&#13;
<p>The strength of USE is its speed and visibility: by considering all resources, you are unlikely to overlook any issues. It will, however, only find certain types of issues–bottlenecks and errors–and should be considered as one tool in a larger toolbox.</p>&#13;
<p data-type="attribution">Brendan Gregg</p>&#13;
</blockquote>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Business Metrics" data-type="sect2"><div class="sect2" id="businessmetrics">&#13;
<h2>Business Metrics</h2>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="business" data-type="indexterm" id="idm45979374228256"/>We’ve looked at application and service metrics (<a data-type="xref" href="#redpattern">“Services: The RED Pattern”</a>), which are likely to be of most interest to developers, and infrastructure metrics (<a data-type="xref" href="#usepattern">“Resources: The USE Pattern”</a>), which are helpful to ops and platform engineers. But what about the business? Can observability help managers and executives understand how the business is performing and give them useful input for business decisions? And what metrics would contribute to this?</p>&#13;
&#13;
<p><a data-primary="key performance indicators (KPIs)" data-type="indexterm" id="idm45979374224960"/><a data-primary="KPIs (key performance indicators)" data-type="indexterm" id="idm45979374224288"/>Most businesses already track the key performance indicators (KPIs) that matter to them, such as sales revenue, profit margin, and cost of customer acquisition. These metrics usually come from the finance department and don’t need support from developers and infrastructure staff.</p>&#13;
&#13;
<p>But there are other useful business metrics that can be generated by applications and services. For example, a subscription business, such as a software-as-a-service (SaaS) product, needs to know data about its subscribers:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Funnel analytics (how many people hit the landing page, how many click through to the sign-up page, how many complete the transaction, and so on)</p>&#13;
</li>&#13;
<li>&#13;
<p>Rate of sign-ups and cancellations (churn)</p>&#13;
</li>&#13;
<li>&#13;
<p>Revenue per customer (useful for calculating monthly recurring revenue, average revenue per customer, and lifetime value of a customer)</p>&#13;
</li>&#13;
<li>&#13;
<p>Effectiveness of help and support pages (for example, percentage of people who answered yes to the question “Did this page solve your problem?”)</p>&#13;
</li>&#13;
<li>&#13;
<p>Traffic to the <em>system status</em> announcement page (which often spikes when there are outages or degraded services)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Much of this information is often easier to gather by generating real-time metrics data from your applications, rather than by trying to analyze after-the-fact by processing logs and querying databases. When you’re instrumenting your applications to produce metrics, don’t neglect information that is important to the business.</p>&#13;
&#13;
<p>There isn’t necessarily a clear line between the observability information the business and customer engagement experts need, and what the technical experts need. In fact, there’s a lot of overlap. It’s wise to discuss metrics at an early stage with all the stakeholders involved, and agree on what data needs to be collected, how often, how it’s aggregated, and so on.</p>&#13;
&#13;
<p>Nonetheless, these two (or more) groups have different questions to ask of the observability data that you’re gathering, so each will need its own view on that data. You can use the common <em>data lake</em> to create dashboards (see <a data-type="xref" href="#dashboards">“Graphing Metrics with Dashboards”</a>) and reports for each of the different groups involved.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Metrics" data-type="sect2"><div class="sect2" id="k8smetrics">&#13;
<h2>Kubernetes Metrics</h2>&#13;
&#13;
<p><a data-primary="Kubernetes" data-secondary="metrics" data-type="indexterm" id="idm45979374211232"/>We’ve talked about observability and metrics in general terms, and looked at different types of data and ways to analyze it. So how does all this apply to Kubernetes? What metrics is it worth tracking for Kubernetes clusters, and what kinds of decisions can they help us make?</p>&#13;
&#13;
<p><a data-primary="cAdvisor" data-type="indexterm" id="idm45979374209584"/>At the lowest level, a tool called <code>cAdvisor</code> monitors the resource usage and performance statistics for the containers running on each cluster node—for example, how much CPU, memory, and disk space each container is using.</p>&#13;
&#13;
<p>Kubernetes itself consumes this <code>cAdvisor</code> data by querying the kubelet, and uses the information to make decisions about scheduling, autoscaling, and so on. But you can also export this data to a third-party metrics service, where you can graph it and alert on it. For example, it would be useful to track how much CPU and memory each container is using.</p>&#13;
&#13;
<p><a data-primary="kube-state-metrics" data-type="indexterm" id="idm45979374207056"/>You can also monitor Kubernetes itself, using a tool called <a href="https://oreil.ly/ZW25p"><code>kube-state-metrics</code></a>. This listens to the Kubernetes API and reports information about logical objects such as nodes, Pods, and Deployments. This data can also be very useful for cluster observability. For example, if there are replicas configured for a Deployment that can’t currently be scheduled for some reason (perhaps the cluster doesn’t have enough capacity), you probably want to know about it.</p>&#13;
&#13;
<p>As usual, the problem is not a shortage of metrics data, but deciding which key metrics to focus on, track, and visualize. Here are some suggestions.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster health metrics" data-type="sect3"><div class="sect3" id="idm45979374204160">&#13;
<h3>Cluster health metrics</h3>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="health metrics" data-type="indexterm" id="idm45979374202784"/><a data-primary="metrics" data-secondary="cluster health" data-type="indexterm" id="idm45979374201808"/>To monitor the health and performance of your cluster at the top level, you should be looking at least at the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Number of nodes</p>&#13;
</li>&#13;
<li>&#13;
<p>Node health status</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of Pods per node, and overall</p>&#13;
</li>&#13;
<li>&#13;
<p>Resource usage/allocation per node, and overall</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>These overview metrics will help you understand how your cluster is performing, whether it has enough capacity, how its usage is changing over time, and whether you need to expand or reduce the cluster.</p>&#13;
&#13;
<p>If you’re using a managed Kubernetes service such as GKE, unhealthy nodes will be detected automatically and autorepaired (providing autorepair is enabled for your cluster and node pool). It’s still useful to know if you’re getting an unusual number of failures, which may indicate an underlying problem.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deployment metrics" data-type="sect3"><div class="sect3" id="idm45979374195088">&#13;
<h3>Deployment metrics</h3>&#13;
&#13;
<p><a data-primary="deployments" data-secondary="metrics for" data-type="indexterm" id="idm45979374193888"/><a data-primary="metrics" data-secondary="deployments" data-type="indexterm" id="idm45979374192912"/>For all your deployments, it’s worth knowing:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Number of deployments</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of configured replicas per deployment</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of unavailable replicas per deployment</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>It’s especially useful to be able to track this information over time if you have enabled some of the various autoscaling options available in Kubernetes (see <a data-type="xref" href="ch06.html#autoscaling">“Autoscaling”</a>). Data on unavailable replicas in particular will help alert you about capacity issues.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Container metrics" data-type="sect3"><div class="sect3" id="idm45979374186896">&#13;
<h3>Container metrics</h3>&#13;
&#13;
<p><a data-primary="containers" data-secondary="metrics for" data-type="indexterm" id="idm45979374185728"/><a data-primary="metrics" data-secondary="containers" data-type="indexterm" id="idm45979374184752"/>At the container level, the most useful things to know are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Number of containers/Pods per node, and overall</p>&#13;
</li>&#13;
<li>&#13;
<p>Resource usage for each container against its requests/limits (see <a data-type="xref" href="ch05.html#resourcerequests">“Resource Requests”</a>)</p>&#13;
</li>&#13;
<li>&#13;
<p>Liveness/readiness of containers</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of container/Pod restarts</p>&#13;
</li>&#13;
<li>&#13;
<p>Network in/out traffic and errors for each container</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Because Kubernetes automatically restarts containers that have failed or exceeded their resource limits, you need to know how often this is happening. An excessive number of restarts may tell you there’s a problem with a particular container. If a container is regularly busting its resource limits, that could be a sign of a program bug, or maybe just that you need to increase the limits, such as giving it more memory.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Application metrics" data-type="sect3"><div class="sect3" id="idm45979374176528">&#13;
<h3>Application metrics</h3>&#13;
&#13;
<p><a data-primary="application deployment" data-secondary="metrics for" data-type="indexterm" id="idm45979374174624"/><a data-primary="metrics" data-secondary="application" data-type="indexterm" id="idm45979374173648"/>Whichever language or software platform your application uses, there’s probably a library or tool available to allow you to export custom metrics from it. These are primarily useful for developers and operations teams to be able to see what the application is doing, how often it’s doing it, and how long it takes. These are key indicators of performance problems or availability issues.</p>&#13;
&#13;
<p>The choice of application metrics to capture and export depends on exactly what your application does. But there are some common patterns. For example, if your service consumes messages from a queue, processes them, and takes some action based on the message, you might want to report the following metrics:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Number of messages received</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of successfully processed messages</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of messages still in the queue waiting to be processed</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of invalid or erroneous messages</p>&#13;
</li>&#13;
<li>&#13;
<p>Time to process and act on each message</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of successful actions generated</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of failed actions</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Similarly, if your application is primarily request-driven, you can use the RED pattern (see <a data-type="xref" href="#redpattern">“Services: The RED Pattern”</a>):</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Requests received</p>&#13;
</li>&#13;
<li>&#13;
<p>Errors returned</p>&#13;
</li>&#13;
<li>&#13;
<p>Duration (time to handle each request)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>It can be difficult to know what metrics are going to be useful when you’re at an early stage of development. If in doubt, record everything. Metrics are relatively cheap for most applications to output and for time-series databases to store; you may discover an unforeseen production issue a long way down the line, thanks to metrics data that didn’t seem important at the time.</p>&#13;
<blockquote data-type="quote">&#13;
<p>If it moves, graph it. Even it doesn’t move, graph it anyway, because it might someday.</p>&#13;
<p data-type="attribution"><a href="https://oreil.ly/HTMse">Laurie Denness</a> (Bloomberg)</p>&#13;
</blockquote>&#13;
&#13;
<p>If you are going to have your application generate business metrics (see <a data-type="xref" href="#businessmetrics">“Business Metrics”</a>), you can calculate and export these as custom metrics too.</p>&#13;
&#13;
<p>Another thing that may be useful to the business is to see how your applications are performing against any Service Level Objectives (SLO) or Service Level Agreements (SLA) that you may have with customers, or how vendor services are performing against SLOs. You could create a custom metric to show the target request duration (for example, 200 ms), and create a dashboard that overlays this on the actual current performance.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Runtime metrics" data-type="sect3"><div class="sect3" id="idm45979374175872">&#13;
<h3>Runtime metrics</h3>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="runtime" data-type="indexterm" id="idm45979374153408"/>At the runtime level, most metrics libraries will also report useful data about what the program is doing, such as:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Number of processes/threads/goroutines</p>&#13;
</li>&#13;
<li>&#13;
<p>Heap and stack usage</p>&#13;
</li>&#13;
<li>&#13;
<p>Nonheap memory usage</p>&#13;
</li>&#13;
<li>&#13;
<p>Network I/O buffer pools</p>&#13;
</li>&#13;
<li>&#13;
<p>Garbage collector runs and pause durations (for garbage-collected languages)</p>&#13;
</li>&#13;
<li>&#13;
<p>File descriptors/network sockets in use</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This kind of information can be very valuable for diagnosing poor performance, or even crashes. For example, it’s quite common for long-running applications to gradually use more and more memory until they are killed and restarted due to exceeding Kubernetes resource limits. Application runtime metrics may help you work out exactly where this memory is going, especially in combination with custom metrics about what the application is doing.</p>&#13;
&#13;
<p>Now that you have some idea what metrics data is worth capturing, in the next section we’ll look at what to <em>do</em> with this data: in other words, how to analyze it.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Analyzing Metrics" data-type="sect1"><div class="sect1" id="idm45979374277600">&#13;
<h1>Analyzing Metrics</h1>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="analyzing" data-type="indexterm" id="ix_16-metrics-adoc1"/>Data is not the same thing as understanding. In order to get useful information out of the raw data we’ve captured, we need to aggregate, process, and analyze it, which means doing <em>statistics</em> on it. Statistics can be a slippery business, especially in the abstract, so let’s illustrate this discussion with a concrete example: <em>request duration</em>.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#redpattern">“Services: The RED Pattern”</a>, we mentioned that you should track the duration metric for service requests, but we didn’t say exactly how to do that. What precisely do we mean by <em>duration</em>? Usually, we’re interested in the time the user has to wait to get a response to some request.</p>&#13;
&#13;
<p>With a website, for example, we might define <em>duration</em> as the time between when the user connects to the server and when the server first starts sending data in response. (The user’s total waiting time is actually longer than that because making the connection takes some time, and so does reading the response data and rendering it in a browser. We usually don’t have access to that data, though, so we just capture what we can.)</p>&#13;
&#13;
<p>And every request has a different duration, so how do we aggregate the data for hundreds or even thousands of requests into a single number?</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What’s Wrong with a Simple Average?" data-type="sect2"><div class="sect2" id="idm45979374136800">&#13;
<h2>What’s Wrong with a Simple Average?</h2>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="averages" data-type="indexterm" id="idm45979374135280"/>The obvious answer is to take the average. But, on closer inspection, what <em>average</em> means isn’t necessarily straightforward. An old joke in statistics is that the average person has slightly less than two legs. To put it another way, most people have more than the average number of legs. How can this be?</p>&#13;
&#13;
<p>Most people have two legs, but some have one or none, bringing down the overall average. (Possibly some people have more than two, but many more people have fewer than two.) A simple average doesn’t give us much useful information about leg distribution in the population, or about most people’s experience of leg ownership.</p>&#13;
&#13;
<p>There is also more than one kind of average. You probably know that the commonplace notion of <em>average</em> refers to the <em>mean</em>. The mean of a set of values is the total of all the values, divided by the number of values. For example, the mean age of a group of three people is the total of their ages divided by 3.</p>&#13;
&#13;
<p><a data-primary="median" data-type="indexterm" id="idm45979374130800"/>The <em>median</em>, on the other hand, refers to the value that would divide the set into two equal halves, one containing values larger than the median, and the other containing smaller values. For example, in any group of people, half of them are taller than the median height, by definition, and half of them are shorter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Means, Medians, and Outliers" data-type="sect2"><div class="sect2" id="idm45979374129104">&#13;
<h2>Means, Medians, and Outliers</h2>&#13;
&#13;
<p>What’s the problem with taking a straightforward average (mean) of request duration? One important problem is that the mean is easily skewed by <em>outliers</em>: one or two extreme values can distort the average quite a bit.</p>&#13;
&#13;
<p>Therefore, the median, which is less affected by outliers, is a more helpful way of averaging metrics than the mean. If the median latency for a service is one second, half your users experience a latency less than one second, and half experience more.</p>&#13;
&#13;
<p><a data-type="xref" href="#img-anscombe-quartet">Figure 16-2</a> shows how averages can be misleading. All four sets of data have the same mean value, but look very different when shown graphically (statisticians know this example as <em>Anscombe’s quartet</em>). Incidentally, this is also a good way to demonstrate the importance of graphing data, rather than just looking at raw numbers.</p>&#13;
&#13;
<figure><div class="figure" id="img-anscombe-quartet">&#13;
<img alt="Scatter plots of four datasets with the same mean" src="assets/cnd2_1602.png"/>&#13;
<h6><span class="label">Figure 16-2. </span>These four datasets all have the same average (mean) value <a href="https://oreil.ly/ieutR">(image</a> by Schutz, CC BY-SA 3.0).</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discovering Percentiles" data-type="sect2"><div class="sect2" id="idm45979374121392">&#13;
<h2>Discovering Percentiles</h2>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="percentiles" data-type="indexterm" id="idm45979374120192"/><a data-primary="percentiles" data-type="indexterm" id="idm45979374119216"/>When we talk about metrics for observing request-driven systems, we’re usually interested in knowing what the <em>worst</em> latency experience is for users, rather than the average. After all, having a median latency of 1 second for all users is no comfort to the small group who may be experiencing latencies of 10 seconds or more.</p>&#13;
&#13;
<p>The way to get this information is to break down the data into <em>percentiles</em>. The 90th percentile latency (often referred to as <em>P90</em>) is the value that is greater than that experienced by 90% of your users. To put it another way, 10% of users will experience a latency higher than the P90 value.</p>&#13;
&#13;
<p>Expressed in this language, the median is the 50th percentile, or P50. Other percentiles that are often measured in observability are P95 and P99, the 95th and 99th percentile, respectively.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Applying Percentiles to Metrics Data" data-type="sect2"><div class="sect2" id="idm45979374115472">&#13;
<h2>Applying Percentiles to Metrics Data</h2>&#13;
&#13;
<p>Igor Wiedler of Travis CI has produced a nice <a href="https://igor.io/latency">demonstration</a> of what this means in concrete terms, starting from a dataset of 135,000 requests to a production service over 10 minutes (<a data-type="xref" href="#img-latency-raw">Figure 16-3</a>). As you can see, this data is noisy and spiky, and it’s not easy to draw any useful conclusions from it in a raw state.</p>&#13;
&#13;
<figure><div class="figure" id="img-latency-raw">&#13;
<img alt="Graph of raw latency values" src="assets/cnd2_1603.png"/>&#13;
<h6><span class="label">Figure 16-3. </span>Raw latency data for 135,000 requests, in ms</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now let’s see what happens if we average that data over 10-second intervals (<a data-type="xref" href="#img-latency-avg">Figure 16-4</a>). This looks wonderful: all the data points are below 50 ms. So it looks as though most of our users are experiencing latencies of less than 50 ms. But is that really true?</p>&#13;
&#13;
<figure><div class="figure" id="img-latency-avg">&#13;
<img alt="Graph of average latency values (all very low)" src="assets/cnd2_1604.png"/>&#13;
<h6><span class="label">Figure 16-4. </span>Average (mean) latency for the same data, over 10-second intervals</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s graph the P99 latency instead. This is the maximum latency observed, if we discard the highest 1% of samples. It looks very different (<a data-type="xref" href="#img-latency-p99">Figure 16-5</a>). Now we see a jagged pattern with most of the values clustering between 0 and 500 ms, with several requests spiking close to 1,000 ms.</p>&#13;
&#13;
<figure><div class="figure" id="img-latency-p99">&#13;
<img alt="Graph of P99 latency (spiking up to 1 second)" src="assets/cnd2_1605.png"/>&#13;
<h6><span class="label">Figure 16-5. </span>P99 (99th percentile) latency for the same data</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="We Usually Want to Know the Worst" data-type="sect2"><div class="sect2" id="idm45979374102768">&#13;
<h2>We Usually Want to Know the Worst</h2>&#13;
&#13;
<p>Since we disproportionately notice slow web requests, the P99 data is likely to give us a more realistic picture of the latency experienced by users. For example, consider a high-traffic website with 1 million page views per day. If the P99 latency is 10 seconds, then 10,000 page views take longer than 10 seconds. That’s a lot of unhappy users.</p>&#13;
&#13;
<p>But it gets worse: in distributed systems, each page view may require tens or even hundreds of internal requests to fulfill. If the P99 latency of each internal service is 10s, and 1 page view makes 10 internal requests, then the number of slow page views rises to 100,000 per day. Now around 10% of users are unhappy, which is a <a href="https://oreil.ly/zO9HV">big problem</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Beyond Percentiles" data-type="sect2"><div class="sect2" id="idm45979374099296">&#13;
<h2>Beyond Percentiles</h2>&#13;
&#13;
<p>One problem with percentile latencies, as implemented by many metrics services, is that requests tend to be sampled locally, and statistics then aggregated centrally. Consequently, you end up with your P99 latency being an average of the P99 latencies reported by each agent, potentially across hundreds of agents.</p>&#13;
&#13;
<p>Well, a percentile is already an average, and trying to average averages is a well-known <a href="https://oreil.ly/ZHCMU">statistical trap</a>.<sup><a data-type="noteref" href="ch16.html#idm45979374096544" id="idm45979374096544-marker">1</a></sup> The result is not necessarily the same as the real average.</p>&#13;
&#13;
<p>Depending how we choose to aggregate the data, the final P99 latency figure can vary by as much as a factor of 10. That doesn’t bode well for a meaningful result. Unless your metrics service ingests every single raw event and produces a true average, this figure will be unreliable.</p>&#13;
&#13;
<p>Engineer <a href="https://oreil.ly/XgFDV">Yan Cui</a> suggests that a better approach is to monitor what’s <em>wrong</em>, not what’s <em>right</em>:</p>&#13;
<blockquote>&#13;
<p>What could we use instead of per­centiles as the pri­ma­ry met­ric to mon­i­tor our application’s per­for­mance with and alert us when it starts to dete­ri­o­rate?</p>&#13;
&#13;
<p>If you go back to your SLOs or SLAs, you prob­a­bly have some­thing along the lines of “99% of requests should com­plete in 1s or less.” In oth­er words, less than 1% of requests is allowed to take more than 1s to com­plete.</p>&#13;
&#13;
<p>So what if we mon­i­tor the per­cent­age of requests that are over the thresh­old instead? To alert us when our SLAs are vio­lat­ed, we can trig­ger alarms when that per­cent­age is greater than 1% over some pre­de­fined time win­dow.</p>&#13;
<p data-type="attribution">Yan Cui</p>&#13;
</blockquote>&#13;
&#13;
<p>If each agent submits a metric of total requests and the number of requests that were over threshold, we <em>can</em> usefully average that data to produce a percentage of requests that exceeded SLO—and alert on it.<a data-startref="ix_16-metrics-adoc1" data-type="indexterm" id="idm45979374088160"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Graphing Metrics with Dashboards" data-type="sect1"><div class="sect1" id="dashboards">&#13;
<h1>Graphing Metrics with Dashboards</h1>&#13;
&#13;
<p><a data-primary="dashboards" data-type="indexterm" id="ix_16-metrics-adoc2"/><a data-primary="metrics" data-secondary="dashboards" data-type="indexterm" id="ix_16-metrics-adoc3"/>So far in this chapter we’ve learned about why metrics are useful, what metrics we should record, and some useful statistical techniques for analyzing them in bulk. All well and good, but what are we actually going to <em>do</em> with all these metrics?</p>&#13;
&#13;
<p>The answer is simple: we’re going to graph them, group them into dashboards, and possibly alert on them. We’ll talk about alerting in the next section, but for now, let’s look at some tools and techniques for graphing and dashboarding.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Use a Standard Layout for All Services" data-type="sect2"><div class="sect2" id="idm45979374081808">&#13;
<h2>Use a Standard Layout for All Services</h2>&#13;
&#13;
<p><a data-primary="dashboards" data-secondary="layout of" data-type="indexterm" id="idm45979374080416"/>When you have more than a handful of services, it makes sense to always lay out your dashboards in the same way for each service. Someone responding to an on-call page can glance at the dashboard for the affected service and know how to interpret it immediately, without having to be familiar with that specific service.</p>&#13;
&#13;
<p>Tom Wilkie, in a <a href="https://oreil.ly/GTzpX">Weaveworks blog post</a>, suggests the following standard format (see <a data-type="xref" href="#img-dashboard-2">Figure 16-6</a>):</p>&#13;
&#13;
<ul class="less_space pagebreak-before">&#13;
<li>&#13;
<p>One row per service</p>&#13;
</li>&#13;
<li>&#13;
<p>Request and error rate on the left, with errors as a percentage of requests</p>&#13;
</li>&#13;
<li>&#13;
<p>Latency on the right</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<figure><div class="figure" id="img-dashboard-2">&#13;
<img alt="Dashboard screenshot" src="assets/cnd2_1606.png"/>&#13;
<h6><span class="label">Figure 16-6. </span>Weaveworks’ suggested dashboard layout for services</h6>&#13;
</div></figure>&#13;
&#13;
<p>You don’t have to use this exact layout; the important thing is that you always use the same layout for every dashboard, and that everyone is familiar with it. You should review your key dashboards regularly (at least once a week), looking at the previous week’s data, so that everyone knows what <em>normal</em> looks like.</p>&#13;
&#13;
<p>The <em>requests, errors, duration</em> dashboard works well for services (see <a data-type="xref" href="#redpattern">“Services: The RED Pattern”</a>). For resources, such as cluster nodes, disks, and networks, the most useful things to know are usually <em>utilization, saturation, errors</em> (see <a data-type="xref" href="#usepattern">“Resources: The USE Pattern”</a>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Build an Information Radiator with Primary Dashboards" data-type="sect2"><div class="sect2" id="radiator">&#13;
<h2>Build an Information Radiator with Primary Dashboards</h2>&#13;
&#13;
<p><a data-primary="dashboards" data-secondary="primary dashboards" data-type="indexterm" id="idm45979374065472"/><a data-primary="information radiator" data-type="indexterm" id="idm45979374064496"/><a data-primary="metrics" data-secondary="radiator" data-type="indexterm" id="idm45979374063824"/>If you have a hundred services, you have a hundred dashboards, but you probably won’t look at them very often. It’s still important to have that information available (to help spot which service is failing, for example), but at this scale you need a more general overview.</p>&#13;
&#13;
<p>To do this, make a primary dashboard that shows requests, errors, and duration across <em>all</em> your services, in aggregate. Don’t do anything fancy like stacked area charts; stick to simple line graphs of total requests, total error percentage, and total latency. These are easier to interpret, and more accurate visualizations than complex charts.</p>&#13;
&#13;
<p>Ideally, you’ll be using an <em>information radiator</em> (also known as a wallboard, or Big Visible Chart). This is a large screen showing key observability data that is visible to everybody in the relevant team or office. Or, for distributed teams, maybe this is the homepage of the monitoring website that everyone sees when they first log in. The purpose of an information radiator is:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>To show the current system status at a glance</p>&#13;
</li>&#13;
<li>&#13;
<p>To send a clear message about which metrics the team considers important</p>&#13;
</li>&#13;
<li>&#13;
<p>To make people familiar with what <em>normal</em> looks like</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>What should you include on this radiator screen? Only vital information. <em>Vital</em>, in the sense of <em>really important</em>, but also in the sense of <em>vital signs</em>: information that tells you about the life of the system.</p>&#13;
&#13;
<p>The vital signs monitors you’ll see next to a hospital bed are a good example. They show the key metrics for human beings: heart rate, blood pressure, oxygen saturation, temperature, and breathing rate. There are many other metrics you could track for a patient, and they have medically important uses, but at the primary dashboard level, these are the key ones. Any serious medical problem will show up in one or more of these metrics; everything else is a matter of diagnostics.</p>&#13;
&#13;
<p>Similarly, your information radiator should show the vital signs of your business or service. If it has numbers, it should probably have no more than four or five numbers. If it has graphs, it should have no more than four or five graphs.</p>&#13;
&#13;
<p>It’s tempting to cram too much information into a dashboard so it looks complicated and technical. That’s not the goal. The goal is to focus on a few key things and make them easily visible from across a room (see <a data-type="xref" href="#img-radiator">Figure 16-7</a>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dashboard Things That Break" data-type="sect2"><div class="sect2" id="idm45979374051664">&#13;
<h2>Dashboard Things That Break</h2>&#13;
&#13;
<p><a data-primary="dashboards" data-secondary="for specific services" data-secondary-sortas="specific" data-type="indexterm" id="idm45979374050272"/>Apart from your main information radiator, and dashboards for individual services and resources, you may want to create dashboards for specific metrics that tell you important things about the system. You might be able to think of some of these things already, based on the system architecture. But another useful source of information is <em>things that break</em>.</p>&#13;
&#13;
<figure><div class="figure" id="img-radiator">&#13;
<img alt="Dashboard showing request and latency data" src="assets/cnd2_1607.png"/>&#13;
<h6><span class="label">Figure 16-7. </span>Example information radiator produced by <a href="https://oreil.ly/lyEwc">Grafana Dash Gen</a></h6>&#13;
</div></figure>&#13;
&#13;
<p>Every time you have an incident or outage, look for a metric, or combination of metrics, which would have alerted you to this problem in advance. For example, if you have a production outage caused by a server running out of disk space, it’s possible that a graph of disk space on that server would have warned you beforehand that the available space was trending downward and heading into outage territory.</p>&#13;
&#13;
<p>We’re not talking here about problems that happen over a period of minutes or even hours; those are usually caught by automated alerts (see <a data-type="xref" href="#alerting">“Alerting on Metrics”</a>). Rather, we’re interested in the slow-moving icebergs that draw closer over days or weeks. Those are dangers that, if you don’t spot them and take avoiding action, will sink your system at the worst possible time.</p>&#13;
&#13;
<p>After an incident, always ask, “What would have warned us about this problem in advance, if only we’d been aware of it?” If the answer is a piece of data you already had but didn’t pay attention to, take action to highlight that data. A dashboard is one possible way to do this.</p>&#13;
&#13;
<p>While alerts can tell you that some value has exceeded a preset threshold, you may not always know in advance what the danger level is. A graph lets you visualize how that value is behaving over long periods of time, and helps you detect problematic trends before they actually affect the system.<a data-startref="ix_16-metrics-adoc3" data-type="indexterm" id="idm45979374041584"/><a data-startref="ix_16-metrics-adoc2" data-type="indexterm" id="idm45979374040880"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Alerting on Metrics" data-type="sect1"><div class="sect1" id="alerting">&#13;
<h1>Alerting on Metrics</h1>&#13;
&#13;
<p><a data-primary="alerting" data-type="indexterm" id="ix_16-metrics-adoc4"/><a data-primary="metrics" data-secondary="alerting" data-type="indexterm" id="ix_16-metrics-adoc5"/>You might be surprised that we’ve spent most of this chapter talking about observability and monitoring without mentioning alerts. For some people, alerts are what monitoring is all about. We think that philosophy needs to change, for a number of reasons.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What’s Wrong with Alerts?" data-type="sect2"><div class="sect2" id="idm45979374035808">&#13;
<h2>What’s Wrong with Alerts?</h2>&#13;
&#13;
<p>Alerts indicate some unexpected deviation from a stable, working state. Well, distributed systems don’t have those states!</p>&#13;
&#13;
<p>As we’ve mentioned, large-scale distributed systems are never completely <em>up</em>; they’re almost always in a state of partially degraded service (see <a data-type="xref" href="ch15.html#neverup">“Cloud native applications are never “up””</a>). They have so many metrics that if you alert every time some metric goes outside normal limits, you’d be sending hundreds of pages a day to no good purpose:</p>&#13;
<blockquote data-type="quote">&#13;
<p>&#13;
People are over-paging themselves because their observability blows and they don’t trust their tools to let them reliably debug and diagnose the problem. So they get tens or hundreds of alerts, which they pattern-match for clues about what the root cause might be. They’re flying blind.&#13;
&#13;
In the chaotic future we’re all hurtling toward, you actually have to have the discipline to have radically <i>fewer</i> paging alerts, not more. Request rate, latency, error rate, saturation.</p>&#13;
<p data-type="attribution"><a href="https://oreil.ly/FiRbV">Charity Majors</a></p>&#13;
</blockquote>&#13;
&#13;
<p>For some unfortunate people, on-call alert pages are a way of life. This is a bad thing, not just for the obvious human reasons. Alert fatigue is a well-known issue in medicine, where clinicians can rapidly become desensitized by constant alarms, making them more likely to overlook a serious issue when it does arise.</p>&#13;
&#13;
<p>For a monitoring system to be useful, it has to have a very high signal-to-noise ratio. False alarms are not only annoying, but dangerous: they reduce trust in the system, and condition people that alerts can be safely ignored.</p>&#13;
&#13;
<p>Excessive, incessant, and irrelevant alarms were a major factor in the <a href="https://oreil.ly/cXEOk">Three Mile Island disaster</a>, and even when individual alarms are well designed, operators can be overwhelmed by too many of them going off <span class="keep-together">simultaneously</span>.</p>&#13;
&#13;
<p>An alert should mean one very simple thing: <a href="https://oreil.ly/pMZqD"><em>action needs to be taken now, by a person</em></a>.</p>&#13;
&#13;
<p>If no action is needed, no alert is needed. If action needs to happen <em>sometime</em>, but not right now, the alert can be downgraded to a lower priority notification, like an email or chat message. If the action can be taken by an automated system, then automate it: don’t wake up a valuable human being.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="On-Call Should Not Be Hell" data-type="sect2"><div class="sect2" id="on-call">&#13;
<h2>On-Call Should Not Be Hell</h2>&#13;
&#13;
<p><a data-primary="on-call, easing burden of" data-type="indexterm" id="idm45979374021456"/>While the idea of being on-call for your own services is key to the DevOps philosophy, it’s equally important that being on-call should be as painless an experience as possible.</p>&#13;
&#13;
<p>Alert pages should be a rare and exceptional occurrence. When they do happen, there should be a well-established and effective procedure for handling them, which puts as little strain as possible on the responder.</p>&#13;
&#13;
<p>Nobody should be on-call all the time. If this is the case, add more people to the rotation. You don’t need to be a subject-matter expert to be on-call: your main task is to triage the problem, decide if it needs action, and escalate it to the right people.</p>&#13;
&#13;
<p>While the burden of on-call should be fairly distributed, people’s personal circumstances differ. If you have a family, or other commitments outside work, it may not be so easy for you to take on-call shifts. It takes careful and sensitive management to arrange on-call in a way that’s fair to everybody.</p>&#13;
&#13;
<p>If the job involves being on-call, that should be made clear to the person when they’re hired. Expectations about the frequency and circumstances of on-call shifts should be written into their contract. It’s not fair to hire someone for a strictly nine–to-five job, and then decide you also want them to be on-call nights and weekends.</p>&#13;
&#13;
<p>On-call should be properly compensated with cash, time off in lieu, or some other meaningful benefit. This applies whether or not you actually receive any alerts; when you’re on-call, to some extent you’re at work.</p>&#13;
&#13;
<p>There should also be a hard limit on the amount of time someone can spend on-call. People with more spare time or energy may want to volunteer to help reduce the stress on their coworkers, and that’s great, but don’t let anyone take on too much.</p>&#13;
&#13;
<p>Recognize that when you put people on-call, you are spending human capital. Spend it wisely.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Urgent, Important, and Actionable Alerts" data-type="sect2"><div class="sect2" id="idm45979374015440">&#13;
<h2>Urgent, Important, and Actionable Alerts</h2>&#13;
&#13;
<p>If alerts are so terrible, why are we talking about them at all? Well, you still need alerts. Things go wrong, blow up, fall over, and grind to a halt—usually at the most inconvenient time.</p>&#13;
&#13;
<p>Observability is wonderful, but you can’t find a problem when you’re not looking for one. Dashboards are great, but you don’t pay somebody to sit looking at a dashboard all day. For detecting an outage or issue that’s happening right now, and drawing a human’s attention to it, you can’t beat automated alerts based on thresholds.</p>&#13;
&#13;
<p>For example, you might want the system to alert you if error rates for a given service exceed 10% for some period of time, like five minutes. You might generate an alert when P99 latency for a service goes above some fixed value, like 1000 ms.</p>&#13;
&#13;
<p>In general, if a problem has real or potential business impact, and action needs to be taken now, by a person, it’s a possible candidate for an urgent alert notification.</p>&#13;
&#13;
<p>Don’t alert on every metric. Out of hundreds, or possibly thousands, of metrics, you should have only a handful of metrics that can generate alerts. Even when they do generate alerts, that doesn’t necessarily mean you need to page somebody.</p>&#13;
&#13;
<p>Pages should be restricted to only <em>urgent</em>, <em>important</em>, and <em>actionable</em> alerts:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Alerts that are important, but not urgent, can be dealt with during normal working hours. Only things that can’t wait till morning should be paged.</p>&#13;
</li>&#13;
<li>&#13;
<p>Alerts that are urgent, but not important, don’t justify waking someone up. For example, the failure of a little-used internal service that doesn’t affect customers.</p>&#13;
</li>&#13;
<li>&#13;
<p>If there’s no immediate action that can be taken to fix it, there’s no point paging about it.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>For everything else, you can send asynchronous notifications: emails, chat messages, support tickets, project issues, and so on. They will be seen and dealt with in a timely fashion, if your system is working properly. You don’t need to send someone’s cortisol levels skyrocketing by waking them up in the middle of the night with a blaring alarm.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Track Your Alerts, Out-of-Hours Pages, and Wake-Ups" data-type="sect2"><div class="sect2" id="idm45979374004800">&#13;
<h2>Track Your Alerts, Out-of-Hours Pages, and Wake-Ups</h2>&#13;
&#13;
<p>Your people are just as critical to your infrastructure as your cloud servers and Kubernetes clusters, in fact, more so. It makes sense, then, to monitor what’s happening to your people in just the same way as you monitor what’s happening with your services.</p>&#13;
&#13;
<p>The number of alerts sent in a given week is a good indicator of the overall health and stability of your system. The number of urgent pages, especially the number of pages sent out of hours, on weekends, and during normal sleep times, is a good indicator of your team’s overall health and morale.</p>&#13;
&#13;
<p>You should set a budget for the number of urgent pages, especially out of hours, and it should be very low. One or two out-of-hours pages per on-call engineer per week should probably be the limit. If you’re regularly exceeding this, you need to fix the alerts, fix the system, or hire more engineers.</p>&#13;
&#13;
<p>Review all urgent pages at least weekly, and fix or eliminate any false alarms or unnecessary alerts. If you don’t take this seriously, people won’t take your alerts seriously. And if you regularly interrupt people’s sleep and private life with unnecessary alerts, they will start looking for better jobs.<a data-startref="ix_16-metrics-adoc5" data-type="indexterm" id="idm45979374000784"/><a data-startref="ix_16-metrics-adoc4" data-type="indexterm" id="idm45979374000080"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Metrics Tools and Services" data-type="sect1"><div class="sect1" id="idm45979373999280">&#13;
<h1>Metrics Tools and Services</h1>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="tools and services" data-type="indexterm" id="ix_16-metrics-adoc6"/>Now let’s get into some specifics. What tools or services should you use to collect, analyze, and communicate metrics? In <a data-type="xref" href="ch15.html#dontbuildyourown">“Don’t build your own monitoring infrastructure”</a>, we made the point that, when faced with a commodity problem, you should use a commodity solution. Does that mean you should necessarily use a third-party, hosted metrics service like Datadog or New Relic?</p>&#13;
&#13;
<p>The answer here isn’t quite so clear-cut. While these services offer lots of powerful features, they can be expensive, especially at scale. The decision to run your own metrics servers or not will largely depend on your situation, including how many applications you manage and how much data you are collecting. If you decide to set up your own metrics infrastructure, there is an excellent free and open source product available.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prometheus" data-type="sect2"><div class="sect2" id="prometheus">&#13;
<h2>Prometheus</h2>&#13;
&#13;
<p><a data-primary="Prometheus" data-secondary="benefits of" data-type="indexterm" id="idm45979373993136"/>The de facto standard metrics solution in the cloud native world is <a href="https://prometheus.io">Prometheus</a>. It’s very widely used, especially with Kubernetes, and almost everything can interoperate with Prometheus in some way, so it’s the first thing you should consider when you’re thinking about metrics-monitoring options.</p>&#13;
&#13;
<p>Prometheus is an open source systems-monitoring and alerting toolkit, based on time-series metrics data. The core of Prometheus is a server that collects and stores metrics. It also has various other optional components, such as an alerting tool (<a href="https://oreil.ly/jaKyF">Alertmanager</a>), and client libraries for programming languages such as Go, which you can use to instrument your applications.</p>&#13;
&#13;
<p>It all sounds rather complicated, but in practice it’s very simple. You can install Prometheus in your Kubernetes cluster with one command, using the <a href="https://oreil.ly/P2Qym">community Helm chart</a>. It will then gather metrics automatically from the cluster, and also from any applications you tell it to, using a process called <em>scraping</em>.</p>&#13;
&#13;
<p>Prometheus scrapes metrics by making an HTTP connection to your application on a prearranged port, and downloading whatever metrics data is available. It then stores the data in its database, where it will be available for you to query, graph, or alert on.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p><a data-primary="Prometheus" data-secondary="pull monitoring in" data-type="indexterm" id="idm45979373986304"/>Prometheus’s approach to collecting metrics is called <em>pull</em> monitoring. In this scheme, the monitoring server contacts the application and requests metrics data. The opposite approach, called <em>push</em>, and used by some other monitoring tools such as StatsD, works the other way: applications contact the monitoring server to send it metrics. Prometheus also supports the push model with their <a href="https://oreil.ly/NAucC">Pushgateway</a> component.</p>&#13;
</div>&#13;
&#13;
<p>Like Kubernetes itself, Prometheus is inspired by Google’s own infrastructure. It was developed at SoundCloud, <a data-primary="Borgmon" data-type="indexterm" id="idm45979373982576"/>but it takes many of its ideas from a tool called Borgmon. Borgmon, as the name suggests, was designed to monitor Google’s Borg container orchestration system (see <a data-type="xref" href="ch01.html#borgtok8s">“From Borg to Kubernetes”</a>):</p>&#13;
<blockquote data-type="quote">&#13;
<p>Kubernetes directly builds on Google’s decade-long experience with their own cluster scheduling system, Borg. Prometheus’s bonds to Google are way looser but it draws a lot of inspiration from Borgmon, the internal monitoring system Google came up with at about the same time as Borg.&#13;
&#13;
In a very sloppy comparison, you could say that Kubernetes is Borg for mere mortals, while Prometheus is Borgmon for mere mortals. Both are “second systems” trying to iterate on the good parts while avoiding the mistakes and dead ends of their ancestors.</p>&#13;
<p data-type="attribution"><a class="orm:hideurl" href="https://www.oreilly.com/ideas/google-infrastructure-for-everyone-else">Björn Rabenstein</a> (SoundCloud)</p>&#13;
</blockquote>&#13;
&#13;
<p>You can read more about Prometheus on its <a href="https://prometheus.io">site</a>, including instructions on how to install and configure it for your environment.</p>&#13;
&#13;
<p>While Prometheus itself is focused on the job of collecting and storing metrics, there are other high-quality open source options for graphing, dashboarding, and alerting. <a data-primary="Grafana" data-type="indexterm" id="idm45979373976432"/><a href="https://grafana.com">Grafana</a> is a powerful and capable graphing engine for time-series data (<a data-type="xref" href="#img-grafana">Figure 16-8</a>).</p>&#13;
&#13;
<p><a data-primary="Alertmanager" data-type="indexterm" id="idm45979373973872"/>The Prometheus project includes a tool called <a href="https://oreil.ly/jaKyF">Alertmanager</a>, which works well with Prometheus but can also operate independently of it. Alertmanager’s job is to receive alerts from various sources, including Prometheus servers, and process them (see <a data-type="xref" href="#alerting">“Alerting on Metrics”</a>).</p>&#13;
&#13;
<p>The first step in processing alerts is to deduplicate them. Alertmanager can then group alerts it detects to be related; for example, a major network outage might result in hundreds of individual alerts, but Alertmanager can group all of these into a single message so that responders aren’t overwhelmed with pages.</p>&#13;
&#13;
<p>Finally, Alertmanager will route the processed alerts to an appropriate notification service, such as PagerDuty, Slack, or email.</p>&#13;
&#13;
<p><a data-primary="Prometheus" data-secondary="metrics format" data-type="indexterm" id="idm45979373970016"/>Conveniently, the Prometheus metrics format is supported by a very wide range of tools and services, and <a data-primary="OpenMetrics" data-type="indexterm" id="idm45979373968784"/>this de facto standard is now the basis for <a href="https://openmetrics.io">OpenMetrics</a>, a Cloud Native Computing Foundation project to produce a neutral standard format for metrics data. Many popular hosted monitoring tools such as Amazon CloudWatch, Operations Suite, Datadog, and New Relic can import and understand Prometheus data.</p>&#13;
&#13;
<figure><div class="figure" id="img-grafana">&#13;
<img alt="Grafana dashboard" src="assets/cnd2_1608.png"/>&#13;
<h6><span class="label">Figure 16-8. </span>Grafana dashboard showing Prometheus data</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Google Operations Suite" data-type="sect2"><div class="sect2" id="idm45979373964704">&#13;
<h2>Google Operations Suite</h2>&#13;
&#13;
<p><a data-primary="Prometheus" data-secondary="alternatives to" data-type="indexterm" id="ix_16-metrics-adoc7"/>Operations <a data-primary="Google Operations Suite" data-type="indexterm" id="idm45979373962128"/><a data-primary="Operations Suite" data-type="indexterm" id="idm45979373961424"/>Suite was previously called Stackdriver, and although now a part of <span class="keep-together">Google</span>, it’s not limited to Google Cloud: it also works with AWS. The Cloud Monitoring component can collect, graph, and alert on metrics and log data from a variety of sources. It will autodiscover and monitor your cloud resources, including VMs, databases, and Kubernetes clusters. Operations Suite brings all this data into a central web console where you can create custom dashboards and alerts.</p>&#13;
&#13;
<p>Operations Suite understands how to get operational metrics from such popular software tools as PostgreSQL, NGINX, Cassandra, and Elasticsearch. If you want to include your own custom metrics from your applications, you can use Operations Suite’s client library to export whatever data you want. It also offers the ability to run a managed Prometheus instance for you, allowing you to continue using your existing Prometheus exporters and Grafana dashboards.</p>&#13;
&#13;
<p>If you’re in Google Cloud, Operations Suite is free for all GCP-related metrics; for custom metrics, or metrics from other cloud platforms, you pay per megabyte of monitoring data per month.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="AWS CloudWatch" data-type="sect2"><div class="sect2" id="idm45979373958080">&#13;
<h2>AWS CloudWatch</h2>&#13;
&#13;
<p><a data-primary="AWS CloudWatch" data-type="indexterm" id="idm45979373956880"/><a data-primary="CloudWatch" data-type="indexterm" id="idm45979373956176"/>Amazon’s own cloud monitoring product, CloudWatch, has a similar feature set to Operations Suite. It integrates with all AWS services, and you can export custom metrics using the CloudWatch SDK or command-line tool.</p>&#13;
&#13;
<p>CloudWatch has a free tier that allows you to gather <em>basic</em> metrics (such as CPU utilization for VMs) at five-minute intervals, a certain number of dashboards and alarms, and so on. Over and above those you pay per metric, per dashboard, or per alarm, and you can also pay for high-resolution metrics (one-minute intervals) on a per-instance basis.</p>&#13;
&#13;
<p><a href="https://aws.amazon.com/cloudwatch">CloudWatch</a> is basic, but effective. If your primary cloud infrastructure is AWS, CloudWatch is a good place to start working with metrics, and for small deployments it may be all you ever need.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Azure Monitor" data-type="sect2"><div class="sect2" id="idm45979373952528">&#13;
<h2>Azure Monitor</h2>&#13;
&#13;
<p><a data-primary="Azure Monitor" data-type="indexterm" id="idm45979373951328"/><a href="https://oreil.ly/FPvPR">Azure Monitor</a> is the Microsoft equivalent of GCP’s Operations Suite or AWS CloudWatch. It collects logs and metrics data from all your Azure resources, including Kubernetes clusters, and allows you to visualize and alert on it. It also offers a Prometheus-based metric scraper so that you do not need to use a different instrumentation tool in your applications if you already have Prometheus configured.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Datadog" data-type="sect2"><div class="sect2" id="datadog">&#13;
<h2>Datadog</h2>&#13;
&#13;
<p><a data-primary="Datadog" data-type="indexterm" id="idm45979373947472"/>In comparison to the cloud providers’ built-in tools like Operations Suite and CloudWatch, <a href="https://www.datadoghq.com">Datadog</a> is a very sophisticated and powerful monitoring and analytics platform. It offers integrations for over 250 platforms and services, including all the cloud services from major providers, and popular software such as Jenkins, NGINX, Consul, PostgreSQL, and MySQL.</p>&#13;
&#13;
<p><a data-primary="APM (application performance monitoring)" data-type="indexterm" id="idm45979373945296"/><a data-primary="application performance monitoring (APM)" data-type="indexterm" id="idm45979373944496"/>Datadog also offers an application performance monitoring (APM) component, along with a log aggregation product, designed to help you monitor and analyze how your applications are performing. Whether you use Go, Java, Ruby, or any other software platform, Datadog can gather metrics, logs, and traces from your software, and answer questions for you like:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>What is the user experience like for a specific, individual user of my service?</p>&#13;
</li>&#13;
<li>&#13;
<p>Who are the 10 customers who see the slowest responses on a particular <span class="keep-together">endpoint?</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Which of my various distributed services are contributing to the overall latency of requests?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Together with the usual dashboarding (see <a data-type="xref" href="#img-datadog">Figure 16-9</a>) and alerting features (automatable via the Datadog API and client libraries, including Terraform), Datadog also provides features like anomaly detection, powered by machine learning, and they also support collecting Prometheus metrics from your applications.</p>&#13;
&#13;
<figure><div class="figure" id="img-datadog">&#13;
<img alt="Datadog screenshot" src="assets/cnd2_1609.png"/>&#13;
<h6><span class="label">Figure 16-9. </span>Datadog dashboard</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="New Relic" data-type="sect2"><div class="sect2" id="idm45979373935632">&#13;
<h2>New Relic</h2>&#13;
&#13;
<p><a data-primary="New Relic" data-type="indexterm" id="idm45979373934400"/>New Relic is a very well established and widely used metrics platform focused on application performance monitoring (APM). Its chief strength is in diagnosing performance problems and bottlenecks inside applications and distributed systems (see <a data-type="xref" href="#img-new-relic">Figure 16-10</a>). However, it also offers infrastructure metrics and monitoring, alerting, software analytics, and everything else you’d expect.</p>&#13;
&#13;
<p>If you’re in the market for a premium corporate metrics platform, you’ll probably be looking either at New Relic (slightly more application focused) or Datadog (slightly more infrastructure focused). Both also offer good infrastructure as code support; for example, you can create monitoring dashboards and alerts for both New Relic and Datadog using official Terraform providers<a data-startref="ix_16-metrics-adoc7" data-type="indexterm" id="idm45979373931584"/>.<a data-startref="ix_16-metrics-adoc6" data-type="indexterm" id="idm45979373930752"/></p>&#13;
&#13;
<figure><div class="figure" id="img-new-relic">&#13;
<img alt="New Relic interface" src="assets/cnd2_1610.png"/>&#13;
<h6><span class="label">Figure 16-10. </span>New Relic APM dashboard</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45979373927856">&#13;
<h1>Summary</h1>&#13;
&#13;
<p><em>Measure twice, cut once</em> is a favorite saying of many engineers. In the cloud native world, without proper metrics and observability data it’s very difficult to know what’s going on. On the other hand, once you open the metrics floodgates, too much information can be just as useless as too little.</p>&#13;
&#13;
<p>The trick is to gather the right data in the first place, process it in the right way, use it to answer the right questions, visualize it in the right way, and use it to alert the right people at the right time about the right things.</p>&#13;
&#13;
<p>If you forget everything else in this chapter, remember this:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Focus on the key metrics for each service: requests, errors, and duration (RED). For each resource: utilization, saturation, and errors (USE).</p>&#13;
</li>&#13;
<li>&#13;
<p>Instrument your apps to expose custom metrics, both for internal observability and for business KPIs.</p>&#13;
</li>&#13;
<li>&#13;
<p>Useful Kubernetes metrics include, at the cluster level, the number of nodes, Pods per node, and resource usage of nodes.</p>&#13;
</li>&#13;
<li>&#13;
<p>At the deployment level, track deployments and replicas, especially unavailable replicas, which might indicate a capacity problem.</p>&#13;
</li>&#13;
<li>&#13;
<p>At the container level, track resource usage per container, liveness/readiness states, restarts, network traffic, and network errors.</p>&#13;
</li>&#13;
<li>&#13;
<p>Build a dashboard for each service, using a standard layout and a primary information radiator that reports the vital signs of the whole system.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you alert on metrics, alerts should be urgent, important, and actionable. Alert noise creates fatigue and damages morale.</p>&#13;
</li>&#13;
<li>&#13;
<p>Track and review the number of urgent pages your team receives, especially wake-ups and weekends.</p>&#13;
</li>&#13;
<li>&#13;
<p>The de facto standard metrics solution in the cloud native world is Prometheus, and almost everything speaks the Prometheus data format.</p>&#13;
</li>&#13;
<li>&#13;
<p>Popular third-party managed metrics services include Google Operations Suite, Amazon CloudWatch, Datadog, and New Relic.<a data-startref="ix_16-metrics-adoc0" data-type="indexterm" id="idm45979373913968"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45979374096544"><sup><a href="ch16.html#idm45979374096544-marker">1</a></sup> <a href="https://oreil.ly/ZHCMU">The Wikipedia entry on Simpson’s paradox</a> provides more information.</p></div></div></section></body></html>