- en: Chapter 9\. Failure Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a tree falls in a forest and no one is around to hear it, does it make a
    sound?
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unknown Author
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In order for a system to appropriately react to failures, failures should be
    detected in a timely manner. A faulty process might get contacted even though
    it won’t be able to respond, increasing latencies and reducing overall system
    availability.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Detecting failures in asynchronous distributed systems (i.e., without making
    any timing assumptions) is extremely difficult as it’s impossible to tell whether
    the process has crashed, or is running slowly and taking an indefinitely long
    time to respond. We discussed a problem related to this one in [“FLP Impossibility”](ch08.html#flp_impossibility).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Terms such as *dead*, *failed*, and *crashed* are usually used to describe a
    process that has stopped executing its steps completely. Terms such as *unresponsive*,
    *faulty*, and *slow* are used to describe *suspected* processes, which may actually
    be dead.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Failures may occur on the *link* level (messages between processes are lost
    or delivered slowly), or on the *process* level (the process crashes or is running
    slowly), and slowness may not always be distinguishable from failure. This means
    there’s always a trade-off between wrongly suspecting alive processes as dead
    (producing *false-positives*), and delaying marking an unresponsive process as
    dead, giving it the benefit of doubt and expecting it to respond eventually (producing
    *false-negatives*).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: A *failure detector* is a local subsystem responsible for identifying failed
    or unreachable processes to exclude them from the algorithm and guarantee liveness
    while preserving safety.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Liveness and safety are the properties that describe an algorithm’s ability
    to solve a specific problem and the correctness of its output. More formally,
    *liveness* is a property that guarantees that a specific intended event *must*
    occur. For example, if one of the processes has failed, a failure detector *must*
    detect that failure. *Safety* guarantees that unintended events will *not* occur.
    For example, if a failure detector has marked a process as dead, this process
    had to be, in fact, dead [[LAMPORT77]](app01.html#LAMPORT77) [[RAYNAL99]](app01.html#RAYNAL99)
    [[FREILING11]](app01.html#FREILING11).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: From a practical perspective, excluding failed processes helps to avoid unnecessary
    work and prevents error propagation and cascading failures, while reducing availability
    when excluding potentially suspected alive processes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Failure-detection algorithms should exhibit several essential properties. First
    of all, every nonfaulty member should eventually notice the process failure, and
    the algorithm should be able to make progress and eventually reach its final result.
    This property is called *completeness*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'We can judge the quality of the algorithm by its *efficiency*: how fast the
    failure detector can identify process failures. Another way to do this is to look
    at the *accuracy* of the algorithm: whether or not the process failure was precisely
    detected. In other words, an algorithm is *not* accurate if it falsely accuses
    a live process of being failed or is not able to detect the existing failures.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of the relationship between efficiency and accuracy as a tunable
    parameter: a more efficient algorithm might be less precise, and a more accurate
    algorithm is usually less efficient. It is provably impossible to build a failure
    detector that is both accurate and efficient. At the same time, failure detectors
    are allowed to produce false-positives (i.e., falsely identify live processes
    as failed and vice versa) [[CHANDRA96]](app01.html#CHANDRA96).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Failure detectors are an essential prerequisite and an integral part of many
    consensus and atomic broadcast algorithms, which we’ll be discussing later in
    this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Many distributed systems implement failure detectors by using *heartbeats*.
    This approach is quite popular because of its simplicity and strong completeness.
    Algorithms we discuss here assume the absence of Byzantine failures: processes
    do not attempt to intentionally lie about their state or states of their neighbors.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Heartbeats and Pings
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can query the state of remote processes by triggering one of two periodic
    processes:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: We can trigger a ping, which sends messages to remote processes, checking if
    they are still alive by expecting a response within a specified time period.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can trigger a *heartbeat* when the process is actively notifying its peers
    that it’s still running by sending messages to them.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll use pings as an example here, but the same problem can be solved using
    heartbeats, producing similar results.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Each process maintains a list of other processes (alive, dead, and suspected
    ones) and updates it with the last response time for each process. If a process
    fails to respond to a ping message for a longer time, it is marked as *suspected*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-1](#failure_detectors_1) shows the normal functioning of a system:
    process `P1` is querying the state of neighboring node `P2`, which responds with
    an acknowledgment.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![dbin 0901](assets/dbin_0901.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-1\. Pings for failure detection: normal functioning, no message delays'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In contrast, [Figure 9-2](#failure_detectors_2) shows how acknowledgment messages
    are delayed, which might result in marking the active process as down.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![dbin 0902](assets/dbin_0902.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-2\. Pings for failure detection: responses are delayed, coming after
    the next message is sent'
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many failure-detection algorithms are based on heartbeats and timeouts. For
    example, Akka, a popular framework for building distributed systems, has an implementation
    of a [deadline failure detector](https://databass.dev/links/41), which uses heartbeats
    and reports a process failure if it has failed to register within a fixed time
    interval.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has several potential downsides: its precision relies on the
    careful selection of ping frequency and timeout, and it does not capture process
    visibility from the perspective of other processes (see [“Outsourced Heartbeats”](#outsourced_heartbeats)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Timeout-Free Failure Detector
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some algorithms avoid relying on timeouts for detecting failures. For example,
    Heartbeat, a *timeout-free* failure detector [[AGUILERA97]](app01.html#AGUILERA97),
    is an algorithm that only counts heartbeats and allows the application to detect
    process failures based on the data in the heartbeat counter vectors. Since this
    algorithm is timeout-free, it operates under *asynchronous* system assumptions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm assumes that any two correct processes are connected to each other
    with a *fair path*, which contains only fair links (i.e., if a message is sent
    over this link infinitely often, it is also received infinitely often), and each
    process is aware of the existence of *all* other processes in the network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Each process maintains a list of neighbors and counters associated with them.
    Processes start by sending heartbeat messages to their neighbors. Each message
    contains a path that the heartbeat has traveled so far. The initial message contains
    the first sender in the path and a unique identifier that can be used to avoid
    broadcasting the same message multiple times.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: When the process receives a new heartbeat message, it increments counters for
    all participants present in the path and sends the heartbeat to the ones that
    are not present there, appending itself to the path. Processes stop propagating
    messages as soon as they see that all the known processes have already received
    it (in other words, process IDs appear in the path).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Since messages are propagated through different processes, and heartbeat paths
    contain aggregated information received from the neighbors, we can (correctly)
    mark an unreachable process as alive even when the direct link between the two
    processes is faulty.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Heartbeat counters represent a global and normalized view of the system. This
    view captures how the heartbeats are propagated relative to one another, allowing
    us to compare processes. However, one of the shortcomings of this approach is
    that interpreting heartbeat counters may be quite tricky: we need to pick a threshold
    that can yield reliable results. Unless we can do that, the algorithm will falsely
    mark active processes as suspected.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Outsourced Heartbeats
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative approach, used by the Scalable Weakly Consistent Infection-style
    Process Group Membership Protocol (SWIM) [[GUPTA01]](app01.html#GUPTA01) is to
    use *outsourced heartbeats* to improve reliability using information about the
    process liveness from the perspective of its neighbors. This approach does not
    require processes to be aware of all other processes in the network, only a subset
    of connected peers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 9-3](#failure_detectors_3), process `P[1]` sends a ping
    message to process `P[2]`. `P[2]` doesn’t respond to the message, so `P[1]` proceeds
    by selecting multiple random members (`P[3]` and `P[4]`). These random members
    try sending heartbeat messages to `P[2]` and, if it responds, forward acknowledgments
    back to `P[1]`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![dbin 0903](assets/dbin_0903.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. “Outsourcing” heartbeats
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This allows accounting for both direct and indirect reachability. For example,
    if we have processes `P[1]`, `P[2]`, and `P[3]`, we can check the state of `P[3]`
    from the perspective of both `P[1]` and `P[2]`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Outsourced heartbeats allow reliable failure detection by distributing responsibility
    for deciding across the group of members. This approach does not require broadcasting
    messages to a broad group of peers. Since outsourced heartbeat requests can be
    triggered in parallel, this approach can collect more information about suspected
    processes quickly, and allow us to make more accurate decisions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Phi-Accrual Failure Detector
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of treating node failure as a binary problem, where the process can
    be only in two states: up or down, a *phi-accrual* (φ-accrual) failure detector
    [[HAYASHIBARA04]](app01.html#HAYASHIBARA04) has a continuous scale, capturing
    the probability of the monitored process’s crash. It works by maintaining a sliding
    window, collecting arrival times of the most recent heartbeats from the peer processes.
    This information is used to approximate arrival time of the *next* heartbeat,
    compare this approximation with the actual arrival time, and compute the *suspicion
    level* `φ`: how certain the failure detector is about the failure, given the current
    network conditions.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm works by collecting and sampling arrival times, creating a view
    that can be used to make a reliable judgment about node health. It uses these
    samples to compute the value of `φ`: if this value reaches a threshold, the node
    is marked as down. This failure detector dynamically adapts to changing network
    conditions by adjusting the scale on which the node can be marked as a suspect.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'From the architecture perspective, a phi-accrual failure detector can be viewed
    as a combination of three subsystems:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Collecting liveness information through pings, heartbeats, or request-response
    sampling.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Making a decision on whether or not the process should be marked as suspected.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Action
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: A callback executed whenever the process is marked as suspected.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The monitoring process collects and stores data samples (which are assumed to
    follow a normal distribution) in a fixed-size window of heartbeat arrival times.
    Newer arrivals are added to the window, and the oldest heartbeat data points are
    discarded.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Distribution parameters are estimated from the sampling window by determining
    the mean and variance of samples. This information is used to compute the probability
    of arrival of the message within `t` time units after the previous one. Given
    this information, we compute `φ`, which describes how likely we are to make a
    correct decision about a process’s liveness. In other words, how likely it is
    to make a mistake and receive a heartbeat that will contradict the calculated
    assumptions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: This approach was developed by researchers from the Japan Advanced Institute
    of Science and Technology, and is now used in many distributed systems; for example,
    [Cassandra](https://databass.dev/links/42) and [Akka](https://databass.dev/links/43)
    (along with the aforementioned deadline failure detector).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Gossip and Failure Detection
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach that avoids relying on a single-node view to make a decision
    is a gossip-style failure detection service [[VANRENESSE98]](app01.html#VANRENESSE98),
    which uses *gossip* (see [“Gossip Dissemination”](ch12.html#gossip_dissipation))
    to collect and distribute states of neighboring processes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Each member maintains a list of other members, their *heartbeat counters*, and
    timestamps, specifying when the heartbeat counter was incremented for the last
    time. Periodically, each member increments its heartbeat counter and distributes
    its list to a random neighbor. Upon the message receipt, the neighboring node
    merges the list with its own, updating heartbeat counters for the other neighbors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Nodes also periodically check the list of states and heartbeat counters. If
    any node did not update its counter for long enough, it is considered failed.
    This timeout period should be chosen carefully to minimize the probability of
    false-positives. How often members have to communicate with each other (in other
    words, worst-case bandwidth) is capped, and can grow at most linearly with a number
    of processes in the system.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-4](#failure_detectors_4) shows three communicating processes sharing
    their heartbeat counters:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: a) All three can communicate and update their timestamps.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b) `P3` isn’t able to communicate with `P1`, but its timestamp `t[6]` can still
    be propagated through `P2`.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: c) `P3` crashes. Since it doesn’t send updates anymore, it is detected as failed
    by other processes.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![dbin 0904](assets/dbin_0904.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Replicated heartbeat table for failure detection
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This way, we can detect crashed nodes, as well as the nodes that are unreachable
    by any other cluster member. This decision is reliable, since the view of the
    cluster is an aggregate from multiple nodes. If there’s a link failure between
    the two hosts, heartbeats can still propagate through other processes. Using gossip
    for propagating system states increases the number of messages in the system,
    but allows information to spread more reliably.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Reversing Failure Detection Problem Statement
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since propagating the information about failures is not always possible, and
    propagating it by notifying every member might be expensive, one of the approaches,
    called *FUSE* (failure notification service) [[DUNAGAN04]](app01.html#DUNAGAN04),
    focuses on reliable and cheap failure propagation that works even in cases of
    network partitions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: To detect process failures, this approach arranges all active processes in groups.
    If one of the groups becomes unavailable, all participants detect the failure.
    In other words, every time a single process failure is detected, it is converted
    and propagated as a *group failure*. This allows detecting failures in the presence
    of any pattern of disconnects, partitions, and node failures.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Processes in the group periodically send ping messages to other members, querying
    whether they’re still alive. If one of the members cannot respond to this message
    because of a crash, network partition, or link failure, the member that has initiated
    this ping will, in turn, stop responding to ping messages itself.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-5](#failure_detectors_5) shows four communicating processes:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'a) Initial state: all processes are alive and can communicate.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b) `P[2]` crashes and stops responding to ping messages.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: c) `P[4]` detects the failure of `P[2]` and stops responding to ping messages
    itself.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: d) Eventually, `P[1]` and `P[3]` notice that both `P[1]` and `P[2]` do not respond,
    and process failure propagates to the entire group.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![dbin 0905](assets/dbin_0905.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. FUSE failure detection
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All failures are propagated through the system from the source of failure to
    all other participants. Participants gradually stop responding to pings, converting
    from the individual node failure to the group failure.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use the absence of communication as a means of propagation. An advantage
    of using this approach is that every member is guaranteed to learn about group
    failure and adequately react to it. One of the downsides is that a link failure
    separating a single process from other ones can be converted to the group failure
    as well, but this can be seen as an advantage, depending on the use case. Applications
    can use their own definitions of propagated failures to account for this scenario.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Failure detectors are an essential part of any distributed system. As shown
    by the FLP Impossibility result, no protocol can guarantee consensus in an asynchronous
    system. Failure detectors help to augment the model, allowing us to solve a consensus
    problem by making a trade-off between accuracy and completeness. One of the significant
    findings in this area, proving the usefulness of failure detectors, was described
    in [[CHANDRA96]](app01.html#CHANDRA96), which shows that solving consensus is
    possible even with a failure detector that makes an infinite number of mistakes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 失效探测器是任何分布式系统的重要组成部分。正如FLP不可能性结果所示，在异步系统中没有协议能够保证一致性。失效探测器有助于扩展模型，通过在准确性和完整性之间进行权衡来解决一致性问题。在这一领域的一个重要发现，证明了失效探测器的有用性，可以在[[CHANDRA96]](app01.html#CHANDRA96)中找到描述，该研究表明，即使是一个频繁出错的失效探测器，也可以解决一致性问题。
- en: 'We’ve covered several algorithms for failure detection, each using a different
    approach: some focus on detecting failures by direct communication, some use broadcast
    or gossip for spreading the information around, and some opt out by using quiescence
    (in other words, absence of communication) as a means of propagation. We now know
    that we can use heartbeats or pings, hard deadlines, or continuous scales. Each
    one of these approaches has its own upsides: simplicity, accuracy, or precision.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了几种失效检测算法，每种算法采用不同的方法：一些专注于通过直接通信检测故障，一些使用广播或八卦传播信息，还有一些通过静态（换句话说，缺乏通信）来传播信息。现在我们知道可以使用心跳或ping、硬截止时间或连续刻度。每一种方法都有其优势：简单性、准确性或精确度。
