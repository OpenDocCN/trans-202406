<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Cluster Architecture, Installation, &#10;and Configuration"><div class="chapter" id="cluster_architecture_installation_configuration">
<h1><span class="label">Chapter 2. </span>Cluster Architecture, Installation, 
<span class="keep-together">and Configuration</span></h1>


<p><a data-type="indexterm" data-primary="clusters" id="clu_ch"/>According to the name of the chapter, the first section of the curriculum refers to typical tasks you’d expect of a Kubernetes administrator. Those tasks include understanding the architectural components of a Kubernetes cluster, setting up a cluster from scratch, and maintaining a cluster going forward.</p>

<p>Interestingly, this section also covers the security aspects of a cluster, more specifically role-based access control (RBAC). You are expected to understand how to map permissions for operations to API resources for a set of users or processes.</p>

<p>At the end of this chapter, you will understand the tools and procedures for installing and maintaining a Kubernetes cluster. Moreover, you’ll know how to configure RBAC for representative, real-world use cases.</p>

<p>At a high level, this chapter covers the following concepts:</p>

<ul>
<li>
<p>Understanding RBAC</p>
</li>
<li>
<p>Installing of a cluster with <code>kubeadm</code></p>
</li>
<li>
<p>Upgrading a version of a Kubernetes cluster with <code>kubeadm</code></p>
</li>
<li>
<p>Backing up and restoring etcd with <code>etcdctl</code></p>
</li>
<li>
<p>Understanding a highly available Kubernetes cluster</p>
</li>
</ul>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Role-Based Access Control"><div class="sect1" id="idm45322734131824">
<h1>Role-Based Access Control</h1>

<p><a data-type="indexterm" data-primary="clusters" data-secondary="RBAC (role-based access control)" id="clu_rbac"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="about" id="idm45322734128304"/>In Kubernetes you need to be authenticated before you are allowed to make a request to an API resource. A cluster administrator usually has access to all resources and operations. The easiest way to operate a cluster is to provide everyone with an admin account. While “admin access for everyone” sounds fantastic as you grow your business, it comes with a considerable amount of risk. Users may accidentally delete a Secret Kubernetes object, which likely breaks one or many applications and therefore has a tremendous impact on end users. As you can imagine, this approach is not a good idea for production environments that run mission-critical applications.</p>

<p>As with other production systems, only certain users should have full access, whereas the majority of users have read-only access (and potentially access to mutate the system) depending on the role. For example, application developers do not need to manage cluster nodes. They only need to tend to the objects required to run and configure their application.</p>

<p>RBAC defines policies for users, groups, and processes by allowing or disallowing access to manage API resources. Enabling and configuring RBAC is mandatory for any organization with a strong emphasis on security. For the exam, you need to understand the involved RBAC API resource types and how to create and configure them in different scenarios.</p>








<section data-type="sect2" data-pdf-bookmark="RBAC High-Level Overview"><div class="sect2" id="idm45322734126288">
<h2>RBAC High-Level Overview</h2>

<p><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="high-level overview" id="idm45322734124880"/>RBAC helps with implementing a variety of use cases:</p>

<ul>
<li>
<p>Establishing a system for users with different roles to access a set of Kubernetes resources</p>
</li>
<li>
<p>Controlling processes running in a Pod and the operations they can perform via the Kubernetes API</p>
</li>
<li>
<p>Limiting the visibility of certain resources per namespace</p>
</li>
</ul>

<p>RBAC consists of three key building blocks, as shown in <a data-type="xref" href="#rbac_key_building_blocks">Figure 2-1</a>. Together, they connect API primitives and their allowed operations to the so-called subject, which is a user, a group, or a ServiceAccount.</p>

<p><a data-type="indexterm" data-primary="subjects, in RBAC (role-based access control)" id="idm45322734118640"/><a data-type="indexterm" data-primary="resources, in RBAC (role-based access control)" id="idm45322734117840"/><a data-type="indexterm" data-primary="verbs, in RBAC (role-based access control)" id="idm45322734117136"/>The following list breaks down the responsibilities by terminology:</p>
<dl>
<dt>Subject</dt>
<dd>
<p>The user or process that wants to access a resource</p>
</dd>
<dt>Resource</dt>
<dd>
<p>The Kubernetes API resource type (e.g., a Deployment or node)</p>
</dd>
<dt>Verb</dt>
<dd>
<p>The operation that can be executed on the resource (e.g., creating a Pod or deleting a Service)</p>
</dd>
</dl>

<figure><div id="rbac_key_building_blocks" class="figure"><div class="border-box"><img src="Images/ckas_0201.png" alt="ckas 0201" width="1101" height="349"/></div><h6><span class="label">Figure 2-1. </span>RBAC key building blocks</h6></div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating a Subject"><div class="sect2" id="idm45322733167264">
<h2>Creating a Subject</h2>

<p><a data-type="indexterm" data-primary="subjects, in RBAC (role-based access control)" id="idm45322733166064"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="creating subjects" id="idm45322733165328"/>In the context of RBAC, you can use a user account, service account, or a group as a subject. Users and groups are not stored in etcd, the Kubernetes database, and are meant for processes running outside of the cluster. Service accounts exists as objects in Kubernetes and are used by processes running inside of the cluster. In this section, you’ll learn how to create them.</p>










<section data-type="sect3" data-pdf-bookmark="User accounts and groups"><div class="sect3" id="idm45322733164160">
<h3>User accounts and groups</h3>

<p><a data-type="indexterm" data-primary="user accounts/groups" id="idm45322733162784"/>Kubernetes does not represent a user as with an API resource. The user is meant to be managed by the administrator of a Kubernetes cluster, which then distributes the credentials of the account to the real person or to be used by an external process.</p>

<p><a data-type="indexterm" data-primary="authentication strategies, for managing RBAC subjects" id="idm45322733161696"/><a data-type="indexterm" data-primary="X.509 client certificate" id="idm45322733161024"/><a data-type="indexterm" data-primary="basic authentication" id="idm45322733160288"/><a data-type="indexterm" data-primary="bearer tokens" id="idm45322733159616"/>Calls to the API server with a user need to be authenticated. Kubernetes offers a variety of authentication methods for those API requests. <a data-type="xref" href="#authentication_strategies_for_managing_rbac_subjects">Table 2-1</a> shows different ways of authenticating RBAC subjects.</p>
<table id="authentication_strategies_for_managing_rbac_subjects">
<caption><span class="label">Table 2-1. </span>Authentication strategies for managing RBAC subjects</caption>
<thead>
<tr>
<th>Authentication strategy</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>X.509 client certificate</p></td>
<td><p>Uses an OpenSSL client certificate to authenticate</p></td>
</tr>
<tr>
<td><p>Basic authentication</p></td>
<td><p>Uses username and password to authenticate</p></td>
</tr>
<tr>
<td><p>Bearer tokens</p></td>
<td><p>Uses OpenID (a flavor of OAuth2) or webhooks as a way to authenticate</p></td>
</tr>
</tbody>
</table>

<p>To keep matters simple, the following steps demonstrate the creation of a user that uses an OpenSSL client certificate to authenticate. Those actions have to be performed with the cluster-admin Role object. During the exam, you will not have to create a user yourself. You can assume that the relevant setup has been performed for you. Therefore, you will not need to memorize the following steps<a data-type="indexterm" data-primary="certificate sign request (CSR)" id="idm45322734173424"/><a data-type="indexterm" data-primary="CSR (certificate sign request)" id="idm45322734172656"/>:</p>
<ol>
<li>
<p>Log into the Kubernetes control plane node and create a temporary directory that will hold the generated keys. Navigate into the directory:</p>

<pre data-type="programlisting"><strong>$ mkdir cert &amp;&amp; cd cert</strong></pre>
</li>
<li>
<p>Create a private key using the <code>openssl</code> executable. Provide an expressive file name, such as <code>&lt;username&gt;.key</code>:</p>

<pre data-type="programlisting"><strong>$ openssl genrsa -out johndoe.key 2048</strong>
Generating RSA private key, 2048 bit long modulus
..............................<code>+
..</code>+
e is 65537 (0x10001)
<strong>$ ls</strong>
johndoe.key</pre>
</li>
<li>
<p>Create a certificate sign request (CSR) in a file with the extension <code>.csr</code>. You need to provide the private key from the previous step. The <code>-subj</code> option provides the username (CN) and the group (O). The following command uses the username <code>johndoe</code> and the group named <code>cka-study-guide</code>. To avoid assigning the user to a group, leave off the /O component of the assignment:</p>

<pre data-type="programlisting"><strong>$ openssl req -new -key johndoe.key -out johndoe.csr -subj</strong> \
  <strong>"/CN=johndoe/O=cka-study-guide"</strong>
<strong>$ ls</strong>
johndoe.csr johndoe.key</pre>
</li>
<li>
<p>Lastly, sign the CSR with the Kubernetes cluster certificate authority (CA). The CA can usually be found in the directory <code>/etc/kubernetes/pki</code> and needs to contain the files <code>ca.crt</code> and <code>ca.key</code>. We are going to use minikube here, which stores those files in the directory pass:[&lt;code&gt;~/.minikube&lt;/code&gt;. The following command signs the CSR and makes it valid for 364 days:</p>
<pre data-type="programlisting">
<strong>$ openssl x509 -req -in johndoe.csr -CA <sub>/.minikube/ca.crt -CAkey</sub></strong> \
  <strong>/.minikube/ca.key -CAcreateserial -out johndoe.crt -days 364</strong>
Signature ok
subject=/CN=johndoe/O=cka-study-guide
Getting CA Private Key
</pre>
</li>
<li>
<p>Create the user in Kubernetes by setting a user entry in kubeconfig for <code>johndoe</code>. Point to the CRT and key file. Set a context entry in kubeconfig for <code>johndoe</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl config set-credentials johndoe</strong> \
  <strong>--client-certificate=johndoe.crt --client-key=johndoe.key</strong>
User "johndoe" set.
<strong>$ kubectl config set-context johndoe-context --cluster=minikube</strong> \
  <strong>--user=johndoe</strong>
Context "johndoe-context" modified.</pre>
</li>
<li>
<p>To switch to the user, use the context named <code>johndoe-context</code>. You can check the current context using the command <code>config current-context</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl config use-context johndoe-context</strong>
Switched to context "johndoe-context".
<strong>$ kubectl config current-context</strong>
johndoe-context</pre>
</li>

</ol>
</div></section>













<section data-type="sect3" data-pdf-bookmark="ServiceAccount"><div class="sect3" id="idm45322734148608">
<h3>ServiceAccount</h3>

<p><a data-type="indexterm" data-primary="ServiceAccount" data-secondary="about" id="idm45322734147168"/><a data-type="indexterm" data-primary="Helm" data-secondary="about" id="idm45322733914128"/><a data-type="indexterm" data-primary="RESTful HTTP calls" id="idm45322733913184"/>A user represents a real person who commonly interacts with the Kubernetes cluster using the <code>kubectl</code> executable or the UI dashboard. Some service applications like <a href="https://helm.sh">Helm</a> running inside of a Pod need to interact with the Kubernetes cluster by making requests to the API server via RESTful HTTP calls. For example, a Helm chart would define multiple Kubernetes objects required for a business application. Kubernetes uses a ServiceAccount to authenticate the Helm service process with the API server through an authentication token. This ServiceAccount can be assigned to a Pod and mapped to RBAC rules.</p>

<p>A Kubernetes cluster already comes with a ServiceAccount, the <code>default</code> ServiceAccount that lives in the <code>default</code> namespace. Any Pod that doesn’t explicitly assign a ServiceAccount uses the <code>default</code> ServiceAccount.</p>

<p>To create a custom ServiceAccount imperatively, run the <code>create serviceaccount</code> command:</p>

<pre data-type="programlisting"><strong>$ kubectl create serviceaccount build-bot</strong>
serviceaccount/build-bot created</pre>

<p>The declarative way to create a ServiceAccount looks very straightforward. You simply provide the appropriate <code>kind</code> and a name, as shown in <a data-type="xref" href="#a_yaml_manifest_defining_a_serviceaccount">Example 2-1</a>.</p>
<div id="a_yaml_manifest_defining_a_serviceaccount" data-type="example">
<h5><span class="label">Example 2-1. </span>A YAML manifest defining a ServiceAccount</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ServiceAccount</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">build-bot</code><code class="w"/></pre></div>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Listing ServiceAccounts"><div class="sect2" id="idm45322731970256">
<h2>Listing ServiceAccounts</h2>

<p><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="listing ServiceAccounts" id="idm45322731525360"/><a data-type="indexterm" data-primary="ServiceAccount" data-secondary="listing" id="idm45322732774960"/><a data-type="indexterm" data-primary="get serviceaccounts command" id="idm45322731661440"/><a data-type="indexterm" data-primary="commands" data-secondary="get serviceaccounts" id="idm45322731572832"/>Listing the ServiceAccounts can be achieved with the <code>get serviceaccounts</code> command. As you can see in the following output, the <code>default</code> namespace lists the <code>default</code> ServiceAccount and the custom ServiceAccount we just created:</p>

<pre data-type="programlisting"><strong>$ kubectl get serviceaccounts</strong>
NAME        SECRETS   AGE
build-bot   1         78s
default     1         93d</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Rendering ServiceAccount Details"><div class="sect2" id="idm45322731845344">
<h2>Rendering ServiceAccount Details</h2>

<p><a data-type="indexterm" data-primary="Secret type" data-secondary="about" id="idm45322731843968"/><a data-type="indexterm" data-primary="ServiceAccount" data-secondary="rendering details" id="idm45322731703424"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="rendering ServiceAccount details" id="idm45322731702480"/>Upon object creation, the API server creates a Secret holding the API token and assigns it to the ServiceAccount. The Secret and token names use the ServiceAccount name as a prefix. You can discover the details of a ServiceAccount using the <code>describe serviceaccount</code> command, as shown here:</p>

<pre data-type="programlisting"><strong>$ kubectl describe serviceaccount build-bot</strong>
Name:                build-bot
Namespace:           default
Labels:              &lt;none&gt;
Annotations:         &lt;none&gt;
Image pull secrets:  &lt;none&gt;
Mountable secrets:   build-bot-token-rvjnz
Tokens:              build-bot-token-rvjnz
Events:              &lt;none&gt;</pre>

<p>Consequently, you should be able to find a Secret object for the <code>default</code> and the <code>build-bot</code> ServiceAccount:</p>

<pre data-type="programlisting"><strong>$ kubectl get secrets</strong>
NAME                    TYPE                                  DATA   AGE
build-bot-token-rvjnz   kubernetes.io/service-account-token   3      20m
default-token-qgh5n     kubernetes.io/service-account-token   3      93d</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Assigning a ServiceAccount to a Pod"><div class="sect2" id="idm45322733451920">
<h2>Assigning a ServiceAccount to a Pod</h2>

<p><a data-type="indexterm" data-primary="Pods" data-secondary="assigning ServiceAccounts to" id="idm45322733450672"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="assigning ServiceAccount to Pods" id="idm45322731389472"/><a data-type="indexterm" data-primary="ServiceAccount" data-secondary="assigning to Pods" id="idm45322731388592"/><a data-type="indexterm" data-primary="run command" id="idm45322731387648"/><a data-type="indexterm" data-primary="commands" data-secondary="run" id="idm45322731386976"/>For a ServiceAccount to take effect, it needs to be assigned to a Pod running the application intended to make API calls. Upon Pod creation, you can use the command-line option <code>--serviceaccount</code> in conjunction with the <code>run</code> command:</p>

<pre data-type="programlisting"><strong>$ kubectl run build-observer --image=alpine --restart=Never</strong> \
  <strong>--serviceaccount=build-bot</strong>
pod/build-observer created</pre>

<p>Alternatively, you can directly assign the ServiceAccount in the YAML manifest of a Pod, Deployment, Job, or CronJob using the field <code>serviceAccountName</code>. <a data-type="xref" href="#a_yaml_manifest_assigning_a_serviceaccount_to_a_pod">Example 2-2</a> shows the definition of a ServiceAccount to a Pod.</p>
<div id="a_yaml_manifest_assigning_a_serviceaccount_to_a_pod" data-type="example">
<h5><span class="label">Example 2-2. </span>A YAML manifest assigning a ServiceAccount to a Pod</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">build-observer</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">serviceAccountName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">build-bot</code><code class="w"/>
<code class="p">...</code><code class="w"/></pre></div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Understanding RBAC API Primitives"><div class="sect2" id="idm45322732308208">
<h2>Understanding RBAC API Primitives</h2>

<p><a data-type="indexterm" data-primary="RoleBinding API primitive" data-secondary="about" id="idm45322731982784"/><a data-type="indexterm" data-primary="API primitives" id="idm45322731981904"/><a data-type="indexterm" data-primary="primitives" id="idm45322731981232"/><a data-type="indexterm" data-primary="Role API primitive" id="idm45322732321248"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="API primitives" id="idm45322732320576"/>With those key concepts in mind, let’s take a look at the Kubernetes API primitives that implement the RBAC functionality:</p>
<dl>
<dt>Role</dt>
<dd>
<p>The Role API primitive declares the API resources and their operations this rule should operate on. For example, you may want to say “allow listing and deleting of Pods,” or you may express “allow watching the logs of Pods,” or even both with the same Role. Any operation that is not spelled out explicitly is disallowed as soon as it is bound to the subject.</p>
</dd>
<dt>RoleBinding</dt>
<dd>
<p>The RoleBinding API primitive <em>binds</em> the Role object to the subject(s). It is the glue for making the rules active. For example, you may want to say “bind the Role that permits updating Services to the user John Doe.”</p>
</dd>
</dl>

<p><a data-type="xref" href="#rbac_primitives">Figure 2-2</a> shows the relationship between the involved API primitives. Keep in mind that the image renders only a selected list of API resource types and operations.</p>

<figure><div id="rbac_primitives" class="figure"><div class="border-box"><img src="Images/ckas_0202.png" alt="ckas 0202" width="1276" height="426"/></div><h6><span class="label">Figure 2-2. </span>RBAC primitives</h6></div></figure>

<p>The following sections demonstrate the namespace-wide usage of Roles and RoleBindings, but the same operations and attributes apply to cluster-wide Roles and RoleBindings, discussed in <a data-type="xref" href="#cluster-wide-rbac">“Namespace-wide and Cluster-wide RBAC”</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Default User-Facing Roles"><div class="sect2" id="idm45322732311488">
<h2>Default User-Facing Roles</h2>

<p><a data-type="indexterm" data-primary="default user-facing roles" id="idm45322732310320"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="default user-facing roles" id="idm45322732309648"/><a data-type="indexterm" data-primary="roles" data-secondary="default user-facing" id="idm45322732280240"/><a data-type="indexterm" data-primary="cluster-admin role" id="idm45322732279296"/><a data-type="indexterm" data-primary="admin role" id="idm45322732278624"/><a data-type="indexterm" data-primary="edit role" id="idm45322732277952"/><a data-type="indexterm" data-primary="view role" id="idm45322732277280"/>Kubernetes defines a set of default Roles. You can assign them to a subject via a RoleBinding or define your own, custom Roles depending on your needs. <a data-type="xref" href="#default_user_facing_roles">Table 2-2</a> describes the default user-facing Roles.</p>
<table id="default_user_facing_roles">
<caption><span class="label">Table 2-2. </span>Default User-Facing Roles</caption>
<thead>
<tr>
<th>Default ClusterRole</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>cluster-admin</p></td>
<td><p>Allows read and write access to resources across all namespaces.</p></td>
</tr>
<tr>
<td><p>admin</p></td>
<td><p>Allows read and write access to resources in namespace including Roles and RoleBindings.</p></td>
</tr>
<tr>
<td><p>edit</p></td>
<td><p>Allows read and write access to resources in namespace except Roles and RoleBindings. Provides access to Secrets.</p></td>
</tr>
<tr>
<td><p>view</p></td>
<td><p>Allows read-only access to resources in namespace except Roles, RoleBindings, and Secrets.</p></td>
</tr>
</tbody>
</table>

<p>To define new Roles and RoleBindings, you will have to use a context that allows for creating or modifying them, that is, cluster-admin or admin.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating Roles"><div class="sect2" id="idm45322732264848">
<h2>Creating Roles</h2>

<p><a data-type="indexterm" data-primary="create role command" id="idm45322732263296"/><a data-type="indexterm" data-primary="commands" data-secondary="create role" id="idm45322732262432"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="creating roles" id="idm45322732261488"/><a data-type="indexterm" data-primary="roles" data-secondary="creating" id="idm45322732260528"/>Roles can be created imperatively with the <code>create role</code> command. The most important options for the command are <code>--verb</code> for defining the verbs aka operations, and <code>--resource</code> for declaring a list of API resources. The following command creates a new Role for the resources Pod, Deployment, and Service with the verbs <code>list</code>, <code>get</code>, and <code>watch</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl create role read-only --verb=list,get,watch</strong> \
  <strong>--resource=pods,deployments,services</strong>
role.rbac.authorization.k8s.io/read-only created</pre>

<p>Declaring multiple verbs and resources for a single imperative <code>create role</code> command can be declared as a comma-separated list for the corresponding command-line option or as multiple arguments. For example, <code>--verb=list,get,watch</code> and <code>--verb=list --verb=get --verb=watch</code> carry the same instructions. You may also use the wildcard “*” to refer to all verbs or resources.</p>

<p>The command-line option <code>--resource-name</code> spells out one or many object names that the policy rules should apply to. A name of a Pod could be <code>nginx</code> and listed here with its name. Providing a list of resource names is optional. If no names have been provided, then the provided rules apply to all objects of a resource type.</p>

<p>The declarative approach can become a little lengthy. As you can see in <a data-type="xref" href="#a_yaml_manifest_defining_a_role">Example 2-3</a>, the section <code>rules</code> lists the resources and verbs. Resources with an API group, like Deployments that use the API version <code>apps/v1</code>, need to explicitly declare it under the attribute <code>apiGroups</code>. All other resources (e.g., Pods and Services), simply use an empty string as their API version doesn’t contain a group. Be aware that the imperative command for creating a Role automatically determines the API group.</p>
<div id="a_yaml_manifest_defining_a_role" data-type="example">
<h5><span class="label">Example 2-3. </span>A YAML manifest defining a Role</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Role</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">read-only</code><code class="w"/>
<code class="nt">rules</code><code class="p">:</code><code class="w"/>
<code class="p-Indicator">-</code><code class="w"> </code><code class="nt">apiGroups</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">""</code><code class="w"/>
<code class="w">  </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">pods</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">services</code><code class="w"/>
<code class="w">  </code><code class="nt">verbs</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">list</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">get</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">watch</code><code class="w"/>
<code class="p-Indicator">-</code><code class="w"> </code><code class="nt">apiGroups</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">apps</code><code class="w"/>
<code class="w">  </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">deployments</code><code class="w"/>
<code class="w">  </code><code class="nt">verbs</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">list</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">get</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">watch</code><code class="w"/></pre></div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Listing Roles"><div class="sect2" id="idm45322732236256">
<h2>Listing Roles</h2>

<p><a data-type="indexterm" data-primary="roles" data-secondary="listing" id="idm45322732235248"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="listing roles" id="idm45322733514544"/>Once the Role has been created, its object can be listed. The list of Roles renders only the name and the creation timestamp. Each of the listed roles does not give away any of its details:</p>

<pre data-type="programlisting"><strong>$ kubectl get roles</strong>
NAME        CREATED AT
read-only   2021-06-23T19:46:48Z</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Rendering Role Details"><div class="sect2" id="idm45322733512304">
<h2>Rendering Role Details</h2>

<p><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="rendering role details" id="idm45322733511104"/><a data-type="indexterm" data-primary="roles" data-secondary="rendering details of" id="idm45322733510160"/><a data-type="indexterm" data-primary="describe command" id="idm45322733509216"/><a data-type="indexterm" data-primary="commands" data-secondary="describe" id="idm45322733508544"/>You can inspect the details of a Role using the <code>describe</code> command. The output renders a table that maps a resource to its permitted verbs. This cluster has no resources created, so the list of resource names in the following console output is empty:</p>

<pre data-type="programlisting"><strong>$ kubectl describe role read-only</strong>
Name:         read-only
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
PolicyRule:
  Resources         Non-Resource URLs  Resource Names  Verbs
  ---------         -----------------  --------------  -----
  pods              []                 []              [list get watch]
  services          []                 []              [list get watch]
  deployments.apps  []                 []              [list get watch]</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating RoleBindings"><div class="sect2" id="idm45322733505856">
<h2>Creating RoleBindings</h2>

<p><a data-type="indexterm" data-primary="create rolebinding command" id="idm45322733504448"/><a data-type="indexterm" data-primary="commands" data-secondary="create rolebinding" id="idm45322733503584"/><a data-type="indexterm" data-primary="RoleBinding API primitive" data-secondary="creating" id="idm45322733502608"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="creating RoleBindings" id="idm45322733501696"/>The imperative command creating a RoleBinding object is <code>create rolebinding</code>. To bind a Role to the RoleBinding, use the <code>--role</code> command-line option. The subject type can be assigned by declaring the options <code>--user</code>, <code>--group</code>, or <code>--serviceaccount</code>. The following command creates the RoleBinding with the name <code>read-only-binding</code> to the user called <code>johndoe</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl create rolebinding read-only-binding --role=read-only --user=johndoe</strong>
rolebinding.rbac.authorization.k8s.io/read-only-binding created</pre>

<p><a data-type="xref" href="#a_yaml_manifest_defining_a_rolebinding">Example 2-4</a> shows a YAML manifest representing the RoleBinding. You can see from the structure that a role can be mapped to one or many subjects. The data type is an array indicated by the dash character under the attribute <code>subjects</code>. At this time, only the user <code>johndoe</code> has been assigned.</p>
<div id="a_yaml_manifest_defining_a_rolebinding" data-type="example">
<h5><span class="label">Example 2-4. </span>A YAML manifest defining a RoleBinding</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">RoleBinding</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">read-only-binding</code><code class="w"/>
<code class="nt">roleRef</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">apiGroup</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io</code><code class="w"/>
<code class="w">  </code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Role</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">read-only</code><code class="w"/>
<code class="nt">subjects</code><code class="p">:</code><code class="w"/>
<code class="p-Indicator">-</code><code class="w"> </code><code class="nt">apiGroup</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io</code><code class="w"/>
<code class="w">  </code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">User</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">johndoe</code><code class="w"/></pre></div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Listing RoleBindings"><div class="sect2" id="idm45322733631472">
<h2>Listing RoleBindings</h2>

<p><a data-type="indexterm" data-primary="RoleBinding API primitive" data-secondary="listing" id="idm45322733630464"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="listing RoleBindings" id="idm45322733629552"/>The most important information the list of RoleBindings gives away is the associated Role. The following command shows that the RoleBinding <code>read-only-binding</code> has been mapped to the Role <code>read-only</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl get rolebindings</strong>
NAME                ROLE             AGE
read-only-binding   Role/read-only   24h</pre>

<p>The output does not provide an indication of the subjects. You will need to render the details of the object for more information, as described in the next section.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Rendering RoleBinding Details"><div class="sect2" id="idm45322727911568">
<h2>Rendering RoleBinding Details</h2>

<p><a data-type="indexterm" data-primary="RoleBinding API primitive" data-secondary="rendering details of" id="idm45322727910400"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="rendering RoleBinding details" id="idm45322727909360"/><a data-type="indexterm" data-primary="describe command" id="idm45322727908384"/><a data-type="indexterm" data-primary="commands" data-secondary="describe" id="idm45322727907712"/>RoleBindings can be inspected using the <code>describe</code> command. The output renders a table of subjects and the assigned role. The following example renders the descriptive representation of the RoleBinding named <code>read-only-binding</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl describe rolebinding read-only-binding</strong>
Name:         read-only-binding
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  Role
  Name:  read-only
Subjects:
  Kind  Name     Namespace
  ----  ----     ---------
  User  johndoe</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Seeing the RBAC Rules in Effect"><div class="sect2" id="idm45322727904576">
<h2>Seeing the RBAC Rules in Effect</h2>

<p><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="viewing rules in effect" id="idm45322727896512"/><a data-type="indexterm" data-primary="cluster-admin role" id="idm45322727895664"/>Let’s see how Kubernetes enforces the RBAC rules for the scenario we set up so far. First, we’ll create a new Deployment with the <code>cluster-admin</code> credentials. In Minikube, this user is assigned to the context <code>minikube</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl config current-context</strong>
minikube
<strong>$ kubectl create deployment myapp --image=nginx --port=80 --replicas=2</strong>
deployment.apps/myapp created</pre>

<p>Now, we’ll switch the context for the user <code>johndoe</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl config use-context johndoe-context</strong>
Switched to context "johndoe-context".</pre>

<p>Remember that the user <code>johndoe</code> is permitted to list deployments. We’ll verify that by using the <code>get deployments</code> command:</p>

<pre data-type="programlisting"><strong>$ kubectl get deployments</strong>
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
myapp   2/2     2            2           8s</pre>

<p>The RBAC rules only allow listing Deployments, Pods, and Services. The following command tries to list the ReplicaSets, which results in an error:</p>

<pre data-type="programlisting"><strong>$ kubectl get replicasets</strong>
Error from server (Forbidden): replicasets.apps is forbidden: User "johndoe" \
cannot list resource "replicasets" in API group "apps" in the namespace "default"</pre>

<p class="pagebreak-before">A similar behavior can be observed when trying to use other verbs than <code>list</code>, <code>get</code>, or <code>watch</code>. The following command tries to delete a Deployment:</p>

<pre data-type="programlisting"><strong>$ kubectl delete deployment myapp</strong>
Error from server (Forbidden): deployments.apps "myapp" is forbidden: User \
"johndoe" cannot delete resource "deployments" in API group "apps" in the \
namespace "default"</pre>

<p>At any given time, you can check a user’s permissions with the <code>auth can-i</code> command. The command gives you the option to list all permissions or check a specific permission:</p>

<pre data-type="programlisting"><strong>$ kubectl auth can-i --list --as johndoe</strong>
Resources          Non-Resource URLs   Resource Names   Verbs
...
pods               []                  []               [list get watch]
services           []                  []               [list get watch]
deployments.apps   []                  []               [list get watch]
<strong>$ kubectl auth can-i list pods --as johndoe</strong>
yes</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Namespace-wide and Cluster-wide RBAC"><div class="sect2" id="cluster-wide-rbac">
<h2>Namespace-wide and Cluster-wide RBAC</h2>

<p><a data-type="indexterm" data-primary="namespace-wide RBAC" id="idm45322727880000"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="namespace-wide" id="idm45322727879296"/><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="cluster-wide" id="idm45322727878288"/><a data-type="indexterm" data-primary="cluster-wide RBAC" id="idm45322727877328"/>Roles and RoleBindings apply to a particular namespace. You will have to specify the namespace at the time of creating both objects. Sometimes, a set of Roles and Rolebindings needs to apply to multiple namespaces or even the whole cluster. For a cluster-wide definition, Kubernetes offers the API resource types ClusterRole and ClusterRoleBinding. The configuration elements are effectively the same. The only difference is the value of the <code>kind</code> attribute:</p>

<ul>
<li>
<p>To define a cluster-wide Role, use the imperative subcommand <code>clusterrole</code> or the kind <code>ClusterRole</code> in the YAML manifest.</p>
</li>
<li>
<p>To define a cluster-wide RoleBinding, use the imperative subcommand <code>clusterrolebinding</code> or the kind <code>ClusterRoleBinding</code> in the YAML manifest.</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Aggregating RBAC Rules"><div class="sect2" id="idm45322727871920">
<h2>Aggregating RBAC Rules</h2>

<p><a data-type="indexterm" data-primary="RBAC (role-based access control)" data-secondary="aggregating rules" id="idm45322727870352"/><a data-type="indexterm" data-primary="aggregation, of RBAC rules" id="idm45322727869360"/><a data-type="indexterm" data-primary="ClusterRoles, aggregating" id="idm45322727868672"/>Existing ClusterRoles can be aggregated to avoid having to redefine a new, composed set of rules that likely leads to duplication of instructions. For example, say you wanted to combine a user-facing role with a custom Role. An aggregated ClusterRule can merge rules via label selection without having to copy-paste the existing rules into one.</p>

<p>Say we defined two ClusterRoles shown in Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#a_yaml_manifest_defining_a_clusterrole_for_listing_pods">2-5</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#a_yaml_manifest_defining_a_clusterrole_for_deleting_services">2-6</a>. The ClusterRole <code>list-pods</code> allows for listing Pods and the ClusterRole <code>delete-services</code> allows for deleting Services.</p>
<div id="a_yaml_manifest_defining_a_clusterrole_for_listing_pods" data-type="example">
<h5><span class="label">Example 2-5. </span>A YAML manifest defining a ClusterRole for listing Pods</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ClusterRole</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">list-pods</code><code class="w"/>
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac-example</code><code class="w"/>
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">rbac-pod-list</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="nt">rules</code><code class="p">:</code><code class="w"/>
<code class="p-Indicator">-</code><code class="w"> </code><code class="nt">apiGroups</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">""</code><code class="w"/>
<code class="w">  </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">pods</code><code class="w"/>
<code class="w">  </code><code class="nt">verbs</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">list</code><code class="w"/></pre></div>
<div id="a_yaml_manifest_defining_a_clusterrole_for_deleting_services" data-type="example">
<h5><span class="label">Example 2-6. </span>A YAML manifest defining a ClusterRole for deleting Services</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ClusterRole</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">delete-services</code><code class="w"/>
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac-example</code><code class="w"/>
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">rbac-service-delete</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="nt">rules</code><code class="p">:</code><code class="w"/>
<code class="p-Indicator">-</code><code class="w"> </code><code class="nt">apiGroups</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">""</code><code class="w"/>
<code class="w">  </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">services</code><code class="w"/>
<code class="w">  </code><code class="nt">verbs</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">delete</code><code class="w"/></pre></div>

<p>To aggregate those rules, ClusterRoles can specify an <code>aggregationRule</code>. This attribute describes the label selection rules. <a data-type="xref" href="#a_yaml_manifest_defining_a_clusterrole_with_aggregated_fules">Example 2-7</a> shows an aggregated ClusterRole defined by an array of <code>matchLabels</code> criteria. The ClusterRole does not add its own rules as indicated by <code>rules:</code> <code>[]</code>; however, there’s no limiting factor that would disallow it.</p>
<div id="a_yaml_manifest_defining_a_clusterrole_with_aggregated_fules" data-type="example">
<h5><span class="label">Example 2-7. </span>A YAML manifest defining a ClusterRole with aggregated rules</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ClusterRole</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">pods-services-aggregation-rules</code><code class="w"/>
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac-example</code><code class="w"/>
<code class="nt">aggregationRule</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">clusterRoleSelectors</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">rbac-pod-list</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">rbac-service-delete</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="nt">rules</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[]</code><code class="w"/></pre></div>

<p>We can verify the proper aggregation behavior of the ClusterRole by describing the object. You can see in the following output that both ClusterRoles,  <code>list-pods</code> and <code>delete-services</code>, have been taken into account:</p>

<pre data-type="programlisting"><strong>$ kubectl describe clusterroles pods-services-aggregation-rules -n rbac-example</strong>
Name:         pods-services-aggregation-rules
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  services   []                 []              [delete]
  pods       []                 []              [list]</pre>

<p><a data-type="indexterm" data-primary="" data-startref="clu_rbac" id="idm45322727659856"/>For more information on ClusterRole label selection rules, see the <a href="https://oreil.ly/J6k3m">official documentation</a>. The page also explains how to aggregate the default user-facing ClusterRoles.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Creating and Managing a Kubernetes Cluster"><div class="sect1" id="idm45322734130880">
<h1>Creating and Managing a Kubernetes Cluster</h1>

<p><a data-type="indexterm" data-primary="clusters" data-secondary="creating and managing" id="clu_cm"/>When thinking about the typical tasks of a Kubernetes administrator, I am sure that at least one of the following bread-and-butter activities comes to mind:</p>

<ul>
<li>
<p>Bootstrapping a control plane node</p>
</li>
<li>
<p>Bootstrapping worker nodes and joining them to the cluster</p>
</li>
<li>
<p>Upgrading a cluster to a newer version</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="kubeadm tool" id="idm45322727652160"/>The low-level command-line tool for performing cluster bootstrapping operations is called <code>kubeadm</code>. It is not meant for provisioning the underlying infrastructure. That’s the purpose of infrastructure automation tools like Ansible and Terraform. To install <code>kubeadm</code>, follow the <a href="https://oreil.ly/gKq4m">installation instructions</a> in the official Kubernetes 
<span class="keep-together">documentation.</span></p>

<p>While not explicitly stated in the CKA frequently asked questions (FAQ) page, you can assume that the <code>kubeadm</code> executable has been preinstalled for you. The following sections describe the processes for creating and managing a Kubernetes cluster on a high level and will use <code>kubeadm</code> heavily. For more detailed information, see the step-by-step Kubernetes reference documentation I will point out for each of the tasks.</p>








<section data-type="sect2" data-pdf-bookmark="Installing a Cluster"><div class="sect2" id="idm45322727594240">
<h2>Installing a Cluster</h2>

<p><a data-type="indexterm" data-primary="clusters" data-secondary="installing" id="clu_ins"/><a data-type="indexterm" data-primary="installing" data-secondary="clusters" id="ins_clu"/><a data-type="indexterm" data-primary="Minikube" id="idm45322727590288"/><a data-type="indexterm" data-primary="Docker Desktop" id="idm45322727589616"/>The most basic topology of a Kubernetes cluster consists of a single node that acts as the control plane and the worker node at the same time. By default, many developer-centric Kubernetes installations like minikube or Docker Desktop start with this configuration. While a single-node cluster may be a good option for a Kubernetes playground, it is not a good foundation for scalability and high-availability reasons. At the very least, you will want to create a cluster with a single control plane and one or many nodes handling the workload.</p>

<p>This section explains how to install a cluster with a single control plane and one worker node. You can repeat the worker node installation process to add more worker nodes to the cluster. You can find a full description of the <a href="https://oreil.ly/8visY">installation steps</a> in the official Kubernetes documentation. <a data-type="xref" href="#cluster_installation_process">Figure 2-3</a> illustrates the installation process.</p>

<figure><div id="cluster_installation_process" class="figure"><div class="border-box"><img src="Images/ckas_0203.png" alt="ckas 0203" width="1294" height="1026"/></div><h6><span class="label">Figure 2-3. </span>Process for a cluster installation process</h6></div></figure>










<section data-type="sect3" data-pdf-bookmark="Initializing the Control Plane Node"><div class="sect3" id="idm45322727584752">
<h3>Initializing the Control Plane Node</h3>

<p><a data-type="indexterm" data-primary="control plane nodes" data-secondary="initializing" id="idm45322727583248"/>Start by initializing the control plane on the control plane node. The control plane is the machine responsible for hosting the API server, etcd, and other components important to managing the Kubernetes cluster.</p>

<p>Open an interactive shell to the control plane node using the <code>ssh</code> command. The following command targets the control plane node named <code>kube-control-plane</code> running Ubuntu 18.04.5 LTS:</p>

<pre data-type="programlisting"><strong>$ ssh kube-control-plane</strong>
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)
...</pre>

<p><a data-type="indexterm" data-primary="kubeadm init command" id="idm45322727579600"/><a data-type="indexterm" data-primary="commands" data-secondary="kubeadm init" id="idm45322727578896"/>Initialize the control plane using the <code>kubeadm init</code> command. You will need to add the following two command-line options: provide the IP addresses for the Pod network with the option <code>--pod-network-cidr</code>. With the option <code>--apiserver-advertise-address</code>, you can declare the IP address the API Server will advertise to listen on.</p>

<p><a data-type="indexterm" data-primary="kubeadm join command" id="idm45322727576224"/><a data-type="indexterm" data-primary="commands" data-secondary="kubeadm join" id="idm45322727575520"/>The console output renders a <code>kubeadm join</code> command. Keep that command around for later. It is important for joining worker nodes to the cluster in a later step.</p>
<div data-type="tip"><h1>Retrieving the join command for worker nodes</h1>
<p><a data-type="indexterm" data-primary="join command" id="idm45322727572720"/><a data-type="indexterm" data-primary="commands" data-secondary="join" id="idm45322727572112"/>You can always retrieve the <code>join</code> command by running <code>kubeadm token create --print-join-command</code> on the control plane node should you lose it.</p>
</div>

<p><a data-type="indexterm" data-primary="Classless Inter-Domain Routing (CIDR)" id="idm45322727569792"/><a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing)" id="idm45322727569024"/>The following command uses <code>172.18.0.0/16</code> for the Classless Inter-Domain Routing (CIDR) and IP address <code>10.8.8.10</code> for the API server:</p>

<pre data-type="programlisting"><strong>$ sudo kubeadm init --pod-network-cidr 172.18.0.0/16</strong> \
  <strong>--apiserver-advertise-address 10.8.8.10</strong>
...
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on \
each as root:

kubeadm join 10.8.8.10:6443 --token fi8io0.dtkzsy9kws56dmsp \
    --discovery-token-ca-cert-hash \
    sha256:cc89ea1f82d5ec460e21b69476e0c052d691d0c52cce83fbd7e403559c1ebdac</pre>

<p><a data-type="indexterm" data-primary="init command" id="idm45322727565616"/><a data-type="indexterm" data-primary="commands" data-secondary="init" id="idm45322727564912"/>After the <code>init</code> command has finished, run the necessary commands from the console output to start the cluster as nonroot user:</p>

<pre data-type="programlisting"><strong>$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</strong></pre>

<p><a data-type="indexterm" data-primary="Container Network Interface (CNI) plugin" id="idm45322727562192"/><a data-type="indexterm" data-primary="CNI (Container Network Interface) plugin" id="idm45322727561392"/><a data-type="indexterm" data-primary="Calico" id="idm45322727560688"/><a data-type="indexterm" data-primary="Flannel" id="idm45322727560016"/><a data-type="indexterm" data-primary="Weave Net" id="idm45322727559344"/>You must deploy a <a href="https://oreil.ly/t6eJ7">Container Network Interface (CNI) plugin</a> so that Pods can communicate with each other. You can pick from a wide range of networking plugins listed in the <a href="https://oreil.ly/1Y7MF">Kubernetes documentation</a>. Popular plugins include Flannel, Calico, and Weave Net. Sometimes you will see the term “add-ons” in the documentation, which is synonymous with plugin.</p>

<p>The CKA exam will most likely ask you to install a specific add-on. Most of the installation instructions live on external web pages, not permitted to be used during the exam. Make sure that you search for the relevant instructions in the official Kubernetes documentation. For example, you can find the installation instructions for Weave Net <a href="https://oreil.ly/86YpI">here</a>. The following command installs the Weave Net objects:</p>
<pre data-type="programlisting">
<strong>$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=</strong> \
  <strong>$(kubectl version | base64 | tr -d '\n')"</strong>
serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.apps/weave-net created
</pre>

<p>Verify that the control plane node indicates the “Ready” status using the command <code>kubectl get nodes</code>. It might take a couple of seconds before the node transitions from the “NotReady” status to the “Ready” status. You have an issue with your node installation in case the status transition does not occur. Refer to <a data-type="xref" href="ch07.xhtml#troubleshooting">Chapter 7</a> for debugging strategies:</p>

<pre data-type="programlisting"><strong>$ kubectl get nodes</strong>
NAME                 STATUS   ROLES                  AGE   VERSION
kube-control-plane   Ready    control-plane,master   24m   v1.21.2</pre>

<p><a data-type="indexterm" data-primary="commands" data-secondary="exit" id="idm45322727551360"/><a data-type="indexterm" data-primary="exit command" id="idm45322727550384"/>Exit the control plane node using the <code>exit</code> command:</p>

<pre data-type="programlisting"><strong>$ exit</strong>
logout
...</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Joining the Worker Nodes"><div class="sect3" id="idm45322727584128">
<h3>Joining the Worker Nodes</h3>

<p><a data-type="indexterm" data-primary="worker nodes" data-secondary="joining" id="idm45322727546624"/>Worker nodes are responsible for handling the workload scheduled by the control plane. Examples of workloads are Pods, Deployments, Jobs, and CronJobs. To add a worker node to the cluster so that it can be used, you will have to run a couple of commands, as described next.</p>

<p><a data-type="indexterm" data-primary="ssh command" id="idm45322727545264"/><a data-type="indexterm" data-primary="commands" data-secondary="ssh" id="idm45322727544560"/>Open an interactive shell to the worker node using the <code>ssh</code> command. The following command targets the worker node named <code>kube-worker-1</code> running Ubuntu 18.04.5 LTS:</p>

<pre data-type="programlisting"><strong>$ ssh kube-worker-1</strong>
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)
...</pre>

<p><a data-type="indexterm" data-primary="kubeadm join command" id="idm45322727541232"/><a data-type="indexterm" data-primary="commands" data-secondary="kubeadm join" id="idm45322727540528"/>Run the <code>kubeadm join</code> command provided by the <code>kubeadm init</code> console output on the control plane node. The following command shows an example. Remember that the token and SHA256 hash will be different for you:</p>
<pre data-type="programlistinng">
<strong>$ sudo kubeadm join 10.8.8.10:6443 --token fi8io0.dtkzsy9kws56dmsp</strong> \
  <strong>--discovery-token-ca-cert-hash</strong> \
  <strong>sha256:cc89ea1f82d5ec460e21b69476e0c052d691d0c52cce83fbd7e403559c1ebdac</strong>
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with \
'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file \
"/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with \
flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control plane to see this node join the cluster.
</pre>

<p><a data-type="indexterm" data-primary="kubectl get nodes command" id="idm45322727535888"/><a data-type="indexterm" data-primary="commands" data-secondary="kubectl get nodes" id="idm45322727535120"/><a data-type="indexterm" data-primary="" data-startref="clu_ins" id="idm45322727534176"/><a data-type="indexterm" data-primary="" data-startref="ins_clu" id="idm45322727533232"/>You won’t be able to run the <code>kubectl get nodes</code> command from the worker node without copying the administrator kubeconfig file from the control plane node. Follow the <a href="https://oreil.ly/AIM8a">instructions</a> in the Kubernetes documentation to do so or log back into the control plane node. Here, we are just going to log back into the control plane node. You should see that the worker node has joined the cluster and is in a “Ready” status:</p>

<pre data-type="programlisting"><strong>$ ssh kube-control-plane</strong>
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)
...
<strong>$ kubectl get nodes</strong>
NAME                 STATUS   ROLES                  AGE     VERSION
kube-control-plane   Ready    control-plane,master   5h49m   v1.21.2
kube-worker-1        Ready    &lt;none&gt;                 15m     v1.21.2</pre>

<p>You can repeat the process for any other worker node you want to add to the cluster.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Managing a Highly Available Cluster"><div class="sect2" id="managing_ha_cluster">
<h2>Managing a Highly Available Cluster</h2>

<p><a data-type="indexterm" data-primary="HA (high-availability) clusters" id="idm45322727526960"/><a data-type="indexterm" data-primary="high-availability (HA) clusters" id="idm45322727526192"/><a data-type="indexterm" data-primary="clusters" data-secondary="managing highly available" id="idm45322727525504"/>Single control plane clusters are easy to install; however, they present an issue when the node is lost. Once the control plane node becomes unavailable, any ReplicaSet running on a worker node cannot re-create a Pod due to the inability to talk back to the scheduler running on a control plane node. Moreover, clusters cannot be accessed externally anymore (e.g., via <code>kubectl</code>), as the API server cannot be reached.</p>

<p>High-availability (HA) clusters help with scalability and redundancy. For the exam, you will need to have a basic understanding about configuring them and their implications. Given the complexity of standing up an HA cluster, it’s unlikely that you’ll be asked to perform the steps during the exam. For a full discussion on setting up HA clusters, see the <a href="https://oreil.ly/17ZDL">relevant page</a> in the Kubernetes documentation.</p>

<p><a data-type="indexterm" data-primary="stacked etcd topology" id="idm45322727522496"/>The <em>stacked etcd topology</em> involves creating two or more control plane nodes where etcd is colocated on the node. <a data-type="xref" href="#stacked_etcd_topology">Figure 2-4</a> shows a representation of the topology with three control plane nodes.</p>

<figure><div id="stacked_etcd_topology" class="figure"><div class="border-box"><img src="Images/ckas_0204.png" alt="ckas 0204" width="932" height="707"/></div><h6><span class="label">Figure 2-4. </span>Stacked etcd topology with three control plane nodes</h6></div></figure>

<p>Each of the control plane nodes hosts the API server, the scheduler, and the 
<span class="keep-together">controller manager.</span> Worker nodes communicate with the API server through a load balancer. It is recommended to operate this cluster topology with a minimum of 
<span class="keep-together">three control</span> plane nodes for redundancy reasons due to the tight coupling of etcd to the control plane node. By default, <code>kubeadm</code> will create an etcd instance when joining a control plane node to the cluster.</p>

<p class="pagebreak-before"><a data-type="indexterm" data-primary="external etcd topology" id="idm45322727515072"/>The <em>external etcd node</em> topology separates etcd from the control plane node by running it on a dedicated machine. <a data-type="xref" href="#external_etcd_node_topology">Figure 2-5</a> shows a setup with three control plane nodes, each of which run etcd on a different machine.</p>

<figure><div id="external_etcd_node_topology" class="figure"><div class="border-box"><img src="Images/ckas_0205.png" alt="ckas 0205" width="1175" height="628"/></div><h6><span class="label">Figure 2-5. </span>External etcd node topology</h6></div></figure>

<p>Similar to the stacked etcd topology, each control plane node hosts the API server, the scheduler, and the controller manager. The worker nodes communicate with them through a load balancer. The main difference here is that the etcd instances run on a separate host. This topology decouples etcd from other control plane functionality and therefore has less of an impact on redundancy when a control plane node is lost. As you can see in the illustration, this topology requires twice as many hosts as the stacked etcd topology.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Upgrading a Cluster Version"><div class="sect2" id="idm45322727528320">
<h2>Upgrading a Cluster Version</h2>

<p><a data-type="indexterm" data-primary="clusters" data-secondary="upgrading versions of" id="clu_up"/><a data-type="indexterm" data-primary="upgrading" data-secondary="cluster versions" id="up_clu"/>Over time, you will want to upgrade the Kubernetes version of an existing cluster to pick up bug fixes and new features. The upgrade process has to be performed in a controlled manner to avoid the disruption of workload currently in execution and to prevent the corruption of cluster nodes.</p>

<p>It is recommended to upgrade from a minor version to a next higher one (e.g., from 1.18.0 to 1.19.0), or from a patch version to a higher one (e.g., from 1.18.0 to 1.18.3). Abstain from jumping up multiple minor versions to avoid unexpected side effects. You can find a full description of the <a href="https://oreil.ly/2dCfk">upgrade steps</a> in the official Kubernetes documentation. <a data-type="xref" href="#cluster_version_upgrade">Figure 2-6</a> illustrates the upgrade process.</p>

<figure><div id="cluster_version_upgrade" class="figure"><div class="border-box"><img src="Images/ckas_0206.png" alt="ckas 0206" width="1439" height="1696"/></div><h6><span class="label">Figure 2-6. </span>Process for a cluster version upgrade</h6></div></figure>










<section data-type="sect3" data-pdf-bookmark="Upgrading control plane nodes"><div class="sect3" id="idm45322727502176">
<h3>Upgrading control plane nodes</h3>

<p><a data-type="indexterm" data-primary="control plane nodes" data-secondary="upgrading" id="cpn_up"/><a data-type="indexterm" data-primary="upgrading" data-secondary="control plane nodes" id="up_cpn"/>As explained earlier, a Kubernetes cluster may employ one or many control plane nodes to better support high-availability and scalability concerns. When upgrading a cluster version, this change needs to happen for control plane nodes one at a time.</p>

<p><a data-type="indexterm" data-primary="ssh command" id="idm45322727497440"/><a data-type="indexterm" data-primary="commands" data-secondary="ssh" id="idm45322727496736"/>Pick one of the control plane nodes that contains the kubeconfig file (located at <code>/etc/kubernetes/admin.conf</code>), and open an interactive shell to the control plane node using the <code>ssh</code> command. The following command targets the control plane node named <code>kube-control-plane</code> running Ubuntu 18.04.5 LTS:</p>

<pre data-type="programlisting"><strong>$ ssh kube-control-plane</strong>
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)
...</pre>

<p>First, check the nodes and their Kubernetes versions. In this setup, all nodes run on version 1.18.0. We are dealing with only a single control plane node and a single worker node:</p>

<pre data-type="programlisting"><strong>$ kubectl get nodes</strong>
NAME                 STATUS   ROLES    AGE     VERSION
kube-control-plane   Ready    master   4m54s   v1.18.0
kube-worker-1        Ready    &lt;none&gt;   3m18s   v1.18.0</pre>

<p>Start by upgrading the <code>kubeadm</code> version. Identify the version you’d like to upgrade to. On Ubuntu machines, you can use the following <code>apt-get</code> command. The version format usually includes a patch version (e.g., <code>1.20.7-00</code>). Check the Kubernetes documentation if your machine is running a different operating system:</p>

<pre data-type="programlisting"><strong>$ sudo apt update</strong>
...
<strong>$ sudo apt-cache madison kubeadm</strong>
   kubeadm |  1.21.2-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.21.1-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.21.0-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.8-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.7-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.6-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.5-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.4-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.2-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.1-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
   kubeadm |  1.20.0-00 | http://apt.kubernetes.io kubernetes-xenial/main \
   amd64 Packages
...</pre>

<p>Upgrade <code>kubeadm</code> to a target version. Say you’d want to upgrade to version <code>1.19.0-00</code>. The following series of commands installs <code>kubeadm</code> with that specific version and checks the currently installed version to verify:</p>

<pre data-type="programlisting"><strong>$ sudo apt-mark unhold kubeadm &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install</strong> \
  <strong>-y kubeadm=1.19.0-00 &amp;&amp; sudo apt-mark hold kubeadm</strong>
Canceled hold on kubeadm.
...
Unpacking kubeadm (1.19.0-00) over (1.18.0-00) ...
Setting up kubeadm (1.19.0-00) ...
kubeadm set on hold.
<strong>$ sudo apt-get update &amp;&amp; sudo apt-get install -y --allow-change-held-packages</strong> \
  <strong>kubeadm=1.19.0-00</strong>
...
kubeadm is already the newest version (1.19.0-00).
0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.
<strong>$ kubeadm version</strong>
kubeadm version: &amp;version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.0", \
GitCommit:"e19964183377d0ec2052d1f1fa930c4d7575bd50", GitTreeState:"clean", \
BuildDate:"2020-08-26T14:28:32Z", GoVersion:"go1.15", Compiler:"gc", \
Platform:"linux/amd64"}</pre>

<p>Check which versions are available to upgrade to and validate whether your current cluster is upgradable. You can see in the output of the following command that we could upgrade to version <code>1.19.12</code>. For now, we’ll stick with <code>1.19.0</code>:</p>

<pre data-type="programlisting"><strong>$ sudo kubeadm upgrade plan</strong>
...
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
I0708 17:32:53.037895   17430 version.go:252] remote version is much newer: \
v1.21.2; falling back to: stable-1.19
[upgrade/versions] Latest stable version: v1.19.12
[upgrade/versions] Latest version in the v1.18 series: v1.18.20
...
You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.19.12

Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.12.
...</pre>

<p>As described in the console output, we’ll start the upgrade for the control plane. The process may take a couple of minutes. You may have to upgrade the CNI plugin as well. Follow the provider instructions for more information:</p>

<pre data-type="programlisting"><strong>$ sudo kubeadm upgrade apply v1.19.0</strong>
...
[upgrade/version] You have chosen to change the cluster version to "v1.19.0"
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
...
[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.19.0". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed \
with upgrading your kubelets if you haven't already done so.</pre>

<p>Drain the control plane node by evicting the workload. Any new workload won’t be schedulable on the node until uncordoned:</p>

<pre data-type="programlisting"><strong>$ kubectl drain kube-control-plane --ignore-daemonsets</strong>
node/kube-control-plane cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-qndb9, \
kube-system/kube-proxy-vpvms
evicting pod kube-system/calico-kube-controllers-65f8bc95db-krp72
evicting pod kube-system/coredns-f9fd979d6-2brkq
pod/calico-kube-controllers-65f8bc95db-krp72 evicted
pod/coredns-f9fd979d6-2brkq evicted
node/kube-control-plane evicted</pre>

<p>Upgrade the kubelet and the <code>kubectl</code> tool to the same version:</p>

<pre data-type="programlisting"><strong>$ sudo apt-mark unhold kubelet kubectl &amp;&amp; sudo apt-get update &amp;&amp; sudo</strong> \
  <strong>apt-get install -y kubelet=1.19.0-00 kubectl=1.19.0-00 &amp;&amp; sudo apt-mark</strong> \
  <strong>hold kubelet kubectl</strong>
...
Setting up kubelet (1.19.0-00) ...
Setting up kubectl (1.19.0-00) ...
kubelet set on hold.
kubectl set on hold.</pre>

<p>Restart the kubelet process:</p>

<pre data-type="programlisting"><strong>$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet</strong></pre>

<p>Reenable the control plane node back so that the new workload can become 
<span class="keep-together">schedulable:</span></p>

<pre data-type="programlisting"><strong>$ kubectl uncordon kube-control-plane</strong>
node/kube-control-plane uncordoned</pre>

<p>The control plane nodes should now show the usage of Kubernetes 1.19.0:</p>

<pre data-type="programlisting"><strong>$ kubectl get nodes</strong>
NAME                 STATUS   ROLES    AGE   VERSION
kube-control-plane   Ready    master   21h   v1.19.0
kube-worker-1        Ready    &lt;none&gt;   21h   v1.18.0</pre>

<p><a data-type="indexterm" data-primary="" data-startref="cpn_up" id="idm45322727467888"/><a data-type="indexterm" data-primary="" data-startref="up_cpn" id="idm45322727466912"/><a data-type="indexterm" data-primary="exit command" id="idm45322727465968"/><a data-type="indexterm" data-primary="commands" data-secondary="exit" id="idm45322727465296"/>Exit the control plane node using the <code>exit</code> command:</p>

<pre data-type="programlisting"><strong>$ exit</strong>
logout
...</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Upgrading worker nodes"><div class="sect3" id="idm45322727501232">
<h3>Upgrading worker nodes</h3>

<p><a data-type="indexterm" data-primary="upgrading" data-secondary="worker nodes" id="idm45322727461616"/><a data-type="indexterm" data-primary="worker nodes" data-secondary="upgrading" id="idm45322727460640"/>Pick one of the worker nodes, and open an interactive shell to the node using the <code>ssh</code> command. The following command targets the worker node named <code>kube-worker-1</code> running Ubuntu 18.04.5 LTS:</p>

<pre data-type="programlisting"><strong>$ ssh kube-worker-1</strong>
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)
...</pre>

<p>Upgrade <code>kubeadm</code> to a target version. This is the same command you used for the control plane node, as explained earlier:</p>

<pre data-type="programlisting"><strong>$ sudo apt-mark unhold kubeadm &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install</strong> \
  <strong>-y kubeadm=1.19.0-00 &amp;&amp; sudo apt-mark hold kubeadm</strong>
Canceled hold on kubeadm.
...
Unpacking kubeadm (1.19.0-00) over (1.18.0-00) ...
Setting up kubeadm (1.19.0-00) ...
kubeadm set on hold.
<strong>$ kubeadm version</strong>
kubeadm version: &amp;version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.0", \
GitCommit:"e19964183377d0ec2052d1f1fa930c4d7575bd50", GitTreeState:"clean", \
BuildDate:"2020-08-26T14:28:32Z", GoVersion:"go1.15", Compiler:"gc", \
Platform:"linux/amd64"}</pre>

<p>Upgrade the kubelet configuration:</p>
<pre data-type="programlisting">
<strong>$ sudo kubeadm upgrade node</strong>
[upgrade] Reading configuration from the cluster...
[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system \
get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks
[preflight] Skipping prepull. Not a control plane node.
[upgrade] Skipping phase. Not a control plane node.
[kubelet-start] Writing kubelet configuration to file \
"/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your \
package manager.
</pre>

<p>Drain the worker node by evicting the workload. Any new workload won’t be schedulable on the node until uncordoned:</p>

<pre data-type="programlisting"><strong>$ kubectl drain kube-worker-1 --ignore-daemonsets</strong>
node/kube-worker-1 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-2hrxg, \
kube-system/kube-proxy-qf6nl
evicting pod kube-system/calico-kube-controllers-65f8bc95db-kggbr
evicting pod kube-system/coredns-f9fd979d6-7zm4q
evicting pod kube-system/coredns-f9fd979d6-tlmhq
pod/calico-kube-controllers-65f8bc95db-kggbr evicted
pod/coredns-f9fd979d6-7zm4q evicted
pod/coredns-f9fd979d6-tlmhq evicted
node/kube-worker-1 evicted</pre>

<p>Upgrade the kubelet and the <code>kubectl</code> tool with the same command used for the control plane node:</p>

<pre data-type="programlisting"><strong>$ sudo apt-mark unhold kubelet kubectl &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get</strong> \
<strong>install -y kubelet=1.19.0-00 kubectl=1.19.0-00 &amp;&amp; sudo apt-mark hold kubelet</strong> \
<strong>kubectl</strong>
...
Setting up kubelet (1.19.0-00) ...
Setting up kubectl (1.19.0-00) ...
kubelet set on hold.
kubectl set on hold.</pre>

<p>Restart the kubelet process:</p>

<pre data-type="programlisting"><strong>$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet</strong></pre>

<p>Reenable the worker node so that the new workload can become schedulable:</p>

<pre data-type="programlisting"><strong>$ kubectl uncordon kube-worker-1</strong>
node/kube-worker-1 uncordoned</pre>

<p><a data-type="indexterm" data-primary="commands" data-secondary="kubectl get nodes" id="idm45322727444928"/><a data-type="indexterm" data-primary="kubectl get nodes command" id="idm45322727443952"/><a data-type="indexterm" data-primary="" data-startref="clu_cm" id="idm45322727443216"/><a data-type="indexterm" data-primary="" data-startref="clu_up" id="idm45322727442272"/><a data-type="indexterm" data-primary="" data-startref="up_clu" id="idm45322727441328"/>Listing the nodes should now show version 1.19.0 for the worker node. You won’t be able to run <code>kubectl get nodes</code> from the worker node without copying the administrator kubeconfig file from the control plane node. Follow the <a href="https://oreil.ly/NGHaQ">instructions</a> in the Kubernetes documentation to do so or log back into the control plane node:</p>

<pre data-type="programlisting"><strong>$ kubectl get nodes</strong>
NAME                 STATUS   ROLES    AGE   VERSION
kube-control-plane   Ready    master   24h   v1.19.0
kube-worker-1        Ready    &lt;none&gt;   24h   v1.19.0</pre>

<p>Exit the worker node using the <code>exit</code> command:</p>

<pre data-type="programlisting"><strong>$ exit</strong>
logout
...</pre>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Backing Up and Restoring etcd"><div class="sect1" id="idm45322727435776">
<h1>Backing Up and Restoring etcd</h1>

<p><a data-type="indexterm" data-primary="backing up etcd" id="bue_ab"/><a data-type="indexterm" data-primary="etcd" data-secondary="backing up" id="etcd_b"/><a data-type="indexterm" data-primary="clusters" data-secondary="backing up and restoring etcd" id="clu_ba"/><a data-type="indexterm" data-primary="restoring etcd" id="idm45322727430784"/><a data-type="indexterm" data-primary="etcd" data-secondary="restoring" id="idm45322727430112"/>Kubernetes stores both the declared and observed states of the cluster in the distributed etcd key-value store. It’s important to have a backup plan in place that can help you with restoring the data in case of data corruption. Backing up the data should happen periodically in short time frames to avoid losing as little historical data as possible.</p>

<p><a data-type="indexterm" data-primary="snapshot file" id="idm45322727428784"/><a data-type="indexterm" data-primary="etcdctl tool" id="idm45322727428080"/>The backup process stores the ectd data in a so-called snapshot file. This snapshot file can be used to restore the etcd data at any given time. You can encrypt the snapshot file to protect sensitive information. The tool <code>etcdctl</code> is central to the backup and restore procedure.</p>

<p>As an administrator, you will need to understand how to use the tool for both operations. You may need to install <code>etcdctl</code> if it is not available on the control plane node yet. You can find <a href="https://oreil.ly/CrI28">installation instructions</a> in the etcd GitHub repository. <a data-type="xref" href="#backing_up_restoring_etcd">Figure 2-7</a> visualizes the etcd backup and restoration process.</p>

<figure><div id="backing_up_restoring_etcd" class="figure"><div class="border-box"><img src="Images/ckas_0207.png" alt="ckas 0207" width="1206" height="920"/></div><h6><span class="label">Figure 2-7. </span>Process for a backing up and restoring etcd</h6></div></figure>

<p>Depending on your cluster topology, your cluster may consist of one or many etcd instances. Refer to the section “High-Availability Cluster Setup” for more information on how to set it up. The following sections explain a single-node etcd cluster setup. You can find <a href="https://oreil.ly/PvS5u">additional instructions</a> on the backup and restoration process for multinode etcd clusters in the official Kubernetes documentation.</p>








<section data-type="sect2" data-pdf-bookmark="Backing Up etcd"><div class="sect2" id="idm45322727420752">
<h2>Backing Up etcd</h2>

<p><a data-type="indexterm" data-primary="ssh command" id="idm45322727419216"/><a data-type="indexterm" data-primary="commands" data-secondary="ssh" id="idm45322727418512"/>Open an interactive shell to the machine hosting etcd using the <code>ssh</code> command. The following command targets the control plane node named <code>kube-control-plane</code> running Ubuntu 18.04.5 LTS:</p>

<pre data-type="programlisting"><strong>$ ssh kube-control-plane</strong>
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)
...</pre>

<p>Check the installed version of <code>etcdctl</code> to verify that the tool has been installed. On this node, the version is 3.4.14:</p>

<pre data-type="programlisting"><strong>$ etcdctl version</strong>
etcdctl version: 3.4.14
API version: 3.4</pre>

<p>Etcd is deployed as a Pod in the <code>kube-system</code> namespace. Inspect the version by describing the Pod. In the following output, you will find that the version is 3.4.13-0:</p>

<pre data-type="programlisting"><strong>$ kubectl get pods -n kube-system</strong>
NAME                                       READY   STATUS    RESTARTS   AGE
...
etcd-kube-control-plane                    1/1     Running   0          33m
...
<strong>$ kubectl describe pod etcd-kube-control-plane -n kube-system</strong>
...
Containers:
  etcd:
    Container ID:  docker://28325c63233edaa94e16691e8082e8d86f5e7da58c0fb54 \
    d95d68dec6e80cf54
    Image:         k8s.gcr.io/etcd:3.4.3-0
    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:4afb99b4690b418 \
    ffc2ceb67e1a17376457e441c1f09ab55447f0aaf992fa646
...</pre>

<p><a data-type="indexterm" data-primary="describe command" id="idm45322727410832"/><a data-type="indexterm" data-primary="commands" data-secondary="describe" id="idm45322727409712"/>The same <code>describe</code> command reveals the configuration of the etcd service. Look 
<span class="keep-together">for the</span> value of the option <code>--listen-client-urls</code> for the endpoint URL. In the following output, the host is <code>localhost</code>, and the port is <code>2379</code>. The server certificate is located at <code>/etc/kubernetes/pki/etcd/server.crt</code> defined by the option 
<span class="keep-together"><code>--cert-file</code>.</span> The CA certificate can be found at <code>/etc/kubernetes/pki/etcd/ca.crt</code> specified by the option <code>--trusted-ca-file</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl describe pod etcd-kube-control-plane -n kube-system</strong>
...
Containers:
  etcd:
    ...
    Command:
      etcd
      ...
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=/etc/kubernetes/pki/etcd/server.key
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
...</pre>

<p><a data-type="indexterm" data-primary="commands" data-secondary="etcdctl" id="idm45322727402208"/><a data-type="indexterm" data-primary="etcdctl command" id="idm45322727401232"/>Use the <code>etcdctl</code> command to create the backup with version 3 of the tool. For a good starting point, copy the command from the <a href="https://oreil.ly/LuM2P">official Kubernetes documentation</a>. Provide the mandatory command-line options <code>--cacert</code>, <code>--cert</code>, and <code>--key</code>. The option <code>--endpoints</code> is not needed as we are running the command on the same server as etcd. After running the command, the file <code>/tmp/etcd-backup.db</code> has been created:</p>

<pre data-type="programlisting"><strong>$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt</strong> \
  <strong>--cert=/etc/kubernetes/pki/etcd/server.crt</strong> \
  <strong>--key=/etc/kubernetes/pki/etcd/server.key</strong> \
  <strong>snapshot save /opt/etcd-backup.db</strong>
{"level":"info","ts":1625860312.3468597, \
"caller":"snapshot/v3_snapshot.go:119", \
"msg":"created temporary db file","path":"/opt/etcd-backup.db.part"}
{"level":"info","ts":"2021-07-09T19:51:52.356Z", \
"caller":"clientv3/maintenance.go:200", \
"msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1625860312.358686, \
"caller":"snapshot/v3_snapshot.go:127", \
"msg":"fetching snapshot","endpoint":"127.0.0.1:2379"}
{"level":"info","ts":"2021-07-09T19:51:52.389Z", \
"caller":"clientv3/maintenance.go:208", \
"msg":"completed snapshot read; closing"}
{"level":"info","ts":1625860312.392891, \
"caller":"snapshot/v3_snapshot.go:142", \
"msg":"fetched snapshot","endpoint":"127.0.0.1:2379", \
"size":"2.3 MB","took":0.045987318}
{"level":"info","ts":1625860312.3930364, \
"caller":"snapshot/v3_snapshot.go:152", \
"msg":"saved","path":"/opt/etcd-backup.db"}
Snapshot saved at /opt/etcd-backup.db</pre>

<p><a data-type="indexterm" data-primary="" data-startref="bue_ab" id="idm45322727393344"/><a data-type="indexterm" data-primary="" data-startref="etcd_b" id="idm45322727392368"/>Exit the node using the <code>exit</code> command:</p>

<pre data-type="programlisting"><strong>$ exit</strong>
logout
...</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Restoring etcd"><div class="sect2" id="idm45322727420128">
<h2>Restoring etcd</h2>

<p><a data-type="indexterm" data-primary="etcd" data-secondary="restoring" id="idm45322727388320"/><a data-type="indexterm" data-primary="restoring etcd" id="idm45322727387344"/>You created a backup of etcd and stored it in a safe space. There’s nothing else to do at this time. Effectively, it’s your insurance policy that becomes relevant when disaster strikes. In the case of a disaster scenario, the data in etcd gets corrupted or the machine managing etcd experiences a physical storage failure. That’s the time when you want to pull out the etcd backup for restoration.</p>

<p><a data-type="indexterm" data-primary="commands" data-secondary="restore" id="idm45322727385872"/><a data-type="indexterm" data-primary="restore command" id="idm45322727384672"/>To restore etcd from the backup, use the <code>etcdctl snapshot restore</code> command. At a minimum, provide the <code>--data-dir</code> command-line option. Here, we are using the data directory <code>/tmp/from-backup</code>. After running the command, you should be able to find the restored backup in the directory <code>/var/lib/from-backup</code>:</p>

<pre data-type="programlisting"><strong>$ sudo ETCDCTL_API=3 etcdctl --data-dir=/var/lib/from-backup snapshot restore</strong> \
  <strong>/opt/etcd-backup.db</strong>
{"level":"info","ts":1625861500.5752304, \
"caller":"snapshot/v3_snapshot.go:296", \
"msg":"restoring snapshot","path":"/opt/etcd-backup.db", \
"wal-dir":"/var/lib/from-backup/member/wal", \
"data-dir":"/var/lib/from-backup", \
"snap-dir":"/var/lib/from-backup/member/snap"}
{"level":"info","ts":1625861500.6146874, \
"caller":"membership/cluster.go:392", \
"msg":"added member","cluster-id":"cdf818194e3a8c32", \
"local-member-id":"0", \
"added-peer-id":"8e9e05c52164694d", \
"added-peer-peer-urls":["http://localhost:2380"]}
{"level":"info","ts":1625861500.6350253, \
"caller":"snapshot/v3_snapshot.go:309", \
"msg":"restored snapshot","path":"/opt/etcd-backup.db", \
"wal-dir":"/var/lib/from-backup/member/wal", \
"data-dir":"/var/lib/from-backup", \
"snap-dir":"/var/lib/from-backup/member/snap"}
<strong>$ sudo ls /var/lib/from-backup</strong>
member</pre>

<p>Edit the YAML manifest of the etcd Pod, which can be found at <code>/etc/kubernetes/manifests/etcd.yaml</code>. Change the value of the attribute <code>spec.volumes.hostPath</code> with the name <code>etcd-data</code> from the original value <code>/var/lib/etcd</code> to <code>/var/lib/from-backup</code>:</p>

<pre data-type="programlisting"><strong>$ cd /etc/kubernetes/manifests/
$ sudo vim etcd.yaml</strong>
...
spec:
  volumes:
  ...
  - hostPath:
      path: /var/lib/from-backup
      type: DirectoryOrCreate
    name: etcd-data
...</pre>

<p>The <code>etcd-kube-control-plane</code> Pod will be re-created, and it points to the restored backup directory:</p>

<pre data-type="programlisting"><strong>$ kubectl get pod etcd-kube-control-plane -n kube-system</strong>
NAME                      READY   STATUS    RESTARTS   AGE
etcd-kube-control-plane   1/1     Running   0          5m1s</pre>

<p>In case the Pod doesn’t transition into the “Running” status, try to delete it manually with the command <code>kubectl delete pod etcd-kube-control-plane -n kube-system</code>.</p>

<p><a data-type="indexterm" data-primary="" data-startref="clu_ba" id="idm45322727372928"/><a data-type="indexterm" data-primary="exit command" id="idm45322727371952"/><a data-type="indexterm" data-primary="commands" data-secondary="exit" id="idm45322727371280"/>Exit the node using the <code>exit</code> command:</p>

<pre data-type="programlisting"><strong>$ exit</strong>
logout
...</pre>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45322727389264">
<h1>Summary</h1>

<p><a data-type="indexterm" data-primary="clusters" data-secondary="about" id="idm45322727367616"/>Production-ready Kubernetes clusters should employ security policies to control which users and what processes can manage objects. Role-based access control (RBAC) defines those rules. RBAC introduces specific API resources that
map subjects to the operations allowed for particular objects. Rules can be defined on a namespace or cluster level using the API resource types Role, ClusterRole, RoleBinding, and ClusterRoleBinding. To avoid duplication of rules, ClusterRoles can be aggregated with the help of label selection.</p>

<p>As a Kubernetes administrator, you need to be familiar with typical tasks involving the management of the cluster nodes.
The primary tool for installing new nodes and upgrading a node version is <code>kubeadm</code>. The cluster topology of such a cluster can vary. For optimal results with redundancy and scalability, consider configuring the cluster with a high-availability setup that uses three or more control plane nodes and dedicated etcd hosts.</p>

<p>Backing up the etcd database should be performed as a periodic process to prevent the loss of crucial data in the event of a node or storage corruption. You can use the tool <code>etcdctl</code> to back up and restore etcd from the control plane node or via an API endpoint.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exam Essentials"><div class="sect1" id="idm45322727364528">
<h1>Exam Essentials</h1>
<dl>
<dt><a data-type="indexterm" data-primary="clusters" data-secondary="exam essentials" id="idm45322727362816"/><a data-type="indexterm" data-primary="exam essentials" data-secondary="clusters" id="idm45322727361840"/>Know how to define RBAC rules</dt>
<dd>
<p>Defining RBAC rules involves a couple of moving parts: the subject defined by users, groups, and ServiceAccounts; the RBAC-specific API resources on the namespace and cluster level; and, finally, the verbs that allow the corresponding operations on the Kubernetes objects. Practice the creation of subjects, and how to tie them together to form the desired access rules. Ensure that you verify the correct behavior with different constellations.</p>
</dd>
<dt>Know how to create and manage a Kubernetes cluster</dt>
<dd>
<p>Installing new cluster nodes and upgrading the version of an existing cluster node are typical tasks performed by a Kubernetes administrator. You do not need to memorize all the steps involved. The documentation provides a step-by-step, easy-to-follow manual for those operations. For upgrading a cluster version, it is recommended to jump up by a single minor version or multiple patch versions before tackling the next higher version. High-availability clusters help with redundancy and scalability. For the exam, you will need to understand the different HA topologies though it’s unlikely that you’ll have to configure one of them as the process would involve a suite of different hosts.</p>
</dd>
<dt>Practice backing up and restoring etcd</dt>
<dd>
<p>The process for etcd disaster recovery is not as well documented as you’d expect. Practice the backup and a restoration process hands-on a couple of times to get the hang of it. Remember to point the control plane node(s) to the restored snapshot file to recover the data.</p>
</dd>
</dl>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Sample Exercises"><div class="sect1" id="idm45322727356928">
<h1>Sample Exercises</h1>

<p><a data-type="indexterm" data-primary="clusters" data-secondary="sample exercises" id="idm45322727355728"/><a data-type="indexterm" data-primary="sample exercises" data-secondary="clusters" id="idm45322727354752"/><a data-type="indexterm" data-primary="" data-startref="clu_ch" id="idm45322727353808"/>Solutions to these exercises are available in the <a data-type="xref" href="app01.xhtml#appendix-a">Appendix</a>.</p>
<ol>
<li>
<p>Create the ServiceAccount named <code>api-access</code> in a new namespace called <code>apps</code>.</p>
</li>
<li>
<p>Create a ClusterRole with the name <code>api-clusterrole</code>, and create a ClusterRoleBinding named <code>api-clusterrolebinding</code>. Map the ServiceAccount from the previous step to the API resources <code>pods</code> with the operations <code>watch</code>, <code>list</code>, and <code>get</code>.</p>
</li>
<li>
<p>Create a Pod named <code>operator</code> with the image <code>nginx:1.21.1</code> in the namespace <code>apps</code>. Expose the container port 80. Assign the ServiceAccount <code>api-access</code> to the Pod. Create another Pod named <code>disposable</code> with the image <code>nginx:1.21.1</code> in the namespace <code>rm</code>. Do not assign the ServiceAccount to the Pod.</p>
</li>
<li>
<p>Open an interactive shell to the Pod named <code>operator</code>. Use the command-line tool <code>curl</code> to make an API call to list the Pods in the namespace <code>rm</code>. What response do you expect? Use the command-line tool <code>curl</code> to make an API call to delete the Pod <code>disposable</code> in the namespace <code>rm</code>. Does the response differ from the first call? You can find information about how to interact with Pods using the API via HTTP in the <a href="https://oreil.ly/SZls9">reference guide</a>.</p>
</li>
<li>
<p>Navigate to the directory <em>app-a/ch02/upgrade-version</em> of the checked-out GitHub repository <a href="https://oreil.ly/jUIq8"><em>bmuschko/cka-study-guide</em></a>. Start up the VMs running the cluster using the command <code>vagrant up</code>. Upgrade all nodes of the cluster from Kubernetes 1.20.4 to 1.21.2. The cluster consists of a single control plane node named <code>k8s-control-plane</code>, and three worker nodes named <code>worker-1</code>, <code>worker-2</code>, and <code>worker-3</code>. Once done, shut down the cluster using <code>vagrant destroy -f</code>.</p>

<p><em>Prerequisite:</em> This exercise requires the installation of the tools <a href="https://oreil.ly/sasln">Vagrant</a> and 
<span class="keep-together"><a href="https://oreil.ly/9Cvg9">VirtualBox</a>.</span></p>
</li>
<li>
<p>Navigate to the directory <em>app-a/ch02/backup-restore-etcd</em> of the checked-out GitHub repository <a href="https://oreil.ly/jUIq8"><em>bmuschko/cka-study-guide</em></a>. Start up the VMs running the cluster using the command <code>vagrant up</code>. The cluster consists of a single control plane node named <code>k8s-control-plane</code> and two worker nodes named <code>worker-1</code> and <code>worker-2</code>. The <code>etcdctl</code> tool has been preinstalled on the node <code>k8s-control-plane</code>. Back up etcd to the snapshot file <code>/opt/etcd.bak</code>. Restore etcd from the snapshot file. Use the data directory <code>/var/bak</code>. Once done, shut down the cluster using <code>vagrant destroy -f</code>.</p>

<p><em>Prerequisite:</em> This exercise requires the installation of the tools <a href="https://oreil.ly/sasln">Vagrant</a> and 
<span class="keep-together"><a href="https://oreil.ly/9Cvg9">VirtualBox</a>.</span></p>
</li>

</ol>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45322727320000">
<h5>Interactive Exam Practice</h5>
<p>Get more hands-on training and test your CKA exam readiness by working through our interactive CKA labs. Each step of the lab must be completed correctly before you can move to the next step. If you get stuck, you can view the solution and learn how to complete the step.</p>

<p>The following labs cover material from this chapter:</p>

<ul>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492095460">Setting Up RBAC for a Service Account</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492095477">Setting Up RBAC for a User</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492095484">Aggregating RBAC Rules</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492095507">Installing a Cluster</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492095514">Upgrading a Cluster Version</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492095521">Backing Up and Restoring etcd</a></p>
</li>
</ul>
</div></aside>
</div></section>







</div></section></div></body></html>