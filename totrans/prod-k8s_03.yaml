- en: Chapter 2\. Deployment Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step to using Kubernetes in production is obvious: make Kubernetes
    exist. This includes installing systems to provision Kubernetes clusters and to
    manage future upgrades. Being that Kubernetes is a distributed software system,
    deploying Kubernetes largely boils down to a software installation exercise. The
    important difference compared with most other software installs is that Kubernetes
    is intrinsically tied to the infrastructure. As such, the software installation
    and the infrastructure it’s being installed on need to be simultaneously solved
    for.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will first address preliminary questions around deploying
    Kubernetes clusters and how much you should leverage managed services and existing
    products or projects. For those that heavily leverage existing services, products,
    and projects, most of this chapter may not be of interest because about 90% of
    the content in this chapter covers how to approach custom automation. This chapter
    can still be of interest if you are evaluating tools for deploying Kubernetes
    so that you can reason about the different approaches available. For those in
    the uncommon position of having to build custom automation for deploying Kubernetes,
    we will address overarching architectural concerns, including special considerations
    for etcd as well as how to manage the various clusters under management. We will
    also look at useful patterns for managing the various software installations as
    well as the infrastructure dependencies and will break down the various cluster
    components and demystify how they fit together. We’ll also look at ways to manage
    the add-ons you install to the base Kubernetes cluster as well as strategies for
    upgrading Kubernetes and the add-on components that make up your application platform.
  prefs: []
  type: TYPE_NORMAL
- en: Managed Service Versus Roll Your Own
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get further into the topic of deployment models for Kubernetes, we
    should address the idea of whether you should even *have* a full deployment model
    for Kubernetes. Cloud providers offer managed Kubernetes services that mostly
    alleviate the deployment concerns. You should still develop reliable, declarative
    systems for provisioning these managed Kubernetes clusters, but it may be advantageous
    to abstract away most of the details of *how* the cluster is brought up.
  prefs: []
  type: TYPE_NORMAL
- en: Managed Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The case for using managed Kubernetes services boils down to savings in engineering
    effort. There is considerable technical design and implementation in properly
    managing the deployment and life cycle of Kubernetes. And remember, Kubernetes
    is just one component of your application platform—the container orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, with a managed service you get a Kubernetes control plane that you
    can attach worker nodes to at will. The obligation to scale, ensure availability,
    and manage the control plane is alleviated. These are each significant concerns.
    Furthermore, if you already use a cloud provider’s existing services you get a
    leg up. For example, if you are in Amazon Web Services (AWS) and already use Fargate
    for serverless compute, Identity and Access Management (IAM) for role-based access
    control, and CloudWatch for observability, you can leverage these with their Elastic
    Kubernetes Service (EKS) and solve for several concerns in your app platform.
  prefs: []
  type: TYPE_NORMAL
- en: It is not unlike using a managed database service. If your core concern is an
    application that serves your business needs, and that app requires a relational
    database, but you cannot justify having a dedicated database admin on staff, paying
    a cloud provider to supply you with a database can be a huge boost. You can get
    up and running faster. The managed service provider will manage availability,
    take backups, and perform upgrades on your behalf. In many cases this is a clear
    benefit. But, as always, there is a trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Roll Your Own
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The savings available in using a managed Kubernetes service come with a price
    tag. You pay with a lack of flexibility and freedom. Part of this is the threat
    of vendor lock-in. The managed services are generally offered by cloud infrastructure
    providers. If you invest heavily in using a particular vendor for your infrastructure,
    it is highly likely that you will design systems and leverage services that will
    not be vendor neutral. The concern is that if they raise their prices or let their
    service quality slip in the future, you may find yourself painted into a corner.
    Those experts you paid to handle concerns you didn’t have time for may now wield
    dangerous power over your destiny.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can diversify by using managed services from multiple providers,
    but there will be deltas between the way they expose features of Kubernetes, and
    which features are exposed could become an awkward inconsistency to overcome.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, you may prefer to roll your own Kubernetes. There is a vast
    array of knobs and levers to adjust on Kubernetes. This configurability makes
    it wonderfully flexible and powerful. If you invest in understanding and managing
    Kubernetes itself, the app platform world is your oyster. There will be no feature
    you cannot implement, no requirement you cannot meet. And you will be able to
    implement that seamlessly across infrastructure providers, whether they be public
    cloud providers, or your own servers in a private datacenter. Once the different
    infrastructure inconsistencies are accounted for, the Kubernetes features that
    are exposed in your platform will be consistent. And the developers that use your
    platform will not care—and may not even know—who is providing the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Just keep in mind that developers will care only about the features of the platform,
    not the underlying infrastructure or who provides it. If you are in control of
    the features available, and the features you deliver are consistent across infrastructure
    providers, you have the freedom to deliver a superior experience to your devs.
    You will have control of the Kubernetes version you use. You will have access
    to all the flags and features of the control plane components. You will have access
    to the underlying machines and the software that is installed on them as well
    as the static Pod manifests that are written to disk there. You will have a powerful
    and dangerous tool to use in the effort to win over your developers. But never
    ignore the obligation you have to learn the tool well. A failure to do so risks
    injuring yourself and others with it.
  prefs: []
  type: TYPE_NORMAL
- en: Making the Decision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The path to glory is rarely clear when you begin the journey. If you are deciding
    between a managed Kubernetes service or rolling your own clusters, you are much
    closer to the beginning of your journey with Kubernetes than the glorious final
    conclusion. And the decision of managed service versus roll your own is fundamental
    enough that it will have long-lasting implications for your business. So here
    are some guiding principles to aid the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should lean toward a managed service if:'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of understanding Kubernetes sounds terribly arduous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The responsibility for managing a distributed software system that is critical
    to the success of your business sounds dangerous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inconveniences of restrictions imposed by vendor-provided features seem
    manageable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have faith in your managed service vendor to respond to your needs and be
    a good business partner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should lean toward rolling your own Kubernetes if:'
  prefs: []
  type: TYPE_NORMAL
- en: The vendor-imposed restrictions make you uneasy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have little or no faith in the corporate behemoths that provide cloud compute
    infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are excited by the power of the platform you can build around Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You relish the opportunity to leverage this amazing container orchestrator to
    provide a delightful experience to your devs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you decide to use a managed service, consider skipping most of the remainder
    of this chapter. [“Add-ons”](#addons) and [“Triggering Mechanisms”](#triggering_mechanisms)
    are still applicable to your use case, but the other sections in this chapter
    will not apply. If, on the other hand, you are looking to manage your own clusters,
    read on! Next we’ll dig more into the deployment models and tools you should consider.
  prefs: []
  type: TYPE_NORMAL
- en: Automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are to undertake designing a deployment model for your Kubernetes clusters,
    the topic of automation is of the utmost importance. Any deployment model will
    need to keep this as a guiding principle. Removing human toil is critical to reduce
    cost and improve stability. Humans are costly. Paying the salary for engineers
    to execute routine, tedious operations is money not spent on innovation. Furthermore,
    humans are unreliable. They make mistakes. Just one error in a series of steps
    may introduce instability or even prevent the system from working at all. The
    upfront engineering investment to automate deployments using software systems
    will pay dividends in saved toil and troubleshooting in the future.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to manage your own cluster life cycle, you must formulate your
    strategy for doing this. You have a choice between using a prebuilt Kubernetes
    installer or developing your own custom automation from the ground up. This decision
    has parallels with the decision between managed services versus roll your own.
    One path gives you great power, control, and flexibility but at the cost of engineering
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: Prebuilt Installer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are now countless open source and enterprise-supported Kubernetes installers
    available. Case in point: there are currently 180 Kubernetes Certified Service
    Providers listed on the [CNCF’s website](https://www.cncf.io/certification/kcsp).
    Some you will need to pay money for and will be accompanied by experienced field
    engineers to help get you up and running as well as support staff you can call
    on in times of need. Others will require research and experimentation to understand
    and use. Some installers—usually the ones you pay money for—will get you from
    zero to Kubernetes with the push of a button. If you fit the prescriptions provided
    and options available, and your budget can accommodate the expense, this installer
    method could be a great fit. At the time of this writing, using prebuilt installers
    is the approach we see most commonly in the field.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom Automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some amount of custom automation is commonly required even if using a prebuilt
    installer. This is usually in the form of integration with a team’s existing systems.
    However, in this section we’re talking about developing a custom Kubernetes installer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are beginning your journey with Kubernetes or changing direction with
    your Kubernetes strategy, the homegrown automation route is likely your choice
    only if all of the following apply:'
  prefs: []
  type: TYPE_NORMAL
- en: You have more than just one or two engineers to devote to the effort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have engineers on staff with deep Kubernetes experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have specialized requirements that no managed service or prebuilt installer
    satisfies well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most of the remainder of this chapter is for you if one of the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: You fit the use case for building custom automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are evaluating installers and want to gain a deeper insight into what good
    patterns look like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This brings us to details of building custom automation to install and manage
    Kubernetes clusters. Underpinning all these concerns should be a clear understanding
    of your platform requirements. These should be driven primarily by the requirements
    of your app development teams, particularly those that will be the earliest adopters.
    Do not fall into the trap of building a platform in a vacuum without close collaboration
    with the consumers of the platform. Make early prerelease versions of your platform
    available to dev teams for testing. Cultivate a productive feedback loop for fixing
    bugs and adding features. The successful adoption of your platform depends upon
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Next we will cover architecture concerns that should be considered before any
    implementation begins. This includes deployment models for etcd, separating deployment
    environments into tiers, tackling challenges with managing large numbers of clusters,
    and what types of node pools you might use to host your workloads. After that,
    we’ll get into Kubernetes installation details, first for the infrastructure dependencies,
    then, the software that is installed on the clusters’ virtual or physical machines,
    and finally for the containerized components that constitute the control plane
    of a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture and Topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers the architectural decisions that have broad implications
    for the systems you use to provision and manage your Kubernetes clusters. They
    include the deployment model for etcd and the unique considerations you must take
    into account for that component of the platform. Among these topics is how you
    organize the various clusters under management into tiers based on the service
    level objectives (SLOs) for them. We will also look at the concept of node pools
    and how they can be used for different purposes within a given cluster. And, lastly,
    we will address the methods you can use for federated management of your clusters
    and the software you deploy to them.
  prefs: []
  type: TYPE_NORMAL
- en: etcd Deployment Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the database for the objects in a Kubernetes cluster, etcd deserves special
    consideration. etcd is a distributed data store that uses a consensus algorithm
    to maintain a copy of the your cluster’s state on multiple machines. This introduces
    network considerations for the nodes in an etcd cluster so they can reliably maintain
    that consensus over their network connections. It has unique network latency requirements
    that we need to design for when considering network topology. We’ll cover that
    topic in this section and also look at the two primary architectural choices to
    make in the deployment model for etcd: dedicated versus colocated and whether
    to run in a container or install directly on the host.'
  prefs: []
  type: TYPE_NORMAL
- en: Network considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default settings in etcd are designed for the latency in a single datacenter.
    If you distribute etcd across multiple datacenters, you should test the average
    round-trip between members and tune the heartbeat interval and election timeout
    for etcd if need be. We strongly discourage the use of etcd clusters distributed
    across different regions. If using multiple datacenters for improved availability,
    they should at least be in close proximity within a region.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated versus colocated
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very common question we get about how to deploy is whether to give etcd its
    own dedicated machines or to colocate them on the control plane machines with
    the API server, scheduler, controller manager, etc. The first thing to consider
    is the size of clusters you will be managing, i.e., the number of worker nodes
    you will run per cluster. The trade-offs around cluster sizes will be discussed
    later in the chapter. Where you land on that subject will largely inform whether
    you dedicate machines to etcd. Obviously etcd is crucial. If etcd performance
    is compromised, your ability to control the resources in your cluster will be
    compromised. As long as your workloads don’t have dependencies on the Kubernetes
    API, they should not suffer, but keeping your control plane healthy is still very
    important.
  prefs: []
  type: TYPE_NORMAL
- en: If you are driving a car down the street and the steering wheel stops working,
    it is little comfort that the car is still driving down the road. In fact, it
    may be terribly dangerous. For this reason, if you are going to be placing the
    read and write demands on etcd that come with larger clusters, it is wise to dedicate
    machines to them to eliminate resource contention with other control plane components.
    In this context, a “large” cluster is dependent upon the size of the control plane
    machines in use but should be at least a topic of consideration with anything
    above 50 worker nodes. If planning for clusters with over 200 workers, it’s best
    to just plan for dedicated etcd clusters. If you do plan smaller clusters, save
    yourself the management overhead and infrastructure costs—go with colocated etcd.
    Kubeadm is a popular Kubernetes bootstrapping tool that you will likely be using;
    it supports this model and will take care of the associated concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Containerized versus on host
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next common question revolves around whether to install etcd on the machine
    or to run it in a container. Let’s tackle the easy answer first: if you’re running
    etcd in a colocated manner, run it in a container. When leveraging kubeadm for
    Kubernetes bootstrapping, this configuration is supported and well tested. It
    is your best option. If, on the other hand, you opt for running etcd on dedicated
    machines your options are as follows: you can install etcd on the host, which
    gives you the opportunity to bake it into machine images and eliminate the additional
    concerns of having a container runtime on the host. Alternatively, if you run
    in a container, the most useful pattern is to install a container runtime and
    kubelet on the machines and use a static manifest to spin up etcd. This has the
    advantage of following the same patterns and install methods as the other control
    plane components. Using repeated patterns in complex systems is useful, but this
    question is largely a question of preference.'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Tiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Organizing your clusters according to tiers is an almost universal pattern
    we see in the field. These tiers often include testing, development, staging,
    and production. Some teams refer to these as different “environments.” However,
    this is a broad term that can have different meanings and implications. We will
    use the term *tier* here to specifically address the different types of clusters.
    In particular, we’re talking about the SLOs and SLAs that may be associated with
    the cluster, as well as the purpose for the cluster, and where the cluster sits
    in the path to production for an application, if at all. What exactly these tiers
    will look like for different organizations varies, but there are common themes
    and we will describe what each of these four tiers commonly mean. Use the same
    cluster deployment and life cycle management systems across all tiers. The heavy
    use of these systems in the lower tiers will help ensure they will work as expected
    when applied to critical production clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs: []
  type: TYPE_NORMAL
- en: Clusters in the testing tier are single-tenant, ephemeral clusters that often
    have a time-to-live (TTL) applied such that they are automatically destroyed after
    a specified amount of time, usually less than a week. These are spun up very commonly
    by platform engineers for the purpose of testing particular components or platform
    features they are developing. Testing tiers may also be used by developers when
    a local cluster is inadequate for local development, or as a subsequent step to
    testing on a local cluster. This is more common when an app dev team is initially
    containerizing and testing their application on Kubernetes. There is no SLO or
    SLA for these clusters. These clusters would use the latest version of a platform,
    or perhaps optionally a pre-alpha release.
  prefs: []
  type: TYPE_NORMAL
- en: Development
  prefs: []
  type: TYPE_NORMAL
- en: Development tier clusters are generally “permanent” clusters without a TTL.
    They are multitenant (where applicable) and have all the features of a production
    cluster. They are used for the first round of integration tests for applications
    and are used to test the compatibility of application workloads with alpha versions
    of the platform. Development tiers are also used for general testing and development
    for the app dev teams. These clusters normally have an SLO but not a formal agreement
    associated with them. The availability objectives will often be near production-level,
    at least during business hours, because outages will impact developer productivity.
    In contrast, the applications have zero SLO or SLA when running on dev clusters
    and are very frequently updated and in constant flux. These clusters will run
    the officially released alpha and/or beta version of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Staging
  prefs: []
  type: TYPE_NORMAL
- en: Like those in the development tier, clusters in the staging tier are also permanent
    clusters and are commonly used by multiple tenants. They are used for final integration
    testing and approval before rolling out to live production. They are used by stakeholders
    that are not actively developing the software running there. This would include
    project managers, product owners, and executives. This may also include customers
    or external stakeholders who need access to prerelease versions of software. They
    will often have a similar SLO to development clusters. Staging tiers may have
    a formal SLA associated with them if external stakeholders or paying customers
    are accessing workloads on the cluster. These clusters will run the officially
    released beta version of the platform if strict backward compatibility is followed
    by the platform team. If backward compatibility cannot be guaranteed, the staging
    cluster should run the same stable release of the platform as used in production.
  prefs: []
  type: TYPE_NORMAL
- en: Production
  prefs: []
  type: TYPE_NORMAL
- en: Production tier clusters are the money-makers. These are used for customer-facing,
    revenue-producing applications and websites. Only approved, production-ready,
    stable releases of software are run here. And only the fully tested and approved
    stable release of the platform is used. Detailed well-defined SLOs are used and
    tracked. Often, legally binding SLAs apply.
  prefs: []
  type: TYPE_NORMAL
- en: Node Pools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Node pools are a way to group together types of nodes within a single Kubernetes
    cluster. These types of nodes may be grouped together by way of their unique characteristics
    or by way of the role they play. It’s important to understand the trade-offs of
    using node pools before we get into details. The trade-off often revolves around
    the choice between using multiple node pools within a single cluster versus provisioning
    separate, distinct clusters. If you use node pools, you will need to use Node
    selectors on your workloads to make sure they end up in the appropriate node pool.
    You will also likely need to use Node taints to prevent workloads without Node
    selectors from inadvertently landing where they shouldn’t. Additionally, the scaling
    of nodes within your cluster becomes more complicated because your systems have
    to monitor distinct pools and scale each separately. If, on the other hand, you
    use distinct clusters you displace these concerns into cluster management and
    software federation concerns. You will need more clusters. And you will need to
    properly target your workloads to the right clusters. [Table 2-1](#node_pool_pros_and_cons)
    summarizes these pros and cons of using node pools.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Node pool pros and cons
  prefs: []
  type: TYPE_NORMAL
- en: '| Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reduced number of clusters under management | Node selectors for workloads
    will often be needed |'
  prefs: []
  type: TYPE_TB
- en: '| Smaller number of target clusters for workloads | Node taints will need to
    be applied and managed |'
  prefs: []
  type: TYPE_TB
- en: '|  | More complicated cluster scaling operations |'
  prefs: []
  type: TYPE_TB
- en: A characteristic-based node pool is one that consists of nodes that have components
    or attributes that are required by, or suited to, some particular workloads. An
    example of this is the presence of a specialized device like a graphics processing
    unit (GPU). Another example of a characteristic may be the type of network interface
    it uses. One more could be the ratio of memory to CPU on the machine. We will
    discuss the reasons you may use nodes with different ratios of these resources
    in more depth later on in [“Infrastructure”](#infrastructure). Suffice to say
    for now, all these characteristics lend themselves to different types of workloads,
    and if you run them collectively in the same cluster, you’ll need to group them
    into pools to manage where different Pods land.
  prefs: []
  type: TYPE_NORMAL
- en: A role-based node pool is one that has a particular function and that you often
    want to insulate from resource contention. The nodes sliced out into a role-based
    pool don’t necessarily have peculiar characteristics, just a different function.
    A common example is to dedicate a node pool to the ingress layer in your cluster.
    In the example of an ingress pool, the dedicated pool not only insulates the workloads
    from resource contention (particularly important in this case since resource requests
    and limits are not currently available for network usage) but also simplifies
    the networking model and the specific nodes that are exposed to traffic from sources
    outside the cluster. In contrast to the characteristic-based node pool, these
    roles are often not a concern you can displace into distinct clusters because
    the machines play an important role in the function of a particular cluster. That
    said, do ensure you are slicing off nodes into a pool for good reason. Don’t create
    pools indiscriminately. Kubernetes clusters are complex enough. Don’t complicate
    your life more than you need to.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that you will most likely need to solve the multicluster management
    problems that many distinct clusters bring, regardless of whether you use node
    pools. There are very few enterprises that use Kubernetes that don’t accumulate
    a large number of distinct clusters. There are a large variety of reasons for
    this. So if you are tempted to introduce characteristic-based node pools, consider
    investing the engineering effort into developing and refining your multicluster
    management. Then you unlock the opportunity to seamlessly use distinct clusters
    for the different machine characteristics you need to provide.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Federation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cluster federation broadly refers to how to centrally manage all the clusters
    under your control. Kubernetes is like a guilty pleasure. When you discover how
    much you enjoy it, you can’t have just one. But, similarly, if you don’t keep
    that habit under control, it can become messy. Federation strategies are ways
    for enterprises to manage their software dependencies so they don’t spiral into
    costly, destructive addictions.
  prefs: []
  type: TYPE_NORMAL
- en: A common, useful approach is to federate regionally and then globally. This
    lessens the blast radius of, and reduces the computational load for, these federation
    clusters. When you first begin federation efforts, you may not have the global
    presence or volume of infrastructure to justify a multilevel federation approach,
    but keep it in mind as a design principle in case it becomes a future requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss some important related subjects in this area. In this section,
    we’ll look at how management clusters can help with consolidating and centralizing
    regional services. We’ll consider how we can consolidate the metrics for workloads
    in various clusters. And we’ll discuss how this impacts the managing workloads
    that are deployed across different clusters in a centrally managed way.
  prefs: []
  type: TYPE_NORMAL
- en: Management clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Management clusters are what they sound like: Kubernetes clusters that manage
    other clusters. Organizations are finding that, as their usage expands and as
    the number of clusters under management increases, they need to leverage software
    systems for smooth operation. And, as you might expect, they often use Kubernetes-based
    platforms to run this software. [Cluster API](https://cluster-api.sigs.k8s.io)
    has become a popular project for accomplishing this. It is a set of Kubernetes
    operators that use custom resources such as Cluster and Machine resources to represent
    other Kubernetes clusters and their components. A common pattern used is to deploy
    the Cluster API components to a management cluster for deploying and managing
    the infrastructure for other workload clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a management cluster in this manner does have flaws, however. It is usually
    prudent to strictly separate concerns between your production tier and other tiers.
    Therefore, organizations will often have a management cluster dedicated to production.
    This further increases the management cluster overhead. Another problem is with
    cluster autoscaling, which is a method of adding and removing worker nodes in
    response to the scaling of workloads. The Cluster Autoscaler typically runs in
    the cluster that it scales so as to watch for conditions that require scaling
    events. But the management cluster contains the controller that manages the provisioning
    and decommissioning of those worker nodes. This introduces an external dependency
    on the management cluster for any workload cluster that uses Cluster Autoscaler,
    as illustrated in [Figure 2-1](#cluster_autoscaler_accessing_a_management_cluster_to_trigger_scaling_events).
    What if the management cluster becomes unavailable at a busy time that your cluster
    needs to scale out to meet demand?
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0201](assets/prku_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Cluster Autoscaler accessing a management cluster to trigger scaling
    events.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One strategy to remedy this is to run the Cluster API components in the workload
    cluster in a self-contained manner. In this case, the Cluster and Machine resources
    will also live there in the workload cluster. You can still use the management
    cluster for creation and deletion of clusters, but the workload cluster becomes
    largely autonomous and free from the external dependency on the management cluster
    for routine operations, such as autoscaling, as shown in [Figure 2-2](#cluster_autoscaler_accessing_a_local_cluster_api_component_to_perform_scaling_events).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0202](assets/prku_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Cluster Autoscaler accessing a local Cluster API component to perform
    scaling events.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This pattern also has the distinct advantage that if any other controllers or
    workloads in the cluster have a need for metadata or attributes contained in the
    Cluster API custom resources, they can access them by reading the resource through
    the local API. There is no need to access the management cluster API. For example,
    if you have a Namespace controller that changes its behavior based on whether
    it is in a development or production cluster, that is information that can already
    be contained in the Cluster resource that represents the cluster in which it lives.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, management clusters also often host shared or regional services
    that are accessed by systems in various other clusters. These are not so much
    *management* functions. Management clusters are often just a logical place to
    run these shared services. Examples of these shared services include CI/CD systems
    and container registries.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When managing a large number of clusters, one of the challenges that arises
    is the collection of metrics from across this infrastructure and bringing them—or
    a subset thereof—into a central location. High-level measurable data points that
    give you a clear picture of the health of the clusters and workloads under management
    is a critical concern of cluster federation. [Prometheus](https://prometheus.io)
    is a mature open source project that many organizations use to gather and store
    metrics. Whether you use it or not, the model it uses for federation is very useful
    and worth looking at so as to replicate with the tools you use, if possible. It
    supports the regional approach to federation by allowing federated Prometheus
    servers to scrape subsets of metrics from other, lower-level Prometheus servers.
    So it will accommodate any federation strategy you employ. [Chapter 9](ch09.html#observability_chapter)
    explores this topic in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: Federated software deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another important concern when managing various, remote clusters is how to manage
    deployment of software to those clusters. It’s one thing to be able to manage
    the clusters themselves, but it’s another entirely to organize the deployment
    of end-user workloads to these clusters. These are, after all, the point of having
    all these clusters. Perhaps you have critical, high-value workloads that must
    be deployed to multiple regions for availability purposes. Or maybe you just need
    to organize where workloads get deployed based on characteristics of different
    clusters. How you make these determinations is a challenging problem, as evidenced
    by the relative lack of consensus around a good solution to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes community has attempted to tackle this problem in a way that
    is broadly applicable for some time. The most recent incarnation is [KubeFed](https://github.com/kubernetes-sigs/kubefed).
    It also addresses cluster configurations, but here we’re concerned more with the
    definitions of workloads that are destined for multiple clusters. One of the useful
    design concepts that has emerged is the ability to federate any API type that
    is used in Kubernetes. For example, you can use federated versions of Namespace
    and Deployment types and for declaring how resources should be applied to multiple
    clusters. This is a powerful notion in that you can centrally create a FederatedDeployment
    resource in one management cluster and have that manifest as multiple remote Deployment
    objects being created in other clusters. However, we expect to see more advances
    in this area in the future. At this time, the most common way we still see in
    the field to manage this concern is with CI/CD tools that are configured to target
    different clusters when workloads are deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the broad architectural concerns that will frame how
    your fleet of clusters is organized and managed, let’s dig into the infrastructure
    concerns in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes deployment is a software installation process with a dependency on
    IT infrastructure. A Kubernetes cluster can be spun up on one’s laptop using virtual
    machines or Docker containers. But this is merely a simulation for testing purposes.
    For production use, various infrastructure components need to be present, and
    they are often provisioned as a part of the Kubernetes deployment itself.
  prefs: []
  type: TYPE_NORMAL
- en: A useful production-ready Kubernetes cluster needs some number of computers
    connected to a network to run on. To keep our terminology consistent, we’ll use
    the term *machines* for these computers. Those machines may be virtual or physical.
    The important issue is that you are able to provision these machines, and a primary
    concern is the method used to bring them online.
  prefs: []
  type: TYPE_NORMAL
- en: You may have to purchase hardware and install them in a datacenter. Or you may
    be able to simply request the needed resources from a cloud provider to spin up
    machines as needed. Whatever the process, you need machines as well as properly
    configured networking, and this needs to be accounted for in your deployment model.
  prefs: []
  type: TYPE_NORMAL
- en: As an important part of your automation efforts, give careful consideration
    to the automation of infrastructure management. Lean away from manual operations
    such as clicking through forms in an online wizard. Lean toward using declarative
    systems that instead call an API to bring about the same result. This automation
    model requires the ability to provision servers, networking, and related resources
    on demand, as with a cloud provider such as Amazon Web Services, Microsoft Azure,
    or Google Cloud Platform. However, not all environments have an API or web user
    interface to spin up infrastructure. Vast production workloads run in datacenters
    filled with servers that are purchased and installed by the company that will
    use them. This needs to happen well before the Kubernetes software components
    are installed and run. It’s important we draw this distinction and identify the
    models and patterns that apply usefully in each case.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will address the challenges of running Kubernetes on bare metal
    in contrast to using virtual machines for the nodes in your Kubernetes clusters.
    We will then discuss cluster sizing trade-offs and the implications that has for
    your cluster life cycle management. Subsequently, we will go over the concerns
    you should take into account for the compute and networking infrastructure. And,
    finally, this will lead us to some specific strategies for automating the infrastructure
    management for your Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Bare Metal Versus Virtualized
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When exploring Kubernetes, many ponder whether the relevance of the virtual
    machine layer is necessary. Don’t containers largely do the same thing? Would
    you essentially be running two layers of virtualization? The answer is, not necessarily.
    Kubernetes initiatives can be wildly successful atop bare metal or virtualized
    environments. Choosing the right medium to deploy to is critical and should be
    done through the lens of problems solved by various technologies and your team’s
    maturity in these technologies.
  prefs: []
  type: TYPE_NORMAL
- en: The virtualization revolution changed how the world provisions and manages infrastructure.
    Historically, infrastructure teams used methodologies such as PXE booting hosts,
    managing server configurations, and making ancillary hardware, such as storage,
    available to servers. Modern virtualized environments abstract all of this behind
    APIs, where resources can be provisioned, mutated, and deleted at will without
    knowing what the underlying hardware looks like. This model has been proven throughout
    datacenters with vendors such as VMware and in the cloud where the majority of
    compute is running atop some sort of hypervisor. Thanks to these advancements,
    many newcomers operating infrastructure in the cloud native world may never know
    about some of those underlying hardware concerns. The diagram in [Figure 2-3](#comparison_of_administrator_interactions_when_provisioning_and_managing_bare_metal)
    is not an exhaustive representation of the difference between virtualization and
    bare metal, but more so how the interaction points tend to differ.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0203](assets/prku_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Comparison of administrator interactions when provisioning and
    managing bare metal compute infrastructure versus virtual machines.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The benefits of the virtualized models go far beyond having a unified API to
    interact with. In virtualized environments, we have the benefit of building many
    virtual servers within our hardware server, enabling us to slice each computer
    into fully isolated machines where we can:'
  prefs: []
  type: TYPE_NORMAL
- en: Easily create and clone machines and machine images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run many operating systems on the same server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize server usage by dedicating variant amounts of resources based on application
    needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change resource settings without disrupting the server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programmatically control what hardware servers have access to, e.g., NICs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run unique networking and routing configurations per server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This flexibility also enables us to scope operational concerns on a smaller
    basis. For example, we can now upgrade one host without impacting all others running
    on the hardware. Additionally, with many of the mechanics available in virtualized
    environments, the creating and destroying of servers is typically more efficient.
    Virtualization has its own set of trade-offs. There is, typically, overhead incurred
    when running further away from the metal. Many hyper-performance–sensitive applications,
    such as trading applications, may prefer running on bare metal. There is also
    overhead in running the virtualization stack itself. In edge computing, for cases
    such as telcos running their 5G networks, they may desire running against the
    hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve completed a brief review of the virtualization revolution, let’s
    examine how this has impacted us when using Kubernetes and container abstractions
    because these force our point of interaction even higher up the stack. [Figure 2-4](#operator_interactions_when_using_kubernetes)
    illustrates what this looks like through an operator’s eyes at the Kubernetes
    layer. The underlying computers are viewed as a “sea of compute” where workloads
    can define what resources they need and will be scheduled appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0204](assets/prku_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Operator interactions when using Kubernetes.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s important to note that Kubernetes clusters have several integration points
    with the underlying infrastructure. For example, many use CSI-drivers to integrate
    with storage providers. There are multiple infra management projects that enable
    requesting new hosts from the provider and joining the cluster. And, most commonly,
    organizations rely on Cloud Provider Integrations (CPIs), which do additional
    work, such as provisioning load balancers outside of the cluster to route traffic
    within.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, there are a lot of conveniences infrastructure teams lose when
    leaving virtualization behind—things that Kubernetes *does not* inherently solve.
    However, there are several projects and integration points with bare metal that
    make this space evermore promising. Bare metal options are becoming available
    through major cloud providers, and bare metal–exclusive IaaS services like Packet
    (recently acquired by [Equinix Metal](https://metal.equinix.com)) are gaining
    market share. Success with bare metal is not without its challenges, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Significantly larger nodes
  prefs: []
  type: TYPE_NORMAL
- en: Larger nodes cause (typically) more Pods per node. When thousands of Pods per
    node are needed to make good use of your hardware, operations can become more
    complicated. For example, when doing in-place upgrades, needing to drain a node
    to upgrade it means you may trigger 1000+ rescheduling events.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic scaling
  prefs: []
  type: TYPE_NORMAL
- en: How to get new nodes up quickly based on workload or traffic needs.
  prefs: []
  type: TYPE_NORMAL
- en: Image provisioning
  prefs: []
  type: TYPE_NORMAL
- en: Quickly baking and distributing machine images to keep cluster nodes as immutable
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of load balancer API
  prefs: []
  type: TYPE_NORMAL
- en: Need to provide ingress routing from outside of the cluster to the Pod network
    within.
  prefs: []
  type: TYPE_NORMAL
- en: Less sophisticated storage integration
  prefs: []
  type: TYPE_NORMAL
- en: Solving for getting network-attached storage to Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Multitenant security concerns
  prefs: []
  type: TYPE_NORMAL
- en: When hypervisors are in play, we have the luxury of ensuring security-sensitive
    containers run on dedicated hypervisors. Specifically we can slice up a physical
    server in any arbitrary way and make container scheduling decisions based on that.
  prefs: []
  type: TYPE_NORMAL
- en: These challenges are absolutely solvable. For example, the lack of load balancer
    integration can be solved with projects like [kube-vip](https://kube-vip.io) or
    [metallb](https://metallb.universe.tf). Storage integration can be solved by integrating
    with a ceph cluster. However, the key is that containers aren’t a new-age virtualization
    technology. Under the hood, containers are (in most implementations) using Linux
    kernel primitives to make processes feel isolated from others on a host. There’s
    an endless number of trade-offs to continue unpacking, but in essence, our guidance
    when choosing between cloud providers (virtualization), on-prem virtualization,
    and bare metal is to consider what option makes the most sense based on your technical
    requirements and your organization’s operational experience. If Kubernetes is
    being considered a replacement for a virtualization stack, reconsider exactly
    what Kubernetes solves for. Remember that learning to operate Kubernetes and enabling
    teams to operate Kubernetes is already an undertaking. Adding the complexity of
    completely changing how you manage your infrastructure underneath it significantly
    grows your engineering effort and risk.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Sizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integral to the design of your Kubernetes deployment model and the planning
    for infrastructure is the cluster sizes you plan to use. We’re often asked, “How
    many worker nodes should be in production clusters?” This is a distinct question
    from, “How many worker nodes are needed to satisfy workloads?” If you plan to
    use one, single production cluster to rule them all, the answer to both questions
    will be the same. However, that is a unicorn we never see in the wild. Just as
    a Kubernetes cluster allows you to treat server machines as cattle, modern Kubernetes
    installation methods and cloud providers allow you to treat the clusters themselves
    as cattle. And every enterprise that uses Kubernetes has at least a small herd.
  prefs: []
  type: TYPE_NORMAL
- en: 'Larger clusters offer the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Better resource utilization
  prefs: []
  type: TYPE_NORMAL
- en: Each cluster comes with a control plane overhead cost. This includes etcd and
    components such as the API server. Additionally, you’ll run a variety of platform
    services in each cluster; for example, proxies via Ingress controllers. These
    components add overhead. A larger cluster minimizes replication of these services.
  prefs: []
  type: TYPE_NORMAL
- en: Fewer cluster deployments
  prefs: []
  type: TYPE_NORMAL
- en: If you run your own bare metal compute infrastructure, as opposed to provisioning
    it on-demand from a cloud provider or on-prem virtualization platform, spinning
    clusters up and down as needed, scaling those clusters as demands dictate becomes
    less feasible. Your cluster deployment strategy can afford to be less automated
    if you execute that deployment strategy less often. It is entirely possible the
    engineering effort to fully automate cluster deployments would be greater than
    the engineering effort to manage a less automated strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Simpler cluster and workload management profile
  prefs: []
  type: TYPE_NORMAL
- en: If you have fewer production clusters, the systems you use to allocate, federate,
    and manage these concerns need not be as streamlined and sophisticated. Federated
    cluster and workload management across fleets of clusters is complex and challenging.
    The community has been working on this. Large teams at enormous enterprises have
    invested heavily in bespoke systems for this. And these efforts have enjoyed limited
    success thus far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Smaller clusters offer the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Smaller blast radius
  prefs: []
  type: TYPE_NORMAL
- en: Cluster failures will impact fewer workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Tenancy flexibility
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides all the mechanisms needed to build a multitenant platform.
    However, in some cases you will spend far less engineering effort by provisioning
    a new cluster for a particular tenant. For example, if one tenant needs access
    to a cluster-wide resource like Custom Resource Definitions, and another tenant
    needs stringent guarantees of isolation for security and/or compliance, it may
    be justified to dedicate clusters to such teams, especially if their workloads
    demand significant compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: Less tuning for scale
  prefs: []
  type: TYPE_NORMAL
- en: As clusters scale into several hundred workers, we often encounter issues of
    scale that need to be solved for. These issues vary case to case, but bottlenecks
    in your control plane can occur. Engineering effort will need to be expended in
    troubleshooting and tuning clusters. Smaller clusters considerably reduce this
    expenditure.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrade options
  prefs: []
  type: TYPE_NORMAL
- en: Using smaller clusters lends itself more readily to replacing clusters in order
    to upgrade them. Cluster replacements certainly come with their own challenges,
    and these are covered later in this chapter in [“Upgrades”](#upgrades), but this
    replacement strategy is an attractive upgrade option in many cases, and operating
    smaller clusters can make it even more attractive.
  prefs: []
  type: TYPE_NORMAL
- en: Node pool alternative
  prefs: []
  type: TYPE_NORMAL
- en: If you have workloads with specialized concerns such as GPUs or memory optimized
    nodes, and your systems readily accommodate lots of smaller clusters, it will
    be trivial to run dedicated clusters to accommodate these kinds of specialized
    concerns. This alleviates the complexity of managing multiple node pools as discussed
    earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Compute Infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To state the obvious, a Kubernetes cluster needs machines. Managing pools of
    these machines is the core purpose, after all. An early consideration is what
    types of machines you should choose. How many cores? How much memory? How much
    onboard storage? What grade of network interface? Do you need any specialized
    devices such as GPUs? These are all concerns that are driven by the demands of
    the software you plan to run. Are the workloads compute intensive? Or are they
    memory hungry? Are you running machine learning or AI workloads that necessitate
    GPUs? If your use case is very typical in that your workloads fit general-purpose
    machines’ compute-to-memory ratio well, and if your workloads don’t vary greatly
    in their resource consumption profile, this will be a relatively simple exercise.
    However, if you have less typical and more diverse software to run, this will
    be a little more involved. Let’s consider the different types of machines to consider
    for your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: etcd machines (optional)
  prefs: []
  type: TYPE_NORMAL
- en: This is an optional machine type that is only applicable if you run a dedicated
    etcd clusters for your Kubernetes clusters. We covered the trade-offs with this
    option in an earlier section. These machines should prioritize disk read/write
    performance, so never use old spinning disk hard drives. Also consider dedicating
    a storage disk to etcd, even if running etcd on dedicated machines, so that etcd
    suffers no contention with the OS or other programs for use of the disk. Also
    consider network performance, including proximity on the network, to reduce network
    latency between machines in a given etcd cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane nodes (required)
  prefs: []
  type: TYPE_NORMAL
- en: These machines will be dedicated to running control plane components for the
    cluster. They should be general-purpose machines that are sized and numbered according
    to the anticipated size of the cluster as well as failure tolerance requirements.
    In a larger cluster, the API server will have more clients and manage more traffic.
    This can be accommodated with more compute resources per machine, or more machines.
    However, components like the scheduler and controller-manager have only one active
    leader at any given time. Increasing capacity for these cannot be achieved with
    more replicas the way it can with the API server. Scaling vertically with more
    compute resources per machine must be used if these components become starved
    for resources. Additionally, if you are colocating etcd on these control plane
    machines, the same considerations for etcd machines noted above also apply.
  prefs: []
  type: TYPE_NORMAL
- en: Worker Nodes (required)
  prefs: []
  type: TYPE_NORMAL
- en: These are general-purpose machines that host non–control plane workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Memory optimized Nodes (optional)
  prefs: []
  type: TYPE_NORMAL
- en: If you have workloads that have a memory profile that doesn’t make them a good
    fit for general-purpose worker nodes, you should consider a node pool that is
    memory optimized. For example, if you are using AWS general-purpose M5 instance
    types for worker nodes that have a CPU:memory ratio of 1CPU:4GiB, but you have
    a workload that consumes resources at a ratio of 1CPU:8GiB, these workloads will
    leave unused CPU when resources are requested (reserved) in your cluster at this
    ratio. This inefficiency can be overcome by using memory-optimized nodes such
    as the R5 instance types on AWS, which have a ratio of 1CPU:8GiB.
  prefs: []
  type: TYPE_NORMAL
- en: Compute optimized Nodes (optional)
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if you have workloads that fit the profile of a compute-optimized
    node such as the C5 instance type in AWS with 1CPU:2GiB, you should consider adding
    a node pool with these machine types for improved efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized hardware Nodes (optional)
  prefs: []
  type: TYPE_NORMAL
- en: A common hardware ask is GPUs. If you have workloads (e.g., machine learning)
    requiring specialized hardware, adding a node pool in your cluster and then targeting
    those nodes for the appropriate workloads will work well.
  prefs: []
  type: TYPE_NORMAL
- en: Networking Infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Networking is easy to brush off as an implementation detail, but it can have
    important impacts on your deployment models. First, let’s examine the elements
    that you will need to consider and design for.
  prefs: []
  type: TYPE_NORMAL
- en: Routability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You almost certainly do not want your cluster nodes exposed to the public internet.
    The convenience of being able to connect to those nodes from anywhere almost never
    justifies the threat exposure. You will need to solve for gaining access to those
    nodes should you need to connect to them, but a bastion host or jump box that
    is well secured and that will allow SSH access, and in turn allow you to connect
    to cluster nodes, is a low barrier to hop.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are more nuanced questions to answer, such as network access
    on your private network. There will be services on your network that will need
    connectivity to and from your cluster. For example, it is common to need connectivity
    with storage arrays, internal container registries, CI/CD systems, internal DNS,
    private NTP servers, etc. Your cluster will also usually need access to public
    resources such as public container registries, even if via an outbound proxy.
  prefs: []
  type: TYPE_NORMAL
- en: If outbound public internet access is out of the question, those resources such
    as open source container images and system packages will need to be made available
    in some other way. Lean toward simpler systems that are consistent and effective.
    Lean away from, if possible, mindless mandates and human approval for infrastructure
    needed for cluster deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use availability zones (AZs) to help maintain uptime where possible. For clarity,
    an availability zone is a datacenter that has a distinct power source and backup
    as well as a distinct connection to the public internet. Two subnets in a datacenter
    with a shared power supply do not constitute two availability zones. However,
    two distinct datacenters that are in relatively close proximity to one another
    and have a low-latency, high-bandwidth network connection between them do constitute
    a pair of availability zones. Two AZs is good. Three is better. More than that
    depends of the level of catastrophe you need to prepare for. Datacenters have
    been known to go down. For multiple datacenters in a region to suffer simultaneous
    outages is possible, but rare and would often indicate a kind of disaster that
    will require you to consider how critical your workloads are. Are you running
    workloads necessary to national defense, or an online store? Also consider where
    you need redundancy. Are you building redundancy for your workloads? Or the control
    plane of the cluster itself? In our experience it is acceptable to run etcd across
    AZs but, if doing so, revisit [“Network considerations”](#network_considerations).
    Keep in mind that distributing your control plane across AZs gives redundant *control*
    over the cluster. Unless your workloads depend on the cluster control plane (which
    should be avoided), your workload availability will not be affected by a control
    plane outage. What will be affected is your ability to make any changes to your
    running software. A control plane outage is not trivial. It is a high-priority
    emergency. But it is not the same as an outage for user-facing workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will need a load balancer for the Kubernetes API servers. Can you programmatically
    provision a load balancer in your environment? If so, you will be able to spin
    up and configure it as a part of the deployment of your cluster’s control plane.
    Think through the access policies to your cluster’s API and, subsequently, what
    firewalls your load balancer will sit behind. You almost certainly will not make
    this accessible from the public internet. Remote access to your cluster’s control
    plane is far more commonly done so via a VPN that provides access to the local
    network that your cluster resides on. On the other hand, if you have workloads
    that are publicly exposed, you will need a separate and distinct load balancer
    that connects to your cluster’s ingress. In most cases this load balancer will
    serve all incoming requests to the various workloads in your cluster. There is
    little value in deploying a load balancer and cluster ingress for each workload
    that is exposed to requests from outside the cluster. If running a dedicated etcd
    cluster, do not put a load balancer between the Kubernetes API and etcd. The etcd
    client that the API uses will handle the connections to etcd without the need
    for a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Automation Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In automating the infrastructure components for your Kubernetes clusters, you
    have some strategic decisions to make. We’ll break this into two categories, the
    first being the tools that exist today that you can leverage. Then, we’ll talk
    about how Kubernetes operators can be used in this regard. Recognizing that automation
    capabilities will look very different for bare metal installations, we will start
    from the assumption that you have an API with which to provision machines or include
    them in a pool for Kubernetes deployment. If that is not the case, you will need
    to fill in the gaps with manual operations up to the point where you do have programmatic
    access and control. Let’s start with some of the tools you may have at your disposal.
  prefs: []
  type: TYPE_NORMAL
- en: Infra management tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tools such as [Terraform](https://www.terraform.io) and [CloudFormation for
    AWS](https://aws.amazon.com/cloudformation) allow you to declare the desired state
    for your compute and networking infrastructure and then apply that state. They
    use data formats or configuration languages that allow you to define the outcome
    you require in text files and then tell a piece of software to satisfy the desired
    state declared in those text files.
  prefs: []
  type: TYPE_NORMAL
- en: They are advantageous in that they use tooling that engineers can readily adopt
    and get outcomes with. They are good at simplifying the process of relatively
    complex infrastructure provisioning. They excel when you have a prescribed set
    of infrastructure that needs to be stamped out repeatedly and there is not a lot
    of variance between instances of the infrastructure. It greatly lends itself to
    the principle of immutable infrastructure because the repeatability is reliable,
    and infrastructure *replacement* as opposed to *mutation* becomes quite manageable.
  prefs: []
  type: TYPE_NORMAL
- en: These tools begin to decline in value when the infrastructure requirements become
    significantly complex, dynamic, and dependent on variable conditions. For example,
    if you are designing Kubernetes deployment systems across multiple cloud providers,
    these tools will become cumbersome. Data formats like JSON and configuration languages
    are not good at expressing conditional statements and looping functions. This
    is where general-purpose programming languages shine.
  prefs: []
  type: TYPE_NORMAL
- en: In development stages, infra management tools are very commonly used successfully.
    They are indeed used to manage production environments in certain shops, too.
    But they become cumbersome to work with over time and often take on a kind of
    technical debt that is almost impossible to pay down. For these reasons, strongly
    consider using or developing Kubernetes operators for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes operators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If infra management tools impose limitations that warrant writing software using
    general-purpose programming languages, what form should that software take? You
    could write a web application to manage your Kubernetes infrastructure. Or a command-line
    tool. If considering custom software development for this purpose, strongly consider
    Kubernetes operators.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of Kubernetes, operators use custom resources and custom-built
    Kubernetes controllers to manage systems. Controllers use a method of managing
    state that is powerful and reliable. When you create an instance of a Kubernetes
    resource, the controller responsible for that resource kind is notified by the
    API server via its watch mechanism and then uses the declared desired state in
    the resource spec as instructions to fulfill the desired state. So extending the
    Kubernetes API with new resource kinds that represent infrastructure concerns,
    and developing Kubernetes operators to manage the state of these infrastructure
    resources, is very powerful. The topic of Kubernetes operators is covered in more
    depth in [Chapter 11](ch11.html#building_platform_services).
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly what the Cluster API project is. It is a collection of Kubernetes
    operators that can be used to manage the infrastructure for Kubernetes clusters.
    And you can certainly leverage that open source project for your purposes. In
    fact, we would recommend you examine this project to see if it may fit your needs
    before starting a similar project from scratch. And if it doesn’t fulfill your
    requirements, could your team get involved in contributing to that project to
    help develop the features and/or supported infrastructure providers that you require?
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes offers excellent options for automating the management of containerized
    software deployments. Similarly, it offers considerable benefits for cluster infrastructure
    automation strategies through the use of Kubernetes operators. Strongly consider
    using and, where possible, contributing to projects such as Cluster API. If you
    have custom requirements and prefer to use infrastructure management tools, you
    can certainly be successful with this option. However, your solutions will have
    less flexibility and more workarounds due to the limitations of using configuration
    languages and formats rather than full-featured programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Installations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the machines for your cluster are spun up, they will need an operating
    system, certain packages installed, and configurations written. You will also
    need some utility or program to determine environmental and other variable values,
    apply them, and coordinate the process of starting the Kubernetes containerized
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two broad strategies commonly used here:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration management tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Configuration management tools such as Ansible, Chef, Puppet, and Salt gained
    popularity in a world where software was installed on virtual machines and run
    directly on the host. These tools are quite magnificent for automating the configuration
    of multitudes of remote machines. They follow varying models but, in general,
    administrators can declaratively prescribe how a machine must look and apply that
    prescription in an automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: These config management tools are excellent in that they allow you to reliably
    establish machine consistency. Each machine can get an effectively identical set
    of software and configurations installed. And it is normally done with declarative
    recipes or playbooks that are checked into version control. These all make them
    a positive solution.
  prefs: []
  type: TYPE_NORMAL
- en: Where they fall short in a Kubernetes world is the speed and reliability with
    which you can bring cluster nodes online. If the process you use to join a new
    worker node to a cluster includes a config management tool performing installations
    of packages that pull assets over network connections, you are adding significant
    time to the join process for that cluster node. Furthermore, errors occur during
    configuration and installation. Everything from temporarily unavailable package
    repositories to missing or incorrect variables can cause a config management process
    to fail. This interrupts the cluster node join altogether. And if you’re relying
    on that node to join an autoscaled cluster that is resource constrained, you may
    well invoke or prolong an availability problem.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using machine images is a superior alternative. If you use machine images with
    all required packages installed, the software is ready to run as soon as the machine
    boots up. There is no package install that depends on the network and an available
    package repo. Machine images improve the reliability of the node joining the cluster
    and considerably shorten the lead time for the node to be ready to accept traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The added beauty of this method is that you can often use the config management
    tools you are familiar with to build the machine images. For example, using [Packer](https://www.packer.io)
    from HashiCorp you can employ Ansible to build an Amazon Machine Image and have
    that prebuilt image ready to apply to your instances whenever they are needed.
    An error running an Ansible playbook to build a machine image is not a big deal.
    In contrast, having a playbook error occur that interrupts a worker node joining
    a cluster could induce a significant production incident.
  prefs: []
  type: TYPE_NORMAL
- en: You can—and should—still keep the assets used for builds in version control,
    and all aspects of the installations can remain declarative and clear to anyone
    that inspects the repository. Anytime upgrades or security patches need to occur,
    the assets can be updated, committed and, ideally, run automatically according
    to a pipeline once merged.
  prefs: []
  type: TYPE_NORMAL
- en: Some decisions involve difficult trade-offs. Some are dead obvious. This is
    one of those. Use prebuilt machine images.
  prefs: []
  type: TYPE_NORMAL
- en: What to Install
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So what do you need to install on the machine?
  prefs: []
  type: TYPE_NORMAL
- en: To start with the most obvious, you need an operating system. A Linux distribution
    that Kubernetes is commonly used and tested with is the safe bet. RHEL/CentOS
    or Ubuntu are easy choices. If you have enterprise support for, or if you are
    passionate about, another distro and you’re willing to invest a little extra time
    in testing and development, that is fine, too. Extra points if you opt for a distribution
    designed for containers such as [Flatcar Container Linux](https://www.flatcar-linux.org).
  prefs: []
  type: TYPE_NORMAL
- en: To continue in order of obviousness, you will need a container runtime such
    as docker or containerd. When running containers, one must have a container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Next is the kubelet. This is the interface between Kubernetes and the containers
    it orchestrates. This is the component that is installed on the machine that coordinates
    the containers. Kubernetes is a containerized world. Modern conventions follow
    that Kubernetes itself runs in containers. With that said, the kubelet is one
    of the components that runs as a regular binary or process on the host. There
    have been attempts to run the kubelet as a container, but that just complicates
    things. Don’t do that. Install the kubelet on the host and run the rest of Kubernetes
    in containers. The mental model is clear and the practicalities hold true.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have a Linux OS, a container runtime to run containers, and an interface
    between Kubernetes and the container runtime. Now we need something that can bootstrap
    the Kubernetes control plane. The kubelet can get containers running, but without
    a control plane it doesn’t yet know what Kubernetes Pods to spin up. This is where
    kubeadm and static Pods come in.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeadm is far from the only tool that can perform this bootstrapping. But it
    has gained wide adoption in the community and is used successfully in many enterprise
    production systems. It is a command-line program that will, in part, stamp out
    the static Pod manifests needed to get Kubernetes up and running. The kubelet
    can be configured to watch a directory on the host and run Pods for any Pod manifest
    it finds there. Kubeadm will configure the kubelet appropriately and deposit the
    manifests as needed. This will bootstrap the core, essential Kubernetes control
    plane components, notably etcd, kube-apiserver, kube-scheduler, and kube-controller-manager.
  prefs: []
  type: TYPE_NORMAL
- en: Thereafter, the kubelet will get all further instructions to create Pods from
    manifests submitted to the Kubernetes API. Additionally, kubeadm will generate
    bootstrap tokens you can use to securely join other nodes to your shiny new cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you will need some kind of *bootstrap utility*. The Cluster API project
    uses Kubernetes custom resources and controllers for this. But a command-line
    program installed on the host also works well. The primary function of this utility
    is to call kubeadm and manage runtime configurations. When the machine boots,
    arguments provided to the utility allow it to configure the bootstrapping of Kubernetes.
    For example, in AWS you can leverage user data to run your bootstrap utility and
    pass arguments to it that will inform which flags should be added to the kubeadm
    command or what to include in a kubeadm config file. Minimally, it will include
    a runtime config that tells the bootstrap utility whether to create a new cluster
    with `kubeadm init` or join the machine to an existing cluster with `kubeadm join`.
    It should also include a secure location to store the bootstrap token if initializing,
    or to retrieve the bootstrap token if joining. These tokens ensure only approved
    machines are attached to your cluster, so treat them with care. To gain a clear
    idea of what runtime configs you will need to provide to your bootstrap utility,
    run through a manual install of Kubernetes using kubeadm, which is well documented
    in the official docs. As you run through those steps it will become apparent what
    will be needed to meet your requirements in your environment. [Figure 2-5](#bootstrapping_a_machine_to_initialize_kubernetes)
    illustrates the steps involved in bringing up a new machine to create the first
    control plane node in a new Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0205](assets/prku_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Bootstrapping a machine to initialize Kubernetes.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we’ve covered what to install on the machines that are used as part
    of a Kubernetes cluster, let’s move on to the software that runs in containers
    to form the control plane for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Containerized Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The static manifests used to spin up a cluster should include those essential
    components of the control plane: etcd, kube-apiserver, kube-scheduler, and kube-controller-manager.
    You can provide additional custom Pod manifests as needed, but strictly limit
    them to Pods that absolutely need to run before the Kubernetes API is available
    or registered into a federated system. If a workload can be installed by way of
    the API server later on, do so. Any Pods created with static manifests can be
    managed only by editing those static manifests on the machine’s disk, which is
    much less accessible and prone to automation.'
  prefs: []
  type: TYPE_NORMAL
- en: If using kubeadm, which is strongly recommended, the static manifests for your
    control plane, including etcd, will be created when a control plane node is initialized
    with `kubeadm init`. Any flag specifications you need for these components can
    be passed to kubeadm using the kubeadm config file. The bootstrap utility that
    we discussed in the previous section that calls kubeadm can write a templated
    kubeadm config file, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid customizing the static Pod manifests directly with your bootstrap utility.
    If really necessary, you can perform separate static manifest creation and cluster
    initialization steps with kubeadm that will give you the opportunity to inject
    customization if needed, but only do so if it’s important and cannot be achieved
    via the kubeadm config. A simpler, less complicated bootstrapping of the Kubernetes
    control plane will be more robust, faster, and will be far less likely to break
    with Kubernetes version upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeadm will also generate self-signed TLS assets that are needed to securely
    connect components of your control plane. Again, avoid tinkering with this. If
    you have security requirements that demand using your corporate CA as a source
    of trust, then you can do so. If this is a requirement, it’s important to be able
    to automate the acquisition of the intermediate authority. And keep in mind that
    if your cluster bootstrapping systems are secure, the trust of the self-signed
    CA used by the control plane will be secure and will be valid only for the control
    plane of a single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the nuts and bolts of installing Kubernetes, let’s dive
    into the immediate concerns that come up once you have a running cluster. We’ll
    begin with approaches for getting the essential add-ons installed onto Kubernetes.
    These add-ons constitute the components you need to have in addition to Kubernetes
    to deliver a production-ready application platform. Then we’ll get into the concerns
    and strategies for carrying out upgrades to your platform.
  prefs: []
  type: TYPE_NORMAL
- en: Add-ons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster add-ons broadly cover those additions of platform services layered onto
    a Kubernetes cluster. We will not cover *what* to install as a cluster add-on
    in this section. That is essentially the topic of the rest of the chapters in
    this book. Rather, this is a look at *how* to go about installing the components
    that will turn your raw Kubernetes cluster into a production-ready, developer-friendly
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: The add-ons that you add to a cluster should be considered as a part of the
    deployment model. Add-on installation will usually constitute the final phase
    of a cluster deployment. These add-ons should be managed and versioned in combination
    with the Kubernetes cluster itself. It is useful to consider Kubernetes and the
    add-ons that comprise the platform as a package that is tested and released together
    since there will inevitably be version and configuration dependencies between
    certain platform components.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeadm installs “required” add-ons that are necessary to pass the Kubernetes
    project’s conformance tests, including cluster DNS and kube-proxy, which implements
    Kubernetes Service resources. However, there are many more, similarly critical
    components that will need to be applied after kubeadm has finished its work. The
    most glaring example is a container network interface plug-in. Your cluster will
    not be good for much without a Pod network. Suffice to say you will end up with
    a significant list of components that need to be added to your cluster, usually
    in the form of DaemonSets, Deployments, or StatefulSets that will add functionality
    to the platform you’re building on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, in [“Architecture and Topology”](#arch_and_topology), we discussed
    cluster federation and the registration of new clusters into that system. That
    is usually a precursor to add-on installation because the systems and definitions
    for installation often live in a management cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever the architecture used, once registration is achieved, the installation
    of cluster add-ons can be triggered. This installation process will be a series
    of calls to the cluster’s API server to create the Kubernetes resources needed
    for each component. Those calls can come from a system outside the cluster or
    inside.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to installing add-ons is to use a continuous delivery pipeline
    using existing tools such as Jenkins. The “continuous” part is irrelevant in this
    context since the trigger is not a software update but rather a new target for
    installation. The “continuous” part of CI and CD usually refers to automated rollouts
    of software once new changes have been merged into a branch of version-controlled
    source code. Triggering installations of cluster add-on software into a newly
    deployed cluster is an entirely different mechanism but is useful in that the
    pipeline generally contains the capabilities needed for the installations. All
    that is needed to implement is the call to run a pipeline in response to the creation
    of a new cluster along with any variables to perform proper installation.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach that is more native to Kubernetes is to use a Kubernetes operator
    for the task. This more advanced approach involves extending the Kubernetes API
    with one or more custom resources that allow you to define the add-on components
    for the cluster and their versions. It also involves writing the controller logic
    that can execute the proper installation of the add-on components given the defined
    spec in the custom resource.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is useful in that it provides a central, clear source of truth
    for what the add-ons are for a cluster. But more importantly, it offers the opportunity
    to programmatically manage the ongoing life cycle of these add-ons. The drawback
    is the complexity of developing and maintaining more complex software. If you
    take on this complexity, it should be because you will implement those day-2 upgrades
    and ongoing management that will greatly reduce future human toil. If you stop
    at day-1 installation and do not develop the logic and functionality to manage
    upgrades, you will be taking on a significant software engineering cost for little
    ongoing benefit. Kubernetes operators offer the most value in ongoing operational
    management with their watch functionality of the custom resources that represent
    desired state.
  prefs: []
  type: TYPE_NORMAL
- en: To be clear, the add-on operator concept isn’t necessarily entirely independent
    from external systems such as a CI/CD. In reality, they are far more likely to
    be used in conjunction. For example, you may use a CD pipeline to install the
    operator and add-on custom resources and then let the operator take over. Also,
    the operator will likely need to fetch manifests for installation, perhaps from
    a code repository that contains templated Kubernetes manifests for the add-ons.
  prefs: []
  type: TYPE_NORMAL
- en: Using an operator in this manner reduces external dependencies, which drives
    improved reliability. However, external dependencies cannot be eliminated altogether.
    Using an operator to solve add-ons should be undertaken only when you have engineers
    that know the Kubernetes operator pattern well and have experience leveraging
    it. Otherwise, stick with tools and systems that your team knows well while you
    advance the knowledge and experience of your team in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'That brings us to the conclusion of the “day 1” concerns: the systems to install
    a Kubernetes cluster and its add-ons. Now we will turn to the “day 2” concern
    of upgrades.'
  prefs: []
  type: TYPE_NORMAL
- en: Upgrades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster life cycle management is closely related to cluster deployment. A cluster
    deployment system doesn’t necessarily need to account for future upgrades; however,
    there are enough overlapping concerns to make it advisable. At the very least,
    your upgrade strategy needs to be solved before going to production. Being able
    to deploy the platform without the ability to upgrade and maintain it is hazardous
    at best. When you see production workloads running on versions of Kubernetes that
    are way behind the latest release, you are looking at the outcome of developing
    a cluster deployment system that has been deployed to production before upgrade
    capabilities were added to the system. When you first go to production with revenue-producing
    workloads running, considerable engineering budget will be spent attending to
    features you find missing or to sharp edges you find your team cutting themselves
    on. As time goes by, those features will be added and the sharp edges removed,
    but the point is they will naturally take priority while the upgrade strategy
    sits in the backlog getting stale. Budget early for those day-2 concerns. Your
    future self will thank you.
  prefs: []
  type: TYPE_NORMAL
- en: In addressing this subject of upgrades we will first look at versioning your
    platform to help ensure dependencies are well understood for the platform itself
    and for the workloads that will use it. We will also address how to approach planning
    for rollbacks in the event something goes wrong and the testing to verify that
    everything has gone according to plan. Finally, we will compare and contrast specific
    strategies for upgrading Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Platform Versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, version your platform and document the versions of all software
    used in that platform. That includes the operating system version of the machines
    and all packages installed on them, such as the container runtime. It obviously
    includes the version of Kubernetes in use. And it should also include the version
    of each add-on that is added to make up your application platform. It is somewhat
    common for teams to adopt the Kubernetes version for their platform so that everyone
    knows that version 1.18 of the application platform uses Kubernetes version 1.18
    without any mental overhead or lookup. This is of trivial importance compared
    to the fact of just doing the versioning and documenting it. Use whatever system
    your team prefers. But have the system, document the system, and use it religiously.
    My only objection to pinning your platform version to any component of that system
    is that it may occasionally induce confusion. For example, if you need to update
    your container runtime’s version due to a security vulnerability, you should reflect
    that in the version of your platform. If using semantic versioning conventions,
    that would probably look like a change to the bugfix version number. That may
    be confused with a version change in Kubernetes itself, i.e., v1.18.5 → 1.18.6\.
    Consider giving your platform its own independent version numbers, especially
    if using semantic versioning that follows the major/minor/bugfix convention. It’s
    almost universal that software has its own independent version with dependencies
    on other software and their versions. If your platform follows those same conventions,
    the meaning will be immediately clear to all engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Plan to Fail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start from the premise that something will go wrong during the upgrade process.
    Imagine yourself in the situation of having to recover from a catastrophic failure,
    and use that fear and anguish as motivation to prepare thoroughly for that outcome.
    Build automation to take and restore backups for your Kubernetes resources—both
    with direct etcd snapshots as well as Velero backups taken through the API. Do
    the same for the persistent data used by your applications. And address disaster
    recovery for your critical applications and their dependencies directly. For complex,
    stateful, distributed applications it will likely not be enough to merely restore
    the application’s state and Kubernetes resources without regard to order and dependencies.
    Brainstorm all the possible failure modes, and develop automated recovery systems
    to remedy and then test them. For the most critical workloads and their dependencies,
    consider having standby clusters ready to fail over to—and then automate and test
    those fail-overs where possible.
  prefs: []
  type: TYPE_NORMAL
- en: Consider your rollback paths carefully. If an upgrade induces errors or outages
    that you cannot immediately diagnose, having rollback options is good insurance.
    Complex distributed systems can take time to troubleshoot, and that time can be
    extended by the stress and distraction of production outages. Predetermined playbooks
    and automation to fall back on are more important than ever when dealing with
    complex Kubernetes-based platforms. But be practical and realistic. In the real
    world, rollbacks are not always a good option. For example, if you’re far enough
    along in an upgrade process, rolling all earlier changes back may be a terrible
    idea. Think that through ahead of time, know where your points of no return are,
    and strategize before you execute those operations live.
  prefs: []
  type: TYPE_NORMAL
- en: Integration Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a well-documented versioning system that includes all component versions
    is one thing, but how you manage these versions is another. In systems as complex
    as Kubernetes-based platforms, it is a considerable challenge to ensure everything
    integrates and works together as expected every time. Not only is compatibility
    between all components of the platform critical, but compatibility between the
    workloads that run on the platform and the platform itself must also be tested
    and confirmed. Lean toward platform agnosticism for your applications to reduce
    possible problems with platform compatibility, but there are many instances when
    application workloads yield tremendous value when leveraging platform features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit testing for all platform components is important, along with all other
    sound software engineering practices. But integration testing is equally vital,
    even if considerably more challenging. An excellent tool to aid in this effort
    is the Sonobuoy conformance test utility. It is most commonly used to run the
    upstream Kubernetes end-to-end tests to ensure you have a properly running cluster;
    i.e., all the cluster’s components are working together as expected. Often teams
    will run a Sonobuoy scan after a new cluster is provisioned to automate what would
    normally be a manual process of examining control plane Pods and deploying test
    workloads to ensure the cluster is properly operational. However, I suggest taking
    this a couple of steps further. Develop your own plug-ins that test the specific
    functionality and features of your platform. Test the operations that are critical
    to your organization’s requirements. And run these scans routinely. Use a Kubernetes
    CronJob to run at least a subset of plug-ins, if not the full suite of tests.
    This is not exactly available out of the box today but can be achieved with a
    little engineering and is well worth the effort: expose the scan results as metrics
    that can be displayed in dashboards and alerted upon. These conformance scans
    can essentially test that the various parts of a distributed system are working
    together to produce the functionality and features you expect to be there and
    constitute a very capable automated integration testing approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, integration testing must be extended to the applications that run on
    the platform. Different integration testing strategies will be employed by different
    app dev teams, and this may be largely out of the platform team’s hands, but advocate
    strongly for it. Run the integrations tests on a cluster that closely resembles
    the production environment, but more on that shortly. This will be more critical
    for workloads that leverage platform features. Kubernetes operators are a compelling
    example of this. These extend the Kubernetes API and are naturally deeply integrated
    with the platform. And if you’re using an operator to deploy and manage the life
    cycle for any of your organization’s software systems, it is imperative that you
    perform integration tests across versions of your platform, especially when Kubernetes
    version upgrades are involved.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’re going to look at three strategies for upgrading your Kubernetes-based
    platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster replacement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node replacement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-place upgrades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re going to address them in order of highest cost with lowest risk to lowest
    cost with highest risk. As with most things, there is a trade-off that eliminates
    the opportunity for a one-size-fits-all, universally ideal solution. The costs
    and benefits need to be considered to find the right solution for your requirements,
    budget, and risk tolerance. Furthermore, within each strategy, there are degrees
    of automation and testing that, again, will depend on factors such as engineering
    budget, risk tolerance, and upgrade frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind, these strategies are not mutually exclusive. You can use combinations.
    For example, you could perform in-place upgrades for a dedicated etcd cluster
    and then use node replacements for the rest of the Kubernetes cluster. You *can*
    also use different strategies in different tiers where the risk tolerances are
    different. However, it is advisable to use the same strategy everywhere so that
    the methods you employ in production have first been thoroughly tested in development
    and staging.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whichever strategy you employ, a few principles remain constant: test thoroughly
    and automate as much as is practical. If you build automation to perform actions
    and test that automation thoroughly in testing, development, and staging clusters,
    your production upgrades will be far less likely to produce issues for end users
    and far less likely to invoke stress in your platform operations team.'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster replacement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cluster replacement is the highest cost, lowest risk solution. It is low risk
    in that it follows immutable infrastructure principles applied to the entire cluster.
    An upgrade is performed by deploying an entirely new cluster alongside the old.
    Workloads are migrated from the old cluster to the new. The new, upgraded cluster
    is scaled out as needed as workloads are migrated on. The old cluster’s worker
    nodes are scaled in as workloads are moved off. But throughout the upgrade process
    there is an addition of an entirely distinct new cluster and the costs associated
    with it. The scaling out of the new and scaling in of the old mitigates this cost,
    which is to say that if you are upgrading a 300-node production cluster, you do
    not need to provision a new cluster with 300 nodes at the outset. You would provision
    a cluster with, say, 20 nodes. And when the first few workloads have been migrated,
    you can scale in the old cluster that has reduced usage and scale out the new
    to accommodate other incoming workloads. The use of cluster autoscaling and cluster
    overprovisioning can make this quite seamless, but upgrades alone are unlikely
    to be a sound justification for using those technologies. There are two common
    challenges when tackling a cluster replacement.
  prefs: []
  type: TYPE_NORMAL
- en: The first is managing ingress traffic. As workloads are migrated from one cluster
    to the next, traffic will need to be rerouted to the new, upgraded cluster. This
    implies that DNS for the publicly exposed workloads does not resolve to the cluster
    ingress, but rather to a global service load balancer (GSLB) or reverse proxy
    that, in turn, routes traffic to the cluster ingress. This gives you a point from
    which to manage traffic routing into multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The other is persistent storage availability. If using a storage service or
    appliance, the same storage needs to be accessible from both clusters. If using
    a managed service such as a database service from a public cloud provider, you
    must ensure the same service is available from both clusters. In a private datacenter
    this could be a networking and firewalling question. In the public cloud it will
    be a question of networking and availability zones; for example, AWS EBS volumes
    are available from specific availability zones. And managed services in AWS often
    have specific Virtual Private Clouds (VPCs) associated. You may consider using
    a single VPC for multiple clusters for this reason. Oftentimes Kubernetes installers
    assume a VPC per cluster, but this isn’t always the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will concern yourself with workload migrations. Primarily, we’re
    talking about the Kubernetes resources themselves—the Deployments, Services, ConfigMaps,
    etc. You can do this workload migration in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Redeploy from a declared source of truth
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the existing resources over from the old cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first option would likely involve pointing your deployment pipeline at the
    new cluster and having it redeploy the same resource to the new cluster. This
    assumes the source of truth for your resource definitions that you have in version
    control is reliable and that no in-place changes have taken place. In reality,
    this is quite uncommon. Usually, humans, controllers, and other systems have made
    in-place changes and adjustments. If this is the case, you will need go with option
    2 and make a copy of the existing resources and deploy them to the new cluster.
    This is where a tool like Velero can be extremely valuable. Velero is more commonly
    touted as a backup tool, but its value as a migration tool is as high or possibly
    even higher. Velero can take a snapshot of all resources in your cluster, or a
    subset. So if you migrate workloads one Namespace at a time, you can take snapshots
    of each Namespace at the time of migration and restore those snapshots into the
    new cluster in a highly reliable manner. It takes these snapshots not directly
    from the etcd data store, but rather through the Kubernetes API, so as long as
    you can provide access to Velero to the API server for both clusters, this method
    can be very useful. [Figure 2-6](#migrating_workloads_between_clusters_with_a_backup)
    illustrates this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0206](assets/prku_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Migrating workloads between clusters with a backup and restore
    using Velero.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Node replacement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The node replacement option represents a middle ground for cost and risk. It
    is a common approach and is supported by Cluster API. It is a palatable option
    if you’re managing larger clusters and compatibility concerns are well understood.
    Those compatibility concerns represent one of the biggest risks for this method
    because you are upgrading the control plane in-place as far as your cluster services
    and workloads are concerned. If you upgrade Kubernetes in-place and an API version
    that one of your workloads is using is no longer present, your workload could
    suffer an outage. There are several ways to mitigate this:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the Kubernetes release notes. Before rolling out a new version of your
    platform that includes a Kubernetes version upgrade, read the CHANGELOG thoroughly.
    Any API deprecations or removals are well documented there, so you will have plenty
    of advance notice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test thoroughly before production. Run new versions of your platform extensively
    in development and staging clusters before rolling out to production. Get the
    latest version of Kubernetes running in dev shortly after it is released and you
    will be able to thoroughly test and still have recent releases of Kubernetes running
    in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid tight coupling with the API. This doesn’t apply to platform services that
    run in your cluster. Those, by their nature, need to integrate closely with Kubernetes.
    But keep your end user, production workloads as platform-agnostic as possible.
    Don’t have the Kubernetes API as a dependency. For example, your application should
    know nothing of Kubernetes Secrets. It should simply consume an environment variable
    or read a file that is exposed to it. That way, as long as the manifests used
    to deploy your app are upgraded, the application workload itself will continue
    to run happily, regardless of API changes. If you find that you want to leverage
    Kubernetes features in your workloads, consider using a Kubernetes operator. An
    operator outage should not affect the availability of your application. An operator
    outage will be an urgent problem to fix, but it will not be one your customers
    or end users should see, which is a world of difference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The node replacement option can be very beneficial when you build machine images
    ahead of time that are well tested and verified. Then you can bring up new machines
    and readily join them to the cluster. The process will be rapid because all updated
    software, including operating system and packages, are already installed and the
    processes to deploy those new machines can use much the same process as original
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: When replacing nodes for your cluster, start with the control plane. If you’re
    running a dedicated etcd cluster, start there. The persistent data for your cluster
    is critical and must be treated carefully. If you encounter a problem upgrading
    your first etcd node, if you are properly prepared, it will be relatively trivial
    to abort the upgrade. If you upgrade all your worker nodes and the Kubernetes
    control plane, then find yourself with issues upgrading etcd, you are in a situation
    where rolling back the entire upgrade is not practical—you need to remedy the
    live problem as a priority. You have lost the opportunity to abort the entire
    process, regroup, retest, and resume later. You need to solve that problem or
    at the very least diligently ensure that you can leave the existing versions as-is
    safely for a time.
  prefs: []
  type: TYPE_NORMAL
- en: For a dedicated etcd cluster, consider replacing nodes subtractively; i.e.,
    remove a node and then add in the upgraded replacement, as opposed to first adding
    a node to the cluster and then removing the old. This method gives you the opportunity
    to leave the member list for each etcd node unchanged. Adding a fourth member
    to a three-node etcd cluster, for example, will require an update to all etcd
    nodes’ member list, which will require a restart. It will be far less disruptive
    to drop a member and replace it with a new one that has the same IP address as
    the old, if possible. The etcd documentation on upgrades is excellent and may
    lead you to consider doing in-place upgrades for etcd. This will necessitate in-place
    upgrades to OS and packages on the machine as applicable, but this will often
    be quite palatable and perfectly safe.
  prefs: []
  type: TYPE_NORMAL
- en: For the control plane nodes, they can be replaced additively. Using `kubeadm
    join` with the `--control-plane` flag on new machines that have the upgraded Kubernetes
    binaries—kubeadm, kubectl, kubelet—installed. As each of the control plane nodes
    comes online and is confirmed operational, one old-versioned node can be drained
    and then deleted. If you are running etcd colocated on the control plane nodes,
    include etcd checks when confirming operationality and etcdctl to manage the members
    of the cluster as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Then you can proceed to replace the worker nodes. These can be done additively
    or subtractively—one at a time or several at a time. A primary concern here is
    cluster utilization. If your cluster is highly utilized, you will want to add
    new worker nodes before draining and removing existing nodes to ensure you have
    sufficient compute resources for the displaced Pods. Again, a good pattern is
    to use machine images that have all the updated software installed that are brought
    online and use `kubeadm join` to be added to the cluster. And, again, this could
    be implemented using many of the same mechanisms as used in cluster deployment.
    [Figure 2-7](#performing_upgrades_by_replacing_nodes_in_a_cluster) illustrates
    this operation of replacing control plane nodes one at a time and worker nodes
    in batches.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0207](assets/prku_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Performing upgrades by replacing nodes in a cluster.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In-place upgrades
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In-place upgrades are appropriate in resource-constrained environments where
    replacing nodes is not practical. The rollback path is more difficult and, hence,
    the risk is higher. But this can and should be mitigated with comprehensive testing.
    Keep in mind as well that Kubernetes in production configurations is a highly
    available system. If in-place upgrades are done one node at a time, the risk is
    reduced. So, if using a config management tool such as Ansible to execute the
    steps of this upgrade operation, resist the temptation to hit all nodes at once
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: For etcd nodes, following the documentation for that project, you will simply
    take each node offline, one at a time, performing the upgrade for OS, etcd, and
    other packages, and then bringing it back online. If running etcd in a container,
    consider pre-pulling the image in question prior to bringing the member offline
    to minimize downtime.
  prefs: []
  type: TYPE_NORMAL
- en: For the Kubernetes control plane and worker nodes, if kubeadm was used for initializing
    the cluster, that tool should also be used for upgrades. The upstream docs have
    detailed instructions on how to perform this process for each minor version upgrade
    from 1.13 forward. At the risk of sounding like a broken record, as always, plan
    for failure, automate as much as possible, and test extensively.
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to end of upgrade options. Now, let’s circle back around to the
    beginning of the story—what mechanisms you use to trigger these cluster provisioning
    and upgrade options. We’re tackling this topic last because it requires the context
    of everything we’ve covered so far in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering Mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve looked at all the concerns to solve for in your Kubernetes deployment
    model, it’s useful to consider the triggering mechanisms that fire off the automation
    for installation and management, whatever form that takes. Whether using a Kubernetes
    managed service, a prebuilt installer, or your own custom automation built from
    the ground up, how you fire off cluster builds, cluster scaling, and cluster upgrades
    is important.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes installers generally have a CLI tool that can be used to initiate
    the installation process. However, using that tool in isolation leaves you without
    a single source of truth or cluster inventory record. Managing your cluster inventory
    is difficult when you don’t have a list of that inventory.
  prefs: []
  type: TYPE_NORMAL
- en: A GitOps approach has become popular in recent years. In this case the source
    of truth is a code repository that contains the configurations for the clusters
    under management. When configurations for a new cluster are committed, automation
    is triggered to provision a new cluster. When existing configurations are updated,
    automation is triggered to update the cluster, perhaps to scale the number of
    worker nodes or perform an upgrade of Kubernetes and the cluster add-ons.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach that is more Kubernetes-native is to represent clusters and
    their dependencies in Kubernetes custom resources and then use Kubernetes operators
    to respond to the declared state in those custom resources by provisioning clusters.
    This is the approach taken by projects like Cluster API. The sources of truth
    in this case are the Kubernetes resources stored in etcd in the management cluster.
    However, multiple management clusters for different regions or tiers are commonly
    employed. Here, the GitOps approach can be used in conjunction whereby the cluster
    resource manifests are stored in source control and the pipeline submits the manifests
    to the appropriate management cluster. In this way, you get the best of both the
    GitOps and Kubernetes-native worlds.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing a deployment model for Kubernetes, consider carefully what managed
    services or existing Kubernetes installers (free and licensed) you may leverage.
    Keep automation as a guiding principle for all the systems you build. Wrap your
    wits around all the architecture and topology concerns, particularly if you have
    uncommon requirements that need to be met. Think through the infrastructure dependencies
    and how to integrate them into your deployment process. Consider carefully how
    to manage the machines that will comprise your clusters. Understand the containerized
    components that will form the control plane of your cluster. Develop consistent
    patterns for installing the cluster add-ons that will provide the essential features
    of your app platform. Version your platform and get your day-2 management and
    upgrade paths in place before you put production workloads on your clusters.
  prefs: []
  type: TYPE_NORMAL
