- en: Chapter 6\. Service Routing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章 服务路由
- en: 'Service routing is a crucial capability of a Kubernetes-based platform. While
    the container networking layer takes care of the low-level primitives that connect
    Pods, developers need higher-level mechanisms to interconnect services (i.e.,
    east-west service routing) and to expose applications to their clients (i.e.,
    north-south service routing). Service routing encompasses three concerns that
    provide such mechanisms: Services, Ingress, and service mesh.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 服务路由是基于 Kubernetes 的平台的关键能力。虽然容器网络层负责连接 Pod 的底层原语，但开发人员需要更高级的机制来互连服务（即东西向服务路由），以及将应用程序暴露给其客户端（即南北向服务路由）。服务路由涵盖了三个提供这种机制的关注点：Services、Ingress
    和服务网格。
- en: Services provide a way to treat a set of Pods as a single unit or network service.
    They provide load balancing and routing features that enable horizontal scaling
    of applications across the cluster. Furthermore, Services offer service discovery
    mechanisms that applications can use to discover and interact with their dependencies.
    Finally, Services also provide layer 3/4 mechanisms to expose workloads to network
    clients outside of the cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Services 提供了一种将一组 Pod 视为单个单元或网络服务的方式。它们提供了负载均衡和路由功能，支持应用程序在集群中的水平扩展。此外，Services
    还提供了应用程序可以用来发现和与它们的依赖关系交互的服务发现机制。最后，Services 还提供第 3/4 层机制，将工作负载暴露给集群外的网络客户端。
- en: Ingress handles north-south routing in the cluster. It serves as an entry point
    into workloads running in the cluster, mainly HTTP and HTTPS services. Ingress
    provides layer 7 load balancing capabilities that enable more granular traffic
    routing than Services. The load balancing of traffic is handled by an Ingress
    controller, which must be installed in the cluster. Ingress controllers leverage
    proxy technologies such as Envoy, NGINX, or HAProxy. The controller gets the Ingress
    configuration from the Kubernetes API and configures the proxy accordingly.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 处理集群中的南北向路由。它充当进入集群中运行的工作负载的入口，主要是 HTTP 和 HTTPS 服务。Ingress 提供第 7 层负载均衡功能，使得比
    Services 更精细的流量路由成为可能。流量的负载均衡由 Ingress 控制器处理，该控制器必须安装在集群中。Ingress 控制器利用诸如 Envoy、NGINX
    或 HAProxy 等代理技术。控制器从 Kubernetes API 获取 Ingress 配置，并相应地配置代理。
- en: A service mesh is a service routing layer that provides advanced routing, security,
    and observability features. It is mainly concerned with east-west service routing,
    but some implementations can also handle north-south routing. Services in the
    mesh communicate with each other through proxies that augment the connection.
    The use of proxies makes meshes compelling, as they enhance workloads without
    changes to source code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是一个提供高级路由、安全性和可观察性功能的服务路由层。它主要关注东西向服务路由，但某些实现也可以处理南北向路由。网格中的服务通过增强连接的代理相互通信。代理的使用使网格具有吸引力，因为它们可以增强工作负载而无需更改源代码。
- en: This chapter digs into these service routing capabilities, which are critical
    in production Kubernetes platforms. First, we will discuss Services, the different
    Service types, and how they are implemented. Next, we will explore Ingress, Ingress
    controllers, and the different considerations to take into account when running
    Ingress in production. Finally, we will cover service meshes, how they work on
    Kubernetes, and considerations to make when adopting a service mesh in a production
    platform.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了在生产 Kubernetes 平台中至关重要的服务路由能力。首先，我们将讨论服务（Services）、不同的服务类型以及它们的实现方式。接下来，我们将探索入口（Ingress）、Ingress
    控制器以及在生产环境中运行 Ingress 需要考虑的不同因素。最后，我们将介绍服务网格（service mesh）、它们在 Kubernetes 上的工作原理以及在生产平台中采用服务网格时需要考虑的因素。
- en: Kubernetes Services
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 服务
- en: The Kubernetes Service is foundational to service routing. The Service is a
    network abstraction that provides basic load balancing across several Pods. In
    most cases, workloads running in the cluster use Services to communicate with
    each other. Using Services instead of Pod IPs is preferred because of the fungible
    nature of Pods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的 Service 在服务路由中具有基础性作用。Service 是一个网络抽象层，提供跨多个 Pod 的基本负载均衡。在大多数情况下，集群中运行的工作负载使用
    Services 来相互通信。由于 Pod 的可替代性质，使用 Services 而不是 Pod IP 更为推荐。
- en: In this section, we will review Kubernetes Services and the different Service
    types. We will also look at Endpoints, another Kubernetes resource that is intimately
    related to Services. We will then dive into the Service implementation details
    and discuss kube-proxy. Finally, we will discuss Service Discovery and considerations
    to make for the in-cluster DNS server.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将审查 Kubernetes 服务及其不同的服务类型。我们还将查看 Endpoints，这是与服务密切相关的另一个 Kubernetes
    资源。然后，我们将深入探讨服务的实现细节并讨论 kube-proxy。最后，我们将讨论服务发现以及在集群内 DNS 服务器的考虑事项。
- en: The Service Abstraction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务抽象
- en: The Service is a core API resource in Kubernetes that load balances traffic
    across multiple Pods. The Service does load balancing at the L3/L4 layers in the
    OSI model. It takes a packet with a destination IP and port and forwards it to
    a backend Pod.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是 Kubernetes 中的核心 API 资源，用于在多个 Pod 之间负载均衡流量。服务在 OSI 模型的 L3/L4 层进行负载均衡。它接收带有目标
    IP 和端口的数据包，并将其转发到后端 Pod。
- en: Load balancers typically have a frontend and a backend pool. Services do as
    well. The frontend of a Service is the ClusterIP. The ClusterIP is a virtual IP
    address (VIP) that is accessible from within the cluster. Workloads use this VIP
    to communicate with the Service. The backend pool is a collection of Pods that
    satisfy the Service’s Pod selector. These Pods receive the traffic destined for
    the Cluster IP. [Figure 6-1](#the_service_has_a_frontend_and_a_backend_pool) depicts
    the frontend of a Service and its backend pool.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器通常具有前端和后端池。服务也是如此。服务的前端是 ClusterIP。ClusterIP 是集群内可访问的虚拟 IP 地址（VIP）。工作负载使用此
    VIP 与服务进行通信。后端池是满足服务 Pod 选择器的一组 Pod。这些 Pod 接收发送到 Cluster IP 的流量。[图 6-1](#the_service_has_a_frontend_and_a_backend_pool)
    描述了服务的前端及其后端池。
- en: '![prku 0601](assets/prku_0601.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0601](assets/prku_0601.png)'
- en: Figure 6-1\. The Service has a frontend and a backend pool. The frontend is
    the ClusterIP, while the backend is a set of Pods.
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 服务具有前端和后端池。前端是 ClusterIP，而后端是一组 Pod。
- en: Service IP Address Management
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务 IP 地址管理
- en: As we discussed in the previous chapter, you configure two ranges of IP addresses
    when deploying Kubernetes. On the one hand, the Pod IP range or CIDR block provides
    IP addresses to each Pod in the cluster. On the other hand, the Service CIDR block
    provides the IP addresses for Services in the cluster. This CIDR is the range
    that Kubernetes uses to assign ClusterIPs to Services.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章讨论的那样，在部署 Kubernetes 时，您配置了两个 IP 地址范围。一方面，Pod IP 范围或 CIDR 块为集群中的每个 Pod
    提供 IP 地址。另一方面，服务 CIDR 块为集群中的服务提供 IP 地址。这个 CIDR 是 Kubernetes 用来为服务分配 ClusterIP
    的范围。
- en: The API server handles the IP Address Management (IPAM) for Kubernetes Services.
    When you create a Service, the API Server (with the help of etcd) allocates an
    IP address from the Service CIDR block and writes it to the Service’s ClusterIP
    field.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: API 服务器处理 Kubernetes 服务的 IP 地址管理（IPAM）。创建服务时，API 服务器（借助 etcd 的帮助）从服务 CIDR 块中分配一个
    IP 地址，并将其写入服务的 ClusterIP 字段。
- en: When creating Services, you can also specify the ClusterIP in the Service specification.
    In this case, the API Server makes sure that the requested IP address is available
    and within the Services CIDR block. With that said, explicitly setting ClusterIPs
    is an antipattern.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务时，您还可以在服务规范中指定 ClusterIP。在这种情况下，API 服务器确保所请求的 IP 地址可用，并在服务 CIDR 块内。话虽如此，显式设置
    ClusterIP 是一种反模式。
- en: The Service resource
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Service 资源
- en: The Service resource contains the configuration of a given Service, including
    the name, type, ports, etc. [Example 6-1](#service_definitions_that_exposes_nginz_on_a_clusterip)
    is an example Service definition in its YAML representation named `nginx`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 服务资源包含给定服务的配置，包括名称、类型、端口等。[示例 6-1](#service_definitions_that_exposes_nginz_on_a_clusterip)
    是其 YAML 表示中命名为 `nginx` 的示例服务定义。
- en: Example 6-1\. Service definition that exposes NGINX on a ClusterIP
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 在 ClusterIP 上公开 NGINX 的服务定义
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_service_routing_CO1-1)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO1-1)'
- en: The Pod selector. Kubernetes uses this selector to find the Pods that belong
    to this Service.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 选择器。Kubernetes 使用此选择器查找属于此服务的 Pod。
- en: '[![2](assets/2.png)](#co_service_routing_CO1-2)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO1-2)'
- en: Ports that are accessible through the Service.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可通过服务访问的端口。
- en: '[![3](assets/3.png)](#co_service_routing_CO1-3)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_service_routing_CO1-3)'
- en: Kubernetes supports TCP, UDP, and SCTP protocols in Services.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 支持服务中的 TCP、UDP 和 SCTP 协议。
- en: '[![4](assets/4.png)](#co_service_routing_CO1-4)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_service_routing_CO1-4)'
- en: Port where the Service can be reached.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以访问服务的端口。
- en: '[![5](assets/5.png)](#co_service_routing_CO1-5)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_service_routing_CO1-5)'
- en: Port where the backend Pod is listening, which can be different than the port
    exposed by the Service (the `port` field above).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 后端Pod正在侦听的端口，这可以不同于服务暴露的端口（上面的`port`字段）。
- en: '[![6](assets/6.png)](#co_service_routing_CO1-6)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_service_routing_CO1-6)'
- en: Cluster IP that Kubernetes allocated for this Service.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为该服务分配的ClusterIP。
- en: The Service’s Pod selector determines the Pods that belong to the Service. The
    Pod selector is a collection of key/value pairs that Kubernetes evaluates against
    Pods in the same Namespace as the Service. If a Pod has the same key/value pairs
    in their labels, Kubernetes adds the Pod’s IP address to the backend pool of the
    Service. The management of the backend pool is handled by the Endpoints controller
    through Endpoints resources. We will discuss Endpoints in more detail later in
    this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的Pod选择器确定属于该服务的Pod。Pod选择器是一组键/值对，Kubernetes会将其与同一Namespace中的Pod进行匹配。如果一个Pod的标签中有相同的键/值对，Kubernetes会将该Pod的IP地址添加到服务的后端池中。后端池的管理由Endpoints控制器通过Endpoints资源处理。我们稍后在本章节会详细讨论Endpoints。
- en: Service types
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务类型
- en: Up to this point, we have mainly talked about the ClusterIP Service, which is
    the default Service type. Kubernetes offers multiple Service types that offer
    additional features besides the Cluster IP. In this section, we will discuss each
    Service type and how they are useful.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要讨论了默认的ClusterIP服务类型。Kubernetes还提供了多种服务类型，除了Cluster IP外还提供了其他附加功能。在本节中，我们将讨论每种服务类型及其用途。
- en: ClusterIP
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ClusterIP
- en: We have already discussed this Service type in the previous sections. To recap,
    the ClusterIP Service creates a virtual IP address (VIP) that is backed by one
    or more Pods. Usually, the VIP is available only to workloads running inside the
    cluster. [Figure 6-2](#the_clusterip_service_is_a_vip_that_is_accessible_to_workloads_running_within_the_cluster)
    shows a ClusterIP Service.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中已经讨论过这种服务类型。回顾一下，ClusterIP服务创建一个虚拟IP地址（VIP），由一个或多个Pod支持。通常，这个VIP仅对运行在集群内的工作负载可用。[图6-2](#the_clusterip_service_is_a_vip_that_is_accessible_to_workloads_running_within_the_cluster)展示了一个ClusterIP服务。
- en: '![prku 0602](assets/prku_0602.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0602](assets/prku_0602.png)'
- en: Figure 6-2\. The ClusterIP Service is a VIP that is accessible to workloads
    running within the cluster.
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2. ClusterIP服务是一个虚拟IP，可供集群内的工作负载访问。
- en: NodePort
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NodePort
- en: The NodePort Service is useful when you need to expose a Service to network
    clients outside of the cluster, such as existing applications running in VMs or
    users of a web application.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort Service在需要将服务暴露给集群外的网络客户端时非常有用，比如运行在虚拟机中的现有应用程序或Web应用程序的用户。
- en: As the name suggests, the NodePort Service exposes the Service on a port across
    all cluster nodes. The port is assigned randomly from a configurable port range.
    Once assigned, all nodes in the cluster listen for connections on the given port.
    [Figure 6-3](#the_nodeport_service_opens_a_random_port_on_all_cluster_nodes) shows
    a NodePort Service.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，NodePort服务在所有集群节点上的一个端口上暴露服务。端口从可配置的端口范围中随机分配。一旦分配，集群中的所有节点都会监听该端口上的连接。[图6-3](#the_nodeport_service_opens_a_random_port_on_all_cluster_nodes)展示了一个NodePort服务。
- en: '![prku 0603](assets/prku_0603.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0603](assets/prku_0603.png)'
- en: Figure 6-3\. The NodePort Service opens a random port on all cluster nodes.
    Clients outside of the cluster can reach the Service through this port.
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3. NodePort服务在所有集群节点上打开一个随机端口。集群外的客户端可以通过此端口访问服务。
- en: The primary challenge with NodePort Services is that clients need to know the
    Service’s node port number and the IP address of at least one cluster node to
    reach the Service. This is problematic because nodes can fail or be removed from
    the cluster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort服务的主要挑战在于客户端需要知道服务的节点端口号以及至少一个集群节点的IP地址才能访问该服务。这是有问题的，因为节点可能会失败或从集群中移除。
- en: A common way to solve this challenge is to use an external load balancer in
    front of the NodePort Service. With this approach, clients don’t need to know
    the IP addresses of cluster nodes or the Service’s port number. Instead, the load
    balancer functions as the single entry point to the Service.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个挑战的一种常见方法是在NodePort服务前使用外部负载均衡器。通过这种方式，客户端不需要知道集群节点的IP地址或服务的端口号。相反，负载均衡器作为服务的单一入口点。
- en: The downside to this solution is that you need to manage external load balancers
    and update their configuration constantly. Did a developer create a new NodePort
    Service? Create a new load balancer. Did you add a new node to the cluster? Add
    the new node to the backend pool of all load balancers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案的缺点在于，您需要不断管理外部负载均衡器并更新其配置。开发人员创建了新的 NodePort 服务？那就创建一个新的负载均衡器。您在集群中添加了新节点？那就将新节点添加到所有负载均衡器的后端池中。
- en: In most cases, there are better alternatives to using a NodePort Service. The
    LoadBalancer Service, which we’ll discuss next, is one of those options. Ingress
    controllers are another option, which we’ll explore later in this chapter in [“Ingress”](#ingress).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，使用 NodePort 服务都有更好的替代方案。LoadBalancer 服务就是其中之一，我们将在接下来讨论它。另一个选择是 Ingress
    控制器，在本章后面的 [“Ingress”](#ingress) 中我们将进一步探讨。
- en: LoadBalancer
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LoadBalancer
- en: The LoadBalancer Service builds upon the NodePort Service to address some of
    its downsides. At its core, the LoadBalancer Service is a NodePort Service under
    the hood. However, the LoadBalancer Service has additional functionality that
    is satisfied by a controller.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer 服务建立在 NodePort 服务基础之上，以解决部分问题。从本质上讲，LoadBalancer 服务在内部是一个 NodePort
    服务。然而，LoadBalancer 服务具有额外的功能，需要由控制器来满足。
- en: The controller, also known as a cloud provider integration, is responsible for
    automatically gluing the NodePort Service with an external load balancer. In other
    words, the controller takes care of creating, managing, and configuring external
    load balancers in response to the configuration of LoadBalancer Services in the
    cluster. The controller does this by interacting with APIs that provision or configure
    load balancers. This interaction is depicted in [Figure 6-4](#the_loadbalancer_service_leverages_a_cloud_provider_integration).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器，也称为云提供商集成，负责自动将 NodePort 服务与外部负载均衡器连接起来。换句话说，控制器负责根据集群中 LoadBalancer 服务的配置创建、管理和配置外部负载均衡器。控制器通过与提供或配置负载均衡器的
    API 进行交互来实现这一点。此交互在 [Figure 6-4](#the_loadbalancer_service_leverages_a_cloud_provider_integration)
    中有所描述。
- en: '![prku 0604](assets/prku_0604.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0604](assets/prku_0604.png)'
- en: Figure 6-4\. The LoadBalancer Service leverages a cloud provider integration
    to create an external load balancer, which forwards traffic to the node ports.
    At the node level, the Service is the same as a NodePort.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. LoadBalancer 服务利用云提供商集成创建外部负载均衡器，将流量转发到节点端口。在节点级别上，该服务与 NodePort 相同。
- en: Kubernetes has built-in controllers for several cloud providers, including Amazon
    Web Services (AWS), Google Cloud, and Microsoft Azure. These integrated controllers
    are usually called in-tree cloud providers, as they are implemented inside the
    Kubernetes source code tree.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 针对多个云提供商内置了控制器，包括 Amazon Web Services (AWS)、Google Cloud 和 Microsoft
    Azure。这些集成控制器通常被称为内树云提供商，因为它们是在 Kubernetes 源代码树内实现的。
- en: As the Kubernetes project evolved, out-of-tree cloud providers emerged as an
    alternative to in-tree providers. Out-of-tree providers enabled load balancer
    vendors to provide their implementations of the LoadBalancer Service control loop.
    At this time, Kubernetes supports both in-tree and out-of-tree providers. However,
    the community is quickly adopting out-of-tree providers, given that the in-tree
    providers are deprecated.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Kubernetes 项目的发展，出树云提供商作为内树提供商的替代方案出现了。出树提供商使得负载均衡器供应商能够提供其 LoadBalancer
    服务控制循环的实现。目前，Kubernetes 同时支持内树和出树提供商。然而，鉴于内树提供商已被弃用，社区正在迅速采纳出树提供商。
- en: ExternalName
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ExternalName
- en: The ExternalName Service type does not perform any kind of load balancing or
    proxying. Instead, an ExternalName Service is primarily a service discovery construct
    implemented in the cluster’s DNS. An ExternalName Service maps a cluster Service
    to a DNS name. Because there is no load balancing involved, Services of this type
    lack ClusterIPs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName 服务类型不执行任何形式的负载均衡或代理。相反，ExternalName 服务主要是在集群 DNS 中实现的服务发现构造。ExternalName
    服务将集群服务映射到一个 DNS 名称。由于没有涉及负载均衡，这种类型的服务缺乏 ClusterIP。
- en: ExternalName Services are useful in different ways. Piecemeal application migration
    efforts, for example, can benefit from ExternalName Services. If you migrate components
    of an application to Kubernetes while leaving some of its dependencies outside,
    you can use an ExternalName Service as a bridge while you complete the migration.
    Once you migrate the entire application, you can change the Service type to a
    ClusterIP without having to change the application deployment.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName服务在不同场景中非常有用。 例如，分阶段的应用程序迁移工作可以从ExternalName服务中受益。 如果您将应用程序的某些组件迁移到Kubernetes并将一些依赖项保留在外部，则可以在完成迁移时使用ExternalName服务作为桥梁。
    一旦整个应用程序迁移完成，您可以将服务类型更改为ClusterIP，而无需更改应用程序部署。
- en: Even though useful in creative ways, the ExternalName Service is probably the
    least common Service type in use.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在创造性方式中非常有用，但ExternalName 服务可能是使用最少的服务类型之一。
- en: Headless Service
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无头服务
- en: Like the ExternalName Service, the Headless Service type does not allocate a
    ClusterIP or provide any load balancing. The Headless Service merely functions
    as a way to register a Service and its Endpoints in the Kubernetes API and the
    cluster’s DNS server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与ExternalName服务类似，无头服务类型不分配ClusterIP或提供任何负载平衡。 无头服务仅作为注册服务及其端点在Kubernetes API和集群DNS服务器中的一种方式。
- en: Headless Services are useful when applications need to connect to specific replicas
    or Pods of a service. Such applications can use service discovery to find all
    the Pod IPs behind the Service and then establish connections to specific Pods.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序需要连接服务的特定副本或Pod时，无头服务非常有用。 这类应用程序可以使用服务发现找到服务背后所有Pod的IP，然后建立到特定Pod的连接。
- en: Supported communication protocols
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持的通信协议
- en: 'Kubernetes Services support a specific set of protocols: TCP, UDP, and SCTP.
    Each port listed in a Service resource specifies the port number and the protocol.
    Services can expose multiple ports with different protocols. For example, the
    following snippet shows the YAML definition of the `kube-dns` Service. Notice
    how the list of ports includes TCP port 53 and UDP port 53:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务支持一组特定的协议：TCP、UDP和SCTP。 在服务资源中列出的每个端口都指定了端口号和协议。 服务可以使用不同协议公开多个端口。
    例如，以下代码片段显示了kube-dns服务的YAML定义。 请注意，端口列表包括TCP端口53和UDP端口53：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we’ve discussed up to this point, Services load balance traffic across Pods.
    The Service API resource represents the frontend of the load balancer. The backend,
    or the collection of Pods that are behind the load balancer, are tracked by the
    Endpoints resource and controller, which we will discuss next.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们到目前为止讨论的那样，服务负载均衡流量跨越Pod。 服务API资源代表负载均衡器的前端。 后端或在负载均衡器后面的Pod集合由Endpoints资源和控制器跟踪，接下来我们将讨论这些。
- en: Endpoints
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端点
- en: The Endpoints resource is another API object that is involved in the implementation
    of Kubernetes Services. Every Service resource has a sibling Endpoints resource.
    If you recall the load balancer analogy, you can think of the Endpoints object
    as the pool of IP addresses that receive traffic. [Figure 6-5](#relationship_between_the_service_and_the_endpoint_resources)
    shows the relationship between a Service and an Endpoint.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoints资源是参与Kubernetes服务实现的另一个API对象。 每个服务资源都有一个相应的Endpoints资源。 如果您回忆起负载均衡器的类比，可以将Endpoints对象视为接收流量的IP地址池。
    [图 6-5](#relationship_between_the_service_and_the_endpoint_resources) 显示了服务与端点之间的关系。
- en: '![prku 0605](assets/prku_0605.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0605](assets/prku_0605.png)'
- en: Figure 6-5\. Relationship between the Service and the Endpoints resources.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5. 服务和端点资源之间的关系。
- en: The Endpoints resource
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Endpoints资源
- en: 'An example Endpoints resource for the `nginx` Service in [Example 6-1](#service_definitions_that_exposes_nginz_on_a_clusterip)
    might look like this (some extraneous fields have been removed):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[示例 6-1](#service_definitions_that_exposes_nginz_on_a_clusterip) 中nginx服务的Endpoints资源可能如下所示（已删除一些无关字段）：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example, there are two Pods backing the `nginx` Service. Network traffic
    destined to the `nginx` ClusterIP is load balanced across these two Pods. Also
    notice how the port is 8080 and not 80\. This port matches the `targetPort` field
    specified in the Service. It is the port that the backend Pods are listening on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，有两个支持nginx服务的Pod。 发送到nginx ClusterIP的网络流量在这两个Pod之间进行负载平衡。 还要注意端口为8080而不是80。
    此端口与服务中指定的targetPort字段匹配。 这是后端Pod正在侦听的端口。
- en: The Endpoints controller
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端点控制器
- en: An interesting thing about the Endpoints resource is that Kubernetes creates
    it automatically when you create a Service. This is somewhat different from other
    API resources that you usually interact with.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Endpoints 资源的一个有趣之处是，在创建 Service 时，Kubernetes 会自动创建它。这在你通常与之交互的其他 API 资源中略有不同。
- en: The Endpoints controller is responsible for creating and maintaining the Endpoints
    objects. Whenever you create a Service, the Endpoints controller creates the sibling
    Endpoints resource. More importantly, it also updates the list of IPs within the
    Endpoints object as necessary.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoints 控制器负责创建和维护 Endpoints 对象。每当创建一个 Service 时，Endpoints 控制器都会创建相应的 Endpoints
    资源。更重要的是，它还根据需要更新 Endpoints 对象中的 IP 列表。
- en: The controller uses the Service’s Pod selector to find the Pods that belong
    to the Service. Once it has the set of Pods, the controller grabs the Pod IP addresses
    and updates the Endpoints resource accordingly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器使用 Service 的 Pod 选择器来查找属于该 Service 的 Pod。一旦获取了一组 Pod，控制器就会获取 Pod 的 IP 地址，并相应地更新
    Endpoints 资源。
- en: 'Addresses in the Endpoints resource can be in one of two sets: (ready) addresses
    and notReadyAddresses. The Endpoints controller determines whether an address
    is ready by inspecting the corresponding Pod’s Ready condition. The Pod’s Ready
    condition, in turn, depends on multiple factors. One of them, for example, is
    whether the Pod was scheduled. If the Pod is pending (not scheduled), its Ready
    condition is false. Ultimately, a Pod is considered ready when it is running and
    passing its readiness probe.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoints 资源中的地址可以分为两组：(ready) addresses 和 notReadyAddresses。Endpoints 控制器通过检查对应
    Pod 的 Ready 条件来确定地址是否准备就绪。而 Pod 的 Ready 条件又取决于多个因素。例如，其中一个因素是 Pod 是否已调度。如果 Pod
    处于挂起状态（未调度），其 Ready 条件为 false。最终，只有在 Pod 正在运行并通过其就绪探针时，它才被认为是准备就绪的。
- en: Pod readiness and readiness probes
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod 的就绪状态和就绪探针
- en: In the previous section, we discussed how the Endpoints controller determines
    whether a Pod IP address is ready to accept traffic. But how does Kubernetes tell
    whether a Pod is ready or not?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了 Endpoints 控制器如何确定 Pod IP 地址是否准备好接收流量。但 Kubernetes 如何知道 Pod 是否准备好？
- en: 'There are two complementary methods that Kubernetes uses to determine Pod readiness:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用两种互补的方法来确定 Pod 的就绪状态：
- en: Platform information
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 平台信息
- en: Kubernetes has information about the workloads under its management. For example,
    the system knows whether the Pod has been successfully scheduled to a node. It
    also knows whether the Pod’s containers are up and running.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 管理的工作负载信息丰富。例如，系统知道 Pod 是否成功调度到节点上。它还知道 Pod 的容器是否正在运行。
- en: Readiness probes
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探针
- en: Developers can configure readiness probes on their workloads. When set, the
    kubelet probes the workload periodically to determine if it is ready to receive
    traffic. Probing Pods for readiness is more powerful than determining readiness
    based on platform information because the probe checks for application-specific
    concerns. For example, the probe can check whether the application’s internal
    initialization process has completed.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以在其工作负载上配置就绪探针。设置后，kubelet 定期探测工作负载，以确定其是否准备好接收流量。相比基于平台信息确定就绪状态，通过探测 Pod
    就绪更为强大，因为探针可以检查应用程序特定的问题。例如，探针可以检查应用程序的内部初始化过程是否已完成。
- en: Readiness probes are essential. Without them, the cluster would route traffic
    to workloads that might not be able to handle it, which would result in application
    errors and irritated end users. Ensure that you always define readiness probes
    in the applications you deploy to Kubernetes. In [Chapter 14](ch14.html#application_considerations_chapter),
    we will further discuss readiness probes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探针至关重要。没有它们，集群可能会将流量路由到可能无法处理的工作负载，导致应用程序错误和终端用户的不满。确保您在部署到 Kubernetes 的应用程序中始终定义就绪探针。在[第
    14 章](ch14.html#application_considerations_chapter)中，我们将进一步讨论就绪探针。
- en: The EndpointSlices resource
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EndpointSlices 资源
- en: The EndpointSlices resource is an optimization implemented in Kubernetes v1.16\.
    It addresses scalability issues that can arise with the Endpoints resource in
    [large cluster deployments](https://oreil.ly/H8rHC). Let’s review these issues
    and explore how EndpointSlices help.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: EndpointSlices 资源是 Kubernetes v1.16 中的一项优化。它解决了在[大型集群部署](https://oreil.ly/H8rHC)中可能出现的
    Endpoints 资源的可伸缩性问题。让我们来看看这些问题，并探讨 EndpointSlices 是如何帮助解决的。
- en: To implement Services and make them routable, each node in the cluster watches
    the Endpoints API and subscribes for changes. Whenever an Endpoints resource is
    updated, it must be propagated to all nodes in the cluster to take effect. A scaling
    event is a good example. Whenever there is a change to the set of Pods in the
    Endpoints resource, the API server sends the entire updated object to all the
    cluster nodes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现服务并使其可路由，集群中的每个节点都会监视 Endpoints API 并订阅变化。每当 Endpoints 资源更新时，必须将其传播到集群中的所有节点才能生效。扩展事件是一个很好的例子。每当
    Endpoints 资源中的 Pod 集合发生变化时，API 服务器会将整个更新后的对象发送到所有集群节点。
- en: 'This approach to handling the Endpoints API does not scale well with larger
    clusters for multiple reasons:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多种原因，这种处理 Endpoints API 的方法在较大的集群中效果不佳：
- en: Large clusters contain many nodes. The more nodes in the cluster, the more updates
    need to be sent when Endpoints objects change.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型集群包含许多节点。集群中的节点越多，当 Endpoints 对象发生变化时，需要发送的更新就越多。
- en: The larger the cluster, the more Pods (and Services) you can host. As the number
    of Pods increases, the frequency of Endpoints resource updates also grows.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群越大，您可以托管的 Pod（和服务）就越多。随着 Pod 数量的增加，Endpoints 资源更新的频率也增加。
- en: The size of Endpoints resources increases as the number of Pods that belong
    to the Service grows. Larger Endpoints objects require more network and storage
    resources.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着服务中属于的 Pod 数量增加，Endpoints 资源的大小也会增加。较大的 Endpoints 对象需要更多的网络和存储资源。
- en: The EndpointSlices resource fixes these issues by splitting the set of endpoints
    across multiple resources. Instead of placing all the Pod IP addresses in a single
    Endpoints resource, Kubernetes splits the addresses across various EndpointSlice
    objects. By default, EndpointSlice objects are limited to 100 endpoints.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: EndpointSlices 资源通过将端点集合分割到多个资源中来修复这些问题。Kubernetes 不再将所有 Pod IP 地址放在单个 Endpoints
    资源中，而是将地址分布在各种 EndpointSlice 对象中。默认情况下，EndpointSlice 对象仅限于 100 个端点。
- en: Let’s explore a scenario to better understand the impact of EndpointSlices.
    Consider a Service with 10,000 endpoints, which would result in 100 EndpointSlice
    objects. If one of the endpoints is removed (due to a scale-in event, for example),
    the API server sends the affected EndpointSlice object to each node. Sending a
    single EndpointSlice with 100 endpoints is much more efficient than sending a
    single Endpoints resource with thousands of endpoints.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一个场景，以更好地理解 EndpointSlices 的影响。假设一个具有 10,000 个端点的服务，这将导致 100 个 EndpointSlice
    对象。如果移除其中一个端点（例如由于规模缩小事件），API 服务器会将受影响的 EndpointSlice 对象发送到每个节点。将具有 100 个端点的单个
    EndpointSlice 发送要比将具有数千个端点的单个 Endpoints 资源效率高得多。
- en: To summarize, the EndpointSlices resource improves the scalability of Kubernetes
    by splitting a large number of endpoints into a set of EndpointSlice objects.
    If you are running a platform that has Services with hundreds of endpoints, you
    might benefit from the EndpointSlice improvements. Depending on your Kubernetes
    version, the EndpointSlice functionality is opt-in. If you are running Kubernetes
    v1.18, you must set a feature flag in kube-proxy to enable the use of EndpointSlice
    resources. Starting with Kubernetes v1.19, the EndpointSlice functionality will
    be enabled by default.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，EndpointSlices 资源通过将大量的端点分割为一组 EndpointSlice 对象来改进 Kubernetes 的可扩展性。如果您运行的平台具有数百个端点的服务，您可能会从
    EndpointSlice 的改进中受益。根据您的 Kubernetes 版本，EndpointSlice 功能是可选的。如果您运行的是 Kubernetes
    v1.18，您必须在 kube-proxy 中设置一个功能标志才能启用 EndpointSlice 资源的使用。从 Kubernetes v1.19 开始，默认情况下将启用
    EndpointSlice 功能。
- en: Service Implementation Details
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务实施详细信息
- en: Until now, we’ve talked about Services, Endpoints, and what they provide to
    workloads in a Kubernetes cluster. But how does Kubernetes implement Services?
    How does it all work?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了 Kubernetes 集群中的服务、Endpoints 及其提供的功能。但是 Kubernetes 如何实现服务？它是如何工作的？
- en: In this section, we will discuss the different approaches available when it
    comes to realizing Services in Kubernetes. First, we will talk about the overall
    kube-proxy architecture. Next, we will review the different kube-proxy data plane
    modes. Finally, we will discuss alternatives to kube-proxy, such as CNI plug-ins
    that are capable of taking over kube-proxy’s responsibilities.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在 Kubernetes 中实现服务时可用的不同方法。首先，我们将讨论整体的 kube-proxy 架构。接下来，我们将审查不同的
    kube-proxy 数据平面模式。最后，我们将讨论 kube-proxy 的替代方案，如能够接管 kube-proxy 职责的 CNI 插件。
- en: Kube-proxy
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kube-proxy
- en: Kube-proxy is an agent that runs on every cluster node. It is primarily responsible
    for making Services available to the Pods running on the local node. It achieves
    this by watching the API server for Services and Endpoints and programming the
    Linux networking stack (using iptables, for example) to handle packets accordingly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy 是在每个集群节点上运行的代理程序。它的主要责任是通过监视 API 服务器的 Services 和 Endpoints，并编程 Linux
    网络堆栈（例如使用 iptables）来处理数据包，从而使 Services 对运行在本地节点上的 Pods 可用。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Historically, kube-proxy acted as a network proxy between Pods running on the
    node and Services. This is where the kube-proxy name came from. As the Kubernetes
    project evolved, however, kube-proxy stopped being a proxy and became more of
    a node agent or localized control plane.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在历史上，kube-proxy 充当节点上运行的 Pods 与 Services 之间的网络代理。这也是 kube-proxy 名称的由来。然而，随着
    Kubernetes 项目的发展，kube-proxy 不再充当代理，而是更多地成为节点代理或本地化控制平面。
- en: 'Kube-proxy supports three modes of operation: userspace, iptables, and IPVS.
    The userspace proxy mode is seldom used, since iptables and IPVS are better alternatives.
    Thus, we will only cover the iptables and IPVS modes in the following sections
    of this chapter.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy 支持三种操作模式：userspace、iptables 和 IPVS。由于 iptables 和 IPVS 更优，所以用户空间代理模式很少使用。因此，我们只会在本章的后续部分涵盖
    iptables 和 IPVS 模式。
- en: 'Kube-proxy: iptables mode'
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kube-proxy：iptables 模式
- en: The iptables mode is the default kube-proxy mode at the time of writing (Kubernetes
    v1.18). It is safe to say that the iptables mode is the most prevalent across
    cluster installations today.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时（Kubernetes v1.18），iptables 模式是默认的 kube-proxy 模式。可以肯定地说，iptables 模式是当前集群安装中最普遍的模式。
- en: In the iptables mode, kube-proxy leverages the network address translation (NAT)
    features of iptables.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 iptables 模式下，kube-proxy 利用 iptables 的网络地址转换 (NAT) 功能。
- en: ClusterIP Services
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ClusterIP Services
- en: To realize ClusterIP Services, kube-proxy programs the Linux kernel’s NAT table
    to perform Destination NAT (DNAT) on packets destined for Services. The DNAT rules
    replace the packet’s destination IP address with the IP address of a Service endpoint
    (a Pod IP address). Once replaced, the network handles the packet as if it was
    originally sent to the Pod.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现 ClusterIP Services，kube-proxy 编程 Linux 内核的 NAT 表以对目标为 Services 的数据包执行目标地址
    NAT (DNAT)。DNAT 规则将数据包的目标 IP 地址替换为 Service 端点的 IP 地址（Pod 的 IP 地址）。替换后，网络处理该数据包，就像它最初发送到
    Pod 一样。
- en: 'To load balance traffic across multiple Service endpoints, kube-proxy uses
    multiple iptables chains:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在多个 Service 端点之间负载均衡流量，kube-proxy 使用多个 iptables 链：
- en: Services chain
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Services chain
- en: Top-level chain that contains a rule for each Service. Each rule checks whether
    the destination IP of the packet matches the ClusterIP of the Service. If it does,
    the packet is sent to the Service-specific chain.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 包含每个 Service 规则的顶级链。每个规则检查数据包的目标 IP 是否与 Service 的 ClusterIP 匹配。如果匹配，则将数据包发送到特定于
    Service 的链。
- en: Service-specific chain
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 特定于 Service 的链
- en: Each Service has its iptables chain. This chain contains a rule per Service
    endpoint. Each rule uses the `statistic` iptables extension to select a target
    endpoint randomly. Each endpoint has 1/n probability of being selected, where
    n is the number of endpoints. Once selected, the packet is sent to the Service
    endpoint chain.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Service 都有自己的 iptables 链。此链包含每个 Service 端点的规则。每个规则使用 `statistic` iptables
    扩展随机选择一个目标端点。每个端点被选中的概率为 1/n，其中 n 是端点的数量。一旦选择了端点，数据包就会被发送到 Service 端点链。
- en: Service endpoint chain
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Service 端点链
- en: Each Service endpoint has an iptables chain that performs DNAT on the packet.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Service 端点都有一个执行 DNAT 的 iptables 链。
- en: 'The following listing of iptables rules shows an example of a ClusterIP Service.
    The Service is called `nginx` and has three endpoints (extraneous iptables rules
    have been removed for brevity):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下面列出的 iptables 规则示例显示了一个 ClusterIP Service 的示例。该 Service 名为 `nginx`，有三个端点（为简洁起见，删除了多余的
    iptables 规则）：
- en: '[PRE3]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_service_routing_CO2-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO2-1)'
- en: This is the top-level chain. It has rules for all the Services in the cluster.
    Notice how the `KUBE-SVC-4N57TFCL4MD7ZTDA` rule specifies a destination IP of
    10.97.85.96\. This is the `nginx` Service’s ClusterIP.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是顶级链。它包含集群中所有 Services 的规则。请注意，`KUBE-SVC-4N57TFCL4MD7ZTDA` 规则指定目标 IP 为 10.97.85.96。这是
    `nginx` Service 的 ClusterIP。
- en: '[![2](assets/2.png)](#co_service_routing_CO2-2)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO2-2)'
- en: This is the chain of the `nginx` Service. Notice how there is a rule for each
    Service endpoint with a given probability of matching the rule.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `nginx` Service 的链。请注意，每个 Service 端点都有一个匹配规则的概率。
- en: '[![3](assets/3.png)](#co_service_routing_CO2-3)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_service_routing_CO2-3)'
- en: This chain corresponds to one of the Service endpoints. (SEP stands for Service
    endpoint.) The last rule in this chain is the one that performs DNAT to forward
    the packet to the endpoint (or Pod).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这条链路对应于一个服务端点（SEP代表服务端点）。该链路中的最后一条规则执行DNAT，将数据包转发到端点（或Pod）。
- en: NodePort and LoadBalancer Services
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NodePort和LoadBalancer服务
- en: When it comes to NodePort and LoadBalancer Services, kube-proxy configures iptables
    rules similar to those used for ClusterIP Services. The main difference is that
    the rules match packets based on their destination port number. If they match,
    the rule sends the packet to the Service-specific chain where DNAT happens. The
    snippet below shows the iptables rules for a NodePort Service called `nginx` listening
    on port 31767.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用NodePort和LoadBalancer服务时，kube-proxy配置iptables规则类似于用于ClusterIP服务的规则。主要区别在于规则基于它们的目标端口号匹配数据包。如果匹配，则规则将数据包发送到服务特定的链路，其中进行DNAT。下面的片段显示了NodePort服务`nginx`监听端口31767的iptables规则。
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_service_routing_CO3-1)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO3-1)'
- en: Kube-proxy programs iptables rules for NodePort Services in the `KUBE-NODEPORTS`
    chain.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy为NodePort服务在`KUBE-NODEPORTS`链中编程iptables规则。
- en: '[![2](assets/2.png)](#co_service_routing_CO3-2)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO3-2)'
- en: 'If the packet has `tcp: 31767` as the destination port, it is sent to the Service-specific
    chain. This chain is the Service-specific chain we saw in callout 2 in the previous
    code snippet.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '如果数据包的目标端口是`tcp: 31767`，则将其发送到服务特定的链路。这个链路是我们在前一个代码片段的callout 2中看到的服务特定的链路。'
- en: In addition to programming the iptables rules, kube-proxy opens the port assigned
    to the NodePort Service and holds it open. Holding on to the port has no function
    from a routing perspective. It merely prevents other processes from claiming it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了编程iptables规则外，kube-proxy还打开分配给NodePort服务的端口并保持其开放。从路由的角度来看，保持端口开放没有实际功能，它只是阻止其他进程声明该端口。
- en: 'A key consideration to make when using NodePort and LoadBalancer Services is
    the Service’s external traffic policy setting. The external traffic policy determines
    whether the Service routes external traffic to node-local endpoints (`externalTrafficPolicy:
    Local`) or cluster-wide endpoints (`externalTrafficPolicy: Cluster`). Each policy
    has benefits and trade-offs, as discussed next.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '使用NodePort和LoadBalancer服务时需要考虑的一个关键因素是服务的外部流量策略设置。外部流量策略确定服务是否将外部流量路由到节点本地端点（`externalTrafficPolicy:
    Local`）或整个集群范围的端点（`externalTrafficPolicy: Cluster`）。每种策略都有其优点和权衡，接下来会讨论。'
- en: When the policy is set to `Local`, the Service routes traffic to endpoints (Pods)
    running on the node receiving the traffic. Routing to a local endpoint has two
    important benefits. First, there is no SNAT involved so the source IP is preserved,
    making it available to the workload. And second, there is no additional network
    hop that you would otherwise incur when forwarding traffic to another node. With
    that said, the `Local` policy also has downsides. Mainly, traffic that reaches
    a node that lacks Service endpoints is dropped. For this reason, the `Local` policy
    is usually combined with an external load balancer that health-checks the nodes.
    When the node doesn’t have an endpoint for the Service, the load balancer does
    not send traffic to the node, given that the health check fails. [Figure 6-6](#loadbalancer_service_with_local_external_traffic_policy)
    illustrates this functionality. Another downside of the `Local` policy is the
    potential for unbalanced application load. For example, if a node has three Service
    endpoints, each endpoint receives 33% of the traffic. If another node has a single
    endpoint, it receives 100% of the traffic. This imbalance can be mitigated by
    spreading the Pods with anti-affinity rules or using a DaemonSet to schedule the
    Pods.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当策略设置为`Local`时，服务将流量路由到运行在接收流量的节点上的端点（Pods）。路由到本地端点有两个重要的好处。首先，没有涉及SNAT，因此源IP被保留，可供工作负载使用。其次，在将流量转发到另一个节点时，没有额外的网络跳跃。话虽如此，`Local`策略也有缺点。主要是，到达缺少服务端点的节点的流量会被丢弃。因此，通常将`Local`策略与对节点进行健康检查的外部负载均衡器结合使用。当节点没有服务端点时，负载均衡器不会将流量发送到该节点，因为健康检查失败。图 6-6 (#loadbalancer_service_with_local_external_traffic_policy) 说明了此功能。`Local`策略的另一个缺点是可能导致应用负载不平衡。例如，如果一个节点有三个服务端点，则每个端点接收到33%的流量。如果另一个节点只有一个端点，则它接收到100%的流量。可以通过使用反亲和性规则扩展Pods或使用DaemonSet来调度Pods来减轻这种不平衡。
- en: '![prku 0606](assets/prku_0606.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0606](assets/prku_0606.png)'
- en: Figure 6-6\. LoadBalancer Service with `Local` external traffic policy. The
    external load balancer runs health checks against the nodes. Any node that does
    not have a Service endpoint is removed from the load balancer’s backend pool.
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 使用 `Local` 外部流量策略的负载均衡服务。外部负载均衡器对节点运行健康检查。任何没有服务端点的节点都将从负载均衡器的后端池中移除。
- en: If you have a Service that handles a ton of external traffic, using the `Local`
    external policy is usually the right choice. However, if you do not have a load
    balancer at your disposal, you should use the `Cluster` external traffic policy.
    With this policy, traffic is load balanced across all endpoints in the cluster,
    as shown in [Figure 6-7](#loadbalancer_service_with_cluster_external_traffic_policy).
    As you can imagine, the load balancing results in the loss of the Source IP due
    to SNAT. It can also result in an additional network hop. However, the `Cluster`
    policy does not drop external traffic, regardless of where the endpoint Pods are
    running.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个处理大量外部流量的服务，通常使用 `Local` 外部流量策略是正确的选择。但是，如果没有可用的负载均衡器，应使用 `Cluster` 外部流量策略。使用此策略，流量将在集群中的所有端点间进行负载均衡，如
    [图 6-7](#loadbalancer_service_with_cluster_external_traffic_policy) 所示。可以想象，负载均衡会导致源
    IP 的丢失（由于 SNAT）。它还可能导致额外的网络跳跃。但是，`Cluster` 策略不会丢弃外部流量，无论端点 Pod 运行在何处。
- en: '![prku 0607](assets/prku_0607.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0607](assets/prku_0607.png)'
- en: Figure 6-7\. LoadBalancer Service with `Cluster` external traffic policy. Nodes
    that do not have node-local endpoints forward the traffic to an endpoint on another
    node.
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 使用 `Cluster` 外部流量策略的负载均衡服务。没有本地端点的节点将流量转发到另一节点上的端点。
- en: Connection tracking (conntrack)
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连接跟踪（conntrack）
- en: When the kernel’s networking stack performs DNAT on a packet destined to a Service,
    it adds an entry to the connection tracking (conntrack) table. The table tracks
    the translation performed so that it is applied to any additional packet destined
    to the same Service. The table is also used to remove the NAT from response packets
    before sending them to the source Pod.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当内核的网络堆栈对要发送到服务的数据包执行目的地址转换（DNAT）时，会向连接跟踪（conntrack）表中添加一个条目。该表跟踪执行的转换，以便对发送到同一服务的任何额外数据包应用相同的转换。该表还用于在将响应数据包发送到源
    Pod 之前移除响应数据包中的 NAT。
- en: Each entry in the table maps the pre-NAT protocol, source IP, source port, destination
    IP, and destination port onto the post-NAT protocol, source IP, source port, destination
    IP, and destination port. (Entries include additional information but are not
    relevant in this context.) [Figure 6-8](#connection_tracking_conntrack_table_entry_that_tracks)
    depicts a table entry that tracks the connection from a Pod (`192.168.0.9`) to
    a Service (`10.96.0.14`). Notice how the destination IP and port change after
    the DNAT.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表中的每个条目将预转换协议、源 IP、源端口、目的 IP 和目的端口映射到后转换协议、源 IP、源端口、目的 IP 和目的端口。（条目包含其他信息，但在此上下文中不相关。）[图 6-8](#connection_tracking_conntrack_table_entry_that_tracks)
    描述了一个跟踪从 Pod (`192.168.0.9`) 到服务 (`10.96.0.14`) 的连接的表条目。注意 DNAT 后目的 IP 和端口的变化。
- en: '![prku 0608](assets/prku_0608.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0608](assets/prku_0608.png)'
- en: Figure 6-8\. Connection tracking (conntrack) table entry that tracks the connection
    from a Pod (`192.168.0.9`) to a Service (`10.96.0.14`).
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. 跟踪连接（conntrack）表条目，跟踪来自 Pod (`192.168.0.9`) 到服务 (`10.96.0.14`) 的连接。
- en: Tip
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When the conntrack table fills up, the kernel starts dropping or rejecting connections,
    which can be problematic for some applications. If you are running workloads that
    handle many connections and notice connection issues, you may need to tune the
    maximum size of the conntrack table on your nodes. More importantly, you should
    monitor the conntrack table utilization and alert when the table is close to being
    full.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当连接跟踪表填满时，内核开始丢弃或拒绝连接，这可能对某些应用程序造成问题。如果您正在运行处理大量连接的工作负载，并且注意到连接问题，可能需要调整节点上连接跟踪表的最大大小。更重要的是，应当监视连接跟踪表的使用情况，并在表接近填满时发出警报。
- en: Masquerade
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 掩码（Masquerade）
- en: You may have noticed that we glossed over the `KUBE-MARK-MASQ` iptables rules
    listed in the previous examples. These rules are in place for packets that arrive
    at a node from outside the cluster. To route such packets properly, the Service
    fabric needs to masquerade/source NAT the packets when forwarding them to another
    node. Otherwise, response packets will contain the IP address of the Pod that
    handled the request. The Pod IP in the packet would cause a connection issue,
    as the client initiated the connection to the node and not the Pod.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们在上一个示例中忽略了 `KUBE-MARK-MASQ` iptables 规则。这些规则适用于从集群外部到达节点的数据包。为了正确路由这些数据包，服务
    fabric 需要在将其转发到另一个节点时对其进行伪装/源 NAT。否则，响应数据包将包含处理请求的 Pod 的 IP 地址。数据包中的 Pod IP 将导致连接问题，因为客户端发起连接到节点而不是
    Pod。
- en: Masquerading is also used to egress from the cluster. When Pods connect to external
    services, the source IP must be the IP address of the node where the Pod is running
    instead of the Pod IP. Otherwise, the network would drop response packets because
    they would have the Pod IP as the destination IP address.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Masquerading 也用于从集群中出口流量。当 Pod 连接到外部服务时，源 IP 必须是运行 Pod 的节点的 IP 地址，而不是 Pod 的
    IP 地址。否则，网络会丢弃响应数据包，因为它们的目标 IP 地址将是 Pod 的 IP 地址。
- en: Performance concerns
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能问题
- en: The iptables mode has served and continues to serve Kubernetes clusters well.
    With that said, you should be aware of some performance and scalability limitations,
    as these can arise in large cluster deployments.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: iptables 模式至今已经在 Kubernetes 集群中发挥了重要作用，并继续如此。尽管如此，您应该注意某些性能和可扩展性限制，特别是在大型集群部署中可能会出现这些问题。
- en: Given the structure of the iptables rules and how they work, whenever a Pod
    establishes a new connection to a Service, the initial packet traverses the iptables
    rules until it matches one of them. In the worst-case scenario, the packet needs
    to traverse the entire collection of iptables rules.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 iptables 规则的结构和工作方式，每当 Pod 向 Service 建立新连接时，初始数据包会遍历 iptables 规则，直到匹配其中之一。在最坏的情况下，数据包需要遍历整个
    iptables 规则集。
- en: The iptables mode suffers from O(n) time complexity when it processes packets.
    In other words, the iptables mode scales linearly with the number of Services
    in the cluster. As the number of Services grows, the performance of connecting
    to Services gets worse.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当 iptables 模式处理数据包时，其时间复杂度为 O(n)。换句话说，iptables 模式的性能随着集群中服务数量的增加呈线性增长。随着服务数量的增加，连接到服务的性能变差。
- en: Perhaps more important, updates to the iptables rules also suffer at large scale.
    Because iptables rules are not incremental, kube-proxy needs to write out the
    entire table for every update. In some cases, these updates can even take minutes
    to complete, which risks sending traffic to stale endpoints. Furthermore, kube-proxy
    needs to hold the iptables lock (`/run/xtables.lock`) during these updates, which
    can cause contention with other processes that need to update the iptables rules,
    such as CNI plug-ins.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 或许更重要的是，对 iptables 规则的更新在大规模情况下也会受到影响。因为 iptables 规则不是增量的，kube-proxy 需要为每次更新写入整个表格。在某些情况下，这些更新甚至可能需要几分钟来完成，这会导致将流量发送到过时的端点。此外，kube-proxy
    在这些更新期间需要保持 iptables 锁（`/run/xtables.lock`），这可能会与需要更新 iptables 规则的其他进程（例如 CNI
    插件）发生争用。
- en: Linear scaling is an undesirable quality of any system. With that said, based
    on [tests](https://oreil.ly/YJAu9) performed by the Kubernetes community, you
    should not notice any performance degradation unless you are running clusters
    with tens of thousands of Services. If you are operating at that scale, however,
    you might benefit from the IPVS mode in kube-proxy, which we’ll discuss in the
    following section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 线性扩展是任何系统的不良特性。尽管如此，根据 Kubernetes 社区的 [测试](https://oreil.ly/YJAu9) 结果，除非运行具有成千上万个服务的集群，否则您不应该注意到性能下降。然而，如果您在这种规模下操作，您可能会从
    kube-proxy 的 IPVS 模式中受益，我们将在以下章节讨论。
- en: 'Kube-proxy: IP Virtual Server (IPVS) mode'
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kube-proxy：IP 虚拟服务器（IPVS）模式
- en: IPVS is a load balancing technology built into the Linux kernel. Kubernetes
    added support for IPVS in kube-proxy to address the scalability limitations and
    performance issues of the iptables mode.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: IPVS 是嵌入到 Linux 内核中的负载均衡技术。Kubernetes 在 kube-proxy 中增加了对 IPVS 的支持，以解决 iptables
    模式的可扩展性和性能问题。
- en: As discussed in the previous section, the iptables mode uses iptables rules
    to implement Kubernetes Services. The iptables rules are stored in a list, which
    packets need to traverse in its entirety in the worst-case scenario. IPVS does
    not suffer from this problem because it was originally designed for load balancing
    use cases.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节所讨论的，iptables 模式使用 iptables 规则来实现 Kubernetes 服务。这些 iptables 规则存储在列表中，在最坏的情况下，数据包需要完整遍历这些规则。IPVS
    并不会遇到这个问题，因为它最初是为负载均衡场景设计的。
- en: The IPVS implementation in the Linux kernel uses hash tables to find the destination
    of a packet. Instead of traversing the list of Services when a new connection
    is established, IPVS immediately finds the destination Pod based on the Service
    IP address.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 内核中的 IPVS 实现使用哈希表来查找数据包的目的地。当建立新连接时，IPVS 不会遍历服务列表，而是立即根据服务 IP 地址找到目标 Pod。
- en: Let’s discuss how kube-proxy in IPVS mode handles each of the Kubernetes Service
    types.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论 IPVS 模式下的 kube-proxy 如何处理每种 Kubernetes 服务类型。
- en: ClusterIP Services
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ClusterIP 服务
- en: 'When handling Services that have a ClusterIP, kube-proxy in `ipvs` mode does
    a couple of things. First, it adds the IP address of the ClusterIP Service to
    a dummy network interface on the node called `kube-ipvs0`, as shown in the following
    snippet:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理具有 ClusterIP 的服务时，`ipvs` 模式下的 kube-proxy 会执行一些操作。首先，它将 ClusterIP 服务的 IP 地址添加到节点上称为
    `kube-ipvs0` 的虚拟网络接口中，如以下片段所示：
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After updating the dummy interface, kube-proxy creates an IPVS virtual service
    with the IP address of the ClusterIP Service. Finally, for each Service endpoint,
    it adds an IPVS real server to the IPVS virtual service. The following snippet
    shows the IPVS virtual service and real servers for a ClusterIP Service with three
    endpoints:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 更新虚拟接口后，kube-proxy 使用 ClusterIP 服务的 IP 地址创建一个 IPVS 虚拟服务。最后，对于每个服务端点，它将一个 IPVS
    真实服务器添加到 IPVS 虚拟服务中。以下片段显示了具有三个端点的 ClusterIP 服务的 IPVS 虚拟服务和真实服务器：
- en: '[PRE6]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_service_routing_CO4-1)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO4-1)'
- en: This is the IPVS virtual service. Its IP address is the IP address of the ClusterIP
    Service.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 IPVS 虚拟服务。其 IP 地址是 ClusterIP Service 的 IP 地址。
- en: '[![2](assets/2.png)](#co_service_routing_CO4-2)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO4-2)'
- en: This is one of the IPVS real servers. It corresponds to one of the Service endpoints
    (Pods).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 IPVS 的一个真实服务器之一。它对应于服务端点（Pod）之一。
- en: NodePort and LoadBalancer Services
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NodePort 和 LoadBalancer 服务
- en: 'For NodePort and LoadBalancer Services, kube-proxy creates an IPVS virtual
    service for the Service’s cluster IP. Kube-proxy also creates an IPVS virtual
    service for each of the node’s IP addresses and the loopback address. For example,
    the following snippet shows a listing of the IPVS virtual services created for
    a NodePort Service listening on TCP port 30737:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 NodePort 和 LoadBalancer 服务，kube-proxy 为服务的集群 IP 创建一个 IPVS 虚拟服务。Kube-proxy
    还为每个节点的 IP 地址和回环地址创建一个 IPVS 虚拟服务。例如，以下片段显示了监听 TCP 端口 30737 的 NodePort 服务所创建的 IPVS
    虚拟服务清单：
- en: '[PRE7]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_service_routing_CO5-1)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO5-1)'
- en: IPVS virtual service listening on the node’s IP address.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: IPVS 虚拟服务监听在节点的 IP 地址上。
- en: '[![2](assets/2.png)](#co_service_routing_CO5-2)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO5-2)'
- en: IPVS virtual service listening on the Service’s cluster IP address.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: IPVS 虚拟服务监听在服务的 ClusterIP 地址上。
- en: '[![3](assets/3.png)](#co_service_routing_CO5-3)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_service_routing_CO5-3)'
- en: IPVS virtual service listening on `localhost`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: IPVS 虚拟服务监听在 `localhost` 上。
- en: '[![4](assets/4.png)](#co_service_routing_CO5-4)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_service_routing_CO5-4)'
- en: IPVS virtual service listening on a secondary network interface on the node.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: IPVS 虚拟服务监听在节点的第二网络接口上。
- en: Running without kube-proxy
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在没有 kube-proxy 的情况下运行
- en: Historically, kube-proxy has been a staple in all Kubernetes deployments. It
    is a vital component that makes Kubernetes Services work. As the community evolves,
    however, we could start seeing Kubernetes deployments that do not have kube-proxy
    running. How is this possible? What handles Services instead?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，kube-proxy 是所有 Kubernetes 部署的一个重要组件。它是使 Kubernetes 服务正常工作的关键组成部分。然而，随着社区的发展，我们可能开始看到一些不运行
    kube-proxy 的 Kubernetes 部署。这是如何实现的？谁来处理服务？
- en: With the advent of extended Berkeley Packet Filters (eBPF), CNI plug-ins such
    as [Cilium](https://oreil.ly/sWoh5) and [Calico](https://oreil.ly/0jrKG) can absorb
    kube-proxy’s responsibilities. Instead of handling Services with iptables or IPVS,
    the CNI plug-ins program Services right into the Pod networking data plane. Using
    eBPF improves the performance and scalability of Services in Kubernetes, given
    that the eBPF implementation uses hash tables for endpoint lookups. It also improves
    Service update processing, as it can handle individual Service updates efficiently.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 随着扩展的伯克利数据包过滤器（eBPF）的出现，诸如 [Cilium](https://oreil.ly/sWoh5) 和 [Calico](https://oreil.ly/0jrKG)
    等 CNI 插件可以吸收 kube-proxy 的职责。这些 CNI 插件不再使用 iptables 或 IPVS 处理服务，而是直接将服务编程到 Pod
    网络数据平面中。使用 eBPF 可以提高 Kubernetes 中服务的性能和可扩展性，因为 eBPF 实现使用哈希表进行端点查找。它还改进了服务更新处理，因为它能够高效地处理单个服务的更新。
- en: Removing the need for kube-proxy and optimizing Service routing is a worthy
    feat, especially for those operating at scale. However, it is still early days
    when it comes to running these solutions in production. For example, the Cilium
    implementation requires newer kernel versions to support a kube-proxy-less deployment
    (at the time of writing, the latest Cilium version is `v1.8`). Similarly, the
    Calico team discourages the use of eBPF in production because it is still in tech
    preview. (At the time of writing, the latest calico version is `v3.15.1`.) Over
    time, we expect to see kube-proxy replacements become more common. Cilium even
    supports running its proxy replacement capabilities alongside other CNI plug-ins
    (referred to as [CNI chaining](https://oreil.ly/jZ-2r)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 消除 kube-proxy 的需求并优化服务路由是一个值得称道的成就，尤其是对于大规模操作的用户。然而，在生产环境中运行这些解决方案仍处于早期阶段。例如，Cilium
    实现要求较新的内核版本来支持无 kube-proxy 的部署（在撰写本文时，最新的 Cilium 版本是 `v1.8`）。同样，Calico 团队不鼓励在生产环境中使用
    eBPF，因为它仍处于技术预览阶段（在撰写本文时，最新的 Calico 版本是 `v3.15.1`）。随着时间的推移，我们预计 kube-proxy 的替代方案会变得更加普遍。Cilium
    甚至支持在其他 CNI 插件旁边运行其代理替代功能（称为 [CNI chaining](https://oreil.ly/jZ-2r)）。
- en: Service Discovery
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现
- en: Service discovery provides a mechanism for applications to discover services
    that are available on the network. While not a *routing* concern, service discovery
    is intimately related to Kubernetes Services.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现提供了一种机制，使应用程序能够发现网络上可用的服务。虽然不是 *路由* 关注的内容，但服务发现与 Kubernetes 服务密切相关。
- en: 'Platform teams may wonder whether they need to introduce a dedicated service
    discovery system to a cluster, such as Consul. While possible, it is typically
    not necessary, as Kubernetes offers service discovery to all workloads running
    in the cluster. In this section, we will discuss the different service discovery
    mechanisms available in Kubernetes: DNS-based service discovery, API-based service
    discovery, and environment variable-based service discovery.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 平台团队可能会疑惑是否需要在集群中引入专用的服务发现系统，例如 Consul。虽然这是可能的，但通常是不必要的，因为 Kubernetes 为运行在集群中的所有工作负载提供了服务发现。在本节中，我们将讨论
    Kubernetes 中可用的不同服务发现机制：基于 DNS 的服务发现、基于 API 的服务发现和基于环境变量的服务发现。
- en: Using DNS
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 DNS
- en: Kubernetes provides service discovery over DNS to workloads running inside the
    cluster. Conformant Kubernetes deployments run a DNS server that integrates with
    the Kubernetes API. The most common DNS server in use today is [CoreDNS](https://coredns.io),
    an open source, extensible DNS server.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 为运行在集群内部的工作负载提供了基于 DNS 的服务发现。符合规范的 Kubernetes 部署运行一个与 Kubernetes
    API 集成的 DNS 服务器。今天最常用的 DNS 服务器是 [CoreDNS](https://coredns.io)，这是一个开源的、可扩展的 DNS
    服务器。
- en: 'CoreDNS watches resources in the Kubernetes API server. For each Kubernetes
    Service, CoreDNS creates a DNS record with the following format: `<service-name>.<namespace-name>.svc.cluster.local`.
    For example, a Service called `nginx` in the `default` Namespace gets the DNS
    record `nginx.default.svc.cluster.local`. But how can Pods use these DNS records?'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: CoreDNS 在 Kubernetes API 服务器中监视资源。对于每个 Kubernetes 服务，CoreDNS 创建具有以下格式的 DNS 记录：`<service-name>.<namespace-name>.svc.cluster.local`。例如，名为
    `nginx` 的服务在 `default` 命名空间中将获得 DNS 记录 `nginx.default.svc.cluster.local`。但是 Pod
    如何使用这些 DNS 记录？
- en: 'To enable DNS-based service discovery, Kubernetes configures CoreDNS as the
    DNS resolver for Pods. When setting up a Pod’s sandbox, the kubelet writes an
    */etc/resolv.conf* that specifies CoreDNS as the nameserver and injects the config
    file into the container. The */etc/resolv.conf* file of a Pod looks something
    like this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用基于 DNS 的服务发现，Kubernetes 为 Pod 配置 CoreDNS 作为 DNS 解析器。在设置 Pod 的沙盒时，kubelet
    会将 */etc/resolv.conf* 写入容器中，指定 CoreDNS 作为命名服务器，并将配置文件注入容器中。Pod 的 */etc/resolv.conf*
    文件如下所示：
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Given this configuration, Pods send DNS queries to CoreDNS whenever they try
    to connect to a Service by name.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个配置，Pod 在尝试按名称连接到服务时会向 CoreDNS 发送 DNS 查询。
- en: 'Another interesting trick in the resolver configuration is the use of `ndots`
    and `search` to simplify DNS queries. When a Pod wants to reach a Service that
    exists in the same Namespace, it can use the Service’s name as the domain name
    instead of the fully qualified domain name (`nginx.default.svc.cluster.local`):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器配置中另一个有趣的技巧是使用 `ndots` 和 `search` 简化 DNS 查询。当 Pod 想要连接同一命名空间中存在的服务时，可以将服务的名称用作域名，而不是完全限定域名（`nginx.default.svc.cluster.local`）：
- en: '[PRE9]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, when a Pod wants to reach a Service in another Namespace, it can
    do so by appending the Namespace name to the Service name:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当 Pod 想要连接另一个命名空间中的服务时，可以通过将命名空间名称附加到服务名称来实现：
- en: '[PRE10]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: One thing to consider with the `ndots` configuration is its impact on applications
    that communicate with services outside of the cluster. The `ndots` parameter specifies
    how many dots must appear in a domain name for it to be considered an absolute
    or fully qualified name. When resolving a name that’s not fully qualified, the
    system attempts various lookups using the items in the `search` parameter, as
    seen in the following example. Thus, when applications resolve cluster-external
    names that are not fully qualified, the resolver consults the cluster DNS server
    with multiple futile requests before attempting to resolve the name as an absolute
    name. To avoid this issue, you can use fully qualified domain names in your applications
    by adding a `.` at the end of the name. Alternatively, you can tune the DNS configuration
    of the Pod via the `dnsConfig` field in the Pod’s specification.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 `ndots` 配置对与集群外部服务通信的应用程序的影响。`ndots` 参数指定域名中必须出现多少个点号才能被视为绝对或完全限定名称。当解析非完全限定名称时，系统会使用
    `search` 参数中的条目进行多次查找，如下例所示。因此，当应用程序解析非完全限定的集群外部名称时，解析器会在尝试将名称解析为绝对名称之前向集群 DNS
    服务器发出多个无用的请求。为了避免此问题，可以通过在应用程序中添加 `.` 结尾的完全限定域名来使用完全限定域名。另外，您还可以通过 Pod 规范中的 `dnsConfig`
    字段调整 Pod 的 DNS 配置。
- en: 'The following snippet shows the impact of the `ndots` configuration on Pods
    that resolve external names. Notice how resolving a name that has less dots than
    the configured `ndots` results in multiple DNS queries, while resolving an absolute
    name results in a single query:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段展示了 `ndots` 配置对解析外部名称的 Pod 的影响。请注意，在解析少于配置的 `ndots` 的点数的名称时，会导致多次 DNS
    查询，而解析绝对名称则只产生单个查询：
- en: '[PRE11]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_service_routing_CO6-1)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO6-1)'
- en: Attempt to resolve a name with less than 5 dots (not fully qualified). The resolver
    performs multiple lookups, one per item in the `search` field of */etc/resolv.conf*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试解析少于 5 个点的名称（非完全限定）。解析器执行多次查找，每个查找项在 */etc/resolv.conf* 的 `search` 字段中。
- en: '[![2](assets/2.png)](#co_service_routing_CO6-2)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO6-2)'
- en: Attempt to resolve a fully qualified name. The resolver performs a single lookup.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试解析完全限定名称。解析器执行单次查找。
- en: Overall, service discovery over DNS is extremely useful, as it lowers the barrier
    for applications to interact with Kubernetes Services.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，通过 DNS 进行服务发现非常有用，因为它降低了应用程序与 Kubernetes 服务交互的障碍。
- en: Using the Kubernetes API
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Kubernetes API
- en: Another way to discover Services in Kubernetes is by using the Kubernetes API.
    The community maintains various Kubernetes client libraries in different languages,
    including Go, Java, Python, and others. Some application frameworks, such as Spring,
    also support service discovery through the Kubernetes API.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中发现服务的另一种方法是使用 Kubernetes API。社区维护着不同语言的各种 Kubernetes 客户端库，包括 Go、Java、Python
    等。一些应用程序框架（如Spring）也通过 Kubernetes API 支持服务发现。
- en: Using the Kubernetes API for service discovery can be useful in specific scenarios.
    For example, if your applications need to be aware of Service endpoint changes
    as soon as they happen, they would benefit from watching the API.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定情况下，使用 Kubernetes API 进行服务发现非常有用。例如，如果您的应用程序需要在服务端点更改时立即知道，它们将从监视 API 中受益。
- en: The main downside of performing service discovery through the Kubernetes API
    is that you tightly couple the application to the underlying platform. Ideally,
    applications should be unaware of the platform. If you do choose to use the Kubernetes
    API for service discovery, consider building an interface that abstracts the Kubernetes
    details away from your business logic.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Kubernetes API 执行服务发现的主要缺点是将应用程序紧密耦合到底层平台。理想情况下，应用程序应该不了解平台的存在。如果选择使用 Kubernetes
    API 进行服务发现，请考虑构建一个接口，将 Kubernetes 的详细信息从业务逻辑中抽象出来。
- en: Using environment variables
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用环境变量
- en: 'Kubernetes injects environment variables into Pods to facilitate service discovery.
    For each Service, Kubernetes sets multiple environment variables according to
    the Service definition. The environment variables for an `nginx` ClusterIP Service
    listening on port 80 look as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 将环境变量注入到 Pod 中以促进服务发现。对于每个 Service，Kubernetes 根据 Service 的定义设置多个环境变量。例如，监听端口
    80 的 `nginx` ClusterIP Service 的环境变量如下：
- en: '[PRE12]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The downside to this approach is that environment variables cannot be updated
    without restarting the Pod. Thus, Services must be in place before the Pod starts
    up.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法的缺点是环境变量无法在不重启 Pod 的情况下更新。因此，必须在 Pod 启动之前准备好服务。
- en: DNS Service Performance
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNS 服务性能
- en: As mentioned in the previous section, offering DNS-based service discovery to
    workloads on your platform is crucial. As the size of your cluster and number
    of applications grows, the DNS service can become a bottleneck. In this section,
    we will discuss techniques you can use to provide a performant DNS service.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在您的平台上为工作负载提供基于 DNS 的服务发现非常重要。随着集群规模和应用程序数量的增长，DNS 服务可能成为瓶颈。在本节中，我们将讨论您可以使用的技术，以提供高性能的
    DNS 服务。
- en: DNS cache on each node
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个节点上的 DNS 缓存
- en: The Kubernetes community maintains a DNS cache add-on called [NodeLocal DNSCache](https://oreil.ly/lQdTH).
    The add-on runs a DNS cache on each node to address multiple problems. First,
    the cache reduces the latency of DNS lookups, given that workloads get their answers
    from the local cache (assuming a cache hit) instead of reaching out to the DNS
    server (potentially on another node). Second, the load on the CoreDNS servers
    goes down, as workloads are leveraging the cache most of the time. Finally, in
    the case of a cache miss, the local DNS cache upgrades the DNS query to TCP when
    reaching out to the central DNS service. Using TCP instead of UDP improves the
    reliability of the DNS query.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 社区维护名为 [NodeLocal DNSCache](https://oreil.ly/lQdTH) 的 DNS 缓存插件。该插件在每个节点上运行
    DNS 缓存，以解决多个问题。首先，缓存减少了 DNS 查询的延迟，因为工作负载可以从本地缓存中获取答案（假设命中缓存），而不是向 DNS 服务器请求（可能在另一个节点上）。其次，由于大部分时间工作负载使用缓存，CoreDNS
    服务器的负载减少了。最后，在缓存未命中的情况下，本地 DNS 缓存在向中央 DNS 服务发出 DNS 查询时将其升级为 TCP。使用 TCP 而不是 UDP
    提高了 DNS 查询的可靠性。
- en: The DNS cache runs as a DaemonSet on the cluster. Each replica of the DNS cache
    intercepts the DNS queries that originate from their node. There’s no need to
    change application code or configuration to use the cache. The node-level architecture
    of the NodeLocal DNSCache add-on is depicted in [Figure 6-9](#node_level_architecture_of_the_nodelocal_dnscache_add_on).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 缓存作为 DaemonSet 在集群中运行。每个 DNS 缓存副本拦截其节点上发起的 DNS 查询。无需更改应用程序代码或配置即可使用该缓存。NodeLocal
    DNSCache 插件的节点级架构如 [图 6-9](#node_level_architecture_of_the_nodelocal_dnscache_add_on)
    所示。
- en: '![prku 0609](assets/prku_0609.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0609](assets/prku_0609.png)'
- en: Figure 6-9\. Node-level architecture of the NodeLocal DNSCache add-on. The DNS
    cache intercepts DNS queries and responds immediately if there’s a cache hit.
    In the case of a cache miss, the DNS cache forwards the query to the cluster DNS
    service.
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. NodeLocal DNSCache 插件的节点级架构。DNS 缓存拦截 DNS 查询，并在有缓存命中时立即响应。在缓存未命中的情况下，DNS
    缓存将查询转发给集群 DNS 服务。
- en: Auto-scaling the DNS server deployment
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动扩展 DNS 服务器部署
- en: In addition to running the node-local DNS cache in your cluster, you can automatically
    scale the DNS Deployment according to the size of the cluster. Note that this
    strategy does not leverage the Horizontal Pod Autoscaler. Instead, it uses the
    [cluster Proportional Autoscaler](https://oreil.ly/432we), which scales workloads
    based on the number of nodes in the cluster.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在集群中运行节点本地DNS缓存外，您还可以根据集群的大小自动调整DNS部署。请注意，此策略不利用水平Pod自动缩放器。相反，它使用[集群比例自动缩放器](https://oreil.ly/432we)，根据集群中的节点数量来调整工作负载。
- en: The Cluster Proportional Autoscaler runs as a Pod in the cluster. It has a configuration
    flag to set the workload that needs autoscaling. To autoscale DNS, you must set
    the target flag to the CoreDNS (or kube-dns) Deployment. Once running, the autoscaler
    polls the API server every 10 seconds (by default) to get the number of nodes
    and CPU cores in the cluster. Then, it adjusts the number of replicas in the CoreDNS
    Deployment if necessary. The desired number of replicas is governed by a configurable
    replicas-to-nodes ratio or replicas-to-cores ratio. The ratios to use depend on
    your workloads and how DNS-intensive they are.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 集群比例自动缩放器作为一个Pod在集群中运行。它有一个配置标志来设置需要自动缩放的工作负载。要自动缩放DNS，必须将目标标志设置为CoreDNS（或kube-dns）部署。一旦运行，自动缩放器每10秒（默认）轮询API服务器以获取集群中节点和CPU核心的数量。然后，如果需要，它会调整CoreDNS部署的副本数。所需的副本数由可配置的副本-节点比率或副本-核心比率控制。要使用的比率取决于您的工作负载及其对DNS的需求程度。
- en: In most cases, using node-local DNS cache is sufficient to offer a reliable
    DNS service. However, autoscaling DNS is another strategy you can use when autoscaling
    clusters with a wide-enough range of minimum and maximum nodes.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，使用节点本地DNS缓存足以提供可靠的DNS服务。但是，当自动缩放具有足够范围的最小和最大节点的集群时，自动缩放DNS是另一种可用的策略。
- en: Ingress
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress
- en: As we’ve discussed in [Chapter 5](ch05.html#chapter5), workloads running in
    Kubernetes are typically not accessible from outside the cluster. This is not
    a problem if your applications do not have external clients. Batch workloads are
    a great example of such applications. Realistically, however, most Kubernetes
    deployments host web services that do have end users.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第五章](ch05.html#chapter5)中讨论的那样，运行在Kubernetes中的工作负载通常无法从集群外访问。如果您的应用程序没有外部客户端，则这不是问题。批处理工作负载是这类应用程序的一个很好的例子。然而，现实情况是，大多数Kubernetes部署托管具有端用户的Web服务。
- en: Ingress is an approach to exposing services running in Kubernetes to clients
    outside of the cluster. Even though Kubernetes does not fulfill the Ingress API
    out of the box, it is a staple in any Kubernetes-based platform. It is not uncommon
    for off-the-shelf Kubernetes applications and cluster add-ons to expect that an
    Ingress controller is running in the cluster. Moreover, your developers will need
    it to be able to run their applications successfully in Kubernetes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress是将运行在Kubernetes中的服务暴露给集群外客户端的一种方法。尽管Kubernetes默认不支持Ingress API，但它是任何基于Kubernetes的平台的标配。通常情况下，现成的Kubernetes应用程序和集群附加组件都期望在集群中运行Ingress控制器。此外，您的开发人员需要它来成功地在Kubernetes中运行其应用程序。
- en: This section aims to guide you through the considerations you must make when
    implementing Ingress in your platform. We will review the Ingress API, the most
    common ingress traffic patterns that you will encounter, and the crucial role
    of Ingress controllers in a Kubernetes-based platform. We will also discuss different
    ways to deploy Ingress controllers and their trade-offs. Finally, we will address
    common challenges you can run into and explore helpful integrations with other
    tools in the ecosystem.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在指导您在平台中实现Ingress时必须考虑的因素。我们将审查Ingress API、您将遇到的最常见的Ingress流量模式以及在基于Kubernetes的平台中Ingress控制器的关键角色。我们还将讨论不同的部署Ingress控制器的方法及其权衡。最后，我们将解决您可能遇到的常见挑战，并探讨与生态系统中其他工具的有益集成。
- en: The Case for Ingress
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress的案例
- en: 'Kubernetes Services already provide ways to route traffic to Pods, so why would
    you need an additional strategy to achieve the same thing? As much as we are fans
    of keeping platforms simple, the reality is that Services have important limitations
    and downsides:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务已经提供了将流量路由到Pod的方法，那么为什么还需要额外的策略来实现相同的功能呢？尽管我们喜欢保持平台简单，但事实是服务存在重要的限制和缺点：
- en: Limited routing capabilities
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 有限的路由能力
- en: Services route traffic according to the destination IP and port of incoming
    requests. This can be useful for small and relatively simple applications, but
    it quickly breaks down for more substantial, microservices-based applications.
    These kinds of applications require smarter routing features and other advanced
    capabilities.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 服务根据传入请求的目标 IP 和端口路由流量。这对于小型和相对简单的应用程序可能很有用，但对于更大规模的基于微服务的应用程序，这种方法很快就会失效。这些类型的应用程序需要更智能的路由功能和其他高级能力。
- en: Cost
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: If you are running in a cloud environment, each LoadBalancer Service in your
    cluster creates an external load balancer, such as an ELB in the case of AWS.
    Running a separate load balancer for each Service in your platform can quickly
    become cost-prohibitive.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在云环境中运行，集群中每个 LoadBalancer 服务都会创建一个外部负载均衡器，例如 AWS 中的 ELB。为平台中每个服务运行单独的负载均衡器可能很快变得成本高昂。
- en: Ingress addresses both these limitations. Instead of being limited to load balancing
    at layer 3/4 of the OSI model, Ingress provides load balancing and routing capabilities
    at layer 7\. In other words, Ingress operates at the application layer, which
    results in more advanced routing features.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 解决了这些限制。不再局限于 OSI 模型的第 3/4 层负载均衡，Ingress 在第 7 层提供负载均衡和路由功能。换句话说，Ingress
    在应用层操作，这导致了更先进的路由特性。
- en: Another benefit of Ingress is that it removes the need to have multiple load
    balancers or entry points into the platform. Because of the advanced routing capabilities
    available in Ingress, such as the ability to route HTTP requests based on the
    `Host` header, you can route all the service traffic to a single entry point and
    let the Ingress controller take care of demultiplexing the traffic. This dramatically
    reduces the cost of bringing traffic into your platform.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 的另一个好处是无需多个负载均衡器或平台入口点。由于 Ingress 提供的先进路由能力，例如根据 `Host` 头部路由 HTTP 请求的能力，您可以将所有服务流量路由到单一入口点，并让
    Ingress 控制器处理流量的复用。这极大地降低了将流量引入您的平台的成本。
- en: The ability to have a single ingress point into the platform also reduces the
    complexity of noncloud deployments. Instead of potentially having to manage multiple
    external load balancers with a multitude of NodePort Services, you can operate
    a single external load balancer that routes traffic to the Ingress controllers.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 平台只需一个入口点，也减少了非云部署的复杂性。不再需要管理多个带有大量 NodePort 服务的外部负载均衡器，只需操作一个外部负载均衡器即可将流量路由到
    Ingress 控制器。
- en: Even though Ingress solves most of the downsides related to Kubernetes Services,
    the latter are still needed. Ingress controllers themselves run inside the platform
    and thus need to be exposed to clients that exist outside. And you can use a Service
    (either a NodePort or LoadBalancer) to do so. Besides, most Ingress controllers
    shine when it comes to load balancing HTTP traffic. If you want to be able to
    host applications that use other protocols, you might have to use Services alongside
    Ingress, depending on the capabilities of your Ingress controller.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Ingress 解决了与 Kubernetes 服务相关的大部分问题，但后者仍然是必需的。Ingress 控制器本身运行在平台内部，因此需要暴露给存在于外部的客户端。您可以使用
    Service（无论是 NodePort 还是 LoadBalancer）来实现。此外，大多数 Ingress 控制器在处理 HTTP 流量时表现出色。如果想要能够托管使用其他协议的应用程序，可能需要根据
    Ingress 控制器的功能来决定是否同时使用 Service。
- en: The Ingress API
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress API
- en: The Ingress API enables application teams to expose their services and configure
    request routing according to their needs. Because of Ingress’s focus on HTTP routing,
    the Ingress API resource provides different ways to route traffic according to
    the properties of incoming HTTP requests.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress API 允许应用团队公开其服务，并根据其需求配置请求路由。由于 Ingress 主要关注 HTTP 路由，因此 Ingress API
    资源提供了根据传入 HTTP 请求属性路由流量的不同方法。
- en: 'A common routing technique is routing traffic according to the `Host` header
    of HTTP requests. For example, given the following Ingress configuration, HTTP
    requests with the `Host` header set to `bookhotels.com` are routed to one service,
    while requests destined to `bookflights.com` are routed to another:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的路由技术是根据 HTTP 请求的 `Host` 头部路由流量。例如，根据以下 Ingress 配置，`Host` 头部设置为 `bookhotels.com`
    的 HTTP 请求将路由到一个服务，而目标为 `bookflights.com` 的请求将路由到另一个服务：
- en: '[PRE13]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Hosting applications on specific subdomains of a cluster-wide domain name is
    a common approach we encounter in Kubernetes. In this case, you assign a domain
    name to the platform, and each application gets a subdomain. Keeping with the
    travel theme in the previous example, an example of subdomain-based routing for
    a travel booking application could have `hotels.cluster1.useast.example.com` and
    `flights.cluster1.useast.example.com`. Subdomain-based routing is one of the best
    strategies you can employ. It also enables other interesting use cases, such as
    hosting tenants of a software-as-a-service (SaaS) application on tenant-specific
    domain names (`tenantA.example.com` and `tenantB.example.com`, for example). We
    will further discuss how to implement subdomain-based routing in a later section.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，将应用程序托管在集群范围域名的特定子域名上是一种常见的方法。在这种情况下，你为平台分配一个域名，每个应用程序都有一个子域名。延续前面旅行主题的例子，旅行预订应用程序基于子域名的路由示例可以是
    `hotels.cluster1.useast.example.com` 和 `flights.cluster1.useast.example.com`。基于子域名的路由是你可以采用的最佳策略之一。它还可以支持其他有趣的用例，比如在特定租户域名上托管软件即服务
    (SaaS) 应用程序的租户（例如 `tenantA.example.com` 和 `tenantB.example.com`）。我们将在后面的章节进一步讨论如何实现基于子域名的路由。
- en: The Ingress API supports features beyond host-based routing. Through the evolution
    of the Kubernetes project, Ingress controllers extended the Ingress API. Unfortunately,
    these extensions were made using annotations instead of evolving the Ingress resource.
    The problem with using annotations is that they don’t have a schema. This can
    result in a poor user experience, as there is no way for the API server to catch
    misconfigurations. To address this issue, some Ingress controllers provide Custom
    Resource Definitions (CRDs). These resources have well-defined APIs offering features
    otherwise not available through Ingress. Contour, for example, provides an `HTTPProxy`
    custom resource. While leveraging these CRDs gives you access to a broader array
    of features, you give up the ability to swap Ingress controllers if necessary.
    In other words, you “lock” yourself into a specific controller.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress API 支持超越基于主机的路由的功能。通过 Kubernetes 项目的演进，Ingress 控制器扩展了 Ingress API。不幸的是，这些扩展是通过注解而不是通过演变
    Ingress 资源来完成的。使用注解的问题在于它们没有模式。这可能导致用户体验不佳，因为 API 服务器无法捕捉到配置错误。为了解决这个问题，一些 Ingress
    控制器提供了自定义资源定义 (CRD)。这些资源具有定义良好的 API，提供了通过 Ingress 无法获得的功能。例如，Contour 提供了一个名为 `HTTPProxy`
    的自定义资源。虽然利用这些 CRD 使您能够访问更广泛的功能，但如果需要，您可能会放弃更换 Ingress 控制器的能力。换句话说，您会“锁定”自己到特定的控制器中。
- en: Ingress Controllers and How They Work
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress 控制器及其工作原理
- en: If you can recall the first time you played with Kubernetes, you probably ran
    into a puzzling scenario with Ingress. You downloaded a bunch of sample YAML files
    that included a Deployment and an Ingress and applied them to your cluster. You
    noticed that the Pod came up just fine, but you were not able to reach it. The
    Ingress resource was essentially doing nothing. You probably wondered, What’s
    going on here?
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得第一次与 Kubernetes 玩耍的情景，你可能会遇到 Ingress 的一个令人困惑的场景。你下载了一堆示例 YAML 文件，其中包括一个
    Deployment 和一个 Ingress，并将它们应用到你的集群。你注意到 Pod 很好地启动了，但是却无法访问它。实际上，Ingress 资源什么也没有做。你可能会想，这是怎么回事？
- en: Ingress is one of those APIs in Kubernetes that are left to the platform builder
    to implement. In other words, Kubernetes exposes the Ingress interface and expects
    another component to provide the implementation. This component is commonly called
    an *Ingress* *controller*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 是 Kubernetes 中留给平台构建者实现的 API 之一。换句话说，Kubernetes 暴露了 Ingress 接口，并期望另一个组件提供实现。这个组件通常被称为
    *Ingress* *controller*。
- en: An Ingress controller is a platform component that runs in the cluster. The
    controller is responsible for watching the Ingress API and acting according to
    the configuration defined in Ingress resources. In most implementations, the Ingress
    controller is paired with a reverse proxy, such as NGINX or Envoy. This two-component
    architecture is comparable to other software-defined networking systems, in that
    the controller is the *control plane* of the Ingress controller, while the proxy
    is the *data plane* component. [Figure 6-10](#the_ingress_controller_watches_various_resources_in_the_api)
    shows the control plane and data plane of an Ingress controller.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器是运行在集群中的平台组件。控制器负责监视 Ingress API，并根据 Ingress 资源中定义的配置进行操作。在大多数实现中，Ingress
    控制器与反向代理配对，如 NGINX 或 Envoy。这种两组件架构与其他软件定义网络系统类似，其中控制器是 Ingress 控制器的控制平面组件，而代理是数据平面组件。[图 6-10](#the_ingress_controller_watches_various_resources_in_the_api)
    展示了 Ingress 控制器的控制平面和数据平面。
- en: '![prku 0610](assets/prku_0610.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0610](assets/prku_0610.png)'
- en: Figure 6-10\. The Ingress controller watches various resources in the API server
    and configures the proxy accordingly. The proxy handles incoming traffic and forwards
    it to Pods, according to the Ingress configuration.
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. Ingress 控制器监视 API 服务器中的各种资源，并相应地配置代理。代理处理传入的流量，并根据 Ingress 配置转发到 Pod。
- en: The control plane of the Ingress controller connects to the Kubernetes API and
    watches a variety of resources, such as Ingress, Services, Endpoints, and others.
    Whenever these resources change, the controller receives a watch notification
    and configures the data plane to act according to the desired state declared in
    the Kubernetes API.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器的控制平面连接到 Kubernetes API，并监视各种资源，如 Ingress、Services、Endpoints 等。每当这些资源发生变化时，控制器会收到监视通知，并配置数据平面以依据
    Kubernetes API 中声明的期望状态进行操作。
- en: The data plane handles the routing and load balancing of network traffic. As
    mentioned before, the data plane is usually implemented with an off-the-shelf
    proxy.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面处理网络流量的路由和负载平衡。正如之前提到的，数据平面通常使用现成的代理实现。
- en: Because the Ingress API builds on top of the Service abstraction, Ingress controllers
    have a choice between forwarding traffic through Services or sending it directly
    to Pods. Most Ingress controllers opt for the latter. They don’t use the Service
    resource, other than to validate that the Service referenced in the Ingress resource
    exists. When it comes to routing, most controllers forward traffic to the Pod
    IP addresses listed in the corresponding Endpoints object. Routing traffic directly
    to the Pod bypasses the Service layer, which reduces latency and adds different
    load balancing strategies.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Ingress API 建立在 Service 抽象之上，Ingress 控制器可以选择通过 Services 转发流量，或直接发送到 Pods。大多数
    Ingress 控制器选择后者。它们不使用 Service 资源，仅用于验证 Ingress 资源中引用的 Service 是否存在。在路由方面，大多数控制器将流量转发到对应
    Endpoints 对象中列出的 Pod IP 地址。直接将流量路由到 Pod 可以绕过 Service 层，从而降低延迟并添加不同的负载平衡策略。
- en: Ingress Traffic Patterns
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress 流量模式
- en: A great aspect of Ingress is that each application gets to configure routing
    according to its needs. Typically, each application has different requirements
    when it comes to handling incoming traffic. Some might require TLS termination
    at the edge. Some might want to handle TLS themselves, while others might not
    support TLS at all (hopefully, this is not the case).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 的一个重要方面是每个应用程序都可以根据自己的需求配置路由。通常，每个应用程序在处理传入流量时有不同的需求。有些可能需要在边缘进行 TLS
    终止。有些可能希望自己处理 TLS，而另一些可能根本不支持 TLS（希望不是这种情况）。
- en: In this section, we will explore the common ingress traffic patterns that we
    have encountered. This should give you an idea of what Ingress can provide to
    your developers and how Ingress can fit into your platform offering.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨我们遇到的常见 Ingress 流量模式。这应该让您了解 Ingress 能为开发人员提供什么，并且 Ingress 如何适应您的平台需求。
- en: HTTP proxying
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HTTP 代理
- en: HTTP proxying is the bread-and-butter of Ingress. This pattern involves exposing
    one or more HTTP-based services and routing traffic according to the HTTP requests’
    properties. We have already discussed routing based on the `Host` header. Other
    properties that can influence routing decisions include the URL path, the request
    method, request headers, and more, depending on the Ingress controller.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 代理是 Ingress 的核心功能。该模式涉及暴露一个或多个基于 HTTP 的服务，并根据 HTTP 请求的属性进行流量路由。我们已经讨论了基于
    `Host` 头的路由。其他可能影响路由决策的属性包括 URL 路径、请求方法、请求头等，具体取决于 Ingress 控制器。
- en: The following Ingress resource exposes the `app1` Service at `app1.example.com`.
    Any incoming request that has a matching `Host` HTTP header is sent to an `app1`
    Pod.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 Ingress 资源将 `app1` 服务暴露在 `app1.example.com` 上。任何具有匹配 `Host` HTTP 头的传入请求都将发送到
    `app1` Pod。
- en: '[PRE14]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once applied, the preceding configuration results in the data plane flow depicted
    in [Figure 6-11](#path_of_an_http_request_from_the_client_to_the_target_pod).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 应用以上配置后，数据平面流向如图 [6-11](#path_of_an_http_request_from_the_client_to_the_target_pod)
    所示。
- en: '![prku 0611](assets/prku_0611.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0611](assets/prku_0611.png)'
- en: Figure 6-11\. Path of an HTTP request from the client to the target Pod through
    the Ingress controller.
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-11\. 客户端到目标 Pod 通过 Ingress 控制器的 HTTP 请求路径。
- en: HTTP proxying with TLS
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TLS 的 HTTP 代理
- en: Supporting TLS encryption is table-stakes for Ingress controllers. This ingress
    traffic pattern is the same as HTTP proxying from a routing perspective. However,
    clients communicate with the Ingress controller over a secure TLS connection instead
    of plain-text HTTP.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 支持 TLS 加密对于 Ingress 控制器来说是基本要求。从路由的角度来看，这种 Ingress 流量模式与 HTTP 代理相同。然而，客户端通过安全的
    TLS 连接与 Ingress 控制器通信，而不是明文 HTTP。
- en: The following example shows an Ingress resource that exposes `app1` with TLS.
    The controller gets the TLS serving certificate and key from the referenced Kubernetes
    Secret.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例显示了一个将 `app1` 以 TLS 方式暴露的 Ingress 资源。控制器从引用的 Kubernetes Secret 获取 TLS 服务证书和密钥。
- en: '[PRE15]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Ingress controllers support different configurations when it comes to the connection
    between the Ingress controller and the backend service. The connection between
    the external client and the controller is secure (TLS), while the connection between
    the Ingress controller and the backend application does not have to be. Whether
    the connection between the controller and the backend is secure depends on whether
    the application is listening for TLS connections. By default, most Ingress controllers
    terminate TLS and forward requests over an unencrypted connection, as depicted
    in [Figure 6-12](#Ingress_controller_handling_an_HTTPS).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到 Ingress 控制器与后端服务之间的连接时，Ingress 控制器支持不同的配置。外部客户端与控制器之间的连接是安全的（TLS），而控制器与后端应用程序之间的连接则不一定要安全。控制器与后端之间的连接是否安全取决于应用程序是否监听
    TLS 连接。默认情况下，大多数 Ingress 控制器会终止 TLS 并通过非加密连接转发请求，如图 [6-12](#Ingress_controller_handling_an_HTTPS)
    所示。
- en: '![prku 0612](assets/prku_0612.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0612](assets/prku_0612.png)'
- en: Figure 6-12\. Ingress controller handling an HTTPS request by terminating TLS
    and forwarding the request to the backend Pod over an unencrypted connection.
  id: totrans-272
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-12\. Ingress 控制器通过终止 TLS 处理 HTTPS 请求，并将请求转发到后端 Pod，使用非加密连接。
- en: In the case where a secure connection to the backend is required, the Ingress
    controller terminates the TLS connection at the edge and establishes a new TLS
    connection with the backend (illustrated in [Figure 6-13](#ingress_controller_terminating_tls_and_establishing_a_new_tls)).
    The reestablishment of the TLS connection is sometimes not appropriate for certain
    applications, such as those that need to perform the TLS handshake with their
    clients. In these situations, TLS passthrough, which we will discuss further later,
    is a viable alternative.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要与后端建立安全连接的情况下，Ingress 控制器会在边缘终止 TLS 连接，并与后端建立新的 TLS 连接（如图 [6-13](#ingress_controller_terminating_tls_and_establishing_a_new_tls)
    所示）。重新建立 TLS 连接有时对某些应用程序不合适，例如那些需要与其客户端进行 TLS 握手的应用程序。在这些情况下，我们将在后面进一步讨论 TLS 穿透作为可行的替代方案。
- en: '![prku 0613](assets/prku_0613.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0613](assets/prku_0613.png)'
- en: Figure 6-13\. Ingress controller terminating TLS and establishing a new TLS
    connection with the backend Pod when handling HTTPS requests.
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-13\. Ingress 控制器在处理 HTTPS 请求时终止 TLS 并与后端 Pod 建立新的 TLS 连接。
- en: Layer 3/4 proxying
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 3/4 层代理
- en: Even though the Ingress API’s primary focus is layer 7 proxying (HTTP traffic),
    some Ingress controllers can proxy traffic at layer 3/4 (TCP/UDP traffic). This
    can be useful if you need to expose applications that do not speak HTTP. When
    evaluating Ingress controllers, you must keep this in mind, as support for layer
    3/4 proxying varies across controllers.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Ingress API 的主要重点是第 7 层代理（HTTP 流量），一些 Ingress 控制器可以在第 3/4 层代理（TCP/UDP 流量）中代理流量。如果需要暴露不使用
    HTTP 的应用程序，则这可能很有用。在评估 Ingress 控制器时，您必须牢记这一点，因为各个控制器对第 3/4 层代理的支持程度不同。
- en: The main challenge with proxying TCP or UDP services is that Ingress controllers
    listen on a limited number of ports, usually 80 and 443\. As you can imagine,
    exposing different TCP or UDP services on the same port is impossible without
    a strategy to distinguish the traffic. Ingress controllers solve this problem
    in different ways. Some, such as Contour, support proxying of only TLS encrypted
    TCP connections that use the Server Name Indication (SNI) TLS extension. The reason
    for this is that Contour needs to know where the traffic is headed. And when using
    SNI, the target domain name is available (unencrypted) in the ClientHello message
    of the TLS handshake. Because TLS and SNI are dependent on TCP, Contour does not
    support UDP proxying.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 代理 TCP 或 UDP 服务的主要挑战在于，Ingress 控制器通常仅监听有限数量的端口，通常是 80 和 443。可以想象，如果没有区分流量的策略，将不可能在同一端口上暴露不同的
    TCP 或 UDP 服务。不同的 Ingress 控制器通过不同的方式解决了这个问题。例如，Contour 支持仅代理使用 Server Name Indication（SNI）TLS
    扩展的 TLS 加密 TCP 连接。这样做的原因是 Contour 需要知道流量的目的地。在使用 SNI 时，目标域名在 TLS 握手的 ClientHello
    消息中是可用的（未加密）。由于 TLS 和 SNI 依赖于 TCP，Contour 不支持 UDP 代理。
- en: 'The following is a sample HTTPProxy Custom Resource, which is supported by
    Contour. Layer 3/4 proxying is one of those cases where a Custom Resource provides
    a better experience than the Ingress API:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例 HTTPProxy 自定义资源，Contour 支持此资源。在第 3/4 层代理的常见情况下，自定义资源提供比 Ingress API
    更好的体验：
- en: '[PRE16]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With the preceding configuration, Contour reads the server name in the SNI extension
    and proxies the traffic to the backend TCP service. [Figure 6-14](#the_ingress_controller_inspects_the_sni_header_to_determine_the_backend_terminates_the_tls_connection)
    illustrates this capability.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述配置中，Contour 读取 SNI 扩展中的服务器名称，并将流量代理到后端 TCP 服务。[图 6-14](#the_ingress_controller_inspects_the_sni_header_to_determine_the_backend_terminates_the_tls_connection)
    展示了此功能。
- en: '![prku 0614](assets/prku_0614.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0614](assets/prku_0614.png)'
- en: Figure 6-14\. The Ingress controller inspects the SNI header to determine the
    backend, terminates the TLS connection, and forwards the TCP traffic to the Pod.
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-14\. Ingress 控制器检查 SNI 头以确定后端，终止 TLS 连接，并将 TCP 流量转发到 Pod。
- en: Other Ingress controllers expose configuration parameters that you can use to
    tell the underlying proxy to bind additional ports for layer 3/4 proxying. You
    then map these additional ports to specific services running in the cluster. This
    is the approach that the community-led NGINX Ingress controller takes for layer
    3/4 proxying.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 Ingress 控制器会公开配置参数，您可以使用这些参数告知底层代理为第 3/4 层代理绑定附加端口。然后，您将这些额外的端口映射到集群中运行的特定服务。这是由社区主导的
    NGINX Ingress 控制器在第 3/4 层代理中采用的方法。
- en: A common use case of layer 3/4 proxying is TLS passthrough. TLS passthrough
    involves an application that exposes a TLS endpoint and the need to handle the
    TLS handshake directly with the client. As we discussed in the “HTTP proxying
    with TLS” pattern, the Ingress controller usually terminates the client-facing
    TLS connection. The TLS termination is necessary so that the Ingress controller
    can inspect the HTTP request, which would otherwise be encrypted. However, with
    TLS passthrough, the Ingress controller does not terminate TLS and instead proxies
    the secure connection to a backend Pod. [Figure 6-15](#when_tls_passthrough_is_enabled_the_ingress_controller_inspects)
    depicts TLS passthrough.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3/4 层代理的常见用例是 TLS 穿透。TLS 穿透涉及一个应用程序暴露 TLS 端点，并需要直接与客户端进行 TLS 握手处理。正如我们在“使用
    TLS 进行 HTTP 代理”的模式中讨论的那样，Ingress 控制器通常会终止面向客户端的 TLS 连接。这种 TLS 终止是必需的，以便 Ingress
    控制器可以检查 HTTP 请求，否则该请求将会被加密。然而，使用 TLS 穿透时，Ingress 控制器不会终止 TLS，而是将安全连接代理到后端 Pod。[图 6-15](#when_tls_passthrough_is_enabled_the_ingress_controller_inspects)
    描述了 TLS 穿透。
- en: '![prku 0615](assets/prku_0615.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0615](assets/prku_0615.png)'
- en: Figure 6-15\. When TLS passthrough is enabled, the Ingress controller inspects
    the SNI header to determine the backend and forwards the TLS connection accordingly.
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15。当启用TLS透传时，入口控制器检查SNI头部以确定后端，并相应地转发TLS连接。
- en: Choosing an Ingress Controller
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个入口控制器
- en: There are several Ingress controllers that you can choose from. In our experience,
    the NGINX Ingress controller is one of the most commonly used. However, that does
    not mean it is best for your application platform. Other choices include Contour,
    HAProxy, Traefik, and more. In keeping with this book’s theme, our goal is not
    to tell you which to use. Instead, we aim to equip you with the information you
    need to make this decision. We will also highlight significant trade-offs where
    applicable.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种入口控制器可以选择。根据我们的经验，NGINX入口控制器是最常用的之一。但这并不意味着它对你的应用平台最好。其他选择包括Contour、HAProxy、Traefik等等。与本书主题一致，我们的目标不是告诉你应该使用哪个，而是为你提供做出这个决定所需的信息。我们还将突出显示适用时的重大权衡。
- en: 'Stepping back a bit, the primary goal of an Ingress controller is to handle
    application traffic. Thus, it is natural to turn to the applications as the primary
    factor when selecting an Ingress controller. Specifically, what are the features
    and requirements that your applications need? The following is a list of criteria
    that you can use to evaluate Ingress controllers from an application support perspective:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 退一步来看，入口控制器的主要目标是处理应用程序流量。因此，在选择入口控制器时，自然而然地转向应用程序作为主要因素。具体来说，你的应用程序需要哪些特性和要求？以下是你可以从应用支持的角度评估入口控制器的标准列表：
- en: Do applications expose HTTPS endpoints? Do they need to handle the TLS handshake
    with the client directly, or is it okay to terminate TLS at the edge?
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否公开HTTPS端点？它们是否需要直接与客户端处理TLS握手，或者在边缘终止TLS会合适？
- en: What SSL ciphers do the applications use?
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序使用哪些SSL密码？
- en: Do applications need session affinity or sticky sessions?
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否需要会话亲和性或粘性会话？
- en: Do applications need advanced request routing capabilities, such as HTTP header-based
    routing, cookie-based routing, HTTP method-based routing, and others?
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否需要高级请求路由能力，例如基于HTTP头部的路由、基于Cookie的路由、基于HTTP方法的路由等等？
- en: Do applications have different load balancing algorithm requirements, such as
    round-robin, weighted least request, or random?
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否需要不同的负载均衡算法要求，例如轮询、加权最少请求或随机？
- en: Do applications need support for Cross-Origin Resource Sharing (CORS)?
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否需要支持跨源资源共享（CORS）？
- en: Do applications offload authentication concerns to an external system? Some
    Ingress controllers provide authentication features that you can leverage to provide
    a common authentication mechanism across applications.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否将身份验证问题卸载到外部系统？一些入口控制器提供身份验证功能，你可以利用这些功能提供应用程序之间的通用认证机制。
- en: Are there any applications that need to expose TCP or UDP endpoints?
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有应用程序需要公开TCP或UDP端点？
- en: Does the application need the ability to rate-limit incoming traffic?
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否需要能力来限制传入流量的速率？
- en: In addition to application requirements, another crucial consideration to make
    is your organization’s experience with the data plane technology. If you are already
    intimately familiar with a specific proxy, it is usually a safe bet to start there.
    You will already have a good understanding of how it works, and more importantly,
    you will know its limitations and how to troubleshoot it.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 除了应用程序需求之外，另一个关键考虑因素是你的组织对数据平面技术的经验。如果你已经对特定的代理非常熟悉，通常从那里开始是一个安全的选择。你将已经对它的工作原理有了很好的理解，更重要的是，你将了解它的局限性以及如何进行故障排除。
- en: Supportability is another critical factor to consider. Ingress is an essential
    component of your platform. It exists right in the middle of your customers and
    the services they are trying to reach. When things go wrong with your Ingress
    controller, you want to have access to the support you need when facing an outage.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 支持性是另一个需要考虑的关键因素。入口是你平台的一个重要组成部分。它存在于你的客户和他们试图访问的服务之间的中间。当入口控制器出现问题时，你希望在面对停机时能够获得所需的支持。
- en: Finally, remember that you can run multiple Ingress controllers in your platform
    using Ingress classes. Doing so increases the complexity and management of your
    platform, but it is necessary in some cases. The higher the adoption of your platform
    and the more production workloads you are running, the more features they will
    demand from your Ingress tier. It is entirely possible that you will end up having
    a set of requirements that cannot be fulfilled with a single Ingress controller.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请记住您可以使用 Ingress 类在平台上运行多个 Ingress 控制器。这样做会增加平台的复杂性和管理难度，但在某些情况下是必要的。您的平台越受欢迎，并且运行的生产工作负载越多，他们对您的
    Ingress 层的需求就越多。完全有可能您最终会有一组无法通过单个 Ingress 控制器满足的需求。
- en: Ingress Controller Deployment Considerations
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress 控制器部署注意事项
- en: Regardless of the Ingress controller, there is a set of considerations that
    you should keep in mind when it comes to deploying and operating the Ingress tier.
    Some of these considerations can also have an impact on the applications running
    on the platform.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是哪种 Ingress 控制器，在部署和运行 Ingress 层时都需要考虑一些因素。这些考虑因素中的一些也可能对平台上运行的应用程序产生影响。
- en: Dedicated Ingress nodes
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 专用的 Ingress 节点
- en: Dedicating (or reserving) a set of nodes to run the Ingress controller and thus
    serve as the cluster’s “edge” is a pattern that we have found very successful.
    [Figure 6-16](#dedicated_ingress_nodes_are_reserved_for_the_ingress_controller)
    illustrates this deployment pattern. At first, it might seem wasteful to use dedicated
    ingress nodes. However, our philosophy is, if you can afford to run dedicated
    control plane nodes, you can probably afford to dedicate nodes to the layer that
    is in the critical path for all workloads on the cluster. Using a dedicated node
    pool for Ingress brings considerable benefits.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 将一组节点专门用于运行 Ingress 控制器，并因此作为集群“边缘”的节点，是一种我们发现非常成功的模式。[图 6-16](#dedicated_ingress_nodes_are_reserved_for_the_ingress_controller)展示了这种部署模式。起初，使用专用的
    Ingress 节点可能会显得浪费。然而，我们的理念是，如果您有能力运行专用的控制平面节点，那么您可能也有能力为集群上所有工作负载的关键路径层专用节点。为
    Ingress 使用专用节点池带来了显著的好处。
- en: '![prku 0616](assets/prku_0616.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0616](assets/prku_0616.png)'
- en: Figure 6-16\. Dedicated ingress nodes are reserved for the Ingress controller.
    The ingress nodes serve as the “edge” of the cluster or the Ingress tier.
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-16\. 专用的 Ingress 节点专门为 Ingress 控制器保留。Ingress 节点充当集群的“边缘”或 Ingress 层。
- en: The primary benefit is resource isolation. Even though Kubernetes has support
    for configuring resource requests and limits, we have found that platform teams
    can struggle with getting those parameters right. This is especially true when
    the platform team is at the beginning of their Kubernetes journey and is unaware
    of the implementation details that underpin resource management (e.g., the Completely
    Fair Scheduler, cgroups). Furthermore, at the time of writing, Kubernetes does
    not support resource isolation for network I/O or file descriptors, making it
    challenging to guarantee the fair sharing of these resources.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 主要好处是资源隔离。尽管 Kubernetes 支持配置资源请求和限制，但我们发现平台团队在设置这些参数时可能会遇到困难。特别是当平台团队在他们的 Kubernetes
    之旅的开始阶段，并且不了解支持资源管理的实现细节时（例如，完全公平调度器、cgroups）。此外，在撰写本文时，Kubernetes 不支持网络 I/O 或文件描述符的资源隔离，这使得保证这些资源的公平共享变得具有挑战性。
- en: Another reason for running Ingress controllers on dedicated nodes is compliance.
    We have encountered that a large number of organizations have pre-established
    firewall rules and other security practices that can be incompatible with Ingress
    controllers. Dedicated ingress nodes are useful in these environments, as it is
    typically easier to get exceptions for a subset of cluster nodes instead of all
    of them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在专用节点上运行 Ingress 控制器的另一个原因是合规性。我们发现许多组织拥有预设的防火墙规则和其他安全实践，这些规则可能与 Ingress 控制器不兼容。在这些环境中，专用的
    Ingress 节点非常有用，因为通常更容易为集群节点的一个子集获取异常，而不是全部节点。
- en: Finally, limiting the number of nodes that run the Ingress controller can be
    helpful in bare-metal or on-premises installations. In such deployments, the Ingress
    tier is fronted by a hardware load balancer. In most cases, these are traditional
    load balancers that lack APIs and must be statically configured to route traffic
    to a specific set of backends. Having a small number of ingress nodes eases the
    configuration and management of these external load balancers.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在裸金属或本地安装中限制运行 Ingress 控制器的节点数量可能会有所帮助。在这种部署中，Ingress 层面由硬件负载均衡器作为前端。在大多数情况下，这些是传统的负载均衡器，缺乏
    API，并且必须静态配置以将流量路由到特定的后端集合。少量的 Ingress 节点可以简化这些外部负载均衡器的配置和管理。
- en: Overall, dedicating nodes to Ingress can help with performance, compliance,
    and managing external load balancers. The best approach to implement dedicated
    ingress nodes is to label and taint the ingress nodes. Then, deploy the Ingress
    controller as a DaemonSet that (1) tolerates the taint, and (2) has a node selector
    that targets the ingress nodes. With this approach, ingress node failures must
    be accounted for, as Ingress controllers will not run on nodes other than those
    reserved for Ingress. In the ideal case, failed nodes are automatically replaced
    with new nodes that can continue handling Ingress traffic.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，将节点专用于 Ingress 可以帮助提高性能、合规性和管理外部负载均衡器。实施专用 Ingress 节点的最佳方法是对 Ingress 节点进行标签和污点设置。然后，将
    Ingress 控制器部署为一个 DaemonSet，该 DaemonSet (1) 允许容忍该污点，并且 (2) 具有一个节点选择器，指定目标为 Ingress
    节点。采用这种方法时，必须考虑 Ingress 节点的故障处理，因为 Ingress 控制器不会在未保留给 Ingress 的节点上运行。在理想情况下，故障节点会自动替换为可以继续处理
    Ingress 流量的新节点。
- en: Binding to the host network
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绑定到主机网络
- en: 'To optimize the ingress traffic path, you can bind your Ingress controller
    to the underlying host’s network. By doing so, incoming requests bypass the Kubernetes
    Service fabric and reach the Ingress controller directly. When enabling host networking,
    ensure that the Ingress controller’s DNS policy is set to `ClusterFirstWithHostNet`.
    The following snippet shows the host networking and DNS policy settings in a Pod
    template:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 要优化 Ingress 流量路径，可以将 Ingress 控制器绑定到底层主机的网络上。通过这样做，传入请求会绕过 Kubernetes Service
    体系结构，直接到达 Ingress 控制器。在启用主机网络时，请确保 Ingress 控制器的 DNS 策略设置为 `ClusterFirstWithHostNet`。以下代码片段显示了
    Pod 模板中的主机网络和 DNS 策略设置：
- en: '[PRE17]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: While running the Ingress controller directly on the host network can increase
    performance, you must keep in mind that doing so removes the network namespace
    boundary between the Ingress controller and the node. In other words, the Ingress
    controller has full access to all network interfaces and network services available
    on the host. This has implications on the Ingress controller’s threat model. Namely,
    it lowers the bar for an adversary to perform lateral movement in the case of
    a data plane proxy vulnerability. Additionally, attaching to the host network
    is a privileged operation. Thus, the Ingress controller needs elevated privileges
    or exceptions to run as a privileged workload.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管直接在主机网络上运行 Ingress 控制器可以增加性能，但必须记住，这样做会移除 Ingress 控制器与节点之间的网络命名空间边界。换句话说，Ingress
    控制器可以完全访问主机上所有网络接口和可用的网络服务。这对于 Ingress 控制器的威胁模型有影响。换句话说，它降低了在数据平面代理漏洞案例中对敌对行动进行侧向移动的门槛。此外，绑定到主机网络是一个特权操作。因此，Ingress
    控制器需要提升的特权或例外来作为特权工作负载运行。
- en: Even then, we’ve found that binding to the host network is worth the trade-off
    and is usually the best way to expose the platform’s Ingress controllers. The
    ingress traffic arrives directly at the controller’s gate, instead of traversing
    the Service stack (which can be suboptimal, as discussed in [“Kubernetes Services”](#kubernetes_services)).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，我们发现将 Ingress 控制器绑定到主机网络是值得权衡的，并且通常是暴露平台 Ingress 控制器的最佳方式。Ingress 流量直接到达控制器的网关，而不是经过
    Service 栈（如 [“Kubernetes Services”](#kubernetes_services) 中讨论的可能不太理想）。
- en: Ingress controllers and external traffic policy
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ingress 控制器和外部流量策略
- en: Unless configured properly, using a Kubernetes Service to expose the Ingress
    controller impacts the performance of the Ingress data plane.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未正确配置，使用 Kubernetes Service 来暴露 Ingress 控制器会影响 Ingress 数据平面的性能。
- en: If you recall from [“Kubernetes Services”](#kubernetes_services), a Service’s
    external traffic policy determines how to handle traffic that’s coming from outside
    the cluster. If you are using a NodePort or LoadBalancer Service to expose the
    Ingress controller, ensure that you set the external traffic policy to `Local`.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回顾一下[“Kubernetes Services”](#kubernetes_services)，一个服务的外部流量策略决定如何处理来自集群外部的流量。如果您正在使用
    NodePort 或 LoadBalancer 服务来暴露 Ingress 控制器，请确保将外部流量策略设置为`Local`。
- en: Using the `Local` policy avoids unnecessary network hops, as the external traffic
    reaches the local Ingress controller instead of hopping to another node. Furthermore,
    the `Local` policy doesn’t use SNAT, which means the client IP address is visible
    to applications handling the requests.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Local`策略可以避免不必要的网络跳跃，因为外部流量到达本地 Ingress 控制器而不是跳到另一个节点。此外，`Local`策略不使用 SNAT，这意味着客户端
    IP 地址对处理请求的应用程序是可见的。
- en: Spread Ingress controllers across failure domains
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨失败域分布 Ingress 控制器
- en: To ensure the high-availability of your Ingress controller fleet, use Pod anti-affinity
    rules to spread the Ingress controllers across different failure domains.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保 Ingress 控制器群的高可用性，使用 Pod 反亲和性规则将 Ingress 控制器分布在不同的故障域中。
- en: DNS and Its Role in Ingress
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNS 及其在入口中的作用
- en: As we have discussed in this chapter, applications running on the platform share
    the ingress data plane, and thus share that single entry point into the platform’s
    network. As requests come in, the Ingress controller’s primary responsibility
    is to disambiguate traffic and route it according to the Ingress configuration.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中讨论的那样，运行在平台上的应用程序共享 Ingress 数据平面，因此共享平台网络的单个入口点。随着请求的到来，Ingress 控制器的主要责任是消除流量的歧义并根据
    Ingress 配置进行路由。
- en: One of the primary ways to determine the destination of a request is by the
    target hostname (the `Host` header in the case of HTTP or SNI in the case of TCP),
    turning DNS into an essential player of your Ingress implementation. We will discuss
    two of the main approaches that are available when it comes to DNS and Ingress.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 确定请求的目标的主要方式之一是通过目标主机名（在 HTTP 的情况下是`Host`头部，在 TCP 的情况下是 SNI），这使得 DNS 成为您的 Ingress
    实现的一个重要组成部分。我们将讨论 DNS 和 Ingress 时可用的两种主要方法。
- en: Wildcard DNS record
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通配符 DNS 记录
- en: One of the most successful patterns we continuously use is to assign a domain
    name to the environment and slice it up by assigning subdomains to different applications.
    We sometimes call this “subdomain-based routing.” The implementation of this pattern
    involves creating a wildcard DNS record (e.g., `*.bearcanoe.com`) that resolves
    to the Ingress tier of the cluster. Typically, this is a load balancer that is
    in front of the Ingress controllers.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们持续采用的最成功模式之一是将一个域名分配给环境，并通过将子域名分配给不同的应用程序来切割它。我们有时称之为“基于子域的路由”。这种模式的实现涉及创建一个通配符
    DNS 记录（例如，`*.bearcanoe.com`），将其解析为集群的 Ingress 层。通常情况下，这是一个位于 Ingress 控制器前面的负载均衡器。
- en: 'There are several benefits to using a wildcard DNS record for your Ingress
    controllers:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 使用通配符 DNS 记录为您的 Ingress 控制器带来了几个好处：
- en: Applications can use any path under their subdomain, including the root path
    (`/`). Developers don’t have to spend engineering hours to make their apps work
    on subpaths. In some cases, applications expect to be hosted at the root path
    and do not work otherwise.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序可以使用其子域名下的任何路径，包括根路径（`/`）。开发者不必花费工程师的时间来让他们的应用程序在子路径上运行。在某些情况下，应用程序期望在根路径上托管，并且在其他情况下则无法正常工作。
- en: The DNS implementation is relatively straightforward. There is no integration
    necessary between Kubernetes and your DNS provider.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS 的实现相对简单。不需要在 Kubernetes 和您的 DNS 提供商之间进行集成。
- en: The single wildcard DNS record removes DNS propagation concerns that could arise
    when using different domain names for each application.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个通配符 DNS 记录消除了使用每个应用程序的不同域名可能出现的 DNS 传播问题。
- en: Kubernetes and DNS integration
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 和 DNS 整合
- en: An alternative to using a wildcard DNS record is to integrate your platform
    with your DNS provider. The Kubernetes community maintains a controller that offers
    this integration called [external-dns](https://github.com/kubernetes-sigs/external-dns).
    If you are using a DNS provider that is supported, consider using this controller
    to automate the creation of domain names.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用通配符 DNS 记录的替代方法是将您的平台与 DNS 提供商集成。Kubernetes 社区维护了一个名为 [external-dns](https://github.com/kubernetes-sigs/external-dns)
    的控制器，提供了这种集成。如果您使用受支持的 DNS 提供商，请考虑使用此控制器自动创建域名。
- en: 'As you might expect from a Kubernetes controller, external-dns continuously
    reconciles the DNS records in your upstream DNS provider and the configuration
    defined in Ingress resources. In other words, external-dns creates, updates, and
    deletes DNS records according to changes that happen in the Ingress API. External-dns
    needs two pieces of information to configure the DNS records, both of which are
    part of the Ingress resource: the desired hostname, which is in the Ingress specification,
    and the target IP address, which is available in the status field of the Ingress
    resource.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能期望的那样，external-dns 不断协调您上游 DNS 提供程序中的 DNS 记录和在 Ingress 资源中定义的配置。换句话说，external-dns
    根据 Ingress API 中发生的更改创建、更新和删除 DNS 记录。External-dns 需要两个信息来配置 DNS 记录，这两个信息都是 Ingress
    资源的一部分：所需的主机名在 Ingress 规范中，目标 IP 地址在 Ingress 资源的状态字段中。
- en: 'Integrating the platform with your DNS provider can be useful if you need to
    support multiple domain names. The controller takes care of automatically creating
    DNS records as needed. However, it is important to keep the following trade-offs
    in mind:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要支持多个域名，将平台与您的 DNS 提供程序集成可能很有用。控制器负责根据需要自动创建 DNS 记录。然而，重要的是要记住以下权衡：
- en: You have to deploy an additional component (external-dns) into your cluster.
    An additional add-on brings about more complexity into your deployments, given
    that you have to operate, maintain, monitor, version, and upgrade one more component
    in your platform.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须将额外的组件（external-dns）部署到您的集群中。额外的插件会给您的部署带来更多复杂性，因为您需要操作、维护、监控、版本化和升级平台中的一个额外组件。
- en: If external-dns does not support your DNS provider, you have to develop your
    own controller. Building and maintaining a controller requires engineering effort
    that could be spent on higher-value efforts. In these situations, it is best to
    simply implement a wildcard DNS record.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 external-dns 不支持您的 DNS 提供程序，您必须开发自己的控制器。构建和维护控制器需要工程努力，这些努力本应用于更高价值的工作。在这些情况下，最好简单实现一个通配符
    DNS 记录。
- en: Handling TLS Certificates
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理 TLS 证书
- en: Ingress controllers need certificates and their corresponding private keys to
    serve applications over TLS. Depending on your Ingress strategy, managing certificates
    can be cumbersome. If your cluster hosts a single domain name and implements subdomain-based
    routing, you can use a single wildcard TLS certificate. In some cases, however,
    clusters host applications across a variety of domains, making it challenging
    to manage certificates efficiently. Furthermore, your security team might frown
    upon the usage of wildcard certificates. In any case, the Kubernetes community
    has rallied around a certificate management add-on that eases the minting and
    management of certificates. The add-on is aptly called [cert-manager](https://cert-manager.io).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器需要证书及其相应的私钥以通过 TLS 提供应用程序服务。根据您的 Ingress 策略，管理证书可能会很麻烦。如果您的集群托管单个域名并实现基于子域的路由，则可以使用单个通配符
    TLS 证书。然而，在某些情况下，集群托管跨多个域的应用程序，这使得有效管理证书变得具有挑战性。此外，您的安全团队可能不赞成使用通配符证书。无论如何，Kubernetes
    社区已经围绕一个名为 [cert-manager](https://cert-manager.io) 的证书管理插件进行了集体努力。
- en: Cert-manager is a controller that runs in your cluster. It installs a set of
    CRDs that enable declarative management of Certificate Authorities (CAs) and Certificates
    via the Kubernetes API. More importantly, it supports different certificate issuers,
    including ACME-based CAs, HashiCorp Vault, Venafi, etc. It also offers an extension
    point to implement custom issuers, when necessary.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`cert-manager` 是在您的集群中运行的控制器。它安装了一组 CRD，通过 Kubernetes API 实现对证书颁发机构（CA）和证书的声明式管理。更重要的是，它支持不同的证书发行者，包括基于
    ACME 的 CA、HashiCorp Vault、Venafi 等。它还提供了一个扩展点来在必要时实现自定义发行者。'
- en: 'The certificate minting features of cert-manager revolve around issuers and
    certificates. Cert-manager has two issuer Custom Resources. The Issuer resource
    represents a CA that signs certificates in a specific Kubernetes Namespace. If
    you want to issue certificates across all Namespaces, you can use the ClusterIssuer
    resource. The following is a sample ClusterIssuer definition that uses a private
    key stored in a Kubernetes Secret named `platform-ca-key-pair`:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '`cert-manager` 的证书铸造功能围绕着发行者和证书展开。`cert-manager` 有两种发行者自定义资源。发行者资源代表在特定 Kubernetes
    命名空间中签署证书的 CA。如果要跨所有命名空间发行证书，可以使用 ClusterIssuer 资源。以下是一个使用存储在 Kubernetes 秘钥中的私钥
    `platform-ca-key-pair` 的示例 ClusterIssuer 定义：'
- en: '[PRE18]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The great thing about cert-manager is that it integrates with the Ingress API
    to automatically mint certificates for Ingress resources. For example, given the
    following Ingress object, cert-manager automatically creates a certificate key
    pair suitable for TLS:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: cert-manager 的优点在于它与 Ingress API 集成，自动为 Ingress 资源签发证书。例如，给定以下 Ingress 对象，cert-manager
    会自动创建适用于 TLS 的证书密钥对：
- en: '[PRE19]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_service_routing_CO7-1)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO7-1)'
- en: The `cert-manager.io/cluster-issuer` annotation tells cert-manager to use the
    `prod-ca-issuer` to mint the certificate.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`cert-manager.io/cluster-issuer` 注解告诉 cert-manager 使用 `prod-ca-issuer` 来签发证书。'
- en: '[![2](assets/2.png)](#co_service_routing_CO7-2)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO7-2)'
- en: Cert-manager stores the certificate and private key in a Kubernetes Secret called
    `bearcanoe-cert-key-pair`.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Cert-manager 将证书和私钥存储在名为 `bearcanoe-cert-key-pair` 的 Kubernetes Secret 中。
- en: Behind the scenes, cert-manager handles the certificate request process, which
    includes generating a private key, creating a certificate signing request (CSR),
    and submitting the CSR to the CA. Once the issuer mints the certificate, cert-manager
    stores it in the `bearcanoe-cert-key-pair` certificate. The Ingress controller
    can then pick it up and start serving the application over TLS. [Figure 6-17](#cert_manager_watches_the_ingress_api_and_requests_a_certificate)
    depicts the process in more detail.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，cert-manager 处理证书请求过程，包括生成私钥，创建证书签名请求（CSR），并将 CSR 提交给 CA。一旦签发者签发证书，cert-manager
    将其存储在 `bearcanoe-cert-key-pair` 证书中。然后，Ingress 控制器可以获取证书并开始通过 TLS 提供应用程序。[图 6-17](#cert_manager_watches_the_ingress_api_and_requests_a_certificate)
    更详细地描述了该过程。
- en: '![prku 0617](assets/prku_0617.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0617](assets/prku_0617.png)'
- en: Figure 6-17\. Cert-manager watches the Ingress API and requests a certificate
    from a Certificate Authority when the Ingress resource has the `cert-manager.io/cluster-issuer`
    annotation.
  id: totrans-352
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-17\. Cert-manager 监视 Ingress API，并在 Ingress 资源具有 `cert-manager.io/cluster-issuer`
    注解时向证书颁发机构请求证书。
- en: As you can see, cert-manager simplifies certificate management on Kubernetes.
    Most platforms we’ve encountered use cert-manager in some capacity. If you leverage
    cert-manager in your platform, consider using an external system such as Vault
    as the CA. Integrating cert-manager with an external system instead of using a
    CA backed by a Kubernetes Secret is a more robust and secure solution.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，cert-manager 简化了 Kubernetes 上的证书管理。我们遇到的大多数平台都以某种方式使用 cert-manager。如果您在平台上使用
    cert-manager，请考虑使用诸如 Vault 等外部系统作为 CA。将 cert-manager 与外部系统集成，而不是使用由 Kubernetes
    Secret 支持的 CA，是一种更强大和安全的解决方案。
- en: Service Mesh
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格
- en: As the industry continues to adopt containers and microservices, service meshes
    have gained immense popularity. While the term “service mesh” is relatively new,
    the concepts that it encompasses are not. Service meshes are a rehash of preexisting
    ideas in service routing, load balancing, and telemetry. Before the rise of containers
    and Kubernetes, hyperscale internet companies implemented service mesh precursors
    as they ran into challenges with microservices. Twitter, for example, created
    [Finagle](https://twitter.github.io/finagle), a Scala library that all its microservices
    embedded. It handled load balancing, circuit breaking, automatic retries, telemetry,
    and more. Netflix developed [Hystrix](https://github.com/Netflix/Hystrix), a similar
    library for Java applications.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 随着行业继续采用容器和微服务，服务网格变得非常流行。虽然术语“服务网格”相对较新，但它所包含的概念并非如此。服务网格是服务路由、负载平衡和遥测的预先存在的理念的再次整合。在容器和
    Kubernetes 兴起之前，超大规模的互联网公司实现了服务网格的前身，因为他们在微服务方面遇到了挑战。例如，Twitter 开发了 [Finagle](https://twitter.github.io/finagle)，一个
    Scala 库，所有微服务都内嵌了该库。它处理负载平衡、熔断、自动重试、遥测等功能。Netflix 开发了 [Hystrix](https://github.com/Netflix/Hystrix)，一个类似的
    Java 库。
- en: Containers and Kubernetes changed the landscape. Service meshes are no longer
    language-specific libraries like their precursors. Today, service meshes are distributed
    systems themselves. They consist of a control plane that configures a collection
    of proxies that implement the data plane. The routing, load balancing, telemetry,
    and other capabilities are built into the proxy instead of the application. The
    move to the proxy model has enabled even more apps to take advantage of these
    features, as there’s no need to make code changes to participate in a mesh.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 容器和 Kubernetes 改变了格局。服务网格不再像它们的前身那样是特定语言的库。今天，服务网格本身就是分布式系统。它们包含一个控制平面，该控制平面配置一组代理，这些代理实现了数据平面。路由、负载平衡、遥测和其他功能都内置于代理中，而不是应用程序中。向代理模型的转移使更多的应用程序能够利用这些功能，因为不需要对代码进行更改来参与网格。
- en: 'Service meshes provide a broad set of features that can be categorized across
    three pillars:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格提供了一系列广泛的功能，可以分为三大支柱：
- en: Routing and reliability
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 路由和可靠性
- en: Advanced traffic routing and reliability features such as traffic shifting,
    traffic mirroring, retries, and circuit breaking.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 高级流量路由和可靠性功能，例如流量转移、流量镜像、重试和断路器。
- en: Security
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性
- en: Identity and access control features that enable secure communication between
    services, including identity, certificate management, and mutual TLS (mTLS).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 身份和访问控制功能，支持服务之间的安全通信，包括身份、证书管理和双向 TLS（mTLS）。
- en: Observability
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性
- en: Automated gathering of metrics and traces from all the interactions happening
    in the mesh.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 自动收集来自网格中所有交互的指标和跟踪信息。
- en: Throughout the rest of this chapter, we are going to discuss service mesh in
    more detail. Before we do so, however, let’s return to this book’s central theme
    and ask “Do we need a service mesh?” Service meshes have risen in popularity as
    some organizations see them as a golden bullet to implement the aforementioned
    features. However, we have found that organizations should carefully consider
    the impact of adopting a service mesh.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分中，我们将更详细地讨论服务网格。然而，在这样做之前，让我们回到本书的中心主题，并问：“我们需要服务网格吗？”一些组织将服务网格视为实现前述功能的灵丹妙药，因此服务网格越来越受欢迎。然而，我们发现组织应仔细考虑采用服务网格的影响。
- en: When (Not) to Use a Service Mesh
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时（不）使用服务网格
- en: A service mesh can provide immense value to an application platform and the
    applications that run atop. It offers an attractive feature set that your developers
    will appreciate. At the same time, a service mesh brings a ton of complexity that
    you must deal with.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格可以为应用平台及其上运行的应用程序提供巨大的价值。它提供了一组吸引力强大的功能，您的开发人员会很喜欢。与此同时，服务网格带来了大量复杂性，您必须处理这些复杂性。
- en: Kubernetes is a complex distributed system. Up to this point in the book, we
    have touched on some of the building blocks you need to create an application
    platform atop Kubernetes, and there are still a bunch of chapters left. The reality
    is that building a successful Kubernetes-based application platform is a lot of
    work. Keep this in mind when you are considering a service mesh. Tackling a service
    mesh implementation while you are beginning your Kubernetes journey will slow
    you down, if not take you down the path to failure.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个复杂的分布式系统。到目前为止，在本书中，我们已经涉及了构建基于 Kubernetes 的应用平台所需的一些基本组件，还有很多章节尚未讲解。事实是，构建一个成功的基于
    Kubernetes 的应用平台是一项艰巨的工作。在考虑使用服务网格时，请记住这一点。在您开始 Kubernetes 之旅时实施服务网格将会减慢您的步伐，甚至可能导致失败。
- en: We have seen these cases firsthand while working in the field. We have worked
    with platform teams who were blinded by the shiny features of a service mesh.
    Granted, those features would make their platform more attractive to developers
    and thus increase the platform’s adoption. However, timing is important. Wait
    until you gain operational experience in production before thinking about service
    mesh.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在实地工作中，我们亲眼见证了这些情况。我们与平台团队合作过，他们被服务网格的诱人功能所蒙蔽。诚然，这些功能将使他们的平台对开发人员更具吸引力，从而增加平台的采用率。然而，时间是重要的。在开始考虑服务网格之前，等到在生产环境中获得操作经验。
- en: Perhaps more critical is for you to understand your requirements or the problems
    you are trying to solve. Putting the cart before the horse will not only increase
    the chances of your platform failing but also result in wasted engineering effort.
    A fitting example of this mistake is an organization that dove into service mesh
    while developing a Kubernetes-based platform that was not yet in production. “We
    want a service mesh because we need everything it provides,” they said. Twelve
    months later, the only feature they were using was the mesh’s Ingress capabilities.
    No mutual TLS, no fancy routing, no tracing. Just Ingress. The engineering effort
    to get a dedicated Ingress controller ready for production is far less than a
    full-featured mesh implementation. There’s something to be said for getting a
    minimum viable product into production and then iterating to add features moving
    forward.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 或许更重要的是你理解你的需求或者你试图解决的问题。把车放在马前面不仅会增加平台失败的几率，还会导致工程资源的浪费。这种错误的一个典型例子是一个组织在开发基于
    Kubernetes 的平台时，还未投入生产就盲目采用了服务网格。“我们需要服务网格，因为它提供的所有功能我们都需要”，他们说。十二个月后，他们唯一使用的功能只有网格的
    Ingress 能力。没有双向 TLS，没有复杂的路由，没有跟踪。只有 Ingress。为了使专用的 Ingress 控制器准备投入生产的工程资源远远少于全功能网格的实现。在将最小可行产品投入生产后，逐步增加功能也有其积极的一面。
- en: After reading this, you might feel like we think there’s no place for a service
    mesh in an application platform. Quite the opposite. A service mesh can solve
    a ton of problems if you have them, and it can bring a ton of value if you take
    advantage of it. In the end, we have found that a successful service mesh implementation
    boils down to timing it right and doing it for the right reasons.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读完本节后，你可能会觉得我们认为应用平台中没有服务网格的位置。恰恰相反，如果有需求，服务网格可以解决大量问题，并且如果能够充分利用，它可以带来巨大的价值。最终，我们发现一个成功的服务网格实现取决于选择合适的时机和出于正确的动机。
- en: The Service Mesh Interface (SMI)
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务网格接口（SMI）
- en: Kubernetes provides interfaces for a variety of pluggable components. These
    interfaces include the Container Runtime Interface (CRI), the Container Networking
    Interface (CNI), and others. As we’ve seen throughout the book, these interfaces
    are what makes Kubernetes such an extensible foundation. Service mesh is slowly
    but surely becoming an important ingredient of a Kubernetes platform. Thus, the
    service mesh community collaborated to build the Service Mesh Interface, or SMI.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了各种可插拔组件的接口。这些接口包括容器运行时接口（CRI）、容器网络接口（CNI）等。正如本书所述，正是这些接口使 Kubernetes
    成为一个可扩展的基础设施。服务网格正在逐步成为 Kubernetes 平台的重要组成部分。因此，服务网格社区合作建立了服务网格接口（SMI）。
- en: Similar to the other interfaces we’ve already discussed, the SMI specifies the
    interaction between Kubernetes and a service mesh. With that said, the SMI is
    different than other Kubernetes interfaces in that it is not part of the core
    Kubernetes project. Instead, the SMI project leverages CRDs to specify the interface.
    The SMI project also houses libraries to implement the interface, such as the
    SMI SDK for Go.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们已经讨论过的其他接口类似，SMI 指定了 Kubernetes 与服务网格之间的交互。然而，SMI 与其他 Kubernetes 接口不同的是，它不是核心
    Kubernetes 项目的一部分。相反，SMI 项目利用自定义资源定义（CRD）来指定接口。SMI 项目还包括用于实现接口的库，例如用于 Go 的 SMI
    SDK。
- en: 'The SMI covers the three pillars we discussed in the previous section with
    a set of CRDs. The Traffic Split API is concerned with routing and splitting traffic
    across a number of services. It enables percent-based traffic splitting, which
    enables different deployment scenarios such as blue-green deployments and A/B
    testing. The following snippet is an example of a TrafficSplit that performs a
    canary deployment of the “flights” web service:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: SMI 通过一组 CRD 覆盖了我们在前一节讨论的三大支柱。Traffic Split API 关注于跨多个服务的流量路由和分割。它支持基于百分比的流量分割，从而实现不同的部署场景，如蓝绿部署和
    A/B 测试。以下片段展示了一个 TrafficSplit 的示例，用于执行“flights” Web 服务的金丝雀部署：
- en: '[PRE20]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_service_routing_CO8-1)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO8-1)'
- en: The top-level Service that clients connect to (i.e., `flights.bookings.cluster.svc.local`).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 客户连接的顶层服务（即`flights.bookings.cluster.svc.local`）。
- en: '[![2](assets/2.png)](#co_service_routing_CO8-2)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO8-2)'
- en: The backend Services that receive the traffic. The v1 version receives 70% of
    traffic and the v2 version receives the rest.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 接收流量的后端服务。v1 版本接收 70% 的流量，而 v2 版本接收其余的流量。
- en: The Traffic Access Control and Traffic Specs APIs work together to implement
    security features such as access control. The Traffic Access Control API provides
    CRDs to control the service interactions that are allowed in the mesh. With these
    CRDs, developers can specify access control policy that determines which services
    can talk to each other and under what conditions (list of allowed HTTP methods,
    for example). The Traffic Specs API offers a way to describe traffic, including
    an `HTTPRouteGroup` CRD for HTTP traffic and a `TCPRoute` for TCP traffic. Together
    with the Traffic Access Control CRDs, these apply policy at the application level.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 流量访问控制和流量规格API一起实现安全功能，如访问控制。流量访问控制API提供CRD来控制网格中允许的服务交互。借助这些CRD，开发人员可以指定访问控制策略，确定哪些服务可以在什么条件下进行通信（例如允许的HTTP方法列表）。流量规格API提供了描述流量的方法，包括用于HTTP流量的`HTTPRouteGroup`
    CRD和用于TCP流量的`TCPRoute`。与流量访问控制CRD一起，这些在应用程序级别应用策略。
- en: 'For example, the following HTTPRouteGroup and TrafficTarget allow all requests
    from the bookings service to the payments service. The HTTPRouteGroup resource
    describes the traffic, while the TrafficTarget specifies the source and destination
    services:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下HTTPRouteGroup和TrafficTarget允许来自bookings服务到payments服务的所有请求。HTTPRouteGroup资源描述流量，而TrafficTarget指定了源和目标服务：
- en: '[PRE21]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_service_routing_CO9-1)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO9-1)'
- en: Allow all requests in this HTTPRouteGroup.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 允许在这个HTTPRouteGroup中的所有请求。
- en: '[![2](assets/2.png)](#co_service_routing_CO9-2)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO9-2)'
- en: The destination service. In this case, the Pods using the `payments` Service
    Account in the `payments` Namespace.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 目标服务。在本例中，使用`payments`命名空间中`payments`服务账户的Pod。
- en: '[![3](assets/3.png)](#co_service_routing_CO9-3)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_service_routing_CO9-3)'
- en: The HTTPRouteGroups that control the traffic between the source and destination
    services.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 控制源和目标服务之间流量的HTTPRouteGroups。
- en: '[![4](assets/4.png)](#co_service_routing_CO9-4)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_service_routing_CO9-4)'
- en: The source service. In this case, the Pods using the `flights` Service Account
    in the `bookings` Namespace.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 源服务。在本例中，使用`bookings`命名空间中`flights`服务账户的Pod。
- en: 'Finally, the Traffic Metrics API provides the telemetry functionality of a
    service mesh. This API is somewhat different than the rest in that it defines
    outputs instead of mechanisms to provide inputs. The Traffic Metrics API defines
    a standard to expose service metrics. Systems that need these metrics, such as
    monitoring systems, autoscalers, dashboards, and others, can consume them in a
    standardized fashion. The following snippet shows an example TrafficMetrics resource
    that exposes metrics for traffic between two Pods:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，流量度量API提供了服务网格的遥测功能。这个API与其他API稍有不同，因为它定义的是输出而不是提供输入的机制。流量度量API定义了公开服务度量标准。需要这些度量标准的系统，如监控系统、自动缩放器、仪表板等，可以以标准化的方式使用它们。以下片段显示了一个展示两个Pod之间流量度量指标的示例TrafficMetrics资源：
- en: '[PRE22]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The SMI is one of the newest interfaces in the Kubernetes community. While still
    under development and iteration, it paints the picture of where we are headed
    as a community. As with other interfaces in Kubernetes, the SMI enables platform
    builders to offer a service mesh using portable and provider-agnostic APIs, further
    increasing the value, flexibility, and power of Kubernetes.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: SMI是Kubernetes社区中最新的接口之一。尽管仍在开发和迭代中，但它描绘了我们作为一个社区未来的图景。与Kubernetes中的其他接口一样，SMI使平台构建者能够使用便携和与提供者无关的API来提供服务网格，进一步增加了Kubernetes的价值、灵活性和能力。
- en: The Data Plane Proxy
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据平面代理
- en: The data plane of a service mesh is a collection of proxies that connect services
    together. The [Envoy proxy](https://www.envoyproxy.io) is one of the most popular
    service proxies in the cloud native ecosystem. Originally developed at Lyft, it
    has quickly become a prevalent building block in cloud native systems since it
    was open sourced in [late 2016](https://oreil.ly/u5fCD).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格的数据平面是连接服务的一组代理。[Envoy代理](https://www.envoyproxy.io)是云原生生态系统中最流行的服务代理之一。最初由Lyft开发，自2016年末开源以来，迅速成为云原生系统中的重要组成部分。
- en: Envoy is used in Ingress controllers ([Contour](https://projectcontour.io)),
    API gateways ([Ambassador](https://www.getambassador.io), [Gloo](https://docs.solo.io/gloo/latest)),
    and, you guessed it, service meshes ([Istio](https://istio.io), [OSM](https://github.com/openservicemesh/osm)).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy 用于 Ingress 控制器（[Contour](https://projectcontour.io)）、API 网关（[Ambassador](https://www.getambassador.io)、[Gloo](https://docs.solo.io/gloo/latest)）以及服务网格（[Istio](https://istio.io)、[OSM](https://github.com/openservicemesh/osm)）。
- en: One of the reasons why Envoy is such a good building block is its support for
    dynamic configuration over a gRPC/REST API. Open source proxies that predate Envoy
    were not designed for environments as dynamic as Kubernetes. They used static
    configuration files and required restarts for configuration changes to take effect.
    Envoy, on the other hand, offers the xDS (* discovery service) APIs for dynamic
    configuration (depicted in [Figure 6-18](#envoy_supports_dynamic_configuration_via_the_xds_api)).
    It also supports hot restarts, which allow Envoy to reinitialize without dropping
    any active connections.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy 之所以成为优秀的构建块之一，原因之一在于其支持通过 gRPC/REST API 实现动态配置。早期的开源代理程序在 Kubernetes 这样动态环境下并未设计得如
    Envoy 那样，它们使用静态配置文件，需要重启才能生效配置更改。相反，Envoy 提供了 xDS（*discovery service*）API，用于动态配置（详见
    [图 6-18](#envoy_supports_dynamic_configuration_via_the_xds_api)）。此外，Envoy 还支持热重启功能，允许在重新初始化时不中断任何活动连接。
- en: '![prku 0618](assets/prku_0618.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0618](assets/prku_0618.png)'
- en: Figure 6-18\. Envoy supports dynamic configuration via the XDS APIs. Envoy connects
    to a configuration server and requests its configuration using LDS, RDS, EDS,
    CDS, and other xDS APIs.
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-18\. Envoy 通过 XDS API 支持动态配置。Envoy 连接到配置服务器并使用 LDS、RDS、EDS、CDS 和其他 xDS API
    请求其配置。
- en: Envoy’s xDS is a collection of APIs that includes the Listener Discovery Service
    (LDS), the Cluster Discovery Service (CDS), the Endpoints Discovery Service (EDS),
    the Route Discovery Service (RDS), and more. An Envoy *configuration server* implements
    these APIs and behaves as the source of dynamic configuration for Envoy. During
    startup, Envoy reaches out to a configuration server (typically over gRPC) and
    subscribes to configuration changes. As things change in the environment, the
    configuration server streams changes to Envoy. Let’s review the xDS APIs in more
    detail.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy 的 xDS 是一组 API 集合，包括 Listener Discovery Service (LDS)、Cluster Discovery
    Service (CDS)、Endpoints Discovery Service (EDS)、Route Discovery Service (RDS)
    等。一个 Envoy *configuration server* 实现这些 API，并作为 Envoy 动态配置的源头。在启动期间，Envoy 连接到配置服务器（通常通过
    gRPC）并订阅配置更改。随着环境变化，配置服务器会向 Envoy 流式传输变更。让我们更详细地审视 xDS API。
- en: The LDS API configures Envoy’s *Listeners*. Listeners are the entry point into
    the proxy. Envoy can open multiple Listeners that clients can connect to. A typical
    example is listening on ports 80 and 443 for HTTP and HTTPS traffic.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: LDS API 配置了 Envoy 的 *Listeners*。Listeners 是代理的入口点，Envoy 可以打开多个 Clients 可以连接的
    Listener。典型示例是监听端口 80 和 443 以处理 HTTP 和 HTTPS 流量。
- en: Each Listener has a set of filter chains that determine how to handle incoming
    traffic. The HTTP connection manager filter leverages the RDS API to obtain routing
    configuration. The routing configuration tells Envoy how to route incoming HTTP
    requests. It provides details around virtual hosts and request matching (path-based,
    header-based, and others).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Listener 都有一组过滤器链，决定如何处理传入的流量。HTTP 连接管理器过滤器利用 RDS API 获取路由配置。路由配置告诉 Envoy
    如何路由传入的 HTTP 请求，提供有关虚拟主机和请求匹配（基于路径、基于标头等）的详细信息。
- en: Each route in the routing configuration references a *Cluster*. A cluster is
    a collection of *Endpoints* that belong to the same service. Envoy discovers Clusters
    and Endpoints using the CDS and EDS APIs, respectively. Interestingly enough,
    the EDS API does not have an Endpoint object per se. Instead, Endpoints are assigned
    to clusters using *ClusterLoadAssignment* objects.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 路由配置中的每条路由引用了一个 *Cluster*。Cluster 是属于同一服务的一组 *Endpoints*。Envoy 使用 CDS 和 EDS
    API 发现 Cluster 和 Endpoints。有趣的是，EDS API 实际上没有 Endpoint 对象，而是使用 *ClusterLoadAssignment*
    对象将 Endpoints 分配给 Clusters。
- en: While digging into the details of the xDS APIs merits its own book, we hope
    the preceding overview gives you an idea of how Envoy works and its capabilities.
    To summarize, listeners bind to ports and accept connections from clients. Listeners
    have filter chains that determine what to do with incoming connections. For example,
    the HTTP filter inspects requests and maps them to clusters. Each cluster has
    one or more endpoints that end up receiving and handling the traffic. [Figure 6-19](#envoy_configuration_with_a_listener_that_binds_to_port_80)
    shows a graphical representation of these concepts and how they relate to each
    other.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深入探讨xDS API的细节值得一本专门的书籍，但我们希望前述概述能让您了解Envoy的工作原理及其功能。总结一下，监听器绑定到端口并接受来自客户端的连接。监听器具有过滤器链，决定如何处理传入连接。例如，HTTP过滤器检查请求并将其映射到集群。每个集群有一个或多个端点，最终接收并处理流量。[图 6-19](#envoy_configuration_with_a_listener_that_binds_to_port_80)展示了这些概念的图形表示及其相互关系。
- en: '![prku 0619](assets/prku_0619.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0619](assets/prku_0619.png)'
- en: Figure 6-19\. Envoy configuration with a Listener that binds to port 80\. The
    Listener has an HTTP connection manager filter that references a routing configuration.
    The routing config matches requests with `/` prefix and forwards requests to the
    `my_service` cluster, which has three endpoints.
  id: totrans-406
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-19\. Envoy配置，监听器绑定到端口80\. 监听器具有HTTP连接管理器过滤器，引用路由配置。路由配置匹配以`/`前缀开头的请求，并将其转发到`my_service`集群，该集群有三个端点。
- en: Service Mesh on Kubernetes
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes上的服务网格
- en: In the previous section, we discussed how the data plane of a service mesh provides
    connectivity between services. We also talked about Envoy as a data plane proxy
    and how it supports dynamic configuration through the xDS APIs. To build a service
    mesh on Kubernetes, we need a control plane that configures the mesh’s data plane
    according to what’s happening inside the cluster. The control plane needs to understand
    Services, Endpoints, Pods, etc. Furthermore, it needs to expose Kubernetes Custom
    Resources that developers can use to configure the service mesh.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们讨论了服务网格的数据平面如何为服务之间提供连接。我们还谈到了Envoy作为数据平面代理，以及它通过xDS API支持动态配置的方式。要在Kubernetes上构建服务网格，我们需要一个控制平面，根据集群内部的情况配置网格的数据平面。控制平面需要理解服务、端点、Pod等概念。此外，它需要暴露Kubernetes自定义资源，供开发人员用来配置服务网格。
- en: 'One of the most popular service mesh implementations for Kubernetes is Istio.
    Istio implements a control plane for an Envoy-based service mesh. The control
    plane is implemented in a component called istiod, which itself has three primary
    sub-components: Pilot, Citadel, and Galley. Pilot is an Envoy configuration server.
    It implements the xDS APIs and streams the configuration to the Envoy proxies
    running alongside the applications. Citadel is responsible for certificate management
    inside the mesh. It mints certificates that are used to establish service identity
    and mutual TLS. Finally, Galley interacts with external systems such as Kubernetes
    to obtain configuration. It abstracts the underlying platform and translates configuration
    for the other istiod components. [Figure 6-20](#istio_control_plane_interactions)
    shows the interactions between the Istio control plane components.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中最流行的服务网格实现之一是Istio。Istio实现了基于Envoy的服务网格的控制平面。控制平面实现在一个名为istiod的组件中，它本身有三个主要子组件：Pilot、Citadel和Galley。Pilot是一个Envoy配置服务器，它实现了xDS
    API，并将配置流式传输到与应用程序并行运行的Envoy代理。Citadel负责网格内的证书管理，它颁发证书用于建立服务身份和相互TLS。最后，Galley与Kubernetes等外部系统交互以获取配置。它抽象了底层平台并为其他istiod组件翻译配置。[图 6-20](#istio_control_plane_interactions)显示了Istio控制平面组件之间的交互。
- en: '![prku 0620](assets/prku_0620.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0620](assets/prku_0620.png)'
- en: Figure 6-20\. Istio control plane interactions.
  id: totrans-411
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-20\. Istio控制平面交互。
- en: 'Istio provides other capabilities besides configuring the data plane of the
    service mesh. First, Istio includes a mutating admission webhook that injects
    an Envoy sidecar into Pods. Every Pod that participates in the mesh has an Envoy
    sidecar that handles all the incoming and outgoing connections. The mutating webhook
    improves the developer experience on the platform, given that developers don’t
    have to manually add the sidecar proxy to all of their application deployment
    manifests. The platform injects the sidecar automatically with both an opt-in
    and opt-out model. With that said, merely injecting the Envoy proxy sidecar alongside
    the workload does not mean the workload will automatically start sending traffic
    through Envoy. Thus, Istio uses an init-container to install iptables rules that
    intercept the Pod’s network traffic and routes it to Envoy. The following snippet
    (trimmed for brevity) shows the Istio init-container configuration:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 在配置服务网格的数据平面之外还提供了其他功能。首先，Istio 包括一个变更接受的 Webhook，将 Envoy Sidecar 注入到
    Pod 中。参与网格的每个 Pod 都有一个处理所有入站和出站连接的 Envoy Sidecar。变更接受的 Webhook 提升了平台上的开发体验，开发人员无需手动将
    Sidecar 代理添加到所有应用程序部署清单中，平台会自动以选择加入和退出的模式注入 Sidecar。尽管如此，仅仅注入 Envoy 代理 Sidecar
    到工作负载旁边并不意味着工作负载会自动开始通过 Envoy 发送流量。因此，Istio 使用一个 init-container 安装 iptables 规则，拦截
    Pod 的网络流量并将其路由到 Envoy。以下摘录（为简洁起见进行了修剪）显示了 Istio init-container 的配置：
- en: '[PRE23]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_service_routing_CO10-1)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_service_routing_CO10-1)'
- en: Istio installs an iptables rule that captures all outbound traffic and sends
    it to Envoy at this port.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 安装了一个 iptables 规则，捕获所有出站流量并将其发送到该端口的 Envoy。
- en: '[![2](assets/2.png)](#co_service_routing_CO10-2)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_service_routing_CO10-2)'
- en: Istio installs an iptables rule that captures all inbound traffic and sends
    it to Envoy at this port.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 安装了一个 iptables 规则，捕获所有入站流量并将其发送到该端口的 Envoy。
- en: '[![3](assets/3.png)](#co_service_routing_CO10-3)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_service_routing_CO10-3)'
- en: List of CIDRs to redirect to Envoy. In this case, we are redirecting all CIDRs.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 要重定向到 Envoy 的 CIDR 列表。在这种情况下，我们重定向所有 CIDR。
- en: '[![4](assets/4.png)](#co_service_routing_CO10-4)'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_service_routing_CO10-4)'
- en: List of ports to redirect to Envoy. In this case, we are redirecting all ports.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 要重定向到 Envoy 的端口列表。在这种情况下，我们重定向所有端口。
- en: 'Now that we’ve discussed Istio’s architecture, let’s discuss some of the service
    mesh features that are typically used. One of the more common requirements we
    run into in the field is service authentication and encryption of service-to-service
    traffic. This feature is covered by the Traffic Access Control APIs in the SMI.
    Istio and most service mesh implementations use mutual TLS to achieve this. In
    Istio’s case, mutual TLS is enabled by default for all services that are participating
    in the mesh. The workload sends unencrypted traffic to the sidecar proxy. The
    sidecar proxy upgrades the connection to mTLS and sends it along to the sidecar
    proxy on the other end. By default, the services can still receive non-TLS traffic
    from other services outside of the mesh. If you want to enforce mTLS for all interactions,
    Istio supports a `STRICT` mode that configures all services in the mesh to accept
    only TLS-encrypted requests. For example, you can enforce strict mTLS at the cluster
    level with the following configuration in the `istio-system` Namespace:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了 Istio 的架构，让我们来讨论一些通常使用的服务网格特性。我们在现场遇到的更常见的需求之一是服务间的身份验证和服务间流量的加密。这个功能由
    SMI 中的流量访问控制 API 提供支持。Istio 和大多数服务网格实现都使用双向 TLS 来实现这一点。在 Istio 的情况下，默认情况下，所有参与网格的服务都启用了双向
    TLS。工作负载将未加密的流量发送到 Sidecar 代理。Sidecar 代理升级连接到 mTLS 并将其发送到另一端的 Sidecar 代理。默认情况下，服务仍然可以从网格外的其他服务接收非
    TLS 流量。如果您想强制所有交互使用 mTLS，Istio 支持 `STRICT` 模式，该模式配置网格中的所有服务只接受 TLS 加密的请求。例如，您可以在
    `istio-system` 命名空间中的配置中全局启用严格的 mTLS：
- en: '[PRE24]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Traffic management is another key concern handled by a service mesh. Traffic
    management is captured in the Traffic Split API of the SMI, even though Istio’s
    traffic management features are more advanced. In addition to traffic splitting
    or shifting, Istio supports fault injection, circuit breaking, mirroring, and
    more. When it comes to traffic shifting, Istio uses two separate Custom Resources
    for configuration: VirtualService and DestinationRule.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 流量管理是服务网格处理的另一个关键问题。流量管理在 SMI 的 Traffic Split API 中被捕获，尽管 Istio 的流量管理功能更为先进。除了流量分割或转移外，Istio
    还支持故障注入、熔断、镜像和更多功能。当涉及到流量转移时，Istio 使用两个独立的自定义资源进行配置：VirtualService 和 DestinationRule。
- en: The *VirtualService* resource creates services in the mesh and specifies how
    traffic is routed to them. It specifies the hostname of the service and rules
    that control the destination of the requests. For example, the VirtualService
    can send 90% of traffic to one destination and send the rest to another. Once
    the VirtualService evaluates the rules and chooses a destination, it sends the
    traffic along to a specific subset of a *DestinationRule*.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VirtualService* 资源在网格中创建服务并指定流量路由方式。它指定了服务的主机名以及控制请求目的地的规则。例如，VirtualService
    可以将 90% 的流量发送到一个目的地，将其余的发送到另一个目的地。一旦 VirtualService 评估了规则并选择了目的地，它会将流量发送到 *DestinationRule*
    的特定子集之一。'
- en: The *DestinationRule* resource lists the “real” backends that are available
    for a given Service. Each backend is captured in a separate subset. Each subset
    can have its own routing configuration, such as load balancing policy, mutual
    TLS mode, and others.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DestinationRule* 资源列出了特定服务可用的“真实”后端。每个后端都在单独的子集中。每个子集可以有自己的路由配置，如负载均衡策略、双向
    TLS 模式等。'
- en: 'As an example, let’s consider a scenario where we want to slowly roll out version
    2 of a service. We can use the following DestinationRule and VirtualService to
    achieve this. The DestinationRule creates two service subsets: v1 and v2\. The
    VirtualService references these subsets. It sends 90% of traffic to the v1 subset
    and 10% of the traffic to the v2 subset:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，让我们考虑一个场景，我们想要逐步推出服务的第二个版本。我们可以使用以下 DestinationRule 和 VirtualService 来实现这一目标。DestinationRule
    创建了两个服务子集：v1 和 v2。VirtualService 引用了这些子集。它将 90% 的流量发送到 v1 子集，将 10% 的流量发送到 v2 子集：
- en: '[PRE25]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Service observability is another feature that is commonly sought after. Because
    there’s a proxy between all services in the mesh, deriving service-level metrics
    is straightforward. Developers get these metrics without having to instrument
    their applications. The metrics are exposed in the Prometheus format, which makes
    them available to a wide range of monitoring systems. The following is an example
    metric captured by the sidecar proxy (some labels removed for brevity). The metric
    shows that there have been 7183 successful requests from the flights booking service
    to the payment processing service:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 服务可观测性是另一个常被追求的特性。由于网格中所有服务之间都有代理，因此推导服务级别的指标是直接的。开发人员可以获取这些指标，而无需为其应用程序进行工具化。这些指标以
    Prometheus 格式公开，可供广泛的监控系统使用。以下是由 sidecar 代理捕获的示例指标（为简洁起见移除了一些标签）。该指标显示，来自航班预订服务到支付处理服务的成功请求数量为
    7183：
- en: '[PRE26]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Overall, Istio offers all of the features that are captured in the SMI. However,
    it does not yet implement the SMI APIs (Istio v1.6). The SMI community maintains
    an [adapter](https://github.com/servicemeshinterface/smi-adapter-istio) that you
    can use to make the SMI APIs work with Istio. We discussed Istio mainly because
    it is the service mesh that we’ve most commonly encountered in the field. With
    that said, there are other meshes available in the Kubernetes ecosystem, including
    Linkerd, Consul Connect, Maesh, and more. One of the things that varies across
    these implementations is the data plane architecture, which we’ll discuss next.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Istio 提供了所有在 SMI 中捕获的特性。但是，它尚未实现 SMI API（Istio v1.6）。SMI 社区维护了一个 [适配器](https://github.com/servicemeshinterface/smi-adapter-istio)，您可以使用它将
    SMI API 与 Istio 兼容。我们主要讨论 Istio，因为这是我们在现场最常遇到的服务网格。话虽如此，Kubernetes 生态系统中还有其他网格，包括
    Linkerd、Consul Connect、Maesh 等。这些实现之间变化的一件事是数据平面架构，我们将在接下来讨论。
- en: Data Plane Architecture
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据平面架构
- en: 'A service mesh is a highway that services can use to communicate with each
    other. To get onto this highway, services use a proxy that serves as the on-ramp.
    Service meshes follow one of two architecture models when it comes to the data
    plane: the sidecar proxy or the node proxy.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是服务之间可以用来通信的高速公路。为了进入这条高速公路，服务使用作为匝道的代理。在数据平面方面，服务网格遵循旁路代理或节点代理两种架构模型之一。
- en: Sidecar proxy
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 旁路代理
- en: The sidecar proxy is the most common architecture model among the two. As we
    discussed in the previous section, Istio follows this model to implement its data
    plane with Envoy proxies. Linkerd uses this approach as well. In essence, service
    meshes that follow this pattern deploy the proxy inside the workload’s Pod, running
    alongside the service. Once deployed, the sidecar proxy intercepts all the communications
    into and out of the service, as depicted in [Figure 6-21](#pods_participating_in_the_mesh).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种架构模型中，旁路代理是最常见的模型之一。正如我们在前一节中讨论的那样，Istio 使用这种模型来实现其通过 Envoy 代理的数据平面。Linkerd
    也采用了这种方法。实质上，遵循此模式的服务网格在工作负载的 Pod 内部部署代理，与服务一起运行。一旦部署，旁路代理拦截进出服务的所有通信，如图 [6-21](#pods_participating_in_the_mesh)
    所示。
- en: '![prku 0621](assets/prku_0621.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0621](assets/prku_0621.png)'
- en: Figure 6-21\. Pods participating in the mesh have a sidecar proxy that intercepts
    the Pod’s network traffic.
  id: totrans-437
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-21\. 参与网格的 Pod 具有拦截 Pod 网络流量的旁路代理。
- en: When compared to the node proxy approach, the sidecar proxy architecture can
    have greater impact on services when it comes to data plane upgrades. The upgrade
    involves rolling all the service Pods, as there is no way to upgrade the sidecar
    without re-creating the Pods.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点代理方法相比，旁路代理架构在数据平面升级时对服务的影响可能更大。升级涉及滚动所有服务的 Pod，因为没有办法在不重新创建 Pod 的情况下升级旁路代理。
- en: Node proxy
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点代理
- en: The node proxy is an alternative data plane architecture. Instead of injecting
    a sidecar proxy into each service, the service mesh consists of a single proxy
    running on each node. Each node proxy handles the traffic for all services running
    on their node, as depicted in [Figure 6-22](#the_node_proxy_model_involves_a_single_service_mesh_proxy).
    Service meshes that follow this architecture include [Consul Connect](https://www.consul.io/docs/connect)
    and [Maesh](https://containo.us/maesh). The first version of Linkerd used node
    proxies as well, but the project has since moved to the sidecar model in version
    2.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 节点代理是一种备选的数据平面架构。与向每个服务注入旁路代理不同，服务网格由每个节点上运行的单个代理组成。每个节点代理处理其节点上运行的所有服务的流量，如图
    [6-22](#the_node_proxy_model_involves_a_single_service_mesh_proxy) 所示。遵循此架构的服务网格包括
    [Consul Connect](https://www.consul.io/docs/connect) 和 [Maesh](https://containo.us/maesh)。Linkerd
    的第一个版本也使用了节点代理，但是在第二个版本中项目已经转向了旁路代理模型。
- en: When compared to the sidecar proxy architecture, the node proxy approach can
    have greater performance impact on services. Because the proxy is shared by all
    the services on a node, services can suffer from noisy neighbor problems and the
    proxy can become a network bottleneck.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 与旁路代理架构相比，节点代理方法对服务可能有更大的性能影响。因为代理由节点上所有服务共享，服务可能会遭受邻居干扰问题，代理也可能成为网络瓶颈。
- en: '![prku 0622](assets/prku_0622.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0622](assets/prku_0622.png)'
- en: Figure 6-22\. The node proxy model involves a single service mesh proxy that
    handles the traffic for all services on the node.
  id: totrans-443
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-22\. 节点代理模型涉及处理节点上所有服务流量的单个服务网格代理。
- en: Adopting a Service Mesh
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用服务网格
- en: Adopting a service mesh can seem like a daunting task. Should you deploy it
    to an existing cluster? How do you avoid affecting workloads that are already
    running? How can you selectively onboard services for testing?
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 采用服务网格可能看起来像是一项艰巨的任务。你应该将其部署到现有集群吗？如何避免影响已经运行的工作负载？如何有选择地引入服务进行测试？
- en: In this section, we will explore the different considerations you should make
    when introducing a service mesh to your application platform.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨在引入服务网格到应用平台时您应该考虑的不同因素。
- en: Prioritize one of the pillars
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优先考虑支柱之一
- en: One of the first things to do is to prioritize one of the service mesh pillars.
    Doing so will allow you to narrow the scope, both from an implementation and testing
    perspective. Depending on your requirements (which you’ve established if you’re
    adopting a service mesh, right?), you might prioritize mutual TLS, for example,
    as the first pillar. In this case, you can focus on deploying the PKI necessary
    to support this feature. No need to worry about setting up a tracing stack or
    spending development cycles testing traffic routing and management.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的事情之一是优先考虑服务网格的某一个支柱。这样做将使您能够从实施和测试的角度缩小范围。根据您的需求（如果您正在采用服务网格，那么您已经建立了需求，对吧？），例如，您可能会将双向TLS作为第一个支柱优先考虑。在这种情况下，您可以专注于部署支持此功能所需的PKI。无需担心设置跟踪堆栈或花费开发周期测试流量路由和管理。
- en: Focusing on one of the pillars enables you to learn about the mesh, understand
    how it behaves in your platform, and gain operational expertise. Once you feel
    comfortable, you can implement additional pillars, as necessary. In essence, you
    will be more successful if you follow a piecemeal deployment instead of a big-bang
    implementation.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点放在其中一个支柱上使您能够了解网格在您的平台上的行为，并获得运营专业知识。一旦您感到舒适，可以根据需要实施额外的支柱。实际上，如果您采用分阶段部署而不是一次性实施，您将更加成功。
- en: Deploy to a new or an existing cluster?
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署到新集群还是现有集群？
- en: Depending on your platform’s life cycle and topology, you might have a choice
    between deploying the service mesh to a new, fresh cluster or adding it to an
    existing cluster. When possible, prefer going down the new cluster route. This
    eliminates any potential disruption to applications that would otherwise be running
    in an existing cluster. If your clusters are ephemeral, deploying the service
    mesh to a new cluster should be a natural path to follow.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的平台生命周期和拓扑结构，您可能可以选择将服务网格部署到新的、全新的集群，或者将其添加到现有集群中。在可能的情况下，最好选择新集群路线。这样可以消除对正在现有集群中运行的应用程序可能造成的任何潜在干扰。如果您的集群是短暂存在的，将服务网格部署到新集群应该是一个自然的选择。
- en: 'In situations where you must introduce the service mesh into an existing cluster,
    make sure to perform extensive testing in your development and testing tiers.
    More importantly, offer an onboarding window that allows development teams to
    experiment and test their services with the mesh before rolling it out to the
    staging and production tiers. Finally, provide a mechanism that allows applications
    to opt into being part of the mesh. A common way to enable the opt-in mechanism
    is to provide a Pod annotation. Istio, for example, provides an annotation (`sidecar.istio.io/inject`)
    that determines whether the platform should inject the sidecar proxy into the
    workload, which is visible in the following snippet:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在必须将服务网格引入现有集群的情况下，请务必在开发和测试层进行广泛测试。更重要的是，在上线到暂存和生产层之前，提供一个允许开发团队实验和测试其服务与网格配合使用的入门窗口。最后，提供一个机制，允许应用程序选择加入网格。启用选择加入机制的常见方式是提供一个Pod注释。例如，Istio提供了一个注释（`sidecar.istio.io/inject`），该注释决定平台是否应该将sidecar代理注入工作负载中，如下片段中所示：
- en: '[PRE27]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Handling upgrades
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理升级
- en: When offering a service mesh as part of your platform, you must have a solid
    upgrade strategy in place. Keep in mind that the service mesh data plane is in
    the critical path that connects your services, including your cluster’s edge (regardless
    of whether you are using the mesh’s Ingress gateway or another Ingress controller).
    What happens when there’s a CVE that affects the mesh’s proxy? How will you handle
    upgrades effectively? Do not adopt a service mesh without understanding these
    concerns and having a well-established upgrade strategy.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在将服务网格作为平台的一部分提供时，您必须制定一个稳固的升级策略。请记住，服务网格数据平面处于连接您的服务的关键路径上，包括您集群的边缘（无论您使用的是网格的入口网关还是其他入口控制器）。当存在影响网格代理的CVE时会发生什么？您如何有效地处理升级？不要在不了解这些问题并没有建立良好升级策略的情况下采用服务网格。
- en: The upgrade strategy must account for both the control plane and the data plane.
    The control plane upgrade carries less risk, as the mesh’s data plane should continue
    to function without it. With that said, do not discount control plane upgrades.
    You should understand the version compatibility between the control plane and
    the data plane. If possible, follow a canary upgrade pattern, as recommended by
    the [Istio project](https://oreil.ly/TZj7F). Also make sure to review any service
    mesh Custom Resource Definition (CRD) changes and whether they impact your services.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 升级策略必须考虑到控制平面和数据平面。控制平面升级的风险较小，因为网格的数据平面应该继续在没有控制平面的情况下运行。话虽如此，不要忽略控制平面升级。你应该理解控制平面与数据平面的版本兼容性。如果可能，按照
    [Istio 项目](https://oreil.ly/TZj7F)推荐的暗能升级模式进行操作。同时，请确保检查任何服务网格自定义资源定义（CRD）更改及其是否会影响你的服务。
- en: The data plane upgrade is more involved, given the number of proxies running
    on the platform and the fact that the proxy is handling service traffic. When
    the proxy runs as a sidecar, the entire Pod must be re-created to upgrade the
    proxy as Kubernetes doesn’t support in-place upgrades of containers. Whether you
    do a full data plane upgrade or a slow rollout of the new data plane proxy depends
    on the reason behind the upgrade. One one hand, if you are upgrading the data
    plane to handle a vulnerability in the proxy, you must re-create every single
    Pod that participates in the mesh to address the vulnerability. As you can imagine,
    this can be disruptive to some applications. If, on the other hand, you are upgrading
    to take advantage of new features or bug fixes, you can let the new version of
    the proxy roll out as Pods are created or moved around in the cluster. This slower,
    less disruptive upgrade results in version sprawl of the proxy, which may be acceptable
    as long as the service mesh supports it. Regardless of why you are upgrading,
    always use your development and testing tiers to practice and validate service
    mesh upgrades.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 由于平台上运行的代理数量较多，并且代理正在处理服务流量，因此数据平面升级更为复杂。当代理作为侧卡运行时，为了升级代理，必须重新创建整个 Pod，因为 Kubernetes
    不支持容器就地升级。无论是进行完整的数据平面升级还是新数据平面代理的缓慢滚动升级，取决于升级背后的原因。如果你升级数据平面以处理代理中的漏洞，你必须重新创建参与网格的每个
    Pod，以解决漏洞。正如你所想的，这可能对某些应用程序造成干扰。如果另一方面，你升级是为了利用新的功能或修复漏洞，可以让代理的新版本在 Pod 创建或移动到集群中时逐步推出。这种较慢的且不具破坏性的升级导致了代理的版本增长，只要服务网格支持，这可能是可以接受的。无论你是出于何种原因进行升级，总是要利用开发和测试层练习和验证服务网格升级。
- en: Another thing to keep in mind is that meshes typically have a narrow set of
    Kubernetes versions they can support. How does a Kubernetes upgrade affect your
    service mesh? Does leveraging a service mesh hinder your ability to upgrade Kubernetes
    as soon as a new version is released? Given that Kubernetes APIs are relatively
    stable, this should not be the case. However, it is possible and something to
    keep in mind.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要牢记的是，网格通常仅支持一系列 Kubernetes 版本。Kubernetes 升级如何影响你的服务网格？使用服务网格会妨碍你尽早升级 Kubernetes
    吗？考虑到 Kubernetes API 相对稳定，这应该不会发生。然而，这是可能的，并且需要注意。
- en: Resource overhead
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源开销
- en: One of the primary trade-offs of using a service mesh is the resource overhead
    that it carries, especially in the sidecar architecture. As we’ve discussed, the
    service mesh injects a proxy into each Pod in the cluster. To get its job done,
    the proxy consumes resources (CPU and memory) that would otherwise be available
    to other services. When adopting a service mesh, you must understand this overhead
    and whether the trade-off is worth it. If you are running the cluster in a datacenter,
    the overhead is probably palatable. However, the overhead might prevent you from
    using a service mesh in edge deployments where resource constraints are tighter.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务网格的一个主要权衡因素是它所携带的资源开销，尤其是在侧卡架构下。正如我们讨论过的，服务网格会向集群中的每个 Pod 注入一个代理。为了完成工作，代理消耗了其他服务可用的资源（CPU
    和内存）。在采用服务网格时，你必须理解这种开销，是否值得进行权衡。如果你在数据中心运行集群，那么这个开销可能是可接受的。然而，这种开销可能会阻止你在资源约束更为紧张的边缘部署中使用服务网格。
- en: Perhaps more important, a service mesh introduces latency between services given
    that the service calls are traversing a proxy on both the source and the destination
    services. While the proxies used in service meshes are usually highly performant,
    it is important to understand the latency overhead they introduce and whether
    your application can function given the overhead.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，服务网格在服务之间引入了延迟，因为服务调用在源服务和目标服务上都经过代理。虽然服务网格中使用的代理通常性能很高，但理解它们引入的延迟开销以及您的应用程序能否在这种开销下运行是非常重要的。
- en: When evaluating a service mesh, spend time investigating its resource overhead.
    Even better, run performance tests with your services to understand how the mesh
    behaves under load.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估服务网格时，花些时间调查其资源开销是非常重要的。更好的方法是通过与您的服务一起运行性能测试，了解在负载下网格的行为。
- en: Certificate Authority for mutual TLS
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于互相 TLS 的证书颁发机构
- en: The identity features of a service mesh are usually based on X.509 certificates.
    Proxies in the mesh use these certificates to establish mutual TLS (mTLS) connections
    between services.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格的身份特性通常基于 X.509 证书。网格中的代理使用这些证书在服务之间建立互相 TLS（mTLS）连接。
- en: Before being able to leverage the mTLS features of a service mesh, you must
    establish a certificate management strategy. While the mesh is usually responsible
    for minting the service certificates, it is up to you to determine the Certificate
    Authority (CA). In most cases, a service mesh uses a self-signed certificate as
    the CA. However, mature service meshes allow you to bring your own CA, if necessary.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够利用服务网格的 mTLS 功能之前，您必须建立证书管理策略。虽然网格通常负责签发服务证书，但确定证书颁发机构（CA）是由您决定的。在大多数情况下，服务网格使用自签名证书作为
    CA。但是，成熟的服务网格允许您在必要时使用自己的 CA。
- en: Because the service mesh handles service-to-service communications, using a
    self-signed CA is adequate. The CA is essentially an implementation detail that
    is invisible to your applications and their clients. With that said, security
    teams can disapprove of the use of self-signed CAs. When adopting a service mesh,
    make sure to bring your security team into the conversation.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 由于服务网格处理服务间的通信，使用自签名 CA 是足够的。CA 本质上是对应用程序及其客户端不可见的实现细节。尽管如此，安全团队可能不赞成使用自签名 CA。在采用服务网格时，请确保将安全团队引入讨论。
- en: If using a self-signed CA for mTLS is not viable, you will have to provide a
    CA certificate and key that the service mesh can use to mint certificates. Alternatively,
    you can integrate with an external CA, such as Vault, when an integration is available.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用自签名 CA 来进行 mTLS 不可行，您将不得不提供一个 CA 证书和密钥，以供服务网格用于签发证书。或者，在集成可用时，您可以与 Vault
    等外部 CA 进行集成。
- en: Multicluster service mesh
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多集群服务网格
- en: Some service meshes offer multicluster capabilities that you can use to extend
    the mesh across multiple Kubernetes clusters. The goal of these capabilities is
    to connect services running in different clusters through a secure channel that
    is transparent to the application. Multicluster meshes increase the complexity
    of your platform. They can have both performance and fault-domain implications
    that developers might have to be aware of. In any case, while creating multicluster
    meshes might seem attractive, you should avoid them until you gain the operational
    knowledge to run a service mesh successfully within a single cluster.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 一些服务网格提供多集群功能，您可以使用这些功能将网格扩展到多个 Kubernetes 集群中。这些功能的目标是通过一个对应用程序透明的安全通道连接运行在不同集群中的服务。多集群网格增加了平台的复杂性。它们可能会对性能和故障域有影响，开发人员可能需要了解这些影响。总之，虽然创建多集群网格可能看起来很有吸引力，但在成功在单个集群中运行服务网格之前，应该避免使用它们。
- en: Summary
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: Service routing is a crucial concern when building an application platform atop
    Kubernetes. Services provide layer 3/4 routing and load balancing capabilities
    to applications. They enable applications to communicate with other services in
    the cluster without worrying about changing Pod IPs or failing cluster nodes.
    Furthermore, developers can use NodePort and LoadBalancer Services to expose their
    applications to clients outside of the cluster.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建基于 Kubernetes 的应用程序平台时，服务路由是一个关键问题。服务提供了第三/四层的路由和负载均衡功能给应用程序。它们使应用程序能够与集群中的其他服务通信，而无需担心
    Pod IP 的变化或者集群节点的故障。此外，开发人员可以使用 NodePort 和 LoadBalancer 服务将其应用程序暴露给集群外的客户端。
- en: Ingress builds on top of Services to provide richer routing capabilities. Developers
    can use the Ingress API to route traffic according to application-level concerns,
    such as the `Host` header of the request or the path that the client is trying
    to reach. The Ingress API is satisfied by an Ingress controller, which you must
    deploy before using Ingress resources. Once installed, the Ingress controller
    handles incoming requests and routes them according to the Ingress configuration
    defined in the API.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 构建在服务之上，以提供更丰富的路由能力。开发人员可以使用 Ingress API 根据应用级别的关注点来路由流量，例如请求的 `Host`
    头或客户端尝试访问的路径。在使用 Ingress 资源之前，您必须部署 Ingress 控制器来满足 Ingress API 的需求。一旦安装完成，Ingress
    控制器将处理传入的请求，并根据 API 中定义的 Ingress 配置来路由这些请求。
- en: If you have a large portfolio of microservices-based applications, your developers
    might benefit from a service mesh’s capabilities. When using a service mesh, services
    communicate with each other through proxies that augment the interaction. Service
    meshes can provide a variety of features, including traffic management, mutual
    TLS, access control, automated service metrics gathering, and more. Like other
    interfaces in the Kubernetes ecosystem, the Service Mesh Interface (SMI) aims
    to enable platform operators to use a service mesh without tying themselves to
    specific implementations. However, before adopting a service mesh, ensure that
    you have the operational expertise in your team to operate an additional distributed
    system on top of Kubernetes.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您拥有基于微服务的大型应用组合，您的开发人员可能会从服务网格的能力中受益。在使用服务网格时，服务通过增强交互的代理彼此通信。服务网格可以提供多种功能，包括流量管理、双向
    TLS、访问控制、自动服务指标收集等。与 Kubernetes 生态系统中的其他接口一样，服务网格接口（SMI）旨在使平台运营商能够使用服务网格而不将自己限制在特定的实现上。然而，在采用服务网格之前，请确保您的团队具备操作经验，以管理
    Kubernetes 之上的额外分布式系统。
