- en: Chapter 8\. Resource Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focus on the best practices for managing and optimizing
    Kubernetes resources. We discuss workload scheduling, cluster management, pod
    resource management, namespace management, and scaling applications. We also dive
    into some of the advanced scheduling techniques that Kubernetes provides through
    affinity, anti-affinity, taints, tolerations, and nodeSelectors.
  prefs: []
  type: TYPE_NORMAL
- en: We show you how to implement resource limits, resource requests, pod Quality
    of Service, `PodDisruptionBudget`s, `LimitRanger`s, and anti-affinity policies.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes scheduler is one of the main components that is hosted in the
    control plane. The scheduler allows Kubernetes to make placement decisions for
    pods deployed to the cluster. It deals with optimization of resources based on
    constraints of the cluster as well as user-specified constraints. It uses a scoring
    algorithm that is based on predicates and priorities.
  prefs: []
  type: TYPE_NORMAL
- en: Predicates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first function Kubernetes uses to make a scheduling decision is the predicate
    function, which determines what nodes the pods can be scheduled on. It implies
    a hard constraint, so it returns a value of true or false. An example would be
    when a pod requests 4 GB of memory and a node cannot satisfy this requirement.
    The node would return a false value and would be removed from viable nodes for
    the pod to be scheduled to. Another example would be if the node is set to unschedulable;
    it would then be removed from the scheduling decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheduler checks the predicates based on order of restrictiveness and complexity.
    As of this writing, the following are the predicates that the scheduler checks
    for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Priorities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whereas predicates indicate a true or false value and dismiss a node for scheduling,
    the priority value ranks all the valid nodes based on a relative value. The following
    priorities are scored for nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The scores will be added, and then a node is given its final score to indicate
    its priority. For example, if a pod requires 600 millicores and there are two
    nodes, one with 900 millicores available and one with 1,800 millicores, the node
    with 1,800 millicores available will have a higher priority.
  prefs: []
  type: TYPE_NORMAL
- en: If nodes are returned with the same priority, the scheduler will use a `selectHost()`
    function, which selects a node in a round-robin fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Scheduling Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For most cases, Kubernetes does a good job of optimally scheduling pods for
    you. It takes into account pods that are placed only on nodes that have sufficient
    resources. It also tries to spread pods from the same ReplicaSet across nodes
    to increase availability and will balance resource utilization. When this is not
    good enough, Kubernetes gives you the flexibility to influence how resources are
    scheduled. For example, you might want to schedule pods across availability zones
    to mitigate a zonal failure causing downtime to your application. You might also
    want to colocate pods to a specific host for performance benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Affinity and Anti-Affinity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pod affinity and anti-affinity let you set rules to place pods relative to other
    pods. These rules allow you to modify the scheduling behavior and override the
    scheduler’s placement decisions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an anti-affinity rule would allow you to spread pods from a ReplicaSet
    across multiple datacenter zones. It does this by utilizing keylabels set on the
    pods. Setting the key/value pairs instructs the scheduler to schedule the pods
    on the same node (affinity) or prevent the pods from scheduling on the same nodes
    (anti-affinity).
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an example of setting a pod anti-affinity rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This manifest of an NGINX deployment has four replicas and the selector label
    `app=frontend`. The deployment has a PodAntiAffinity stanza configured that will
    ensure that the scheduler does not colocate replicas on a single node. This ensures
    that if a node fails, there are still enough replicas of NGINX to serve data from
    its cache.
  prefs: []
  type: TYPE_NORMAL
- en: nodeSelector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A nodeSelector is the easiest way to schedule pods to a particular node. It
    uses label selectors with key/value pairs to make the scheduling decision. For
    example, you might want to schedule pods to a specific node that has specialized
    hardware, such as a GPU. You might ask, “Can’t I do this with a node taint?” The
    answer is, yes, you can. The difference is that you use a nodeSelector when you
    want to *request* a GPU-enabled node, whereas a taint *reserves* a node for only
    GPU workloads. You can use both node taints and nodeSelectors together to reserve
    the nodes for only GPU workloads, and use the nodeSelector to automatically select
    a node with a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an example of labeling a node and using a nodeSelector in the
    pod specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s create a pod specification with a nodeSelector key/value of `disktype:
    ssd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the nodeSelector schedules the pod to only nodes that have the label
    `disktype=ssd`:'
  prefs: []
  type: TYPE_NORMAL
- en: Taints and Tolerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Taints* are used on nodes to repel pods from being scheduled on them. But
    isn’t that what anti-affinity is for? Yes, but taints take a different approach
    than pod anti-affinity and serve a different use case. For example, you might
    have pods that require a specific performance profile, and you do not want to
    schedule any other pods to the specific node. Taints work in conjunction with
    *tolerations*, which allow you to override tainted nodes. The combination of the
    two gives you fine-grained control over anti-affinity rules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, you will use taints and tolerations for the following use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Specialized node hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dedicated node resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding degraded nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple taint types affect scheduling and running containers:'
  prefs: []
  type: TYPE_NORMAL
- en: NoSchedule
  prefs: []
  type: TYPE_NORMAL
- en: A hard taint that prevents scheduling on the node
  prefs: []
  type: TYPE_NORMAL
- en: PreferNoSchedule
  prefs: []
  type: TYPE_NORMAL
- en: Schedules only if pods cannot be scheduled on other nodes
  prefs: []
  type: TYPE_NORMAL
- en: NoExecute
  prefs: []
  type: TYPE_NORMAL
- en: Evicts pods already running on the node
  prefs: []
  type: TYPE_NORMAL
- en: NodeCondition
  prefs: []
  type: TYPE_NORMAL
- en: Taints a node if it meets a specific condition
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-1](#kubernetes_taints_and_tolerations) shows an example of a node
    that is tainted with `gpu=true:NoSchedule`. Pod Spec 1 has a toleration key with
    `gpu`, so it will be scheduled to the tainted node. Pod Spec 2 has a toleration
    key of `no-gpu`, so it will not be scheduled to the node.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes taints and tolerations](assets/kbp2_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Kubernetes taints and tolerations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When a pod cannot be scheduled due to tainted nodes, you’ll see an error message
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve seen how we can manually add taints to affect scheduling, there
    is also the powerful concept of *taint-based eviction*, which allows the eviction
    of running pods. For example, if a node becomes unhealthy due to a bad disk drive,
    the taint-based eviction can reschedule the pods on the host to another healthy
    node in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Resource Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important aspects of managing applications in Kubernetes is
    appropriately managing pod resources. Managing pod resources consists of managing
    CPU and memory to optimize the overall utilization of your Kubernetes cluster.
    You can manage these resources at the container level and at the namespace level.
    There are other resources, such as network and storage, but Kubernetes doesn’t
    yet have a way to set requests and limits for those resources.
  prefs: []
  type: TYPE_NORMAL
- en: For the scheduler to optimize resources and make intelligent placement decisions,
    it needs to understand the requirements of an application. As an example, if a
    container (application) needs a minimum of 2 GB to perform, we need to define
    this in our pod specification so the scheduler knows that the container requires
    2 GB of memory on the host to which it schedules the container.
  prefs: []
  type: TYPE_NORMAL
- en: Resource Request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Kubernetes resource *request* defines that a container requires *X* amount
    of CPU or memory to be scheduled. If you were to specify in the pod specification
    that a container requires 8 GB for its resource request and all your nodes have
    7.5 GB of memory, the pod would not be scheduled. If the pod is not able to be
    scheduled, it will go into a *pending* state until the required resources are
    available. So let’s look at how this works in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine the available free resources in your cluster, use `kubectl top`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look like this (the memory size might be different for your
    cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As this example shows, the largest amount of memory available to a host is
    7,500 Mi, so let’s schedule a pod that requests 8,000 Mi of memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the pod will stay pending, and if you look at the events on the
    pods, you’ll see that no nodes are available to schedule the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the event should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Resource Limits and Pod Quality of Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes resource *limits* define the maximum CPU or memory that a pod is
    given. When you specify limits for CPU and memory, each takes a different action
    when it reaches the specified limit. With CPU limits, the container is throttled
    from using more than its specified limit. With memory limits, the pod is restarted
    if it reaches its limit. The pod might be restarted on the same host or a different
    host within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifying limits for containers is a best practice to ensure that applications
    are allotted their fair share of resources within the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When a pod is created, it’s assigned one of the following Quality of Service
    (QoS) classes:'
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burstable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best effort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pod is assigned a QoS of *guaranteed* when CPU and memory both have request
    and limits that match. A *burstable* QoS is when the limits are set higher than
    the request, meaning that the container is guaranteed its request, but it can
    also burst to the limit set for the container. A pod is assigned *best effort*
    when no request or limits are set for the containers in the pod.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-2](#kubernetes_qos) depicts how QoS is assigned to pods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes QoS](assets/kbp2_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Kubernetes QoS
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With guaranteed QoS, if you have multiple containers in your pod, you’ll need
    to have memory request and limits set for each container, and you’ll also need
    CPU request and limits set for each container. If the request and limits are not
    set for all containers, they will not be assigned guaranteed QoS.
  prefs: []
  type: TYPE_NORMAL
- en: PodDisruptionBudgets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At some point in time, Kubernetes might need to *evict* pods from a host. There
    are two types of evictions: *voluntary* and *involuntary* disruptions. Involuntary
    disruptions can be caused by hardware failure, network partitions, kernel panics,
    or a node being out of resources. Voluntary evictions can be caused by performing
    maintenance on the cluster, the Cluster Autoscaler deallocating nodes, or updating
    pod templates. To minimize the impact to your application, you can set a `PodDisruptionBudget`
    to ensure uptime of the application when pods need to be evicted. A `PodDisruptionBudget`
    allows you to set a policy on the minimum available and maximum unavailable pods
    during voluntary eviction events. An example of a voluntary eviction would be
    when draining a node to perform maintenance on the node.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might specify that no more than 20% of pods belonging to your
    application can be down at a given time. You could also specify this policy in
    terms of *X* number of replicas that must always be available.
  prefs: []
  type: TYPE_NORMAL
- en: Minimum available
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following example, we set a `PodDisruptionBudget` to handle a minimum
    available to 5 for app: frontend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `PodDisruptionBudget` specifies that for the frontend app
    there must always be five replica pods available at any given time. In this scenario,
    an eviction can evict as many pods as it wants, as long as five are available.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum unavailable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the next example, we set a `PodDisruptionBudget` to handle a maximum unavailable
    to 20% for the frontend app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `PodDisruptionBudget` specifies that no more than 20% of
    replica pods can be unavailable at any given time. In this scenario, an eviction
    can evict a maximum of 20% of pods during a voluntary disruption.
  prefs: []
  type: TYPE_NORMAL
- en: It’s essential that when designing your Kubernetes cluster you think about the
    sizing of the cluster resources so that you can handle a number of failed nodes.
    For example, if you have a four-node cluster and one node fails, you will be losing
    a quarter of your cluster capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When specifying a `PodDisruptionBudget` as a percentage, it might not correlate
    to a specific number of pods. For example, if your application has seven pods
    and you specify `maxAvailable` to `50%`, it’s not clear whether that is three
    or four pods. In this case, Kubernetes rounds up to the closest integer, so the
    `maxAvailable` would be four pods.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Resources by Using Namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Namespaces* in Kubernetes give you a nice logical separation of resources
    deployed to a cluster. This allows you to set resource quotas per namespace, Role-Based
    Access Control (RBAC) per namespace, and also network policies per namespace.
    It gives you soft multitenancy features so you can separate out workloads in a
    cluster without dedicating specific infrastructure to a team or application. This
    allows you to get the most out of your cluster resource while also maintaining
    a logical form of separation.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could create a namespace per team and give each team a quota
    on the number of resources that it can utilize, such as CPU and memory.
  prefs: []
  type: TYPE_NORMAL
- en: When designing how you want to configure a namespace, you should think about
    how you want to control access to a specific set of applications. If you have
    multiple teams that will be using a single cluster, it is typically best to allocate
    a namespace to each team. If the cluster is dedicated to only one team, it might
    make sense to allocate a namespace for each service deployed to the cluster. There’s
    no single solution to this; your team organization and responsibilities will drive
    the design.
  prefs: []
  type: TYPE_NORMAL
- en: 'After deploying a Kubernetes cluster, you’ll see the following namespaces in
    your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-system`'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes internal components are deployed here, such as `coredns`, `kube-proxy`,
    and `metrics-server`.
  prefs: []
  type: TYPE_NORMAL
- en: '`default`'
  prefs: []
  type: TYPE_NORMAL
- en: This is the default namespace that is used when you don’t specify a namespace
    in the resource object.
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-public`'
  prefs: []
  type: TYPE_NORMAL
- en: Used for anonymous and unauthenticated content, and reserved for system usage.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll want to avoid using the default namespace because users are not mandated
    to deploy applications within specific resource constraints, and it can lead to
    resource contention. You should also avoid using the `kube-system` namespace for
    your applications because it is used for Kubernetes internal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with namespaces, you need to use the `–namespace` flag, or `-n`
    for short, when working with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also set your `kubectl` context to a specific namespace, which is useful
    so that you don’t need to add the `–namespace` flag with every command. You can
    set your namespace context by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When dealing with multiple namespaces and clusters, it can be a pain to set
    different namespaces and cluster context. We’ve found that using [kubens](https://oreil.ly/ryavL)
    and [kubectx](https://oreil.ly/kVBiL) can help make it easy to switch between
    these different namespaces and contexts.
  prefs: []
  type: TYPE_NORMAL
- en: ResourceQuota
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When multiple teams or applications share a single cluster, it’s important
    to set up `ResourceQuota`s on your namespaces. `ResourceQuota`s allow you to divvy
    up the cluster in logical units so that no single namespace can consume more than
    its share of resources in the cluster. The following resources can have a quota
    set for them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute resources:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.cpu`: Sum of CPU requests cannot exceed this amount'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.cpu`: Sum of CPU limits cannot exceed this amount'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.memory`: Sum of memory requests cannot exceed this amount'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limit.memory`: Sum of memory limits cannot exceed this amount'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storage resources:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.storage`: Sum of storage requests cannot exceed this value'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persistentvolumeclaims`: The total number of PersistentVolume claims that
    can exist in the namespace'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storageclass.request`: Volume claims associated with the specified storage-class
    cannot exceed this value'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storageclass.pvc`: The total number of PersistentVolume claims that can exist
    in the namespace'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Object count quotas (only an example set):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: count/pvc
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: count/services
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: count/deployments
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: count/replicasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see from this list, Kubernetes gives you fine-grained control over
    how you carve up resource quotas per namespace. This allows you to more efficiently
    operate resource usage in a multitenant cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how these quotas actually work by setting up a quota on a namespace.
    Apply the following YAML file to the `team-1` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This example sets quotas for CPU, memory, and storage on the `team-1` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s try to deploy an application to see how the resource quotas affect
    the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This deployment will fail with the following error due to the memory quota
    exceeding 2 Gi of memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As this example demonstrates, setting resource quotas can let you deny deployment
    of resources based on policies you set for the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: LimitRange
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve discussed setting `request` and `limits` at the container level, but what
    happens if the user forgets to set these in the pod specification? Kubernetes
    provides an admission controller that allows you to automatically set these when
    none are indicated in the specification.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a namespace to work with quotas and `LimitRange`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply a `LimitRange` to the namespace to apply `defaultRequest` in `limits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Save this to *limitranger.yaml* and then run `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the `LimitRange` applies default limits and requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s describe the pod to see what requests and limits were set on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following requests and limits set on the pod specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to use `LimitRange` when using `ResourceQuota`s, because if no
    request or limits are set in the specification, the deployment will be rejected.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first decisions you need to make when deploying a cluster is the
    instance size you’ll want to use within your cluster. This becomes more of an
    art than science, especially when you’re mixing workloads in a single cluster.
    You’ll first want to identify a good starting point for the cluster; aiming for
    a good balance of CPU and memory is one option. After you’ve decided on a sensible
    size for the cluster, you can use a couple of Kubernetes core primitives to manage
    the scaling of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Manual scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes makes it easy to scale your cluster, especially if you’re using tools
    like Kops or a managed Kubernetes offering. Scaling your cluster manually is typically
    just choosing a new number of nodes, and the service will add the new nodes to
    your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: These tools also allow you to create node pools, which allows you to add new
    instance types to an already running cluster. This becomes very useful when running
    mixed workloads within a single cluster. For example, one workload might be more
    CPU driven, whereas the other workloads might be memory-driven applications. Node
    pools allow you to mix multiple instance types within a single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: But perhaps you don’t want to manually do this and want it to autoscale. There
    are things that you need to take into consideration with cluster autoscaling,
    and we have found that most users are better off starting with just manually scaling
    their nodes proactively when resources are needed. If your workloads are highly
    variable, cluster autoscaling can be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster autoscaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes provides a Cluster Autoscaler add-on that allows you to set the minimum
    nodes available to a cluster and also the maximum number of nodes to which your
    cluster can scale. The Cluster Autoscaler bases its scale decision on when a pod
    goes pending. For example, if the Kubernetes scheduler tries to schedule a pod
    with a memory request of 4,000 Mib and the cluster has only 2,000 Mib available,
    the pod will go into a pending state. After the pod is pending, the Cluster Autoscaler
    will add a node to the cluster. As soon as the new node is added to the cluster,
    the pending pod is scheduled to the node. The downside of the Cluster Autoscaler
    is that a new node is added only before a pod goes pending, so your workload may
    end up waiting for a new node to come online when it is scheduled. As of Kubernetes
    v1.15, the Cluster Autoscaler doesn’t support scaling based on custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The Cluster Autoscaler can also reduce the size of the cluster after resources
    are no longer needed. When the resources are no longer needed, it will drain the
    node and reschedule the pods to new nodes in the cluster. You’ll want to use a
    `PodDisruptionBudget` to ensure that you don’t negatively affect your application
    when it performs its drain operation to remove the node from the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Application Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes provides multiple ways to scale applications in your cluster. You
    can scale an application by manually changing the number of replicas within a
    deployment. You can also change the ReplicaSet or replication controller, but
    we don’t recommend managing your applications through those implementations. Manual
    scaling is perfectly fine for workloads that are static or when you know the times
    that the workload spikes, but for workloads that experience sudden spikes or workloads
    that are not static, manual scaling is not ideal for the application. Happily,
    Kubernetes also provides a Horizontal Pod Autoscaler (HPA) to automatically scale
    workloads for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first look at how you can manually scale a deployment by applying the
    following Deployment manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This example deploys three replicas of our frontend service. We then can scale
    this deployment by using the `kubectl scale` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This results in five replicas of our frontend service. This is great, but let’s
    look at how we can add some intelligence and automatically scale the application
    based on metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with HPA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kubernetes HPA allows you to scale your deployments based on CPU, memory,
    or custom metrics. It performs a watch on the deployment and pulls metrics from
    the Kubernetes `metrics-server`. It also allows you to set the minimum and maximum
    number of pods available. For example, you can define an HPA policy that sets
    the minimum number of pods to 3 and the maximum number of pods to 10, and it scales
    when the deployment reaches 80% CPU usage. Setting the minimum and maximum is
    critical because you don’t want the HPA to scale the replicas to an infinite amount
    due to an application bug or issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HPA has the following default setting for sync metrics, upscaling, and
    downscaling replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '`horizontal-pod-autoscaler-sync-period`'
  prefs: []
  type: TYPE_NORMAL
- en: Default of 30 seconds for syncing metrics
  prefs: []
  type: TYPE_NORMAL
- en: '`horizontal-pod-autoscaler-upscale-delay`'
  prefs: []
  type: TYPE_NORMAL
- en: Default of three minutes between two upscale operations
  prefs: []
  type: TYPE_NORMAL
- en: '`horizontal-pod-autoscaler-downscale-delay`'
  prefs: []
  type: TYPE_NORMAL
- en: Default of five minutes between two downscale operations
  prefs: []
  type: TYPE_NORMAL
- en: You can change the defaults by using their relative flags, but you need to be
    careful when doing so. If your workload is extremely variable, it’s worth playing
    around with the settings to optimize them for your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go ahead and set up an HPA policy for the frontend application you deployed
    in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, expose the deployment on port 80:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, set the autoscale policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This sets the policy to scale your app from a minimum of 1 replica to a maximum
    of 10 replicas and will invoke the scale operation when the CPU load reaches 50%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate some load so that we can see the deployment autoscale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You might need to wait a few minutes to see the replicas scale up automatically.
  prefs: []
  type: TYPE_NORMAL
- en: HPA with Custom Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#configuration_secrets_and_rbac), we introduced the
    role that the metrics server plays in monitoring our systems in Kubernetes. With
    the Metrics Server API, we can also support scaling our applications with custom
    metrics. The Custom Metrics API and Metrics Aggregator allow third-party providers
    to plug in and extend the metrics, and HPA can then scale based on these external
    metrics. For example, instead of just basic CPU and memory metrics, you could
    scale based on a metric you’re collecting on an external storage queue. By utilizing
    custom metrics for autoscaling, you have the ability to scale application-specific
    metrics or external service metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical Pod Autoscaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Vertical Pod Autoscaler (VPA) differs from the HPA in that it doesn’t scale
    replicas; instead, it automatically scales requests. Earlier in the chapter, we
    talked about setting requests on our pods and how that guarantees *X* amount of
    resources for a given container. The VPA frees you from manually adjusting these
    requests and automatically scales up and scales down pod requests for you. For
    workloads that can’t scale out due to their architecture, this works well for
    automatically scaling the resources. For example, a MySQL database doesn’t scale
    the same way as a stateless web frontend. With MySQL, you might want to set the
    Master nodes to automatically scale up based on workload.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VPA is more complex than the HPA, and it consists of three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recommender`'
  prefs: []
  type: TYPE_NORMAL
- en: Monitors the current and past resource consumption, and provides recommended
    values for the container’s CPU and memory requests
  prefs: []
  type: TYPE_NORMAL
- en: '`Updater`'
  prefs: []
  type: TYPE_NORMAL
- en: Checks which of the pods have the correct resources set, and if they don’t,
    kills them so that they can be re-created by their controllers with the updated
    requests
  prefs: []
  type: TYPE_NORMAL
- en: '`Admission Plugin`'
  prefs: []
  type: TYPE_NORMAL
- en: Sets the correct resource requests on new pods
  prefs: []
  type: TYPE_NORMAL
- en: 'Vertical scaling has two objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the maintenance cost, by automating configuration of resource requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving utilization of cluster resources, while minimizing the risk of containers
    running out of memory or getting CPU starved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource Management Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Utilize pod anti-affinity to spread workloads across multiple availability zones
    to ensure high availability for your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re using specialized hardware, such as GPU-enabled nodes, ensure that
    only workloads that need GPUs are scheduled to those nodes by utilizing taints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `NodeCondition` taints to proactively avoid failing or degraded nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply nodeSelectors to your pod specifications to schedule pods to specialized
    hardware that you have deployed in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before going to production, experiment with different node sizes to find a good
    mix of cost and performance for node types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re deploying a mix of workloads with different performance characteristics,
    utilize node pools to have mixed node types in a single cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you set memory and CPU limits for all pods deployed to your cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize `ResourceQuota`s to ensure that multiple teams or applications are allotted
    their fair share of resources in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement `LimitRange` to set default limits and requests for pod specifications
    that don’t set limits or requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with manual cluster scaling until you understand your workload profiles
    on Kubernetes. You can use autoscaling, but it comes with additional considerations
    around node spin-up time and cluster scale down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the HPA for workloads that are variable and that have unexpected spikes
    in their usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how you can optimally manage Kubernetes and application
    resources. Kubernetes provides many built-in features to manage resources that
    you can use to maintain a reliable, highly utilized, and efficient cluster. Cluster
    and pod sizing can be difficult at first, but through monitoring your applications
    in production you can discover ways to optimize your resources.
  prefs: []
  type: TYPE_NORMAL
