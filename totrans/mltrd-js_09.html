<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Analysis" class="calibre4"><div class="preface" id="ch_benchmarks">
<h1 class="calibre12"><span class="keep-together">Chapter 8. </span>Analysis</h1>


<p class="author1">By now you should be pretty familiar with building multithreaded applications using JavaScript, whether it be code that runs in a user’s browser or your server, or even applications that employ both. And, while this book provides a lot of use cases and reference material, at no point did it say “you should add multithreading to your application,” and there’s an important reason for this.</p>

<p class="author1">By and large the main reason to add workers to an application is to increase performance. But this trade-off comes with a cost of added complexity. The <em class="calibre7">KISS principle</em>, meaning “Keep It Simple, Stupid,” <a data-type="indexterm" data-primary="KISS (Keep It Simple Stupid)" id="idm45995909968856" class="calibre6"/>suggests that your applications should be so stupidly simple that anyone can quickly look at the code and get an understanding of it. Being able to read code after it has been written is of paramount importance and simply adding threads to a program without purpose is an absolute violation of KISS.</p>

<p class="author1">There are absolutely good reasons to add threads to an application, and as long as you’re measuring performance and confirming that speed gains outweigh added maintenance costs, then you’ve found yourself a situation deserving of threads. But how do you identify situations where threads will or will not help without going through all the work of implementing them? And how do you go about measuring performance impact?</p>






<section data-type="sect1" data-pdf-bookmark="When Not to Use" class="calibre4"><div class="preface" id="ch_benchmarks_sec_avoid">
<h1 class="calibre13">When Not to Use</h1>

<p class="author1">Threading is not a magic bullet capable of solving an application’s performance problems. It is usually not the lowest-hanging fruit when it comes to performance, either, and should often be done as a final effort. This is particularly true in JavaScript, where multithreading isn’t as widely understood by the community as other languages. Adding threading support may require heavy changes to an application, which means your effort-to-performance gains will likely be higher if you first hunt down other code inefficiencies first.</p>

<p class="author1">Once that’s done, and you’ve made your application performant in other areas, you are then left with the question, “Is now a good time to add multithreading?” The rest of this section contains some situations where adding threads will most likely not provide any performance benefits. This can help you avoid going through some of the discovery work.</p>








<section data-type="sect2" data-pdf-bookmark="Low Memory Constraints" class="calibre4"><div class="preface" id="idm45995909963464">
<h2 class="calibre37">Low Memory Constraints</h2>

<p class="author1">There is some additional memory overhead <a data-type="indexterm" data-primary="memory" data-secondary="low memory constraints" id="memlowmemcon" class="calibre6"/><a data-type="indexterm" data-primary="low memory constraints" id="lowmemcon" class="calibre6"/>incurred when instantiating multiple threads in JavaScript. This is because the browser needs to allocate additional memory for the new JavaScript environment—this includes things like globals and APIs available to your code as well as under-the-hood memory used by the engine itself. This overhead might prove to be minimal in a normal server environment in the case of Node.js or a beefy laptop in the case of browsers. But it could be a hindrance if you’re running code on an embedded ARM device with 512 MB of RAM or donated netbooks in a K–12 classroom.</p>

<p class="author1">What’s the memory impact of additional threads? It’s a little hard to quantify, and it changes depending on the JavaScript engine and platform. The safe answer is that, like most performance aspects, you should measure it in a real-world environment. But we can certainly try to get some concrete numbers.</p>

<p class="author1">First, let’s consider a dead simple Node.js program <a data-type="indexterm" data-primary="Node.js" data-secondary="timer" id="idm45995909957400" class="calibre6"/>that just kicks off a timer and doesn’t pull in any third-party modules. This program looks like the following:</p>

<pre data-type="programlisting" data-code-language="javascript" class="calibre38"><code class="calibre18">#!/usr/bin/env node</code>

<code class="kr">const</code> <code class="p">{</code> <code class="nx">Worker</code> <code class="p">}</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s">'worker_threads'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">count</code> <code class="o">=</code> <code class="nb">Number</code><code class="p">(</code><code class="nx">process</code><code class="p">.</code><code class="nx">argv</code><code class="p">[</code><code class="mi">2</code><code class="p">])</code> <code class="o">||</code> <code class="mi">0</code><code class="p">;</code>

<code class="kr">for</code> <code class="p">(</code><code class="kr">let</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="nx">count</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
  <code class="kr">new</code> <code class="nx">Worker</code><code class="p">(</code><code class="nx">__dirname</code> <code class="o">+</code> <code class="s">'/worker.js'</code><code class="p">);</code>
<code class="p">}</code>

<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s">`PID: </code><code class="si">${</code><code class="nx">process</code><code class="p">.</code><code class="nx">pid</code><code class="si">}</code><code class="s">, ADD THREADS: </code><code class="si">${</code><code class="nx">count</code><code class="si">}</code><code class="s">`</code><code class="p">);</code>
<code class="nx">setTimeout</code><code class="p">(()</code> <code class="o">=&gt;</code> <code class="p">{},</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">60</code> <code class="o">*</code> <code class="mi">60</code> <code class="o">*</code> <code class="mi">1000</code><code class="p">);</code></pre>

<p class="author1">Running the program and measuring memory usage looks like this:</p>

<pre data-type="programlisting" class="calibre38"># Terminal 1
$ node leader.js 0
# PID 10000

# Terminal 2
$ pstree 10000 -pa # Linux only
$ ps -p 10000 -o pid,vsz,rss,pmem,comm,args</pre>

<p class="author1">The <code class="calibre18">pstree</code> command displays the <a data-type="indexterm" data-primary="pstree command" id="idm45995909857544" class="calibre6"/>threads used by the program. It displays the main V8 JavaScript thread, as well as some of the background threads covered in <a data-type="xref" href="ch01.xhtml#sec_hidden_threads" class="calibre6">“Hidden Threads”</a>. Here is an example output from the command:</p>

<pre data-type="programlisting" class="calibre38">node,10000 ./leader.js
  ├─{node},10001
  ├─{node},10002
  ├─{node},10003
  ├─{node},10004
  ├─{node},10005
  └─{node},10006</pre>

<p class="author1">The <code class="calibre18">ps</code> command displays information <a data-type="indexterm" data-primary="ps command" id="idm45995909853992" class="calibre6"/>about the process, notably the memory usage of the process. Here’s an example of the output from the command:</p>

<pre data-type="programlisting" class="calibre38">  PID    VSZ   RSS  %MEM COMMAND    COMMAND
66766 1409260 48212   0.1 node      node ./leader.js</pre>

<p class="author1">There are two important variables <a data-type="indexterm" data-primary="virtual memory" data-secondary="size" id="idm45995909851624" class="calibre6"/><a data-type="indexterm" data-primary="memory" data-secondary="virtual memory" data-tertiary="size" id="idm45995909850648" class="calibre6"/>here used to measure the memory usage of a program, both of them measured in kilobytes. The first here is <code class="calibre18">VSZ</code>, or <em class="calibre7">virtual memory size</em>, which is the memory the process can access including swapped memory, allocated memory, and even memory used by shared libraries (such as TLS), approximately 1.4 GB. The <a data-type="indexterm" data-primary="memory" data-secondary="resident set size" id="idm45995909848152" class="calibre6"/><a data-type="indexterm" data-primary="physical memory, resident set size" id="idm45995909847176" class="calibre6"/>next is <code class="calibre18">RSS</code>, or <em class="calibre7">resident set size</em>, which is the amount of physical memory currently being used by the process, approximately 48 MB.</p>

<p class="author1">Measuring memory can be a little hand wavy, and it’s tricky to estimate how many processes can actually fit in memory. In this case, we’ll mostly be looking at the RSS value.</p>

<p class="author1">Now, let’s consider a more complicated version of the program using threads. Again, the same dead simple timer will be used, but in this case there will be a total of four threads created. In this case a new <em class="calibre7">worker.js</em> file is required:</p>

<pre data-type="programlisting" data-code-language="javascript" class="calibre38"><code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s">`WPID: </code><code class="si">${</code><code class="nx">process</code><code class="p">.</code><code class="nx">pid</code><code class="si">}</code><code class="s">`</code><code class="p">);</code>
<code class="nx">setTimeout</code><code class="p">(()</code> <code class="o">=&gt;</code> <code class="p">{},</code> <code class="mi">1</code> <code class="o">*</code> <code class="mi">60</code> <code class="o">*</code> <code class="mi">60</code> <code class="o">*</code> <code class="mi">1000</code><code class="p">);</code></pre>

<p class="author1">Running the <em class="calibre7">leader.js</em> program with a numerical argument greater than 0 allows the program to create additional workers. <a data-type="xref" href="#table_thread_overhead" class="calibre6">Table 8-1</a> is a listing of the memory usage output from <code class="calibre18">ps</code> for each of the different iterations of additional threads.</p>
<table id="table_thread_overhead" class="calibre46">
<caption class="calibre47"><span class="keep-together">Table 8-1. </span>Thread memory overhead with Node.js v16.5</caption>
<thead class="calibre48">
<tr class="calibre49">
<th class="calibre50">Add Threads</th><th class="calibre50">VSZ</th><th class="calibre50">RSS</th><th class="calibre50">SIZE</th></tr>
</thead>
<tbody class="calibre51">
<tr class="calibre49">
  <td class="calibre52">0</td> <td class="calibre52">318,124 KB</td> <td class="calibre52">31,836 KB</td> <td class="calibre52">47,876 KB</td>
</tr>
<tr class="calibre53">
  <td class="calibre52">1</td> <td class="calibre52">787,880 KB</td> <td class="calibre52">38,372 KB</td> <td class="calibre52">57,772 KB</td>
</tr>
<tr class="calibre49">
  <td class="calibre52">2</td> <td class="calibre52">990,884 KB</td> <td class="calibre52">45,124 KB</td> <td class="calibre52">68,228 KB</td>
</tr>
<tr class="calibre53">
  <td class="calibre52">4</td> <td class="calibre52">1,401,500 KB</td> <td class="calibre52">56,160 KB</td> <td class="calibre52">87,708 KB</td>
</tr>
<tr class="calibre49">
  <td class="calibre52">8</td> <td class="calibre52">2,222,732 KB</td> <td class="calibre52">78,396 KB</td> <td class="calibre52">126,672 KB</td>
</tr>
<tr class="calibre53">
  <td class="calibre52">16</td> <td class="calibre52">3,866,220 KB</td> <td class="calibre52">122,992 KB</td> <td class="calibre52">205,420 KB</td>
</tr>
</tbody>
</table>

<p class="author1"><a data-type="xref" href="#chart_threads_memory" class="calibre6">Figure 8-1</a> displays the correlation between RSS memory and thread count.</p>

<figure class="calibre29"><div id="chart_threads_memory" class="figure">
<img src="Images/mtjs_0801.png" alt="Comparison of thread count with memory usage" class="calibre71"/>
<h6 class="calibre30"><span class="keep-together">Figure 8-1. </span>Memory usage increases with each additional thread</h6>
</div></figure>

<p class="author1">With this information it appears that the added RSS memory overhead of instantiating each new thread, using Node.js 16.5 on an x86 processor, is approximately 6 MB. Again, this number is a bit hand wavy, and you’ll need to measure it in your particular situation. Of course, the memory overhead is compounded when the threads pull in more modules. If you were to instantiate heavy frameworks and web servers in each thread you <a data-type="indexterm" data-primary="memory" data-secondary="low memory constraints" data-startref="memlowmemcon" id="idm45995909726696" class="calibre6"/><a data-type="indexterm" data-primary="low memory constraints" data-startref="lowmemcon" id="idm45995909725448" class="calibre6"/>may end up adding hundreds of megabytes of memory to your 
<span class="keep-together">process</span>.</p>
<div data-type="warning" epub:type="warning" class="calibre24"><h6 class="calibre25">Warning</h6>
<p class="author1">While it’s becoming increasingly rare to find them, programs running on a 32-bit computer or smart phone have a maximum addressable memory space of 4 GB. This limit is shared across any threads in the program.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Low Core Count" class="calibre4"><div class="preface" id="ch_benchmarks_sec_avoid_ss_lowcore">
<h2 class="calibre37">Low Core Count</h2>

<p class="author1">Your application will run slower in situations <a data-type="indexterm" data-primary="cores, number of" id="corcount" class="calibre6"/>where it has fewer cores. This is especially true if the machine has a single core, and it can also be true if it has two cores. Even if you employ a thread pool in your application and scale the pool based on the core count, the application will be slower if it creates a single worker thread. When creating an additional thread, the application now has at least two threads (the main and the worker), and the two threads will compete with each other for attention.</p>

<p class="author1">Another reason your application will slow down is that there is additional overhead when it comes to communicating between threads. With a single core and two threads, even if the two never compete for resources, i.e., the main thread has no work to do while the worker is running and vice versa, there is still an overhead when performing message passing between the two threads.</p>

<p class="author1">This might not be a huge deal. For example, if you create a distributable application that runs in many environments, often running on multicore systems and infrequently on single-core systems, then this overhead might be OK. But if you’re building an application that almost entirely runs in a single-core environment, you would likely be better off by not adding threading at all. That is, you probably shouldn’t build an app that takes advantage of your beefy multicore developer laptop and then ship it to production where a container orchestrator limits the app to a single core.</p>

<p class="author1">How much of a performance loss are we talking? On the Linux operating system it’s straightforward to tell the OS that a program, and all of its threads, should only run on a subset of CPU cores. The use of this command allows developers to test the effects of running a multithreaded application in a low core environment. If you’re using a Linux-based computer, then feel free to run these examples; if not, a summary will be provided.</p>

<p class="author1">First, go back to the <em class="calibre7">ch6-thread-pool/</em> example that you created in <a data-type="xref" href="ch06.xhtml#ch_patterns_sec_threadpool" class="calibre6">“Thread Pool”</a>. Execute the application so that it creates a worker pool with two workers:</p>

<pre data-type="programlisting" class="calibre38">$ THREADS=2 STRATEGY=leastbusy node main.js</pre>

<p class="author1">Note that with a thread pool of 2, the application has three JavaScript environments available, and <code class="calibre18">libuv</code> should have a default pool of 5, leading to a total of about eight threads as of Node.js v16. With the program running and able to access all of the cores on your machine, you’re ready to run a quick benchmark. Execute the following command to send a barrage of requests to the server:</p>

<pre data-type="programlisting" class="calibre38">$ npx autocannon http://localhost:1337/</pre>

<p class="author1">In this case we’re just interested in the average request rate, identified in the last table of the output with the Req/Sec row and the Avg column. In one sample run the value of 17.5 was returned.</p>

<p class="author1">Kill the server with Ctrl+C and run it again. But this time use the <code class="calibre18">taskset</code> command to force the process (and all of its child threads) to use the same CPU core:</p>

<pre data-type="programlisting" class="calibre38"># Linux only command
$ THREADS=2 STRATEGY=leastbusy taskset -c 0 node main.js</pre>

<p class="author1">In this case the two <a data-type="indexterm" data-primary="THREADS variable" id="idm45995909782536" class="calibre6"/><a data-type="indexterm" data-primary="STRATEGY variable" id="idm45995909781800" class="calibre6"/>environment variables <code class="calibre18">THREADS</code> and <code class="calibre18">STRATEGY</code> are set, then the <code class="calibre18">taskset</code> command is run. The <code class="calibre18">-c 0</code> flag tells the command to only allow the program to use the 0th CPU. The arguments that follow are then treated as the command to run. Note that <a data-type="indexterm" data-primary="taskset command" id="idm45995909779080" class="calibre6"/>the <code class="calibre18">taskset</code> command can also be used to modify an already running process. When that happens the command displays some useful output to tell you what happens. Here’s a copy of that output when the command is used on a computer with 16 cores:</p>

<pre data-type="programlisting" class="calibre38">pid 211154's current affinity list: 0-15
pid 211154's new affinity list: 0</pre>

<p class="author1">In this case it says that the program used to have access to all 16 cores (0–15), but now it only has access to one (0).</p>

<p class="author1">With the program running and locked to a single CPU core to emulate an environment with fewer cores available, run the same benchmark command again:</p>

<pre data-type="programlisting" class="calibre38">$ npx autocannon http://localhost:1337/</pre>

<p class="author1">In one such run the average requests per second has been reduced to 8.32. This means that the throughput of this particular program, when trying to use three JavaScript threads in a single-core environment, leads to a performance of 48% when compared to having access to all cores!</p>

<p class="author1">A natural question might be: in order to maximize the throughput of the <em class="calibre7">ch6-thread-pool</em> application, how large should the thread pool be and how many cores should be provided to the application? To find an answer, 16 permutations of the benchmark were applied to the application and the performance was measured. The length of the test was doubled to two minutes to help reduce any outlying requests. A tabular version of this data is provided in <a data-type="xref" href="#table_core_vs_thread_perf" class="calibre6">Table 8-2</a>.</p>
<table id="table_core_vs_thread_perf" class="calibre46">
<caption class="calibre47"><span class="keep-together">Table 8-2. </span>Available cores versus thread pool size and how it affects throughput</caption>
<thead class="calibre48">
<tr class="calibre49">
<th class="calibre50"/>
<th class="calibre50">1 core</th>
<th class="calibre50">2 cores</th>
<th class="calibre50">3 cores</th>
<th class="calibre50">4 cores</th>
</tr>
</thead>
<tbody class="calibre51">
<tr class="calibre49">
<td class="calibre52"><p class="author1">1 thread</p></td>
<td class="calibre52"><p class="author1">8.46</p></td>
<td class="calibre52"><p class="author1">9.08</p></td>
<td class="calibre52"><p class="author1">9.21</p></td>
<td class="calibre52"><p class="author1">9.19</p></td>
</tr>
<tr class="calibre53">
<td class="calibre52"><p class="author1">2 threads</p></td>
<td class="calibre52"><p class="author1">8.69</p></td>
<td class="calibre52"><p class="author1">9.60</p></td>
<td class="calibre52"><p class="author1">17.61</p></td>
<td class="calibre52"><p class="author1">17.28</p></td>
</tr>
<tr class="calibre49">
<td class="calibre52"><p class="author1">3 threads</p></td>
<td class="calibre52"><p class="author1">8.23</p></td>
<td class="calibre52"><p class="author1">9.38</p></td>
<td class="calibre52"><p class="author1">16.92</p></td>
<td class="calibre52"><p class="author1">16.91</p></td>
</tr>
<tr class="calibre53">
<td class="calibre52"><p class="author1">4 threads</p></td>
<td class="calibre52"><p class="author1">8.47</p></td>
<td class="calibre52"><p class="author1">9.57</p></td>
<td class="calibre52"><p class="author1">17.44</p></td>
<td class="calibre52"><p class="author1">17.75</p></td>
</tr>
</tbody>
</table>

<p class="author1">A graph of the data has been reproduced in <a data-type="xref" href="#graph_core_vs_thread_perf" class="calibre6">Figure 8-2</a>.</p>

<p class="author1">In this case there is an obvious performance benefit when the number of threads dedicated to the thread pool is at least two and the number of cores available to the application is at least three. Other than that, there isn’t anything too interesting about the data. When measuring the effects of cores versus threads in a real-world application, you will likely see more interesting performance trade-offs.</p>

<p class="author1">One question posed by this data is: why doesn’t adding more than two threads or three threads make the application any faster? Answering questions like these will require hypotheses, experimenting with application code, and trying to erase any bottlenecks. In this case it may be that the main thread is so busy coordinating, handling requests, and communicating <a data-type="indexterm" data-primary="cores, number of" data-startref="corcount" id="idm45995909550024" class="calibre6"/>with threads, that the worker threads aren’t able to get much work done.</p>

<figure class="calibre29"><div id="graph_core_vs_thread_perf" class="figure">
<img src="Images/mtjs_0802.png" alt="Two Threads and three cores" class="calibre72"/>
<h6 class="calibre30"><span class="keep-together">Figure 8-2. </span>Available cores versus thread pool size and how it affects throughput</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Containers Versus Threads" class="calibre4"><div class="preface" id="idm45995909795176">
<h2 class="calibre37">Containers Versus Threads</h2>

<p class="author1">When it comes to writing server <a data-type="indexterm" data-primary="containers versus threads" id="idm45995909545912" class="calibre6"/>software, like with Node.js, the rule of thumb is that processes should scale horizontally. This is a fancy term meaning you should run multiple redundant versions of the program in an isolated manner—such as within a Docker container. Horizontal scaling benefits performance in a way that allows developers to fine-tune the performance of the whole fleet of applications. Such tuning can’t be performed as easily when the scaling primitive happens within the program, in the form of a thread pool.</p>

<p class="author1">Orchestrators, such as Kubernetes, are tools that run containers across multiple servers. They make it easy to scale an application on demand; during the holiday season an engineer can manually increase the number of instances running. Orchestrators can also dynamically change the scale depending on other heuristics like CPU usage, traffic throughput, and even the size of a work queue.</p>

<p class="author1">How might this dynamic scaling look if it were performed within an application at runtime? Well, certainly the available thread pool would need to be resized. There would also need to be some sort of communication in place, allowing an engineer to send messages to the processes to resize the pool; perhaps an additional server needs to listen on a port for such administrative commands. Such functionality then requires additional complexity to be added to the application code.</p>

<p class="author1">While adding additional processes instead of increasing thread count increases overall resource consumption, not to mention the overhead of wrapping processes in a container, larger companies usually prefer the scaling flexibility of this approach.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="When to Use" class="calibre4"><div class="preface" id="ch_benchmarks_sec_usage">
<h1 class="calibre13">When to Use</h1>

<p class="author1">Sometimes you’ll get lucky and will end up with a problem that benefits greatly from a multithreaded solution. Here are some of the most straightforward characteristics of such a problem to keep an eye out for:</p>
<dl class="calibre14">
<dt class="calibre15">Embarrassingly parallel</dt>
<dd class="calibre16">
<p class="calibre17">This is a class of problems where a <a data-type="indexterm" data-primary="parallelism" id="idm45995909537672" class="calibre6"/>large task can be broken down into smaller tasks and very little or no sharing of state is required. One such problem is the Game of Life simulation covered in <a data-type="xref" href="ch05.xhtml#ch_adv_shared_mem_sec_app" class="calibre6">“Example Application: Conway’s Game of Life”</a>. With that problem, the game grid can be subdivided into smaller grids, and each grid can be dedicated to an individual thread.</p>
</dd>
<dt class="calibre15">Heavy math</dt>
<dd class="calibre16">
<p class="calibre17">Another characteristic of problems that are a <a data-type="indexterm" data-primary="math, heavy use" id="idm45995909534520" class="calibre6"/>good fit for threads are those that involve a heavy use of math, aka CPU-intensive work. Sure, one might say that everything a computer does is math, but the inverse of a math-heavy application is one that is I/O heavy, or one that mostly deals with network operations. Consider a password hash cracking tool that has a weak SHA1 digest of a password. Such tools may work by running the Secure Hash Algorithm 1 (SHA1) algorithm over every possible combination of 10 character passwords, which is a lot of number crunching indeed.</p>
</dd>
<dt class="calibre15">MapReduce-friendly problems</dt>
<dd class="calibre16">
<p class="calibre17">MapReduce is a programming model that is <a data-type="indexterm" data-primary="MapReduce" id="idm45995909531832" class="calibre6"/>inspired by functional programming. This model is often used for large-scale data processing that has been spread across many different machines. MapReduce is broken into two pieces. The first is Map, which accepts a list of values and produces a list of values. The second is Reduce, where the list of values are iterated on again, and a singular value is produced. A single-threaded version of this could be created in JavaScript using <code class="calibre18">Array#map()</code> and <code class="calibre18">Array#reduce()</code>, but a multithreaded version requires different threads processing subsets of the lists of data. A search engine uses Map to scan millions of documents for keywords, then Reduce to score and rank them, providing a user with a page of relevant results. Database systems like Hadoop and MongoDB benefit from MapReduce.</p>
</dd>
<dt class="calibre15">Graphics processing</dt>
<dd class="calibre16">
<p class="calibre17">A lot of graphics processing tasks <a data-type="indexterm" data-primary="graphics processing" id="idm45995909528072" class="calibre6"/>also benefit from multiple threads. Much like the Game of Life problem, which operates on a grid of cells, images are represented as a grid of pixels. In both cases the value at each coordinate can be represented as a number, though Game of Life uses a single 1-bit number while images are more likely to use 3 or 4 bytes (red, green, blue, and optional alpha transparency). Image filtering then becomes a problem of subdividing an image into smaller images, having threads in a thread-pool process with the smaller images in parallel, then updating the interface once the change is complete.</p>
</dd>
</dl>

<p class="author1">This isn’t a complete list of all the situations in which you should use multithreading; it’s just a list of some of the most obvious use cases.</p>

<p class="author1">One of the repeating themes is that problems that don’t require shared data, or at least that don’t require coordinated reads and writes to shared data, are easier to model using multiple threads. Though it’s generally beneficial to write code that doesn’t have many side effects, this benefit is compounded when writing multithreaded code.</p>

<p class="author1">Another use case that’s particularly beneficial to JavaScript applications is that of template rendering. Depending on <a data-type="indexterm" data-primary="template rendering" id="temprend" class="calibre6"/><a data-type="indexterm" data-primary="libraries" data-secondary="template rendering and" id="idm45995909523640" class="calibre6"/>the library used, the rendering of a template might be done using a string that represents the raw template and an object that contains variables to modify the template. With such use cases there usually isn’t much global state to consider, just the two inputs, while a single string output is returned. This is the case with the popular template rendering packages <code class="calibre18">mustache</code> and <code class="calibre18">handlebars</code>. Offloading template rendering from the main thread of a Node.js application seems like a reasonable place to gain performance.</p>

<p class="author1">Let’s test this assumption out. Create a new <a data-type="indexterm" data-primary="code samples" data-secondary="ch8-template-render" id="idm45995909520568" class="calibre6"/>directory named <em class="calibre7">ch8-template-render/</em>. Inside this directory, copy and paste the existing <em class="calibre7">ch6-thread-pool/rpc-worker.js</em> file from <a data-type="xref" href="ch06.xhtml#ex_threadpool_rpcworker_1" class="calibre6">Example 6-3</a>. Although the file will work fine unmodified, you should comment out the <code class="calibre18">console.log()</code> statement so that it doesn’t slow down the benchmark.</p>

<p class="author1">You’ll also want to initialize an npm project and install some basic packages. You can do this by running the following commands:</p>

<pre data-type="programlisting" class="calibre38">$ npm init -y
$ npm install fastify@3 mustache@4</pre>

<p class="author1">Next, create a file named <em class="calibre7">server.js</em>. This represents an HTTP application that performs basic HTML rendering when it receives a request. This benchmark is going to use some real-world packages instead of loading built-in modules for everything. Start the file off with the contents of <a data-type="xref" href="#ex_template_server_1" class="calibre6">Example 8-1</a>.</p>
<div id="ex_template_server_1" data-type="example" class="calibre26">
<h5 class="calibre27"><span class="keep-together">Example 8-1. </span><em class="calibre7">ch8-template-render/server.js</em> (part 1)</h5>

<pre data-type="programlisting" data-code-language="javascript" class="calibre28"><code class="calibre18">#!/usr/bin/env node</code>
<code class="c">// npm install fastify@3 mustache@4</code>

<code class="kr">const</code> <code class="nx">Fastify</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s">'fastify'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">RpcWorkerPool</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s">'./rpc-worker.js'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">worker</code> <code class="o">=</code> <code class="kr">new</code> <code class="nx">RpcWorkerPool</code><code class="p">(</code><code class="s">'./worker.js'</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="s">'leastbusy'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s">'./template.js'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">server</code> <code class="o">=</code> <code class="nx">Fastify</code><code class="p">();</code></pre></div>

<p class="author1">The file starts off by instantiating the Fastify web framework, as well as a worker pool with four workers. The application also loads a module named <em class="calibre7">template.js</em> that will be used to render templates used by the web application.</p>

<p class="author1">Now, you’re ready to declare some routes and to tell the server to listen for requests. Keep editing the file by adding the content from <a data-type="xref" href="#ex_template_server_2" class="calibre6">Example 8-2</a> to it.</p>
<div id="ex_template_server_2" data-type="example" class="calibre26">
<h5 class="calibre27"><span class="keep-together">Example 8-2. </span><em class="calibre7">ch8-template-render/server.js</em> (part 2)</h5>

<pre data-type="programlisting" data-code-language="javascript" class="calibre28"><code class="nx">server</code><code class="p">.</code><code class="nx">get</code><code class="p">(</code><code class="s">'/main'</code><code class="p">,</code> <code class="nx">async</code> <code class="p">(</code><code class="nx">request</code><code class="p">,</code> <code class="nx">reply</code><code class="p">)</code> <code class="o">=&gt;</code>
  <code class="nx">template</code><code class="p">.</code><code class="nx">renderLove</code><code class="p">({</code> <code class="nx">me</code><code class="o">:</code> <code class="s">'Thomas'</code><code class="p">,</code> <code class="nx">you</code><code class="o">:</code> <code class="s">'Katelyn'</code> <code class="p">}));</code>

<code class="nx">server</code><code class="p">.</code><code class="nx">get</code><code class="p">(</code><code class="s">'/offload'</code><code class="p">,</code> <code class="nx">async</code> <code class="p">(</code><code class="nx">request</code><code class="p">,</code> <code class="nx">reply</code><code class="p">)</code> <code class="o">=&gt;</code>
  <code class="nx">worker</code><code class="p">.</code><code class="nx">exec</code><code class="p">(</code><code class="s">'renderLove'</code><code class="p">,</code> <code class="p">{</code> <code class="nx">me</code><code class="o">:</code> <code class="s">'Thomas'</code><code class="p">,</code> <code class="nx">you</code><code class="o">:</code> <code class="s">'Katelyn'</code> <code class="p">}));</code>

<code class="nx">server</code><code class="p">.</code><code class="nx">listen</code><code class="p">(</code><code class="mi">3000</code><code class="p">,</code> <code class="p">(</code><code class="nx">err</code><code class="p">,</code> <code class="nx">address</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>
  <code class="kr">if</code> <code class="p">(</code><code class="nx">err</code><code class="p">)</code> <code class="kr">throw</code> <code class="nx">err</code><code class="p">;</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s">`listening on: </code><code class="si">${</code><code class="nx">address</code><code class="si">}</code><code class="s">`</code><code class="p">);</code>
<code class="p">});</code></pre></div>

<p class="author1">Two routes have been introduced to the application. The first is <code class="calibre18">GET /main</code> and will perform the rendering of the request in the main thread. This represents a single-threaded application. The second route is <code class="calibre18">GET /offload</code>, where the rendering work will be offloaded to a separate worker thread. Finally, the server is instructed to listen on port 3000.</p>

<p class="author1">At this point the application is functionally complete. But as an added bonus, it would be nice to be able to quantify the amount of work that the server is busy doing. While it’s true that we can primarily test the efficiency of this application by using an HTTP request benchmark, sometimes it’s nice to look at other numbers as well. Add the content from <a data-type="xref" href="#ex_template_server_3" class="calibre6">Example 8-3</a> to finish off the file.</p>
<div id="ex_template_server_3" data-type="example" class="calibre26">
<h5 class="calibre27"><span class="keep-together">Example 8-3. </span><em class="calibre7">ch8-template-render/server.js</em> (part 3)</h5>

<pre data-type="programlisting" data-code-language="javascript" class="calibre28"><code class="kr">const</code> <code class="nx">timer</code> <code class="o">=</code> <code class="nx">process</code><code class="p">.</code><code class="nx">hrtime</code><code class="p">.</code><code class="nx">bigint</code><code class="p">;</code>
<code class="nx">setInterval</code><code class="p">(()</code> <code class="o">=&gt;</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="nx">start</code> <code class="o">=</code> <code class="nx">timer</code><code class="p">();</code>
  <code class="nx">setImmediate</code><code class="p">(()</code> <code class="o">=&gt;</code> <code class="p">{</code>
    <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s">`delay: </code><code class="si">${</code><code class="p">(</code><code class="nx">timer</code><code class="p">()</code> <code class="o">-</code> <code class="nx">start</code><code class="p">).</code><code class="nx">toLocaleString</code><code class="p">()</code><code class="si">}</code><code class="s">ns`</code><code class="p">);</code>
  <code class="p">});</code>
<code class="p">},</code> <code class="mi">1000</code><code class="p">);</code></pre></div>

<p class="author1">This code uses a <code class="calibre18">setInterval</code> call that runs every second. It wraps a <code class="calibre18">setImmediate()</code> call, measuring current time in nanoseconds before and after the call is made. It’s not perfect, but it is one way to approximate how much load the process is currently receiving. As the event loop for the process gets busier, the number that is reported will get higher. Also, the busyness of the event loop affects the delay of asynchronous operations throughout the process. Keeping this number lower therefore correlates to a more performant application.</p>

<p class="author1">Next, create a file named <em class="calibre7">worker.js</em>. Add the content from <a data-type="xref" href="#ex_template_worker" class="calibre6">Example 8-4</a> to it.</p>
<div id="ex_template_worker" data-type="example" class="calibre26">
<h5 class="calibre27"><span class="keep-together">Example 8-4. </span><em class="calibre7">ch8-template-render/worker.js</em></h5>

<pre data-type="programlisting" data-code-language="javascript" class="calibre28"><code class="kr">const</code> <code class="p">{</code> <code class="nx">parentPort</code> <code class="p">}</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s">'worker_threads'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">template</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s">'./template.js'</code><code class="p">);</code>

<code class="kr">function</code> <code class="nx">asyncOnMessageWrap</code><code class="p">(</code><code class="nx">fn</code><code class="p">)</code> <code class="p">{</code>
  <code class="kr">return</code> <code class="nx">async</code> <code class="kr">function</code><code class="p">(</code><code class="nx">msg</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">parentPort</code><code class="p">.</code><code class="nx">postMessage</code><code class="p">(</code><code class="nx">await</code> <code class="nx">fn</code><code class="p">(</code><code class="nx">msg</code><code class="p">));</code>
  <code class="p">}</code>
<code class="p">}</code>

<code class="kr">const</code> <code class="nx">commands</code> <code class="o">=</code> <code class="p">{</code>
  <code class="nx">renderLove</code><code class="o">:</code> <code class="p">(</code><code class="nx">data</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="nx">template</code><code class="p">.</code><code class="nx">renderLove</code><code class="p">(</code><code class="nx">data</code><code class="p">)</code>
<code class="p">};</code>

<code class="nx">parentPort</code><code class="p">.</code><code class="nx">on</code><code class="p">(</code><code class="s">'message'</code><code class="p">,</code> <code class="nx">asyncOnMessageWrap</code><code class="p">(</code><code class="nx">async</code> <code class="p">({</code> <code class="nx">method</code><code class="p">,</code> <code class="nx">params</code><code class="p">,</code> <code class="nx">id</code> <code class="p">})</code> <code class="o">=&gt;</code> <code class="p">({</code>
  <code class="nx">result</code><code class="o">:</code> <code class="nx">await</code> <code class="nx">commands</code><code class="p">[</code><code class="nx">method</code><code class="p">](...</code><code class="nx">params</code><code class="p">),</code> <code class="nx">id</code>
<code class="p">})));</code></pre></div>

<p class="author1">This is a modified version of the worker file that you created before. In this case a single command is used, <code class="calibre18">renderLove()</code>, which accepts an object with key value pairs to be used by the template rendering function.</p>

<p class="author1">Finally, create a file named <em class="calibre7">template.js</em>, and add the content from <a data-type="xref" href="#ex_template_template" class="calibre6">Example 8-5</a> to it.</p>
<div id="ex_template_template" data-type="example" class="calibre26">
<h5 class="calibre27"><span class="keep-together">Example 8-5. </span><em class="calibre7">ch8-template-render/template.js</em></h5>

<pre data-type="programlisting" data-code-language="javascript" class="calibre28"><code class="kr">const</code> <code class="nx">Mustache</code> <code class="o">=</code> <code class="nx">require</code><code class="p">(</code><code class="s">'mustache'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">love_template</code> <code class="o">=</code> <code class="s">"&lt;em&gt;{{me}} loves {{you}}&lt;/em&gt; "</code><code class="p">.</code><code class="nx">repeat</code><code class="p">(</code><code class="mi">80</code><code class="p">);</code>

<code class="nx">module</code><code class="p">.</code><code class="nx">exports</code><code class="p">.</code><code class="nx">renderLove</code> <code class="o">=</code> <code class="p">(</code><code class="nx">data</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="nx">result</code> <code class="o">=</code> <code class="nx">Mustache</code><code class="p">.</code><code class="nx">render</code><code class="p">(</code><code class="nx">love_template</code><code class="p">,</code> <code class="nx">data</code><code class="p">);</code>
  <code class="c">// Mustache.clearCache();</code>
  <code class="kr">return</code> <code class="nx">result</code><code class="p">;</code>
<code class="p">};</code></pre></div>

<p class="author1">In a real-world application, this file might be used for reading template files from disk and substituting values, exposing a complete list of templates. For this simple example just a single template renderer is exported and a single hard-coded template is used. This template uses two variables, <code class="calibre18">me</code> and <code class="calibre18">you</code>. The string is repeated many times to approach the length of a template that a real application might use. The longer the template, the longer it takes to render.</p>

<p class="author1">Now that the files have been created, you’re ready to run the application. Run the following commands to run the server and then to launch a benchmark against it:</p>

<pre data-type="programlisting" class="calibre38"># Terminal 1
$ node server.js

# Terminal 2
$ npx autocannon -d 60 http://localhost:3000/main
$ npx autocannon -d 60 http://localhost:3000/offload</pre>

<p class="author1">On a test run on a beefy 16-core laptop, when rendering templates entirely in the main thread, the application had an average throughput of 13,285 requests per second. However, when running the same test while offloading template rendering to a worker thread, the average throughput was 18,981 requests per second. In this particular situation it means the throughput increased by about 43%.</p>

<p class="author1">The event loop latency also decreased <a data-type="indexterm" data-primary="event loops, latency" id="idm45995908938776" class="calibre6"/>significantly. Sampling the time it takes to call <code class="calibre18">setImmediate()</code> while the process is idle gets us about 87 μs on average. When performing template rendering in the main thread, the latency averages 769 μs. The same samples taken when offloading rendering to a worker thread are on average 232 μs. Subtracting out the idle state from both values means it’s about a 4.7x improvement when using threads. <a data-type="xref" href="#chart_eventloop_delay" class="calibre6">Figure 8-3</a> compares these samples over time during the 60-second benchmark.</p>

<figure class="calibre29"><div id="chart_eventloop_delay" class="figure">
<img src="Images/mtjs_0803.png" alt="The event loop is always further delayed when single threaded" class="calibre73"/>
<h6 class="calibre30"><span class="keep-together">Figure 8-3. </span>Event loop delay when using single thread versus multiple threads</h6>
</div></figure>

<p class="author1">Does this mean you should run out and refactor your applications to offload rendering to another thread? Not necessarily. With this contrived example the application was made faster with the additional threads, but this was done on a 16-core machine. It’s very likely that your production applications have access to fewer cores.</p>

<p class="author1">That said, the biggest performance differentiator while testing this was the size of the templates. When they’re a lot smaller, like without repeating the string, it’s faster to render the templates in a single thread. The reason it’s going to be slower is that the overhead of passing the template data between threads is going to be much larger than the time it takes to render a tiny template.</p>

<p class="author1">As with all benchmarks, take this one with a grain of salt. You’ll need to test such changes with your application in a production <a data-type="indexterm" data-primary="template rendering" data-startref="temprend" id="idm45995909041544" class="calibre6"/>environment to know for sure if it benefits from additional threads or not.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary of Caveats" class="calibre4"><div class="preface" id="ch_benchmarks_sec_caveats">
<h1 class="calibre13">Summary of Caveats</h1>

<p class="author1">This is a combined list of the aforementioned caveats when working with threads in JavaScript:</p>
<dl class="calibre14">
<dt class="calibre15">Complexity</dt>
<dd class="calibre16">
<p class="calibre17">Applications tend to be more <a data-type="indexterm" data-primary="memory" data-secondary="application complexity" id="idm45995909037288" class="calibre6"/>complex when using shared memory. This is especially true if you are hand-writing calls with <code class="calibre18">Atomics</code> and manually working with <code class="calibre18">SharedBufferArray</code> instances. Now, admittedly, a lot of this complexity can be hidden from the application through the use of a third-party module. In such a case it can be possible to represent your workers in a clean manner, communicating with them from the main thread, and having all the intercommunication and coordination abstracted away.</p>
</dd>
<dt class="calibre15">Memory overhead</dt>
<dd class="calibre16">
<p class="calibre17">There is additional memory overhead <a data-type="indexterm" data-primary="memory" data-secondary="overhead" id="idm45995909033576" class="calibre6"/>with each thread that is added to a program. This memory overhead is compounded if a lot of modules are being loaded in each thread. Although the overhead might not be a huge deal on modern computers, it is worth testing on the end hardware the code will ultimately run on just to be safe. One way to help alleviate this issue is to audit the code that is being loaded in separate threads. Make sure you’re not unnecessarily loading the kitchen sink!</p>
</dd>
<dt class="calibre15">No shared objects</dt>
<dd class="calibre16">
<p class="calibre17">The inability to share objects <a data-type="indexterm" data-primary="objects" data-secondary="shared" id="idm45995909030680" class="calibre6"/>between threads can make it difficult to easily convert a single-threaded application to a multithreaded one. Instead, when it comes to mutating objects, you’ll need to pass messages around that end up mutating an object that lives in a single location.</p>
</dd>
<dt class="calibre15">No DOM access</dt>
<dd class="calibre16">
<p class="calibre17">Only the main thread of a browser-based <a data-type="indexterm" data-primary="DOM (Document Object Model)" data-secondary="access" id="idm45995909027960" class="calibre6"/>application has access to the DOM. This can make it difficult to offload UI rendering tasks to another thread. That said, it’s entirely possible for the main thread to be in charge of DOM mutation while additional threads can do the heavy lifting and return data changes to the main thread to update the UI.</p>
</dd>
<dt class="calibre15">Modified APIs</dt>
<dd class="calibre16">
<p class="calibre17">Along the same lines as the lack of DOM access, there are slight changes to APIs available <a data-type="indexterm" data-primary="API (application programming interface)" data-secondary="modified" id="idm45995909024856" class="calibre6"/>in threads. In the browser this means no calls to <code class="calibre18">alert()</code>, and individual worker types have even more rules, like disallowing blocking <code class="calibre18">XMLHttpRequest#open()</code> requests, <code class="calibre18">localStorage</code> restrictions, top-level <code class="calibre18">await</code>, etc. While some concerns are a little fringe, it does mean that not all code can run unmodified in every possible JavaScript context. Documentation is your friend when dealing with this.</p>
</dd>
<dt class="calibre15">Structured clone algorithm constraints</dt>
<dd class="calibre16">
<p class="calibre17">There are some constraints on the <a data-type="indexterm" data-primary="structured clone algorithm" id="idm45995909020648" class="calibre6"/>structured clone algorithm that may make it difficult to pass certain class instances between different threads. Currently, even if two threads have access to the same class definition, instances of the class passed between threads become plain <code class="calibre18">Object</code> instances. While it’s possible to rehydrate the data back into a class instance, it does require manual effort.</p>
</dd>
<dt class="calibre15">Browsers require special headers</dt>
<dd class="calibre16">
<p class="calibre17">When working with shared memory <a data-type="indexterm" data-primary="memory" data-secondary="browsers, headers" id="idm45995909017656" class="calibre6"/><a data-type="indexterm" data-primary="browser" data-secondary="shared memory and" id="idm45995909016680" class="calibre6"/>in the browser via <code class="calibre18">SharedArrayBuffer</code>, the server must supply two additional headers in the request for the HTML document used by the page. If you have complete control of the server, then these headers may be easy to introduce. However, in certain hosting environments, it might be difficult or impossible to supply such headers. Even the package used in this book to host a local server required modifications to enable the headers.</p>
</dd>
<dt class="calibre15">Thread preparedness detection</dt>
<dd class="calibre16">
<p class="calibre17">There is no built-in functionality to <a data-type="indexterm" data-primary="threads" data-secondary="preparedness" data-tertiary="detection" id="idm45995909013400" class="calibre6"/>know when a spawned thread is ready to work with shared memory. Instead, a solution must first be built that essentially pings the thread and then waits until a response has been received.</p>
</dd>
</dl>
</div></section>







</div></section></div></body></html>