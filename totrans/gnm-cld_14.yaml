- en: Chapter 13\. Assembling Your Own Workspace in Terra
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。在Terra中组装你自己的工作空间
- en: In Chapters [11](ch11.xhtml#running_many_workflows_conveniently_in) and [12](ch12.xhtml#interactive_analysis_in_jupyter_noteboo),
    you learned how  to use workflows and interactive notebooks in Terra using prebaked
    workspaces. Now, it’s time for you to learn to bake your own so you can build
    your own analyses within the Terra framework. This is an area provides a lot of
    options and multiple valid approaches, so rather than attempting to provide a
    one-size-fits-all path, we’re going to walk through three scenarios.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.xhtml#running_many_workflows_conveniently_in)和[第12章](ch12.xhtml#interactive_analysis_in_jupyter_noteboo)中，你学习了如何在Terra中使用预先准备好的工作空间来运行工作流和交互式笔记本。现在，是时候学习如何自己创建工作空间，以便在Terra框架内构建自己的分析了。这个领域提供了很多选项和多种有效的方法，所以我们不打算提供一个适用于所有情况的路径，而是将会介绍三种场景。
- en: In the first scenario, we re-create the [book tutorial workspace](https://oreil.ly/n7oOr)
    from its base components to demonstrate the key mechanisms involved in assembling
    a workspace from the ground up. In the second and third scenarios, we show you
    how to take advantage of existing workspaces to minimize the amount of work you
    have to do when starting a new project. In one case, we explain how to add data
    to an existing workspace that is already set up for a particular analysis, such
    as the official GATK Best Practices workspaces. In the other, we demonstrate how
    to build an analysis around data exported from the Terra Data Library. However,
    before we dive into those three scenarios, let’s explore the data management strategy
    that we’re applying in all three cases.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个场景中，我们重新创建了[书籍教程工作空间](https://oreil.ly/n7oOr)，从基础组件开始演示如何从头开始组装工作空间的关键机制。在第二和第三个场景中，我们将向你展示如何利用现有的工作空间来最小化在启动新项目时需要做的工作量。在一个情况下，我们解释了如何向已经为特定分析设置好的现有工作空间添加数据，例如官方GATK最佳实践工作空间。在另一个情况下，我们演示了如何围绕从Terra数据库导出的数据构建分析。然而，在我们深入探讨这三种场景之前，让我们先探讨一下我们在所有三种情况下都应用的数据管理策略。
- en: Managing Data Inside and Outside of Workspaces
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在工作空间内外管理数据
- en: One of the most important aspects of moving your work to the cloud is designing
    a data management strategy that will be sustainable for the long term, especially
    if you expect to work with large datasets that will serve as input for multiple
    projects. It’s a complex enough topic that a full discussion would be beyond the
    scope of this book, and entire books cover that subject alone. However, it’s worth
    taking some time to talk through a few key considerations that apply specifically
    in the context of Terra and shape how we decide where data should reside.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 将工作迁移到云端最重要的一个方面之一是设计一种可持续长期使用的数据管理策略，特别是如果你希望处理作为多个项目输入的大型数据集。这是一个足够复杂的主题，以至于完整讨论超出了本书的范围，整本书都可以单独讨论这个主题。然而，花一些时间讨论一下在Terra环境中特别适用的几个关键考虑因素是值得的，这些因素会影响我们如何决定数据存放的位置。
- en: The Workspace Bucket as Data Repository
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作空间存储桶作为数据存储库
- en: As you learned in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in),
    each Terra workspace is created with a dedicated GCS bucket. You can store any
    data that you want in the workspace bucket, and all notebooks as well as logs
    and outputs from workflows that you run in that workspace will be stored there.
     However, there’s no obligation to store your *input* data in the workspace bucket.
    You can compute on data that resides anywhere in GCS as long as you can provide
    the relevant pointers to the files, assuming that you are able to grant access
    to that data to your Terra account (more on that in a minute).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[第11章](ch11.xhtml#running_many_workflows_conveniently_in)中学到的，每个Terra工作空间都创建有专用的GCS存储桶。你可以在工作空间存储任何你想要的数据，以及你在该工作空间中运行的所有笔记本、工作流的日志和输出。然而，并没有义务将*输入*数据存储在工作空间的存储桶中。只要你能提供文件的相关指针并且能够授权给你的Terra帐户访问这些数据（稍后会详细讨论这一点），你可以计算存储在GCS任何位置的数据。
- en: 'This is important to note for a couple of reasons. First, if you’re going to
    use the same data as input for multiple projects, you don’t want to have to maintain
    copies of the data in each workspace because you’ll get charged storage costs
    for each bucket. Instead, you can put the data in one location and point to that
    location from wherever you need to use it. Second, be aware that the workspace
    bucket lives and dies with your workspace: if you delete the workspace, the bucket
    and its contents will also be deleted. Related to this, when you clone a workspace,
    the only bucket content that the clone inherits from its parent is the notebooks
    directory. The clone will inherit a copy of the parent’s data tables, retaining
    links to the data in the parent’s bucket, but not the files themselves. This is
    called making a *shallow copy* of the workspace contents. If you then delete the
    parent workspace, the links in the clone’s data tables will break, and you will
    no longer be able to run analyses on the affected data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个重要的原因需要注意。首先，如果您打算将同一数据用作多个项目的输入，您不希望必须在每个工作空间中维护数据的副本，因为每个存储桶都会产生存储费用。相反，您可以将数据放在一个位置，并从需要使用它的任何地方指向该位置。其次，请注意，工作空间存储桶随着您的工作空间而生存和消亡：如果删除工作空间，则存储桶及其内容也将被删除。与此相关的是，当您克隆一个工作空间时，克隆仅从其父项继承笔记本目录的存储桶内容。克隆将继承父项的数据表的副本，并保留到父项存储桶中数据的链接，但不包括文件本身。这被称为对工作空间内容进行*浅复制*。如果随后删除父工作空间，则克隆的数据表中的链接将断开，您将无法再对受影响的数据进行分析。
- en: As a result, we generally recommend storing datasets in dedicated *master* workspaces
    that are kept separate from analysis workspaces, with more narrow permissions
    restricting the number of people authorized to modify or delete them. Alternatively,
    you could also store the data in buckets that you manage outside of Terra, as
    we did for the example data provided with this book. The advantage of storing
    the data outside of Terra is that you (or whoever owns the billing account for
    the bucket) have full administrative control over it. That gives you the freedom
    to do things like setting granular per-file permissions or making contents fully
    public, which are not currently possible in Terra workspace buckets for security
    reasons. It also doesn’t hurt that you can choose meaningful names for the buckets
    you create yourself, whereas Terra assigns long, abstract names that are not very
    human-friendly. However, if you decide to follow that route, you’ll need to enable
    Terra to access any private data that you manage yourself outside of Terra, as
    explained in the next section.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这样做，我们通常建议将数据集存储在专门的*主*工作空间中，与分析工作空间分开，并且更严格的权限限制了可以修改或删除它们的人数。或者，您也可以将数据存储在您在
    Terra 之外管理的存储桶中，就像我们为本书提供的示例数据所做的那样。将数据存储在 Terra 之外的优势在于，您（或者拥有存储桶计费账户的人）完全控制它。这样可以自由设置像每个文件的详细权限或者完全公开内容这样的事情，而这些在
    Terra 工作空间桶目前出于安全原因是不可能的。此外，您还可以选择有意义的名称来命名您自己创建的存储桶，而不是 Terra 分配的长而抽象的名称，这些名称对人类来说不够友好。不过，如果您决定采用这种方式，您将需要启用
    Terra 访问您在 Terra 之外自行管理的任何私有数据，如下一节所述。
- en: Accessing Private Data That You Manage Outside of Terra
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问您在 Terra 之外管理的私有数据
- en: 'So far, we’ve worked with data that’s either in public buckets or in the workspace’s
    own bucket, which is managed by Terra. However, you will eventually want to access
    data that resides in a private bucket that is not managed by Terra. At that point,
    you will encounter an unexpected complication: even if you own that private bucket,
    you will need to go through an additional authentication step that involves the
    GCP console and a *proxy group account*. Here’s what you need to know and do to
    get past this hurdle.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用的数据要么位于公共存储桶中，要么位于由 Terra 管理的工作空间自己的存储桶中。然而，您最终可能需要访问位于由 Terra 不管理的私有存储桶中的数据。在那时，您将遇到一个意外的复杂性：即使您拥有该私有存储桶，您也需要经过额外的身份验证步骤，涉及到
    GCP 控制台和*代理组帐户*。以下是您需要了解和执行以克服此障碍的步骤。
- en: Whenever you issue an instruction to do something in GCP through a Terra service,
    the Terra system doesn’t actually use your individual account in its request to
    GCP. Instead, it uses a service account that Terra creates for you. In fact, you
    can have multiple service accounts created for you by Terra because you get one
    for each billing project to which you have access. All of your service accounts
    are collected into a *proxy group*, which Terra uses to manage your credentials
    to various resources. This has benefits for both security and convenience under
    the hood.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每当您通过Terra服务发出指令以执行GCP中的操作时，Terra系统实际上并不使用您的个人账户。相反，它使用Terra为您创建的服务账户。事实上，您可以通过Terra为您有权访问的每个计费项目获得多个服务账户。所有这些服务账户都汇总到一个*代理组*中，Terra使用它来管理您对各种资源的凭据。这在安全性和便利性方面都有益。
- en: 'Most of the time, you don’t need to know about this because any resources that
    you create or manage within Terra (like a workspace and its bucket, or a notebook)
    are automatically shared with your proxy group account. In addition, whenever
    someone shares something with you in Terra, the same thing happens: your proxy
    group account is automatically included in the fun, so you can seamlessly work
    with those resources from within Terra. However, when you start connecting to
    resources that are not managed by Terra, the Google system that controls access
    permissions doesn’t automatically know that your proxy group account is allowed
    to act as your proxy. So, you need to identify your proxy group account in the
    Google permissions system, and specify what it should be allowed to do.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，您不需要了解这些内容，因为您在Terra内创建或管理的任何资源（如工作区及其存储桶或笔记本电脑）都会自动与您的代理组账户共享。此外，每当有人在Terra中与您共享资源时，相同的情况也会发生：您的代理组账户会自动包含在其中，因此您可以无缝地使用这些资源。然而，当您开始连接不受Terra管理的资源时，控制访问权限的Google系统不会自动知道您的代理组账户被允许作为您的代理。因此，您需要在Google权限系统中标识您的代理组账户，并指定它应该被允许执行的操作。
- en: 'Fortunately, this is not too difficult if you know what to do, and we’re about
    to walk you through the process for the most common case: accessing a GCS bucket
    that is not controlled by Terra. For other resources, the process would be essentially
    the same.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，如果您知道该怎么做，这并不太困难，我们即将为您介绍最常见情况的过程：访问一个不受Terra控制的GCS存储桶。对于其他资源，该过程基本相同。
- en: First, you need to find out your proxy group account identifier. There are several
    ways to do that. The simplest is to look it up in your [Terra user profile](https://app.terra.bio/#profile),
    where it is displayed as shown in [Figure 13-1](#the_proxy_group_identifier_displayed_in).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要查找您的代理组账户标识符。有几种方法可以做到这一点。最简单的方法是在您的[Terra用户资料](https://app.terra.bio/#profile)中查找，如[图 13-1](#the_proxy_group_identifier_displayed_in)所示。
- en: '![The proxy group identifier displayed in the user profile.](Images/gitc_1301.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![用户资料中显示的代理组标识符。](Images/gitc_1301.png)'
- en: Figure 13-1\. The proxy group identifier displayed in the user profile.
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. 用户资料中显示的代理组标识符。
- en: With that in hand, you can head over to the GCS console and look up your *external
    bucket*; that is, the bucket you originally created for the exercises in [Chapter 4](ch04.xhtml#first_steps_in_the_cloud).
    Go to the Bucket details page and find the Permissions panel, which lists all
    accounts authorized to access the bucket in some capacity, as shown in [Figure 13-2](#the_bucket_permissions_panel_showing_ac).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个信息，您可以前往GCS控制台并查找您的*外部存储桶*；也就是说，您最初为[第四章](ch04.xhtml#first_steps_in_the_cloud)中的练习创建的存储桶。转到存储桶详情页面，并找到权限面板，其中列出了所有被授权以某种方式访问该存储桶的账户，如[图 13-2](#the_bucket_permissions_panel_showing_ac)所示。
- en: '![The bucket permissions panel showing accounts with access to the bucket.](Images/gitc_1302.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![显示具有访问权限的存储桶的权限面板。](Images/gitc_1302.png)'
- en: Figure 13-2\. The bucket permissions panel showing accounts with access to the
    bucket.
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. 显示具有访问权限的存储桶的权限面板。
- en: Click the “Add members” button to open the relevant page and add your pet service
    account identifier in the “New members” field. In the “Select a role” drop-down
    menu, scroll if needed to select the Storage service in the left column and then
    further select the Storage Object Admin role in the right column, as shown in
    [Figure 13-3](#granting_access_to_a_bucket_to_a_new_me). Click Save to confirm.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“添加成员”按钮打开相关页面，并在“新成员”字段中添加您的宠物服务帐户标识符。在“选择角色”下拉菜单中，如有需要，请滚动选择左列中的存储服务，然后进一步选择右列中的存储对象管理员角色，如图[13-3](#granting_access_to_a_bucket_to_a_new_me)所示。点击保存以确认。
- en: '![Granting access to a bucket to a new member.](Images/gitc_1303.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![向新成员授予对存储桶的访问权限。](Images/gitc_1303.png)'
- en: Figure 13-3\. Granting access to a bucket to a new member.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-3\. 向新成员授予对存储桶的访问权限。
- en: After you’ve done this, you will be able to access external GCP resources such
    as buckets not managed by Terra from within your notebooks and workflows. For
    any buckets that you don’t administer yourself, you’ll need to ask an administrator
    to do this procedure on your behalf.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这些操作之后，您将能够从笔记本和工作流中访问外部 GCP 资源，例如不受 Terra 管理的存储桶。对于您没有管理权限的任何存储桶，您需要请求管理员代表您执行此过程。
- en: Accessing Data in the Terra Data Library
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问 Terra 数据库中的数据
- en: As you might recall from the context-setting discussion in [Chapter 1](ch01.xhtml#introduction),
    in which we originally introduced the platform, Terra includes a [Data Library](https://oreil.ly/VD6cJ)
    that provides connections to datasets hosted in GCP by various organizations.
    Some of those datasets are simply provided in Terra workspaces, like the 1000
    Genomes High Coverage dataset that you’ll use in the second and third scenarios
    in this chapter. You’ll have the opportunity to try out a few ways to take advantage
    of that type of data repository.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能从[第一章](ch01.xhtml#introduction)的上下文设置讨论中回想起的那样，我们最初介绍的平台 Terra 包括一个[数据库](https://oreil.ly/VD6cJ)，通过该数据库可以连接到由各种组织在
    GCP 上托管的数据集。其中一些数据集仅在 Terra 工作空间中提供，例如您将在本章第二和第三个场景中使用的 1000 Genomes 高覆盖数据集。您将有机会尝试几种利用这种类型数据存储库的方式。
- en: Other datasets are hosted in independent repositories that you access through
    dedicated interfaces called *data explorers*, which enable you to select subsets
    of the data based on metadata properties and then export them to a regular workspace.
    We won’t cover those in detail but encourage you to explore the library and try
    retrieving data from the [ENCODE](https://oreil.ly/lBDFN) or [NeMo](https://oreil.ly/qCWuS)
    repositories, for example. Both of these are fully public and have very different
    interfaces, so they make for interesting navigation practice.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据集托管在独立的存储库中，您可以通过称为*数据浏览器*的专用界面访问这些存储库。数据浏览器使您能够基于元数据属性选择数据子集，然后将其导出到常规工作空间。我们不会详细介绍这些内容，但建议您探索该库，并尝试从[ENCODE](https://oreil.ly/lBDFN)或[NeMo](https://oreil.ly/qCWuS)存储库检索数据。这两个存储库都是完全公开的，并且具有非常不同的界面，因此它们非常适合进行导航练习。
- en: Most of the datasets in the Terra Data Library are restricted to authorized
    researchers because they contain protected health information, and the access
    modalities depend on the project host organization. Some, like [TCGA](https://oreil.ly/Dsnm4),
    are mediated through dbGAP credentials, which you can link in your Terra user
    profile to gain automatic access if you are already authorized. Others are managed
    outside of Terra and require interaction with the project maintainers. If you
    are interested in learning more about any of the datasets in the library, click
    through to their project page, where you will typically find a description of
    access requirements.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Terra 数据库中的大多数数据集仅限于授权研究人员访问，因为它们包含受保护的健康信息，并且访问模式取决于项目主机组织。例如，像[TCGA](https://oreil.ly/Dsnm4)这样的数据集通过
    dbGAP 凭据进行介入，如果您已经获得授权，您可以在 Terra 用户配置文件中链接这些凭据以自动访问。其他数据集则是在 Terra 之外进行管理，并需要与项目维护者进行交互。如果您对库中的任何数据集感兴趣，点击转到其项目页面，通常可以找到访问要求的描述。
- en: Given the current heterogeneity of the data repositories included in the library,
    it is not yet trivial to use these resources effectively, especially if you intend
    to cross-analyze multiple datasets—which is unfortunate because that is one of
    the key attractions of moving to the cloud. This is an area of ongoing development,
    with many organizations actively collaborating to improve the level of interoperability
    and usability of these resources. We are already seeing early adopters successfully
    harness these resources in their research, and we are optimistic that upcoming
    improvements will make it easier for a wider range of investigators to do so as
    well.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于当前库中包含的数据存储库的异质性，要有效地使用这些资源还不是件轻松的事，特别是如果您打算交叉分析多个数据集的话——这是不幸的，因为这是迁移到云的一个关键吸引点之一。这是一个正在持续发展的领域，许多组织正在积极合作，以提高这些资源的互操作性和可用性水平。我们已经看到早期采用者成功地利用这些资源进行研究，并且我们对即将到来的改进持乐观态度，这将使更广泛的调查者更容易做到这一点。
- en: For now, however, let’s dial back our ambition by a couple of notches and focus
    on getting the basics in place. Back to work!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在让我们把我们的抱负降低几个档次，专注于确保基础设施到位。回到工作！
- en: Re-Creating the Tutorial Workspace from Base Components
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从基础组件重新创建教程工作空间
- en: Our most immediate goal in this chapter is to equip you with the knowledge and
    skills to assemble a complete yet basic workspace on your own. We’ve chosen to
    do this by having you re-create the tutorial workspace from Chapters [11](ch11.xhtml#running_many_workflows_conveniently_in)
    and [12](ch12.xhtml#interactive_analysis_in_jupyter_noteboo) given that you’ve
    been using it extensively in previous chapters and it contains all the basic elements
    of a complete workspace in a fairly simple form. As we guide you through the process,
    we’re going to focus on the most commonly used mechanisms for pulling in the various
    components (data, code, etc.), and to avoid overwhelming you, we’re intentionally
    not going to discuss every option that is available on the platform.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们最紧迫的目标是为您提供知识和技能，让您能够自己组装一个完整而基础的工作空间。我们选择通过让您重新创建第[11](ch11.xhtml#running_many_workflows_conveniently_in)章和第[12](ch12.xhtml#interactive_analysis_in_jupyter_noteboo)章的教程工作空间来实现这一点，因为您在之前的章节中已经广泛使用它，并且它以相当简单的形式包含了完整工作空间的所有基本元素。在引导您完成这个过程时，我们将专注于用于引入各种组件（数据、代码等）的最常用机制，并且为了避免让您感到不知所措，我们有意选择不讨论平台上的每一个可用选项。
- en: 'Are you ready to get started? Because we’ll have you check the model workspace
    multiple times during the course of this section, we recommend that you open two
    separate browser windows (or tabs): one for the model workspace and another for
    the workspace that you are about to create.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好开始了吗？因为在本节课程中我们将让你多次检查模型工作空间，我们建议您打开两个单独的浏览器窗口（或标签页）：一个用于模型工作空间，另一个用于您即将创建的工作空间。
- en: Creating a New Workspace
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个新的工作空间
- en: In your second browser tab or window, navigate to the page that lists workspaces
    to which you have access, either by selecting [Your Workspaces](https://oreil.ly/3fL7H)
    in the collapsible navigation menu or View Workspaces from the Terra landing page.
    You should see plus sign next to the Workspaces header in the top left of the
    page. Click that now to create a new blank workspace.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的第二个浏览器标签页或窗口中，导航至列出您可以访问的工作空间的页面，可以通过折叠导航菜单中选择[您的工作空间](https://oreil.ly/3fL7H)，或从
    Terra 登录页面选择查看工作空间。您应该在页面左上角的工作空间标题旁看到一个加号标志。现在点击它以创建一个新的空白工作空间。
- en: Note
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: By default, this page lists only private workspaces that either you have created
    yourself or that were shared with you under the “My workspaces” tab, excluding
    workspaces that are publicly accessible to everyone. You can view public workspaces
    by selecting one of the other workspace category tabs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，此页面仅列出您自己创建的或共享给您的“我的工作空间”标签下的私人工作空间，不包括对所有人公开的工作空间。您可以通过选择其他工作空间类别标签之一来查看公共工作空间。
- en: In the workspace creation dialog that pops up, give your new workspace a name
    and select a billing project, just as you’ve done when cloning workspaces in previous
    chapters. Because this is a brand-new workspace, the Name and Description fields
    will initially be blank. The name of your workspace must be unique within your
    billing project, but aside from that, you have a lot of freedom in naming, including
    using spaces in the name, as you can see in the example in [Figure 13-4](#the_quotation_markcreate_a_new_workspac).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![The “Create a New Workspace” dialog box.](Images/gitc_1304.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. The Create a New Workspace dialog box.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Description field is what will show up in the Dashboard of your new workspace.
    It’s good practice to enter something informative for future reference, even if
    it’s optional. You’ll have the opportunity to edit it later, though, so don’t
    sweat the details.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The Authorization domain field allows you to restrict access to the workspace
    and all its content to a specific group of users, which you must define separately.
    This is a useful feature for securing private information, but we’re not going
    to demonstrate its use here, so leave this field blank. See the [article in the
    Terra user guide](https://oreil.ly/ikgz9) if you’re interested in learning more
    about this feature.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Click the Create button; you’re then directed to the Dashboard page of your
    brand-new workspace. Feel free to click through the various tabs to inspect the
    contents, but as you’ll quickly notice, there’s not much to see there, with the
    exception of the description you provided (if you provided one) and a link to
    the dedicated GCS bucket that Terra created for your workspace. You might also
    notice the small widget showing the estimated cost per month of the workspace,
    which corresponds to the cost of storing the bucket contents. That cost estimate
    does not include the charges resulting from the analyses you might perform in
    the workspace. Right now, the cost estimate is zero because there’s nothing in
    there. Yay?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Not that we want you to spend money, but this empty workspace is begging for
    some content, so let’s figure out how to load it up. Because this is supposed
    to be a re-creation of the tutorial from [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in),
    we’re going to follow the same order of operations. Our first stop, therefore,
    will be the Workflows panel to set up the `HaplotypeCaller` workflow.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Adding the Workflow to the Methods Repository and Importing It into the Workspace
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you might recall, the workflow that you used in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in)
    was the same `HaplotypeCaller` workflow that you worked with in previous chapters.
    For the purposes of the tutorial, we had already deposited the workflow in Terra’s
    internal Methods Repository, so technically you could just go look it up and import
    it into your workspace. However, we want to empower you to bring in your own private
    workflows, so we’re going to have you deposit your own copy of the `HaplotypeCaller`
    WDL and import that into your workspace.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能记得的那样，在[第 11 章](ch11.xhtml#running_many_workflows_conveniently_in)中使用的工作流程与您在之前章节中使用的`HaplotypeCaller`工作流程相同。出于教程目的，我们已经将工作流程存储在
    Terra 的内部方法库中，因此您实际上可以直接查找并导入它到您的工作空间。然而，我们希望您能够导入您自己的私有工作流程，因此我们将让您存储自己的`HaplotypeCaller`
    WDL 的副本，并将其导入您的工作空间中。
- en: 'In your blank workspace, navigate to the Workflows panel and click the large
    box labeled “Find a workflow.” This opens a dialog that lists a selection of example
    workflows as well as two workflow repositories that contain additional workflows:
    Dockstore and the Broad Methods Repository. Click the latter to access the Methods
    Repository.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在空白工作区中，导航到工作流面板，点击标有“查找工作流”的大框。这将打开一个对话框，列出一些示例工作流以及两个工作流程库：Dockstore 和 Broad
    Methods Repository。点击后者以访问方法库。
- en: You may need to sign into your Google account again and accept a set of terms
    and conditions. Find the Create New Method button (which might be renamed to Create
    New Workflow) to open the workflow creation page. Provide the information as shown
    in [Figure 13-5](#the_create_new_method_page_in_the_broad), substituting your
    own namespace, which can be your username or another identifier that is likely
    to be unique to you. You can get the original WDL file rom the book [GitHub repository](https://oreil.ly/GGMoF);
    either open the text file and copy the contents into the WDL text box, or use
    the “Load from file” link to upload it to the repository. By default, the optional
    Documentation field will be populated using the block of comment text at the top
    of the WDL file. The Synopsis box allows you to add a one-line summary of what
    the workflow does, whereas the Snapshot Comment box is intended to summarize what
    has changed if you upload new versions of the same workflow. You can leave the
    latter blank.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要重新登录您的 Google 账户并接受一组条款和条件。找到“创建新方法”按钮（可能被重命名为“创建新工作流”），以打开工作流创建页面。按照[图
    13-5](#the_create_new_method_page_in_the_broad)中显示的信息提供信息，替换您自己的命名空间，它可以是您的用户名或另一个对您来说可能是唯一的标识符。您可以从书中的[GitHub
    仓库](https://oreil.ly/GGMoF)获取原始 WDL 文件；打开文本文件并将内容复制到 WDL 文本框中，或使用“从文件加载”链接将其上传到仓库。默认情况下，可选的文档字段将使用
    WDL 文件顶部的注释文本块填充。简介框允许您添加工作流的一行摘要，而快照注释框则旨在总结上传相同工作流的新版本时发生的更改。您可以将后者留空。
- en: '![The Create New Method page in the Broad Methods Repository.](Images/gitc_1305.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![广泛方法库中的创建新方法页面。](Images/gitc_1305.png)'
- en: Figure 13-5\. The Create New Method page in the Broad Methods Repository.
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-5\. 广泛方法库中的创建新方法页面。
- en: When you click the Upload button, the system validates the syntax of your WDL
    (using `Womtool` under the hood). Assuming that everything is fine, it will create
    a new workflow entry in the repository under the namespace you provided and present
    you with a summary page, as shown in [Figure 13-6](#summary_page_for_the_newly_created_work).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当您点击上传按钮时，系统将使用`Womtool`在后台验证您的 WDL 语法。假设一切正常，它将在您提供的命名空间下创建一个新的工作流条目，并向您显示摘要页面，如[图
    13-6](#summary_page_for_the_newly_created_work)所示。
- en: '![Summary page for the newly created workflow.](Images/gitc_1306.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![新创建工作流程的摘要页面。](Images/gitc_1306.png)'
- en: Figure 13-6\. Summary page for the newly created workflow.
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-6\. 新创建工作流程的摘要页面。
- en: Take a moment to click the Permissions button, which opens a dialog that allows
    you to share your workflow with specific people, or make it fully public. Feel
    free to do either of those things as you prefer; just keep in mind that if and
    when you do share your workspace with others, they will be able to view and run
    your workflow only if you have done this. That being said, you will be able to
    come back to this page at a later date to do so if needed. We just bring this
    up now because we frequently see people run into errors after their collaborator
    forgot to share a workflow along with their workspace.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请稍等片刻，点击“权限”按钮，打开一个对话框，允许你与特定人分享你的工作流，或者完全公开它。随意选择你喜欢的任何一种方式；只需记住，如果你将你的工作空间与他人分享，只有在你这样做的情况下，他们才能查看和运行你的工作流。话虽如此，如果有需要，你随后可以回到这个页面来执行这些操作。我们现在提到这一点是因为我们经常看到人们在他们的合作者忘记分享工作流和工作空间后遇到错误。
- en: Incidentally, the workflow summary page is also what you would see if you had
    found another workflow in the Methods Repository, either by searching for it or
    following a link shared by a collaborator. As a result, the following instructions
    will apply regardless of whether you were the one who created the workflow in
    the first place.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，工作流摘要页面也是你在方法库中找到另一个工作流时会看到的内容，无论是通过搜索还是通过合作者分享的链接。因此，以下说明将适用于无论你是不是第一次创建工作流。
- en: Creating a Configuration Quickly with a JSON File
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 JSON 文件快速创建配置
- en: After you’re done sharing the workflow (or not sharing; we’re not judging),
    click the Export to Workspace button. If a dialog box opens prompting you to select
    a method configuration, click the Use Blank Configuration button. In the next
    dialog box prompting you to select a destination workspace, select your blank
    workspace in the drop-down menu and click Export to Workspace. Finally, one last
    dialog box might appear, asking if you want to “go to the edit page now.” Click
    Yes—and we swear this is the last dialog box. You should now arrive back at the
    Workflows page in Terra, with your brand-new workflow configuration page in front
    of you.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成工作流程的分享（或不分享；我们不评判），请点击“导出到工作空间”按钮。如果弹出对话框提示你选择方法配置，请点击“使用空白配置”按钮。在下一个对话框中，提示你选择目标工作空间时，在下拉菜单中选择你的空白工作空间，然后点击“导出到工作空间”。最后，可能会出现最后一个对话框，询问你是否要“立即转到编辑页面”。点击“是”，我们发誓这是最后一个对话框了。你现在应该回到
    Terra 的工作流页面，面前是你全新的工作流配置页面。
- en: Configuring the workflow is going to be fairly straightforward. First, because
    we’re following the flow of [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in),
    make sure to select the “Run workflow with inputs defined by file paths” option
    (located under the workflow documentation summary), which allows you to set up
    the workflow configuration with file paths rather than using the data tables.
    Then, turn your attention to the Inputs page, which should be lit up with little
    orange exclamation marks indicating that some inputs are missing. In fact, all
    of them are missing because you used a blank configuration and the workflow itself
    does not specify any default values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 配置工作流程将会非常简单。首先，因为我们遵循第 11 章的流程（在[ch11.xhtml#running_many_workflows_conveniently_in](ch11.xhtml#running_many_workflows_conveniently_in)），请确保选择“使用文件路径定义的输入运行工作流”选项（位于工作流文档摘要下方），这允许你使用文件路径设置工作流配置，而不是使用数据表。然后，转到输入页面，这个页面应该会显示一些小橙色感叹号，表示有一些输入是缺失的。事实上，所有输入都缺失，因为你使用了空白配置，而工作流本身并未指定任何默认值。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The orange exclamation marks will also show up if you have an input that is
    badly formatted or of the wrong variable type, like a number instead of a file.
    Try hovering over one of them to see the error message; all such symbols in Terra
    generally display me information when you hover over them and can sometimes hold
    the answer to your problems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个格式错误或者变量类型错误的输入（比如一个数字而不是文件），橙色感叹号也会显示出来。尝试将鼠标悬停在其中一个上面以查看错误消息；在 Terra
    中，这些符号通常在你悬停时显示更多信息，有时会解决你的问题。
- en: So you need to plug in the appropriate file paths and parameter values on the
    Inputs page. You could look up each input value individually in the original workspace
    and type them manually, but there’s a less tedious way to do it. Can you guess
    what it is? That’s right, make Jason do all the work. Er, we mean the JSON *inputs*
    file, of course. (Yes, that’s a lame joke, but if it helps you remember that you
    can upload a JSON file of inputs, it will have been worth the shame we’re feeling
    right now.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您需要在输入页面上插入适当的文件路径和参数值。您可以在原始工作区中单独查找每个输入值并手动输入，但还有一种不那么繁琐的方法。你能猜到是什么吗？没错，让
    JSON *inputs* 文件来完成所有工作。呃，我们指的是 JSON 输入文件，当然。（是的，这是个差劲的笑话，但如果它能帮助您记住您可以上传一个 JSON
    文件作为输入，那么我们现在感到的羞愧也是值得的。）
- en: Because we previously ran this same workflow directly through Cromwell in [Chapter 10](ch10.xhtml#running_single_workflows_at_scale_with),
    we have [a JSON file](https://oreil.ly/I5Yzl) that specifies all the necessary
    inputs here in the book bucket. You just need to retrieve a local copy of the
    file and then use the “Drag or click to upload json” option on the Inputs page
    (right side next to the Search Inputs box) to add the input definitions it contains
    into your configuration. This should populate all of the fields on the page. Click
    the Save button and confirm that there are no longer any orange exclamation marks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们之前通过 Cromwell 直接运行了相同的工作流，见[第 10 章](ch10.xhtml#running_single_workflows_at_scale_with)，在本书桶中有[一个
    JSON 文件](https://oreil.ly/I5Yzl)，指定了所有必要的输入。您只需获取文件的本地副本，然后在输入页面（在“搜索输入”框的右侧）使用“拖拽或点击上传
    json”选项将其包含的输入定义添加到您的配置中。这应该填充页面上的所有字段。点击保存按钮，并确认页面上没有任何橙色感叹号。
- en: At this point, your workflow should be fully configured and ready to run. Feel
    free to test it by clicking the “Run analysis” button and following the rest of
    the procedure for monitoring execution as you did previously in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，您的工作流应该已完全配置并准备好运行。随时点击“运行分析”按钮进行测试，并按照您之前在[第 11 章](ch11.xhtml#running_many_workflows_conveniently_in)中进行监视执行的其余步骤。
- en: With that, you have successfully imported a WDL workflow into Terra by way of
    the Broad Methods Repository, and configured it to run on a single sample using
    the direct file paths configuration option. That’s great because it means that
    you’re now able to take any WDL workflow you find out in the world and test it
    in Terra, assuming that you have the correct test data and an example JSON file
    available.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Broad Methods Repository 的方式成功将 WDL 工作流导入到 Terra，并使用直接文件路径配置选项配置为在单个样本上运行。这很棒，因为这意味着您现在可以将世界上任何找到的
    WDL 工作流在 Terra 中测试，假设您有正确的测试数据和一个示例 JSON 文件可用。
- en: However, what happens when you’ve successfully tested your workflow and you
    want to launch it on multiple samples at once? As you learned in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in),
    that’s where the data table comes in. You just reproduced the first exercise in
    that chapter, which demonstrated that you can run a workflow using paths to the
    files entered directly in the inputs configuration form. However, to launch the
    workflow on multiple samples, you need to set up a data table that lists the samples
    and the corresponding file paths on the workspace’s Data page. Then, you can configure
    the workflow to run on rows in the data table as you did in the second exercise
    in that chapter. In the next section, we show you how to set up your data table.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当您成功测试工作流并希望同时启动多个样本时会发生什么？正如您在[第 11 章](ch11.xhtml#running_many_workflows_conveniently_in)中学到的那样，这就是数据表发挥作用的地方。您刚刚重现了该章节中的第一个练习，该练习展示了您可以使用直接输入配置表单中的文件路径来运行工作流。但是，要在多个样本上启动工作流，您需要设置一个数据表，在工作区的数据页面上列出样本及其相应的文件路径。然后，您可以像该章节中的第二个练习那样配置工作流以在数据表中的行上运行。在接下来的部分中，我们将向您展示如何设置您的数据表。
- en: Adding the Data Table
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加数据表
- en: Remember that our tutorial workspace used an example dataset that resides in
    a public bucket in GCS, which we manage outside of Terra. The *sample* table on
    the Data page listed each sample, identified by a name that is unique within that
    table, along with the file paths to the corresponding BAM file and its associated
    index file, each in a column of its own.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们的教程工作空间使用了一个位于 GCS 中的公共桶中的示例数据集，我们在 Terra 外部进行管理。数据页面上的*样本*表列出了每个样本，通过在各自列中的文件路径来标识，其中
    BAM 文件及其关联的索引文件各自在一列中。
- en: 'So the question is, how do you re-create the *sample* data table in our blank
    workspace? What does Terra expect from you? In a nutshell, you need to create
    a *load file* in a plain-text format with tab-separated values. A [Terra documentation
    article](https://oreil.ly/lONJL) goes into more detail, but basically, it’s a
    spreadsheet that you save in TSV format. It’s reasonably straightforward except
    for a small twist that often trips people up on their first attempt: the file
    needs to have a header containing column names, and the first column must be the
    unique identifier for each row. Here’s a pro tip: rather than trying to create
    a load file from scratch, you can download an existing one from any workspace
    that has data tables and use that as a starting point. Here we’re going to be
    extra lazy and download the table TSV file from the original tutorial workspace,
    look at it and discuss a few key points, and then upload it as is to our blank
    workspace.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题是，你如何在我们的空白工作空间中重新创建*样本*数据表？Terra希望你做什么？简而言之，你需要创建一个以制表符分隔值的纯文本格式的*加载文件*。一篇[Terra文档文章](https://oreil.ly/lONJL)详细介绍了这一点，但基本上，它是一个你保存为TSV格式的电子表格。这相当简单，除了一个小技巧，通常会让初次尝试的人摔倒：文件需要有包含列名的头部，并且第一列必须是每行的唯一标识符。这里有一个专业建议：与其试图从头开始创建加载文件，不如从包含数据表的任何工作空间下载一个现成的文件，并以此作为起点。在这里，我们会非常懒惰，从原始教程工作空间下载表格TSV文件，查看它并讨论一些关键点，然后将其不加修改地上传到我们的空白工作空间。
- en: In the browser tab or window where you opened the original tutorial workspace,
    navigate to the Data page and click the blue Download All Rows button. This should
    trigger the download in your browser, and because it’s a very small text file,
    the transfer should complete immediately. Find the downloaded file and open it
    in your preferred spreadsheet editor; we’re using Google Sheets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在你打开原始教程工作空间的浏览器标签或窗口中，导航至数据页面，并点击蓝色的“下载所有行”按钮。这应该会在你的浏览器中触发下载，由于这是一个非常小的文本文件，传输应该会立即完成。找到下载的文件并在你偏爱的电子表格编辑器中打开；我们使用Google
    Sheets。
- en: Note
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can also use a plain-text editor for viewing the file contents, but we recommend
    using a spreadsheet editor whenever you modify TSV load tables because it reduces
    the risk of messing up the tab-delimited format.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看文件内容时，你也可以使用纯文本编辑器，但我们建议在修改TSV加载表时使用电子表格编辑器，因为这样可以减少搞乱制表符分隔格式的风险。
- en: 'As you can see in [Figure 13-7](#a_sample_data_table_from_the_tutorial_w),
    this file contains the raw content from the sample data table in the original
    workspace, so instead of seeing just *father.bam* with a hyperlink, for example,
    we have the full path to where the BAM file resides in the storage bucket. In
    addition, this table has a header row that contains the names of the columns as
    displayed in the workspace, with one exception: the name of the first column is
    preceded by *entity*: in the file, which is not the case in the workspace. That
    little detail marks the most important takeaway of this whole section: the column
    that contains the unique identifier for each row must be the first listed in the
    load file, and its name must end with the *_id* suffix. When you upload the load
    file into Terra, the system will derive the name of the data table itself from
    the name of that column, by chopping off both the *entity*: prefix and *_id* suffix.
    If you don’t include a column named following that pattern, Terra will reject
    your table. To be clear, the name that you give to the load file has no effect
    on the name of the table because it will be created in Terra.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[图 13-7](#a_sample_data_table_from_the_tutorial_w)中所看到的那样，这个文件包含了原始工作空间中样本数据表的原始内容，因此，例如，与超链接*father.bam*不同，我们在存储桶中看到了BAM文件的完整路径。此外，该表具有包含列名的头行，这些列名显示在工作空间中，但有一个例外：文件中第一列的名称前面有*entity*：前缀，在工作空间中则不是这样。这个小细节标志着本节的最重要的要点：包含每行唯一标识符的列必须是加载文件中列的第一列，并且它的名称必须以*_id*后缀结尾。当你将加载文件上传到Terra时，系统将根据该列的名称（去掉*entity*：前缀和*_id*后缀）派生出数据表的名称本身。如果你不包括按照这种模式命名的列，Terra将拒绝你的表。清楚地说，你给加载文件的名称不会影响表的名称，因为表将在Terra中创建。
- en: '![A sample data table from the tutorial workspace, viewed in Google Sheets.](Images/gitc_1307.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![从Google Sheets查看的教程工作空间中的样本数据表。](Images/gitc_1307.png)'
- en: Figure 13-7\. A sample data table from the tutorial workspace, viewed in Google
    Sheets.
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-7\. 从教程工作空间查看的样本数据表。
- en: Let’s try this out in practice. In your blank workspace, navigate to the Data
    page and click the “+” icon adjacent to TABLES on the DATA menu to bring up the
    TSV load file import dialog box, as shown in [Figure 13-8](#tsv_load_file_import_aright_parenthesis).
    You can drag and drop the file or click the link to open a file browser; use either
    method as you prefer to upload the TSV file that we were just looking at.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在实践中尝试一下。在您的空白工作空间中，导航到数据页面，单击数据菜单上TABLES旁边的“+”图标，以打开TSV加载文件导入对话框框，如[图13-8](#tsv_load_file_import_aright_parenthesis)所示。您可以拖放文件或单击链接打开文件浏览器；使用您喜欢的任何方法来上传我们刚才查看的TSV文件。
- en: '![TSV load file import A) button, and B) dialog.](Images/gitc_1308.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![TSV加载文件导入A）按钮和B）对话框。](Images/gitc_1308.png)'
- en: Figure 13-8\. TSV load file import A) button, and B) dialog.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-8. TSV加载文件导入A）按钮和B）对话框。
- en: 'If the upload completes successfully, you should now see the sample table listed
    in the data menu. Click its name to view its content, and compare it to the original:
    if you didn’t make any changes, it should be identical.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上传成功完成，您现在应该可以在数据菜单中看到示例表。单击其名称以查看其内容，并将其与原始内容进行比较：如果您没有进行任何更改，则应完全相同。
- en: Feel free to experiment with this sample load file to test the behavior of the
    TSV import function. For example, try modifying the name of the first column and
    see what happens. You can also try adding more columns with different kinds of
    content, like plain text, numbers, file paths, and lists of elements (use array
    formatting, with square brackets and commas to separate elements). And try modifying
    rows and creating more rows, either by adding rows to the same file or by making
    a new file with different rows. Just remember that if you’re editing the load
    file in a spreadsheet editor, as we recommend, you’ll need to make sure to save
    the file in TSV format.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试使用此示例加载文件来测试TSV导入功能的行为。例如，尝试修改第一列的名称并观察结果。您还可以尝试添加更多列，包括纯文本、数字、文件路径和元素列表（使用数组格式，用方括号和逗号分隔元素）。还可以尝试修改行并创建更多行，无论是在同一个文件中添加行还是创建具有不同行的新文件。只需记住，如果您正在电子表格编辑器中编辑加载文件（正如我们推荐的那样），您需要确保将文件保存为TSV格式。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注：
- en: Many spreadsheet editors (including Microsoft Excel) call this format *Tab-delimited
    Text* and will save the file with a *.txt* extension instead of *.tsv*, which
    is absolutely fine; Terra will not care about the extension as long as the contents
    are formatted correctly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 许多电子表格编辑器（包括Microsoft Excel）将此格式称为*制表符分隔文本*，并将文件保存为*.txt*扩展名而不是*.tsv*，这完全没问题；只要内容格式正确，Terra不会关心扩展名。
- en: 'If your data page ends up getting a bit messy as a result of this experimentation,
    don’t worry: you can delete rows or even entire tables; just select the relevant
    checkboxes (use the checkbox in the header to select an entire table) and click
    the trashcan icon that appears in the lower right of the window. The one limitation
    is that it’s not currently possible to delete columns from tables, so if you want
    to get rid of unwanted columns, you’ll need to delete the entire table and redo
    the upload procedure. For that reason, it can be a good idea to store versioned
    copies of what you consider the “good” states of your tables when you’re working
    on an actual project, especially if you’re planning to add data over time. There
    isn’t yet any built-in functionality to do that in Terra, so it’s a manual process
    of downloading the TSV and storing it somewhere (for example, in the workspace
    bucket), and in another location outside the workspace if you want to be extra
    cautious.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于此类实验而导致您的数据页面变得有些混乱，请不要担心：您可以删除行甚至整个表格；只需选择相关复选框（使用页眉中的复选框来选择整个表格），然后单击窗口右下角出现的垃圾桶图标。唯一的限制是目前无法从表格中删除列，因此如果您想要去除不需要的列，则需要删除整个表格并重新进行上传程序。出于这个原因，在您真正的项目中工作时，特别是如果您计划随时间添加数据，将您认为是“良好”状态的表格的版本副本存储起来可能是个好主意。在Terra中还没有内置此功能，因此这是一个手动的过程，需要将TSV下载并存储在某个地方（例如工作空间存储桶）以及在其他位置外部存储，如果您想要额外谨慎的话。
- en: After you’re done having fun with the data tables, move on to the resource data
    in the Workspace Data table.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在您完成与数据表的乐趣后，请转移到工作空间数据表中的资源数据。
- en: Filling in the Workspace Resource Data Table
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填写工作空间资源数据表
- en: As you might recall, the purpose of this table is to hold variables that we
    might want to use in multiple workflow configurations, like the genome reference
    sequence file, for example, or the GATK Docker container. This allows you to configure
    it only once, and just point to the variable in any configuration that needs it.
    Not only do you not need to ever look up the file path again, but if you decide
    to update the version or location of the file, you need to do it in only one place.
    That being said, you might not always want updates to be propagated to every use
    of a given resource file or parameter setting. When you go to set up your own
    analysis, be sure to think carefully about how you want to manage common resources
    and “default” parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this data table has a very simple structure because it’s all just
    key:value pairs, and it comes with an easy form-like interface for adding and
    modifying elements. You can simply use that interface to copy over the contents
    of the table from the original workspace, if you don’t mind the manual process.
    Alternatively, you can download the contents of the original in TSV format by
    clicking the Download All Rows link, and upload that without modification in your
    workspace using the Upload TSV link. You can even drag the file from your local
    filesystem into the table area if you prefer. Feel free to experiment and choose
    the method that suits you best. With that, you’re done setting up the data, so
    it’s time to get back to the workflow.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Workflow Configuration That Uses the Data Tables
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your not-so-blank-anymore workspace, navigate back to the Workflows page
    and look at the configuration listed there, but don’t open it. Click the circle
    with three vertically stacked dots, which you might by now recognize as the symbol
    that Terra uses to provide action options for a particular asset, be it a workspace,
    workflow, or notebook. On the action menu that opens, select “Duplicate workflow”
    and then give the copy a name that will indicate that it is going to use the data
    table, as we did in the tutorial workspace.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s currently not possible to simply rename a workflow configuration in place.
    If you want to give the “file paths” version of your workflow configuration a
    name that is equally explicit as we did in the tutorial workspace, you’ll need
    to create another duplicate with the name you want and then delete the original.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the copy of the configuration that you want to modify to use the data
    table, and this time, switch the configuration option to “Process multiple workflows.”
    On the drop-down menu, select the *sample* table and choose whether to select
    a subset of the data or go with the default behavior, which is to run the workflow
    on all items in the table. Now comes the more tedious part of this exercise, which
    is to connect the input assignments on the Inputs page to specific columns in
    the data table or variables in the workspace resource data table. Unfortunately,
    this time you don’t have a prefilled JSON available as a shortcut, but on the
    bright side, the Inputs page has an assistive autocomplete feature that speeds
    up the process considerably. For each variable that needs to be connected (excluding
    a couple of runtime parameters that we simply leave hardcoded, which you can look
    up in the original workspace), start typing either `**workspace**` or `**this**`
    in the relevant text box. This brings up a contextual menu listing all options
    from the relevant table: `this` points to whatever table you selected on the configuration’s
    drop-down menu, and `workspace` always points to the workspace resource data,
    which lists the reference genome sequence, Docker images, and so on.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 打开要修改以使用数据表的配置副本，并在这次将配置选项切换到“处理多个工作流程”。在下拉菜单中，选择*sample*表，并选择是选择数据子集还是采用默认行为，即在表中的所有项目上运行工作流程。现在是这项练习更繁琐的部分，即将输入页面上的输入分配连接到数据表的特定列或工作空间资源数据表中的变量。不幸的是，这次你没有可用的预填充JSON作为快捷方式，但值得一提的是，输入页面具有辅助自动完成功能，可以显著加快这一过程。对于每个需要连接的变量（不包括我们只是留下硬编码的几个运行时参数，你可以在原始工作区中查找），在相关文本框中开始输入`**workspace**`或`**this**`。这将显示一个上下文菜单，列出相关表的所有选项：`this`指向您在配置下拉菜单中选择的任何表，而`workspace`始终指向工作空间资源数据，其中列出了参考基因组序列、Docker镜像等。
- en: Another way to use the assistive autocomplete feature on the Inputs page is
    to start typing part of the input variable name in the corresponding input text
    box. If the relevant data table column or workspace resource variable was set
    up with the same name as the input variable, as is the case in our tutorial workspace,
    this will typically pull up a much shorter list of matching values. For example,
    typing `**ref**` in one of the input fields would bring up only the workspace
    reference sequence, its index file, and it dictionary file. This approach can
    be blissfully faster when you’re dealing with a large number of variables, but
    it is not guaranteed to work for every configuration because it relies on the
    names matching well enough, which will not always be the case. Some people just
    want to watch the world burn.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入页面上使用辅助自动完成功能的另一种方法是在相应的输入文本框中开始输入输入变量名称的一部分。如果相关数据表列或工作空间资源变量与输入变量名称相同，正如我们教程工作区的情况一样，这通常会显示出一系列匹配值。例如，在一个输入字段中输入`**ref**`将仅显示工作空间参考序列、其索引文件和字典文件。当您处理大量变量时，这种方法可能会更快，但不能保证对每个配置都有效，因为它依赖于名称是否足够匹配，这并非总是情况。有些人只是想看着世界燃烧。
- en: Go ahead and fill out the Inputs page, referring to the original workspace if
    you become stuck at any point. When you’re done, make sure to click the Save button
    and confirm that there are no more exclamation marks left, as occurred previously.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 继续填写输入页面，如果在任何时候遇到困难，请参考原始工作区。完成后，请确保单击保存按钮，并确认没有像之前那样留下任何感叹号。
- en: What do you think, is this workflow ready to launch or what? What. Meaning,
    you do have one more configuration task left, which was not applicable in the
    “file paths” round. Pop over to the Outputs page, which requires you to indicate
    what you want to do with the workflow outputs. To be clear, this won’t change
    where the files are written; that is determined for you by the built-in Cromwell
    server, as you learned in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in).
    What this does is determine under what *column name* the outputs will be added
    to the data table. You can click the “Use defaults” link to simply use the output
    variable names, or you can specify something different—either a column that already
    exists, or a new name that the system will use to create a new column. In our
    workspace, we chose to use the defaults.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now you’re done. Go ahead and launch the workflow to try it out and
    then sit back and delight in the knowledge that you’ve just leveled up in a big
    way. Figuring out how to use the data table effectively is generally considered
    one of the most challenging aspects of using Terra, and here you are. You’re not
    an expert yet—that will come when you master the art of using multiple linked
    data tables, like participants and sample sets—but you’re most of the way there.
    We cover that in the next scenario, in which you’ll import data from the Terra
    Data Library into a GATK Best Practices workspace.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Before we get to that, however, we still need to tackle the Jupyter Notebook
    portion of the tutorial workspace, which we used in [Chapter 12](ch12.xhtml#interactive_analysis_in_jupyter_noteboo).
    But don’t worry, it’s going to be mercifully brief—if you take the easy road,
    that is.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Adding the Notebook and Checking the Runtime Environment
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your increasingly well-equipped workspace, navigate to the Notebooks page,
    and take a guess at what should be your next step. Technically, you have a choice
    here. You could create a blank new notebook and re-create the original by typing
    in all the cells. To do that, click the box labeled Create New Notebook, and then,
    in the dialog that opens, give it a name and select Python 3 as the language.
    Then, you can work through [Chapter 12](ch12.xhtml#interactive_analysis_in_jupyter_noteboo)
    a second time, typing the commands yourself instead of just running the cells
    in the prebaked version as you did last time. Alternatively, you can take the
    easy road and simply upload a copy of the original notebook, which like everything
    else is available in the GitHub repository. Specifically, the cleared copy and
    the previously run copy of the notebook are [here](https://oreil.ly/jbM8T). Retrieve
    a local copy of the file and use the “Drag or Click to Add an ipynb File” option
    on the Notebooks page to upload it to your workspace.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Whichever road you choose to take, you don’t need to customize the runtime environment
    again if you created your workspace under the same billing project as the one
    you used to work through [Chapter 12](ch12.xhtml#interactive_analysis_in_jupyter_noteboo),
    because your runtime is the same across all workspaces within a billing project.
    Convenient, isn’t it? Perhaps, but it’s also a bit of a cop-out; if you were truly
    building this workspace from scratch, you would need to go back and follow the
    instructions in [Chapter 12](ch12.xhtml#interactive_analysis_in_jupyter_noteboo)
    again to customize the runtime environment. So, enjoy the opportunity to be lazy,
    but keep in mind that in a different context, you might still need to care about
    the runtime environment.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪条路，如果您在与您用于通过[第12章](ch12.xhtml#interactive_analysis_in_jupyter_noteboo)进行工作的同一计费项目下创建了工作区，则无需再次定制运行时环境，因为您在计费项目内的所有工作区中运行时环境都是相同的。方便，不是吗？也许是，但这也有点懒惰；如果您真的要从头开始构建此工作区，您将需要返回并重新按照[第12章](ch12.xhtml#interactive_analysis_in_jupyter_noteboo)中的说明定制运行时环境。所以，享受懒惰的机会，但请记住，在不同的情境下，您可能仍然需要关注运行时环境。
- en: When you’re done playing with the notebook(s) in your workspace, take a step
    back and check that you have successfully re-created all functional aspects of
    the tutorial workspace. Does it all work? Well done, you! You now know all the
    basic mechanisms involved in assembling your own workspace from individual components.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在工作区中使用笔记本完成所有教程工作的功能方面时，请退后一步，检查您是否成功重新创建了所有功能方面的教程工作区。一切都正常吗？做得好！您现在已经了解如何从单个组件中组装自己的工作区的所有基本机制。
- en: Documenting Your Workspace and Sharing It
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录您的工作区并分享它
- en: The one thing you haven’t done yet to match (or outdo) the original tutorial
    workspace is to fill out the workspace description in the Dashboard, beyond whatever
    short placeholder you put in there when you created the workspace. If you have
    a little bit of steam left in you, we encourage you to do that now while your
    memory is fresh. All it takes is clicking the editing symbol that looks like a
    pencil on the Dashboard page, which opens up a Markdown editor that includes a
    graphical toolbar and a split-screen preview mode.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您还没有做到与原始教程工作区匹配（或超越）的唯一事情是填写仪表板中工作区描述的内容，除了您创建工作区时放入的短暂占位符之外。如果您还有一点精力，我们鼓励您在记忆尚新的时候立即完成这项工作。只需点击看起来像是仪表板页面上铅笔的编辑符号，它会打开一个包含图形工具栏和分屏预览模式的
    Markdown 编辑器。
- en: We don’t usually like to say anything is self-explanatory, because that kind
    of qualifier just makes you feel even worse if you end up struggling with the
    thing in question, but this one is as close as it gets to deserving that label.
    Take it for a spin, click all the buttons, and see what happens. Messing around
    with the workspace description is one of the only things you can do on the cloud
    that is absolutely free, and in a private clone, it’s completely harmless, so
    enjoy it. This is also a great opportunity to write up some notes about what you
    learned in the process of working through the Terra chapters, perhaps even some
    warnings to your future self about things you struggled with and need to watch
    out for.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不喜欢说任何事情是不言自明的，因为这种修饰语只会让您在处理有问题的事物时感觉更糟，但这个就是最接近应得到那个标签的东西。试一试吧，点击所有按钮，看看会发生什么。在云端处理工作区描述是唯一完全免费的事情之一，在私有克隆中，它完全是无害的，所以请享受。这也是一个很好的机会，可以在您通过处理
    Terra 章节的过程中写下一些关于您学到的内容的笔记，也许还有关于您未来可能会遇到的问题的警告。
- en: Finally, consider sharing your workspace with a friend or colleague. To do so,
    click the same circle-with-dots symbol you used to clone the tutorial workspace
    in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in). This opens
    a small dialog box in which you can add their email address and set the privileges
    you want to give them. Click Add User and then be sure to click Save in the lower-right
    corner. If your intended recipient doesn’t yet have a Terra account, they will
    receive an email inviting them to join, along with a link to your workspace. They
    don’t need to have their own billing project if they just want to look at the
    workspace contents.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Starting from a GATK Best Practices Workspace
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you just experienced, setting up a new workspace from its base components
    takes a nontrivial amount of effort. Not that it’s necessarily *difficult*—you
    probably perform far more complex operations on a daily basis as part of your
    work—but there are a lot of little steps to follow, so until you’ve done it a
    bunch of times, you’ll probably need to refer to these instructions or to your
    own notes throughout the process.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that there are various opportunities to take shortcuts. For
    example, if you simply want to run the official GATK Best Practices workflows
    as provided in the featured workspaces by the GATK support team, you can skip
    an entire part of this process outright by starting from the relevant workspace.
    Those workspaces already contain example data and resource data as well as the
    workflows themselves, fully configured and ready to run. All you really need to
    do is clone the workspace of interest and add the data that you want to run the
    workflow(s) on to the data tables.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we run through that scenario so you can get a sense of what
    that entails in practice, the potential complications, and your options for customizing
    the analysis. The lessons from this scenario will generally apply beyond the GATK
    workspaces, to any case for which you have access to a workspace that is already
    populated with the basic components of an analysis. We think of this as the “just
    add water” scenario, which is an attractive model for tool developers to enable
    researchers to use their tools appropriately and with minimal effort. It’s also
    a promising model for boosting the computational reproducibility of published
    papers given that such a workspace constitutes the ultimate methods supplement.
    We discuss the mechanics and implications of this in more detail in [Chapter 14](ch14.xhtml#making_a_fully_reproducible_paper).
    For now, let’s get to work on that GATK Best Practices workspace.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Cloning a GATK Best Practices Workspace
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to use the [Germline-SNPs-Indels-GATK4-hg38 workspace](https://oreil.ly/2I8RE),
    which showcases the GATK Best Practices for short variant discovery implemented
    as three separate workflows. The first workflow, named `1_Processing_for_Variant_Discovery`,
    takes in raw sequencing data and outputs an analysis BAM file for a single sample.
    The second, `2_HaplotypeCaller_GVCF`, takes that BAM file and runs variant calling
    to produce a single-sample GVCF file. Finally, the third, `3_Joint_Discovery`,
    takes multiple GVCF files and applies joint calling to produce a multisample callset
    for the cohort of interest.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to that workspace now and clone it as you have done previously with
    other workspaces. When you’re in your clone, have a quick look around to become
    acquainted with its contents. You’ll find a detailed description in the Dashboard,
    three data tables and a set of predefined resources on the Data page, and the
    three fully configured workflows mentioned earlier on the Workflows page.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Examining GATK Workspace Data Tables to Understand How the Data Is Structured
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the three data tables on the Data page. One table
    is named *participant*. It contains a header line showing the name of the single
    column, *participant_id*, and a single row, populated by our beloved NA12878,
    or *mother* as we call her in this book:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '| participant_id |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| NA12878 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: This identifies her as a study *participant*; in other words, the actual person
    from whom biological samples were originally collected to produce the data that
    we will ultimately analyze.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The `participant_id` attribute is the unique identifier for the participant
    and is used to index the table. If you were to add attributes to the table (for
    example, some phenotype information like the participant’s height, weight, and
    health status), they would be added as columns to the right of the identifier.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'The second table is called *sample* and contains multiple columns as well as
    two sample rows below the header. This table is more complicated than the previous
    one largely because in addition to the minimum inputs required to run the workflow,
    it contains outputs of previous runs of the workflow. Here is a minimal version
    of the table without those outputs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '| sample_id | flowcell_unmapped_bams_list | participant _id |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| NA12878 | NA12878.ubams.list | NA12878 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| NA12878_small | NA12878_24RG_small.txt | NA12878 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: In this table, you see that the leftmost column is *sample_id*, the unique identifier
    of each sample, which is used as index value for that table. Jumping briefly to
    the other end of the table, the rightmost column is *participant_id*. Can you
    guess what’s happening here? That’s right, this is how we indicate which sample
    belongs to which participant. The *sample* table is linking back to the *participant*
    table. This might seem boring, but it’s actually quite important, because by establishing
    this relationship, we make it possible for the system to follow those references.
    Using a similar relationship between two tables, we’ll be able to do things like
    configure a workflow to run on a set of samples to analyze them jointly instead
    of launching the workflow individually on each sample. If you find that confusing,
    don’t worry about it now; we discuss it in more detail further down. For now,
    just remember that these tables are connected to each other.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表格中，你会看到最左边的列是*sample_id*，每个样本的唯一标识符，用作该表格的索引值。快速跳到表格的另一端，最右边的列是*participant_id*。你能猜到这里发生了什么吗？没错，这是我们用来指示哪个样本属于哪个参与者的方式。*sample*
    表格在回指到 *participant* 表格。这可能看起来无聊，但实际上非常重要，因为通过建立这种关系，我们使系统能够跟踪这些引用。通过两个表格之间类似的关系，我们将能够执行像配置工作流一样的任务，以便联合分析样本，而不是在每个样本上单独启动工作流。如果你觉得这很混乱，现在不用担心；我们将在后面更详细地讨论这个问题。现在只需记住，这些表格彼此连接着。
- en: The middle column, *flowcell_unmapped_bams_list*, points to sequencing data
    that has been generated from a biological sample. Specifically, this data is provided
    in the form of a list of unmapped BAM files that contain sequencing data from
    individual read groups. As you might recall from [Chapter 2](ch02.xhtml#genomics_in_a_nutshell_a_primer_for_new),
    the read groups are subsets of sequence data generated from the same sample in
    different lanes of the flowcell. The data processing portion of the workflow expects
    these subsets of data to be provided in separate BAM files so that it can process
    them separately for the first few steps and then merge the subsets in a single
    file containing all the data for that sample.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 中间列 *flowcell_unmapped_bams_list* 指向从生物样本生成的测序数据。具体而言，这些数据以未映射 BAM 文件列表的形式提供，其中包含来自不同流式细胞流道的同一样本的测序数据。工作流程的数据处理部分期望提供这些数据子集的分离
    BAM 文件，以便它可以分别处理这些数据的前几个步骤，然后将这些子集合并到一个包含该样本所有数据的单个文件中。
- en: For a given sample, the list of unmapped BAMs will be the primary input to the
    *Processing_for_Variant_Discovery* workflow. You can verify this by taking a look
    at the workflow configuration, where the input variable `flowcell_unmapped_bams`
    is set to `this.flowcell_unmapped_bams_list`. Recall from our [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in)
    forays into launching workflows on data tables that syntax reads out as “for each
    row in the table, give the value in the *flowcell_unmapped_bams_list* column to
    this variable.” The main output of that workflow will be added under the *analysis_ready_bam*
    column, which you can see in the full table in the workspace. That column in turn
    will serve as the input for the *HaplotypeCaller_GVCF* workflow, which will then
    output its contents into the *gvcf* column.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的样本，未映射的 BAM 文件列表将是 *Processing_for_Variant_Discovery* 工作流的主要输入。你可以通过查看工作流配置来验证这一点，其中输入变量
    `flowcell_unmapped_bams` 设置为 `this.flowcell_unmapped_bams_list`。回想一下我们在 [第11章](ch11.xhtml#running_many_workflows_conveniently_in)
    中对数据表启动工作流的探索，语法读出为“对于表中的每一行，将 *flowcell_unmapped_bams_list* 列中的值传递给这个变量。”该工作流的主要输出将添加到工作区中完整表格的
    *analysis_ready_bam* 列下。这一列反过来将作为 *HaplotypeCaller_GVCF* 工作流的输入，后者将其内容输出到 *gvcf*
    列中。
- en: The two sample rows correspond to two versions of the original whole genome
    dataset derived from participant *NA12878*. In the first row, the NA12878 sample
    is the full-scale dataset, whereas the *NA12878_small* sample in the second row
    is a downsampled version, meaning that it contains only a subset of the original
    data. The purpose of the downsampled version is to run tests more quickly and
    cheaply.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 两个样本行对应于从参与者 *NA12878* 派生的原始全基因组数据集的两个版本。在第一行中，NA12878 样本是全尺度数据集，而第二行中的 *NA12878_small*
    样本是经过降采样的版本，这意味着它仅包含原始数据的一个子集。降采样版本的目的是更快速、更便宜地进行测试。
- en: 'Finally, the third table is called *sample_set*, which might or might not sound
    familiar, depending on how much you paid attention in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in).
    Do you remember what happened when you ran a workflow on rows in the data table?
    The system automatically created a row in a new *sample_set* table, listing the
    samples on which you had launched the workflow. In this workspace, you can see
    a sample set called *one_sample,* with a link labeled *1 entity* under the *samples*
    column, as shown here simplified:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '| sample_set_id | samples |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| one_sample | 1 entity |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: If you click that, a window opens and displays a list referencing the NA12878
    sample. This is another example of a connection between tables, and we’re going
    to take advantage of this one specifically in a very short while.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The system doesn’t actually show how this sample set was created, but from what
    we know about the workflows, we can deduce that running either the first or the
    second workflow in this workspace on the NA12878 sample would replicate the creation
    of the sample set. Mind you, it’s also possible to create sample sets manually;
    we show how that works in a little bit.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s interesting here is that the *one_sample* row in this sample set has
    output files associated with it, which was not the case when we ran workflows
    in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in). Can you guess
    what that might mean? We’ll give you a hint: the *output_vcf* column contains
    a VCF of variant calls. Got it? Yes, this is the result of running the *Joint_Discovery*
    workflow on the sample set: it took as input the list of samples referenced in
    the *samples* column of the sample set, and attached its VCF output to the sample
    set. This is a somewhat artificial example given that the sample set contains
    only one sample, so we’ll run this again on a more realistic sample set later
    in this section in order to give you a more meaningful experience of this logic.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve dissected the data tables in this way, we can summarize the
    structure of the dataset and the relationships between its component entities.
    The result, illustrated in [Figure 13-9](#visual_representation_of_the_data_model),
    is what we call the *data model*: Participant NA12878 has two associated Samples,
    NA12878 and NA12878_small, and one Sample Set references the NA12878 Sample.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Visual representation of the data model—the structure of the example dataset.](Images/gitc_1309.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Figure 13-9\. The data model—the structure of the example dataset.
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that having a sample identifier that is identical to a participant identifier
    normally happens only when there is just one sample per participant. In a realistic
    research context in which a participant has multiple samples associated with it,
    you would expect to have different identifiers. You would also expect the two
    sample datasets to have originated from different biological samples or to have
    been generated using different assays, whereas we know that in this case they
    originated from the same one. Again, this is due to the somewhat artificial nature
    of the example data. In the next step of this scenario, we’re going to look at
    some data that follows more realistic expectations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有在一个参与者只有一个样本的情况下，样本标识符与参与者标识符相同才会发生。在现实的研究背景中，一个参与者可能会有多个相关样本，因此你预期会有不同的标识符。此外，你预期这两个样本数据集可能源自不同的生物样本或者是使用不同的分析方法生成，而我们知道在这种情况下，它们来自同一个样本。再次强调，这是由于示例数据的某种人为特性。在此场景的下一步中，我们将会查看一些符合更现实期望的数据。
- en: Getting to Know the 1000 Genomes High Coverage Dataset
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 1000 基因组高覆盖数据集
- en: As we discussed earlier, the Data Library provides access to a collection of
    datasets hosted by various organizations in several kinds of data repositories.
    Accordingly, the protocol for retrieving data varies depending on the repository.
    In this exercise, we retrieve data from a repository that hosts a whole-genome
    dataset of the 2,504 samples from Phase 3 of the 1000 Genomes Project, which were
    recently resequenced by the New York Genome Center as part of the [AnVIL project](https://oreil.ly/Z2vfO).
    This particular repository simply consists of a [public workspace](https://oreil.ly/195yq)
    containing only data tables, which serves as a simple but effective form of data
    repository at this scale.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，数据库提供了对由各种组织托管在几种数据库中的数据集的访问。因此，根据数据库的不同，检索数据的协议也有所不同。在这个练习中，我们从一个仅包含来自第三阶段
    1000 基因组计划的 2,504 个样本的全基因组数据集的仓库中检索数据，这些数据最近由纽约基因组中心作为 [AnVIL 项目](https://oreil.ly/Z2vfO)
    的一部分重新测序。这个特定的仓库只是一个包含数据表的 [公共工作空间](https://oreil.ly/195yq)，在这个规模下作为数据仓库的一种简单而有效的形式。
- en: 'Go to the 1000 Genomes data workspace now, either by clicking the direct link
    above or by browsing the Data Library. If you choose the latter route, be aware
    that there is another 1000 Genomes Project data repository, as shown in [Figure 13-10](#the_terra_data_library_contains_two_dif),
    but it’s quite different: that one contains the Low Coverage sequence data for
    the full 3,500 study participants, whereas we want the new High Coverage sequence
    data from the 2,504 participants who were included in Phase 3 of the project.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在前往 1000 基因组数据工作空间，可以通过点击上面的直接链接或浏览数据库来实现。如果选择后者，请注意，如 [图 13-10](#the_terra_data_library_contains_two_dif)
    所示，还有另一个 1000 基因组项目数据仓库，但它与我们所需的大不相同：那一个包含了完整 3,500 名研究参与者的低覆盖序列数据，而我们需要的是来自项目第三阶段的
    2,504 名参与者的新高覆盖序列数据。
- en: '![The Terra Data Library contains two different repositories of data from the
    1000 Genomes Project.](Images/gitc_1310.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![Terra 数据库包含来自 1000 基因组计划的两个不同的数据仓库。](Images/gitc_1310.png)'
- en: Figure 13-10\. The Terra Data Library contains two repositories of data from
    the 1000 Genomes Project.
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-10。Terra 数据库包含来自 1000 基因组计划的两个数据库。
- en: 'When you’re in the workspace, head over to the Data page to check out the data
    tables. You’ll see the same three tables as we described in the GATK workspace:
    *sample*, *participant*, and *sample_set*. As expected, the *participant* table
    lists the 2,504 participants in the dataset. Similarly, the *sample* table lists
    the paths to the location of the corresponding sequence data files, which are
    provided in CRAM format as well as a GVCF file produced by running a `HaplotypeCaller`
    pipeline on the sequence data. In addition, both contain a lot of metadata fields
    that were not present in the GATK workspace, including information about the data
    generation stage (type of instrumentation, library preparation protocol, etc.)
    and the population of origin of the study participant. The presence of all this
    metadata is a big indicator that this is a more realistic dataset, compared to
    the toy example data in the GATK workspace. Meanwhile, the *sample_set* table
    contains a list of all 2,504 samples associated with the sample set name *1000G-high-coverage-2019-all*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在工作区时，转到数据页面查看数据表。你会看到与我们在GATK工作区中描述的相同的三个表：*样本*、*参与者*和*样本集*。正如预期的那样，*参与者*表列出了数据集中的2,504名参与者。同样，*样本*表列出了对应序列数据文件位置的路径，这些文件以CRAM格式提供，以及通过运行`HaplotypeCaller`流程生成的GVCF文件。此外，两者都包含了许多在GATK工作区中不存在的元数据字段，包括关于数据生成阶段（仪器类型、文库制备协议等）和研究参与者的原始人口信息。所有这些元数据的存在是一个很大的指标，表明这是一个更真实的数据集，与GATK工作区中的玩具示例数据相比。与此同时，*样本集*表包含了与样本集名称*1000G-high-coverage-2019-all*相关的所有2,504个样本的列表。
- en: 'We can summarize the data model for this dataset as follows: each Participant
    has a single associated Sample, and there is one Sample Set that references all
    the Samples, as shown in [Figure 13-11](#visual_representation_of_the_data_mode).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结这个数据集的数据模型如下：每个参与者都有一个关联的样本，并且有一个参考所有样本的样本集，如[图13-11](#visual_representation_of_the_data_mode)所示。
- en: '![Visual representation of the data model for the 1000 Genomes High Coverage
    dataset.](Images/gitc_1311.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![1000基因组高覆盖数据集的数据模型的可视化表示。](Images/gitc_1311.png)'
- en: Figure 13-11\. The data model for the 1000 Genomes High Coverage dataset.
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-11\. 1000基因组高覆盖数据集的数据模型。
- en: 'At this point, we have a confession to make: all this time we’ve spent looking
    at the data tables of the two workspaces wasn’t mere data tourism. It all had
    a very specific purpose, to answer the unspoken question that you might or might
    not have seen coming: will their data models be compatible?'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有一个坦白要做：我们花在两个工作区的数据表上的所有时间并不仅仅是数据旅游。这一切都有一个非常具体的目的，回答一个你可能已经看到或者可能没有看到的未明说的问题：它们的数据模型是否兼容？
- en: And the good news is yes, they seem to be compatible, meaning that you can combine
    their data tables without causing a conflict, and you should be able to run a
    *federated data analysis* across samples from both datasets. Admittedly the degree
    of federation here is not really meaningful because only one person’s genomic
    information is in the GATK workspace, but the underlying principle will apply
    equally to other datasets that you might want to bring together into a single
    workspace.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 而且好消息是是的，它们似乎是兼容的，这意味着你可以将它们的数据表合并而不会引起冲突，你应该能够对来自两个数据集的样本进行*联合数据分析*。诚然，这里的联合程度并不真正有意义，因为只有一个人的基因组信息在GATK工作区中，但这个基本原则同样适用于你可能想要合并到单个工作区中的其他数据集。
- en: Now that we’re satisfied that we should be able to use the data in our workspace,
    let’s figure out how to actually perform the transfer of information.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们满意地认为我们应该能够在我们的工作区中使用数据，让我们弄清楚如何实际执行信息的传输。
- en: Copying Data Tables from the 1000 Genomes Workspace
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从1000基因组工作区复制数据表
- en: 'There are two main ways to do this: a point-and-click approach, and a load
    file–based approach. Let’s begin with the point-and-click approach because it
    abstracts away some of the complexity involved in using load files.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要方法可以做到这一点：一种是点对点的方法，另一种是基于加载文件的方法。让我们从点对点的方法开始，因为它将一些在使用加载文件时涉及的复杂性抽象化。
- en: On the Data page of the 1000 Genomes High Coverage workspace, click the *sample*
    table and choose a few samples by selecting their corresponding checkboxes on
    the left. Locate and click the three-dots symbol above the table to open the action
    menu (as you did in [Chapter 11](ch11.xhtml#running_many_workflows_conveniently_in)
    to configure data inputs for the workflow) and select “Export to Workspace” as
    shown in [Figure 13-12](#the_copy_data_to_workspace_dialog_boxdo).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在1000基因组高覆盖工作区的数据页面上，点击*样本*表，并通过左侧选择其对应复选框选择几个样本。在表格上方找到并点击三点符号以打开操作菜单（就像你在[第11章](ch11.xhtml#running_many_workflows_conveniently_in)中为工作流配置数据输入时所做的那样），然后选择“导出到工作区”，如[图13-12](#the_copy_data_to_workspace_dialog_boxdo)所示。
- en: '![The Copy Data to Workspace dialog box.](Images/gitc_1312.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![复制数据到工作区对话框。](Images/gitc_1312.png)'
- en: Figure 13-12\. The Copy Data to Workspace dialog box.
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-12。复制数据到工作区对话框。
- en: This should open a dialog box with the list of samples and a workspace selection
    menu. Select your clone of the GATK workspace and then click Copy. Keep in mind
    that despite what the buttons seem to imply, we’re not going to copy over any
    files; we just want to copy over the metadata in the data tables, which include
    the paths to the locations of the files in GCS. So, rest assured you’re not suddenly
    going to be on the hook for a big data storage bill.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打开一个对话框，其中列出了样本列表和工作区选择菜单。选择你的GATK工作区的克隆，然后点击复制。请记住，尽管按钮似乎暗示我们要复制文件，但我们只想复制数据表中的元数据，其中包括文件在GCS中的位置路径。所以，请放心，你不会突然间面临大额数据存储费用的问题。
- en: You should see a confirmation message that asks you whether you want to stay
    where you are (in the 1000 Genomes workspace) or go to where you copied the data
    (the GATK workspace). Select the latter option so we can go look at what the transfer
    produced. If everything worked as expected, you should see the samples you selected
    listed in the *sample* table. However, you will not see the corresponding participants
    listed in the *participant* table, because the system does not automatically copy
    over the data from other tables to which the selected data refers. The only exceptions
    to this are sets, like the sample sets we’ve been working with. If you select
    a set and copy that over, the system will also copy over the contents of the set.
    If you also want to copy over the participants, you need to do it as a separate
    step, either by selecting the rows of interest using the checkboxes or by defining
    a participant set using a load file.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一个确认消息，询问你是否要留在当前位置（1000基因组工作区）还是转到你复制数据的位置（GATK工作区）。选择后者选项，这样我们可以查看转移产生的结果。如果一切按预期工作，你应该会在*样本*表中看到你选择的样本列表。然而，在*参与者*表中，你不会看到对应的参与者列表，因为系统不会自动复制所选数据引用的其他表中的数据。唯一的例外是集合，例如我们一直在使用的样本集。如果你选择一个集合并复制它，系统也会复制集合的内容。如果你还想复制参与者，你需要作为一个单独的步骤进行，要么通过选择复选框选择感兴趣的行，要么通过使用加载文件定义参与者集合。
- en: If you play around with the “copy data to workspace” functionality a little
    bit, you will quickly realize that this approach is fairly limiting for dealing
    with large datasets. Why? Because when you try to select all rows in a table,
    even using the checkbox in the upper-left corner of the table, the system really
    selects only the items that are displayed on the page. Because the maximum allowed
    for that is one hundred items, the set approach is your only option for copying
    over more than one hundred samples at a time using the point-and-click approach.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微尝试一下“复制数据到工作区”的功能，你很快就会意识到，这种方法在处理大型数据集时相当有限。为什么呢？因为当你尝试选择表中的所有行时，即使使用表格左上角的复选框，系统实际上只会选择显示在页面上的项目。由于允许的最大数量为一百个项目，所以使用集合方法是你通过点和点击方法一次复制超过一百个样本的唯一选择。
- en: So now let’s try the load file–based approach, which has some advantages if
    you are working with large datasets or need to tweak something before copying
    over the data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试基于加载文件的方法，如果你正在处理大型数据集或需要在复制数据之前进行某些调整，则这种方法具有一些优势。
- en: Using TSV Load Files to Import Data from the 1000 Genomes Workspace
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TSV加载文件从1000基因组工作区导入数据
- en: Technically, you’ve already done this—that’s how we had you copy over the data
    table from the original book tutorial workspace to re-create your own earlier
    in this chapter. You selected the table, clicked the Download All Rows button,
    retrieved the downloaded file, and then uploaded it to the new workspace. Boom.
    All our samples are belong to you.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你之前已经做过这个——这就是我们让你在本章早些时候复制原始书籍教程工作空间中的数据表来重新创建自己的工作空间的方式。你选择了表格，点击了“下载所有行”按钮，获取了下载的文件，然后上传到了新的工作空间。完成。所有我们的样本都属于你。
- en: 'However, this time there’s a twist: several data tables are involved, and some
    of them have references to the others. As a result, there’s going to be an order
    of precedence: you must start with the table that doesn’t have any references
    to the others because Terra can’t handle references to things that it hasn’t seen
    defined yet. For example, you can’t upload a sample set before having uploaded
    the samples to which the sample set refers. This is an annoying limitation, and
    you could imagine having the system simply create a stand-in for any reference
    to something that it doesn’t recognize. Yet we need to work with the system in
    its current state, so the bottom line is this: order matters; deal with it.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一次有点不同：涉及到几个数据表，并且其中一些表彼此有引用。因此，有一个优先顺序：你必须从没有引用其他表的表开始，因为Terra不能处理对尚未定义的事物的引用。例如，你不能在上传样本集之前先上传样本，因为样本集引用了这些样本。这是一个令人恼火的限制，你可以想象系统简单地为任何它不认识的引用创建一个替代对象。然而，我们需要按照当前状态来操作系统，因此底线是：顺序很重要；接受这个事实吧。
- en: 'In practice, for this exercise, you need to download the TSVs for the three
    data tables from the 1000 Genomes workspace, as previously described (in any order),
    and then upload them in the correct order to your GATK workspace clone. What is
    the correct order, you ask? Here’s a hint: look at the visual representation of
    the data model in [Figure 13-11](#visual_representation_of_the_data_mode) and
    use the direction of the arrows to inform your decision. What do you think? That’s
    right, you first upload the participants, then the samples, and then the sample
    set. Go ahead and do that now.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，为了进行这个练习，你需要从1000基因组工作空间下载三个数据表的TSV文件，如之前所述（以任意顺序），然后按照正确的顺序上传到你的GATK工作空间克隆中。你会问正确的顺序是什么？这里有个提示：查看数据模型在[图 13-11](#visual_representation_of_the_data_mode)中的可视化表示，并根据箭头的方向做出决策。你认为呢？没错，你首先上传参与者，然后是样本，最后是样本集。现在就去做吧。
- en: 'Oh wait, did you encounter something you didn’t expect while doing that? Indeed:
    there are *two* load files for the sample set: one that defines the table itself
    and names the sample sets that it contains, identified as *entity:sample_set_id*,
    and one that lists the members that each sample set contains, identified as *membership:sample_set_id*.
    Have a look at their contents to get a sense of how they relate to each other.
    And relate they do; because the membership TSV refers to the name of the sample
    set defined in the entity TSV, you’ll need to upload the entity list first, then
    the membership list.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，等等，你在执行这个过程中遇到了一些意外的事情吗？确实：样本集有*两个*加载文件：一个定义表本身并命名它包含的样本集，标识为*entity:sample_set_id*，另一个列出每个样本集包含的成员，标识为*membership:sample_set_id*。查看它们的内容以了解它们之间的关系。它们确实有关联；因为成员TSV引用了实体TSV中定义的样本集的名称，所以你需要先上传实体列表，然后再上传成员列表。
- en: Note
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Dealing with load files and the precedence rules is one of the more awkward,
    and frankly tedious, parts of setting up data in Terra. We expect that future
    developments will address this by providing some kind of wizard-style functionality
    to smooth out the sharp edges.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 处理加载文件和优先顺序规则是在Terra中设置数据中比较尴尬和枯燥的部分之一。我们期待未来的发展将通过提供某种向导式功能来平滑这些尖锐的边缘。
- en: 'On the bright side, after you’re done with that, you should see all 2,504 samples
    from the 1000 Genomes dataset listed in your GATK workspace as well as their corresponding
    participants. However, there’s a problem. Can you see it? Take a good look at
    the columns in the sample table. Most of the columns were different between the
    two original tables, of course, because the GATK workspace version mainly had
    output files from the workflows that are showcased in the workspace, and the 1000
    Genomes workspace version mainly had metadata about the provenance of the data.
    However, they were supposed to have one thing in common: GVCF files. Can you find
    them? Yes, they do all have GVCF files—but they’re in different columns.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: In the original GATK workspace, the name of the column containing the GVCF files
    was *gvcf* in all lowercase, whereas in the 1000 Genomes workspace it was *gVCF*
    in mixed case. Incidentally, the columns containing their respective index files
    are also named with subtle differences, *gvcf_index* versus *gVCF_TBI*, where
    TBI refers to the extension of the index format generated by a utility called
    Tabix.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Well, shucks. We were going to surprise you by saying, “Look, now you can run
    a joint calling analysis of your NA12878 sample combined with all of the 1000
    Genomes Phase 3 data. Isn’t that cool?” But it’s not going to work if the GVCF
    files are in different columns (sad face Emoji).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Chin up; this is fixable. You still have the TSV files that you used to upload
    the 1000 Genomes data tables, right? Just open the *sample* TSV file and rename
    the two columns to match the corresponding names in the GATK workspace. No need
    to change anything else, just those two column names. Then, upload the TSV and
    see what happens: now the GVCF files from the 1000 Genomes samples and their index
    files also show up in the right columns. Crisis averted! The old columns with
    the mismatching names are still there, but you can ignore them. Literally, in
    fact; you can hide them (and any others you don’t care about) to reduce the visual
    clutter. Simply click the gear icon in the upper-right corner of the table to
    open the table display menu. You can clear the checkboxes for the column names
    to hide them and even reorder them to suit your preferences.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s admittedly a little annoying that you can’t simply edit column names in
    place or delete unwanted columns. The import dialog box feels a bit limited, as
    well—we would love to be able to preview the data and get a summary count of rows
    and columns, for example, before confirming the import operation. We’re looking
    forward to seeing these aspects of the interface improve as Terra matures.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we were able to get past this trivial little naming mismatch by
    editing just two column names. However, minor as it might seem, this stumbling
    block is symptomatic of a much larger problem: there is not enough standardization
    around how datasets are structured and how their attributes are named. Almost
    any attempt to federate datasets from different sources can quickly turn into
    an exercise in frustration as you find yourself battling conflicting schemas and
    naming conventions. There is no universal solution (yet), but when you encounter
    this kind of issue, it can be helpful to start by reducing the problem to the
    smallest set of components that need to be reconciled. For example, try to define
    the core data model of each dataset; that is, determine the key pieces of data
    and how they relate to one another. From there, you can gauge what it would take
    to make the datasets compatible to the extent that you need them to be.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们成功地通过编辑仅两个列名来克服了这个微小的命名不匹配问题。然而，尽管看起来微不足道，这个障碍表面上只是一个很小的问题，但它实际上是一个更大问题的症结所在：在数据集的结构化和属性命名方面缺乏足够的标准化。几乎任何试图将来自不同来源的数据集进行联合的尝试，都可能迅速变成一种挫败感的练习，因为您会发现自己在处理冲突的模式和命名约定。虽然目前没有普遍适用的解决方案，但当您遇到这类问题时，将问题简化到需要协调解决的最小组件集可能会有所帮助。例如，尝试定义每个数据集的核心数据模型；也就是说，确定关键数据部分及其之间的关系。从这里开始，您可以评估使数据集兼容所需程度所需的工作量。
- en: 'In our case, we now have what we need: all our samples are defined in the sample
    table and have a GVCF file listed in the *gvcf* column as well as a corresponding
    index file in the *gvcf_index* column. Anything else is irrelevant to what we
    want to do next, which is to apply joint calling to all the samples in the workspace.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，现在我们已经拥有所需的一切：所有样本都在样本表中定义，并在*gvcf*列中列出了一个GVCF文件，以及在*gvcf_index*列中列出了相应的索引文件。其他任何内容对我们接下来想要做的事情都不相关，即对工作空间中的所有样本应用联合调用。
- en: Running a Joint-Calling Analysis on the Federated Dataset
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在联合数据集上运行联合调用分析
- en: To cap off this scenario, we’re going to run the *3_Joint_Discovery* workflow
    that is preconfigured in this workspace, which applies the GATK Best Practices
    for joint-calling germline short variants on a cohort of samples as described
    in [Chapter 6](ch06.xhtml#best_practices_for_germline_short_varia). We’re going
    to run it on a subset of the samples for testing purposes, but we’ll provide pointers
    for scaling up the analysis in case you want to try running it on all the samples.
    In any case, the workflow will produce a multisample VCF containing variant calls
    for the set of samples that we choose to include.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个场景，我们将运行预先配置在这个工作空间中的*3_Joint_Discovery*工作流程，该工作流程应用了GATK最佳实践，用于对一组样本进行联合调用生殖系短变异，详见[第6章](ch06.xhtml#best_practices_for_germline_short_varia)。我们将在一部分样本上运行以进行测试，但我们将提供指引，以便您尝试在所有样本上运行它。无论如何，该工作流程将生成一个包含我们选择包括的样本的多样本VCF文件，其中包含变异调用信息。
- en: As we admitted earlier, calling this a *federated dataset* is a bit of an exaggeration
    given that we’re really adding just one sample to the 1000 Genomes Phase 3 data.
    However, the principles we’re discussing would apply equally if you were now to
    add more samples to this workspace. For example, you might want to use the 1000
    Genomes data to pad your analysis of a small cohort in order to maximize the benefits
    you can get from doing joint calling, as is recommended in the GATK documentation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前承认的那样，将这称为*联合数据集*有点夸张，因为实际上我们只是将一个样本添加到1000基因组第3阶段数据中。然而，我们讨论的原则如果您现在想要将更多样本添加到此工作空间中，同样适用。例如，您可能希望使用1000基因组数据来填充您对小队的分析，以最大化您从联合调用中获得的好处，正如GATK文档建议的那样。
- en: The workflow is already configured, so let’s have a look at what it expects.
    Go to the Workflow page and click the *3_Joint_Discovery* workflow to view the
    configuration details. First, we’re going to look at which table the workflow
    is configured to run on. The data selection drop-down menu shows Sample Set, which
    means that we’ll need to provide a sample set from the *sample_set* table. That
    table currently holds two sample sets; one that lists the NA12878 sample alone,
    and the other that lists the full 1000 Genomes Phase 3 cohort that we imported
    into the workspace. We need a sample set that lists some samples from the 1000
    Genomes cohort and the NA12878 sample from the GATK workspace, so let’s create
    one now.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’re going to use 25 samples because that is how much data the predefined configuration
    of the workflow can accommodate; we’ll provide guidance for scaling up once we’ve
    completed the test at this scale.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: To quickly create a test sample set, set the data table selector to *sample_set*
    under Step 1, and click Select Data under Step 2\. In the dialog box that opens
    up, select the “Create a new set from selected samples” option. You can select
    the checkboxes of individual samples or select the checkbox at the top of the
    column to select all 25 samples that are displayed by default. When you’ve selected
    the samples, you can use the box labeled “Selected samples will be saved as a
    new set named” to specify a name for the sample set or leave the default autogenerated
    name as is. Press OK to confirm and return to the workflow configuration page.
    Note that, at this point, you haven’t actually created the new sample set; you’ve
    just set up the system to create it when you press the Launch button.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: But don’t press the button yet! We want to show you another way to create a
    sample set, not just because we have a mean streak (we do), but also to equip
    you to deal with more-complex situations.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface for creating sets can be too limiting when you’re working with
    a lot more samples than can be displayed on a single screen, so we’re also going
    to show you how to use the manual TSV approach to do this. This is going to be
    a two-step process, similar to what you did earlier: create a new sample set using
    an *entity* load file and then provide a list of its members using a *membership*
    load file. The first one is really minimal because it’s just the one column header,
    *entity:sample_set_id*, and the name of the new sample set in the next row, as
    shown here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '| entity:sample_set_id |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| federated-dataset |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: You can save this as a TSV file and upload it to your workspace as previously,
    or you can take advantage of the alternative option illustrated in [Figure 13-13](#direct_text_import_of_tsv_formatted_dat),
    which involves copying and pasting the two rows into a text field. Annoyingly,
    it’s not possible to type text directly into this text box or edit what you paste
    in, but it does cut out a few clicks from the import process.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Direct text import of TSV-formatted data table content.](Images/gitc_1313.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![TSV格式数据表内容的直接文本导入。](Images/gitc_1313.png)'
- en: Figure 13-13\. Direct text import of TSV-formatted data table content.
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-13\. TSV格式数据表内容的直接文本导入。
- en: Uploading that content creates a new row in the *sample_set* table, but the
    new sample set has no samples associated with it yet. To remedy that, we need
    to make a *membership* TSV file containing the list of samples that we want to
    run on. We like to start from existing data tables because it reduces the chance
    that we’ll get something wrong, especially those finicky header lines. To do so,
    download the TSV files corresponding to the sample set table and open the *sample_set_membership.tsv*
    file in your spreadsheet editor. As shown in [Figure 13-14](#start_and_end_rows_of_the_membership_lo),
    you should see the full list of 1000 Genomes samples in the second column, with
    the name of the original 1000 Genomes sample set displayed in the first column
    of each row. If you scroll all the way to the end of the file, you’ll also see
    the NA12878 sample from the GATK workspace, which is assigned to the *one_sample*
    set.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上传该内容将在*sample_set*表中创建一个新行，但新的样本集尚无样本与之关联。为了解决这个问题，我们需要制作一个包含我们要运行的样本列表的*membership*
    TSV文件。我们喜欢从现有数据表开始，因为这样可以减少出错的可能性，特别是那些棘手的标题行。为此，请下载与样本集表对应的TSV文件，并在电子表格编辑器中打开*sample_set_membership.tsv*文件。如[图 13-14](#start_and_end_rows_of_the_membership_lo)所示，您应该看到第二列中显示的1000基因组样本的完整列表，每行的第一列显示原始1000基因组样本集的名称。如果您滚动到文件的末尾，您还将看到来自GATK工作区的NA12878样本，该样本被分配到*one_sample*集。
- en: '![Start and end rows of the membership load file sample_set_membership.tsv.](Images/gitc_1314.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![成员加载文件*sample_set_membership.tsv*的起始和结束行。](Images/gitc_1314.png)'
- en: Figure 13-14\. Start and end rows of the membership load file sample_set_membership.tsv.
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-14\. 成员加载文件*sample_set_membership.tsv*的起始和结束行。
- en: We’re going to take a subset of this and transform it into a membership list
    that associates the samples we choose with our newly created sample set, *federated-dataset*.
    Start by deleting all but 24 of the samples that belong to the 1000 Genomes cohort
    from the list, so that you’re left with 25 samples in total in the list. It doesn’t
    matter which samples you keep. Then, use the Find and Replace or Rename function
    of your editor (typically found on the Edit menu) to change the sample set name
    in the first column to *federated-dataset* for all rows, as demonstrated in [Figure 13-15](#updated_membership_load_file_sample_set).
    Make sure to also replace the sample set name for the NA12878 sample. Then, save
    and upload the file as you’ve done previously.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从中提取一部分，并将其转换为成员列表，将我们选择的样本与我们新创建的*联合数据集*样本集相关联。首先，删除属于1000基因组队列的样本中除24个之外的所有样本，以便列表中总共剩下25个样本。不管保留哪些样本，然后，使用您编辑器的查找和替换或重命名功能（通常在编辑菜单中找到）来更改第一列中的样本集名称为*联合数据集*，如[图 13-15](#updated_membership_load_file_sample_set)所示。确保还替换NA12878样本的样本集名称。然后，保存并像之前一样上传文件。
- en: '![Updated membership load file sample_set_membership.tsv assigning 25 samples
    to the federated-dataset sample set.](Images/gitc_1315.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![更新的成员加载文件示例_set_membership.tsv分配25个样本到联合数据集样本集。](Images/gitc_1315.png)'
- en: Figure 13-15\. Updated membership load file sample_set_membership.tsv assigning
    25 samples to the federated-dataset sample set.
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-15\. 更新的成员加载文件*sample_set_membership.tsv*将25个样本分配给*联合数据集*样本集。
- en: After you’ve uploaded this membership list, the *sample_set* table should now
    list the 25 samples as belonging to the *federated-dataset* sample set, as shown
    in [Figure 13-16](#the_sample_set_table_showing_the_three).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 上传了这个成员列表后，*sample_set* 表现在应该将这25个样本列为*联合数据集*样本集的成员，如[图 13-16](#the_sample_set_table_showing_the_three)所示。
- en: '![The sample_set table showing the three sample sets.](Images/gitc_1316.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![显示三个样本集的*sample_set*表。](Images/gitc_1316.png)'
- en: Figure 13-16\. The sample_set table showing the three sample sets.
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-16\. *sample_set* 表现在显示的三个样本集。
- en: By the way, this shows you that you can add or modify rows by uploading TSV
    content for just the parts of the table you want to update or augment. You don’t
    need to reproduce the full table every time. We feel this makes up a bit for not
    being able to just create the sample sets in the graphical interface.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，这表明您可以通过仅上传要更新或扩展的表中部分内容的TSV内容来添加或修改行。您不需要每次都重现整个表。我们觉得这在不能在图形界面中创建样本集时弥补了一些不足。
- en: Note
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: In case you’re wondering why we’re including only the full-scale *NA12878* sample
    from the GATK workspace and not the downsampled *NA12878_small* sample, it’s because
    they come from the same original person so it would be redundant to use both.
    Because the downsampled one has less data, it’s less likely to produce meaningful
    results, hence that’s the one we eliminate.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a sample set that lists all of the samples that we want to
    use in our analysis, we can proceed with the next step. Select the checkbox on
    the left of the *federated-dataset* row in the *sample_set* table and then click
    the “Open with” button, select Workflow, and finally, select *3_Joint_Discovery*
    to initiate the workflow submission. As we’ve shown you previously, you can choose
    to kick off the process from a selection of data on the Data page like this, or
    you can do it from the Workflows page, as you prefer. The advantage of doing it
    as we just described is that now your workflow is already set to run on the right
    sample set; otherwise, you would need to use the Select Data menu to do it when
    you get to the configuration page.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, all that remains for us to do is check the remainder of the
    configuration, starting with the Inputs. There are a lot of input fields so we’re
    not going to look at all of them; instead, let’s focus on the main file inputs.
    Based on what you learned about joint calling in [Chapter 6](ch06.xhtml#best_practices_for_germline_short_varia),
    you should have an idea of what kind of input variable you’re looking for: one
    that refers to a list of GVCFs. Sure enough, if you scroll down a bit, you’ll
    find the workflow variable named *input_gvcfs*, as shown in [Figure 13-17](#input_configuration_details_for_the_inp).
    The *Type* of the variable is *Array[File]*, which means the workflow expects
    us to provide a list of files as input, so that checks out. You also see a corresponding
    variable set up for the list of index files.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Input configuration details for the input_gvcfs and input_gvcfs_indices variables.](Images/gitc_1317.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 13-17\. Input configuration details for the input_gvcfs and input_gvcfs_indices
    variables.
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now if you look at the value provided in the rightmost column of the input configuration
    panel, you see *this.samples.gvcf*. That should seem both familiar and new at
    the same time. Familiar because it appears to be a variation of the *this.something*
    syntax that we’ve seen and used previously, which allows us to say, “For each
    row of the table that the workflow runs on, give the content of the *something*
    column to this input variable.” We used this previously when launching the `HaplotypeCaller`
    workflow on multiple samples for parallel execution; this syntax allowed us to
    easily wire up the *input_bam* variable to take the input BAM file for each workflow
    invocation, without having to specify any files explicitly.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Yet it’s also a bit new because it includes an extra element, forming a *this.other.something*
    syntax. Can you guess what’s going on there? This is really cool; it’s the payoff
    from establishing the connections between tables that we mentioned earlier in
    the chapter. This syntax is basically saying, “For each sample set, look up the
    list in its *samples* column, and then go to the *sample* table and round up all
    the GVCF files from the corresponding rows.” You can use this *.other.* element
    to refer to any list of rows from another table, query them for a particular field,
    and return a list of the corresponding elements. And that is how we generate the
    list of input GVCF files based on the list of samples linked in the sample set.
    As a bonus, the order in which the list is made is consistent, so we can assign
    *this.samples.gvcf* and *this.samples.gvcf_index* to two different variables,
    and rest assured that the lists of GVCF files and their index files will be in
    the same sample order.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这也有点新鲜，因为它包含了一个额外的元素，形成了*this.other.something*语法。你能猜到那里发生了什么吗？这真的很酷；这是建立在前面章节中提到的表之间连接的回报。这种语法基本上是在说，“对于每个样本集，查找其*samples*列中的列表，然后转到*sample*表，并收集对应行中所有GVCF文件。”你可以使用*.other.*元素引用另一个表中的任何行列表，查询它们的特定字段，并返回相应元素的列表。这就是我们基于样本集中链接的样本列表生成输入GVCF文件列表的方法。作为奖励，列表生成的顺序是一致的，所以我们可以将*this.samples.gvcf*和*this.samples.gvcf_index*分配给两个不同的变量，并放心这些GVCF文件列表及其索引文件列表将按照相同的样本顺序排列。
- en: This is one of the key benefits of having a well-designed data model in place;
    you can take advantage of the relationships between data entities at different
    levels. You can even daisy-chain them several levels deep; there is theoretically
    no limit to how many *.other.* lookups you could do. For more examples of what
    you can do along these lines, check out the data model used in the somatic Best
    Practices workspaces, where each participant has a tumor sample and a normal sample.
    In that data model, one additional table lists the Tumor-Normal pairs and another
    lists sets of pairs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是拥有良好设计的数据模型的关键好处之一；你可以利用不同级别数据实体之间的关系。甚至可以将它们级联多层深；理论上你可以无限次数地进行*.other.*查找。有关你可以沿着这些线进行的更多示例，请查看用于体细胞最佳实践工作空间中使用的数据模型，其中每个参与者都有一个肿瘤样本和一个正常样本。在该数据模型中，另一个表列出了肿瘤-正常配对，另一个表列出了配对集。
- en: Feel free to scroll through the rest of the Inputs configuration page. Then,
    when you feel like you have a good grasp of how the inputs are wired up, head
    over to the Outputs for one last check before you launch the workflow. The outputs
    are all set up along a *this.output_** pattern, and you should see a line that
    says, “References to outputs will be written to Tables / sample_set,” which tells
    you that the final VCF and related output files will be attached to the sample
    set. This makes sense because by definition the results of a joint calling analysis
    pertain to all of the samples that are included.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 随意浏览其余的输入配置页面。当你觉得你已经对输入如何连接有了很好的掌握时，可以前往输出页面进行最后的检查，然后再启动工作流。输出都按照*this.output_*模式设置，你应该能看到一行写着，“输出将写入到Tables
    / sample_set”，这告诉你最终的VCF和相关输出文件将附加到样本集。这是有道理的，因为根据联合调用分析的定义，结果适用于所有包含的样本。
- en: Finally, go ahead and launch the workflow. You can monitor the execution and
    explore results the same way as in previous exercises. On 25 samples, our test
    run of the workflow cost $10 and took about 10 hours (with a couple of preemptions)
    to run to completion. The longest-running step by far was `ImportGVCFs`, which
    ran for about three hours per parallelized segment of the genome and spent half
    that time in the file localization stage. That’s a classic indication that this
    step would strongly benefit from being optimized to stream data. The other long-running
    task that is parallelized, `GenotypeGVCFs`, took about one hour per parallelized
    segment. Finally, the variant recalibration step, which is not parallelized, took
    about two hours.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，继续启动工作流。你可以像之前的练习一样监控执行并探索结果。在25个样本上，我们的工作流测试运行花费了$10，并且大约耗时10小时（包括几次抢占）才能完成运行。迄今为止运行时间最长的步骤是`ImportGVCFs`，每个基因组的并行化段大约运行了三个小时，并且其中一半时间用于文件本地化阶段。这明显表明，该步骤非常需要优化以流式传输数据。另一个长时间运行的并行化任务是`GenotypeGVCFs`，每个并行化段大约需要一小时。最后，变异重校准步骤，这不是并行化的，大约需要两个小时。
- en: We recommend running this again with a few cohort sizes and comparing the timing
    diagram for the various runs in order to understand how the time and cost of this
    workflow scale, depending on the number of samples. You’ll need to create new
    sample sets by following the same instructions as earlier, but keeping however
    many samples you’re interested in. Be sure to give each sample set a different
    name and do the entity TSV step to add it to the table; otherwise, it will just
    update the sample set you originally created. If you choose to run the workflow
    on a larger number of samples than the 25 we’ve tested so far, you’ll need to
    make a few configuration changes according to the instructions in the workspace
    dashboard, under the *3_Joint_Discovery* workflow input requirements. In a nutshell,
    you’ll need to allocate more disk space to account for the larger number of sample
    GVCFs that will need to be localized, using the disk override input variables.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 建议再次运行此流程，使用几种不同的队列大小，并比较不同运行的时间图，以了解这个工作流程在样本数量不同的情况下时间和成本的缩放情况。您需要按照之前的指示创建新的样本集，但是可以保留您感兴趣的样本数量。确保为每个样本集起一个不同的名称，并执行实体TSV步骤将其添加到表中；否则，它将只更新您最初创建的样本集。如果您选择在超过我们迄今为止测试的25个样本的更多样本上运行工作流程，则根据工作区仪表板中*3_Joint_Discovery*工作流程输入要求中的说明，需要进行一些配置更改。简而言之，您需要为更多的样本GVCF分配更多的磁盘空间，使用磁盘覆盖输入变量。
- en: The truth of the matter is that this version of the joint discovery workflow
    suffers from key scaling inefficiencies, including that it does not use streaming,
    as we pointed out a moment ago, and does not automatically handle resource allocation
    scaling. We used it for this exercise because it’s conveniently available and
    fully supported by the GATK support team. Frankly, it’s a great way to observe
    the consequences of inefficiencies that might seem small individually but would
    cause substantial difficulties at the scale of thousands of samples. However,
    we do not recommend attempting to run it on the full cohort—in fact, we wouldn’t
    advise going above a few hundred samples until you’ve at least run some preliminary
    scale testing to gauge how the workflow time and cost increase with the number
    of samples.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，这个联合发现工作流程的这个版本存在关键的扩展效率问题，包括我们刚才指出的不使用流式处理，并且不自动处理资源分配缩放。我们之所以使用它进行这个练习，是因为它方便获取，并且得到GATK支持团队的全面支持。坦率地说，这是观察效率低下后果的绝佳方式，这些问题看似个别不大，但在成千上万的样本规模下将导致重大困难。然而，我们不建议尝试在完整队列上运行它——事实上，在您至少进行了一些初步规模测试以评估工作流程时间和成本随样本数量增加而增加之前，我们不建议超过几百个样本。
- en: If you need something that scales better out of the box, an [alternative version
    of this workflow](https://oreil.ly/WI0NE) is supposed to scale better, but as
    of this writing, it has not gone through the GATK support team’s publishing process,
    so it is not officially supported. In addition, its input requirements do not
    allow you to run it directly on the data tables in your workspace because it expects
    a sample map file that lists sample names and GVCF files as shown in [this example](https://oreil.ly/mpFnK)
    (but with multiple lines, one for each sample). To get past this obstacle, you
    could make such a file based on the *sample* table in your workspace and add it
    as a property of the sample set, or if you’re feeling adventurous, you could even
    write a short WDL task that would run before the main workflow. We leave this
    as the proverbial exercise for the reader.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要一个更好地适应开箱即用的解决方案，一个[此工作流程的替代版本](https://oreil.ly/WI0NE)据说能够更好地扩展，但截至本文撰写时，尚未通过GATK支持团队的发布流程，因此它没有得到官方支持。此外，它的输入要求不允许您直接在工作区的数据表上运行它，因为它期望一个样本映射文件，列出样本名称和GVCF文件，如[此示例](https://oreil.ly/mpFnK)（但有多行，每行一个样本）。要克服这个障碍，您可以基于工作区中的*样本*表创建这样一个文件，并将其添加为样本集的属性；或者如果您感到有冒险精神，甚至可以编写一个简短的WDL任务，在主工作流程之前运行。我们将这留给读者作为一种谚语式的练习。
- en: 'This scenario started out with the goal of illustrating a few shortcuts that
    you can take when assembling a workspace to run GATK Best Practices, but in the
    process, you also picked up some bonus nuggets of knowledge: what to watch out
    for when combining data from different origins, how to set up an analysis on the
    resulting federated dataset, and how a well-crafted data model empowers you to
    pull together data across different levels. To cap it all off, you ran a complex
    analysis across a cohort of multiple whole-genome samples without breaking a sweat.
    Nicely done.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景最初旨在演示在运行GATK最佳实践时可以采用的一些快捷方式，但在过程中，你还掌握了一些额外的知识点：从不同来源组合数据时需要注意的事项，如何设置分析结果的联合数据集，以及一个精心制作的数据模型如何帮助你整合不同层次的数据。最后，你在多个全基因组样本的群体中进行了复杂的分析，毫不费力地完成了。做得好！
- en: For our last scenario of the chapter, we’re going to flip the order of operations
    and see how that affects our workspace-building process.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的最后一个场景，我们将颠倒操作顺序，看看这如何影响我们的工作空间构建过程。
- en: Building a Workspace Around a Dataset
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 围绕数据集构建工作空间
- en: 'So far, we’ve been taking a very tool-first approach in our scenarios, mainly
    because our primary focus is on teaching you how to work with a certain range
    of tools within the cloud computing framework offered by GCP and Terra. Accordingly,
    our guiding pattern has been to set up tools and then bring in data. However,
    we recognize that in practice, many of you will follow a data-first approach:
    bring in data and then figure out how to apply various tools.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的场景中，我们主要采取了一种以工具为先的方法，主要是因为我们的主要焦点是教你如何在GCP和Terra提供的云计算框架中使用一定范围的工具。因此，我们的指导模式是先设置工具，然后引入数据。但是，我们意识到实际上，很多人会采取数据为先的方法：先引入数据，然后再考虑如何应用各种工具。
- en: In this last scenario, we’re going to walk through an example of what that would
    look like applied to the same 1000 Genomes dataset that we used in the previous
    scenario. This time, instead of cloning the GATK workspace and pulling in the
    1000 Genomes data from the library, we’re going to clone the 1000 Genomes workspace
    and pull in a GATK workflow from the public tool repository Dockstore.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这最后一个场景中，我们将演示如何应用于与前一个场景中使用的相同的1000基因组数据集相同的示例。这一次，我们不再克隆GATK工作空间并从库中的1000基因组数据中提取数据，而是将克隆1000基因组工作空间，并从公共工具仓库Dockstore中导入一个GATK工作流。
- en: Cloning the 1000 Genomes Data Workspace
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克隆1000基因组数据工作空间
- en: Navigate back to the 1000 Genomes High Coverage dataset workspace in the Data
    Library and clone it as you’ve done previously, specifying a name and a billing
    project. As we discussed at the beginning of this chapter, *cloning* a workspace
    makes a shallow copy of its contents, which means that the data files in the bucket
    will not be copied to the clone. The clone’s data tables will simply point to
    the original file locations; you can assure yourself that this is true by looking
    up the file locations in the clone and in the original. This is equivalent to
    the result of the copy operation we performed in the previous scenario, first
    by using the data copy option in the interface, and then by repurposing the original
    workspace’s TSV load files.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到数据库中的1000基因组高覆盖率数据工作空间，并像之前一样克隆它，指定一个名称和一个计费项目。正如我们在本章开头讨论的那样，*克隆*一个工作空间将浅层复制其内容，这意味着存储桶中的数据文件不会被复制到克隆版本中。克隆的数据表只是简单地指向原始文件的位置；通过查看克隆和原始文件的文件位置，你可以确信这一点。这相当于我们在上一个场景中执行的复制操作的结果，首先通过界面中的数据复制选项，然后通过重新使用原始工作空间的TSV加载文件。
- en: Note
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In your clone, feel free to replace some or all of the description in the Dashboard
    with a link to the original to make space for your own notes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的克隆版本中，可以随意替换仪表板中的一些或全部描述，以链接到原始内容，以便为你自己的笔记留出空间。
- en: Importing a Workflow from Dockstore
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Dockstore导入工作流
- en: Now the question becomes how do we get some workflows in to analyze this data?
    As usual in Terra, you have a few options, depending on what you’re trying to
    achieve. First, you could simply use the internal Methods Repository as you did
    previously—either put in your own workflow or browse the public section to see
    if you can find a workflow you like. However, the internal Methods Repository
    is used only by people who do their work in Terra, so it’s probably missing a
    lot of interesting workflows that are developed by others in the wider biomedical
    community.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to simply copy over a workflow from another workspace; for
    example, you could grab the *3_Joint_Discovery* workflow from the GATK Best Practices
    workspace (either the original or the clone you used in the previous scenario)
    by using the “Copy to another workspace” option in the workspace actions menu
    (circle icon with stacked dots). However, that again limits you to workflows that
    have already been brought into Terra. In practice, you will likely find some interesting
    workflows outside of Terra, so we’re going to show you how to import workflows
    from Dockstore, an increasingly popular registry targeted at the biomedical community
    that supports workflows written in WDL, CWL, and Nextflow.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: In your clone of the 1000 Genomes workspace, navigate to the Workflows page
    and click the Find a Workflow button, as you’ve done previously. This time, instead
    of choosing the internal Methods Repository, select the Dockstore option. This
    takes you to the Dockstore website, where you’ll be able to browse a collection
    of WDL workflows. On the lefthand menu, you should see a search box that says
    “Enter search term.” Type `**joint discovery**` and view the list of results in
    the righthand panel (there is no button to press), which will include several
    GATK workflows, as shown in [Figure 13-18](Images/#search_results_for_quotjoint_discoveryq).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Search results for "joint discovery" in Dockstore.](Images/gitc_1318.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 13-18\. Search results for “joint discovery” in Dockstore.
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some of the workflows listed in the results are different versions of the same
    workflow, registered separately for historical reasons. Unfortunately, the information
    summarized in the list of search results does not provide a way to easily differentiate
    between them, so you might sometimes need to click through to the detailed descriptions
    to understand how they differ.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Find the workflow called [*gatk-workflows/gatk4-germline-snps-indels/joint-discovery-gatk4*](https://oreil.ly/Kfndw)
    and click through to its detailed information page. This workflow’s name and description
    should feel at least a little bit familiar because this is actually the source
    of the *3_Joint_Discovery* workflow that you used in the previous scenario. The
    code itself is in GitHub, in one of the repositories under the gatk-workflows
    organization used by the GATK support team to publish official GATK workflows.
    If you click the Versions tab on the workflow details page, you can see a list
    of code development branches and releases from the GitHub repository. There should
    be one version tagged as *default* by the person who registered the workflow,
    which amounts to them saying, “Unless you know otherwise, this is the version
    you should use.”
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: It can sometimes be interesting to look at this to see whether the workflow
    you’re interested in is under active development; for example, in this case, as
    of this writing, we see that the default was set to version 1.1.1 in May 2019,
    yet recent work was done in a development branch as recently as January 2020\.
    Perhaps those changes will have been released by the time you read this, and a
    new default version will have been set.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: You can explore the various other tabs for yourself, but we’d like to point
    out a couple more that we find especially useful in addition to those we’ve already
    reviewed. The first is the Files tab, which references the WDL source file under
    Descriptor Files, and a JSON file of inputs under Test Parameter Files. This is
    boring now, but it will come in handy later. The other tab we really like is the
    DAG tab, which stands for *directed acyclic graph* and refers to the workflow
    graph. This offers you an interactive visualization of the workflow akin to what
    you generated with `Womtool` in [Chapter 9](ch09.xhtml#deciphering_real_genomics_workflows),
    though the display is slightly different, as you can see in [Figure 13-19](Images/#visualization_of_the_joint_discovery_wo).
    By default, this just shows tasks in the workflow, which is a pretty neat way
    to get a high-level view of how the workflow is wired up without looking at the
    WDL code. There is also an option to switch to a different visualization tool
    made by a company called EPAM, which provides a much more granular view that includes
    inputs and outputs. This is very cool for a deep dive, but the amount of detail
    can be a bit overwhelming.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization of the Joint Discovery workflow provided in the DAG tab in
    Dockstore.](Images/gitc_1319.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 13-19\. The Joint Discovery workflow provided in the DAG tab in Dockstore.
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you’re done exploring the workflow details in the Dockstore interface,
    look for the box in the right menu labeled “Launch with,” which offers you several
    options for running the workflow, including Terra. Click the Terra button to return
    to Terra with the workflow in your pocket. In the import screen that asks you
    which workspace you want to put it in, select your clone of the 1000 Genomes workspace.
    You will land on the configuration page of the newly imported workflow in your
    workspace.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Workflow to Use the Data Tables
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Inputs page will be completely bare, so the first thing we recommend doing
    is uploading the example JSON file of inputs that was listed on the Files tab
    in Dockstore. Yes, this is why we mentioned that boring tab. Unfortunately, currently
    there is no way to have Terra import that file along with the workflow itself
    (obvious feature request right there, folks), so you’ll need to download a copy
    and then upload it to your workflow configuration using the Upload a JSON option.
    When the upload is complete, save the configuration and check that there are no
    remaining error indicators (orange exclamation marks) on the Inputs page. Make
    sure you select the option to run the workflow directly on file paths and then
    try running the imported workflow to make sure that everything works out of the
    box.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Assuming the test run completes successfully, you’re going to need to reconfigure
    the inputs to point to the data tables instead of direct file paths. This can
    be the most difficult part of the process, because you must figure out which inputs
    need to be plugged into the data tables, which ones make sense to move to the
    workspace resources table, and which one to just leave in the configuration. A
    good rule of thumb for figuring out the first set is to look at the workflow documentation
    for a description of its input requirements. There won’t always be one in the
    workflows you find out in the field, but when there is one, it’s usually highly
    illuminating with regard to what you should consider to be the main inputs of
    the workflow. In the case of this particular workflow, you should know what to
    wire up because we just spent a good chunk of this chapter poking at it, so go
    do that now. If you get stumped, remember that you can look at the original GATK
    workspace for a model of what you’re aiming for.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Regarding what’s appropriate to move to the workspace resources table, it’s
    really just a matter of judgment and preference because there is no real technical
    difference as far as Terra is concerned. You could simply decide to leave everything
    in the workflow until you import another workflow and realize they utilize the
    same resources, and plan to move the relevant resources over to the workspace
    data table at that time.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: When you have the configuration set up, you’ll need to define a sample set by
    following the same instructions as we gave you in the previous scenario. In fact,
    at this point, all the remaining work is almost exactly the same as the last two
    sections of the previous scenario, so we don’t feel the need to walk you through
    every step again; we’re confident that you can do it on your own. Good luck!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you’re feeling a bit short-changed because we made you import the
    same workflow as in the previous section, here’s a suggestion for a stretch assignment
    to flex your new muscles on a new-to-you workflow. Sometimes, you might want to
    redo the GVCF calling step if enough has changed in the algorithms or the supporting
    resources (e.g., if there is a new reference genome build), but the sequencing
    data files provided with the 1000 Genomes High Coverage dataset are all in CRAM
    format, which the GATK team does not yet recommend using for direct data access
    when using streaming. As a result, you need to find either a CRAM-to-BAM conversion
    workflow to run as a preprocessing step, or a GATK `Haplotype-Caller` workflow
    that starts with a built-in CRAM-to-BAM conversion task. That should ring a bell
    if you paid attention in [Chapter 9](ch09.xhtml#deciphering_real_genomics_workflows).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: So go ahead and try finding the appropriate workflow or combination of workflows
    to achieve that with the 1000 Genomes data, using the various approaches and resources
    we’ve given you over the past few chapters. If you get really stuck, let us know
    by posting an issue in the [GitHub repository](https://oreil.ly/genomics-repo),
    and we’ll help you work through it. If a lot of people report difficulties with
    the stretch assignments, we’ll consider posting step-by-step solutions in the
    blog.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Wrap-Up and Next Steps
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to assemble your own workspace in Terra from
    data and code components originating from various sources, both internal and external
    to Terra. We put a lot of emphasis on thinking about how datasets are structured
    in order to empower you to take advantage of the power of linked data tables in
    Terra and to combine data from different sources in general. You practiced importing
    data from the Data Library and running a federated analysis that included data
    from the 1000 Genomes Project, setting the stage for large-scale analyses. You
    also learned to import workflows from Dockstore, a tool and workflow repository
    that can connect to several other platforms besides Terra. At this point, you
    have all the necessary foundations to be able to assemble and execute your own
    scalable analysis from publicly or privately available tools and datasets on the
    cloud. In [Chapter 14](ch14.xhtml#making_a_fully_reproducible_paper), the final
    chapter, we walk through an example of a workspace that reproduces an end-to-end
    analysis from a published paper to illustrate current capabilities, obstacles,
    and perspectives for achieving optimal computational reproducibility in biomedical
    research.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
