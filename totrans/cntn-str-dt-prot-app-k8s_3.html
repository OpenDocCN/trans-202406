<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Enterprise Storage for Kubernetes"><div class="chapter" id="enterprise_storage_for_kubernetes">
      <h1><span class="label">Chapter 2. </span>Enterprise Storage for Kubernetes</h1>
      <p>To make Kubernetes storage ready for the enterprise, one must solve the problems of elasticity and scale to meet the velocity and volume of big data. Cloud native storage solutions have emerged for orchestrating pools of persistent storage that are <em>software defined</em>, meaning that they are abstracted away from the underlying hardware. Unlike software-defined storage (SDS) solutions for VM-based environments, cloud native software-defined storage runs natively as containers that can be managed by the same orchestration system that handles application containers. This paves the way for dynamic, elastic storage for containerized applications running on Kubernetes at scale. This chapter discusses Kubernetes storage concepts, how software-defined storage brings scale to Kubernetes, and how the CSI works with software-defined storage.</p>
      <section data-type="sect1" data-pdf-bookmark="Kubernetes Storage Concepts"><div class="sect1" id="kubernetes_storage_concepts">
        <h1>Kubernetes Storage Concepts</h1>
        <p>Kubernetes represents application entities as <em>primitives</em>, which are the basic Kubernetes building blocks. Primitives represent real or logical entities so that Kubernetes can manage them as if they were software objects. Storage is no exception. Kubernetes provides a number of storage primitives, including the following:</p>
        <dl>
          <dt>PersistentVolume (PV)</dt>
          <dd>
            <p>A unit of persistent storage </p>
          </dd>
          <dt>PersistentVolumeClaim (PVC)</dt>
          <dd>
            <p>A storage request, which becomes the binding between a PV and a pod</p>
          </dd>
          <dt>StatefulSet</dt>
          <dd>
            <p>An object that manages the identity of a set of pods</p>
          </dd>
          <dt>StorageClass</dt>
          <dd>
            <p>Describes the classes of storage the cluster offers</p>
          </dd>
        </dl>
        <p>There are many other primitives, of course, but these four are interesting as we consider how to bring enterprise-ready scale to Kubernetes storage.</p>
        <section data-type="sect2" data-pdf-bookmark="PersistentVolume"><div class="sect2" id="persistentvolume_pv">
          <h2>PersistentVolume</h2>
          <p>A PV is an object that represents storage at a specific location. PVs provide the ability to keep data longer than the lifetime of the workload or pod that uses the volume. PVs can store a workload’s data on networked storage, on a cloud provider, or locally on the pod where the workload is running (<a data-type="xref" href="#fig_1_local_cloud_and_networked_persistentvolume_locat">Figure 2-1</a>). </p>
          <figure><div id="fig_1_local_cloud_and_networked_persistentvolume_locat" class="figure">
            <img src="Images/csdp_0201.png" alt="Local  cloud  and networked PersistentVolume locations" width="880" height="496"/>
            <h6><span class="label">Figure 2-1. </span>Local, cloud, and networked PersistentVolume locations</h6>
          </div></figure>
          <p>Kubernetes manages every PV’s lifecycle, defining the following stages:</p>
          <dl>
            <dt>
              Provisioning
            </dt>
            <dd>
              <p>A storage administrator creates the PV statically ahead of time, or dynamically using a <em>StorageClass</em>, which is<em> </em>a resource that provides a way for the administrator to describe the storage.</p>
            </dd>
            <dt class="pagebreak-before less_space">
              Binding
            </dt>
            <dd>
              <p>A PVC binds the PV to a specific container.</p>
            </dd>
            <dt>
              Use
            </dt>
            <dd>
              <p>Workloads running on the container use the PVC to access the PV.</p>
            </dd>
            <dt>
              Release
            </dt>
            <dd>
              <p>The container removes its claim to the volume by removing the PVC.</p>
            </dd>
            <dt>
              Retention
            </dt>
            <dd>
              <p>While the data is needed, the PV retains it, even across container and pod lifecycles.</p>
            </dd>
            <dt>
              Deletion and reclamation
            </dt>
            <dd>
              <p>Kubernetes deletes the data when it is no longer needed, reclaiming the storage space for use by other volumes.</p>
            </dd>
          </dl>
          <p>The storage administrator can provision PVs dynamically as needed, or ahead of time based on predicted storage needs. PVs define additional details about the data, including lifecycle policy, routes, IP addresses, and credentials.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="PersistentVolumeClaim"><div class="sect2" id="persistentvolumeclaim_pvc">
          <h2>PersistentVolumeClaim</h2>
          <p>A PVC is both a request for storage and an identifier that establishes a claim on the storage once it’s granted. PVs on their own are not owned by specific applications or projects. A PVC requests access to a PV using one of the following access modes:</p>
          <dl>
           <dt>ReadWriteOnce (RWO)</dt>
           <dd>Read-write access by all pods on a single node</dd>
           <dt>ReadOnlyMany (ROX)</dt>
           <dd>Read-only access by multiple nodes</dd>
           <dt>ReadWriteMany (RWX)</dt>
           <dd>Read-write access by multiple nodes</dd>
           <dt>ReadWriteOncePod (RWOP)</dt>
           <dd>Read-write access by a single pod only</dd>
          </dl>
 <p class="pagebreak-before less_space">PVs and PVCs work together as follows:</p>
          <ol>
            <li>
              <p>An application developer creates one or more PVCs describing the storage resources the application needs.</p>
            </li>
            <li>
              <p>The storage administrator can either create PVs explicitly in response, or create a StorageClass that can dynamically provision new PVs as needed.</p>
            </li>
            <li>
              <p>Kubernetes manages the binding of PVCs to PVs.</p>
            </li>
          </ol>
          <p>Together, PVs and PVCs provide a way for pods to define requests based on the storage requirements of their containers and applications. The storage administrator can either configure dynamic <em>provisioners</em> that allocate storage and a PV in response to these requests, or create PVs in anticipation of an application’s storage needs. When the storage is granted, the cluster finds the PV associated with the PVC and mounts it for the pod. In other words, the pod uses the PVC as a volume. The PV is exclusively available to the pod as long as it’s needed.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="StatefulSet"><div class="sect2" id="statefulset">
          <h2>StatefulSet</h2>
          <p>A StatefulSet is the primitive that manages the deployment and scaling of a set of pods, maintaining a unique ID for each pod so that it can be identified for the purposes of persistent data or networking, or when the pod migrates to a different node. By providing unique, persistent IDs for pods, the StatefulSet API lets administrators manage the deployment and scaling of a set of pods. When individual pods fail, the persistent IDs help restore connections between their replacements and the existing volumes that serve them.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="StorageClass"><div class="sect2" id="storageclass">
          <h2>StorageClass</h2>
          <p>The StorageClass primitive lets the cluster administrator describe the different classes of storage the cluster offers. Storage classes can indicate different policies or levels of service. For example, the administrator might set up different StorageClass objects to represent different backup policies. Users can request a specific storage class by name.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Software-Defined Storage"><div class="sect1" id="software_defined_storage_sds">
        <h1>Software-Defined Storage</h1>
        <p>Just as containerized application architecture decouples application logic from the hardware where the code runs, software-defined storage decouples persistent data and storage policies from storage media. SDS is distributed and hardware agnostic, and can run in a variety of environments including the cloud. By abstracting <span class="keep-together">storage</span> from its hardware, SDS enables the presentation of hardware capacity to applications, users, and other clients as a unified storage pool. SDS makes a storage administrator’s job easier immediately by making it possible to manage a diverse assortment of different storage types consistently without worrying about the different characteristics of each type of storage (<a data-type="xref" href="#fig_2_software_defined_storage">Figure 2-2</a>).</p>
        <figure><div id="fig_2_software_defined_storage" class="figure">
          <img src="Images/csdp_0202.png" alt="Software defined storage" width="722" height="650"/>
          <h6><span class="label">Figure 2-2. </span>Software-defined storage</h6>
        </div></figure>
        <p>Just as Kubernetes brings elasticity, scale, and high availability to compute resources, SDS is the foundation for bringing these important characteristics to storage. Because the storage is abstracted, it’s possible to build systems that distribute and scale storage in different environments, integrating them with Kubernetes or other orchestration systems and building in fault tolerance and high <span class="keep-together">availability</span>.</p>
        <p>The advantages for the enterprise are clear:</p>
        <ul>
          <li>
            <p>Developers and users don’t need to think about storage <span class="keep-together">hardware</span>.</p>
          </li>
          <li>
            <p>Scale becomes a matter of on-demand provisioning.</p>
          </li>
          <li>
            <p>Data can be placed anywhere it is needed, regardless of the environment.</p>
          </li>
        </ul>
        <p>Because SDS presents all available physical storage as a shared pool, the resources can be allocated efficiently, reducing wasted storage space.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Connecting SDS to Kubernetes with the CSI"><div class="sect1" id="connecting_sds_to_kubernetes_with_the_csi">
        <h1>Connecting SDS to Kubernetes with the CSI</h1>
        <p>The CSI is an interface between containerized workloads and a third-party storage layer, making it possible for cloud native applications to create, manage, and use storage outside of Kubernetes. The CSI makes storage available in a pool that all instances of each application can access, keeping instances in sync and making it possible to back up applications consistently. The CSI provides abstracted access to Kubernetes over an interface, meaning that third parties can create plug-ins for accessing storage systems from Kubernetes without touching the core Kubernetes code. Using the CSI, containerized applications can use the normal Kubernetes storage primitives on top of these storage systems <span class="keep-together">(<a data-type="xref" href="#fig_3_connecting_software_defined_storage_to_kubernetes">Figure 2-3</a>)</span>.</p>
        <figure><div id="fig_3_connecting_software_defined_storage_to_kubernetes" class="figure">
          <img src="Images/csdp_0203.png" alt="Connecting software defined storage to Kubernetes with the CSI" width="722" height="604"/>
          <h6><span class="label">Figure 2-3. </span>Connecting software-defined storage to Kubernetes with the container storage interface</h6>
        </div></figure>
        <p>The CSI gives storage vendors the freedom to design and manage storage as they see fit, providing Kubernetes and other orchestration platforms the freedom to provision and manage storage transparently using native abstractions. At the time of this writing, Kubernetes and the CSI don’t know how to provide application-aware backups, high availability, or other functions necessary for the enterprise, but the CSI makes it possible to add these capabilities at the storage layer itself. This is where SDS comes in.</p>
        <p>Software-designed storage is not limited by hardware or standards, meaning that any required capabilities can be implemented and abstracted away from the underlying mechanics. For this reason, it doesn’t matter that the CSI is a slow-moving standard, as standards should be. By connecting SDS to Kubernetes, the CSI makes it possible to build Kubernetes-ready storage that is granular at the container level, self-healing, and topology aware, without requiring Kubernetes itself to have these capabilities.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Cloud Native Storage: Bringing Scale to Kubernetes Storage"><div class="sect1" id="cloud_native_storage_bringing_scale_to_kubernetes">
        <h1>Cloud Native Storage: Bringing Scale to Kubernetes Storage</h1>
        <p>Adapting traditional SDS for containerized workloads is challenging for a number of reasons.</p>
        <p>First, because containers are dynamic and ephemeral, they require storage that can be provisioned, attached, and deleted instantly, whenever and wherever it is needed. For high availability, it must be possible to create and move volumes as needed, with awareness of topology, and to back them up regularly and automatically. Manual storage provisioning can’t keep up with these needs at scale. </p>
        <p>Second, the number of volumes that physical and virtual servers can support is often insufficient for the number of pods or containers that need storage. A single host might run hundreds of small containers, requiring more volumes than the OS can provide.</p>
        <p>Finally, because the point of containers is to be infrastructure agnostic, they should not care about the physical storage they’re using. Different environments provide different types of storage, often multiple types within a single deployment. It must be possible to move data between storage pools without affecting the way containerized applications run. </p>
        <p>Enter cloud native storage, a new model of SDS designed for distributed, containerized applications. Cloud native storage runs in containers on the cluster, meaning it can be provisioned and orchestrated, offering data locality and Kubernetes-integrated features like <a href="https://github.com/libopenstorage/stork">Stork (Storage Operator Runtime for Kubernetes)</a> to provide storage-aware scheduling. <a data-type="xref" href="#fig_4_cloud_native_storage">Figure 2-4</a> shows how cloud native storage becomes part of Kubernetes itself.</p>
        <figure><div id="fig_4_cloud_native_storage" class="figure">
          <img src="Images/csdp_0204.png" alt="Cloud native storage" width="722" height="604"/>
          <h6><span class="label">Figure 2-4. </span>Cloud native storage</h6>
        </div></figure>
        <p>Cloud native storage must be container aware, and all operations must take place at the level of the application. Snapshots, backups, compression, encryption, and other operations are not relevant to the cluster or the storage pool as a whole, but to the container itself. This key aspect gives operational control over data to the application owner, relieving IT admins of the responsibility for provisioning storage and protecting application data. </p>
        <p>Clearly, redefining storage for Kubernetes at scale requires rethinking the nature of storage itself. To meet the needs of containerized applications at scale, a storage paradigm must be elastic, nimble, able to serve multiple replicas of data to multiple instances of application services concurrently, and decoupled from the application logic itself. In other words, to bring scale to Kubernetes data, storage must be cloud native, which means it must be both software defined and containerized.</p>
      </div></section>
    </div></section></div></body></html>