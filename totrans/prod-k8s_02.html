<html><head></head><body><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. A Path to Production" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter1">&#13;
<h1><span class="label">Chapter 1. </span>A Path to Production</h1>&#13;
&#13;
&#13;
<p>Over the years, the world has experienced wide adoption of Kubernetes within organizations. Its popularity has unquestionably been accelerated by the proliferation of containerized workloads and microservices. As operations, infrastructure, and development teams arrive at this inflection point of needing to build, run, and support these workloads, several are turning to Kubernetes as part of the solution. Kubernetes is a fairly young project relative to other, massive, open source projects such as Linux. Evidenced by many of the clients we work with, it is still early days for most users of Kubernetes. While many organizations have an existing Kubernetes footprint, there are far fewer that have reached production and even less operating at scale. In this chapter, we are going to set the stage for the journey many engineering teams are on with Kubernetes. Specifically, we are going to chart out some key considerations we look at when defining a path to production.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Defining Kubernetes" data-type="sect1"><div class="sect1" id="idm45612003097176">&#13;
<h1>Defining Kubernetes</h1>&#13;
&#13;
<p>Is Kubernetes a platform? Infrastructure? An application? <a data-primary="Kubernetes" data-secondary="about" data-type="indexterm" id="idm45612003095608"/>There is no shortage of thought leaders who can provide you their precise definition of what Kubernetes is. Instead of adding to this pile of opinions, let’s put our energy into clarifying the problems Kubernetes solves. Once defined, we will explore how to build atop this feature set in a way that moves us toward production outcomes. The ideal state of “Production Kubernetes” implies that we have reached a state where workloads are successfully serving production traffic.</p>&#13;
&#13;
<p>The name <em>Kubernetes</em> can be a bit of an umbrella term. A quick browse on GitHub reveals the <code>kubernetes</code> organization contains (at the time of this writing) 69 repositories.  Then there is <code>kubernetes-sigs</code>, which holds around 107 projects. And don’t get us started on the hundreds of Cloud Native Compute Foundation (CNCF) projects that play in this landscape! For the sake of this book, <em>Kubernetes</em> will refer exclusively to the core project. So, what is the core?  The core project is contained in the <a href="https://github.com/kubernetes/kubernetes">kubernetes/kubernetes</a> repository. This is the location for the key components we find in most Kubernetes clusters. When running a cluster with these components, we can expect the following functionality:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Scheduling workloads across many hosts</p>&#13;
</li>&#13;
<li>&#13;
<p>Exposing a declarative, extensible, API for interacting with the system</p>&#13;
</li>&#13;
<li>&#13;
<p>Providing a CLI, <code>kubectl</code>, for humans to interact with the API server</p>&#13;
</li>&#13;
<li>&#13;
<p>Reconciliation from current state of objects to desired state</p>&#13;
</li>&#13;
<li>&#13;
<p>Providing a basic service abstraction to aid in routing requests to and from &#13;
<span class="keep-together">workloads</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Exposing multiple interfaces to support pluggable networking, storage, and more</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>These capabilities create what the project itself claims to be, a <em>production-grade container orchestrator</em>. In simpler terms, Kubernetes provides a way for us to run and schedule containerized workloads on multiple hosts. Keep this primary capability in mind as we dive deeper. Over time, we hope to prove how this capability, while foundational, is only part of our journey to production.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Core Components" data-type="sect2"><div class="sect2" id="idm45612005173016">&#13;
<h2>The Core Components</h2>&#13;
&#13;
<p>What are the components that provide the functionality we have covered?<a data-primary="Kubernetes" data-secondary="core components" data-type="indexterm" id="ix_Kubcmp"/> As we have mentioned, core components reside in the <code>kubernetes/kubernetes</code> repository. Many of us consume these components in different ways. For example, those running managed services such as Google Kubernetes Engine (GKE) are likely to find each component present on hosts. Others may be downloading binaries from repositories or getting signed versions from a vendor. Regardless, anyone can download a Kubernetes release from the <code>kubernetes/kubernetes</code> repository. After downloading and unpacking a release, binaries may be retrieved using the <code>cluster/get-kube-binaries.sh</code> command. This will auto-detect your target architecture and download server and client components. Let’s take a look at this in the following code, and then explore the key components:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>./cluster/get-kube-binaries.sh&#13;
&#13;
Kubernetes release: v1.18.6&#13;
Server: linux/amd64  <code class="o">(</code>to override, <code class="nb">set </code>KUBERNETES_SERVER_ARCH<code class="o">)</code>&#13;
Client: linux/amd64  <code class="o">(</code>autodetected<code class="o">)</code>&#13;
&#13;
Will download kubernetes-server-linux-amd64.tar.gz from https://dl.k8s.io/v1.18.6&#13;
Will download and extract kubernetes-client-linux-amd64.tar.gz&#13;
Is this ok? <code class="o">[</code>Y<code class="o">]</code>/n</pre>&#13;
&#13;
<p>Inside the downloaded server components, likely saved to <em>server/kubernetes-server-${ARCH}.tar.gz</em>, you’ll find the key items that compose a Kubernetes cluster:</p>&#13;
<dl>&#13;
<dt>API Server</dt>&#13;
<dd>&#13;
<p>The primary interaction point for all Kubernetes components and users. This is where we get, add, delete, and mutate objects.<a data-primary="API server" data-type="indexterm" id="idm45612003562808"/> The API server delegates state to a backend, which is most commonly etcd.</p>&#13;
</dd>&#13;
<dt>kubelet</dt>&#13;
<dd>&#13;
<p>The on-host agent that communicates with <a data-primary="kubelet" data-type="indexterm" id="idm45612001816040"/>the API server to report the status of a node and understand what workloads should be scheduled on it. It communicates with the host’s container runtime, such as Docker, to ensure workloads scheduled for the node are started and healthy.</p>&#13;
</dd>&#13;
<dt>Controller Manager</dt>&#13;
<dd>&#13;
<p>A set of controllers, bundled in a single binary, that handle reconciliation of many core objects in Kubernetes.<a data-primary="controller manager" data-type="indexterm" id="idm45612003303256"/> When desired state is declared, e.g., three replicas in a Deployment, a controller within handles the creation of new Pods to satisfy this state.</p>&#13;
</dd>&#13;
<dt>Scheduler</dt>&#13;
<dd>&#13;
<p>Determines where workloads should run based on what it thinks is the optimal node. <a data-primary="scheduler" data-type="indexterm" id="idm45612003016984"/>It uses filtering and scoring to make this decision.</p>&#13;
</dd>&#13;
<dt>Kube Proxy</dt>&#13;
<dd>&#13;
<p>Implements Kubernetes services providing virtual IPs that can route to backend Pods. <a data-primary="kube-proxy" data-type="indexterm" id="idm45612002957560"/>This is accomplished using a packet filtering mechanism on a host such as <code>iptables</code> or <code>ipvs</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>While not an exhaustive list, these are the primary components that make up the core functionality we have discussed. Architecturally, <a data-type="xref" href="#the_primary_components_that_make_up_the_kubernetes_cluster">Figure 1-1</a> shows how these components play together.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Kubernetes architectures have many variations. For example, many clusters run kube-apiserver, kube-scheduler, and kube-controller-manager as containers. This means the control-plane may also run a container-runtime, kubelet, and kube-proxy. These kinds of deployment considerations will be covered in the next chapter.<a data-primary="Kubernetes" data-secondary="core components" data-startref="ix_Kubcmp" data-type="indexterm" id="idm45612002053256"/></p>&#13;
</div>&#13;
&#13;
<figure><div class="figure" id="the_primary_components_that_make_up_the_kubernetes_cluster">&#13;
<img alt="prku 0101" src="assets/prku_0101.png"/>&#13;
<h6><span class="label">Figure 1-1. </span>The primary components that make up the Kubernetes cluster. Dashed borders represent components that are not part of core Kubernetes.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Beyond Orchestration—Extended Functionality" data-type="sect2"><div class="sect2" id="idm45612002326296">&#13;
<h2>Beyond Orchestration—Extended Functionality</h2>&#13;
&#13;
<p>There are areas where<a data-primary="Kubernetes" data-secondary="extended functionality" data-type="indexterm" id="idm45611997383016"/> Kubernetes does more than just orchestrate workloads. As mentioned, the component kube-proxy programs hosts to provide a virtual IP (VIP) experience for workloads. As a result, internal IP addresses are established and route to one or many underlying Pods. This concern certainly goes beyond running and scheduling containerized workloads. In theory, rather than implementing this as part of core Kubernetes, the project could have defined a Service API and required a plug-in to implement the Service abstraction.<a data-primary="Service API" data-secondary="plug-ins implementing" data-type="indexterm" id="idm45612004292200"/> This approach would require users to choose between a variety of plug-ins in the ecosystem rather than including it as core &#13;
<span class="keep-together">functionality</span>.</p>&#13;
&#13;
<p>This is the model many Kubernetes APIs, such as Ingress and NetworkPolicy, take. <a data-primary="NetworkPolicy API" data-type="indexterm" id="idm45612004781304"/><a data-primary="Ingress" data-type="indexterm" id="idm45612004669272"/>For example, creation of an Ingress object in a Kubernetes cluster does not guarantee action is taken. In other words, while the API exists, it is not core functionality. Teams must consider what technology they’d like to plug in to implement this API. For Ingress, many use a controller such as <a href="https://kubernetes.github.io/ingress-nginx">ingress-nginx</a>, which runs in the cluster.<a data-primary="ingress-nginx controller" data-type="indexterm" id="idm45611997439608"/> It implements the API by reading Ingress objects and creating NGINX configurations for NGINX instances pointed at Pods. However, <code>ingress-nginx</code> is one of many options. <a href="https://projectcontour.io">Project Contour</a> implements the same Ingress API but instead programs instances of envoy, the proxy that underlies Contour.<a data-primary="Contour" data-type="indexterm" id="idm45612001466200"/> Thanks to this pluggable model, there are a variety of options available to teams.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Interfaces" data-type="sect2"><div class="sect2" id="idm45612003673176">&#13;
<h2>Kubernetes Interfaces</h2>&#13;
&#13;
<p>Expanding on this idea of adding functionality, we should now explore interfaces.<a data-primary="interfaces (Kubernetes)" data-type="indexterm" id="idm45612002415048"/> Kubernetes interfaces enable us to customize and build on the core functionality. We consider an interface to be a definition or contract on how something can be interacted with. In software development, this parallels the idea of defining functionality, which classes or structs may implement.<a data-primary="plug-ins" data-secondary="interface/plug-in relationship" data-type="indexterm" id="idm45612002324184"/> In systems like Kubernetes, we deploy plug-ins that satisfy these interfaces, providing functionality such as networking.<a data-primary="CRI" data-see="Container Runtime Interface" data-type="indexterm" id="idm45612001529096"/></p>&#13;
&#13;
<p>A specific example of this interface/plug-in relationship is the <a href="https://github.com/kubernetes/cri-api">Container Runtime Interface</a> (CRI). In the early days of Kubernetes, there was a single container runtime supported, Docker.<a data-primary="Container Runtime Interface (CRI)" data-type="indexterm" id="idm45612008930648"/><a data-primary="Docker" data-secondary="alternatives to" data-type="indexterm" id="idm45612003639544"/> While Docker is still present in many clusters today, there is growing interest in using alternatives such as <a href="https://containerd.io">containerd</a> or <a href="https://github.com/cri-o/cri-o">CRI-O</a>. <a data-type="xref" href="#two_workloads_nodes_running_two_different_container_runtimes">Figure 1-2</a> demonstrates this relationship with these two container runtimes.</p>&#13;
&#13;
<figure><div class="figure" id="two_workloads_nodes_running_two_different_container_runtimes">&#13;
<img alt="prku 0102" src="assets/prku_0102.png"/>&#13;
<h6><span class="label">Figure 1-2. </span>Two workload nodes running two different container runtimes. The kubelet sends commands defined in the CRI such as <code>CreateContainer</code> and expects the runtime to satisfy the request and respond.</h6>&#13;
</div></figure>&#13;
&#13;
<p>In many interfaces, commands, such as <code>CreateContainerRequest</code> or <code>PortForwardRequest</code>, are issued as remote procedure calls (RPCs).<a data-primary="RPCs (remote procedure calls), interface commands issued as" data-type="indexterm" id="idm45612003620824"/> In the case of CRI, the communication happens over GRPC and the kubelet expects responses such as <code>CreateContainerResponse</code> and <code>PortForwardResponse</code>. In <a data-type="xref" href="#two_workloads_nodes_running_two_different_container_runtimes">Figure 1-2</a>, you’ll also notice two different models for satisfying CRI. CRI-O was built from the ground up as an implementation of CRI. Thus the kubelet issues these commands directly to it. containerd supports a plug-in that acts as a shim between the kubelet and its own interfaces. Regardless of the exact architecture, the key is getting the container runtime to execute, without the kubelet needing to have operational knowledge of how this occurs for <em>every possible</em> runtime. This concept is what makes interfaces so powerful in how we architect, build, and deploy Kubernetes clusters.</p>&#13;
&#13;
<p>Over time, we’ve even seen some functionality removed from the core project in favor of this plug-in model. These are things that historically existed “in-tree,” meaning within the <code>kubernetes/kubernetes</code> code base. An example of this is <a href="https://github.com/kubernetes/cloud-provider">cloud-provider integrations</a> (CPIs).<a data-primary="cloud provider integration" data-type="indexterm" id="idm45612009750376"/> Most CPIs were traditionally baked into components such as the kube-controller-manager and the kubelet. These integrations typically handled concerns such as provisioning load balancers or exposing cloud provider metadata. Sometimes, especially prior to the creation of the <a href="https://kubernetes-csi.github.io/docs/introduction.html">Container Storage Interface (CSI)</a>, these providers provisioned block storage and made it available to the workloads running in Kubernetes.<a data-primary="Container Storage Interface (CSI)" data-type="indexterm" id="idm45612003293416"/> That’s a lot of functionality to live in Kubernetes, not to mention it needs to be re-implemented for every possible provider! As a better solution, support was moved into its own interface model, e.g., <a href="https://github.com/kubernetes/cloud-provider">kubernetes/cloud-provider</a>, that can be implemented by multiple projects or vendors. Along with minimizing sprawl in the Kubernetes code base, this enables CPI functionality to be managed out of band of the core Kubernetes clusters. This includes common procedures such as upgrades or patching vulnerabilities.</p>&#13;
&#13;
<p>Today, there are several interfaces that enable customization and additional functionality in Kubernetes. What follows is a high-level list, which we’ll expand on throughout<a data-primary="Container Networking Interface (CNI)" data-type="indexterm" id="idm45611997384728"/> chapters in this book:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The Container Networking Interface (CNI) enables networking providers to define how they do things from IPAM to actual packet routing.</p>&#13;
</li>&#13;
<li>&#13;
<p>The Container Storage Interface (CSI) enables storage providers to satisfy intra-cluster workload requests. Commonly implemented for technologies such as ceph, vSAN, and EBS.</p>&#13;
</li>&#13;
<li>&#13;
<p>The Container Runtime Interface (CRI) enables a variety of runtimes, common ones including Docker, containerd, and CRI-O. It also has enabled a proliferation of less traditional runtimes, such as firecracker, which leverages KVM to provision a minimal VM.<a data-primary="virtual machines (VMs)" data-type="indexterm" id="idm45612003631608"/><a data-primary="Container Runtime Interface (CRI)" data-type="indexterm" id="idm45612003630904"/></p>&#13;
</li>&#13;
<li>&#13;
<p>The Service Mesh Interface (SMI) is one of the newer interfaces to hit the Kubernetes ecosystem. It hopes to drive consistency when defining things such as traffic policy, telemetry, and management.<a data-primary="service meshes" data-secondary="Service Mesh Interface" data-type="indexterm" id="idm45612003065080"/><a data-primary="SMI" data-see="service meshes, Service Mesh Interface" data-type="indexterm" id="idm45612003064104"/></p>&#13;
</li>&#13;
<li>&#13;
<p>The Cloud Provider Interface (CPI) enables providers such as VMware, AWS, Azure, and more to write integration points for their cloud services with Kubernetes clusters.<a data-primary="Cloud Provider Interface (CPI)" data-type="indexterm" id="idm45612003062168"/></p>&#13;
</li>&#13;
<li>&#13;
<p>The Open Container Initiative Runtime Spec. (OCI) standardizes image formats ensuring that a container image built from one tool, when compliant, can be run in any OCI-compliant container runtime.<a data-primary="OCI (Open Container Initiative)" data-secondary="runtime specification" data-type="indexterm" id="idm45612003060456"/> This is not directly tied to Kubernetes but has been an ancillary help in driving the desire to have pluggable container runtimes (CRI).</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summarizing Kubernetes" data-type="sect2"><div class="sect2" id="idm45612004530664">&#13;
<h2>Summarizing Kubernetes</h2>&#13;
&#13;
<p>Now we have focused in on the scope of Kubernetes.<a data-primary="Kubernetes" data-secondary="summary of" data-type="indexterm" id="idm45612003057800"/> It is a <em>container orchestrator</em>, with a couple extra features here and there. It also has the ability to be extended and customized by leveraging plug-ins to interfaces. Kubernetes can be foundational for many organizations looking for an elegant means of running their applications. However, let’s take a step back for a moment. If we were to take the current systems used to run applications in your organization and replace them with Kubernetes, would that be enough? For many of us, there is much more involved in the components and machinery that make up our current “application platform.”</p>&#13;
&#13;
<p>Historically, we have witnessed a lot of pain when organizations hold the view of having a “Kubernetes” strategy—or when they assume that Kubernetes will be an adequate forcing function for modernizing how they build and run software. Kubernetes is a technology, a great one, but it really should not be the focal point of where you’re headed in the modern infrastructure, platform, and/or software realm. We apologize if this seems obvious, but you’d be surprised how many executive or higher-level architects we talk to who believe that Kubernetes, by itself, is the answer to problems, when in actuality their problems revolve around application delivery, software development, or organizational/people issues. Kubernetes is best thought of as a piece of your puzzle, one that enables you to deliver platforms for your applications. We have been dancing around this idea of an application platform, which we’ll explore next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Defining Application Platforms" data-type="sect1"><div class="sect1" id="idm45612001291192">&#13;
<h1>Defining Application Platforms</h1>&#13;
&#13;
<p>In our path to production, it is key that we consider the idea of an application platform.<a data-primary="application platforms" data-secondary="defining" data-type="indexterm" id="ix_appplt"/> We define an application platform as a viable place to run workloads. Like most definitions in this book, how that’s satisfied will vary from organization to organization. Targeted outcomes will be vast and desirable to different parts of the business—for example, happy developers, reduction of operational costs, and quicker feedback loops in delivering software are a few. The application platform is often where we find ourselves at the intersection of apps and infrastructure. Concerns such as developer experience (devx) are typically a key tenet in this area.</p>&#13;
&#13;
<p>Application platforms come in many shapes and sizes. Some largely abstract underlying concerns such as the IaaS (e.g., AWS) or orchestrator (e.g., Kubernetes).<a data-primary="Heroku" data-type="indexterm" id="idm45612001287016"/> Heroku is a great example of this model. With it you can easily take a project written in languages like Java, PHP, or Go and, using one command, deploy them to production. Alongside your app runs many platform services you’d otherwise need to operate yourself. Things like metrics collection, data services, and continuous delivery (CD). It also gives you primitives to run highly available workloads that can easily scale. Does Heroku use Kubernetes? Does it run its own datacenters or run atop AWS? Who cares? For Heroku users, these details aren’t important. What’s important is delegating these concerns to a provider or platform that enables developers to spend more time solving business problems. This approach is not unique to cloud services. RedHat’s OpenShift follows a similar model, where Kubernetes is more of an implementation detail and developers and platform operators interact with a set of abstractions on top.<a data-primary="OpenShift" data-type="indexterm" id="idm45612001285224"/><a data-primary="RedHat OpenShift" data-type="indexterm" id="idm45612001284552"/></p>&#13;
&#13;
<p>Why not stop here? If platforms <a data-primary="Cloud Foundry" data-type="indexterm" id="idm45612001283496"/>like Cloud Foundry, OpenShift, and Heroku have solved these problems for us, why bother with Kubernetes? A major trade-off to many prebuilt application platforms is the need to conform to their view of the world.<a data-primary="application platforms" data-secondary="defining" data-tertiary="disadvantages of prebuilt platforms" data-type="indexterm" id="idm45612001282440"/> Delegating ownership of the underlying system takes a significant operational weight off your shoulders. At the same time, if how the platform approaches concerns like service discovery or secret management does not satisfy your organizational requirements, you may not have the control required to work around that issue. Additionally, there is the notion of vendor or opinion lock-in. With abstractions come opinions on how your applications should be architected, packaged, and deployed. This means that moving to another system may not be trivial. For example, it’s significantly easier to move workloads between Google Kubernetes Engine (GKE) and Amazon Elastic Kubernetes Engine (EKS) than it is between EKS and Cloud Foundry.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Spectrum of Approaches" data-type="sect2"><div class="sect2" id="idm45612001280248">&#13;
<h2>The Spectrum of Approaches</h2>&#13;
&#13;
<p>At this point, it is clear there are several approaches to establishing a successful application platform.<a data-primary="application platforms" data-secondary="defining" data-tertiary="approaches to" data-type="indexterm" id="idm45612001278616"/> Let’s make some big assumptions for the sake of demonstration and evaluate theoretical trade-offs between approaches. For the average company we work with, say a mid to large enterprise, <a data-type="xref" href="#the_multitude_of_options_available_to_provider_an_applications">Figure 1-3</a> shows an arbitrary evaluation of approaches.</p>&#13;
&#13;
<p>In the bottom-left quadrant, we see deploying Kubernetes clusters themselves, which has a relatively low engineering effort involved, especially when managed services such as EKS are handling the control plane for you. These are lower on production readiness because most organizations will find that more work needs to be done on top of Kubernetes. However, there are use cases, such as teams that use dedicated cluster(s) for their workloads, that may suffice with just Kubernetes.</p>&#13;
&#13;
<figure><div class="figure" id="the_multitude_of_options_available_to_provider_an_applications">&#13;
<img alt="prku 0103" src="assets/prku_0103.png"/>&#13;
<h6><span class="label">Figure 1-3. </span>The multitude of options available to provide an application platform to developers.</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the bottom right, we have the more established platforms, ones that provide an end-to-end developer experience out of the box. Cloud Foundry is a great example of a project that solves many of the application platform concerns. Running software in Cloud Foundry is more about ensuring the software fits within its opinions. OpenShift, on the other hand, which for most is far more production-ready than just Kubernetes, has more decision points and considerations for how you set it up. Is this flexibility a benefit or a nuisance? That’s a key consideration for you.</p>&#13;
&#13;
<p>Lastly, in the top right, we have building an application platform on top of Kubernetes. Relative to the others, this unquestionably requires the most engineering effort, at least from a platform perspective. However, taking advantage of Kubernetes extensibility means you can create something that lines up with your developer, infrastructure, and business needs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Aligning Your Organizational Needs" data-type="sect2"><div class="sect2" id="idm45612001259128">&#13;
<h2>Aligning Your Organizational Needs</h2>&#13;
&#13;
<p>What’s missing from the graph in <a data-type="xref" href="#the_multitude_of_options_available_to_provider_an_applications">Figure 1-3</a> is a third dimension, a z-axis that demonstrates how aligned the approach is with your requirements. Let’s examine another visual representation.<a data-primary="organizational needs, aligning to application platform" data-type="indexterm" id="idm45612001256552"/><a data-primary="application platforms" data-secondary="defining" data-tertiary="aligning your organizational needs" data-type="indexterm" id="idm45612001255912"/> <a data-type="xref" href="#the_added_complexity_of_the_alignment_of_these">Figure 1-4</a> maps out how this might look when considering platform alignment with organizational needs.</p>&#13;
&#13;
<figure><div class="figure" id="the_added_complexity_of_the_alignment_of_these">&#13;
<img alt="prku 0104" src="assets/prku_0104.png"/>&#13;
<h6><span class="label">Figure 1-4. </span>The added complexity of the alignment of these options with your organizational needs, the z-axis.</h6>&#13;
</div></figure>&#13;
&#13;
<p>In terms of requirements, features, and behaviors you’d expect out of a platform, building a platform is almost always going to be the most aligned. Or at least the most capable of aligning. This is because you can build anything! If you wanted to re-implement Heroku in-house, on top of Kubernetes, with minor adjustments to its capabilities, it is technically possible. However, the cost/reward should be weighed out with the other axes (x and y). Let’s make this exercise more concrete by considering the following needs in a next-generation platform:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Regulations require you to run mostly on-premise</p>&#13;
</li>&#13;
<li>&#13;
<p>Need to support your baremetal fleet along with your vSphere-enabled &#13;
<span class="keep-together">datacenter</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Want to support growing demand for developers to package applications in &#13;
<span class="keep-together">containers</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Need ways to build self-service API mechanisms that move you away from “ticket-based” infrastructure provisioning</p>&#13;
</li>&#13;
<li>&#13;
<p>Want to ensure APIs you’re building atop of are vendor agnostic and not going to cause lock-in because it has cost you millions in the past to migrate off these types of &#13;
<span class="data-center">systems</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Are open to paying enterprise support for a variety of products in the stack, but unwilling to commit to models where the entire stack is licensed per node, core, or application instance</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We must understand our engineering maturity, appetite for building and empowering teams, and available resources to qualify whether building an application platform is a sensible undertaking.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summarizing Application Platforms" data-type="sect2"><div class="sect2" id="idm45612001241944">&#13;
<h2>Summarizing Application Platforms</h2>&#13;
&#13;
<p>Admittedly, what constitutes an application platform remains fairly gray. We’ve focused on a variety of platforms that we believe bring an experience to teams far beyond just workload orchestration. We have also articulated that Kubernetes can be customized and extended to achieve similar outcomes. By advancing our thinking beyond “How do I get a Kubernetes” into concerns such as “What is the current developer workflow, pain points, and desires?” platform and infrastructure teams will be more successful with what they build. With a focus on the latter, we’d argue, you are far more likely to chart a proper path to production and achieve nontrivial adoption. At the end of the day, we want to meet infrastructure, security, and developer requirements to ensure our customers—typically developers—are provided a solution that meets their needs. Often we do not want to simply provide a “powerful” engine that every developer must build their own platform atop of, as jokingly depicted in <a data-type="xref" href="#when_developers_desire_an_end_to_end_experience">Figure 1-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="when_developers_desire_an_end_to_end_experience">&#13;
<img alt="prku 0105" src="assets/prku_0105.png"/>&#13;
<h6><span class="label">Figure 1-5. </span>When developers desire an end-to-end experience (e.g., a driveable car), do not expect an engine without a frame, wheels, and more to suffice.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Building Application Platforms on Kubernetes" data-type="sect1"><div class="sect1" id="idm45612001236472">&#13;
<h1>Building Application Platforms on Kubernetes</h1>&#13;
&#13;
<p>Now we’ve identified Kubernetes as one piece of the puzzle in our path to production.<a data-primary="application platforms" data-secondary="defining" data-startref="ix_appplt" data-type="indexterm" id="idm45612001234904"/><a data-primary="application platforms" data-secondary="building on Kubernetes" data-type="indexterm" id="ix_apppltbld"/> With this, it would be reasonable to wonder “Isn’t Kubernetes just missing stuff then?” The Unix philosophy’s principle of “make each program do one thing well” is a compelling aspiration for the Kubernetes project. We believe its best features are largely the ones it does not have! Especially after being burned with one-size-fits-all platforms that try to solve the world’s problems for you. Kubernetes has brilliantly focused on being a great orchestrator while defining clear interfaces for how it can be built on top of. This can be likened to the foundation of a home.</p>&#13;
&#13;
<p>A good foundation should be structurally sound, able to be built on top of, and provide appropriate interfaces for routing utilities to the home. While important, a foundation alone is rarely a habitable place for our applications to live. Typically, we need some form of home to exist on top of the foundation. Before discussing <em>building</em> on top of a foundation such as Kubernetes, let’s consider a pre-furnished apartment as shown in <a data-type="xref" href="#an_apartment_that_is_move_in_ready_similar">Figure 1-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="an_apartment_that_is_move_in_ready_similar">&#13;
<img alt="prku 0106" src="assets/prku_0106.png"/>&#13;
<h6><span class="label">Figure 1-6. </span>An apartment that is move-in ready. Similar to platform as a service options like Heroku. Illustration by Jessica Appelbaum.</h6>&#13;
</div></figure>&#13;
&#13;
<p>This option, similar to our examples such as Heroku, is habitable with no additional work. There are certainly opportunities to customize the experience inside; however, many concerns are solved for us. As long as we are comfortable with the price of rent and are willing to conform to the nonnegotiable opinions within, we can be successful on day one.</p>&#13;
&#13;
<p>Circling back to Kubernetes, which we have likened to a foundation, we can now look to build that habitable home on top of it, as depicted in <a data-type="xref" href="#building_a_house_similar_to_establishing_an_application_platform">Figure 1-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="building_a_house_similar_to_establishing_an_application_platform">&#13;
<img alt="prku 0107" src="assets/prku_0107.png"/>&#13;
<h6><span class="label">Figure 1-7. </span>Building a house. Similar to establishing an application platform, which Kubernetes is foundational to. Illustration by Jessica Appelbaum.</h6>&#13;
</div></figure>&#13;
&#13;
<p>At the cost of planning, engineering, and maintaining, we can build remarkable platforms to run workloads throughout organizations. This means we’re in complete control of every element in the output. The house can and should be tailored to the needs of the future tenants (our applications). Let’s now break down the various layers and considerations that make this possible.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Starting from the Bottom" data-type="sect2"><div class="sect2" id="idm45612001222360">&#13;
<h2>Starting from the Bottom</h2>&#13;
&#13;
<p>First we must start at the bottom, which includes the technology Kubernetes expects to run.<a data-primary="application platforms" data-secondary="building on Kubernetes" data-tertiary="starting from the bottom" data-type="indexterm" id="idm45612001220712"/> This is commonly a datacenter or cloud provider, which offers compute, storage, and networking. Once established, Kubernetes can be bootstrapped on top. Within minutes you can have clusters living atop the underlying infrastructure. There are several means of bootstrapping Kubernetes, and we’ll cover them in depth in <a data-type="xref" href="ch02.html#deployment_models">Chapter 2</a>.</p>&#13;
&#13;
<p>From the point of Kubernetes clusters existing, we next need to look at a conceptual flow to determine what we should build on top. The key junctures are represented in <a data-type="xref" href="#a_flow_our_teams_may_go_through_in_their_path">Figure 1-8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="a_flow_our_teams_may_go_through_in_their_path">&#13;
<img alt="prku 0108" src="assets/prku_0108.png"/>&#13;
<h6><span class="label">Figure 1-8. </span>A flow our teams may go through in their path to production with &#13;
<span class="keep-together">Kubernetes</span>.</h6>&#13;
</div></figure>&#13;
&#13;
<p>From the point of Kubernetes existing, you can expect to quickly be receiving questions such as:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>“How do I ensure workload-to-workload traffic is fully encrypted?”</p>&#13;
</li>&#13;
<li>&#13;
<p>“How do I ensure egress traffic goes through a gateway guaranteeing a consistent source CIDR?”</p>&#13;
</li>&#13;
<li>&#13;
<p>“How do I provide self-service tracing and dashboards to applications?”</p>&#13;
</li>&#13;
<li>&#13;
<p>“How do I let developers onboard without being concerned about them becoming Kubernetes experts?”</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This list can be endless. It is often incumbent on us to determine which requirements to solve at a platform level and which to solve at an application level. The key here is to deeply understand exiting workflows to ensure what we build lines up with current expectations. If we cannot meet that feature set, what impact will it have on the development teams? Next we can start the building of a platform on top of Kubernetes. In doing so, it is key we stay paired with development teams willing to onboard early and understand the experience to make informed decisions based on quick feedback. After reaching production, this flow should not stop. Platform teams should not expect what is delivered to be a static environment that developers will use for decades. In order to be successful, we must constantly be in tune with our development groups to understand where there are issues or potential missing features that could increase development velocity. A good place to start is considering what level of interaction with Kubernetes we should expect from our developers. This is the idea of how much, or how little, we should abstract.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Abstraction Spectrum" data-type="sect2"><div class="sect2" id="idm45612001207768">&#13;
<h2>The Abstraction Spectrum</h2>&#13;
&#13;
<p>In the past, we’ve heard posturing like, “If your application developers know they’re using <a data-primary="application platforms" data-secondary="building on Kubernetes" data-tertiary="abstraction spectrum" data-type="indexterm" id="idm45612001206280"/>Kubernetes, you’ve failed!” This can be a decent way to look at interaction with Kubernetes, especially if you’re building products or services where the underlying orchestration technology is meaningless to the end user. Perhaps you’re building a database management system (DBMS) that supports multiple database technologies. Whether shards or instances of a database run via Kubernetes, Bosh, or Mesos probably doesn’t matter to your developers! However, taking this philosophy wholesale from a tweet into your team’s success criteria is a dangerous thing to do.<a data-primary="abstractions" data-secondary="considerations in application platform built on Kubernetes" data-type="indexterm" id="idm45612001204312"/> As we layer pieces on top of Kubernetes and build platform services to better serve our customers, we’ll be faced with many points of decision to determine what appropriate abstractions looks like. <a data-type="xref" href="#the_various_ends_of_the_spectrum_starting_with_giving">Figure 1-9</a> provides a visualization of this spectrum.</p>&#13;
&#13;
<figure><div class="figure" id="the_various_ends_of_the_spectrum_starting_with_giving">&#13;
<img alt="prku 0109" src="assets/prku_0109.png"/>&#13;
<h6><span class="label">Figure 1-9. </span>The various ends of the spectrum. Starting with giving each team its own Kubernetes cluster to entirely abstracting Kubernetes from your users, via a platform as a service (PaaS) offering.</h6>&#13;
</div></figure>&#13;
&#13;
<p>This can be a question that keeps platform teams up at night. There’s a lot of merit in providing abstractions. Projects like Cloud Foundry provide a fully baked developer experience—an example being that in the context of a single <code>cf push</code> we can take an application, build it, deploy it, and have it serving production traffic. With this goal and experience as a primary focus, as Cloud Foundry furthers its support for running on top of Kubernetes, we expect to see this transition as more of an implementation detail than a change in feature set. Another pattern we see is the desire to offer more than Kubernetes at a company, but not make developers explicitly choose between technologies. For example, some companies have a Mesos footprint alongside a Kubernetes footprint. They then build an abstraction enabling transparent selection of where workloads land without putting that onus on application developers. It also prevents them from technology lock-in. A trade-off to this approach includes building abstractions on top of two systems that operate differently. This requires significant engineering effort and maturity. Additionally, while developers are eased of the burden around knowing how to interact with Kubernetes or Mesos, they instead need to understand how to use an abstracted company-specific system. In the modern era of open source, developers from all over the stack are less enthused about learning systems that don’t translate between organizations. Lastly, a pitfall we’ve seen is an obsession with abstraction causing an inability to expose key features of Kubernetes. Over time this can become a cat-and-mouse game of trying to keep up with the project and potentially making your abstraction as complicated as the system it’s abstracting.</p>&#13;
&#13;
<p>On the other end of the spectrum are platform groups that wish to offer self-service clusters to development teams. This can also be a great model. It does put the responsibility of Kubernetes maturity on the development teams. Do they understand how Deployments, ReplicaSets, Pods, Services, and Ingress APIs work? Do they have a sense for setting millicpus and how overcommit of resources works? Do they know how to ensure that workloads configured with more than one replica are always scheduled on different nodes? If yes, this is a perfect opportunity to avoid over-engineering an application platform and instead let application teams take it from the Kubernetes layer up.</p>&#13;
&#13;
<p>This model of development teams owning their own clusters is a little less common. Even with a team of humans that have a Kubernetes background, it’s unlikely that they want to take time away from shipping features to determine how to manage the life cycle of their Kubernetes cluster when it comes time to upgrade. There’s so much power in all the knobs Kubernetes exposes, but for many development teams, expecting them to become Kubernetes experts on top of shipping software is unrealistic. As you’ll find in the coming chapters, abstraction does not have to be a binary decision. At a variety of points we’ll be able to make informed decisions on where abstractions make sense. We’ll be determining where we can provide developers the right amount of flexibility while still streamlining their ability to get things done.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Determining Platform Services" data-type="sect2"><div class="sect2" id="idm45612001195032">&#13;
<h2>Determining Platform Services</h2>&#13;
&#13;
<p>When building on top of Kubernetes, a key determination is what features should be built into the <a data-primary="application platforms" data-secondary="building on Kubernetes" data-tertiary="determining platform services" data-type="indexterm" id="idm45612001193384"/><a data-primary="services" data-secondary="determining for application platform" data-type="indexterm" id="idm45612001192072"/>platform relative to solved at the application level. Generally this is something that should be evaluated at a case-by-case basis. For example, let’s assume every Java microservice implements a library that facilitates mutual TLS (mTLS) between services.<a data-primary="mTLS (mutual TLS)" data-type="indexterm" id="idm45612001190712"/> This provides applications a construct for identity of workloads and encryption of data over the network. As a platform team, we need to deeply understand this usage to determine whether it is something we should offer or implement at a platform level. Many teams look to solve this by potentially implementing a technology called a service mesh into the cluster.<a data-primary="service meshes" data-secondary="introducing, pros and cons of" data-type="indexterm" id="idm45612001189528"/> An exercise in trade-offs would reveal the following considerations.</p>&#13;
&#13;
<p>Pros to introducing a service mesh:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Java apps no longer need to bundle libraries to facilitate mTLS.</p>&#13;
</li>&#13;
<li>&#13;
<p>Non-Java applications can take part in the same mTLS/encryption system.</p>&#13;
</li>&#13;
<li>&#13;
<p>Lessened complexity for application teams to solve for.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Cons to introducing a service mesh:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Running a service mesh is not a trivial task. It is another distributed system with operational complexity.</p>&#13;
</li>&#13;
<li>&#13;
<p>Service meshes often introduce features far beyond identity and encryption.</p>&#13;
</li>&#13;
<li>&#13;
<p>The mesh’s identity API might not integrate with the same backend system as used by the existing applications.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Weighing these pros and cons, we can come to the conclusion as to whether solving this problem at a platform level is worth the effort. The key is we don’t need to, and should not strive to, solve every application concern in our new platform. This is another balancing act to consider as you proceed through the many chapters in this book. Several recommendations, best practices, and guidance will be shared, but like anything, you should assess each based on the priorities of your business needs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Building Blocks" data-type="sect2"><div class="sect2" id="idm45612001180200">&#13;
<h2>The Building Blocks</h2>&#13;
&#13;
<p>Let’s wrap up this chapter by concretely identifying key building blocks you will have available as you build a platform. <a data-primary="building blocks, application platform on Kubernetes" data-type="indexterm" id="ix_bldblk"/><a data-primary="application platforms" data-secondary="building on Kubernetes" data-tertiary="building blocks" data-type="indexterm" id="ix_apppltbldblk"/>This includes everything from the foundational components to optional platform services you may wish to implement.</p>&#13;
&#13;
<p>The components in <a data-type="xref" href="#many_of_the_key_building_blocks_involved_in_establishing">Figure 1-10</a> have differing importance to differing audiences.</p>&#13;
&#13;
<figure><div class="figure" id="many_of_the_key_building_blocks_involved_in_establishing">&#13;
<img alt="prku 0110" src="assets/prku_0110.png"/>&#13;
<h6><span class="label">Figure 1-10. </span>Many of the key building blocks involved in establishing an application &#13;
<span class="keep-together">platform</span>.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Some components such as container networking and container runtime are required for every cluster, considering that a Kubernetes cluster that can’t run workloads or allow them to communicate would not be very successful. You are likely to find some components to have variance in whether they should be implemented at all. For example, secret management might not be a platform service you intend to implement if applications already get their secrets from an external secret management &#13;
<span class="keep-together">solution</span>.</p>&#13;
&#13;
<p>Some areas, such as security, are clearly missing from <a data-type="xref" href="#many_of_the_key_building_blocks_involved_in_establishing">Figure 1-10</a>. This is because security is not a feature but more so a result of how you implement everything from the IAAS layer up. Let’s explore these key areas at a high level, with the understanding that we’ll dive much deeper into them throughout this book.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="IAAS/datacenter and Kubernetes" data-type="sect3"><div class="sect3" id="idm45612001168664">&#13;
<h3>IAAS/datacenter and Kubernetes</h3>&#13;
&#13;
<p>IAAS/datacenter and Kubernetes form the foundational layer we have called out many times in this chapter. We don’t mean to trivialize this layer because its stability will directly correlate to that of our platform.<a data-primary="building blocks, application platform on Kubernetes" data-secondary="IAAS/datacenter and Kubernetes" data-type="indexterm" id="idm45612001167160"/><a data-primary="IAAS/datacenter and Kubernetes" data-type="indexterm" id="idm45612001166152"/> However, in modern environments, we spend much less time determining the architecture of our racks to support Kubernetes and a lot more time deciding between a variety of deployment options and topologies. Essentially we need to assess how we are going to provision and make available Kubernetes clusters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Container runtime" data-type="sect3"><div class="sect3" id="idm45612001164760">&#13;
<h3>Container runtime</h3>&#13;
&#13;
<p>The container runtime will faciliate the life cycle management of our workloads on each host. <a data-primary="building blocks, application platform on Kubernetes" data-secondary="container runtime" data-type="indexterm" id="idm45612001163352"/><a data-primary="container runtimes" data-type="indexterm" id="idm45612001162408"/>This is commonly implemented using a technology that can manage containers, such as CRI-O, containerd, and Docker. The ability to choose between these different implementations is thanks to the Container Runtime Interface (CRI). Along with these common examples, there are specialized runtimes that support unique requirements, such as the desire to run a workload in a micro-vm.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Container networking" data-type="sect3"><div class="sect3" id="idm45612001160952">&#13;
<h3>Container networking</h3>&#13;
&#13;
<p>Our choice of container networking will<a data-primary="container networking" data-type="indexterm" id="idm45612001159656"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="container networking" data-type="indexterm" id="idm45612001158952"/> commonly address IP address management (IPAM) of workloads and routing protocols to facilitate communication.<a data-primary="IP address management (IPAM)" data-type="indexterm" id="idm45612001157784"/> Common technology choices include Calico or Cilium, which is thanks to the Container Networking Interface (CNI). By plugging a container networking technology into the cluster, the kubelet can request IP addresses for the workloads it starts. Some plug-ins go as far as implementing service abstractions on top of the Pod network.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Storage integration" data-type="sect3"><div class="sect3" id="idm45612001156312">&#13;
<h3>Storage integration</h3>&#13;
&#13;
<p>Storage integration covers what we do when the on-host disk storage just won’t cut it.<a data-primary="storage integration" data-type="indexterm" id="idm45612001154616"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="storage integration" data-type="indexterm" id="idm45612001153912"/> In modern Kubernetes, more and more organizations are shipping stateful workloads to their clusters. These workloads require some degree of certainty that the state will be resilient to application failure or rescheduling events. Storage can be supplied by common systems such as vSAN, EBS, Ceph, and many more. The ability to choose between various backends is facilitated by the Container Storage Interface (CSI). Similar to CNI and CRI, we are able to deploy a plug-in to our cluster that understands how to satisfy the storage needs requested by the application.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service routing" data-type="sect3"><div class="sect3" id="idm45612001152040">&#13;
<h3>Service routing</h3>&#13;
&#13;
<p>Service routing is the facilitation of traffic to and from the workloads we run in Kubernetes.<a data-primary="service routing" data-type="indexterm" id="idm45612001150632"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="service routing" data-type="indexterm" id="idm45612001149928"/> Kubernetes offers a Service API, but this is typically a stepping stone for support of more feature-rich routing capabilities. Service routing builds on container networking and creates higher-level features such as layer 7 routing, traffic patterns, and much more. Many times these are implemented using a technology called an Ingress controller.<a data-primary="Ingress" data-secondary="Ingress controllers" data-type="indexterm" id="idm45612001148520"/><a data-primary="controllers" data-secondary="Ingress" data-see="Ingress; Ingress controllers" data-type="indexterm" id="idm45612001147576"/> At the deeper side of service routing comes a variety of service meshes. This technology is fully featured with mechanisms such as service-to-service mTLS, observability, and support for applications mechanisms such as circuit &#13;
<span class="keep-together">breaking</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Secret management" data-type="sect3"><div class="sect3" id="idm45612001145064">&#13;
<h3>Secret management</h3>&#13;
&#13;
<p>Secret management covers the management and distribution of sensitive data needed by workloads.<a data-primary="secret management" data-type="indexterm" id="idm45612001143592"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="secret management" data-type="indexterm" id="idm45612001142888"/> Kubernetes offers a Secrets API where sensitive data can be interacted with. However, out of the box, many clusters don’t have robust enough secret management and encryption capabilities demanded by several enterprises. This is largely a conversation around defense in depth. At a simple level, we can ensure data is encrypted before it is stored (encryption at rest). At a more advanced level, we can provide integration with various technologies focused on secret management, such as Vault or Cyberark.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Identity" data-type="sect3"><div class="sect3" id="idm45612001141064">&#13;
<h3>Identity</h3>&#13;
&#13;
<p>Identity covers the authentication of humans and workloads.<a data-primary="identity" data-type="indexterm" id="idm45612001139768"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="identity" data-type="indexterm" id="idm45612001139064"/> A common initial ask of cluster administrators is how to authenticate users against a system such as LDAP or a cloud provider’s IAM system. Beyond humans, workloads may wish to identify themselves to support zero-trust networking models where impersonation of workloads is far more challenging. This can be facilitated by integrating an identity provider and using mechanisms such as mTLS to verify a workload.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Authorization/admission control" data-type="sect3"><div class="sect3" id="idm45612001137336">&#13;
<h3>Authorization/admission control</h3>&#13;
&#13;
<p>Authorization is the next step after we can verify the identity of a human or workload.<a data-primary="building blocks, application platform on Kubernetes" data-secondary="authorization/admission control" data-type="indexterm" id="idm45612001136072"/><a data-primary="authorization" data-type="indexterm" id="idm45612001135064"/> When users or workloads interact with the API server, how do we grant or deny their access to resources?<a data-primary="API server" data-secondary="access to resources through" data-type="indexterm" id="idm45612001134136"/> Kubernetes offers an RBAC feature with resource/verb-level controls, but what about custom logic specific to authorization inside our organization?<a data-primary="admission control" data-type="indexterm" id="idm45612001132888"/> Admission control is where we can take this a step further by building out validation logic that can be as simple as looking over a static list of rules to dynamically calling other systems to determine the correct authorization response.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Software supply chain" data-type="sect3"><div class="sect3" id="idm45612001131576">&#13;
<h3>Software supply chain</h3>&#13;
&#13;
<p>The software supply chain covers the entire life cycle of getting software in source code to runtime.<a data-primary="software supply chain" data-type="indexterm" id="idm45612001130168"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="software supply chain" data-type="indexterm" id="idm45612001129464"/><a data-primary="CI/CD (continuous integration/continuous delivery)" data-type="indexterm" id="idm45612001128552"/> This involves the common concerns around continuous integration (CI) and continuous delivery (CD). Many times, developers’ primary interaction point is the pipelines they establish in these systems. Getting the CI/CD systems working well with Kubernetes can be paramount to your platform’s success. Beyond CI/CD are concerns around the storage of artifacts, their safety from a vulnerability standpoint, and ensuring integrity of images that will be run in your cluster.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Observability" data-type="sect3"><div class="sect3" id="idm45612001127032">&#13;
<h3>Observability</h3>&#13;
&#13;
<p>Observability is the umbrella term for all things that help us understand what’s happening with our clusters.<a data-primary="observability" data-type="indexterm" id="idm45612001125608"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="observability" data-type="indexterm" id="idm45612001124904"/> This includes at the system and application layers. Typically, we think of observability to cover three key areas. These are logs, metrics, and tracing. <a data-primary="logging" data-type="indexterm" id="idm45612001123688"/><a data-primary="metrics" data-type="indexterm" id="idm45612001123016"/><a data-primary="tracing" data-type="indexterm" id="idm45612001122344"/>Logging typically involves forwarding log data from workloads on the host to a target backend system. From this system we can aggregate and analyze logs in a consumable way. Metrics involves capturing data that represents some state at a point in time. We often aggregate, or scrape, this data into some system for analysis. Tracing has largely grown in popularity out of the need to understand the interactions between the various services that make up our application stack. As trace data is collected, it can be brought up to an aggregate system where the life of a request or response is shown via some form of context or correlation ID.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Developer abstractions" data-type="sect3"><div class="sect3" id="idm45612001120632">&#13;
<h3>Developer abstractions</h3>&#13;
&#13;
<p>Developer abstractions are the tools and platform services we put in place to make developers successful in our platform.<a data-primary="developer abstractions" data-type="indexterm" id="idm45612001119192"/><a data-primary="abstractions" data-secondary="developer, application platform on Kubernetes" data-type="indexterm" id="idm45612001118488"/><a data-primary="building blocks, application platform on Kubernetes" data-secondary="developer abstractions" data-type="indexterm" id="idm45612001117576"/> As discussed earlier, abstraction approaches live on a spectrum. Some organizations will choose to make the usage of Kubernetes completely transparent to the development teams. Other shops will choose to expose many of the powerful knobs Kubernetes offers and give significant flexibility to every developer. Solutions also tend to focus on the developer onboarding experience, ensuring they can be given access and secure control of an environment they can utilize in the platform.<a data-primary="building blocks, application platform on Kubernetes" data-startref="ix_bldblk" data-type="indexterm" id="idm45612001116040"/><a data-primary="application platforms" data-secondary="building on Kubernetes" data-startref="ix_apppltbldblk" data-tertiary="building blocks" data-type="indexterm" id="idm45612001115128"/><a data-primary="application platforms" data-secondary="building on Kubernetes" data-startref="ix_apppltbld" data-type="indexterm" id="idm45612001113640"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45612001235976">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we have explored ideas spanning Kubernetes, application platforms, and even building application platforms on Kubernetes. Hopefully this has gotten you thinking about the variety of areas you can jump into in order to better understand how to build on top of this great workload orchestrator. For the remainder of the book we are going to dive into these key areas and provide insight, anecdotes, and recommendations that will further build your perspective on platform building. Let’s jump in and start down this path to production!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>