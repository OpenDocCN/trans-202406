<html><head></head><body><section data-pdf-bookmark="Chapter 6. Automated Placement" data-type="chapter" epub:type="chapter"><div class="chapter" id="AutomatedPlacement">&#13;
<h1><span class="label">Chapter 6. </span>Automated Placement</h1>&#13;
&#13;
&#13;
<p><em>Automated Placement</em> is<a data-primary="Automated Placement" data-type="indexterm" id="autoplace06"/><a data-primary="scheduler" data-secondary="Automated Placement" data-type="indexterm" id="Saplace06"/> the core function of the Kubernetes scheduler for assigning new Pods to nodes that match container resource requests and honor scheduling policies. This pattern describes the principles of the Kubernetes scheduling algorithm and how to influence<a data-primary="Pods" data-secondary="influencing Pod assignments" data-type="indexterm" id="idm45902103902704"/> the placement decisions from the outside.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect1"><div class="sect1" id="idm45902103901536">&#13;
<h1>Problem</h1>&#13;
&#13;
<p>A<a data-primary="problems" data-secondary="Pods, influencing assignments of" data-type="indexterm" id="idm45902103900240"/> reasonably sized microservices-based system consists of tens or even hundreds of isolated processes. Containers and Pods do provide nice abstractions for packaging and deployment but do not solve the problem of placing these processes on suitable nodes. With a large and ever-growing number of microservices, assigning and placing them individually to nodes is not a manageable activity.</p>&#13;
&#13;
<p>Containers have dependencies among themselves, dependencies to nodes, and resource demands, and all of that changes over time too. The resources available on a cluster also vary over time, through shrinking or extending the cluster or by having it consumed by already-placed containers. The way we place containers impacts the availability, performance, and capacity of the distributed systems as well. All of that makes scheduling containers to nodes a moving target.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect1"><div class="sect1" id="idm45902103897968">&#13;
<h1>Solution</h1>&#13;
&#13;
<p>In Kubernetes, assigning Pods to nodes is done by the scheduler. It is a part of Kubernetes that is highly configurable, and it is still evolving and improving. In this chapter, we cover the main scheduling control mechanisms, driving forces that affect the placement, why to choose one or the other option, and the resulting consequences. The Kubernetes scheduler is a potent and time-saving tool. It plays a fundamental role in the Kubernetes platform as a whole, but similar to other Kubernetes components (API Server, <a data-primary="Automated Placement" data-type="indexterm" id="idm45902103896240"/>Kubelet), it can be run in isolation or not used at all.</p>&#13;
&#13;
<p>At a very high level, the main operation the Kubernetes scheduler performs is to retrieve each newly created Pod definition from the API Server and assign it to a node. It finds the most suitable node for every Pod  (as long as there is such a node), whether that is for the initial application placement, scaling up, or when moving an application from an unhealthy node to a healthier one. It does this by considering runtime dependencies, resource requirements, and guiding policies for high availability; by spreading Pods horizontally; and also by colocating Pods nearby for performance and low-latency interactions. However, for the scheduler to do its job correctly and allow declarative placement, it needs nodes with available capacity and containers with declared resource profiles and guiding policies in place. Let’s look at each of these in more detail.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Available Node Resources" data-type="sect2"><div class="sect2" id="idm45902103894896">&#13;
<h2>Available Node Resources</h2>&#13;
&#13;
<p>First<a data-primary="nodes" data-secondary="available node resources" data-type="indexterm" id="idm45902103893360"/> of all, the Kubernetes cluster needs to have nodes with enough resource capacity to run new Pods. Every node has capacity available for running Pods, and the scheduler ensures that the sum of the container resources requested for a Pod is less than the available allocatable node capacity. Considering a node dedicated only to Kubernetes, its capacity is calculated using the following formula in <a data-type="xref" href="#ex-node-resources">Example 6-1</a>.</p>&#13;
<div data-type="example" id="ex-node-resources">&#13;
<h5><span class="label">Example 6-1. </span>Node capacity</h5>&#13;
<pre data-type="programlisting">&#13;
<em>Allocatable</em> [capacity for application pods] =&#13;
    <em>Node Capacity</em> [available capacity on a node]&#13;
        - <em>Kube-Reserved</em> [Kubernetes daemons like kubelet, container runtime]&#13;
        - <em>System-Reserved</em> [Operating System daemons like sshd, udev]&#13;
        - <em>Eviction Thresholds</em> [Reserved memory to prevent system OOMs]&#13;
</pre></div>&#13;
&#13;
<p>If you don’t reserve resources for system daemons that power the OS and Kubernetes itself, the Pods can be scheduled up to the full capacity of the node, which may cause Pods and<a data-primary="daemons" data-secondary="resources for system daemons" data-type="indexterm" id="idm45902103886752"/> system daemons to compete for resources, leading to resource starvation issues on the node. Even then, memory pressure on the node can affect all Pods running on it through OOMKilled errors or cause the node to go temporarily offline. OOMKilled is an error message displayed when the Linux kernel’s Out-of-Memory (OOM) killer terminates a process because the system is out of memory. Eviction thresholds are the last resort for the Kubelet to reserve memory on the node and attempt to evict Pods when the available memory drops below the reserved value.</p>&#13;
&#13;
<p>Also keep in mind that if containers are running on a node that is not managed by Kubernetes, the resources used by these containers are not reflected in the node capacity calculations by Kubernetes. A workaround is to run a placeholder Pod that doesn’t do anything but has only resource requests for CPU and memory &#13;
<span class="keep-together">corresponding</span> to the untracked containers’ resource use amount. Such a Pod is created only to represent and reserve the resource consumption of the untracked containers and helps the scheduler build a better resource model of the node.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Container Resource Demands" data-type="sect2"><div class="sect2" id="idm45902103884432">&#13;
<h2>Container Resource Demands</h2>&#13;
&#13;
<p>Another important requirement<a data-primary="containers" data-secondary="resource demands" data-type="indexterm" id="idm45902103882912"/> for an efficient Pod placement is to define the containers’ runtime dependencies and resource demands. We<a data-primary="Predictable Demands" data-type="indexterm" id="idm45902103881680"/><a data-primary="Predictable Demands" data-secondary="Automated Placement" data-type="indexterm" id="idm45902103881008"/> covered that in more detail in <a data-type="xref" data-xrefstyle="chap-num-title" href="ch02.html#PredictableDemands">Chapter 2, “Predictable Demands”</a>. It boils down to having containers that declare their resource profiles (with <code>request</code> and <code>limit</code>) and environment dependencies such as storage or ports. Only then are Pods optimally assigned to nodes and can run without affecting one another and facing resource starvation during peak usage.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scheduler Configurations" data-type="sect2"><div class="sect2" id="idm45902103877744">&#13;
<h2>Scheduler Configurations</h2>&#13;
&#13;
<p>The next piece of the puzzle is having the right filtering or priority configurations for your cluster needs. The scheduler has a<a data-primary="default scheduling alteration" data-type="indexterm" id="idm45902103875984"/> default set of predicate and priority policies<a data-primary="predicate and priority policies" data-type="indexterm" id="idm45902103875136"/> configured that is good enough for most use cases. In Kubernetes versions before v1.23, a scheduling policy can be used to configure the predicates and priorities of a scheduler. Newer versions of Kubernetes moved to scheduling profiles to achieve the same effect. This new approach exposes the different steps of the scheduling process as an extension point and allows you to configure plugins that override the default implementations of the steps. <a data-type="xref" href="#ex-scheduler-plugin">Example 6-2</a> demonstrates how to override the <code>PodTopologySpread</code> plugin from the <code>score</code> step with custom plugins.</p>&#13;
<div data-type="example" id="ex-scheduler-plugin">&#13;
<h5><span class="label">Example 6-2. </span>A scheduler configuration</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubescheduler.config.k8s.io/v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">KubeSchedulerConfiguration</code><code class="w">&#13;
</code><code class="nt">profiles</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">plugins</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">score</code><code class="p">:</code><code class="w">                          </code><a class="co" href="#callout_automated_placement_CO1-1" id="co_automated_placement_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">disabled</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PodTopologySpread</code><code class="w">     </code><a class="co" href="#callout_automated_placement_CO1-2" id="co_automated_placement_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">enabled</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">MyCustomPlugin</code><code class="w">        </code><a class="co" href="#callout_automated_placement_CO1-3" id="co_automated_placement_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">          </code><code class="nt">weight</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_automated_placement_CO1-1" id="callout_automated_placement_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The plugins in this phase provide a score to each node that has passed the filtering phase.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO1-2" id="callout_automated_placement_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This plugin implements topology spread constraints that we will see later in the chapter.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO1-3" id="callout_automated_placement_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The disabled plugin in the previous step is replaced by a new one.</p></dd>&#13;
</dl></div>&#13;
<div data-type="caution"><h6>Caution</h6>&#13;
<p>Scheduler plugins and<a data-primary="custom schedulers" data-type="indexterm" id="idm45902101624720"/> custom schedulers should be defined only by an administrator as part of the cluster configuration. As a regular user deploying applications on a cluster, you can just refer to predefined schedulers.</p>&#13;
</div>&#13;
&#13;
<p>By default, the scheduler uses the default-scheduler profile with default plugins. It is also possible to run multiple schedulers on the cluster, or multiple profiles on the scheduler, and allow Pods to specify which profile to use. Each profile must have a unique name. Then when defining a Pod, you can add the field <code>.spec.schedulerName</code> with the name of your profile to the Pod specification, and the Pod will be processed by the desired scheduler profile.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scheduling Process" data-type="sect2"><div class="sect2" id="idm45902105323904">&#13;
<h2>Scheduling Process</h2>&#13;
&#13;
<p>Pods get assigned to nodes with certain capacities based on<a data-primary="placement policies" data-type="indexterm" id="idm45902105322336"/><a data-primary="Pods" data-secondary="scheduling" data-type="indexterm" id="idm45902105321632"/> placement policies. For completeness, <a data-type="xref" href="#img-scheduler">Figure 6-1</a> visualizes at a high level how these elements get together and the main steps a Pod goes through when being scheduled.</p>&#13;
&#13;
<figure class="width-100"><div class="figure" id="img-scheduler">&#13;
<img alt="A Pod to node assignment process" src="assets/kup2_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>A Pod-to-node assignment process</h6>&#13;
</div></figure>&#13;
&#13;
<p>As soon as a Pod is created that is not assigned to a node yet, it gets picked by the scheduler together with all the available nodes and the set of filtering and priority policies. In the first stage, the scheduler applies the filtering policies and removes all nodes that do not qualify. Nodes that meet the Pod’s scheduling requirements are called <em>feasible nodes</em>. In the second stage, the scheduler runs a set of functions to score the remaining feasible nodes and orders them by weight. In the last stage, the scheduler notifies the API server about the assignment decision, which is the primary outcome of the scheduling process. This whole process is also referred to as <em>scheduling</em>, <em>placement</em>, <em>node assignment</em>, or <em>binding</em>.</p>&#13;
&#13;
<p>In most cases, it is better to let the scheduler do the Pod-to-node assignment and not micromanage the placement logic. However, on some occasions, you may want to force the assignment of a Pod to a specific node or group of nodes. This assignment can be done using a node selector.<a data-primary="nodes" data-secondary="nodeSelector" data-type="indexterm" id="idm45902105314096"/>  The <code>.spec.nodeSelector</code> Pod field specifies a map of key-value pairs that must be present as labels on the node for the node to be eligible to run the Pod. For example, let’s say you want to force a Pod to run on a specific node where you have<a data-primary="solid-state drive (SSD)" data-type="indexterm" id="idm45902105312576"/><a data-primary="SSD (solid-state drive)" data-type="indexterm" id="idm45902105311872"/> SSD storage or<a data-primary="GPU" data-type="indexterm" id="idm45902105311072"/> GPU acceleration hardware. With the Pod definition in <a data-type="xref" href="#ex-node-selector">Example 6-3</a> that has <code>nodeSelector</code> matching  <code>disktype: ssd</code>, only nodes that are labeled with <code>disktype=ssd</code> will be eligible to run the Pod.</p>&#13;
<div data-type="example" id="ex-node-selector">&#13;
<h5><span class="label">Example 6-3. </span>Node selector based on type of disk available</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">k8spatterns/random-generator:1.0</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">nodeSelector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">disktype</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ssd</code><code class="w">      </code><a class="co" href="#callout_automated_placement_CO2-1" id="co_automated_placement_CO2-1"><img alt="1" src="assets/1.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_automated_placement_CO2-1" id="callout_automated_placement_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Set of node labels a node must match to be considered the node of this Pod.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>In addition to specifying custom labels to your nodes, you can use some of the default labels that are present on every node. Every node has a unique <code>kubernetes.io/hostname</code> label that can be used to place a Pod on a node by its hostname. Other default labels that indicate the OS, architecture, and instance type can be useful for placement too.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Node Affinity" data-type="sect2"><div class="sect2" id="idm45902105291568">&#13;
<h2>Node Affinity</h2>&#13;
&#13;
<p>Kubernetes supports<a data-primary="nodes" data-secondary="node affinity" data-type="indexterm" id="idm45902105289328"/> many more flexible ways to configure the scheduling processes. One such feature is <em>node affinity</em>, which is a more expressive way of the node selector approach described previously that allows specifying rules as either required or preferred. <em>Required rules</em> must be met for a Pod to be scheduled to a node, whereas preferred rules only imply preference by increasing the weight for the matching nodes without making them mandatory. In addition, the node affinity feature greatly expands the types of constraints you can express by making the language more expressive with operators such as <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>, <code>Gt</code>, or <code>Lt</code>. <a data-type="xref" href="#ex-node-affinity">Example 6-4</a> demonstrates how node affinity is declared.</p>&#13;
<div data-type="example" id="ex-node-affinity">&#13;
<h5><span class="label">Example 6-4. </span>Pod with node affinity</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">affinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">nodeAffinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">requiredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w">   </code><a class="co" href="#callout_automated_placement_CO3-1" id="co_automated_placement_CO3-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">nodeSelectorTerms</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w">                             </code><a class="co" href="#callout_automated_placement_CO3-2" id="co_automated_placement_CO3-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">numberCores</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Gt</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="w"> </code><code class="s">"</code><code class="s">3</code><code class="s">"</code><code class="w"> </code><code class="p-Indicator">]</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">preferredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w">  </code><a class="co" href="#callout_automated_placement_CO3-3" id="co_automated_placement_CO3-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">weight</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">preference</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">matchFields</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">metadata.name</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">NotIn</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">values</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="w"> </code><code class="s">"</code><code class="s">control-plane-node</code><code class="s">"</code><code class="w"> </code><code class="p-Indicator">]</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">k8spatterns/random-generator:1.0</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_automated_placement_CO3-1" id="callout_automated_placement_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Hard requirement that the node must have more than three cores (indicated by a node label) to be considered in the scheduling process. The rule is not reevaluated during execution if the conditions on the node change.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO3-2" id="callout_automated_placement_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Match on labels. In this example, all nodes are matched that have a label <code>numberCores</code> with a value greater than 3.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO3-3" id="callout_automated_placement_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Soft requirements, which is a list of selectors with weights. For every node, the sum of all weights for matching selectors is calculated, and the highest-valued node is chosen, as long as it matches the hard requirement.</p></dd>&#13;
</dl></div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Affinity and Anti-Affinity" data-type="sect2"><div class="sect2" id="idm45902103802256">&#13;
<h2>Pod Affinity and Anti-Affinity</h2>&#13;
&#13;
<p><em>Pod affinity</em><a data-primary="Pods" data-secondary="affinity and antiaffinity" data-type="indexterm" id="idm45902103800624"/> is a more powerful way of scheduling and should be used when <code>nodeSelector</code> is not enough. This mechanism allows you to constrain which nodes a Pod can run based on label or field matching. It doesn’t allow you to express dependencies among Pods to dictate where a Pod should be placed relative to other Pods. To express how Pods should be spread to achieve high availability, or be packed and colocated together to improve latency, you can use Pod affinity and anti-affinity.</p>&#13;
&#13;
<p>Node affinity works at node granularity, but Pod affinity is not limited to nodes and can express rules at various topology levels based on the Pods already running on a node. Using the<a data-primary="topologyKey" data-type="indexterm" id="idm45902103798704"/> <code>topologyKey</code> field, and the matching labels, it is possible to enforce more fine-grained rules, which combine rules on domains like node, rack, cloud provider zone, and region, as demonstrated in <a data-type="xref" href="#ex-pod-affinity">Example 6-5</a>.</p>&#13;
<div data-type="example" id="ex-pod-affinity">&#13;
<h5><span class="label">Example 6-5. </span>Pod with Pod affinity</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">affinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">podAffinity</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">requiredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w">  </code><a class="co" href="#callout_automated_placement_CO4-1" id="co_automated_placement_CO4-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">labelSelector</code><code class="p">:</code><code class="w">                                 </code><a class="co" href="#callout_automated_placement_CO4-2" id="co_automated_placement_CO4-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">          </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">confidential</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">high</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">topologyKey</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">security-zone</code><code class="w">                     </code><a class="co" href="#callout_automated_placement_CO4-3" id="co_automated_placement_CO4-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">podAntiAffinity</code><code class="p">:</code><code class="w">                                   </code><a class="co" href="#callout_automated_placement_CO4-4" id="co_automated_placement_CO4-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="nt">preferredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w"> </code><a class="co" href="#callout_automated_placement_CO4-5" id="co_automated_placement_CO4-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">weight</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">100</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">podAffinityTerm</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">labelSelector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">            </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">              </code><code class="nt">confidential</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">none</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">topologyKey</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubernetes.io/hostname</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">k8spatterns/random-generator:1.0</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_automated_placement_CO4-1" id="callout_automated_placement_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Required rules for the Pod placement concerning other Pods running on the target node.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO4-2" id="callout_automated_placement_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Label selector to find the Pods to be colocated with.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO4-3" id="callout_automated_placement_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The nodes on which Pods with labels <code>confidential=high</code> are running are supposed to carry a <code>security-zone</code> label. The Pod defined here is scheduled to a node with the same label and value.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO4-4" id="callout_automated_placement_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Anti-affinity rules to find nodes where a Pod would <em>not</em> be placed.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO4-5" id="callout_automated_placement_CO4-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Rule describing that the Pod should not (but could) be placed on any node where a Pod with the label <code>confidential=none</code> is running.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Similar to node affinity, there are hard and soft requirements for Pod affinity and anti-affinity, called <code>requiredDuringSchedulingIgnoredDuringExecution</code> and <code>preferredDuringSchedulingIgnoredDuringExecution</code>, respectively. Again, as with node affinity, the <code>IgnoredDuringExecution</code> suffix is in the field name, which exists for future extensibility reasons. At the moment, if the labels on the node change and affinity rules are no longer valid, the Pods continue running,<sup><a data-type="noteref" href="ch06.html#idm45902104710704" id="idm45902104710704-marker">1</a></sup> but in the future, runtime changes may also be taken into account.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Topology Spread Constraints" data-type="sect2"><div class="sect2" id="idm45902104709888">&#13;
<h2>Topology Spread Constraints</h2>&#13;
&#13;
<p>Pod affinity rules<a data-primary="topology spread constraints" data-type="indexterm" id="idm45902104708384"/><a data-primary="Pods" data-secondary="topology spread constraints" data-type="indexterm" id="idm45902104707680"/> allow the placement of unlimited Pods to a single topology, whereas Pod anti-affinity disallows Pods to colocate in the same topology. Topology spread constraints give you more fine-grained control to evenly distribute Pods on your cluster and achieve better cluster utilization or high availability of applications.</p>&#13;
&#13;
<p>Let’s look at an example to understand how topology spread constraints can help. Let’s suppose we have an application with two replicas and a two-node cluster. To avoid downtime and a single point of failure, we can use Pod anti-affinity rules to prevent the coexistence of the Pods on the same node and spread them into both nodes. While this setup makes sense, it will prevent you from performing rolling upgrades because the third replacement Pod cannot be placed on the existing nodes because of the Pod anti-affinity constraints. We will have to either add another node or change the Deployment strategy from rolling to recreate. Topology spread constraints would be a better solution in this situation as they allow you to tolerate some degree of uneven Pod distribution when the cluster is running out of resources. <a data-type="xref" href="#topology-spread-constraint">Example 6-6</a> allows the placement of the third rolling deployment Pod on one of the two nodes because it allows imbalances—i.e., a skew of one Pod.</p>&#13;
<div data-type="example" id="topology-spread-constraint">&#13;
<h5><span class="label">Example 6-6. </span>Pod with topology spread constraints</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bar</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">topologySpreadConstraints</code><code class="p">:</code><code class="w">                  </code><a class="co" href="#callout_automated_placement_CO5-1" id="co_automated_placement_CO5-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">maxSkew</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w">                                </code><a class="co" href="#callout_automated_placement_CO5-2" id="co_automated_placement_CO5-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">topologyKey</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">topology.kubernetes.io/zone</code><code class="w">  </code><a class="co" href="#callout_automated_placement_CO5-3" id="co_automated_placement_CO5-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">whenUnsatisfiable</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">DoNotSchedule</code><code class="w">          </code><a class="co" href="#callout_automated_placement_CO5-4" id="co_automated_placement_CO5-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">labelSelector</code><code class="p">:</code><code class="w">                            </code><a class="co" href="#callout_automated_placement_CO5-5" id="co_automated_placement_CO5-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bar</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">k8spatterns/random-generator:1.0</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_automated_placement_CO5-1" id="callout_automated_placement_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Topology spread constraints are defined in the <code>topologySpreadConstraints</code> field of the Pod spec.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO5-2" id="callout_automated_placement_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p><code>maxSkew</code> defines the maximum degree to which Pods can be unevenly distributed in the topology.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO5-3" id="callout_automated_placement_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>A topology domain is a logical unit of your infrastructure. And a <code>topologyKey</code> is the key of the Node label where identical values are considered to be in the same topology.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO5-4" id="callout_automated_placement_CO5-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The <code>whenUnsatisfiable</code> field defines what action should be taken when &#13;
<span class="keep-together"><code>maxSkew</code></span> can’t be satisfied. <code>DoNotSchedule</code> is a hard constraint preventing the scheduling of Pods, whereas <code>ScheduleAnyway</code> is a soft constraint that gives scheduling priority to nodes that reduce cluster imbalance.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO5-5" id="callout_automated_placement_CO5-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p><code>labelSelector</code> Pods that match this selector are grouped together and counted when spreading them to satisfy the constraint.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Topology spread constraints is a feature that is still evolving at the time of this writing. Built-in cluster-level topology spread constraints allow certain imbalances based on default Kubernetes labels and give you the ability to honor or ignore node affinity and taint policies.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Taints and Tolerations" data-type="sect2"><div class="sect2" id="idm45902104267376">&#13;
<h2>Taints and Tolerations</h2>&#13;
&#13;
<p>A<a data-primary="taints and tolerations" data-type="indexterm" id="tandt06"/> more advanced feature that controls where Pods can be scheduled and allowed to run is based on taints and tolerations. While node affinity is a property of Pods that allows them to choose nodes, taints and tolerations are the opposite. They allow the nodes to control which Pods should or should not be scheduled on them. A <em>taint</em> is a characteristic of the node, and when it is present, it prevents Pods from scheduling onto the node unless the Pod has toleration for the taint.&#13;
In that sense, taints and tolerations can be considered an <em>opt-in</em> to allow scheduling on nodes that by default are not available for scheduling, whereas affinity rules are an <em>opt-out</em> by explicitly selecting on which nodes to run and thus exclude all the nonselected nodes.</p>&#13;
&#13;
<p>A<a data-primary="kubectl" data-secondary="adding taints to nodes" data-type="indexterm" id="idm45902100555648"/><a data-primary="nodes" data-secondary="adding taints to" data-type="indexterm" id="idm45902100554672"/><a data-primary="NoSchedule" data-type="indexterm" id="idm45902100553728"/> taint is added to a node by using <code>kubectl</code>: <code>kubectl taint nodes control-plane-node node-role.kubernetes.io/control-plane="true":NoSchedule</code>, which has the effect shown in <a data-type="xref" href="#ex-taint">Example 6-7</a>. A matching toleration is added to a Pod as shown in <a data-type="xref" href="#ex-toleration">Example 6-8</a>. Notice that the values for <code>key</code> and <code>effect</code> in the <code>taints</code> section of <a data-type="xref" href="#ex-taint">Example 6-7</a> and the <code>tolerations</code> section in <a data-type="xref" href="#ex-toleration">Example 6-8</a> are the same.</p>&#13;
<div data-type="example" id="ex-taint">&#13;
<h5><span class="label">Example 6-7. </span>Tainted node</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Node</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">control-plane-node</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">taints</code><code class="p">:</code><code class="w">                                   </code><a class="co" href="#callout_automated_placement_CO6-1" id="co_automated_placement_CO6-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">effect</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">NoSchedule</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">node-role.kubernetes.io/control-plane</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">value</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">true</code><code class="s">"</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_automated_placement_CO6-1" id="callout_automated_placement_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Mark this node as unschedulable except when a Pod tolerates this taint.</p></dd>&#13;
</dl></div>&#13;
<div data-type="example" id="ex-toleration">&#13;
<h5><span class="label">Example 6-8. </span>Pod tolerating node taints</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">k8spatterns/random-generator:1.0</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">tolerations</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">node-role.kubernetes.io/control-plane</code><code class="w"> </code><a class="co" href="#callout_automated_placement_CO7-1" id="co_automated_placement_CO7-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Exists</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">effect</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">NoSchedule</code><code class="w">                         </code><a class="co" href="#callout_automated_placement_CO7-2" id="co_automated_placement_CO7-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_automated_placement_CO7-1" id="callout_automated_placement_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Tolerate (i.e., consider for scheduling) nodes, which have a taint with key <code>node-role.kubernetes.io/control-plane</code>. On production clusters, this taint is set on the control plane node to prevent scheduling of Pods on this node. A toleration like this allows this Pod to be installed on the control plane node nevertheless.</p></dd>&#13;
<dt><a class="co" href="#co_automated_placement_CO7-2" id="callout_automated_placement_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Tolerate only when the taint specifies a <code>NoSchedule</code> effect. This field can be empty here, in which case the toleration applies to every effect.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>There are hard taints that prevent scheduling on a node (<code>effect=NoSchedule</code>), soft taints that try to avoid scheduling on a node<a data-primary="PreferNoSchedule" data-type="indexterm" id="idm45902104519664"/> (<code>effect=PreferNoSchedule</code>), and taints that can evict already-running Pods from a node<a data-primary="NoExecute" data-type="indexterm" id="idm45902104518416"/> (<code>effect=NoExecute</code>).</p>&#13;
&#13;
<p>Taints and tolerations allow for complex use cases like having dedicated nodes for an exclusive set of Pods, or force eviction of Pods from problematic nodes by tainting those nodes.</p>&#13;
&#13;
<p>You can influence the placement based on the application’s high availability and performance needs, but try not to limit the scheduler too much and back yourself into a corner where no more Pods can be scheduled and there are too many stranded resources. For example, if your containers’ resource requirements are too coarse-grained, or nodes are too small, you may end up with stranded resources in nodes that are not utilized.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#img-resources">Figure 6-2</a>, we can see node A has 4 GB of memory that cannot be utilized as there is no CPU left to place other containers. Creating containers with smaller resource requirements may help improve this situation. Another solution is to use the Kubernetes<a data-primary="Kubernetes" data-secondary="descheduler" data-type="indexterm" id="idm45902104475104"/><a data-primary="Descheduler" data-type="indexterm" id="idm45902104474160"/> <em>descheduler</em>, which helps defragment nodes and improve their utilization.</p>&#13;
&#13;
<figure><div class="figure" id="img-resources">&#13;
<img alt="Processes scheduled to nodes and stranded resources" src="assets/kup2_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>Processes scheduled to nodes and stranded resources</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Once a Pod is assigned to a node, the job of the scheduler is done, and it does not change the placement of the Pod unless the Pod is deleted and recreated without a node assignment. As you have seen, with time, this can lead to resource fragmentation and poor utilization of cluster resources. Another potential issue is that the scheduler decisions are based on its cluster view at the point in time when a new Pod is scheduled. If a cluster is dynamic and the resource profile of the nodes changes or new nodes are added, the scheduler will not rectify its previous Pod placements. Apart from changing the node capacity, you may also alter the labels on the nodes that affect placement, but past placements are not rectified.</p>&#13;
&#13;
<p>All of these scenarios can be addressed by the descheduler. The Kubernetes descheduler is an optional feature that is typically run as a Job whenever a cluster administrator decides it is a good time to tidy up and defragment a cluster by rescheduling the Pods. The descheduler comes with some predefined policies that can be enabled and tuned or disabled.</p>&#13;
&#13;
<p>Regardless of the policy used, the descheduler avoids evicting the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Node- or cluster-critical Pods</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods not managed by a ReplicaSet, Deployment, or Job, as these Pods cannot be recreated</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods<a data-primary="DaemonSets" data-secondary="avoiding Pod eviction" data-type="indexterm" id="idm45902104466352"/> managed by a DaemonSet</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods that have local storage</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods with PodDisruptionBudget, where eviction would violate its rules</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods that have a non-nil <code>DeletionTimestamp</code> field set</p>&#13;
</li>&#13;
<li>&#13;
<p>Deschedule Pod itself (achieved by marking itself as a critical Pod)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Of course, all evictions respect Pods’ QoS levels by choosing <em>Best-Efforts</em> Pods first, then <em>Burstable</em> Pods, and finally <em>Guaranteed</em> Pods as candidates for eviction. See <a data-type="xref" data-xrefstyle="chap-num-title" href="ch02.html#PredictableDemands">Chapter 2, “Predictable Demands”</a>, for a detailed explanation of these QoS levels.<a data-primary="" data-startref="tandt06" data-type="indexterm" id="idm45902104458112"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect1"><div class="sect1" id="idm45902103897472">&#13;
<h1>Discussion</h1>&#13;
&#13;
<p>Placement<a data-primary="Predictable Demands" data-secondary="Automated Placement" data-type="indexterm" id="idm45902104455568"/><a data-primary="Predictable Demands" data-type="indexterm" id="idm45902104454560"/> is the art of assigning Pods to nodes. You want to have as minimal intervention as possible, as the combination of multiple configurations can be hard to predict.&#13;
In simpler scenarios, scheduling Pods based on resource constraints should be sufficient. If you follow the guidelines from <a data-type="xref" data-xrefstyle="chap-num-title" href="ch02.html#PredictableDemands">Chapter 2, “Predictable Demands”</a>, and declare all the resource needs of a container, the scheduler will do its job and place the Pod on the most feasible node possible.</p>&#13;
&#13;
<p>However, in more realistic scenarios, you may want to schedule Pods to specific nodes according to other constraints such as data locality, Pod colocality, application high availability, and efficient cluster resource utilization. In these cases, there are multiple ways to steer the scheduler toward the desired deployment topology.</p>&#13;
&#13;
<p><a data-type="xref" href="#img-placement">Figure 6-3</a> shows one approach to thinking and making sense of the different scheduling techniques in Kubernetes.</p>&#13;
&#13;
<figure><div class="figure" id="img-placement">&#13;
<img alt="Pod-to-Node and Pod-to-Pod dependencies" src="assets/kup2_0603.png"/>&#13;
<h6><span class="label">Figure 6-3. </span>Pod-to-Pod and Pod-to-Node and dependencies</h6>&#13;
</div></figure>&#13;
&#13;
<p>Start by identifying the forces and dependencies between the Pod and the nodes (for example, based on dedicated hardware capabilities or efficient resource utilization). Use the following node affinity techniques to direct the Pod to the desired nodes, or use anti-affinity techniques to steer the Pod away from the undesired nodes:</p>&#13;
<dl>&#13;
<dt>nodeName</dt>&#13;
<dd>&#13;
<p>This field provides the simplest form of hard wiring a Pod to a node. This field should ideally be populated by the scheduler, which is driven by policies rather than manual node assignment. Assigning a Pod to a node through this approach prevents the scheduling of the Pod to any other node. If the named node has no capacity, or the node doesn’t exist, the Pod will never run. This throws us back into the pre-Kubernetes era, when we explicitly needed to specify the nodes to run our applications. Setting this field manually is not a Kubernetes best practice and should be used only as an exception<a data-primary="nodeName" data-type="indexterm" id="idm45902104446752"/>.</p>&#13;
</dd>&#13;
<dt>nodeSelector</dt>&#13;
<dd>&#13;
<p>A node selector is a label map.<a data-primary="nodes" data-secondary="nodeSelector" data-type="indexterm" id="idm45902104444608"/> For the Pod to be eligible to run on a node, the Pod must have the indicated key-value pairs as the label on the node. Having put some meaningful labels on the Pod and the node (which you should do anyway), a node selector is one of the simplest recommended mechanisms for controlling the scheduler choices.</p>&#13;
</dd>&#13;
<dt>Node affinity</dt>&#13;
<dd>&#13;
<p>This<a data-primary="nodes" data-secondary="node affinity" data-type="indexterm" id="idm45902104442224"/> rule improves the manual node assignment approaches and allows a Pod to express dependency toward nodes using logical operators and constraints that provides fine-grained control. It also offers soft and hard scheduling requirements that control the strictness of node affinity constraints.</p>&#13;
</dd>&#13;
<dt>Taints and tolerations</dt>&#13;
<dd>&#13;
<p>Taints and tolerations<a data-primary="taints and tolerations" data-type="indexterm" id="idm45902104439808"/> allow the node to control which Pods should or should not be scheduled on them without modifying existing Pods. By default, Pods that don’t have tolerations for the node taint will be rejected or evicted from the node. Another advantage of taints and tolerations is that if you expand the Kubernetes cluster by adding new nodes with new labels, you don’t need to add the new labels on all Pods but only on those that should be placed on the new nodes.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Once the desired correlation between a Pod and the nodes is expressed in Kubernetes terms, identify the dependencies between different Pods. Use Pod affinity techniques for Pod colocation for tightly coupled applications, and use Pod anti-affinity techniques to spread Pods on nodes and avoid a single point of failure:</p>&#13;
<dl>&#13;
<dt>Pod affinity and anti-affinity</dt>&#13;
<dd>&#13;
<p>These<a data-primary="Pods" data-secondary="affinity and antiaffinity" data-type="indexterm" id="idm45902104436432"/> rules allow scheduling based on Pods’ dependencies on other Pods rather than nodes. Affinity rules help for colocating tightly coupled application stacks composed of multiple Pods on the same topology for low-latency and data locality requirements. The anti-affinity rule, on the other hand, can spread Pods across your cluster among failure domains to avoid a single point of failure, or prevent resource-intensive Pods from competing for resources by avoiding placing them on the same node.</p>&#13;
</dd>&#13;
<dt>Topology spread constraints</dt>&#13;
<dd>&#13;
<p>To use these features, platform administrators have to label nodes and provide topology information such as regions, zones, or other user-defined domains. Then, a workload author creating the Pod configurations must be aware of the underlying cluster topology and specify the topology spread constraints. You can also specify multiple topology spread constraints, but all of them must be satisfied for a Pod to be placed. You must ensure that they do not conflict with one another.&#13;
You can also combine this feature with NodeAffinity and NodeSelector to filter nodes where evenness should be applied. In that case, be sure to understand the difference: multiple topology spread constraints are about calculating the result set independently and producing an AND-joined result, while combining it with NodeAffinity and NodeSelector, on the other hand, filters results of node constraints.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p class="pagebreak-before">In some scenarios, all of these scheduling configurations might not be flexible enough to express bespoke scheduling requirements. In that case, you may have to customize and tune the scheduler configuration or even provide a custom scheduler implementation that can understand your custom needs:</p>&#13;
<dl>&#13;
<dt>Scheduler tuning</dt>&#13;
<dd>&#13;
<p>The<a data-primary="default scheduling alteration" data-type="indexterm" id="idm45902104431536"/> default scheduler is responsible for the placement of new Pods onto nodes within the cluster, and it does it well. However, it is possible to alter one or more stages in the filtering and prioritization phases. This mechanism with extension points and plugins is specifically designed to allow small alterations without the need for a completely new scheduler implementation.</p>&#13;
</dd>&#13;
<dt>Custom scheduler</dt>&#13;
<dd>&#13;
<p>If<a data-primary="custom schedulers" data-type="indexterm" id="idm45902104428928"/> none of the preceding approaches is good enough, or if you have complex scheduling requirements, you can also write your own custom scheduler. A custom scheduler can run instead of, or alongside, the standard Kubernetes scheduler. A hybrid approach is to have a “scheduler extender” process that the standard Kubernetes scheduler calls out to as a final pass when making scheduling decisions. This way, you don’t have to implement a full scheduler but only provide HTTP APIs to filter and prioritize nodes. The advantage of having your scheduler is that you can consider factors outside of the Kubernetes cluster like hardware cost, network latency, and better utilization while assigning Pods to nodes. You can also use multiple custom schedulers alongside the default scheduler and configure which scheduler to use for each Pod. Each scheduler could have a different set of policies dedicated to a subset of the Pods.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>To sum up, there are lots of ways to control the Pod placement, and choosing the right approach or combining multiple approaches can be overwhelming. The takeaway from this chapter is this: size and declare container resource profiles, and label Pods and nodes for the best resource-consumption-driven scheduling results. If that doesn’t deliver the desired scheduling outcome, start with small and iterative changes. Strive for a minimal policy-based influence on the Kubernetes scheduler to express node dependencies and then inter-Pod dependencies.<a data-primary="" data-startref="autoplace06" data-type="indexterm" id="idm45902104427424"/><a data-primary="" data-startref="Saplace06" data-type="indexterm" id="idm45902104426448"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="More Information" data-type="sect1"><div class="sect1" id="idm45902104425248">&#13;
<h1>More Information</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/N-iAz">Automated Placement Example</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/QlbMB">Assigning Pods to Nodes</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/iPbBT">Scheduler Configuration</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/qkp60">Pod Topology Spread Constraints</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/appyT">Configure Multiple Schedulers</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/4lPFX">Descheduler for Kubernetes</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/oNGSR">Disruptions</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/w9tKY">Guaranteed Scheduling for Critical Add-On Pods</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/_MODM">Keep Your Kubernetes Cluster Balanced: The Secret to High Availability</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/6Tog3">Advanced Kubernetes Pod to Node Scheduling</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45902104710704"><sup><a href="ch06.html#idm45902104710704-marker">1</a></sup> However, if node labels change and allow for unscheduled Pods to match their node affinity selector, these Pods are scheduled on this node.</p></div></div></section></body></html>