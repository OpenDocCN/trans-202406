- en: Chapter 7\. Secret Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In any application stack, we are almost guaranteed to run into secret data.
    This is the data that applications want to keep, well, secret. Commonly, we associate
    secrets with credentials. Often these credentials are used to access systems within
    or external to the cluster, such as databases or message queues. We also run into
    secret data when using private keys, which may support our application’s ability
    to perform mutual TLS with other applications. These kinds of concerns are covered
    in [Chapter 11](ch11.html#building_platform_services). The existence of secrets
    bring in many operational concerns to consider, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Secret rotation policies
  prefs: []
  type: TYPE_NORMAL
- en: How long is a secret allowed to remain before it must be changed?
  prefs: []
  type: TYPE_NORMAL
- en: Key (encryption) rotation policies
  prefs: []
  type: TYPE_NORMAL
- en: Assuming secret data is encrypted at the application layer before being persisted
    to disk, how long is an encryption key allowed to stay around before it must be
    rotated?
  prefs: []
  type: TYPE_NORMAL
- en: Secret storage policies
  prefs: []
  type: TYPE_NORMAL
- en: What requirements must be satisfied in order to store secret data? Do you need
    to persist secrets to isolated hardware? Do you need your secret management solution
    to integrate with a hardware security module (HSM)?
  prefs: []
  type: TYPE_NORMAL
- en: Remediation plan
  prefs: []
  type: TYPE_NORMAL
- en: If secret(s) or encryption key(s) are compromised, how do you plan to remediate?
    Can your plan or automation be run without impact to applications?
  prefs: []
  type: TYPE_NORMAL
- en: A good starting point is to determine what layer to offer secret management
    for your applications. Some organizations choose to not solve this at a platform
    level and instead expect application teams to inject secrets into their applications
    dynamically. For example, if an organization is running a secret management system
    such as Vault, applications can talk directly to the API to authenticate and retrieve
    secrets. Application frameworks may even offer libraries to talk directly to these
    systems. For example, Spring offers the spring-vault project to authenticate against
    Vault, retrieve secrets, and inject their values directly into Java classes. While
    possible to do at the application layer, many platform teams aspire to offer enterprise-grade
    secret capabilities as platform services, perhaps in a way that application developers
    need not be concerned with how the secret got there or what external provider
    (e.g., Vault) is being used under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll dive into how to think about secret data in Kubernetes.
    We’ll start at lower-level layers Kubernetes runs on and work up to the APIs Kubernetes
    exposes that make secret data available to workloads. Like many topics in this
    book, you’ll find these considerations and recommendations to live on a spectrum—one
    end of the spectrum includes how secure you’re willing to get relative to engineering
    effort and tolerance for risk, and the other end focuses on what level of abstractions
    you’d like to provide to developers consuming this platform.
  prefs: []
  type: TYPE_NORMAL
- en: Defense in Depth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Protection of our secret data largely comes down to what depths we’re willing
    to go to make it secure. As much as we’d like to say we always choose the most
    secure options, the reality is we make sensible decisions that keep us “safe enough”
    and ideally harden them over time. This comes with risk and technical debt that
    is quintessential to our work. However, there’s no denying that a misjudgment
    of what is “safe enough” can quickly make us famous, and not in a good way. In
    the following sections, we’ll look at these layers of security and call out some
    of the most critical points.
  prefs: []
  type: TYPE_NORMAL
- en: Defense can start literally at the physical layer. A prime example is Google.
    It has multiple [whitepapers](https://cloud.google.com/security/overview/whitepaper),
    and even a video on [YouTube](https://oreil.ly/dtHUx), that describe its approach
    to datacenter security. This includes metal detectors, vehicle barriers capable
    of stopping a semi truck, and several layers of building security just to get
    into the datacenter. This attention to detail extends beyond what is live and
    racked. When drives are retired, Google has authorized staff zero-out the data
    and then potentially crush and shred the drives. While the subject of physical
    security is interesting, this book will not go into depth around physical security
    of your datacenter, but the steps cloud providers take to ensure the security
    of their hardware are amazing, and that’s just the start.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say a human did somehow get access to a disk before it was zeroed out
    or crushed. Most cloud providers and datacenters are securing their physical disks
    by ensuring the drive is encrypted at rest. Providers may do this with their own
    encryption keys and/or they allow customers to provide their own keys, which makes
    it near impossible for providers to access your unencrypted data. This is a perfect
    example of defense in depth. We are protected from a physical standpoint, intra-datacenter,
    and we extend that to encrypting data on the physical disks themselves, closing
    off further opportunities for bad actors internally to do anything with users’
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Disk Encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a closer look at the disk encryption domain. There are several ways
    to encrypt disks. A common method in Linux for full block encryption is leveraging
    Linux Unified Key System (LUKS). LUKS works in conjunction with the dm-crypt encryption
    subsystem, which has been available in the Linux kernel since version 2.6\. For
    dedicated storage systems such as vSAN, ceph, or Gluster, each provide one or
    many means to provide encryption at rest. In cloud providers, the default encryption
    behavior can vary. For AWS, you should explore its documentation to enable encryption
    for Elastic Block Storage. AWS offers the ability to enable encryption by default,
    which we recommend as a best-practice setting. Google Cloud, on the other hand,
    performs encryption at rest as its default mode. Similar to AWS, it can be configured
    with the Key Management Service (KMS), which enables you to customize the encryption
    behavior, such as providing your own encryption keys.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the trust you do or don’t have for your cloud provider or datacenter
    operators, we highly recommend encryption at rest as your default practice. Encryption
    at rest essentially means the data is *stored* encrypted. Not only is this good
    for mitigating attack vectors, but it provides some protection against possible
    mistakes. For example, the world of virtual machines has made it trivial to create
    snapshots of hosts. Snapshots end up like any other file, data that is too easy
    to accidentally expose to an internal or external network. In the spirit of defense
    in depth, we should protect ourselves against this scenario where, if we select
    the wrong button via a UI or field in an API, the leaked data is useless for those
    without private key access. [Figure 7-1](#permission_setting) shows the UI for
    how easily these permissions can be toggled.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0701](assets/prku_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Permission setting on an AWS snapshot, as the warning says “making
    public” will give others access to create a volume from this snapshot and, potentially,
    access the data.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Transport Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a better understanding of encrypted data at rest, how about data that is
    actively in flight? Without having explored the Kubernetes secret architecture
    yet, let’s look at the paths a secret can take when being transported between
    services. [Figure 7-2](#diagram_demonstrating_points_where_a_secret_may_pass_over_the_wire)
    shows some of the interaction points for secrets. The arrows represent the secret
    moving through the network between hosts.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0702](assets/prku_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Diagram demonstrating points where a secret may pass over the wire.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The figure shows secret data moving across the network to reach different hosts.
    No matter how strong our encryption at rest strategy is, if any of the interaction
    points do not communicate over TLS, we have exposed our secret data. As seen [Figure 7-2](#diagram_demonstrating_points_where_a_secret_may_pass_over_the_wire),
    this includes the human-to-system interaction, kubectl, and the system-to-system
    interaction, kubelet to API server. In summary, it’s crucial communications with
    the API server and etcd that happen exclusively over TLS. We won’t spend much
    time discussing this need as it is the default for almost every mode of installing
    or bootstrapping Kubernetes clusters. Often the only configuration you may wish
    to do is to provide a Certificate Authority (CA) to generate the certificates.
    However, keep in mind these certificates are internal to Kubernetes system components.
    With this in mind, you might not need to overwrite the default CA Kubernetes will
    generate.
  prefs: []
  type: TYPE_NORMAL
- en: Application Encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Application encryption is the encryption we perform within our system components
    or workloads running in Kubernetes. Application encryption can have many layers
    itself. For example, a workload in Kubernetes can encrypt data before persisting
    it to Kubernetes, which could then encrypt it, and then persisting it to etcd
    where it’ll be encrypted at the filesystem level. The first two encryption points
    are considered “application-level.” That’s a lot of encryption!
  prefs: []
  type: TYPE_NORMAL
- en: 'While we won’t always have encryption or decryption take place at that many
    levels, there is something to be said about data being encrypted at least once
    at the application level. Consider what we’ve talked about thus far: encryption
    over TLS and encryption at rest. If we’d stopped there, we’d have a decent start.
    When secret data is in flight, it will be encrypted, and it’ll also be encrypted
    on the physical disk. But what about on the running system? While the bits persisted
    to disk may be encrypted, if a user were to gain access to the system they would
    likely be able to read the data! Consider your encrypted desktop computer where
    you may keep sensitive credentials in a dotfile (we’ve all done it). If I steal
    your computer and try to access this data by sledding the drive, I won’t be able
    to get the information I’m after. However, if I succeed at booting your computer
    *and logging in as you*, I now have full access to this data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Application encryption is the act of encrypting that data with a key at the
    userspace level. In this computer example, I could use a (strongly) password protected
    gpg key to encrypt that dotfile, requiring my user to decrypt it before it can
    be used. Writing a simple script can automate this process and you’re off to the
    races with a far *deeper* security model. As the attacker logged in as you, even
    the decryption key is useless because without the password it’s just useless bits.
    The same consideration applies to Kubernetes. Going forward we’re going to assume
    two things are set up in your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Encryption at rest is enabled in the filesystem and/or storage systems used
    by Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TLS is enabled for all Kubernetes components and etcd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this, let’s begin our exploration of encryption at the Kubernetes application
    level.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes Secret API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Kubernetes Secret API is one of the most-used APIs in Kubernetes. While
    there are many ways for us to populate the Secret objects, the API provides a
    consistent means for workloads to interact with secret data. Secret objects heavily
    resemble ConfigMaps. They also have similar mechanics around how workloads can
    consume the objects, via environment variables or volume data. Consider the following
    Secret object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `data` field, `dbuser` and `dbkey` are base64 encoded. All Kubernetes
    secret data is. If you wish to submit nonencoded string data to the API server,
    you can use the `stringData` field as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When applied, the `stringData` will be encoded at the API server and applied
    to etcd. A common misconception is that Kubernetes is encoding this data as a
    practice of security. This is not the case. Secret data can contain all kinds
    of strange characters or binary data. In order to ensure it’s stored correctly,
    it is base64 encoded. By default, the key mechanism for ensuring that Secrets
    aren’t compromised is RBAC. Understanding the implications of RBAC verbs as they
    pertain to Secrets is crucial to prevent introducing vectors of attack:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get`'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the data of a known secret by its name.
  prefs: []
  type: TYPE_NORMAL
- en: '`list`'
  prefs: []
  type: TYPE_NORMAL
- en: Get a list of all secrets and/or *secret data*.
  prefs: []
  type: TYPE_NORMAL
- en: '`watch`'
  prefs: []
  type: TYPE_NORMAL
- en: Watch any secret change and/or change to *secret data*.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, small RBAC mistakes, such as giving the user list access,
    expose every Secret in a Namespace or, worse, the entire cluster if a ClusterRoleBinding
    is accidentally used. The truth is, in many cases, users don’t need any of these
    permissions. This is because a user’s RBAC does not determine what secrets the
    workload can have access to. Generally the kubelet is responsible for making the
    secret available to the container(s) in a Pod. In summary, as long as your Pod
    references a valid secret, the kubelet will make it available through the means
    you specify. There are a few options to how we expose the secret in the workload,
    which is covered next.
  prefs: []
  type: TYPE_NORMAL
- en: Secret Consumption Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For workloads wishing to consume a Secret, there are several choices. The preference
    on how secret data is ingested may depend on the application. However, there are
    trade-offs to the approach you choose. In the coming sections, we’ll look at three
    means of consuming secret data in workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Environment variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Secret data may be injected into environment variables. In the workload YAML,
    an arbitrary key and reference to the secret may be specified. This can be a nice
    feature for workloads moving onto Kubernetes that already expect an environment
    variable by reducing the need to change application codes. Consider the following
    Pod example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_secret_management_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The environment variable key that will be available in the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_secret_management_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the Secret object in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_secret_management_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The key in the Secret object that should be injected into the `USER` variable.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to exposing secrets in environment variables is their inability
    to be hot reloaded. A change in a Secret object will not be reflected until the
    Pod is re-created. This could occur through manual intervention or system events
    such as the need to reshedule. Additionally, it is worth calling out that some
    consider secrets in environment variables to be less secure than reading from
    volume mounts. This point can be debated, but it is fair to call out some of the
    common opportunities to leak. Namely, when processes or container runtimes are
    inspected, there may be ways to see the environment variables in plain text. Additionally,
    some frameworks, libraries, or languages may support debug or crash modes where
    they dump (spew) environment variables out to the logs. Before using environment
    variables, these risks should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Volumes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alternatively, secret objects may be injected via volumes. In the workload’s
    YAML, a volume is configured where the secret is referenced. The container that
    the secret should be injected into references that volume using a `volumeMount`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_secret_management_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Pod-level volumes available for mounting. Name specified must be referenced
    in the mount.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_secret_management_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The volume object to mount into the container filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_secret_management_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Where in the container filesystem the mount is made available.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this Pod manifest, the secret data is available under */etc/credentials*
    with each key/value pair in the secret object getting its own file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The biggest benefit to the volume approach is that secrets may be updated dynamically,
    without the Pod restarting. When a change to the secret is seen, the kubelet will
    reload the secret and it’ll show as updated in the container’s filesystem. It’s
    important to call out that the kubelet is using tmpfs, on Linux, to ensure secret
    data is stored exclusively in memory. We can see this by examining the mount table
    file on the Linux host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If the `nginx` Pod is removed from this host, this mount is discarded. With
    this model in mind, it’s especially important to consider that Secret data should
    *never* be large in size. Ideally it holds credentials or keys and is never used
    as a pseudo database.
  prefs: []
  type: TYPE_NORMAL
- en: From an application perspective, a simple watch on the directory or file, then
    re-injecting values into the application, is all it would take to handle a secret
    change. No need to understand or communicate with the Kubernetes API server. This
    is an ideal pattern we’ve seen success with in many workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Client API Consumption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last consumption model, Client API Consumption, is not a core Kubernetes
    feature. This model puts the onus on the application to communicate with the kube-apiserver
    to retrieve Secret(s) and inject them into the application. There are several
    frameworks and libraries out there that make communicating with Kubernetes trivial
    for your application. For Java, Spring’s Spring Cloud Kubernetes brings this functionality
    to Spring applications. It takes the commonly used Spring PropertySource type
    and enables it to be wired up by connecting to Kubernetes and retreiving Secrets
    and/or ConfigMaps.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered consumption of secrets at a workload level, it is time
    to talk about storing secret data.
  prefs: []
  type: TYPE_NORMAL
- en: Secret Data in etcd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like most Kubernetes objects, Secrets are stored in etcd. By default, *no* encryption
    is done at the Kubernetes layer before persisting Secrets to etcd. [Figure 7-3](#default_secret)
    shows the flow of a secret from manifest to etcd.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0703](assets/prku_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Default secret data flow in Kubernetes (the colocated etcd is sometimes
    run on a separate host).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While Kubernetes did *not* encrypt the secret data, this does not imply access
    to the data upon gaining hardware access. Remember that encryption at rest can
    be performed on disks through methods such as Linux Unified Key Setup (LUKS),
    where physical access to hardware gives you access only to encrypted data. For
    many cloud providers and enterprise datacenters, this is a default mode of operation.
    However, should we gain `ssh` access to the server running etcd and a user that
    has privileges or can escalate to see its filesystem, then we can potentially
    gain access to the secret data.
  prefs: []
  type: TYPE_NORMAL
- en: For some cases, this default model can be acceptable. etcd can be run external
    to the Kubernetes API server, ensuring it is separated by at least a hypervisor.
    In this model, an attacker would likely need to obtain root access to the etcd
    node, find the data location, and then read the secrets from the etcd database.
    The other entry point would be an adversary getting root access to the API server,
    locating the API server and etcd certs, then impersonating the API server in communicating
    with etcd to read secrets. Both cases assume potentially other breaches. For example,
    the attacker would have had to gain access to the internal network or subnet that
    is running the control-plane components. Additionally, they’d need to get the
    appropriate key to `ssh` into the node. Frankly, it’s far more likely that an
    RBAC mistake or application compromise would expose a secret before this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the threat model, let’s work through an example of how
    an attacker could gain access to Secrets. Let’s consider the case where an attacker
    SSHs and gains root access to the kube-apiserver node. The attacker could set
    up a script as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The certificate and key locations seen in this snippet are the default when
    Kubernetes was bootstrapped by kubeadm, which is also used by many tools such
    as cluster-api. etcd stores the secret data within the directory */registry/secrets/${NAMESPACE}/${SECRET_NAME}*.
    Using this script to get a secret, named `login1`, would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have successfully compromised the secret `login1`.
  prefs: []
  type: TYPE_NORMAL
- en: Even though storing Secrets without encryption can be acceptable, many platform
    operators choose not to stop here. Kubernetes supports a few ways to encrypt the
    data within etcd, furthering the depth of your defense with respect to secrets.
    These include models to support encryption at rest (where encryption occurs at
    the Kubernetes layer) before it rests in etcd. These models include static-key
    encryption and envelope encryption.
  prefs: []
  type: TYPE_NORMAL
- en: Static-Key Encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kubernetes API server supports encrypting secrets at rest. This is achieved
    by providing the Kubernetes API server with an encryption key, which it will use
    to encrypt all secret objects before persisting them to etcd. [Figure 7-4](#encryption_key)
    shows the flow of a secret when static-key encryption is in play.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0704](assets/prku_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. The relationship between an encryption key, on the API server,
    being used to encrypt secrets before storing them in etcd.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The key, held within an `EncryptionConfiguration`, is used to encrypt and decrypt
    Secret objects as they move through the API server. Should an attacker get access
    to etcd, they would see the encrypted data within, meaning the Secret data is
    not compromised. Keys can be created using a variety of providers, including secretbox,
    aescbc, and aesgcm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each provider has its own trade-offs, and we recommend working with your security
    team to select the appropriate option. Kubernetes issue #81127 is a good read
    on some considerations around these providers. If your enterprise needs to comply
    with standards such as the Federal Information Processing Standards (FIPS), these
    choices should be carefully considered. In our example we’ll use secretbox, which
    acts as a fairly performant and secure encryption provider.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up static-key encryption, we must generate a 32-byte key. Our encryption
    and decryption model is symmetric, so a single key serves both purposes. How you
    generate a key can vary enterprise to enterprise. Using a Linux host, we can easily
    use `/dev/urandom` if we’re satisfied with its entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this key data, an EncryptionConfiguration should be added to all nodes
    running a kube-apiserver. This static file should be added using configuration
    management such as ansible or KubeadmConfigSpec if using Cluster API. This ensures
    keys can be added, deleted, and rotated. The following example assumes the configuration
    is stored at */etc/kubernetes/pki/secrets/encryption-config.yaml*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The list of providers is ordered, meaning encryption will always occur using
    the first key and decryption will be attempted in order of keys listed. Identity
    is the default plain-text provider and should be last. If it’s first, secrets
    will not be encrypted.
  prefs: []
  type: TYPE_NORMAL
- en: To respect the preceding configuration, every instance of the kube-apiserver
    must be updated to load the EncryptionConfiguration locally. In */etc/kubernetes/manifests/kube-apiserver.yaml*,
    an argument can be added as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once the kube-apiserver(s) restart, this change will take effect and secrets
    will be encrypted before being sent to etcd. The kube-apiserver restart may be
    automatic. For example, when using static Pods to run the API server, a change
    to the manifest file will trigger a restart. Once you’re past stages of experimentation,
    it’s recommended you pre-provision hosts with this file and ensure the `encryption-provider`
    is enabled by default. The EncryptionConfiguration file can be added using configuration
    management tools such as Ansible or, with cluster-api, by setting that static
    file in a kubeadmConfigSpec. Note this cluster-api approach will put the EncryptionConfiguration
    in user data; make sure the user data is encrypted! Adding the `encryption-provider-config`
    flag to the API server can be done by adding the argument to the `apiServer` within
    a ClusterConfiguration, assuming you’re using kubeadm. Otherwise, ensure the flag
    is present based on your mechanism for starting the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate the encryption, you can apply a new secret object to the API server.
    Assuming the secret is named `login2`, using the script from the previous section
    we can retrieve it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see the data is fully encrypted in etcd. Note there is metadata
    specifying which provider (`secretbox`) and key (`secret-key-1`) was used to do
    the encryption. This is important to Kubernetes as it supports many providers
    and keys at once. Any object created before the encryption key was set; let’s
    assume `login1` can be queried and will still show up in plain text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This demonstrates two important concepts. One, `login1` is *not* encrypted.
    While the encryption key is in place, only newly created or altered secret objects
    will be encrypted using this key. Secondly, when passing back through the kube-apiserver,
    no provider/key mapping is present and no decryption will be attempted. This latter
    concept is important because it is highly recommended you rotate encryption keys
    over a defined span. Let’s say you rotate once every three months. When three
    months have elapsed, we’d alter the EncryptionConfiguation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It is *crucial* that `secret-key-1` is not removed. While it will not be used
    for new encryption, it is used for existing secret objects, previously encrypted
    by it, for decryption! The removal of this key will prevent the API server from
    returning secret objects, such as `login2`, to clients. Since this key is first,
    it will be used for all new encryption. When secret objects are updated, they
    will be re-encrypted using this new key over time. Until then, the original key
    can remain in the list as a fallback decryption option. If you delete the key,
    you’ll see the following responses from your client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Envelope Encryption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes 1.10 and later supports integrating with a KMS to achieve envelope
    encryption. Envelope encryption involves two keys: the key encryption key (KEK)
    and the data encryption key (DEK). KEKs are stored externally in a KMS and aren’t
    at risk unless the KMS provider is compromised. KEKs are used to encrypt DEKs,
    which are responsible for encrypting Secret objects. Each Secret object gets its
    own unique DEK to encrypt and decrypt the data. Since DEKs are encrypted by a
    KEK, they can be stored with the data itself, preventing the kube-apiserver from
    needing to be aware of many keys. Architecturally, the flow of envelope encryption
    would look like the diagram shown in [Figure 7-5](#envelope_encryption).'
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0705](assets/prku_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Flow to encrypt secrets using envelope encryption. The KMS layer
    lives outside the cluster.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There can be some variance in how this flow works, based on a KMS provider,
    but generally this demonstrates how envelope encryption functions. There are multiple
    benefits to this model:'
  prefs: []
  type: TYPE_NORMAL
- en: KMS is external to Kubernetes, increasing security via isolation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralization of KEKs enables easy rotation of keys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separation of DEK and KEK means that secret data is never sent to or known by
    the KMS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KMS is concerned only with decrypting DEKs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encryption of DEKs means they are easy to store alongside their secret, making
    management of keys in relation to their secrets easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The provider plug-ins work by running a privileged container implementing a
    gRPC server that can communicate with a remote KMS. This container runs exclusively
    on master nodes where a kube-apiserver is present. Then, similar to setting up
    encryption in the previous section, an EncryptionConfiguration must be added to
    master nodes with settings to communicate with the KMS plug-in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming the EncryptionConfiguration is saved on each master node at */etc/kubernetes/pki/secrets/encryption-config.yaml*,
    the kube-apiserver arguments must be updated to include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Changing the value should restart the kube-apiserver. If it doesn’t, a restart
    is required for the change to take effect.
  prefs: []
  type: TYPE_NORMAL
- en: From a design perspective, this is a viable model. However, KMS plug-in implementations
    are scarce and the ones that do exist are immature. When we wrote this book, the
    following data points are true. There are no tagged releases for the aws-encryption-provider
    (AWS) or the k8s-cloudkms-plugin (Google). Azure’s plug-in kubernetes-kms has
    notable limitations, such as no support for key rotation. So with the exception
    of running in a managed service, such as GKE where the KMS plug-in is automatically
    available and supported by Google, usage may prove unstable. Lastly, the only
    cloud provider-agnostic KMS plug-in available was kubernetes-vault-kms-plugin,
    which was only partially implemented and has been archived (abandoned).
  prefs: []
  type: TYPE_NORMAL
- en: External Providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is not what we’d consider an enterprise-grade secret store. While
    it does offer a Secret API that will be used for things like Service Accounts,
    for enterprise secret data it may fall short. There is nothing inherently wrong
    with using it to store application secrets, as long as the risks and options are
    understood, which is largely what this chapter has described thus far! However,
    many of our clients demand more than what the Secret API can offer, especially
    those working in sectors such as financial services. These users need capabilities
    such as integration with a hardware security module (HSM) and have advanced key
    rotation policies.
  prefs: []
  type: TYPE_NORMAL
- en: Our guidance is generally to start with what Kubernetes offers and see if the
    approaches to harden its security (i.e., encryption) are adequate. As described
    in the previous section, KMS encryption models that offer envelope encryption
    provide a pretty strong story around the safety of secret data in etcd. If we
    need to extend beyond this (and we often do), we then look to what secret management
    tooling preexists that the engineering team(s) have operational knowledge of.
    Running secret management systems in a production-ready capacity can be a challenging
    task, similar to running any stateful service where the data contained within
    needs to be not only highly available but protected from potential attackers.
  prefs: []
  type: TYPE_NORMAL
- en: Vault
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vault is an open source project by HashiCorp. It is by far the most popular
    project we run into with our clients when it comes to secret management solutions.
    Vault has found several ways to integrate in the cloud native space. Work has
    been done around providing first-class integration in frameworks such as Spring
    and in Kubernetes itself. One emerging pattern is to run Vault within Kubernetes
    and enable Vault to use the TokenReview API to authenticate requests against the
    Kubernetes API Server. Next, we’ll explore two common Kubernetes integration points,
    including sidecar and initContainer injection along with a newer approach, CSI
    integration.
  prefs: []
  type: TYPE_NORMAL
- en: Cyberark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cyberark is another popular option we see with clients. As a company, it’s been
    around for a while, and often we find preexisting investments to exist and a desire
    to integrate Kubernetes with it. Cyberark offers a Credential Provider and Dynamic
    Access Provider (DAP). DAP provides multiple enterprise mechanisms Kubernetes
    administrators may want to integrate with. Similar to Vault, it supports the ability
    to use initContainers alongside your application to communicate with DAP.
  prefs: []
  type: TYPE_NORMAL
- en: Injection Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once an external secret store is available to workloads in Kubernetes, there
    are several options for retrieval. This section covers these approaches, our recommendations,
    and trade-offs. We’ll cover each design approach to consuming secrets and describe
    Vault’s implementation.
  prefs: []
  type: TYPE_NORMAL
- en: This approach runs an initContainer and/or sidecar container to communicate
    with an external secret store. Typically, secrets are injected into the Pod’s
    filesystem, making them available to all containers running in a Pod. We highly
    recommend this approach when possible. The major benefit is that it decouples
    the secret store entirely from the application. However, this does make the platform
    more complex, as facilitating secret injection is now an offering of the Kubernetes-based
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Vault’s implementation of this model uses a MutatingWebhook pointed at a vault-agent-injector.
    As Pods are created, based on annotations, the vault-agent-injector adds an initContainer
    (used for retrieval of the initial secret) and a sidecar container to keep secrets
    updated, if needed. [Figure 7-6](#sidecar_injection_architecture_along_with_my_app)
    demonstrates this flow of interaction between the Pod and Vault.
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0706](assets/prku_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Sidecar injection architecture. Along with my-app-container, all
    Vault Pods are run as sidecars.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The configuration of the MutatingWebhook that will inject these vault-specific
    containers is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The MutatingWebhook is invoked on every Pod CREATE or UPDATE event. While evaluation
    will occur on every Pod, not every Pod will be mutated, or injected with a vault-agent.
    The vault-agent-injector is looking for two annotations in every Pod spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vault.hashicorp.com/agent-inject: "true"`'
  prefs: []
  type: TYPE_NORMAL
- en: Instructs the injector to include a vault-agent initContainer, which retrieves
    secrets and writes them to the Pod’s filesystem, prior to other containers starting.
  prefs: []
  type: TYPE_NORMAL
- en: '`vault.hashicorp.com/agent-inject-status: "update"`'
  prefs: []
  type: TYPE_NORMAL
- en: Instructs the injector to include a vault-agent sidecar, which runs alongside
    the workload. It will update the secret, should it change in Vault. The initContainer
    still runs in this mode. This parameter is optional and when it is not included,
    the sidecar is not added.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the vault-agent-injector does a mutation based on `vault.hashicorp.com/agent-inject:
    "true"`, the following is added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When the vault-agent-injector sees the annotation `vault.hashicorp.com/agent-inject-status:
    "update"`, the following is added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With the agents present, they will retrieve and download secrets based on the
    Pod annotations, such as the following annotation that requests a database secret
    from Vault:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the secret value will be persisted as if a Go map was printed out.
    Syntactically, it appears as follows. All secrets are put into */vault/secrets*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure that formatting of a secret is optimal for consumption, Vault supports
    adding templates into the annotation of Pods. This uses standard Go templating.
    For example, to create a JDBC connection string, the following template can be
    applied to a secret named `creds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A primary area of complexity in this model is authentication and authorization
    of the requesting Pod. Vault provides several [authentication methods](https://www.vaultproject.io/docs/auth).
    When running Vault within Kubernetes and especially in this sidecar injection
    model, you may wish to set up Vault to authenticate against Kubernetes so that
    Pods can provide their existing Service Account tokens as identity. Setting up
    this authentication mechanism appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_secret_management_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This environment variable should be present in the Vault Pod by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_secret_management_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Location of this Pod’s Service Account token, used to auth against the Kubernetes
    API server when performing TokenReview requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'When requests for secrets enter Vault, the requester’s Service Account can
    then be validated by Vault. Vault does this by communicating through the Kubernetes
    TokenReview API to validate the identity of the requester. Assuming the identity
    is validated, Vault must then determine whether the Service Account is authorized
    to access the secret. These authorization policies and bindings between Service
    Accounts and policies must be configured and maintained within Vault. In Vault,
    a policy is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This has created a policy in Vault referred to as `team-a`, which provides
    read access to all the secrets within *secret/data/team-a/*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to associate the requester’s Service Account with the policy
    so Vault can authorize access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_secret_management_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the requester’s Service Account.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_secret_management_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Namespace of the requester.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_secret_management_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Binding to associate this account to one or many policies.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_secret_management_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Duration the vault-specific authorization token should live for. Once expired,
    authn/authz is performed again.
  prefs: []
  type: TYPE_NORMAL
- en: The vault-specific process we have explored so far likely applies to any variety
    of external secret management stores. You’ll be faced with some amount of overhead
    regarding integration of identity and authorization around secret access when
    dealing with systems beyond Kubernetes core.
  prefs: []
  type: TYPE_NORMAL
- en: CSI Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A newer approach to secret store integration is to leverage the secrets-store-csi-driver.
    At the time of this writing, this is a Kubernetes subproject within kubernetes-sigs.
    This approach enables integration with secret management systems at a lower level.
    Namely, it enables Pods to gain access to externally hosted secrets without running
    a sidecar or initContainer to inject secret data into the Pod. The result is secret
    interaction feeling more like a platform service and less like something applications
    need to integrate with. The secrets-store-csi-driver runs a driver Pod (as a DaemonSet)
    on every host, simliar to how you’d expect a CSI driver to work with a storage
    provider.
  prefs: []
  type: TYPE_NORMAL
- en: The driver then relies on a provider that is responsible for secret lookup in
    the external system. In the case of Vault, this would involve installing the `vault-provider`
    binary on every host. The binary location is expected to be where the driver’s
    `provider-dir` mount is set. This binary may preexist on the host or, most commonly,
    it is installed via a DaemonSet-like process. The overall architecture would appear
    close to what’s shown in [Figure 7-7](#CSI_driver).
  prefs: []
  type: TYPE_NORMAL
- en: '![prku 0707](assets/prku_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. CSI driver interaction flow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is a fairly new approach that seems promising based on its UX and ability
    to abstract secret providers. However, it does pose additional challenges. For
    example, how is identity handled when the Pod itself is not requesting the secret?
    This is something the driver and/or provider must figure out since they’re making
    requests on behalf of the Pod. For now, we can look at the primary API, which
    includes the SecretProviderClass. To interact with an external system such as
    Vault, the SecretProviderClass would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_secret_management_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the location of Vault and would have the Service name (`vault`) followed
    by the Namespace `secret-store`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_secret_management_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the path in Vault the Key/Value object was written to.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_secret_management_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the actual object to lookup in `team-a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the SecretProviderClass in place, a Pod can consume and reference this
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'When this Pod starts, the driver and provider attempt to retrieve the secret
    data. The secret data will appear in a volume mount as any Kubernetes secret would,
    assuming authentication and authorization to the external provider is successful.
    From the driver Pod on the node, you can examine the logs to see the command sent
    to the provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In summary, secret-store-csi-drive is an approach worth keeping an eye on. Over
    time, if the project stabilizes and providers begin to mature, we could see the
    approach becoming common for those building application platforms on top of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets in the Declarative World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common aspiration of application deployments, continuous integration, and
    continuous delivery is to move purely into a declarative model. This is the same
    model used in Kubernetes where you declare a desired state and over time controllers
    work to reconcile the desired state with current state. For application developers
    and DevOps teams, these aspirations commonly surface in a pattern called GitOps.
    A central tenet of most GitOps approaches is to use one or many git repositories
    as the source of truth for workloads. When a commit is seen on some branch or
    tag, it can be picked up by build and deploy processes, often inside a cluster.
    This eventually aims to make the available workload capable of receiving traffic.
    Models such as GitOps are covered at greater length in [Chapter 15](ch15.html#chapter15).
  prefs: []
  type: TYPE_NORMAL
- en: When taking a purist-declarative approach, secret data creates a unique challenge.
    Sure you can commit your configurations alongside your code, but what about the
    credentials and keys used by your application? We have a feeling an API key showing
    up in a commit might make some people unhappy. There are some ways around this.
    One is, of course, to keep secret data outside of this declarative model and repent
    your sins toward the GitOps gods. Another is to consider “sealing” your secret
    data, in a way that accessing the data exposes nothing about the meaningful value,
    which is what we’ll explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Sealing Secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can we truly seal a secret? The concept is nothing new. Using asymmetric
    cryptography, we can ensure a way to encrypt secrets, commit them to places, and
    not worry about anyone exposing the data. In this model, we have an encryption
    key (typically public) and a decryption key (typically private). The idea is that
    any secret created by the encryption key cannot have its value compromised without
    the private key being compromised. Of course, we need to ensure many things to
    stay safe in this model, such as choose a cipher we can trust, ensure the private
    key is *always* safe, and establish both encryption key and secret data rotation
    policies. A model we’ll explore in the coming sections is how this looks when
    a private key is generated in the cluster, and developers can be distributed their
    own encryption key that they can use on their secret data.
  prefs: []
  type: TYPE_NORMAL
- en: Sealed Secrets Controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bitnami-labs/sealed-secrets is a commonly used, open source project for achieving
    what has been described. However, should you choose alternative tooling or build
    something yourself, the key concepts are unlikely to change drastically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key component to this project is a sealed-secret-controller that runs inside
    the cluster. By default, it generates the keys needed to perform encryption and
    decryption. On the client side, developers use a command-line utility called kubeseal.
    Being that we’re using asymmetric encryption, kubeseal needs to know only about
    the public key (for encryption). Once developers encrypt their data using it,
    they won’t even be able to decrypt the values directly. To get started, we first
    deploy the controller to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the controller will create encryption and decryption keys for us.
    However, it is possible to bring your own certificates. The public (cert) and
    private (key) are stored in a Kubernetes Secret under *kube-system/sealed-secret-key*.
    The next step is allowing developers to retrieve the encryption key so they can
    get to work. This should *not* be done by accessing the Kubernetes Secret directly.
    Instead, the controller exposes an endpoint that can be used to retrieve the encryption
    key. How you access this service is up to you, but clients need to be able to
    call it using the following command, which has its flow detailed in [Figure 7-8](#sealed_secret):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![prku 0708](assets/prku_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. Sealed-secret-controller architecture.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the public key is loaded in kubeseal, you can generate SealedSecret CRDs
    that contain (encrypted) secret data. These CRDs are stored in etcd. The sealed-secret-controller
    makes the secrets available using standard Kubernetes Secrets. To ensure SealedSecret
    data is converted to a Secret correctly, you can specify templates in the SealedSecret
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start with a Kubernetes Secret, like any other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To “seal” the secret, you can run kubeseal against it and generate an encrypted
    output in JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding SealedSecret object can be placed anywhere. As long as the sealing
    key, held by the sealed-secret-controller is not compromised, the data will be
    safe. Rotation is especially important in this model, which is covered in a subsequent
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Once applied, the flow and storage look as described in [Figure 7-9](#sealed_secret_controller).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Secret object made by the sealed-secret-controller is owned by its corresponding
    SealedSecret CRD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![prku 0709](assets/prku_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. Sealed-secret-controller interaction around managing sealed and
    unsealed secrets.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This means that if the SealedSecret is deleted, its corresponding Secret object
    will be garbage collected.
  prefs: []
  type: TYPE_NORMAL
- en: Key Renewal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the sealed-secret private key is leaked (perhaps due to RBAC misconfiguration),
    every secret should be considered compromised. It’s especially important that
    the sealing key is renewed on an interval and that you understand the scope of
    “renewal.” The default behavior is for this key to be renewed every 30 days. It
    does not replace the existing keys; instead, it is appended to the existing list
    of keys capable of unsealing the data. However, the new key is used for all new
    encryption activity. Most importantly, existing sealed secrets are not re-encrypted.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the event of a leaked key, you should:'
  prefs: []
  type: TYPE_NORMAL
- en: Immediately rotate your encryption key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotate all existing secrets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that just re-encrypting isn’t good enough. For example, someone could
    easily go into git history, find the old encrypted asset, and use the compromised
    key on it. Generally speaking, you should have rotation and renewal strategies
    for passwords and keys, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SealedSecrets uses a trick where the Namespace is used during encryption. This
    provides an isolation mechanic where a SealedSecret truly belongs to the Namespace
    it was created in and cannot just be moved between them. Generally, this default
    behavior is the most secure and should just be left as is. However, it does support
    configurable access policies, which are covered in the sealed-secrets documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another key consideration for sealed-secret models is deployment topologies
    involving many clusters. Many of these topologies treat clusters ephemerally.
    In cases such as these, it may be harder to run sealed-secret-style controllers
    because—unless you are sharing private keys among them all—you now need to worry
    about having unique keys for each cluster. Additionally, the point of interaction
    a developer has to get the encryption key (as described in previous sections)
    goes from one cluster to many. While by no means an impossible problem to solve,
    it is worth considering.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Secrets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application consumption of secrets is highly dependent on the language and frameworks
    at play. While variance is high, there are general best practices we recommend
    and encourage application developers to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Always Audit Secret Interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Kubernetes cluster should be configured with auditing enabled. Auditing allows
    you to specify the events that occur around specific resources. This will tell
    you when and by whom a resource was interacted with. For mutations, it will also
    detail what changed. Auditing secret events is critical in reacting to access
    issues. For details about auditing, see the cluster audit documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Leak Secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While leaking secrets is never desirable, in multitenant Kubernetes environments
    it’s important to consider how secrets can be leaked. A common occurrence is to
    accidentally log a secret. For example, we have seen this a few times when platform
    engineers build operators (covered in [Chapter 11](ch11.html#building_platform_services)).
    These operators often deal with secrets for the systems they are managing and
    potentially external systems they need to connect to. During the development phase,
    it can be common to log this secret data for the sake of debugging. Logs go to
    stdout/stderr and are, in many Kubernetes-based platforms, forwarded to a log
    analysis platform. This means the secret may pass in plain text through many environments
    and systems.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is primarily a declarative system. Developers write manifests that
    can easily contain secret data, especially when testing. Developers should work
    with caution to ensure that secrets used while testing don’t get committed into
    source control repositories.
  prefs: []
  type: TYPE_NORMAL
- en: Prefer Volumes Over Environment Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common ways to access secrets provided by Kubernetes is to propagate
    the value into an environment variable or volumes. For most applications, volumes
    should be preferred. Environment variables can have a higher chance of being leaked
    through various means—for example, an echo command performed while testing or
    a framework automatically dumping environment variables on startup or during a
    crash. This doesn’t mean these concerns are inherently solved for with volumes!
  prefs: []
  type: TYPE_NORMAL
- en: Security aside, the key benefit to app developers is that when secrets change,
    volumes are automatically updated; this will enable hot-reloading of secrets such
    as tokens. For a secret change to take place with environment variables, Pods
    must be restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Make Secret Store Providers Unknown to Your Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several approaches an application can take to retrieve and consume
    its required secrets. These can range from calling a secret store within business
    logic to expecting an environment variable to be set on startup. Following the
    philosophy of separation of concerns, we recommend implementing secret consumption
    in a way that whether Kubernetes, Vault, or other providers are managing the secret
    does not matter to the application. Achieving this makes your application portable
    and platform agnostic, and it reduces the complexity of your app’s interaction.
    Complexity is reduced because for an application to retrieve secrets from a provider
    it needs to both understand how to talk to the provider and be able to authenticate
    for communication with the provider.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this provider-agnostic implementation, applications should prefer
    loading secrets from environment variables or volumes. As we said earlier, volumes
    are the most ideal. In this model, an application will assume the presence of
    secrets in one or many volumes. Since volumes can be updated dynamically (without
    Pod restart) the application can watch the filesystem if a hot-reload of secrets
    is desired. By consuming from the container’s local filesystem, it does not matter
    whether the backing store is Kubernetes or otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Some application frameworks, such as Spring, include libraries to communicate
    directly to the API server and auto-inject secrets and configuration. While these
    utilities are convenient, consider the points just discussed to determine what
    approaches hold the most value to your application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ve explored the Kubernetes Secret API, ways to interact with
    secrets, means of storing secrets, how to seal secrets, and some best practices.
    With this knowledge, it’s important we consider the amount of depth we’re interested
    in protecting, and with that, determining how to prioritize solving for each layer.
  prefs: []
  type: TYPE_NORMAL
