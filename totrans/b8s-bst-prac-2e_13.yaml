- en: Chapter 13\. Integrating External Services with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many of the chapters in this book, we’ve discussed how to build, deploy,
    and manage services in Kubernetes. However, the truth is that systems don’t exist
    in a vacuum, and most of the services that we build will need to interact with
    systems and services that exist outside of the Kubernetes cluster in which they’re
    running. This might be necessary because we are building new services being accessed
    by legacy infrastructure running in virtual or physical machines. Additionally,
    it might be because the services we are building need to access preexisting databases
    or other services that are running on physical infrastructure in an on-premises
    datacenter. Finally, you might have multiple Kubernetes clusters with services
    you need to interconnect. For all these reasons, the ability to expose, share,
    and build services that span the boundary of your Kubernetes cluster is an important
    part of building real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Importing Services into Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common pattern for connecting Kubernetes with external services consists
    of a Kubernetes Service that is consuming a service that exists outside of the
    Kubernetes cluster. Often, this is because Kubernetes is being used for new application
    development or is serving as an interface for a legacy resource like an on-premises
    database. In many existing applications, parts of the application are easier to
    move than others. For example, a database with mission-critical data may be required
    to stay on premises for reasons of data governance, compliance, or business continuity.
    At the same time, there are significant benefits to building new interfaces to
    these legacy databases in Kubernetes. If every migration to Kubernetes required
    a lift and shift of the entire application, then many applications would be required
    to stay with their legacy implementations forever. Instead, this chapter shows
    how you can integrate cloud native development of new applications with existing
    services such as databases that may be running on traditional virtual machines,
    bare metal servers, or even mainframes.
  prefs: []
  type: TYPE_NORMAL
- en: When we consider the task of making an external service accessible from Kubernetes,
    the first challenge is simply to get the networking to work correctly. The details
    of making networking operational are specific to both the location of the database
    and the location of the Kubernetes cluster. As a result, they are beyond the scope
    of this book, but generally, cloud-based Kubernetes providers enable the deployment
    of a cluster into a user-provided virtual network (VNET), and those virtual networks
    can then be peered up with an on-premises network.
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve established network connectivity between pods in the Kubernetes
    cluster and the on-premises resource, the next challenge is to make the external
    service look and feel like a Kubernetes Service. In Kubernetes, service discovery
    occurs via Domain Name System (DNS) lookups, so, to make our external database
    feel like it is a native part of Kubernetes, we need to make the database discoverable
    in the same DNS. We’ll get into the details of how to do this next.
  prefs: []
  type: TYPE_NORMAL
- en: Selector-Less Services for Stable IP Addresses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first way to achieve this is with a *selector-less* Kubernetes Service.
    When you create a Kubernetes Service without a selector, there are no pods whose
    labels match the nonexistent service selector; thus, no load balancing is performed.
    Instead, you can program this selector-less service to have endpoints that are
    the specific IP address(es) of the external resource you want to add to the Kubernetes
    cluster. That way, when a Kubernetes pod performs a lookup for `your-database`,
    the built-in Kubernetes DNS server will translate that to a service IP address
    of your external service. Here is an example of a selector-less service for an
    external database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When the service exists, you need to update its endpoints to contain the database
    IP address serving at `24.1.2.3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 13-1](#figure1401) depicts how this integrates a service within Kubernetes.
    As you can see, the pod looks up the service in the cluster DNS server as it would
    for any other Kubernetes Service. But instead of being given the IP address of
    another pod in the Kubernetes cluster, it is instead given an IP address that
    corresponds to the resource outside of the Kubernetes cluster. In this way, the
    developer may not even know the service is implemented outside of the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![images/figure-14-1.png](assets/kbp2_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. Service integration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: CNAME-Based Services for Stable DNS Names
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous example assumed that the external resource you were trying to integrate
    with your Kubernetes cluster had a stable IP address. Although this is often true
    of physical on-premises resources, depending on the network topology, it might
    not always be true. It is also significantly less likely to be true in a cloud
    environment where virtual machine (VM) IP addresses are more dynamic. Alternatively,
    the service might have multiple replicas sitting behind a single DNS-based load
    balancer. In these situations, the external service you are trying to bridge into
    your cluster doesn’t have a stable IP address, but it does have a stable DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: 'For these instances, you can define a CNAME-based Kubernetes Service. If you’re
    not familiar with DNS records, a CNAME, or *Canonical Name*, record indicates
    that a particular DNS address should be translated to a different *Canonical*
    DNS name. For example, a CNAME record for *foo.com* that contains *bar.com* indicates
    that anyone looking up *foo.com* should perform a recursive lookup for *bar.com*
    to obtain the correct IP address. You can use Kubernetes Services to define CNAME
    records in the Kubernetes DNS server. For example, if you have an external database
    with a DNS name of *database.myco.com*, you might create a CNAME *Service* that
    is named `myco-database`. Such a Service looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With a Service defined in this way, any pod that does a lookup for `myco-database`
    will be recursively resolved to *database.myco.com*. Of course, to make this work,
    the DNS name of your external resource *also* needs to be resolvable from the
    Kubernetes DNS servers. If the DNS name is globally accessible (e.g., from a well-known
    DNS service provider), this will automatically work. However, if the DNS of the
    external service is located in a company-local DNS server (e.g., a DNS server
    that services only internal traffic), the Kubernetes cluster might not know by
    default how to resolve queries to this corporate DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: To set up the cluster’s DNS server to communicate with an alternate DNS resolver,
    you need to adjust its configuration. You do this by updating a Kubernetes ConfigMap
    with a configuration file for the DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: CNAME records are a useful way to map external services with stable DNS names
    to names that are discoverable within your cluster. At first it might seem counterintuitive
    to remap a well-known DNS address to a cluster-local DNS address, but the consistency
    of having all services look and feel the same is usually worth the small amount
    of added complexity. Additionally, because the CNAME Service, like all Kubernetes
    Services, is defined per namespace, you can use namespaces to map the same service
    name (e.g., `database`) to different external services (e.g., `canary` or `production`),
    depending on the Kubernetes namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Active Controller-Based Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a limited set of circumstances, neither of the previous methods for exposing
    external services within Kubernetes is feasible. Generally, this is because there
    is neither a stable DNS address nor a single stable IP address for the service
    that you want to expose within the Kubernetes cluster. In such circumstances,
    exposing the external service within the Kubernetes cluster is significantly more
    complicated, but it isn’t impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, you need some understanding of how Kubernetes Services work
    under the hood. Kubernetes Services are made up of two different resources: the
    Service resource, with which you are doubtless familiar, and the Endpoints resource
    that represents the IP addresses that make up the service. In normal operation,
    the Kubernetes controller manager populates the endpoints of a service based on
    the selector in the service. However, if you create a selector-less service, as
    in the first stable-IP approach, the Endpoints resource for the service will not
    be populated because no pods are selected. In this situation, you need to supply
    the control loop to create and populate the correct Endpoints resource. You need
    to dynamically query your infrastructure to obtain the IP addresses for the service
    external to Kubernetes that you want to integrate and then populate your service’s
    endpoints with these IP addresses. After you do this, the mechanisms of Kubernetes
    take over and program both the DNS server and the `kube-proxy` correctly to load-balance
    traffic to your external service. [Figure 13-2](#fig-external-service) presents
    a complete picture of how this works in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: '![images/figure-14-2.png](assets/kbp2_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. An external service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exporting Services from Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we explored how to import preexisting services to Kubernetes,
    but you might also need to export services from Kubernetes to the preexisting
    environments. This might occur because you have a legacy internal application
    for customer management that needs access to a new API you are developing in a
    cloud native infrastructure. Alternatively, you might be building new microservice-based
    APIs but you need to interface with a preexisting traditional web application
    firewall (WAF) because of internal policy or regulatory requirements. Regardless
    of the reason, being able to expose services from a Kubernetes cluster to other
    internal applications is a critical design requirement for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: This can be challenging because in many Kubernetes installations, the pod IP
    addresses are not routable addresses from outside the cluster. Via tools like
    flannel, or other networking providers, routing is established within a Kubernetes
    cluster to facilitate communication between pods and also between nodes and pods,
    but the same routing is not generally extended to arbitrary machines in the same
    network. In many cases the IP ranges given to pods are distinct from the IP space
    of a corporate network and routing is not possible. Furthermore, in the case of
    cloud to on-premises connectivity, the IP addresses of the pods are not always
    advertised back across a VPN or network peering relationship into the on-premises
    network. Consequently, setting up routing between a traditional application and
    Kubernetes pods is the key task to enable the export of Kubernetes-based services.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting Services by Using Internal Load Balancers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The easiest way to export from Kubernetes is by using the built-in `Service`
    object. If you have any previous experience with Kubernetes, no doubt you have
    seen how you can connect a cloud-based load balancer to bring external traffic
    to a collection of pods in the cluster. However, you might not have realized that
    most clouds also offer an *internal* load balancer. The internal load balancer
    provides the same capabilities to map a virtual IP address to a collection of
    pods, but that virtual IP address is drawn from an internal IP address space (e.g.,
    `10.0.0.0/24`), so it is routable only from within that virtual network. You activate
    an internal load balancer by adding a cloud-specific annotation to your Service
    load balancer. For example, in Microsoft Azure, you add the `service.beta.kubernetes.io/azure-load-balancer-internal:
    "true"` annotation. On Amazon Web Services (AWS), the annotation is `service.beta.kubernetes.io/aws-load-balancer-internal:
    0.0.0.0/0`. You place annotations in the `metadata` field in the Service resource
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When you export a Service via an internal load balancer, you receive a stable,
    routable IP address that is visible on the virtual network outside of the cluster.
    Then, you can either use that IP address directly or set up internal DNS resolution
    to provide discovery for your exported service.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting Services on NodePorts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, in on-premises installations, cloud-based internal load balancers
    are unavailable. In this context using a NodePort-based service is often a good
    solution. A Service of type NodePort exports a listener on every node in the cluster
    that forwards traffic from the node’s IP address and selected port into the Service
    that you defined, as shown in [Figure 13-3](#figure-node-port).
  prefs: []
  type: TYPE_NORMAL
- en: '![images/figure-14-3.png](assets/kbp2_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. A NodePort-based service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s an example YAML file for a NodePort service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Following the creation of a Service of type NodePort, Kubernetes automatically
    selects a port for the Service; you can get that port from the Service by looking
    at the `spec.ports[*].nodePort` field. If you want to choose the port yourself,
    you can specify it when you create the Service, but the NodePort must be within
    the configured range for the cluster. The default for this range are ports between
    `30000` and `30999`.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes’ work is done when the Service is exposed on this port. To export
    it to an existing application outside of the cluster, you (or your network administrator)
    will need to make it discoverable. Depending on the way your application is configured,
    you might be able to give your application a list of `${node}:${port}` pairs,
    and the application will perform client-side load balancing. Alternatively, you
    might need to configure a physical or virtual load balancer within your network
    to direct traffic from a virtual IP address to this list of `${node}:${port}`
    backends. The specific details for this configuration will differ depending on
    your environment.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating External Machines and Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If neither of the previous solutions work well for you, perhaps because you
    want tighter integration for dynamic service discovery, the final choice for exposing
    Kubernetes Services to outside applications is to directly integrate the machine(s)
    running the application into the Kubernetes cluster’s service discovery and networking
    mechanisms. This is significantly more invasive and complicated than either of
    the previous approaches, and you should use it only when necessary for your application
    (which should be infrequently). In some managed Kubernetes environments, it might
    not even be possible.
  prefs: []
  type: TYPE_NORMAL
- en: When integrating an external machine into the cluster for networking, you need
    to ensure that the pod network routing and DNS-based service discovery both work
    correctly. The easiest way to do this is to run the kubelet on the machine that
    you want to join to the cluster, but disable scheduling in the cluster. Joining
    a kubelet node to a cluster is beyond the scope of this book, but there are numerous
    other books or online resources that describe how to achieve this. When the node
    is joined, you need to immediately mark it as unschedulable using the `kubectl
    cordon ...` command to prevent any additional work being scheduled on it. This
    cordoning will not prevent DaemonSets from landing pods onto the node, and thus
    the pods for both the KubeProxy and network routing will land on the machine and
    make Kubernetes-based services discoverable from any application running on that
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: The approach we just described is quite invasive to the node because it requires
    installing Docker or some other container runtime. As a result, it might not be
    feasible in many environments. A lighter weight but more complex approach is to
    just run the `kube-proxy` as a process on the machine and adjust the machine’s
    DNS server. Assuming that you can set up pod routing to work correctly, running
    the `kube-proxy` will set up machine-level networking so that Kubernetes Service
    virtual IP addresses will be remapped to the pods that make up that Service. If
    you also change the machine’s DNS to point to the Kubernetes cluster DNS server,
    you will have effectively enabled Kubernetes discovery on a machine that is not
    part of the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these approaches are complicated and advanced, and you should not take
    them lightly. If you find yourself considering this level of service discovery
    integration, ask yourself whether it may be easier to actually bring the service
    you are connecting to the cluster into the cluster itself. We cover this in [Chapter 16](ch16.html#managing_state_and_stateful_application).
  prefs: []
  type: TYPE_NORMAL
- en: Sharing Services Between Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous sections have described how to connect Kubernetes applications
    to outside services and how to connect outside services to Kubernetes applications,
    but another significant use case is connecting services *between* Kubernetes clusters.
    This may be to achieve East-West failover between different regional Kubernetes
    clusters, or it might be to link together services run by different teams. The
    process of achieving this interaction is actually a combination of the designs
    described in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to expose the Service within the first Kubernetes cluster to
    enable network traffic to flow. Let’s assume that you’re in a cloud environment
    that supports internal load balancers, and that you receive a virtual IP address
    for that internal load balancer of 10.1.10.1\. Next, you need to integrate this
    virtual IP address into the second Kubernetes cluster to enable service discovery.
    You achieve this in the same manner as importing an external application into
    Kubernetes (we covered this in [“Importing Services into Kubernetes”](#importing)).
    You create a selector-less service and set its IP address to be 10.1.10.1\. With
    these two steps you have integrated service discovery and connectivity between
    services within your two Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: These steps are fairly manual, and although this might be acceptable for a small,
    static set of services, if you want to enable tighter or automatic service integration
    between clusters, it makes sense to write a cluster daemon that runs in both clusters
    to perform the integration. This daemon would watch the first cluster for Services
    with a particular annotation, say something like `myco.com/exported-service`;
    all Services with this annotation would then be imported into the second cluster
    via selector-less services. Likewise, the same daemon would garbage-collect and
    delete any services that are exported into the second cluster but are no longer
    present in the first. If you set up such daemons in each of your regional clusters,
    you can enable dynamic, East-West connectivity between all clusters in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: There has also been recent work within the Kubernetes project to define a Multi-Cluster
    Service API. This work is experimental and can be found within the [Multi-Cluster
    Service](https://oreil.ly/ZXZi4) project on GitHub. At the time of writing, the
    experimental nature of this project means that it is probably not suitable for
    production use-cases, but it shows the future direction of multi-cluster service
    management in the Kubernetes ecosystem. As it moves from alpha to beta and eventually
    to general availability, this implementation of Service sharing will make it much
    easier to build cross-cluster microservice applications. Even today, tools such
    as the Fleet cluster manager in Microsoft Azure are starting to implement these
    Multi-Cluster Service APIs in response to user needs.
  prefs: []
  type: TYPE_NORMAL
- en: Third-Party Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, this chapter has described the various ways to import, export, and connect
    services that span Kubernetes clusters and some outside resource. If you have
    previous experience with service mesh technologies, these concepts might seem
    quite familiar. Indeed, there are a variety of third-party tools and projects
    that you can use to interconnect services both with Kubernetes and with arbitrary
    applications and machines. Generally, these tools provide a lot of functionality,
    but they are also significantly more complex operationally than the approaches
    described earlier. However, if you find yourself building more and more networking
    interconnectivity, you should explore the space of service meshes, which is rapidly
    iterating and evolving. Nearly all these third-party tools have an open source
    component, but they also offer commercial support that can reduce the operational
    overhead of running additional infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting Cluster and External Services Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Establish network connectivity between the cluster and on-premises. Networking
    can be varied between different sites, clouds, and cluster configurations, but
    first ensure that pods can talk to on-premises machines and vice versa.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To access services outside of the cluster, you can use selector-less services
    and directly program in the IP address of the machine (e.g., the database) with
    which you want to communicate. If you don’t have fixed IP addresses, you can instead
    use CNAME services to redirect to a DNS name. If you have neither a DNS name nor
    fixed services, you might need to write a dynamic operator that periodically synchronizes
    the external service IP addresses with the Kubernetes Service endpoints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To export services from Kubernetes, use internal load balancers or NodePort
    services. Internal load balancers are typically easier to use in public cloud
    environments where they can be bound to the Kubernetes Service itself. When such
    load balancers are unavailable, NodePort services can expose the service on all
    the machines in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can achieve connections between Kubernetes clusters through a combination
    of these two approaches, exposing a service externally that is then consumed as
    a selector-less service in the other Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the real world, not every application is cloud native. Building production-ready
    applications often involves connecting preexisting systems with newer applications.
    This chapter described how you can integrate Kubernetes with legacy applications
    and also how to integrate different services running across multiple distinct
    Kubernetes clusters. Unless you have the luxury of building something brand new,
    cloud native development will always require legacy integration. The techniques
    described in this chapter will help you achieve that.
  prefs: []
  type: TYPE_NORMAL
