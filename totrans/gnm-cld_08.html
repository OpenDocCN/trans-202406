<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. First Steps with GATK"><div class="chapter" id="first_steps_with_gatk">
<h1><span class="label">Chapter 5. </span>First Steps with GATK</h1>

<p>In <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>, we set you up to do work in the cloud. This chapter is all about getting you oriented and comfortable with the GATK and related tools. We begin by covering the basics, including computational requirements, command-line syntax, and common options. We show you how to spin up the GATK Docker container on the GCP VM you set up in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a> so that you can run real commands at scale with minimal effort. Then we work through a simple example of variant calling with the most widely used GATK tool, <code>HaplotypeCaller</code>. We explore some basic filtering mechanisms to give you a feel for working with variant calls and their context annotations, which play an important role in filtering results. Finally, we introduce the real-world GATK Best Practices workflows, which are guidelines for getting the best results possible out of your variant discovery analyses.</p>

<section data-type="sect1" data-pdf-bookmark="Getting Started with GATK"><div class="sect1" id="getting_started_with_gatk">
<h1>Getting Started with GATK</h1>

<p>GATK is an open source software package developed at the Broad Institute. As its full name suggests, GATK is a <em>toolkit</em>, not a single tool—where we define <em>tool</em> as the individual functional unit that you will invoke by name to perform a particular data transformation or analysis task. <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="about" data-type="indexterm" id="idm45625633894840"/>GATK contains a fairly large collection of these individual tools, some designed to convert data from one format to another, to collect metrics about the data or, most notably, to run actual computational analyses on the data. All of these tools are provided within a single packaged executable. In fact, starting with version 4.0, which is sometimes referred to as <em>GATK4</em>, the GATK executable also bundles all tools from the<a contenteditable="false" data-primary="Picard toolkit" data-type="indexterm" id="idm45625633892472"/> Picard toolkit, another popular open source genomics package that has historically focused on sequence metrics collection and file format operations.</p>

<div data-type="note" epub:type="note" id="the_wrench_in_the_gatk_logocomma_shown"><h6>Note</h6>
<p>The names of GATK tools are almost all boring (except maybe Funcotator), and most of them are self-explanatory, which is as it should be. With more than two hundred tools in the toolkit, no one wants to deal with cutesy names that give no indication as to what each tool does.</p>

<figure class="no-frame width-20"><div id="the_gatk_logodot" class="figure"><img alt="The GATK logo." src="Images/gitc_05in01.png" width="300" height="168"/></div></figure>
</div>

<p>Note that because the toolkit is fully open source, the public release includes tools that are under active development or experimentation; those tools are marked as such in the documentation and typically include a disclaimer in their output logs.</p>

<p>GATK tools are primarily designed to be run from the command line. In various scenarios, you can use them without ever running the commands directly; for example, either through a graphical user interface (GUI) that wraps the tools or in scripted workflows.<a contenteditable="false" data-primary="workflows" data-secondary="large-scale and routine uses of GATK via" data-type="indexterm" id="idm45625633886664"/> In fact, most large-scale and routine uses of GATK are typically done through workflows, as we discuss in more detail in <a data-type="xref" href="ch08.xhtml#automating_analysis_execution_with_work">Chapter 8</a>. Yet, to get started with the toolkit, we like to peel away all these layers and just run individual commands in the terminal, to demonstrate basic usage and provide visibility into how you can customize the tools’ operation.</p>

<section data-type="sect2" data-pdf-bookmark="Operating Requirements"><div class="sect2" id="operating_requirements">
<h2>Operating Requirements</h2>

<p>Because GATK is written in Java, you can run it almost anywhere as long as you have the proper version of Java installed<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="operating requirements for" data-type="indexterm" id="ix_GATK1stops"/> on your machine (as of this writing, JRE or JDK 1.8), except on Microsoft Windows, which is not supported.<a contenteditable="false" data-primary="Java" data-secondary="JDK 1.8 or JRE required for GATK" data-type="indexterm" id="idm45625633879800"/> No installation is necessary in the traditional sense; in its simplest form, all you need to do is download the GATK package, place its contents in a convenient directory on your hard drive (or server filesystem) and, optionally, add the location of the GATK wrapper script to your path.</p>

<p>However, a subset of tools (especially the newer ones that use fancy machine learning libraries) have additional R and/or Python dependencies, which can be quite painful to install.<a contenteditable="false" data-primary="R language" data-secondary="GATK dependencies in" data-type="indexterm" id="idm45625633877368"/><a contenteditable="false" data-primary="Python" data-secondary="GATK dependencies in" data-type="indexterm" id="idm45625633875992"/> Also, this exposes you to annoying conflicts with the requirements of other tools that you might need to use. For example, suppose that you are using a version of GATK that requires NumPy 1.13.3, but you also need to use a different tool that requires NumPy 1.12.0. This could quickly become a nightmare, where you update a package to enable one analysis only to find later that another of your scripts no longer runs successfully.</p>

<p>You can use a package manager like <a href="https://docs.conda.io">Conda</a> to mitigate this problem.<a contenteditable="false" data-primary="Conda package manager, GATK Conda script" data-type="indexterm" id="idm45625633872888"/> There is an official GATK Conda script that you can use to replicate the environment required for GATK, but frankly, it’s not a perfect solution.</p>

<p>The best<a contenteditable="false" data-primary="containers" data-secondary="container images for GATK versions" data-type="indexterm" id="idm45625633870824"/> way to deal with this is to use a container, which provides a fully self-contained environment that encapsulates all requirements and dependencies, as described in <a data-type="xref" href="ch03.xhtml#computing_technology_basics_for_life_sc">Chapter 3</a> (theory) and <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a> (practice).  Every GATK release comes with a container image that matches that version’s requirements exactly, and it’s just a <code>docker pull</code> away.<a contenteditable="false" data-primary="docker pull command" data-type="indexterm" id="idm45625633866600"/> That means no more time and effort wasted setting up and managing your work environment—a huge win for portability and reproducibility of complex analyses.</p>

<div data-type="note" epub:type="note" id="the_somewhat_stern_looking_owl_in_figur"><h6>Note</h6>
<p>Meet Mo, the mascot of the GATK nightly builds, the process that generates a new build of the GATK every night with the day’s latest developments.<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="Mo, nightly builds mascot" data-type="indexterm" id="idm45625633863832"/></p>

<figure class="no-frame width-20"><div id="mocomma_the_nightly_builds_owldot" class="figure"><img alt="Mo, the nightly builds owl." src="Images/gitc_05in02.png" width="685" height="1081"/></div></figure>

<p>Mo looks sleepy because he was up all night running tests and working to make sure the bleeding-edge types could try out the very latest code with minimal hassle. Mo’s blue plumage, fez, and bowtie are an homage to the British TV show character Doctor Who as played by actor Matt Smith. Because, why not? Bow ties are cool (though GATK does prefer BWA for short read alignments). He is named after the first person ever to ask whether the owl had a name (he didn’t).</p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Command-Line Syntax"><div class="sect2" id="command_line_syntax">
<h2>Command-Line Syntax</h2>

<p>Syntax is <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="operating requirements for" data-startref="ix_GATK1stops" data-type="indexterm" id="idm45625633857640"/>admittedly not the most riveting <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="command-line systax" data-type="indexterm" id="idm45625633855848"/>topic, but it’s essential to learn as soon as possible how to properly phrase instructions to the program. So let’s begin by reviewing the basics. <a contenteditable="false" data-primary="command line" data-secondary="GATK command-line syntax" data-type="indexterm" id="idm45625633854120"/>You’re not going to run any commands just yet, but we’ll get there soon.</p>

<p>As mentioned earlier, GATK is a Java-based command-line toolkit. Normally, the syntax for any Java-based <a contenteditable="false" data-primary="Java" data-secondary="syntax for Java-based command-line programs" data-type="indexterm" id="idm45625633852056"/>command-line program is</p>

<pre data-type="programlisting">
$ java -jar program.jar [program arguments]</pre>

<p>where <code>java</code> invokes the Java system on your machine to create a Java Virtual Machine (JVM).<a contenteditable="false" data-primary="JVM (Java Virtual Machine)" data-type="indexterm" id="idm45625633848936"/> The JVM is a virtual environment in which your program will run. <a contenteditable="false" data-primary="JAR (Java ARchive) files" data-type="indexterm" id="idm45625633847592"/>Then, the command specifies the JAR file; JAR stands for <em>Java ARchive</em>, because it’s a collection of Java code ZIPped into a single file. Finally, the command specifies the arguments that are intended for the program itself.</p>

<p>However, since version 4.0, the GATK team provides a Python wrapper script that makes it easier to compose GATK command lines without needing to think about the Java syntax. The wrapper script takes care of calling the appropriate JAR and sets some parameters that you would otherwise need to specify manually.</p>

<p>So, instead of what we just showed you, the basic syntax for calling GATK is now:</p>

<pre data-type="programlisting">
$ gatk ToolName [tool arguments]</pre>

<p>Here, <code><em>ToolName</em></code> is the name of the tool that you want to run; for example, <code>HaplotypeCaller</code>, the germline short variant caller.<a contenteditable="false" data-primary="ToolName argument, gatk command" data-type="indexterm" id="idm45625633842024"/> The tool name argument is <em>positional</em>, meaning that it must be the first argument provided to the GATK command. After that, everything else is usually nonpositional, except for a few exceptions: some tools have pairs of arguments that go together, and Spark-related arguments (which we get to shortly) always come last. Tool names and arguments are all case sensitive.</p>

<p>In some<a contenteditable="false" data-primary="JVM (Java Virtual Machine)" data-secondary="specifying parameters for" data-type="indexterm" id="idm45625633839544"/> situations, you might need to specify parameters for the JVM, like the maximum amount of memory it can use on your system. In the generic Java command, you would place them between <code>java</code> and <code>-jar</code>; for example:</p>

<pre data-type="programlisting">
$ java -Xmx4G -XX:+PrintGCDetails -jar program.jar [program arguments]</pre>

<p>In your GATK commands, you pass them<a contenteditable="false" data-primary="gatk --java-options command" data-type="indexterm" id="idm45625633835656"/> in through a new argument, <code>--java-options</code>, like this:</p>

<pre data-type="programlisting">
$ gatk --java-options "-Xmx4G -XX:+PrintGCDetails" ToolName [tool arguments]</pre>

<p>Finally, you can expect the majority of GATK argument names to follow <em>POSIX convention</em>, in which <a contenteditable="false" data-primary="POSIX convention, in GATK argument names" data-type="indexterm" id="idm45625633832088"/>the name<a contenteditable="false" data-primary="argument names (GATK), POSIX convention for" data-type="indexterm" id="idm45625633830728"/> is prefixed by two dashes (<code>--</code>) and where applicable, words are separated by single dashes (<code>-</code>). A minority of very commonly used arguments accept a short name, prefixed by a single dash (<code>-</code>), that is often a single capital letter. Arguments that do not follow these conventions include Picard tool arguments and a handful of legacy arguments.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Multithreading with Spark"><div class="sect2" id="multithreading_with_spark">
<h2>Multithreading with Spark</h2>

<p>Earlier versions of the GATK used some homegrown, hand-rolled code to provide multithreading functionality. <a contenteditable="false" data-primary="Spark framework" data-secondary="multithreading with, in GATK" data-type="indexterm" id="ix_Sprkmlti"/><a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="multithreading with Spark" data-type="indexterm" id="ix_GATKlstSprk"/>If you’ve been using old versions of GATK, you might be familiar with the engine arguments <code>-nt</code> and <code>-nct</code>. Starting with GATK4, all that is gone; the GATK team chose to replace it with Spark, which has emerged as a much more robust, reliable, and performant option.</p>

<p>The relevant Spark library is bundled inside GATK itself, so there is no additional software to install. It works right out of the box, on just about any system—contrary to surprisingly widespread belief, you do not need a special Spark cluster to run Spark-enabled tools. However, some tools have computational requirements that exceed the capabilities of single-core machines. As you’ll see at several points in this book, one of the advantages of cloud infrastructure is that we’re able to pick and choose suitable machines on demand, depending on the needs of a given analysis.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="which_gatk_tools_are_spark_enabledquest">
<h5>Which GATK Tools Are Spark Enabled?</h5>

<p>Not all GATK tools are Spark enabled. Some are not Spark enabled at all, some exist in a <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="multithreading with Spark" data-tertiary="GATK tools that are Spark-enabled" data-type="indexterm" id="idm45625633818392"/>non-Spark<a contenteditable="false" data-primary="Spark framework" data-secondary="multithreading with, in GATK" data-tertiary="Spark-enabled GATK tools" data-type="indexterm" id="idm45625633816424"/> version and a Spark-enabled version, and some exist only in a Spark-enabled version. This can be a bit confusing, so here is some guidance for navigating your options. Generally, the name of Spark-enabled tools end with the suffix <code>*Spark</code>, though this convention is not strictly enforced. When in doubt, check the individual tool documentation pages under the <a href="https://oreil.ly/qjW9v">Tool Index</a>, which specify whether the tool supports Spark as well as any limitations such as experimental or beta status.</p>

<dl>
	<dt>GATK tools that existed in version 3.8 and earlier</dt>
	<dd>
	<p>Most of the “classic” GATK tools that predate GATK4 are <em>not</em> Spark enabled. A subset of these also exist in Spark-enabled form, which are distinguished by the suffix <code>*Spark</code> appended to the name of the original tool. Some of these are still in an experimental state and should not be used in high-stakes work such as in clinical analyses. See the log output and the tool documentation for any applicable warnings and notes on development status.</p>

	<p><em>Example:</em> <code>HaplotypeCaller</code> versus <code>HaplotypeCallerSpark</code></p>
	</dd>
	<dt>Original Picard tools versus Spark-enabled reimplementations</dt>
	<dd>
	<p>No Spark-enabled tools are in the standalone Picard package. However, a subset of GATK tools that were originally in Picard were reimplemented in GATK4 and were Spark enabled in the process. These are also identified with the suffix <code>*Spark</code> appended to their original Picard name. Several of these are now considered sufficiently mature for production work and are significantly faster (in wall-clock time) compared to the original.</p>

	<p><em>Example:</em> <code>MarkDuplicates</code> versus <code>MarkDuplicatesSpark</code>, and <code>SortSam</code> versus <code>SortSamSpark</code>.</p>
	</dd>
	<dt>Natively Spark-enabled tools</dt>
	<dd>
	<p>Many of the tools that were written for GATK4 were given native Spark support. Those that we are aware of have the <code>*Spark</code> suffix in their name, but a few tools might not follow this naming convention. Again, when in doubt, check the tool documentation page.</p>

	<p><em>Example:</em> <code>PathSeqScoreSpark</code>, <code>FindBreakpointEvidenceSpark</code></p>
	</dd>
	<dt>Spark pipelines</dt>
	<dd>
	<p>One of the best ways to take full advantage of Spark’s strengths is to wire up multiple tools into a complete pipeline that runs on Spark from start to finish. GATK developers have been experimenting with this approach and have produced several such pipelines, which are listed as individual tools in the <a href="https://oreil.ly/M39ID">Tool Index</a> and are recognizable by the <code>*PipelineSpark</code> suffix appended to their name. Most of these are still considered experimental; the only exception currently is <code>PathSeq​Pipeline⁠Spark</code>.</p>

	<p><em>Example:</em> <code>StructuralVariationDiscoveryPipelineSpark</code>, <code>ReadsPipelineSpark</code></p>
	</dd>
	<dt>Computational requirements</dt>
	<dd>
	<p>All GATK tools, whether Spark enabled or not, can in principle be run on a “normal” single-core machine, but some tools might not perform well under those conditions because of the scale of the computations they need to perform and the corresponding resource requirements. See the tool documentation for the specific requirements of each tool.</p>
	</dd>
</dl>
</div></aside>

<p>To use Spark multithreading, the basic syntax is fairly straightforward; we just need to add a few arguments to the regular GATK command line. Note that when we add Spark arguments, we first add a <code>--</code> separator (two dashes), which signals to the parser system that what follows is intended for the Spark subsystem.</p>

<p>In the simplest case, called <em>local execution</em>, the program executes all the work on the same machine where we’re launching the command.<a contenteditable="false" data-primary="--spark-master local flag (gatk command)" data-primary-sortas="spark-master local" data-type="indexterm" id="idm45625633791208"/> To do that, simply add the argument <code>--spark-master 'local[*]'</code> to the base command line. The <code>--spark-master local</code> part instructs the Spark subsystem to send all work to the machine’s own local cores, and the value in brackets specifies the number of cores to use. To use all available cores, replace the number by an asterisk. Here is an example command showing a Spark-enabled tool being run on four cores of a normal local machine:</p>

<pre data-type="programlisting">
gatk MySparkTool \
    -R data/reference.fasta \
    -I data/sample1.bam \
    -O data/variants.vcf \
    -- \
    --spark-master 'local[4]'</pre>

<p>If you have<a contenteditable="false" data-primary="clusters" data-secondary="access to Spark cluster in GATK tools" data-type="indexterm" id="idm45625633787064"/> access to a Spark cluster, the Spark-enabled tools are going to be extra happy, though you might need to provide additional parameters to use them effectively. We don’t cover that in detail, but here are examples of the syntax you would use to send the work to your cluster depending on your cluster type:</p>

<ul>
	<li>
	<p>Run on the cluster at 23.195.26.187, port 7077:</p>

	<pre data-type="programlisting">
--spark-runner SPARK --spark-master spark://23.195.26.187:7077</pre>
	</li>
	<li>
	<p>Run on the cluster called <code>my_cluster</code> in Google Dataproc:</p>

	<pre data-type="programlisting">
--spark-runner GCS --cluster my_cluster</pre>
	</li>
</ul>

<p>That covers just about all you need to know about Spark for now, because using the cloud allows us to sidestep some of the issues that made Spark so valuable in other settings.<a contenteditable="false" data-primary="Spark framework" data-secondary="multithreading with, in GATK" data-startref="ix_Sprkmlti" data-type="indexterm" id="idm45625633780472"/><a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="multithreading with Spark" data-startref="ix_GATKlstSprk" data-type="indexterm" id="idm45625633778760"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Running GATK in Practice"><div class="sect2" id="running_gatk_in_practice">
<h2>Running GATK in Practice</h2>

<p>By this point, you’re probably itching to run some actual GATK commands, so let’s get started. <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="running in practice" data-type="indexterm" id="ix_GATK1strun"/>We’re going to assume that you are using the VM that you set up in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a> and that you have also already pulled and tested the GATK container image as instructed in that same chapter.<a contenteditable="false" data-primary="virtual machines (VMs)" data-secondary="starting up your custom VM" data-type="indexterm" id="idm45625633772200"/></p>

<p>If your VM is still up and running, go ahead back into it using SSH, unless you never even left. If it’s stopped, head on over <a contenteditable="false" data-primary="GCP console" data-secondary="VM instance management page" data-type="indexterm" id="idm45625633770232"/>to the GCP console <a href="https://oreil.ly/sGeug">VM instance management page</a>, start your VM, and then log back into it.</p>

<section data-type="sect3" data-pdf-bookmark="Docker setup and test invocation"><div class="sect3" id="docker_setup_and_test_invocation">
<h3>Docker setup and test invocation</h3>

<p>As a reminder, this was the <a contenteditable="false" data-primary="Docker" data-secondary="setup and test invocation for GATK" data-type="indexterm" id="idm45625633765816"/>command that<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="running in practice" data-tertiary="Docker setup and test invocation" data-type="indexterm" id="idm45625633764248"/> you ran in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>:</p>

<pre data-type="programlisting">
$ docker run -v ~/book:/home/book -it us.gcr.io/broad-gatk/gatk:4.1.3.0 /bin/bash</pre>

<p>If your container is still running, you can hop back in and resume your session.<a contenteditable="false" data-primary="containers" data-secondary="Docker, setup and test invocation for GATK" data-type="indexterm" id="idm45625633760296"/> As we showed in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>, you just need to determine the container ID by running <code>docker ps</code> and then <code>run docker attach <em>ID</em></code>. If you had shut down your container last time, simply run the full command again.</p>

<p>When the container is ready to go, your terminal prompt will change to something like this:</p>

<pre data-type="programlisting">
(gatk) root@ce442edab970:/gatk#</pre>

<p><code>This</code> indicates that you are logged into the container. Going forward, we use the hash sign (<code>#)</code> as the terminal prompt character to indicate when you are running commands within the container.<a contenteditable="false" data-primary="# (hash sign) in terminal prompt" data-type="indexterm" id="idm45625633753800"/></p>

<p>You can use the usual shell commands to navigate within the container<a contenteditable="false" data-primary="Google Cloud Shell" data-secondary="using commands to navigate in GATK container" data-type="indexterm" id="idm45625633752216"/> and check out what’s in there:</p>

<pre class="small" data-type="programlisting">
# ls
GATKConfig.EXAMPLE.properties   gatk-package-4.1.3.0-spark.jar  gatkdoc
README.md                       gatk-spark.jar                  gatkenv.rc
gatk                            gatk.jar                        install_R_packages.R
gatk-completion.sh              gatkPythonPackageArchive.zip    run_unit_tests.sh
gatk-package-4.1.3.0-local.jar  gatkcondaenv.yml                scripts
</pre>

<p>As you can see, the container includes several <em>.jar</em> files as well as the <code>gatk</code> wrapper script, which we use in our command lines to avoid having to deal with the <em>.jar file</em>s directly. <a contenteditable="false" data-primary="gatk wrapper script" data-type="indexterm" id="idm45625633747304"/>The <code>gatk</code> executable is included in the preset user path, so we can invoke it from anywhere inside the container. For example, you can invoke GATK’s help/usage output by simply typing the <code>gatk</code> command in your terminal:</p>

<pre class="small" data-type="programlisting">
# gatk
Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool)
   gatk AnyTool toolArgs
Usage template for Spark tools (will NOT work on non-Spark tools)
   gatk SparkTool toolArgs  [ -- --spark-runner &lt;LOCAL | SPARK | GCS&gt; sparkArgs ]
Getting help
   gatk --list       Print the list of available tools
   gatk Tool --help  Print help on a particular tool
Configuration File Specification
    --gatk-config-file             PATH/TO/GATK/PROPERTIES/FILE

gatk forwards commands to GATK and adds some sugar for submitting spark jobs

  --spark-runner &lt;target&gt;    controls how spark tools are run
    valid targets are:
    LOCAL:      run using the in-memory spark runner
    SPARK:      run using spark-submit on an existing cluster
                --spark-master must be specified
                --spark-submit-command may be specified to control the Spark submit command
                arguments to spark-submit may optionally be specified after --
    GCS:        run using Google cloud dataproc
                commands after the -- will be passed to dataproc
                --cluster &lt;your-cluster&gt; must be specified after the --
                spark properties and some common spark-submit parameters will be translated
                to dataproc equivalents
  --dry-run      may be specified to output the generated command line without running it
  --java-options 'OPTION1[ OPTION2=Y ... ]''   optional - pass the given string of options to 
                 the java JVM at runtime.
                Java options MUST be passed inside a single string with space-separated values
</pre>

<p>You might recall that you already ran this when you tested your Docker setup in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>. Hopefully the outputs make more sense now that you are now an expert in GATK syntax.</p>

<p>To get help or usage information for a specific tool—for <a contenteditable="false" data-primary="help" data-secondary="for GATK tools" data-type="indexterm" id="idm45625633740552"/>example, the germline short variant caller <code>HaplotypeCaller—</code>try this:</p>

<pre data-type="programlisting">
# gatk HaplotypeCaller --help</pre>

<p>This is convenient for looking up an argument while you’re working, but most GATK tools spit out a lot of information that can be difficult to read in a terminal. You can access the same information in a more readable format in the <a href="https://oreil.ly/9OiEJ">Tool Index</a> on the GATK website.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Running a real GATK command"><div class="sect3" id="running_a_real_gatk_command">
<h3>Running a real GATK command</h3>

<p>Now that we know we have<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="running in practice" data-tertiary="running a real GATK command" data-type="indexterm" id="idm45625633734264"/> the software ready to go, let’s run a real GATK command on some actual input data. The inputs here are files from the book data bundle that you copied to your VM in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>.</p>

<p>Let’s begin by moving into the <em>/home/book/data/germline</em> directory and creating a new directory named <em>sandbox</em> to hold the outputs of the commands that we’re going to run:</p>

<pre data-type="programlisting">
# cd /home/book/data/germline
# mkdir sandbox</pre>

<p>Now you can run your first real GATK command!<a contenteditable="false" data-primary="\ (backslash),  ending lines in multiline commands" data-type="indexterm" id="idm45625633728584"/>  You can either type it as shown in the example that follows, on multiple lines, or you can put everything on a single line if you remove the backslash (\) characters:</p>

<pre data-type="programlisting">
# gatk HaplotypeCaller \
    -R ref/ref.fasta \
    -I bams/mother.bam \
    -O sandbox/mother_variants.vcf</pre>

<p>This uses <code>HaplotypeCaller</code>, GATK’s current <a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="running GATK command for" data-type="indexterm" id="idm45625633725384"/>flagship caller for germline SNPs and indels, to do a basic run of germline short variant calling on a very small input dataset, so it should take less than two minutes to run. It will spit out a lot of console output; let’s have a look at the most important bits.</p>

<p>The header, which lets you know what you’re running:</p>

<pre data-type="programlisting">
Using GATK jar /gatk/gatk-package-4.1.3.0-local.jar
Running:
   java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_wri
te_samtools=true -Dsamjdk.use_async_io_wri
te_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.3.0-
local.jar HaplotypeCaller -R ref/ref.fas
ta -I bams/mother.bam -O sandbox/mother_variants.vcf
09:47:17.371 INFO  NativeLibraryLoader - Loading libgkl_compression.so from
jar:file:/gatk/gatk-package-4.1.3.0-local.
jar!/com/intel/gkl/native/libgkl_compression.so
09:47:17.719 INFO  HaplotypeCaller - -----------------------------------------
-------------------
09:47:17.721 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.3.0
09:47:17.721 INFO  HaplotypeCaller - For support and documentation go to
https://software.broadinstitute.org/gatk/
09:47:17.722 INFO  HaplotypeCaller - Executing as root@3f30387dc651 on Linux
v5.0.0-1011-gcp amd64
09:47:17.723 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM
v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12
09:47:17.724 INFO  HaplotypeCaller - Start Date/Time: August 20, 2019 9:47:17 AM
UTC</pre>

<p>The progress meter, which informs you how far along you are:</p>

<pre data-type="programlisting">
09:47:18.347 INFO  ProgressMeter - Starting traversal
09:47:18.348 INFO  ProgressMeter -        Current Locus  Elapsed Minutes
Regions Processed   Regions/Minute
09:47:22.483 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated; at
least 10 samples must have called geno
types
09:47:28.371 INFO  ProgressMeter -          20:10028825              0.2
33520         200658.5
09:47:38.417 INFO  ProgressMeter -          20:10124905              0.3
34020         101709.1
09:47:48.556 INFO  ProgressMeter -          20:15857445              0.5
53290         105846.1
09:47:58.718 INFO  ProgressMeter -          20:16035369              0.7
54230          80599.5
09:48:08.718 INFO  ProgressMeter -          20:21474713              0.8
72480          86337.1
09:48:18.718 INFO  ProgressMeter -          20:55416713              1.0
185620         184482.4</pre>

<p>And the footer, which signals when the run is complete and how much time it took:</p>

<pre data-type="programlisting">
09:48:20.714 INFO  ProgressMeter - Traversal complete. Processed 210982 total
regions in 1.0 minutes.
09:48:20.738 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call: 
0.045453468000000004
09:48:20.739 INFO  PairHMM - Total compute time in PairHMM 
computeLogLikelihoods(): 6.333675601
09:48:20.739 INFO  SmithWatermanAligner - Total compute time in java 
Smith-Waterman: 6.18 sec
09:48:20.739 INFO  HaplotypeCaller - Shutting down engine
[August 20, 2019 9:48:20 AM UTC]
org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done.
Elapsed time: 1.06 minutes.
Runtime.totalMemory()=717225984</pre>

<p>If you see all of that, hurray! You have successfully run a proper GATK command that called variants on a real person’s genomic sequence data. We dig into the output results and how to interpret them in more detail later in this chapter, but for now you can rejoice at having achieved this first milestone of running GATK in the cloud.</p>

<p>If, however, anything went wrong and you’re not seeing the expected output, start by checking your spelling and all file paths. Our experience suggests that typos, case errors, and incorrect file paths are the leading cause of command-line errors worldwide. Human brains are wired to make approximations and often show us what we expect to see, not what is actually there, so it’s really easy to miss small errors. And remember that case is important! Make sure to use a terminal font that differentiates between lowercase <code>l</code> and uppercase <code>I</code>.</p>

<p>For additional help troubleshooting GATK issues, you can reach out to the <a href="https://oreil.ly/YfsSM">GATK support forum staff</a>. Make sure to mention that you are following the instructions in this book to provide the necessary context.<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="help from support forum staff" data-type="indexterm" id="idm45625633713352"/></p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Running a Picard command within GATK4"><div class="sect3" id="running_a_picard_command_within_gatk4">
<h3>Running a Picard command within GATK4</h3>

<p>As mentioned earlier, the Picard toolkit is included as a library within the GATK4 executable.<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="running in practice" data-tertiary="running Picard command in GATK4" data-type="indexterm" id="idm45625633710120"/> This is convenient because many genomics pipelines involve a mix of both Picard and GATK tools. <a contenteditable="false" data-primary="Picard toolkit" data-secondary="running Picard command from GATK4" data-type="indexterm" id="idm45625633708152"/>Having both toolkits in a single executable means that you have fewer separate packages to keep up-to-date, and it largely guarantees that the tools will produce compatible outputs because they are tested together before release.</p>

<p>Running a Picard tool from within GATK is simple. In a nutshell, you use the same syntax as for GATK tools, calling on the name of the tool that you want to run and providing the relevant arguments and parameters in the rest of the command after that.<a contenteditable="false" data-primary="ValidateSamFile, running from GATK4" data-type="indexterm" id="idm45625633705704"/> For example, to run the classic quality control tool <code>ValidateSamFile</code>, you would run this command:</p>

<pre data-type="programlisting">
# gatk ValidateSamFile \
    -R ref/ref.fasta \
    -I bams/mother.bam \
    -O sandbox/mother_validation.txt</pre>

<p>If you previously used Picard tools from the standalone executable produced by the Picard project maintainers, you’ll notice that the syntax we use here (and with GATK4 in general) has been adapted from the original Picard style (e.g., <code>I=sample1.bam)</code> to match the GATK style (e.g., <code>-I sample1.bam</code>). This was done to make command syntax consistent across tools, so that you don’t need to think about whether any given tool that you want to use is a Picard tool or a “native” GATK tool when composing your commands. This does mean, however, that if you have existing scripts that use the standalone Picard executable, you will need to convert the command syntax to use them with the GATK package.<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="running in practice" data-startref="ix_GATK1strun" data-type="indexterm" id="idm45625633701256"/></p>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Getting Started with Variant Discovery"><div class="sect1" id="getting_started_with_variant_discovery">
<h1>Getting Started with Variant Discovery</h1>

<p>Enough with <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="getting started with variant discovery" data-type="indexterm" id="ix_GATK1stVD"/>the syntax <a contenteditable="false" data-primary="variant discovery in GATK" data-type="indexterm" id="ix_vardisGATK"/>examples; let’s call some variants!</p>

<p>What follows is not considered part of the GATK Best Practices, which we introduce in the last section of this chapter. At this stage, we are just presenting a simplified approach to variant discovery in order to demonstrate in an accessible way the key principles of how the tools work. In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.xhtml#best_practices_for_germline_short_varia">6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.xhtml#gatk_best_practices_for_somatic_variant">7</a>, we go over the GATK Best Practices workflows for three major use cases, and we highlight specific steps that are particularly important with additional details.</p>

<section data-type="sect2" data-pdf-bookmark="Calling Germline SNPs and Indels with HaplotypeCaller"><div class="sect2" id="calling_germline_snps_and_indels_with_h">
<h2>Calling Germline SNPs and Indels with HaplotypeCaller</h2>

<p>We’re going to<a contenteditable="false" data-primary="variant discovery in GATK" data-secondary="calling germline SNPs and indels with HaplotypeCaller" data-type="indexterm" id="ix_vardisGATKHC"/> look for <a contenteditable="false" data-primary="germline short sequence variants" data-secondary="finding with GATK HaplotypeCaller" data-type="indexterm" id="ix_grmlSNP"/>germline short sequence<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="getting started with variant discovery" data-tertiary="calling germline SNPs and indels with HaplotypeCaller" data-type="indexterm" id="ix_GATK1stVDHC"/> variants <a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calling germline SNPs and indels with" data-type="indexterm" id="ix_HaCacall"/>because that’s the most<a contenteditable="false" data-primary="indels" data-secondary="and germline SNPs, calling with HaplotypeCaller" data-type="indexterm" id="ix_indelcall"/> common type of variant analysis in both research <a contenteditable="false" data-primary="single-nucleotide polymorphisms (SNPs)" data-secondary="calling germline SNPs and indels with HaplotypeCaller" data-type="indexterm" id="ix_SNPscall"/>and clinical applications today, using the <code>HaplotypeCaller</code> tool. We briefly go over how the <code>HaplotypeCaller</code> works, and then we do a hands-on exercise to examine how this plays out in practice.</p>

<p>In the hands-on exercises in this chapter and the next, we use a version of the human genome reference (b37) that is not the most recent (hg38). This is because a lot of GATK tutorial examples were developed when b37 (with its almost-twin hg19) was still the reigning reference, and the relevant materials have not yet been regenerated with the new reference. Many researchers in the field have not yet moved to the newer reference because doing so requires a lot of work and validation effort. For this book, we chose to accept the reality of this hybrid environment rather than trying to sweep it out of sight. Learning to be aware of the existence of different references and navigating different sets of resource data is an important part of the discipline of computational genomics.</p>

<section data-type="sect3" data-pdf-bookmark="HaplotypeCaller in a nutshell"><div class="sect3" id="haplotypecaller_in_a_nutshell">
<h3>HaplotypeCaller in a nutshell</h3>

<p><code>HaplotypeCaller</code> is designed to identify germline SNPs and indels with a high level of sensitivity and accuracy.  One of its distinguishing features is that it uses local <em>de novo</em> assembly of haplotypes to model possible variation. <a contenteditable="false" data-primary="de novo genome assembly" data-secondary="local, with HaplotypeCaller" data-type="indexterm" id="idm45625633672360"/>If that sounds like word salad to you, all you really need to know is that whenever the program encounters a region showing signs of variation, it completely realigns the reads in that region before proceeding to call variants. This allows the <code>HaplotypeCaller</code> to gain accuracy in regions that are traditionally difficult to call; for example, when they contain different types of variants close to one another. It also makes the <code>HaplotypeCaller</code> much better at calling indels than traditional position-based callers like the old <code>UnifiedGenotyper</code> and Samtools <code>mpileup</code>.</p>

<p><code>HaplotypeCaller</code> operates in four distinct stages, <a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calling germline SNPs and indels with" data-tertiary="four stages of operation" data-type="indexterm" id="idm45625633667896"/>illustrated in <a data-type="xref" href="#the_four_stages_of_haplotypecallerapost">Figure 5-1</a> and outlined in the list that follows. Note that this description refers to established computational biology terms and algorithms that we chose not to explain in detail here; if you would like to learn more about these concepts, we recommend consulting the GATK documentation and forum for pointers to appropriate study materials.</p>

<dl>
	<dt>1. Define active regions</dt>
	<dd>The tool determines which regions of the genome it needs to operate on based on the presence of significant evidence for variation, such as mismatches or gaps in read alignments.</dd>
	<dt>2. Determine haplotypes by reassembly of the active region</dt>
	<dd>For each active region, the program builds a De Bruijn–like assembly graph to realign the reads within the bounds of the active region. This allows it to make a list of all the possible haplotypes supported by the data. The program then realigns each haplotype against the reference sequence using the Smith-Waterman algorithm in order to identify potentially variant sites.</dd>
	<dt>3. Determine likelihoods of the haplotypes given the read data</dt>
	<dd>For each active region, the program then performs a pairwise alignment of each read against each haplotype using the PairHMM algorithm. This produces a matrix of likelihoods of haplotypes given the read data. These likelihoods are then marginalized to obtain the likelihoods of alleles per read for each potentially variant site.</dd>
	<dt>4. Assign sample genotypes</dt>
	<dd>For each potentially variant site, the program applies Bayes’ rule, using the likelihoods of alleles given the read data, to calculate the posterior probabilities of each genotype per sample given the read data observed for that sample. The most likely genotype is then assigned to the sample.</dd>
</dl>

<figure><div id="the_four_stages_of_haplotypecallerapost" class="figure"><img alt="The four stages of HaplotypeCaller’s operation." src="Images/gitc_0501.png" width="1440" height="978"/>
<h6><span class="label">Figure 5-1. </span>The four stages of HaplotypeCaller’s operation.</h6>
</div></figure>

<p>The final <a contenteditable="false" data-primary="Variant Content Format (VCF) files" data-secondary="output of HaplotypeCaller" data-type="indexterm" id="idm45625633657064"/>output of <code>HaplotypeCaller</code> is a VCF file containing variant call records, which include detailed genotype information and a variety of statistics that describe the context of the variant and reflect the quality of the data used to make the call.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Running HaplotypeCaller and examining the output"><div class="sect3" id="running_haplotypecaller_and_examining_t">
<h3>Running HaplotypeCaller and examining the output</h3>

<p>Let’s run <code>HaplotypeCaller</code> in its simplest form <a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calling germline SNPs and indels with" data-tertiary="running HaplotypeCaller and examining output" data-type="indexterm" id="ix_HaCacallrun"/>on a single <a contenteditable="false" data-primary="variant discovery in GATK" data-secondary="calling germline SNPs and indels with HaplotypeCaller" data-tertiary="running HaplotypeCaller and examining output" data-type="indexterm" id="ix_vardisGATKHCrun"/>sample to become familiar with its operation. This command requires a reference genome file in FASTA format (<code>-R</code>) as well as one or more files with the sequence data to analyze in BAM format <span class="keep-together">(<code>-I</code>)</span>. <a contenteditable="false" data-primary="FASTA (reference genome format)" data-secondary="BAM files as input to HaplotypeCaller" data-type="indexterm" id="idm45625633645656"/><a contenteditable="false" data-primary="BAMs (Binary Alignment Maps)" data-secondary="BAM files as input to HaplotypeCaller" data-type="indexterm" id="idm45625633644216"/>It will output a file of variant calls in VCF format (<code>-O</code>). Optionally, the tool can take an interval or list of intervals to process (<code>-L</code>), which we use here to restrict analysis to a small slice of data in the interest of time (this can also be used for the purpose of parallelizing execution, as you’ll see a little later in this book):</p>

<pre data-type="programlisting">
# gatk HaplotypeCaller \
    -R ref/ref.fasta \
    -I bams/mother.bam \
    -O sandbox/mother_variants.200k.vcf \
    -L 20:10,000,000-10,200,000</pre>

<p>After the command has run to completion (after about a minute), we need to copy the output<a contenteditable="false" data-primary="storage buckets" data-secondary="copying HaplotypeCaller output file to bucket for viewing in IGV" data-type="indexterm" id="idm45625633639944"/> file to a bucket <a contenteditable="false" data-primary="IGV (Integrated Genome Viewer)" data-secondary="viewing HaplotypeCaller output in" data-type="indexterm" id="ix_IGVHAout"/>so that we can view it in IGV, as we demonstrated at the end of <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>. The simplest way to do this<a contenteditable="false" data-primary="SSH (Secure Shell)" data-secondary="opening second SSH terminal to VM from GCP console" data-type="indexterm" id="idm45625633635496"/> is to open a second SSH terminal to the VM from the GCP console so that we can run <code>gsutil</code> commands, <a contenteditable="false" data-primary="gsutil" data-secondary="copying contents of germline sandbox directory to storage bucket" data-type="indexterm" id="idm45625633633512"/>which is more convenient to do from outside the container. If at any point you lose track of which window is which or where you should be running <a contenteditable="false" data-primary="$ (dollar sign)" data-secondary="VM prompt ending in" data-type="indexterm" id="idm45625633631768"/>a given command, remember that the VM prompt ends in a dollar sign (<code>$</code>) and the container<a contenteditable="false" data-primary="# (hash sign) in terminal prompt" data-type="indexterm" id="idm45625633629768"/> prompt ends in a hash (<code>#</code>). Just match them up, and you’ll be OK.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Alternatively, you can copy the files directly from within the container, but first you need to run <code>gcloud init</code>, as described in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>, from within the container, which has its own authentication scope.<a contenteditable="false" data-primary="gcloud (Google Cloud SDK)" data-secondary="init command" data-type="indexterm" id="idm45625633625576"/> </p>
</div>

<p>In the second SSH terminal window that is now connected to your VM, move from the home <a contenteditable="false" data-primary="sandbox directory" data-secondary="moving SSH terminal for home directory into germline sandbox directory" data-type="indexterm" id="idm45625633623384"/>directory into the germline <em>sandbox</em> directory:</p>

<pre data-type="programlisting">
$ cd ~/book/data/germline/sandbox</pre>

<p>Next, you’re going to want to copy the contents of the <em>sandbox</em> directory to the storage bucket that you created in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>, using the <code>gsutil</code> tool. However, it’s going to be annoying if you need to constantly replace the name we use for our bucket with your own, so let’s take a minute to set up an environment variable. It’s going to serve as an alias so that you can just copy and paste our commands directly from now on.</p>

<p>Create the <code>BUCKET</code> variable by running the following command, replacing the <code><em>my-bucket</em></code> part with the name of your own bucket, as you did in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a> (last time!):</p>

<pre data-type="programlisting">
$ export BUCKET="gs://my-bucket"</pre>

<p>Now, check <a contenteditable="false" data-primary="echo command" data-type="indexterm" id="idm45625633614104"/>that the variable was set properly by running <code>echo</code>, making sure to add the <a contenteditable="false" data-primary="$ (dollar sign)" data-secondary="preceding variables" data-type="indexterm" id="idm45625633612312"/>dollar sign in front of <code>BUCKET</code>:</p>

<pre data-type="programlisting">
$ echo $BUCKET
gs://my-bucket</pre>

<p>Finally, you can copy the file that you want to view to your bucket:</p>

<pre data-type="programlisting">
$ gsutil cp mother_variants.200k.vcf* $BUCKET/germline-sandbox/
Copying file://mother_variants.200k.vcf [Content-Type=text/vcard]...
Copying file://mother_variants.200k.vcf.idx [Content-Type=application/octet-
stream]...
- [2 files][101.1 KiB/101.1 KiB]                                               
Operation completed over 2 objects/101.1 KiB.</pre>

<p>When the copy operation<a contenteditable="false" data-primary="BAMs (Binary Alignment Maps)" data-secondary="BAM files as input to HaplotypeCaller" data-tertiary="loading into IGV" data-type="indexterm" id="idm45625633607608"/> is complete, you’re going to go to your desktop IGV application and load the reference genome, the input BAM file, and the output VCF file.<a contenteditable="false" data-primary="Variant Content Format (VCF) files" data-secondary="loading HaplotypeCaller output VCF file into IGV" data-type="indexterm" id="idm45625633605560"/> We covered that procedure in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>, so you might feel comfortable doing that on your own. But just in case it’s been a while, let’s go through the main steps together briefly. Feel free to refer to the screenshots in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a> if you need a refresher.</p>

<p>All of the actions in the following numbered list are to be performed in the IGV application unless otherwise specified:</p>

<ol>
	<li>
	<p>In IGV, in the top menu, check that your Google login is active.</p>
	</li>
	<li>
	<p>On the drop-down menu on the left, verify that you have the “Human hg19” reference selected. Technically, our data is aligned to the b37 reference genome, but for pure viewing purposes (not analysis), the two are interchangeable.</p>
	</li>
	<li>
	<p>From the top menu bar, select File &gt; Load from URL and then paste the following file path for the BAM file into the top field:</p>

	<p><em>gs://genomics-in-the-cloud/v1/data/germline/bams/mother.bam</em></p>

	<p>You can leave the second field blank if you’re using IGV version 2.7.2 or later; IGV will automatically retrieve the index file. Click OK.</p>
	</li>
	<li>
	<p>Get the file path for the output VCF file that you just copied to your bucket. You can get it by running <strong><code>echo $BUCKET/germline-sandbox/mother_variants.200k.vcf</code></strong> in your VM terminal.</p>
	</li>
	<li>
	<p>From the top menu bar, select File &gt; Load from URL and then, in the first field, paste the VCF file path that you got from the previous step. You can leave the second field blank again. Click OK.</p>
	</li>
</ol>

<p>At this point, you should have all the data loaded and ready to go, but you don’t see anything yet because by default IGV shows you the full span of the genome, and at that scale, the small slice of data we’re working with is not visible. To zoom in on the data, in the location box, enter the coordinates <code><strong>20:10,002,294-10,002,623</strong></code> and then click Go. You should see something very similar to <a data-type="xref" href="#the_original_bam_file_and_output_vcf_fi">Figure 5-2</a>.</p>

<p>The variant track showing the contents of the VCF file is displayed on top, and below is the read data track showing the contents of the BAM file. Variants are indicated as small vertical bars (colored in red in <a data-type="xref" href="#the_original_bam_file_and_output_vcf_fi">Figure 5-2</a>), and sequence reads are represented as gray horizontal bars with a pointy bit indicating the read orientation. In the read data, we see various mismatches (small colored bars) and gaps (purple I-shaped symbols for insertions; deletions would show up as black horizontal bars).</p>

<figure class="no-frame"><div id="the_original_bam_file_and_output_vcf_fi" class="figure"><img alt="The original BAM file and output VCF file loaded in IGV." src="Images/gitc_0502.png" width="1442" height="818"/>
<h6><span class="label">Figure 5-2. </span>The original BAM file and output VCF file loaded in IGV.</h6>
</div></figure>

<p>We see that <code>HaplotypeCaller</code> called two variants within this region. You can click each little red bar in the variant track to get more details on each one. On the right, we have a homozygous SNP with excellent support in the read data. That seems likely to be correct. On the left, we have a homozygous insertion of three T bases, yet we see only a small proportion of reads with the insertion point symbol that support this call. How is this possible, when so few reads seem to support an insertion at this <span class="keep-together">position?</span></p>

<p>Your first reflex when you encounter something like this (a call that doesn’t make sense, especially if it’s an indel) should be to<a contenteditable="false" data-primary="soft clips" data-secondary="turning on in genome viewer" data-type="indexterm" id="idm45625633584264"/> turn on the display of <em>soft-clipped</em> sequences in the genome viewer you’re using. <em>Soft clips</em> are segments of reads that are marked<a contenteditable="false" data-primary="BWA mapper" data-type="indexterm" id="idm45625633581848"/> by the mapper (in this case BWA) as being not useful, so they are hidden by default by IGV. The mapper introduces soft clips in the<a contenteditable="false" data-primary="CIGAR (Concise Idiosyncratic Gapped Alignment Report)" data-secondary="soft clips, read data and" data-type="indexterm" id="idm45625633580472"/> CIGAR string of a read when it encounters bases, typically toward the end of the read, that it is not able to align to the reference to its satisfaction based on the scoring model it employs. At some point, the mapper gives up trying to align the bases, and just puts in the S operator to signify that downstream tools should disregard the following bases, even though they are still present in the read record.<a contenteditable="false" data-primary="hard clipping" data-type="indexterm" id="idm45625633578376"/> Some mappers even remove those bases, which is called <em>hard clipping</em>, and uses the H operator instead of S.</p>

<p>Soft clips are interesting to us because they often occur in the vicinity of insertions, especially large insertions. So let’s turn on soft clips in IGV in the Preferences. In the top menu bar, select View &gt; Preferences and click the Alignments tab to bring up the relevant settings. Select the checkbox labeled “Show soft-clipped bases,” as shown in <a data-type="xref" href="#igv_alignment_settingsdot">Figure 5-3</a>, and then click Save to close the Preferences pane.</p>

<figure class="no-frame"><div id="igv_alignment_settingsdot" class="figure"><img alt="IGV alignment settings." src="Images/gitc_0503.png" width="1424" height="947"/>
<h6><span class="label">Figure 5-3. </span>IGV alignment settings.</h6>
</div></figure>

<p>As we see in <a data-type="xref" href="#turning_on_the_display_of_soft_clips_sh">Figure 5-4</a>, with soft-clip display turned on, the region we’ve been looking at lights up with mismatching bases! That’s a fair amount of sequence that we were just ignoring because BWA didn’t know what to do with it. <code>HaplotypeCaller</code>, on the other hand, will have taken it into account when it called variants because the first thing it does within an active region is to throw out all the read alignment information and build a graph to realign them, as we discussed earlier.</p>

<figure class="no-frame"><div id="turning_on_the_display_of_soft_clips_sh" class="figure"><img alt="Turning on the display of soft clips shows a lot of information that was hidden." src="Images/gitc_0504.png" width="1441" height="557"/>
<h6><span class="label">Figure 5-4. </span>Turning on the display of soft clips shows a lot of information that was <span class="keep-together">hidden.</span></h6>
</div></figure>

<p>So now we know that <code>HaplotypeCaller</code> had access to this additional information and somehow decided that it supported the presence of an insertion at this position. But the extra sequence we see here is unaligned, so how do we check <code>HaplotypeCaller</code>’s work? The tool has a parameter called <code>-bamout</code>, which makes <code>HaplotypeCaller</code> output a BAM file, or a <em>bamout</em>, showing the result of the assembly graph-based realignment it performs internally. That’s great because it means that we can see exactly what <code>HaplotypeCaller</code> saw when it made that call.<a contenteditable="false" data-primary="variant discovery in GATK" data-secondary="calling germline SNPs and indels with HaplotypeCaller" data-startref="ix_vardisGATKHCrun" data-tertiary="running HaplotypeCaller and examining output" data-type="indexterm" id="idm45625633564264"/><a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calling germline SNPs and indels with" data-startref="ix_HaCacallrun" data-tertiary="running HaplotypeCaller and examining output" data-type="indexterm" id="idm45625633562152"/><a contenteditable="false" data-primary="IGV (Integrated Genome Viewer)" data-secondary="viewing HaplotypeCaller output in" data-startref="ix_IGVHAout" data-type="indexterm" id="idm45625633560184"/> </p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Generating an output BAM to troubleshoot a surprising call"><div class="sect3" id="generating_an_output_bam_to_troubleshoo">
<h3>Generating an output BAM to troubleshoot a surprising call</h3>

<p>Let’s generate the bamout so that <a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calling germline SNPs and indels with" data-tertiary="generating output BAM to troubleshoot call" data-type="indexterm" id="ix_HaCacallbamout"/>we can get a <a contenteditable="false" data-primary="BAMs (Binary Alignment Maps)" data-secondary="bamout generation to troubleshoot HaplotypeCaller call" data-type="indexterm" id="ix_BAMout"/>better understanding of what <code>HaplotypeCaller</code> thinks it’s seeing here.  Run the command that follows, which should look very similar: it’s basically the same as the previous one except we changed the name of the output VCF (to avoid overwriting the first one), we narrowed the interval we’re looking at, and we specified a name for the realigned output BAM file using the new <code>-bamout</code> argument. As a reminder, we’re running this from the <em>/home/book/data/germline</em> directory:</p>

<pre data-type="programlisting">
# gatk HaplotypeCaller \
    -R ref/ref.fasta \
    -I bams/mother.bam \
    -O sandbox/mother_variants.snippet.debug.vcf \
    -bamout sandbox/mother_variants.snippet.debug.bam \
    -L 20:10,002,000-10,003,000</pre>

<p>Because we’re interested in looking at only that particular region, we give the tool a narrowed interval with <code>-L 20:10,002,000-10,003,000</code>, to make the runtime shorter.</p>

<p>After that has run successfully, copy the bamout (<em>sandbox/mother_variants.snippet.debug.bam</em>) to your bucket and then load it in IGV, as we’ve previously described. It should load in a new track at the bottom of the viewer, as shown in <a data-type="xref" href="#realigned_reads_in_the_bamout_file_left">Figure 5-5</a>. You should still be zoomed in on the same coordinates (20:10,002,294-10,002,623), and have the original <code>mother.bam</code> track loaded for comparison. If you have a small screen, you can switch the view setting for the BAM files to Collapsed by right-clicking anywhere in the window where you see sequence reads.</p>

<figure class="no-frame"><div id="realigned_reads_in_the_bamout_file_left" class="figure"><img alt="Realigned reads in the bamout file (bottom track)." src="Images/gitc_0505.png" width="1441" height="750"/>
<h6><span class="label">Figure 5-5. </span>Realigned reads in the bamout file (bottom track).</h6>
</div></figure>

<p>The bottom track shows us that after realignment by <code>HaplotypeCaller</code>, almost all the reads support the insertion, and the messy soft clips that we see in the original BAM file are gone. This confirms that <code>HaplotypeCaller</code> utilized the soft-clipped sequences during the realignment stage and took that sequence into account when calling variants. Incidentally, if you expand the reads in the output BAM (right-click &gt; Expanded view), and you can see that all of the insertions are in phase with the neighboring C/T variant. Such consistency tends to be a good sign; whereas if you had inconsistent segregation between neighboring variants, you would worry that at least one of them is an artifact.</p>

<p>So, to summarize, this shows that <code>HaplotypeCaller</code> found a different alignment after performing its local graph assembly step. The reassembled region provided <code>HaplotypeCaller</code> with enough evidence to call the indel, whereas a position-based caller like UnifiedGenotyper would have missed it.</p>

<p>That being said, there is still a bit more to the bamout than meets the eye—or, at least, what you can see in this view of IGV. Right-click the <code>mother_variants.snippet.debug.bam</code> track to open the view options menu. Select “Color alignments by,” and then choose “read group.” Your gray reads should now be colored similar to the screenshot shown in <a data-type="xref" href="#bamout_shows_artificial_haplotypes_cons">Figure 5-6</a>.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We have described most figures in a way that makes sense whether you’re reading a grayscale or color version of this book, but color coding is the feature we want to explain for figures like <a data-type="xref" data-xrefstyle="select:labelnumber" href="#bamout_shows_artificial_haplotypes_cons">5-6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#bamout_shows_support_per_haplotypedot">5-7</a>. For that reason, you can find all the color figures for the book in our <a href="https://oreil.ly/genomics-repo">online repository</a>.</p>
</div>

<figure class="no-frame"><div id="bamout_shows_artificial_haplotypes_cons" class="figure"><img alt="Bamout shows artificial haplotypes constructed by HaplotypeCaller." src="Images/gitc_0506.png" width="1441" height="254"/>
<h6><span class="label">Figure 5-6. </span>Bamout shows artificial haplotypes constructed by HaplotypeCaller.</h6>
</div></figure>

<p>Some of the first reads, those shown in in one shade at the top of the pile in <a data-type="xref" href="#bamout_shows_artificial_haplotypes_cons">Figure 5-6</a>, are not <em>real</em> reads. These represent artificial haplotypes that were constructed by <code>HaplotypeCaller</code> and are tagged with a special read group identifier, RG:Z:ArtificialHaplotypeRG, to differentiate them from actual reads. You can click an artificial read to see this tag under Read Group.</p>

<p>Interestingly, the tool seems to have considered three possible haplotypes, including two with insertions of different lengths: either two bases or three bases inserted, before ultimately choosing the three-base case as being most likely to be real. To examine this a bit more closely, let’s separate these artificial reads to the top of the track. Right-click the track, and then, on the menu that opens, select “Group alignments by” and then choose “read group.” Next let’s color the reads differently. Right-click and select “Color alignments by,” choose “tag,” and then type <code><strong>HC</strong></code>.</p>

<p><code>HaplotypeCaller</code> labels reads that have unequivocal support for a particular haplotype with an HC tag value that refers to the corresponding haplotype. Now the color shows us which reads in the lower track support which of the artificial haplotypes in the top track, as we can see in <a data-type="xref" href="#bamout_shows_support_per_haplotypedot">Figure 5-7</a>. Gray reads are unassigned, which means there was some ambiguity about which haplotype they support best.</p>

<p>This shows us that <code>HaplotypeCaller</code> was presented with more reads supporting the three-base insertion unambiguously compared to<a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calling germline SNPs and indels with" data-startref="ix_HaCacallbamout" data-tertiary="generating output BAM to troubleshoot call" data-type="indexterm" id="idm45625633523896"/> the <a contenteditable="false" data-primary="BAMs (Binary Alignment Maps)" data-secondary="bamout generation to troubleshoot HaplotypeCaller call" data-startref="ix_BAMout" data-type="indexterm" id="idm45625633521736"/>two-base insertion case.<a contenteditable="false" data-primary="single-nucleotide polymorphisms (SNPs)" data-secondary="calling germline SNPs and indels with HaplotypeCaller" data-startref="ix_SNPscall" data-type="indexterm" id="idm45625633519880"/><a contenteditable="false" data-primary="indels" data-secondary="and germline SNPs, calling with HaplotypeCaller" data-startref="ix_indelcall" data-type="indexterm" id="idm45625633518136"/><a contenteditable="false" data-primary="germline short sequence variants" data-secondary="finding with GATK HaplotypeCaller" data-startref="ix_grmlSNP" data-type="indexterm" id="idm45625633516456"/><a contenteditable="false" data-primary="variant discovery in GATK" data-secondary="calling germline SNPs and indels with HaplotypeCaller" data-startref="ix_vardisGATKHC" data-type="indexterm" id="idm45625633514776"/><a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calling germline SNPs and indels with" data-startref="ix_HaCacall" data-type="indexterm" id="idm45625633513080"/><a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="getting started with variant discovery" data-startref="ix_GATK1stVDHC" data-tertiary="calling germline SNPs and indels with HaplotypeCaller" data-type="indexterm" id="idm45625633511416"/></p>

<figure class="no-frame"><div id="bamout_shows_support_per_haplotypedot" class="figure"><img alt="Bamout shows support per haplotype." src="Images/gitc_0507.png" width="724" height="452"/>
<h6><span class="label">Figure 5-7. </span>Bamout shows support per haplotype.</h6>
</div></figure>

</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Filtering Based on Variant Context Annotations"><div class="sect2" id="filtering_based_on_variant_context_anno">
<h2>Filtering Based on Variant Context Annotations</h2>

<p>At this <a contenteditable="false" data-primary="variant discovery in GATK" data-secondary="filtering based on variant context annotations" data-type="indexterm" id="ix_vardisGATKfil"/>point, we <a contenteditable="false" data-primary="filtering" data-secondary="based on variant context annotations in GATK" data-type="indexterm" id="ix_fltrVCann"/>have a callset of potential<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="getting started with variant discovery" data-tertiary="filtering based on variant context annotations" data-type="indexterm" id="ix_GATK1stVDfil"/> variants, but<a contenteditable="false" data-primary="variant context annotations" data-secondary="filtering based on, in GATK" data-type="indexterm" id="ix_VCA"/> we know from the earlier overview of variant filtering that this callset is likely to contain many <em>false-positive calls</em>; that is, calls that are caused by technical artifacts and do not actually correspond to real biological variation. We need to get rid of as many of those as we can without losing real variants. How do we do that?</p>

<p>A commonly used approach for filtering germline short variants is to use <em>variant context annotations</em>, which are statistics captured during the variant calling process that summarize the quality and quantity of evidence that was observed for each variant. For example, some variant context annotations describe what the sequence context was like around the variant site (were there a lot of repeated bases? more GC or more AT?), how many reads covered it, how many reads covered each allele, what proportion of reads were in forward versus reverse orientation, and so on. We can choose thresholds for each annotation and set a <em>hard filtering</em> policy that says, for example, “For any given variant, if the value of this annotation is greater than a threshold value of <em>X</em>, we consider the variant to be real; otherwise, we filter it out.”</p>

<p>Let’s now look at how annotation values are distributed in some real data, how that compares to a truth set, and see what that tells us about how we can use the annotations for filtering.</p>

<section data-type="sect3" data-pdf-bookmark="Understanding variant context annotations"><div class="sect3" id="understanding_variant_context_annotatio">
<h3>Understanding variant context annotations</h3>

<p>We’re going to look at a variant callset made from the same WGS data called <code>mother.bam</code> that<a contenteditable="false" data-primary="variant context annotations" data-secondary="filtering based on, in GATK" data-tertiary="understanding variant context annotations" data-type="indexterm" id="idm45625633491480"/> we used earlier for our <code>HaplotypeCaller</code> tutorial, except here we have calls from the entire chromosome 20 instead of just a small slice. The Mother data (normally identified as sample NA12878) has been extensively studied as part of a benchmarking project carried out by a team at the National Institute of Standards and Technology (NIST), called Genome in a Bottle (GiaB). Among many other useful resources, the GiaB project has produced a highly curated truth set for the Mother data based on consensus methods involving multiple variant calling tools and pipelines. Note that the term <em>truth set</em> here is not meant as a claim of absolute truth, given that it is practically impossible to achieve absolute truth under the experimental conditions in question, but to reflect that we are very confident that <em>most</em> of the variant calls it contains are probably real and that there are probably <em>very few</em> variants missing from it.</p>

<p>Thanks to this resource, we can take a variant callset that we made ourselves from the original data and annotate each of our variants with information based on the GiaB truth set, if it is present there. If a variant in our callset is not present in the truth set, we’ll assume that it is a false positive—in other words, that we were wrong. This will not always be correct, but it is a reasonable approximation for the purpose of this section. As a result, we’ll be able to contrast the annotation value distributions for variants from our callset that we believe are real to those that we believe are false. If there is a clear difference, we should be able to derive some filtering rules.</p>

<p>Let’s pull up a few lines of the original variant callset.<a contenteditable="false" data-primary="zcat utility" data-type="indexterm" id="idm45625633485416"/> We use the common utility <code>zcat</code> (a version of <code>cat</code> that can read gzipped files) to read in the variant data.  We then pipe (|) the variant <a contenteditable="false" data-primary="| (pipe symbol), piping data between utilities" data-type="indexterm" id="idm45625633483128"/>data to another common utility, <code>grep</code>, with the instruction to<a contenteditable="false" data-primary="grep utility" data-type="indexterm" id="idm45625633481400"/> discard lines starting with <code>##</code> (i.e., header lines in the VCF). Finally, we pipe (|) the remaining lines to a text-viewing utility, head, with the instruction to display only the first three lines of the file (<code>-3</code>). As a reminder, we’re still running all of this from the <em>/home/book/data/germline</em> directory.</p>

<pre class="small" data-type="programlisting">
# zcat vcfs/motherSNP.vcf.gz | grep -v '##' | head -3
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  NA12878
20      61098   .       C       T       465.13  .
AC=1;AF=0.500;AN=2;BaseQRankSum=0.516;ClippingRankSum=0.00;DP=44;ExcessHet=3.0103;FS=0.000;
MQ=59.48;MQRankSum=0.803;QD=10.57;ReadPosRankSum=1.54;SOR=0.603
GT:AD:DP:GQ:PL0/1:28,16:44:99:496,0,938
20      61795   .       G       T       2034.16 .       AC=1;AF=0.500;AN=2;
BaseQRankSum=-6.330e-01;ClippingRankSum=0.00;DP=60;ExcessHet=3.9794;FS=0.000;MQ=59.81;
MQRankSum=0.00;QD=17.09;ReadPosRankSum=1.23;SOR=0.723        
GT:AD:DP:GQ:PL 0/1:30,30:60:99:1003,0,1027
</pre>

<p>This shows <a contenteditable="false" data-primary="Variant Content Format (VCF) files" data-secondary="output of HaplotypeCaller" data-tertiary="variant context annotations in" data-type="indexterm" id="idm45625633476632"/>you the raw VCF content, which can be a little rough to read—more challenging than when we were looking at the variant calls with IGV earlier in this chapter, right? If you need a refresher on the VCF format, feel free to pause and go back to the genomics primer in <a data-type="xref" href="ch02.xhtml#genomics_in_a_nutshell_a_primer_for_new">Chapter 2</a>, in which we explain how VCFs are structured.</p>

<p>Basically, what we’re looking for here are the strings of annotations, separated by <span class="keep-together">semicolons</span>, that were produced by <code>HaplotypeCaller</code> to summarize the properties of the data at and around the location of each variant. For example, we see that the first variant in our callset has an annotation called QD with a value of 10.57:</p>

<pre data-type="programlisting">
AC=1;AF=0.500;AN=2;BaseQRankSum=0.516;ClippingRankSum=0.00;DP=44;ExcessHet=
3.0103;FS=0.000;MQ=59.48;MQRankSum=0.803;QD=10.57;ReadPosRankSum=1.54;SOR=0.603 </pre>

<p>OK, fine, but what does that mean? Well, that’s something we discuss in a little bit. For now, we just want you to think of these as various metrics that we’re going to use to evaluate how much we trust each variant call.</p>

<p>And here are a few lines of the variant callset showing the annotations derived from the truth set, extracted by using the same command as in our previous example:</p>

<pre class="small" data-type="programlisting">
# zcat vcfs/motherSNP.giab.vcf.gz | grep -v '##' | head -3
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  INTEGRATION
20      61098   rs6078030       C       T       50      PASS
callable=CS_CGnormal_callable,CS_HiSeqPE300xfreebayes_callable;callsetnames=CGnormal,
HiSeqPE300xfreebayes,HiSeqPE300xGATK;callsets=3;datasetnames=CGnormal,HiSeqPE300x;
datasets=2;datasetsmissingcall=10XChromium,IonExome,SolidPE50x50bp,SolidSE75bp;filt=
CS_HiSeqPE300xGATK_filt,CS_10XGATKhaplo_filt,CS_SolidPE50x50GATKHC_filt;platformnames=
CG,Illumina;platforms=2GT:PS:DP:ADALL:AD:GQ    0/1:.:542:132,101:30,25:604
20      61795   rs4814683       G       T       50      PASS
callable=CS_HiSeqPE300xGATK_callable,CS_CGnormal_callable,CS_HiSeqPE300xfreebayes_
callable;callsetnames=HiSeqPE300xGATK,CGnormal,HiSeqPE300xfreebayes,10XGATKhaplo,
SolidPE50x50GATKHC,SolidSE75GATKHC;callsets=6;datasetnames=HiSeqPE300x,CGnormal,
10XChromium,SolidPE50x50bp,SolidSE75bp;datasets
=5;datasetsmissingcall=IonExome;platformnames=Illumina,CG,10X,Solid;platforms=4
GT:PS:DP:ADALL:AD:GQ    0/1:.:769:172,169:218,205:1337</pre>

<p>Did you notice that there are a lot more annotations in these calls, and they’re quite different? The annotations you see here are not actually variant context annotations in the same sense as before. These describe information at a higher level; they refer not to the actual sequence data, but to a meta-analysis that produced the calls by combining multiple callsets derived from multiple pipelines and data types.<a contenteditable="false" data-primary="callsets annotation" data-type="indexterm" id="ix_cllset"/> For example, the <code>callsets</code> annotation counts how many of the multiple callsets used for making the GiaB truth set agreed on each variant call, as an indicator of confidence. You can see that the first variant record has <code>callsets=3</code>, whereas the second record has <code>callsets=6</code>, so we will trust the second call more than the first.</p>

<p>In the next section, we use those meta-analysis annotations to inform our evaluation of our own callset.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Plotting variant context annotation data"><div class="sect3" id="plotting_variant_context_annotation_dat">
<h3>Plotting variant context annotation data</h3>

<p>We prepared some plots that show the distribution of<a contenteditable="false" data-primary="variant context annotations" data-secondary="filtering based on, in GATK" data-tertiary="plotting variant context annotation data" data-type="indexterm" id="ix_VCAplot"/> values for<a contenteditable="false" data-primary="plotting variant context annotation data" data-type="indexterm" id="ix_plotVCA"/> a few annotations that are usually very informative with regard to the quality of variant calls. We made two kinds of plots: density plots and scatter plots. Plotting the <em>density of values</em> for a single annotation enables us to see the overall range and distribution of values observed in a callset. Combining this with some basic knowledge of what each <span class="keep-together">annotation</span> represents and how it is calculated, we can make a first estimation of value thresholds <a contenteditable="false" data-primary="false positive calls, separating from true positive calls" data-type="indexterm" id="idm45625633455432"/>that segregate false-positive calls (FPs) from true-positive calls (TPs). Plotting the <em>scatter of values</em> for two annotations, one against the other, additionally shows us the trade-offs we make when setting a threshold on annotation values individually. The full protocol for generating the plots involves a few more GATK commands and some R code; they are all documented in detail in a tutorial that we reference again in <a data-type="xref" href="ch12.xhtml#interactive_analysis_in_jupyter_noteboo">Chapter 12</a> when we cover interactive analysis.</p>

<p>The annotation that people most commonly use to filter their callsets, rightly or wrongly, is the confidence score <a contenteditable="false" data-primary="QUAL annotations" data-type="indexterm" id="ix_QUAL"/>QUAL. In <a data-type="xref" href="#aright_parenthesis_density_plot_of_qual">Figure 5-8</a>, we plotted the density of the distribution observed for QUAL in our sample. It’s not terribly readable, because of a very long tail of extremely high values. What’s going on there? Well, you wouldn’t be alone in thinking that the higher the QUAL value, the better the chance that the variant is real, given that QUAL is supposed to represent our confidence in a variant call. And for the bulk of the callset, that is indeed true.<a contenteditable="false" data-primary="DP (depth of coverage) annotations" data-type="indexterm" id="ix_DPann"/> However, QUAL has a dirty secret: its value becomes horribly inflated in areas of extremely high depth of coverage. This is because each read contributes a little to the QUAL score, so variants in regions with deep coverage can have artificially inflated QUAL scores, giving the impression that the call is supported by more evidence than it really is.</p>

<figure><div id="aright_parenthesis_density_plot_of_qual" class="figure"><img alt="A) Density plot of QUAL; B) scatter plot of QUAL versus DP." src="Images/gitc_0508.png" width="1440" height="705"/>
<h6><span class="label">Figure 5-8. </span>Density plot of QUAL (left); scatter plot of QUAL versus DP (right).</h6>
</div></figure>

<p>We can see this relationship in action by making a scatter plot of QUAL against DP, the depth of coverage, shown in <a data-type="xref" href="#aright_parenthesis_density_plot_of_qual">Figure 5-8</a>. That plot shows clearly that the handful of variants with extremely high QUAL values also have extremely high DP values. If you were to look one up in IGV, you would see that those calls are basically junk—they are poorly supported by the read data and located in regions that are very messy. This tells us we can fairly confidently disregard those calls. So let’s zoom into the QUAL plot, this time restricting the x-axis to eliminate extremely high values. In <a data-type="xref" href="#density_plot_of_qual_aright_parenthesis">Figure 5-9</a>, we’re still looking at all calls in aggregate, whereas in <a data-type="xref" href="#density_plot_of_qual_aright_parenthesis">Figure 5-9</a> we stratify the calls based on how well they are supported in the GiaB callset, as indicated by the number of callsets in which they were present.</p>

<figure><div id="density_plot_of_qual_aright_parenthesis" class="figure"><img alt="Density plot of QUAL: A) all calls together; B) stratified by callsets annotation." src="Images/gitc_0509.png" width="1440" height="705"/>
<h6><span class="label">Figure 5-9. </span>Density plot of QUAL: all calls together (left); stratified by callsets <span class="keep-together">annotation</span> (right).</h6>
</div></figure>

<p>You can see on the plot that most of the low-credibility calls (under the curve that reaches the highest vertical point) are clustering at the low end of the QUAL range, but you also see that the distribution is continuous; there’s no clear separation between bad calls and good calls, limiting the usefulness of this annotation for filtering. It’s probable that the depth-driven inflation effect is still causing us trouble even at more reasonable levels of coverage. So let’s see if we can get that confounding factor out of the equation.<a contenteditable="false" data-primary="DP (depth of coverage) annotations" data-startref="ix_DPann" data-type="indexterm" id="idm45625633436872"/><a contenteditable="false" data-primary="QD (QualByDepth) annotations" data-type="indexterm" id="idm45625633435480"/> Enter QD, for QualByDepth: the QD annotation puts the variant confidence QUAL score into perspective by normalizing for the amount of coverage available, which compensates for the inflation phenomenon that we noted a moment ago.<a contenteditable="false" data-primary="QUAL annotations" data-startref="ix_QUAL" data-type="indexterm" id="idm45625633433992"/></p>

<figure><div id="density_plot_of_qd_aright_parenthesis_a" class="figure"><img alt="Density plot of QD: A) all calls together; B) stratified by callsets annotation." src="Images/gitc_0510.png" width="1440" height="705"/>
<h6><span class="label">Figure 5-10. </span>Density plot of QD: all calls together (left); stratified by callsets annotation (right).</h6>
</div></figure>

<p>Even without the truth set stratification option, we already see in <a data-type="xref" href="#density_plot_of_qd_aright_parenthesis_a">Figure 5-10</a> that the shape of the density curve suggests this annotation is going to be more helpful. And indeed, when we run with the split turned on, we see a clear pattern with more separation between “bad variants” and the rest in <a data-type="xref" href="#density_plot_of_qd_aright_parenthesis_a">Figure 5-10</a>. You see the small “shoulder” on the low end of the QD distribution? Those are all the very-low-quality variant calls that are most probably junk, as confirmed by the GiaB stratification.<a contenteditable="false" data-primary="callsets annotation" data-startref="ix_cllset" data-type="indexterm" id="idm45625633427688"/> In general, the QD value tends to be a rather reliable indicator of variant call quality.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="another_thing_thatapostrophes_neat_abou">
<h5>Interpreting the Shape of the QD Distribution</h5>

<p>Another thing that’s neat about QD is that when you’re looking at QD annotation values from a single sample, you can see the variants segregating into two camel humps. <a contenteditable="false" data-primary="homozygous-variant calls" data-type="indexterm" id="idm45625633423960"/><a contenteditable="false" data-primary="heterozygous-variant calls" data-type="indexterm" id="idm45625633422840"/>These correspond to heterozygous- versus homozygous-variant calls, and they cluster around QD values that are predictable relative to one another.</p>

<p>You can generally expect that the second hump will be centered on a value that’s twice the value of the first. This is because of the way QD is calculated: it is normalized against the amount of read depth <em>that supports the alternate allele</em>! And on average, you should expect homozygous-variant calls to have twice as much supporting depth for the alternate allele compared to heterozygous calls. So, for heterozygous calls, the denominator in the QD calculation is twice as large, and therefore the final QD value is half as much as you’d see for a homozygous-variant call.</p>

<p>This can be useful for new experiments for which you don’t have the luxury of having a truth set available. Even though you can’t easily predict what the “good” values of QD should be, you can plot the QD density distribution and make some reasonable assumptions based on its shape and where the shoulder and the camel humps line up. Just keep in mind that this shape is this clear only for a single sample, because when you add more samples, you will have many sites where some samples are heterozygous and others are homozygous, causing the humps to blend together. On the bright side, the shoulder-of-crap should become more clearly defined as you add more <span class="keep-together">samples.</span></p>
</div></aside>

<p>Because the various annotations measure different aspects of the variant context, it’s interesting to examine how well they might perform when combined. One way to approach this question is to combine the scatter and density plots we’ve been making into a single figure, like <a data-type="xref" href="#a_scatter_plot_with_marginal_densities">Figure 5-11</a>, which shows both the two-way scatter of two annotations (here, QD and DP) and their respective density curves on the sides of the scatter plot. This provides us with some insight into how probable true variants seem to cluster within the annotation space. If nothing else, the blobs of color in the middle (which is where the “goodies” are clustering) are reasonably clearly demarcated from the gray smear of likely false positives on the left side of the plot.</p>

<figure><div id="a_scatter_plot_with_marginal_densities" class="figure"><img alt="A scatter plot with marginal densities of QD versus DP." src="Images/gitc_0511.png" width="480" height="480"/>
<h6><span class="label">Figure 5-11. </span>A scatter plot with marginal densities of QD versus DP.</h6>
</div></figure>

<p>So what do we do with this insight? We use it as a guide to set thresholds with reasonable confidence on each annotation, reflecting how much each of them contributes to allowing us to weed out different subsets of false-positive variants. The resulting combined filtering rule might be more powerful than the sum of its parts.<a contenteditable="false" data-primary="variant context annotations" data-secondary="filtering based on, in GATK" data-startref="ix_VCAplot" data-tertiary="plotting variant context annotation data" data-type="indexterm" id="idm45625633413528"/><a contenteditable="false" data-primary="plotting variant context annotation data" data-startref="ix_plotVCA" data-type="indexterm" id="idm45625633411480"/></p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Applying hard filters to germline SNPs and indels"><div class="sect3" id="applying_hard_filters_to_germline_snps">
<h3>Applying hard filters to germline SNPs and indels</h3>

<p>Looking at the plots<a contenteditable="false" data-primary="hard filtering tools in GATK" data-type="indexterm" id="ix_hardfil"/> in the previous section<a contenteditable="false" data-primary="variant context annotations" data-secondary="filtering based on, in GATK" data-tertiary="applying hard filters to germline SNPs and indels" data-type="indexterm" id="ix_VCAhf"/> gave us some insight into what the variant context annotations represent and how we can use them to derive some rudimentary filtering rules. Now let’s see how we can apply those rules to our callset.</p>

<p>Earlier, we looked at the QualByDepth annotation, which is captured in VCF records as QD. The plots we made showed that variants <a contenteditable="false" data-primary="VariantFiltration tool" data-type="indexterm" id="idm45625633403448"/>with a QD value below 2 are most likely not real, so let’s filter out sites with QD &lt; 2 using the GATK tool <code>VariantFiltration,</code> as follows. We give it a reference genome (<code>-R</code>), the VCF file of variants we want to filter (<code>-V</code>), and we specify an output filename (<code>-O</code>). In addition, we also provide a filtering expression stating the filtering logic to apply (here, <code>QD &lt; 2.0</code> means reject any variants <a contenteditable="false" data-primary="QD (QualByDepth) annotations" data-secondary="applying hard filtering to" data-type="indexterm" id="idm45625633399832"/>with a QD value below 2.0) and the name to give to the filter (here, <code>QD2</code>):</p>

<pre data-type="programlisting">
# gatk VariantFiltration \
    -R ref/ref.fasta \
    -V vcfs/motherSNP.vcf.gz \
    --filter-expression "QD &lt; 2.0" \
    --filter-name "QD2" \
    -O sandbox/motherSNP.QD2.vcf.gz</pre>

<p>This produces a VCF with all the original variants; those that failed the filter are annotated with the filter name in the <code>FILTER</code> column, whereas those that passed are marked <code>PASS</code>:</p>

<pre data-type="programlisting">
# zcat sandbox/motherSNP.QD2.vcf.gz | grep -v '##' | head -3
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  NA12878
20      61098   .       C       T       465.13  PASS
AC=1;AF=0.500;AN=2;BaseQRankSum=0.516;ClippingRankSum=0.00;DP=44;
ExcessHet=3.0103;FS=0.000;MQ=59.48;MQRankSum=0.803;QD=10.57;
ReadPosRankSum=1.54;SOR=0.603GT:AD:DP:GQ:PL  0/1:28,16:44:99:496,0,938
20      61795   .       G       T       2034.16 PASS   
AC=1;AF=0.500;AN=2;BaseQRankSum=-6.330e-01;ClippingRankSum=0.00;DP=60;
ExcessHet=3.9794;FS=0.000;MQ=59.81;MQRankSum=0.00;QD=17.09;
ReadPosRankSum=1.23;SOR=0.723        GT:AD:DP:GQ:PL	0/1:30,30:60:99:1003,0,1027
</pre>

<p>The two variant records shown above both passed the filtering criteria, so their <code>FILTER</code> column shows <code>PASS</code>. Try viewing more records in the file to find some that have been filtered. You can do this in the terminal or in IGV, as you prefer.</p>

<p>Now, what if we wanted to combine filters? We could complement the QD filter by also filtering out the calls with extremely high DP because we saw those are heavily associated with false positives. <a contenteditable="false" data-primary="DP (depth of coverage) annotations" data-secondary="applying hard filtering to" data-type="indexterm" id="idm45625633391976"/>We could run <code>VariantFiltration</code> again on the previous output with the new filter, or we could go back to the original data and apply both filters in one go like this:</p>

<pre data-type="programlisting">
# gatk VariantFiltration \
    -R ref/ref.fasta \
    -V vcfs/motherSNP.vcf.gz \
    --filter-expression "QD &lt; 2.0 || DP &gt; 100.0" \
    --filter-name "lowQD_highDP" \
    -O sandbox/motherSNP.QD2.DP100.vcf.gz</pre>

<p>The double pipe (<code>||</code>) in the filtering expression is equivalent to a logical OR operator.<a contenteditable="false" data-primary="|| (logical OR) operator in GATK filtering expressions" data-type="indexterm" id="idm45625633387896"/> You can<a contenteditable="false" data-primary="logical OR (||) operator in GATK filtering expressions" data-type="indexterm" id="idm45625633386488"/> use it any number of times, and you can also use <code>&amp;&amp;</code> for the logical AND operator, but  <a contenteditable="false" data-primary="&amp;&amp; (logical AND) operator in GATK filtering commands" data-type="indexterm" id="idm45625633384776"/>beware<a contenteditable="false" data-primary="logical AND operator (&amp;&amp;) in GATK filtering commands" data-type="indexterm" id="idm45625633383448"/> of building overly complex filtering queries. Sometimes, it is better to keep it simple and split your queries into separate components, both for clarity and for testing purposes.</p>

<p>The GATK hard-filtering tools offer various options for tuning these commands, including deciding how to handle missing values. Keep in mind, however, that hard filtering is a relatively simplistic filtering technique, and it is not considered part of the GATK Best Practices. The advantage of this approach is that it is fairly intuitive if you have a good understanding of what the annotations mean and how they are calculated. In fact, it can be very effective in the hands of an expert analyst. However, it gives you a lot of dials to tune, can be overwhelming for newcomers, and tends to require new analytical work for every new project because the filtering thresholds that are appropriate for one dataset are often not directly applicable to other datasets.</p>

<p>We included hard filtering here because it’s an accessible way to explore what happens when you apply filters to a callset. For most “real” work, we recommend applying the appropriate machine learning approaches covered in <a data-type="xref" href="ch06.xhtml#best_practices_for_germline_short_varia">Chapter 6</a>.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This concludes the hands-on portion of this chapter, so you can stop your VM now, as discussed in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>. We don’t recommend<a contenteditable="false" data-primary="variant context annotations" data-secondary="filtering based on, in GATK" data-startref="ix_VCAhf" data-tertiary="applying hard filters to germline SNPs and indels" data-type="indexterm" id="idm45625633376920"/><a contenteditable="false" data-primary="filtering" data-secondary="based on variant context annotations in GATK" data-startref="ix_fltrVCann" data-type="indexterm" id="idm45625633374824"/><a contenteditable="false" data-primary="variant discovery in GATK" data-secondary="filtering based on variant context annotations" data-startref="ix_vardisGATKfil" data-type="indexterm" id="idm45625633373144"/> deleting<a contenteditable="false" data-primary="variant context annotations" data-secondary="filtering based on, in GATK" data-startref="ix_VCA" data-type="indexterm" id="idm45625633371320"/><a contenteditable="false" data-primary="hard filtering tools in GATK" data-startref="ix_hardfil" data-type="indexterm" id="idm45625633369608"/> it, because<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="getting started with variant discovery" data-startref="ix_GATK1stVDfil" data-tertiary="filtering based on variant context annotations" data-type="indexterm" id="idm45625633368088"/> you <a contenteditable="false" data-primary="variant discovery in GATK" data-startref="ix_vardisGATK" data-type="indexterm" id="idm45625633365944"/>will need it in the<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="getting started with variant discovery" data-startref="ix_GATK1stVD" data-type="indexterm" id="idm45625633364392"/> next chapter. </p>
</div>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Introducing the GATK Best Practices"><div class="sect1" id="introducing_the_gatk_best_practices">
<h1>Introducing the GATK Best Practices</h1>

<p>It’s all well and good to know how to run individual tools and work through a slice of an example as<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="introducing Best Practices workflows" data-type="indexterm" id="ix_GATKBP"/> we<a contenteditable="false" data-primary="Best Practices workflows (GATK)" data-type="indexterm" id="ix_BPwkf"/> did in this chapter, but that’s very different from doing an actual end-to-end analysis. That’s where the GATK Best Practices come in.</p>

<p>The GATK Best Practices are workflows developed by the GATK team for performing variant discovery analysis in high-throughput sequencing data.<a contenteditable="false" data-primary="variant discovery in GATK" data-secondary="analysis in high-throughput sequencing data, Best Practices workflows for" data-type="indexterm" id="idm45625633356664"/> These workflows describe the data processing and analysis steps that are necessary to produce high-quality variant calls, which can then be used in a variety of downstream applications. As of this writing, there are GATK Best Practices workflows for four standard types of variant discovery use cases—germline short variants, somatic short variants, germline copy-number variants, and somatic copy-number alterations—plus a few specialized use cases—mitochondrial analysis, metagenomic analysis, and liquid biopsy analysis. All GATK workflows currently designated “Best Practices” are designed to operate on DNA sequencing data, though the GATK methods development team has recently started a new project to extend the scope of GATK to the RNAseq world.</p>

<p>The GATK Best Practices started out as simply a series of recommendations, with example command lines showing how the tools involved should typically be invoked. More recently, the GATK team has started providing reference implementations for each specific use case in the form of scripts written in WDL, which we <a contenteditable="false" data-primary="Workflow Description Language (WDL)" data-secondary="Best Practices workflows in GATK" data-type="indexterm" id="idm45625633353640"/>cover in detail in <a data-type="xref" href="ch08.xhtml#automating_analysis_execution_with_work">Chapter 8</a>. The reference implementation WDLs demonstrate how to achieve optimal scientific results given certain inputs. Keep in mind, however, that those implementations are optimized for runtime performance and cost-effectiveness on the computational platform used by Broad Institute, which we use later in this book, but other implementations might produce functionally equivalent results.</p>

<p>Finally, it’s important to keep in mind that although the team validates its workflow implementations quite extensively, at this time it does so almost exclusively on human data produced with Illumina short read technology. So if you are working with different types of data, organisms, or experimental designs, you might need to adapt certain branches of the workflow as well as certain parameter selections, values, and accessory resources such as databases of previously identified variants. For help with these topics, we recommend consulting the GATK website and support forum, which is populated by a large and active community of researchers and bioinformaticians as well as the GATK support staff, who are responsive to questions.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="best_practices_evolution_and_deviations">
<h5>Best Practices: Evolution and Deviations</h5>

<p>The term <em>GATK Best Practices</em> carries specific meaning and corresponds to specific actionable recommendations that change over time.<a contenteditable="false" data-primary="Best Practices workflows (GATK)" data-secondary="evolution and deviations" data-type="indexterm" id="idm45625633347176"/> If someone hands you a script and tells you, “This runs the GATK Best Practices,” start by asking which version of GATK it uses, when it was written, and what key steps it includes. Both the software and the usage recommendations evolve in step with the rapid pace of technological and methodological innovation in the field of genomics; so, what was Best Practice last year (let alone in 2010) might no longer be applicable.</p>

<p>Furthermore, any GATK workflow that has been significantly adapted or customized, whether for performance reasons or to fit a use case that the GATK team does not explicitly cover, should be referred to as “based on” or “adapted from” GATK Best Practices. When in doubt about whether a particular customization constitutes a significant divergence, you are welcome to reach out to the GATK support team, which will review the available information and help you evaluate its significance and possible consequences for your work.</p>
</div></aside>

<section data-type="sect2" data-pdf-bookmark="Best Practices Workflows Covered in This Book"><div class="sect2" id="best_practices_workflows_covered_in_thi">
<h2>Best Practices Workflows Covered in This Book</h2>

<p>In this book, we cover <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="introducing Best Practices workflows" data-tertiary="Best Practices workflows covered in this book" data-type="indexterm" id="idm45625633342152"/>the following GATK Best Practices <a contenteditable="false" data-primary="Best Practices workflows (GATK)" data-secondary="covered in this book" data-type="indexterm" id="idm45625633340216"/>workflows in detail, as represented in <a data-type="xref" href="Images/#table_of_standard_variant_discovery_use">Figure 5-12</a>:</p>

<ul>
	<li>
	<p>Germline short<a contenteditable="false" data-primary="germline short sequence variants" data-secondary="Best Practices workflow" data-type="indexterm" id="idm45625633336600"/> sequence variants (<a data-type="xref" href="ch06.xhtml#best_practices_for_germline_short_varia">Chapter 6</a>)</p>
	</li>
	<li>
	<p>Somatic short <a contenteditable="false" data-primary="somatic short sequence alterations, Best Practices workflow" data-type="indexterm" id="idm45625633332952"/>sequence alterations (<a data-type="xref" href="ch07.xhtml#gatk_best_practices_for_somatic_variant">Chapter 7</a>)</p>
	</li>
	<li>
	<p>Somatic <a contenteditable="false" data-primary="somatic copy-number alterations, GATK Best Practices" data-type="indexterm" id="idm45625633329688"/>copy-number alterations (<a data-type="xref" href="ch07.xhtml#gatk_best_practices_for_somatic_variant">Chapter 7</a>)</p>
	</li>
</ul>

<figure><div id="table_of_standard_variant_discovery_use" class="figure"><img alt="Table of standard variant discovery use cases covered by GATK Best Practices." src="Images/gitc_0512.png" width="1440" height="476"/>
<h6><span class="label">Figure 5-12. </span>Table of standard variant discovery use cases covered by GATK Best <span class="keep-together">Practices.</span></h6>
</div></figure>

<section data-type="sect2" data-pdf-bookmark="Other Major Use Cases"><div class="sect2" id="other_major_use_cases">
<h2>Other Major Use Cases</h2>

<p>If you are interested in the following use cases, we suggest that you visit the <a href="https://oreil.ly/3LqiZ">Best Practices section of the GATK website</a>, which<a contenteditable="false" data-primary="Best Practices workflows (GATK)" data-secondary="other major use cases" data-type="indexterm" id="idm45625633322024"/> offers <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="introducing Best Practices workflows" data-tertiary="other major use cases" data-type="indexterm" id="idm45625633320504"/>documentation for all GATK Best Practices pipelines that are currently available or in <span class="keep-together">development</span>:</p>

<ul>
	<li>
	<p>Germline copy number variation</p>
	</li>
	<li>
	<p>Structural variation</p>
	</li>
	<li>
	<p>Mitochondrial variation</p>
	</li>
	<li>
	<p>Blood biopsy</p>
	</li>
	<li>
	<p>Pathogen/contaminant identification</p>
	</li>
</ul>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Wrap-Up and Next Steps"><div class="sect1" id="wrap_up_and_next_steps-id00002">
<h1>Wrap-Up and Next Steps</h1>

<p>In this chapter, we introduced you to GATK, the Broad Institute’s popular open source software package for genomic analysis.<a contenteditable="false" data-primary="Best Practices workflows (GATK)" data-startref="ix_BPwkf" data-type="indexterm" id="idm45625633310920"/><a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="introducing Best Practices workflows" data-startref="ix_GATKBP" data-type="indexterm" id="idm45625633309576"/> You learned how to compose GATK commands and practiced running them in a GATK container on your VM in GCP. Then, we walked you through an introductory example of germline short variant analysis with <code>HaplotypeCaller</code> on some real data, and you got a taste of how to <span class="keep-together">interpret</span> and investigate variant calls with the open source IGV. We also discussed variant filtering and talked through some of the concepts and basic methodology involved. Finally, we introduced the GATK Best Practices workflows, which feature heavily in the next few chapters. In fact, the time has come for you to tackle your first end-to-end genomic analysis: the GATK Best Practices for germline short variant discovery, from unaligned sequence read data all the way to an expertly filtered set of variant calls that can be used for downstream analysis. Let’s head over to <a data-type="xref" href="ch06.xhtml#best_practices_for_germline_short_varia">Chapter 6</a> and get started!</p>
</div></section>
</div></section></div>



  </body></html>