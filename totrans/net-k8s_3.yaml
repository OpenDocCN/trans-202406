- en: Chapter 3\. Container Networking Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 容器网络基础知识
- en: Now that we’ve discussed networking basics and Linux networking, we’ll discuss
    how networking is implemented in containers. Like networking, containers have
    a long history. This chapter will review the history, discuss various options
    for running containers, and explore the networking setup available. The industry,
    for now, has settled on Docker as the container runtime standard. Thus, we’ll
    dive into the Docker networking model, explain how the CNI differs from the Docker
    network model, and end the chapter with examples of networking modes with Docker
    containers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了网络基础知识和Linux网络，接下来我们将讨论容器中网络是如何实现的。与网络类似，容器也有着悠久的历史。本章将回顾历史，讨论运行容器的各种选项，并探索可用的网络设置。目前，行业已经将Docker作为容器运行时的标准。因此，我们将深入探讨Docker网络模型，解释CNI与Docker网络模型的区别，并通过Docker容器的网络模式示例结束本章。
- en: Introduction to Containers
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器简介
- en: In this section, we will discuss the evolution of running applications that
    has led us to containers. Some, rightfully, will talk about containers as not
    being real. They are yet another abstraction of the underlying technology in the
    OS kernel. Being technically right misses the point of the technology and leads
    us nowhere down the road of solving the hard problem that is application management
    and deployment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论导致我们使用容器运行应用程序的演变过程。一些人会说容器不是真正的。它们只是操作系统内核底层技术的另一种抽象。技术上是正确的，但却忽略了技术的实质，也没有助于解决应用程序管理和部署这一艰难问题的路程。
- en: Applications
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序
- en: 'Running applications has always had its challenges. There are many ways to
    serve applications nowadays: in the cloud, on-prem, and, of course, with containers.
    Application developers and system administrators face many issues, such as dealing
    with different versions of libraries, knowing how to complete deployments, and
    having old versions of the application itself. For the longest time, developers
    of applications had to deal with these issues. Bash scripts and deployment tools
    all have their drawbacks and issues. Every new company has its way of deploying
    applications, so every new developer has to learn these techniques. Separation
    of duties, permissions controls, and maintaining system stability require system
    administrators to limit access to developers for deployments. Sysadmins also manage
    multiple applications on the same host machine to drive up that machine’s efficiency,
    thus creating contention between developers wanting to deploy new features and
    system administrators wanting to maintain the whole ecosystem’s stability.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 运行应用程序始终具有其挑战性。现在有很多种方法来提供应用程序：云端、本地部署，当然还有容器。应用程序开发人员和系统管理员面临许多问题，例如处理不同版本的库、完成部署的方法以及管理旧版本应用程序本身。长期以来，应用程序开发人员不得不处理这些问题。Bash脚本和部署工具都有其缺点和问题。每家新公司都有自己的应用程序部署方式，因此每个新开发人员都必须学习这些技术。分工、权限控制和维护系统稳定性要求系统管理员限制开发人员对部署的访问。系统管理员还需管理同一主机上的多个应用程序，以提高该机器的效率，从而在开发人员希望部署新功能与系统管理员希望维护整个生态系统之间造成竞争。
- en: A general-purpose OS supports as many types of applications as possible, so
    its kernel includes all kinds of drivers, protocol libraries, and schedulers.
    [Figure 3-1](#img-begining) shows one machine, with one operating system, but
    there are many ways to deploy an application to that host. Application deployment
    is a problem all organizations must solve.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通用操作系统支持尽可能多类型的应用程序，因此其内核包括各种驱动程序、协议库和调度程序。[图 3-1](#img-begining)展示了一台机器，配备一种操作系统，但是有多种方法可以将应用程序部署到该主机上。应用程序部署是所有组织都必须解决的问题。
- en: '![neku 0301](Images/neku_0301.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0301](Images/neku_0301.png)'
- en: Figure 3-1\. Application server
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1 应用服务器
- en: From a networking perspective, with one operating system, there is one TCP/IP
    stack. That single stack creates issues with port conflicts on the host machine.
    System administrators host multiple applications on the same machine to increase
    the machine’s utilization, and each application will have to run on its port.
    So now, the system administrators, the application developers, and the network
    engineers have to coordinate all of this together. More tasks to add to the deployment
    checklist are creating troubleshooting guides and dealing with all the IT requests.
    Hypervisors are a way to increase one host machine’s efficiency and remove the
    one operating system/networking stack issues.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络的角度来看，一个操作系统只有一个 TCP/IP 堆栈。这一单一堆栈会在主机机器上创建端口冲突问题。系统管理员在同一台机器上托管多个应用程序，每个应用程序必须在其端口上运行。因此，现在，系统管理员、应用程序开发人员和网络工程师都必须共同协调这一切。增加部署清单中的任务包括创建故障排除指南并处理所有
    IT 请求。虚拟化软件是提高一台主机机器效率、解决单一操作系统/网络堆栈问题的一种方法。
- en: Hypervisor
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化软件
- en: A hypervisor emulates hardware resources, CPU, and memory from a host machine
    to create guest operating systems or virtual machines. In 2001, VMware released
    its x86 hypervisor; earlier versions included IBM’s z/Architecture and FreeBSD
    jails. The year 2003 saw the release of Xen, the first open source hypervisor,
    and in 2006 Kernel-based Virtual Machine (KVM) was released. A hypervisor allows
    system administrators to share the underlying hardware with multiple guest operating
    systems; [Figure 3-2](#img-hypervisor) demonstrates this. This resource sharing
    increases the host machine’s efficiency, alleviating one of the sysadmins issues.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化软件从主机机器模拟硬件资源、CPU 和内存，创建客户操作系统或虚拟机。2001 年，VMware 发布了其 x86 虚拟化软件；早期版本包括 IBM
    的 z/Architecture 和 FreeBSD jails。2003 年发布了第一个开源虚拟化软件 Xen，2006 年发布了基于内核的虚拟化软件（KVM）。虚拟化软件允许系统管理员与多个客户操作系统共享底层硬件；[图 3-2](#img-hypervisor)
    展示了这一点。这种资源共享提高了主机机器的效率，缓解了系统管理员的问题。
- en: Hypervisors also gave each application development team a separate networking
    stack, removing the port conflict issues on shared systems. For example, team
    A’s Tomcat application can run on port 8080, while team B’s can also run on port
    8080 since each application can now have its guest operating system with a separate
    network stack. Library versions, deployment, and other issues remain for the application
    developer. How can they package and deploy everything their application needs
    while maintaining the efficiency introduced by the hypervisor and virtual machines?
    This concern led to the development of containers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化软件还为每个应用开发团队提供了独立的网络堆栈，解决了共享系统上端口冲突的问题。例如，团队 A 的 Tomcat 应用可以在端口 8080 上运行，而团队
    B 的应用也可以在端口 8080 上运行，因为每个应用现在都可以有其独立的客户操作系统和网络堆栈。但对于应用开发人员来说，仍然存在库版本、部署和其他问题。他们如何打包和部署应用程序所需的一切，同时保持虚拟化软件和虚拟机引入的效率？这一问题促使了容器的发展。
- en: '![neku 0302](Images/neku_0302.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0302](Images/neku_0302.png)'
- en: Figure 3-2\. Hypervisor
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 虚拟化软件
- en: Containers
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器
- en: In [Figure 3-3](#img-containers), we see the benefits of the containerization
    of applications; each container is independent. Application developers can use
    whatever they need to run their application without relying on underlying libraries
    or host operating systems. Each container also has its own network stack. The
    container allows developers to package and deploy applications while maintaining
    efficiencies for the host machine.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 3-3](#img-containers) 中，我们看到应用程序容器化的好处；每个容器都是独立的。应用程序开发人员可以使用他们需要的任何东西来运行他们的应用程序，而不依赖于底层库或主机操作系统。每个容器还有自己的网络堆栈。容器允许开发人员在保持主机机器效率的同时打包和部署应用程序。
- en: '![neku 0303](Images/neku_0303.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0303](Images/neku_0303.png)'
- en: Figure 3-3\. Containers running on host OS
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 运行在主机操作系统上的容器
- en: 'With any technology comes a history of changes, competitors, and innovations,
    and containers are no different. The following is a list of terms that can be
    confusing when learning about containers. First, we list the distinction between
    container runtimes, discuss each runtime’s functionality, and show how they relate
    to Kubernetes. The functionality of container runtimes breaks down to “high level”
    and “low level”:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 任何技术都有其变化、竞争和创新历史，容器也不例外。以下是学习容器时可能令人困惑的术语列表。首先，我们列出了容器运行时的区别，讨论了每个运行时的功能，并展示了它们与
    Kubernetes 的关系。容器运行时的功能可以分为“高级”和“低级”：
- en: Container
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 容器
- en: A running container image.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 运行中的容器镜像。
- en: Image
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图像
- en: A container image is the file that is pulled down from a registry server and
    used locally as a mount point when starting a container.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 容器镜像是从注册服务器拉取并在本地作为挂载点使用的文件。
- en: Container engine
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 容器引擎
- en: A container engine accepts user requests via command-line options to pull images
    and run a container.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 容器引擎通过命令行选项接受用户请求来拉取镜像并运行容器。
- en: Container runtime
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时
- en: The container runtime is the low-level piece of software in a container engine
    that deals with running a container.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时是容器引擎中处理运行容器的底层软件组件。
- en: Base image
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基础镜像
- en: A starting point for container images; to reduce build image sizes and complexity,
    users can start with a base image and make incremental changes on top of it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 容器镜像的起点；为了减少构建镜像的大小和复杂性，用户可以从基础镜像开始，并在其上进行增量更改。
- en: Image layer
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像层
- en: Repositories are often referred to as images or container images, but actually
    they are made up of one or more layers. Image layers in a repository are connected
    in a parent-child relationship. Each image layer represents changes between itself
    and the parent layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库通常被称为镜像或容器镜像，但实际上它们由一个或多个层组成。仓库中的镜像层以父子关系连接。每个镜像层代表其与父层之间的变化。
- en: Image format
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像格式
- en: Container engines have their own container image format, such as LXD, RKT, and
    Docker.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 容器引擎具有自己的容器镜像格式，如LXD、RKT和Docker。
- en: Registry
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注册表
- en: A registry stores container images and allows for users to upload, download,
    and update container images.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注册表存储容器镜像，并允许用户上传、下载和更新容器镜像。
- en: Repository
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库
- en: Repositories can be equivalent to a container image. The important distinction
    is that repositories are made up of layers and metadata about the image; this
    is the manifest.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库可以等同于一个容器镜像。重要的区别在于仓库由层和有关镜像的元数据组成；这是清单。
- en: Tag
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 标签
- en: A tag is a user-defined name for different versions of a container image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是容器镜像不同版本的用户定义名称。
- en: Container host
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 容器主机
- en: The container host is the system that runs the container with a container engine.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 容器主机是运行带有容器引擎的系统。
- en: Container orchestration
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排
- en: This is what Kubernetes does! It dynamically schedules container workloads for
    a cluster of container hosts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Kubernetes的工作方式！它动态地为容器主机集群调度容器工作负载。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Cgroups and namespaces are Linux primitives to create containers; they are discussed
    in the next section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Cgroups和命名空间是用于创建容器的Linux原语；它们将在下一节中讨论。
- en: 'An example of “low-level” functionality is creating cgroups and namespaces
    for containers, the bare minimum to run one. Developers require more than that
    when working with containers. They need to build and test containers and deploy
    them; these are considered a “high-level” functionality. Each container runtime
    offers various levels of functionality. The following is a list of high and low
    functionality:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: “低级”功能的一个示例是为容器创建cgroups和命名空间，这是运行容器的最低要求。开发人员在处理容器时需要更多功能。他们需要构建和测试容器，并将其部署；这些被认为是“高级”功能。每个容器运行时都提供各种功能级别。以下是高级和低级功能的列表：
- en: Low-level container runtime functionality
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 低级容器运行时功能
- en: Creating containers
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建容器
- en: Running containers
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行容器
- en: High-level container runtime functionality
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 高级容器运行时功能
- en: Formatting container images
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式化容器镜像
- en: Building container images
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建容器镜像
- en: Managing container images
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理容器镜像
- en: Managing instances of containers
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理容器实例
- en: Sharing container images
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享容器镜像
- en: 'Over the next few pages, we will discuss runtimes that implement the previous
    functionality. Each of the following projects has its strengths and weaknesses
    to provide high- and low-level functionality. Some are good to know about for
    historical reasons but no longer exist or have merged with other projects:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几页中，我们将讨论实现前述功能的运行时。以下每个项目都有其优点和缺点，以提供高级和低级功能。一些项目因历史原因而值得了解，但已不复存在或已与其他项目合并：
- en: Low-level container runtimes
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 低级容器运行时
- en: LXC
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LXC
- en: C API for creating Linux containers
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建Linux容器的C API
- en: runC
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: runC
- en: CLI for OCI-compliant containers
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: OCI兼容容器的命令行界面
- en: High-level container runtimes
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 高级容器运行时
- en: containerd
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: containerd
- en: Container runtime split off from Docker, a graduated CNCF project
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从Docker分离出来的容器运行时，一个毕业的CNCF项目
- en: CRI-O
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CRI-O
- en: Container runtime interface using the Open Container Initiative (OCI) specification,
    an incubating CNCF project
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开放容器倡议（OCI）规范的容器运行时接口，一个孵化中的CNCF项目
- en: Docker
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Docker
- en: Open source container platform
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 开源容器平台
- en: lmctfy
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: lmctfy
- en: Google containerization platform
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Google 容器化平台
- en: rkt
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: rkt
- en: CoreOS container specification
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 容器规范
- en: OCI
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OCI
- en: OCI promotes common, minimal, open standards, and specifications for container
    technology.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: OCI 促进容器技术的通用、最小、开放标准和规范。
- en: 'The idea for creating a formal specification for container image formats and
    runtimes allows a container to be portable across all major operating systems
    and platforms to ensure no undue technical barriers. The three values guiding
    the OCI project are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 创建容器图像格式和运行时的正式规范的想法允许容器在所有主要操作系统和平台之间可移植，以确保没有不必要的技术障碍。 OCI 项目的三个价值观如下：
- en: Composable
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可组合
- en: Tools for managing containers should have clean interfaces. They should also
    not be bound to specific projects, clients, or frameworks and should work across
    all platforms.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 管理容器的工具应具有清晰的界面。它们也不应绑定到特定的项目、客户端或框架，并且应该在所有平台上工作。
- en: Decentralized
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化
- en: The format and runtime should be well specified and developed by the community,
    not one organization. Another goal of the OCI project is independent implementations
    of tools to run the same container.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 格式和运行时应由社区明确定义和开发，而不是一个组织。 OCI 项目的另一个目标是独立实现工具以运行相同的容器。
- en: Minimalist
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 极简主义者
- en: The OCI spec strives to do several things well, be minimal and stable, and enable
    innovation and experimentation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OCI 规范力求做好几件事，保持简洁和稳定，促进创新和实验。
- en: Docker donated a draft for the base format and runtime. It also donated code
    for a reference implementation to the OCI. Docker took the contents of the libcontainer
    project, made it run independently of Docker, and donated it to the OCI project.
    That codebase is runC, which can be found on [GitHub](https://oreil.ly/A49v0).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 捐赠了一个基础格式和运行时的草案。它还捐赠了用于 OCI 的参考实现代码。 Docker 使用 libcontainer 项目的内容，使其独立于
    Docker 运行，并捐赠给 OCI 项目。该代码库是 runC，可以在 [GitHub](https://oreil.ly/A49v0) 找到。
- en: Let’s discuss several early container initiatives and their capabilities. This
    section will end with where Kubernetes is with container runtimes and how they
    work together.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论几个早期的容器倡议及其能力。本节将结束 Kubernetes 在容器运行时及其如何协同工作的地方。
- en: LXC
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LXC
- en: 'Linux Containers, LXC, was created in 2008\. LXC combines cgroups and namespaces
    to provide an isolated environment for running applications. LXC’s goal is to
    create an environment as close as possible to a standard Linux without the need
    for a separate kernel. LXC has separate components: the `liblxc` library, several
    programming language bindings, Python versions 2 and 3, Lua, Go, Ruby, Haskell,
    a set of standard tools, and container templates.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 容器，LXC，创建于 2008 年。LXC 结合 cgroups 和命名空间，为运行应用程序提供了一个隔离的环境。LXC 的目标是尽可能接近标准
    Linux 环境，而无需单独的内核。LXC 有独立的组件：`liblxc` 库、几种编程语言绑定、Python 版本 2 和 3、Lua、Go、Ruby、Haskell、一套标准工具和容器模板。
- en: runC
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: runC
- en: 'runC is the most widely used container runtime developed initially as part
    of Docker and was later extracted as a separate tool and library. runC is a command-line
    tool for running applications packaged according to the OCI format and is a compliant
    implementation of the OCI spec. runC uses `libcontainer`, which is the same container
    library powering a Docker engine installation. Before version 1.11, the Docker
    engine was used to manage volumes, networks, containers, images, etc. Now, the
    Docker architecture has several components, and the runC features include the
    following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: runC 是最广泛使用的容器运行时，最初作为 Docker 的一部分开发，后来作为单独的工具和库进行提取。runC 是一个命令行工具，用于运行按照 OCI
    格式打包的应用程序，并且是 OCI 规范的兼容实现。runC 使用的是 `libcontainer`，这与 Docker 引擎安装中的容器库相同。在 1.11
    版本之前，Docker 引擎用于管理卷、网络、容器、镜像等。现在，Docker 架构有几个组件，而 runC 的特性包括以下内容：
- en: Full support for Linux namespaces, including user namespaces
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全面支持 Linux 命名空间，包括用户命名空间
- en: Native support for all security features available in Linux
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 Linux 中所有可用的安全特性的本地支持
- en: SELinux, AppArmor, seccomp, control groups, capability drop, `pivot_root`, UID/GID
    dropping, etc.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SELinux、AppArmor、seccomp、控制组、能力降低、`pivot_root`、UID/GID 降低等。
- en: Native support of Windows 10 containers
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows 10 容器的本地支持
- en: Planned native support for the entire hardware manufacturer’s ecosystem
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划为整个硬件制造商生态系统提供本地支持
- en: A formally specified configuration format, governed by the OCI under the Linux
    Foundation
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 Linux 基金会的 OCI 管理的正式指定的配置格式
- en: containerd
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: containerd
- en: containerd is a high-level runtime that was split off from Docker. containerd
    is a background service that acts as an API facade for various container runtimes
    and OSs. containerd has various components that provide it with high-level functionality.
    containerd is a service for Linux and Windows that manages its host system’s complete
    container life cycle, image transfer, storage, container execution, and network
    attachment. containerd’s client CLI tool is `ctr`, and it is for development and
    debugging purposes for direct communication with containerd. containerd-shim is
    the component that allows for daemonless containers. It resides as the parent
    of the container’s process to facilitate a few things. containerd allows the runtimes,
    i.e., runC, to exit after it starts the container. This way, we do not need the
    long-running runtime processes for containers. It also keeps the standard I/O
    and other file descriptors open for the container if containerd and Docker die.
    If the shim does not run, then the pipe’s parent side would be closed, and the
    container would exit. containerd-shim also allows the container’s exit status
    to be reported back to a higher-level tool like Docker without having the container
    process’s actual parent do it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: containerd 是从 Docker 中拆分出来的一个高级运行时。containerd 是一个后台服务，作为各种容器运行时和操作系统的 API 门面。containerd
    有各种组件提供高级功能。containerd 是 Linux 和 Windows 的服务，管理其主机系统的完整容器生命周期、镜像传输、存储、容器执行和网络附加。containerd
    的客户端 CLI 工具是 `ctr`，用于开发和调试直接与 containerd 通信。containerd-shim 是允许无守护程序容器的组件。它作为容器进程的父进程驻留，以便完成几件事情。containerd
    允许运行时（如 runC）在启动容器后退出。这样，我们就不需要为容器保持长时间运行的运行时进程。它还保持容器的标准 I/O 和其他文件描述符，如果 containerd
    和 Docker 终止，则会关闭管道的父侧，容器会退出。如果 shim 未运行，则容器的退出状态也会向像 Docker 这样的高级工具报告，而不需要容器进程的实际父进程来执行此操作。
- en: lmctfy
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: lmctfy
- en: Google started lmctfy as its open source Linux container technology in 2013\.
    lmctfy is a high-level container runtime that provides the ability to create and
    delete containers but is no longer actively maintained and was porting over to
    libcontainer, which is now containerd. lmctfy provided an API-driven configuration
    without developers worrying about the details of cgroups and namespace internals.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Google 在 2013 年以其开源 Linux 容器技术 lmctfy 开始。lmctfy 是一个高级容器运行时，提供创建和删除容器的能力，但目前已不再积极维护，并且被移植到现在的
    containerd 中。lmctfy 提供了 API 驱动的配置，无需开发者关心 cgroups 和命名空间内部的细节。
- en: rkt
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: rkt
- en: rkt started at CoreOS as an alternative to Docker in 2014\. It is written in
    Go, uses pods as its basic compute unit, and allows for a self-contained environment
    for applications. rkt’s native image format was the App Container Image (ACI),
    defined in the App Container spec; this was deprecated in favor of the OCI format
    and specification support. It supports the CNI specification and can run Docker
    images and OCI images. The rkt project was archived in February 2020 by the maintainers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: rkt 在 2014 年作为 Docker 的替代品在 CoreOS 开始。它用 Go 语言编写，以 pod 作为基本计算单元，并允许为应用程序提供自包含环境。rkt
    的原生镜像格式是 App Container Image (ACI)，在 App Container 规范中定义；这已被 OCI 格式和规范支持所取代。它支持
    CNI 规范，并且可以运行 Docker 镜像和 OCI 镜像。rkt 项目由维护者在 2020 年 2 月归档。
- en: Docker
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker
- en: 'Docker, released in 2013, solved many of the problems that developers had running
    containers end to end. It has all this functionality for developers to create,
    maintain, and deploy containers:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 在 2013 年发布，解决了开发者在容器端到端运行中遇到的许多问题。它具备所有这些功能，供开发者创建、维护和部署容器：
- en: Formatting container images
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式化容器镜像
- en: Building container images
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建容器镜像
- en: Managing container images
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理容器镜像
- en: Managing instances of containers
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理容器实例
- en: Sharing container images
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享容器镜像
- en: Running containers
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行容器
- en: '[Figure 3-4](#img-docker-eng) shows us the architecture of the Docker engine
    and its various components. Docker began as a monolith application, building all
    the previous functionality into a single binary known as the *Docker engine*.
    The engine contained the Docker client or CLI that allows developers to build,
    run, and push containers and images. The Docker server runs as a daemon to manage
    the data volumes and networks for running containers. The client communicates
    to the server through the Docker API. It uses containerd to manage the container
    life cycle, and it uses runC to spawn the container process.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-4](#img-docker-eng) 展示了Docker引擎及其各个组件的架构。Docker最初是一个单体应用程序，将所有先前的功能构建成一个称为*Docker引擎*的单一二进制文件。该引擎包含了允许开发人员构建、运行和推送容器和镜像的Docker客户端或CLI。Docker服务器作为守护进程运行，用于管理运行容器的数据卷和网络。客户端通过Docker
    API与服务器通信。它使用containerd管理容器生命周期，并使用runC生成容器进程。'
- en: '![neku 0304](Images/neku_0304.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0304](Images/neku_0304.png)'
- en: Figure 3-4\. Docker engine
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. Docker引擎
- en: In the past few years, Docker has broken apart this monolith into separate components.
    To run a container, the Docker engine creates the image and passes it to containerd.
    containerd calls containerd-shim, which uses runC to run the container. Then,
    containerd-shim allows the runtime (runC in this case) to exit after it starts
    the container. This way, we can run daemonless containers because we do not need
    the long-running runtime processes for containers.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，Docker已经将这个单体应用程序拆分成了单独的组件。要运行一个容器，Docker引擎创建镜像并将其传递给containerd。containerd调用containerd-shim，后者使用runC来运行容器。然后，containerd-shim允许运行时（在这种情况下是runC）在启动容器后退出。这样，我们可以运行无守护进程的容器，因为我们不需要为容器运行长时间运行的运行时进程。
- en: Docker provides a separation of concerns for application developers and system
    administrators. It allows the developers to focus on building their apps, and
    system admins focus on deployment. Docker provides a fast development cycle; to
    test new versions of Golang for our web app, we can update the base image and
    run tests against it. Docker provides application portability between running
    on-premise, in the cloud, or in any other data center. Its motto is to build,
    ship, and run anywhere. A new container can quickly be provisioned for scalability
    and run more apps on one host machine, increasing that machine’s efficiency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Docker为应用程序开发人员和系统管理员提供了关注点分离。它允许开发人员专注于构建他们的应用程序，而系统管理员专注于部署。Docker提供了快速的开发周期；要测试我们Web应用程序的新版本的Golang，我们可以更新基础镜像并对其运行测试。Docker在本地运行、云端或任何其他数据中心中运行时提供了应用程序可移植性。其座右铭是构建、发布和随处运行。可以快速为可伸缩性而部署新的容器，并在一个主机上运行更多的应用程序，提高该主机的效率。
- en: CRI-O
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CRI-O
- en: CRI-O is an OCI-based implementation of the Kubernetes CRI, while the OCI is
    a set of specifications that container runtime engines must implement. Red Hat
    started the CRI project in 2016 and in 2019 contributed it to the CNCF. CRI is
    a plugin interface that enables Kubernetes, via `Kubelet`, to communicate with
    any container runtime that satisfies the CRI interface. CRI-O development began
    in 2016 after the Kubernetes project introduced CRI, and CRI-O 1.0 was released
    in 2017\. The CRI-O is a lightweight CRI runtime made as a Kubernetes-specific
    high-level runtime built on gRPC and Protobuf over a UNIX socket. [Figure 3-5](#cri-place)
    points out where the CRI fits into the whole picture with the Kubernetes architecture.
    CRI-O provides stability in the Kubernetes project, with a commitment to passing
    Kubernetes tests.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CRI-O是基于OCI的Kubernetes CRI实现，OCI是容器运行时引擎必须实现的一组规范。Red Hat在2016年启动了CRI项目，并于2019年将其贡献给CNCF。CRI是一个插件接口，使Kubernetes能够通过`Kubelet`与满足CRI接口的任何容器运行时通信。在Kubernetes项目引入CRI后，CRI-O的开发于2016年开始，CRI-O
    1.0于2017年发布。CRI-O是一个轻量级的CRI运行时，作为基于gRPC和Protobuf的UNIX套接字上的专用于Kubernetes的高级运行时。[图 3-5](#cri-place)
    指出了CRI在整个Kubernetes架构中的位置。CRI-O在Kubernetes项目中提供稳定性，并致力于通过Kubernetes测试。
- en: '![neku 0305](Images/neku_0305.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0305](Images/neku_0305.png)'
- en: Figure 3-5\. CRI about Kubernetes
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. Kubernetes的CRI
- en: There have been many companies, technologies, and innovations in the container
    space. This section has been a brief history of that. The industry has landed
    on making sure the container landscape remains an open OCI project for all to
    use across various ways to run containers. Kubernetes has helped shaped this effort
    as well with the adaption of the CRI-O interface. Understanding the components
    of the container is vital to all administrators of container deployments and developers
    using containers. A recent example of this importance is in Kubernetes 1.20, where
    dockershim support will be deprecated. The Docker runtime utilizing the dockershim
    for administrators is deprecated, but developers can still use Docker to build
    OCI-compliant containers to run.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器领域，已经涌现出许多公司、技术和创新。本节简要介绍了这方面的历史。行业已决定确保容器景观保持作为供所有人使用的开放 OCI 项目的状态。Kubernetes
    在这方面的努力也为此做出了贡献，通过 CRI-O 接口的采用。理解容器的组件对所有容器部署的管理员和使用容器的开发者都至关重要。最近的一个例子是 Kubernetes
    1.20 中，将停用 dockershim 支持。管理员使用 dockershim 的 Docker 运行时已被弃用，但开发者仍然可以使用 Docker 构建符合
    OCI 标准的容器来运行。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The first CRI implementation was the dockershim, which provided a layer of abstraction
    in front of the Docker engine.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 CRI 实现是 dockershim，它在 Docker 引擎前提供了一层抽象。
- en: Now we will dive deeper into the container technology that powers them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将更深入地了解支持容器技术。
- en: Container Primitives
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器基元
- en: No matter if you are using Docker or containerd, runC starts and manages the
    actual containers for them. In this section, we will review what runC takes care
    of for developers from a container perspective. Each of our containers has Linux
    primitives known as *control groups* and *namespaces*. [Figure 3-6](#namespaces)
    shows an example of what this looks like; cgroups control access to resources
    in the kernel for our containers, and namespaces are individual slices of resources
    to manage separately from the root namespaces, i.e., the host.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您使用 Docker 还是 containerd，runC 都会启动和管理它们的实际容器。在本节中，我们将从容器的角度审视 runC 为开发者处理的内容。我们的每个容器都有作为
    Linux 原语的 *控制组* 和 *命名空间*。[图 3-6](#namespaces) 展示了这一外观的示例；cgroups 控制我们容器内核资源的访问，而命名空间则是独立的资源片段，可以与根命名空间（即主机）分开管理。
- en: '![neku 0306](Images/neku_0306.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0306](Images/neku_0306.png)'
- en: Figure 3-6\. Namespaces and control groups
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 命名空间和控制组
- en: To help solidify these concepts, let’s dig into control groups and namespaces
    a bit further.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助巩固这些概念，让我们进一步深入控制组和命名空间。
- en: Control Groups
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制组
- en: 'In short, a cgroup is a Linux kernel feature that limits, accounts for, and
    isolates resource usage. Initially released in Linux 2.6.24, cgroups allow administrators
    to control different CPU systems and memory for particulate processes. Cgroups
    are provided through pseudofilesystems and are maintained by the core kernel code
    in cgroups. These separate subsystems maintain various cgroups in the kernel:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，cgroup 是 Linux 内核的一个功能，用于限制、统计和隔离资源使用。最初在 Linux 2.6.24 中发布，cgroups 允许管理员控制不同
    CPU 系统和内存的特定进程。cgroups 通过伪文件系统提供，并由 cgroups 中的核心内核代码维护。这些单独的子系统在内核中维护各种 cgroups：
- en: CPU
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CPU
- en: The process can be guaranteed a minimum number of CPU shares.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 可以保证进程至少有一定数量的 CPU 分享。
- en: Memory
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 内存
- en: These set up memory limits for a process.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置了进程的内存限制。
- en: Disk I/O
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘 I/O
- en: This and other devices are controlled via the device’s cgroup subsystem.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些和其他设备通过设备 cgroup 子系统进行控制。
- en: Network
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 网络
- en: This is maintained by the `net_cls` and marks packets leaving the cgroup.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这由 `net_cls` 维护，并标记离开 cgroup 的数据包。
- en: '`lscgroup` is a command-line tool that lists all the cgroups currently in the
    system.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`lscgroup` 是一个列出系统中当前所有 cgroups 的命令行工具。'
- en: runC will create the cgroups for the container at creation time. A cgroup controls
    how much of a resource a container can use, while namespaces control what processes
    inside the container can see.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: runC 将在容器创建时创建这些 cgroup。cgroup 控制容器可以使用的资源量，而命名空间控制容器内的进程可以看到的内容。
- en: Namespaces
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名空间
- en: 'Namespaces are features of the Linux kernel that isolate and virtualize system
    resources of a collection of processes. Here are examples of virtualized resources:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间是 Linux 内核的特性，用于隔离和虚拟化一组进程的系统资源。以下是虚拟化资源的示例：
- en: PID namespace
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: PID 命名空间
- en: Processes ID, for process isolation
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 进程 ID，用于进程隔离
- en: Network namespace
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 网络命名空间
- en: Manages network interfaces and a separate networking stack
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 管理网络接口和单独的网络堆栈
- en: IPC namespace
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: IPC 命名空间
- en: Manages access to interprocess communication (IPC) resources
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 管理对进程间通信（IPC）资源的访问
- en: Mount namespace
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 挂载命名空间
- en: Manages filesystem mount points
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 管理文件系统挂载点
- en: UTS namespace
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: UTS命名空间
- en: UNIX time-sharing; allows single hosts to have different host and domain names
    for different processes
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: UNIX时间共享；允许单个主机为不同的进程拥有不同的主机和域名
- en: UID namespaces
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: UID命名空间
- en: User ID; isolates process ownership with separate user and group assignments
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 用户ID；使用单独的用户和组分配隔离进程所有权
- en: A process’s user and group IDs can be different inside and outside a user’s
    namespace. A process can have an unprivileged user ID outside a user namespace
    while at the same time having a user ID of 0 inside the container user namespace.
    The process has root privileges for execution inside the user namespace but is
    unprivileged for operations outside the namespace.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 进程的用户和组ID在用户命名空间内外可能不同。在用户命名空间内，进程可以具有非特权用户ID，同时在容器用户命名空间内具有用户ID 0。进程在用户命名空间内具有root权限执行，但在命名空间外部进行操作时无特权。
- en: '[Example 3-1](#namespace_single) is an example of how to inspect the namespaces
    for a process. All information for a process is on the `/proc` filesystem in Linux.
    PID 1’s PID namespace is `4026531836`, and listing all the namespaces shows that
    the PID namespace IDs match.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-1](#namespace_single)展示了如何检查进程的命名空间的示例。Linux中所有进程的信息都在`/proc`文件系统中。PID
    1的PID命名空间是`4026531836`，列出所有命名空间显示PID命名空间ID匹配。'
- en: Example 3-1\. Namespaces of a single process
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-1\. 单个进程的命名空间
- en: '[PRE0]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Figure 3-7](#containers) shows that effectively these two Linux primitives
    allow application developers to control and manage their applications separate
    from the hosts and other applications either in containers or by running natively
    on the host.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-7](#containers)显示，这两个Linux原语有效地允许应用程序开发人员控制和管理其应用程序，与主机和其他应用程序分开，无论是在容器中还是通过在主机上本地运行。'
- en: '![neku 0307](Images/neku_0307.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0307](Images/neku_0307.png)'
- en: Figure 3-7\. Cgroups and namespaces powers combined
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. Cgroups和命名空间的综合作用
- en: The following examples use Ubuntu 16.04 LTS Xenial Xerus. If you want to follow
    along on your system, more information can be found in this book’s code repo.
    The repo contains the tools and configurations for building the Ubuntu VM and
    Docker containers. Let’s get started with setting up and testing our namespaces.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用Ubuntu 16.04 LTS Xenial Xerus。如果您想在您的系统上跟随操作，可以在本书的代码库中找到更多信息。该代码库包含用于构建Ubuntu虚拟机和Docker容器的工具和配置。让我们开始设置和测试我们的命名空间。
- en: Setting Up Namespaces
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置命名空间
- en: '[Figure 3-8](#root-container-namespaces) outlines a basic container network
    setup. In the following pages, we will walk through all the Linux commands that
    the low-level runtimes complete for container network creation.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-8](#root-container-namespaces)概述了基本容器网络设置。在接下来的页面中，我们将详细介绍用于容器网络创建的所有Linux命令，这些命令由低级运行时完成。'
- en: '![neku 0308](Images/neku_0308.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0308](Images/neku_0308.png)'
- en: Figure 3-8\. Root network namespace and container network namespace
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8\. 根网络命名空间和容器网络命名空间
- en: 'The following steps show how to create the networking setup shown in [Figure 3-8](#root-container-namespaces):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了如何创建[图3-8](#root-container-namespaces)所示的网络设置：
- en: Create a host with a root network namespace.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建具有根网络命名空间的主机。
- en: Create a new network namespace.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建新的网络命名空间。
- en: Create a veth pair.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建veth对。
- en: Move one side of the veth pair into a new network namespace.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将veth对的一侧移入新的网络命名空间。
- en: Address side of the veth pair inside the new network namespace.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理新网络命名空间中veth对的一侧。
- en: Create a bridge interface.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建桥接口。
- en: Address the bridge interface.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理桥接口。
- en: Attach the bridge to the host interface.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将桥接到主机接口。
- en: Attach one side of the veth pair to the bridge interface.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将veth对的一侧附加到桥接口。
- en: Profit.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获利。
- en: 'The following are all the Linux commands needed to create the network namespace,
    bridge, and veth pairs and wire them together:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建网络命名空间、桥接和veth对以及将它们连接在一起所需的所有Linux命令：
- en: '[PRE1]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s dive into an example and outline each command.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入一个示例并概述每个命令。
- en: The `ip` Linux command sets up and controls the network namespaces.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Linux命令`ip`设置并控制网络命名空间。
- en: Note
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more information about `ip` on its [man page](https://oreil.ly/jBKL7).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在其[man页](https://oreil.ly/jBKL7)上找到更多关于`ip`的信息。
- en: In [Example 3-2](#example0302), we have used Vagrant and VirtualBox to create
    a fresh installation of Ubuntu for our testing purposes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 3-2](#example0302)中，我们使用Vagrant和VirtualBox创建了一个新的Ubuntu安装，用于我们的测试目的。
- en: Example 3-2\. Ubuntu testing virtual machine
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-2\. Ubuntu 测试虚拟机
- en: '[PRE2]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Refer to the book repo for the Vagrantfile to reproduce this.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 参考书籍存储库的Vagrantfile以重现此过程。
- en: Note
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[Vagrant](https://oreil.ly/o8Qo0) is a local virtual machine manager created
    by HashiCorp.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[Vagrant](https://oreil.ly/o8Qo0) 是由 HashiCorp 创建的本地虚拟机管理器。'
- en: 'After Vagrant boots our virtual machine, we can use Vagrant to `ssh` into this
    VM:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在Vagrant启动我们的虚拟机后，我们可以使用Vagrant来`ssh`进入这个虚拟机：
- en: '[PRE3]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*IP forwarding* is an operating system’s ability to accept incoming network
    packets on one interface, recognize them for another, and pass them on to that
    network accordingly. When enabled, IP forwarding allows a Linux machine to receive
    incoming packets and forward them. A Linux machine acting as an ordinary host
    would not need to have IP forwarding enabled because it generates and receives
    IP traffic for its purposes. By default, it is turned off; let’s enable it on
    our Ubuntu instance:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*IP转发* 是操作系统接受一个接口上的传入网络数据包，识别其目的地，然后相应地传送到该网络的能力。启用IP转发允许Linux机器接收并转发传入的数据包。作为普通主机的Linux机器通常不需要启用IP转发，因为它生成和接收IP流量用于自身目的。默认情况下，它是关闭的；让我们在我们的Ubuntu实例上启用它：'
- en: '[PRE4]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With our install of the Ubuntu instance, we can see that we do not have any
    additional network namespaces, so let’s create one:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们安装的Ubuntu实例，我们可以看到我们没有任何额外的网络命名空间，所以让我们创建一个：
- en: '[PRE5]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`ip netns` allows us to control the namespaces on the server. Creating one
    is as easy as typing `ip netns add net1`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`ip netns` 允许我们在服务器上控制命名空间。创建一个像输入`ip netns add net1`这样简单：'
- en: '[PRE6]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we work through this example, we can see the network namespace we just created:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过此示例工作时，我们可以看到我们刚刚创建的网络命名空间：
- en: '[PRE7]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have a new network namespace for our container, we will need a veth
    pair for communication between the root network namespace and the container network
    namespace `net1`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为容器的新网络命名空间创建了一个新的网络命名空间，我们需要一个veth对来在根网络命名空间和容器网络命名空间`net1`之间通信。
- en: '`ip` again allows administrators to create the veth pairs with a straightforward
    command. Remember from [Chapter 2](ch02.xhtml#linux_networking) that veth comes
    in pairs and acts as a conduit between network namespaces, so packets from one
    end are automatically forwarded to the other.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`ip` 再次允许管理员使用简单命令创建veth对。请从[第2章](ch02.xhtml#linux_networking)记住，veth成对出现，充当网络命名空间之间的通道，因此，一个端口的数据包会自动转发到另一个端口。'
- en: '[PRE8]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Tip
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Interfaces 4 and 5 are the veth pairs in the command output. We can also see
    which are paired with each other, `veth1@veth0` and `veth0@veth1`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接口4和5是命令输出中的veth对。我们还可以看到它们彼此配对，`veth1@veth0` 和 `veth0@veth1`。
- en: 'The `ip link list` command verifies the veth pair creation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`ip link list` 命令验证了veth对的创建：'
- en: '[PRE9]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now let’s move `veth1` into the new network namespace created previously:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将`veth1`移入之前创建的新网络命名空间：
- en: '[PRE10]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`ip netns exec` allows us to verify the network namespace’s configuration.
    The output verifies that `veth1` is now in the network namespace `net`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`ip netns exec` 允许我们验证网络命名空间的配置。输出验证了`veth1`现在位于网络命名空间`net1`中：'
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Network namespaces are entirely separate TCP/IP stacks in the Linux kernel.
    Being a new interface and in a new network namespace, the veth interface will
    need IP addressing in order to carry packets from the `net1` namespace to the
    root namespace and beyond the host:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 网络命名空间是Linux内核中完全分离的TCP/IP堆栈。作为新接口并位于新网络命名空间中，veth接口需要IP地址以便从`net1`命名空间中的根命名空间和主机向外传递数据包：
- en: '[PRE12]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As with host networking interfaces, they will need to be “turned on”:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 与主机网络接口一样，它们需要“打开”：
- en: '[PRE13]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The state has now transitioned to `LOWERLAYERDOWN`. The status `NO-CARRIER`
    points in the right direction. Ethernet needs a cable to be connected; our upstream
    veth pair is not on yet either. The `veth1` interface is up and addressed but
    effectively still “unplugged”:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 状态现在过渡为`LOWERLAYERDOWN`。状态`NO-CARRIER`指向正确方向。以太网需要连接电缆才能连接；我们的上行veth对也尚未启动。`veth1`接口已上线且寻址，但实际上仍然处于“未连接”状态：
- en: '[PRE14]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s turn up the `veth0` side of the pair now:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打开配对的`veth0`端：
- en: '[PRE15]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now the veth pair inside the `net1` namespace is `UP`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`net1`命名空间内的veth对为`UP`：
- en: '[PRE16]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Both sides of the veth pair report up; we need to connect the root namespace
    veth side to the bridge interface. Make sure to select the interface you’re working
    with, in this case `enp0s8`; it may be different for others:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: veth对的两端都报告为up；我们需要将根命名空间的veth端连接到桥接接口。确保选择你正在使用的接口，在这种情况下是`enp0s8`；对其他情况可能不同：
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see that the enp0s8 and veth0 report are part of the bridge `br0` interface,
    `master br0 state up`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，enp0s8 和 veth0 报告是桥接接口 `br0` 的一部分，`master br0 state up`。
- en: 'Next, let’s test connectivity to our network namespace:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们测试对我们的网络命名空间的连接性：
- en: '[PRE18]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Our new network namespace does not have a default route, so it does not know
    where to route our packets for the `ping` requests:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新网络命名空间没有默认路由，因此不知道如何路由我们的 `ping` 请求的数据包：
- en: '[PRE19]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s try that again:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次：
- en: '[PRE20]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Success! We have created the bridge interface and veth pairs, migrated one to
    the new network namespace, and tested connectivity. [Example 3-3](#Namespace-Recap)
    is a recap of all the commands we ran to accomplish that.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们已创建了桥接接口和 veth 对，将其中一个迁移到新的网络命名空间，并测试了连接性。[示例 3-3](#Namespace-Recap) 是我们完成这一操作时运行的所有命令的总结。
- en: Example 3-3\. Recap network namespace creation
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-3\. 总结网络命名空间的创建
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For a developer not familiar with all these commands, that is a lot to remember
    and very easy to bork up! If the bridge information is incorrect, it could take
    down an entire part of the network with network loops. These issues are ones that
    system administrators would like to avoid, so they prevent developers from making
    those types of networking changes on the system. Fortunately, containers help
    remove the developers’ strain to remember all these commands and alleviate system
    admins’ fear of giving devs access to run those commands.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉所有这些命令的开发人员来说，这是很多需要记住的东西，而且非常容易出错！如果桥接信息不正确，可能会导致整个网络部分发生网络环路。这些问题是系统管理员希望避免的，因此他们防止开发人员在系统上进行这些类型的网络更改。幸运的是，容器帮助减少了开发人员需要记住所有这些命令的负担，并减轻了系统管理员允许开发人员运行这些命令所带来的恐惧。
- en: These  commands  are  all  needed  just  for  the  network  namespace  for 
    *every*  container  creation  and  deletion.  The  namespace  creation  in  [Example 3-3](#Namespace-Recap)
    is  the  container runtime’s job. Docker manages this for us, in its way. The
    CNI project standardizes the network creation for all systems. The CNI, much like
    the OCI, is a way for developers to standardize and prioritize specific tasks
    for managing parts of the container’s life cycle. In later sections, we will discuss
    CNI.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令对于每次容器的 *每次* 创建和删除的网络命名空间 *都是* 必要的。在 [示例 3-3](#Namespace-Recap) 中的命名空间创建是容器运行时的工作。Docker
    以其自己的方式管理这些。CNI 项目标准化了所有系统的网络创建。CNI 类似于 OCI，是开发人员标准化和优化容器生命周期特定任务的方式。在后面的部分中，我们将讨论
    CNI。
- en: Container Network Basics
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器网络基础知识
- en: The previous section showed us all the commands needed to create namespaces
    for our networking. Let’s investigate how Docker does this for us. We also only
    used the bridge mode; there several other modes for container networking. This
    section will deploy several Docker containers and examine their networking and
    explain how containers communicate externally to the host and with each other.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节展示了创建网络命名空间所需的所有命令。让我们来研究 Docker 是如何为我们完成这些操作的。我们只使用了桥接模式；容器网络还有几种其他模式。本节将部署多个
    Docker 容器，检查它们的网络，并解释容器如何与主机外部和彼此通信。
- en: 'Let’s start by discussing the several network “modes” used when working with
    containers:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论与容器一起工作时使用的几种网络“模式”开始：
- en: None
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 无
- en: No networking disables networking for the container. Use this mode when the
    container does not need network access.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 无网络禁用容器的网络访问。当容器不需要网络访问时使用此模式。
- en: Bridge
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接
- en: In bridge networking, the container runs in a private network internal to the
    host. Communication with other containers in the network is open. Communication
    with services outside the host goes through Network Address Translation (NAT)
    before exiting the host. Bridge mode is the default mode of networking when the
    `--net` option is not specified.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在桥接网络中，容器在主机内部的私有网络中运行。与网络中的其他容器通信是开放的。与主机外的服务通信在退出主机之前经过网络地址转换（NAT）。当未指定 `--net`
    选项时，桥接模式是网络的默认模式。
- en: Host
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 主机
- en: In host networking, the container shares the same IP address and the network
    namespace as that of the host. Processes running inside this container have the
    same network capabilities as services running directly on the host. This mode
    is useful if the container needs access to network resources on the hosts. The
    container loses the benefit of network segmentation with this mode of networking.
    Whoever is deploying the containers will have to manage and contend with the ports
    of services running this node.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机网络中，容器与主机共享相同的IP地址和网络命名空间。运行在这个容器内的进程具有与直接在主机上运行服务相同的网络功能。如果容器需要访问主机上的网络资源，这种模式非常有用。但是，使用这种网络模式容器将失去网络分段的好处。部署容器的人将需要管理和竞争运行在该节点上的服务的端口。
- en: Warning
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The host networking driver works only on Linux hosts. Docker Desktop for Mac
    and Windows, or Docker EE for Windows Server, does not support host networking
    mode.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 主机网络驱动仅适用于Linux主机。Docker Desktop for Mac和Windows或Docker EE for Windows Server不支持主机网络模式。
- en: Macvlan
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Macvlan
- en: 'Macvlan uses a parent interface. That interface can be a host interface such
    as eth0, a subinterface, or even a bonded host adapter that bundles Ethernet interfaces
    into a single logical interface. Like all Docker networks, Macvlan networks are
    segmented from each other, providing access within a network, but not between
    networks. Macvlan allows a physical interface to have multiple MAC and IP addresses
    using Macvlan subinterfaces. Macvlan has four types: Private, VEPA, Bridge (which
    Docker default uses), and Passthrough. With a bridge, use NAT for external connectivity.
    With Macvlan, since hosts are directly mapped to the physical network, external
    connectivity can be done using the same DHCP server and switch that the host uses.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Macvlan使用一个父接口。该接口可以是主机接口（如eth0）、子接口，甚至是将以太网接口捆绑成单个逻辑接口的绑定主机适配器。与所有Docker网络一样，Macvlan网络彼此分隔，提供网络内的访问，但不能在网络之间访问。Macvlan允许物理接口使用多个MAC和IP地址，使用Macvlan子接口。Macvlan有四种类型：私有、VEPA、桥接（Docker默认使用）和透传。使用桥接时，使用NAT进行外部连接。与Macvlan不同，由于主机直接映射到物理网络，外部连接可以使用与主机相同的DHCP服务器和交换机。
- en: Warning
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Most cloud providers block Macvlan networking. Administrative access to networking
    equipment is needed.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云提供商阻止Macvlan网络。需要管理员访问网络设备。
- en: IPvlan
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: IPvlan
- en: 'IPvlan is similar to Macvlan, with a significant difference: IPvlan does not
    assign MAC addresses to created subinterfaces. All subinterfaces share the parent’s
    interface MAC address but use different IP addresses. IPvlan has two modes, L2
    or L3\. In IPvlan, L2, or layer 2, mode is analog to the Macvlan bridge mode.
    IPvlan L3, or layer 3, mode masquerades as a layer 3 device between the subinterfaces
    and parent interface.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: IPvlan类似于Macvlan，但有一个重要区别：IPvlan不为创建的子接口分配MAC地址。所有子接口共享父接口的接口MAC地址，但使用不同的IP地址。IPvlan有两种模式，L2或L3。在IPvlan中，L2或第二层模式类似于Macvlan桥接模式。IPvlan
    L3或第三层模式伪装为子接口和父接口之间的第三层设备。
- en: Overlay
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Overlay
- en: Overlay allows for the extension of the same network across hosts in a container
    cluster. The overlay network virtually sits on top of the underlay/physical networks.
    Several open source projects create these overlay networks, which we will discuss
    later in the chapter.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Overlay允许在容器集群中跨主机扩展同一网络。Overlay网络实际上位于底层/物理网络之上。几个开源项目创建这些Overlay网络，我们将在本章后面讨论。
- en: Custom
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义
- en: Custom bridge networking is the same as bridge networking but uses a bridge
    explicitly created for that container. An example of using this would be a container
    that runs on a database bridge network. A separate container can have an interface
    on the default and database bridge, enabling it to communicate with both networks
    as needed.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义桥接网络与桥接网络相同，但使用专门为该容器创建的桥接。一个使用案例是运行在数据库桥接网络上的容器。另一个容器可以在默认和数据库桥接上有一个接口，从而能够根据需要与两个网络通信。
- en: Container-defined networking allows a container to share the address and network
    configuration of another container. This sharing enables process isolation between
    containers, where each container runs one service but where services can still
    communicate with one another on `127.0.0.1`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 容器定义的网络允许一个容器共享另一个容器的地址和网络配置。这种共享使得容器之间能够进行进程隔离，每个容器运行一个服务，但服务仍然可以在`127.0.0.1`上相互通信。
- en: 'To test all these modes, we need to continue to use a Vagrant Ubuntu host but
    now with Docker installed. Docker for Mac and Windows does not support host networking
    mode, so we must use Linux for this example. You can do this with the provisioned
    machine in [Example 1-1](ch01.xhtml#minmal_web_server_in_go) or use the Docker
    Vagrant version in the book’s code repo. The Ubuntu Docker install directions
    are as follows if you want to do it manually:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试所有这些模式，我们需要继续使用一个已安装Docker的Vagrant Ubuntu主机。Mac和Windows上的Docker不支持主机网络模式，因此我们必须在Linux上进行这个示例。您可以使用书中代码库中提供的Example
    1-1中的已配置机器，或者使用Docker Vagrant版本。如果您想手动操作，Ubuntu上安装Docker的步骤如下：
- en: '[PRE23]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now that we have the host up, let’s begin investigating the different networking
    setups we have to work with in Docker. [Example 3-4](#docker_networks) shows that
    Docker creates three network types during the install: bridge, host, and none.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启动了主机，让我们开始研究在Docker中可以使用的不同网络设置。[示例 3-4](#docker_networks)显示Docker在安装期间创建了三种网络类型：桥接、主机和无网络。
- en: Example 3-4\. Docker networks
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-4\. Docker网络
- en: '[PRE24]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The default is a Docker bridge, and a container gets attached to it and provisioned
    with an IP address in the `172.17.0.0/16` default subnet. [Example 3-5](#docker_bridge_interfaces)
    is a view of Ubuntu’s default interfaces and the Docker install that creates the
    `docker0` bridge interface for the host.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 默认是Docker桥接，容器附加到其中并配备了`172.17.0.0/16`默认子网中的IP地址。[示例 3-5](#docker_bridge_interfaces)显示了Ubuntu的默认接口以及创建`docker0`桥接接口的Docker安装情况。
- en: Example 3-5\. Docker bridge interface
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-5\. Docker桥接口
- en: '[PRE25]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](Images/1.png)](#co_container_networking_basics_CO1-1)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_container_networking_basics_CO1-1)'
- en: This is the loopback interface.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这是回环接口。
- en: '[![2](Images/2.png)](#co_container_networking_basics_CO1-2)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_container_networking_basics_CO1-2)'
- en: enp0s3 is our NAT’ed virtual box interface.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: enp0s3是我们NAT的虚拟桥接接口。
- en: '[![3](Images/3.png)](#co_container_networking_basics_CO1-3)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_container_networking_basics_CO1-3)'
- en: enp0s8 is the host interface; this is on the same network as our host and uses
    DHCP to get the `192.168.1.19` address of default Docker bridge.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: enp0s8是主机接口；它位于与我们主机相同的网络上，并使用DHCP获取`192.168.1.19`地址的默认Docker桥接。
- en: '[![4](Images/4.png)](#co_container_networking_basics_CO1-4)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_container_networking_basics_CO1-4)'
- en: The default Docker container interface uses bridge mode.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Docker容器接口使用桥接模式。
- en: '[Example 3-6](#docker_bridge) started a busybox container with the `docker
    run` command and requested that the Docker returns the container’s IP address.
    Docker default NATed address is `172.17.0.0/16`, with our busybox container getting
    `172.17.0.2`.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-6](#docker_bridge)使用`docker run`命令启动了一个忙碌的busybox容器，并请求Docker返回容器的IP地址。
    Docker的默认NAT地址是`172.17.0.0/16`，我们的busybox容器得到的是`172.17.0.2`。'
- en: Example 3-6\. Docker bridge
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-6\. Docker桥接
- en: '[PRE26]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The host networking in [Example 3-7](#docker_host_networking) shows that the
    container shares the same network namespace as the host. We can see that the interfaces
    are the same as that of the host; enp0s3, enp0s8, and docker0 are present in the
    container `ip a` command output.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-7](#docker_host_networking)中的主机网络显示，容器与主机共享相同的网络命名空间。我们可以看到接口与主机的相同；enp0s3、enp0s8和docker0都存在于容器的`ip
    a`命令输出中。'
- en: Example 3-7\. Docker host networking
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-7\. Docker主机网络
- en: '[PRE27]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'From the veth bridge example previously set up, let’s see how much simpler
    it is when Docker manages that for us. To view this, we need a process to keep
    the container running. The following command starts up a busybox container and
    drops into an `sh` command line:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 根据先前设置的veth桥接示例，让我们看看当Docker为我们管理时，它是多么简单。为了查看这一点，我们需要一个进程来保持容器运行。以下命令启动一个busybox容器并进入`sh`命令行：
- en: '[PRE28]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We have a loopback interface, `lo`, and an Ethernet interface `eth0` connected
    to veth12, with a Docker default IP address of `172.17.0.2`. Since our previous
    command only outputted an `ip a` result and the container exited afterward, Docker
    reused the IP address `172.17.0.2` for the running busybox container:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个回环接口`lo`，和一个连接到veth12的以太网接口`eth0`，Docker的默认IP地址为`172.17.0.2`。由于我们之前的命令仅输出了一个`ip
    a`结果，容器随后退出，Docker重新使用IP地址`172.17.0.2`用于运行的busybox容器：
- en: '[PRE29]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Running the `ip r` inside the container’s network namespace, we can see that
    the container’s route table is automatically set up as well:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器的网络命名空间中运行`ip r`，我们可以看到容器的路由表也自动设置好了：
- en: '[PRE30]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we open a new terminal and `vagrant ssh` into our Vagrant Ubuntu instance
    and run the `docker ps` command, it shows all the information in the running busybox
    container:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在新终端中打开并通过 `vagrant ssh` 进入我们的 Vagrant Ubuntu 实例，并运行 `docker ps` 命令，它将显示运行中的
    busybox 容器的所有信息：
- en: '[PRE31]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can see the veth interface Docker set up for the container `veth68b6f80@if11`
    on the same host’s networking namespace. It is a member of the bridge for `docker0`
    and is turned on `master docker0 state UP`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在同一主机的网络命名空间中看到 Docker 为容器 `veth68b6f80@if11` 设置的 veth 接口。它是 `docker0` 桥的成员，并且状态为
    `master docker0 state UP`：
- en: '[PRE32]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The Ubuntu host’s route table shows Docker’s routes for reaching containers
    running on the host:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Ubuntu 主机的路由表显示 Docker 用于访问在主机上运行的容器的路由：
- en: '[PRE33]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'By default, Docker does not add the network namespaces it creates to `/var/run`
    where `ip netns list` expects newly created network namespaces. Let’s work through
    how we can see those namespaces now. Three steps are required to list the Docker
    network namespaces from the `ip` command:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Docker 不会将它创建的网络命名空间添加到 `/var/run`，而 `ip netns list` 期望新创建的网络命名空间在其中。现在让我们通过三个步骤列出
    Docker 网络命名空间：
- en: Get the running container’s PID.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取运行容器的 PID。
- en: Soft link the network namespace from `/proc/PID/net/` to `/var/run/netns`.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网络命名空间软链接从 `/proc/PID/net/` 到 `/var/run/netns`。
- en: List the network namespace.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出网络命名空间。
- en: '`docker ps` outputs the container ID needed to inspect the running PID on the
    host PID namespace:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker ps` 输出了需要在主机 PID 命名空间中检查的运行中容器的容器 ID：'
- en: '[PRE34]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`docker inspect` allows us to parse the output and get the host’s process’s
    PID. If we run `ps -p` on the host PID namespace, we can see it is running `sh`,
    which tracks our `docker run` command:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker inspect` 允许我们解析输出并获取主机进程的 PID。如果我们在主机 PID 命名空间中运行 `ps -p`，我们可以看到它正在运行
    `sh`，跟踪我们的 `docker run` 命令：'
- en: '[PRE35]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`1f3f62ad5e02` is the container ID, and `25719` is the PID of the busybox container
    running `sh`, so now we can create a symbolic link for the container’s network
    namespace created by Docker to where `ip` expects with the following command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`1f3f62ad5e02` 是容器 ID，`25719` 是运行 `sh` 的 busybox 容器的 PID，因此现在我们可以为 Docker 创建的容器网络命名空间创建一个符号链接，使其与
    `ip` 期望的位置对应：'
- en: '[PRE36]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When using the container ID and process ID from the examples, keep in mind they
    will be different on your systems.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用示例中的容器 ID 和进程 ID 时，请记住它们可能在您的系统上有所不同。
- en: 'Now the `ip netns exec` commands return the same IP address, `172.17.0.2`,
    that the `docker exec` command does:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `ip netns exec` 命令返回与 `docker exec` 命令相同的 IP 地址 `172.17.0.2`：
- en: '[PRE37]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can verify with `docker exec` and run `ip an` inside the busybox container.
    The IP address, MAC address, and network interfaces all match the output:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `docker exec` 验证并在 busybox 容器内运行 `ip an`。IP 地址、MAC 地址和网络接口都与输出匹配：
- en: '[PRE38]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Docker starts our container; creates the network namespace, the veth pair, and
    the docker0 bridge (if it does not already exist); and then attaches them all
    for every container creation and deletion, in a single command! That is powerful
    from an application developer’s perspective. There’s no need to remember all those
    Linux commands and possibly break the networking on a host. This discussion has
    mostly been about a single host. How Docker coordinates container communication
    between hosts in a cluster is discussed in the next section.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 启动我们的容器；创建网络命名空间、veth 对和 docker0 桥（如果尚不存在）；然后在每次容器创建和删除时都将它们附加在一起，只需一个命令！从应用程序开发者的角度来看，这非常强大。无需记住所有这些
    Linux 命令，并且可能在主机上破坏网络。这次讨论主要集中在单个主机上。Docker 如何在集群中协调容器之间的通信将在下一节讨论。
- en: Docker Networking Model
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker 网络模型
- en: 'Libnetwork is Docker’s take on container networking, and its design philosophy
    is in the container networking model (CNM). Libnetwork implements the CNM and
    works in three components: the sandbox, endpoint, and network. The sandbox implements
    the management of the Linux network namespaces for all containers running on the
    host. The network component is a collection of endpoints on the same network.
    Endpoints are hosts on the network. The network controller manages all of this
    via APIs in the Docker engine.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Libnetwork 是 Docker 的容器网络架构，其设计哲学体现在容器网络模型（CNM）中。Libnetwork 实现了 CNM，并通过 Docker
    引擎中的 API 管理 Linux 上运行的所有容器的网络命名空间。网络组件是同一网络上的多个端点的集合。端点是网络上的主机。网络控制器通过 Docker
    引擎的 API 管理所有这些。
- en: On the endpoint, Docker uses `iptables` for network isolation. The container
    publishes a port to be accessed externally. Containers do not receive a public
    IPv4 address; they receive a private RFC 1918 address. Services running on a container
    must be exposed port by port, and container ports have to be mapped to the host
    port so conflicts are avoided. When Docker starts, it creates a virtual bridge
    interface, `docker0`, on the host machine and assigns it a random IP address from
    the private 1918 range. This bridge passes packets between two connected devices,
    just like a physical bridge does. Each new container gets one interface automatically
    attached to the `docker0` bridge; [Figure 3-9](#docker-bridge) represents this
    and is similar to the approach we demonstrated in the previous sections.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在端点上，Docker使用`iptables`进行网络隔离。容器公开一个端口以供外部访问。容器不会收到公共IPv4地址，而是接收私有RFC 1918地址。运行在容器上的服务必须逐端口暴露，并且容器端口必须映射到主机端口以避免冲突。当Docker启动时，它在主机上创建一个虚拟桥接口`docker0`，并为其分配来自私有1918范围的随机IP地址。该桥接像物理桥接一样在两个连接设备之间传递数据包。每个新容器都会自动附加到`docker0`桥接上；[图3-9](#docker-bridge)展示了这一过程，与我们在前面部分展示的方法类似。
- en: '![neku 0309](Images/neku_0309.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0309](Images/neku_0309.png)'
- en: Figure 3-9\. Docker bridge
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. Docker桥接
- en: 'The CNM maps the network modes to drives we have already discussed. Here is
    a list of the networking mode and the Docker engine equivalent:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: CNM将网络模式映射到我们已经讨论过的驱动程序。以下是网络模式及其Docker引擎等效的列表：
- en: Bridge
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接
- en: Default Docker bridge (see [Figure 3-9](#docker-bridge), and our previous examples
    show this)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Docker桥接（参见[图3-9](#docker-bridge)，我们之前的示例中展示过这一点）
- en: Custom or Remote
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义或远程
- en: User-defined bridge, or allows users to create or use their plugin
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 用户定义的桥接，或允许用户创建或使用其插件
- en: Overlay
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖
- en: Overlay
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖
- en: 'Null'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 'Null'
- en: No networking options
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 无网络选项
- en: Bridge networks are for containers running on the same host. Communicating with
    containers running on different hosts can use an overlay network. Docker uses
    the concept of local and global drivers. Local drivers, a bridge, for example,
    are host-centric and do not do cross-node coordination. That is the job of global
    drivers such as Overlay. Global drivers rely on libkv, a key-value store abstraction,
    to coordinate across machines. The CNM does not provide the key-value store, so
    external ones like Consul, etcd, and Zookeeper are needed.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接网络用于在同一主机上运行的容器之间通信。与在不同主机上运行的容器通信可以使用覆盖网络。Docker使用本地和全局驱动程序的概念。例如，本地驱动程序（如桥接）以主机为中心，不进行跨节点协调，这是全局驱动程序（如Overlay）的工作。全局驱动程序依赖于libkv，一个键值存储抽象层，在多台机器之间协调。CNM不提供键值存储，因此需要外部存储如Consul、etcd和Zookeeper。
- en: The next section will discuss in depth the technologies enabling overlay networks.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将深入讨论启用覆盖网络的技术。
- en: Overlay Networking
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: Thus far, our examples have been on a single host, but production applications
    at scale do not run on a single host. For applications running in containers on
    separate nodes to communicate, several issues need to be solved, such as how to
    coordinate routing information between hosts, port conflicts, and IP address management,
    to name a few. One technology that helps with routing between hosts for containers
    is a VXLAN. In [Figure 3-10](#vxlan-hosts), we can see a layer 2 overlay network
    created with a VXLAN running over the physical L3 network.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的示例都是在单个主机上，但是大规模生产应用程序并不在单个主机上运行。为了容器在不同节点上通信，需要解决几个问题，如如何在主机之间协调路由信息、端口冲突和IP地址管理等。一个帮助容器在主机之间路由的技术是VXLAN。在[图3-10](#vxlan-hosts)中，我们可以看到在物理L3网络上运行的一个层2覆盖网络创建了一个VXLAN。
- en: We briefly discussed VXLANs in [Chapter 1](ch01.xhtml#networking_introduction),
    but a more in-depth explanation of how the data transfer works to enable the container-to-container
    communication is warranted here.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](ch01.xhtml#networking_introduction)简要讨论了VXLAN，但是在这里有必要更详细地解释数据传输如何实现容器间通信。
- en: '![neku 0310](Images/neku_0310.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0310](Images/neku_0310.png)'
- en: Figure 3-10\. VXLAN tunnel
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. VXLAN隧道
- en: A VXLAN is an extension of the VLAN protocol creating 16 million unique identifiers.
    Under IEEE 802.1Q, the maximum number of VLANs on a given Ethernet network is
    4,094\. The transport protocol over a physical data center network is IP plus
    UDP. VXLAN defines a MAC-in-UDP encapsulation scheme where the original layer
    2 frame has a VXLAN header added wrapped in a UDP IP packet. [Figure 3-11](#docker-overlay-advance)
    shows the IP packet encapsulated in the UDP packet and its headers.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN是VLAN协议的扩展，创建了1600万个唯一标识符。在IEEE 802.1Q下，给定以太网网络上的VLAN最大数量为4094。在物理数据中心网络上的传输协议是IP加UDP。VXLAN定义了一种MAC-in-UDP封装方案，其中原始第二层帧增加了一个VXLAN头部，并包装在一个UDP
    IP数据包中。[图 3-11](#docker-overlay-advance)显示了IP数据包封装在UDP数据包及其头部中。
- en: A VXLAN packet is a MAC-in-UDP encapsulated packet. The layer 2 frame has a
    VXLAN header added to it and is placed in a UDP-IP packet. The VXLAN identifier
    is 24 bits. That is how a VXLAN can support 16 million segments.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN 数据包是一个MAC-in-UDP封装的数据包。第二层帧增加了一个VXLAN头，并放置在UDP-IP数据包中。VXLAN标识符为24位。这就是为什么VXLAN可以支持1600万个段的原因。
- en: '[Figure 3-11](#docker-overlay-advance) is a more detailed version of [Chapter 1](ch01.xhtml#networking_introduction).
    We have the VXLAN tunnel endpoints, VTEPs, on both hosts, and they are attached
    to the host’s bridge interfaces with the containers attached to the bridge. The
    VTEP performs data frame encapsulation and decapsulation. The VTEP peer interaction
    ensures that the data gets forwarded to the relevant destination container addresses.
    The data leaving the containers is encapsulated with VXLAN information and transferred
    over the VXLAN tunnels to be de-encapsulated by the peer VTEP.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-11](#docker-overlay-advance)是[第一章](ch01.xhtml#networking_introduction)的详细版本。我们在两个主机上有VXLAN隧道端点（VTEP），它们附加到主机的桥接口，并连接到容器的桥接。VTEP执行数据帧的封装和解封装。VTEP对等体交互确保数据转发到相关目标容器地址。离开容器的数据使用VXLAN信息封装，并通过VXLAN隧道传输，由对等VTEP解封装。'
- en: Overlay networking enables cross-host communication on the network for containers.
    The CNM still has other issues that make it incompatible with Kubernetes. The
    Kubernetes maintainers decided to use the CNI project started at CoreOS. It is
    simpler than CNM, does not require daemons, and is designed to be cross-platform.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络（Overlay networking）允许容器在网络上进行跨主机通信。CNM 仍然存在其他问题，使其与 Kubernetes 不兼容。Kubernetes
    的维护者决定使用 CoreOS 发起的 CNI 项目。这个项目比 CNM 更简单，不需要守护进程，并且设计成跨平台的。
- en: '![neku 0311](Images/neku_0311.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0311](Images/neku_0311.png)'
- en: Figure 3-11\. VXLAN tunnel detailed
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. VXLAN 隧道详细信息
- en: Container Network Interface
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器网络接口（Container Network Interface）
- en: CNI is the software interface between the container runtime and the network
    implementation. There are many options to choose from when implementing a CNI;
    we will discuss a few notable ones. CNI started at CoreOS as part of the rkt project;
    it is now a CNCF project. The CNI project consists of a specification and libraries
    for developing plugins to configure network interfaces in Linux containers. CNI
    is concerned with a container’s network connectivity by allocating resources when
    the container gets created and removing them when deleted. A CNI plugin is responsible
    for associating a network interface to the container network namespace and making
    any necessary changes to the host. It then assigns the IP to the interface and
    sets up the routes for it. [Figure 3-12](#cni) outlines the CNI architecture.
    The container runtime uses a configuration file for the host’s network information;
    in Kubernetes, the Kubelet also uses this configuration file. The CNI and container
    runtime communicate with each other and apply commands to the configured CNI plugin.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 是容器运行时与网络实现之间的软件接口。在实现CNI时有很多选择；我们将讨论几个显著的选择。CNI 最初是 CoreOS 的 rkt 项目的一部分；现在是
    CNCF 项目。CNI 项目包括规范和用于开发 Linux 容器中配置网络接口的插件的库。CNI 关注容器的网络连接性，通过在容器创建时分配资源，并在删除时移除它们。CNI
    插件负责将网络接口关联到容器网络命名空间，并对主机进行任何必要的更改。然后，它为接口分配IP并为其设置路由。[图 3-12](#cni)概述了CNI架构。容器运行时使用主机的网络信息配置文件；在
    Kubernetes 中，Kubelet 也使用这个配置文件。CNI 和容器运行时相互通信，并对配置的CNI插件应用命令。
- en: '![neku 0312](Images/neku_0312.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![neku 0312](Images/neku_0312.png)'
- en: Figure 3-12\. CNI architecture
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-12\. CNI 架构
- en: 'There are several open source projects that implement CNI plugins with various
    features and functionality. Here is an outline of several:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源项目实现了带有各种功能和功能的 CNI 插件。以下是几个概述：
- en: Cilium
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium
- en: Cilium is open source software for securing network connectivity between application
    containers. Cilium is an L7/HTTP-aware CNI and can enforce network policies on
    L3–L7 using an identity-based security model decoupled from network addressing.
    A Linux technology eBPF powers it.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 是用于保护应用程序容器之间网络连接的开源软件。Cilium 是一个 L7/HTTP 感知的 CNI，可以使用基于身份的安全模型在 L3-L7
    上执行网络策略，与网络寻址分离。它由 Linux 技术 eBPF 提供支持。
- en: Flannel
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel
- en: Flannel is a simple way to configure a layer 3 network fabric designed for Kubernetes.
    Flannel focuses on networking. Flannel uses the Kubernetes cluster’s existing
    `etcd` datastore to store its state information to avoid providing a dedicated
    one.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 是为 Kubernetes 设计的一种简单配置层 3 网络结构的方法。Flannel 专注于网络。Flannel 使用 Kubernetes
    集群的现有`etcd`数据存储来存储其状态信息，以避免提供专用存储。
- en: Calico
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: Calico
- en: According to Calico, it “combines flexible networking capabilities with run-anywhere
    security enforcement to provide a solution with native Linux kernel performance
    and true cloud-native scalability.” It has full network policy support and works
    well in conjunction with other CNIs. Calico does not use an overlay network. Instead,
    Calico configures a layer 3 network that uses the BGP routing protocol to route
    packets between hosts. Calico can also integrate with Istio, a service mesh, to
    interpret and enforce policy for workloads within the cluster, both at the service
    mesh and the network infrastructure layers.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Calico 的说法，“将灵活的网络功能与随处可运行的安全执行相结合，提供具有本地 Linux 内核性能和真正云原生可伸缩性的解决方案。” 它具有完整的网络策略支持，并与其他
    CNI 配合良好。Calico 不使用覆盖网络。相反，Calico 配置了一个使用 BGP 路由协议在主机之间路由数据包的层 3 网络。Calico 还可以与
    Istio，一个服务网格，集成，以解释和执行集群内工作负载的策略，无论是在服务网格层还是网络基础设施层。
- en: AWS
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: AWS
- en: AWS has its open source implementation of a CNI, the AWS VPC CNI. It provides
    high throughput and availability by being directly on the AWS network. There is
    low latency using this CNI by providing little overhead because of no additional
    overlay network and minimal network jitter running on the AWS network. Cluster
    and network administrators can apply existing AWS VPC networking and security
    best practices for building Kubernetes networks on AWS. They can accomplish those
    best practices because the AWS CNI includes the capability to use native AWS services
    like VPC flow logs for analyzing network events and patterns, VPC routing policies
    for traffic management, and security groups and network access control lists for
    network traffic isolation. We will discuss more about the AWS VPC CNI in [Chapter 6](ch06.xhtml#kubernetes_and_cloud_networking).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 拥有自己的 CNI 的开源实现，即 AWS VPC CNI。通过直接位于 AWS 网络上，它提供高吞吐量和可用性。通过在 AWS 网络上运行，提供了低延迟，因为没有额外的覆盖网络和在
    AWS 网络上运行的最小网络抖动。集群和网络管理员可以应用现有的 AWS VPC 网络和安全最佳实践来构建在 AWS 上的 Kubernetes 网络。他们可以实现这些最佳实践，因为
    AWS CNI 包括使用原生 AWS 服务的能力，如用于分析网络事件和模式的 VPC 流日志，用于流量管理的 VPC 路由策略，以及用于网络流量隔离的安全组和网络访问控制列表。我们将在[第
    6 章](ch06.xhtml#kubernetes_and_cloud_networking)中更多地讨论 AWS VPC CNI。
- en: Note
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The Kubernetes.io website offers a [list of the CNI options available](https://oreil.ly/imDMP).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes.io 网站提供了一个[可用的 CNI 选项列表](https://oreil.ly/imDMP)。
- en: There are many more options for a CNI, and it is up to the cluster administrator,
    network admins, and application developers to best decide which CNI solves their
    business use cases. In later chapters, we will walk through use cases and deploy
    several to help admins make a decision.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多 CNI 的选项，最佳决定权在于集群管理员、网络管理员和应用程序开发人员，以最好地决定哪种 CNI 解决了他们的业务用例。在后续章节中，我们将讨论用例并部署几种以帮助管理员做出决定。
- en: In our next section, we will walk through container connectivity examples using
    the Golang web server and Docker.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一节中，我们将使用 Golang Web 服务器和 Docker 来演示容器连接性的例子。
- en: Container Connectivity
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器连接性
- en: Like we experimented with in the previous chapter, we will use the Go minimal
    web server to walk through the concept of container connectivity. We will explain
    what is happening at the container level when we deploy the web server as a container
    on our Ubuntu host.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在前一章中进行的实验一样，我们将使用 Go 极简 Web 服务器来演示容器连接性的概念。当我们在 Ubuntu 主机上将 Web 服务器部署为容器时，我们将解释容器级别发生了什么。
- en: 'The following are the two networking scenarios we will walk through:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将要讨论的两种网络场景：
- en: Container to container on the Docker host
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 主机上的容器之间的连接
- en: Container to container on separate hosts
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同主机上的容器之间的连接
- en: The Golang web server is hardcoded to run on port 8080, `http.ListenAndServe("0.0.0.0:8080",
    nil)`, as we can see in [Example 3-8](#minimal_web_server_in_go).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Golang Web 服务器硬编码在端口 8080 上运行，`http.ListenAndServe("0.0.0.0:8080", nil)`，正如我们在
    [示例 3-8](#minimal_web_server_in_go) 中所见。
- en: Example 3-8\. Minimal web server in Go
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-8\. Go 中的最小 Web 服务器
- en: '[PRE39]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To provision our minimal Golang web server, we need to create it from a Dockerfile.
    [Example 3-9](#dockerfile_for_golang_minimal_webserver) displays our Golang web
    server’s Dockerfile. The Dockerfile contains instructions to specify what to do
    when building the image. It begins with the `FROM` instruction and specifies what
    the base image should be. The `RUN` instruction specifies a command to execute.
    Comments start with `#`. Remember, each line in a Dockerfile creates a new layer
    if it changes the image’s state. Developers need to find the right balance between
    having lots of layers created for the image and the readability of the Dockerfile.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置我们的最小化 Golang Web 服务器，我们需要从 Dockerfile 创建它。[示例 3-9](#dockerfile_for_golang_minimal_webserver)
    显示了我们的 Golang Web 服务器的 Dockerfile。Dockerfile 包含了指定构建镜像时要执行的指令。它从 `FROM` 指令开始，指定基础镜像应该是什么。`RUN`
    指令指定要执行的命令。注释以 `#` 开始。请记住，如果改变了镜像的状态，Dockerfile 中的每一行都会创建一个新层。开发人员需要在为镜像创建很多层和
    Dockerfile 的可读性之间找到平衡。
- en: Example 3-9\. Dockerfile for Golang minimal web server
  id: totrans-359
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-9\. Golang 最小化 Web 服务器的 Dockerfile
- en: '[PRE40]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[![1](Images/1.png)](#co_container_networking_basics_CO2-1)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_container_networking_basics_CO2-1)'
- en: Since our web server is written in Golang, we can compile our Go server in a
    container to reduce the image’s size to only the compiled Go binary. We start
    by using the Golang base image with version 1.15 for our web server.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的 Web 服务器是用 Golang 编写的，我们可以在容器中编译我们的 Go 服务器，以将镜像大小减小到仅包含编译后的 Go 二进制文件。我们首先使用版本为
    1.15 的 Golang 基础镜像来启动我们的 Web 服务器。
- en: '[![2](Images/2.png)](#co_container_networking_basics_CO2-2)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_container_networking_basics_CO2-2)'
- en: '`WORKDIR` sets the working directory for all the subsequent commands to run
    from.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '`WORKDIR` 将工作目录设置为后续所有命令要从中运行的目录。'
- en: '[![3](Images/3.png)](#co_container_networking_basics_CO2-3)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_container_networking_basics_CO2-3)'
- en: '`COPY` copies the `web-server.go` file that defines our application as the
    working directory.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY` 复制 `web-server.go` 文件，该文件定义了我们应用的工作目录。'
- en: '[![4](Images/4.png)](#co_container_networking_basics_CO2-4)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_container_networking_basics_CO2-4)'
- en: '`RUN` instructs Docker to compile our Golang application in the builder container.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '`RUN` 指示 Docker 在构建容器中编译我们的 Golang 应用程序。'
- en: '[![5](Images/5.png)](#co_container_networking_basics_CO2-5)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_container_networking_basics_CO2-5)'
- en: Now to run our application, we define `FROM` for the application base image,
    again as `golang:1.15`; we can further minimize the final size of the image by
    using other minimal images like alpine.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了运行我们的应用程序，我们定义 `FROM` 作为应用程序的基础镜像，同样使用 `golang:1.15`；我们可以通过使用像 alpine 这样的其他小型镜像进一步减小镜像的最终大小。
- en: '[![6](Images/6.png)](#co_container_networking_basics_CO2-6)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_container_networking_basics_CO2-6)'
- en: Being a new container, we again set the working directory to `/opt`.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个新的容器，我们再次将工作目录设置为 `/opt`。
- en: '[![7](Images/7.png)](#co_container_networking_basics_CO2-7)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_container_networking_basics_CO2-7)'
- en: '`COPY` here will copy the compiled Go binary from the builder container into
    the application container.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY` 在此将从构建容器复制编译好的 Go 二进制文件到应用容器中。'
- en: '[![8](Images/8.png)](#co_container_networking_basics_CO2-8)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_container_networking_basics_CO2-8)'
- en: '`CMD` instructs Docker that the command to run our application is to start
    our web server.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`CMD` 指示 Docker 运行我们的应用程序的命令是启动我们的 Web 服务器。'
- en: 'There are some Dockerfile best practices that developers and admins should
    adhere to when containerizing their applications:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在将应用程序容器化时，开发人员和管理员应该遵守一些 Dockerfile 最佳实践：
- en: Use one `ENTRYPOINT` per Dockerfile. The `ENTRYPOINT` or `CMD` tells Docker
    what process starts inside the running container, so there should be only one
    running process; containers are all about process isolation.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 Dockerfile 只应使用一个 `ENTRYPOINT`。`ENTRYPOINT` 或 `CMD` 告诉 Docker 在运行的容器内部启动什么进程，因此应该只有一个运行中的进程；容器完全依赖于进程隔离。
- en: To cut down on the container layers, developers should combine similar commands
    into one using & & and \. Each new command in the Dockerfile adds a layer to the
    Docker container image, thus increasing its storage.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了减少容器的层次，开发人员应该使用 `&&` 和 `\` 将类似的命令合并为一个命令。每个新命令在 Dockerfile 中都会为 Docker 容器镜像添加一个新层，从而增加其存储空间。
- en: Use the caching system to improve the containers’ build times. If there is no
    change to a layer, it should be at the top of the Dockerfile. Caching is part
    of the reason that the order of statements is essential. Add files that are least
    likely to change first and the ones most likely to change last.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缓存系统来改进容器的构建时间。如果一个层没有变化，它应该位于Dockerfile的顶部。缓存是语句顺序至关重要的一部分。首先添加最不可能更改的文件，然后添加最有可能更改的文件。
- en: Use multistage builds to reduce the size of the final image drastically.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多阶段构建可以显著减小最终镜像的大小。
- en: Do not install unnecessary tools or packages. Doing this will reduce the containers’
    attack surface and size, reducing network transfer times from the registry to
    the hosts running the containers.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要安装不必要的工具或软件包。这样做可以减少容器的攻击面和大小，从而降低从注册表到运行容器的主机的网络传输时间。
- en: Let’s build our Golang web server and review the Docker commands to do so.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的Golang Web服务器并审查执行此操作的Docker命令。
- en: '`docker build` instructs Docker to build our images from the Dockerfile instructions:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker build`指示Docker根据Dockerfile指令构建我们的镜像：'
- en: '[PRE41]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The Golang minimal web server for our testing has the container ID `72fd05de6f73`,
    which is not friendly to read, so we can use the `docker tag` command to give
    it a friendly name:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 用于我们测试的Golang最小Web服务器具有容器ID `72fd05de6f73`，这不容易阅读，因此我们可以使用`docker tag`命令为其提供一个友好的名称：
- en: '[PRE42]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`docker images` returns the list of locally available images to run. We have
    one from the test on the Docker installation and the busybox we have been using
    to test our networking setup. If a container is not available locally, it is downloaded
    from the registry; network load times impact this, so we need to have as small
    an image as possible:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker images`返回本地可用镜像的列表以运行。我们有一个来自Docker安装测试的测试镜像和用于测试我们的网络设置的busybox。如果本地没有可用的容器，则会从注册表下载；网络加载时间会影响这一点，因此我们需要尽可能小的镜像：'
- en: '[PRE43]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`docker ps` shows us the running containers on the host. From our network namespace
    example, we still have one running busybox container:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker ps`显示我们主机上运行的容器。从我们的网络命名空间示例中，我们仍然有一个正在运行的busybox容器：'
- en: '[PRE44]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`docker logs` will print out any logs that the container is producing from
    standard out; currently, our busybox image is not printing anything out for us
    to see:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker logs`将打印出容器从标准输出产生的任何日志；目前，我们的busybox镜像没有打印出我们可以查看的任何内容：'
- en: '[PRE45]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`docker exec` allows devs and admins to execute commands inside the Docker
    container. We did this previously while investigating the Docker networking setups:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker exec`允许开发者和管理员在Docker容器内执行命令。我们之前在调查Docker网络设置时就这样做了：'
- en: '[PRE46]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note
  id: totrans-396
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more commands for the Docker CLI in the [documentation](https://oreil.ly/xWkad).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Docker CLI文档](https://oreil.ly/xWkad)中可以找到更多有关Docker CLI的命令。
- en: In the previous section, we built the Golang web server as a container. To test
    the connectivity, we will also employ the `dnsutils` image used by end-to-end
    testing for Kubernetes. That image is available from the Kubernetes project at
    `gcr.io/kubernetes-e2e-test-images/dnsutils:1.3`.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们将Golang Web服务器构建为一个容器。为了测试连通性，我们还将使用`dnsutils`镜像，该镜像被Kubernetes端到端测试使用。该镜像可以从Kubernetes项目的`gcr.io/kubernetes-e2e-test-images/dnsutils:1.3`获取。
- en: 'The image name will copy the Docker images from the Google container registry
    to our local Docker filesystem:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像名称将从Google容器注册表复制Docker镜像到我们的本地Docker文件系统：
- en: '[PRE47]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now that our Golang application can run as a container, we can explore the container
    networking scenarios.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的Golang应用程序可以作为容器运行，我们可以探索容器网络的各种场景。
- en: Container to Container
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器与容器
- en: 'Our first walk-through is the communication between two containers running
    on the same host. We begin by starting the `dnsutils` image and getting in a shell:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍两个运行在同一主机上的容器之间的通信。我们从启动`dnsutils`镜像并进入shell开始：
- en: '[PRE48]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The default Docker network setup gives the `dnsutils` image connectivity to
    the internet:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Docker网络设置使`dnsutils`镜像能够连接到互联网：
- en: '[PRE49]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The Golang web server starts with the default Docker bridge; in a separate
    SSH connection, then our Vagrant host, we start the Golang web server with the
    following command:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Golang Web服务器从默认的Docker桥接开始；在一个单独的SSH连接中，然后我们的Vagrant主机上，我们使用以下命令启动Golang Web服务器：
- en: '[PRE50]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The `-it` options are for interactive processes (such as a shell); we must use
    `-it` to allocate a TTY for the container process. `-d` runs the container in
    detached mode; this allows us to continue to use the terminal and outputs the
    full Docker container ID. The `-p` is probably the essential option in terms of
    the network; this one creates the port connections between the host and the containers.
    Our Golang web server runs on port 8080 and exposes that port on port 80 on the
    host.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '`-it` 选项用于交互式进程（例如shell）；我们必须使用 `-it` 分配一个TTY给容器进程。`-d` 在分离模式下运行容器；这允许我们继续使用终端并输出完整的Docker容器ID。`-p`
    在网络方面可能是最重要的选项；它在主机和容器之间创建端口连接。我们的Golang Web服务器在端口8080上运行，并在主机上的端口80上暴露该端口。'
- en: '`docker ps` verifies that we now have two containers running: the Go web server
    container with port 8080 exposed on the host port 80 and the shell running inside
    our `dnsutils` container:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker ps` 验证我们现在运行了两个容器：Go Web服务器容器在主机端口80上暴露了端口8080，并且在我们的 `dnsutils` 容器中运行的Shell：'
- en: '[PRE51]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s use the `docker inspect` command to get the Docker IP address of the
    Golang web server container:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `docker inspect` 命令获取Golang Web服务器容器的Docker IP地址：
- en: '[PRE52]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'On the `dnsutils` image, we can use the Docker network address of the Golang
    web server `172.17.0.2` and the container port `8080`:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `dnsutils` 镜像上，我们可以使用Golang Web服务器的Docker网络地址 `172.17.0.2` 和容器端口 `8080`：
- en: '[PRE53]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Each container can reach the other over the `docker0` bridge and the container
    ports because they are on the same Docker host and the same network. The Docker
    host has routes to the container’s IP address to reach the container on its IP
    address and port:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 每个容器都可以通过 `docker0` 桥接和容器端口互相访问，因为它们位于同一台Docker主机和相同网络上。Docker主机通过路由到容器的IP地址和端口来访问容器的IP地址：
- en: '[PRE54]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'But it does not for the Docker IP address and host port from the `docker run`
    command:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 但是对于来自 `docker run` 命令的Docker IP地址和主机端口，则不起作用：
- en: '[PRE55]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now for the reverse, using the loopback interface, we demonstrate that the
    host can reach the web server only on the host port exposed, 80, not the Docker
    port, 8080:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行反向操作，使用回环接口，我们演示主机只能在主机端口80上暴露的Web服务器上访问网页服务器，而不能在Docker端口8080上进行访问：
- en: '[PRE56]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now back on the `dnsutils`, the same is true: the `dnsutils` image on the Docker
    network, using the Docker IP address of the Go web container, can use only the
    Docker port, 8080, not the exposed host port 80:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到 `dnsutils` 上，情况也是一样的：在Docker网络上的 `dnsutils` 镜像中，使用Go Web容器的Docker IP地址，只能使用Docker端口8080，而不能使用暴露的主机端口80：
- en: '[PRE57]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now to show it is an entirely separate stack, let’s try the `dnsutils` loopback
    address and both the Docker port and the exposed host port:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了表明这是一个完全分开的堆栈，让我们尝试 `dnsutils` 回环地址以及Docker端口和暴露的主机端口：
- en: '[PRE58]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Neither works as expected; the `dnsutils` image has a separate network stack
    and does not share the Go web server’s network namespace. Knowing why it does
    not work is vital in Kubernetes to understand since pods are a collection of containers
    that share the same network namespace. Now we will examine how two containers
    communicate on two separate hosts.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 预期中都不起作用；`dnsutils` 镜像有单独的网络堆栈，不共享Go Web服务器的网络命名空间。了解为什么不起作用对于理解Kubernetes至关重要，因为Pod是共享相同网络命名空间的一组容器。现在我们将研究两个容器如何在两个分离的主机上进行通信。
- en: Container to Container Separate Hosts
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器到容器分开的主机
- en: Our previous example showed us how a container network runs on a local system,
    but how can two containers across the network on separate hosts communicate? In
    this example, we will deploy containers on separate hosts and investigate that
    and how it differs from being on the same host.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的示例向我们展示了如何在本地系统上运行容器网络，但是两个容器如何在分开的主机网络上通信呢？在本例中，我们将在分开的主机上部署容器，并调查这一点以及与在同一主机上的不同之处。
- en: 'Let’s start a second Vagrant Ubuntu host, `host-2`, and SSH into it as we did
    with our Docker host. We can see that our IP address is different from the Docker
    host running our Golang web server:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动第二个Vagrant Ubuntu主机 `host-2`，并像我们的Docker主机一样SSH进入它。我们可以看到我们的IP地址与运行Golang
    Web服务器的Docker主机不同：
- en: '[PRE59]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We can access our web server from the Docker host’s IP address, `192.168.1.20`,
    on port 80 exposed in the `docker run` command options. Port 80 is exposed on
    the Docker host but not reachable on container port 8080 with the host IP address:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Docker主机的IP地址 `192.168.1.20` 访问我们在 `docker run` 命令选项中暴露的端口80的Web服务器。端口80在Docker主机上暴露，但无法通过容器端口8080和主机IP地址访问：
- en: '[PRE60]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The same is true if `host-2` tries to reach the container on the containers’
    IP address, using either the Docker port or the host port. Remember, Docker uses
    the private address range, `172.17.0.0/16`:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `host-2` 尝试使用 Docker 端口或主机端口到达容器的 IP 地址，情况也是如此。请记住，Docker 使用私有地址范围 `172.17.0.0/16`。
- en: '[PRE61]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: For the host to route to the Docker IP address, it uses an overlay network or
    some external routing outside Docker. Routing is also external to Kubernetes;
    many CNIs help with this issue, and this is explored when looking at deploy clusters
    in [Chapter 6](ch06.xhtml#kubernetes_and_cloud_networking).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 要使主机路由到 Docker IP 地址，需要使用叠加网络或一些 Docker 外部的外部路由。路由也是 Kubernetes 外部的，许多 CNI 在解决此问题时提供帮助，并且这在部署集群时会进行探讨，参见
    [第6章](ch06.xhtml#kubernetes_and_cloud_networking)。
- en: The previous examples used the Docker default network bridge with exposed ports
    to the hosts. That is how `host-2` was able to communicate to the Docker container
    running on the Docker host. This chapter only scratches the surface of container
    networks. There are many more abstractions to explore, like ingress and egress
    traffic to the entire cluster, service discovery, and routing internal and external
    to the cluster. Later chapters will continue to build on these container networking
    basics.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子使用了 Docker 默认的网络桥接方式，将端口暴露到主机上。这就是 `host-2` 能够与运行在 Docker 主机上的 Docker 容器通信的方法。本章仅仅触及了容器网络的表面。还有许多抽象概念需要探索，比如整个集群的入站和出站流量、服务发现，以及集群内外的路由。后续章节将继续深入探讨这些容器网络基础知识。
- en: Conclusion
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this introduction to container networking, we worked through how containers
    have evolved to help with application deployment and advance host efficiency by
    allowing and segmenting multiple applications on a host. We have walked through
    the myriad history of containers with the various projects that have come and
    gone. Containers are powered and managed with namespaces and cgroups, features
    inside the Linux kernel. We walked through the abstractions that container runtimes
    maintain for application developers and learned how to deploy them ourselves.
    Understanding those Linux kernel abstractions is essential to deciding which CNI
    to deploy and its trade-offs and benefits. Administrators now have a base understanding
    of how container runtimes manage the Linux networking abstractions.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个容器网络介绍中，我们深入了解了容器如何发展，帮助应用程序部署，并通过允许在主机上容纳和分割多个应用程序来提高主机效率。我们回顾了容器的悠久历史，以及已经来去的各种项目。容器通过
    Linux 内核中的命名空间和控制组（cgroups）进行动力驱动和管理。我们了解了容器运行时为应用程序开发人员维护的抽象，并学习了如何自己部署它们。理解这些
    Linux 内核抽象对于决定部署哪种 CNI、以及其权衡和好处至关重要。管理员现在对容器运行时如何管理 Linux 网络抽象有了基础的理解。
- en: We have completed the basics of container networking! Our knowledge has expanded
    from using a simple network stack to running different unrelated stacks inside
    our containers. Knowing about namespaces, how ports are exposed, and communication
    flow empowers administrators to troubleshoot networking issues quickly and prevent
    downtime of their applications running in a Kubernetes cluster. Troubleshooting
    port issues or testing if a port is open on the host, on the container, or across
    the network is a must-have skill for any network engineer and indispensable for
    developers to troubleshoot their container issues. Kubernetes is built on these
    basics and abstracts them for developers. The next chapter will review how Kubernetes
    creates those abstractions and integrates them into the Kubernetes networking
    model.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了容器网络的基础知识！我们的知识从使用简单的网络堆栈扩展到在容器内部运行不同的不相关堆栈。了解命名空间、端口如何暴露以及通信流程，使管理员能够快速解决网络问题，并防止其在运行在
    Kubernetes 集群中的应用程序中造成停机时间。快速解决端口问题或者测试主机、容器或网络之间的端口是否开放，是任何网络工程师必备的技能，也是开发人员解决其容器问题不可或缺的技能。Kubernetes
    建立在这些基础之上，并为开发人员提供了抽象。下一章将审视 Kubernetes 如何创建这些抽象，并将它们整合到 Kubernetes 网络模型中。
