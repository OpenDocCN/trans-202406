- en: Chapter 3\. Container Networking Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve discussed networking basics and Linux networking, we’ll discuss
    how networking is implemented in containers. Like networking, containers have
    a long history. This chapter will review the history, discuss various options
    for running containers, and explore the networking setup available. The industry,
    for now, has settled on Docker as the container runtime standard. Thus, we’ll
    dive into the Docker networking model, explain how the CNI differs from the Docker
    network model, and end the chapter with examples of networking modes with Docker
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the evolution of running applications that
    has led us to containers. Some, rightfully, will talk about containers as not
    being real. They are yet another abstraction of the underlying technology in the
    OS kernel. Being technically right misses the point of the technology and leads
    us nowhere down the road of solving the hard problem that is application management
    and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running applications has always had its challenges. There are many ways to
    serve applications nowadays: in the cloud, on-prem, and, of course, with containers.
    Application developers and system administrators face many issues, such as dealing
    with different versions of libraries, knowing how to complete deployments, and
    having old versions of the application itself. For the longest time, developers
    of applications had to deal with these issues. Bash scripts and deployment tools
    all have their drawbacks and issues. Every new company has its way of deploying
    applications, so every new developer has to learn these techniques. Separation
    of duties, permissions controls, and maintaining system stability require system
    administrators to limit access to developers for deployments. Sysadmins also manage
    multiple applications on the same host machine to drive up that machine’s efficiency,
    thus creating contention between developers wanting to deploy new features and
    system administrators wanting to maintain the whole ecosystem’s stability.'
  prefs: []
  type: TYPE_NORMAL
- en: A general-purpose OS supports as many types of applications as possible, so
    its kernel includes all kinds of drivers, protocol libraries, and schedulers.
    [Figure 3-1](#img-begining) shows one machine, with one operating system, but
    there are many ways to deploy an application to that host. Application deployment
    is a problem all organizations must solve.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0301](Images/neku_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Application server
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From a networking perspective, with one operating system, there is one TCP/IP
    stack. That single stack creates issues with port conflicts on the host machine.
    System administrators host multiple applications on the same machine to increase
    the machine’s utilization, and each application will have to run on its port.
    So now, the system administrators, the application developers, and the network
    engineers have to coordinate all of this together. More tasks to add to the deployment
    checklist are creating troubleshooting guides and dealing with all the IT requests.
    Hypervisors are a way to increase one host machine’s efficiency and remove the
    one operating system/networking stack issues.
  prefs: []
  type: TYPE_NORMAL
- en: Hypervisor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A hypervisor emulates hardware resources, CPU, and memory from a host machine
    to create guest operating systems or virtual machines. In 2001, VMware released
    its x86 hypervisor; earlier versions included IBM’s z/Architecture and FreeBSD
    jails. The year 2003 saw the release of Xen, the first open source hypervisor,
    and in 2006 Kernel-based Virtual Machine (KVM) was released. A hypervisor allows
    system administrators to share the underlying hardware with multiple guest operating
    systems; [Figure 3-2](#img-hypervisor) demonstrates this. This resource sharing
    increases the host machine’s efficiency, alleviating one of the sysadmins issues.
  prefs: []
  type: TYPE_NORMAL
- en: Hypervisors also gave each application development team a separate networking
    stack, removing the port conflict issues on shared systems. For example, team
    A’s Tomcat application can run on port 8080, while team B’s can also run on port
    8080 since each application can now have its guest operating system with a separate
    network stack. Library versions, deployment, and other issues remain for the application
    developer. How can they package and deploy everything their application needs
    while maintaining the efficiency introduced by the hypervisor and virtual machines?
    This concern led to the development of containers.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0302](Images/neku_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Hypervisor
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Figure 3-3](#img-containers), we see the benefits of the containerization
    of applications; each container is independent. Application developers can use
    whatever they need to run their application without relying on underlying libraries
    or host operating systems. Each container also has its own network stack. The
    container allows developers to package and deploy applications while maintaining
    efficiencies for the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0303](Images/neku_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Containers running on host OS
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With any technology comes a history of changes, competitors, and innovations,
    and containers are no different. The following is a list of terms that can be
    confusing when learning about containers. First, we list the distinction between
    container runtimes, discuss each runtime’s functionality, and show how they relate
    to Kubernetes. The functionality of container runtimes breaks down to “high level”
    and “low level”:'
  prefs: []
  type: TYPE_NORMAL
- en: Container
  prefs: []
  type: TYPE_NORMAL
- en: A running container image.
  prefs: []
  type: TYPE_NORMAL
- en: Image
  prefs: []
  type: TYPE_NORMAL
- en: A container image is the file that is pulled down from a registry server and
    used locally as a mount point when starting a container.
  prefs: []
  type: TYPE_NORMAL
- en: Container engine
  prefs: []
  type: TYPE_NORMAL
- en: A container engine accepts user requests via command-line options to pull images
    and run a container.
  prefs: []
  type: TYPE_NORMAL
- en: Container runtime
  prefs: []
  type: TYPE_NORMAL
- en: The container runtime is the low-level piece of software in a container engine
    that deals with running a container.
  prefs: []
  type: TYPE_NORMAL
- en: Base image
  prefs: []
  type: TYPE_NORMAL
- en: A starting point for container images; to reduce build image sizes and complexity,
    users can start with a base image and make incremental changes on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: Image layer
  prefs: []
  type: TYPE_NORMAL
- en: Repositories are often referred to as images or container images, but actually
    they are made up of one or more layers. Image layers in a repository are connected
    in a parent-child relationship. Each image layer represents changes between itself
    and the parent layer.
  prefs: []
  type: TYPE_NORMAL
- en: Image format
  prefs: []
  type: TYPE_NORMAL
- en: Container engines have their own container image format, such as LXD, RKT, and
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Registry
  prefs: []
  type: TYPE_NORMAL
- en: A registry stores container images and allows for users to upload, download,
    and update container images.
  prefs: []
  type: TYPE_NORMAL
- en: Repository
  prefs: []
  type: TYPE_NORMAL
- en: Repositories can be equivalent to a container image. The important distinction
    is that repositories are made up of layers and metadata about the image; this
    is the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: Tag
  prefs: []
  type: TYPE_NORMAL
- en: A tag is a user-defined name for different versions of a container image.
  prefs: []
  type: TYPE_NORMAL
- en: Container host
  prefs: []
  type: TYPE_NORMAL
- en: The container host is the system that runs the container with a container engine.
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration
  prefs: []
  type: TYPE_NORMAL
- en: This is what Kubernetes does! It dynamically schedules container workloads for
    a cluster of container hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cgroups and namespaces are Linux primitives to create containers; they are discussed
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of “low-level” functionality is creating cgroups and namespaces
    for containers, the bare minimum to run one. Developers require more than that
    when working with containers. They need to build and test containers and deploy
    them; these are considered a “high-level” functionality. Each container runtime
    offers various levels of functionality. The following is a list of high and low
    functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: Low-level container runtime functionality
  prefs: []
  type: TYPE_NORMAL
- en: Creating containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-level container runtime functionality
  prefs: []
  type: TYPE_NORMAL
- en: Formatting container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing instances of containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Over the next few pages, we will discuss runtimes that implement the previous
    functionality. Each of the following projects has its strengths and weaknesses
    to provide high- and low-level functionality. Some are good to know about for
    historical reasons but no longer exist or have merged with other projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Low-level container runtimes
  prefs: []
  type: TYPE_NORMAL
- en: LXC
  prefs: []
  type: TYPE_NORMAL
- en: C API for creating Linux containers
  prefs: []
  type: TYPE_NORMAL
- en: runC
  prefs: []
  type: TYPE_NORMAL
- en: CLI for OCI-compliant containers
  prefs: []
  type: TYPE_NORMAL
- en: High-level container runtimes
  prefs: []
  type: TYPE_NORMAL
- en: containerd
  prefs: []
  type: TYPE_NORMAL
- en: Container runtime split off from Docker, a graduated CNCF project
  prefs: []
  type: TYPE_NORMAL
- en: CRI-O
  prefs: []
  type: TYPE_NORMAL
- en: Container runtime interface using the Open Container Initiative (OCI) specification,
    an incubating CNCF project
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs: []
  type: TYPE_NORMAL
- en: Open source container platform
  prefs: []
  type: TYPE_NORMAL
- en: lmctfy
  prefs: []
  type: TYPE_NORMAL
- en: Google containerization platform
  prefs: []
  type: TYPE_NORMAL
- en: rkt
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS container specification
  prefs: []
  type: TYPE_NORMAL
- en: OCI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OCI promotes common, minimal, open standards, and specifications for container
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea for creating a formal specification for container image formats and
    runtimes allows a container to be portable across all major operating systems
    and platforms to ensure no undue technical barriers. The three values guiding
    the OCI project are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Composable
  prefs: []
  type: TYPE_NORMAL
- en: Tools for managing containers should have clean interfaces. They should also
    not be bound to specific projects, clients, or frameworks and should work across
    all platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Decentralized
  prefs: []
  type: TYPE_NORMAL
- en: The format and runtime should be well specified and developed by the community,
    not one organization. Another goal of the OCI project is independent implementations
    of tools to run the same container.
  prefs: []
  type: TYPE_NORMAL
- en: Minimalist
  prefs: []
  type: TYPE_NORMAL
- en: The OCI spec strives to do several things well, be minimal and stable, and enable
    innovation and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Docker donated a draft for the base format and runtime. It also donated code
    for a reference implementation to the OCI. Docker took the contents of the libcontainer
    project, made it run independently of Docker, and donated it to the OCI project.
    That codebase is runC, which can be found on [GitHub](https://oreil.ly/A49v0).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss several early container initiatives and their capabilities. This
    section will end with where Kubernetes is with container runtimes and how they
    work together.
  prefs: []
  type: TYPE_NORMAL
- en: LXC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linux Containers, LXC, was created in 2008\. LXC combines cgroups and namespaces
    to provide an isolated environment for running applications. LXC’s goal is to
    create an environment as close as possible to a standard Linux without the need
    for a separate kernel. LXC has separate components: the `liblxc` library, several
    programming language bindings, Python versions 2 and 3, Lua, Go, Ruby, Haskell,
    a set of standard tools, and container templates.'
  prefs: []
  type: TYPE_NORMAL
- en: runC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'runC is the most widely used container runtime developed initially as part
    of Docker and was later extracted as a separate tool and library. runC is a command-line
    tool for running applications packaged according to the OCI format and is a compliant
    implementation of the OCI spec. runC uses `libcontainer`, which is the same container
    library powering a Docker engine installation. Before version 1.11, the Docker
    engine was used to manage volumes, networks, containers, images, etc. Now, the
    Docker architecture has several components, and the runC features include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Full support for Linux namespaces, including user namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native support for all security features available in Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SELinux, AppArmor, seccomp, control groups, capability drop, `pivot_root`, UID/GID
    dropping, etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Native support of Windows 10 containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planned native support for the entire hardware manufacturer’s ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A formally specified configuration format, governed by the OCI under the Linux
    Foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: containerd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: containerd is a high-level runtime that was split off from Docker. containerd
    is a background service that acts as an API facade for various container runtimes
    and OSs. containerd has various components that provide it with high-level functionality.
    containerd is a service for Linux and Windows that manages its host system’s complete
    container life cycle, image transfer, storage, container execution, and network
    attachment. containerd’s client CLI tool is `ctr`, and it is for development and
    debugging purposes for direct communication with containerd. containerd-shim is
    the component that allows for daemonless containers. It resides as the parent
    of the container’s process to facilitate a few things. containerd allows the runtimes,
    i.e., runC, to exit after it starts the container. This way, we do not need the
    long-running runtime processes for containers. It also keeps the standard I/O
    and other file descriptors open for the container if containerd and Docker die.
    If the shim does not run, then the pipe’s parent side would be closed, and the
    container would exit. containerd-shim also allows the container’s exit status
    to be reported back to a higher-level tool like Docker without having the container
    process’s actual parent do it.
  prefs: []
  type: TYPE_NORMAL
- en: lmctfy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google started lmctfy as its open source Linux container technology in 2013\.
    lmctfy is a high-level container runtime that provides the ability to create and
    delete containers but is no longer actively maintained and was porting over to
    libcontainer, which is now containerd. lmctfy provided an API-driven configuration
    without developers worrying about the details of cgroups and namespace internals.
  prefs: []
  type: TYPE_NORMAL
- en: rkt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: rkt started at CoreOS as an alternative to Docker in 2014\. It is written in
    Go, uses pods as its basic compute unit, and allows for a self-contained environment
    for applications. rkt’s native image format was the App Container Image (ACI),
    defined in the App Container spec; this was deprecated in favor of the OCI format
    and specification support. It supports the CNI specification and can run Docker
    images and OCI images. The rkt project was archived in February 2020 by the maintainers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Docker, released in 2013, solved many of the problems that developers had running
    containers end to end. It has all this functionality for developers to create,
    maintain, and deploy containers:'
  prefs: []
  type: TYPE_NORMAL
- en: Formatting container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing instances of containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 3-4](#img-docker-eng) shows us the architecture of the Docker engine
    and its various components. Docker began as a monolith application, building all
    the previous functionality into a single binary known as the *Docker engine*.
    The engine contained the Docker client or CLI that allows developers to build,
    run, and push containers and images. The Docker server runs as a daemon to manage
    the data volumes and networks for running containers. The client communicates
    to the server through the Docker API. It uses containerd to manage the container
    life cycle, and it uses runC to spawn the container process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0304](Images/neku_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Docker engine
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the past few years, Docker has broken apart this monolith into separate components.
    To run a container, the Docker engine creates the image and passes it to containerd.
    containerd calls containerd-shim, which uses runC to run the container. Then,
    containerd-shim allows the runtime (runC in this case) to exit after it starts
    the container. This way, we can run daemonless containers because we do not need
    the long-running runtime processes for containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker provides a separation of concerns for application developers and system
    administrators. It allows the developers to focus on building their apps, and
    system admins focus on deployment. Docker provides a fast development cycle; to
    test new versions of Golang for our web app, we can update the base image and
    run tests against it. Docker provides application portability between running
    on-premise, in the cloud, or in any other data center. Its motto is to build,
    ship, and run anywhere. A new container can quickly be provisioned for scalability
    and run more apps on one host machine, increasing that machine’s efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: CRI-O
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CRI-O is an OCI-based implementation of the Kubernetes CRI, while the OCI is
    a set of specifications that container runtime engines must implement. Red Hat
    started the CRI project in 2016 and in 2019 contributed it to the CNCF. CRI is
    a plugin interface that enables Kubernetes, via `Kubelet`, to communicate with
    any container runtime that satisfies the CRI interface. CRI-O development began
    in 2016 after the Kubernetes project introduced CRI, and CRI-O 1.0 was released
    in 2017\. The CRI-O is a lightweight CRI runtime made as a Kubernetes-specific
    high-level runtime built on gRPC and Protobuf over a UNIX socket. [Figure 3-5](#cri-place)
    points out where the CRI fits into the whole picture with the Kubernetes architecture.
    CRI-O provides stability in the Kubernetes project, with a commitment to passing
    Kubernetes tests.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0305](Images/neku_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. CRI about Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There have been many companies, technologies, and innovations in the container
    space. This section has been a brief history of that. The industry has landed
    on making sure the container landscape remains an open OCI project for all to
    use across various ways to run containers. Kubernetes has helped shaped this effort
    as well with the adaption of the CRI-O interface. Understanding the components
    of the container is vital to all administrators of container deployments and developers
    using containers. A recent example of this importance is in Kubernetes 1.20, where
    dockershim support will be deprecated. The Docker runtime utilizing the dockershim
    for administrators is deprecated, but developers can still use Docker to build
    OCI-compliant containers to run.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first CRI implementation was the dockershim, which provided a layer of abstraction
    in front of the Docker engine.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will dive deeper into the container technology that powers them.
  prefs: []
  type: TYPE_NORMAL
- en: Container Primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter if you are using Docker or containerd, runC starts and manages the
    actual containers for them. In this section, we will review what runC takes care
    of for developers from a container perspective. Each of our containers has Linux
    primitives known as *control groups* and *namespaces*. [Figure 3-6](#namespaces)
    shows an example of what this looks like; cgroups control access to resources
    in the kernel for our containers, and namespaces are individual slices of resources
    to manage separately from the root namespaces, i.e., the host.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0306](Images/neku_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Namespaces and control groups
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To help solidify these concepts, let’s dig into control groups and namespaces
    a bit further.
  prefs: []
  type: TYPE_NORMAL
- en: Control Groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In short, a cgroup is a Linux kernel feature that limits, accounts for, and
    isolates resource usage. Initially released in Linux 2.6.24, cgroups allow administrators
    to control different CPU systems and memory for particulate processes. Cgroups
    are provided through pseudofilesystems and are maintained by the core kernel code
    in cgroups. These separate subsystems maintain various cgroups in the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  prefs: []
  type: TYPE_NORMAL
- en: The process can be guaranteed a minimum number of CPU shares.
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs: []
  type: TYPE_NORMAL
- en: These set up memory limits for a process.
  prefs: []
  type: TYPE_NORMAL
- en: Disk I/O
  prefs: []
  type: TYPE_NORMAL
- en: This and other devices are controlled via the device’s cgroup subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs: []
  type: TYPE_NORMAL
- en: This is maintained by the `net_cls` and marks packets leaving the cgroup.
  prefs: []
  type: TYPE_NORMAL
- en: '`lscgroup` is a command-line tool that lists all the cgroups currently in the
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: runC will create the cgroups for the container at creation time. A cgroup controls
    how much of a resource a container can use, while namespaces control what processes
    inside the container can see.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Namespaces are features of the Linux kernel that isolate and virtualize system
    resources of a collection of processes. Here are examples of virtualized resources:'
  prefs: []
  type: TYPE_NORMAL
- en: PID namespace
  prefs: []
  type: TYPE_NORMAL
- en: Processes ID, for process isolation
  prefs: []
  type: TYPE_NORMAL
- en: Network namespace
  prefs: []
  type: TYPE_NORMAL
- en: Manages network interfaces and a separate networking stack
  prefs: []
  type: TYPE_NORMAL
- en: IPC namespace
  prefs: []
  type: TYPE_NORMAL
- en: Manages access to interprocess communication (IPC) resources
  prefs: []
  type: TYPE_NORMAL
- en: Mount namespace
  prefs: []
  type: TYPE_NORMAL
- en: Manages filesystem mount points
  prefs: []
  type: TYPE_NORMAL
- en: UTS namespace
  prefs: []
  type: TYPE_NORMAL
- en: UNIX time-sharing; allows single hosts to have different host and domain names
    for different processes
  prefs: []
  type: TYPE_NORMAL
- en: UID namespaces
  prefs: []
  type: TYPE_NORMAL
- en: User ID; isolates process ownership with separate user and group assignments
  prefs: []
  type: TYPE_NORMAL
- en: A process’s user and group IDs can be different inside and outside a user’s
    namespace. A process can have an unprivileged user ID outside a user namespace
    while at the same time having a user ID of 0 inside the container user namespace.
    The process has root privileges for execution inside the user namespace but is
    unprivileged for operations outside the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 3-1](#namespace_single) is an example of how to inspect the namespaces
    for a process. All information for a process is on the `/proc` filesystem in Linux.
    PID 1’s PID namespace is `4026531836`, and listing all the namespaces shows that
    the PID namespace IDs match.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. Namespaces of a single process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-7](#containers) shows that effectively these two Linux primitives
    allow application developers to control and manage their applications separate
    from the hosts and other applications either in containers or by running natively
    on the host.'
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0307](Images/neku_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Cgroups and namespaces powers combined
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following examples use Ubuntu 16.04 LTS Xenial Xerus. If you want to follow
    along on your system, more information can be found in this book’s code repo.
    The repo contains the tools and configurations for building the Ubuntu VM and
    Docker containers. Let’s get started with setting up and testing our namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 3-8](#root-container-namespaces) outlines a basic container network
    setup. In the following pages, we will walk through all the Linux commands that
    the low-level runtimes complete for container network creation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0308](Images/neku_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Root network namespace and container network namespace
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following steps show how to create the networking setup shown in [Figure 3-8](#root-container-namespaces):'
  prefs: []
  type: TYPE_NORMAL
- en: Create a host with a root network namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new network namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a veth pair.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move one side of the veth pair into a new network namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Address side of the veth pair inside the new network namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a bridge interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Address the bridge interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach the bridge to the host interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach one side of the veth pair to the bridge interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Profit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following are all the Linux commands needed to create the network namespace,
    bridge, and veth pairs and wire them together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s dive into an example and outline each command.
  prefs: []
  type: TYPE_NORMAL
- en: The `ip` Linux command sets up and controls the network namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can find more information about `ip` on its [man page](https://oreil.ly/jBKL7).
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 3-2](#example0302), we have used Vagrant and VirtualBox to create
    a fresh installation of Ubuntu for our testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. Ubuntu testing virtual machine
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Refer to the book repo for the Vagrantfile to reproduce this.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Vagrant](https://oreil.ly/o8Qo0) is a local virtual machine manager created
    by HashiCorp.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After Vagrant boots our virtual machine, we can use Vagrant to `ssh` into this
    VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*IP forwarding* is an operating system’s ability to accept incoming network
    packets on one interface, recognize them for another, and pass them on to that
    network accordingly. When enabled, IP forwarding allows a Linux machine to receive
    incoming packets and forward them. A Linux machine acting as an ordinary host
    would not need to have IP forwarding enabled because it generates and receives
    IP traffic for its purposes. By default, it is turned off; let’s enable it on
    our Ubuntu instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With our install of the Ubuntu instance, we can see that we do not have any
    additional network namespaces, so let’s create one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`ip netns` allows us to control the namespaces on the server. Creating one
    is as easy as typing `ip netns add net1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As we work through this example, we can see the network namespace we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a new network namespace for our container, we will need a veth
    pair for communication between the root network namespace and the container network
    namespace `net1`.
  prefs: []
  type: TYPE_NORMAL
- en: '`ip` again allows administrators to create the veth pairs with a straightforward
    command. Remember from [Chapter 2](ch02.xhtml#linux_networking) that veth comes
    in pairs and acts as a conduit between network namespaces, so packets from one
    end are automatically forwarded to the other.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Interfaces 4 and 5 are the veth pairs in the command output. We can also see
    which are paired with each other, `veth1@veth0` and `veth0@veth1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ip link list` command verifies the veth pair creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s move `veth1` into the new network namespace created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`ip netns exec` allows us to verify the network namespace’s configuration.
    The output verifies that `veth1` is now in the network namespace `net`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Network namespaces are entirely separate TCP/IP stacks in the Linux kernel.
    Being a new interface and in a new network namespace, the veth interface will
    need IP addressing in order to carry packets from the `net1` namespace to the
    root namespace and beyond the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As with host networking interfaces, they will need to be “turned on”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The state has now transitioned to `LOWERLAYERDOWN`. The status `NO-CARRIER`
    points in the right direction. Ethernet needs a cable to be connected; our upstream
    veth pair is not on yet either. The `veth1` interface is up and addressed but
    effectively still “unplugged”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s turn up the `veth0` side of the pair now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the veth pair inside the `net1` namespace is `UP`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Both sides of the veth pair report up; we need to connect the root namespace
    veth side to the bridge interface. Make sure to select the interface you’re working
    with, in this case `enp0s8`; it may be different for others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the enp0s8 and veth0 report are part of the bridge `br0` interface,
    `master br0 state up`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s test connectivity to our network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our new network namespace does not have a default route, so it does not know
    where to route our packets for the `ping` requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try that again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Success! We have created the bridge interface and veth pairs, migrated one to
    the new network namespace, and tested connectivity. [Example 3-3](#Namespace-Recap)
    is a recap of all the commands we ran to accomplish that.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. Recap network namespace creation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: For a developer not familiar with all these commands, that is a lot to remember
    and very easy to bork up! If the bridge information is incorrect, it could take
    down an entire part of the network with network loops. These issues are ones that
    system administrators would like to avoid, so they prevent developers from making
    those types of networking changes on the system. Fortunately, containers help
    remove the developers’ strain to remember all these commands and alleviate system
    admins’ fear of giving devs access to run those commands.
  prefs: []
  type: TYPE_NORMAL
- en: These  commands  are  all  needed  just  for  the  network  namespace  for 
    *every*  container  creation  and  deletion.  The  namespace  creation  in  [Example 3-3](#Namespace-Recap)
    is  the  container runtime’s job. Docker manages this for us, in its way. The
    CNI project standardizes the network creation for all systems. The CNI, much like
    the OCI, is a way for developers to standardize and prioritize specific tasks
    for managing parts of the container’s life cycle. In later sections, we will discuss
    CNI.
  prefs: []
  type: TYPE_NORMAL
- en: Container Network Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section showed us all the commands needed to create namespaces
    for our networking. Let’s investigate how Docker does this for us. We also only
    used the bridge mode; there several other modes for container networking. This
    section will deploy several Docker containers and examine their networking and
    explain how containers communicate externally to the host and with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by discussing the several network “modes” used when working with
    containers:'
  prefs: []
  type: TYPE_NORMAL
- en: None
  prefs: []
  type: TYPE_NORMAL
- en: No networking disables networking for the container. Use this mode when the
    container does not need network access.
  prefs: []
  type: TYPE_NORMAL
- en: Bridge
  prefs: []
  type: TYPE_NORMAL
- en: In bridge networking, the container runs in a private network internal to the
    host. Communication with other containers in the network is open. Communication
    with services outside the host goes through Network Address Translation (NAT)
    before exiting the host. Bridge mode is the default mode of networking when the
    `--net` option is not specified.
  prefs: []
  type: TYPE_NORMAL
- en: Host
  prefs: []
  type: TYPE_NORMAL
- en: In host networking, the container shares the same IP address and the network
    namespace as that of the host. Processes running inside this container have the
    same network capabilities as services running directly on the host. This mode
    is useful if the container needs access to network resources on the hosts. The
    container loses the benefit of network segmentation with this mode of networking.
    Whoever is deploying the containers will have to manage and contend with the ports
    of services running this node.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The host networking driver works only on Linux hosts. Docker Desktop for Mac
    and Windows, or Docker EE for Windows Server, does not support host networking
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Macvlan
  prefs: []
  type: TYPE_NORMAL
- en: 'Macvlan uses a parent interface. That interface can be a host interface such
    as eth0, a subinterface, or even a bonded host adapter that bundles Ethernet interfaces
    into a single logical interface. Like all Docker networks, Macvlan networks are
    segmented from each other, providing access within a network, but not between
    networks. Macvlan allows a physical interface to have multiple MAC and IP addresses
    using Macvlan subinterfaces. Macvlan has four types: Private, VEPA, Bridge (which
    Docker default uses), and Passthrough. With a bridge, use NAT for external connectivity.
    With Macvlan, since hosts are directly mapped to the physical network, external
    connectivity can be done using the same DHCP server and switch that the host uses.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Most cloud providers block Macvlan networking. Administrative access to networking
    equipment is needed.
  prefs: []
  type: TYPE_NORMAL
- en: IPvlan
  prefs: []
  type: TYPE_NORMAL
- en: 'IPvlan is similar to Macvlan, with a significant difference: IPvlan does not
    assign MAC addresses to created subinterfaces. All subinterfaces share the parent’s
    interface MAC address but use different IP addresses. IPvlan has two modes, L2
    or L3\. In IPvlan, L2, or layer 2, mode is analog to the Macvlan bridge mode.
    IPvlan L3, or layer 3, mode masquerades as a layer 3 device between the subinterfaces
    and parent interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Overlay
  prefs: []
  type: TYPE_NORMAL
- en: Overlay allows for the extension of the same network across hosts in a container
    cluster. The overlay network virtually sits on top of the underlay/physical networks.
    Several open source projects create these overlay networks, which we will discuss
    later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Custom
  prefs: []
  type: TYPE_NORMAL
- en: Custom bridge networking is the same as bridge networking but uses a bridge
    explicitly created for that container. An example of using this would be a container
    that runs on a database bridge network. A separate container can have an interface
    on the default and database bridge, enabling it to communicate with both networks
    as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Container-defined networking allows a container to share the address and network
    configuration of another container. This sharing enables process isolation between
    containers, where each container runs one service but where services can still
    communicate with one another on `127.0.0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test all these modes, we need to continue to use a Vagrant Ubuntu host but
    now with Docker installed. Docker for Mac and Windows does not support host networking
    mode, so we must use Linux for this example. You can do this with the provisioned
    machine in [Example 1-1](ch01.xhtml#minmal_web_server_in_go) or use the Docker
    Vagrant version in the book’s code repo. The Ubuntu Docker install directions
    are as follows if you want to do it manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the host up, let’s begin investigating the different networking
    setups we have to work with in Docker. [Example 3-4](#docker_networks) shows that
    Docker creates three network types during the install: bridge, host, and none.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-4\. Docker networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The default is a Docker bridge, and a container gets attached to it and provisioned
    with an IP address in the `172.17.0.0/16` default subnet. [Example 3-5](#docker_bridge_interfaces)
    is a view of Ubuntu’s default interfaces and the Docker install that creates the
    `docker0` bridge interface for the host.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-5\. Docker bridge interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_container_networking_basics_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the loopback interface.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_container_networking_basics_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: enp0s3 is our NAT’ed virtual box interface.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_container_networking_basics_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: enp0s8 is the host interface; this is on the same network as our host and uses
    DHCP to get the `192.168.1.19` address of default Docker bridge.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_container_networking_basics_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The default Docker container interface uses bridge mode.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 3-6](#docker_bridge) started a busybox container with the `docker
    run` command and requested that the Docker returns the container’s IP address.
    Docker default NATed address is `172.17.0.0/16`, with our busybox container getting
    `172.17.0.2`.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-6\. Docker bridge
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The host networking in [Example 3-7](#docker_host_networking) shows that the
    container shares the same network namespace as the host. We can see that the interfaces
    are the same as that of the host; enp0s3, enp0s8, and docker0 are present in the
    container `ip a` command output.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-7\. Docker host networking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'From the veth bridge example previously set up, let’s see how much simpler
    it is when Docker manages that for us. To view this, we need a process to keep
    the container running. The following command starts up a busybox container and
    drops into an `sh` command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a loopback interface, `lo`, and an Ethernet interface `eth0` connected
    to veth12, with a Docker default IP address of `172.17.0.2`. Since our previous
    command only outputted an `ip a` result and the container exited afterward, Docker
    reused the IP address `172.17.0.2` for the running busybox container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the `ip r` inside the container’s network namespace, we can see that
    the container’s route table is automatically set up as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we open a new terminal and `vagrant ssh` into our Vagrant Ubuntu instance
    and run the `docker ps` command, it shows all the information in the running busybox
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the veth interface Docker set up for the container `veth68b6f80@if11`
    on the same host’s networking namespace. It is a member of the bridge for `docker0`
    and is turned on `master docker0 state UP`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The Ubuntu host’s route table shows Docker’s routes for reaching containers
    running on the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Docker does not add the network namespaces it creates to `/var/run`
    where `ip netns list` expects newly created network namespaces. Let’s work through
    how we can see those namespaces now. Three steps are required to list the Docker
    network namespaces from the `ip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the running container’s PID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Soft link the network namespace from `/proc/PID/net/` to `/var/run/netns`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List the network namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`docker ps` outputs the container ID needed to inspect the running PID on the
    host PID namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`docker inspect` allows us to parse the output and get the host’s process’s
    PID. If we run `ps -p` on the host PID namespace, we can see it is running `sh`,
    which tracks our `docker run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`1f3f62ad5e02` is the container ID, and `25719` is the PID of the busybox container
    running `sh`, so now we can create a symbolic link for the container’s network
    namespace created by Docker to where `ip` expects with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using the container ID and process ID from the examples, keep in mind they
    will be different on your systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the `ip netns exec` commands return the same IP address, `172.17.0.2`,
    that the `docker exec` command does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify with `docker exec` and run `ip an` inside the busybox container.
    The IP address, MAC address, and network interfaces all match the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Docker starts our container; creates the network namespace, the veth pair, and
    the docker0 bridge (if it does not already exist); and then attaches them all
    for every container creation and deletion, in a single command! That is powerful
    from an application developer’s perspective. There’s no need to remember all those
    Linux commands and possibly break the networking on a host. This discussion has
    mostly been about a single host. How Docker coordinates container communication
    between hosts in a cluster is discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Networking Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Libnetwork is Docker’s take on container networking, and its design philosophy
    is in the container networking model (CNM). Libnetwork implements the CNM and
    works in three components: the sandbox, endpoint, and network. The sandbox implements
    the management of the Linux network namespaces for all containers running on the
    host. The network component is a collection of endpoints on the same network.
    Endpoints are hosts on the network. The network controller manages all of this
    via APIs in the Docker engine.'
  prefs: []
  type: TYPE_NORMAL
- en: On the endpoint, Docker uses `iptables` for network isolation. The container
    publishes a port to be accessed externally. Containers do not receive a public
    IPv4 address; they receive a private RFC 1918 address. Services running on a container
    must be exposed port by port, and container ports have to be mapped to the host
    port so conflicts are avoided. When Docker starts, it creates a virtual bridge
    interface, `docker0`, on the host machine and assigns it a random IP address from
    the private 1918 range. This bridge passes packets between two connected devices,
    just like a physical bridge does. Each new container gets one interface automatically
    attached to the `docker0` bridge; [Figure 3-9](#docker-bridge) represents this
    and is similar to the approach we demonstrated in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0309](Images/neku_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Docker bridge
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The CNM maps the network modes to drives we have already discussed. Here is
    a list of the networking mode and the Docker engine equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: Bridge
  prefs: []
  type: TYPE_NORMAL
- en: Default Docker bridge (see [Figure 3-9](#docker-bridge), and our previous examples
    show this)
  prefs: []
  type: TYPE_NORMAL
- en: Custom or Remote
  prefs: []
  type: TYPE_NORMAL
- en: User-defined bridge, or allows users to create or use their plugin
  prefs: []
  type: TYPE_NORMAL
- en: Overlay
  prefs: []
  type: TYPE_NORMAL
- en: Overlay
  prefs: []
  type: TYPE_NORMAL
- en: 'Null'
  prefs: []
  type: TYPE_NORMAL
- en: No networking options
  prefs: []
  type: TYPE_NORMAL
- en: Bridge networks are for containers running on the same host. Communicating with
    containers running on different hosts can use an overlay network. Docker uses
    the concept of local and global drivers. Local drivers, a bridge, for example,
    are host-centric and do not do cross-node coordination. That is the job of global
    drivers such as Overlay. Global drivers rely on libkv, a key-value store abstraction,
    to coordinate across machines. The CNM does not provide the key-value store, so
    external ones like Consul, etcd, and Zookeeper are needed.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will discuss in depth the technologies enabling overlay networks.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay Networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, our examples have been on a single host, but production applications
    at scale do not run on a single host. For applications running in containers on
    separate nodes to communicate, several issues need to be solved, such as how to
    coordinate routing information between hosts, port conflicts, and IP address management,
    to name a few. One technology that helps with routing between hosts for containers
    is a VXLAN. In [Figure 3-10](#vxlan-hosts), we can see a layer 2 overlay network
    created with a VXLAN running over the physical L3 network.
  prefs: []
  type: TYPE_NORMAL
- en: We briefly discussed VXLANs in [Chapter 1](ch01.xhtml#networking_introduction),
    but a more in-depth explanation of how the data transfer works to enable the container-to-container
    communication is warranted here.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0310](Images/neku_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. VXLAN tunnel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A VXLAN is an extension of the VLAN protocol creating 16 million unique identifiers.
    Under IEEE 802.1Q, the maximum number of VLANs on a given Ethernet network is
    4,094\. The transport protocol over a physical data center network is IP plus
    UDP. VXLAN defines a MAC-in-UDP encapsulation scheme where the original layer
    2 frame has a VXLAN header added wrapped in a UDP IP packet. [Figure 3-11](#docker-overlay-advance)
    shows the IP packet encapsulated in the UDP packet and its headers.
  prefs: []
  type: TYPE_NORMAL
- en: A VXLAN packet is a MAC-in-UDP encapsulated packet. The layer 2 frame has a
    VXLAN header added to it and is placed in a UDP-IP packet. The VXLAN identifier
    is 24 bits. That is how a VXLAN can support 16 million segments.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-11](#docker-overlay-advance) is a more detailed version of [Chapter 1](ch01.xhtml#networking_introduction).
    We have the VXLAN tunnel endpoints, VTEPs, on both hosts, and they are attached
    to the host’s bridge interfaces with the containers attached to the bridge. The
    VTEP performs data frame encapsulation and decapsulation. The VTEP peer interaction
    ensures that the data gets forwarded to the relevant destination container addresses.
    The data leaving the containers is encapsulated with VXLAN information and transferred
    over the VXLAN tunnels to be de-encapsulated by the peer VTEP.'
  prefs: []
  type: TYPE_NORMAL
- en: Overlay networking enables cross-host communication on the network for containers.
    The CNM still has other issues that make it incompatible with Kubernetes. The
    Kubernetes maintainers decided to use the CNI project started at CoreOS. It is
    simpler than CNM, does not require daemons, and is designed to be cross-platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0311](Images/neku_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. VXLAN tunnel detailed
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Container Network Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNI is the software interface between the container runtime and the network
    implementation. There are many options to choose from when implementing a CNI;
    we will discuss a few notable ones. CNI started at CoreOS as part of the rkt project;
    it is now a CNCF project. The CNI project consists of a specification and libraries
    for developing plugins to configure network interfaces in Linux containers. CNI
    is concerned with a container’s network connectivity by allocating resources when
    the container gets created and removing them when deleted. A CNI plugin is responsible
    for associating a network interface to the container network namespace and making
    any necessary changes to the host. It then assigns the IP to the interface and
    sets up the routes for it. [Figure 3-12](#cni) outlines the CNI architecture.
    The container runtime uses a configuration file for the host’s network information;
    in Kubernetes, the Kubelet also uses this configuration file. The CNI and container
    runtime communicate with each other and apply commands to the configured CNI plugin.
  prefs: []
  type: TYPE_NORMAL
- en: '![neku 0312](Images/neku_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. CNI architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are several open source projects that implement CNI plugins with various
    features and functionality. Here is an outline of several:'
  prefs: []
  type: TYPE_NORMAL
- en: Cilium
  prefs: []
  type: TYPE_NORMAL
- en: Cilium is open source software for securing network connectivity between application
    containers. Cilium is an L7/HTTP-aware CNI and can enforce network policies on
    L3–L7 using an identity-based security model decoupled from network addressing.
    A Linux technology eBPF powers it.
  prefs: []
  type: TYPE_NORMAL
- en: Flannel
  prefs: []
  type: TYPE_NORMAL
- en: Flannel is a simple way to configure a layer 3 network fabric designed for Kubernetes.
    Flannel focuses on networking. Flannel uses the Kubernetes cluster’s existing
    `etcd` datastore to store its state information to avoid providing a dedicated
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Calico
  prefs: []
  type: TYPE_NORMAL
- en: According to Calico, it “combines flexible networking capabilities with run-anywhere
    security enforcement to provide a solution with native Linux kernel performance
    and true cloud-native scalability.” It has full network policy support and works
    well in conjunction with other CNIs. Calico does not use an overlay network. Instead,
    Calico configures a layer 3 network that uses the BGP routing protocol to route
    packets between hosts. Calico can also integrate with Istio, a service mesh, to
    interpret and enforce policy for workloads within the cluster, both at the service
    mesh and the network infrastructure layers.
  prefs: []
  type: TYPE_NORMAL
- en: AWS
  prefs: []
  type: TYPE_NORMAL
- en: AWS has its open source implementation of a CNI, the AWS VPC CNI. It provides
    high throughput and availability by being directly on the AWS network. There is
    low latency using this CNI by providing little overhead because of no additional
    overlay network and minimal network jitter running on the AWS network. Cluster
    and network administrators can apply existing AWS VPC networking and security
    best practices for building Kubernetes networks on AWS. They can accomplish those
    best practices because the AWS CNI includes the capability to use native AWS services
    like VPC flow logs for analyzing network events and patterns, VPC routing policies
    for traffic management, and security groups and network access control lists for
    network traffic isolation. We will discuss more about the AWS VPC CNI in [Chapter 6](ch06.xhtml#kubernetes_and_cloud_networking).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Kubernetes.io website offers a [list of the CNI options available](https://oreil.ly/imDMP).
  prefs: []
  type: TYPE_NORMAL
- en: There are many more options for a CNI, and it is up to the cluster administrator,
    network admins, and application developers to best decide which CNI solves their
    business use cases. In later chapters, we will walk through use cases and deploy
    several to help admins make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we will walk through container connectivity examples using
    the Golang web server and Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Container Connectivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like we experimented with in the previous chapter, we will use the Go minimal
    web server to walk through the concept of container connectivity. We will explain
    what is happening at the container level when we deploy the web server as a container
    on our Ubuntu host.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the two networking scenarios we will walk through:'
  prefs: []
  type: TYPE_NORMAL
- en: Container to container on the Docker host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container to container on separate hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Golang web server is hardcoded to run on port 8080, `http.ListenAndServe("0.0.0.0:8080",
    nil)`, as we can see in [Example 3-8](#minimal_web_server_in_go).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-8\. Minimal web server in Go
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: To provision our minimal Golang web server, we need to create it from a Dockerfile.
    [Example 3-9](#dockerfile_for_golang_minimal_webserver) displays our Golang web
    server’s Dockerfile. The Dockerfile contains instructions to specify what to do
    when building the image. It begins with the `FROM` instruction and specifies what
    the base image should be. The `RUN` instruction specifies a command to execute.
    Comments start with `#`. Remember, each line in a Dockerfile creates a new layer
    if it changes the image’s state. Developers need to find the right balance between
    having lots of layers created for the image and the readability of the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-9\. Dockerfile for Golang minimal web server
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_container_networking_basics_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Since our web server is written in Golang, we can compile our Go server in a
    container to reduce the image’s size to only the compiled Go binary. We start
    by using the Golang base image with version 1.15 for our web server.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_container_networking_basics_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`WORKDIR` sets the working directory for all the subsequent commands to run
    from.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_container_networking_basics_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '`COPY` copies the `web-server.go` file that defines our application as the
    working directory.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_container_networking_basics_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: '`RUN` instructs Docker to compile our Golang application in the builder container.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_container_networking_basics_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Now to run our application, we define `FROM` for the application base image,
    again as `golang:1.15`; we can further minimize the final size of the image by
    using other minimal images like alpine.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_container_networking_basics_CO2-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Being a new container, we again set the working directory to `/opt`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_container_networking_basics_CO2-7)'
  prefs: []
  type: TYPE_NORMAL
- en: '`COPY` here will copy the compiled Go binary from the builder container into
    the application container.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_container_networking_basics_CO2-8)'
  prefs: []
  type: TYPE_NORMAL
- en: '`CMD` instructs Docker that the command to run our application is to start
    our web server.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some Dockerfile best practices that developers and admins should
    adhere to when containerizing their applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Use one `ENTRYPOINT` per Dockerfile. The `ENTRYPOINT` or `CMD` tells Docker
    what process starts inside the running container, so there should be only one
    running process; containers are all about process isolation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To cut down on the container layers, developers should combine similar commands
    into one using & & and \. Each new command in the Dockerfile adds a layer to the
    Docker container image, thus increasing its storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the caching system to improve the containers’ build times. If there is no
    change to a layer, it should be at the top of the Dockerfile. Caching is part
    of the reason that the order of statements is essential. Add files that are least
    likely to change first and the ones most likely to change last.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use multistage builds to reduce the size of the final image drastically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not install unnecessary tools or packages. Doing this will reduce the containers’
    attack surface and size, reducing network transfer times from the registry to
    the hosts running the containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s build our Golang web server and review the Docker commands to do so.
  prefs: []
  type: TYPE_NORMAL
- en: '`docker build` instructs Docker to build our images from the Dockerfile instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The Golang minimal web server for our testing has the container ID `72fd05de6f73`,
    which is not friendly to read, so we can use the `docker tag` command to give
    it a friendly name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`docker images` returns the list of locally available images to run. We have
    one from the test on the Docker installation and the busybox we have been using
    to test our networking setup. If a container is not available locally, it is downloaded
    from the registry; network load times impact this, so we need to have as small
    an image as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`docker ps` shows us the running containers on the host. From our network namespace
    example, we still have one running busybox container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`docker logs` will print out any logs that the container is producing from
    standard out; currently, our busybox image is not printing anything out for us
    to see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`docker exec` allows devs and admins to execute commands inside the Docker
    container. We did this previously while investigating the Docker networking setups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can find more commands for the Docker CLI in the [documentation](https://oreil.ly/xWkad).
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we built the Golang web server as a container. To test
    the connectivity, we will also employ the `dnsutils` image used by end-to-end
    testing for Kubernetes. That image is available from the Kubernetes project at
    `gcr.io/kubernetes-e2e-test-images/dnsutils:1.3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image name will copy the Docker images from the Google container registry
    to our local Docker filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now that our Golang application can run as a container, we can explore the container
    networking scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Container to Container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first walk-through is the communication between two containers running
    on the same host. We begin by starting the `dnsutils` image and getting in a shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The default Docker network setup gives the `dnsutils` image connectivity to
    the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The Golang web server starts with the default Docker bridge; in a separate
    SSH connection, then our Vagrant host, we start the Golang web server with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The `-it` options are for interactive processes (such as a shell); we must use
    `-it` to allocate a TTY for the container process. `-d` runs the container in
    detached mode; this allows us to continue to use the terminal and outputs the
    full Docker container ID. The `-p` is probably the essential option in terms of
    the network; this one creates the port connections between the host and the containers.
    Our Golang web server runs on port 8080 and exposes that port on port 80 on the
    host.
  prefs: []
  type: TYPE_NORMAL
- en: '`docker ps` verifies that we now have two containers running: the Go web server
    container with port 8080 exposed on the host port 80 and the shell running inside
    our `dnsutils` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use the `docker inspect` command to get the Docker IP address of the
    Golang web server container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'On the `dnsutils` image, we can use the Docker network address of the Golang
    web server `172.17.0.2` and the container port `8080`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Each container can reach the other over the `docker0` bridge and the container
    ports because they are on the same Docker host and the same network. The Docker
    host has routes to the container’s IP address to reach the container on its IP
    address and port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'But it does not for the Docker IP address and host port from the `docker run`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now for the reverse, using the loopback interface, we demonstrate that the
    host can reach the web server only on the host port exposed, 80, not the Docker
    port, 8080:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now back on the `dnsutils`, the same is true: the `dnsutils` image on the Docker
    network, using the Docker IP address of the Go web container, can use only the
    Docker port, 8080, not the exposed host port 80:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now to show it is an entirely separate stack, let’s try the `dnsutils` loopback
    address and both the Docker port and the exposed host port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Neither works as expected; the `dnsutils` image has a separate network stack
    and does not share the Go web server’s network namespace. Knowing why it does
    not work is vital in Kubernetes to understand since pods are a collection of containers
    that share the same network namespace. Now we will examine how two containers
    communicate on two separate hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Container to Container Separate Hosts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our previous example showed us how a container network runs on a local system,
    but how can two containers across the network on separate hosts communicate? In
    this example, we will deploy containers on separate hosts and investigate that
    and how it differs from being on the same host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start a second Vagrant Ubuntu host, `host-2`, and SSH into it as we did
    with our Docker host. We can see that our IP address is different from the Docker
    host running our Golang web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access our web server from the Docker host’s IP address, `192.168.1.20`,
    on port 80 exposed in the `docker run` command options. Port 80 is exposed on
    the Docker host but not reachable on container port 8080 with the host IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The same is true if `host-2` tries to reach the container on the containers’
    IP address, using either the Docker port or the host port. Remember, Docker uses
    the private address range, `172.17.0.0/16`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: For the host to route to the Docker IP address, it uses an overlay network or
    some external routing outside Docker. Routing is also external to Kubernetes;
    many CNIs help with this issue, and this is explored when looking at deploy clusters
    in [Chapter 6](ch06.xhtml#kubernetes_and_cloud_networking).
  prefs: []
  type: TYPE_NORMAL
- en: The previous examples used the Docker default network bridge with exposed ports
    to the hosts. That is how `host-2` was able to communicate to the Docker container
    running on the Docker host. This chapter only scratches the surface of container
    networks. There are many more abstractions to explore, like ingress and egress
    traffic to the entire cluster, service discovery, and routing internal and external
    to the cluster. Later chapters will continue to build on these container networking
    basics.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this introduction to container networking, we worked through how containers
    have evolved to help with application deployment and advance host efficiency by
    allowing and segmenting multiple applications on a host. We have walked through
    the myriad history of containers with the various projects that have come and
    gone. Containers are powered and managed with namespaces and cgroups, features
    inside the Linux kernel. We walked through the abstractions that container runtimes
    maintain for application developers and learned how to deploy them ourselves.
    Understanding those Linux kernel abstractions is essential to deciding which CNI
    to deploy and its trade-offs and benefits. Administrators now have a base understanding
    of how container runtimes manage the Linux networking abstractions.
  prefs: []
  type: TYPE_NORMAL
- en: We have completed the basics of container networking! Our knowledge has expanded
    from using a simple network stack to running different unrelated stacks inside
    our containers. Knowing about namespaces, how ports are exposed, and communication
    flow empowers administrators to troubleshoot networking issues quickly and prevent
    downtime of their applications running in a Kubernetes cluster. Troubleshooting
    port issues or testing if a port is open on the host, on the container, or across
    the network is a must-have skill for any network engineer and indispensable for
    developers to troubleshoot their container issues. Kubernetes is built on these
    basics and abstracts them for developers. The next chapter will review how Kubernetes
    creates those abstractions and integrates them into the Kubernetes networking
    model.
  prefs: []
  type: TYPE_NORMAL
