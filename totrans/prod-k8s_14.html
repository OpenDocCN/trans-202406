<html><head></head><body><section data-pdf-bookmark="Chapter 13. Autoscaling" data-type="chapter" epub:type="chapter"><div class="chapter" id="autoscaling_chapter">&#13;
<h1><span class="label">Chapter 13. </span>Autoscaling</h1>&#13;
&#13;
&#13;
<p>The ability to automatically scale workload capacity is one of the compelling benefits of cloud native systems.<a data-primary="autoscaling" data-type="indexterm" id="ix_autos"/>  If you have applications that encounter significant changes in capacity demands, autoscaling can reduce costs and reduce engineering toil in managing those applications.<a data-primary="scaling" data-secondary="autoscaling" data-seealso="autoscaling" data-type="indexterm" id="idm45611975352136"/>  Autoscaling is the process whereby we increase and decrease the capacity of our workloads without human intervention.  This begins with leveraging metrics to provide an indicator for when application capacity should be scaled.  It includes tuning settings that respond to those metrics.  And it culminates in systems to actually expand and contract the resources available to an application to accommodate the work it must perform.</p>&#13;
&#13;
<p>While autoscaling can provide wonderful benefits, it’s important to recognize when you should <em>not</em> employ autoscaling.  Autoscaling introduces complexity into your application management.  Besides initial setup, you will very likely need to revisit and tune the configuration of your autoscaling mechanisms.  Therefore, if an application’s capacity demands do not change markedly, it may be perfectly acceptable to provision for the highest traffic volumes an app will handle.  If your application load alters at predictable times, the manual effort to adjust capacity at those times may be trivial enough that investing in autoscaling may not be justified.  As with virtually all technology, leverage them only when the long-term benefit outweighs the setup and maintenance of the system.</p>&#13;
&#13;
<p>We’re going to <a data-primary="autoscaling" data-secondary="workload" data-type="indexterm" id="idm45611975348312"/>divide the subject of autoscaling into two broad categories:</p>&#13;
<dl>&#13;
<dt>Workload autoscaling</dt>&#13;
<dd>&#13;
<p>The automated management of the capacity for individual workloads</p>&#13;
</dd>&#13;
<dt>Cluster autoscaling</dt>&#13;
<dd>&#13;
<p>The automated management of the capacity of the underlying platform that hosts workloads<a data-primary="autoscaling" data-secondary="cluster" data-type="indexterm" id="idm45611975344280"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>As we examine these approaches, keep in mind<a data-primary="autoscaling" data-secondary="primary motivations for" data-type="indexterm" id="idm45611975342664"/> the common primary motivations for autoscaling:</p>&#13;
<dl>&#13;
<dt>Cost management</dt>&#13;
<dd>&#13;
<p>This is most relevant when you are renting your servers from a public cloud provider or being charged internally for your usage of virtualized infrastructure.<a data-primary="cost management" data-type="indexterm" id="idm45611975339848"/>  Cluster autoscaling allows you to dynamically adjust the number of machines you pay for.  In order to achieve this elasticity in your infrastructure you will need to leverage workload autoscaling to manage the capacity of the relevant applications within the cluster.</p>&#13;
</dd>&#13;
<dt>Capacity management</dt>&#13;
<dd>&#13;
<p>If you have a static set of infrastructure to leverage, autoscaling gives you an opportunity <a data-primary="capacity management" data-type="indexterm" id="idm45611975337336"/>to dynamically manage the allocation of that fixed capacity.  For example, an application that provides services to your business’s end users will often have days and times when it is busiest.  Workload autoscaling allows an application to dynamically expand its capacity and consume large amounts of a cluster when needed.  It also allows it to contract and make room for other workloads.  Perhaps you have batch workloads that can take advantage of the unused compute resources during off hours.  Cluster autoscaling can remove considerable human toil in managing compute infrastructure capacity since the number of machines used by your clusters is adjusted without human intervention.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Autoscaling is compelling for applications that fluctuate in load and traffic.  Without<a data-primary="autoscaling" data-secondary="for applications fluctuating in load and traffic" data-secondary-sortas="applications" data-type="indexterm" id="idm45611975335160"/> autoscaling, you have two options:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Chronically overprovision your application capacity, incurring additional cost to your business.</p>&#13;
</li>&#13;
<li>&#13;
<p>Alert your engineers for manual scaling operations, incurring additional toil in your operations.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In this chapter, we will first explore how to approach autoscaling, and how to design software to leverage these systems.  Then we will dive into details of specific systems we can use to autoscale our applications in Kubernetes-based platforms.  This will include horizontal and vertical autoscaling, including the metrics we should use to trigger scaling events.  We will also look at  scaling workloads in proportion to the cluster itself, as well as an example of custom autoscaling you might consider.  Finally, in <a data-type="xref" href="#cluster_autoscaling">“Cluster Autoscaling”</a>, we will address the scaling of the platform itself so that it can accommodate significant changes in demand from the workloads it hosts.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Types of Scaling" data-type="sect1"><div class="sect1" id="types_of_scaling">&#13;
<h1>Types of Scaling</h1>&#13;
&#13;
<p>In software engineering, scaling generally <a data-primary="scaling" data-secondary="types of" data-type="indexterm" id="idm45611975327496"/><a data-primary="autoscaling" data-secondary="types of scaling" data-type="indexterm" id="idm45611975326520"/>falls into two categories:</p>&#13;
<dl>&#13;
<dt>Horizontal scaling</dt>&#13;
<dd>&#13;
<p>This involves changing the number of identical replicas of a workload.<a data-primary="horizontal scaling" data-type="indexterm" id="idm45611975323976"/>  This is either the number of Pods for a particular application or the number of nodes in a cluster that hosts applications.  Future references to horizontal scaling will use the terms “out” or “in” when referring to increasing or decreasing the number of Pods or nodes.</p>&#13;
</dd>&#13;
<dt>Vertical scaling</dt>&#13;
<dd>&#13;
<p>This involves altering the resource capacity of a single instance.<a data-primary="vertical scaling" data-type="indexterm" id="idm45611975321576"/>  For an application, this is changing the resource requests and/or limits for the containers of the application.  For nodes of a cluster, this generally involves changing the amount of CPU and memory resources available.  Future references to vertical scaling will use the terms “up” or “down” to refer to these changes.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In systems that have a need to dynamically scale, i.e., have frequent, significant changes in load, prefer horizontal scaling where possible.  Vertical scaling is limited by the largest machine you have available to use.  Furthermore, increasing capacity with vertical scaling involves a restart for the application.  Even in virtualized environments where dynamic scaling of machines is possible, Pods will need to be restarted as resource requests and limits cannot be dynamically updated at this time.  Compare this with horizontal scaling, where existing instances need not restart and capacity is dynamically increased by adding replicas.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Application Architecture" data-type="sect1"><div class="sect1" id="idm45611975318968">&#13;
<h1>Application Architecture</h1>&#13;
&#13;
<p>The topic of autoscaling <a data-primary="autoscaling" data-secondary="application architecture" data-type="indexterm" id="idm45611975317384"/>is particularly important to service-oriented systems.<a data-primary="application architecture, autoscaling and" data-type="indexterm" id="idm45611975316216"/>  One of the benefits of decomposing applications into distinct components is the ability to scale different parts of an application independently.  We were doing this with n-tier architectures well before cloud native emerged.  It became commonplace to separate web applications from their relational databases and scale the web app independently.  <a data-primary="microservices" data-secondary="autoscaling and" data-type="indexterm" id="idm45611975314952"/>With microservice architecture we can extend this further.  For example, an enterprise website may have a service that powers its online store, which is distinct from a service that serves blog posts.  When there is a marketing event, the online store can be scaled while the blog service is not affected and may remain unchanged.</p>&#13;
&#13;
<p>With this opportunity to scale different services independently, you are able to more efficiently utilize the infrastructure used by your workloads.  However, you introduce the management overhead of scaling many distinct workloads.  Automating this scaling process becomes very imporant.  At a certain point, it becomes essential.</p>&#13;
&#13;
<p>Autoscaling lends itself well to smaller, more nimble workloads that have tiny image sizes and fast startup times.  If the time required to pull a container image onto a given node is short, and if the time it takes for the application to start once the container is created is also short, the workload can respond to scaling events quickly.  Capacity can be adjusted much more readily.  Applications with image sizes over a gigabyte and startup scripts that run for minutes are far less suited to responding to changes in load.  Workloads like this are not good candidates for autoscaling, so keep this in mind when designing and building your apps.</p>&#13;
&#13;
<p>It’s also important to recognize that autoscaling will involve stopping instances of the app.  This doesn’t apply when workloads scale out, of course.  However, that which scales out, must scale back in.  That will involve stopping running instances.  And with vertically scaled workloads, restarts are required to update resource allocations.  In either case, your application’s ability to gracefully shut down will be important.  <a data-type="xref" href="ch14.html#application_considerations_chapter">Chapter 14</a> covers this topic in detail.</p>&#13;
&#13;
<p>Now that we’ve addressed the design concerns to keep in mind, let’s dive into the details of autoscaling workloads in Kubernetes clusters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Workload Autoscaling" data-type="sect1"><div class="sect1" id="idm45611974039320">&#13;
<h1>Workload Autoscaling</h1>&#13;
&#13;
<p>This section will focus on autoscaling application workloads.<a data-primary="workloads" data-secondary="autoscaling" data-type="indexterm" id="ix_wklauto"/><a data-primary="autoscaling" data-secondary="workload" data-type="indexterm" id="ix_ASwkl"/>  This involves monitoring some metric and adjusting workload capacity without human intervention.  While this sounds like a set-it-and-forget-it operation, don’t treat it that way, especially in initial stages.  Even after you load test your autoscaling configurations, you need to ensure the behavior you get in production is what you intended.  Load tests don’t always mimic production conditions accurately.  So once in production, you will want to check in on the application to verify that it is scaling at the right thresholds and your objectives for efficiency and end-user experience are being met.  Strongly consider setting alerts so that you get notified of significant scaling events to review and tweak its behavior as needed.</p>&#13;
&#13;
<p>Most of this section will address the Horizontal Pod Autoscaler and the Vertical Pod Autoscaler.  These are the most common tools used for autoscaling workloads on Kubernetes.  We’ll also dig into the metrics your workload uses to trigger scaling events and when you should consider custom application metrics for this purpose.  We’ll also look at the Cluster Proportional Autoscaler and the use cases where that makes sense.  Lastly, we’ll touch on custom methods beyond these particular tools you might consider.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Horizontal Pod Autoscaler" data-type="sect2"><div class="sect2" id="idm45611974033816">&#13;
<h2>Horizontal Pod Autoscaler</h2>&#13;
&#13;
<p>The Horizontal Pod Autoscaler (HPA) is the most common tool used for autoscaling workloads in Kubernetes-based platforms.<a data-primary="Horizontal Pod Autoscaler (HPA)" data-type="indexterm" id="ix_HPA"/><a data-primary="autoscaling" data-secondary="workload" data-tertiary="Horizontal Pod Autoscaler" data-type="indexterm" id="ix_ASwklHPA"/><a data-primary="workloads" data-secondary="autoscaling" data-tertiary="Horizontal Pod Autoscaler" data-type="indexterm" id="ix_wklautoHPA"/>  It is natively supported by Kubernetes with the HorizontalPodAutoscaler resource and a controller bundled into the kube-controller-manager.<a data-primary="CPU consumption" data-secondary="using as metric for workload autoscaling" data-type="indexterm" id="idm45611974028312"/><a data-primary="metrics" data-secondary="CPU or memory consumption, using for workload autoscaling" data-type="indexterm" id="idm45611974027464"/><a data-primary="memory consumption" data-secondary="using as metric for workload autoscaling" data-type="indexterm" id="idm45611974026616"/>  If you are using CPU or memory consumption as a metric for autoscaling your workload, the barrier to entry is low for using the HPA.</p>&#13;
&#13;
<p>In this case, you can use the <a href="https://oreil.ly/S0vbj">Kubernetes Metrics Server</a> to make the PodMetrics available to the HPA. <a data-primary="PodMetrics" data-type="indexterm" id="idm45611974024264"/><a data-primary="Kubernetes Metrics Server" data-type="indexterm" id="idm45611974023656"/><a data-primary="Metrics Server" data-type="indexterm" id="idm45611974023048"/> The Metrics Server collects CPU and memory usage metrics for containers from the kubelets in the cluster and makes them available through the resource metrics API in PodMetrics resources.  The Metrics Server leverages the <a href="https://oreil.ly/eXDcl">Kubernetes API aggregation layer</a>.  <a data-primary="Kubernetes API" data-secondary="aggregation layer" data-type="indexterm" id="idm45611974021448"/>Requests for resources in the API group and version <code>metrics.k8s.io/v1beta1</code> will be proxied to the Metrics Server.</p>&#13;
&#13;
<p><a data-type="xref" href="#horizontal_pod_autoscaling">Figure 13-1</a> illustrates how the components carry out this function.  <a data-primary="Metrics Server" data-secondary="collecting resource usage metrics for containers" data-type="indexterm" id="idm45611974019000"/>The Metrics Server collects resource usage metrics for the containers on the platform.  It gets this data from the kubelets running on each node in the cluster and makes that data available to clients that need to access it.  <a data-primary="API server" data-secondary="horizontal Pod autoscaling" data-type="indexterm" id="idm45611974017784"/>The HPA controller queries the Kubernetes API server to retrieve that resource usage data every 15 seconds, by default.  The Kubernetes API proxies the requests to the Metrics Server, which serves the requested data.  The HPA controller maintains a watch on the HorizontalPodAutoscaler resource type and uses the configuration defined there to determine if the number of replicas for an application is appropriate.<a data-primary="Horizontal Pod Autoscaler (HPA)" data-secondary="HorizontalPodAutoscaler resource" data-type="indexterm" id="idm45611974016376"/> <a data-type="xref" href="#an_example_of_deployment_and_horizontalpdodautoscaler_manifests">Example 13-1</a> demonstrates how this determination is made.<a data-primary="Deployment resource" data-type="indexterm" id="idm45611974014664"/>  The app is most commonly defined with a Deployment resource, and, when the HPA controller determines that the replica count needs to be adjusted, it updates the relevant Deployment through the API server.  Subsequently, the Deployment controller responds by updating the ReplicaSet, which leads to a change in the number of Pods.</p>&#13;
&#13;
<figure><div class="figure" id="horizontal_pod_autoscaling">&#13;
<img alt="prku 1301" src="assets/prku_1301.png"/>&#13;
<h6><span class="label">Figure 13-1. </span>Horizontal Pod autoscaling.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The desired state for an HPA is declared in the HorizontalPodAutoscaler resource, as demonstrated in the following example.  The <code>targetCPUUtilizationPercentage</code> is used to determine replica count for the target workload.</p>&#13;
<div data-type="example" id="an_example_of_deployment_and_horizontalpdodautoscaler_manifests">&#13;
<h5><span class="label">Example 13-1. </span>An example of Deployment and HorizontalPodAutoscaler manifests</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">apps/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Deployment</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">selector</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">matchLabels</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code>  </code><code class="nt">template</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">labels</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code>    </code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">containers</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code>        </code><code class="nt">image</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample-image:1.0</code><code>&#13;
</code><code>        </code><code class="nt">resources</code><code class="p">:</code><code>&#13;
</code><code>          </code><code class="nt">requests</code><code class="p">:</code><code>&#13;
</code><code>            </code><code class="nt">cpu</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">100m</code><code class="s">"</code><code>  </code><a class="co" href="#callout_autoscaling_CO1-1" id="co_autoscaling_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="nn">---</code><code>&#13;
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">autoscaling/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">HorizontalPodAutoscaler</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">scaleTargetRef</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">apps/v1</code><code>&#13;
</code><code>    </code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Deployment</code><code>&#13;
</code><code>    </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code>  </code><code class="nt">minReplicas</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">1</code><code>  </code><a class="co" href="#callout_autoscaling_CO1-2" id="co_autoscaling_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="nt">maxReplicas</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">3</code><code>  </code><a class="co" href="#callout_autoscaling_CO1-3" id="co_autoscaling_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="nt">targetCPUUtilizationPercentage</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">75</code><code>  </code><a class="co" href="#callout_autoscaling_CO1-4" id="co_autoscaling_CO1-4"><img alt="4" src="assets/4.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_autoscaling_CO1-1" id="callout_autoscaling_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>A <code>resources.requests</code> value must be set for the metric being used.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO1-2" id="callout_autoscaling_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The replicas will never scale in below this value.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO1-3" id="callout_autoscaling_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The replicas will never scale out beyond this value.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO1-4" id="callout_autoscaling_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The desired CPU utilization.  If the actual utilization goes significantly beyond this value, the replica count will be increased; if significantly below, decreased.</p></dd>&#13;
</dl></div>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If <a data-primary="metrics" data-secondary="multiple, workload autoscaling on" data-type="indexterm" id="idm45611973987752"/>you have a use case to use multiple metrics, e.g., CPU <em>and</em> memory, to trigger scaling events, you can use the <code>autoscaling/v2beta2</code> API.<a data-primary="autoscaling/v2beta2 API" data-type="indexterm" id="idm45611973985800"/>  In this case, the HPA controller will calculate the appropriate number of replicas based on each metric individually, and then apply the highest value.</p>&#13;
</div>&#13;
&#13;
<p>This is the most common and readily used autoscaling method, is widely applicable, and is relatively uncomplicated to implement.  However, it’s important to understand the limitations of this method:</p>&#13;
<dl>&#13;
<dt>Not all workloads can scale horizontally</dt>&#13;
<dd>&#13;
<p>For applications that cannot share load among distinct instances, horizontal scaling is useless.<a data-primary="Horizontal Pod Autoscaler (HPA)" data-secondary="not all workloads scale horizontally" data-type="indexterm" id="idm45611973982216"/>  This is true for some stateful workloads and leader-elected applications.  For these use cases you may consider vertical Pod autoscaling.</p>&#13;
</dd>&#13;
<dt>The cluster size will limit scaling</dt>&#13;
<dd>&#13;
<p>As an application scales out, it may run out of capacity available in the worker nodes of a cluster.  This can be solved by provisioning sufficient capacity ahead of time, using alerts to prompt your platform operators to add capacity manually, or by using cluster autoscaling, which is discussed in another section of this chapter.</p>&#13;
</dd>&#13;
<dt>CPU and memory may not be the right metric to use for scaling decisions</dt>&#13;
<dd>&#13;
<p>If your workload exposes a custom metric that better identifies a need to scale, it can be used.  We will cover that use case later in this chapter.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Avoid autoscaling your workload based on a metric that does not always change in proportion to the load placed on the application.<a data-primary="Horizontal Pod Autoscaler (HPA)" data-secondary="avoiding faulty metrics for autoscaling" data-type="indexterm" id="idm45611973976424"/><a data-primary="metrics" data-secondary="faulty, workload autoscaling based on" data-type="indexterm" id="idm45611973975464"/><a data-primary="CPU consumption" data-secondary="when not to use as workload autoscaling metric" data-type="indexterm" id="idm45611973974504"/>  The most common autoscaling metric is CPU.  However, if a particular workload’s CPU doesn’t change significantly with added load, and instead consumes memory in more direct proportion to increased load, do not use CPU.  A less obvious example is if a workload consumes added CPU at startup.  During normal operation, CPU may be a perfectly useful trigger for autoscaling.  However, a startup CPU spike will be interpreted by the HPA as a trigger for a scaling event even though traffic has not induced the spike.  There are ways to mitigate this with kube-controller-manager flags such as <code>--horizontal-pod-autoscaler-cpu-initialization-period</code>, which will provide a startup grace period, or the <code>--horizontal-pod-autoscaler-sync-period</code>, which allows you to increase the time between scaling evaluations.  But note that these flags are set on the kube-controller-manager.  These will affect all HPAs across the entire cluster, which will impact workloads that do <em>not</em> have high startup CPU consumption.  You could wind up reducing the responsiveness of the HPA for workloads cluster-wide.  If you find your team employing workarounds to make CPU consumption work as a trigger for your autoscaling needs, consider using a more representative custom metric.  Perhaps number of HTTP requests received would make a better measuring stick, for example.</p>&#13;
</div>&#13;
&#13;
<p>That wraps up the Horizontal Pod Autoscaler.  Next, we’ll look at another form of autoscaling available in Kubernetes: vertical Pod autoscaling.<a data-primary="workloads" data-secondary="autoscaling" data-startref="ix_wklautoHPA" data-tertiary="Horizontal Pod Autoscaler" data-type="indexterm" id="idm45611973970488"/><a data-primary="autoscaling" data-secondary="workload" data-startref="ix_ASwklHPA" data-tertiary="Horizontal Pod Autoscaler" data-type="indexterm" id="idm45611973968952"/><a data-primary="Horizontal Pod Autoscaler (HPA)" data-startref="ix_HPA" data-type="indexterm" id="idm45611973967448"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vertical Pod Autoscaler" data-type="sect2"><div class="sect2" id="idm45611974033224">&#13;
<h2>Vertical Pod Autoscaler</h2>&#13;
&#13;
<p>For reasons covered earlier in <a data-type="xref" href="#types_of_scaling">“Types of Scaling”</a>, vertically scaling workloads is a less common requirement.<a data-primary="Vertical Pod Autoscaler (VPA)" data-type="indexterm" id="ix_VPA"/><a data-primary="workloads" data-secondary="autoscaling" data-tertiary="Vertical Pod Autoscaler" data-type="indexterm" id="ix_wklautoVPA"/><a data-primary="autoscaling" data-secondary="workload" data-tertiary="Vertical Pod Autoscaler" data-type="indexterm" id="ix_ASwklVPA"/>  Furthermore, automating vertical scaling is more complex to implement in Kubernetes.<a data-primary="VPA" data-see="Vertical Pod Autoscaler" data-type="indexterm" id="idm45611973959672"/>  While the HPA is included in core Kubernetes, the VPA needs to be implemented by deploying three distinct controller components in addition to the Metrics Server.  For these reasons, the <a href="https://oreil.ly/TxeiY">Vertical Pod Autoscaler (VPA)</a> is less commonly used than the HPA.</p>&#13;
&#13;
<p>The VPA consists of three <a data-primary="Vertical Pod Autoscaler (VPA)" data-secondary="components" data-type="indexterm" id="idm45611973957432"/>distinct components:</p>&#13;
<dl>&#13;
<dt>Recommender</dt>&#13;
<dd>&#13;
<p>Determines optimum container CPU and/or memory request values based on usage in the PodMetrics resource for the Pod in question.<a data-primary="Recommender (VPA)" data-type="indexterm" id="idm45611973954392"/></p>&#13;
</dd>&#13;
<dt>Admission plug-in</dt>&#13;
<dd>&#13;
<p>Mutates the resource requests and limits on new Pods when they are created based on the recommender’s recommendation.</p>&#13;
</dd>&#13;
<dt>Updater</dt>&#13;
<dd>&#13;
<p>Evicts Pods so that they may have updated values applied by the admission &#13;
<span class="keep-together">plug-in</span>.<a data-primary="Updater (VPA)" data-type="indexterm" id="idm45611973950280"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#vertical_pod_autoscaling">Figure 13-2</a> illustrates the interaction of components with the VPA.</p>&#13;
&#13;
<figure><div class="figure" id="vertical_pod_autoscaling">&#13;
<img alt="prku 1302" src="assets/prku_1302.png"/>&#13;
<h6><span class="label">Figure 13-2. </span>Vertical Pod autoscaling.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The desired state for a VPA is declared in the<a data-primary="Vertical Pod Autoscaler (VPA)" data-secondary="configuring vertical autoscaling" data-type="indexterm" id="idm45611973945944"/><a data-primary="Pods" data-secondary="Pod resource and VerticalPodAutoscaler" data-type="indexterm" id="idm45611973944888"/> VerticalPodAutoscaler resource as demonstrated in <a data-type="xref" href="#ex_13-2">Example 13-2</a>.</p>&#13;
<div data-type="example" id="ex_13-2">&#13;
<h5><span class="label">Example 13-2. </span>A Pod resource and the VerticalPodAutoscaler resource that configures vertical autoscaling</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Pod</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">containers</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code>    </code><code class="nt">image</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample-image:1.0</code><code>&#13;
</code><code>    </code><code class="nt">resources</code><code class="p">:</code><code>  </code><a class="co" href="#callout_autoscaling_CO2-1" id="co_autoscaling_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>      </code><code class="nt">requests</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">cpu</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">100m</code><code>&#13;
</code><code>        </code><code class="nt">memory</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">50Mi</code><code>&#13;
</code><code>      </code><code class="nt">limits</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">cpu</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">100m</code><code>&#13;
</code><code>        </code><code class="nt">memory</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">50Mi</code><code>&#13;
</code><code class="nn">---</code><code>&#13;
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">autoscaling.k8s.io/v1beta2</code><code class="s">"</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">VerticalPodAutoscaler</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">targetRef</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">v1</code><code class="s">"</code><code>&#13;
</code><code>    </code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Pod</code><code>&#13;
</code><code>    </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">sample</code><code>&#13;
</code><code>  </code><code class="nt">resourcePolicy</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">containerPolicies</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">containerName</code><code class="p">:</code><code> </code><code class="s">'</code><code class="s">*</code><code class="s">'</code><code>  </code><a class="co" href="#callout_autoscaling_CO2-2" id="co_autoscaling_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>        </code><code class="nt">minAllowed</code><code class="p">:</code><code>  </code><a class="co" href="#callout_autoscaling_CO2-3" id="co_autoscaling_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>          </code><code class="nt">cpu</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">100m</code><code>&#13;
</code><code>          </code><code class="nt">memory</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">50Mi</code><code>&#13;
</code><code>        </code><code class="nt">maxAllowed</code><code class="p">:</code><code>  </code><a class="co" href="#callout_autoscaling_CO2-4" id="co_autoscaling_CO2-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>          </code><code class="nt">cpu</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">1</code><code>&#13;
</code><code>          </code><code class="nt">memory</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">500Mi</code><code>&#13;
</code><code>        </code><code class="nt">controlledResources</code><code class="p">:</code><code> </code><code class="p-Indicator">[</code><code class="s">"</code><code class="s">cpu</code><code class="s">"</code><code class="p-Indicator">,</code><code> </code><code class="s">"</code><code class="s">memory</code><code class="s">"</code><code class="p-Indicator">]</code><code>  </code><a class="co" href="#callout_autoscaling_CO2-5" id="co_autoscaling_CO2-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>  </code><code class="nt">updatePolicy</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">updateMode</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Recreate</code><code>  </code><a class="co" href="#callout_autoscaling_CO2-6" id="co_autoscaling_CO2-6"><img alt="6" src="assets/6.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_autoscaling_CO2-1" id="callout_autoscaling_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The VPA will maintain the requests:limit ratio when updating values.  In this guaranteed QOS example, any change to requests will result in an identical change to the limits.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO2-2" id="callout_autoscaling_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This scaling policy will apply to every container—just one in this example.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO2-3" id="callout_autoscaling_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Resources requests will not be set below these values.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO2-4" id="callout_autoscaling_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Resources requests will not be set above these values.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO2-5" id="callout_autoscaling_CO2-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Specifies the resources being autoscaled.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO2-6" id="callout_autoscaling_CO2-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>There are three <code>updateMode</code> options. <code>Recreate</code> mode will activate autoscaling. <code>Initial</code> mode will apply admission control to set resource values when created, but will never evict any Pods. <code>Off</code> mode will recommend resource values but never automatically change them.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>We very rarely see the VPA in full <code>Recreate</code> mode in the field.  However, using it in <code>Off</code> mode can also be valuable.  While comprehensive load testing and profiling of applications is recommended and preferable before they go to production, that’s not always the reality.  In corporate environments with deadlines, workloads are often deployed to production before the resource consumption profile is well understood.  This commonly leads to overrequested resources as a safety measure, which often results in poor utilization of infrastructure.  In these cases, the VPA can be used to recommend values that are then evaluated and manually updated by engineers once production load has been applied.  This gives them peace of mind that workloads will not be evicted at peak usage times, which is particularly important if an app does not yet gracefully shut down.<a data-primary="Vertical Pod Autoscaler (VPA)" data-secondary="recommendations" data-type="indexterm" id="idm45611973531528"/>  But, because the VPA recommends values, it saves some of the toil in reviewing resource usage metrics and determining optimum values.  In this use case, it is not an autoscaler, but rather a resource tuning aid.</p>&#13;
&#13;
<p>To get <a data-primary="Recommender (VPA)" data-secondary="recommendations from" data-type="indexterm" id="idm45611973529528"/>recommendations from a VPA in <code>Off</code> mode, run <code>kubectl describe vpa &lt;vpa name&gt;</code>.  You will get an output similar to <a data-type="xref" href="#ex_13-3">Example 13-3</a> under the <code>Status</code> section.</p>&#13;
<div data-type="example" id="ex_13-3">&#13;
<h5><span class="label">Example 13-3. </span>Vertical Pod Autoscaler recommendation</h5>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">  Recommendation:&#13;
    Container Recommendations:&#13;
      Container Name:  coredns&#13;
      Lower Bound:&#13;
        Cpu:     25m&#13;
        Memory:  262144k&#13;
      Target:&#13;
        Cpu:     25m&#13;
        Memory:  262144k&#13;
      Uncapped Target:&#13;
        Cpu:     25m&#13;
        Memory:  262144k&#13;
      Upper Bound:&#13;
        Cpu:     427m&#13;
        Memory:  916943343</pre></div>&#13;
&#13;
<p>It will provide recommendations for each container.  Use the <code>Target</code> value as a baseline recommendation for the CPU and memory requests.<a data-primary="Vertical Pod Autoscaler (VPA)" data-startref="ix_VPA" data-type="indexterm" id="idm45611973522088"/><a data-primary="workloads" data-secondary="autoscaling" data-startref="ix_wklautoVPA" data-tertiary="Vertical Pod Autoscaler" data-type="indexterm" id="idm45611973521240"/><a data-primary="autoscaling" data-secondary="workload" data-startref="ix_ASwklVPA" data-tertiary="Vertical Pod Autoscaler" data-type="indexterm" id="idm45611973644808"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Autoscaling with Custom Metrics" data-type="sect2"><div class="sect2" id="idm45611973965896">&#13;
<h2>Autoscaling with Custom Metrics</h2>&#13;
&#13;
<p>If CPU and memory consumption are not good metrics by which to scale a particular workload, you can leverage custom metrics as an alternative.<a data-primary="metrics" data-secondary="custom" data-tertiary="using in workload autoscaling" data-type="indexterm" id="idm45611973641656"/><a data-primary="workloads" data-secondary="autoscaling" data-tertiary="using custom metrics" data-type="indexterm" id="idm45611973640440"/><a data-primary="autoscaling" data-secondary="workload" data-tertiary="using custom metrics" data-type="indexterm" id="idm45611973639224"/>  We can still use tools like the HPA.  However, we will change the source of metrics used to trigger the autoscaling.  The first step is to expose the appropriate custom metrics from your application. <a data-type="xref" href="ch14.html#application_considerations_chapter">Chapter 14</a> addresses how to go about doing this.</p>&#13;
&#13;
<p>Next, you will need to expose the custom metrics to the autoscaler.  This will require a custom metrics server that will be used instead of the Kubernetes Metrics Server that we looked at earlier.<a data-primary="Prometheus" data-secondary="autoscaling with custom metrics from" data-type="indexterm" id="idm45611973517016"/>  Some vendors, such as Datadog, provide systems to do this in Kubernetes.  You can also do it with Prometheus, assuming you have a Prometheus server that is scraping and storing the app’s custom metrics, which is covered in <a data-type="xref" href="ch10.html#chapter10">Chapter 10</a>.  In this case, we can use the <a href="https://oreil.ly/vDgk3">Prometheus Adapter</a> to serve the custom metrics.<a data-primary="Prometheus Adapter" data-type="indexterm" id="idm45611973514104"/></p>&#13;
&#13;
<p>The Prometheus Adapter will retrieve the custom metrics from Prometheus’ HTTP API and expose them through the Kubernetes API.  Like the Metrics Server, the Prometheus Adapter uses Kubernetes API aggregation to instruct Kubernetes to proxy requests for metrics APIs to the Prometheus Adapter.  In fact, in addition to the custom metrics API, the Prometheus Adapter implements the resource metrics API that allows you to entirely replace the Metrics Server functionality with the Prometheus Adapter.  Additionally, it implements the external metrics API that offers the opportunity to scale an application based on metrics external to the cluster.</p>&#13;
&#13;
<p>When leveraging custom metrics for horizontal autoscaling, Prometheus scrapes those metrics from your app.  The Prometheus Adapter gets those metrics from Prometheus and exposes them through the Kubernetes API server.  The HPA queries those metrics and scales your application accordingly, as shown in <a data-type="xref" href="#horizontal_pod_autoscaling_with_custom_metrics">Figure 13-3</a>.</p>&#13;
&#13;
<p>While leveraging custom metrics in this way introduces some added complexity, if you are already exposing useful metrics from your workloads and using Prometheus to monitor them, replacing Metrics Server with the Prometheus Adapter is not a huge leap.  And the additional autoscaling opportunities it opens up make it well worth considering.</p>&#13;
&#13;
<figure><div class="figure" id="horizontal_pod_autoscaling_with_custom_metrics">&#13;
<img alt="prku 1303" src="assets/prku_1303.png"/>&#13;
<h6><span class="label">Figure 13-3. </span>Horizontal Pod autoscaling with custom metrics.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Proportional Autoscaler" data-type="sect2"><div class="sect2" id="idm45611973673752">&#13;
<h2>Cluster Proportional Autoscaler</h2>&#13;
&#13;
<p>The <a href="https://oreil.ly/2ATBG">Cluster Proportional Autoscaler</a> (CPA) is a horizontal workload autoscaler that scales replicas based on the number of nodes (or a subset of nodes) in the cluster.<a data-primary="workloads" data-secondary="autoscaling" data-tertiary="Cluster Proportional Autoscaler" data-type="indexterm" id="idm45611973671416"/><a data-primary="autoscaling" data-secondary="workload" data-tertiary="Cluster Proportional Autoscaler" data-type="indexterm" id="idm45611973670136"/><a data-primary="Cluster Proportional Autoscaler (CPA)" data-type="indexterm" id="idm45611973668904"/>  So, unlike the HPA, it does not rely on any of the metrics APIs.  Therefore, it does not have a dependency on the Metrics Server or Prometheus Adapter.  Also, it is not configured with a Kubernetes resource, but rather uses flags to configure target workloads and a ConfigMap for scaling configuration. <a data-type="xref" href="#cluster_proportional_autoscaling">Figure 13-4</a> illustrates the CPA’s much simpler operational model.</p>&#13;
&#13;
<figure><div class="figure" id="cluster_proportional_autoscaling">&#13;
<img alt="prku 1304" src="assets/prku_1304.png"/>&#13;
<h6><span class="label">Figure 13-4. </span>Cluster proportional autoscaling.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The CPA has a narrower use case.  Workloads that need to scale in proportion to the cluster are generally limited to platform services.  When considering the CPA, evaluate whether an HPA would provide a better solution, especially if you are already leveraging the HPA with other workloads.  If you are already using HPAs, you will have the Metrics Server or Prometheus Adapter already deployed to implement the necessary metrics APIs.  So deploying another autoscaler, and the maintenance overhead that goes with it, may not be the best option.  Alternatively, in a cluster where HPAs are <em>not</em> already in use, and the CPA provides the functionality you need, it becomes more attractive due to its simple operational model.</p>&#13;
&#13;
<p>There are two scaling methods used by the CPA:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The linear method scales your application in direct proportion to how many nodes or cores are in the cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>The ladder method uses a step function to determine the proportion of nodes:replicas and/or cores:replicas.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We have seen the CPA used with success for services like cluster DNS where clusters are allowed to scale to hundreds of worker nodes.  In cases such as this, the traffic and demand for a service at 5 nodes is going to be drastically different than at 300 nodes, so this approach can be quite useful.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Custom Autoscaling" data-type="sect2"><div class="sect2" id="idm45611973496760">&#13;
<h2>Custom Autoscaling</h2>&#13;
&#13;
<p>On the subject of workload autoscaling, so far we’ve discussed some specific tools available in the community: the HPA, VPA, and CPA along with the Metrics Server and Prometheus Adapter.<a data-primary="workloads" data-secondary="autoscaling" data-tertiary="custom autoscaling" data-type="indexterm" id="idm45611973495416"/><a data-primary="autoscaling" data-secondary="workload" data-tertiary="custom autoscaling" data-type="indexterm" id="idm45611973494328"/>  But autoscaling your workloads is not limited to this tool set.  Any automated method you can employ that implements the scaling behavior you require falls into the same category.  For example, if you know the days and times when traffic increases for your application, you can implement something as simple as a Kubernetes CronJob that updates the replica count on the relevant Deployment.  In fact, if you can leverage a simple, straightforward method such as this, lean toward the simpler solution.  A system with fewer moving parts is less likely to produce unexpected results.</p>&#13;
&#13;
<p>This wraps up the approaches to autoscaling workloads.  We’ve looked at several ways to approach this using core Kubernetes, community-developed add-on components, and custom solutions.  Next, we’re going to look at autoscaling the substrate that hosts these workloads: the Kubernetes cluster itself.<a data-primary="autoscaling" data-secondary="workload" data-startref="ix_ASwkl" data-type="indexterm" id="idm45611973491816"/><a data-primary="workloads" data-secondary="autoscaling" data-startref="ix_wklauto" data-type="indexterm" id="idm45611973490728"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Autoscaling" data-type="sect1"><div class="sect1" id="cluster_autoscaling">&#13;
<h1>Cluster Autoscaling</h1>&#13;
&#13;
<p>The Kubernetes <a href="https://oreil.ly/Q5Xdp">Cluster Autoscaler (CA)</a> provides an automated solution for horizontally scaling the worker nodes in your cluster.<a data-primary="autoscaling" data-secondary="cluster" data-type="indexterm" id="ix_AScls"/><a data-primary="Cluster Autoscaler (CA)" data-type="indexterm" id="ix_CAclsau"/><a data-primary="clusters" data-secondary="autoscaling" data-type="indexterm" id="ix_clsAS"/>  It provides a solution to one of the limitations of HPAs and can alleviate significant toil around capacity and cost management for your Kubernetes infrastructure.<a data-primary="autoscaling" data-secondary="cluster" data-tertiary="Cluster Autoscaler" data-type="indexterm" id="ix_ASclsCA"/><a data-primary="CA" data-see="Cluster Autoscaler" data-type="indexterm" id="idm45611973482184"/></p>&#13;
&#13;
<p>As platform teams adopt your Kubernetes-based platform, you will need to manage the clusters’ capacity as new tenants are onboarded.  This can be a manual, routine review process.  It can also be alert-driven, whereby you use alerting rules on usage metrics to notify you of situations where workers need to be added or removed.  Or you can fully automate the operation such that you can simply add and remove tenants and let the CA manage the cluster scaling to accommodate.</p>&#13;
&#13;
<p>Furthermore, if you are leveraging workload autoscaling with significant fluctuation in resource consumption, the story for CA becomes even more compelling.<a data-primary="Horizontal Pod Autoscaler (HPA)" data-secondary="replica count increase as load increases" data-type="indexterm" id="idm45611973479896"/>  As load increases on an HPA-managed workload, its replica count will increase.  If you run out of compute resources in your cluster, some of the Pods will not be scheduled and remain in a <code>Pending</code> state.  CA looks for this exact condition, calculates the number of nodes needed to satisfy the shortage, and adds new nodes to your cluster.  The diagram in <a data-type="xref" href="#cluster_autoscaler_scaling_out_nodes_in_response_to_pod_replicas_scaling_out">Figure 13-5</a> shows the cluster scaling out to accommodate a horizontally scaling application.</p>&#13;
&#13;
<figure><div class="figure" id="cluster_autoscaler_scaling_out_nodes_in_response_to_pod_replicas_scaling_out">&#13;
<img alt="prku 1305" src="assets/prku_1305.png"/>&#13;
<h6><span class="label">Figure 13-5. </span>Cluster Autoscaler scaling out nodes in response to Pod replicas scaling out.</h6>&#13;
</div></figure>&#13;
&#13;
<p>On the other side of the coin, when load reduces and the HPA scales in the Pods for an application, the CA will look for nodes that have been underutilized for an extended period.  If the Pods on the underutilized nodes can be rescheduled to other nodes in the cluster, the CA will deprovision the underutilized nodes to scale the cluster in.</p>&#13;
&#13;
<p>One thing to keep in mind when you invoke this dynamic management of worker nodes is that it will inevitably shuffle the distribution of Pods across your nodes.  The Kubernetes scheduler will generally spread Pods evenly around your worker nodes when they are first created.  However, once a Pod is running, the scheduling decision that determined its home will not be reevaluated unless it is evicted.  So when a particular application horizontally scales out and then back in, you may end up with Pods unevenly spread across your worker nodes.  In some cases you may end up with many replicas for a Deployment clustered on just a few nodes.  If this presents a threat to a workload’s node failure tolerance, you can use the <a href="https://github.com/kubernetes-sigs/descheduler">Kubernetes descheduler</a> to evict them according to different policies.<a data-primary="descheduler" data-type="indexterm" id="idm45611973473176"/>  Once evicted, the Pods will be rescheduled.  This will help rebalance their distribution across nodes.  We have not found many cases where there was a genuine compelling need to do this, but it is an available option.</p>&#13;
&#13;
<p>As you might imagine, there are infrastructure management concerns to plan for if you are considering cluster autoscaling.  Firstly, you will need to use one of the supported cloud providers that are documented in the project repo.  Next you will have to give permissions to CA to create and destroy machines for you.</p>&#13;
&#13;
<p>These infrastructure management concerns change somewhat if you use the CA with the <a href="https://github.com/kubernetes-sigs/cluster-api">Cluster API</a> project.  Cluster API<a data-primary="Cluster API" data-secondary="using Cluster Autoscaler with" data-type="indexterm" id="idm45611973470248"/> uses its own Kubernetes operators to manage cluster infrastructure.  In this case, instead of connecting directly with the cloud provider to add and remove worker nodes, CA offloads this operation to Cluster API.<a data-primary="MachineDeployment resource" data-type="indexterm" id="idm45611973469048"/>  The CA simply updates the replicas in a <code>MachineDeployment</code> resource, which is reconciled by Cluster API controllers.  This removes the need to use a cloud provider that’s compatible with CA (however, you <em>will</em> need to check whether there is a Cluster API provider for your cloud provider).  The permissions issue is also offloaded to Cluster API components.  This is a better model in many ways.  However, Cluster API is commonly implemented using management clusters.  This introduces external dependencies for cluster autoscaling that should be considered.  This topic is covered further in <a data-type="xref" href="ch02.html#management_clusters">“Management clusters”</a>.</p>&#13;
&#13;
<p>The scaling behavior of CA is quite configurable.<a data-primary="Cluster Autoscaler (CA)" data-secondary="configurable scaling behavior" data-type="indexterm" id="idm45611973465992"/>  The CA is configured using flags that are documented in the project’s <a href="https://oreil.ly/DzQ0J">FAQ on GitHub</a>.  <a data-type="xref" href="#ex_13-4">Example 13-4</a> shows a CA Deployment manifest for AWS and includes examples of how to set some common flags.<a data-primary="AWS (Amazon Web Services)" data-secondary="Cluster Autoscaler Deployment manifest targeting" data-type="indexterm" id="idm45611973463544"/><a data-primary="Cluster Autoscaler (CA)" data-secondary="Deployment manifest for AWS" data-type="indexterm" id="idm45611973462696"/></p>&#13;
<div data-type="example" id="ex_13-4">&#13;
<h5><span class="label">Example 13-4. </span>CA Deployment manifest targeting an Amazon Web Services autoscaling group</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">apps/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Deployment</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">aws-cluster-autoscaler</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">replicas</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">1</code><code>&#13;
</code><code>  </code><code class="nt">selector</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">matchLabels</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">app.kubernetes.io/name</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">aws-cluster-autoscaler</code><code class="s">"</code><code>&#13;
</code><code>  </code><code class="nt">template</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">labels</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">app.kubernetes.io/name</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">aws-cluster-autoscaler</code><code class="s">"</code><code>&#13;
</code><code>    </code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">containers</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="p-Indicator">-</code><code> </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">aws-cluster-autoscaler</code><code>&#13;
</code><code>          </code><code class="nt">image</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v1.18</code><code class="s">"</code><code>&#13;
</code><code>          </code><code class="nt">command</code><code class="p">:</code><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">./cluster-autoscaler</code><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--cloud-provider=aws</code><code>  </code><a class="co" href="#callout_autoscaling_CO3-1" id="co_autoscaling_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--namespace=kube-system</code><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--nodes=1:10:worker-auto-scaling-group</code><code>  </code><a class="co" href="#callout_autoscaling_CO3-2" id="co_autoscaling_CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--logtostderr=true</code><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--stderrthreshold=info</code><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--v=4</code><code>&#13;
</code><code>          </code><code class="nt">env</code><code class="p">:</code><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">AWS_REGION</code><code>&#13;
</code><code>              </code><code class="nt">value</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">us-east-2</code><code class="s">"</code><code>&#13;
</code><code>          </code><code class="nt">livenessProbe</code><code class="p">:</code><code>&#13;
</code><code>            </code><code class="nt">httpGet</code><code class="p">:</code><code>&#13;
</code><code>              </code><code class="nt">path</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">/health-check</code><code>&#13;
</code><code>              </code><code class="nt">port</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">8085</code><code>&#13;
</code><code>          </code><code class="nt">ports</code><code class="p">:</code><code>&#13;
</code><code>            </code><code class="p-Indicator">-</code><code> </code><code class="nt">containerPort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">8085</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_autoscaling_CO3-1" id="callout_autoscaling_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Configures the supported cloud provider; AWS in this case.</p></dd>&#13;
<dt><a class="co" href="#co_autoscaling_CO3-2" id="callout_autoscaling_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This flag configures the CA to update an AWS autoscaling group called <code>worker-auto-scaling-group</code>.  It allows CA to scale the number of machines in this group between 1 and 10.<a data-primary="autoscaling" data-secondary="cluster" data-startref="ix_ASclsCA" data-tertiary="Cluster Autoscaler" data-type="indexterm" id="idm45611973166376"/></p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Cluster autoscaling can be extremely useful.<a data-primary="autoscaling" data-secondary="cluster" data-tertiary="benefits and concerns with" data-type="indexterm" id="idm45611973164488"/>  It unlocks one of the compelling benefits offered by cloud native infrastructure.  However, it introduces nontrivial complexity.  Ensure you load test and understand well how the system will behave before you rely on it to autonomously manage the scaling of business-critical platforms in production.  One important consideration is to clearly understand the upper limits that you will be reaching with your cluster.  If your platform hosts significant workload capacity and you allow your cluster to scale to hundreds of nodes, understand where it will scale to before components of the platform start to introduce bottlenecks.  More discussion around cluster sizing can be found in <a data-type="xref" href="ch02.html#deployment_models">Chapter 2</a>.</p>&#13;
&#13;
<p>Another consideration with cluster autoscaling is the speed at which your clusters will scale when the need arises.  This is where overprovisioning may help.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Cluster Overprovisioning" data-type="sect2"><div class="sect2" id="idm45611973160984">&#13;
<h2>Cluster Overprovisioning</h2>&#13;
&#13;
<p>It’s important to remember that Cluster Autoscaler responds to <code>Pending</code> Pods that couldn’t be scheduled due to insufficient compute resources in the cluster.<a data-primary="autoscaling" data-secondary="cluster" data-tertiary="cluster overprovisioning" data-type="indexterm" id="idm45611973158520"/><a data-primary="clusters" data-secondary="autoscaling" data-tertiary="cluster overprovisioning" data-type="indexterm" id="idm45611973353176"/>  So at the moment the CA takes action to scale out the cluster nodes, your cluster is already full.  This means that, if not managed properly, your scaling workloads could suffer from a shortage of capacity for the time it takes for new nodes to become available for scheduling.  This is where <a href="https://oreil.ly/vXij5">cluster-overprovisioner</a> can help.</p>&#13;
&#13;
<p>First it’s important to understand how long it takes for new nodes to spin up, join the cluster, and become ready to accept workloads.  Once this is understood, you can address the best solution for your situation:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Set the target utilization in your HPAs sufficiently low so that your workloads are scaled out well before the application is at full capacity.  This could provide the buffer that allows for time to provision nodes.  It relieves the need for overprovisioning the cluster, but if you need to account for particularly sharp increases in load, you may need to set that target utilization too low to guard against capacity shortages.  This leads to a situation where you have chronically overprovisioned workload capacity to account for rare events.</p>&#13;
</li>&#13;
<li>&#13;
<p>Another solution is to use cluster overprovisioning.  With this method, you put empty nodes on standby to provide the buffer for workloads that are scaling out.  This will relieve the need to set target utilization on HPAs artificially low in preparation for high load events.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Cluster overprovisioning works by deploying Pods that do the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Request enough resources to reserve virtually all resources for a node</p>&#13;
</li>&#13;
<li>&#13;
<p>Consume no actual resources</p>&#13;
</li>&#13;
<li>&#13;
<p>Use a priority class that causes them to be evicted as soon as any other Pod &#13;
<span class="keep-together">needs it</span></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With the resource requests on the overprovisioner Pod set to reserve an entire node, you can then adjust the number of standby nodes with the number of replicas on the overprovisioner Deployment.  Overprovisioning for a particular event or marketing campaign can be achieved by simply increasing the number of replicas on the overprovisioner Deployment.</p>&#13;
&#13;
<p><a data-type="xref" href="#cluster_overprovisioning">Figure 13-6</a> illustrates what this looks like.  This illustration shows just a single Pod replica, but it can be as many as you need to provide adequate buffer for scaling events.</p>&#13;
&#13;
<figure><div class="figure" id="cluster_overprovisioning">&#13;
<img alt="prku 1306" src="assets/prku_1306.png"/>&#13;
<h6><span class="label">Figure 13-6. </span>Cluster overprovisioning.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The node occupied by the overprovisioner Pod is now on standby for whenever it becomes needed by another Pod in the cluster.  You can accomplish this by creating a priority class with <code>value: -1</code> and then applying this to the overprovisioner Deployment.  This will make all other workloads a higher priority by default.  Should a Pod from another workload need the resources, the overprovisioner Pod will be immediately evicted, making way for the scaling workload.  The overprovisioner Pod will go into a <code>Pending</code> state, which will trigger the Cluster Autoscaler to provision a new node to sit on standby, as shown in <a data-type="xref" href="#scaling_out_with_cluster_overprivisoner">Figure 13-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="scaling_out_with_cluster_overprivisoner">&#13;
<img alt="prku 1307" src="assets/prku_1307.png"/>&#13;
<h6><span class="label">Figure 13-7. </span>Scaling out with cluster-overprovisioner.</h6>&#13;
</div></figure>&#13;
&#13;
<p>With Cluster Autoscaler and cluster-overprovisioner, you have effective mechanisms to horizontally scale your Kubernetes clusters, which dovetails very nicely with horizontally scaling workloads.  We haven’t covered vertically scaling clusters here because we have not found a use for it that isn’t solved by horizontal scaling.<a data-primary="autoscaling" data-secondary="cluster" data-startref="ix_AScls" data-type="indexterm" id="idm45611973333720"/><a data-primary="Cluster Autoscaler (CA)" data-startref="ix_CAclsau" data-type="indexterm" id="idm45611973332472"/><a data-primary="clusters" data-secondary="autoscaling" data-startref="ix_clsAS" data-type="indexterm" id="idm45611973331528"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611973160296">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>If you have applications that are subject to significant changes in capacity requirements, lean toward using horizontal scaling, if possible.  Develop the apps that you will autoscale to play nicely with being stopped and started frequently and expose custom metrics if CPU or memory are not good metrics to trigger scaling.  Test your autoscaling to ensure it behaves as you expect to optimize efficiency and end-user experience.  If your workloads will scale beyond the capacity of your cluster, consider autoscaling the cluster itself.  And if your scaling events are particularly sharp, consider putting nodes on standby with cluster-overprovisioner.<a data-primary="autoscaling" data-startref="ix_autos" data-type="indexterm" id="idm45611973328616"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>