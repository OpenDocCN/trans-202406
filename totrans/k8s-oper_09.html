<html><head></head><body><section data-pdf-bookmark="Chapter 9. Operator Philosophy" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm45261330713752">&#13;
<h1><span class="label">Chapter 9. </span>Operator Philosophy</h1>&#13;
&#13;
&#13;
<p>We’ve noted the problems Operators aim to solve, and you’ve stepped through detailed examples of how to build Operators with the SDK.<a data-primary="Operator philosophy" data-type="indexterm" id="ix_Opphil"/> You’ve also seen how to distribute Operators in a coherent way with OLM. Let’s try to connect those tactics to the strategic ideas that underpin them to understand an existential question: what are Operators for?</p>&#13;
&#13;
<p>The Operator concept descends from Site Reliability Engineering (SRE). Back in <a data-type="xref" href="ch01.html#chapter_introduction">Chapter 1</a>, we talked about Operators as software SREs. Let’s review some key SRE tenets to understand how Operators apply them.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="SRE for Every Application" data-type="sect1"><div class="sect1" id="idm45261328117144">&#13;
<h1>SRE for Every Application</h1>&#13;
&#13;
<p>SRE began at Google in response to the challenges of running large systems with ever-increasing numbers of users and features.<a data-primary="Operator philosophy" data-secondary="SRE for every application" data-type="indexterm" id="idm45261328115736"/><a data-primary="site reliability engineering (SRE) for every application" data-type="indexterm" id="idm45261328114696"/> A key SRE objective is allowing services to grow without forcing the teams that run them to grow in direct proportion. To run systems at dramatic scale without a team of dramatic size, SREs write code to handle deployment, operations, and maintenance tasks. SREs create software that runs other software, keeps it running, and manages it over time. SRE is a wider set of management and engineering techniques with automation as a central principle. You may have heard its goal referred to by different names, like “autonomous” or “self-driving” software. In the Operator Maturity Model we introduced in <a data-type="xref" href="ch04.html#fig4-1">Figure 4-1</a>, we call it “Auto Pilot.”</p>&#13;
&#13;
<p>Operators and the Operator Framework make it easier to implement this kind of automation for applications that run on Kubernetes. Kubernetes orchestrates service deployments, making some of the work of horizontal scaling or failure recovery automatic for stateless applications. It represents distributed system resources as API abstractions. Using Operators, developers can extend those practices to complex applications.</p>&#13;
&#13;
<p>The well-known “SRE book” <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/site-reliability-engineering/9781491929117/"><em>Site Reliability Engineering</em></a> (O’Reilly), by Betsy Beyer et al. (eds.), is the authoritative guide to SRE principles. Google engineer Carla Geisser’s comments in it typify the automation element of SRE: “If a human operator needs to touch your system during normal operations, you have a bug.”<sup><a data-type="noteref" href="ch09.html#idm45261328109704" id="idm45261328109704-marker">1</a></sup> SREs write code to fix those bugs. Operators are a logical place to program those fixes for a broad class of applications on Kubernetes. An Operator reduces human intervention bugs by automating the regular chores that keep its application running.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Toil Not, Neither Spin" data-type="sect1"><div class="sect1" id="idm45261328108056">&#13;
<h1>Toil Not, Neither Spin</h1>&#13;
&#13;
<p>SRE tries to reduce toil by creating software to perform the tasks required to operate a system. <em>Toil</em> is defined in this context<a data-primary="site reliability engineering (SRE) for every application" data-secondary="toil not, neither spin philosophy" data-type="indexterm" id="ix_SREtoil"/><a data-primary="toil not, neither spin philosophy" data-type="indexterm" id="ix_toilspin"/> as work that is “automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.”<sup><a data-type="noteref" href="ch09.html#idm45261328103512" id="idm45261328103512-marker">2</a></sup></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Automatable: Work Your Computer Would Like" data-type="sect2"><div class="sect2" id="idm45261328102408">&#13;
<h2>Automatable: Work Your Computer Would Like</h2>&#13;
&#13;
<p>Work is automatable if a machine could do it. If a task needs the discernment of human judgment, a machine can’t do it. For example, expense reports are subjected to a variety of machine-driven boundary checking, but usually some final human review is required—of items the automated process flagged as out of bounds, if not of every receipt. The approval of reports within bounds may be automatable; the final acceptance or rejection of out-of-bounds cases may not. Work that could be automated by software should be automated by software if it is also repetitive. The cost of building software to perform a repetitive task can be amortized over a lifetime of repetitions.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Running in Place: Work of No Enduring Value" data-type="sect2"><div class="sect2" id="idm45261328100200">&#13;
<h2>Running in Place: Work of No Enduring Value</h2>&#13;
&#13;
<p>It can be uncomfortable to think of some work as having no value, but in SRE terms, work is “devoid of enduring value” if doing the work doesn’t change the service. Backing up a database server is one example. The database doesn’t go faster, serve more requests, or become inherently more reliable when you back it up. It also doesn’t stop working. Yet despite having no enduring value, backups are clearly worth doing. This kind of work often makes a good job for an Operator.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Growing Pains: Work That Expands with the System" data-type="sect2"><div class="sect2" id="idm45261328098088">&#13;
<h2>Growing Pains: Work That Expands with the System</h2>&#13;
&#13;
<p>You might design a service so that it scales in the horizontal plane, to serve more requests or run more instances of the service. But if adding a new instance requires an engineer to configure computers and wire them to a network, scaling the service is anything but automatic. In the worst cases of this kind of toil, ops work might scale linearly with your service. Every 10% of service growth—10% more users, 10% more requests per second, or a new feature that uses 10% more CPU—means 10% more custodial labor.<a data-primary="toil not, neither spin philosophy" data-startref="ix_toilspin" data-type="indexterm" id="idm45261328096200"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Manual scaling: Just like in the bad old days" data-type="sect3"><div class="sect3" id="idm45261328095032">&#13;
<h3>Manual scaling: Just like in the bad old days</h3>&#13;
&#13;
<p>Imagine running the stateless web server from <a data-type="xref" href="ch01.html#chapter_introduction">Chapter 1</a>. You deploy three instances on three VMs.<a data-primary="scaling" data-secondary="manual" data-type="indexterm" id="idm45261328092744"/> To add more web server capacity, you spin up new VMs, assign them (unique) IP addresses, and assign (per-IP) ports where the web server binaries listen. Next, you inform the load balancer of the new endpoints so it can route some requests there.</p>&#13;
&#13;
<p>As designed and provisioned, it’s true that your simple stateless web server can grow with demand. It can serve more users and add more features by spreading an increasing load over multiple instances. But the team that runs the service will always have to grow along with it. This effect gets worse as the system gets larger, because adding one VM won’t meaningfully increase the capacity of a thousand instances.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Automating horizontal scaling: Kubernetes replicas" data-type="sect3"><div class="sect3" id="idm45261328090344">&#13;
<h3>Automating horizontal scaling: Kubernetes replicas</h3>&#13;
&#13;
<p>If you deploy your stateless web server on Kubernetes instead, you can scale it up and down with little more than a <code>kubectl</code> command. <a data-primary="scaling" data-secondary="automated horizontal scaling, Kubernetes replicas" data-type="indexterm" id="idm45261328088440"/><a data-primary="replicas" data-secondary="automated horizontal scaling, Kubernetes replicas" data-type="indexterm" id="idm45261328087336"/>This is an example of Kubernetes as an implementation of SRE’s automation principles at the platform level. Kubernetes abstracts the infrastructure where the web servers run and the IP addresses and ports through which they serve connections. You don’t have to configure each new web server instance when scaling up, or deliberately free IPs from your range when scaling down. You don’t have to program the load balancer to deliver traffic to a new instance. Software does those chores instead.<a data-primary="site reliability engineering (SRE) for every application" data-secondary="toil not, neither spin philosophy" data-startref="ix_SREtoil" data-type="indexterm" id="idm45261328085720"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Operators: Kubernetes Application Reliability Engineering" data-type="sect1"><div class="sect1" id="idm45261328084136">&#13;
<h1>Operators: Kubernetes Application Reliability Engineering</h1>&#13;
&#13;
<p>Operators extend Kubernetes to extend the principle of automation to complex, stateful applications running on the platform.<a data-primary="Operators" data-secondary="application reliability engineering with" data-type="indexterm" id="idm45261328082584"/><a data-primary="application reliability engineering with Kubernetes Operators" data-type="indexterm" id="idm45261328081576"/> Consider an Operator that manages an application with its own notions of clustering. When the etcd Operator replaces a failed etcd cluster member, it arranges a new pod’s membership by configuring it and the existing cluster with endpoints and authentication.</p>&#13;
&#13;
<p>If you are on a team responsible for managing internal services, Operators will enable you to capture expert procedures in software and expand the system’s “normal <span class="keep-together">operations”:</span> that is, the set of conditions it can handle automatically. If you’re developing a Kubernetes native application, an Operator lets you think about how users toil to run your app and save them the trouble. You can build Operators that not only run and upgrade an application, but respond to errors or slowing performance.</p>&#13;
&#13;
<p>Control loops in Kubernetes watch resources and react when they don’t match some desired state. Operators let you customize a control loop for resources that represent your application. The first Operator concerns are usually automatic deployment and self-service provisioning of the operand. Beyond that first level of the maturity model, an Operator should know its application’s critical state and how to repair it. The Operator can then be extended to observe key application metrics and act to tune, repair, or report on them.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Managing Application State" data-type="sect2"><div class="sect2" id="idm45261328077768">&#13;
<h2>Managing Application State</h2>&#13;
&#13;
<p>An application often has internal state that needs to be synchronized or maintained between replicas.<a data-primary="application state, managing" data-type="indexterm" id="idm45261328076360"/> Once an Operator handles installation and deployment, it can move farther along the maturity model by keeping such shared state in line among a dynamic group of pods. Any application with its own concept of a cluster, such as many databases and file servers, has this kind of shared application state. It may include authentication resources, replication arrangements, or writer/reader relationships. An Operator can configure this shared state for a new replica, allowing it to expand or restore the application’s cluster with new members. An Operator might rectify external resources its application requires. For example, consider manipulating an external load balancer’s routing rules as replicas die and new ones replace them.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Golden Signals Sent to Software" data-type="sect2"><div class="sect2" id="idm45261328074584">&#13;
<h2>Golden Signals Sent to Software</h2>&#13;
&#13;
<p>Beyer at al. suggest monitoring the “four golden signals”<sup><a data-type="noteref" href="ch09.html#idm45261328073208" id="idm45261328073208-marker">3</a></sup> for the clearest immediate sense of a system’s health. <a data-primary="golden signals sent to software" data-type="indexterm" id="idm45261328071912"/>These characteristics of a service’s basic operation are a good place to start planning what your Operator should watch. In the SRE book that popularized them, golden signals convey something about a system’s state important enough to trigger a call to a human engineer.<sup><a data-type="noteref" href="ch09.html#idm45261328070680" id="idm45261328070680-marker">4</a></sup> When designing Operators, you should think of anything that might result in a call to a person as a bug you can fix.</p>&#13;
&#13;
<p><em>Site Reliability Engineering</em> lists the four golden signals as <em>latency</em>, <em>traffic</em>, <em>errors</em>, and <em>saturation</em>.<sup><a data-type="noteref" href="ch09.html#idm45261328066776" id="idm45261328066776-marker">5</a></sup> Accurate measurements of these four areas,<a data-primary="latency" data-type="indexterm" id="idm45261328065496"/><a data-primary="errors" data-type="indexterm" id="idm45261328064824"/> adapted to the metrics that best represent a particular application’s condition, ensure a reasonable understanding of the application’s health. An Operator can monitor these signals and take application-specific actions when they depict a known condition, problem, or error. Let’s take a closer look:</p>&#13;
<dl>&#13;
<dt>Latency</dt>&#13;
<dd>&#13;
<p>Latency is how long it takes to do something. It is commonly understood as the elapsed time between a request and its completion. For instance, in a network, latency is measured as the time it takes to send a packet of data between two points. An Operator might measure application-specific, internal latencies like the lag time between actions in a game client and responses in the game engine.</p>&#13;
</dd>&#13;
<dt>Traffic</dt>&#13;
<dd>&#13;
<p>Traffic measures how frequently a service is requested. HTTP requests per second is the standard measurement of web service traffic. Monitoring regimes often split this measurement between static assets and those that are dynamically generated. It makes more sense to monitor something like transactions per second for a database or file server.</p>&#13;
</dd>&#13;
<dt>Errors</dt>&#13;
<dd>&#13;
<p>Errors are failed requests, like an HTTP 500-series error. In a web service, you might have an HTTP success code but see scripting exceptions or other client-side errors on the successfully delivered page. It may also be an error to exceed some latency guarantee or performance policy, like a guarantee to respond to any request within a time limit.</p>&#13;
</dd>&#13;
<dt>Saturation</dt>&#13;
<dd>&#13;
<p>Saturation is a gauge of a service’s consumption of a limited resource.<a data-primary="saturation" data-type="indexterm" id="idm45261328057048"/> These measurements focus on the most limited resources in a system, typically CPU, memory, and I/O. There are two key ideas in monitoring saturation. First, performance gets worse even before a resource is 100% utilized. For instance, some filesystems perform poorly when more than about 90% full, because the time it takes to create a file increases as available space decreases. Because of similar effects in nearly any system, saturation monitors should usually respond to a high-water mark of less than 100%. Second, measuring saturation can help you anticipate some problems before they happen. Dividing a file service’s free space by the rate at which an application writes data lets your Operator estimate the time remaining until storage is full.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Operators can iterate toward running your service on auto pilot by measuring and reacting to golden signals that demand increasingly complex operations chores. Apply this analysis each time your application needs human help, and you have a basic plan for iterative development of an Operator.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Seven Habits of Highly Successful Operators" data-type="sect1"><div class="sect1" id="idm45261328054504">&#13;
<h1>Seven Habits of Highly Successful Operators</h1>&#13;
&#13;
<p>Operators grew out of work at CoreOS during 2015 and 2016.<a data-primary="Operator philosophy" data-secondary="habits of highly successful Operators" data-type="indexterm" id="ix_OpPhilHab"/> User experience with the Operators built there and continuing at Red Hat, and in the wider community, have helped refine seven guidelines set out as the concept of Kubernetes Operators solidified:<sup><a data-type="noteref" href="ch09.html#idm45261328051544" id="idm45261328051544-marker">6</a></sup></p>&#13;
<ol>&#13;
<li>&#13;
<p><em>An Operator should run as a single Kubernetes deployment</em>.</p>&#13;
&#13;
<p>You installed the etcd Operator in <a data-type="xref" href="ch02.html#running_operators">Chapter 2</a> from one manifest, without the OLM machinery introduced in <a data-type="xref" href="ch08.html#operator_lifecyle_manager">Chapter 8</a>. While you provide a CSV and other assets to make an OLM bundle for an Operator, OLM still uses that single manifest to deploy the Operator on your behalf.</p>&#13;
&#13;
<p>To illustrate this, although you usually need to configure RBAC and a service account, you can add the etcd Operator to a Kubernetes cluster with a single command. It is just a deployment:</p>&#13;
<pre data-code-language="bash" data-type="programlisting">&#13;
<code class="nv">$ </code><strong><code>kubectl</code><code> </code><code>create</code><code> </code><code>-f</code><code> </code><code>https://raw.githubusercontent.com/</code><code class="se">\&#13;
</code><code>  </code><code>kubernetes-operators-book/chapters/master/ch03/</code><code>&#13;
       </code><code>etcd-operator-deployment.yaml</code></strong><code>&#13;
</code></pre>&#13;
</li>&#13;
<li>&#13;
<p><em>Operators should define new custom resource types on the cluster</em>.</p>&#13;
&#13;
<p>Think of the etcd examples back in <a data-type="xref" href="ch02.html#running_operators">Chapter 2</a>. You created a CRD, and in it you defined<a data-primary="custom resources (CRs)" data-secondary="Operators defining new types on clusters" data-type="indexterm" id="idm45261328030904"/> a new kind of resource, the EtcdCluster. That kind represents instances of the operand, a running etcd cluster managed by the Operator. Users create new application instances by creating new custom resources of the application’s kind.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Operators should use appropriate Kubernetes abstractions whenever possible</em>.</p>&#13;
&#13;
<p>Don’t write new code when API calls can do the same thing in a more consistent and widely tested manner.<a data-primary="Kubernetes" data-secondary="Operators using Kubernetes abstractions where possible" data-type="indexterm" id="idm45261328027944"/> Some quite useful Operators do little more than manipulate some set of standard resources in a way that suits their application.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Operator termination should not affect the operand</em>.</p>&#13;
&#13;
<p>When an Operator stops or is deleted from the cluster, the application it manages should continue to function.<a data-primary="operands" data-secondary="Operator termination not affecting" data-type="indexterm" id="idm45261328007832"/> Return to your cluster and delete either the etcd or the Visitors Site Operator. While you won’t have automatic recovery from failures, you’ll still be able to use the application features of the operand in the absence of the Operator. You can visit the Visitors Site or retrieve a key-value pair from etcd even when the respective Operator isn’t running.</p>&#13;
&#13;
<p>Note that removing a CRD does affect the operand application. In fact, deleting a CRD will in turn delete its CR instances.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>An Operator should understand the resource types created by any previous versions</em>.</p>&#13;
&#13;
<p>Operators should be backward compatible with the structures of their predecessors.&#13;
This places a premium on designing carefully and for simplicity, because the resources you define in version 1 will necessarily live on.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>An Operator should coordinate application upgrades</em>.</p>&#13;
&#13;
<p>Operators should coordinate upgrades of their operands, potentially including rolling upgrades across<a data-primary="application upgrades, Operator coordinating" data-type="indexterm" id="idm45261328002392"/> an application cluster and almost certainly including the ability to roll back to a previous version when there is a problem. Keeping software up to date is necessary toil, because only the latest software has the latest fixes for bugs and security vulnerabilities. Automating this upgrade toil is an ideal job for an Operator.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Operators should be thoroughly tested, including chaos testing</em>.</p>&#13;
&#13;
<p>Your application and its relationship to its infrastructure constitute a complex distributed system.<a data-primary="chaos testing" data-type="indexterm" id="idm45261327999464"/> You’re going to trust your Operator to manage that system. <a href="https://oreil.ly/K8IUR">Chaos testing</a> intentionally causes failures of system components to discover unexpected errors or degradation. It’s good practice to build a test suite that subjects your Operator to simulated errors and the outright disappearance of pods, nodes, and networking to see where failures arise or cascade between components as their dependencies collapse beneath them.<a data-primary="Operator philosophy" data-secondary="habits of highly successful Operators" data-startref="ix_OpPhilHab" data-type="indexterm" id="idm45261327997608"/></p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45261327996008">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Operators tend to advance through phases of maturity ranging from automatic installs, through seamless application upgrades, to a steady normal state of “auto pilot” where they react to and correct emergent issues of performance and stability in their operands. Each phase aims to end a little more human toil.</p>&#13;
&#13;
<p>Making an Operator to distribute, deploy, and manage your application makes it easier to run it on Kubernetes and allows the application to leverage Kubernetes features. An Operator that follows the seven habits outlined here is readily deployed, and can itself be managed through its lifecycle by OLM. That Operator makes its operand easier to run, manage, and potentially to implement. By monitoring its application’s golden signals, an Operator can make informed decisions and free engineers from rote operations tasks.<a data-primary="Operator philosophy" data-startref="ix_Opphil" data-type="indexterm" id="idm45261327993448"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45261328109704"><sup><a href="ch09.html#idm45261328109704-marker">1</a></sup> Beyer et al. (eds.), <em>Site Reliability Engineering</em>, 119.</p><p data-type="footnote" id="idm45261328103512"><sup><a href="ch09.html#idm45261328103512-marker">2</a></sup> Beyer et al. (eds.), <em>Site Reliability Engineering</em>, 120.</p><p data-type="footnote" id="idm45261328073208"><sup><a href="ch09.html#idm45261328073208-marker">3</a></sup> Beyer et al. (eds.), <em>Site Reliability Engineering</em>, 139.</p><p data-type="footnote" id="idm45261328070680"><sup><a href="ch09.html#idm45261328070680-marker">4</a></sup> Beyer et al. (eds.), <em>Site Reliability Engineering</em>, 140.</p><p data-type="footnote" id="idm45261328066776"><sup><a href="ch09.html#idm45261328066776-marker">5</a></sup> Beyer et al. (eds.), <em>Site Reliability Engineering</em>, 139.</p><p data-type="footnote" id="idm45261328051544"><sup><a href="ch09.html#idm45261328051544-marker">6</a></sup> Brandon Phillips, “Introducing Operators,” CoreOS Blog, November 3, 2016, <a href="https://oreil.ly/PtGuh"><em class="hyperlink">https://oreil.ly/PtGuh</em></a>.</p></div></div></section></body></html>