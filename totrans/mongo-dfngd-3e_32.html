<html><head></head><body><div id="sbo-rt-content"><section data-type="appendix" epub:type="appendix" data-pdf-bookmark="Appendix B. MongoDB Internals"><div class="appendix" id="appendix_see"><h1><span class="label">Appendix B. </span>MongoDB Internals</h1><p>It<a data-type="indexterm" data-primary="MongoDB" data-secondary="internals" id="MDBint00b"/> is not necessary to understand MongoDB’s internals to use it
  effectively, but they may be of interest to developers who wish to work on
  tools, contribute, or simply understand what’s happening under the hood.
  This appendix covers some of the basics. The MongoDB source code is
  available at <!--ULINK WITHOUT TEXT NODE.--><a href="https://github.com/mongodb/mongo"><em class="hyperlink">https://github.com/mongodb/mongo</em></a>.</p><section data-type="sect1" data-pdf-bookmark="BSON"><div class="sect1" id="sect1_d1e12746"><h1>BSON</h1><p>Documents<a data-type="indexterm" data-primary="JSON, MongoDB documents resembling" id="idm45882333197336"/><a data-type="indexterm" data-primary="BSON (Binary JSON) format" id="idm45882333196408"/> in MongoDB are an abstract concept—the concrete
    representation of a document varies depending on the driver/language being
    used. Because documents are used extensively for communication in MongoDB,
    there also needs to be a representation of documents that is shared by all
    drivers, tools, and processes in the MongoDB ecosystem. That
    representation is called Binary JSON, or <span class="firstterm">BSON</span> (no
    one knows where the J went).</p><p>BSON is a lightweight binary format capable of representing any
    MongoDB document as a string of bytes. The database understands BSON, and
    BSON is the format in which documents are saved to disk.</p><p>When a driver is given a document to insert, use as a query, and so
    on, it will encode that document to BSON before sending it to the server.
    Likewise, documents being returned to the client from the server are sent
    as BSON strings. This BSON data is decoded by the driver to its native
    document representation before being returned to the client.</p><p>The BSON format has three primary goals:</p><dl><dt>Efficiency</dt><dd><p>BSON is designed to represent data efficiently, without using
          much extra space. In the worst case BSON is slightly less efficient
          than JSON, and in the best case (e.g., when storing binary data or
          large numerics), it is much more efficient.</p></dd><dt>Traversability</dt><dd><p>In some cases, BSON does sacrifice space efficiency to make
          the format easier to traverse. For example, string values are
          prefixed with a length rather than relying on a terminator to
          signify the end of a string. This traversability is useful when the
          MongoDB server needs to introspect documents.</p></dd><dt>Performance</dt><dd><p>Finally, BSON is designed to be fast to encode to and decode
          from. It uses C-style representations for types, which are fast to
          work with in most programming <span class="keep-together">languages</span>.</p></dd></dl><p>For the exact BSON specification, see <!--ULINK WITHOUT TEXT
    NODE.--><a href="http://www.bsonspec.org"><em class="hyperlink">http://www.bsonspec.org</em></a>.</p></div></section><section data-type="sect1" data-pdf-bookmark="Wire Protocol"><div class="sect1" id="sect1_d1e12809"><h1>Wire Protocol</h1><p>Drivers<a data-type="indexterm" data-primary="wire protocol" id="idm45882333165544"/> access the MongoDB server using a lightweight TCP/IP wire
    protocol<a data-type="indexterm" data-primary="TCP/IP wire protocol" id="idm45882333164680"/>. The protocol is documented on the <a href="https://oreil.ly/rVJAr">MongoDB
    documentation site</a> but basically consists of a thin wrapper around
    BSON data. For example, an insert message consists of 20 bytes of header
    data (which includes a code telling the server to perform an insert and
    the message length), the collection name to insert into, and a list of
    BSON documents to insert.</p></div></section><section data-type="sect1" data-pdf-bookmark="Data Files"><div class="sect1" id="sect1_d1e12821"><h1>Data Files</h1><p>Inside<a data-type="indexterm" data-primary="data" data-secondary="data files internals" id="idm45882333161112"/> the MongoDB data directory, which is <em class="filename">/data/db/</em> by default, a separate file will be
    stored for each collection and each index. The filenames do not correspond
    to the names of the collections or indexes, but you can use the <code class="function">stats</code> within the <em>mongo</em>
    shell to identify the related file for a specific collection. The <code>"wiredTiger.uri"</code> field will contain the name of
    the file to look for in the MongoDB data directory.</p><p>Using <code class="function">stats</code> on the
    <em>sample_mflix</em> database for the
    <em>movies</em> collection provides
    “collection-14--2146526997547809066” as result in the <code>"wiredTiger.uri"</code> field:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code><code class="nx">db</code><code class="p">.</code><code class="nx">movies</code><code class="p">.</code><code class="nx">stats</code><code class="p">()</code>
<code class="p">{</code>
    <code class="s2">"ns"</code> <code class="o">:</code> <code class="s2">"sample_mflix.movies"</code><code class="p">,</code>
    <code class="s2">"size"</code> <code class="o">:</code> <code class="mi">65782298</code><code class="p">,</code>
    <code class="s2">"count"</code> <code class="o">:</code> <code class="mi">45993</code><code class="p">,</code>
    <code class="s2">"avgObjSize"</code> <code class="o">:</code> <code class="mi">1430</code><code class="p">,</code>
    <code class="s2">"storageSize"</code> <code class="o">:</code> <code class="mi">45445120</code><code class="p">,</code>
    <code class="s2">"capped"</code> <code class="o">:</code> <code class="kc">false</code><code class="p">,</code>
    <code class="s2">"wiredTiger"</code> <code class="o">:</code> <code class="p">{</code>
        <code class="s2">"metadata"</code> <code class="o">:</code> <code class="p">{</code>
            <code class="s2">"formatVersion"</code> <code class="o">:</code> <code class="mi">1</code>
        <code class="p">},</code>
        <code class="s2">"creationString"</code> <code class="o">:</code> <code class="s2">"access_pattern_hint=none,allocation_size=4KB,\</code>
<code class="s2">    app_metadata=(formatVersion=1),assert=(commit_timestamp=none,\</code>
<code class="s2">    read_timestamp=none),block_allocation=best,\</code>
<code class="s2">    block_compressor=snappy,cache_resident=false,checksum=on,\</code>
<code class="s2">    colgroups=,collator=,columns=,dictionary=0,\</code>
<code class="s2">    encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,\</code>
<code class="s2">    huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,\</code>
<code class="s2">    immutable=false,internal_item_max=0,internal_key_max=0,\</code>
<code class="s2">    internal_key_truncate=true,internal_page_max=4KB,key_format=q,\</code>
<code class="s2">    key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,\</code>
<code class="s2">    leaf_value_max=64MB,log=(enabled=true),lsm=(auto_throttle=true,\</code>
<code class="s2">    bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,\</code>
<code class="s2">    bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,\</code>
<code class="s2">    chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),\</code>
<code class="s2">    merge_max=15,merge_min=0),memory_page_image_max=0,\</code>
<code class="s2">    memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,\</code>
<code class="s2">    prefix_compression=false,prefix_compression_min=4,source=,\</code>
<code class="s2">    split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,\</code>
<code class="s2">    type=file,value_format=u"</code><code class="p">,</code>
        <code class="s2">"type"</code> <code class="o">:</code> <code class="s2">"file"</code><code class="p">,</code>
        <code class="s2">"uri"</code> <code class="o">:</code> <code class="s2">"statistics:table:collection-14--2146526997547809066"</code><code class="p">,</code>
    <code class="p">...</code>
<code class="p">}</code></pre><p>The file’s details can then be verified within the MongoDB data
    directory:</p><pre data-type="programlisting" data-code-language="javascript"><code class="nx">ls</code> <code class="o">-</code><code class="nx">alh</code> <code class="nx">collection</code><code class="o">-</code><code class="mi">14</code><code class="o">--</code><code class="mi">2146526997547809066</code><code class="p">.</code><code class="nx">wt</code>
<code class="o">-</code><code class="nx">rw</code><code class="o">-------</code>  <code class="mi">1</code> <code class="nx">braz</code>  <code class="nx">staff</code>  <code class="mi">43</code><code class="nx">M</code> <code class="mi">28</code> <code class="nx">Sep</code> <code class="mi">23</code><code class="o">:</code><code class="mi">33</code> <code class="nx">collection</code><code class="o">-</code><code class="mi">14</code><code class="o">--</code><code class="mi">2146526997547809066</code><code class="p">.</code><code class="nx">wt</code></pre><p>It’s possible to use the aggregation framework to find the URI for
    each index in a specific collection using the following:</p><pre data-type="programlisting" data-code-language="javascript"><code class="nx">db</code><code class="p">.</code><code class="nx">movies</code><code class="p">.</code><code class="nx">aggregate</code><code class="p">([{</code>
      <code class="nx">$collStats</code><code class="o">:</code><code class="p">{</code><code class="nx">storageStats</code><code class="o">:</code><code class="p">{}}}]).</code><code class="nx">next</code><code class="p">().</code><code class="nx">storageStats</code><code class="p">.</code><code class="nx">indexDetails</code>
<code class="p">{</code>
    <code class="s2">"_id_"</code> <code class="o">:</code> <code class="p">{</code>
    <code class="s2">"metadata"</code> <code class="o">:</code> <code class="p">{</code>
        <code class="s2">"formatVersion"</code> <code class="o">:</code> <code class="mi">8</code><code class="p">,</code>
        <code class="s2">"infoObj"</code> <code class="o">:</code> <code class="s2">"{ \"v\" : 2, \"key\" : { \"_id\" : 1 },\</code>
<code class="s2">         \"name\" : \"_id_\", \"ns\" : \"sample_mflix.movies\" }"</code>
    <code class="p">},</code>
    <code class="s2">"creationString"</code> <code class="o">:</code> <code class="s2">"access_pattern_hint=none,allocation_size=4KB,\</code>
<code class="s2">    app_metadata=(formatVersion=8,infoObj={ \"v\" : 2, \"key\" : \</code>
<code class="s2">     { \"_id\" : 1 },\"name\" : \"_id_\", \"ns\" : \"sample_mflix.movies\" }),\</code>
<code class="s2">     assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,\</code>
<code class="s2">     block_compressor=,cache_resident=false,checksum=on,colgroups=,collator=,\</code>
<code class="s2">     columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,\</code>
<code class="s2">     format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,\</code>
<code class="s2">     immutable=false,internal_item_max=0,internal_key_max=0,\</code>
<code class="s2">     internal_key_truncate=true,internal_page_max=16k,key_format=u,key_gap=10,\</code>
<code class="s2">     leaf_item_max=0,leaf_key_max=0,leaf_page_max=16k,leaf_value_max=0,\</code>
<code class="s2">     log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,\</code>
<code class="s2">     bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,\</code>
<code class="s2">     chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,\</code>
<code class="s2">     suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,\</code>
<code class="s2">     memory_page_max=5MB,os_cache_dirty_max=0,os_cache_max=0,\</code>
<code class="s2">     prefix_compression=true,prefix_compression_min=4,source=,\</code>
<code class="s2">     split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,\</code>
<code class="s2">     value_format=u"</code><code class="p">,</code>
    <code class="s2">"type"</code> <code class="o">:</code> <code class="s2">"file"</code><code class="p">,</code>
    <code class="s2">"uri"</code> <code class="o">:</code> <code class="s2">"statistics:table:index-17--2146526997547809066"</code><code class="p">,</code>
<code class="p">...</code>
    <code class="s2">"$**_text"</code> <code class="o">:</code> <code class="p">{</code>
<code class="p">...</code>
      <code class="s2">"uri"</code> <code class="o">:</code> <code class="s2">"statistics:table:index-29--2146526997547809066"</code><code class="p">,</code>
<code class="p">...</code>
    <code class="s2">"genres_1_imdb.rating_1_metacritic_1"</code> <code class="o">:</code> <code class="p">{</code>
<code class="p">...</code>
      <code class="s2">"uri"</code> <code class="o">:</code> <code class="s2">"statistics:table:index-30--2146526997547809066"</code><code class="p">,</code>
<code class="p">...</code>
<code class="p">}</code></pre><p>WiredTiger stores each collection or index in a single arbitrarily
    large file. The only limits that impact the potential maximum size of this
    file are filesystem size limits.</p><p>WiredTiger writes a new copy of the full document whenever that
    document is updated. The old copy on disk is flagged for reuse and will
    eventually be overwritten at a future point, typically during the next
    checkpoint. This recycles the space used within the WiredTiger file.
    The<a data-type="indexterm" data-primary="compact command" id="idm45882333049656"/> <code>compact</code> command can be
    run to move the data within this file to the start, leaving empty space at
    the end. At regular intervals, WiredTiger removes this excess empty space
    by truncating the file. At the end of the compaction process, the excess
    space is returned to the filesystem.</p></div></section><section data-type="sect1" data-pdf-bookmark="Namespaces"><div class="sect1" id="sect1_d1e12862"><h1>Namespaces</h1><p>Each<a data-type="indexterm" data-primary="namespaces" data-secondary="basics of" id="idm45882332843944"/> database is organized into <em>namespaces</em>,
    which are mapped to WiredTiger files. This abstraction separates the
    storage engine’s internal details from the MongoDB query layer.</p></div></section><section data-type="sect1" data-pdf-bookmark="WiredTiger Storage Engine"><div class="sect1" id="sect1_d1e12904"><h1>WiredTiger Storage Engine</h1><p>The<a data-type="indexterm" data-primary="WiredTiger storage engine" id="idm45882332840920"/><a data-type="indexterm" data-primary="storage engine, default" id="idm45882332840088"/> default storage engine for MongoDB is the WiredTiger
    storage engine. When the server starts up, it opens the data files and
    begins the checkpointing and journaling processes. It works in conjunction
    with the operating system, whose responsibility is focused on paging data
    in and out as well as flushing data to disk. This storage engine has
    several important properties:</p><ul><li><p>Compression<a data-type="indexterm" data-primary="compression" id="idm45882332838136"/> is on by default for collections and for indexes. The
        default compression algorithm is Google’s snappy. Other options
        include Facebook’s Zstandard (zstd) and zlib, or indeed no
        compression. This minimizes storage use in the database at the expense
        of additional CPU requirements.</p></li><li><p>Document-level concurrency<a data-type="indexterm" data-primary="concurrency" id="idm45882332836424"/> <a data-type="indexterm" data-primary="document-level concurrency" id="idm45882332835464"/>allows for updates on different documents from multiple
        clients in a collection as the same time. WiredTiger uses MultiVersion
        Concurrency Control (MVCC)<a data-type="indexterm" data-primary="MultiVersion Concurrency Control (MVCC)" id="idm45882332834312"/> to isolate read and write operations to ensure clients
        see a consistent point-in-time view of the data at the start of an
        operation.</p></li><li><p>Checkpointing<a data-type="indexterm" data-primary="checkpoints" id="idm45882332832840"/> creates a consistent point-in-time snapshot of the data
        and occurs every 60 seconds. It involves writing all the data in the
        snapshot to disk and updating the related metadata.</p></li><li><p>Journaling<a data-type="indexterm" data-primary="journaling" id="idm45882332831256"/> with checkpointing ensures there is no point in time
        where data might be lost if there was a failure of a <em class="filename">mongod</em> process. WiredTiger uses a
        write-ahead log (journal) that stores modifications before they
        are<a data-type="indexterm" data-startref="MDBint00b" id="idm45882332829320"/>
        applied.</p></li></ul></div></section></div></section></div>



  </body></html>