<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Kubernetes Performance and Security"><div class="chapter" id="kubernetes_performance_and_security">
      <h1><span class="label">Chapter 3. </span>Kubernetes Performance <span class="keep-together">and Security</span></h1>
      <p>As Kubernetes deployments become larger and more prevalent, the enterprise is turning its focus to performance and security, which are important for both business continuity and cost control. Performance helps save cost by maximizing infrastructure usage and preventing unnecessary resource scaling. Security helps control cost and ensure business continuity by protecting assets, including customer data and other proprietary information.</p>
      <section data-type="sect1" data-pdf-bookmark="What Hasn’t Changed: Performance, Availability, and Security Requirements"><div class="sect1" id="what_hasn_t_changed_performance_availability_an">
        <h1>What Hasn’t Changed: Performance, Availability, and Security Requirements</h1>
        <p>As applications have evolved from monolithic on-premises deployments, to virtual machines, to software as a service (SaaS) and platform as a service (PaaS) offerings running in the cloud, to modern containerized cloud native applications, business requirements haven’t changed. Enterprise applications running on Kubernetes must meet nonnegotiable business requirements such as high availability, data protection, and strict performance metrics, all while operating across hybrid clouds or multiple data centers.</p>
        <p>Performance requires using resources appropriately, ensuring that there is enough capability to serve the needs of the business without overprovisioning or overspending. Availability involves taking hardware failures and network latency in stride, and meeting SLAs without blinking an eye. Security means protecting sensitive resources and data through encryption, access controls, and defense in depth while making them available to people and processes with a legitimate business case for access. </p>
        <p>Traditional availability mostly meant uptime monitoring and alerts. Making sure the application frontend was running was not only sufficient to identify downtime, it was often the only available sign of a problem. Fortunately, monolithic software was comparatively simple, with predictable user interactions. End-to-end request monitoring was unimportant, and might have seemed absurd to IT teams at the time. Performance was less of a concern; as long as applications were up and behaving properly, IT and site reliability teams were happy.</p>
        <p>Performance, availability, and security must now be managed at the container level.</p>
        <p>The services that make up today’s containerized applications are highly interdependent, meaning that the failure of one can take down part or all of a highly complex, mission-critical application. Keeping applications running properly involves maintaining multiple instances of critical services, load-balancing among them, and moving or restarting them when failures occur. </p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="High Availability at a Global Scale"><div class="sect1" id="high_availability_at_a_global_scale">
        <h1>High Availability at a Global Scale</h1>
        <p>Global high availability is a nonnegotiable business requirement for the enterprise. This must go beyond keeping individual clusters running when a few control plane nodes or worker nodes fail. Application state, including configuration and distributed data, must be available without interruption everywhere it’s expected. The bottom line is that cloud native high availability requires awareness of each application’s persistent data, metadata, configuration, and running state. </p>
        <p>High availability within a cluster can be as simple as replicating containers, volumes, and Kubernetes services so that at least a minimum number of instances of each is available continually. The goal is to ensure that there is no single point of failure. High availability across local or geographical areas involves replicating data and applications among separate data centers, either synchronously or asynchronously. These strategies permit the enterprise to choose strategies that balance the cost of hardware or cloud resources against the goals of the business.</p>
        <p>The key to high availability, whether local or global, is that the storage and data replication mechanisms are application consistent, meaning that they capture all the data and metadata necessary to keep services running or to restart them without interruption to the business flow. This includes not only persistent data but application configuration and running state as well. Simply backing up VMs or making copies of volumes is not enough, because they only capture part of the data any given service or application needs to continue running. Furthermore, because individual volumes and containers aren’t guaranteed to last, there must be a way to identify applications and data so that the replacement instances can be associated with each other correctly.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Data Mobility "><div class="sect1" id="data_mobility">
        <h1>Data Mobility </h1>
        <p>One key to high availability is <em>data mobility</em>, defined as the ability to move or replicate data quickly between clusters in a single data center or cloud, or between clouds. This capability supports not only high availability, but also upgrades, migrations, scaling, and disaster recovery. </p>
        <p>As the enterprise embraces the cloud, the importance of data mobility becomes more and more apparent. It’s clear that a single deployment environment or provider is no longer sufficient for all business needs. Organizations need the flexibility to make trade-offs between public and private clouds, on-premises servers, and edge data centers as needed to increase agility, meet regulatory requirements for data locality, deliver better service, and keep costs under control. </p>
        <p>As Kubernetes comes into play, it’s crucial to be able to move large volumes of data among diverse multicloud and hybrid cloud environments. While Kubernetes makes it easy to deploy applications anywhere, it has long been more difficult to move the underlying data. What is needed is an approach that makes it as easy to move persistent volumes as it is to migrate application containers.</p>
        <p>Container-native storage is half the battle. By providing data and storage management across infrastructure types and providers, a container-native storage pool provides a home for data in any deployment environment. But traditional data migration and <span class="keep-together">replication</span> approaches, which are time-consuming and operationally complex, aren’t up to the task of migrating data when required. What’s needed is a container-aware data orchestration layer that can provide the declarative automation for state that Kubernetes provides for applications themselves. In cases where an application moves, it’s often not practical to move all application data on short notice. Data mobility must include the capability of moving the most immediately important data first, to allow the application to start working in its new location quickly by minimizing the amount of data that must be moved.</p>
        <p>Data mobility enables the enterprise to improve operations in a number of ways. By moving low-priority applications to auxiliary clusters, the enterprise can free up capacity on critical clusters, making room for additional data or replication. Automating the movement of data makes it easier to test new versions, promoting workloads from development to staging clusters or maintaining two or more live environments. When moving applications and data among environments or taking a cluster offline to perform hardware maintenance and upgrades, data mobility is key to continuous <span class="keep-together">availability</span>. </p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Tuning Kubernetes Data for Enterprise-Scale Performance"><div class="sect1" id="tuning_kubernetes_data_for_enterprise_scale_perfor">
        <h1>Tuning Kubernetes Data for Enterprise-Scale Performance</h1>
        <p>As the enterprise drives cloud native applications to greater and greater scale, tuning Kubernetes data is increasingly important to balance performance, cost, and availability. Although there are many factors that affect the performance of a Kubernetes cluster, one of the most important is to make the right choices about data. There are two significant considerations: overall storage configuration and data placement.</p>
        <p>Storage configuration means determining what kind of storage hardware you need and how you set it up. You can set up a single storage pool or multiple pools that each define their own StorageClass, for example. You’ll need to think about cost versus capacity and speed, determine how much memory and CPU to allow for use by the storage layer, and how many different types of storage you will need. These questions are especially important in on-premises data center environments, where the decisions have ramifications that can last for years. In cloud or hybrid environments, there is more flexibility to start small and grow or change as needed. </p>
        <p><em>Data placement</em>, or <em>data topology</em>, refers to strategies for managing where data is stored, usually to meet either performance or availability goals. For maximum performance, one strategy is <em>hyperconvergence</em>, which keeps a workload and its data together in a single node (<a data-type="xref" href="#fig_1_hyperconverged_topology">Figure 3-1</a>). If the node goes down, the data is lost, but that might be acceptable. </p>
        <figure><div id="fig_1_hyperconverged_topology" class="figure">
          <img src="Images/csdp_0301.png" alt="Hyperconverged topology" width="633" height="496"/>
          <h6><span class="label">Figure 3-1. </span>Hyperconverged topology</h6>
        </div></figure>
        <p>In hyperconverged topologies, storage is usually a software-defined layer, often on commodity servers shared by the compute nodes. Hyperconvergence is very flexible, allowing storage to scale in step with the compute workload. However, because the failure of a single node affects not only the workload on that server but the storage system as well, hyperconverged topologies can be more difficult to manage from the point of view of both operations and security. Storing additional replicas of the data on other nodes allows a new instance of the workload to recover to a true hyperconverged topology after a failure, starting from the most recent data written to the replicas.</p>
        <p>For higher availability, data can be separated from workloads. Keeping compute and storage nodes separate (called <em>disaggregation</em>) provides slower performance but prevents loss in the event of a node failure. For a very dynamic environment, where the number of compute nodes increases and decreases in response to workload demand, one strategy is to separate compute and storage into their own clusters (<a data-type="xref" href="#fig_2_disaggregated_topology">Figure 3-2</a>). This way, scaling and management <span class="keep-together">operations</span> in the compute cluster don’t interfere with the storage cluster, and vice versa.</p>
        <figure><div id="fig_2_disaggregated_topology" class="figure">
          <img src="Images/csdp_0302.png" alt="Disaggregated topology" width="1081" height="526"/>
          <h6><span class="label">Figure 3-2. </span>Disaggregated topology</h6>
        </div></figure>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Keeping the Cluster Secure"><div class="sect1" id="keeping_the_cluster_secure">
        <h1>Keeping the Cluster Secure</h1>
        <p>Modern security practices use a <em>defense in depth</em> strategy, which means that multiple layers of security controls work together, providing redundancy so that a breach in one layer doesn’t grant access to a critical system. Cloud native security is no different: protections at the levels of the cloud, the cluster, the container, and the code itself build upon each other to protect applications and data from unauthorized access. </p>
        <p>The environment is the first layer of defense. If the data center or cloud is secure, it is easier to protect the cluster and its services. The cluster, in turn, must provide security mechanisms at the level of the physical node, the VM, the container, and so on. </p>
        <p>At the container level, services must be able to communicate securely with SDS. Kubernetes provides mechanisms for controlling the access privileges of containers, limiting resource usage, and preventing containers from taking dangerous or unwanted actions. The storage layer must do its part by encrypting data both at rest and in motion, restricting access to specific clients, and using application and API knowledge to permit only sensible data transactions.</p>
      </div></section>
    </div></section></div></body></html>