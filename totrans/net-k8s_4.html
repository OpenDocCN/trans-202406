<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Kubernetes Networking Introduction"><div class="chapter" id="kubernetes_networking_introduction">
<h1><span class="label">Chapter 4. </span>Kubernetes Networking Introduction</h1>


<p>Now that we have covered Linux and container networking’s critical components, we are ready to discuss Kubernetes
networking in greater detail. In this chapter, we will discuss how pods connect internally and externally to the
cluster. We will also cover how the internal components of Kubernetes connect. Higher-level network abstractions
around discovery and load balancing, such as services and ingresses, will be covered in the next chapter.</p>

<p>Kubernetes <a data-type="indexterm" data-primary="communication" data-secondary="networking challenges with" id="idm46219940015400"/><a data-type="indexterm" data-primary="networking" data-secondary="issues in" id="idm46219940014424"/>networking looks to solve these four networking issues:</p>

<ul>
<li>
<p>Highly coupled container-to-container communications</p>
</li>
<li>
<p>Pod-to-pod communications</p>
</li>
<li>
<p>Pod-to-service communications</p>
</li>
<li>
<p>External-to-service communications</p>
</li>
</ul>

<p>The Docker <a data-type="indexterm" data-primary="Docker container technology" data-secondary="limitations of private networks of" id="idm46219940008968"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="benefits of" id="idm46219940007880"/><a data-type="indexterm" data-primary="system administrators" data-secondary="networking communication issues for" id="idm46219939990520"/><a data-type="indexterm" data-primary="ports" data-secondary="communication problems between" id="idm46219939989672"/>networking model uses a virtual bridge network by default, which is defined per host and is a private
network where containers attach. The container’s IP address is allocated a private IP address, which implies
containers running on different machines cannot communicate with each other. Developers will have to map host ports
to container ports and then proxy the traffic to reach across nodes with Docker. In this scenario, it is up to the
Docker administrators to avoid port clashes between containers; usually, this is the system administrators. The
Kubernetes networking handles this differently.</p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="The Kubernetes Networking Model"><div class="sect1" id="idm46219939987944">
<h1>The Kubernetes Networking Model</h1>

<p>The Kubernetes <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="overview of model for" id="ch4_term1"/>networking model natively supports <a data-type="indexterm" data-primary="cluster networking" data-secondary="multihost" id="idm46219939984648"/>multihost cluster networking. Pods can communicate with each other
by default,  regardless of which host they are deployed on. <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="standardization with" id="idm46219939983432"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI for" id="idm46219939982424"/>Kubernetes relies on the CNI
project to comply with the following requirements:</p>

<ul>
<li>
<p>All containers must communicate with each other without NAT.</p>
</li>
<li>
<p>Nodes can communicate with containers without NAT.</p>
</li>
<li>
<p>A container’s IP address is the same as those outside the container that it sees itself as.</p>
</li>
</ul>

<p>The unit of <a data-type="indexterm" data-primary="pods" data-secondary="overview of" id="ch4_term2"/>work in Kubernetes is called a <em>pod</em>. A pod contains one or more containers,
which are always scheduled and run “together” on the same node. This connectivity allows individual instances of a
service to be separated into distinct containers. For example, a developer may choose to run a service in one container
and a log forwarder in another container. Running processes in distinct containers allows them to have separate
resource quotas (e.g., “the log forwarder cannot use more than 512 MB of memory”). It also allows container build and
deployment machinery to be separated by reducing the scope necessary to build a container.</p>

<p>The following is a minimal pod definition. We have omitted many options. Kubernetes manages various fields, such as the
status of the pods, that are read-only:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web</code>
    <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web:v0.0.1</code>
    <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
      <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code></pre>

<p>Kubernetes users typically do not create pods directly. Instead, <a data-type="indexterm" data-primary="workloads, Kubernetes" id="idm46219939889816"/><a data-type="indexterm" data-primary="deployment resource, Kubernetes" id="idm46219939930584"/>users create a high-level workload, such as a
deployment, which manages pods according to some intended spec. In the case of a deployment, as shown in <a data-type="xref" href="#img-deployment">Figure 4-1</a>,
users specify a <em>template</em> for pods, along with how many pods (often called <em>replicas</em>) that they want to exist.
There are several other ways to manage workloads such as ReplicaSets and StatefulSets that we will review in the next chapter. Some
provide abstractions over an intermediate type, while others manage pods directly. There  are also <a data-type="indexterm" data-primary="CRDs (custom resource definitions)" id="idm46219939927752"/><a data-type="indexterm" data-primary="custom resource definitions (CRDs)" id="idm46219939927080"/>third-party
workload types, in the form of custom resource definitions (CRDs). Workloads in Kubernetes are a complex topic, and
we will only attempt to cover the very basics and the parts applicable to the networking stack.</p>

<figure><div id="img-deployment" class="figure">
<img src="Images/neku_0401.png" alt="neku 0401" width="857" height="482"/>
<h6><span class="label">Figure 4-1. </span>The relationship between a deployment and pods</h6>
</div></figure>

<p>Pods themselves are ephemeral, meaning they are deleted and replaced with new versions of themselves.
The short life span of pods is one of the main surprises and challenges to developers and operators familiar with
more semipermanent, traditional physical or virtual machines. Local disk state, node scheduling, and IP addresses
will all be replaced regularly during a pod’s life cycle.</p>

<p>A pod has a <a data-type="indexterm" data-primary="cluster networking" data-secondary="complexity of IP addresses with" id="idm46219939922968"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with pods and pod networks" data-secondary-sortas="pods and pod networks" id="ch4_term3"/>unique IP address, which is shared by all containers in the pod. The primary motivation behind giving
every pod an IP address is to remove constraints around port numbers. In Linux, only one program can listen on a
given address, port, and protocol. If <a data-type="indexterm" data-primary="ports" data-secondary="communication problems between" id="idm46219939920072"/>pods did not have unique IP addresses, then two pods on a node could contend
for the same port (such as two web servers, both trying to listen on port 80). If they were the same, it would require a
runtime configuration to fix, such as a <code>--port</code> flag. Alternatively, it would take an ugly script to update a config file in the
case of third-party software.</p>

<p>In some cases, third-party software could not run on custom ports at all, which would require more complex workarounds,
such as <code>iptables</code> DNAT rules on the node. Web servers have the additional problem of expecting conventional port
numbers in their software, such as 80 for HTTP and 443 for HTTPS. Breaking from these conventions requires
reverse-proxying through a load balancer or making downstream consumers aware of the various ports (which is much
easier for internal systems than external ones). Some systems, such as Google’s Borg, use this model. Kubernetes
chose the IP per pod model to be more comfortable for developers to adopt and make it easier to run third-party workloads.
Unfortunately for us, allocating and routing an IP address for every pod adds <em>substantial</em> complexity to a
Kubernetes cluster.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>By default, <a data-type="indexterm" data-primary="cluster networking" data-secondary="default-open status of" id="idm46219939915160"/>Kubernetes will allow any traffic to or from any pod.
This passive connectivity means, among other things,
that any pod in a cluster can connect to any other pod in that same cluster.
That can easily lead to abuse,
especially if services do not use authentication or if an attacker obtains credentials.</p>

<p>See <a data-type="xref" href="#pCNIp">“Popular CNI Plugins”</a> for more.</p>
</div>

<p>Pods created and <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="issues with" id="idm46219939911912"/><a data-type="indexterm" data-primary="networking" data-secondary="issues in" id="idm46219939910904"/>deleted with their own IP addresses can cause issues for beginners who do not understand this
behavior. Suppose we have a small service running on Kubernetes, in the form of a deployment with three pod replicas.
When <a data-type="indexterm" data-primary="rolling upgrades, Kubernetes" id="idm46219939909592"/>someone updates a container image in the deployment, Kubernetes performs a <em>rolling upgrade</em>,
deleting old pods and creating new pods using the new container image. These new pods will likely have new IP
addresses, making the old IP addresses unreachable. It can be a common beginner’s mistake to reference pod IPs in
config or DNS records manually, only to have them fail to resolve. This error is what services and endpoints attempt
to solve, and this is discussed in the next chapter.</p>

<p>When explicitly creating a pod, it is possible to specify the IP address. <a data-type="indexterm" data-primary="StatefulSets, Kubernetes" id="idm46219939907512"/>StatefulSets are a built-in workload
type intended for workloads such as databases, which maintain a pod identity concept and give a new pod the same name
and IP address as the pod it replaces. There <a data-type="indexterm" data-primary="CRDs (custom resource definitions)" id="idm46219939906440"/><a data-type="indexterm" data-primary="custom resource definitions (CRDs)" id="idm46219939905752"/>are other examples in the form of third-party CRDs, and it is possible to write a CRD for specific networking purposes.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Custom resources are extensions of the Kubernetes API defined by the writer. It allows software developers to
customize the installation of their software in a Kubernetes environment. You can find more information on writing a CRD in the <a href="https://oreil.ly/vVcrE">documentation</a>.</p>
</div>

<p>Every Kubernetes <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="Kubelet and pod readiness in" data-seealso="Kubelet, Kubernetes" id="idm46219939902088"/><a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="functionality of" id="idm46219939900840"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubernetes plugins with" id="idm46219939899896"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI plug-ins for" id="idm46219939898936"/>node runs a component called the <em>Kubelet</em>, which manages pods on the node.
The networking functionality in the Kubelet comes from API interactions with a CNI plugin on the node.
The CNI plugin is what manages pod IP addresses and individual container network provisioning. We <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="standardization with" id="idm46219939897224"/>mentioned the
eponymous interface portion of the CNI in the previous chapter; the CNI defines a standard interface to manage a
container’s network. The reason for making the CNI an interface is to have an interoperable standard,
where there are multiple CNI plugin implementations. The CNI plugin is responsible for assigning pod IP addresses and
maintaining a route between all (applicable) pods. Kubernetes does not ship with a default CNI plugin,
which means that in a standard installation of Kubernetes, pods cannot use the <a data-type="indexterm" data-startref="ch4_term1" id="idm46219939895512"/><a data-type="indexterm" data-startref="ch4_term2" id="idm46219939894840"/><a data-type="indexterm" data-startref="ch4_term3" id="idm46219939894168"/>network.</p>

<p>Let’s begin the discussion on how the pod network is enabled by the CNI and the different network layouts.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Node and Pod Network Layout"><div class="sect1" id="idm46219939987352">
<h1>Node and Pod Network Layout</h1>

<p>The cluster must <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="node and pod layouts in" id="ch4_term4"/>have a group of IP addresses that it controls to assign an IP address to a pod, for example, <code>10.1.0.0/16</code>. Nodes <a data-type="indexterm" data-primary="cluster networking" data-secondary="L3 requirement for" id="idm46219939862360"/><a data-type="indexterm" data-primary="Network (Internet) layer, TCP/IP (L3)" data-secondary="IP addresses and" id="idm46219939861512"/>and pods must have L3 connectivity in this IP address space. Recall from <a data-type="xref" href="ch01.xhtml#networking_introduction">Chapter 1</a> that in L3, the Internet
layer, connectivity means packets with an IP address can route to a host with that IP address. It is <a data-type="indexterm" data-primary="Transport layer, TCP/IP (L4)" data-secondary="firewalls and" id="idm46219939859512"/>important to
note that the ability to deliver <em>packets</em> is more fundamental than creating connections (an L4 concept). In L4,
firewalls may choose to allow connections from host A to B but reject connections initiating from host B to A. L4 connections from A to B, connections at L3, A to B and B to A, must be allowed. <a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="handshakes" id="idm46219939857688"/>Without L3 connectivity,
TCP handshakes would not be possible, as the SYN-ACK could not be delivered.</p>

<p>Generally, pods <a data-type="indexterm" data-primary="Link layer (L2)" data-secondary="connectivity limitations of" id="idm46219939856200"/><a data-type="indexterm" data-primary="MAC (Media Access Control) sublayer" data-secondary="addresses of" id="idm46219939855208"/>do not have MAC addresses. Therefore, L2 connectivity to pods is not possible. The CNI will determine
this for pods.</p>

<p>There are no requirements in Kubernetes about L3 connectivity to the outside world. Although the majority of clusters
have internet connectivity, some are more isolated for security reasons.</p>

<p>We will broadly discuss <a data-type="indexterm" data-primary="egress" id="idm46219939853000"/><a data-type="indexterm" data-primary="ingress, defined" id="idm46219939852296"/>both ingress (traffic leaving a host or cluster) and egress (traffic entering a host or cluster).
Our use of “ingress” here shouldn’t be confused with the Kubernetes ingress resource, which is a specific HTTP mechanism to route traffic to Kubernetes services.</p>

<p>There are <a data-type="indexterm" data-primary="pods" data-secondary="cluster network layout options for" id="ch4_term5"/><a data-type="indexterm" data-primary="nodes" data-secondary="cluster network layout options for" id="ch4_term6"/>broadly three approaches, with many variations, to structuring a cluster’s network: isolated, flat, and
island networks. We will discuss the general approaches here and then get more in-depth into specific implementation
details when covering CNI plugins later this chapter.</p>








<section data-type="sect2" data-pdf-bookmark="Isolated Networks"><div class="sect2" id="idm46219939847704">
<h2>Isolated Networks</h2>

<p>In an <a data-type="indexterm" data-primary="cluster networking" data-secondary="with isolated networks" data-secondary-sortas="isolated networks" id="ch4_term7"/><a data-type="indexterm" data-primary="isolated networks layout" id="ch4_term8"/>isolated cluster network, nodes are routable on the broader network (i.e., hosts that are not part of the
cluster can reach nodes in the cluster), but pods are not. <a data-type="xref" href="#img-isolated-clusters">Figure 4-2</a> shows such a cluster. Note that pods cannot
reach other pods (or any other hosts) outside the cluster.</p>

<p>Because the cluster is not routable from the broader network,
multiple clusters can even use the same IP address space.
Note that the Kubernetes API server will need to be routable from the broader network,
if external systems or users should be able to access the Kubernetes API.
Many managed Kubernetes providers have a “secure cluster” option like this,
where no direct traffic is possible between the cluster and the internet.</p>

<p>That isolation <a data-type="indexterm" data-primary="security" data-secondary="limited connectivity for" id="idm46219939841256"/>to the local cluster can be splendid for security if the cluster’s workloads permit/require such a setup,
such as clusters for batch processing.
However, it is not reasonable for all clusters. The <a data-type="indexterm" data-primary="proxies" id="idm46219939839896"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with Kubernetes pods" data-secondary-sortas="Kubernetes pods" id="idm46219939839224"/>majority of clusters will need to reach and/or be reached by external systems,
such as clusters that must support services that have dependencies on the broader internet.
Load balancers and proxies can be used to breach this barrier and allow internet traffic into or out of an isolated <a data-type="indexterm" data-startref="ch4_term7" id="idm46219939837576"/><a data-type="indexterm" data-startref="ch4_term8" id="idm46219939836904"/>cluster.</p>

<figure><div id="img-isolated-clusters" class="figure">
<img src="Images/neku_0402.png" alt="neku 0402" width="1163" height="1148"/>
<h6><span class="label">Figure 4-2. </span>Two isolated clusters in the same network</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Flat Networks"><div class="sect2" id="idm46219939833944">
<h2>Flat Networks</h2>

<p>In a flat <a data-type="indexterm" data-primary="cluster networking" data-secondary="with flat networks" data-secondary-sortas="flat networks" id="ch4_term9"/><a data-type="indexterm" data-primary="flat networks layout" id="ch4_term10"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with pods and pod networks" data-secondary-sortas="pods and pod networks" id="ch4_term11"/><a data-type="indexterm" data-primary="routing" data-secondary="for external traffic" data-secondary-sortas="external traffic" id="ch4_term12"/>network, all pods have an IP address that is routable from the broader network.
Barring firewall rules, any host on the network can route to any pod inside or outside the cluster. This
configuration has numerous upsides around network simplicity and performance. Pods can connect directly to arbitrary
hosts in the network.</p>

<p>Note in <a data-type="xref" href="#img-flat-clusters">Figure 4-3</a> that no <a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="in Kubernetes pods" data-secondary-sortas="Kubernetes pods" id="ch4_term13"/>two nodes’ pod CIDRs overlap between the two clusters, and therefore no two pods will be
assigned the same IP address. Because the broader network can route every pod IP address to that pod’s node, any host
on the network is reachable to and from any pod.</p>

<p>This openness allows any host with sufficient service discovery data to decide which pod will receive those packets.
A <a data-type="indexterm" data-primary="load balancing" data-secondary="with Kubernetes pods" data-secondary-sortas="Kubernetes pods" id="idm46219939822744"/>load balancer outside the cluster can load balance pods, such as a gRPC client in another cluster.</p>

<figure><div id="img-flat-clusters" class="figure">
<img src="Images/neku_0403.png" alt="neku 0403" width="1115" height="1148"/>
<h6><span class="label">Figure 4-3. </span>Two clusters in the same flat network</h6>
</div></figure>

<p>External pod traffic (and incoming pod traffic, when the connection’s destination is a specific pod IP address)
has low latency and low overhead. Any <a data-type="indexterm" data-primary="proxies" id="ch4_term14"/><a data-type="indexterm" data-primary="latency, routing" id="idm46219939817752"/><a data-type="indexterm" data-primary="routing" data-secondary="latency in" id="idm46219939817080"/>form of proxying or packet rewriting incurs a latency and processing cost,
which is small but nontrivial (especially in an application architecture that involves many backend services, where
each delay adds up).</p>

<p>Unfortunately, this model requires a large, contiguous IP address space for each cluster (i.e., a range of IP
addresses where every IP address in the range is under your control). Kubernetes requires a single CIDR for pod IP
addresses (for each IP family). This model is achievable with a private subnet (such as 10.0.0.0/8 or 172.16.0.0/12);
however, it is much harder and more expensive to do with public IP addresses, especially IPv4 addresses.
Administrators will need to use NAT to connect a cluster running in a private IP address space to the <a data-type="indexterm" data-startref="ch4_term13" id="idm46219939814840"/>internet.</p>

<p>Aside from <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubernetes plugins with" id="idm46219939813592"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI plug-ins for" id="idm46219939812520"/><a data-type="indexterm" data-primary="system administrators" data-secondary="networking communication issues for" id="idm46219939811576"/>needing a large IP address space, administrators also need an easily programmable network.
The CNI plugin must allocate pod IP addresses and ensure a route exists to a given pod’s node.</p>

<p>Flat <a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="flat layout for" id="idm46219939809896"/><a data-type="indexterm" data-primary="subnets" data-secondary="private" id="idm46219939808872"/><a data-type="indexterm" data-primary="private subnets" id="idm46219939807928"/>networks, on a private subnet, are easy to achieve in a cloud provider environment. The vast majority of cloud
provider networks will provide large private subnets and have an API (or even preexisting CNI plugins) for IP address
allocation and route <a data-type="indexterm" data-startref="ch4_term9" id="idm46219939806856"/><a data-type="indexterm" data-startref="ch4_term10" id="idm46219939806184"/>management.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Island Networks"><div class="sect2" id="idm46219939805256">
<h2>Island Networks</h2>

<p>Island cluster <a data-type="indexterm" data-primary="cluster networking" data-secondary="with island networks" data-secondary-sortas="island networks" id="idm46219939803880"/><a data-type="indexterm" data-primary="island networks layout" id="idm46219939802792"/>networks are, at a high level, a combination of isolated and flat 
<span class="keep-together">networks.</span></p>

<p>In an island cluster setup, as shown in <a data-type="xref" href="#img-island-clusters">Figure 4-4</a>, nodes have L3 connectivity with the broader network, but pods do
not. Traffic to and from pods must pass through some form of proxy, through nodes. Most often, this is <a data-type="indexterm" data-primary="masquerading connections, iptables" id="idm46219939799624"/><a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="iptables and" id="idm46219939798984"/><a data-type="indexterm" data-primary="SNAT (source NAT)" id="idm46219939798072"/><a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="with Kubernetes pods" data-secondary-sortas="Kubernetes pods" id="idm46219939797400"/>achieved by
<code>iptables</code> source NAT on a pod’s packets leaving the node. This setup, called <em>masquerading</em>, uses SNAT to rewrite
packet sources from the pod’s IP address to the node’s IP address (refer to <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a> for a refresher on SNAT). In other words, packets
appear to be “from” the node, rather than the pod.</p>

<p>Sharing an <a data-type="indexterm" data-startref="ch4_term14" id="idm46219939793848"/><a data-type="indexterm" data-primary="firewalls" data-secondary="IP addresses and" id="idm46219939793144"/>IP address while also using NAT hides the individual pod IP addresses. IP address–based firewalling and
recognition becomes difficult across the cluster boundary. Within a cluster, it is still apparent which IP address is
which pod (and, therefore, which application). Pods in other clusters, or other hosts on the broader network, will no longer
have that mapping. IP address-based firewalling and allow lists are not sufficient security on their own but are a
valuable and sometimes required layer.</p>

<p>Now let’s see how we configure any of these network <a data-type="indexterm" data-startref="ch4_term5" id="idm46219939790856"/><a data-type="indexterm" data-startref="ch4_term6" id="idm46219939790152"/><a data-type="indexterm" data-startref="ch4_term11" id="idm46219939789480"/><a data-type="indexterm" data-startref="ch4_term12" id="idm46219939788808"/><a data-type="indexterm" data-startref="ch4_term15" id="idm46219939788136"/><a data-type="indexterm" data-startref="ch4_term16" id="idm46219939787464"/>layouts with the <code>kube-controller-manager</code>. <em>Control plane</em> refers to <a data-type="indexterm" data-primary="control plane, Kubernetes" data-secondary="defined" id="idm46219939785800"/><a data-type="indexterm" data-primary="data plane" id="idm46219939784824"/>all the functions and processes that determine which path to use to send the packet or frame. <em>Data plane</em> refers to all the functions and processes that forward packets/frames from one interface to another based on control plane logic.</p>

<figure><div id="img-island-clusters" class="figure">
<img src="Images/neku_0404.png" alt="neku 0404" width="1303" height="1148"/>
<h6><span class="label">Figure 4-4. </span>Two in the “island network” configuration</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="kube-controller-manager Configuration"><div class="sect2" id="idm46219939781352">
<h2>kube-controller-manager Configuration</h2>

<p>The <code>kube-controller-manager</code> runs most <a data-type="indexterm" data-primary="kube-controller-manager" id="ch4_term17"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="kube-controller-manager in" id="ch4_term18"/>individual Kubernetes controllers in one binary and one process, where most
Kubernetes logic lives. At a high level, a controller in Kubernetes terms is software that watches resources and
takes action to synchronize or enforce a specific state (either the desired state or reflecting the current state as
a status). Kubernetes has many controllers, which generally “own” a specific object type or specific operation.</p>

<p><code>kube-controller-manager</code> includes multiple controllers that manage the Kubernetes network stack. <a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="with kube-controller-manager" data-secondary-sortas="kube-controller-manager" id="idm46219939776008"/><a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="in Kubernetes pods" data-secondary-sortas="Kubernetes pods" id="idm46219939774696"/>Notably,
administrators set the cluster CIDR here.</p>

<p><code>kube-controller-manager</code>, due to running a significant number of controllers, also has a substantial number of flags.
<a data-type="xref" href="#table_title">Table 4-1</a> highlights some <a data-type="indexterm" data-primary="allocate-node-cidrs flag, Kubernetes" id="idm46219939771480"/><a data-type="indexterm" data-primary="cluster-CIDR flag, Kubernetes" id="idm46219939770808"/><a data-type="indexterm" data-primary="configure-cloud-routes flag, Kubernetes" id="idm46219939770168"/><a data-type="indexterm" data-primary="flags" data-secondary="with kube-controller-manager" data-secondary-sortas="kube-controller-manager" id="idm46219939769528"/><a data-type="indexterm" data-primary="node-CIDR-mask-size flag, Kubernetes" id="idm46219939768296"/><a data-type="indexterm" data-primary="service-cluster-ip-range flag, Kubernetes" id="idm46219939767608"/>notable network configuration flags.</p>
<table id="table_title">
<caption><span class="label">Table 4-1. </span><code>Kube-controller-manager</code> options</caption>
<thead>
<tr>
<th>Flag</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><code>--allocate-node-cidrs</code></p></td>
<td><p>true</p></td>
<td><p>Sets whether CIDRs for pods should be allocated and set on the cloud provider.</p></td>
</tr>
<tr>
<td><p><span class="keep-together"><code>--CIDR-allocator-type string</code></span></p></td>
<td><p>RangeAllocator</p></td>
<td><p>Type of CIDR allocator to use.</p></td>
</tr>
<tr>
<td><p><code>--cluster-CIDR</code></p></td>
<td/>
<td><p>CIDR range from which to assign pod IP addresses. Requires <code>--allocate-node-cidrs</code> to be true.
If <code>kube-controller-manager</code> has <code>IPv6DualStack</code> enabled,
<code>--cluster-CIDR</code> accepts a comma-separated pair of IPv4 and IPv6 CIDRs.</p></td>
</tr>
<tr>
<td><p><code>--configure-cloud-routes</code></p></td>
<td><p>true</p></td>
<td><p>Sets whether CIDRs should be allocated by <code>allocate-node-cidrs</code> and configured on the cloud provider.</p></td>
</tr>
<tr>
<td><p><code>--node-CIDR-mask-size</code></p></td>
<td><p>24 for IPv4 clusters, 64 for IPv6 clusters</p></td>
<td><p>Mask size for the node CIDR in a cluster. Kubernetes will assign each node <code>2^(node-CIDR-mask-size)</code> IP addresses.</p></td>
</tr>
<tr>
<td><p><code>--node-CIDR-mask-size-ipv4</code></p></td>
<td><p>24</p></td>
<td><p>Mask size for the node CIDR in a cluster. Use this flag in dual-stack clusters to allow both IPv4 and IPv6 settings.</p></td>
</tr>
<tr>
<td><p><code>--node-CIDR-mask-size-ipv6</code></p></td>
<td><p>64</p></td>
<td><p>Mask size for the node CIDR in a cluster. Use this flag in dual-stack clusters to allow both IPv4 and IPv6 settings.</p></td>
</tr>
<tr>
<td><p><code>--service-cluster-ip-range</code></p></td>
<td/>
<td><p>CIDR range for services in the cluster to allocate service ClusterIPs. Requires <code>--allocate-node-cidrs</code> to be true.
If <code>kube-controller-manager</code> has <code>IPv6DualStack</code> enabled,
<code>--service-cluster-ip-range</code> accepts a comma-separated pair of IPv4 and IPv6 CIDRs.</p></td>
</tr>
</tbody>
</table>
<div data-type="tip"><h6>Tip</h6>
<p>All Kubernetes binaries have documentation for their flags in the <a data-type="indexterm" data-startref="ch4_term17" id="idm46219939737992"/><a data-type="indexterm" data-startref="ch4_term18" id="idm46219939737208"/>online docs. See all <code>kube-controller-manager</code> options in the 
<span class="keep-together"><a href="https://oreil.ly/xDGIE">documentation</a>.</span></p>
</div>

<p>Now that we have discussed high-level network architecture and network configuration in the Kubernetes control plane,
let’s look closer at how Kubernetes worker nodes handle networking.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="The Kubelet"><div class="sect1" id="idm46219939892360">
<h1>The Kubelet</h1>

<p>The Kubelet is a <a data-type="indexterm" data-primary="Kubelet, Kubernetes" id="ch4_term79"/>single binary that runs on every worker node in a cluster. At a high level, the Kubelet is
responsible for managing any pods scheduled to the node and providing status updates for the node and pods on it.
However, the Kubelet primarily acts as a coordinator for other software on the node. The <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubelet and" id="idm46219939731176"/><a data-type="indexterm" data-primary="CRI runtime" id="idm46219939730264"/><a data-type="indexterm" data-primary="container runtime" id="idm46219939729592"/>Kubelet manages a container
networking implementation (via the CNI) and a container runtime (via the CRI).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We define <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="worker nodes in" id="idm46219939727688"/><a data-type="indexterm" data-primary="worker nodes, Kubernetes" id="idm46219939726680"/><a data-type="indexterm" data-primary="nodes" data-secondary="worker nodes as" id="idm46219939725992"/><a data-type="indexterm" data-primary="pods" data-secondary="worker nodes with" id="idm46219939725048"/>worker nodes as Kubernetes nodes that can run pods.
Some clusters technically run the API server and <code>etcd</code> on restricted worker nodes.
This setup can allow control plane components to be managed with the same automation as typical workloads but exposes
additional failure modes and security vulnerabilities.</p>
</div>

<p>When a controller (or user) creates a pod in the Kubernetes API, it initially exists as only the pod API object.
The <a data-type="indexterm" data-primary="nodes" data-secondary="scheduling of pod and" id="idm46219939722584"/>Kubernetes scheduler watches for such a pod and attempts to select a valid node to schedule the pod to.
There are several constraints to this scheduling. Our pod with its CPU/memory requests must not exceed the unrequested
CPU/memory remaining on the node. Many selection options are available, such as affinity/anti-affinity to labeled
nodes or other labeled pods or taints on nodes. <a data-type="indexterm" data-primary="nodeName field, Kubernetes" id="idm46219939721080"/>Assuming the scheduler finds a node that satisfies all the pod’s
constraints, the scheduler writes that node’s name to our pod’s <code>nodeName</code> field. Let’s say Kubernetes schedules the
pod to <code>node-1</code>:</p>

<pre data-type="programlisting" data-code-language="bash">apiVersion: v1
kind: Pod
metadata:
  name: example
spec:
  nodeName: <code class="s2">"node-1"</code>
  containers:
    - name: example
      image: example:1.0</pre>

<p>The  Kubelet  on  <code>node-1</code>  watches  for  all  of  the  pods  scheduled  to  it.  The equivalent 
<span class="keep-together"><code>kubectl</code></span> command would be
<code>kubectl get pods -w --field-selector spec.nodeName=node-1</code>. When  the  Kubelet  observes  that  our  pod  exists 
but  is not present on the node, it creates it. We will skip over the CRI details and the 
<span class="keep-together">creation</span> of the container
itself. Once the container exists, the Kubelet makes an <code>ADD</code> call to the CNI, which tells the CNI plugin to create
the pod network. We <a data-type="indexterm" data-startref="ch4_term4" id="idm46219939712472"/>will cover the interface and plugins in our next section.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Pod Readiness and Probes"><div class="sect1" id="idm46219939711640">
<h1>Pod Readiness and Probes</h1>

<p>Pod readiness <a data-type="indexterm" data-primary="pods" data-secondary="readiness of" id="ch4_term19"/>is an additional indication of whether the pod is ready to serve traffic.
Pod readiness determines whether the pod address shows up in the <code>Endpoints</code> object from an external source.
Other <a data-type="indexterm" data-primary="deployment resource, Kubernetes" data-secondary="pods and" id="idm46219939707704"/>Kubernetes resources that manage pods, like deployments, take pod readiness into account for decision-making,
such as advancing during a rolling update. During rolling deployment, a new
pod becomes ready, but a service, network policy, or load balancer is not yet prepared for the new pod due to
whatever reason. This may cause service disruption or loss of backend capacity. It should be noted that if a pod spec
does contain probes of any type, Kubernetes defaults to success for all three types.</p>

<p>Users can specify pod readiness checks in the pod spec.
From there, the <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="for status of pod readiness" data-secondary-sortas="status of pod readiness" id="idm46219939704984"/><a data-type="indexterm" data-primary="status of pod readiness, Kubernetes" id="idm46219939703800"/>Kubelet executes the specified check and updates the pod status based on successes or failures.</p>

<p>Probes effect the <code>.Status.Phase</code> field of a pod. The following is a list of the pod phases and their descriptions:</p>
<dl>
<dt>Pending</dt>
<dd>
<p>The pod has been accepted by the cluster, but one or more of the containers has not been set
up and made ready to run. This includes the time a pod spends waiting to be scheduled as well as the time spent
downloading container images over the network.</p>
</dd>
<dt>Running</dt>
<dd>
<p>The  pod  has  been  scheduled  to  a  node,  and  all  the  containers  have  been  created. At least one container
is still running or is in the process of starting or restarting. Note that some containers may be in a failed state, such as in a 
<span class="keep-together">CrashLoopBackoff.</span></p>
</dd>
<dt>Succeeded</dt>
<dd>
<p>All containers in the pod have terminated in success and will not be restarted.</p>
</dd>
<dt>Failed</dt>
<dd>
<p>All containers in the pod have terminated, and at least one container has terminated in failure. That is,
the container either exited with nonzero status or was terminated by the system.</p>
</dd>
<dt>Unknown</dt>
<dd>
<p>For some reason the state of the pod could not be determined. This phase typically occurs due to an error
in communicating with the Kubelet where the pod should be running.</p>
</dd>
</dl>

<p>The Kubelet performs <a data-type="indexterm" data-primary="health checks/probes" data-secondary="on pods by Kubelet" data-secondary-sortas="pods by Kubelet" id="ch4_term20"/><a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="with probes for health of pods" data-secondary-sortas="probes for health of pods" id="ch4_term21"/><a data-type="indexterm" data-primary="liveness probes, Kubelet" id="ch4_term22"/><a data-type="indexterm" data-primary="readiness probes, Kubelet" id="ch4_term23"/><a data-type="indexterm" data-primary="startup probes, Kubelet" id="ch4_term24"/>several types of health checks for individual containers in a pod: <em>liveness probes</em> (<code>livenessProbe</code>),
<em>readiness probes</em> (<code>readinessProbe</code>), and <em>startup probes</em> (<code>startupProbe</code>).
The Kubelet (and, by extension, the node itself) must be able to connect to
all containers running on that node in order to perform any HTTP health checks.</p>

<p>Each probe has one of three results:</p>
<dl>
<dt>Success</dt>
<dd>
<p>The container passed the diagnostic.</p>
</dd>
<dt>Failure</dt>
<dd>
<p>The container failed the diagnostic.</p>
</dd>
<dt>Unknown</dt>
<dd>
<p>The diagnostic failed, so no action should be taken.</p>
</dd>
</dl>

<p>The probes can be exec probes, which attempt to execute a binary within the container, TCP probes, or HTTP probes.
If the probe fails more than the <code>failureThreshold</code> number of times, Kubernetes will consider the check to have failed.
The effect of this depends on the type of probe.</p>

<p>When a container’s readiness probe fails, the Kubelet does not terminate it. Instead, the Kubelet writes the
failure to the pod’s status.</p>

<p>If the liveness probes fail, the Kubelet will terminate the container.
Liveness probes can easily cause unexpected failures if misused or misconfigured. The intended use case for
liveness probes is to let the Kubelet know when to restart a container. However, as humans, we quickly learn that if
“something is wrong, restart it” is a dangerous strategy. For example, suppose we create a liveness probe that
loads the main page of our web app. Further, suppose that some change in the system, outside our container’s code,
causes the main page to return a 404 or 500 error. There are frequent causes of such a scenario, such as a backend database
failure, a required service failure, or a feature flag change that exposes a bug. In any of these
scenarios, the <a data-type="indexterm" data-primary="restarts, container" id="idm46219939659272"/>liveness probe would restart the container.
At best, this would be unhelpful; restarting the container will not solve a problem elsewhere in the system and
could quickly worsen the problem. Kubernetes has container restart backoffs (<code>CrashLoopBackoff</code>), which add increasing delay
to restarting failed containers. With enough pods or rapid enough failures, the application may go from having an
error on the home page to being hard-down. Depending on the application, pods may also lose cached data upon a
restart; it may be strenuous to fetch or impossible to fetch during the hypothetical degradation.
Because of this, use liveness probes with caution. When pods use them, they only depend on the container they are
testing, with no other dependencies. Many engineers have specific health check endpoints,
which provide minimal validation of criteria, such as “PHP is running and serving my API.”</p>

<p>A startup probe can provide a grace period before a liveness probe can take effect.
Liveness probes will not terminate a container before the startup probe has succeeded.
An example use case is to allow a container to take many minutes to start, but to terminate a container quickly if it becomes unhealthy after starting.</p>

<p>In <a data-type="xref" href="#kubernetes_podsec_for_golang_minimal_webs_server">Example 4-1</a>, our Golang <a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="podspec for" id="idm46219939655000"/><a data-type="indexterm" data-primary="HTTP GET command" id="idm46219939654024"/>web server has a liveness probe that performs an HTTP GET on port 8080 to the path <code>/healthz</code>, while the readiness probe uses <code>/</code> on the same port.</p>
<div id="kubernetes_podsec_for_golang_minimal_webs_server" data-type="example" class="less_space pagebreak-before">
<h5><span class="label">Example 4-1. </span>Kubernetes podspec for Golang minimal webserver</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">labels</code><code class="p">:</code>
    <code class="nt">test</code><code class="p">:</code> <code class="l-Scalar-Plain">liveness</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web</code>
    <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web:v0.0.1</code>
    <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
    <code class="nt">livenessProbe</code><code class="p">:</code>
      <code class="nt">httpGet</code><code class="p">:</code>
        <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/healthz</code>
        <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
      <code class="nt">initialDelaySeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>
      <code class="nt">periodSeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>
    <code class="nt">readinessProbe</code><code class="p">:</code>
      <code class="nt">httpGet</code><code class="p">:</code>
        <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>
        <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
      <code class="nt">initialDelaySeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>
      <code class="nt">periodSeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code></pre></div>

<p>This status <a data-type="indexterm" data-primary="ReplicaSet controller, Kubernetes" id="idm46219943441128"/>does not affect the pod itself, but other Kubernetes mechanisms react to it. One key example is
ReplicaSets (and, by extension, deployments). A failing readiness probe causes the ReplicaSet controller to count the
pod as unready, giving rise to a halted deployment when too many new pods are <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="readiness probes and" id="idm46219939573992"/><a data-type="indexterm" data-primary="endpointslices, Kubernetes" data-secondary="readiness probes and" id="idm46219939573144"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="endpoints" id="idm46219939572232"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="endpointslices" id="idm46219939571320"/>unhealthy. The <code>Endpoints</code>/<code>EndpointsSlice</code>
controllers also react to failing readiness probes. If a pod’s readiness probe fails, the pod’s IP address will not
be in the endpoint object, and the service will not route traffic to it. We will discuss services and endpoints more
in the next 
<span class="keep-together">chapter.</span></p>

<p>The <code>startupProbe</code> will inform the Kubelet whether the application inside the container is started. This probe takes
precedent over the others. If a <code>startupProbe</code> is defined in the pod spec, all other probes are disabled. Once the
<code>startupProbe</code> succeeds, the Kubelet will begin running the other probes. But if the startup probe fails, the Kubelet kills the
container, and the container executes its restart policy. Like the others, if a <code>startupProbe</code> does not exist, the
default state is <a data-type="indexterm" data-startref="ch4_term22" id="idm46219939566024"/><a data-type="indexterm" data-startref="ch4_term23" id="idm46219939565320"/><a data-type="indexterm" data-startref="ch4_term24" id="idm46219939564648"/>success.</p>

<p>Probe <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="configurable options for probes in" id="idm46219939563560"/>configurable options:</p>
<dl>
<dt>initialDelaySeconds</dt>
<dd>
<p>Amount of seconds after the container starts before liveness or readiness probes are
initiated. Default 0; Minimum 0.</p>
</dd>
<dt>periodSeconds</dt>
<dd>
<p>How often probes are performed. Default 10; Minimum 1.</p>
</dd>
<dt>timeoutSeconds</dt>
<dd>
<p>Number of seconds after which the probe times out. Default 1; Minimum 1.</p>
</dd>
<dt>successThreshold</dt>
<dd>
<p>Minimum consecutive successes for the probe to be successful after failing.
Default 1; must be 1 for liveness and startup probes; Minimum 1.</p>
</dd>
<dt>failureThreshold</dt>
<dd>
<p>When a probe fails, Kubernetes will try this many times before giving up. Giving up in the case
of the liveness probe means the container will restart. For readiness probe, the pod will be marked Unready. Default 3;
Minimum 1.</p>
</dd>
</dl>

<p>Application <a data-type="indexterm" data-primary="readinessGates, Kubernetes" id="idm46219939554024"/>developers can also use readiness gates to help determine when the application inside the
pod is ready. Available and stable since Kubernetes 1.14, to use readiness gates, manifest writers will add
<code>readiness gates</code>  in the pod’s spec to specify a list of additional conditions that the Kubelet evaluates for
pod readiness. That is <a data-type="indexterm" data-primary="conditionType attribute, Kubernetes" id="idm46219939552472"/>done in the <code>ConditionType</code> attribute of the readiness gates in the pod spec.  The <code>ConditionType</code>
is a condition in the pod’s condition list with a matching type. Readiness gates are controlled by the current state
of <code>status.condition</code> fields for the pod, and if the Kubelet cannot find such a condition in the <code>status.conditions</code>
field of a pod, the status of the condition is defaulted to False.</p>

<p>As you can see in the following example, the <code>feature-Y</code> readiness gate is true, while <code>feature-X</code> is false, so the pod’s status
is ultimately false:</p>

<pre data-type="programlisting" data-code-language="bash">kind: Pod
…
spec:
  readinessGates:
  - conditionType: www.example.com/feature-X
  - conditionType: www.example.com/feature-Y
…
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: 2021-04-25T00:00:00Z
    status: <code class="s2">"False"</code>
    <code class="nb">type</code>: Ready
  - lastProbeTime: null
    lastTransitionTime: 2021-04-25T00:00:00Z
    status: <code class="s2">"False"</code>
    <code class="nb">type</code>: www.example.com/feature-X
  - lastProbeTime: null
    lastTransitionTime: 2021-04-25T00:00:00Z
    status: <code class="s2">"True"</code>
    <code class="nb">type</code>: www.example.com/feature-Y
  containerStatuses:
  - containerID: docker://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    ready : <code class="nb">true</code></pre>

<p>Load balancers <a data-type="indexterm" data-primary="load balancing" data-secondary="with Kubernetes pods" data-secondary-sortas="Kubernetes pods" id="idm46219939542920"/>like the AWS ALB can use the readiness gate as part of the pod life cycle before sending traffic to it.</p>

<p>The Kubelet <a data-type="indexterm" data-primary="API, Kubernetes" id="idm46219939541288"/><a data-type="indexterm" data-primary="cluster networking" data-secondary="data flow of" id="idm46219939470568"/>must be able to connect to the Kubernetes API server. In <a data-type="xref" href="#cluster-data-flow">Figure 4-5</a>, we can see all the connections made
by all the components in a cluster:</p>
<dl>
<dt>CNI</dt>
<dd>
<p>Network <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubelet and" id="idm46219939467000"/>plugin in Kubelet that enables networking to get IPs for pods and 
<span class="keep-together">services.</span></p>
</dd>
<dt>gRPC</dt>
<dd>
<p>API to communicate from the API server to <code>etcd</code>.</p>
</dd>
<dt>Kubelet</dt>
<dd>
<p>All Kubernetes nodes have a Kubelet that ensures that any pod assigned to it are running and configured in the
desired state.</p>
</dd>
<dt>CRI</dt>
<dd>
<p>The gRPC API <a data-type="indexterm" data-primary="container runtime" id="idm46219939536408"/><a data-type="indexterm" data-primary="CRI runtime" id="idm46219939535672"/><a data-type="indexterm" data-primary="gRPC service" id="idm46219939535000"/>compiled in Kubelet, allowing Kubelet to talk to container runtimes
using gRPC API. The container runtime provider must adapt it to the CRI API to allow Kubelet to talk to containers using the OCI
Standard (runC).  CRI consists of protocol buffers and gRPC API and libraries.</p>
</dd>
</dl>

<figure><div id="cluster-data-flow" class="figure">
<img src="Images/neku_0405.png" alt="neku 0405" width="1057" height="892"/>
<h6><span class="label">Figure 4-5. </span>Cluster data flow between components</h6>
</div></figure>

<p>Communication between the pods and the Kubelet is made possible by the CNI. In our next <a data-type="indexterm" data-startref="ch4_term19" id="idm46219939531320"/><a data-type="indexterm" data-startref="ch4_term20" id="idm46219939530616"/><a data-type="indexterm" data-startref="ch4_term21" id="idm46219939529944"/><a data-type="indexterm" data-startref="ch4_term79" id="idm46219939524872"/>section, we
will discuss the CNI specification with examples from several popular CNI projects.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="The CNI Specification"><div class="sect1" id="idm46219939710728">
<h1>The CNI Specification</h1>

<p>The CNI <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="overview of" id="idm46219939522776"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="specifications of" id="idm46219939521800"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI for" id="ch4_term27"/>specification itself is quite simple. According to the specification, there are four operations that a CNI
plugin must support:</p>
<dl>
<dt>ADD</dt>
<dd>
<p>Add a container to the network.</p>
</dd>
<dt>DEL</dt>
<dd>
<p>Delete a container from the network.</p>
</dd>
<dt>CHECK</dt>
<dd>
<p>Return an error if there is a problem with the container’s network.</p>
</dd>
<dt>VERSION</dt>
<dd>
<p>Report version information about the plugin.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>The full CNI spec is available on <a href="https://oreil.ly/1uYWl">GitHub</a>.</p>
</div>

<p>In <a data-type="xref" href="#cni-configuration">Figure 4-6</a>, we can see <a data-type="indexterm" data-primary="binaries, CNI plugins and" id="idm46219939446040"/><a data-type="indexterm" data-primary="stdin, JSON" id="idm46219939445432"/><a data-type="indexterm" data-primary="stdout, JSON" id="idm46219939444824"/>how Kubernetes (or the <em>runtime</em>, as the CNI project refers to container orchestrators)
invokes CNI plugin operations by executing binaries. Kubernetes supplies any configuration for the command in JSON to
<code>stdin</code> and receives the command’s output in JSON through <code>stdout</code>. CNI plugins frequently have very simple binaries,
which act as a wrapper for Kubernetes to call, while the binary makes an HTTP or RPC API call to a persistent backend.
CNI maintainers have discussed changing this to an HTTP or RPC model, based on performance issues when frequently
launching Windows processes.</p>

<p>Kubernetes uses only one CNI plugin at a time, though the CNI specification allows for multiplugin setups (i.e.,
assigning multiple IP addresses to a container). <a data-type="indexterm" data-primary="Multus CNI plug-in" id="idm46219939441816"/>Multus is a CNI plugin that works around this limitation in
Kubernetes by acting as a fan-out to multiple CNI <a data-type="indexterm" data-startref="ch4_term25" id="idm46219939440824"/><a data-type="indexterm" data-startref="ch4_term26" id="idm46219939440152"/>plugins.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>At the time of writing, the CNI spec is at version 0.4. It has not changed drastically over the years and appears
unlikely to change in the future—maintainers of the specification plan to release version 1.0 soon.</p>
</div>

<figure><div id="cni-configuration" class="figure">
<img src="Images/neku_0406.png" alt="cni-config" width="1052" height="788"/>
<h6><span class="label">Figure 4-6. </span>CNI configuration</h6>
</div></figure>
</div></section>













<section data-type="sect1" data-pdf-bookmark="CNI Plugins"><div class="sect1" id="idm46219939435896">
<h1>CNI Plugins</h1>

<p>The CNI plugin has <a data-type="indexterm" data-primary="routing" data-secondary="CNI plugins for checking" id="idm46219939434392"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="CNI plugins for" id="ch4_term28"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubernetes plugins with" id="ch4_term29"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI plug-ins for" id="ch4_term30"/>two primary responsibilities: allocate and assign unique IP addresses for pods and ensure that
routes exist within Kubernetes to each pod IP address. These responsibilities mean that the overarching network that
the cluster resides in dictates CNI plugin behavior. For example, if there are too few IP addresses or it is not
possible to attach sufficient IP addresses to a node, <a data-type="indexterm" data-primary="cluster administrators" data-secondary="CNI selection by" id="idm46219939429208"/>cluster admins will need to use a CNI plugin that supports an
overlay network. The hardware stack, or cloud provider used, typically dictates which CNI options are suitable.
<a data-type="xref" href="ch06.xhtml#kubernetes_and_cloud_networking">Chapter 6</a> will talk about the major cloud platforms and how the network design impacts CNI choice.</p>

<p>To use the <a data-type="indexterm" data-primary="network-plugin=cni, Kubernetes" id="idm46219939426408"/>CNI, add <code>--network-plugin=cni</code> to the Kubelet’s startup arguments. By default, the Kubelet reads CNI
configuration from the directory <code>/etc/cni/net.d/</code> and expects to find the CNI binary in <code>/opt/cni/bin/</code>. Admins can
override the configuration location with <code>--cni-config-dir=&lt;directory&gt;</code>, and the CNI binary directory with
<code>--cni-bin-dir=&lt;directory&gt;</code>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Managed Kubernetes offerings, and many “distros” of Kubernetes, come with a CNI preconfigured.</p>
</div>

<p>There are two <a data-type="indexterm" data-primary="cluster networking" data-secondary="with flat networks" data-secondary-sortas="flat networks" id="idm46219939421432"/><a data-type="indexterm" data-primary="cluster networking" data-secondary="with overlay networks" data-secondary-sortas="overlay networks" id="idm46219939420184"/><a data-type="indexterm" data-primary="flat CNI network model" id="idm46219939418936"/><a data-type="indexterm" data-primary="overlay CNI network model" id="idm46219939418264"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="network models for" id="idm46219939417624"/>broad categories of CNI network models: flat networks and overlay networks. In a flat network, the CNI
driver uses IP addresses from the cluster’s network, which typically requires many IP addresses to be available to
the cluster. In an overlay network, the <a data-type="indexterm" data-primary="underlay networks, Kubernetes" id="idm46219939416264"/>CNI driver creates a secondary network within Kubernetes, which uses the
cluster’s network (called the <em>underlay network</em>) to send packets. Overlay networks create a virtual network within
the cluster. In an overlay network, the CNI plugin encapsulates packets. We discussed overlays in greater detail in
<a data-type="xref" href="ch03.xhtml#container_networking_basics">Chapter 3</a>. Overlay networks add substantial complexity and do not allow hosts on the cluster network to connect
directly to pods. However, overlay networks allow the cluster network to be much smaller, as only the nodes must be
assigned IP addresses on that network.</p>

<p>CNI plugins also typically need a way to communicate state between nodes. Plugins take very different approaches,
such as storing data in the Kubernetes API, in a dedicated database.</p>

<p>The CNI plugin is also responsible for calling IPAM plugins for IP addressing.</p>








<section data-type="sect2" data-pdf-bookmark="The IPAM Interface"><div class="sect2" id="idm46219939412648">
<h2>The IPAM Interface</h2>

<p>The CNI spec has a <a data-type="indexterm" data-primary="IPAM (IP Address Management) interface" id="idm46219939411064"/>second interface, the IP Address Management (IPAM) interface, to reduce duplication of IP
allocation code in CNI plugins. The IPAM plugin must determine and output the interface IP address, gateway, and
routes, as shown in <a data-type="xref" href="#example_ipam_plugin_output_from_the_cni_04_specification_docs">Example 4-2</a>. The IPAM interface is <a data-type="indexterm" data-primary="stdin, JSON" id="idm46219939409224"/><a data-type="indexterm" data-primary="stdout, JSON" id="idm46219939408552"/><a data-type="indexterm" data-primary="binaries, CNI plugins and" id="idm46219939407880"/>similar to the CNI: a binary with JSON input to <code>stdin</code> and JSON
output from <code>stdout</code>.</p>
<div id="example_ipam_plugin_output_from_the_cni_04_specification_docs" data-type="example">
<h5><span class="label">Example 4-2. </span>Example IPAM plugin output, from the CNI 0.4 specification docs</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
  <code class="nt">"cniVersion"</code><code class="p">:</code> <code class="s2">"0.4.0"</code><code class="p">,</code>
  <code class="nt">"ips"</code><code class="p">:</code> <code class="p">[</code>
      <code class="p">{</code>
          <code class="nt">"version"</code><code class="p">:</code> <code class="s2">"&lt;4-or-6&gt;"</code><code class="p">,</code>
          <code class="nt">"address"</code><code class="p">:</code> <code class="s2">"&lt;ip-and-prefix-in-CIDR&gt;"</code><code class="p">,</code>
          <code class="nt">"gateway"</code><code class="p">:</code> <code class="s2">"&lt;ip-address-of-the-gateway&gt;"</code>  <code class="err">(optional)</code>
      <code class="p">},</code>
      <code class="err">...</code>
  <code class="p">],</code>
  <code class="nt">"routes"</code><code class="p">:</code> <code class="p">[</code>                                       <code class="err">(optional)</code>
      <code class="p">{</code>
          <code class="nt">"dst"</code><code class="p">:</code> <code class="s2">"&lt;ip-and-prefix-in-cidr&gt;"</code><code class="p">,</code>
          <code class="nt">"gw"</code><code class="p">:</code> <code class="s2">"&lt;ip-of-next-hop&gt;"</code>                  <code class="err">(optional)</code>
      <code class="p">},</code>
      <code class="err">...</code>
  <code class="p">]</code>
  <code class="s2">"dns"</code><code class="p">:</code> <code class="p">{</code>                                          <code class="err">(optional)</code>
    <code class="nt">"nameservers"</code><code class="p">:</code> <code class="err">&lt;list-of-nameservers&gt;</code>            <code class="err">(optional)</code>
    <code class="s2">"domain"</code><code class="p">:</code> <code class="err">&lt;name-of-local-domain&gt;</code>                <code class="err">(optional)</code>
    <code class="s2">"search"</code><code class="p">:</code> <code class="err">&lt;list-of-search-domains&gt;</code>              <code class="err">(optional)</code>
    <code class="s2">"options"</code><code class="p">:</code> <code class="err">&lt;list-of-options&gt;</code>                    <code class="err">(optional)</code>
  <code class="p">}</code>
<code class="p">}</code></pre></div>

<p>Now we <a data-type="indexterm" data-startref="ch4_term28" id="idm46219939402024"/>will review several of the options available for cluster administrators to choose from when deploying a CNI.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Popular CNI Plugins"><div class="sect2" id="pCNIp">
<h2>Popular CNI Plugins</h2>

<p>Cilium is <a data-type="indexterm" data-primary="Cilium" data-secondary="as CNI plugin" data-secondary-sortas="CNI plugin" id="ch4_term31"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Cilium as" id="ch4_term32"/><a data-type="indexterm" data-primary="open source projects" id="idm46219939283688"/>open source software for transparently securing network connectivity between application containers.
Cilium is <a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="with Cilium" data-secondary-sortas="Cilium" id="idm46219939282760"/>an L7/HTTP-aware CNI and can enforce network policies on L3–L7 using an identity-based security model
decoupled from the network addressing. The <a data-type="indexterm" data-primary="eBPF (Berkeley Packet Filtering)" data-secondary="Linux kernel operations with" id="idm46219939281256"/>Linux technology eBPF, which we discussed in <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a>, is what powers
Cilium. Later in this chapter, we will do a deep dive into <code>NetworkPolicy</code> objects; for now know that they are effectively pod-level
firewalls.</p>

<p>Flannel <a data-type="indexterm" data-primary="Flannel CNI plugin" id="idm46219939278728"/>focuses on the network and is a simple and easy way to configure a layer 3 network fabric designed for
Kubernetes. If a cluster requires functionalities like network policies, an admin must deploy
other CNIs, such as Calico. Flannel uses the Kubernetes cluster’s existing <code>etcd</code> to store its state information to
avoid providing a dedicated data store.</p>

<p>According to <a data-type="indexterm" data-primary="Calico CNI plugin" id="idm46219939276776"/>Calico, it “combines flexible networking capabilities with run-anywhere security enforcement to provide
a solution with native Linux kernel performance and true cloud-native scalability.” <a data-type="indexterm" data-primary="Border Gateway Protocol (BGP)" id="idm46219939275704"/>Calico does not use an overlay
network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between
hosts. <a data-type="indexterm" data-primary="Istio service mesh" id="idm46219939274760"/>Calico can also integrate with Istio, a service mesh, to interpret and enforce policy for workloads within the
cluster at the service mesh and network infrastructure layers.</p>

<p><a data-type="xref" href="#a_brief_overview_of_major_cni_plugins">Table 4-2</a> gives a brief overview of the <a data-type="indexterm" data-primary="Network (Internet) layer, TCP/IP (L3)" data-secondary="CNI plug-ins at" id="idm46219939272664"/><a data-type="indexterm" data-primary="Weave Net CNI plugin" id="idm46219939271656"/>major CNI plugins to choose from.</p>
<table id="a_brief_overview_of_major_cni_plugins">
<caption><span class="label">Table 4-2. </span>A brief overview of major CNI plugins</caption>
<thead>
<tr>
<th>Name</th>
<th>NetworkPolicy support</th>
<th>Data storage</th>
<th>Network setup</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Cilium</p></td>
<td><p>Yes</p></td>
<td><p>etcd or consul</p></td>
<td><p>Ipvlan(beta), veth, L7 aware</p></td>
</tr>
<tr>
<td><p>Flannel</p></td>
<td><p>No</p></td>
<td><p>etcd</p></td>
<td><p>Layer 3 IPv4 overlay network</p></td>
</tr>
<tr>
<td><p>Calico</p></td>
<td><p>Yes</p></td>
<td><p>etcd or Kubernetes API</p></td>
<td><p>Layer 3 network using BGP</p></td>
</tr>
<tr>
<td><p>Weave Net</p></td>
<td><p>Yes</p></td>
<td><p>No external cluster store</p></td>
<td><p>Mesh overlay network</p></td>
</tr>
</tbody>
</table>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Full instructions for running KIND, Helm, and Cilium are in the book’s GitHub repo.</p>
</div>

<p>Let’s deploy <a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="with Cilium" data-secondary-sortas="Cilium" id="ch4_term34"/><a data-type="indexterm" data-primary="Cilium" data-secondary="example deployment of" id="ch4_term35"/><a data-type="indexterm" data-primary="cluster networking" data-secondary="local deployment for" id="ch4_term36"/>Cilium for testing with our Golang web server in <a data-type="xref" href="#kind_configuration_for_cilium_local_deploy">Example 4-3</a>. We will need a Kubernetes cluster for
deploying Cilium. One of the <a data-type="indexterm" data-primary="KIND (Kubernetes in Docker) cluster" id="ch4_term33"/>easiest ways we have found to deploy clusters for testing locally is KIND, which stands for Kubernetes in
Docker. It <a data-type="indexterm" data-primary="Helm deployment" id="idm46219939247480"/><a data-type="indexterm" data-primary="YAML configuration file" id="idm46219939246808"/>will allow us to create a cluster with a YAML configuration file and then, using Helm, deploy Cilium to that
cluster.</p>
<div id="kind_configuration_for_cilium_local_deploy" data-type="example">
<h5><span class="label">Example 4-3. </span>KIND configuration for Cilium local deploy</h5>

<pre data-type="programlisting">kind: Cluster <a class="co" id="co_kubernetes_networking_introduction_CO1-1" href="#callout_kubernetes_networking_introduction_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a>
apiVersion: kind.x-k8s.io/v1alpha4 <a class="co" id="co_kubernetes_networking_introduction_CO1-2" href="#callout_kubernetes_networking_introduction_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a>
nodes: <a class="co" id="co_kubernetes_networking_introduction_CO1-3" href="#callout_kubernetes_networking_introduction_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a>
- role: control-plane <a class="co" id="co_kubernetes_networking_introduction_CO1-4" href="#callout_kubernetes_networking_introduction_CO1-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a>
- role: worker <a class="co" id="co_kubernetes_networking_introduction_CO1-5" href="#callout_kubernetes_networking_introduction_CO1-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a>
- role: worker <a class="co" id="co_kubernetes_networking_introduction_CO1-6" href="#callout_kubernetes_networking_introduction_CO1-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a>
- role: worker <a class="co" id="co_kubernetes_networking_introduction_CO1-7" href="#callout_kubernetes_networking_introduction_CO1-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a>
networking: <a class="co" id="co_kubernetes_networking_introduction_CO1-8" href="#callout_kubernetes_networking_introduction_CO1-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a>
disableDefaultCNI: true <a class="co" id="co_kubernetes_networking_introduction_CO1-9" href="#callout_kubernetes_networking_introduction_CO1-9"><img src="Images/9.png" alt="9" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-1" href="#co_kubernetes_networking_introduction_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Specifies that we are configuring a KIND cluster</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-2" href="#co_kubernetes_networking_introduction_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The version of KIND’s config</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-3" href="#co_kubernetes_networking_introduction_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>The list of nodes in the cluster</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-4" href="#co_kubernetes_networking_introduction_CO1-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>One control plane node</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-5" href="#co_kubernetes_networking_introduction_CO1-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Worker <a data-type="indexterm" data-primary="worker nodes, Kubernetes" id="idm46219939213800"/><a data-type="indexterm" data-primary="nodes" data-secondary="worker nodes as" id="idm46219939213000"/>node 1</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-6" href="#co_kubernetes_networking_introduction_CO1-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>Worker node 2</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-7" href="#co_kubernetes_networking_introduction_CO1-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a></dt>
<dd><p>Worker node 3</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-8" href="#co_kubernetes_networking_introduction_CO1-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a></dt>
<dd><p>KIND configuration options for networking</p></dd>
<dt><a class="co" id="callout_kubernetes_networking_introduction_CO1-9" href="#co_kubernetes_networking_introduction_CO1-9"><img src="Images/9.png" alt="9" width="12" height="12"/></a></dt>
<dd><p>Disables the default networking option so that we can deploy Cilium</p></dd>
</dl></div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Instructions for configuring a KIND cluster and more can be found in the <a href="https://oreil.ly/12BRh">documentation</a>.</p>
</div>

<p>With the KIND cluster configuration YAML, we can use KIND to create that cluster with the following command. If this is
the first time you’re running it, it will take <a data-type="indexterm" data-primary="Docker container technology" data-secondary="images from" id="idm46219939170232"/>some time to download all the Docker images for the working and control plane
Docker images:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kind create cluster --config<code class="o">=</code>kind-config.yaml
Creating cluster <code class="s2">"kind"</code> ...
✓ Ensuring node image <code class="o">(</code>kindest/node:v1.18.
2<code class="o">)</code> Preparing nodes
✓ Writing configuration Starting control-plane
Installing StorageClass Joining worker nodes Set kubectl context to <code class="s2">"kind-kind"</code>
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Have a question, bug, or feature request?
Let us know! https://kind.sigs.k8s.io/#community ߙ⊭---

Always verify that the cluster is up and running with kubectl.</pre>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl cluster-info --context kind-kind
Kubernetes master -&gt; control plane is running at https://127.0.0.1:59511
KubeDNS is running at
https://127.0.0.1:59511/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
To further debug and diagnose cluster problems, use <code class="s1">'kubectl cluster-info dump.'</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The cluster nodes will remain in state NotReady until Cilium deploys the network. This is normal behavior for the cluster.</p>
</div>

<p>Now that our cluster is running locally, we can <a data-type="indexterm" data-primary="Helm deployment" id="idm46219939158696"/>begin installing Cilium using Helm, a Kubernetes deployment tool.
According to its documentation, Helm is the preferred way to install Cilium. First, we need to add the Helm repo for
Cilium. Optionally, you can download the Docker images for Cilium and finally instruct KIND to load the Cilium images
into the cluster:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>helm repo add cilium https://helm.cilium.io/
<code class="c"># Pre-pulling and loading container images is optional.</code>
<code class="nv">$ </code>docker pull cilium/cilium:v1.9.1
kind load docker-image cilium/cilium:v1.9.1</pre>

<p>Now that the prerequisites for Cilium are <a data-type="indexterm" data-startref="ch4_term33" id="idm46219939164664"/>completed, we can install it in our cluster with Helm. There are many
configuration options for Cilium, and Helm configures options with <code>--set NAME_VAR=VAR</code>:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>helm install cilium cilium/cilium --version 1.10.1 <code class="se">\</code>
  --namespace kube-system

NAME: Cilium
LAST DEPLOYED: Fri Jan  <code class="m">1</code> 15:39:59 2021
NAMESPACE: kube-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
You have successfully installed Cilium with Hubble.

Your release version is 1.10.1.

For any further <code class="nb">help</code>, visit https://docs.cilium.io/en/v1.10/gettinghelp/</pre>

<p>Cilium installs several pieces in the cluster: the agent, the client, the operator, and the <code>cilium-cni</code> plugin:</p>
<dl>
<dt>Agent</dt>
<dd>
<p>The <a data-type="indexterm" data-primary="agent, Cilium" id="idm46219939131144"/>Cilium agent, <code>cilium-agent</code>, runs on each node in the cluster. The agent accepts configuration through
Kubernetes APIs that describe networking, service load balancing, network policies, and visibility and monitoring
requirements.</p>
</dd>
<dt>Client (CLI)</dt>
<dd>
<p>The <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Cilium" data-secondary-sortas="Cilium" id="idm46219939143720"/>Cilium CLI client (Cilium) is a command-line tool installed along with the Cilium agent. It
interacts with the REST API on the same node. The CLI allows developers to inspect the state and status of the local agent. It
also provides tooling to access the eBPF maps to validate their state directly.</p>
</dd>
<dt>Operator</dt>
<dd>
<p>The <a data-type="indexterm" data-primary="operator, Cilium" id="idm46219939140744"/>operator is responsible for managing duties in the cluster, which should be handled per cluster and
not per node.</p>
</dd>
<dt>CNI Plugin</dt>
<dd>
<p>The CNI plugin (<code>cilium-cni</code>) interacts with the Cilium API of the node to trigger the configuration to
provide networking, load balancing, and network policies pods.</p>
</dd>
</dl>

<p>We can observe all these components being deployed in the cluster with the <code>kubectl -n kube-system get pods --watch</code>
command:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl -n kube-system get pods --watch
NAME                                         READY   STATUS
cilium-65kvp                                 0/1     Init:0/2
cilium-node-init-485lj                       0/1     ContainerCreating
cilium-node-init-79g68                       1/1     Running
cilium-node-init-gfdl8                       1/1     Running
cilium-node-init-jz8qc                       1/1     Running
cilium-operator-5b64c54cd-cgr2b              0/1     ContainerCreating
cilium-operator-5b64c54cd-tblbz              0/1     ContainerCreating
cilium-pg6v8                                 0/1     Init:0/2
cilium-rsnqk                                 0/1     Init:0/2
cilium-vfhrs                                 0/1     Init:0/2
coredns-66bff467f8-dqzql                     0/1     Pending
coredns-66bff467f8-r5nl6                     0/1     Pending
etcd-kind-control-plane                      1/1     Running
kube-apiserver-kind-control-plane            1/1     Running
kube-controller-manager-kind-control-plane   1/1     Running
kube-proxy-k5zc2                             1/1     Running
kube-proxy-qzhvq                             1/1     Running
kube-proxy-v54p4                             1/1     Running
kube-proxy-xb9tr                             1/1     Running
kube-scheduler-kind-control-plane            1/1     Running
cilium-operator-5b64c54cd-tblbz              1/1     Running</pre>

<p>Now that we have deployed Cilium, we can <a data-type="indexterm" data-primary="Cilium" data-secondary="connectivity check for" id="ch4_term37"/>run the Cilium connectivity check to ensure it is running correctly:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl create ns cilium-test
namespace/cilium-test created

<code class="nv">$ </code>kubectl apply -n cilium-test <code class="se">\</code>
-f <code class="se">\</code>
https://raw.githubusercontent.com/strongjz/advanced_networking_code_examples/
master/chapter-4/connectivity-check.yaml

deployment.apps/echo-a created
deployment.apps/echo-b created
deployment.apps/echo-b-host created
deployment.apps/pod-to-a created
deployment.apps/pod-to-external-1111 created
deployment.apps/pod-to-a-denied-cnp created
deployment.apps/pod-to-a-allowed-cnp created
deployment.apps/pod-to-external-fqdn-allow-google-cnp created
deployment.apps/pod-to-b-multi-node-clusterip created
deployment.apps/pod-to-b-multi-node-headless created
deployment.apps/host-to-b-multi-node-clusterip created
deployment.apps/host-to-b-multi-node-headless created
deployment.apps/pod-to-b-multi-node-nodeport created
deployment.apps/pod-to-b-intra-node-nodeport created
service/echo-a created
service/echo-b created
service/echo-b-headless created
service/echo-b-host-headless created
ciliumnetworkpolicy.cilium.io/pod-to-a-denied-cnp created
ciliumnetworkpolicy.cilium.io/pod-to-a-allowed-cnp created
ciliumnetworkpolicy.cilium.io/pod-to-external-fqdn-allow-google-cnp created</pre>

<p>The connectivity test will deploy a series of Kubernetes deployments that will use various connectivity paths.
Connectivity paths come with and without service load balancing and in various network policy combinations. The pod
name indicates the connectivity variant, and the readiness and liveness gate indicates the success or failure of the
test:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl get pods -n cilium-test -w
NAME                                                     READY   STATUS
<code class="nb">echo</code>-a-57cbbd9b8b-szn94                                  1/1     Running
<code class="nb">echo</code>-b-6db5fc8ff8-wkcr6                                  1/1     Running
<code class="nb">echo</code>-b-host-76d89978c-dsjm8                              1/1     Running
host-to-b-multi-node-clusterip-fd6868749-7zkcr           1/1     Running
host-to-b-multi-node-headless-54fbc4659f-z4rtd           1/1     Running
pod-to-a-648fd74787-x27hc                                1/1     Running
pod-to-a-allowed-cnp-7776c879f-6rq7z                     1/1     Running
pod-to-a-denied-cnp-b5ff897c7-qp5kp                      1/1     Running
pod-to-b-intra-node-nodeport-6546644d59-qkmck            1/1     Running
pod-to-b-multi-node-clusterip-7d54c74c5f-4j7pm           1/1     Running
pod-to-b-multi-node-headless-76db68d547-fhlz7            1/1     Running
pod-to-b-multi-node-nodeport-7496df84d7-5z872            1/1     Running
pod-to-external-1111-6d4f9d9645-kfl4x                    1/1     Running
pod-to-external-fqdn-allow-google-cnp-5bc496897c-bnlqs   1/1     Running</pre>

<p>Now that Cilium manages our network for the cluster, we will use it later in this chapter for a <code>NetworkPolicy</code>
overview. Not all CNI plugins will support <code>NetworkPolicy</code>, so that is an important detail when deciding which plugin to <a data-type="indexterm" data-startref="ch4_term27" id="idm46219939055240"/><a data-type="indexterm" data-startref="ch4_term29" id="idm46219939025816"/><a data-type="indexterm" data-startref="ch4_term30" id="idm46219939025288"/><a data-type="indexterm" data-startref="ch4_term31" id="idm46219939024616"/><a data-type="indexterm" data-startref="ch4_term32" id="idm46219939023944"/><a data-type="indexterm" data-startref="ch4_term34" id="idm46219939023272"/><a data-type="indexterm" data-startref="ch4_term35" id="idm46219939022600"/><a data-type="indexterm" data-startref="ch4_term36" id="idm46219939021928"/><a data-type="indexterm" data-startref="ch4_term37" id="idm46219938987944"/>use.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="kube-proxy"><div class="sect1" id="idm46219939287752">
<h1>kube-proxy</h1>

<p><code>kube-proxy</code> is another per-node <a data-type="indexterm" data-primary="kube-proxy, Kubernetes" id="ch4_term38"/>daemon in Kubernetes, like Kubelet.
<code>kube-proxy</code> provides basic load balancing functionality
within the cluster.
It implements services and <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="with kube-proxy" data-secondary-sortas="kube-proxy" id="idm46219938983768"/><a data-type="indexterm" data-primary="endpointslices, Kubernetes" data-secondary="with kube-proxy" data-secondary-sortas="kube-proxy" id="idm46219938982520"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="endpoints" id="idm46219938981336"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="endpointslices" id="idm46219938980424"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with kube-proxy" data-secondary-sortas="kube-proxy" id="idm46219939010472"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="with kube-proxy" data-secondary-sortas="kube-proxy" id="idm46219939009256"/><a data-type="indexterm" data-primary="pods" data-secondary="with kube-proxy" data-secondary-sortas="kube-proxy" id="ch4_term40"/>relies on <code>Endpoints</code>/<code>EndpointSlices</code>,
two API objects that we will discuss in detail in the next chapter on networking abstractions.
It may help to reference that section, but the following is the relevant and quick explanation:</p>

<ul>
<li>
<p>Services define a load balancer for a set of pods.</p>
</li>
<li>
<p>Endpoints (and endpoint slices) list a set of ready pod IPs. They are created automatically from a service, with the same pod selector as the service.</p>
</li>
</ul>

<p>Most types of <a data-type="indexterm" data-primary="ClusterIP service, Kubernetes" id="idm46219939002504"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="ClusterIP" id="idm46219939001800"/>services have an IP address for the service, called the cluster IP address, which is not routable outside the
cluster. <code>kube-proxy</code> is responsible for routing requests to a service’s cluster IP address to healthy pods. <code>kube-proxy</code> is by
far the most common implementation for Kubernetes services, but there are alternatives to <code>kube-proxy</code>, such as a
replacement mode Cilium. A substantial amount of our content on routing in <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a> is applicable to <code>kube-proxy</code>,
particularly when debugging service connectivity or performance.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Cluster IP addresses are not typically routable from outside a 
<span class="keep-together">cluster.</span></p>
</div>

<p><code>kube-proxy</code> has four <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="kube-proxy modes in" id="ch4_term39"/>modes, which change its runtime mode and exact feature set: <code>userspace</code>, <code>iptables</code>, <code>ipvs</code>, and <code>kernelspace</code>.
You can specify the mode using 
<span class="keep-together"><code>--proxy-mode &lt;mode&gt;</code>.</span> It’s worth noting that all modes rely on <code>iptables</code> to some extent.</p>








<section data-type="sect2" data-pdf-bookmark="userspace Mode"><div class="sect2" id="idm46219938952648">
<h2>userspace Mode</h2>

<p>The first and <a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="userspace mode" id="idm46219938951064"/><a data-type="indexterm" data-primary="userspace mode, kube-proxy" id="idm46219938950056"/>oldest mode is <code>userspace</code> mode. In <code>userspace</code> mode, <code>kube-proxy</code> runs a web server and routes all service
IP addresses to the web server, using <code>iptables</code>. The web server terminates connections and proxies the request to a pod in the service’s endpoints.
<code>userspace</code> mode is no longer commonly used, and we suggest avoiding it unless you have a clear reason to use it.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="iptables Mode"><div class="sect2" id="idm46219938946792">
<h2>iptables Mode</h2>

<p><code>iptables</code> mode <a data-type="indexterm" data-primary="iptables, Linux" data-secondary="as kube-proxy mode" data-secondary-sortas="kube-proxy mode" id="ch4_term41"/><a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="iptables mode" id="ch4_term42"/>uses <code>iptables</code> entirely. It is the default mode, and the most commonly used
(this may be in part because <code>IPVS</code> mode graduated to GA stability more recently, and <code>iptables</code> is a familiar Linux technology).</p>

<p><code>iptables</code> mode performs connection fan-out, instead of true load balancing. In <a data-type="indexterm" data-primary="backend destinations" data-secondary="iptables and" id="idm46219938927512"/>other words, <code>iptables</code> mode will route a connection
to a backend pod, and all requests made using that connection will go to the same pod, until the connection is terminated.
This is simple and has predictable behavior in ideal scenarios
(e.g., successive requests in the same connection will be able to benefit from any local caching in backend pods).
It can also be unpredictable when dealing with long-lived connections, such as HTTP/2 connections (notably, HTTP/2 is the transport for gRPC).
Suppose you have two pods, <code>X</code> and <code>Y</code>, serving a service, and you replace <code>X</code> with <code>Z</code> during a normal rolling update.
The older pod <code>Y</code> still has all the existing connections, plus half of the connections that needed to be reestablished when pod <code>X</code> shut down,
leading to substantially more traffic being served by pod <code>Y</code>. There are many scenarios like this that lead to unbalanced traffic.</p>

<p>Recall our examples in the “Practical iptables” section in <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a>. In it, we <a data-type="indexterm" data-primary="iptables, Linux" data-secondary="random routing with" id="idm46219938920856"/>showed that <code>iptables</code> could be configured with a list of IP addresses
and random routing probabilities, such that connections would be made randomly between all IP addresses.
Given a service that has healthy backend pods <code>10.0.0.1</code>, <code>10.0.0.2</code>, <code>10.0.0.3</code>, and <code>10.0.0.4</code>, <code>kube-proxy</code> would <a data-type="indexterm" data-primary="routing" data-secondary="with kube-proxy" data-secondary-sortas="kube-proxy" id="idm46219938916872"/>create sequential rules that route connections like so:</p>

<ul>
<li>
<p>25% of connections go to <code>10.0.0.1</code>.</p>
</li>
<li>
<p>33.3% of unrouted connections go to <code>10.0.0.2</code>.</p>
</li>
<li>
<p>50% of unrouted connections go to <code>10.0.0.3</code>.</p>
</li>
<li>
<p>All unrouted connections go to <code>10.0.0.4</code>.</p>
</li>
</ul>

<p class="pagebreak-before">This may seem unintuitive and leads some engineers to assume that <code>kube-proxy</code> is misrouting traffic (especially because
few people look at <code>kube-proxy</code> when services work as expected).
The crucial detail is that each routing rule happens for connections that <em>haven’t</em> been routed in a prior rule.
The final rule routes all connections to <code>10.0.0.4</code> (because the connection has to go <em>somewhere</em>), the semifinal rule has a 50%
chance of routing to <code>10.0.0.3</code> as a choice of two IP addresses, and so on.
Routing randomness scores are always calculated as <code>1 / ${remaining number of IP addresses}</code>.</p>

<p>Here are the <code>iptables</code> forwarding <a data-type="indexterm" data-primary="forwarding" data-secondary="kube-dns service rules for" id="idm46219938904568"/><a data-type="indexterm" data-primary="kube-dns service cluster" id="idm46219938903544"/>rules for the <code>kube-dns</code> service in a cluster.
In our example, the <code>kube-dns</code> service’s cluster IP address is <code>10.96.0.10</code>.
This output has been filtered and reformatted for clarity:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo iptables -t nat -L KUBE-SERVICES
Chain KUBE-SERVICES <code class="o">(</code><code class="m">2</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination

/* kube-system/kube-dns:dns cluster IP */ udp dpt:domain
KUBE-MARK-MASQ  udp  -- !10.217.0.0/16        10.96.0.10
/* kube-system/kube-dns:dns cluster IP */ udp dpt:domain
KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  anywhere  10.96.0.10
/* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:domain
KUBE-MARK-MASQ  tcp  -- !10.217.0.0/16        10.96.0.10
/* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:domain
KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  anywhere 10.96.0.10 ADDRTYPE
    match dst-type LOCAL
/* kubernetes service nodeports<code class="p">;</code> NOTE: this must be the
    last rule in this chain */
KUBE-NODEPORTS  all  --  anywhere             anywhere</pre>

<p>There are a pair of <a data-type="indexterm" data-primary="UDP (User Datagram textProtocol)" data-secondary="kube-dns rules from" id="idm46219938899128"/>UDP and TCP rules for <code>kube-dns</code>. We’ll focus on the UDP ones.</p>

<p>The first <a data-type="indexterm" data-primary="masquerading connections, iptables" id="idm46219938897288"/>UDP rule marks any connections to the service
that <em>aren’t</em> originating from a pod IP address (<code>10.217.0.0/16</code> is the default pod network CIDR)
for masquerading.</p>

<p>The next UDP rule has the chain <code>KUBE-SVC-TCOU7JCQXEZGVUNU</code> as its target.
Let’s take a closer look:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo iptables -t nat -L KUBE-SVC-TCOU7JCQXEZGVUNU
Chain KUBE-SVC-TCOU7JCQXEZGVUNU <code class="o">(</code><code class="m">1</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination

/* kube-system/kube-dns:dns */
KUBE-SEP-OCPCMVGPKTDWRD3C  all -- anywhere anywhere  statistic mode
    random probability 0.50000000000
/* kube-system/kube-dns:dns */
KUBE-SEP-VFGOVXCRCJYSGAY3  all -- anywhere anywhere</pre>

<p>Here we see a chain with a 50% chance of executing, and the chain that will execute otherwise.
If we check the first of those chains, we see it routes to <code>10.0.1.141</code>, one of our two CoreDNS pods’ IPs:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo iptables -t nat -L KUBE-SEP-OCPCMVGPKTDWRD3C
Chain KUBE-SEP-OCPCMVGPKTDWRD3C <code class="o">(</code><code class="m">1</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination

/* kube-system/kube-dns:dns */
KUBE-MARK-MASQ  all  --  10.0.1.141           anywhere
/* kube-system/kube-dns:dns */ udp to:10.0.1.141:53
DNAT       udp  --  anywhere             anywhere</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="ipvs Mode"><div class="sect2" id="idm46219938946168">
<h2>ipvs Mode</h2>

<p><code>ipvs</code> mode uses <a data-type="indexterm" data-startref="ch4_term42" id="idm46219938866360"/><a data-type="indexterm" data-primary="IPVS (IP Virtual Server), Linux" data-secondary="as kube-proxy mode" data-secondary-sortas="kube-proxy mode" id="idm46219938865624"/><a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="IPVS mode" id="idm46219938864440"/>IPVS, covered in <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a>, instead of <code>iptables</code>, for connection load balancing.
<code>ipvs</code> mode supports <a data-type="indexterm" data-primary="source hashing (sh), IPVS mode" id="idm46219938861704"/><a data-type="indexterm" data-primary="shortest expected delay (sed), IPVS mode" id="idm46219938839400"/><a data-type="indexterm" data-primary="round-robin (rr), IPVS mode" id="idm46219938838696"/><a data-type="indexterm" data-primary="never queue (nq), IPVS mode" id="idm46219938838056"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with IPVS" data-secondary-sortas="IPVS" id="idm46219938837416"/><a data-type="indexterm" data-primary="least connection (lc), IPVS mode" id="idm46219938836200"/>six load balancing modes, specified with <code>--ipvs-scheduler</code>:</p>

<ul>
<li>
<p><code>rr</code>: Round-robin</p>
</li>
<li>
<p><code>lc</code>: Least connection</p>
</li>
<li>
<p><code>dh</code>: Destination hashing</p>
</li>
<li>
<p><code>sh</code>: Source hashing</p>
</li>
<li>
<p><code>sed</code>: Shortest expected delay</p>
</li>
<li>
<p><code>nq</code>: Never queue</p>
</li>
</ul>

<p><code>Round-robin</code> (<code>rr</code>) is the default load balancing mode. It is the closest analog to <code>iptables</code> mode’s behavior (in that connections are made fairly evenly regardless of pod state),
though <code>iptables</code> mode does not actually perform round-robin routing.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="kernelspace Mode"><div class="sect2" id="idm46219938809688">
<h2>kernelspace Mode</h2>

<p><code>kernelspace</code> is the <a data-type="indexterm" data-primary="kernelspace mode, kube-proxy" id="idm46219938807848"/><a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="kernelspace mode" id="idm46219938807144"/><a data-type="indexterm" data-primary="Windows, applications for" id="idm46219938806200"/><a data-type="indexterm" data-primary="IPVS (IP Virtual Server), Linux" data-secondary="Kubernetes-supported modes of" id="idm46219938805560"/>newest, Windows-only mode. It provides an alternative to 
<span class="keep-together"><code>userspace</code></span> mode for Kubernetes on Windows,
as <code>iptables</code> and <code>ipvs</code> are specific to Linux.</p>

<p>Now that we’ve covered the basics of pod-to-pod traffic in <a data-type="indexterm" data-startref="ch4_term38" id="idm46219938802536"/><a data-type="indexterm" data-startref="ch4_term39" id="idm46219938801832"/><a data-type="indexterm" data-startref="ch4_term40" id="idm46219938801160"/>Kubernetes,
let’s take a look at <code>NetworkPolicy</code> and securing pod-to-pod traffic.</p>
</div></section>





</div></section>













<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="NetworkPolicy"><div class="sect1" id="idm46219938987016">
<h1>NetworkPolicy</h1>

<p>Kubernetes’ default <a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="overview of" id="ch4_term43"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="NetworkPolicy overview and" id="ch4_term44"/>behavior is to allow traffic between any two pods in the cluster network. This behavior is a
deliberate design choice for ease of adoption and flexibility of configuration, but it is highly undesirable in
practice. Allowing any system to make (or receive) arbitrary connections creates risk.
An <a data-type="indexterm" data-primary="attacker strategies" data-secondary="NetworkPolicies and" id="idm46219938777992"/>attacker can probe systems and can potentially exploit captured credentials or find weakened or missing authentication.
Allowing arbitrary connections also makes it easier to exfiltrate data from a system through a compromised workload.
All in <a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="security with" id="idm46219938776664"/><a data-type="indexterm" data-primary="security" data-secondary="with NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219938775704"/><a data-type="indexterm" data-primary="security" data-secondary="limited connectivity for" id="idm46219938774488"/>all, we <em>strongly</em> discourage running real clusters without <code>NetworkPolicy</code>. Since all pods can communicate with
all other pods, we strongly recommend that application owners use <code>NetworkPolicy</code> objects along with other application-layer
security measures, such as <a data-type="indexterm" data-primary="mTLS (mutual Transport Layer Security) for encryption" id="idm46219938771960"/>authentication tokens or mutual Transport Layer Security (mTLS), for any network
communication.</p>

<p><code>NetworkPolicy</code> is a resource type in Kubernetes that <a data-type="indexterm" data-primary="firewalls" data-secondary="NetworkPolicy and" id="idm46219938769992"/>contains allow-based firewall rules. Users can add
<code>NetworkPolicy</code> objects to restrict connections to and from pods. The <code>NetworkPolicy</code> resource acts as a configuration
for CNI plugins, which themselves are responsible for ensuring connectivity between pods. The Kubernetes API declares
that <code>NetworkPolicy</code> support is optional for CNI drivers, which means that some <a data-type="indexterm" data-primary="Calico CNI plugin" id="idm46219938767432"/><a data-type="indexterm" data-primary="Cilium" data-secondary="as CNI plugin" data-secondary-sortas="CNI plugin" id="idm46219938766728"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubernetes plugins with" id="idm46219938765512"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="NetworkPolicy and" id="idm46219938764600"/><a data-type="indexterm" data-primary="Flannel CNI plugin" id="idm46219938763640"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI plug-ins for" id="idm46219938762968"/>CNI drivers do not support
network policies, as shown in <a data-type="xref" href="#common_cni_plugins">Table 4-3</a>.  If a developer creates a <code>NetworkPolicy</code> when using a CNI driver that does not
support <code>NetworkPolicy</code> objects, it does not affect the pod’s network security. Some CNI drivers, such as enterprise products or
company-internal CNI drivers, may introduce their equivalent of a <code>NetworkPolicy</code>. Some CNI drivers may also have
slightly different “interpretations” of the <code>NetworkPolicy</code> spec.</p>
<table id="common_cni_plugins">
<caption><span class="label">Table 4-3. </span>Common CNI plugins and NetworkPolicy support</caption>
<thead>
<tr>
<th>CNI plugin</th>
<th>NetworkPolicy supported</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Calico</p></td>
<td><p>Yes, and supports additional plugin-specific policies</p></td>
</tr>
<tr>
<td><p>Cilium</p></td>
<td><p>Yes, and supports additional plugin-specific policies</p></td>
</tr>
<tr>
<td><p>Flannel</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p>Kubenet</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>

<p><a data-type="xref" href="#the_broad_structure_of_a_networkpolicy">Example 4-4</a> details a <code>NetworkPolicy</code> object, which contains a <a data-type="indexterm" data-primary="pod selector" id="ch4_term48"/><a data-type="indexterm" data-primary="selector labels" id="idm46219938746776"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="with NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219938746104"/>pod selector, ingress rules, and egress rules. The policy
will apply to all pods in the same namespace as the <code>NetworkPolicy</code> that matches the selector label. This use of selector
labels is <a data-type="indexterm" data-primary="API, Kubernetes" id="idm46219938744184"/>consistent with other Kubernetes APIs: a spec identifies pods by their labels rather than their names or
parent objects.</p>
<div id="the_broad_structure_of_a_networkpolicy" data-type="example">
<h5><span class="label">Example 4-4. </span>The broad structure of a <code>NetworkPolicy</code></h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">NetworkPolicy</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">podSelector</code><code class="p">:</code>
    <code class="nt">matchLabels</code><code class="p">:</code>
      <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">demo</code>
  <code class="nt">policyTypes</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Egress</code>
  <code class="nt">ingress</code><code class="p">:</code> <code class="p-Indicator">[]</code><code class="l-Scalar-Plain">NetworkPolicyIngressRule</code> <code class="c1"># Not expanded</code>
  <code class="nt">egress</code><code class="p">:</code> <code class="p-Indicator">[]</code><code class="l-Scalar-Plain">NetworkPolicyEgressRule</code> <code class="c1"># Not expanded</code></pre></div>

<p>Before getting deep into the API, let’s walk <a data-type="indexterm" data-primary="security" data-secondary="limited connectivity for" id="ch4_term47"/>through a simple example of creating a <code>NetworkPolicy</code> to reduce the scope
of access for some pods. Let’s assume we have two distinct components: <code>demo</code> and <code>demo-DB</code>. As we have no existing
<code>NetworkPolicy</code> in <a data-type="xref" href="#img-unrestricted-pods">Figure 4-7</a>, all pods can communicate with all other pods (including hypothetically unrelated pods, not
shown).</p>

<figure><div id="img-unrestricted-pods" class="figure">
<img src="Images/neku_0407.png" alt="neku 0407" width="639" height="558"/>
<h6><span class="label">Figure 4-7. </span>Pods without <code>NetworkPolicy</code> objects</h6>
</div></figure>

<p>Let’s restrict <code>demo-DB</code>’s access level. If we create the following <code>NetworkPolicy</code> that selects <code>demo-DB</code> pods,
<code>demo-DB</code> pods will be unable to send or receive any traffic:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">NetworkPolicy</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-db</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">podSelector</code><code class="p">:</code>
    <code class="nt">matchLabels</code><code class="p">:</code>
      <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-db</code>
  <code class="nt">policyTypes</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Egress</code></pre>

<p>In <a data-type="xref" href="#img-demodb-pods-no-traffic">Figure 4-8</a>, we can <a data-type="indexterm" data-primary="app command, NetworkPolicy" id="idm46219938622088"/>now see that pods with the label <code>app=demo</code> can no longer create or receive connections.</p>

<figure><div id="img-demodb-pods-no-traffic" class="figure">
<img src="Images/neku_0408.png" alt="neku 0408" width="639" height="558"/>
<h6><span class="label">Figure 4-8. </span>Pods with the app:demo-db label cannot receive or send traffic</h6>
</div></figure>

<p>Having no network access is undesirable for most workloads, including our example database. Our <code>demo-db</code> should
(only) be able to receive connections from <code>demo</code> pods. To do that, we <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="rules and NetworkPolicy in" id="idm46219938617656"/><a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="rules and" id="idm46219938616632"/>must add an ingress rule to the <code>NetworkPolicy</code>:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">NetworkPolicy</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-db</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">podSelector</code><code class="p">:</code>
    <code class="nt">matchLabels</code><code class="p">:</code>
      <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-db</code>
  <code class="nt">policyTypes</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Egress</code>
  <code class="nt">ingress</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">from</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">podSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">demo</code></pre>

<p>Now <code>demo-db</code> pods can receive connections <a data-type="indexterm" data-startref="ch4_term45" id="idm46219938547608"/><a data-type="indexterm" data-startref="ch4_term46" id="idm46219938547000"/><a data-type="indexterm" data-startref="ch4_term47" id="idm46219938546328"/><a data-type="indexterm" data-startref="ch4_term48" id="idm46219938545656"/>only from <code>demo</code> pods.
Moreover, <code>demo-db</code> pods cannot make connections (as shown in <a data-type="xref" href="#img-demodb-pods-no-traffic-2">Figure 4-9</a>).</p>

<figure><div id="img-demodb-pods-no-traffic-2" class="figure">
<img src="Images/neku_0409.png" alt="neku 0409" width="639" height="558"/>
<h6><span class="label">Figure 4-9. </span>Pods with the app:demo-db label cannot create connections, and they can only receive connections from the app:demo pods</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If users <a data-type="indexterm" data-primary="attacker strategies" data-secondary="NetworkPolicies and" id="idm46219938540024"/>can unwittingly or maliciously change labels, they can change how <code>NetworkPolicy</code> objects apply to all pods.
In our prior example, if an attacker was able to edit the <code>app: demo-DB</code> label on a pod in that same namespace, the
<code>NetworkPolicy</code> that we created would no longer apply to that pod. Similarly, an attacker could gain access from
another pod in that namespace if they could add the label <code>app: demo</code> to their compromised pod.</p>
</div>

<p>The previous example is just an <a data-type="indexterm" data-startref="ch4_term43" id="idm46219938536664"/>example; with Cilium we can create these 
<span class="keep-together"><code>NetworkPolicy</code></span> objects for our Golang web server.</p>








<section data-type="sect2" data-pdf-bookmark="NetworkPolicy Example with Cilium"><div class="sect2" id="idm46219938534824">
<h2>NetworkPolicy Example with Cilium</h2>

<p>Our Golang web <a data-type="indexterm" data-primary="Cilium" data-secondary="NetworkPolicy example with" id="ch4_term50"/><a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="with NetworkPolicy example" data-secondary-sortas="NetworkPolicy example" id="ch4_term51"/><a data-type="indexterm" data-primary="kubectl commands" data-secondary="with Network Policy example" data-secondary-sortas="Network Policy example" id="ch4_term52"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="Cilium example in" id="ch4_term53"/><a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="examples of" id="ch4_term54"/>server now connects to a <a data-type="indexterm" data-primary="database, Postgres" id="idm46219938525688"/><a data-type="indexterm" data-primary="Postgres database" id="idm46219938525016"/>Postgres database with no TLS. Also, with no <code>NetworkPolicy</code> objects in place, any
pod on the network can sniff traffic between the Golang web server and the database, which is a potential security
risk. The following is going to deploy our Golang web application and its database and then deploy <code>NetworkPolicy</code> objects
that will only allow connectivity to the database from the web server. Using the same KIND cluster from the Cilium
install, let’s <a data-type="indexterm" data-primary="YAML configuration file" id="idm46219938523000"/>deploy the Postgres database with the following <code>YAML</code> and <code>kubectl</code> commands:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl apply -f database.yaml
service/postgres created
configmap/postgres-config created
statefulset.apps/postgres created</pre>

<p>Here we deploy our web server as a Kubernetes deployment to our KIND cluster:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl apply -f web.yaml
deployment.apps/app created</pre>

<p>To run <a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="with pod for NetworkPolicy" data-secondary-sortas="pod for NetworkPolicy" id="ch4_term49"/><a data-type="indexterm" data-primary="testing for network connections" id="ch4_term55"/>connectivity tests inside the cluster network, we will deploy and use a <code>dnsutils</code> pod that has basic
networking tools like <code>ping</code> and <code>curl</code>:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl apply -f dnsutils.yaml
pod/dnsutils created</pre>

<p>Since we are not deploying a service with an ingress, we can use <code>kubectl port-forward</code> to test connectivity to our
web server:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl port-forward app-5878d69796-j889q 8080:8080</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>More information about <code>kubectl port-forward</code> can be found in the <a href="https://oreil.ly/Ac6jk">documentation</a>.</p>
</div>

<p>Now from our <a data-type="indexterm" data-primary="cURL/curl tool" data-secondary="localhost command with" id="idm46219938427096"/>local terminal, we can reach our API:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>curl localhost:8080/
Hello
<code class="nv">$ </code>curl localhost:8080/healthz
Healthy
<code class="nv">$ </code>curl localhost:8080/data
Database Connected</pre>

<p>Let’s test connectivity to our web server inside the cluster from other pods. To do that, we <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with pods and pod networks" data-secondary-sortas="pods and pod networks" id="idm46219938403672"/>need to get
the IP address of our web server pod:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl get pods -l <code class="nv">app</code><code class="o">=</code>app -o wide
NAME                  READY  STATUS   RESTARTS  AGE  IP            NODE
app-5878d69796-j889q  1/1    Running  <code class="m">0</code>         87m  10.244.1.188  kind-worker3</pre>

<p>Now we can test L4 and L7 connectivity to the web server from the <code>dnsutils</code> pod:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- nc -z -vv 10.244.1.188 8080
10.244.1.188 <code class="o">(</code>10.244.1.188:8080<code class="o">)</code> open
sent 0, rcvd 0</pre>

<p>From our <code>dnsutils</code>, we can test the layer 7 HTTP API access:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 10.244.1.188:8080/
Hello

<code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 10.244.1.188:8080/data
Database Connected

<code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 10.244.1.188:8080/healthz
Healthy</pre>

<p>We can also test this on the database pod. First, we have to retrieve the IP address of the database
pod, <code>10.244.2.189</code>. We can use <code>kubectl</code> with a combination of labels and options to get this information:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl get pods -l <code class="nv">app</code><code class="o">=</code>postgres -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP             NODE
postgres-0   1/1     Running   <code class="m">0</code>          98m   10.244.2.189   kind-worker</pre>

<p>Again, let’s use <code>dnsutils</code> pod to <a data-type="indexterm" data-primary="database, Postgres" id="idm46219938325048"/><a data-type="indexterm" data-primary="Postgres database" id="idm46219938324408"/>test connectivity to the Postgres database over its default port 5432:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- nc -z -vv 10.244.2.189 5432
10.244.2.189 <code class="o">(</code>10.244.2.189:5432<code class="o">)</code> open
sent 0, rcvd 0</pre>

<p>The port is <a data-type="indexterm" data-startref="ch4_term49" id="idm46219938274136"/>open for all to use since no network policies are in place. Now let’s restrict this with a Cilium network
policy. The following commands deploy the network policies so that we can test the secure network connectivity. Let’s
first restrict access to the database pod to only the web server. Apply the network policy that only allows
traffic from the web server pod to the database:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl apply -f layer_3_net_pol.yaml
ciliumnetworkpolicy.cilium.io/l3-rule-app-to-db created</pre>

<p>The Cilium deploy of Cilium objects creates resources that can be retrieved just like pods with <code>kubectl</code>. With
<code>kubectl describe ciliumnetworkpolicies.cilium.io l3-rule-app-to-db</code>, we can see all the information about the rule
deployed <a data-type="indexterm" data-primary="YAML configuration file" id="idm46219938271416"/>via the YAML:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl describe ciliumnetworkpolicies.cilium.io l3-rule-app-to-db
Name:         l3-rule-app-to-db
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  API Version:  cilium.io/v2
Kind:         CiliumNetworkPolicy
Metadata:
Creation Timestamp:  2021-01-10T01:06:13Z
Generation:          1
Managed Fields:
API Version:  cilium.io/v2
Fields Type:  FieldsV1
fieldsV1:
f:metadata:
f:annotations:
.:
f:kubectl.kubernetes.io/last-applied-configuration:
f:spec:
.:
f:endpointSelector:
.:
f:matchLabels:
.:
f:app:
f:ingress:
Manager:         kubectl
Operation:       Update
Time:            2021-01-10T01:06:13Z
Resource Version:  47377
Self Link:
/apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/l3-rule-app-to-db
UID:       71ee6571-9551-449d-8f3e-c177becda35a
Spec:
Endpoint Selector:
Match Labels:
App:  postgres
Ingress:
From Endpoints:
Match Labels:
App:  app
Events:       &lt;none&gt;</pre>

<p>With the network policy <a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="with pod for NetworkPolicy" data-secondary-sortas="pod for NetworkPolicy" id="idm46219938250488"/>applied, the <code>dnsutils</code> pod can no longer reach the database pod; we can see this in the
timeout trying to reach the DB port from the <code>dnsutils</code> pods:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- nc -z -vv -w <code class="m">5</code> 10.244.2.189 5432
nc: 10.244.2.189 <code class="o">(</code>10.244.2.189:5432<code class="o">)</code>: Operation timed out
sent 0, rcvd 0
<code class="nb">command </code>terminated with <code class="nb">exit </code>code 1</pre>

<p>While the web server pod is still connected to the database pod, the <code>/data</code> route connects the web server to the database
and the <code>NetworkPolicy</code> allows it:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 10.244.1.188:8080/data
Database Connected

<code class="nv">$ </code>curl localhost:8080/data
Database Connected</pre>

<p>Now let’s <a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="with Cilium" data-secondary-sortas="Cilium" id="idm46219938301480"/><a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="with HTTP" data-secondary-sortas="HTTP" id="idm46219938300328"/><a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="with NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219938299112"/><a data-type="indexterm" data-primary="HTTP GET command" id="idm46219938148536"/><a data-type="indexterm" data-primary="HTTP" data-secondary="requests, filtering of" id="idm46219938147864"/>apply the layer 7 policy. Cilium is layer 7 aware so that we can block or allow a specific request on the HTTP
URI paths. In our example policy, we allow HTTP GETs on <code>/</code> and <code>/data</code> but do not allow them on <code>/healthz</code>; let’s test that:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl apply -f layer_7_netpol.yml
ciliumnetworkpolicy.cilium.io/l7-rule created</pre>

<p>We can see the policy applied just like any other Kubernetes objects in the API:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl get ciliumnetworkpolicies.cilium.io
NAME      AGE
l7-rule   6m54s

<code class="nv">$ </code>kubectl describe ciliumnetworkpolicies.cilium.io l7-rule
Name:         l7-rule
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  API Version:  cilium.io/v2
Kind:         CiliumNetworkPolicy
Metadata:
  Creation Timestamp:  2021-01-10T00:49:34Z
  Generation:          1
  Managed Fields:
    API Version:  cilium.io/v2
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:kubectl.kubernetes.io/last-applied-configuration:
      f:spec:
        .:
        f:egress:
        f:endpointSelector:
          .:
          f:matchLabels:
            .:
            f:app:
    Manager:         kubectl
    Operation:       Update
    Time:            2021-01-10T00:49:34Z
  Resource Version:  43869
  Self Link:/apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/l7-rule
  UID:               0162c16e-dd55-4020-83b9-464bb625b164
Spec:
  Egress:
    To Ports:
      Ports:
        Port:      8080
        Protocol:  TCP
      Rules:
        Http:
          Method:  GET
          Path:    /
          Method:  GET
          Path:    /data
  Endpoint Selector:
    Match Labels:
      App:  app
Events:     &lt;none&gt;</pre>

<p>As we can see, <code>/</code> and <code>/data</code> are available but not <code>/healthz</code>, precisely what we expect from the <code>NetworkPolicy</code>:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 10.244.1.188:8080/data
Database Connected

<code class="nv">$kubectl</code> <code class="nb">exec </code>dnsutils -- wget -qO- 10.244.1.188:8080/
Hello

<code class="nv">$ </code>kubectl <code class="nb">exec </code>dnsutils -- wget -qO- -T <code class="m">5</code> 10.244.1.188:8080/healthz
wget: error getting response
<code class="nb">command </code>terminated with <code class="nb">exit </code>code 1</pre>

<p>These small examples show how powerful the Cilium network policies can enforce network security inside the cluster. We
highly <a data-type="indexterm" data-primary="cluster administrators" data-secondary="CNI selection by" id="idm46219938083512"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="NetworkPolicy and" id="idm46219938082664"/>recommend that administrators select a CNI that supports network policies and enforce developers’ use of network
policies. Network policies are namespaced, and if teams have similar setups, cluster administrators can and should
enforce that developers define network policies for added <a data-type="indexterm" data-startref="ch4_term44" id="idm46219938123368"/><a data-type="indexterm" data-startref="ch4_term50" id="idm46219938122696"/><a data-type="indexterm" data-startref="ch4_term51" id="idm46219938122024"/><a data-type="indexterm" data-startref="ch4_term52" id="idm46219938121352"/><a data-type="indexterm" data-startref="ch4_term53" id="idm46219938120680"/><a data-type="indexterm" data-startref="ch4_term54" id="idm46219938120008"/><a data-type="indexterm" data-startref="ch4_term55" id="idm46219938119336"/>security.</p>

<p>We used two aspects of the Kubernetes API, labels and selectors; in our next section, we will provide more examples of
how they are used inside a cluster.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Selecting Pods"><div class="sect2" id="idm46219938533912">
<h2>Selecting Pods</h2>

<p>Pods are <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="pods and NetworkPolicy in" id="ch4_term57"/><a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="pods and" id="ch4_term58"/><a data-type="indexterm" data-primary="pod selector" id="ch4_term59"/>unrestricted until they are <em>selected</em> by a <code>NetworkPolicy</code>. If selected, the CNI plugin <a data-type="indexterm" data-primary="egress" id="idm46219938046632"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="with NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219938045928"/>allows pod
ingress or egress only when a matching rule allows it. A <code>NetworkPolicy</code> has a <code>spec.policyTypes</code> field containing a list
of policy types (ingress or egress). For example, if we select a pod with a <code>NetworkPolicy</code> that has ingress listed but
not egress, then ingress will be restricted, and egress will not.</p>

<p>The <code>spec.podSelector</code> field will dictate which pods to apply the <code>NetworkPolicy</code> to. An empty <code>label selector.</code>
(<code>podSelector: {}</code>) will select all pods in the <a data-type="indexterm" data-primary="namespaces, network" data-secondary="in NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219938077272"/>namespace. We <a data-type="indexterm" data-primary="LabelSelectors, Kubernetes" id="ch4_term56"/>will discuss label selectors in more detail shortly.</p>

<p><code>NetworkPolicy</code> objects are <em>namespaced</em> objects, which means they exist in and apply to a specific namespace. The <code>spec
.podSelector</code> field can select pods only when they are in the same namespace as the <code>NetworkPolicy</code>. This means selecting <code>app:
demo</code> will <a data-type="indexterm" data-primary="app command, NetworkPolicy" id="idm46219938072280"/>apply only in the current namespace, and any pods in another namespace with the label <code>app: demo</code> will be
unaffected.</p>

<p>There are multiple workarounds to <a data-type="indexterm" data-primary="firewalls" data-secondary="NetworkPolicy and" id="idm46219938070744"/>achieve firewalled-by-default behavior, including the following:</p>

<ul>
<li>
<p>Creating a blanket deny-all <code>NetworkPolicy</code> object for every namespace,
which will require developers to add additional <code>NetworkPolicy</code> objects to allow desired traffic.</p>
</li>
<li>
<p>Adding a custom CNI plugin that deliberately violates the default-open API behavior.
Multiple CNI plugins have an additional configuration that exposes this kind of behavior.</p>
</li>
<li>
<p>Creating admission policies to require that workloads have a <code>NetworkPolicy</code>.</p>
</li>
</ul>

<p><code>NetworkPolicy</code> objects rely heavily on labels and selectors; for that reason, let’s dive into more complex examples.</p>










<section data-type="sect3" data-pdf-bookmark="The LabelSelector type"><div class="sect3" id="idm46219938028616">
<h3>The LabelSelector type</h3>

<p>This is the first time in this book that we see a <code>LabelSelector</code> in a resource. It is a ubiquitous configuration
element in Kubernetes and will come up many times in the next chapter, so when you get there, it may be helpful to
look back at this section as a reference.</p>

<p>Every object in <a data-type="indexterm" data-primary="metadata field, Kubernetes" id="idm46219938025992"/><a data-type="indexterm" data-primary="ObjectMeta type, Kubernetes" id="idm46219938025192"/>Kubernetes has a <code>metadata</code> field, with the type <code>ObjectMeta</code>. That gives every type the same
metadata fields, like labels. Labels are a map of key-value string pairs:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">labels</code><code class="p">:</code>
    <code class="nt">colour</code><code class="p">:</code> <code class="l-Scalar-Plain">purple</code>
    <code class="nt">shape</code><code class="p">:</code> <code class="l-Scalar-Plain">square</code></pre>

<p>A <code>LabelSelector</code> identifies a group of resources by the present labels (or absent). Very few resources in Kubernetes
will refer to other resources by name. Instead, most resources (<code>NetworkPolicy</code> objects, services, deployments, and other
Kubernetes objects). use label matching with a <code>LabelSelector</code>. <code>LabelSelector</code>s can also be used in API and <code>kubectl</code>
calls and avoid returning irrelevant objects. A <code>LabelSelector</code> has <a data-type="indexterm" data-primary="matchExpressions field, NetworkPolicy" id="ch4_term60"/><a data-type="indexterm" data-primary="matchLabels field, NetworkPolicy" id="ch4_term61"/>two fields: <code>matchExpressions</code> and <code>matchLabels</code>.
The normal behavior for an empty <code>LabelSelector</code> is to select all objects in scope, e.g., all pods in the same
namespace as a <code>NetworkPolicy</code>. <code>matchLabels</code> is the simpler of the two. <code>matchLabels</code> contains a map of key-value pairs.
For an object to match, each key must be present on the object, and that key must have the corresponding value.
<code>matchLabels</code>, often with a single key (e.g., <code>app=example-thing</code>), is usually sufficient for a selector.</p>

<p>In <a data-type="xref" href="#matchlabel_example">Example 4-5</a>, we can see a match object that has both the label <code>colour=purple</code> and the label <code>shape=square</code>.</p>
<div id="matchlabel_example" data-type="example">
<h5><span class="label">Example 4-5. </span><code>matchLabels</code> example</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">matchLabels</code><code class="p">:</code>
  <code class="nt">colour</code><code class="p">:</code> <code class="l-Scalar-Plain">purple</code>
  <code class="nt">shape</code><code class="p">:</code> <code class="l-Scalar-Plain">square</code></pre></div>

<p><code>matchExpressions</code> is more powerful but more complicated. It contains a list of <code>LabelSelectorRequirement</code>s.
All requirements must be true in order for an object to match. <a data-type="xref" href="#labelselectorrequirement_fields">Table 4-4</a> shows all the required fields for a
<code>matchExpressions</code>.</p>
<table id="labelselectorrequirement_fields" class="less_space pagebreak-before">
<caption><span class="label">Table 4-4. </span><code>LabelSelectorRequirement</code> fields</caption>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>key</p></td>
<td><p>The label key this requirement compares against.</p></td>
</tr>
<tr>
<td><p>operator</p></td>
<td><p>One of <code>Exists</code>, <code>DoesNotExist</code>, <code>In</code>, <code>NotIn</code>.
</p><p><code>Exists</code>: Matches an object if there is a label with the key, regardless of the value.
</p><p><code>NotExists</code>: Matches an object if there is no label with the key.
</p><p><code>In</code>: Matches an object if there is a label with the key, and the value is one of the provided values.
</p><p><code>NotIn</code>: Matches an object if there is no label with the key,
<em>or</em> the key’s value is not one of the provided values.</p></td>
</tr>
<tr>
<td><p>values</p></td>
<td><p>A list of string values for the key in question.
It must be empty when the operator is <code>In</code> or <code>NotIn</code>.
It may not be empty when the operator is <code>Exists</code> or <code>NotExists</code>.</p></td>
</tr>
</tbody>
</table>

<p>Let’s look <a data-type="indexterm" data-startref="ch4_term56" id="idm46219937952440"/>at two brief examples of <code>matchExpressions</code>.</p>

<p>The <code>matchExpressions</code> equivalent of our prior <code>matchLabels</code> example is shown in <a data-type="xref" href="#matchexpressions_example_1">Example 4-6</a>.</p>
<div id="matchexpressions_example_1" data-type="example">
<h5><span class="label">Example 4-6. </span><code>matchExpressions</code> example 1</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">matchExpressions</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">colour</code>
    <code class="nt">operator</code><code class="p">:</code> <code class="l-Scalar-Plain">In</code>
    <code class="nt">values</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">purple</code>
  <code class="p-Indicator">-</code> <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">shape</code>
    <code class="nt">operator</code><code class="p">:</code> <code class="l-Scalar-Plain">In</code>
    <code class="nt">values</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">square</code></pre></div>

<p><code>matchExpressions</code> in <a data-type="xref" href="#matchexpressions_example_2">Example 4-7</a>, will match objects with a color not equal to red, orange, or
yellow, and with a shape label.</p>
<div id="matchexpressions_example_2" data-type="example">
<h5><span class="label">Example 4-7. </span><code>matchExpressions</code> example 2</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">matchExpressions</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">colour</code>
    <code class="nt">operator</code><code class="p">:</code> <code class="l-Scalar-Plain">NotIn</code>
    <code class="nt">values</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">red</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">orange</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">yellow</code>
  <code class="p-Indicator">-</code> <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">shape</code>
    <code class="nt">operator</code><code class="p">:</code> <code class="l-Scalar-Plain">Exists</code></pre></div>

<p class="pagebreak-before">Now that we have labels covered, we can discuss <a data-type="indexterm" data-startref="ch4_term57" id="idm46219937901688"/><a data-type="indexterm" data-startref="ch4_term58" id="idm46219937883832"/><a data-type="indexterm" data-startref="ch4_term59" id="idm46219937844904"/><a data-type="indexterm" data-startref="ch4_term60" id="idm46219937844264"/><a data-type="indexterm" data-startref="ch4_term61" id="idm46219937843592"/>rules. Rules will enforce our network policies after a match has been
identified.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Rules"><div class="sect2" id="idm46219938028024">
<h2>Rules</h2>

<p><code>NetworkPolicy</code> objects contain <a data-type="indexterm" data-primary="egress" id="ch4_term62"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="with NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219937839096"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="rules and NetworkPolicy in" id="ch4_term64"/><a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="rules and" id="ch4_term65"/>distinct ingress and egress configuration sections, which contain a list of ingress rules and egress rules, respectively. <code>NetworkPolicy</code> rules act as exceptions, or an “allow list,” to the default block caused by
selecting pods in a policy. Rules cannot block access; they can only add access. If multiple <code>NetworkPolicy</code> objects select a
pod, all rules in each of those <code>NetworkPolicy</code> objects apply. It may make sense to use multiple <code>NetworkPolicy</code> objects for the same
set of pods (for example, declaring application allowances in one policy and infrastructure allowances like telemetry
exporting in another). However, keep in mind that they do not <em>need</em> to be separate <code>NetworkPolicy</code> objects, and with too
many <code>NetworkPolicy</code> objects it can become hard to reason.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>To support <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="with probes for health of pods" data-secondary-sortas="probes for health of pods" id="idm46219937831224"/>health checks and liveness checks from the Kubelet,
the CNI plugin must always allow traffic <a data-type="indexterm" data-startref="ch4_term62" id="idm46219937829672"/><a data-type="indexterm" data-startref="ch4_term63" id="idm46219937829000"/>from a pod’s node.</p>

<p>It is <a data-type="indexterm" data-primary="attacker strategies" data-secondary="NetworkPolicies and" id="idm46219937827784"/>possible to abuse labels if an attacker has access to the node (even without admin privileges).  Attackers can
spoof a node’s IP and deliver packets with the node’s IP address as the source.</p>
</div>

<p>Ingress rules and egress rules are discrete types in the <code>NetworkPolicy</code> API (<code>NetworkPolicyIngressRule</code> and
<code>NetworkPolicyEgressRule</code>). However, they are functionally structured the same way. Each
<code>NetworkPolicyIngressRule</code>/<code>NetworkPolicyEgressRule</code> contains a list of ports
and a <a data-type="indexterm" data-primary="NetworkPolicyPeer rules" id="idm46219937823544"/>list of <code>NetworkPolicyPeers</code>.</p>

<p>A <code>NetworkPolicyPeer</code> has four ways for <a data-type="indexterm" data-primary="ipBlock, NetworkPolicy" id="idm46219937821304"/>rules to refer to networked entities: <code>ipBlock</code>, <code>namespaceSelector</code>,
<code>podSelector</code>, and a combination.</p>

<p><code>ipBlock</code> is useful for allowing traffic to and from external systems. It can be used only on its own in a rule,
without a <code>namespaceSelector</code> or <code>podSelector</code>. <code>ipBlock</code> contains a <a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="with NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219937790120"/>CIDR and an optional <code>except</code> CIDR. The <code>except</code>
CIDR will exclude a sub-CIDR (it must be within the CIDR range). In <a data-type="xref" href="#allow_traffic_example_1">Example 4-8</a>, we allow traffic from all IP addresses in
the range <code>10.0.0.0</code> to <code>10.0.0.255</code>, excluding <code>10.0.0.10</code>. <a data-type="xref" href="#allow_traffic_example_2">Example 4-9</a> allows traffic from all pods in any namespace
labeled <code>group:x</code>.</p>
<div id="allow_traffic_example_1" data-type="example">
<h5><span class="label">Example 4-8. </span>Allow traffic example 1</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">from</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">ipBlock</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">cidr</code><code class="p">:</code> <code class="s">"10.0.0.0/24"</code>
    <code class="p-Indicator">-</code> <code class="nt">except</code><code class="p">:</code> <code class="s">"10.0.0.10"</code></pre></div>
<div id="allow_traffic_example_2" data-type="example">
<h5><span class="label">Example 4-9. </span>Allow traffic example 2</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="c1">#</code>
<code class="nt">from</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">namespaceSelector</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">matchLabels</code><code class="p">:</code>
      <code class="nt">group</code><code class="p">:</code> <code class="l-Scalar-Plain">x</code></pre></div>

<p>In <a data-type="xref" href="#allow_traffic_example_3">Example 4-10</a>, we allow traffic from all pods in any namespace labeled <code>service: x.</code>. <code>podSelector</code> behaves like the
<code>spec.podSelector</code> field that we discussed earlier.<a data-type="indexterm" data-primary="namespaces, network" data-secondary="in NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="ch4_term66"/><a data-type="indexterm" data-primary="namespaceSelector, NetworkPolicy" id="idm46219937755752"/> 
<span class="keep-together">If there</span> is no <code>namespaceSelector</code>, it selects pods in the same namespace as the 
<span class="keep-together"><code>NetworkPolicy</code>.</span></p>
<div id="allow_traffic_example_3" data-type="example">
<h5><span class="label">Example 4-10. </span>Allow traffic example 3</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">from</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">podSelector</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">matchLabels</code><code class="p">:</code>
      <code class="nt">service</code><code class="p">:</code> <code class="l-Scalar-Plain">y</code></pre></div>

<p>If we <a data-type="indexterm" data-primary="pod selector" id="idm46219937711800"/><a data-type="indexterm" data-primary="podSelector, NetworkPolicy" id="idm46219937711192"/>specify a <code>namespaceSelector</code> and a <code>podSelector</code>, the rule selects all pods with the specified pod label
in all namespaces with the specified namespace label. It is common and highly recommended by security experts to keep
the scope of a namespace small; typical namespace scopes are per an app or service group or team. There is a
fourth option shown in <a data-type="xref" href="#allow_traffic_example_4">Example 4-11</a> with a namespace <em>and</em> pod selector. This selector behaves like an AND
condition for the namespace and pod selector: pods must have the matching label and be in a namespace with the matching label.</p>
<div id="allow_traffic_example_4" data-type="example">
<h5><span class="label">Example 4-11. </span>Allow traffic example 4</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">from</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">namespaceSelector</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">matchLabels</code><code class="p">:</code>
      <code class="nt">group</code><code class="p">:</code> <code class="l-Scalar-Plain">monitoring</code>
    <code class="nt">podSelector</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">matchLabels</code><code class="p">:</code>
      <code class="nt">service</code><code class="p">:</code> <code class="l-Scalar-Plain">logscraper</code></pre></div>

<p>Be aware this is a <a data-type="indexterm" data-primary="API, Kubernetes" id="idm46219937671928"/><a data-type="indexterm" data-primary="YAML configuration file" id="idm46219937671544"/>distinct type in the API, although the YAML syntax looks <em>extremely</em> similar. As <code>to</code> and <code>from</code>
sections can have multiple selectors, a single character can make the difference between an <code>AND</code> and an <code>OR</code>, so be
careful when writing policies.</p>

<p>Our earlier <a data-type="indexterm" data-primary="security" data-secondary="with NetworkPolicy" data-secondary-sortas="NetworkPolicy" id="idm46219937631016"/><a data-type="indexterm" data-primary="attacker strategies" data-secondary="NetworkPolicies and" id="idm46219937629736"/>security warning about API access also applies here. If a user can customize the labels on their namespace,
they can make a <code>NetworkPolicy</code> in another namespace apply to their namespace in a way not intended.
In our previous selector example,  if a user can set the label <code>group: monitoring</code> on an arbitrary namespace, they can
potentially send or receive traffic that they are not supposed to. If the <code>NetworkPolicy</code> in question has only a namespace selector, then that namespace label is sufficient to match the policy. If there is also a pod label
in the <code>NetworkPolicy</code> selector, the user will need to set pod labels to match the policy selection. However, in a
typical setup, the service owners will grant create/update permissions on pods in that service’s namespace (directly
on the pod resource or indirectly via a resource like a deployment, which can define pods).</p>

<p>A <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="NetworkPolicy overview and" id="ch4_term67"/><a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="examples of" id="ch4_term68"/>typical <code>NetworkPolicy</code> could look something like this:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">NetworkPolicy</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">store-api</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">store</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">podSelector</code><code class="p">:</code>
    <code class="nt">matchLabels</code><code class="p">:</code> <code class="p-Indicator">{}</code>
  <code class="nt">policyTypes</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Egress</code>
  <code class="nt">ingress</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">from</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">namespaceSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">frontend</code>
      <code class="nt">podSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">frontend</code>
    <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>
      <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
  <code class="nt">egress</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">to</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">namespaceSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">downstream-1</code>
      <code class="nt">podSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">downstream-1</code>
    <code class="p-Indicator">-</code> <code class="nt">namespaceSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">downstream-2</code>
      <code class="nt">podSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">downstream-2</code>
    <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>
      <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code></pre>

<p>In this example, <a data-type="indexterm" data-primary="app command, NetworkPolicy" id="idm46219937615768"/>all pods in our <code>store</code> namespace can receive connections only from pods labeled <code>app: frontend</code>
in a namespace labeled <code>app: frontend</code>. Those pods can only create connections to pods in namespaces where the pod
and namespace both have <code>app: downstream-1</code> or <code>app: downstream-2</code>. In each of these cases, only traffic to port 8080
is allowed. Finally, remember that this policy does not guarantee a matching policy for <code>downstream-1</code> or
<code>downstream-2</code> (see the next example). Accepting these connections does not preclude other policies against pods in our
namespace, adding additional exceptions:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">NetworkPolicy</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">store-to-downstream-1</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">downstream-1</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">podSelector</code><code class="p">:</code>
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">downstream-1</code>
  <code class="nt">policyTypes</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">Ingress</code>
  <code class="nt">ingress</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">from</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">namespaceSelector</code><code class="p">:</code>
        <code class="nt">matchLabels</code><code class="p">:</code>
          <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">store</code>
    <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>
      <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code></pre>

<p>Although they <a data-type="indexterm" data-startref="ch4_term66" id="idm46219937339656"/><a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="overview of" id="idm46219937343928"/>are a “stable” resource (i.e., part of the networking/v1 API), we believe <code>NetworkPolicy</code> objects are still an
early version of network security in Kubernetes. The user experience of configuring <code>NetworkPolicy</code> objects is somewhat rough,
and the default open behavior is highly undesirable.
There is currently a working group to discuss the future of <code>NetworkPolicy</code> and what a v2 API would contain.</p>

<p>CNIs and those who deploy them use labels and selectors to determine which pods are subject to network
restrictions. As we have seen in many of the previous examples, they are an essential part of the Kubernetes API, and
developers and administrators alike must have a thorough knowledge of how to use them.</p>

<p><code>NetworkPolicy</code> objects are an <a data-type="indexterm" data-primary="cluster administrators" data-secondary="NetworkPolicies and" id="idm46219937465768"/>important tool in the cluster administrator’s toolbox. They are the only tool available for
controlling internal cluster traffic, native to the Kubernetes API. We discuss service meshes, which will add
further tools for admins to secure and control workloads, in <a data-type="xref" href="ch05.xhtml#servicemeshes">“Service Meshes”</a>.</p>

<p>Next we will <a data-type="indexterm" data-startref="ch4_term64" id="idm46219937463240"/><a data-type="indexterm" data-startref="ch4_term65" id="idm46219937462504"/><a data-type="indexterm" data-startref="ch4_term67" id="idm46219937461832"/><a data-type="indexterm" data-startref="ch4_term68" id="idm46219937461160"/>discuss another important tool so administrators can understand how it works inside the cluster: the Domain
Name System (DNS).</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="DNS"><div class="sect1" id="idm46219937841752">
<h1>DNS</h1>

<p>DNS is a <a data-type="indexterm" data-primary="DNS (Domain Name System)" data-secondary="in Kubernetes" data-secondary-sortas="Kubernetes" id="ch4_term69"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="DNS in" id="ch4_term70"/>critical piece of infrastructure for any network. In Kubernetes, this is no different, so a brief overview is
warranted. In the following “Services” sections, we will see how much they depend on DNS and why a Kubernetes
distribution cannot declare that it is a conforming Kubernetes distribution without providing a DNS service that
follows the specification. But first, let’s review how DNS works inside 
<span class="keep-together">Kubernetes.</span></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We will not outline the entire specification in this book. If readers are interested in reading more about it, it is
available on <a href="https://oreil.ly/tiB8V">GitHub</a>.</p>
</div>

<p>KubeDNS was <a data-type="indexterm" data-primary="kube-dns service cluster" id="idm46219937452648"/>used in earlier versions of Kubernetes. KubeDNS had several containers within a single pod:
<code>kube-dns</code>, <code>dnsmasq</code>, and <code>sidecar</code>. The <code>kube-dns</code> container watches the Kubernetes API and serves DNS records based on the
Kubernetes DNS specification, <code>dnsmasq</code> provides caching and stub domain support, and <code>sidecar</code> provides <a data-type="indexterm" data-primary="health checks/probes" data-secondary="with CoreDNS" data-secondary-sortas="CoreDNS" id="idm46219937449016"/>metrics and
health checks. Versions of Kubernetes after 1.13 now use the separate component CoreDNS.</p>

<p>There are several differences between CoreDNS and KubeDNS:</p>

<ul>
<li>
<p>For simplicity, CoreDNS runs as a single container.</p>
</li>
<li>
<p>CoreDNS is a Go process that replicates and enhances the functionality of 
<span class="keep-together">Kube-DNS.</span></p>
</li>
<li>
<p>CoreDNS is designed to be a general-purpose DNS server that is backward compatible with Kubernetes, and its
extendable plugins can do more than is provided in the Kubernetes DNS specification.</p>
</li>
</ul>

<p><a data-type="xref" href="#core-dns">Figure 4-10</a> shows <a data-type="indexterm" data-primary="CoreDNS" data-secondary="components of" id="idm46219937442312"/>the components of CoreDNS.  It runs a deployment with a default replica of 2, and for it to
run, CoreDNS needs access to the API server, a ConfigMap to hold its Corefile, a  service to make DNS available to
the cluster, and a deployment to launch and manage its pods. All of this also runs in the <code>kube-system</code> namespace along
with other critical components in the cluster.</p>

<figure><div id="core-dns" class="figure">
<img src="Images/neku_0410.png" alt="core-dns" width="1130" height="626"/>
<h6><span class="label">Figure 4-10. </span>CoreDNS components</h6>
</div></figure>

<p>Like most configuration options, how the <a data-type="indexterm" data-primary="CoreDNS" data-secondary="dnsPolicy and" id="ch4_term71"/><a data-type="indexterm" data-primary="dnsPolicy, Kubernetes" id="ch4_term72"/><a data-type="indexterm" data-primary="pods" data-secondary="CoreDNS/dnsPolicy and" id="ch4_term73"/>pod does DNS queries is in the pod spec under the <code>dnsPolicy</code> attribute.</p>

<p>Outlined in <a data-type="xref" href="#pod_spec_with_dns_configuration">Example 4-12</a>, the pod <a data-type="indexterm" data-primary="ClusterFirstWithHostNet, dnsPolicy" id="idm46219937432760"/>spec has <code>ClusterFirstWithHostNet</code> as 
<span class="keep-together"><code>dnsPolicy</code>.</span></p>
<div id="pod_spec_with_dns_configuration" data-type="example">
<h5><span class="label">Example 4-12. </span>Pod spec with DNS configuration</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">busybox</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">busybox:1.28</code>
    <code class="nt">command</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">sleep</code>
      <code class="p-Indicator">-</code> <code class="s">"3600"</code>
    <code class="nt">imagePullPolicy</code><code class="p">:</code> <code class="l-Scalar-Plain">IfNotPresent</code>
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">busybox</code>
  <code class="nt">restartPolicy</code><code class="p">:</code> <code class="l-Scalar-Plain">Always</code>
  <code class="nt">hostNetwork</code><code class="p">:</code> <code class="l-Scalar-Plain">true</code>
  <code class="nt">dnsPolicy</code><code class="p">:</code> <code class="l-Scalar-Plain">ClusterFirstWithHostNet</code></pre></div>

<p class="pagebreak-before">There are four options for <code>dnsPolicy</code> that
significantly affect how DNS resolutions work inside a pod:</p>
<dl>
<dt><code>Default</code></dt>
<dd>
<p>The pod inherits the name resolution configuration from the node that the pods run on.</p>
</dd>
<dt><code>ClusterFirst</code></dt>
<dd>
<p>Any DNS query that does not match the  cluster domain suffix, such as www.kubernetes.io, is sent
to the upstream name server inherited from the node.</p>
</dd>
<dt><code>ClusterFirstWithHostNet</code></dt>
<dd>
<p>For pods running with <code>hostNetwork</code>, admins should set the DNS policy to

<span class="keep-together"><code>ClusterFirstWithHostNet</code>.</span></p>
</dd>
<dt><code>None</code></dt>
<dd>
<p>All DNS settings use the <code>dnsConfig</code> field in the pod spec.</p>
</dd>
</dl>

<p>If <code>none</code>, developers will have to specify name servers in the pod spec. <code>nameservers:</code> is a list of IP addresses that
the pod will use as DNS servers. There can be at most three IP addresses specified. <code>searches:</code> is a list of DNS search
domains for hostname lookup in the pod. Kubernetes allows for at most six search domains. The following is such an example
spec:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">busybox</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">busybox:1.28</code>
    <code class="nt">command</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">sleep</code>
      <code class="p-Indicator">-</code> <code class="s">"3600"</code>
    <code class="nt">imagePullPolicy</code><code class="p">:</code> <code class="l-Scalar-Plain">IfNotPresent</code>
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">busybox</code>
  <code class="nt">dnsPolicy</code><code class="p">:</code> <code class="s">"None"</code>
  <code class="nt">dnsConfig</code><code class="p">:</code>
    <code class="nt">nameservers</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">1.1.1.1</code>
    <code class="nt">searches</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">ns1.svc.cluster-domain.example</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">my.dns.search.suffix</code></pre>

<p>Others are in the <code>options</code> field, which is a list of objects where each object may have a <code>name</code> property and a <code>value</code> property
(optional).</p>

<p>All of these generated properties merge with <code>resolv.conf</code> from the DNS policy. Regular query options have CoreDNS
going through the following search path:</p>

<pre data-type="programlisting" data-code-language="bash">&lt;service&gt;.default.svc.cluster.local
                ↓
        svc.cluster.local
                ↓
          cluster.local
                ↓
        The host search path</pre>

<p>The host search path comes from the pod DNS policy or CoreDNS <a data-type="indexterm" data-startref="ch4_term71" id="idm46219937132488"/><a data-type="indexterm" data-startref="ch4_term72" id="idm46219937131784"/>policy.</p>

<p>Querying a <a data-type="indexterm" data-primary="Autopath, Kubernetes" id="idm46219937130632"/><a data-type="indexterm" data-primary="latency, routing" id="idm46219937129896"/><a data-type="indexterm" data-primary="routing" data-secondary="latency in" id="idm46219937175016"/>DNS record in Kubernetes can result in many requests and increase latency in applications waiting on DNS
requests to be answered. CoreDNS has a solution for this called Autopath. Autopath allows for server-side search path
completion. It short circuits the client’s search path resolution by stripping the cluster search domain and
performing the lookups on the CoreDNS server; when it finds an answer, it stores the result as a CNAME and returns
with one query instead of five.</p>

<p>Using Autopath <a data-type="indexterm" data-primary="memory issues, CoreDNS and" id="idm46219937173064"/>does increase the memory usage on CoreDNS, however. Make sure to scale the CoreDNS replica’s memory
with the cluster’s size. Make sure to set the requests for memory and CPU for the CoreDNS pods appropriately. <a data-type="indexterm" data-primary="CoreDNS" data-secondary="monitoring" id="idm46219937172008"/>To
monitor CoreDNS, it exports several metrics it exposes, listed here:</p>
<dl>
<dt>coredns build info</dt>
<dd>
<p>Info about CoreDNS itself</p>
</dd>
<dt>dns request count total</dt>
<dd>
<p>Total query count</p>
</dd>
<dt>dns request duration seconds</dt>
<dd>
<p>Duration to process each query</p>
</dd>
<dt>dns request size bytes</dt>
<dd>
<p>The size of the request in bytes</p>
</dd>
<dt>coredns plugin enabled</dt>
<dd>
<p>Indicates whether a plugin is enabled on per server and zone basis</p>
</dd>
</dl>

<p>By combining the <a data-type="indexterm" data-primary="health checks/probes" data-secondary="with CoreDNS" data-secondary-sortas="CoreDNS" id="idm46219937162248"/>pod metrics along with CoreDNS metrics, plugin administrators will ensure that CoreDNS stays healthy
and running inside your cluster.</p>
<div data-type="tip"><h6>Tip</h6>
<p>This is only a brief overview of the metrics available. The entire list is <a href="https://oreil.ly/gm8IO">available</a>.</p>
</div>

<p>Autopath and other metrics are enabled via plugins. This allows CoreDNS to focus on its one task, DNS, but still be
extensible through the plugin framework, much like the CNI <a data-type="indexterm" data-primary="CoreDNS" data-secondary="plugins with" id="ch4_term74"/>pattern. In <a data-type="xref" href="#coredns_plugins">Table 4-5</a>, we see a list of the plugins
currently available. Being an <a data-type="indexterm" data-primary="open source projects" id="idm46219937156056"/>open source project, anyone can contribute a plugin. There are several cloud-specific
ones like router53 that enable serving zone data from AWS route53 service.</p>
<table id="coredns_plugins">
<caption><span class="label">Table 4-5. </span>CoreDNS plugins</caption>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>auto</p></td>
<td><p>Enables serving zone data from an RFC 1035-style master file, which is automatically picked up from disk.</p></td>
</tr>
<tr>
<td><p>autopath</p></td>
<td><p>Allows for server-side search path completion. autopath [ZONE…] RESOLV-CONF.</p></td>
</tr>
<tr>
<td><p>bind</p></td>
<td><p>Overrides the host to which the server should bind.</p></td>
</tr>
<tr>
<td><p>cache</p></td>
<td><p>Enables a frontend cache. cache [TTL] [ZONES…].</p></td>
</tr>
<tr>
<td><p>chaos</p></td>
<td><p>Allows for responding to TXT queries in the CH class.</p></td>
</tr>
<tr>
<td><p>debug</p></td>
<td><p>Disables the automatic recovery upon a crash so that you’ll get a nice stack trace. text2pcap.</p></td>
</tr>
<tr>
<td><p>dnssec</p></td>
<td><p>Enables on-the-fly DNSSEC signing of served data.</p></td>
</tr>
<tr>
<td><p>dnstap</p></td>
<td><p>Enables logging to dnstap. <a href="http://dnstap.info"><em class="hyperlink">http://dnstap.info</em></a> golang: go get -u -v github.com/dnstap/golang-dnstap/dnstap.</p></td>
</tr>
<tr>
<td><p>erratic</p></td>
<td><p>A plugin useful for testing client behavior.</p></td>
</tr>
<tr>
<td><p>errors</p></td>
<td><p>Enables error logging.</p></td>
</tr>
<tr>
<td><p>etcd</p></td>
<td><p>Enables reading zone data from an etcd version 3 instance.</p></td>
</tr>
<tr>
<td><p>federation</p></td>
<td><p>Enables federated queries to be resolved via the kubernetes plugin.</p></td>
</tr>
<tr>
<td><p>file</p></td>
<td><p>Enables serving zone data from an RFC 1035-style master file.</p></td>
</tr>
<tr>
<td><p>forward</p></td>
<td><p>Facilitates proxying DNS messages to upstream resolvers.</p></td>
</tr>
<tr>
<td><p>health</p></td>
<td><p>Enables a health check endpoint.</p></td>
</tr>
<tr>
<td><p>host</p></td>
<td><p>Enables serving zone data from a /etc/hosts style file.</p></td>
</tr>
<tr>
<td><p>kubernetes</p></td>
<td><p>Enables the reading zone data from a Kubernetes cluster.</p></td>
</tr>
<tr>
<td><p>loadbalancer</p></td>
<td><p>Randomizes the order of A, AAAA, and MX records.</p></td>
</tr>
<tr>
<td><p>log enables</p></td>
<td><p>Queries logging to standard output.</p></td>
</tr>
<tr>
<td><p>loop detect</p></td>
<td><p>Simple forwarding loops and halt the server.</p></td>
</tr>
<tr>
<td><p>metadata</p></td>
<td><p>Enables a metadata collector.</p></td>
</tr>
<tr>
<td><p>metrics</p></td>
<td><p>Enables Prometheus metrics.</p></td>
</tr>
<tr>
<td><p>nsid</p></td>
<td><p>Adds an identifier of this server to each reply. RFC 5001.</p></td>
</tr>
<tr>
<td><p>pprof</p></td>
<td><p>Publishes runtime profiling data at endpoints under /debug/pprof.</p></td>
</tr>
<tr>
<td><p>proxy</p></td>
<td><p>Facilitates both a basic reverse proxy and a robust load balancer.</p></td>
</tr>
<tr>
<td><p>reload</p></td>
<td><p>Allows automatic reload of a changed Corefile. Graceful reload.</p></td>
</tr>
<tr>
<td><p>rewrite</p></td>
<td><p>Performs internal message rewriting. rewrite name foo.example.com foo.default.svc.cluster.local.</p></td>
</tr>
<tr>
<td><p>root</p></td>
<td><p>Simply specifies the root of where to find zone files.</p></td>
</tr>
<tr>
<td><p>router53</p></td>
<td><p>Enables serving zone data from AWS route53.</p></td>
</tr>
<tr>
<td><p>secondary</p></td>
<td><p>Enables serving a zone retrieved from a primary server.</p></td>
</tr>
<tr>
<td><p>template</p></td>
<td><p>Dynamic responses based on the incoming query.</p></td>
</tr>
<tr>
<td><p>tls</p></td>
<td><p>Configures the server certificates for TLS and gRPC servers.</p></td>
</tr>
<tr>
<td><p>trace</p></td>
<td><p>Enables OpenTracing-based tracing of DNS requests.</p></td>
</tr>
<tr>
<td><p>whoami</p></td>
<td><p>Returns resolver’s local IP address, port, and transport.</p></td>
</tr>
</tbody>
</table>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A comprehensive list of CoreDNS plugins is <a href="https://oreil.ly/rlXRO">available</a>.</p>
</div>

<p>CoreDNS is <a data-type="indexterm" data-startref="ch4_term74" id="idm46219937046312"/>exceptionally configurable and compatible with the Kubernetes model. We have only scratched the surface of
what CoreDNS is capable of; if you would like to learn more about CoreDNS, we highly <a data-type="indexterm" data-primary="Belamaric, John" id="idm46219937045240"/><a data-type="indexterm" data-primary="Liu, Cricket" id="idm46219937044568"/><a data-type="indexterm" data-primary="Learning CoreDNS (Liu and Belamaric)" id="idm46219937043896"/>recommend reading <a href="https://oreil.ly/O7Xuh"><em>Learning CoreDNS</em></a> by John Belamaric Cricket Liu (O’Reilly).</p>

<p>CoreDNS allows pods to figure out the IP addresses to use to reach applications and servers internal and external to
the cluster. In <a data-type="indexterm" data-startref="ch4_term69" id="idm46219937041640"/><a data-type="indexterm" data-startref="ch4_term70" id="idm46219937040936"/><a data-type="indexterm" data-startref="ch4_term73" id="idm46219937040264"/>our next section, we will discuss more in depth how IPv4 and 6 are managed in a cluster.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="IPv4/IPv6 Dual Stack"><div class="sect1" id="idm46219937459960">
<h1>IPv4/IPv6 Dual Stack</h1>

<p>Kubernetes has <a data-type="indexterm" data-primary="cluster networking" data-secondary="with dual stacks" data-secondary-sortas="dual stacks" id="ch4_term75"/><a data-type="indexterm" data-primary="IPv4/IPv6 dual stack, Kubernetes" id="ch4_term76"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="IPv4/IPv6 dual stack in" id="ch4_term77"/>still-evolving support for running in IPv4/IPv6 “dual-stack” mode, which allows a cluster to use both
IPv4 and IPv6 addresses. Kubernetes has existing stable support for running clusters in IPv6-only mode; however,
running in IPv6-only mode is a barrier to communicating with clients and hosts that support only IPv4. The dual-stack
mode is a critical bridge to allowing IPv6 adoption. We will attempt to describe the current state of dual-stack
networking in Kubernetes as of Kubernetes 1.20, but be aware that it is liable to change substantially in subsequent
releases. The full Kubernetes enhancement proposal (KEP) for dual-stack support is on <a href="https://oreil.ly/T83u5">GitHub</a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>In Kubernetes, a <a data-type="indexterm" data-primary="Kubernetes" data-secondary="alpha features in" id="idm46219937031832"/>feature is “alpha” if the design is not finalized, if the scalability/test coverage/reliability is
insufficient, or if it merely has not proven itself sufficiently in the real world yet. <a data-type="indexterm" data-primary="Kubernetes enhancement proposals (KEPs)" id="idm46219937030488"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="beta resources of" id="idm46219937029800"/><a data-type="indexterm" data-primary="KEPs (Kubernetes enhancement proposals)" id="idm46219937028856"/>Kubernetes Enhancement
Proposals (KEPs) set the bar for an individual feature to graduate to beta and then be stable. Like all alpha features,
Kubernetes disables dual-stack support by default, and the feature must be explicitly enabled.</p>
</div>

<p class="pagebreak-before">IPv4/IPv6 features enable the following features for pod networking:</p>

<ul>
<li>
<p>A single IPv4 and IPv6 address per pod</p>
</li>
<li>
<p>IPv4 and IPv6 services</p>
</li>
<li>
<p>Pod cluster egress routing via IPv4 and IPv6 interfaces</p>
</li>
</ul>

<p>Being an alpha feature, administrators must enable IPv4/IPv6 dual-stack; to do so, the <code>IPv6DualStack</code> feature gate for
the network components must be configured for your cluster. Here is a list of those <a data-type="indexterm" data-primary="kube-apiserver" id="idm46219937022904"/><a data-type="indexterm" data-primary="kube-controller-manager" id="idm46219937022200"/><a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="for dual-stack clusters" data-secondary-sortas="dual-stack clusters" id="idm46219937021528"/><a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="for dual-stack clusters" data-secondary-sortas="dual-stack clusters" id="idm46219937020312"/>dual-stack cluster network options:</p>
<dl>
<dt><code>kube-apiserver</code></dt>
<dd>

<ul>
<li>
<p><code>feature-gates="IPv6DualStack=true"</code></p>
</li>
<li>
<p><code>service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></p>
</li>
</ul>
</dd>
<dt><code>kube-controller-manager</code></dt>
<dd>

<ul>
<li>
<p><code>feature-gates="IPv6DualStack=true"</code></p>
</li>
<li>
<p><code>cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></p>
</li>
<li>
<p><code>service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></p>
</li>
<li>
<p><code>node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code> defaults to /24 for IPv4 and /64 for IPv6</p>
</li>
</ul>
</dd>
<dt><code>kubelet</code></dt>
<dd>

<ul>
<li>
<p><code>feature-gates="IPv6DualStack=true"</code></p>
</li>
</ul>
</dd>
<dt><code>kube-proxy</code></dt>
<dd>

<ul>
<li>
<p><code>cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;</code></p>
</li>
<li>
<p><code>feature-gates="IPv6DualStack=true"</code></p>
</li>
</ul>
</dd>
</dl>

<p>When IPv4/IPv6 is on in a cluster, <a data-type="indexterm" data-primary="ipFamilyPolicy" id="ch4_term78"/>services now have an extra field in which developers can choose the
<code>ipFamilyPolicy</code> to deploy for their application:</p>
<dl>
<dt><code>SingleStack</code></dt>
<dd>
<p>Single-stack service. The control plane allocates a cluster IP for the service, using the first
configured service cluster IP range.</p>
</dd>
<dt><code>PreferDualStack</code></dt>
<dd>
<p>Used only if the cluster has dual stack enabled.  This setting will use the same behavior as <code>SingleStack</code>.</p>
</dd>
<dt><code>RequireDualStack</code></dt>
<dd>
<p>Allocates service cluster IP addresses from both IPv4 and IPv6 address ranges.</p>
</dd>
<dt><code>ipFamilies</code></dt>
<dd>
<p>An array that defines which IP family to use for a single stack or defines the order of IP families for
dual stack; you can choose the address families by setting this field on the service. The allowed values are <code>["IPv4"]</code>, <code>["IPv6"]</code>, and
<code>["IPv4","IPv6"]</code> (dual stack).</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Starting in 1.21, IPv4/IPv6 dual stack defaults to enabled.</p>
</div>

<p>Here is an example service manifest that has  PreferDualStack set to  PreferDualStack:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-service</code>
  <code class="nt">labels</code><code class="p">:</code>
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">MyApp</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">ipFamilyPolicy</code><code class="p">:</code> <code class="l-Scalar-Plain">PreferDualStack</code>
  <code class="nt">selector</code><code class="p">:</code>
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">MyApp</code>
  <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>
      <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm46219937039096">
<h1>Conclusion</h1>

<p>The Kubernetes <a data-type="indexterm" data-startref="ch4_term75" id="idm46219936952920"/><a data-type="indexterm" data-startref="ch4_term76" id="idm46219936952184"/><a data-type="indexterm" data-startref="ch4_term77" id="idm46219936951512"/><a data-type="indexterm" data-startref="ch4_term78" id="idm46219936950840"/>networking model is the basis for how networking is designed to work inside a cluster. The CNI running on the nodes implements the principles set forth in the Kubernetes network model.
The model does not define network security; the
extensibility of Kubernetes allows the CNI to implement network security through network policies.</p>

<p>CNI, DNS, and network security are essential parts of the cluster network; they bridge the gap  between Linux
networking, covered in <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a>,  and container and Kubernetes networking, covered in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.xhtml#container_networking_basics">3</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.xhtml#kubernetes_networking_abstractions">5</a>, respectively.</p>

<p>Choosing the <a data-type="indexterm" data-primary="cluster administrators" data-secondary="CoreDNS and" id="idm46219936945544"/>right CNI requires an evaluation from both the developers’ and administrators’ perspectives. Requirements
need to be laid out and CNIs tested. It is our opinion that a cluster is not complete without a discussion about
network security and CNI that supports it.</p>

<p class="pagebreak-before">DNS is essential; a complete setup and a smooth-running network require network and cluster administrators to be
proficient at scaling CoreDNS in their clusters. An exceptional number of Kubernetes issues stem from DNS and the
misconfiguration of CoreDNS.</p>

<p>The information in this chapter will be important when discussing cloud networking in <a data-type="xref" href="ch06.xhtml#kubernetes_and_cloud_networking">Chapter 6</a> and what options
administrators have when designing and deploying their production cluster networks.</p>

<p>In our next chapter, we will dive into how Kubernetes uses all of this to power its abstractions.</p>
</div></section>







</div></section></div></body></html>