<html><head></head><body><section data-pdf-bookmark="Chapter 8. Working Example of Multicluster Application Delivery" data-type="chapter" epub:type="chapter"><div class="chapter" id="working_example_of_multicluster_applicat">&#13;
<h1><span class="label">Chapter 8. </span>Working Example of Multicluster Application Delivery</h1>&#13;
<p>Let’s take a look at a simple working example of a web application with a <a contenteditable="false" data-primary="MongoDB backing web application" data-type="indexterm" id="idm45358189406184"/><a contenteditable="false" data-primary="web application with backing datastore" data-secondary="about" data-type="indexterm" id="idm45358189404920"/><a contenteditable="false" data-primary="PAC-MAN via OpenShift Pipelines" data-secondary="multicluster application delivery" data-type="indexterm" id="ch08-mad"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="about" data-type="indexterm" id="idm45358189401992"/><a contenteditable="false" data-primary="applications" data-secondary="application delivery example" data-tertiary="about" data-type="indexterm" id="idm45358189400648"/><a contenteditable="false" data-primary="deployments" data-secondary="application deployments" data-see="multicluster application delivery" data-tertiary="multicluster" data-type="indexterm" id="idm45358189398984"/><a contenteditable="false" data-primary="multicluster management" data-secondary="application delivery" data-see="multicluster application delivery" data-type="indexterm" id="idm45358189397048"/><a contenteditable="false" data-primary="working example of application delivery" data-see="multicluster application delivery" data-type="indexterm" id="idm45358189395384"/>backing datastore. For our purposes, we will deploy an app that mimics the game PAC-MAN game from Atari. A user will interact with a dynamic frontend that stores information in a backing MongoDB.</p>&#13;
<p>We will deploy this application across multiple distinct clusters using the techniques discussed in <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>. Each of the clusters will be provisioned from a hub running Open Cluster Management, as discussed in <a data-type="xref" href="ch06.html#multicluster_fleets_provision_and_upgrad">Chapter 6</a>. Further, we will configure an external load balancer provided by a GSLB service hosted by F5. Incoming user requests will be routed from a global domain into one of the specific clusters. If any one of the clusters or the application experiences a problem, then user requests will no longer be routed to that cluster. We are going to go a little further and demonstrate how to integrate off-cluster resources like an F5 DNS Load Balancer Cloud Service, and we will integrate a ServiceNow change ticket into the example.</p>&#13;
<p>In <a data-type="xref" href="#a_working_example_made_up_of_multiple_cl">Figure 8-1</a>, we see our PAC-MAN application running on two OpenShift clusters with a load balancer routing traffic to app instances on either cluster. We can see the hub cluster with various resources that are helping to manage the overall system. Through the rest of this chapter, we will explain what these various parts are doing and provide a walk-through that you can do on your own to experiment with all of the moving parts.</p>&#13;
<figure><div class="figure" id="a_working_example_made_up_of_multiple_cl">&#13;
<img src="assets/hcok_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>A working example made up of multiple clusters and a two-tier web application, managed by an Open Cluster Management hub and integrating off-cluster <span class="keep-together">automation</span></h6>&#13;
</div></figure>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Failure Is Inevitable" data-type="sect1"><div class="sect1" id="failure_is_inevitable">&#13;
<h1 class="less_space">Failure Is Inevitable</h1>&#13;
<p>We have seen from many angles how OpenShift recovers from failures <a contenteditable="false" data-primary="multicluster application delivery" data-secondary="failure handling" data-type="indexterm" id="idm45358189383304"/><a contenteditable="false" data-primary="web application with backing datastore" data-secondary="failure handling" data-type="indexterm" id="idm45358189381832"/><a contenteditable="false" data-primary="applications" data-secondary="application delivery example" data-tertiary="failure handling" data-type="indexterm" id="idm45358189380440"/><a contenteditable="false" data-primary="failure" data-secondary="OpenShift failure handling" data-type="indexterm" id="idm45358189378776"/><a contenteditable="false" data-primary="failure" data-secondary="multicluster availability zones" data-type="indexterm" id="idm45358189377384"/><a contenteditable="false" data-primary="multicluster management" data-secondary="failure handling in OpenShift" data-type="indexterm" id="idm45358189375992"/><a contenteditable="false" data-primary="availability zones" data-secondary="failure in multicluster architecture" data-type="indexterm" id="idm45358189374600"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="failure handling in OpenShift" data-type="indexterm" id="idm45358189373208"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="failure handling in OpenShift" data-type="indexterm" id="idm45358189371544"/>of the underlying infrastructure or cloud provider and how you can achieve a responsive and adaptive open hybrid cloud to support your applications. Let’s review some of the things we’ve talked about.</p>&#13;
<p>In <a data-type="xref" href="ch04.html#single_cluster_availability">Chapter 4</a>, we discussed single cluster availability<a contenteditable="false" data-primary="Kubernetes" data-secondary="availability" data-tertiary="failure handling" data-type="indexterm" id="idm45358189368264"/> where the control plane of a Kubernetes cluster is able to tolerate the failure of any one availability zone. When an availability zone fails in a single cluster, both the control plane and applications running within the cluster can tolerate loss of supporting compute, network, or storage infrastructure. Your applications must take advantage of Kubernetes concepts like services that act as a load balancer within the cluster, routing user requests to multiple supporting application pods. <a contenteditable="false" data-primary="application pod failure" data-secondary="failure handling" data-type="indexterm" id="idm45358189366008"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="application pod failure" data-type="indexterm" id="idm45358189364632"/>When an application pod is no longer healthy (as determined by health checks) or the node supporting the pod is no longer healthy, then the Kubernetes scheduler will look for a new home for the failing pod. We design applications for resiliency by having redundant pods that continue to run and service incoming user requests.</p>&#13;
<p>Now what happens if the cluster cannot reschedule the failing pod because it has exhausted capacity? What happens if more than a single availability zone fails? Under these scenarios, it may be preferable to deploy additional instances of the application to other clusters in alternative regions or even alternative cloud providers.</p>&#13;
<p>As we saw in <a data-type="xref" href="ch06.html#multicluster_fleets_provision_and_upgrad">Chapter 6</a>, we can use OpenShift to provision running clusters across many different cloud providers. The <a href="https://oreil.ly/3J1SW">Open Cluster Management</a> project allows you to manage those clusters from a central control plane, referred to as the “hub” cluster.<a contenteditable="false" data-primary="hub cluster" data-type="indexterm" id="idm45358189359352"/><a contenteditable="false" data-primary="control planes" data-secondary="hub cluster running" data-type="indexterm" id="idm45358189358248"/><a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="hub cluster" data-type="indexterm" id="idm45358189356872"/><a contenteditable="false" data-primary="web application with backing datastore" data-secondary="failure handling" data-tertiary="hub cluster" data-type="indexterm" id="idm45358189355432"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="failure handling" data-tertiary="hub cluster" data-type="indexterm" id="idm45358189353768"/></p>&#13;
<p>Further, in <a data-type="xref" href="ch07.html#multicluster_policy_configuration">Chapter 7</a>, <a contenteditable="false" data-primary="PlacementRules" data-secondary="declarative policies" data-type="indexterm" id="idm45358189350488"/><a contenteditable="false" data-primary="PlacementRules" data-secondary="cluster matched to required configurations" data-type="indexterm" id="idm45358189349080"/><a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="PlacementRules for cluster matching" data-type="indexterm" id="idm45358189347608"/><a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="declarative management" data-type="indexterm" id="idm45358189345912"/>we saw how a given cluster was matched to required configurations through a <code>PlacementRule</code>. Whenever an Open Cluster Management policy was matched against a cluster because the cluster was selected by a <code>PlacementRule</code>, required configuration could be audited or enforced against that cluster. Because OpenShift is very operator-centric both for the control plane and for workloads, using policies to drive declarative configuration is a natural and simple way to ensure that your clusters are ready to support application workloads. An added benefit is that the declarative policies are easily managed in your source control systems if you’re adopting a full or semi-GitOps approach.</p>&#13;
<p>In <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>, <a contenteditable="false" data-primary="PlacementRules" data-type="indexterm" id="idm45358189341224"/><a contenteditable="false" data-primary="multicloud management" data-secondary="PlacementRules for failure tolerance" data-type="indexterm" id="idm45358189340120"/><a contenteditable="false" data-primary="PlacementRules" data-secondary="failure tolerance" data-type="indexterm" id="idm45358189338680"/>we examined how an appropriate <code>PlacementRule</code> can ensure that an application is running across multiple clusters. By running an application across more than one cluster in separate regions, we can now tolerate the failure of an entire cloud region. Alternatively, you can use this when your organization happens to have adopted different cloud providers just due to organizational inertia, because of a merger and acquisition, or because you need to survive the complete outage of a cloud provider. The simplest example of multicluster, though, likely is that you are leveraging your existing datacenter virtualization provider and adopting at least one public cloud provider as well. <code>PlacementRule</code>s help separate the “what” needs to run from the “where” very easily. <code>PlacementRule</code>s also enable your application to adjust if the availability or labeling of clusters changes. That is, if either the <code>PlacementRule</code> is changed or the set of available clusters changes due to additions or removals, then the application components will be dynamically redeployed across new clusters or taken off of failing clusters.</p>&#13;
<p>So, we can create clusters easily (<a data-type="xref" href="ch06.html#multicluster_fleets_provision_and_upgrad">Chapter 6</a>), we can ensure those clusters are configured correctly to support the enterprise configuration and security standards (<a data-type="xref" href="ch07.html#multicluster_policy_configuration">Chapter 7</a>), and we can deliver applications to those clusters (<a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>).</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Multicluster Load Balancing" data-type="sect1"><div class="sect1" id="multicluster_load_balancing">&#13;
<h1>Multicluster Load Balancing</h1>&#13;
<p>How do we ensure that user requests are routed to healthy application <a contenteditable="false" data-primary="multicluster application delivery" data-secondary="load balancing" data-tertiary="DNS Load Balancer Cloud Service" data-type="indexterm" id="ch08-lobaB"/><a contenteditable="false" data-primary="DNS Load Balancer Cloud Service (F5)" data-type="indexterm" id="ch08-lobaC"/><a contenteditable="false" data-primary="F5" data-secondary="DNS Load Balancer Cloud Service" data-type="indexterm" id="ch08-loba"/><a contenteditable="false" data-primary="load balancing" data-secondary="multicluster application delivery" data-type="indexterm" id="ch08-loba8"/><a contenteditable="false" data-primary="web application with backing datastore" data-secondary="multicluster load balancing" data-type="indexterm" id="idm45358189321928"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="load balancing" data-type="indexterm" id="ch08-loba3"/><a contenteditable="false" data-primary="multicluster management" data-secondary="load balancing" data-type="indexterm" id="ch08-loba5"/><a contenteditable="false" data-primary="applications" data-secondary="application delivery example" data-tertiary="load balancing" data-type="indexterm" id="ch08-loba7"/><a contenteditable="false" data-primary="load balancing" data-secondary="multicluster load balancing" data-type="indexterm" id="ch08-loba2"/><a contenteditable="false" data-primary="Global Server Load Balancer (GSLB)" data-type="indexterm" id="ch08-loba6"/><a contenteditable="false" data-primary="GSLB (Global Server Load Balancer)" data-type="indexterm" id="ch08-lobaD"/><a contenteditable="false" data-primary="load balancing" data-secondary="multicluster load balancing" data-tertiary="Global Server Load Balancer" data-type="indexterm" id="ch08-loba9"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="load balancing" data-tertiary="Global Server Load Balancer" data-type="indexterm" id="ch08-loba4"/>instances? For our example, we will use a cloud-based GSLB service managed by F5.</p>&#13;
<p>There are a number of solutions to handle multicluster load balancing. We’ve chosen to use an cloud-based service from F5 for these reasons:</p>&#13;
<ul>&#13;
<li><p>It’s not supported by an operator at the time of this writing (and yet is still important to automate with our application).</p></li>&#13;
<li><p>It’s easy to sign up for a lightweight account through the AWS Marketplace.</p></li>&#13;
<li><p>We expect most datacenters will need to automate how their applications<a contenteditable="false" data-primary="F5" data-secondary="BIG-IP load balancer" data-type="indexterm" id="idm45358189303928"/><a contenteditable="false" data-primary="load balancing" data-secondary="multicluster load balancing" data-tertiary="BIG-IP load balancer" data-type="indexterm" id="idm45358189302376"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="load balancing" data-tertiary="BIG-IP load balancer" data-type="indexterm" id="idm45358189300760"/><a contenteditable="false" data-primary="BIG-IP load balancer (F5)" data-type="indexterm" id="idm45358189299096"/> integrate with an F5 BIG-IP load balancer. Even though the cloud-based service and the F5 BIG-IP appliances use different APIs, we believe the example will give you enough of an understanding that you can adapt the principles that are demonstrated to your own applications. Also, if we used a BIG-IP virtual appliance for the example, it would increase the complexity for users who want to re-create the complete example.</p></li>&#13;
<li><p>The F5 service will work for you even if your clusters run across completely different cloud environments or your own datacenters with public routes exposed.</p></li>&#13;
</ul>&#13;
<p>For your own awareness and reference, here are two other options to provide load balancing across clusters:</p>&#13;
<ul>&#13;
<li><p>The <a href="https://oreil.ly/eyHBj">external-dns</a> project<a contenteditable="false" data-primary="load balancing" data-secondary="multicluster load balancing" data-tertiary="external-dns project" data-type="indexterm" id="idm45358189294440"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="load balancing" data-tertiary="external-dns project" data-type="indexterm" id="idm45358189292744"/><a contenteditable="false" data-primary="external-dns project load balancer" data-type="indexterm" id="idm45358189291080"/> extends the typical internal cluster DNS registry for services into a public DNS record. The project is still somewhat young as of the time of this writing but has a number of supported providers.</p></li>&#13;
<li class="pagebreak-before less_space"><p>The <a href="https://www.k8gb.io">k8gb.io</a> project will set up DNS<a contenteditable="false" data-primary="k8gb.io project load balancer" data-type="indexterm" id="idm45358189288072"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="load balancing" data-tertiary="k8gb.io project" data-type="indexterm" id="idm45358189286904"/><a contenteditable="false" data-primary="load balancing" data-secondary="multicluster load balancing" data-tertiary="k8gb.io project" data-type="indexterm" id="idm45358189285240"/> registries within each cluster. An external DNS entry must be configured in some top-level DNS provider that delegates to the cluster-specific DNS registries. The architecture has the benefit that there is no centralized controller that can be a single point of failure. However, features like latency-based routing are not currently supported (as in the F5 or external-dns options).</p></li>&#13;
</ul>&#13;
<p>In our example application, you will establish a top-level DNS record (e.g., <code>*.www-apps.&lt;<em>clusterName</em>&gt;.&lt;<em>baseDomain</em>&gt;</code>) that delegates to the F5 DNS Load Balancer to resolve the address (e.g., ns1.f5cloudservices.com and ns2.f5cloudservices.com). <a contenteditable="false" data-primary="DNS request flow" data-type="indexterm" id="idm45358189281080"/>Then the general flow of DNS requests to resolve the address will go through the following steps:</p>&#13;
<ol>&#13;
<li><p>User resolves <code>app-name.www-apps.&lt;<em>clusterName</em>&gt;.&lt;<em>baseDomain</em>&gt;</code> against the top-level DNS provider, which redirects the request to the relevant cloud DNS provider for <code>*.&lt;<em>baseDomain</em>&gt;</code>.</p></li>&#13;
<li><p>The <code>&lt;<em>baseDomain</em>&gt;</code> is resolved against your cloud provider’s DNS (e.g., Amazon Route 53).</p></li>&#13;
<li><p>The cloud DNS provider returns a nameserver record for <code>*.www-apps.&lt;<em>clusterName</em>&gt;.&lt;<em>baseDomain</em>&gt;</code> to route the request to one of  ns1.f5cloudservices.com or ns2.f5cloudservices.com.</p></li>&#13;
<li><p>The final resolution request to ns1.f5cloudservices.com or ns2.f5cloudservices.com evaluates the list of managed zones and, based on where the DNS request originated from and current health of backing services, returns the best match to an application route hosted by one of your clusters.</p></li>&#13;
</ol>&#13;
<p>As shown in <a data-type="xref" href="#the_ffive_dns_load_balancer_cloud_servic">Figure 8-2</a>, in the F5 DNS Load Balancer Cloud Service, you will have one DNS load balancer zone for each collection of clusters that may host the application. Each of these zones will correspond to a top-level DNS entry in your cloud provider DNS that delegates to F5 to resolve the correct cluster for a requested application route.</p>&#13;
<figure><div class="figure" id="the_ffive_dns_load_balancer_cloud_servic">&#13;
<img src="assets/hcok_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>The F5 DNS Load Balancer Cloud Service</h6>&#13;
</div></figure>&#13;
<p>When logged into the F5 DNS Load Balancer, you will see your various DNS load balancer services (each of these routes to one or more applications provided across multiple clusters) and the IP endpoints that are currently registered. Each IP endpoint resolves to the application router of one of the clusters hosting one of the application instances.</p>&#13;
<p>Now each time that your application has an instance that becomes available or is removed from a cluster, we need to update the DNS load balancer zone for that application (identified by <code>*.www-apps.&lt;<em>clusterName</em>&gt;.&lt;<em>baseDomain</em>&gt;</code>). <a contenteditable="false" data-primary="F5" data-secondary="DNS Load Balancer Cloud Service" data-tertiary="Ansible automation" data-type="indexterm" id="ch08-ans"/><a contenteditable="false" data-primary="Ansible for automation" data-type="indexterm" id="ch08-ans2"/><a contenteditable="false" data-primary="DNS Load Balancer Cloud Service (F5)" data-secondary="Ansible automation" data-type="indexterm" id="ch08-ans3"/><a contenteditable="false" data-primary="web application with backing datastore" data-secondary="multicluster load balancing" data-tertiary="Ansible automation" data-type="indexterm" id="ch08-ans4"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="load balancing" data-tertiary="Ansible automation" data-type="indexterm" id="ch08-ans5"/><a contenteditable="false" data-primary="applications" data-secondary="application delivery example" data-tertiary="Ansible automation of load balancing" data-type="indexterm" id="ch08-ans6"/>How are we going to do this automatically? Here, we’re going to introduce how you can automate around your clusters whenever something in your environment is not “native” Kubernetes. To accomplish this, we’re going to walk through how Ansible can automate an update to the F5 DNS Load Balancer whenever our application is introduced to a new cluster or removed from an existing cluster.</p>&#13;
<p class="pagebreak-before less_space">You will need to carry out the following prerequisites for running our example <span class="keep-together">application</span>:</p>&#13;
<ol>&#13;
<li><p>Create an account with the F5 DNS Load Balancer Cloud Service. You can do this at <a href="https://oreil.ly/tmmSY">F5 Cloud Services</a> or through the <a href="https://oreil.ly/HF5mw">AWS Marketplace</a>.</p></li>&#13;
<li><p>Delegate your global domain to the F5 DNS nameservers. Create a nameserver-delegating DNS record with the global domain that you will use with F5. You can do this via Route 53 or your DNS provider. The <a href="https://oreil.ly/xK0sK">F5 DNS Load Balancer Cloud Service FAQ</a> answers questions related to this <span class="keep-together">prerequisite</span>.<a contenteditable="false" data-primary="" data-startref="ch08-loba" data-type="indexterm" id="idm45358189247832"/><a contenteditable="false" data-primary="" data-startref="ch08-loba2" data-type="indexterm" id="idm45358189246424"/><a contenteditable="false" data-primary="" data-startref="ch08-loba3" data-type="indexterm" id="idm45358189245048"/><a contenteditable="false" data-primary="" data-startref="ch08-loba4" data-type="indexterm" id="idm45358189243672"/><a contenteditable="false" data-primary="" data-startref="ch08-loba5" data-type="indexterm" id="idm45358189242296"/><a contenteditable="false" data-primary="" data-startref="ch08-loba6" data-type="indexterm" id="idm45358189240920"/><a contenteditable="false" data-primary="" data-startref="ch08-loba7" data-type="indexterm" id="idm45358189239544"/><a contenteditable="false" data-primary="" data-startref="ch08-loba8" data-type="indexterm" id="idm45358189238168"/><a contenteditable="false" data-primary="" data-startref="ch08-loba9" data-type="indexterm" id="idm45358189236792"/><a contenteditable="false" data-primary="" data-startref="ch08-lobaB" data-type="indexterm" id="idm45358189235416"/><a contenteditable="false" data-primary="" data-startref="ch08-lobaC" data-type="indexterm" id="idm45358189234040"/><a contenteditable="false" data-primary="" data-startref="ch08-lobaD" data-type="indexterm" id="idm45358189232664"/></p></li>&#13;
</ol>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Automating Without Operators" data-type="sect1"><div class="sect1" id="automating_without_operators">&#13;
<h1>Automating Without Operators</h1>&#13;
<p>As we’ve covered extensively in this book, Kubernetes makes <a contenteditable="false" data-primary="operators" data-secondary="automating without" data-type="indexterm" id="idm45358189229416"/><a contenteditable="false" data-primary="automations" data-secondary="Ansible for automation" data-type="indexterm" id="ch08-ansaut"/><a contenteditable="false" data-primary="multicluster management" data-secondary="Ansible for automation" data-type="indexterm" id="ch08-ansaut2"/><a contenteditable="false" data-primary="multicloud management" data-secondary="Ansible for automation" data-type="indexterm" id="ch08-ansaut3"/><a contenteditable="false" data-primary="load balancing" data-secondary="Ansible for automating" data-type="indexterm" id="ch08-ansaut4"/>comprehensive use of declarative configuration. While we believe that Kubernetes will underpin most modern applications in the better part of the next decade, we recognize that not all things are Kubernetes native today and may <em>never</em> be Kubernetes native.</p>&#13;
<p>For those aspects of your application across multiple clusters or even multiple clouds, we introduce Ansible as a method to automate any behavior that you want to occur when the system introduces a change dynamically. As discussed in the previous section, we want to have our OpenShift environment automatically place our application instances across multiple clusters. Whenever our application is deployed on a new cluster or has to be removed because a cluster is unhealthy, we want our global load balancer configuration in front of the system to be updated. Since we are relying heavily on automated recovery from failures, we want this behavior to be automatic as well.</p>&#13;
<p>With Ansible, there is a vibrant community that uses automation to simplify the lives of systems administrators managing Linux, containers, clouds, networking devices, security, and so forth. We aren’t going to go into a lot of depth to teach you about Ansible. However, we will introduce some basic concepts so that you can see how it works and evaluate whether it is appropriate for your needs.</p>&#13;
<p>Note that even if you do not have to automate anything outside of the cluster, all of the details covered around availability, multicluster provisioning, configuration, application delivery, and so on still apply.</p>&#13;
<p>For our purposes, you should only need a grasp of the following concepts in Ansible:</p>&#13;
<dl class="pagebreak-before less_space">&#13;
<dt>Playbook</dt>&#13;
<dd>An Ansible playbook is “a blueprint of automation tasks—which are complex<a contenteditable="false" data-primary="Ansible for automation" data-secondary="playbooks" data-type="indexterm" id="idm45358189215704"/><a contenteditable="false" data-primary="tasks" data-secondary="Ansible playbooks" data-type="indexterm" id="idm45358189214328"/> IT actions executed with limited or no human involvement. Ansible playbooks are executed on a set, group, or classification of hosts, which together make up an Ansible inventory.”<sup><a data-type="noteref" href="ch08.html#ch01fn38" id="ch01fn38-marker">1</a></sup></dd>&#13;
<dt>Project</dt>&#13;
<dd>An Ansible project groups together playbooks along with supporting resources<a contenteditable="false" data-primary="Ansible for automation" data-secondary="projects" data-type="indexterm" id="idm45358189209656"/> to run those playbooks. Conveniently, projects can be backed by a source control repository that is used to manage the Ansible playbooks and supporting <span class="keep-together">resources</span>.</dd>&#13;
<dt>Ansible Tower</dt>&#13;
<dd>Ansible Tower is a supported version of the open source project Ansible AWX.<a contenteditable="false" data-primary="Ansible for automation" data-secondary="Ansible AWX open source project" data-type="indexterm" id="idm45358189206392"/><a contenteditable="false" data-primary="Ansible for automation" data-secondary="Ansible Tower" data-type="indexterm" id="idm45358189205048"/> Ansible Tower provides the capabilities to organize a set of Ansible projects and an automation engine that keeps track of credentials, scheduled jobs, job templates to be invoked as needed, available inventory of systems to run playbooks against, and so on. In essence, think of Tower as a way to organize and track everything you need to automate systems management.</dd>&#13;
<dt>Job template</dt>&#13;
<dd>An Ansible job template defines an available playbook from an Ansible project<a contenteditable="false" data-primary="templates" data-secondary="Ansible job templates" data-type="indexterm" id="idm45358189202296"/><a contenteditable="false" data-primary="Ansible for automation" data-secondary="job templates" data-type="indexterm" id="idm45358189200824"/> within Ansible Tower. Job templates can specify exactly which parameters to externalize and which stored credentials to use, and it can associate a list of all invocations of the job template for auditing or diagnostic purposes.</dd>&#13;
<dt>Job</dt>&#13;
<dd>An Ansible job is a running instance of a job template.<a contenteditable="false" data-primary="Ansible for automation" data-secondary="jobs" data-type="indexterm" id="idm45358189198152"/></dd>&#13;
</dl>&#13;
<p>As discussed in the previous section, we are going to use Ansible to update the F5 DNS Load Balancer Cloud Service for our application. We are going to use a slightly modified version of <a href="https://oreil.ly/p2AyH">an open source tool</a> from F5 that incorporates all of the API calls required to update the service implemented in Ansible. We will load the f5-bd-gslb-tool playbooks into an Ansible project and define a job template that invokes the required playbook, accepting parameters for the clusters that currently host the application. For an example of what the Ansible project within Ansible Tower looks like, see <a data-type="xref" href="#an_ansible_project_within_ansible_towers">Figure 8-3</a>.</p>&#13;
<figure class="width-90"><div class="figure" id="an_ansible_project_within_ansible_towers">&#13;
<img src="assets/hcok_0803.png"/>&#13;
<h6><span class="label">Figure 8-3. </span>An Ansible project within Ansible Tower; the contents of the project are managed under source control (via <a class="orm:hideurl" href="https://github.com">GitHub</a>)</h6>&#13;
</div></figure>&#13;
<p>We will also use Ansible to integrate a simple change management process backed by ServiceNow. Our example is derived from published blogs on the topic.<sup><a data-type="noteref" href="ch08.html#ch01fn39" id="ch01fn39-marker">2</a></sup> Many IT organizations and operators still make extensive use of ticket-driven change management processes. In our example application, an Ansible job will run before adjusting the placement of the app to create a change request in ServiceNow. Our example will make only superficial use of ServiceNow, but you will see one way that you may still use your existing process for record-keeping and auditing purposes even when the system undergoes a dynamic, automatic change.</p>&#13;
<p>We have covered several concepts that support application deployment, including the Open Cluster Management Subscription (API Group: apps.open-cluster-management.io/v1) and <code>PlacementRule</code> (API Group: apps.open-cluster-management.io/v1). <a contenteditable="false" data-primary="Ansible for automation" data-secondary="AnsibleJob API object" data-type="indexterm" id="idm45358189186280"/>We now introduce a new API kind that will be part of our example application: <code>AnsibleJob</code> (API Group: tower.ansible.com/v1alpha1). As with other CRDs introduced throughout the book, <code>AnsibleJob</code> is reconciled using an operator known as the Ansible Resource Operator. Let’s deploy the Ansible Resource Operator alongside Ansible Tower so that we can link the execution of Ansible jobs whenever the placement of our application instances is updated by the system. The <code>AnsibleJob</code> to create a change request ticket is shown in <a data-type="xref" href="#the_ansiblejob_api_kind_allows_us_to_inv">Example 8-1</a>.</p>&#13;
<div data-type="example" id="the_ansiblejob_api_kind_allows_us_to_inv">&#13;
<h5><span class="label">Example 8-1. </span>The <code>AnsibleJob</code> API kind allows us to invoke an Ansible job template configured in Ansible Tower; input parameters are sent via the <code>extra_vars</code> parameter</h5>&#13;
</div>&#13;
<pre data-type="programlisting">apiVersion: tower.ansible.com/v1alpha1&#13;
kind: AnsibleJob&#13;
metadata:&#13;
 name: snow-create-change-record&#13;
 namespace: pacman-app&#13;
 labels:&#13;
   app.kubernetes.io/name: pacman&#13;
spec:&#13;
 tower_auth_secret: toweraccess&#13;
 job_template_name: snow-create-change-record&#13;
 extra_vars:&#13;
   app_name: pacman&#13;
   change_request:&#13;
     severity: 2&#13;
     priority: 2&#13;
     implementation_plan: "Updated by Red Hat Advanced Cluster Management for &#13;
Kubernetes"&#13;
     justification: "A new revision was available from the application channel in &#13;
GitHub."&#13;
     description: "The following resources have been updated: [...]"&#13;
     risk_impact_analysis: "Changes are made automatically based on approved &#13;
changes in GitHub."&#13;
     test_plan: "Run synthetic validation tests post-deployment."</pre>&#13;
<p>In terms of order, the <code>AnsibleJob</code> to create a ServiceNow ticket will run before the new placement decisions are applied (a prehook), and the <code>AnsibleJob</code> to update the F5 load balancer will run last (a posthook).<a contenteditable="false" data-primary="" data-startref="ch08-ans" data-type="indexterm" id="idm45358189176520"/><a contenteditable="false" data-primary="" data-startref="ch08-ans2" data-type="indexterm" id="idm45358189175064"/><a contenteditable="false" data-primary="" data-startref="ch08-ans3" data-type="indexterm" id="idm45358189173688"/><a contenteditable="false" data-primary="" data-startref="ch08-ans4" data-type="indexterm" id="idm45358189172312"/><a contenteditable="false" data-primary="" data-startref="ch08-ans5" data-type="indexterm" id="idm45358189171096"/><a contenteditable="false" data-primary="" data-startref="ch08-ans6" data-type="indexterm" id="idm45358189169880"/><a contenteditable="false" data-primary="" data-startref="ch08-ansaut" data-type="indexterm" id="idm45358189168504"/><a contenteditable="false" data-primary="" data-startref="ch08-ansaut2" data-type="indexterm" id="idm45358189167128"/><a contenteditable="false" data-primary="" data-startref="ch08-ansaut3" data-type="indexterm" id="idm45358189165752"/><a contenteditable="false" data-primary="" data-startref="ch08-ansaut4" data-type="indexterm" id="idm45358189164376"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deploying the Example Application" data-type="sect1"><div class="sect1" id="deploying_the_example_application">&#13;
<h1>Deploying the Example Application</h1>&#13;
<p>We’re laying down a lot of capabilities to get to a simple web frontend <a contenteditable="false" data-primary="web application with backing datastore" data-secondary="deploying example application" data-type="indexterm" id="idm45358189161288"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="deploying example application" data-type="indexterm" id="idm45358189159784"/><a contenteditable="false" data-primary="applications" data-secondary="application delivery example" data-tertiary="deploying example application" data-type="indexterm" id="ch08-appapp"/>application—and the goal is to provide you with a realistic example of some of the concerns that you will likely encounter as you adopt Kubernetes and OpenShift as part of your enterprise standards.</p>&#13;
<p>We’ve covered how the application will be replicated across multiple clusters with a global load balancer in front of those multiple application instances and how whenever the placement decisions change, we will have a simple ticket-based record within ServiceNow of the change.</p>&#13;
<p>We  reviewed the PAC-MAN application extensively in <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>, but here is a quick catchup:</p>&#13;
<ul>&#13;
<li><p>PAC-MAN is made up of two deployments (the web frontend application and a backing MongoDB datastore).<a contenteditable="false" data-primary="MongoDB backing web application" data-type="indexterm" id="idm45358189152776"/></p></li>&#13;
<li><p>PAC-MAN exposes two public routes: one for the local cluster and one for the global route provided by our F5 DNS Load Balancer.</p></li>&#13;
<li><p>The PAC-MAN subscription references a Git repository where our Kubernetes manifests are managed. If a change is introduced to those Kubernetes manifests, then the update will automatically be rolled out to all active clusters.</p></li>&#13;
<li><p>The <code>PlacementRule</code> defines a list of conditions that a cluster must match to be selected to host the application.</p></li>&#13;
</ul>&#13;
<p>For our purposes (and because we’re already dealing with several moving parts), we are not doing anything to cluster the set of MongoDB replicas that will back the application across multiple clusters. We could perform additional actions (via Ansible or operators) to cluster the set of MongoDB replicas and make distributed writes persistent whenever a cluster is lost. Alternatively, we could back the state of the PAC-MAN application with a cloud-based MongoDB service. You will likely have your own strong opinions about how persistent the backing datastores of your applications need to be and what the relevant MTBF and MTTR availability metrics should be for the application itself.</p>&#13;
<p>Now that we’ve covered the details of how all of the parts fit together, let’s dive in and get our hands on a running example!</p>&#13;
<p>To simplify this process, fork the example repository. You will need to make a few modifications to be able to deliver the application out of your repository fork.</p> &#13;
<p>Fork the <a href="https://oreil.ly/8nMgb">repository</a> into your own organization and then clone the repository to your laptop:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>$ git clone --branch ansible</strong> &#13;
<strong>git@github.com:SPECIFY_YOUR_GITHUB_ORG/k8s-pacman-app.git</strong>&#13;
</pre>&#13;
<p>All of the paths referenced in the following sections refer to files within this <span class="keep-together">repository</span>.</p>&#13;
<p>You will need to update the values in the following files according to your specific DNS settings for your OpenShift clusters and Route53:</p>&#13;
<ul>&#13;
  <li><em>hack/install-config.yaml</em></li>&#13;
  <li><em>deploy/posthook/f5-update-dns-load-balancer.yaml</em></li>&#13;
  <li><em>deploy/pacman-f5-route.yaml</em></li>&#13;
  <li><em>hack/tower-setup/config/inventory</em></li>&#13;
</ul>&#13;
<p><a data-type="xref" href="#dns_settings_update">Table 8-1</a> shows the required updates.</p>&#13;
<table class="border" id="dns_settings_update">&#13;
<caption><span class="label">Table 8-1. </span>Required file updates</caption>&#13;
<thead>&#13;
 <tr>&#13;
  <th>Key</th>&#13;
  <th>Value</th>&#13;
 </tr>&#13;
</thead>&#13;
<tbody>&#13;
  <tr>&#13;
    <td>SPECIFY_YOUR_CLUSTER_NAME</td>&#13;
    <td>The name of your hub cluster as defined in the <em>install-config.yaml</em>.</td>&#13;
  </tr>&#13;
  <tr>&#13;
    <td>SPECIFY_YOUR_BASE_DOMAIN</td>&#13;
    <td>The value of the base DNS name of your hub cluster as defined in your <em>install-config.yaml</em>.</td>&#13;
  </tr>&#13;
  <tr>&#13;
    <td>SPECIFY_YOUR_CLUSTER_ADDRESS</td>&#13;
    <td>The concatenation of <em>clusterName</em> and <em>clusterBaseDomain</em> separated by a period (e.g., <em>clusterName.clusterBaseDomain</em>).</td>&#13;
  </tr>&#13;
  <tr>&#13;
    <td>SPECIFY_YOUR_OWN_PASSWORD</td>&#13;
    <td>Define your own secure password. When in doubt, the output of <code><strong>uuid</strong></code> can be a useful password.&#13;
</td>&#13;
  </tr>&#13;
  <tr>&#13;
    <td>SPECIFY_YOUR_EMAIL_ADDRESS</td>&#13;
    <td>Used to assign a tag for any cloud resources for tracking purposes. You can also just delete the tag if you do not wish to specify your email address.</td>&#13;
  </tr>&#13;
  <tr>&#13;
    <td>SPECIFY_YOUR_PULL_SECRET</td>&#13;
    <td>An image pull secret that you can download from <a href="https://cloud.redhat.com">Red Hat</a> after logging into the service.</td>&#13;
  </tr>&#13;
  <tr>&#13;
    <td>SPECIFY_YOUR_SSH_RSA_PUBLIC_KEY</td>&#13;
    <td>An SSH public key that will enable you to connect to hosts that are provisioned as part of your OpenShift cluster.</td>&#13;
  </tr>&#13;
 </tbody>&#13;
</table>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Configure Your Hub Cluster" data-type="sect1"><div class="sect1" id="configure_your_hub_cluster">&#13;
<h1>Configure Your Hub Cluster</h1>&#13;
<p>Recall that the hub cluster hosts components for Open Cluster <a contenteditable="false" data-primary="web application with backing datastore" data-secondary="deploying example application" data-tertiary="configuring hub cluster" data-type="indexterm" id="idm45358189118712"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="deploying example application" data-tertiary="configuring hub cluster" data-type="indexterm" id="idm45358189116904"/><a contenteditable="false" data-primary="hub cluster" data-secondary="configuring" data-type="indexterm" id="idm45358189115224"/>Management that allow managed clusters to be managed from a centralized control plane. You can either provision clusters from the hub or import existing OpenShift clusters from a managed provider like Red Hat OpenShift on IBM Cloud or Azure Red Hat <span class="keep-together">OpenShift</span>.</p>&#13;
<p>In the following sections, you will create a cluster to act as the hub, install the Open Cluster Management hub, and provision two clusters to host the example application.</p>&#13;
<section data-pdf-bookmark="Provision an OpenShift Cluster to Host the Open Cluster Management Hub" data-type="sect2"><div class="sect2" id="provision_an_openshift_cluster_to_host_t">&#13;
<h2>Provision an OpenShift Cluster to Host the Open Cluster <span class="keep-together">Management Hub</span></h2>&#13;
<p>Provision an OpenShift 4.5 or later cluster following the default instructions. The cluster should have at least three worker nodes with a total of 18 CPU and 80G of memory or m5.xlarge (times three workers) on AWS EC2.</p>&#13;
<p>The following example <em>install-config.yaml</em> was used to prepare the hub cluster for this example, but so long as your starting cluster has the required capacity, you do not need to provision a cluster exactly like the <em>install-config.yaml</em> in <a data-type="xref" href="#an_example_install_configdotyaml_t">Example 8-2</a>.</p>&#13;
<div data-type="example" id="an_example_install_configdotyaml_t">&#13;
<h5><span class="label">Example 8-2. </span>An example <em>install-config.yaml</em> to provision a cluster to use as a hub</h5>&#13;
</div>&#13;
<pre data-type="programlisting">&#13;
apiVersion: v1&#13;
baseDomain: SPECIFY_YOUR_BASE_DOMAIN&#13;
controlPlane:&#13;
 hyperthreading: Enabled&#13;
 name: master&#13;
 replicas: 3&#13;
 platform:&#13;
   aws:&#13;
     type: m5.xlarge&#13;
     rootVolume:&#13;
       iops: 2000&#13;
       size: 100&#13;
       type: io1&#13;
compute:&#13;
- hyperthreading: Enabled&#13;
 name: worker&#13;
 replicas: 4&#13;
 platform:&#13;
   aws:&#13;
     type: m5.xlarge&#13;
     rootVolume:&#13;
       size: 100&#13;
       type: gp2&#13;
metadata:&#13;
 name: SPECIFY_YOUR_CLUSTER_NAME&#13;
networking:&#13;
 clusterNetwork:&#13;
 - cidr: 10.128.0.0/14&#13;
   hostPrefix: 23&#13;
 machineCIDR: 10.0.0.0/16&#13;
 networkType: OpenShiftSDN&#13;
 serviceNetwork:&#13;
 - 172.30.0.0/16&#13;
platform:&#13;
 aws:&#13;
   region: us-east-1&#13;
   userTags:&#13;
     contact: SPECIFY_YOUR_EMAIL_ADDRESS&#13;
     purpose: demo&#13;
publish: External&#13;
pullSecret: 'SPECIFY_YOUR_PULL_SECRET'&#13;
sshKey: |&#13;
 ssh-rsa SPECIFY_YOUR_SSH_RSA_PUBLIC_KEY&#13;
</pre>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Configure the Open Cluster Management Hub" data-type="sect2"><div class="sect2" id="configure_the_open_cluster_management_hu">&#13;
<h2>Configure the Open Cluster Management Hub</h2>&#13;
<p>For our example, we will deploy Open Cluster Management using Red Hat’s supported product offering, RHACM. Instructions for configuring RHACM were given in <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>. You can also deploy RHACM directly from the Red Hat Operator Catalog or by using the <a href="https://oreil.ly/a1zlD">documented instructions</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Provision Two or More Clusters to Host the Application" data-type="sect2"><div class="sect2" id="provision_two_or_more_clusters_to_host_t">&#13;
<h2>Provision Two or More Clusters to Host the Application</h2>&#13;
<p>Once you have configured the hub cluster, you will be able to register your cloud credentials and provision one or more clusters that will be managed automatically by the hub. These clusters can be provisioned across the same cloud provider in different regions or against multiple cloud providers. The example PAC-MAN app is very lightweight, so you will not need a lot of capacity to run the sample application.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deploy Ansible Tower and the Ansible Resource Operator" data-type="sect1"><div class="sect1" id="deploy_ansible_tower_and_the_ansible_res">&#13;
<h1>Deploy Ansible Tower and the Ansible Resource Operator</h1>&#13;
<p>In addition to a containerized application, we will configure a DNS Load <a contenteditable="false" data-primary="DNS Load Balancer Cloud Service (F5)" data-secondary="Ansible automation" data-type="indexterm" id="ch08-f5aut6"/><a contenteditable="false" data-primary="automations" data-secondary="Ansible for automation" data-type="indexterm" id="ch08-f5aut5"/><a contenteditable="false" data-primary="applications" data-secondary="application delivery example" data-tertiary="Ansible automation of load balancing" data-type="indexterm" id="ch08-f5aut4"/><a contenteditable="false" data-primary="Ansible for automation" data-type="indexterm" id="ch08-f5aut3"/><a contenteditable="false" data-primary="multicloud management" data-secondary="Ansible for automation" data-type="indexterm" id="ch08-f5aut7"/><a contenteditable="false" data-primary="load balancing" data-secondary="Ansible for automating" data-type="indexterm" id="ch08-f5aut2"/><a contenteditable="false" data-primary="F5" data-secondary="DNS Load Balancer Cloud Service" data-tertiary="Ansible automation" data-type="indexterm" id="ch08-f5aut"/><a contenteditable="false" data-primary="web application with backing datastore" data-secondary="deploying example application" data-tertiary="Ansible Tower and resource operator" data-type="indexterm" id="ch08-ansdep"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="deploying example application" data-tertiary="Ansible Tower and resource operator" data-type="indexterm" id="ch08-ansdep2"/><a contenteditable="false" data-primary="Ansible for automation" data-secondary="Ansible Tower" data-tertiary="deploying" data-type="indexterm" id="ch08-ansdep3"/>Balancer provided by F5 and use a ServiceNow developer instance to demonstrate how you might integrate a change management process into the deployment life cycle of your containerized apps. <a contenteditable="false" data-primary="Ansible for automation" data-secondary="Ansible Tower" data-tertiary="documentation online" data-type="indexterm" id="idm45358189076632"/>Detailed instructions on setting up the Ansible Tower container-based installation method are available in the <a href="https://oreil.ly/Uzm3y">Ansible documentation</a>. The following steps capture the specific actions taken to prepare the demo captured in this <span class="keep-together">repository</span>.<a contenteditable="false" data-primary="policies" data-secondary="Ansible Tower project" data-type="indexterm" id="idm45358189073240"/><a contenteditable="false" data-primary="Ansible for automation" data-secondary="Ansible Tower" data-tertiary="policies" data-type="indexterm" id="idm45358189071832"/></p>&#13;
<p>To simplify the configuration of this example, we will leverage some pre-defined policies that help prepare the hub cluster to deploy Ansible.</p>&#13;
<p>Clone the sample repository and apply the policies under <em>hack/manifests/policies</em>:</p>&#13;
<pre data-type="programlisting"><strong>$ cd hack/manifests/policies</strong>&#13;
$ <strong>oc apply -f ansible-tower-policies-subscription.yaml</strong></pre>&#13;
<p>The result of these policies will set up the Ansible Resource Operator, prepare the Ansible Tower project named <code>tower</code>, and <a contenteditable="false" data-primary="storage volumes" data-secondary="PersistentVolumeClaim" data-type="indexterm" id="idm45358189066536"/><a contenteditable="false" data-primary="PersistentVolumeClaim" data-type="indexterm" id="idm45358189065128"/><a contenteditable="false" data-primary="PostgreSQL" data-type="indexterm" id="idm45358189064024"/><a contenteditable="false" data-primary="Ansible for automation" data-secondary="Ansible Tower" data-tertiary="PostgreSQL database" data-type="indexterm" id="idm45358189062920"/>create a <code>PersistentVolumeClaim</code> using the default storage class to support Ansible Tower’s database (PostgreSQL).</p>&#13;
<p>After a few moments, verify that the resources were correctly applied on your hub cluster. You can see that the <code>policy-ansible-tower-prep</code> and <code>policy-auth-provider</code> are compliant from your RHACM web console (under “Govern Risk”). You can also verify the resources that were created as follows:</p>&#13;
<pre data-type="programlisting"><strong>$ oc get subs.operators --all-namespaces</strong>&#13;
NAMESPACE                 NAME                        PACKAGE          &#13;
             SOURCE                CHANNEL&#13;
open-cluster-management   acm-operator-subscription   advanced-cluster-&#13;
management   acm-custom-registry   release-2.2&#13;
tower-resource-operator   awx-resource-operator       awx-resource-&#13;
operator     redhat-operators      release-0.1&#13;
 &#13;
<strong>$ oc get pvc -n tower</strong>&#13;
NAME        STATUS  VOLUME                                CAPACITY  ACCESS &#13;
MODES   STORAGECLASS AGE&#13;
postgresql   Bound    pvc-1554a179-0947-4a65-9af0-81c5f2d8b476   5Gi        RWO      &#13;
         gp2           3d20h</pre>&#13;
<p>Download the Ansible Tower installer release from the <a href="https://oreil.ly/utowL">available releases</a>. Extract the archive into a working directory. Configure the inventory for your OpenShift cluster. The inventory file should be placed directly under the folder for the archive (e.g., <em>ansible-tower-openshift-setup-3.7.2/inventory</em>).</p>&#13;
<p>The inventory in <a data-type="xref" href="#an_example_inventory_file_that_provides">Example 8-3</a> can be placed under the root directory of the extracted release archive. Be sure to override the following:</p>&#13;
<dl>&#13;
<dt><code>SPECIFY_YOUR_OWN_PASSWORD</code></dt>&#13;
<dd>Choose a strong password of at least 16 characters.</dd>&#13;
<dt><code>SPECIFY_YOUR_CLUSTER_ADDRESS</code></dt>&#13;
<dd>Provide the correct hostname of the API server for your OpenShift cluster.</dd>&#13;
<dt><code>SPECIFY_YOUR_OPENSHIFT_CREDENTIALS</code></dt>&#13;
<dd>The password for your OpenShift cluster admin user. Be sure to also override kubeadmin if you have defined an alternate administrative user.</dd>&#13;
</dl>&#13;
<p/>&#13;
<div data-type="example" id="an_example_inventory_file_that_provides">&#13;
<h5><span class="label">Example 8-3. </span>An example inventory file that provides input values for the Ansible Tower installer</h5>&#13;
</div>&#13;
<pre data-type="programlisting">localhost ansible_connection=local ansible_python_interpreter="/usr/bin/env python"&#13;
[all:vars]&#13;
# This will create or update a default admin (superuser) account in Tower&#13;
admin_user=admin&#13;
admin_password='SPECIFY_YOUR_OWN_PASSWORD'&#13;
# Tower Secret key&#13;
# It's *very* important that this stay the same between upgrades or you will &#13;
# lose the ability to decrypt your credentials&#13;
secret_key='SPECIFY_YOUR_OWN_PASSWORD'&#13;
# Database Settings&#13;
# =================&#13;
# Set pg_hostname if you have an external postgres server, otherwise&#13;
# a new postgres service will be created&#13;
# pg_hostname=postgresql&#13;
# If using an external database, provide your existing credentials.&#13;
# If you choose to use the provided containerized Postgres depolyment, these&#13;
# values will be used when provisioning the database.&#13;
pg_username='admin'&#13;
pg_password='SPECIFY_YOUR_OWN_PASSWORD'&#13;
pg_database='tower'&#13;
pg_port=5432&#13;
pg_sslmode='prefer'  # set to 'verify-full' for client-side enforced SSL&#13;
# Note: The user running this installer will need cluster-admin privileges.&#13;
# Tower's job execution container requires running in privileged mode,&#13;
# and a service account must be created for auto peer-discovery to work.&#13;
# Deploy into Openshift&#13;
# =====================&#13;
openshift_host=https://api.SPECIFY_YOUR_CLUSTER_ADDRESS:6443&#13;
openshift_skip_tls_verify=true&#13;
openshift_project=tower&#13;
openshift_user=kubeadmin&#13;
openshift_password=SPECIFY_YOUR_OPENSHIFT_CREDENTIALS&#13;
# If you don't want to hardcode a password here, just do:&#13;
# ./setup_openshift.sh -e openshift_token=$TOKEN&#13;
# Skip this section if you BYO database. This is only used when you want the&#13;
# installer to deploy a containerized Postgres deployment inside of your&#13;
# OpenShift cluster. This is only recommended if you have experience storing and&#13;
# managing persistent data in containerized environments.&#13;
#&#13;
#&#13;
# Name of a PVC you've already provisioned for database:&#13;
openshift_pg_pvc_name=postgresql&#13;
#&#13;
# Or... use an emptyDir volume for the OpenShift Postgres pod.&#13;
# Useful for demos or testing purposes.&#13;
# openshift_pg_emptydir=true&#13;
# Deploy into Vanilla Kubernetes&#13;
# ==============================&#13;
# kubernetes_context=test-cluster&#13;
# kubernetes_namespace=ansible-tower</pre>&#13;
<p>Update the default task image that is used to run the defined jobs. Because the jobs use additional modules, we need to ensure that various Python module dependencies are available.</p>&#13;
<p>In <em>group_vars/all</em>, update the following key:</p>&#13;
<pre data-type="programlisting">kubernetes_task_image: quay.io/mdelder/ansible-tower-task</pre>&#13;
<p>You can build this image and consume it from your own registry by building the <em>Dockerfile.taskimage</em> under <em>hack/tower-setup/container_task_image</em>. Optionally, you could build the task image and publish to your own registry. If you’re using the existing image as previously defined, you will not need to build your own image. Use the correct Ansible version based on the release you downloaded:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>$ cd hack/tower-setup/container_task_image</strong>&#13;
<strong>$ docker build -t quay.io/YOUR_USERID/ansible-tower-task:3.7.2 \&#13;
-f Dockerfile.taskimage</strong>&#13;
<strong>$ docker push quay.io/YOUR_USERID/ansible-tower-task:3.7.2</strong>&#13;
</pre>&#13;
<p>Once you have made the relevant updates to your inventory file, you can run the Ansible Tower installer. Retrieve an authentication token from the OpenShift web console from the “Copy login command” action under your user name:</p>&#13;
<pre data-type="programlisting"><strong>$ ./setup_openshift.sh -e openshift_token=$TOKEN</strong></pre>&#13;
<p>Launch the Tower web console:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>$ open https://$(oc get route -n tower ansible-tower-web-svc \&#13;
-ojsonpath='{.status.ingress[0].host}')</strong>&#13;
</pre>&#13;
<p>Log in with the user and password that you specified in the inventory file. You must then choose your license for Tower. If you have a Red Hat user identity, you can log in and choose the 60-day evaluation license.</p>&#13;
<p>Optionally, you can customize the <em>hack/manifests/ansible-tower-console-link.yaml</em> for your own cluster. Then apply the file (note: you must update the URL within the file before this will work in your cluster):</p>&#13;
<pre data-type="programlisting">&#13;
<strong>$ cd hack/manifests</strong>&#13;
<strong>$ oc apply ansible-tower-console-link.yaml</strong>&#13;
</pre>&#13;
<p>After applying the <code>ConsoleLink</code>, refresh your OpenShift web console and view the shortcut to your Ansible Tower under the Applications drop-down menu in the header.<a contenteditable="false" data-primary="" data-startref="ch08-ansdep" data-type="indexterm" id="idm45358189031336"/><a contenteditable="false" data-primary="" data-startref="ch08-ansdep2" data-type="indexterm" id="idm45358189029960"/><a contenteditable="false" data-primary="" data-startref="ch08-ansdep3" data-type="indexterm" id="idm45358189028584"/></p>&#13;
<section data-pdf-bookmark="Configure Projects for ServiceNow and F5 DNS Load Balancer" data-type="sect2"><div class="sect2" id="configure_projects_for_servicenow_and_ff">&#13;
<h2>Configure Projects for ServiceNow and F5 DNS Load Balancer</h2>&#13;
<p>The example application uses the F5 DNS Load Balancer Cloud Service <a contenteditable="false" data-primary="web application with backing datastore" data-secondary="deploying example application" data-tertiary="configuring projects" data-type="indexterm" id="ch08-consn"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="deploying example application" data-tertiary="configuring projects" data-type="indexterm" id="ch08-consn2"/><a contenteditable="false" data-primary="Ansible for automation" data-secondary="Ansible Tower" data-tertiary="configuring projects" data-type="indexterm" id="ch08-consn3"/>and ServiceNow to demonstrate Ansible automation. This assumes that you have completed the following steps:</p>&#13;
<ol>&#13;
<li><p>Create a developer instance of ServiceNow. If you need a developer instance of ServiceNow, follow the directions at <a href="https://developer.servicenow.com">ServiceNow Developers</a>.</p></li>&#13;
<li><p>Create an account with the F5 DNS Load Balancer Cloud Service. If you need to create an account with <a href="https://oreil.ly/tmmSY">F5 DNS Load Balancer Cloud  Service</a>, you can do this directly or through the <a href="https://oreil.ly/HF5mw">AWS Marketplace</a>.</p></li>&#13;
<li><p>Delegate your global domain to the F5 DNS nameservers. Create a nameserver-delegating DNS record with the global domain that you will use with F5. You can do this via Route 53 or your DNS provider. The <a href="https://oreil.ly/ObL27">F5 DNS Load Balancer Cloud Service FAQ</a> answers questions related to this <span class="keep-together">prerequisite</span>.</p></li>&#13;
</ol>&#13;
<p>Once you have the credentials for these services, you can configure the Ansible Tower instance that you deployed with the two relevant Ansible projects providing the job templates that will be executed as part of the prehook and posthooks that run when the application is placed or removed on a cluster. For convenience, all of the configuration for Ansible Tower is completely automated (using Ansible playbooks that talk to Ansible Tower to create the projects/job templates/jobs/credentials that are needed for the example).</p>&#13;
<p>Create a file named <em>tower_cli.cfg</em> under <em>hack/tower-setup</em> with the following contents:</p>&#13;
<pre data-type="programlisting"># hack/tower-setup/tower_cli.cfg&#13;
[general]&#13;
host = https://ansible-tower-web-svc-tower.apps.cluster.baseDomain&#13;
verify_ssl = false&#13;
#oauth_token = ALTERNATIVELY_USE_A_TOKEN&#13;
username = admin&#13;
# password = SPECIFY_YOUR_OWN_PASSWORD</pre>&#13;
<p>If you’re unsure of the host address for Tower, you can use <code>oc</code> to find the correct value:</p>&#13;
<pre data-type="programlisting"><strong>$ oc get route -n tower ansible-tower-web-svc \&#13;
-ojsonpath='{.status.ingress[0].host}'</strong></pre>&#13;
<p>Create a file named <em>credentials.yml</em> under <em>hack/tower-setup/group_vars</em> with the following contents:</p>&#13;
<pre data-type="programlisting"># User and password for the F5 CloudServices account.&#13;
f5aas_username: SPECIFY_YOUR_F5_USERNAME&#13;
f5aas_password: SPECIFY_YOUR_F5_PASSWORD&#13;
 &#13;
# Credentials for ServiceNow&#13;
snow_username: admin&#13;
snow_password: SPECIFY_YOUR_SERVICENOW_USERNAME&#13;
# Specify your ServiceNow developer instance ID.&#13;
snow_instance: devXXXXX</pre>&#13;
<p>You may need to install required Python libraries:</p>&#13;
<pre data-type="programlisting"># Optionally specify the correct version of python required by Ansible&#13;
# Of course, you must update the PYTHON var specific to your environment&#13;
<strong>$ export PYTHON="/usr/local/Cellar/ansible/2.9.13/libexec/bin/python3.8"</strong>&#13;
<strong>$ $PYTHON -m pip install --upgrade ansible-tower-cli</strong>&#13;
</pre>&#13;
<p>Run the playbook that will talk to Ansible Tower and configure our two projects (one for F5 and one for ServiceNow) and the relevant job templates:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>$ export PYTHON="/usr/local/Cellar/ansible/2.9.13/libexec/bin/python3.8"</strong>&#13;
<strong>$ ansible-playbook -e ansible_python_interpreter="$PYTHON" tower-setup.yml</strong>&#13;
</pre>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Configure the toweraccess Secret and Create the Ansible Tower Token" data-type="sect2"><div class="sect2" id="configure_the_toweraccess_secret_and_cre">&#13;
<h2>Configure the toweraccess Secret and Create the Ansible Tower Token</h2>&#13;
<p>From Ansible Tower, create an authorization token. The authorization token will be used in a secret that the application will reference to invoke the Ansible Tower Jobs. Follow these steps:<a contenteditable="false" data-primary="" data-startref="ch08-f5aut" data-type="indexterm" id="idm45358188998504"/><a contenteditable="false" data-primary="" data-startref="ch08-f5aut2" data-type="indexterm" id="idm45358188997128"/><a contenteditable="false" data-primary="" data-startref="ch08-f5aut3" data-type="indexterm" id="idm45358188995752"/><a contenteditable="false" data-primary="" data-startref="ch08-consn" data-type="indexterm" id="idm45358188994376"/><a contenteditable="false" data-primary="" data-startref="ch08-consn2" data-type="indexterm" id="idm45358188993000"/><a contenteditable="false" data-primary="" data-startref="ch08-consn3" data-type="indexterm" id="idm45358188991624"/><a contenteditable="false" data-primary="" data-startref="ch08-f5aut4" data-type="indexterm" id="idm45358188990248"/><a contenteditable="false" data-primary="" data-startref="ch08-f5aut5" data-type="indexterm" id="idm45358188988872"/><a contenteditable="false" data-primary="" data-startref="ch08-f5aut6" data-type="indexterm" id="idm45358188987496"/><a contenteditable="false" data-primary="" data-startref="ch08-f5aut7" data-type="indexterm" id="idm45358188986120"/></p>&#13;
<ol>&#13;
<li><p>Log in to the Ansible Tower instance:</p>&#13;
<pre data-type="programlisting"><strong>$ open https://$(oc get route -n tower ansible-tower-web-svc \&#13;
-ojsonpath='{.status.ingress[0].host}')</strong></pre></li>&#13;
<li><p>Click on the “admin” user in the header.</p></li>&#13;
<li><p>Click on Tokens.</p></li>&#13;
<li><p>Click on the +.</p></li>&#13;
<li><p>Set the scope to Write.</p></li>&#13;
<li><p>Click Save and be sure to copy and save the value of the token.</p></li>&#13;
<li><p>Create a file <em>named hack/manifests/toweraccess-secret.yaml</em> with the following contents:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
stringData:&#13;
  host: ansible-tower-web-svc-&#13;
tower.apps.SPECIFY_YOUR_CLUSTER_NAME.SPECIFY_YOUR_BASE_DOMAIN&#13;
  token: SPECIFY_YOUR_ANSIBLE_TOWER_ADMIN_TOKEN&#13;
kind: Secret&#13;
metadata:&#13;
  name: toweraccess&#13;
  namespace: pacman-app&#13;
type: Opaque</pre></li>&#13;
</ol>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deploy the pacman-app Example to Your Cluster" data-type="sect1"><div class="sect1" id="deploy_the_pacman_app_example_to_your_cl">&#13;
<h1>Deploy the pacman-app Example to Your Cluster</h1>&#13;
<p>Now we will deploy the manifests that govern our application placement <a contenteditable="false" data-primary="web application with backing datastore" data-secondary="deploying example application" data-tertiary="deploying to cluster" data-type="indexterm" id="ch08-depdep"/><a contenteditable="false" data-primary="multicluster application delivery" data-secondary="deploying example application" data-tertiary="deploying to cluster" data-type="indexterm" id="ch08-depdep2"/>to the hub. By creating the application, subscription, and <code>PlacementRule</code>, you will enable the hub to deploy the application dynamically to one or more of the clusters that you created earlier.</p>&#13;
<p>First, create the project for the application and apply the secret:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>$ oc new-project pacman-app</strong>&#13;
<strong>$ oc apply -f hack/manifests/toweraccess-secret.yaml</strong>&#13;
</pre>&#13;
<p>You can either create the application manifest from the RHACM web console (Managed Applications &gt; Create application) or apply the prebuilt resources from the example Git repository. An example of the final result is provided in <a data-type="xref" href="#the_pac_man_application_manifest_and_sup">Example 8-4</a>.</p>&#13;
<div data-type="example" id="the_pac_man_application_manifest_and_sup">&#13;
<h5><span class="label">Example 8-4. </span>The PAC-MAN application manifest and supporting resources to allow the hub cluster to deploy the application to any managed cluster in the fleet</h5>&#13;
</div>&#13;
<pre data-type="programlisting">---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: Channel&#13;
metadata:&#13;
  name: pacman-app-latest&#13;
  namespace: pacman-app&#13;
  labels:&#13;
    app.kubernetes.io/name: pacman&#13;
  annotations:&#13;
  &#13;
    apps.open-cluster-management.io/github-path: deploy&#13;
spec:&#13;
  type: GitHub&#13;
  pathname: https://github.com/SPECIFY_YOUR_GITHUB_ORG/k8s-pacman-app.git&#13;
  # secretRef:&#13;
  #   name: github-credentials&#13;
---&#13;
apiVersion: app.k8s.io/v1beta1&#13;
kind: Application&#13;
metadata:&#13;
  name: pacman-app&#13;
  namespace: pacman-app&#13;
spec:&#13;
  componentKinds:&#13;
  - group: apps.open-cluster-management.io&#13;
    kind: Subscription&#13;
  descriptor: {}&#13;
  selector:&#13;
    matchExpressions:&#13;
    - key: app.kubernetes.io/name&#13;
      operator: In&#13;
      values:&#13;
      - pacman&#13;
---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: Subscription&#13;
metadata:&#13;
  annotations:&#13;
    apps.open-cluster-management.io/git-branch: ansible&#13;
    apps.open-cluster-management.io/github-path: deploy&#13;
  name: pacman-app&#13;
  namespace: pacman-app&#13;
  labels:&#13;
    app.kubernetes.io/name: pacman&#13;
spec:&#13;
  channel: pacman-app/pacman-app-latest&#13;
  hooksecretref:&#13;
    name: toweraccess&#13;
  placement:&#13;
    placementRef:&#13;
      kind: PlacementRule&#13;
      name: pacman-dev-clusters&#13;
---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: pacman-dev-clusters&#13;
 namespace: pacman-app&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
   type: ManagedClusterConditionAvailable&#13;
 clusterReplicas: 2&#13;
 clusterSelector:&#13;
   # matchExpressions:&#13;
   # - key: region&#13;
   #   operator: In&#13;
   #   values:&#13;
   #   - us-east-1&#13;
   #   - us-west-1&#13;
   #   - europe-west3&#13;
   matchLabels:&#13;
     apps/pacman: deployed&#13;
</pre>&#13;
<p>The <code>channel</code> references the source of Kubernetes manifests in GitHub. Any changes that are made to the supporting GitHub repository will trigger an update across the fleet. The <code>application</code> provides a way to associate a set of subscriptions to a logical unit of deployment and management. The <code>subscription</code> selects a specific branch and directory from within the GitHub repository. You may have a single GitHub repository that feeds multiple subscriptions for one application or multiple applications. Each subscription can be placed independently, so you may have different parts of an application deployed to different clusters. The <code>PlacementRule</code> defines a set of labels and a match expression that must be met for the subscription to be deployed against a managed cluster within the fleet.</p>&#13;
<p>Be aware of how you label your clusters and the supporting <code>PlacementRule</code>. You want to ensure that the <code>PlacementRule</code> indicates the selected clusters in their status <span class="keep-together">conditions</span>:</p>&#13;
<pre data-type="programlisting"><strong>$ oc get placementrule -n pacman-app -oyaml</strong>&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
name: pacman-app-placement-0&#13;
namespace: pacman-app&#13;
spec:&#13;
clusterSelector:&#13;
    matchLabels:&#13;
    apps/pacman: deployed&#13;
status:&#13;
decisions:&#13;
- clusterName: foxtrot-ap-northeast-1&#13;
    clusterNamespace: foxtrot-ap-northeast-1&#13;
- clusterName: foxtrot-gcp-europe&#13;
    clusterNamespace: foxtrot-gcp-europe</pre>&#13;
<p>Now that you have deployed all of the supporting pieces, you can experiment by adding or removing matching clusters or changing the desired labels specified in the <code>PlacementRule</code>. The application instances will be added or removed to supporting clusters while keeping the F5 DNS Load Balancer Cloud Service up to date <span class="keep-together">automatically</span>.</p>&#13;
<p>From the topology view, your application deployment should resemble <a data-type="xref" href="#pac-man_app_config_hooks">Figure 8-4</a>.<a contenteditable="false" data-primary="" data-startref="ch08-mad" data-type="indexterm" id="idm45358188953912"/><a contenteditable="false" data-primary="" data-startref="ch08-appapp" data-type="indexterm" id="idm45358188952536"/><a contenteditable="false" data-primary="" data-startref="ch08-depdep" data-type="indexterm" id="idm45358188951160"/><a contenteditable="false" data-primary="" data-startref="ch08-depdep2" data-type="indexterm" id="idm45358188949784"/></p>&#13;
<figure><div class="figure" id="pac-man_app_config_hooks">&#13;
 <img src="assets/hcok_0804.png"/>&#13;
 <h6><span class="label">Figure 8-4. </span>The PAC-MAN application configured with pre-hooks and post-hooks with Ansible Tower Jobs</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00014">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, we focused on providing a practical example that demonstrates how to deliver an application across multiple clusters, how to manage any parts of your system that are still not natively programmable from Kubernetes operators, and how to integrate a set of clusters behind a global load balancer service. Many specific examples from earlier in the book were used to pull all of this together, including the PAC-MAN application that used  a Tekton pipeline to build its supporting images in <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>. We used policies to configure an operator on our cluster (the Ansible Resource Operator on the hub) and used the Open Cluster Management hub to provision clusters that were used to support the running application. We also introduced one way that you can adapt your existing change management process using Ansible to create change request tickets whenever the system dynamically reacts to a change.</p>&#13;
<p>Hopefully, by now you have a firm grasp of what is possible with Kubernetes and OpenShift and how you can use some of these capabilities to streamline your adoption of these technologies for your organization.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch01fn38"><sup><a href="ch08.html#ch01fn38-marker">1</a></sup> From “What Is an Ansible Playbook?”, Red Hat, <a href="https://oreil.ly/At7XY"><em class="hyperlink">https://oreil.ly/At7XY</em></a>.</p><p data-type="footnote" id="ch01fn39"><sup><a href="ch08.html#ch01fn39-marker">2</a></sup> Colin McCarthy, “Ansible + ServiceNow Part 1: Opening and Closing Tickets,” Red Hat Ansible Blog (June 6, 2019), <a href="https://oreil.ly/NI9ZX"><em class="hyperlink">https://oreil.ly/NI9ZX</em></a>.</p></div></div></section></body></html>