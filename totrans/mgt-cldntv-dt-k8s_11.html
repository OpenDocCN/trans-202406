<html><head></head><body><section data-pdf-bookmark="Chapter 10. Machine Learning and Other Emerging Use Cases" data-type="chapter" epub:type="chapter"><div class="chapter" id="machine_learning_and_other_emerging_use">&#13;
<h1><span class="label">Chapter 10. </span>Machine Learning and Other <span class="keep-together">Emerging Use Cases</span></h1>&#13;
<p><a contenteditable="false" data-primary="ML (machine learning)" data-secondary="about" data-type="indexterm" id="idm46183195384512"/>In previous chapters, we covered traditional data infrastructure including databases, streaming platforms, and analytic engines with a focus on Kubernetes. Now it’s time to start looking beyond, exploring the projects and communities that are beginning to make cloud native their destination, especially concerning AI and ML.</p>&#13;
<p>Any time multiple arrows start pointing in the same direction, it’s worth paying attention. The directional arrows in data infrastructure all point to an overall macro trend of convergence on Kubernetes, supported by several interrelated trends:</p>&#13;
<ul>&#13;
<li><p>Common stacks are emerging for managing compute-intensive AI/ML workloads, including those that leverage specific hardware such as GPUs.</p></li>&#13;
<li><p>Common data formats are helping to promote the efficient movement of data across compute, network, and storage resources.</p></li>&#13;
<li><p>Object storage is becoming a common persistence layer for data infrastructure.</p></li>&#13;
</ul>&#13;
<p>In this chapter, we will look at several emerging technologies that embody these trends, the use cases they enable, and how they contribute to helping you further manage the precious resources of compute, network, and storage. We have chosen a few projects that touch on different aspects of ML and using data—this is by no means an exhaustive survey of every technology in use today. We hear directly from the engineers working on each project and provide some details on how they fit into a cloud native data stack. You are highly encouraged to continue your journey into your interests beyond what is presented here. Follow your curiosity and contribute to the communities supporting new use cases in Kubernetes.</p>&#13;
<section data-pdf-bookmark="The Cloud Native AI/ML Stack" data-type="sect1"><div class="sect1" id="the_cloud_native_aisolidusml_stack">&#13;
<h1>The Cloud Native AI/ML Stack</h1>&#13;
<p><a contenteditable="false" data-primary="ML (machine learning)" data-secondary="cloud native AI/ML stack" data-type="indexterm" id="ml_cn"/><a contenteditable="false" data-primary="cloud native AI/ML stack" data-type="indexterm" id="cn_ab"/>As discussed in <a data-type="xref" href="ch09.html#data_analytics_on_kubernetes">Chapter 9</a>, analytics, AI, and ML on Kubernetes is a topic worthy of more detailed examination. If you aren’t familiar with this specialty in the world of data, it’s an exciting domain that enhances our ability to produce real-time, data-driven decisions at scale. While many of the core algorithms have existed for decades, the nature of this work has been changing rapidly over the past few years. Data science as a profession has traditionally been relegated to the back office, where volumes of historical data were gleaned for insight to find meaning and predict the future. Data scientists rarely had any direct involvement with end-user applications, and their work was disconnected from user-facing applications.</p>&#13;
<p>This began to change with the emergence of the data engineer role. Data engineers build the processing engines and pipelines to productionalize data science and break down silos between disciplines. As is typical for emerging fields in data infrastructure, the largest, most vocal organizations set the tempo for data engineering, and their tools and methods have become the mainstream.</p>&#13;
<p>The real-time nature of data in applications can’t be left just to databases and streaming platforms. Products built by data scientists must be closer to the end user to maximize their effectiveness in applications. Many organizations have recognized this as both a problem and an opportunity: how can we make data science another near-real-time component of application deployments? True to form, when faced with a challenge, the community has risen to the occasion to build new projects and create new disciplines. As a result, a new category of data infrastructure on Kubernetes is emerging alongside the traditional categories of persistence, streaming, and analytics. This new stack consists of tools that support the real-time serving of data specific to AI and ML.</p>&#13;
<section data-pdf-bookmark="AI/ML Definitions" data-type="sect2"><div class="sect2" id="aisolidusml_definitions">&#13;
<h2>AI/ML Definitions</h2>&#13;
<p><a contenteditable="false" data-primary="AI/ML (artificial intelligence/machine learning)" data-secondary="definitions" data-type="indexterm" id="idm46183195204048"/>If you are new to the field of AI/ML, it’s easy to become overwhelmed by the terminology. Before we look at a few cloud native technologies that solve problems in the AI stack, let’s spend some time understanding the new terms and concepts that are critical to understanding this specialty. If you are familiar with AI/ML, you can safely skip to the next section.</p>&#13;
<p>First, let’s briefly review some common terms used in AI/ML. These frequently appear in descriptions of projects and features, and you’ll need to understand them to select the right tools and apply them effectively:</p>&#13;
<dl>&#13;
<dt>Algorithm</dt>&#13;
<dd><a contenteditable="false" data-primary="algorithm" data-type="indexterm" id="idm46183195188016"/>The basic computational building block of ML is the algorithm. Algorithms are expressed in code as a set of instructions to analyze data. Common algorithms include linear regression, decision trees, k-means, and random forest. Data <span class="keep-together">scientists</span> spend their time working with algorithms to gain insights from data. When the procedures and parameters are right, the final, repeatable form is output into models.</dd>&#13;
<dt>Model</dt>&#13;
<dd><a contenteditable="false" data-primary="model" data-type="indexterm" id="idm46183195324800"/>ML aims to build systems that mimic the way humans learn so that they can answer questions based on provided data without explicit programming. Example questions include identifying whether two objects are similar, the likelihood of occurrence of a particular event, or choosing the best option given multiple candidates. The answering system for these questions is described in a mathematical model (or simply <em>model</em> for short). A model acts as a function machine: data that describes a question goes in, and new data that represents an answer comes out.</dd>&#13;
<dt>Feature</dt>&#13;
<dd><a contenteditable="false" data-primary="features" data-type="indexterm" id="idm46183195535792"/>Features are the portions of a more extensive data set relevant to a specific use case. Features are used both to train models and to provide input to models in production. For example, if you wanted to predict the weather, you might select time, location, and temperature from a much larger data set, ignoring other data such as air quality. <em>Feature selection</em> is the process of determining what data you’ll use, which can be an iterative process. When you hear the word <em>feature</em>, you can easily translate that to <em>data</em>.</dd>&#13;
<dt>Training</dt>&#13;
<dd><a contenteditable="false" data-primary="training" data-type="indexterm" id="idm46183195508672"/>A model consists of an algorithm plus data (features) that apply that algorithm to a particular domain. To train a model, training data is passed through the algorithm to help refine the output to match the expected answer based on the data presented. This training data contains the same features that will be used in production, with the key difference that the expected answer is known. For example, given historical temperatures, do the parameters used in the model predict what actually happened? Training is the most resource-intensive phase of ML.</dd>&#13;
<dt>Flow</dt>&#13;
<dd><a contenteditable="false" data-primary="flow" data-type="indexterm" id="idm46183195167488"/>Flow is a shorthand term for <em>workflow</em>. An ML workflow describes the steps required to build a working model. The flow generally includes data collection, preprocessing and cleaning, feature engineering, model training, validation, and performance testing. These are typically fully automated processes.</dd>&#13;
<dt>Vector</dt>&#13;
<dd><a contenteditable="false" data-primary="vector" data-type="indexterm" id="idm46183195676064"/>The classic mathematical definition of a vector is a quantity that indicates direction and magnitude. ML models are mathematical formulas that use numerical data. Since not all source data is represented as numbers, normalizing input data into vector representations is the key to using general-purpose algorithms in ML. Images and text are examples of data that can be vector encoded in the preprocessing step of the flow.</dd>&#13;
<dt>Prediction</dt>&#13;
<dd><a contenteditable="false" data-primary="prediction" data-type="indexterm" id="idm46183195511920"/>Prediction is the step of using the created model to produce a likely answer based on input data. For example, we might ask the expected temperature for a given location, date, and time by using a weather model. The question being answered takes the form “What will happen?”</dd>&#13;
<dt>Inference</dt>&#13;
<dd><a contenteditable="false" data-primary="inference" data-type="indexterm" id="idm46183195590176"/>Inference models look for reasons by reversing the relationship of input and output data. Given an answer, what features contributed to arriving at this answer? Here’s another weather example: based on rainfall, what are the most associated temperatures and barometric pressures? The question being answered is “How did this happen?”</dd>&#13;
<dt>Drift</dt>&#13;
<dd><a contenteditable="false" data-primary="drift" data-type="indexterm" id="idm46183195519376"/>Models are trained with snapshots of data from a point in time. Drift is a condition that occurs as the model loses accuracy due to conditions that have changed over time or are no longer relevant based on the original training data. When drift happens in a model, the solution is to refine the model with updated training data.</dd>&#13;
<dt>Bias</dt>&#13;
<dd><a contenteditable="false" data-primary="bias" data-type="indexterm" id="idm46183195522704"/>Models are only as good as the algorithms used and how those algorithms are trained. Bias can be introduced at several points: in the algorithm itself, sample data that contains user prejudice or faulty measurement, or exclusion of data. In any case, the goal of ML is to be as accurate as possible, and bias is an accuracy measurement. Detecting bias in data is a complex problem and is easier to address early through good data governance and process rigor.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="Guido, Sarah, Introduction to Machine Learning with Python" data-type="indexterm" id="idm46183195227744"/><a contenteditable="false" data-primary="Introduction to Machine Learning with Python (Miller and Guido)" data-type="indexterm" id="idm46183195520848"/><a contenteditable="false" data-primary="Miller, Andreas C., Introduction to Machine Learning with Python" data-type="indexterm" id="idm46183195209344"/>These are some of the key concepts that will help you understand the rest of this section. For a more complete introduction, consider <a class="orm:hideurl" href="https://oreil.ly/6ii4d"><em>Introduction to Machine Learning with Python</em></a> (O’Reilly) by Andreas C. Müller and Sarah Guido, or one of the many quality online courses available from your favorite learning platform.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Defining an AI/ML Stack" data-type="sect2"><div class="sect2" id="defining_an_aisolidusml_stack">&#13;
<h2>Defining an AI/ML Stack</h2>&#13;
<p><a contenteditable="false" data-primary="AI/ML (artificial intelligence/machine learning)" data-secondary="defining stacks" data-type="indexterm" id="idm46183195151936"/><a contenteditable="false" data-primary="stacks, AI/ML (artificial intelligence/machine learning)" data-type="indexterm" id="idm46183195150720"/>Given these definitions, we can describe the elements of a cloud native AI stack and the purposes such a stack can serve. Emerging disciplines and communities have similar implementations with minor variations as various teams innovate to solve their own specific needs. We can identify some common patterns by looking at organizations that use AI/ML in production at scale and the trends around Kubernetes adoption. <a data-type="xref" href="#common_elements_of_a_cloud_native_aisol">Figure 10-1</a> shows some of the typical elements found in architectures currently in production. Without being prescriptive, we’ll use this as an example of the types of tools in the stack and how they might fit together to serve the real-time components of AI/ML.</p>&#13;
<figure><div class="figure" id="common_elements_of_a_cloud_native_aisol">&#13;
<img alt="Common elements of a cloud native AI/ML stack" src="assets/mcdk_1001.png"/>&#13;
<h6><span class="label">Figure 10-1. </span>Common elements of a cloud native AI/ML stack</h6>&#13;
</div></figure>&#13;
<p><a contenteditable="false" data-primary="scikit-learn" data-type="indexterm" id="idm46183195915248"/><a contenteditable="false" data-primary="PyTorch" data-type="indexterm" id="idm46183195915600"/><a contenteditable="false" data-primary="TensorFlow" data-type="indexterm" id="idm46183195342016"/><a contenteditable="false" data-primary="XGBoost" data-type="indexterm" id="idm46183195160688"/><a contenteditable="false" data-primary="Kubeflow" data-type="indexterm" id="idm46183195159584"/>The goal of a cloud native AI/ML stack should be to get the insights produced by AI/ML as close to your users as possible, which means shortening the distance between backend analytic processes and using their output in frontend production systems. Data exploration happens using algorithms provided in libraries such as <a href="https://scikit-learn.org">scikit-learn</a>, <a href="https://pytorch.org">PyTorch</a>, <a href="https://www.tensorflow.org">TensorFlow</a>, and <a href="https://xgboost.readthedocs.io">XGBoost</a> using data stored in databases or static files. Python is the most commonly used language with ML libraries. The systems we discussed in <a data-type="xref" href="ch09.html#data_analytics_on_kubernetes">Chapter 9</a>, including Apache Spark, Dask, and Ray, are used to scale up the processing required to use Python libraries to build models. <a href="https://www.kubeflow.org">Kubeflow</a> and similar tools allow data engineers to create ML workflows for model generation. The workflows output a model file to object storage, providing the bridge between the backend processes and frontend production use.</p>&#13;
<p><a contenteditable="false" data-primary="KServe" data-type="indexterm" id="idm46183195714256"/><a contenteditable="false" data-primary="Seldon" data-type="indexterm" id="idm46183195207568"/><a contenteditable="false" data-primary="BentoML" data-type="indexterm" id="idm46183195137120"/><a contenteditable="false" data-primary="Feast" data-type="indexterm" id="idm46183195136144"/>Models are meant to be used, and this is the role of real-time model serving tools such as <a href="https://github.com/kserve/kserve">KServe</a>, <a href="https://www.seldon.io">Seldon</a>, and <a href="https://www.bentoml.com">BentoML</a>. These tools perform predictions on behalf of applications using existing models from object storage and feature stores such as <a href="https://feast.dev">Feast</a>. Feature stores perform full lifecycle management of feature data, storing new feature data in an online database such as Cassandra, training, and serving features to models.</p>&#13;
<p><a contenteditable="false" data-primary="Apache Solr" data-type="indexterm" id="idm46183195418000"/><a contenteditable="false" data-primary="Milvus" data-type="indexterm" id="idm46183195152656"/><a contenteditable="false" data-primary="Weaviate" data-type="indexterm" id="idm46183195129168"/><a contenteditable="false" data-primary="Qdrant" data-type="indexterm" id="idm46183195128128"/><a contenteditable="false" data-primary="Vald" data-type="indexterm" id="idm46183195127024"/><a contenteditable="false" data-primary="Vearch" data-type="indexterm" id="idm46183195125920"/>Vector similarity search engines are a new but familiar addition to the real-time serving stack for applications. While traditional search engines such as <a href="https://solr.apache.org">Apache Solr</a> provide convenient APIs for text searching, including fuzzy matching, vector similarity search is a more powerful algorithm, helping to answer the question “What is like the thing I currently have?” To do this, it uses relationships in the data instead of just the terms in your search query. Vector similarity supports many formats, including text, video, audio, and anything else that can be analyzed into a vector. Many open source tools implement vector similarity search, including <a href="https://milvus.io">Milvus</a>, <a href="https://weaviate.io">Weaviate</a>, <a href="https://qdrant.tech">Qdrant</a>, <a href="https://vald.vdaas.org">Vald</a>, and <a href="https://github.com/vearch/vearch">Vearch</a>.</p>&#13;
<p>Let’s examine a few of the tools that support frontend ML usage by applications in more detail and learn how they are deployed in Kubernetes: KServe, Feast, and Milvus.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Real-Time Model Serving with KServe" data-type="sect2"><div class="sect2" id="real_time_model_serving_with_kserve">&#13;
<h2>Real-Time Model Serving with KServe</h2>&#13;
<p><a contenteditable="false" data-primary="KServe" data-type="indexterm" id="kser_ab"/><a contenteditable="false" data-primary="ML (machine learning)" data-secondary="operationalizing models with KServe" data-type="indexterm" id="ml_kser"/><a contenteditable="false" data-primary="real-time modeling, with KServe" data-type="indexterm" id="rtm_kser"/>The “last mile” problem of real-time access to analytic products in AI/ML is one that Kubernetes is well poised to solve. Consider the architecture of modern web applications: HTTP servers that seem to simply serve a web page often have much more complexity behind them. The reality is that application logic and data infrastructure are combined to hide the complexity. Much like the HTTP server that listens for requests and serves a web page, a model server hides the complexity of loading and executing models. It focuses on the developer experience after the data science is done.</p>&#13;
<p><em>KServe</em> is a Kubernetes native model server that makes it easy to provide prediction capabilities to applications in production environments. Let’s learn more about the origins and functionality of KServe from one of the project founders.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="operationalizing_machine_learning_model">&#13;
<h5>Operationalizing ML Models with KServe</h5>&#13;
<p><em>With Theofilos Papapanagiotou, Data Science Architect at Prosus</em></p>&#13;
<p><a contenteditable="false" data-primary="Papanagiotou, Theofilos" data-type="indexterm" id="idm46183195562352"/>Google is well known for its contributions to the ML community with projects such as TensorFlow. Based on the framework it used to run TensorFlow internally, Google also created Kubeflow, an open source project to help data scientists and engineers use TensorFlow in production. Kubeflow contains multiple subprojects for different aspects of deploying ML workflows. One subproject that addressed the externalization (making available for use by other systems) of models was called Kubeflow serving (KFServing). Initially, it was built only for TensorFlow, but new contributors joined in and added support for other models such as PyTorch, scikit-learn, and XGBoost. In 2021, KFServing became an independent project from KubeFlow and was renamed KServe.</p>&#13;
<p><a contenteditable="false" data-primary="Knative eventing" data-type="indexterm" id="idm46183195190496"/>The core function of KServe is to provide an API endpoint for deploying previously built ML models in Kubernetes. Deploying each model involves multiple steps. KServe handles the fetching of the model from an object store, loading it into memory, and determining whether the model needs to use CPU or GPU. When GPU is required, KServe manages the copying of the model from CPU memory to GPU memory. This behavior can be specified with just a few lines of YAML, which eliminates a lot of the toil when working with ML in production environments. For SREs, there is additional integration with <a href="https://oreil.ly/IGDcM">Knative Eventing</a> to manage the scale-out, and observability features like metrics and logging. These are expected behaviors of an HTTP API and important aspects of putting ML models in production.</p>&#13;
<p>KServe has many contributors, and all are driven by a similar mission: operationalizing ML to be used by as many people as possible. Data is significant intellectual property to your organization, and data scientists are tasked to build models that make efficient use of that data. The real treasure for an organization is the ability to take those models and apply them to data to make predictions that can be used in your products, which in turn creates added value for your customers. KServe emphasizes using data in real time over pushing it to a data lake where it might be forgotten. For this reason, KServe does not provide a general-purpose data store; it’s simply a hosting system for models. It functions as a microservice in your cloud native application, accepting inference requests containing a list of features. The data returned is a prediction based on the input, and it has to happen quickly, efficiently, and securely.</p>&#13;
<p>Bloomberg is one of the top contributors to KServe, and its use case is an excellent example of how KServe adds value. Bloomberg News is a real-time news feed that has a diminishing time value for its users, so articles it provides must be timely and relevant. Bloomberg uses a massive collection of natural language processing (NLP) models to score incoming news articles from a variety of sources. Each article is labeled, classified, and provided to users through a service it calls the Terminal. This processing isn’t a back-office problem that can be done later, and the inferencing must be updated dynamically. Fortunately, KServe allows the models to be updated on the fly. This sort of problem is common in many mobile applications and SaaS products, and the ease of integration is key.</p>&#13;
<p><a contenteditable="false" data-primary="MLOps (machine learning operations)" data-type="indexterm" id="idm46183195118192"/><a contenteditable="false" data-primary="AI Fairness" data-type="indexterm" id="idm46183195397248"/>Beyond just serving models, KServe also helps manage the lifecycle of ML models. One feature, called an <em>explainer</em>, provides further information about each prediction. For example, it can offer insight into why a decision was made to approve or reject a loan application. KServe does this by providing feature importance and highlighting features in the model that led to the loan decision outcome, such as income level or credit history. Knowing more than just a binary yes or no decision helps build trust in the application. For ML operations (MLOps) you can use feature importance to detect model drift by integrating KServe with other services to compare results with training data to see if the production model is diverging. You can even include bias detection with <a href="https://lfaidata.foundation/projects/ai-fairness-360">AI Fairness</a>, which is now a Linux Foundation incubating project. These features help KServe reduce the effort involved in MLOps.</p>&#13;
<p>ML affects all our lives, from food delivery to entertainment. Serving models dynamically in a Kubernetes environment is a crucial step toward integrating ML and AI in more and more applications, and KServe will play a large role in making that happen.</p>&#13;
</div></aside>&#13;
<p><a contenteditable="false" data-primary="Predictor Service" data-type="indexterm" id="idm46183195555632"/><a contenteditable="false" data-primary="Transformer Service" data-type="indexterm" id="idm46183195282608"/><a data-type="xref" href="#deploying_kserve_in_kubernetes">Figure 10-2</a> shows how KServe is deployed on Kubernetes. The control plane consists of the KServe controller, which manages custom resources known as InferenceServices. Each InferenceService instance contains two microservices, a <code>Transformer</code> Service and a <code>Predictor</code> Service, each consisting of a Deployment and a Service. The Knative framework is used for request processing, treating these as serverless microservices that can scale to zero when they are not being used for maximum efficiency.</p>&#13;
<figure><div class="figure" id="deploying_kserve_in_kubernetes">&#13;
<img alt="Deploying KServe in Kubernetes" src="assets/mcdk_1002.png"/>&#13;
<h6><span class="label">Figure 10-2. </span>Deploying KServe in Kubernetes</h6>&#13;
</div></figure>&#13;
<p>The <code>Transformer</code> Service provides the endpoint for prediction requests from client applications. It also implements a three-stage process of preprocessing, prediction, and post-processing:</p>&#13;
<dl>&#13;
<dt>Preprocessing</dt>&#13;
<dd><a contenteditable="false" data-primary="preprocessing, Transformer Service and" data-type="indexterm" id="idm46183195192592"/>The <code>Transformer</code> Service converts the incoming data into a usable form for the model. For example, you may have a model that predicts whether a hot dog is in a picture. The <code>Transformer</code> Service will convert an incoming picture to a vector before passing it to the inference service. During preprocessing, the <code>Transformer</code> Service also loads feature data from a feature store such as Feast.</dd>&#13;
<dt>Prediction</dt>&#13;
<dd><a contenteditable="false" data-primary="prediction" data-type="indexterm" id="idm46183195131888"/>The <code>Transformer</code> Service delegates the work of prediction to the <code>Predictor</code> Service, which is responsible for loading the model from object storage and executing it using the provided feature data.</dd>&#13;
<dt>Post-processing</dt>&#13;
<dd><a contenteditable="false" data-primary="post-processing, Transformer Service and" data-type="indexterm" id="idm46183195123952"/>The <code>Transformer</code> Service receives the prediction result and performs any needed post-processing to prepare the response to the client application.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="" data-startref="kser_ab" data-type="indexterm" id="idm46183195083632"/><a contenteditable="false" data-primary="" data-startref="ml_kser" data-type="indexterm" id="idm46183195079920"/><a contenteditable="false" data-primary="" data-startref="rtm_kser" data-type="indexterm" id="idm46183195078544"/>If you are familiar with traditional web serving, you can see the helpful analog that model serving creates. Instead of serving HTML pages, KServe covers the modern application needs for serving AI/ML workloads. As a Kuberentes native project, it fits seamlessly into your cloud native datacenter and application stack.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Full Lifecycle Feature Management with Feast" data-type="sect2"><div class="sect2" id="fullen_dashlife_cycle_feature_managemen">&#13;
<h2>Full Lifecycle Feature Management with Feast</h2>&#13;
<p><a contenteditable="false" data-primary="Feast" data-type="indexterm" id="fea_ch"/><a contenteditable="false" data-primary="lifecycle management" data-secondary="with Feast" data-type="indexterm" id="lcm_fea"/>Lifecycle management is a common theme in any data architecture, encompassing how data is added, updated, and deleted over time. Feature stores serve a helpful coordination role by managing the lifecycle of features used by ML models from discovery to their use in production systems, eliminating the versioning and coordination issues that can arise when different teams are involved. How did Feast come to exist?</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="bridging_machine_learning_models_and_da">&#13;
<h5>Bridging ML Models and Data with Feast</h5>&#13;
<p><em>With Willem Pienaar, Principal Engineer, Tecton</em></p>&#13;
<p><a contenteditable="false" data-primary="ML (machine learning)" data-secondary="bridging models with Feast" data-type="indexterm" id="idm46183195146800"/><a contenteditable="false" data-primary="Pienaarm, Willem" data-type="indexterm" id="idm46183195066656"/>The Feast project was born from the experiences of the ML platform team at GoJek. After building out the core ML tooling, we realized our data scientists were struggling to get models into production. We needed a different kind of tooling to enable the data scientists to help themselves.</p> &#13;
&#13;
<p>The same operational rigor we applied to the deployment of traditional data infrastructure was also needed for ML infrastructure. These realizations led to the creation of what we now know as the Feast project. After observing emerging tools from other teams, especially what the Uber team had been doing with Michaelangelo, the idea of a feature store became a first-class priority for us.</p>&#13;
<p><a contenteditable="false" data-primary="GoJek" data-type="indexterm" id="idm46183195065680"/>To help understand what a feature store does, consider the problem space. GoJek had hundreds of millions of users using a variety of services including ride-hailing, food delivery, and digital payments. Each service had an element of ML, requiring many steps to go from the back-office data science team to production. We used tools like Flink to help with the large-scale SQL batch transformation and stream processing required for model creation, and systems like Redis and Cassandra to serve data, but there were remaining problems to operationalize our ML models. We needed a framework on top of those data systems unifying offline and online access, and so the concept of the feature store emerged.</p>&#13;
<p>Feature stores serve as a layer to give models a consistent way to access data, effectively providing a bridge between ML models and data within your organization. In production ML models there are two stages: the training and the online phases. Whether the data is coming from a stream, request data, or a data warehouse, your model can’t have different copies of data in different environments in each stage. During the training phase, a feature store manages scale requirements for data processing when computing data for model export, similar to other big data tools like Spark. In the online phase, the feature store provides low-latency, real-time access to models and, in some cases, derives features in real time, also known as <em>on-demand features</em>. Feast ensures the consistency of data for both phases, and it meets both online and offline requirements. Traditional database systems can provide only a subset of those features. For example, Cassandra supports many of the online features, but not offline scalability or specialized features like point-in-time correctness.</p>&#13;
<p><a contenteditable="false" data-primary="Google" data-type="indexterm" id="idm46183195230640"/>Feast began as a place to store computed features, but as we got further into the problem, we also needed to serve those features in production against our models in a consistent way, as an integrated part of our Job flow. As the Feast project grew, Google became a key collaborator, and within a few months, we had the first working parts of the project. The Kubeflow team at Google suggested we open source the project to make it available to a larger community. With the support of our management, we released a minimum viable product very quickly. So fast, in fact, we released Feast without a lot of things needed to help new users get started, like documentation!</p>&#13;
<p>Despite the minimal state, it became clear that Feast met a huge need as a community quickly formed around the project. Teams having similar issues with ML flows were coming to the same realization that they needed an assistant or data platform for operationalizing ML.</p>&#13;
<p><a contenteditable="false" data-primary="BigQuery" data-type="indexterm" id="idm46183195392080"/><a contenteditable="false" data-primary="Redshift" data-type="indexterm" id="idm46183195075712"/>In the early days of the project, deploying Feast in Kubernetes included a big stack of components. Today, Feast has evolved to be more lightweight; many of the extra components have been stripped away, making it more efficient and easier to manage. The best approach to building ML platforms on Kubernetes is to make your processing components as stateless as possible and store state externally. The registry doesn’t even have to be in Kubernetes since it can just be a file in an object store. Feast is frequently deployed alongside Redis or Cassandra inside Kubernetes and connected externally to data warehouses like BigQuery and Redshift. Providing external access to Feast is an important aspect. This is typically done using an Ingress to access the API server directly. In other cases, KServe is used as an intermediate serving layer to provide a scalable solution when a popular ML model is used by external services.</p>&#13;
<p>The future for Feast is to be more cloud native and fully integrated with Kubernetes. Quite a few challenges remain to be solved in deploying ML in Kubernetes, with the biggest being operational maturity. It still takes quite a bit of work for engineers to install many of the components, and the day two maintenance is more demanding than it should be. More community involvement will help grow the maturity of ML as an emerging part of the Kubernetes data stack.</p>&#13;
</div></aside>&#13;
<p>As Willem noted, the deployment of Feast on Kubernetes is at a basic state of maturity. As no operator or custom resources are defined, you install Feast using a Helm chart. <a data-type="xref" href="#deploying_feast_in_kubernetes">Figure 10-3</a> shows a sample installation using the <a href="https://oreil.ly/GYNjR">example documented on the Feast website</a>, which consists of the feature server and other supporting services.</p>&#13;
<figure><div class="figure" id="deploying_feast_in_kubernetes">&#13;
<img alt="Deploying Feast in Kubernetes" src="assets/mcdk_1003.png"/>&#13;
<h6><span class="label">Figure 10-3. </span>Deploying Feast in Kubernetes</h6>&#13;
</div></figure>&#13;
<p><a contenteditable="false" data-primary="Apache ZooKeeper" data-type="indexterm" id="idm46183195157024"/>Let’s examine these components and how they interact. Data scientists identify features from existing data sources in a process called <em>feature engineering</em> and create features using an interface exposed by the feature server (as defined in <a data-type="xref" href="#bridging_machine_learning_models_and_da">“Bridging ML Models and Data with Feast”</a>). The user can either provide feature data at the time of creating the feature or can connect to various backend services so that the data can be updated continuously. Feast can consume data published to Kafka topics, or through Kubernetes Jobs that pull data from an external source such as a data warehouse. The feature data is stored in an online database such as Redis or Cassandra so that it can be easily served to production applications. ZooKeeper is used to coordinate metadata and service discovery. The Helm chart also supports the ability to deploy Grafana for visualization of metrics. This may sound familiar to you, because the reuse of common building blocks like Redis, ZooKeeper, and Grafana is a pattern we’ve seen used in several other examples in this book.</p>&#13;
<p><a contenteditable="false" data-primary="KServe" data-type="indexterm" id="idm46183195561184"/>When model serving tools like KServe are asked to make predictions, they use the features stored in Feast as a record of truth. Any updated training by data scientists is done using the same feature store, eliminating the need for multiple sources of data. The <code>Transformation</code> Service provides an optional capability to generate new features on demand by performing transformations on existing feature data.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="fea_ch" data-type="indexterm" id="idm46183195110960"/><a contenteditable="false" data-primary="" data-startref="lcm_fea" data-type="indexterm" id="idm46183195080720"/>KServe and Feast are often used together to create a complete real-time model serving stack. Feast performs the dynamic part of feature management, working with online and offline data storage as new features arrive through streaming and data warehouses. KServe handles the dynamic provisioning for the model serving by using the serverless capabilities of Knative. This means that when not in use, KServe can scale to zero and react when new requests arrive, saving valuable resources in your Kubernetes-based AI/ML stack by using only what you need.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Vector Similarity Search with Milvus" data-type="sect2"><div class="sect2" id="vector_similarity_search_with_milvus">&#13;
<h2>Vector Similarity Search with Milvus</h2>&#13;
<p><a contenteditable="false" data-primary="Milvus" data-type="indexterm" id="mil_ab"/><a contenteditable="false" data-primary="VSS (vector similarity search), with Milvus" data-type="indexterm" id="vss_ab"/><a contenteditable="false" data-primary="KNN (k-nearest neighbors) algorithm" data-type="indexterm" id="idm46183195276528"/>Now that we’ve looked at tools that enable you to use ML models and features in production systems, let’s switch gears and look at a different type of AI/ML tool: vector similarity search (VSS). As discussed in <a data-type="xref" href="#aisolidusml_definitions">“AI/ML Definitions”</a>, a vector is a number object representing direction and magnitude from an origin in vector space. VSS is an application of vector mathematics in ML. The k-nearest neighbors (KNN) algorithm is a way to find how “close” two things are next to each other. This algorithm has many variations, but all rely on expressing data as a vector. The data to be searched is vectorized using a CPU-intensive KNN-type algorithm; typically, this is more of a backend process. VSS servers can then index the vector data for less CPU-intensive searching and provide a query mechanism that allows end users to provide a vector and find things that are close to it.</p>&#13;
<p><em>Milvus</em> is one of many servers designed around the emerging field of VSS. Let’s learn how Milvus came to exist and why it’s a great fit for Kubernetes.</p>&#13;
<aside class="pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="a_new_era_of_search_for_kubernetes_appl">&#13;
<h5 class="less_space">A New Era of Search for Kubernetes Applications</h5>&#13;
<p><em>With Xiaofan Luan, Director of Engineering, Zilliz, and Milvus Maintainer</em></p>&#13;
<p><a contenteditable="false" data-primary="FAISS (Facebook AI Similarity Search)" data-type="indexterm" id="idm46183195554800"/><a contenteditable="false" data-primary="HNSW (Hierarchical Navigable Small World)" data-type="indexterm" id="idm46183195553664"/><a contenteditable="false" data-primary="CNN (convolutional neural networks)" data-type="indexterm" id="idm46183195018672"/><a contenteditable="false" data-primary="applications" data-secondary="searching for" data-type="indexterm" id="idm46183195017696"/><a contenteditable="false" data-primary="Luan, Xiaofan" data-type="indexterm" id="idm46183195016480"/>There is a growing community around the newly emerging field of VSS, most notably in the use of libraries such as Facebook AI Similarity Search (FAISS) and Hierarchical Navigable Small World (HNSW). These libraries are used to take the output of computationally expensive ML algorithms and create end-user applications. Algorithms like convolutional neural networks (CNN) can take data including images and generate vectors that are simply a list of numbers. The real value of the analysis comes from what you do with that list of numbers.</p>&#13;
<p><a contenteditable="false" data-primary="Apache Lucene" data-type="indexterm" id="idm46183195141424"/><a contenteditable="false" data-primary="RDBMSs (relational database management systems)" data-type="indexterm" id="idm46183195014912"/>Structured data searching has been a standard feature of traditional relational database management systems (RDBMSs) in which all the values in columns are indexed for fast lookup. Projects like Apache Lucene built on this, making text search a new kind of competency for unstructured data. Users can provide all or part of the text they are searching for and get back multiple results with varying confidence values. Lucene is the engine for higher-level systems such as Apache Solr and Elasticsearch. Combined, they create a data server that is used in almost every kind of application now.</p>&#13;
<p><a contenteditable="false" data-primary="Apache Solr" data-type="indexterm" id="idm46183195026640"/><a contenteditable="false" data-primary="Elasticsearch" data-type="indexterm" id="idm46183195288880"/>Milvus was designed to fulfill a similar purpose as Solr and Elasticsearch. However, instead of working with only text, Milvus exposes a general-purpose VSS capability. It provides a top-level operational server for users who want more than just a library and need a system that can handle important details like durability, failure, and recovery. Milvus is a system that can be deployed and managed in Kubernetes to manage storage and helper features like computation disaggregation. Most importantly, it provides the Milvus API interface for application developers to use VSS in their code to do things that aren’t possible with previous databases.</p>&#13;
<p><a contenteditable="false" data-primary="YOLO (you only look once)" data-type="indexterm" id="idm46183195848080"/>To give an example of how this works, imagine a library containing photos of meals. Using an image analysis tool such as you only look once (YOLO), the objects in the images are separated into main dishes such as a sandwich and various side dishes like french fries. The next step is to process each object by using ResNet to extract its dimensions. The output of ResNet is a 256-dimensional vector for each item, which is then loaded into Milvus and assigned a unique ID. Milvus indexes the different objects so they can be accessed via its search interface. User-facing applications can provide a picture of a hamburger and fries and ask for similar meals based on the indexed images and the similarities.</p>&#13;
<p>Let’s compare this example to the experience of using a text search engine like Elasticsearch. To start, you would need a text description of each meal, which you would index using Lucene, and then you would be able to search for the words “french fries.” Similarly to the way Elasticsearch makes searching text easier, Milvus enables the searching of vectorized video, audio, and even natural language text.</p>&#13;
<p>Milvus 1.0 was deployed as a single node for storing, indexing, and serving data. This worked for anyone needing a simple package, but it wasn’t cloud native or Kubernetes friendly. For the 2.0 release, we decided that Milvus needed to change into a distributed architecture and become more cloud native, separating the compute elements from storage. Our goal was to make Milvus scale horizontally by independent function with an additional benefit of disaster recovery. Four layers are deployed in a Milvus cluster: the access layer, coordinator service, Worker Node, and storage nodes. Breaking Milvus into something similar to microservices reduces the reliance on state to only the storage nodes. The access layer, coordinator service, and Worker Nodes are stateless, making the system much easier to scale up and down and eliminating single points of failure. One of the essential features for the Milvus operator in the 2.0 release was the change to object storage and away from StatefulSets. With these updates, Kubernetes is now the preferred way to deploy Milvus.</p>&#13;
<p><a contenteditable="false" data-primary="LF AI &amp; Data Foundation" data-type="indexterm" id="idm46183195205040"/><a contenteditable="false" data-primary="FPGA (field-programmable gate arrays)" data-type="indexterm" id="idm46183195058192"/>Milvus is now a graduated project under the governance of the <a href="https://oreil.ly/BqwqQ">LF AI &amp; Data Foundation</a>. The projects in this foundation are all looking toward a cloud native future for data and the emergence of AI and ML as a core part of every application. The focus for Milvus post 2.0 is performance. Applications based on AI/ML require fast responses, and search is a speed-dependent operation. Code improvements are a more traditional way of gaining performance, but in the AI/ML world, hardware plays a big part as well. Taking advantage of GPUs or custom field-programmable gate array (FPGA) applications will again help developers take advantage of AI/ML performance advances using a simple API. Overall, we want to provide an easy path for people building cloud native applications to go from the leading edge to mainstream with a great experience.</p>&#13;
</div></aside>&#13;
<p>As Xiaofan mentions, Milvus supports both standalone and clustered deployments, using the four layers described. Both models are supported in Kubernetes via Helm, with the clustered deployment shown in <a data-type="xref" href="#deploying_milvus_in_kubernetes">Figure 10-4</a>.</p>&#13;
&#13;
<p>The access layer contains the proxy Service, which uses a Kubernetes LoadBalancer to route requests from client applications. The services in the coordination layer handle incoming search and index queries, routing them to the core server components in the worker layer that handle queries and manage data storage and indexing. The data nodes manage persistence via files in object storage. The message storage uses Apache Pulsar or Apache Kafka to store the stream of incoming data that is then passed to data nodes.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="cn_ab" data-type="indexterm" id="idm46183195047648"/><a contenteditable="false" data-primary="" data-startref="ml_cn" data-type="indexterm" id="idm46183197402928"/><a contenteditable="false" data-primary="" data-startref="mil_ab" data-type="indexterm" id="idm46183195566656"/><a contenteditable="false" data-primary="" data-startref="vss_ab" data-type="indexterm" id="idm46183195718944"/>As you can see, Milvus is designed to be Kubernetes native, with a horizontally scalable architecture that makes it well poised to scale up to massive data sets including billions or even trillions of vectors.</p>&#13;
<figure><div class="figure" id="deploying_milvus_in_kubernetes">&#13;
<img alt="Deploying Milvus in Kubernetes" src="assets/mcdk_1004.png"/>&#13;
<h6><span class="label">Figure 10-4. </span>Deploying Milvus in Kubernetes</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Efficient Data Movement with Apache Arrow" data-type="sect1"><div class="sect1" id="efficient_data_movement_with_apache_ar">&#13;
<h1>Efficient Data Movement with Apache Arrow</h1>&#13;
<p><a contenteditable="false" data-primary="Apache Arrow, efficient data movement with" data-type="indexterm" id="aa_ab"/><a contenteditable="false" data-primary="data" data-secondary="efficient movement with Apache Arrow" data-type="indexterm" id="data_aa"/>Now that we have explored an AI/ML Kubernetes stack that helps you manage compute resources more efficiently, you might be wondering what can be done with network resources. The “fallacies of distributed computing” we discussed in <a data-type="xref" href="ch01.html#embrace_distributed_computing">“Embrace Distributed Computing”</a> include two important points: the fallacies of believing that bandwidth is infinite and that transport cost is zero. Even when compute and storage resources seem much more finite, it’s easy to forget how easily you can run out of bandwidth. The deeper you get into deploying your data infrastructure into Kubernetes, the more likely it is you will find out. Early adopters of Apache Hadoop often shared that as their clusters grew, their network switches needed to be replaced with the best that could be purchased at the time. Just consider what it takes to sort 10 terabytes of data. How about 1 petabyte? You get the idea.</p>&#13;
<p><a contenteditable="false" data-primary="EBCDIC (Extended Binary Coded Decimal Interchange Code)" data-type="indexterm" id="idm46183195495936"/><a contenteditable="false" data-primary="IBM" data-type="indexterm" id="idm46183195577888"/><em>Apache Arrow</em> is a project that addresses the problem of bandwidth utilization by providing a more efficient format. This actually isn’t an unknown approach in the history of computer science. IBM introduced <a href="https://oreil.ly/MI488">Extended Binary Coded Decimal Interchange Code (EBCDIC)</a> character encoding to create efficiency for the preferred transport of the time: the punch card. Arrow attacks the problem of efficiency from the ground up in order to avoid the endless upgrading to add more resources, proving that the solution to a control problem is never “add more power.” Let’s hear from some experts to learn how this works.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="efficient_data_movement_with_apache_arr">&#13;
<h5>Efficient Data Movement with Apache Arrow</h5>&#13;
<p><em>With Josh Patterson, CEO, Voltron Data, and Keith Kraus, VP of Product, Voltron Data</em></p>&#13;
<p><a contenteditable="false" data-primary="Kraus, Keith" data-type="indexterm" id="idm46183195416144"/><a contenteditable="false" data-primary="Patterson, Josh" data-type="indexterm" id="idm46183194973440"/><a contenteditable="false" data-primary="McKinney, Wes" data-type="indexterm" id="idm46183194972464"/><a contenteditable="false" data-primary="Nadeau, Jacques" data-type="indexterm" id="idm46183194971424"/>As big data technologies like Spark, Kudu, and Cassandra made it possible to move larger amounts of data between systems, it became clear that the computational and performance cost of serializing and deserializing data was getting too high. Wes McKinney and Jacques Nadeau, along with others, made a bid to address this problem with a project called <a href="https://arrow.apache.org">Apache Arrow</a>. Arrow provides a standard way to represent the layout of data so systems can share that data with fewer headaches.</p>&#13;
<p>Arrow uses an in-memory columnar format—that is, data arranged in a tabular format of rows and columns. In traditional relational databases, each record is represented as a row with multiple columns. Arrow pivots this arrangement: data is organized in sequentially ordered columns. This provides significant advantages when searching and processing large amounts of data, especially because of how it aligns with modern CPU architectures.</p>&#13;
<p><a contenteditable="false" data-primary="RPC (remote procedure call)" data-type="indexterm" id="idm46183194969808"/>Arrow Flight is a subproject to bring the same efficiency we see in processing to network communications. Highly connected distributed systems consume network resources quickly, and any efficiency gains quickly make a big difference at high volumes. Flight is a remote procedure call (RPC) layer that drastically reduces resource utilization for communications between data services by eliminating serialization costs. Flight uses gRPC for network efficiency, which enables it to send data in parallel using multiple channels. Using Arrow in all of your Kubernetes native analytics stack reduces resource usage and therefore cost.</p>&#13;
<p>Arrow doesn’t just provide benefits for network utilization; it also has promise for more efficient compute processing for AI/ML workloads. Arrow provides a fast access pattern for data analytics and tabular data that Kubernetes applications can take advantage of. Arrow was one of the first projects in the data analytics space to encourage users to think carefully about the usage of memory and processing hardware, and this timing has coincided nicely with the rise of deep learning. Kubernetes native analytics workloads powered by Arrow will help keep costs low while allowing higher processing volumes.</p>&#13;
<p><a contenteditable="false" data-primary="RDMA (Remote Direct Memory Access)" data-type="indexterm" id="idm46183195269248"/>In fact, Kubernetes was a key driver that moved the Arrow project forward. As the GPU-accelerated stack was being defined around 2018, Kubernetes was emerging as an industry standard, replacing Hadoop YARN as the leading resource management tool for big data processing. The Kubernetes community was developing key features more rapidly, like support for the remote direct memory access (RDMA) protocol and topology awareness of nodes containing GPUs. Kubernetes also supported faster SLAs for cluster operations. With modern GPUs offering 50 times faster processing times, the Job of analyzing dozens of terabytes might take 5 minutes, while scheduling and provisioning the machines with YARN to perform that Job could take 10 minutes. The auto-scaling abilities in Kubernetes offered the right reaction time to match these cyclical workloads. New advanced schedulers such as YuniKorn and Volcano now make those operations even faster and more efficient.</p>&#13;
<p>Finding ways to take advantage of new hardware technology is a critical part of our battle to keep up with the ever-increasing volumes of data created. The trend toward using GPUs for big data processing is already increasing, and adopting Arrow will only make this easier. In fact, the effect on the community has already reached a tipping point. With the momentum of GPUs adopting the Arrow format, data tools have started adopting Arrow for compatibility, helping to cement Arrow as a data interoperability standard. Arrow could be more than just a language-agnostic connector; it could be a hardware connector. We’ve come to believe that an increasing number of systems will become Arrow native in the near future.</p>&#13;
<p>The data and analytics ecosystem will continue to drive the future of Kubernetes and Arrow. Frameworks like Dask and Ray use Python as their underlying compute library, with Arrow used as the format within Pandas DataFrames sent over the wire between workers. Getting your tabular data efficiently over the wire is a huge benefit, and Arrow provides an easy-to-implement standard that is completely interchangeable, open, and widely adopted. It allows future tool developers to focus on the special thing they are building and less on optimizing interconnect.</p>&#13;
<p><a contenteditable="false" data-primary="SIMD (single instruction, multiple data) vectorization" data-type="indexterm" id="idm46183195835648"/><a contenteditable="false" data-primary="Parquet" data-type="indexterm" id="idm46183195669040"/>The Arrow community has become a center of gravity attracting large and innovative projects.  The data and analytics community has a pattern of rebuilding new infrastructure about every 10 years. This time the revolution is defined not by starting over, but by refining the things that we have, biased toward optimizing the primitives. Arrow provides a modular building block that can be used, optimized, extended, and composed with multiple other systems. The groundwork for the next 10 years of data infrastructure can start on a sound foundation learned from the mistakes of the past decade. Then we can focus on problems like improving Parquet, using single instruction, multiple data (SIMD) vectorization, or building storage that could be compacted tightly and quickly. Arrow can be a big part of these solutions because it touches so many systems. Even though its focus on the way we represent data is simple, minor improvements here can have massive ripple effects on our cloud native future.</p>&#13;
</div></aside>&#13;
<p>Using Arrow-enabled projects enables you to share data efficiently, reducing your resource usage across compute, network, and storage. Example usage of Arrow with Spark is shown in <a data-type="xref" href="#moving_data_with_apache_arrow">Figure 10-5</a>.</p>&#13;
<figure><div class="figure" id="moving_data_with_apache_arrow">&#13;
<img alt="Moving data with Apache Arrow" src="assets/mcdk_1005.png"/>&#13;
<h6><span class="label">Figure 10-5. </span>Moving data with Apache Arrow</h6>&#13;
</div></figure>&#13;
<p>Parquet datafiles containing Arrow-formatted data persisted to object storage can be easily loaded without a deserialization step (1). The data can then be analyzed by a Spark application (2), including loading directly into a GPU for processing where available. The same efficiency level is maintained when passing data between Worker Nodes using Arrow Flight (3). The Arrow record batch is sent without any intermediate memory copying or serialization, and the receiver can reconstruct the Arrow record without memory copy or deserialization. The efficient relationship between the remote processes eliminates two things: processing overhead for sending data and the efficient Arrow record format that eliminates wasted bandwidth.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="aa_ab" data-type="indexterm" id="idm46183194930864"/><a contenteditable="false" data-primary="" data-startref="data_aa" data-type="indexterm" id="idm46183195163744"/>At the scale common in Spark applications, the effect on network latency and bandwidth can add up quickly. The network transport savings really keep your data moving, even when volumes reach into terabytes and petabytes. <a href="https://oreil.ly/rve9i">Research</a> performed by Tanveer Ahmad at TU Delft showed a 20 to 30 times efficiency gain using Arrow Flight to move large volumes of data.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Versioned Object Storage with lakeFS" data-type="sect1"><div class="sect1" id="versioned_object_storage_with_lakefs">&#13;
<h1>Versioned Object Storage with lakeFS</h1>&#13;
<p><a contenteditable="false" data-primary="lakeFS, versioned object storage with" data-type="indexterm" id="lake_ab"/><a contenteditable="false" data-primary="versioned object storage, with lakeFS" data-type="indexterm" id="vos_ab"/>Object storage is becoming the standard for cloud native data persistence. It lowers the complexity for services but also points to a different way of thinking about data mutability. Instead of opening a file and providing random access, file storage is precomputed, written once, and read many times. Instead of updating a datafile, you write a new one, but how do you distinguish which datafiles are current? For this reason, object storage presents issues with disk space management. Since there is no concept of managing an entire filesystem, each file is an object in a virtually infinite resource.</p>&#13;
<p><a contenteditable="false" data-primary="Nessie" data-type="indexterm" id="idm46183196516000"/>Object storage APIs are fairly basic with few frills, but data teams need more than just the basics for their use cases. <a href="https://lakefs.io">lakeFS</a> and <a href="https://projectnessie.org">Nessie</a> are two projects trying to make object storage a better fit for emerging workloads on Kubernetes. Let’s examine how lakeFS extends the functionality of object storage for cloud native applications.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="data_integrity_to_let_you_sleep_at_nigh">&#13;
<h5>Data Integrity to Let You Sleep at Night</h5>&#13;
<p><em>With Adi Polak, VP of Developer Experience, Treeverse</em></p>&#13;
<p><a contenteditable="false" data-primary="data" data-secondary="integrity of" data-type="indexterm" id="idm46183194952528"/><a contenteditable="false" data-primary="Polak. Adi" data-type="indexterm" id="idm46183194951216"/>In working as a full-time engineer building big data infrastructure, there were many times I had to manually change data in object storage in our production environment. This task became even more challenging when using complex data formats such as Parquet. On one occasion, I needed to delete some datafiles to resolve a production issue. Unfortunately, I accidentally deleted the wrong data. That meant 20 hours in the office with a very grumpy DevOps team trying to recover the data from backup because, of course, it was customer data. At least in this case, we were aware of the issue. What’s even more concerning are the silent failures that impact data products without us even being aware.</p>&#13;
<p>These problems occur frequently today because our systems are too fragile. We are biased toward action, but we’re human and therefore have a tendency to make mistakes. The result is that bad things happen to good data.</p>&#13;
<p>How does lakeFS help with situations like the one I’ve described? The simplest way to describe lakeFS is that it enables Git-like capabilities for object storage. It allows engineers to perform familiar actions like branch, commit, merge, and revert. This creates new options for the way you use data and enhances workflows.</p>&#13;
<p>For example, a typical use case for using lakeFS is continuous integration/continuous deployment (CI/CD) flows. Data engineers frequently need to reproduce some portion of a data pipeline over different versions of the data, which requires branching. When running on Kubernetes, multiple containers can potentially run the same code over different versions of data. Branching data on the object store creates an isolated environment for experimentation. If there is a mistake in the branch, you can simply revert. This provides the ability to experiment at low cost without harming the original data, which builds trust and allows teams to move faster with safety.</p>&#13;
<p><a contenteditable="false" data-primary="Karau, Holden" data-type="indexterm" id="idm46183195052336"/><a contenteditable="false" data-primary="ACID (atomicity, consistency, isolation, durability) transactions" data-type="indexterm" id="idm46183195232208"/>Another example is trying out a new application to see how it fits into the bigger data flow. Git semantics on data can make a massive difference in complicated scenarios that are typically hard to test. In <a data-type="xref" href="ch09.html#analytics_on_kubernetes_is_the_next_fro">“Analytics on Kubernetes Is the Next Frontier”</a>, Holden Karau addressed the difficulty in testing big data applications. It’s almost impossible to mimic production flows in development or staging environments because of the variety and production data volume. With lakeFS, you can use branching to test with multiple data versions, duplicating the variety and volume seen in production and building confidence in what is being built.</p>&#13;
<p>To integrate with your environment, lakeFS exposes an S3-compatible API endpoint through a stateless service. However, it doesn’t actually serve as storage. LakeFS forks data commands from your application. Loading data to lakeFS is a metadata operation that creates your main branch in lakeFS by creating pointers to the physical data in your S3 bucket. Any additional branch is an atomic metadata operation pointing to the same data as main when created from it. The metadata created by lakeFS is saved to your S3. As long as the user application is using the lakeFS API endpoint, all Git functionality is available. If users want to stop using lakeFS at any point, the original data storage is unaltered and can be used directly by changing the endpoint address used by your application. To roll up any changes while using lakeFS, an offboarding script is available to synchronize any changes before taking lakeFS out of the path. This makes it easy for users to try lakeFS and then adopt or move on without the need to move existing data. The design of lakeFS enables seamless integrations with other parts of your data infrastructure such as Apache Iceberg, Apache Hudi, or Delta Lake, providing the added features of branch, commit, rollback, and merge.</p>&#13;
<p>lakeFS addresses the lack of <a href="https://oreil.ly/mjGnN">atomicity, consistency, isolation, durability (ACID) transactions</a> across multiple systems by providing the ability to have versioned object storage. The consistency level guarantees passthrough from the originating application. However, when multitable operations are performed on an isolated branch, the merge function across all tables is atomic, achieving cross-table consistency.</p>&#13;
<p>We are at an interesting intersection point for data workloads on Kubernetes. Many developers who have been working with distributed data workloads for years think in terms of the Hadoop ecosystem. Now we’re actually bringing in a different type of developer: the application developer who works with Kubernetes. A potential for more friction and errors exists since these developers are not always aware of the infrastructure and the way things have traditionally worked in the big data world.</p>&#13;
<p>Kubernetes is now being used to orchestrate the systems that process data and turn it into products for sale. If the data is not protected, your business is at risk. Organizations need to be able to audit, save, and deliver data reliably, even if it is at a lower SLA. lakeFS is a great fit for Kubernetes deployment because it assumes that the complexity of distributed systems and distributed data will lead to many mistakes around data. That assumption is met with the assurance that any mistake is easily fixed and never devastating, leading to a great night of sleep for your DevOps teams.</p>&#13;
</div></aside>&#13;
<p>Using lakeFS in Kubernetes is a great fit because of its stateless design and declarative deployment. A Helm deployment consists of <a href="https://oreil.ly/GhZMB">configuring</a> the lakeFS service, which then serves as a communication gateway to and from other services.</p>&#13;
<p class="pagebreak-before">Communications into the server emulate S3 object storage, enabling interaction with any data store that supports the S3 API. Incoming communication is bound as a ClusterIP to serve HTTP traffic across one or more stateless lakeFS server Pods managed by a Deployment.</p>&#13;
<p><a contenteditable="false" data-primary="PostgreSQL operators" data-type="indexterm" id="idm46183195118960"/>lakeFS uses PostgreSQL to manage metadata, so users can either provide the endpoint for a running system, as shown in <a data-type="xref" href="#deploying_lakefs_in_kubernetes">Figure 10-6</a>, or lakeFS can run an embedded PostgreSQL server inside the lakeFS Pod for its exclusive use. PostgreSQL is the state management for the stateless lakeFS servers when deployed as a cluster.</p>&#13;
<figure><div class="figure" id="deploying_lakefs_in_kubernetes">&#13;
<img alt="Deploying lakeFS in Kubernetes" src="assets/mcdk_1006.png"/>&#13;
<h6><span class="label">Figure 10-6. </span>Deploying lakeFS in Kubernetes</h6>&#13;
</div></figure>&#13;
<p>The most important connection is to the object storage endpoints that will store the actual data. When users persist data to lakeFS, the actual datafile will pass through to the backend object storage, and versioning metadata is stored in PostgreSQL.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="lake_ab" data-type="indexterm" id="idm46183195653536"/><a contenteditable="false" data-primary="" data-startref="vos_ab" data-type="indexterm" id="idm46183195037936"/>The additional outbound connection is for providing orchestration with other ML infrastructure. Webhooks allow for triggers on action that alert downstream systems when something such as a commit is issued. These triggers serve as a key ingredient to automated ML workflows and other applications.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000009">&#13;
<h1>Summary</h1>&#13;
<p>As you can see, the pipeline of new and exciting ways to work with data in Kubernetes extends well into the future. New projects are addressing the challenges of advanced data workloads according to the cloud native principles of elasticity, scalability, and self-healing.</p>&#13;
<p>These tools give you the ability to manage the critical resources of compute, network and storage. You can better manage compute-intensive workloads such as AI/ML with KServe for the delivery, Feast for model management, and Milvus to operationalize new search methods. Network resources are ruled by the simple laws of volume and speed, and at the volumes of data we can create, every little bit helps. Apache Arrow reduces this volume by creating a common reference frame across applications. Unifying around object storage provides further efficiencies, with tools like lakeFS making object storage easier to consume in ways that are sympathetic to application data storage needs.</p>&#13;
<p>At this point, we’ve examined data infrastructure on Kubernetes from mature areas like storage all the way out to cutting-edge projects for managing AI/ML artifacts such as models and features. Now it’s time to take all the knowledge you’ve gained so far and plan to put it into practice.</p>&#13;
</div></section>&#13;
</div></section></body></html>