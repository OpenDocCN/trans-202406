- en: Chapter 2\. Developer Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes was built for reliably operating software. It simplifies deploying
    and managing applications with an application-oriented API, self-healing properties,
    and useful tools like Deployments for zero downtime rollout of software. Although
    all these tools are useful, they don’t do much to make it easier to develop applications
    for Kubernetes. This is where developer workflows come into play. Even though
    many clusters are designed to run production applications and thus are rarely
    accessed by developer workflows, it is critical to enable development workflows
    to target Kubernetes, and this typically means having a cluster or at least part
    of a cluster that is intended for development. Setting up such a cluster to facilitate
    easy development of applications for Kubernetes is critical to ensuring success
    with Kubernetes. If there is no code being built for your cluster, the cluster
    itself isn’t accomplishing much.
  prefs: []
  type: TYPE_NORMAL
- en: Goals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we describe the best practices for building out development clusters,
    it is worth stating our goals for such clusters. Obviously, the ultimate goal
    is to enable developers to rapidly and easily build applications on Kubernetes,
    but what does that really mean in practice, and how is that reflected in practical
    features of the development cluster?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this, let’s start by identifying phases of developer interaction with
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The first phase is *onboarding*. This is when a new developer joins the team.
    This phase includes giving the user a login to the cluster as well as getting
    them oriented to their first deployment. The goal for this phase is to get a developer’s
    feet wet in a minimal amount of time. You should set a key performance indicator
    (KPI) goal for this process. A reasonable goal would be that a user could go from
    nothing to the current application at HEAD running in less than half an hour.
    Every time someone is new to the team, test how you are doing against this goal.
  prefs: []
  type: TYPE_NORMAL
- en: The second phase is *developing*. This is the day-to-day activity of the developer.
    The goal for this phase is to ensure rapid iteration and debugging. Developers
    need to quickly and repeatedly push code to the cluster. They also need to be
    able to easily test their code and debug it when it isn’t operating properly.
    The KPI for this phase is more challenging to measure, but you can estimate it
    by measuring the time to get a pull request (PR) or change up and running in the
    cluster, or with surveys of the user’s perceived productivity, or both. You will
    also be able to measure this in the overall productivity of your teams.
  prefs: []
  type: TYPE_NORMAL
- en: The third phase is *testing*. This phase is interweaved with developing and
    is used to validate the code before submission and merging. The goals for this
    phase are two-fold. First, the developer should be able to run all tests for their
    environment before a PR is submitted. Second, all tests should automatically run
    before code is merged into the repository. In addition to these goals you should
    also set a KPI for the length of time the tests take to run. As your project becomes
    more complex, it’s natural for more and more tests to take a longer time. As this
    happens, it might become valuable to identify a smaller set of smoke tests that
    a developer can use for initial validation before submitting a PR. You should
    also have a very strict KPI around *test flakiness*. A flaky test is one that
    occasionally (or not so occasionally) fails. In any reasonably active project,
    a flakiness rate of more than one failure per one thousand runs will lead to developer
    friction. You need to ensure that your cluster environment does not lead to flaky
    tests. Whereas sometimes flaky tests occur due to problems in the code, they can
    also occur because of interference in the development environment (e.g., running
    out of resources and noisy neighbors). You should ensure that your development
    environment is free of such issues by measuring test flakiness and acting quickly
    to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Development Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When people begin to think about developing on Kubernetes, one of the first
    choices that occurs is whether to build a single large development cluster or
    to have one cluster per developer. Note that this choice only makes sense in an
    environment in which dynamic cluster creation is easy, such as the public cloud.
    In physical environments, it’s possible that one large cluster is the only choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do have a choice, you should consider the pros and cons of each option.
    If you choose to have a development cluster per user, the significant downside
    of this approach is that it will be more expensive and less efficient, and you
    will have a large number of different development clusters to manage. The extra
    costs come from the fact that each cluster is likely to be heavily underutilized.
    Also, with developers creating different clusters, it becomes more difficult to
    track and garbage-collect resources that are no longer in use. The advantage of
    the cluster-per-user approach is simplicity: each developer can self-service manage
    their own cluster, and from isolation, it’s much more difficult for different
    developers to step on one another’s toes.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a single development cluster will be significantly more efficient;
    you can likely sustain the same number of developers on a shared cluster for one-third
    the price (or less). Plus, it’s much easier for you to install shared cluster
    services, for example, monitoring and logging, which makes it significantly easier
    to produce a developer-friendly cluster. The downside of a shared development
    cluster is the process of user management and potential interference between developers.
    Because the process of adding new users and namespaces to the Kubernetes cluster
    isn’t currently streamlined, you will need to activate a process to onboard new
    developers. Although Kubernetes resource management and Role-Based Access Control
    (RBAC) can reduce the probability that two developers conflict, it is always possible
    that a user will *brick* the development cluster by consuming too many resources
    so that other applications and developers won’t schedule. Additionally, you will
    still need to ensure that developers don’t leak and forget about resources they’ve
    created. This is somewhat easier, though, than the approach in which developers
    each create their own clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Even though both approaches are feasible, generally, our recommendation is to
    have a single large cluster for all developers. Although there are challenges
    in interference between developers, they can be managed, and ultimately the cost
    efficiency and ability to easily add organization-wide capabilities to the cluster
    outweigh the risks of interference. But you will need to invest in a process for
    onboarding developers, resource management, and garbage collection. Our recommendation
    would be to try a single large cluster as a first option. As your organization
    grows (or if it is already large), you might consider having a cluster per team
    or group (10 to 20 people) rather than a giant cluster for hundreds of users.
    This can make both billing and management easier. Moving to multiple clusters
    can make it more complicated to ensure consistency, but tools like fleet management
    can make it easier to manage groups of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a Shared Cluster for Multiple Developers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When setting up a large cluster, the primary goal is to ensure that multiple
    users can simultaneously use the cluster without stepping on one another’s toes.
    The obvious way to separate your different developers is with Kubernetes namespaces.
    Namespaces can serve as scopes for the deployment of services so that one user’s
    frontend service doesn’t interfere with another user’s frontend service. Namespaces
    are also scopes for RBAC, ensuring that one developer cannot accidentally delete
    another developer’s work. Thus, in a shared cluster it makes sense to use a namespace
    as a developer’s workspace. The processes for onboarding users and creating and
    securing a namespace are described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Onboarding Users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you can assign a user to a namespace, you have to onboard that user to
    the Kubernetes cluster itself. To achieve this, there are two options. You can
    use certificate-based authentication to create a new certificate for the user
    and give them a *kubeconfig* file that they can use to log in, or you can configure
    your cluster to use an external identity system (for example, Microsoft Entra
    ID or AWS Identity and Access Management [IAM]) for cluster access.
  prefs: []
  type: TYPE_NORMAL
- en: In general, using an external identity system is a best practice because it
    doesn’t require that you maintain two different sources of identity. Additionally,
    most external systems use short-lived tokens rather than long-lived certificates
    so the accidental disclosure of a token has a time-bound security impact. If at
    all possible you should restrict your developers to proving their identity via
    an external identity provider.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, in some cases this isn’t possible and you need to use certificates.
    Fortunately, you can use the Kubernetes certificate API for creating and managing
    such certificates. Here’s the process for adding a new user to an existing cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to generate a certificate-signing request to generate a new
    certificate. Here is a simple Go program to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates files called *client-key.pem* and *client.csr*. You can then run
    the following script to create and download a new certificate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This script prints out the final information that you can add to a *kubeconfig*
    file to enable that user. Of course, the user has no access privileges, so you
    will need to apply Kubernetes RBAC for the user to grant them privileges to a
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Securing a Namespace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in provisioning a namespace is actually just creating it. You
    can do this using **`kubectl create namespace my-namespace`**.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the truth is that when you create a namespace, you want to attach a bunch
    of metadata to that namespace, for example, the contact information for the team
    that builds the component deployed into the namespace. Generally, this is in the
    form of annotations; you can either generate the YAML file using some templating,
    such as [Jinja](https://oreil.ly/vvtTF) or others, or you can create and then
    annotate the namespace. A simple script to do this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When the namespace is created, you want to secure it by ensuring that you can
    grant access to the namespace to a specific user. To do this, you can bind a role
    to a user in the context of that namespace. You do this by creating a `RoleBinding`
    object within the namespace itself. The `RoleBinding` might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To create it, you simply run `kubectl create -f role-binding.yaml`. Note that
    you can reuse this binding as much as you want as long as you update the namespace
    in the binding to point to the correct namespace. If you ensure that the user
    doesn’t have any other role bindings, you can be assured that this namespace is
    the only part of the cluster to which the user has access. A reasonable practice
    is to also grant reader access to the entire cluster; in this way developers can
    see what others are doing in case it is interfering with their work. Be careful
    in granting such read access, however, because it will include access to secret
    resources in the cluster. Generally, in a development cluster this is OK because
    everyone is in the same organization and the secrets are used only for development;
    however, if this is a concern, then you can create a more fine-grained role that
    eliminates the ability to read secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to limit the resources consumed by a particular namespace to put
    a cap on costs or ensure that resources are fairly distributed among developers,
    you can use the ResourceQuota resource to set a limit to the total number of resources
    that any particular namespace consumes. For example, the following quota limits
    the namespace to 10 cores and 100 GB of memory for both Request and Limit for
    the pods in the namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Managing Namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have seen how to onboard a new user and how to create a namespace
    to use as a workspace, the question remains how to assign a developer to the namespace.
    As with many things, there is no single perfect answer; rather, there are two
    approaches. The first is to give each user their own namespace as part of the
    onboarding process. This is useful because after a user is onboarded, they always
    have a dedicated workspace in which they can develop and manage their applications.
    However, making the developer’s namespace too persistent encourages the developer
    to leave things lying around in the namespace after they are done with them, and
    garbage-collecting and accounting individual resources is more complicated. An
    alternate approach is to temporarily create and assign a namespace with a bounded
    time to live (TTL). This ensures that the developer thinks of the resources in
    the cluster as transient and that it is easy to build automation around the deletion
    of entire namespaces when their TTL has expired.
  prefs: []
  type: TYPE_NORMAL
- en: In the bounded TTL model, when the developer wants to begin a new project, they
    use a tool to allocate a new namespace for the project. When they create the namespace,
    it has a selection of metadata associated with the namespace for management and
    accounting. Obviously, this metadata includes the TTL for the namespace, but it
    also includes the developer to which it is assigned, the resources that should
    be allocated to the namespace (e.g., CPU and memory), and the team and purpose
    of the namespace. This metadata ensures that you can both track resource usage
    and delete the namespace at the right time.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the tooling to allocate namespaces on demand can seem like a challenge,
    but simple tooling is relatively easy to develop. For example, you can achieve
    the allocation of a new namespace with a simple script that creates the namespace
    and prompts for the relevant metadata to attach to the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to get more integrated with Kubernetes, you can use custom resource
    definitions (CRDs) to enable users to dynamically create and allocate new namespaces
    using the `kubectl` tool. If you have the time and inclination, this is definitely
    a good practice because it makes namespace management declarative and also enables
    the use of Kubernetes RBAC.
  prefs: []
  type: TYPE_NORMAL
- en: After you have tooling to enable the allocation of namespaces, you also need
    to add tooling to reap namespaces when their TTL has expired. Again, you can accomplish
    this with a simple script that examines the namespaces and deletes those that
    have an expired TTL.
  prefs: []
  type: TYPE_NORMAL
- en: You can build this script into a container and use a `ScheduledJob` to run it
    at an interval like once per hour. These combined tools can ensure that developers
    can easily allocate independent resources for their project as needed, but those
    resources will also be reaped at the proper interval to ensure that you don’t
    have wasted resources and that old resources don’t get in the way of new development.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster-Level Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to using tooling to allocate and manage namespaces, there are also
    useful cluster-level services, and it’s a good idea to enable them in your development
    cluster. The first is log aggregation to a central Logging as a Service (LaaS)
    system. One of the easiest things for a developer to do to understand the operation
    of their application is to write something to STDOUT. Although you can access
    these logs via `kubectl logs`, that log is limited in length and is not particularly
    searchable. If you instead automatically ship those logs to a LaaS system such
    as a cloud service or an Elasticsearch cluster, developers can easily search through
    logs for relevant information as well as aggregate logging information across
    multiple containers in their service.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Developer Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have successfully set up a shared cluster and we can onboard new
    application developers to the cluster itself, we need to actually get them developing
    their application. Remember that one of the KPIs we are measuring is the time
    from onboarding to an initial application running in the cluster. It’s clear that
    via the just-described onboarding scripts we can quickly authenticate a user to
    a cluster and allocate a namespace, but what about getting started with the application?
    Unfortunately, even though a few techniques can help with this process, it generally
    requires more convention than automation to get the initial application up and
    running. In the following sections, we describe one approach to achieving this;
    it is by no means the only approach or the only solution. You can optionally apply
    the approach as is or be inspired by the ideas to arrive at your own solution.
  prefs: []
  type: TYPE_NORMAL
- en: Initial Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main challenges to deploying an application is the installation of
    all the dependencies. In many cases, especially in modern microservice architectures,
    to even get started developing on one of the microservices requires the deployment
    of multiple dependencies, either databases or other microservices. Although the
    deployment of the application itself is relatively straightforward, the task of
    identifying and deploying all the dependencies to build the complete application
    is often a frustrating case of trial and error married with incomplete or out-of-date
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, it is often valuable to introduce a convention for describing
    and installing dependencies. This can be seen as the equivalent of something like
    `npm install`, which installs all the required JavaScript dependencies. Eventually,
    there is likely to be a tool similar to `npm` that provides this service for Kubernetes-based
    applications, but until then, the best practice is to rely on convention within
    your team.
  prefs: []
  type: TYPE_NORMAL
- en: 'One such option for a convention is the creation of a *setup.sh* script within
    the root directory of all project repositories. The responsibility of this script
    is to create all dependencies within a particular namespace to ensure that all
    the application’s dependencies are correctly created. For example, a setup script
    might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You could then integrate this script with npm by adding the following to your
    *package.json*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With this setup, a new developer can simply run `npm run setup`, and the cluster
    dependencies will be installed. Obviously, this particular integration is Node.js/npm
    specific. In other programming languages, it will make more sense to integrate
    with the language-specific tooling. For example, in Java you might integrate with
    a Maven *pom.xml* file instead.
  prefs: []
  type: TYPE_NORMAL
- en: For more generic workflows, both GitHub and Visual Studio Code have recently
    standardized on “devcontainers,” which are containers that are described by a
    Dockerfile stored in the `.devcontainer/` folder in the repository. When built,
    they construct a complete environment for starting development on that repository.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Active Development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having set up the developer workspace with the required dependencies, the next
    task is to enable developers to iterate on their application quickly. The first
    prerequisite for this is the ability to build and push a container image. Let’s
    assume that you have this already set up; if not, you can read how to do this
    in a number of other online resources and books.
  prefs: []
  type: TYPE_NORMAL
- en: After you have built and pushed a container image, the task is to roll it out
    to the cluster. Unlike traditional rollouts, in the case of developer iteration,
    maintaining availability is really not a concern. Thus, the easiest way to deploy
    new code is to simply delete the Deployment object associated with the previous
    Deployment and then create a new Deployment pointing to the newly built image.
    It is also possible to update an existing Deployment in place, but this will trigger
    the rollout logic in the Deployment resource. Although it is possible to configure
    a Deployment to roll out code quickly, doing so introduces a difference between
    the development environment and the production environment that can be dangerous
    or destabilizing. Imagine, for example, that you accidentally push the development
    configuration of the Deployment into production; you will suddenly deploy new
    versions to production without appropriate testing and delays between phases of
    the rollout. Because of this risk and because there is an alternative, the best
    practice is to delete and recreate the Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like installing dependencies, it is also a good practice to make a script
    for performing this Deployment. An example *deploy.sh* script might look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As before, you can integrate this with existing programming language tooling
    so that (for example) a developer can simply run `npm run deploy` to deploy their
    new code into the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As you build this automation it is often useful to integrate it into a continuous
    integration and delivery (CI/CD) tool such as GitHub Actions, Azure DevOps, or
    Jenkins. Integration with a CI/CD tool makes it much easier to enable further
    automation like automatic deployment on merging a developer’s PR.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Testing and Debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a user has successfully deployed the development version of their application,
    they need to test it and, if there are problems, debug any issues with the application.
    This can also be a hurdle when developing in Kubernetes because it is not always
    clear how to interact with your cluster. The `kubectl` command line is a veritable
    Swiss Army knife of tools to achieve this, from `kubectl logs` to `kubectl exec`
    and `kubectl port-forward`, but learning how to use all the different options
    and achieving familiarity with the tool can take a considerable amount of experience.
    Furthermore, because the tool runs in the terminal, it often requires the composition
    of multiple windows to simultaneously examine both the source code for the application
    and the running application itself.
  prefs: []
  type: TYPE_NORMAL
- en: To streamline the testing and debugging experience, Kubernetes tooling is increasingly
    being integrated into development environments, for example, the open source extension
    for Visual Studio (VS) Code for Kubernetes. The extension is easily installed
    for free from the VS Code marketplace. When installed, it automatically discovers
    any clusters that you already have in your *kubeconfig* file and provides a tree-view
    navigation pane for you to see the contents of your cluster at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to being able to see your cluster state at a glance, the integration
    allows a developer to use the tools available via `kubectl` in an intuitive, discoverable
    way. From the tree view, if you right-click a Kubernetes pod, you can immediately
    use port forwarding to bring a network connection from the pod directly to the
    local machine. Likewise, you can access the logs for the pod or even get a terminal
    within the running container.
  prefs: []
  type: TYPE_NORMAL
- en: The integration of these commands with prototypical user interface expectations
    (e.g., right-click shows a context menu), as well as the integration of these
    experiences alongside the code for the application itself, enables developers
    with minimal Kubernetes experience to rapidly become productive in the development
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Of course this VS Code extension isn’t the only integration between Kubernetes
    and a development environment; there are several others that you can install depending
    on your choice of programming environment and style (`vi`, `emacs`, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a Development Environment Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting up development workflows on Kubernetes is key to productivity and is
    pivotal for productive and happy development teams. Following these best practices
    will help to ensure that developers are up and running quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about developer experience in three phases: onboarding, developing, and
    testing. Make sure that the development environment you build supports all three
    of these phases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a development cluster, you can choose between one large cluster
    and a cluster per developer. There are pros and cons to each, but generally a
    single large cluster is a better approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you add users to a cluster, add them with their own identity and access
    to their own namespace. Use resource limits to restrict how much of the cluster
    they can use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When managing namespaces, think about how you can reap old, unused resources.
    Developers will have bad hygiene about deleting unused things. Use automation
    to clean it up for them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think about cluster-level services like logs and monitoring that you can set
    up for all users. Sometimes, cluster-level dependencies like databases are also
    useful to set up on behalf of all users using templates like Helm charts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve reached a place where creating a Kubernetes cluster, especially in the
    cloud, is a relatively straightforward exercise, but enabling developers to productively
    use such a cluster is significantly less obvious and easy. When thinking about
    enabling developers to successfully build applications on Kubernetes, it’s important
    to think about the key goals around onboarding, iterating, testing, and debugging
    applications. Likewise, it pays to invest in some basic tooling specific to user
    onboarding, namespace provisioning, and cluster services like basic log aggregation.
    Viewing a development cluster and your code repositories as an opportunity to
    standardize and apply best practices will ensure that you have happy and productive
    developers successfully building code to deploy to your production Kubernetes
    clusters.
  prefs: []
  type: TYPE_NORMAL
