- en: Chapter 7\. The Kubernetes Native Database
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 Kubernetes 原生数据库
- en: 'The software industry is flush with terms that define major trends in a single
    word or short phrase. You can see one of them in the title of this book: *cloud
    native*. Another example is *microservice*, a major architectural paradigm that
    touches much of the technology we’re discussing here. More recently, terms like
    *Kubernetes native* and *serverless* have emerged.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 软件行业充斥着用一个单词或短语定义主要趋势的术语。你可以在本书的标题中看到其中之一：*云原生*。另一个例子是 *微服务*，这是一种主要的架构范式，涉及我们在这里讨论的大部分技术。最近，出现了像
    *Kubernetes 原生* 和 *无服务器* 这样的术语。
- en: While succinct and catchy, distilling a complex topic or trend down to a single
    sound bite leaves room for ambiguity, or at least for reasonable questions such
    as “What does this *actually* mean?” To further muddy the waters, terms such as
    these are frequently used in the context of marketing products as a way to gain
    leverage or differentiate against other competitive offerings. Whether the content
    you’re consuming makes an overt statement or it’s just the subtext, you may have
    wondered whether a given technology must be better to run on Kubernetes than other
    offerings because it’s labeled *Kubernetes native*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简洁而引人注目，将一个复杂的主题或趋势归纳为一个单一的声音留下了模糊的空间，或者至少是一些合理的问题，比如“这究竟是什么意思？”进一步混淆视听的是，像这些术语一样经常在营销产品的背景下使用，作为获取优势或与其他竞争产品区分开来的一种方式。无论你正在消费的内容是明确陈述还是仅仅是含蓄的，你可能会想知道，一个技术是否必须在
    Kubernetes 上运行才能比其他提供更好，因为它被标记为 *Kubernetes 原生*。
- en: 'Of course, for these terms to be useful to us in evaluating and picking the
    right technologies for our applications, the real task is to unpack what they
    really mean, as we did with the term *cloud native data* in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra).
    In this chapter, we’ll look at what it means for data technology to be Kubernetes
    native and see if we can arrive at a definition that we can agree on. To do this,
    we’ll examine a couple of projects that claim these terms and derive the common
    principles: TiDB and Astra DB. Are you ready? Let’s dive in!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于我们评估和选择适合我们应用程序的正确技术来说，这些术语是否对我们有用的真正任务是澄清它们实际含义，就像我们在[第1章](ch01.html#introduction_to_cloud_native_data_infra)中对
    *云原生数据* 这个术语所做的那样。在本章中，我们将探讨数据技术被定义为 Kubernetes 原生的含义，并看看是否能得出一个我们可以达成共识的定义。为此，我们将检视几个宣称拥有这些术语的项目，并推导出共同的原则：TiDB
    和 Astra DB。准备好了吗？让我们深入了解吧！
- en: Why a Kubernetes Native Approach Is Needed
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么需要 Kubernetes 原生方法
- en: First, let’s discuss why the idea of a Kubernetes native database came up in
    the first place. Up to this point in the book, we’ve focused on deployment of
    existing databases on Kubernetes including MySQL and Cassandra. These are mature
    databases that were around before Kubernetes existed and have proven themselves
    over time. They have large install bases and user communities, and because of
    this investment, you can see why there’s a large incentive to run these databases
    in Kubernetes environments, and why there has been such interest in creating operators
    to automate them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论为什么首次提出了 Kubernetes 原生数据库的概念。直到本书的这一部分，我们专注于在 Kubernetes 上部署现有的数据库，包括
    MySQL 和 Cassandra。这些都是在 Kubernetes 存在之前就存在并且经过时间验证的成熟数据库。它们拥有庞大的安装基础和用户社区，正因为有这样的投资，你可以理解为什么在
    Kubernetes 环境中运行这些数据库有如此大的激励，并且为什么有兴趣创建操作员来自动化它们。
- en: At the same time, you’ve probably noticed some of the awkwardness in adapting
    these databases to run on Kubernetes. While it is pretty straightforward to point
    a database to Kubernetes-based storage just by changing a mount path, tighter
    integration with Kubernetes to manage databases that consist of multiple nodes
    can be a bit more involved. This can range from relatively simple tasks like deploying
    a legacy management UI in a Pod and exposing access to the HTTP port, to the more
    complex deployment of sidecars that we saw in [Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku)
    to provide APIs for management and metrics collection.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，你可能已经注意到将这些数据库适应 Kubernetes 的一些尴尬之处。虽然通过改变挂载路径简单地将数据库指向基于 Kubernetes 的存储相对直接，但更紧密地集成
    Kubernetes 来管理由多个节点组成的数据库可能会更复杂一些。这可以从相对简单的任务开始，比如在 Pod 中部署传统的管理 UI 并公开对 HTTP
    端口的访问，到我们在[第6章](ch06.html#integrating_data_infrastructure_in_a_ku)中看到的更复杂的部署 sidecar
    以提供管理和指标收集的 API。
- en: The recognition of this complexity has led some innovators to develop new databases
    that are designed to be Kubernetes native from day one. It’s a well-known axiom
    in the database industry that it takes 5–10 years for a new database engine to
    reach a point of maturity. Because of this, these Kubernetes native databases
    tend not to be completely new implementations, but rather refactoring of existing
    databases into microservices that can be scaled independently, while maintaining
    compatibility with existing APIs that developers are accustomed to. Thus, the
    trend of decomposing the monolith has arrived at the data tier. The emerging generation
    of databases will be based on new architectures to truly leverage the benefits
    of Kubernetes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对这种复杂性的认识促使一些创新者从一开始就设计成 Kubernetes 原生的新数据库。在数据库行业中，有一个广为人知的公理是，一个新的数据库引擎需要
    5 到 10 年的时间才能达到成熟阶段。因此，这些 Kubernetes 原生的数据库通常不是全新的实现，而是将现有数据库重构为可以独立扩展的微服务，同时保持与开发人员熟悉的现有
    API 的兼容性。因此，将单体应用分解的趋势已经到达了数据层。新一代的数据库将基于新的架构来真正利用 Kubernetes 的优势。
- en: 'To help us assess what might qualify these new databases as Kubernetes native,
    let’s use the cloud native data principles introduced in [“Principles of Cloud
    Native Data Infrastructure”](ch01.html#principles_of_cloud_native_data_infrast)
    as a guide to formulate some questions to ask how a database interacts with Kubernetes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们评估这些新数据库是否符合 Kubernetes 原生的标准，让我们使用[“云原生数据基础设施原则”](ch01.html#principles_of_cloud_native_data_infrast)中介绍的云原生数据原则作为指南，制定一些问题来询问数据库与
    Kubernetes 的交互方式：
- en: 'Principle 1: Leverage compute, network, and storage as commodity APIs'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 1：将计算、网络和存储作为商品 API 利用
- en: How does the database use Kubernetes compute resources (Pods, Deployments, StatefulSets),
    network resources (Services and Ingress), and storage resources (PersistentVolumes,
    PersistentVolumeClaims, StorageClasses)?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库如何使用 Kubernetes 计算资源（Pods、Deployments、StatefulSets）、网络资源（Services 和 Ingress）以及存储资源（PersistentVolumes、PersistentVolumeClaims、StorageClasses）？
- en: 'Principle 2: Separate the control and data planes'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 2：分离控制平面和数据平面
- en: Is the database deployed and managed by an operator? What custom resources does
    it define? Are other workloads in the control plane besides the operator?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库是否由 operator 部署和管理？它定义了哪些自定义资源？除了 operator 外，控制平面中是否还有其他工作负载？
- en: 'Principle 3: Make observability easy'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 3：简化可观察性
- en: How do the various services in the architecture expose metrics and logs to support
    collection by the Kubernetes control plane and third-party extensions?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 架构中的各种服务如何暴露指标和日志，以便 Kubernetes 控制平面和第三方扩展进行收集？
- en: 'Principle 4: Make the default configuration secure'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 4：使默认配置安全
- en: Do the database and associated operator use Kubernetes Secrets to share credentials,
    and use Roles and RoleBindings to manage access by Role? Do services minimize
    the number of exposed points and require secure access to them?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库和相关的 operator 是否使用 Kubernetes Secrets 共享凭据，并使用 Roles 和 RoleBindings 来管理角色访问？服务是否尽量减少暴露点的数量，并要求对其进行安全访问？
- en: 'Principle 5: Prefer declarative configuration'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 5：优先使用声明式配置
- en: Extending Principle 2, can the database be managed entirely by creating, updating,
    or deleting Helm charts and Kubernetes resources (whether built-in or custom resources),
    or are other tools required?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展原则 2，数据库是否完全可以通过创建、更新或删除 Helm charts 和 Kubernetes 资源（无论是内置还是自定义资源）来管理，或者是否需要其他工具？
- en: In the sections that follow, we’ll explore the answers to these questions for
    two databases and see what else we can learn about what it means to be Kubernetes
    native. That will help us to build a checklist at the end of this chapter that
    will help solidify our definition. (See [“What to Look for in a Kubernetes Native
    Database”](#what_to_look_for_in_a_kubernetes_native) for what we come up with.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨两个数据库的答案，并了解关于什么是 Kubernetes 原生的更多信息。这将帮助我们在本章末尾制定一个清单，以帮助巩固我们的定义。（参见[“在
    Kubernetes 原生数据库中寻找什么”](#what_to_look_for_in_a_kubernetes_native)关于我们得出的内容。）
- en: Hybrid Data Access at Scale with TiDB
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TiDB 实现规模化的混合数据访问
- en: The databases that have received most of our focus in this book so far represent
    two major trends in database architecture that trace their lineage back for decades
    or more. MySQL is a relational database that provides its own flavor of the Standard
    Query Language (SQL), based on rules developed by Edgar Codd in the 1970s.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书中大部分关注的数据库代表了数据库架构中的两大主要趋势，这些趋势可以追溯到数十年甚至更久以前。MySQL是一种关系数据库，提供其自己的SQL标准查询语言，这些规则是Edgar
    Codd在1970年代开发的。
- en: In the early 2000s, companies building web-scale applications began to push
    the limits of what could be accomplished with the relational databases of the
    day. As database sizes began growing beyond what could feasibly be managed on
    a single instance, techniques like sharding were used to scale across multiple
    instances. These were frequently expensive, difficult to operate, and not always
    reliable.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在21世纪初，构建网络规模应用的公司开始挑战当时关系数据库的极限。随着数据库规模超过单一实例的可管理范围，诸如分片等技术开始用于跨多个实例的扩展。这些技术经常昂贵、难以操作，并且不总是可靠。
- en: 'In response to this need, Cassandra and other so-called *NoSQL* databases emerged
    in a period of intense innovation and experimentation. These databases provide
    linear scalability through adding additional nodes. They offer different data
    models, or ways of representing data: for example, key-value stores such as Redis,
    document databases such as MongoDB, graph databases such as Neo4j, and others.
    NoSQL databases tended to provide weaker consistency guarantees and omit support
    for more complex behaviors like transactions and joins to achieve high performance
    and availability at scale, a trade-off documented by Eric Brewer in his [CAP theorem](https://oreil.ly/aJq6M).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对这一需求的响应，Cassandra和其他所谓的*NoSQL*数据库在一个充满创新和实验的时期出现。这些数据库通过增加额外节点实现线性扩展性。它们提供不同的数据模型或数据表示方式，例如Redis的键值存储、MongoDB的文档数据库、Neo4j的图数据库以及其他类型。NoSQL数据库往往提供较弱的一致性保证，并省略了对像事务和连接这样的更复杂行为的支持，以实现高性能和可用性的规模化，这是Eric
    Brewer在他的[CAP定理](https://oreil.ly/aJq6M)中记录的一种权衡。
- en: Because of the continued developer demand for traditional relational semantics
    such as strong consistency, transactions, and joins, multiple teams began to revive
    the idea of supporting these capabilities in distributed databases starting around
    2012\. These so-called *NewSQL* databases were based on more efficient and performant
    consensus algorithms. Two key papers helped drive the emergence of the NewSQL
    movement. First, the [Calvin paper](https://oreil.ly/HLw2M) introduced a global
    consensus protocol which represented a more reliable and performant approach for
    guaranteeing strong consistency, later adopted by FaunaDB and other databases.
    Second, Google’s [Spanner paper](https://oreil.ly/zDl5z) introduced a design for
    a distributed relational database using sharding and a new consensus algorithm
    that leveraged the improved ability of cloud infrastructure to provide time synchronization
    across datacenters. Besides Google Spanner, this approach was implemented by databases
    including CockroachDB and YugabyteDB.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于开发者对传统关系型语义（如强一致性、事务和连接）的需求持续存在，从2012年开始，多个团队开始重新支持在分布式数据库中实现这些功能的想法。这些所谓的*NewSQL*数据库基于更高效和性能更好的共识算法。两篇关键论文推动了NewSQL运动的出现。首先是[Calvin论文](https://oreil.ly/HLw2M)，引入了一个全局共识协议，这代表了一种更可靠和高效的方法，用于保证强一致性，并被FaunaDB和其他数据库后来采纳。其次是Google的[Spanner论文](https://oreil.ly/zDl5z)，介绍了一种使用分片和新共识算法设计的分布式关系数据库，利用云基础设施提供跨数据中心时间同步的能力，这种方法除了Google
    Spanner外，还被CockroachDB和YugabyteDB等数据库实现。
- en: More on Consistency and Consensus
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多关于一致性与共识的内容
- en: While we don’t have space in this book to dive deeply into the trade-offs between
    various consensus algorithms and how they are used to provide various data consistency
    guarantees, an understanding of these concepts is helpful in choosing the right
    data infrastructure for your cloud applications. If you’re interested in learning
    more in this area, Martin Kleppmann’s [*Designing Data-Intensive Applications*](https://oreil.ly/6ndic)
    (O’Reilly) is a great source, especially Chapter 9, “Consistency and Consensus”.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书篇幅有限，无法深入讨论各种共识算法之间的权衡及其如何用于提供各种数据一致性保证，但理解这些概念有助于选择适合云应用程序的正确数据基础架构。如果您对这个领域有兴趣，Martin
    Kleppmann的《*设计数据密集型应用*》（[O'Reilly](https://oreil.ly/6ndic)）是一个很好的资源，特别是第9章“一致性与共识”。
- en: '[TiDB](https://oreil.ly/jZNAI) (where *Ti* stands for *Titanium*) represents
    a continuation of the NewSQL trend in the cloud native space. TiDB is an open
    source, MySQL-compatible database that supports both transactional and analytic
    workloads. It was initially developed and is primarily supported by PingCAP. While
    TiDB is a database designed to embody cloud native principles of scalability and
    elasticity, what makes it especially interesting for our discussion is that it
    has been explicitly designed to run on Kubernetes and to rely on capabilities
    provided by the Kubernetes control plane. In this way, one could argue that TiDB
    is not merely a Kubernetes native database, but also a Kubernetes *only* database.
    Let’s dig into the details.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[TiDB](https://oreil.ly/jZNAI)（其中*Ti*代表*钛*）代表着云原生空间中NewSQL趋势的延续。TiDB是一个开源的、兼容MySQL的数据库，支持事务和分析工作负载。它最初由PingCAP开发，并得到主要支持。虽然TiDB是一个旨在体现可扩展性和弹性的云原生数据库，但特别引人注目的是它明确设计为在Kubernetes上运行，并依赖于Kubernetes控制平面提供的功能。因此，可以说TiDB不仅仅是一个Kubernetes原生数据库，还是一个Kubernetes
    *专属*数据库。让我们深入了解细节。'
- en: TiDB Architecture
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TiDB架构
- en: 'A key characteristic of TiDB which distinguishes it from other databases we’ve
    examined so far in this book is its ability to support transactional and analytic
    workloads. This approach, known as *hybrid transactional/analytical processing
    (HTAP)*, supports both types of queries without a separate extract, transform,
    and load (ETL) process. As shown in [Figure 7-1](#tidb_architecture), TiDB does
    this by providing two database engines under the hood: TiKV and TiFlash. This
    approach was inspired by Google’s [F1 project](https://oreil.ly/lakAf), a layer
    built on top of Spanner.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB的一个关键特征是其能够支持事务和分析工作负载，这使其与本书中迄今为止我们审查过的其他数据库有所区别。这种方法被称为*混合事务/分析处理（HTAP）*，支持两种类型的查询，而无需单独的抽取、转换和加载（ETL）过程。如[图 7-1](#tidb_architecture)所示，TiDB通过在底层提供两个数据库引擎来实现这一点：TiKV和TiFlash。这种方法受到Google的[F1项目](https://oreil.ly/lakAf)的启发，该项目是建立在Spanner之上的一层。
- en: '![TiDB architecture](assets/mcdk_0701.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![TiDB架构](assets/mcdk_0701.png)'
- en: Figure 7-1\. TiDB architecture
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. TiDB架构
- en: 'One key aspect that gives TiDB a cloud native architecture is the packaging
    of compute and storage operations into separate components, each of which is composed
    of independently scalable services organized in clusters. Let’s examine the roles
    of each of these components:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使TiDB具有云原生架构的一个关键方面是将计算和存储操作打包成单独的组件，每个组件由独立可伸缩的服务组成，并组织成集群。让我们详细探讨每个组件的角色：
- en: TiDB
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB
- en: Each TiDB instance is a stateless service that exposes a MySQL endpoint to client
    applications. TiDB parses incoming SQL requests and uses metadata from the Placement
    Driver (PD) to create an execution plan containing queries to make on specific
    TiKV and TiFlash nodes in the storage cluster. TiDB executes these queries, assembles
    the results, and returns to the client application. The TiDB cluster is typically
    deployed with a proxy in front of it to provide load balancing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个TiDB实例是一个无状态的服务，向客户端应用程序暴露一个MySQL端点。TiDB解析传入的SQL请求，并使用来自Placement Driver（PD）的元数据创建执行计划，包含对存储集群中特定TiKV和TiFlash节点的查询。TiDB执行这些查询，组装结果，并返回给客户端应用程序。TiDB集群通常部署在代理之前，以提供负载均衡。
- en: TiKV
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TiKV
- en: The storage cluster consists of a mixture of TiKV and TiFlash nodes. First,
    let’s examine [*TiKV*](https://tikv.org), an open source, distributed key-value
    database that uses [RocksDB](http://rocksdb.org) as its backing storage engine.
    TiKV exposes a custom distributed SQL API that the TiDB nodes use to execute queries
    to store and retrieve data and manage distributed transactions. TiKV stores multiple
    replicas of your data, typically at least three, to support high availability
    and automatic failover. TiKV is a [CNCF graduated project](https://oreil.ly/ypLlC)
    which can be used independently from TiDB, as we’ll discuss later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 存储集群由TiKV和TiFlash节点的混合组成。首先，让我们来看看[*TiKV*](https://tikv.org)，这是一个开源的、分布式键值数据库，使用[RocksDB](http://rocksdb.org)作为其后端存储引擎。TiKV暴露一个自定义的分布式SQL
    API，TiDB节点使用它来执行存储和检索数据以及管理分布式事务的查询。TiKV存储您的数据的多个副本，通常至少三个，以支持高可用性和自动故障转移。TiKV是一个[CNCF毕业项目](https://oreil.ly/ypLlC)，可以独立于TiDB使用，我们稍后将讨论这一点。
- en: TiFlash
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: TiFlash
- en: The storage cluster also includes TiFlash nodes, to which data is replicated
    from TiKV nodes as it is written. TiFlash is a columnar database based on the
    open source [ClickHouse analytic database](https://oreil.ly/PCVlg), which means
    that it organizes data storage in columns rather than rows. Columnar databases
    can provide a significant performance advantage for analytic queries requiring
    the extraction of the same column across multiple rows.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 存储集群还包括 TiFlash 节点，数据写入时从 TiKV 节点复制到这些节点。TiFlash 是基于开源 [ClickHouse 分析数据库](https://oreil.ly/PCVlg)
    的列式数据库，这意味着它将数据存储按列而不是行进行组织。列式数据库在需要跨多行提取同一列的分析查询中可以提供显著的性能优势。
- en: TiSpark
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: TiSpark
- en: This library is built for Apache Spark to support complex OLAP queries. TiSpark
    integrates with the Spark Driver and Spark Executors, providing the capability
    to ingest data from TiFlash instances using the distributed SQL API. We’ll examine
    the Spark architecture and the details of deploying Spark on Kubernetes in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此库专为 Apache Spark 构建，以支持复杂的 OLAP 查询。TiSpark 与 Spark Driver 和 Spark Executors
    集成，通过分布式 SQL API 从 TiFlash 实例中摄取数据。我们将在第 [9 章](ch09.html#data_analytics_on_kubernetes)
    中详细讨论 Spark 架构和在 Kubernetes 上部署 Spark 的细节。
- en: Placement Driver (PD)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Placement Driver（PD）
- en: The PD manages the metadata for a TiDB installation. PD instances are deployed
    in a cluster of at least three nodes. TiDB uses a range-based sharding mechanism
    where the keys in each table are divided into ranges called *regions*. The PD
    is responsible for determining the ranges of data assigned to each region, and
    the TiKV nodes that will store the data for each region. It monitors the amount
    of data in each region and splits regions that become too large, to facilitate
    scaling up, and merging smaller regions to scale down.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: PD 管理 TiDB 安装的元数据。PD 实例部署在至少三个节点的集群中。TiDB 使用基于范围的分片机制，每个表中的键被划分为称为*region*的范围。PD
    负责确定分配给每个 region 的数据范围，以及存储每个 region 数据的 TiKV 节点。它监视每个 region 中的数据量，并在 region
    变得过大时拆分 region 以便扩展，并合并较小的 region 以缩小规模。
- en: Because the TiDB architecture consists of well-defined interfaces between the
    components, it is an extensible architecture in which different pieces can be
    plugged in. For example, TiKV provides a distributed key-value storage solution
    that can be reused in other applications. The [TiPrometheus project](https://oreil.ly/PkmqK)
    is an example, providing a Prometheus-compliant compute layer on top of TiKV.
    For another example, you could provide an alternate implementation of TiKV that
    implements the distributed SQL API on top of a different storage engine.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 TiDB 架构由各组件之间的明确定义接口组成，它是一种可扩展的架构，可以在其中插入不同的部件。例如，TiKV 提供了分布式键值存储解决方案，可以在其他应用程序中重用。[TiPrometheus
    项目](https://oreil.ly/PkmqK) 就是一个示例，它在 TiKV 之上提供了符合 Prometheus 的计算层。另一个例子是，您可以提供
    TiKV 的另一个实现，该实现在不同的存储引擎上实现了分布式 SQL API。
- en: Pluggable Storage Engines
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可插拔存储引擎
- en: In this chapter so far, we’ve made several mentions of “storage engines” or
    “database engines.” This term refers to the part of the database that manages
    the storage and retrieval of data on persistent media. In distributed databases,
    a distinction is often made between the storage engine and the proxy layer which
    sits on top of it to manage data replication between nodes. Chapter 3, “Storage
    and Retrieval,” from [*Designing Data-Intensive Applications*](https://oreil.ly/6ndic)
    includes discussion of storage engine types such as the B-trees used in most relational
    databases and the log-structured merge tree (LSM tree) used in Apache Cassandra
    and other NoSQL databases.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们多次提到“存储引擎”或“数据库引擎”。这个术语指的是数据库的一部分，负责在持久介质上管理数据的存储和检索。在分布式数据库中，通常区分存储引擎和位于其上的代理层，代理层用于管理节点之间的数据复制。来自
    [*设计数据密集型应用*](https://oreil.ly/6ndic) 的第三章“存储和检索”包括对存储引擎类型的讨论，例如大多数关系数据库中使用的 B
    树，以及 Apache Cassandra 和其他 NoSQL 数据库中使用的日志结构合并树（LSM 树）。
- en: 'One interesting aspect of TiDB is the way in which it reuses existing technology.
    We’ve seen examples of this in the usage of components including RocksDB and Spark.
    TiDB also uses algorithms developed by other organizations. Here are a couple
    of examples:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB 的一个有趣之处在于它如何重用现有技术。我们已经在使用组件如 RocksDB 和 Spark 的示例中看到了这一点。TiDB 还使用其他组织开发的算法。以下是几个例子：
- en: Raft consensus protocol
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Raft 共识协议
- en: At the TiDB layer, the [Raft consensus protocol](https://oreil.ly/Oi6Dk) is
    used to manage consistency between replicas. Raft is similar to the Paxos algorithm
    used by Cassandra in terms of its behavior, but it’s designed to be much simpler
    to learn and use. TiDB uses a separate Raft group for each region, where a group
    typically consists of a leader and two or more replicas. If a leader node is lost,
    an election is run to select a new leader, and a new replica can be added to ensure
    the desired number of replicas. In addition, the TiFlash nodes are configured
    as a special type of replica called *learner replicas*. Data is replicated to
    learner replicas from the TiDB nodes, but they cannot be selected as a leader.
    You can read more about how TiDB uses Raft for [high availability](https://oreil.ly/BddzV)
    and other related topics on the [PingCAP blog](https://oreil.ly/Y2YuS).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TiDB 层面，使用 [Raft 共识协议](https://oreil.ly/Oi6Dk) 管理副本之间的一致性。Raft 在行为上类似于 Cassandra
    使用的 Paxos 算法，但其设计更简单易学。TiDB 为每个区域使用一个单独的 Raft 组，其中一个组通常包含一个领导者和两个或更多个副本。如果丢失领导节点，则会运行选举来选择新的领导者，并且可以添加新的副本以确保所需的副本数。此外，TiFlash
    节点被配置为一种特殊类型的副本，称为 *learner replicas*。数据从 TiDB 节点复制到 learner replicas，但它们不能被选为领导者。您可以在
    [PingCAP 博客](https://oreil.ly/Y2YuS) 上进一步了解 TiDB 如何利用 Raft 实现高可用性及其他相关主题。
- en: Percolator transaction management
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Percolator 事务管理
- en: At the TiDB layer, distributed transactions are supported using an implementation
    of the [Percolator algorithm](https://oreil.ly/heMho) with optimizations specific
    to the TiDB project. Percolator was originally developed at Google for supporting
    incremental updates to search indexes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TiDB 层面，使用 [Percolator 算法](https://oreil.ly/heMho) 实现支持分布式事务，该算法进行了 TiDB 项目特定的优化。Percolator
    最初由 Google 开发，用于支持对搜索索引的增量更新。
- en: One of the arguments we’re making in this chapter is that part of what it means
    for data infrastructure to be cloud native is to compose existing APIs, services,
    and algorithms wherever possible, and TiDB is a great example of this.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们提出的一个论点是，数据基础设施成为云原生的一部分，意味着尽可能地组合现有的 API、服务和算法，TiDB 就是一个很好的例子。
- en: Deploying TiDB in Kubernetes
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署 TiDB
- en: While TiDB can be deployed in a variety of ways including bare metal and VMs,
    the TiDB team has invested a large effort in tooling and documentation to make
    TiDB a truly Kubernetes native database. The [TiDB Operator](https://oreil.ly/xZtGq)
    manages TiDB clusters in Kubernetes, including deployment, upgrade, scaling, backup
    and restore, and more.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 TiDB 可以通过裸机和虚拟机等多种方式部署，但 TiDB 团队已经投入大量精力在工具和文档上，使 TiDB 成为真正的 Kubernetes 本地数据库。[TiDB
    Operator](https://oreil.ly/xZtGq) 管理 Kubernetes 中的 TiDB 集群，包括部署、升级、扩展、备份和恢复等操作。
- en: The operator [documentation](https://oreil.ly/iIZc0) provides [quick start guides](https://oreil.ly/5heDA)
    for desktop Kubernetes distributions such as kind, minikube, and Google Kubernetes
    Engine (GKE). These instructions guide you through steps including installing
    CRDs and the TiDB operator using Helm, and a simple TiDB cluster including monitoring
    services. We’ll use the quick start instructions as a vehicle to talk about what
    makes TiDB a Kubernetes native database.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 操作员的 [文档](https://oreil.ly/iIZc0) 提供了桌面 Kubernetes 发行版（如 kind、minikube 和 Google
    Kubernetes Engine (GKE)）的 [快速入门指南](https://oreil.ly/5heDA)。这些说明将引导您完成包括使用 Helm
    安装 CRDs 和 TiDB 操作员，以及包括监控服务的简单 TiDB 集群的步骤。我们将使用这些快速入门说明来介绍 TiDB 成为 Kubernetes
    本地数据库的特点。
- en: Installing the TiDB CRDs
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 TiDB CRDs
- en: 'After making sure you have a Kubernetes cluster that meets the defined prerequisites
    such as having a [default StorageClass](https://oreil.ly/7myfI), the first step
    in deploying TiDB using the operator is installing the CRDs used by the operator.
    This is done using an instruction such as the following (note the actual operator
    version number `v1.3.2` may vary):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 确保拥有符合定义先决条件的 Kubernetes 集群，例如拥有 [默认的 StorageClass](https://oreil.ly/7myfI)，使用操作员部署
    TiDB 的第一步是安装操作员使用的 CRDs。可以通过类似以下的指令完成此操作（请注意实际的操作员版本号 `v1.3.2` 可能会有所不同）：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This results in the creation of several CRDs, which you can observe by running
    the command `kubectl get crd` as we have done in previous chapters. We’ll quickly
    discuss the purpose of each resource since several of them hint at additional
    features of interest:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致创建了多个 CRDs，您可以通过运行 `kubectl get crd` 命令来观察这些 CRD，就像我们在前面的章节中所做的那样。我们将快速讨论每个资源的目的，因为其中几个资源暗示了其他感兴趣的功能：
- en: The TidbCluster is the primary resource that describes the desired configuration
    of a TiDB cluster. We’ll look at an example later.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbCluster 是描述 TiDB 集群所需配置的主要资源。稍后我们将看一个示例。
- en: The TidbMonitor resource is used to deploy a Prometheus-based monitoring stack
    to observe one or more TidbClusters. As we have seen with other projects, Prometheus
    (or at least its API) has become a de facto standard for metrics collection for
    databases and other infrastructure deployed on Kubernetes.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbMonitor 资源用于部署基于 Prometheus 的监控堆栈，以观察一个或多个 TidbCluster。正如我们在其他项目中所见，Prometheus（或其API）已成为在
    Kubernetes 上部署的数据库和其他基础设施的度量收集的事实标准。
- en: The Backup and Restore resources represent the actions of performing a backup
    or restoring from a backup. This is similar to other operators we’ve examined
    previously from the Vitess (see [“PlanetScale Vitess Operator”](ch05.html#planetscale_vitess_operator))
    and K8ssandra ([Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku))
    projects. The TiDB Operator also provides a BackupSchedule resource that can be
    used to configure regular backups.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和恢复资源表示执行备份或从备份中恢复的操作。这类似于我们之前从 Vitess（参见[“PlanetScale Vitess Operator”](ch05.html#planetscale_vitess_operator)）和
    K8ssandra（[第 6 章](ch06.html#integrating_data_infrastructure_in_a_ku)）项目中研究过的其他操作器。TiDB
    Operator 还提供了 BackupSchedule 资源，可用于配置定期备份。
- en: The TidbInitializer is an optional resource that you can create to perform [initialization
    tasks](https://oreil.ly/qFsmu) on a TidbCluster, including setting administrator
    credentials and executing SQL statements for schema creation or initial data loading.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbInitializer 是一个可选资源，您可以创建它来执行[TidbCluster 上的初始化任务](https://oreil.ly/qFsmu)，包括设置管理员凭据并执行用于架构创建或初始数据加载的
    SQL 语句。
- en: The TidbClusterAutoScaler is another optional resource which can be used to
    configure [auto-scaling](https://oreil.ly/wVbf2) behavior of a TidbCluster. The
    number of TiKV or TiDB nodes in a TidbCluster can be configured to scale up or
    down between minimum and maximum limits based on CPU utilization. The addition
    of scaling rules based on other metrics is on the project roadmap. As we discussed
    in [“Choosing Operators”](ch05.html#choosing_operators), auto-scaling is considered
    a feature of an operator at Level 5 or Autopilot, the highest maturity level.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbClusterAutoScaler 是另一个可选资源，用于配置 TidbCluster 的[自动扩展行为](https://oreil.ly/wVbf2)。可以根据
    CPU 利用率配置 TidbCluster 中 TiKV 或 TiDB 节点的最小和最大限制进行扩展。基于其他指标的扩展规则的添加已包含在项目路线图中。正如我们在[“选择运算符”](ch05.html#choosing_operators)中讨论的那样，自动缩放被认为是运算符的
    Level 5 或 Autopilot 功能，是最高成熟度级别的特性。
- en: The TidbNGMonitoring is an optional resource that configures a TidbCluster to
    enable [continuous profiling](https://oreil.ly/2n8k5) down to the system call
    level. The resulting profiling data and flame graph visualizations can be observed
    using the [TiDB Dashboard](https://oreil.ly/23pLs), which is deployed separately.
    This is typically used by project engineers looking to optimize the database,
    but application and platform developers may find this useful as well.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbNGMonitoring 是一个可选资源，配置 TidbCluster 以启用[连续剖析](https://oreil.ly/2n8k5)，可以达到系统调用级别。产生的剖析数据和火焰图可通过单独部署的[TiDB
    仪表盘](https://oreil.ly/23pLs)进行观察。通常由项目工程师用于优化数据库，但应用和平台开发人员也可能会发现其有用。
- en: The DMCluster resource is used to deploy an instance of the [TiDB Data Migration](https://oreil.ly/C5NG0)
    (DM) platform that supports migration of MySQL and MariaDB database instances
    into a TidbCluster. It can also be configured to migrate from an existing TiDB
    installation outside of Kubernetes to a TidbCluster. The ability to deploy data
    migration services alongside a destination TidbCluster in Kubernetes managed by
    the same operator is a great example of what it means to develop data ecosystems
    in Kubernetes, a pattern that we hope to see more of in the future.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMCluster 资源用于部署 TiDB 数据迁移（DM）平台的一个实例，支持将 MySQL 和 MariaDB 数据库实例迁移到 TidbCluster。还可以配置从在
    Kubernetes 外部现有的 TiDB 安装迁移到 TidbCluster。在由同一运算符管理的 Kubernetes 中部署数据迁移服务与目标 TidbCluster
    并行的能力，是在 Kubernetes 中开发数据生态系统的一个极好的示例，这是我们希望在未来看到更多的模式。
- en: For the remainder of this section, we’ll focus on the TidbCluster and TidbMonitoring
    resources.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将重点关注 TidbCluster 和 TidbMonitoring 资源。
- en: Installing the TiDB Operator
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 TiDB Operator
- en: 'After installing the CRDs, the next step is to install the TiDB Operator using
    Helm. You’ll need to add the Helm repository first before installing the TiDB
    Operator in its own Namespace:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 CRD 后，下一步是使用 Helm 安装 TiDB Operator。在安装 TiDB Operator 到其自己的命名空间之前，您需要先添加
    Helm 仓库：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can watch the resulting Pods come online using `kubectl get pods` and referencing
    the `tidb-admin` Namespace. [Figure 7-2](#installing_the_tidb_operator_and_crds)
    provides a summary of the elements that you’ve installed up to this point. This
    includes Deployments to manage the TiDB Operator (labeled as `tidb-controller-manager`)
    and the TiDB Scheduler.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `kubectl get pods` 命令并引用 `tidb-admin` 命名空间来监视生成的 Pod。[第 7-2 图](#installing_the_tidb_operator_and_crds)
    概述了到目前为止安装的元素。这包括用于管理 TiDB Operator（标记为 `tidb-controller-manager`）和 TiDB 调度器的部署。
- en: The TiDB Scheduler is an optional extension to the Kubernetes built-in scheduler.
    While it is deployed by default as part of the TiDB Operator, it can be disabled.
    Assuming the TiDB Scheduler is not disabled, using it for a specific TidbCluster
    still requires opting in by setting the `schedulerName` property to `tidb-scheduler`.
    If this property is set, the TiDB Operator will assign the TiDB Scheduler as the
    scheduler that Kubernetes will use when creating TiKV, and PD Pods.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB 调度器是 Kubernetes 内置调度器的可选扩展。虽然它作为 TiDB Operator 的一部分默认部署，但可以禁用。假设未禁用 TiDB
    调度器，仍需要通过将 `schedulerName` 属性设置为 `tidb-scheduler` 来选择使用它来为特定的 TidbCluster 进行调度。如果设置了此属性，TiDB
    Operator 将分配 TiDB 调度器作为 Kubernetes 创建 TiKV 和 PD Pod 时使用的调度器。
- en: The TiDB Scheduler extends the Kubernetes built-in scheduler to add custom scheduling
    rules for Pods that are part of a TidbCluster, helping to achieve high availability
    of the database while spreading the load evenly across the available Worker Nodes
    in the Kubernetes cluster. While for many types of infrastructure, the existing
    mechanisms Kubernetes offers for influencing the default scheduler such as affinity
    rules, taints, and tolerations are sufficient, TiDB provides a useful example
    of when and how to implement custom scheduling logic. We’ll look at Kubernetes
    scheduler extensions in more detail in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB 调度器扩展了 Kubernetes 内置调度器，为 TidbCluster 中的 Pod 添加自定义调度规则，帮助实现数据库的高可用性，并在
    Kubernetes 集群中的可用 Worker 节点上均匀分布负载。对于许多基础设施类型来说，Kubernetes 提供的影响默认调度器的现有机制，如亲和规则、污点和容忍度，已经足够使用。但
    TiDB 提供了一个有用的例子，说明了何时以及如何实现自定义调度逻辑。我们将在第 9 章中详细讨论 Kubernetes 调度器的扩展。[第 9 章](ch09.html#data_analytics_on_kubernetes)
    提供了更详细的信息。
- en: '![Installing the TiDB Operator and CRDs](assets/mcdk_0702.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![安装 TiDB Operator 和 CRD](assets/mcdk_0702.png)'
- en: Figure 7-2\. Installing the TiDB Operator and CRDs
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 7-2 图。安装 TiDB Operator 和 CRD
- en: TiDB Operator Helm Chart Options
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TiDB Operator Helm 图选项
- en: 'This installation omits usage of a *values.yaml* file, but you can see the
    available options by running following command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此安装未使用 *values.yaml* 文件，但您可以通过运行以下命令查看可用选项：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This includes the option to disable the TiDB Scheduler.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括禁用 TiDB 调度器的选项。
- en: Creating a TidbCluster
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 TidbCluster
- en: 'Once the TiDB Operator has been installed, you’re ready to create a TidbCluster
    resource. While many [example configurations](https://oreil.ly/66uf7) are available
    in the TiDB Operator GitHub repository, let’s use the one referenced in the quick
    start guide:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 TiDB Operator 后，您就可以准备创建 TidbCluster 资源了。虽然 TiDB Operator GitHub 存储库中提供了许多
    [示例配置](https://oreil.ly/66uf7)，但让我们使用快速入门指南中引用的配置。
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'While the TidbCluster is being created, you can reference the contents of this
    file, which look something like this (with comments and some details removed):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 TidbCluster 的过程中，您可以引用此文件的内容，看起来类似于这样（删除了一些注释和细节）：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice that this results in the creation of a TidbCluster named `basic` in the
    `tidb-cluster` Namespace, with one replica each of TiDB, TiKV, and PD, using the
    standard PingCAP images for each. Additional options are used to specify the minimum
    amount of compute and storage resources required to achieve a functioning cluster.
    No TiFlash nodes are included in this simple configuration.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这将在 `tidb-cluster` 命名空间中创建一个名为 `basic` 的 TidbCluster，其中包括 TiDB、TiKV 和 PD
    每个一个副本，使用标准的 PingCAP 镜像。还使用了其他选项来指定实现功能集群所需的最小计算和存储资源量。此简单配置不包括 TiFlash 节点。
- en: TidbCluster API
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TidbCluster API
- en: The full list of options for a TidbCluster can be found as part of the [API](https://oreil.ly/XoC02)
    available in the GitHub repository. This same page includes options for the other
    CRDs used by the TiDB Operator. As you explore the options for these CRDs, you’ll
    see evidence of the common practice of allowing many of the options that will
    be used to specify underlying resources to be overridden (for example, the Pod
    specification that will be set on a Deployment).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库中的[API](https://oreil.ly/XoC02)中找到TidbCluster的完整选项列表。同一页还包括TiDB
    Operator使用的其他CRD的选项。当您探索这些CRD的选项时，您会看到允许覆盖许多用于指定底层资源的选项的常见做法（例如，在部署中设置的Pod规范）的证据。
- en: We encourage you to take the opportunity to use `kubectl` or your favorite visualization
    tool to explore the resources created as part of the TidbCluster, a summary of
    which is provided in [Figure 7-3](#a_basic_tidbcluster).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您利用`kubectl`或您喜欢的可视化工具，探索作为TidbCluster一部分创建的资源。这些资源的摘要在[图 7-3](#a_basic_tidbcluster)中提供。
- en: '![A basic TidbCluster](assets/mcdk_0703.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![一个基本的TidbCluster](assets/mcdk_0703.png)'
- en: Figure 7-3\. A basic TidbCluster
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 一个基本的TidbCluster
- en: As you can see, the TiDB Operator creates StatefulSets to manage the TiDB, TiKV,
    and Placement Driver instances, allocating a PVC for each instance. As an I/O-intensive
    application, the default configuration is to use local PersistentVolumes as the
    backing store.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，TiDB Operator创建了StatefulSets来管理TiDB、TiKV和Placement Driver实例，并为每个实例分配了PVC。作为一个I/O密集型应用程序，默认配置是使用本地PersistentVolumes作为后备存储。
- en: In addition, a Deployment is created to run a Discovery Service which the various
    components use to learn of each other’s location. The Discovery Service performs
    a similar role to that of etcd in other data technologies we’ve examined in the
    book. The TiDB Operator also configures services for each StatefulSet and Deployment
    that facilitate communication within the TiDB cluster as well as exposing capabilities
    to external clients.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还创建了一个Deployment来运行Discovery Service，各个组件使用该服务来了解彼此的位置。Discovery Service在本书中我们检查的其他数据技术中扮演了类似于etcd的角色。TiDB
    Operator还为每个StatefulSet和Deployment配置了服务，以促进TiDB集群内部的通信，并向外部客户端公开功能。
- en: 'The TiDB Operator supports the deployment of a Prometheus monitoring stack
    that can manage one or more TiDB clusters. You can add monitoring to the cluster
    created previously using the following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB Operator支持部署Prometheus监控堆栈，可以管理一个或多个TiDB集群。您可以使用以下命令向先前创建的集群添加监控：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While this is deploying, let’s examine the contents of the *tidb-monitor.yaml*
    configuration file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署过程中，让我们来查看*tidb-monitor.yaml*配置文件的内容：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the TidbMonitor resource can point to one or more TidbClusters.
    This TidbMonitor is configured to manage the `basic` cluster you created previously.
    The TidbMonitor resource also allows you to specify the versions of Prometheus,
    Grafana, and additional tools that are used to initialize and update the monitoring
    stack. If you examine the contents of the `tidb-cluster` Namespace, you’ll see
    additional workloads that have been created to manage these elements.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，TidbMonitor资源可以指向一个或多个TidbCluster。该TidbMonitor配置为管理您之前创建的`basic`集群。TidbMonitor资源还允许您指定用于初始化和更新监控堆栈的Prometheus、Grafana和其他工具的版本。如果您检查`tidb-cluster`命名空间的内容，您将看到已创建用于管理这些元素的附加工作负载。
- en: TiDB uses the Prometheus stack in a similar way to the K8ssandra project, as
    we discussed in [“Unified Monitoring Infrastructure with Prometheus and Grafana”](ch06.html#unified_monitoring_infrastructure_with).
    In both of these projects, the Prometheus stack is supported as an optional extension
    to provide a monitoring capability you can use with very little customization.
    The configurations and provided visualizations focus on the key metrics that drive
    awareness of database health. Even if you are already managing your own monitoring
    infrastructure or using a third-party software-as-a-service (SaaS) solution, the
    configurations and charts can give you a head start on incorporating database
    monitoring into the rest of your observability approach.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB与K8ssandra项目类似地使用Prometheus堆栈，正如我们在[“使用Prometheus和Grafana实现统一监控基础设施”](ch06.html#unified_monitoring_infrastructure_with)中讨论的那样。在这两个项目中，Prometheus堆栈作为可选扩展得到支持，以提供您可以通过极少的定制即可使用的监控功能。配置和提供的可视化重点关注于驱动数据库健康意识的关键指标。即使您已经管理自己的监控基础设施或使用第三方软件即服务（SaaS）解决方案，这些配置和图表也可以帮助您快速整合数据库监控到您的可观察性方法中。
- en: As you can see, TiDB is a database with a flexible, extensible architecture
    that has been designed with cloud native principles in mind. It also has a strong
    bias toward being able to deploy and manage a database effectively in Kubernetes
    and has provided us with some valuable insights on what it means to be Kubernetes
    native. Consult the TiDB documentation for more information on features such as
    [deploying to multiple Kubernetes clusters](https://oreil.ly/NPHxy).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，TiDB是一个具有灵活可扩展架构的数据库，设计时考虑了云原生原则。它还倾向于能够在Kubernetes中有效部署和管理数据库，并为我们提供了一些有价值的见解，说明了什么是Kubernetes原生。有关诸如[部署到多个Kubernetes集群](https://oreil.ly/NPHxy)等功能的更多信息，请参阅TiDB文档。
- en: Serverless Cassandra with DataStax Astra DB
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataStax Astra DB的无服务器Cassandra。
- en: Since the advent of cloud computing in the early 2000s, public cloud providers
    and infrastructure vendors have made continual advances in commoditizing various
    layers of our architectural stacks as service offerings. This trend began with
    offering compute, network, and storage as *infrastructure as a service* (IaaS)
    and proceeded into other trends including *platform as a service* (PaaS), *software
    as a service* (SaaS), and *functions as a service* (FaaS), sometimes conflated
    with the term *serverless*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 自21世纪初云计算的出现以来，公共云提供商和基础设施供应商不断推进将我们架构堆栈的各个层次作为服务提供的商品化进程。这一趋势始于提供计算、网络和存储作为基础设施即服务（IaaS），并进一步涵盖其他趋势，包括平台即服务（PaaS）、软件即服务（SaaS）和函数即服务（FaaS），有时与无服务器（serverless）术语混淆。
- en: 'Most pertinent to our investigation here is the emergence of managed data infrastructure
    offerings known as *database as a service* (DBaaS). This category includes the
    following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们调查最相关的是所谓的*数据库即服务*（DBaaS）的托管数据基础设施提供的出现。该类别包括以下内容：
- en: Traditional databases offered as a managed cloud service, such as Amazon Relational
    Database Service (RDS) and PlanetScale
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统数据库作为托管云服务提供，例如亚马逊关系型数据库服务（RDS）和PlanetScale。
- en: Cloud databases like Google BigTable, Amazon Dynamo, and Snowflake that are
    available only as cloud offerings
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅作为云服务提供的云数据库，如Google BigTable、Amazon Dynamo和Snowflake。
- en: Managed NoSQL or NewSQL databases that can also be run on premises under an
    open source or source available license—for example, MongoDB Atlas, DataStax Astra
    DB, TiDB, and Cockroach DB
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在开源或源代码可用许可下也可以在本地运行的托管NoSQL或NewSQL数据库，例如MongoDB Atlas、DataStax Astra DB、TiDB和Cockroach
    DB。
- en: Over the past several years, many of the vendors behind these DBaaS services
    have begun migrating onto Kubernetes to automate operations, manage compute resources
    more efficiently, and make their solutions portable across clouds. DataStax was
    one of several vendors that began offering Cassandra as a service. These vendors
    typically used an architecture based on running traditional Cassandra clusters
    in a cloud environment, with various “glue code” to integrate aspects like networking,
    monitoring, and management that didn’t quite fit target deployment environments
    like Kubernetes and public cloud IaaS. These include techniques like using sidecars
    to collect metrics and logs, or deploying Cassandra nodes using StatefulSets to
    manage scaling up and down in an orderly fashion.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，许多提供DBaaS服务的供应商已经开始迁移到Kubernetes，以自动化操作，更有效地管理计算资源，并使他们的解决方案在不同云之间可移植。DataStax是几家开始提供Cassandra服务的供应商之一。这些供应商通常使用基于在云环境中运行传统Cassandra集群的架构，使用各种“胶水代码”来集成网络、监控和管理等方面，这些并不完全适合目标部署环境，如Kubernetes和公共云IaaS。这些技术包括使用Sidecar收集指标和日志，或使用StatefulSets部署Cassandra节点以有序地进行扩展和缩减。
- en: 'Even with these workarounds for running in Kubernetes, Cassandra’s monolithic
    architecture doesn’t readily promote the separation of compute and storage, which
    can lead to some awkwardness when scaling. You scale up a Cassandra cluster by
    adding additional nodes, where each node has the following capabilities:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用这些在Kubernetes中运行的变通方法，Cassandra的单体架构并不容易促进计算和存储的分离，这可能在扩展时造成一些尴尬。通过添加额外的节点来扩展Cassandra集群，每个节点都具有以下功能：
- en: Coordination
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 协调
- en: Receiving read and write requests and forwarding them to other nodes as needed
    to achieve the requested number of replicas (also known as *consistency level*)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接收读取和写入请求，并根据需要转发到其他节点以达到所需的副本数量（也称为*一致性级别*）
- en: Writing and reading
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 写入和读取
- en: Writing data to in-memory cache (memtables) and persistent storage (SSTables),
    and reading it back as needed
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据写入内存缓存（memtables）和持久存储（SSTables），并根据需要读取回来
- en: Compaction and repair
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩和修复
- en: Since Cassandra is an LSM-tree database, it does not update datafiles once they
    are written to persistent storage. Compaction and repair are tasks that run in
    the background as separate threads. Compaction helps Cassandra stay performant
    by consolidating SSTables written at different times, ignoring obsolete and deleted
    values. Repair is the process of comparing stored values across nodes to ensure
    consistency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Cassandra是LSM树数据库，一旦数据写入持久存储，就不会更新数据文件。压缩和修复是作为独立线程后台运行的任务。压缩通过合并不同时间写入的SSTables来帮助Cassandra保持高性能，忽略过时和已删除的值。修复是跨节点比较存储值以确保一致性的过程。
- en: Each node in a Cassandra cluster implements all of these capabilities and consumes
    equivalent compute and storage resources. This makes it difficult to scale compute
    and storage independently and can lead to situations where a cluster is overprovisioned
    in compute or storage resources.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra集群中的每个节点都实现了所有这些功能，并消耗等效的计算和存储资源。这使得独立扩展计算和存储变得困难，并可能导致集群在计算或存储资源上过度配置的情况。
- en: 'In 2021, DataStax published a paper entitled [“DataStax Astra DB: Designing
    a Serverless Cloud-Native Database-as-a-Service”](https://oreil.ly/yHSxz) that
    describes a different approach. *Astra DB* is a version of Cassandra that has
    been refactored into microservices to allow more fine-grained scalability and
    to take advantage of the benefits of Kubernetes. In fact, Astra DB is not only
    Kubernetes native; it is essentially a Kubernetes-only database. [Figure 7-4](#astra_db_architecture)
    shows the Astra DB architecture at a high level, broken into a control plane,
    data plane, and supporting infrastructure.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '在2021年，DataStax发表了一篇名为[“DataStax Astra DB: 设计为无服务器云原生数据库即服务的数据库”](https://oreil.ly/yHSxz)的论文，描述了一种不同的方法。*Astra
    DB*是Cassandra的一个版本，经过重构为微服务，以实现更精细的可扩展性，并利用Kubernetes的优势。事实上，Astra DB不仅仅是Kubernetes原生的；它本质上是一种仅适用于Kubernetes的数据库。[图7-4](#astra_db_architecture)展示了Astra
    DB架构的高层次视图，分为控制平面、数据平面和支持基础设施。'
- en: '![Astra DB architecture](assets/mcdk_0704.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Astra DB架构](assets/mcdk_0704.png)'
- en: Figure 7-4\. Astra DB architecture
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. Astra DB架构
- en: 'Let’s do a quick overview of the layers in this architecture:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速概述一下这个架构中的各层：
- en: Astra DB control plane
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Astra DB的控制平面
- en: The control plane is responsible for provisioning Kubernetes clusters in various
    cloud provider regions. It also provisions Astra DB clusters within those Kubernetes
    clusters and provides the APIs that allow clients to create and manage databases,
    either through the Astra DB web application, or programmatically through the DevOps
    API. Jim Dickinson’s blog post [“How We Built the DataStax Astra DB Control Plane”](https://oreil.ly/jhU2Q)
    describes the architecture of the control plane and how it was migrated to be
    Kubernetes native.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面负责在各种云提供商区域中提供 Kubernetes 集群。它还在这些 Kubernetes 集群中配置 Astra DB 集群，并提供 API，允许客户端通过
    Astra DB Web 应用程序或通过 DevOps API 编程方式创建和管理数据库。Jim Dickinson 的博文 [“我们如何构建 DataStax
    Astra DB 控制平面”](https://oreil.ly/jhU2Q) 描述了控制平面的架构以及如何迁移到 Kubernetes 原生。
- en: Astra DB data plane
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Astra DB 数据平面
- en: The data plane is where the actual Astra DB databases run. The data plane consists
    of multiple microservices which together provide the capabilities that would have
    been a part of a single monolithic Cassandra node. Each database is deployed in
    a Kubernetes cluster in a dedicated Namespace and may be shared across multiple
    tenants, as described in more detail later on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面是实际运行 Astra DB 数据库的地方。数据平面由多个微服务组成，这些微服务共同提供了原本作为单个单片式 Cassandra 节点一部分的功能。每个数据库都部署在专用的
    Kubernetes Namespace 中的 Kubernetes 集群中，并且可以跨多个租户共享，稍后会详细描述。
- en: Astra DB infrastructure
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Astra DB 基础设施
- en: Each Kubernetes cluster also contains a set of infrastructure components that
    are shared across the Astra DB databases in that cluster, including etcd, Prometheus,
    and Grafana. etcd is used to store metadata, including the assignment of tenants
    to databases and database schema for each tenant. It also stores information about
    the cluster topology, replacing the role of gossip in the traditional Cassandra
    architecture. Prometheus and Grafana are deployed in a similar way as described
    in other architectures in this book.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Kubernetes 集群还包含一组基础设施组件，这些组件在该集群中的所有 Astra DB 数据库之间共享，包括 etcd、Prometheus
    和 Grafana。etcd 用于存储元数据，包括将租户分配给数据库和每个租户的数据库架构。它还存储有关集群拓扑的信息，取代了传统 Cassandra 架构中的
    gossip 角色。Prometheus 和 Grafana 的部署方式与本书中其他架构中描述的方式类似。
- en: 'Now let’s dig more into a few of the microservices in the data plane:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解数据平面中的几个微服务：
- en: Astra DB Operator
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Astra DB Operator
- en: The Astra DB Operator manages the Kubernetes resources required for each database
    instance as described by a DBInstallation custom resource, as shown in [Figure 7-5](#astra_db_cluster_in_kubernetes).
    Similar to the Cass Operator project we discussed in [“Managing Cassandra in Kubernetes
    with Cass Operator”](ch06.html#managing_cassandra_in_kubernetes_with_c), the Astra
    DB Operator automates many of the operational tasks associated with managing a
    Cassandra cluster that would typically be performed by human operators using *nodetool*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Astra DB Operator 通过管理每个数据库实例所需的 Kubernetes 资源（如 DBInstallation 自定义资源所描述的），如
    [图 7-5](#astra_db_cluster_in_kubernetes) 所示。与我们在 [“使用 Cass Operator 在 Kubernetes
    中管理 Cassandra”](ch06.html#managing_cassandra_in_kubernetes_with_c) 中讨论的 Cass Operator
    项目类似，Astra DB Operator 自动化了许多与管理 Cassandra 集群相关的运维任务，这些任务通常由使用 *nodetool* 的运维人员执行。
- en: Coordination Service
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Coordination Service
- en: The Coordination Service is responsible for handling application queries including
    reads, writes, and schema management. Each Coordination Service is an instance
    of Stargate (as discussed in [“Enabling Developer Productivity with Stargate APIs”](ch06.html#enabling_developer_productivity_with_st)
    that exposes endpoints for CQL and other APIs, with an Astra DB–specific plug-in
    that enables it to route requests intelligently to Data Service instances to actually
    store and retrieve data. Factoring this compute-intensive routing functionality
    into its own microservice enables it to be scaled up or down based on query traffic,
    independent of the volume of data being managed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 协调服务负责处理应用程序查询，包括读取、写入和模式管理。每个协调服务都是 Stargate 的一个实例（如 [“使用 Stargate API 提升开发者生产力”](ch06.html#enabling_developer_productivity_with_st)
    中讨论的），它公开了用于 CQL 和其他 API 的端点，具有专门的 Astra DB 插件，使其能够智能地将请求路由到 Data Service 实例以实际存储和检索数据。将这种计算密集型的路由功能分解为自己的微服务，可以根据查询流量的增减而独立地进行扩展。
- en: Data Service
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据服务
- en: Each Data Service instance is responsible for managing a subset of the data
    for each assigned tenant based on its position in the Cassandra token ring. The
    Data Service takes a tiered approach to data storage, maintaining in-memory data
    structures such as memtables, using local disk for caching, commit logs and indexes,
    and object storage for longer-term persistence of SSTables. The usage of object
    storage is one of the key differentiators of Astra DB from other databases we’ve
    examined so far, and we’ll examine other benefits of this approach throughout
    this section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Data Service实例负责根据其在Cassandra令牌环中的位置管理每个分配租户的数据子集。Data Service采用分层的数据存储方法，维护内存数据结构（如memtables），使用本地磁盘进行缓存、提交日志和索引，并使用对象存储来长期保存SSTable。对象存储的使用是Astra
    DB与迄今为止我们研究过的其他数据库的主要区别之一，我们将在本节中进一步探讨这种方法的其他好处。
- en: Compaction Service
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩服务
- en: The Compaction Service is responsible for performing maintenance tasks including
    compaction and repair on SSTables in object storage. Compaction and repair are
    compute-intensive tasks that experienced Cassandra operators have historically
    scheduled for off-peak hours to limit their impact on cluster performance. In
    Astra DB, these tasks can be performed at any time the need arises without impacting
    query performance. The work is handled by a pool of Compaction Service instances
    which can scale up or down independently to generate repaired, compacted SSTables
    which are immediately accessible to Data Services.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Compaction Service负责在对象存储中的SSTable上执行包括压缩和修复在内的维护任务。压缩和修复是计算密集型任务，历来由经验丰富的Cassandra操作员安排在非高峰时段以限制其对集群性能的影响。在Astra
    DB中，这些任务可以在任何需要时执行，而不会影响查询性能。工作由一个可以独立扩展的Compaction Service实例池处理，生成修复的、压缩的SSTable，这些SSTable可以立即被Data
    Services访问。
- en: IAM Service
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: IAM服务
- en: All incoming application requests are routed through the Identity and Access
    Management (IAM) Service, which uses a standard set of roles and permissions defined
    in the control plane. While Cassandra has long had a pluggable architecture for
    authentication and authorization, factoring this out into its own microservice
    allows for more flexibility and support for additional providers such as Okta.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所有传入的应用程序请求都通过身份和访问管理（IAM）服务路由，该服务使用控制平面中定义的标准角色和权限集。尽管Cassandra长期以来一直具有可插拔的身份验证和授权架构，但将其分解为其自己的微服务可以提供更大的灵活性，并支持诸如Okta之类的其他提供者。
- en: The data plane includes additional services which have been omitted from [Figure 7-4](#astra_db_architecture)
    for simplicity, including a Commitlog Replayer Service for recovery of failed
    Data Service instances, and an Autoscaling Service which uses analytics and machine
    learning to recommend to the operator when to scale the number of instances of
    each service up or down.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面包括了在[图 7-4](#astra_db_architecture)中出于简化而省略的其他服务，包括用于恢复失败的Data Service实例的Commitlog
    Replayer Service，以及使用分析和机器学习建议操作员何时调整每个服务实例数量的Autoscaling Service。
- en: '[Figure 7-5](#astra_db_cluster_in_kubernetes) shows what a typical DBInstallation
    looks like in terms of Kubernetes resources. Let’s walk through a few typical
    interactions focusing on individual instances of key services to demonstrate how
    each resource plays its part.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-5](#astra_db_cluster_in_kubernetes)展示了在Kubernetes资源层面上典型的DBInstallation的样子。让我们逐个展示一些典型的交互，重点关注关键服务的各个实例，以演示每个资源如何发挥其作用。'
- en: A Kubernetes Ingress is configured for each cluster to manage incoming requests
    from client applications (1) and route requests to Coordinator Services by the
    tenant using a Kubernetes Service (2).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个集群配置了一个Kubernetes Ingress来管理来自客户应用程序的传入请求（1），并通过使用Kubernetes Service（2）将请求路由到按租户使用的协调器服务。
- en: '![Astra DB cluster in Kubernetes](assets/mcdk_0705.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![Astra DB在Kubernetes中的集群](assets/mcdk_0705.png)'
- en: Figure 7-5\. Astra DB cluster in Kubernetes
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. Astra DB在Kubernetes中的集群
- en: The Coordinator Service is a stateless service managed by a Deployment (3) which
    delegates authentication and authorization checks on each call to the IAM Service
    (4).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 协调器服务是由一个Deployment管理的无状态服务（3），它在每次调用时将身份验证和授权检查委托给IAM服务（4）。
- en: Authorized requests are then routed to one or more Data Services based on the
    tenant, again using a Kubernetes Service (5).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 授权请求然后根据租户路由到一个或多个Data Services，再次使用Kubernetes Service（5）。
- en: Data Services are managed using StatefulSets (6), which are used to assign each
    instance to a local PersistentVolume used for managing intermediate datafiles
    such as the commit log, which is populated immediately on writes. When possible,
    reads are served directly from in-memory data structures.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 数据服务使用StatefulSets（6）进行管理，用于为每个实例分配本地PersistentVolume，用于管理诸如提交日志等中间数据文件，该文件在写入时立即填充。在可能的情况下，从内存数据结构直接提供读取。
- en: As is typical for Cassandra and other LSM tree storage engines, the Data Service
    occasionally writes SSTable files out to a persistent store (7). For Astra DB,
    that persistent store is an external object store managed by the cloud provider
    for high availability. A separate object storage bucket is used per tenant to
    ensure data privacy.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与Cassandra和其他LSM树存储引擎一样，数据服务偶尔会将SSTable文件写入持久存储（7）。对于Astra DB来说，该持久存储是由云提供商管理的外部对象存储，以实现高可用性。为了确保数据隐私，每个租户使用单独的对象存储桶。
- en: The Compaction Service can perform compaction and repair on SSTables in the
    object store asynchronously (8), with no impact to write and read queries.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 补偿服务可以异步地在对象存储中对SSTable执行压缩和修复（8），不会对写入和读取查询产生影响。
- en: Astra DB also supports multiregion database clusters, which by definition span
    multiple Kubernetes clusters. Coordinator and Data Services are deployed across
    Datacenters (cloud regions) and racks (availability zones) using an approach similar
    to that described for K8ssandra in [“Deploying Multicluster Applications in Kubernetes”](ch06.html#deploying_multicluster_applications_in).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Astra DB还支持多区域数据库集群，其定义跨越多个Kubernetes集群。协调器和数据服务部署在数据中心（云区域）和机架（可用区），使用与K8ssandra中描述的类似方法，如在[“在Kubernetes中部署多集群应用程序”](ch06.html#deploying_multicluster_applications_in)中所述。
- en: Astra DB’s microservice architecture allows it to make more optimal use of compute
    and storage resources and isolate compute-intensive operations, leading to overall
    cost savings to operate Cassandra clusters in the cloud. These cost savings are
    extended by the addition of multitenant features that allow each cluster to be
    shared across multiple tenants. The [Astra DB whitepaper](https://oreil.ly/Zq0yc)
    describes a technique called *shuffle sharding* which is used to match each tenant
    to a subset of the available Coordinator and Data Services, effectively creating
    a separate Cassandra token ring per tenant. As the population of tenants in an
    Astra DB instance changes, this topology can be easily updated to rebalance load
    without downtime, and larger tenants can be configured to use their own dedicated
    databases (DBInstallations). This approach minimizes cost while meeting SLAs for
    performance and availability.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Astra DB的微服务架构使其能够更优化地利用计算和存储资源，并隔离计算密集型操作，从而节省云中操作Cassandra集群的总体成本。通过添加允许每个集群跨多个租户共享的多租户功能，这些成本节省得以延伸。[Astra
    DB白皮书](https://oreil.ly/Zq0yc)描述了一种称为*shuffle sharding*的技术，该技术用于将每个租户与可用的协调器和数据服务子集匹配，有效地为每个租户创建一个单独的Cassandra令牌环。随着Astra
    DB实例中租户的增加，可以轻松更新此拓扑以重新平衡负载而无需停机，并且可以配置较大的租户使用他们自己的专用数据库（DBInstallations）。此方法最小化成本同时满足性能和可用性SLA。
- en: In this section, we’ve focused on the architecture Astra DB uses to provide
    a multitenant, serverless Cassandra that embodies both cloud native and Kubernetes
    native principles using a completely different style of deployment. This continues
    the tradition of the Amazon Dynamo and Google BigTable papers in generating public
    discussion around novel database architectures. In addition, several open source
    projects mentioned in this book including Cass Operator, K8ssandra, and Stargate
    trace their origins to Astra DB. A lot of innovation is going on in areas such
    as the core database, control plane, change data capture, streaming integration,
    data migration, and more, so look for more open source contributions and architecture
    proposals from this team in the future.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们专注于Astra DB用于提供多租户、无服务器Cassandra的架构，该架构融合了云原生和Kubernetes原生原则，使用完全不同的部署风格。这延续了亚马逊Dynamo和谷歌BigTable论文的传统，在公开讨论新型数据库架构方面产生了广泛的讨论。此外，本书中提到的几个开源项目，包括Cass
    Operator、K8ssandra和Stargate，都源于Astra DB。在核心数据库、控制平面、变更数据捕获、流集成、数据迁移等领域，正在进行大量创新，因此请期待未来该团队更多的开源贡献和架构提案。
- en: What to Look for in a Kubernetes Native Database
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes原生数据库中需要关注的内容
- en: After everything you’ve learned in the past few chapters about what it takes
    to deploy and manage various databases on Kubernetes, we are in a great position
    to define what you should look for in a Kubernetes native database.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几章中所学的关于在 Kubernetes 上部署和管理各种数据库所需的一切之后，我们处于一个很好的位置来定义您在 Kubernetes 原生数据库中应寻找的内容。
- en: Basic Requirements
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本要求
- en: 'Following our cloud native data principles, the following are a few areas that
    should be considered basic requirements:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循我们的云原生数据原则，以下是应考虑的一些基本要求：
- en: Maximum leverage of Kubernetes APIs
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 充分利用 Kubernetes API
- en: The database should be as tightly integrated with Kubernetes APIs as possible
    (for example, using PersistentVolumes for both local and remote storage, using
    Services for routing rather than maintaining lists of IPs of other nodes, and
    so on). Kubernetes extension points described in [Chapter 5](ch05.html#automating_database_management_on_kuber)
    should be used to supplement built-in Kubernetes functionality.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库应尽可能与 Kubernetes API 紧密集成（例如，使用持久卷用于本地和远程存储，使用服务进行路由而不是维护其他节点 IP 列表等）。应使用第五章中描述的
    Kubernetes 扩展点来补充内置的 Kubernetes 功能。
- en: In some areas, the existing Kubernetes APIs may not provide the exact behavior
    required for a given database or other application, as demonstrated by the creation
    of alternate StatefulSet implementations by the Vitess and TiDB projects. In these
    cases, every effort should be made to donate improvements back to the Kubernetes
    project.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些领域，现有的 Kubernetes API 可能无法提供给定数据库或其他应用程序所需的确切行为，正如 Vitess 和 TiDB 项目创建备用 StatefulSet
    实现所示。在这些情况下，应尽一切努力将改进捐赠回 Kubernetes 项目。
- en: Automated, declarative management via operators
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过操作员进行自动化的声明式管理
- en: Databases should be deployed and managed on Kubernetes using operators and custom
    resources. Operators should serve as the primary control plane elements for managing
    databases. While it’s arguably helpful to have command-line tools or `kubectl`
    extensions that allow DBAs to intervene manually to optimize database performance
    and fix issues, these are ultimately functions that should be performed by an
    operator as it achieves the higher levels of maturity discussed in [Chapter 5](ch05.html#automating_database_management_on_kuber).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 应使用操作员和自定义资源在 Kubernetes 上部署和管理数据库。操作员应作为管理数据库的主要控制平面元素。虽然拥有允许 DBA 手动干预以优化数据库性能和解决问题的命令行工具或
    `kubectl` 扩展可能是有帮助的，但这些功能最终应由操作员执行，因为它实现了第五章中讨论的更高成熟度水平。
- en: The goal should be that all required changes to a database can be accomplished
    by updating the desired state in a custom resource and letting the operator handle
    the rest. We’ll be in a great place when we can configure a database in terms
    of service-level objectives such as latency, throughput, availability, and cost
    per unit. Operators can determine how many database nodes are needed, what compute
    and storage tiers to use, when to perform backups, and so on.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 目标应该是通过更新自定义资源中的所需状态并让操作员处理其余部分来完成对数据库的所有必需更改。当我们能够根据诸如延迟、吞吐量、可用性和单位成本等服务级目标配置数据库时，我们将处于一个很好的位置。操作员可以确定需要多少个数据库节点，使用什么计算和存储层级，何时执行备份等。
- en: Observable through standard APIs
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 可通过标准 API 进行观察
- en: We’re beginning to see common expectations for observability for data infrastructure
    on Kubernetes in terms of the familiar triad of metrics, logs, and tracing. The
    Prometheus-Grafana stack is somewhat of a de facto standard for metrics collection
    and visualization, with exposure of metrics from database services using the Prometheus
    format as a minimum criteria. Projects providing Prometheus integration should
    be flexible enough to provide their own dedicated stack, or push metrics to an
    existing installation shared with other applications.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始看到关于 Kubernetes 上数据基础设施的可观察性的共同期望，涉及度量、日志和跟踪的熟悉三元组。Prometheus-Grafana 堆栈在度量收集和可视化方面已成为事实上的标准，数据库服务使用
    Prometheus 格式暴露指标作为最低标准。提供 Prometheus 集成的项目应具有足够的灵活性，可以提供其自己的专用堆栈，或将指标推送到与其他应用程序共享的现有安装。
- en: Logs from all database application containers should be pushed to standard output
    (stdout) using sidecars if necessary—so they can be collected by log aggregation
    services. While it may take longer to see adoption for tracing, the ability to
    follow individual client requests through application calls down into the database
    tier through APIs such as OpenTracing will be an extremely powerful debugging
    tool for future cloud native applications.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据库应用程序容器的日志应通过侧车（如果必要）推送到标准输出(stdout)，以便日志聚合服务收集。虽然对于追踪可能需要更长时间来被广泛采用，但通过OpenTracing等API追踪单个客户端请求通过应用程序调用到数据库层将是未来云原生应用程序极为强大的调试工具。
- en: Secure by default
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 默认安全
- en: The Kubernetes project itself provides a great example of what it means to be
    secure by default—for example, by exposing access to ports on Pods and containers
    only when specifically enabled, and by providing primitives like Secrets that
    we can use to protect access to login credentials or sensitive configuration data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes项目本身提供了一个很好的例子，说明了默认安全的含义——例如，只有在特别启用时才公开Pods和容器的端口访问，并提供像Secrets这样的原语，我们可以用来保护登录凭据或敏感配置数据的访问。
- en: Databases and other infrastructure need to make use of these tools and adopt
    industry standards and best practices for zero trust (including changing default
    administrator credentials), limiting exposure of application and management APIs.
    Exposed APIs should prefer encrypted protocols such as HTTPS. Data stored in PersistentVolumes
    should be encrypted, whether this encryption is performed by the application,
    the database, or the StorageClass provider. Audit logs should be provided as part
    of application logging, especially with respect to actions that configure user
    access.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库和其他基础设施需要利用这些工具，并采用零信任的行业标准和最佳实践（包括更改默认管理员凭据），限制应用程序和管理API的暴露。暴露的API应优先使用诸如HTTPS之类的加密协议。存储在PersistentVolumes中的数据应该进行加密，无论是应用程序、数据库还是StorageClass提供者执行加密。审计日志应作为应用程序日志的一部分提供，特别是涉及配置用户访问的操作。
- en: To summarize, a Kubernetes native database is sympathetic to the way that Kubernetes
    works. It maximizes reuse of Kubernetes built-in capabilities instead of bringing
    along its own set of duplicative supporting infrastructure. The experience of
    using a Kubernetes native database is therefore very much like using Kubernetes
    itself.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Kubernetes原生数据库与Kubernetes的工作方式相符。它最大化地重用了Kubernetes内置的能力，而不是带来自己的重复的支持基础设施。因此，使用Kubernetes原生数据库的体验非常类似于使用Kubernetes本身。
- en: The Future of Kubernetes Native
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes原生的未来
- en: As these basic requirements and more advanced expectations for what it means
    to be Kubernetes native solidify, what comes next? We’re starting to see common
    patterns within projects deploying databases on Kubernetes that could point to
    where things are headed in the future. These are admittedly a bit fuzzier, but
    let’s try to bring a couple of them into focus.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对于什么是Kubernetes原生的基本要求和更高级期望的巩固，接下来会发生什么？我们开始看到在部署Kubernetes上的数据库项目中出现的共同模式，这可能指向未来的发展方向。尽管这些模式有些模糊，但让我们试着把其中一些聚焦起来。
- en: Scalability through multidimensional architectures
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多维度架构下的可伸缩性
- en: You may have noticed the repetition of several terms throughout the past few
    chapters such as *multicluster*, *multitenancy*, *microservices*, and *serverless*.
    A common thread uniting these terms is that they represent architectural approaches
    to scalability, as shown in [Figure 7-6](#architectural_approaches_for_scaling_in).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到在过去几章中多次重复出现的一些术语，比如*多集群*、*多租户*、*微服务*和*无服务器*。这些术语的一个共同主题是它们代表了可伸缩性的架构方法，正如[图 7-6](#architectural_approaches_for_scaling_in)所示。
- en: '![Architectural approaches for scaling in multiple dimensions](assets/mcdk_0706.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![多维扩展架构方法](assets/mcdk_0706.png)'
- en: Figure 7-6\. Architectural approaches for scaling in multiple dimensions
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 多维度扩展架构方法
- en: 'Consider how each of these approaches provides an independent axis for scalability.
    The visualization in [Figure 7-6](#architectural_approaches_for_scaling_in) depicts
    the impact of your application as a three-dimensional surface that grows as you
    scale along each axis:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑每种方法如何为可伸缩性提供独立的轴线。在[图 7-6](#architectural_approaches_for_scaling_in)中的可视化展示了你的应用程序的影响，作为一个随着每个轴向扩展而增长的三维表面：
- en: Microservice architectures
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构
- en: Microservice architectures break the various functions of a database into independently
    scalable services. The serverless approach builds on this, encouraging the isolation
    of persistent state to a limited number of stateful services or even external
    services as much as possible. Kubernetes storage APIs in the PersistentVolume
    subsystem make it possible to leverage both local and networked storage options.
    These trends allow a true separation of compute and storage, and scale these resources
    independently.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构将数据库的各种功能分解为独立可扩展的服务。无服务器方法基于此构建，鼓励将持久状态隔离到尽可能少的有状态服务或甚至外部服务。Kubernetes
    持久卷子系统中的存储 API 使得可以利用本地和网络存储选项。这些趋势允许真正地分离计算和存储，并独立地扩展这些资源。
- en: Multicluster
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群
- en: '*Multicluster* refers to the ability to scale an application across multiple
    Kubernetes clusters. Along with related terms like *multiregion*, *multi-datacenter*,
    and *multicloud*, this implies expanding the geographic footprint of the capabilities
    provided across potentially heterogeneous environments. This distribution of capability
    has positive implications for meeting users where they are with minimum latency,
    cloud provider cost optimization, and disaster recovery. As we discussed in [Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku),
    Kubernetes has historically not been as strong in its support for cross-cluster
    networking and service discovery. It will be interesting to track how databases
    and other applications take advantage of expected advances in Kubernetes federation
    in the coming years.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*多集群*指的是跨多个 Kubernetes 集群扩展应用程序的能力。与相关术语如*多区域*、*多数据中心*和*多云*一起，这意味着在可能是异构环境的多个地理位置扩展提供的功能。这种能力的分布对于在最小延迟、云提供商成本优化和灾难恢复方面满足用户需求具有积极的影响。正如我们在[第6章](ch06.html#integrating_data_infrastructure_in_a_ku)中讨论的那样，Kubernetes
    在跨集群网络和服务发现的支持上历史上并不是特别强大。值得关注的是，数据库和其他应用程序如何利用预期中的 Kubernetes 联合进步。'
- en: Multitenancy
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户
- en: This is the ability to share infrastructure between multiple users to achieve
    the most efficient use of resources. As the public cloud providers have demonstrated
    in their IaaS offerings, a multitenant approach can be very effective at providing
    users a low-cost, low-risk access to infrastructure for innovative new projects,
    and then providing additional resources as these applications grow. Adopting a
    multitenant approach for data infrastructure has great potentiial value as well,
    so long as security guarantees are properly met and there is a seamless transition
    path to dedicated infrastructure for high-volume users before they become “noisy
    neighbors.” At this point in time, Kubernetes does not provide explicit support
    for multitenancy, although Namespaces can be a useful tool for providing dedicated
    resources for specific users.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在多个用户之间共享基础设施的能力，以实现资源的最有效使用。正如公共云提供商在其 IaaS 提供中所展示的，多租户方法可以非常有效地为创新性新项目提供低成本、低风险的基础设施访问，并在这些应用程序增长时提供额外资源。在数据基础设施中采用多租户方法也具有很大的潜在价值，前提是安全保证得到适当满足，并且在它们成为“吵闹的邻居”之前，有一个无缝的过渡路径转向专用基础设施。在目前阶段，Kubernetes
    并未提供明确的多租户支持，尽管命名空间可以是为特定用户提供专用资源的有用工具。
- en: While you may not have immediate need for all three of these axes of scalability
    for applications or data infrastructure you’re building, consider how growing
    in each of them can enhance the overall value you’re offering the world.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你建立的应用程序或数据基础设施可能不立即需要这三个扩展性轴的全部，但考虑到在每个方面的增长如何可以增强你所提供的整体价值。
- en: Community-focused innovation through open source and cloud services
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过开源和云服务的社区重点创新
- en: Another pattern you may have noticed in our narrative is the continual innovation
    loop between open source database projects and DBaaS offerings. PingCAP took the
    open source MySQL and ClickHouse databases, created a database service leveraging
    Kubernetes to help it manage the databases at scale, and then released open source
    projects including TiDB and TiFlash. DataStax took open source Cassandra, factored
    it into microservices, added an API layer, and deployed it on Kubernetes for its
    Astra DB, and has created multiple open source projects including Cass Operator,
    K8ssandra, and Stargate. In the spirit of Dynamo, BigTable, Calvin and other papers,
    these companies have open source architectures as well.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的叙述中，您可能已经注意到的另一个模式是开源数据库项目和DBaaS产品之间的持续创新循环。PingCAP采用了开源的MySQL和ClickHouse数据库，并利用Kubernetes创建了一个数据库服务，帮助其在规模上管理数据库，然后发布了包括TiDB和TiFlash在内的开源项目。DataStax采用了开源的Cassandra，将其分解为微服务，添加了API层，并在Kubernetes上部署了其Astra
    DB，并创建了多个开源项目，包括Cass Operator、K8ssandra和Stargate。这些公司在Dynamo、BigTable、Calvin等论文的精神中也开源了架构。
- en: This innovation loop mirrors that of the larger Kubernetes community, in which
    the major cloud providers and storage vendors have helped drive the maturation
    of the core Kubernetes control plane and PersistentVolume subsystem, respectively.
    It’s interesting to observe that the highest momentum and fastest cycle time occurs
    within innovation loops that center around cloud services, rather than around
    the classic open core model focused on enterprise versions of open source projects.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种创新循环反映了更大的Kubernetes社区的情况，其中主要的云服务提供商和存储供应商帮助推动了核心Kubernetes控制平面和PersistentVolume子系统的成熟发展。有趣的是观察到，围绕云服务的创新循环中具有最高的动力和最快的周期，而不是围绕传统的开源项目企业版本的经典开放核心模型。
- en: As a software vendor, providing a cloud service allows you to iterate and evaluate
    new architectures and features more quickly. Flowing these innovations back to
    open source allows you to grow adoption by supporting a flexible consumption model.
    Both “run it yourself” and “rent it from us” become legitimate deployment options
    for your customers, with the ability to flex between approaches for different
    use cases. Customers gain confidence in the overall maturity and security of your
    technology, knowing that the open source version they can inspect and contribute
    to is largely the same as what you are running in your DBaaS.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 作为软件供应商，提供云服务使您能够更快地迭代和评估新架构和功能。将这些创新反馈到开源项目中，使您能够通过支持灵活的消费模型来增加采用率。对于客户来说，“自己运行”和“租用我们的服务”都成为了合法的部署选项，可以根据不同的使用案例灵活选择。客户可以对您的技术的整体成熟度和安全性充满信心，因为他们可以检查和贡献的开源版本与您在DBaaS中运行的基本相同。
- en: 'A final side effect of these innovation trends is an implicit pull toward proven
    architectures and components. Consider these examples:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些创新趋势的最终副作用之一是对经过验证的架构和组件的隐性推动。考虑以下例子：
- en: etcd is used as a metadata store across multiple projects we’ve examined in
    this book, including Vitess and Astra DB.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd被用作我们在本书中多个项目中（包括Vitess和Astra DB）的元数据存储。
- en: TiDB leverages the architecture of F1, implemented the Raft consensus protocol,
    and extended the ClickHouse columnar store.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TiDB利用了F1的架构，实现了Raft一致性协议，并扩展了ClickHouse的列存储。
- en: Astra DB leverages both the PersistentVolume subsystem and S3-compliant object
    storage.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Astra DB利用了PersistentVolume子系统和符合S3标准的对象存储。
- en: Instead of inventing new technologies to solve problems like metadata management
    and distributed transactions, these projects are investing their innovation in
    new features, developer experience, and the scalability axes we’ve examined in
    this chapter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是发明新技术来解决诸如元数据管理和分布式事务等问题，这些项目正在将创新投入到新功能、开发者体验和我们在本章中探讨过的可扩展性轴上。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve taken a deep look at TiDB and Astra DB to search out
    what makes them Kubernetes native. What was the point of this exercise? Our hope
    is that this analysis provides a deeper understanding to help consumers ask more
    insightful questions about the data infrastructure they are consuming, and to
    help those building data infrastructure and ecosystems to create technology that
    meets those expectations. We believe that data infrastructure that is not only
    cloud native but also Kubernetes native will lead to the best outcomes for everyone
    in terms of performance, availability, and cost.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了 TiDB 和 Astra DB，以找出它们成为 Kubernetes 本地化的特质。这项工作的目的是什么？我们希望这种分析能够提供更深入的理解，帮助消费者提出更具洞察力的关于所使用数据基础设施的问题，并帮助那些构建数据基础设施和生态系统的人创建能够满足这些期望的技术。我们相信，不仅是云原生而且是
    Kubernetes 本地化的数据基础设施将为每个人带来最佳的性能、可用性和成本效益的结果。
