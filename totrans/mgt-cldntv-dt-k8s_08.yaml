- en: Chapter 7\. The Kubernetes Native Database
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The software industry is flush with terms that define major trends in a single
    word or short phrase. You can see one of them in the title of this book: *cloud
    native*. Another example is *microservice*, a major architectural paradigm that
    touches much of the technology we’re discussing here. More recently, terms like
    *Kubernetes native* and *serverless* have emerged.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: While succinct and catchy, distilling a complex topic or trend down to a single
    sound bite leaves room for ambiguity, or at least for reasonable questions such
    as “What does this *actually* mean?” To further muddy the waters, terms such as
    these are frequently used in the context of marketing products as a way to gain
    leverage or differentiate against other competitive offerings. Whether the content
    you’re consuming makes an overt statement or it’s just the subtext, you may have
    wondered whether a given technology must be better to run on Kubernetes than other
    offerings because it’s labeled *Kubernetes native*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, for these terms to be useful to us in evaluating and picking the
    right technologies for our applications, the real task is to unpack what they
    really mean, as we did with the term *cloud native data* in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra).
    In this chapter, we’ll look at what it means for data technology to be Kubernetes
    native and see if we can arrive at a definition that we can agree on. To do this,
    we’ll examine a couple of projects that claim these terms and derive the common
    principles: TiDB and Astra DB. Are you ready? Let’s dive in!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Why a Kubernetes Native Approach Is Needed
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s discuss why the idea of a Kubernetes native database came up in
    the first place. Up to this point in the book, we’ve focused on deployment of
    existing databases on Kubernetes including MySQL and Cassandra. These are mature
    databases that were around before Kubernetes existed and have proven themselves
    over time. They have large install bases and user communities, and because of
    this investment, you can see why there’s a large incentive to run these databases
    in Kubernetes environments, and why there has been such interest in creating operators
    to automate them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, you’ve probably noticed some of the awkwardness in adapting
    these databases to run on Kubernetes. While it is pretty straightforward to point
    a database to Kubernetes-based storage just by changing a mount path, tighter
    integration with Kubernetes to manage databases that consist of multiple nodes
    can be a bit more involved. This can range from relatively simple tasks like deploying
    a legacy management UI in a Pod and exposing access to the HTTP port, to the more
    complex deployment of sidecars that we saw in [Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku)
    to provide APIs for management and metrics collection.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The recognition of this complexity has led some innovators to develop new databases
    that are designed to be Kubernetes native from day one. It’s a well-known axiom
    in the database industry that it takes 5–10 years for a new database engine to
    reach a point of maturity. Because of this, these Kubernetes native databases
    tend not to be completely new implementations, but rather refactoring of existing
    databases into microservices that can be scaled independently, while maintaining
    compatibility with existing APIs that developers are accustomed to. Thus, the
    trend of decomposing the monolith has arrived at the data tier. The emerging generation
    of databases will be based on new architectures to truly leverage the benefits
    of Kubernetes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us assess what might qualify these new databases as Kubernetes native,
    let’s use the cloud native data principles introduced in [“Principles of Cloud
    Native Data Infrastructure”](ch01.html#principles_of_cloud_native_data_infrast)
    as a guide to formulate some questions to ask how a database interacts with Kubernetes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 1: Leverage compute, network, and storage as commodity APIs'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: How does the database use Kubernetes compute resources (Pods, Deployments, StatefulSets),
    network resources (Services and Ingress), and storage resources (PersistentVolumes,
    PersistentVolumeClaims, StorageClasses)?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 2: Separate the control and data planes'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Is the database deployed and managed by an operator? What custom resources does
    it define? Are other workloads in the control plane besides the operator?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 3: Make observability easy'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: How do the various services in the architecture expose metrics and logs to support
    collection by the Kubernetes control plane and third-party extensions?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 4: Make the default configuration secure'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Do the database and associated operator use Kubernetes Secrets to share credentials,
    and use Roles and RoleBindings to manage access by Role? Do services minimize
    the number of exposed points and require secure access to them?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 5: Prefer declarative configuration'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Extending Principle 2, can the database be managed entirely by creating, updating,
    or deleting Helm charts and Kubernetes resources (whether built-in or custom resources),
    or are other tools required?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: In the sections that follow, we’ll explore the answers to these questions for
    two databases and see what else we can learn about what it means to be Kubernetes
    native. That will help us to build a checklist at the end of this chapter that
    will help solidify our definition. (See [“What to Look for in a Kubernetes Native
    Database”](#what_to_look_for_in_a_kubernetes_native) for what we come up with.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid Data Access at Scale with TiDB
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The databases that have received most of our focus in this book so far represent
    two major trends in database architecture that trace their lineage back for decades
    or more. MySQL is a relational database that provides its own flavor of the Standard
    Query Language (SQL), based on rules developed by Edgar Codd in the 1970s.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In the early 2000s, companies building web-scale applications began to push
    the limits of what could be accomplished with the relational databases of the
    day. As database sizes began growing beyond what could feasibly be managed on
    a single instance, techniques like sharding were used to scale across multiple
    instances. These were frequently expensive, difficult to operate, and not always
    reliable.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to this need, Cassandra and other so-called *NoSQL* databases emerged
    in a period of intense innovation and experimentation. These databases provide
    linear scalability through adding additional nodes. They offer different data
    models, or ways of representing data: for example, key-value stores such as Redis,
    document databases such as MongoDB, graph databases such as Neo4j, and others.
    NoSQL databases tended to provide weaker consistency guarantees and omit support
    for more complex behaviors like transactions and joins to achieve high performance
    and availability at scale, a trade-off documented by Eric Brewer in his [CAP theorem](https://oreil.ly/aJq6M).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Because of the continued developer demand for traditional relational semantics
    such as strong consistency, transactions, and joins, multiple teams began to revive
    the idea of supporting these capabilities in distributed databases starting around
    2012\. These so-called *NewSQL* databases were based on more efficient and performant
    consensus algorithms. Two key papers helped drive the emergence of the NewSQL
    movement. First, the [Calvin paper](https://oreil.ly/HLw2M) introduced a global
    consensus protocol which represented a more reliable and performant approach for
    guaranteeing strong consistency, later adopted by FaunaDB and other databases.
    Second, Google’s [Spanner paper](https://oreil.ly/zDl5z) introduced a design for
    a distributed relational database using sharding and a new consensus algorithm
    that leveraged the improved ability of cloud infrastructure to provide time synchronization
    across datacenters. Besides Google Spanner, this approach was implemented by databases
    including CockroachDB and YugabyteDB.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: More on Consistency and Consensus
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we don’t have space in this book to dive deeply into the trade-offs between
    various consensus algorithms and how they are used to provide various data consistency
    guarantees, an understanding of these concepts is helpful in choosing the right
    data infrastructure for your cloud applications. If you’re interested in learning
    more in this area, Martin Kleppmann’s [*Designing Data-Intensive Applications*](https://oreil.ly/6ndic)
    (O’Reilly) is a great source, especially Chapter 9, “Consistency and Consensus”.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[TiDB](https://oreil.ly/jZNAI) (where *Ti* stands for *Titanium*) represents
    a continuation of the NewSQL trend in the cloud native space. TiDB is an open
    source, MySQL-compatible database that supports both transactional and analytic
    workloads. It was initially developed and is primarily supported by PingCAP. While
    TiDB is a database designed to embody cloud native principles of scalability and
    elasticity, what makes it especially interesting for our discussion is that it
    has been explicitly designed to run on Kubernetes and to rely on capabilities
    provided by the Kubernetes control plane. In this way, one could argue that TiDB
    is not merely a Kubernetes native database, but also a Kubernetes *only* database.
    Let’s dig into the details.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: TiDB Architecture
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A key characteristic of TiDB which distinguishes it from other databases we’ve
    examined so far in this book is its ability to support transactional and analytic
    workloads. This approach, known as *hybrid transactional/analytical processing
    (HTAP)*, supports both types of queries without a separate extract, transform,
    and load (ETL) process. As shown in [Figure 7-1](#tidb_architecture), TiDB does
    this by providing two database engines under the hood: TiKV and TiFlash. This
    approach was inspired by Google’s [F1 project](https://oreil.ly/lakAf), a layer
    built on top of Spanner.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![TiDB architecture](assets/mcdk_0701.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. TiDB architecture
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One key aspect that gives TiDB a cloud native architecture is the packaging
    of compute and storage operations into separate components, each of which is composed
    of independently scalable services organized in clusters. Let’s examine the roles
    of each of these components:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: TiDB
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Each TiDB instance is a stateless service that exposes a MySQL endpoint to client
    applications. TiDB parses incoming SQL requests and uses metadata from the Placement
    Driver (PD) to create an execution plan containing queries to make on specific
    TiKV and TiFlash nodes in the storage cluster. TiDB executes these queries, assembles
    the results, and returns to the client application. The TiDB cluster is typically
    deployed with a proxy in front of it to provide load balancing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: TiKV
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The storage cluster consists of a mixture of TiKV and TiFlash nodes. First,
    let’s examine [*TiKV*](https://tikv.org), an open source, distributed key-value
    database that uses [RocksDB](http://rocksdb.org) as its backing storage engine.
    TiKV exposes a custom distributed SQL API that the TiDB nodes use to execute queries
    to store and retrieve data and manage distributed transactions. TiKV stores multiple
    replicas of your data, typically at least three, to support high availability
    and automatic failover. TiKV is a [CNCF graduated project](https://oreil.ly/ypLlC)
    which can be used independently from TiDB, as we’ll discuss later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: TiFlash
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The storage cluster also includes TiFlash nodes, to which data is replicated
    from TiKV nodes as it is written. TiFlash is a columnar database based on the
    open source [ClickHouse analytic database](https://oreil.ly/PCVlg), which means
    that it organizes data storage in columns rather than rows. Columnar databases
    can provide a significant performance advantage for analytic queries requiring
    the extraction of the same column across multiple rows.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: TiSpark
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: This library is built for Apache Spark to support complex OLAP queries. TiSpark
    integrates with the Spark Driver and Spark Executors, providing the capability
    to ingest data from TiFlash instances using the distributed SQL API. We’ll examine
    the Spark architecture and the details of deploying Spark on Kubernetes in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Placement Driver (PD)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The PD manages the metadata for a TiDB installation. PD instances are deployed
    in a cluster of at least three nodes. TiDB uses a range-based sharding mechanism
    where the keys in each table are divided into ranges called *regions*. The PD
    is responsible for determining the ranges of data assigned to each region, and
    the TiKV nodes that will store the data for each region. It monitors the amount
    of data in each region and splits regions that become too large, to facilitate
    scaling up, and merging smaller regions to scale down.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Because the TiDB architecture consists of well-defined interfaces between the
    components, it is an extensible architecture in which different pieces can be
    plugged in. For example, TiKV provides a distributed key-value storage solution
    that can be reused in other applications. The [TiPrometheus project](https://oreil.ly/PkmqK)
    is an example, providing a Prometheus-compliant compute layer on top of TiKV.
    For another example, you could provide an alternate implementation of TiKV that
    implements the distributed SQL API on top of a different storage engine.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Pluggable Storage Engines
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter so far, we’ve made several mentions of “storage engines” or
    “database engines.” This term refers to the part of the database that manages
    the storage and retrieval of data on persistent media. In distributed databases,
    a distinction is often made between the storage engine and the proxy layer which
    sits on top of it to manage data replication between nodes. Chapter 3, “Storage
    and Retrieval,” from [*Designing Data-Intensive Applications*](https://oreil.ly/6ndic)
    includes discussion of storage engine types such as the B-trees used in most relational
    databases and the log-structured merge tree (LSM tree) used in Apache Cassandra
    and other NoSQL databases.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'One interesting aspect of TiDB is the way in which it reuses existing technology.
    We’ve seen examples of this in the usage of components including RocksDB and Spark.
    TiDB also uses algorithms developed by other organizations. Here are a couple
    of examples:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Raft consensus protocol
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: At the TiDB layer, the [Raft consensus protocol](https://oreil.ly/Oi6Dk) is
    used to manage consistency between replicas. Raft is similar to the Paxos algorithm
    used by Cassandra in terms of its behavior, but it’s designed to be much simpler
    to learn and use. TiDB uses a separate Raft group for each region, where a group
    typically consists of a leader and two or more replicas. If a leader node is lost,
    an election is run to select a new leader, and a new replica can be added to ensure
    the desired number of replicas. In addition, the TiFlash nodes are configured
    as a special type of replica called *learner replicas*. Data is replicated to
    learner replicas from the TiDB nodes, but they cannot be selected as a leader.
    You can read more about how TiDB uses Raft for [high availability](https://oreil.ly/BddzV)
    and other related topics on the [PingCAP blog](https://oreil.ly/Y2YuS).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Percolator transaction management
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: At the TiDB layer, distributed transactions are supported using an implementation
    of the [Percolator algorithm](https://oreil.ly/heMho) with optimizations specific
    to the TiDB project. Percolator was originally developed at Google for supporting
    incremental updates to search indexes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: One of the arguments we’re making in this chapter is that part of what it means
    for data infrastructure to be cloud native is to compose existing APIs, services,
    and algorithms wherever possible, and TiDB is a great example of this.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Deploying TiDB in Kubernetes
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While TiDB can be deployed in a variety of ways including bare metal and VMs,
    the TiDB team has invested a large effort in tooling and documentation to make
    TiDB a truly Kubernetes native database. The [TiDB Operator](https://oreil.ly/xZtGq)
    manages TiDB clusters in Kubernetes, including deployment, upgrade, scaling, backup
    and restore, and more.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The operator [documentation](https://oreil.ly/iIZc0) provides [quick start guides](https://oreil.ly/5heDA)
    for desktop Kubernetes distributions such as kind, minikube, and Google Kubernetes
    Engine (GKE). These instructions guide you through steps including installing
    CRDs and the TiDB operator using Helm, and a simple TiDB cluster including monitoring
    services. We’ll use the quick start instructions as a vehicle to talk about what
    makes TiDB a Kubernetes native database.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Installing the TiDB CRDs
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After making sure you have a Kubernetes cluster that meets the defined prerequisites
    such as having a [default StorageClass](https://oreil.ly/7myfI), the first step
    in deploying TiDB using the operator is installing the CRDs used by the operator.
    This is done using an instruction such as the following (note the actual operator
    version number `v1.3.2` may vary):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This results in the creation of several CRDs, which you can observe by running
    the command `kubectl get crd` as we have done in previous chapters. We’ll quickly
    discuss the purpose of each resource since several of them hint at additional
    features of interest:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The TidbCluster is the primary resource that describes the desired configuration
    of a TiDB cluster. We’ll look at an example later.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbCluster 是描述 TiDB 集群所需配置的主要资源。稍后我们将看一个示例。
- en: The TidbMonitor resource is used to deploy a Prometheus-based monitoring stack
    to observe one or more TidbClusters. As we have seen with other projects, Prometheus
    (or at least its API) has become a de facto standard for metrics collection for
    databases and other infrastructure deployed on Kubernetes.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbMonitor 资源用于部署基于 Prometheus 的监控堆栈，以观察一个或多个 TidbCluster。正如我们在其他项目中所见，Prometheus（或其API）已成为在
    Kubernetes 上部署的数据库和其他基础设施的度量收集的事实标准。
- en: The Backup and Restore resources represent the actions of performing a backup
    or restoring from a backup. This is similar to other operators we’ve examined
    previously from the Vitess (see [“PlanetScale Vitess Operator”](ch05.html#planetscale_vitess_operator))
    and K8ssandra ([Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku))
    projects. The TiDB Operator also provides a BackupSchedule resource that can be
    used to configure regular backups.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和恢复资源表示执行备份或从备份中恢复的操作。这类似于我们之前从 Vitess（参见[“PlanetScale Vitess Operator”](ch05.html#planetscale_vitess_operator)）和
    K8ssandra（[第 6 章](ch06.html#integrating_data_infrastructure_in_a_ku)）项目中研究过的其他操作器。TiDB
    Operator 还提供了 BackupSchedule 资源，可用于配置定期备份。
- en: The TidbInitializer is an optional resource that you can create to perform [initialization
    tasks](https://oreil.ly/qFsmu) on a TidbCluster, including setting administrator
    credentials and executing SQL statements for schema creation or initial data loading.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbInitializer 是一个可选资源，您可以创建它来执行[TidbCluster 上的初始化任务](https://oreil.ly/qFsmu)，包括设置管理员凭据并执行用于架构创建或初始数据加载的
    SQL 语句。
- en: The TidbClusterAutoScaler is another optional resource which can be used to
    configure [auto-scaling](https://oreil.ly/wVbf2) behavior of a TidbCluster. The
    number of TiKV or TiDB nodes in a TidbCluster can be configured to scale up or
    down between minimum and maximum limits based on CPU utilization. The addition
    of scaling rules based on other metrics is on the project roadmap. As we discussed
    in [“Choosing Operators”](ch05.html#choosing_operators), auto-scaling is considered
    a feature of an operator at Level 5 or Autopilot, the highest maturity level.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbClusterAutoScaler 是另一个可选资源，用于配置 TidbCluster 的[自动扩展行为](https://oreil.ly/wVbf2)。可以根据
    CPU 利用率配置 TidbCluster 中 TiKV 或 TiDB 节点的最小和最大限制进行扩展。基于其他指标的扩展规则的添加已包含在项目路线图中。正如我们在[“选择运算符”](ch05.html#choosing_operators)中讨论的那样，自动缩放被认为是运算符的
    Level 5 或 Autopilot 功能，是最高成熟度级别的特性。
- en: The TidbNGMonitoring is an optional resource that configures a TidbCluster to
    enable [continuous profiling](https://oreil.ly/2n8k5) down to the system call
    level. The resulting profiling data and flame graph visualizations can be observed
    using the [TiDB Dashboard](https://oreil.ly/23pLs), which is deployed separately.
    This is typically used by project engineers looking to optimize the database,
    but application and platform developers may find this useful as well.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TidbNGMonitoring 是一个可选资源，配置 TidbCluster 以启用[连续剖析](https://oreil.ly/2n8k5)，可以达到系统调用级别。产生的剖析数据和火焰图可通过单独部署的[TiDB
    仪表盘](https://oreil.ly/23pLs)进行观察。通常由项目工程师用于优化数据库，但应用和平台开发人员也可能会发现其有用。
- en: The DMCluster resource is used to deploy an instance of the [TiDB Data Migration](https://oreil.ly/C5NG0)
    (DM) platform that supports migration of MySQL and MariaDB database instances
    into a TidbCluster. It can also be configured to migrate from an existing TiDB
    installation outside of Kubernetes to a TidbCluster. The ability to deploy data
    migration services alongside a destination TidbCluster in Kubernetes managed by
    the same operator is a great example of what it means to develop data ecosystems
    in Kubernetes, a pattern that we hope to see more of in the future.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMCluster 资源用于部署 TiDB 数据迁移（DM）平台的一个实例，支持将 MySQL 和 MariaDB 数据库实例迁移到 TidbCluster。还可以配置从在
    Kubernetes 外部现有的 TiDB 安装迁移到 TidbCluster。在由同一运算符管理的 Kubernetes 中部署数据迁移服务与目标 TidbCluster
    并行的能力，是在 Kubernetes 中开发数据生态系统的一个极好的示例，这是我们希望在未来看到更多的模式。
- en: For the remainder of this section, we’ll focus on the TidbCluster and TidbMonitoring
    resources.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将重点关注 TidbCluster 和 TidbMonitoring 资源。
- en: Installing the TiDB Operator
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 TiDB Operator
- en: 'After installing the CRDs, the next step is to install the TiDB Operator using
    Helm. You’ll need to add the Helm repository first before installing the TiDB
    Operator in its own Namespace:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can watch the resulting Pods come online using `kubectl get pods` and referencing
    the `tidb-admin` Namespace. [Figure 7-2](#installing_the_tidb_operator_and_crds)
    provides a summary of the elements that you’ve installed up to this point. This
    includes Deployments to manage the TiDB Operator (labeled as `tidb-controller-manager`)
    and the TiDB Scheduler.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The TiDB Scheduler is an optional extension to the Kubernetes built-in scheduler.
    While it is deployed by default as part of the TiDB Operator, it can be disabled.
    Assuming the TiDB Scheduler is not disabled, using it for a specific TidbCluster
    still requires opting in by setting the `schedulerName` property to `tidb-scheduler`.
    If this property is set, the TiDB Operator will assign the TiDB Scheduler as the
    scheduler that Kubernetes will use when creating TiKV, and PD Pods.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The TiDB Scheduler extends the Kubernetes built-in scheduler to add custom scheduling
    rules for Pods that are part of a TidbCluster, helping to achieve high availability
    of the database while spreading the load evenly across the available Worker Nodes
    in the Kubernetes cluster. While for many types of infrastructure, the existing
    mechanisms Kubernetes offers for influencing the default scheduler such as affinity
    rules, taints, and tolerations are sufficient, TiDB provides a useful example
    of when and how to implement custom scheduling logic. We’ll look at Kubernetes
    scheduler extensions in more detail in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing the TiDB Operator and CRDs](assets/mcdk_0702.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Installing the TiDB Operator and CRDs
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TiDB Operator Helm Chart Options
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This installation omits usage of a *values.yaml* file, but you can see the
    available options by running following command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This includes the option to disable the TiDB Scheduler.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Creating a TidbCluster
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the TiDB Operator has been installed, you’re ready to create a TidbCluster
    resource. While many [example configurations](https://oreil.ly/66uf7) are available
    in the TiDB Operator GitHub repository, let’s use the one referenced in the quick
    start guide:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'While the TidbCluster is being created, you can reference the contents of this
    file, which look something like this (with comments and some details removed):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice that this results in the creation of a TidbCluster named `basic` in the
    `tidb-cluster` Namespace, with one replica each of TiDB, TiKV, and PD, using the
    standard PingCAP images for each. Additional options are used to specify the minimum
    amount of compute and storage resources required to achieve a functioning cluster.
    No TiFlash nodes are included in this simple configuration.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: TidbCluster API
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full list of options for a TidbCluster can be found as part of the [API](https://oreil.ly/XoC02)
    available in the GitHub repository. This same page includes options for the other
    CRDs used by the TiDB Operator. As you explore the options for these CRDs, you’ll
    see evidence of the common practice of allowing many of the options that will
    be used to specify underlying resources to be overridden (for example, the Pod
    specification that will be set on a Deployment).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库中的[API](https://oreil.ly/XoC02)中找到TidbCluster的完整选项列表。同一页还包括TiDB
    Operator使用的其他CRD的选项。当您探索这些CRD的选项时，您会看到允许覆盖许多用于指定底层资源的选项的常见做法（例如，在部署中设置的Pod规范）的证据。
- en: We encourage you to take the opportunity to use `kubectl` or your favorite visualization
    tool to explore the resources created as part of the TidbCluster, a summary of
    which is provided in [Figure 7-3](#a_basic_tidbcluster).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您利用`kubectl`或您喜欢的可视化工具，探索作为TidbCluster一部分创建的资源。这些资源的摘要在[图 7-3](#a_basic_tidbcluster)中提供。
- en: '![A basic TidbCluster](assets/mcdk_0703.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![一个基本的TidbCluster](assets/mcdk_0703.png)'
- en: Figure 7-3\. A basic TidbCluster
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 一个基本的TidbCluster
- en: As you can see, the TiDB Operator creates StatefulSets to manage the TiDB, TiKV,
    and Placement Driver instances, allocating a PVC for each instance. As an I/O-intensive
    application, the default configuration is to use local PersistentVolumes as the
    backing store.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，TiDB Operator创建了StatefulSets来管理TiDB、TiKV和Placement Driver实例，并为每个实例分配了PVC。作为一个I/O密集型应用程序，默认配置是使用本地PersistentVolumes作为后备存储。
- en: In addition, a Deployment is created to run a Discovery Service which the various
    components use to learn of each other’s location. The Discovery Service performs
    a similar role to that of etcd in other data technologies we’ve examined in the
    book. The TiDB Operator also configures services for each StatefulSet and Deployment
    that facilitate communication within the TiDB cluster as well as exposing capabilities
    to external clients.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还创建了一个Deployment来运行Discovery Service，各个组件使用该服务来了解彼此的位置。Discovery Service在本书中我们检查的其他数据技术中扮演了类似于etcd的角色。TiDB
    Operator还为每个StatefulSet和Deployment配置了服务，以促进TiDB集群内部的通信，并向外部客户端公开功能。
- en: 'The TiDB Operator supports the deployment of a Prometheus monitoring stack
    that can manage one or more TiDB clusters. You can add monitoring to the cluster
    created previously using the following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TiDB Operator支持部署Prometheus监控堆栈，可以管理一个或多个TiDB集群。您可以使用以下命令向先前创建的集群添加监控：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While this is deploying, let’s examine the contents of the *tidb-monitor.yaml*
    configuration file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署过程中，让我们来查看*tidb-monitor.yaml*配置文件的内容：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the TidbMonitor resource can point to one or more TidbClusters.
    This TidbMonitor is configured to manage the `basic` cluster you created previously.
    The TidbMonitor resource also allows you to specify the versions of Prometheus,
    Grafana, and additional tools that are used to initialize and update the monitoring
    stack. If you examine the contents of the `tidb-cluster` Namespace, you’ll see
    additional workloads that have been created to manage these elements.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，TidbMonitor资源可以指向一个或多个TidbCluster。该TidbMonitor配置为管理您之前创建的`basic`集群。TidbMonitor资源还允许您指定用于初始化和更新监控堆栈的Prometheus、Grafana和其他工具的版本。如果您检查`tidb-cluster`命名空间的内容，您将看到已创建用于管理这些元素的附加工作负载。
- en: TiDB uses the Prometheus stack in a similar way to the K8ssandra project, as
    we discussed in [“Unified Monitoring Infrastructure with Prometheus and Grafana”](ch06.html#unified_monitoring_infrastructure_with).
    In both of these projects, the Prometheus stack is supported as an optional extension
    to provide a monitoring capability you can use with very little customization.
    The configurations and provided visualizations focus on the key metrics that drive
    awareness of database health. Even if you are already managing your own monitoring
    infrastructure or using a third-party software-as-a-service (SaaS) solution, the
    configurations and charts can give you a head start on incorporating database
    monitoring into the rest of your observability approach.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, TiDB is a database with a flexible, extensible architecture
    that has been designed with cloud native principles in mind. It also has a strong
    bias toward being able to deploy and manage a database effectively in Kubernetes
    and has provided us with some valuable insights on what it means to be Kubernetes
    native. Consult the TiDB documentation for more information on features such as
    [deploying to multiple Kubernetes clusters](https://oreil.ly/NPHxy).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Cassandra with DataStax Astra DB
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the advent of cloud computing in the early 2000s, public cloud providers
    and infrastructure vendors have made continual advances in commoditizing various
    layers of our architectural stacks as service offerings. This trend began with
    offering compute, network, and storage as *infrastructure as a service* (IaaS)
    and proceeded into other trends including *platform as a service* (PaaS), *software
    as a service* (SaaS), and *functions as a service* (FaaS), sometimes conflated
    with the term *serverless*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Most pertinent to our investigation here is the emergence of managed data infrastructure
    offerings known as *database as a service* (DBaaS). This category includes the
    following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Traditional databases offered as a managed cloud service, such as Amazon Relational
    Database Service (RDS) and PlanetScale
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud databases like Google BigTable, Amazon Dynamo, and Snowflake that are
    available only as cloud offerings
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managed NoSQL or NewSQL databases that can also be run on premises under an
    open source or source available license—for example, MongoDB Atlas, DataStax Astra
    DB, TiDB, and Cockroach DB
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the past several years, many of the vendors behind these DBaaS services
    have begun migrating onto Kubernetes to automate operations, manage compute resources
    more efficiently, and make their solutions portable across clouds. DataStax was
    one of several vendors that began offering Cassandra as a service. These vendors
    typically used an architecture based on running traditional Cassandra clusters
    in a cloud environment, with various “glue code” to integrate aspects like networking,
    monitoring, and management that didn’t quite fit target deployment environments
    like Kubernetes and public cloud IaaS. These include techniques like using sidecars
    to collect metrics and logs, or deploying Cassandra nodes using StatefulSets to
    manage scaling up and down in an orderly fashion.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with these workarounds for running in Kubernetes, Cassandra’s monolithic
    architecture doesn’t readily promote the separation of compute and storage, which
    can lead to some awkwardness when scaling. You scale up a Cassandra cluster by
    adding additional nodes, where each node has the following capabilities:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Coordination
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Receiving read and write requests and forwarding them to other nodes as needed
    to achieve the requested number of replicas (also known as *consistency level*)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Writing and reading
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Writing data to in-memory cache (memtables) and persistent storage (SSTables),
    and reading it back as needed
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Compaction and repair
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Since Cassandra is an LSM-tree database, it does not update datafiles once they
    are written to persistent storage. Compaction and repair are tasks that run in
    the background as separate threads. Compaction helps Cassandra stay performant
    by consolidating SSTables written at different times, ignoring obsolete and deleted
    values. Repair is the process of comparing stored values across nodes to ensure
    consistency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Each node in a Cassandra cluster implements all of these capabilities and consumes
    equivalent compute and storage resources. This makes it difficult to scale compute
    and storage independently and can lead to situations where a cluster is overprovisioned
    in compute or storage resources.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2021, DataStax published a paper entitled [“DataStax Astra DB: Designing
    a Serverless Cloud-Native Database-as-a-Service”](https://oreil.ly/yHSxz) that
    describes a different approach. *Astra DB* is a version of Cassandra that has
    been refactored into microservices to allow more fine-grained scalability and
    to take advantage of the benefits of Kubernetes. In fact, Astra DB is not only
    Kubernetes native; it is essentially a Kubernetes-only database. [Figure 7-4](#astra_db_architecture)
    shows the Astra DB architecture at a high level, broken into a control plane,
    data plane, and supporting infrastructure.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Astra DB architecture](assets/mcdk_0704.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Astra DB architecture
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s do a quick overview of the layers in this architecture:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB control plane
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The control plane is responsible for provisioning Kubernetes clusters in various
    cloud provider regions. It also provisions Astra DB clusters within those Kubernetes
    clusters and provides the APIs that allow clients to create and manage databases,
    either through the Astra DB web application, or programmatically through the DevOps
    API. Jim Dickinson’s blog post [“How We Built the DataStax Astra DB Control Plane”](https://oreil.ly/jhU2Q)
    describes the architecture of the control plane and how it was migrated to be
    Kubernetes native.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB data plane
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The data plane is where the actual Astra DB databases run. The data plane consists
    of multiple microservices which together provide the capabilities that would have
    been a part of a single monolithic Cassandra node. Each database is deployed in
    a Kubernetes cluster in a dedicated Namespace and may be shared across multiple
    tenants, as described in more detail later on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB infrastructure
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Each Kubernetes cluster also contains a set of infrastructure components that
    are shared across the Astra DB databases in that cluster, including etcd, Prometheus,
    and Grafana. etcd is used to store metadata, including the assignment of tenants
    to databases and database schema for each tenant. It also stores information about
    the cluster topology, replacing the role of gossip in the traditional Cassandra
    architecture. Prometheus and Grafana are deployed in a similar way as described
    in other architectures in this book.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s dig more into a few of the microservices in the data plane:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB Operator
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The Astra DB Operator manages the Kubernetes resources required for each database
    instance as described by a DBInstallation custom resource, as shown in [Figure 7-5](#astra_db_cluster_in_kubernetes).
    Similar to the Cass Operator project we discussed in [“Managing Cassandra in Kubernetes
    with Cass Operator”](ch06.html#managing_cassandra_in_kubernetes_with_c), the Astra
    DB Operator automates many of the operational tasks associated with managing a
    Cassandra cluster that would typically be performed by human operators using *nodetool*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Coordination Service
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The Coordination Service is responsible for handling application queries including
    reads, writes, and schema management. Each Coordination Service is an instance
    of Stargate (as discussed in [“Enabling Developer Productivity with Stargate APIs”](ch06.html#enabling_developer_productivity_with_st)
    that exposes endpoints for CQL and other APIs, with an Astra DB–specific plug-in
    that enables it to route requests intelligently to Data Service instances to actually
    store and retrieve data. Factoring this compute-intensive routing functionality
    into its own microservice enables it to be scaled up or down based on query traffic,
    independent of the volume of data being managed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Data Service
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Each Data Service instance is responsible for managing a subset of the data
    for each assigned tenant based on its position in the Cassandra token ring. The
    Data Service takes a tiered approach to data storage, maintaining in-memory data
    structures such as memtables, using local disk for caching, commit logs and indexes,
    and object storage for longer-term persistence of SSTables. The usage of object
    storage is one of the key differentiators of Astra DB from other databases we’ve
    examined so far, and we’ll examine other benefits of this approach throughout
    this section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Compaction Service
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The Compaction Service is responsible for performing maintenance tasks including
    compaction and repair on SSTables in object storage. Compaction and repair are
    compute-intensive tasks that experienced Cassandra operators have historically
    scheduled for off-peak hours to limit their impact on cluster performance. In
    Astra DB, these tasks can be performed at any time the need arises without impacting
    query performance. The work is handled by a pool of Compaction Service instances
    which can scale up or down independently to generate repaired, compacted SSTables
    which are immediately accessible to Data Services.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: IAM Service
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: All incoming application requests are routed through the Identity and Access
    Management (IAM) Service, which uses a standard set of roles and permissions defined
    in the control plane. While Cassandra has long had a pluggable architecture for
    authentication and authorization, factoring this out into its own microservice
    allows for more flexibility and support for additional providers such as Okta.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The data plane includes additional services which have been omitted from [Figure 7-4](#astra_db_architecture)
    for simplicity, including a Commitlog Replayer Service for recovery of failed
    Data Service instances, and an Autoscaling Service which uses analytics and machine
    learning to recommend to the operator when to scale the number of instances of
    each service up or down.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-5](#astra_db_cluster_in_kubernetes) shows what a typical DBInstallation
    looks like in terms of Kubernetes resources. Let’s walk through a few typical
    interactions focusing on individual instances of key services to demonstrate how
    each resource plays its part.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes Ingress is configured for each cluster to manage incoming requests
    from client applications (1) and route requests to Coordinator Services by the
    tenant using a Kubernetes Service (2).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Astra DB cluster in Kubernetes](assets/mcdk_0705.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Astra DB cluster in Kubernetes
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Coordinator Service is a stateless service managed by a Deployment (3) which
    delegates authentication and authorization checks on each call to the IAM Service
    (4).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Authorized requests are then routed to one or more Data Services based on the
    tenant, again using a Kubernetes Service (5).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Data Services are managed using StatefulSets (6), which are used to assign each
    instance to a local PersistentVolume used for managing intermediate datafiles
    such as the commit log, which is populated immediately on writes. When possible,
    reads are served directly from in-memory data structures.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: As is typical for Cassandra and other LSM tree storage engines, the Data Service
    occasionally writes SSTable files out to a persistent store (7). For Astra DB,
    that persistent store is an external object store managed by the cloud provider
    for high availability. A separate object storage bucket is used per tenant to
    ensure data privacy.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The Compaction Service can perform compaction and repair on SSTables in the
    object store asynchronously (8), with no impact to write and read queries.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB also supports multiregion database clusters, which by definition span
    multiple Kubernetes clusters. Coordinator and Data Services are deployed across
    Datacenters (cloud regions) and racks (availability zones) using an approach similar
    to that described for K8ssandra in [“Deploying Multicluster Applications in Kubernetes”](ch06.html#deploying_multicluster_applications_in).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB’s microservice architecture allows it to make more optimal use of compute
    and storage resources and isolate compute-intensive operations, leading to overall
    cost savings to operate Cassandra clusters in the cloud. These cost savings are
    extended by the addition of multitenant features that allow each cluster to be
    shared across multiple tenants. The [Astra DB whitepaper](https://oreil.ly/Zq0yc)
    describes a technique called *shuffle sharding* which is used to match each tenant
    to a subset of the available Coordinator and Data Services, effectively creating
    a separate Cassandra token ring per tenant. As the population of tenants in an
    Astra DB instance changes, this topology can be easily updated to rebalance load
    without downtime, and larger tenants can be configured to use their own dedicated
    databases (DBInstallations). This approach minimizes cost while meeting SLAs for
    performance and availability.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve focused on the architecture Astra DB uses to provide
    a multitenant, serverless Cassandra that embodies both cloud native and Kubernetes
    native principles using a completely different style of deployment. This continues
    the tradition of the Amazon Dynamo and Google BigTable papers in generating public
    discussion around novel database architectures. In addition, several open source
    projects mentioned in this book including Cass Operator, K8ssandra, and Stargate
    trace their origins to Astra DB. A lot of innovation is going on in areas such
    as the core database, control plane, change data capture, streaming integration,
    data migration, and more, so look for more open source contributions and architecture
    proposals from this team in the future.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: What to Look for in a Kubernetes Native Database
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After everything you’ve learned in the past few chapters about what it takes
    to deploy and manage various databases on Kubernetes, we are in a great position
    to define what you should look for in a Kubernetes native database.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Basic Requirements
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following our cloud native data principles, the following are a few areas that
    should be considered basic requirements:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Maximum leverage of Kubernetes APIs
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The database should be as tightly integrated with Kubernetes APIs as possible
    (for example, using PersistentVolumes for both local and remote storage, using
    Services for routing rather than maintaining lists of IPs of other nodes, and
    so on). Kubernetes extension points described in [Chapter 5](ch05.html#automating_database_management_on_kuber)
    should be used to supplement built-in Kubernetes functionality.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: In some areas, the existing Kubernetes APIs may not provide the exact behavior
    required for a given database or other application, as demonstrated by the creation
    of alternate StatefulSet implementations by the Vitess and TiDB projects. In these
    cases, every effort should be made to donate improvements back to the Kubernetes
    project.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Automated, declarative management via operators
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Databases should be deployed and managed on Kubernetes using operators and custom
    resources. Operators should serve as the primary control plane elements for managing
    databases. While it’s arguably helpful to have command-line tools or `kubectl`
    extensions that allow DBAs to intervene manually to optimize database performance
    and fix issues, these are ultimately functions that should be performed by an
    operator as it achieves the higher levels of maturity discussed in [Chapter 5](ch05.html#automating_database_management_on_kuber).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The goal should be that all required changes to a database can be accomplished
    by updating the desired state in a custom resource and letting the operator handle
    the rest. We’ll be in a great place when we can configure a database in terms
    of service-level objectives such as latency, throughput, availability, and cost
    per unit. Operators can determine how many database nodes are needed, what compute
    and storage tiers to use, when to perform backups, and so on.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Observable through standard APIs
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: We’re beginning to see common expectations for observability for data infrastructure
    on Kubernetes in terms of the familiar triad of metrics, logs, and tracing. The
    Prometheus-Grafana stack is somewhat of a de facto standard for metrics collection
    and visualization, with exposure of metrics from database services using the Prometheus
    format as a minimum criteria. Projects providing Prometheus integration should
    be flexible enough to provide their own dedicated stack, or push metrics to an
    existing installation shared with other applications.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Logs from all database application containers should be pushed to standard output
    (stdout) using sidecars if necessary—so they can be collected by log aggregation
    services. While it may take longer to see adoption for tracing, the ability to
    follow individual client requests through application calls down into the database
    tier through APIs such as OpenTracing will be an extremely powerful debugging
    tool for future cloud native applications.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Secure by default
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes project itself provides a great example of what it means to be
    secure by default—for example, by exposing access to ports on Pods and containers
    only when specifically enabled, and by providing primitives like Secrets that
    we can use to protect access to login credentials or sensitive configuration data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Databases and other infrastructure need to make use of these tools and adopt
    industry standards and best practices for zero trust (including changing default
    administrator credentials), limiting exposure of application and management APIs.
    Exposed APIs should prefer encrypted protocols such as HTTPS. Data stored in PersistentVolumes
    should be encrypted, whether this encryption is performed by the application,
    the database, or the StorageClass provider. Audit logs should be provided as part
    of application logging, especially with respect to actions that configure user
    access.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, a Kubernetes native database is sympathetic to the way that Kubernetes
    works. It maximizes reuse of Kubernetes built-in capabilities instead of bringing
    along its own set of duplicative supporting infrastructure. The experience of
    using a Kubernetes native database is therefore very much like using Kubernetes
    itself.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Kubernetes Native
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As these basic requirements and more advanced expectations for what it means
    to be Kubernetes native solidify, what comes next? We’re starting to see common
    patterns within projects deploying databases on Kubernetes that could point to
    where things are headed in the future. These are admittedly a bit fuzzier, but
    let’s try to bring a couple of them into focus.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Scalability through multidimensional architectures
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed the repetition of several terms throughout the past few
    chapters such as *multicluster*, *multitenancy*, *microservices*, and *serverless*.
    A common thread uniting these terms is that they represent architectural approaches
    to scalability, as shown in [Figure 7-6](#architectural_approaches_for_scaling_in).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Architectural approaches for scaling in multiple dimensions](assets/mcdk_0706.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Architectural approaches for scaling in multiple dimensions
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider how each of these approaches provides an independent axis for scalability.
    The visualization in [Figure 7-6](#architectural_approaches_for_scaling_in) depicts
    the impact of your application as a three-dimensional surface that grows as you
    scale along each axis:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures break the various functions of a database into independently
    scalable services. The serverless approach builds on this, encouraging the isolation
    of persistent state to a limited number of stateful services or even external
    services as much as possible. Kubernetes storage APIs in the PersistentVolume
    subsystem make it possible to leverage both local and networked storage options.
    These trends allow a true separation of compute and storage, and scale these resources
    independently.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构将数据库的各种功能分解为独立可扩展的服务。无服务器方法基于此构建，鼓励将持久状态隔离到尽可能少的有状态服务或甚至外部服务。Kubernetes
    持久卷子系统中的存储 API 使得可以利用本地和网络存储选项。这些趋势允许真正地分离计算和存储，并独立地扩展这些资源。
- en: Multicluster
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群
- en: '*Multicluster* refers to the ability to scale an application across multiple
    Kubernetes clusters. Along with related terms like *multiregion*, *multi-datacenter*,
    and *multicloud*, this implies expanding the geographic footprint of the capabilities
    provided across potentially heterogeneous environments. This distribution of capability
    has positive implications for meeting users where they are with minimum latency,
    cloud provider cost optimization, and disaster recovery. As we discussed in [Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku),
    Kubernetes has historically not been as strong in its support for cross-cluster
    networking and service discovery. It will be interesting to track how databases
    and other applications take advantage of expected advances in Kubernetes federation
    in the coming years.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*多集群*指的是跨多个 Kubernetes 集群扩展应用程序的能力。与相关术语如*多区域*、*多数据中心*和*多云*一起，这意味着在可能是异构环境的多个地理位置扩展提供的功能。这种能力的分布对于在最小延迟、云提供商成本优化和灾难恢复方面满足用户需求具有积极的影响。正如我们在[第6章](ch06.html#integrating_data_infrastructure_in_a_ku)中讨论的那样，Kubernetes
    在跨集群网络和服务发现的支持上历史上并不是特别强大。值得关注的是，数据库和其他应用程序如何利用预期中的 Kubernetes 联合进步。'
- en: Multitenancy
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户
- en: This is the ability to share infrastructure between multiple users to achieve
    the most efficient use of resources. As the public cloud providers have demonstrated
    in their IaaS offerings, a multitenant approach can be very effective at providing
    users a low-cost, low-risk access to infrastructure for innovative new projects,
    and then providing additional resources as these applications grow. Adopting a
    multitenant approach for data infrastructure has great potentiial value as well,
    so long as security guarantees are properly met and there is a seamless transition
    path to dedicated infrastructure for high-volume users before they become “noisy
    neighbors.” At this point in time, Kubernetes does not provide explicit support
    for multitenancy, although Namespaces can be a useful tool for providing dedicated
    resources for specific users.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在多个用户之间共享基础设施的能力，以实现资源的最有效使用。正如公共云提供商在其 IaaS 提供中所展示的，多租户方法可以非常有效地为创新性新项目提供低成本、低风险的基础设施访问，并在这些应用程序增长时提供额外资源。在数据基础设施中采用多租户方法也具有很大的潜在价值，前提是安全保证得到适当满足，并且在它们成为“吵闹的邻居”之前，有一个无缝的过渡路径转向专用基础设施。在目前阶段，Kubernetes
    并未提供明确的多租户支持，尽管命名空间可以是为特定用户提供专用资源的有用工具。
- en: While you may not have immediate need for all three of these axes of scalability
    for applications or data infrastructure you’re building, consider how growing
    in each of them can enhance the overall value you’re offering the world.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你建立的应用程序或数据基础设施可能不立即需要这三个扩展性轴的全部，但考虑到在每个方面的增长如何可以增强你所提供的整体价值。
- en: Community-focused innovation through open source and cloud services
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过开源和云服务的社区重点创新
- en: Another pattern you may have noticed in our narrative is the continual innovation
    loop between open source database projects and DBaaS offerings. PingCAP took the
    open source MySQL and ClickHouse databases, created a database service leveraging
    Kubernetes to help it manage the databases at scale, and then released open source
    projects including TiDB and TiFlash. DataStax took open source Cassandra, factored
    it into microservices, added an API layer, and deployed it on Kubernetes for its
    Astra DB, and has created multiple open source projects including Cass Operator,
    K8ssandra, and Stargate. In the spirit of Dynamo, BigTable, Calvin and other papers,
    these companies have open source architectures as well.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: This innovation loop mirrors that of the larger Kubernetes community, in which
    the major cloud providers and storage vendors have helped drive the maturation
    of the core Kubernetes control plane and PersistentVolume subsystem, respectively.
    It’s interesting to observe that the highest momentum and fastest cycle time occurs
    within innovation loops that center around cloud services, rather than around
    the classic open core model focused on enterprise versions of open source projects.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: As a software vendor, providing a cloud service allows you to iterate and evaluate
    new architectures and features more quickly. Flowing these innovations back to
    open source allows you to grow adoption by supporting a flexible consumption model.
    Both “run it yourself” and “rent it from us” become legitimate deployment options
    for your customers, with the ability to flex between approaches for different
    use cases. Customers gain confidence in the overall maturity and security of your
    technology, knowing that the open source version they can inspect and contribute
    to is largely the same as what you are running in your DBaaS.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'A final side effect of these innovation trends is an implicit pull toward proven
    architectures and components. Consider these examples:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: etcd is used as a metadata store across multiple projects we’ve examined in
    this book, including Vitess and Astra DB.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TiDB leverages the architecture of F1, implemented the Raft consensus protocol,
    and extended the ClickHouse columnar store.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Astra DB leverages both the PersistentVolume subsystem and S3-compliant object
    storage.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of inventing new technologies to solve problems like metadata management
    and distributed transactions, these projects are investing their innovation in
    new features, developer experience, and the scalability axes we’ve examined in
    this chapter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve taken a deep look at TiDB and Astra DB to search out
    what makes them Kubernetes native. What was the point of this exercise? Our hope
    is that this analysis provides a deeper understanding to help consumers ask more
    insightful questions about the data infrastructure they are consuming, and to
    help those building data infrastructure and ecosystems to create technology that
    meets those expectations. We believe that data infrastructure that is not only
    cloud native but also Kubernetes native will lead to the best outcomes for everyone
    in terms of performance, availability, and cost.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了 TiDB 和 Astra DB，以找出它们成为 Kubernetes 本地化的特质。这项工作的目的是什么？我们希望这种分析能够提供更深入的理解，帮助消费者提出更具洞察力的关于所使用数据基础设施的问题，并帮助那些构建数据基础设施和生态系统的人创建能够满足这些期望的技术。我们相信，不仅是云原生而且是
    Kubernetes 本地化的数据基础设施将为每个人带来最佳的性能、可用性和成本效益的结果。
