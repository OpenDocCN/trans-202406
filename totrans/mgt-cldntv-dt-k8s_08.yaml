- en: Chapter 7\. The Kubernetes Native Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The software industry is flush with terms that define major trends in a single
    word or short phrase. You can see one of them in the title of this book: *cloud
    native*. Another example is *microservice*, a major architectural paradigm that
    touches much of the technology we’re discussing here. More recently, terms like
    *Kubernetes native* and *serverless* have emerged.'
  prefs: []
  type: TYPE_NORMAL
- en: While succinct and catchy, distilling a complex topic or trend down to a single
    sound bite leaves room for ambiguity, or at least for reasonable questions such
    as “What does this *actually* mean?” To further muddy the waters, terms such as
    these are frequently used in the context of marketing products as a way to gain
    leverage or differentiate against other competitive offerings. Whether the content
    you’re consuming makes an overt statement or it’s just the subtext, you may have
    wondered whether a given technology must be better to run on Kubernetes than other
    offerings because it’s labeled *Kubernetes native*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, for these terms to be useful to us in evaluating and picking the
    right technologies for our applications, the real task is to unpack what they
    really mean, as we did with the term *cloud native data* in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra).
    In this chapter, we’ll look at what it means for data technology to be Kubernetes
    native and see if we can arrive at a definition that we can agree on. To do this,
    we’ll examine a couple of projects that claim these terms and derive the common
    principles: TiDB and Astra DB. Are you ready? Let’s dive in!'
  prefs: []
  type: TYPE_NORMAL
- en: Why a Kubernetes Native Approach Is Needed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s discuss why the idea of a Kubernetes native database came up in
    the first place. Up to this point in the book, we’ve focused on deployment of
    existing databases on Kubernetes including MySQL and Cassandra. These are mature
    databases that were around before Kubernetes existed and have proven themselves
    over time. They have large install bases and user communities, and because of
    this investment, you can see why there’s a large incentive to run these databases
    in Kubernetes environments, and why there has been such interest in creating operators
    to automate them.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, you’ve probably noticed some of the awkwardness in adapting
    these databases to run on Kubernetes. While it is pretty straightforward to point
    a database to Kubernetes-based storage just by changing a mount path, tighter
    integration with Kubernetes to manage databases that consist of multiple nodes
    can be a bit more involved. This can range from relatively simple tasks like deploying
    a legacy management UI in a Pod and exposing access to the HTTP port, to the more
    complex deployment of sidecars that we saw in [Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku)
    to provide APIs for management and metrics collection.
  prefs: []
  type: TYPE_NORMAL
- en: The recognition of this complexity has led some innovators to develop new databases
    that are designed to be Kubernetes native from day one. It’s a well-known axiom
    in the database industry that it takes 5–10 years for a new database engine to
    reach a point of maturity. Because of this, these Kubernetes native databases
    tend not to be completely new implementations, but rather refactoring of existing
    databases into microservices that can be scaled independently, while maintaining
    compatibility with existing APIs that developers are accustomed to. Thus, the
    trend of decomposing the monolith has arrived at the data tier. The emerging generation
    of databases will be based on new architectures to truly leverage the benefits
    of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us assess what might qualify these new databases as Kubernetes native,
    let’s use the cloud native data principles introduced in [“Principles of Cloud
    Native Data Infrastructure”](ch01.html#principles_of_cloud_native_data_infrast)
    as a guide to formulate some questions to ask how a database interacts with Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 1: Leverage compute, network, and storage as commodity APIs'
  prefs: []
  type: TYPE_NORMAL
- en: How does the database use Kubernetes compute resources (Pods, Deployments, StatefulSets),
    network resources (Services and Ingress), and storage resources (PersistentVolumes,
    PersistentVolumeClaims, StorageClasses)?
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 2: Separate the control and data planes'
  prefs: []
  type: TYPE_NORMAL
- en: Is the database deployed and managed by an operator? What custom resources does
    it define? Are other workloads in the control plane besides the operator?
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 3: Make observability easy'
  prefs: []
  type: TYPE_NORMAL
- en: How do the various services in the architecture expose metrics and logs to support
    collection by the Kubernetes control plane and third-party extensions?
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 4: Make the default configuration secure'
  prefs: []
  type: TYPE_NORMAL
- en: Do the database and associated operator use Kubernetes Secrets to share credentials,
    and use Roles and RoleBindings to manage access by Role? Do services minimize
    the number of exposed points and require secure access to them?
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 5: Prefer declarative configuration'
  prefs: []
  type: TYPE_NORMAL
- en: Extending Principle 2, can the database be managed entirely by creating, updating,
    or deleting Helm charts and Kubernetes resources (whether built-in or custom resources),
    or are other tools required?
  prefs: []
  type: TYPE_NORMAL
- en: In the sections that follow, we’ll explore the answers to these questions for
    two databases and see what else we can learn about what it means to be Kubernetes
    native. That will help us to build a checklist at the end of this chapter that
    will help solidify our definition. (See [“What to Look for in a Kubernetes Native
    Database”](#what_to_look_for_in_a_kubernetes_native) for what we come up with.)
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid Data Access at Scale with TiDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The databases that have received most of our focus in this book so far represent
    two major trends in database architecture that trace their lineage back for decades
    or more. MySQL is a relational database that provides its own flavor of the Standard
    Query Language (SQL), based on rules developed by Edgar Codd in the 1970s.
  prefs: []
  type: TYPE_NORMAL
- en: In the early 2000s, companies building web-scale applications began to push
    the limits of what could be accomplished with the relational databases of the
    day. As database sizes began growing beyond what could feasibly be managed on
    a single instance, techniques like sharding were used to scale across multiple
    instances. These were frequently expensive, difficult to operate, and not always
    reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to this need, Cassandra and other so-called *NoSQL* databases emerged
    in a period of intense innovation and experimentation. These databases provide
    linear scalability through adding additional nodes. They offer different data
    models, or ways of representing data: for example, key-value stores such as Redis,
    document databases such as MongoDB, graph databases such as Neo4j, and others.
    NoSQL databases tended to provide weaker consistency guarantees and omit support
    for more complex behaviors like transactions and joins to achieve high performance
    and availability at scale, a trade-off documented by Eric Brewer in his [CAP theorem](https://oreil.ly/aJq6M).'
  prefs: []
  type: TYPE_NORMAL
- en: Because of the continued developer demand for traditional relational semantics
    such as strong consistency, transactions, and joins, multiple teams began to revive
    the idea of supporting these capabilities in distributed databases starting around
    2012\. These so-called *NewSQL* databases were based on more efficient and performant
    consensus algorithms. Two key papers helped drive the emergence of the NewSQL
    movement. First, the [Calvin paper](https://oreil.ly/HLw2M) introduced a global
    consensus protocol which represented a more reliable and performant approach for
    guaranteeing strong consistency, later adopted by FaunaDB and other databases.
    Second, Google’s [Spanner paper](https://oreil.ly/zDl5z) introduced a design for
    a distributed relational database using sharding and a new consensus algorithm
    that leveraged the improved ability of cloud infrastructure to provide time synchronization
    across datacenters. Besides Google Spanner, this approach was implemented by databases
    including CockroachDB and YugabyteDB.
  prefs: []
  type: TYPE_NORMAL
- en: More on Consistency and Consensus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we don’t have space in this book to dive deeply into the trade-offs between
    various consensus algorithms and how they are used to provide various data consistency
    guarantees, an understanding of these concepts is helpful in choosing the right
    data infrastructure for your cloud applications. If you’re interested in learning
    more in this area, Martin Kleppmann’s [*Designing Data-Intensive Applications*](https://oreil.ly/6ndic)
    (O’Reilly) is a great source, especially Chapter 9, “Consistency and Consensus”.
  prefs: []
  type: TYPE_NORMAL
- en: '[TiDB](https://oreil.ly/jZNAI) (where *Ti* stands for *Titanium*) represents
    a continuation of the NewSQL trend in the cloud native space. TiDB is an open
    source, MySQL-compatible database that supports both transactional and analytic
    workloads. It was initially developed and is primarily supported by PingCAP. While
    TiDB is a database designed to embody cloud native principles of scalability and
    elasticity, what makes it especially interesting for our discussion is that it
    has been explicitly designed to run on Kubernetes and to rely on capabilities
    provided by the Kubernetes control plane. In this way, one could argue that TiDB
    is not merely a Kubernetes native database, but also a Kubernetes *only* database.
    Let’s dig into the details.'
  prefs: []
  type: TYPE_NORMAL
- en: TiDB Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A key characteristic of TiDB which distinguishes it from other databases we’ve
    examined so far in this book is its ability to support transactional and analytic
    workloads. This approach, known as *hybrid transactional/analytical processing
    (HTAP)*, supports both types of queries without a separate extract, transform,
    and load (ETL) process. As shown in [Figure 7-1](#tidb_architecture), TiDB does
    this by providing two database engines under the hood: TiKV and TiFlash. This
    approach was inspired by Google’s [F1 project](https://oreil.ly/lakAf), a layer
    built on top of Spanner.'
  prefs: []
  type: TYPE_NORMAL
- en: '![TiDB architecture](assets/mcdk_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. TiDB architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One key aspect that gives TiDB a cloud native architecture is the packaging
    of compute and storage operations into separate components, each of which is composed
    of independently scalable services organized in clusters. Let’s examine the roles
    of each of these components:'
  prefs: []
  type: TYPE_NORMAL
- en: TiDB
  prefs: []
  type: TYPE_NORMAL
- en: Each TiDB instance is a stateless service that exposes a MySQL endpoint to client
    applications. TiDB parses incoming SQL requests and uses metadata from the Placement
    Driver (PD) to create an execution plan containing queries to make on specific
    TiKV and TiFlash nodes in the storage cluster. TiDB executes these queries, assembles
    the results, and returns to the client application. The TiDB cluster is typically
    deployed with a proxy in front of it to provide load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: TiKV
  prefs: []
  type: TYPE_NORMAL
- en: The storage cluster consists of a mixture of TiKV and TiFlash nodes. First,
    let’s examine [*TiKV*](https://tikv.org), an open source, distributed key-value
    database that uses [RocksDB](http://rocksdb.org) as its backing storage engine.
    TiKV exposes a custom distributed SQL API that the TiDB nodes use to execute queries
    to store and retrieve data and manage distributed transactions. TiKV stores multiple
    replicas of your data, typically at least three, to support high availability
    and automatic failover. TiKV is a [CNCF graduated project](https://oreil.ly/ypLlC)
    which can be used independently from TiDB, as we’ll discuss later.
  prefs: []
  type: TYPE_NORMAL
- en: TiFlash
  prefs: []
  type: TYPE_NORMAL
- en: The storage cluster also includes TiFlash nodes, to which data is replicated
    from TiKV nodes as it is written. TiFlash is a columnar database based on the
    open source [ClickHouse analytic database](https://oreil.ly/PCVlg), which means
    that it organizes data storage in columns rather than rows. Columnar databases
    can provide a significant performance advantage for analytic queries requiring
    the extraction of the same column across multiple rows.
  prefs: []
  type: TYPE_NORMAL
- en: TiSpark
  prefs: []
  type: TYPE_NORMAL
- en: This library is built for Apache Spark to support complex OLAP queries. TiSpark
    integrates with the Spark Driver and Spark Executors, providing the capability
    to ingest data from TiFlash instances using the distributed SQL API. We’ll examine
    the Spark architecture and the details of deploying Spark on Kubernetes in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: Placement Driver (PD)
  prefs: []
  type: TYPE_NORMAL
- en: The PD manages the metadata for a TiDB installation. PD instances are deployed
    in a cluster of at least three nodes. TiDB uses a range-based sharding mechanism
    where the keys in each table are divided into ranges called *regions*. The PD
    is responsible for determining the ranges of data assigned to each region, and
    the TiKV nodes that will store the data for each region. It monitors the amount
    of data in each region and splits regions that become too large, to facilitate
    scaling up, and merging smaller regions to scale down.
  prefs: []
  type: TYPE_NORMAL
- en: Because the TiDB architecture consists of well-defined interfaces between the
    components, it is an extensible architecture in which different pieces can be
    plugged in. For example, TiKV provides a distributed key-value storage solution
    that can be reused in other applications. The [TiPrometheus project](https://oreil.ly/PkmqK)
    is an example, providing a Prometheus-compliant compute layer on top of TiKV.
    For another example, you could provide an alternate implementation of TiKV that
    implements the distributed SQL API on top of a different storage engine.
  prefs: []
  type: TYPE_NORMAL
- en: Pluggable Storage Engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter so far, we’ve made several mentions of “storage engines” or
    “database engines.” This term refers to the part of the database that manages
    the storage and retrieval of data on persistent media. In distributed databases,
    a distinction is often made between the storage engine and the proxy layer which
    sits on top of it to manage data replication between nodes. Chapter 3, “Storage
    and Retrieval,” from [*Designing Data-Intensive Applications*](https://oreil.ly/6ndic)
    includes discussion of storage engine types such as the B-trees used in most relational
    databases and the log-structured merge tree (LSM tree) used in Apache Cassandra
    and other NoSQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'One interesting aspect of TiDB is the way in which it reuses existing technology.
    We’ve seen examples of this in the usage of components including RocksDB and Spark.
    TiDB also uses algorithms developed by other organizations. Here are a couple
    of examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Raft consensus protocol
  prefs: []
  type: TYPE_NORMAL
- en: At the TiDB layer, the [Raft consensus protocol](https://oreil.ly/Oi6Dk) is
    used to manage consistency between replicas. Raft is similar to the Paxos algorithm
    used by Cassandra in terms of its behavior, but it’s designed to be much simpler
    to learn and use. TiDB uses a separate Raft group for each region, where a group
    typically consists of a leader and two or more replicas. If a leader node is lost,
    an election is run to select a new leader, and a new replica can be added to ensure
    the desired number of replicas. In addition, the TiFlash nodes are configured
    as a special type of replica called *learner replicas*. Data is replicated to
    learner replicas from the TiDB nodes, but they cannot be selected as a leader.
    You can read more about how TiDB uses Raft for [high availability](https://oreil.ly/BddzV)
    and other related topics on the [PingCAP blog](https://oreil.ly/Y2YuS).
  prefs: []
  type: TYPE_NORMAL
- en: Percolator transaction management
  prefs: []
  type: TYPE_NORMAL
- en: At the TiDB layer, distributed transactions are supported using an implementation
    of the [Percolator algorithm](https://oreil.ly/heMho) with optimizations specific
    to the TiDB project. Percolator was originally developed at Google for supporting
    incremental updates to search indexes.
  prefs: []
  type: TYPE_NORMAL
- en: One of the arguments we’re making in this chapter is that part of what it means
    for data infrastructure to be cloud native is to compose existing APIs, services,
    and algorithms wherever possible, and TiDB is a great example of this.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying TiDB in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While TiDB can be deployed in a variety of ways including bare metal and VMs,
    the TiDB team has invested a large effort in tooling and documentation to make
    TiDB a truly Kubernetes native database. The [TiDB Operator](https://oreil.ly/xZtGq)
    manages TiDB clusters in Kubernetes, including deployment, upgrade, scaling, backup
    and restore, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The operator [documentation](https://oreil.ly/iIZc0) provides [quick start guides](https://oreil.ly/5heDA)
    for desktop Kubernetes distributions such as kind, minikube, and Google Kubernetes
    Engine (GKE). These instructions guide you through steps including installing
    CRDs and the TiDB operator using Helm, and a simple TiDB cluster including monitoring
    services. We’ll use the quick start instructions as a vehicle to talk about what
    makes TiDB a Kubernetes native database.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the TiDB CRDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After making sure you have a Kubernetes cluster that meets the defined prerequisites
    such as having a [default StorageClass](https://oreil.ly/7myfI), the first step
    in deploying TiDB using the operator is installing the CRDs used by the operator.
    This is done using an instruction such as the following (note the actual operator
    version number `v1.3.2` may vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the creation of several CRDs, which you can observe by running
    the command `kubectl get crd` as we have done in previous chapters. We’ll quickly
    discuss the purpose of each resource since several of them hint at additional
    features of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: The TidbCluster is the primary resource that describes the desired configuration
    of a TiDB cluster. We’ll look at an example later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TidbMonitor resource is used to deploy a Prometheus-based monitoring stack
    to observe one or more TidbClusters. As we have seen with other projects, Prometheus
    (or at least its API) has become a de facto standard for metrics collection for
    databases and other infrastructure deployed on Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Backup and Restore resources represent the actions of performing a backup
    or restoring from a backup. This is similar to other operators we’ve examined
    previously from the Vitess (see [“PlanetScale Vitess Operator”](ch05.html#planetscale_vitess_operator))
    and K8ssandra ([Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku))
    projects. The TiDB Operator also provides a BackupSchedule resource that can be
    used to configure regular backups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TidbInitializer is an optional resource that you can create to perform [initialization
    tasks](https://oreil.ly/qFsmu) on a TidbCluster, including setting administrator
    credentials and executing SQL statements for schema creation or initial data loading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TidbClusterAutoScaler is another optional resource which can be used to
    configure [auto-scaling](https://oreil.ly/wVbf2) behavior of a TidbCluster. The
    number of TiKV or TiDB nodes in a TidbCluster can be configured to scale up or
    down between minimum and maximum limits based on CPU utilization. The addition
    of scaling rules based on other metrics is on the project roadmap. As we discussed
    in [“Choosing Operators”](ch05.html#choosing_operators), auto-scaling is considered
    a feature of an operator at Level 5 or Autopilot, the highest maturity level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TidbNGMonitoring is an optional resource that configures a TidbCluster to
    enable [continuous profiling](https://oreil.ly/2n8k5) down to the system call
    level. The resulting profiling data and flame graph visualizations can be observed
    using the [TiDB Dashboard](https://oreil.ly/23pLs), which is deployed separately.
    This is typically used by project engineers looking to optimize the database,
    but application and platform developers may find this useful as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DMCluster resource is used to deploy an instance of the [TiDB Data Migration](https://oreil.ly/C5NG0)
    (DM) platform that supports migration of MySQL and MariaDB database instances
    into a TidbCluster. It can also be configured to migrate from an existing TiDB
    installation outside of Kubernetes to a TidbCluster. The ability to deploy data
    migration services alongside a destination TidbCluster in Kubernetes managed by
    the same operator is a great example of what it means to develop data ecosystems
    in Kubernetes, a pattern that we hope to see more of in the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the remainder of this section, we’ll focus on the TidbCluster and TidbMonitoring
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the TiDB Operator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After installing the CRDs, the next step is to install the TiDB Operator using
    Helm. You’ll need to add the Helm repository first before installing the TiDB
    Operator in its own Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can watch the resulting Pods come online using `kubectl get pods` and referencing
    the `tidb-admin` Namespace. [Figure 7-2](#installing_the_tidb_operator_and_crds)
    provides a summary of the elements that you’ve installed up to this point. This
    includes Deployments to manage the TiDB Operator (labeled as `tidb-controller-manager`)
    and the TiDB Scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: The TiDB Scheduler is an optional extension to the Kubernetes built-in scheduler.
    While it is deployed by default as part of the TiDB Operator, it can be disabled.
    Assuming the TiDB Scheduler is not disabled, using it for a specific TidbCluster
    still requires opting in by setting the `schedulerName` property to `tidb-scheduler`.
    If this property is set, the TiDB Operator will assign the TiDB Scheduler as the
    scheduler that Kubernetes will use when creating TiKV, and PD Pods.
  prefs: []
  type: TYPE_NORMAL
- en: The TiDB Scheduler extends the Kubernetes built-in scheduler to add custom scheduling
    rules for Pods that are part of a TidbCluster, helping to achieve high availability
    of the database while spreading the load evenly across the available Worker Nodes
    in the Kubernetes cluster. While for many types of infrastructure, the existing
    mechanisms Kubernetes offers for influencing the default scheduler such as affinity
    rules, taints, and tolerations are sufficient, TiDB provides a useful example
    of when and how to implement custom scheduling logic. We’ll look at Kubernetes
    scheduler extensions in more detail in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing the TiDB Operator and CRDs](assets/mcdk_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Installing the TiDB Operator and CRDs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TiDB Operator Helm Chart Options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This installation omits usage of a *values.yaml* file, but you can see the
    available options by running following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This includes the option to disable the TiDB Scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a TidbCluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the TiDB Operator has been installed, you’re ready to create a TidbCluster
    resource. While many [example configurations](https://oreil.ly/66uf7) are available
    in the TiDB Operator GitHub repository, let’s use the one referenced in the quick
    start guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'While the TidbCluster is being created, you can reference the contents of this
    file, which look something like this (with comments and some details removed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that this results in the creation of a TidbCluster named `basic` in the
    `tidb-cluster` Namespace, with one replica each of TiDB, TiKV, and PD, using the
    standard PingCAP images for each. Additional options are used to specify the minimum
    amount of compute and storage resources required to achieve a functioning cluster.
    No TiFlash nodes are included in this simple configuration.
  prefs: []
  type: TYPE_NORMAL
- en: TidbCluster API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full list of options for a TidbCluster can be found as part of the [API](https://oreil.ly/XoC02)
    available in the GitHub repository. This same page includes options for the other
    CRDs used by the TiDB Operator. As you explore the options for these CRDs, you’ll
    see evidence of the common practice of allowing many of the options that will
    be used to specify underlying resources to be overridden (for example, the Pod
    specification that will be set on a Deployment).
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to take the opportunity to use `kubectl` or your favorite visualization
    tool to explore the resources created as part of the TidbCluster, a summary of
    which is provided in [Figure 7-3](#a_basic_tidbcluster).
  prefs: []
  type: TYPE_NORMAL
- en: '![A basic TidbCluster](assets/mcdk_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. A basic TidbCluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the TiDB Operator creates StatefulSets to manage the TiDB, TiKV,
    and Placement Driver instances, allocating a PVC for each instance. As an I/O-intensive
    application, the default configuration is to use local PersistentVolumes as the
    backing store.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, a Deployment is created to run a Discovery Service which the various
    components use to learn of each other’s location. The Discovery Service performs
    a similar role to that of etcd in other data technologies we’ve examined in the
    book. The TiDB Operator also configures services for each StatefulSet and Deployment
    that facilitate communication within the TiDB cluster as well as exposing capabilities
    to external clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TiDB Operator supports the deployment of a Prometheus monitoring stack
    that can manage one or more TiDB clusters. You can add monitoring to the cluster
    created previously using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'While this is deploying, let’s examine the contents of the *tidb-monitor.yaml*
    configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the TidbMonitor resource can point to one or more TidbClusters.
    This TidbMonitor is configured to manage the `basic` cluster you created previously.
    The TidbMonitor resource also allows you to specify the versions of Prometheus,
    Grafana, and additional tools that are used to initialize and update the monitoring
    stack. If you examine the contents of the `tidb-cluster` Namespace, you’ll see
    additional workloads that have been created to manage these elements.
  prefs: []
  type: TYPE_NORMAL
- en: TiDB uses the Prometheus stack in a similar way to the K8ssandra project, as
    we discussed in [“Unified Monitoring Infrastructure with Prometheus and Grafana”](ch06.html#unified_monitoring_infrastructure_with).
    In both of these projects, the Prometheus stack is supported as an optional extension
    to provide a monitoring capability you can use with very little customization.
    The configurations and provided visualizations focus on the key metrics that drive
    awareness of database health. Even if you are already managing your own monitoring
    infrastructure or using a third-party software-as-a-service (SaaS) solution, the
    configurations and charts can give you a head start on incorporating database
    monitoring into the rest of your observability approach.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, TiDB is a database with a flexible, extensible architecture
    that has been designed with cloud native principles in mind. It also has a strong
    bias toward being able to deploy and manage a database effectively in Kubernetes
    and has provided us with some valuable insights on what it means to be Kubernetes
    native. Consult the TiDB documentation for more information on features such as
    [deploying to multiple Kubernetes clusters](https://oreil.ly/NPHxy).
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Cassandra with DataStax Astra DB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the advent of cloud computing in the early 2000s, public cloud providers
    and infrastructure vendors have made continual advances in commoditizing various
    layers of our architectural stacks as service offerings. This trend began with
    offering compute, network, and storage as *infrastructure as a service* (IaaS)
    and proceeded into other trends including *platform as a service* (PaaS), *software
    as a service* (SaaS), and *functions as a service* (FaaS), sometimes conflated
    with the term *serverless*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most pertinent to our investigation here is the emergence of managed data infrastructure
    offerings known as *database as a service* (DBaaS). This category includes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional databases offered as a managed cloud service, such as Amazon Relational
    Database Service (RDS) and PlanetScale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud databases like Google BigTable, Amazon Dynamo, and Snowflake that are
    available only as cloud offerings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managed NoSQL or NewSQL databases that can also be run on premises under an
    open source or source available license—for example, MongoDB Atlas, DataStax Astra
    DB, TiDB, and Cockroach DB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the past several years, many of the vendors behind these DBaaS services
    have begun migrating onto Kubernetes to automate operations, manage compute resources
    more efficiently, and make their solutions portable across clouds. DataStax was
    one of several vendors that began offering Cassandra as a service. These vendors
    typically used an architecture based on running traditional Cassandra clusters
    in a cloud environment, with various “glue code” to integrate aspects like networking,
    monitoring, and management that didn’t quite fit target deployment environments
    like Kubernetes and public cloud IaaS. These include techniques like using sidecars
    to collect metrics and logs, or deploying Cassandra nodes using StatefulSets to
    manage scaling up and down in an orderly fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with these workarounds for running in Kubernetes, Cassandra’s monolithic
    architecture doesn’t readily promote the separation of compute and storage, which
    can lead to some awkwardness when scaling. You scale up a Cassandra cluster by
    adding additional nodes, where each node has the following capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Coordination
  prefs: []
  type: TYPE_NORMAL
- en: Receiving read and write requests and forwarding them to other nodes as needed
    to achieve the requested number of replicas (also known as *consistency level*)
  prefs: []
  type: TYPE_NORMAL
- en: Writing and reading
  prefs: []
  type: TYPE_NORMAL
- en: Writing data to in-memory cache (memtables) and persistent storage (SSTables),
    and reading it back as needed
  prefs: []
  type: TYPE_NORMAL
- en: Compaction and repair
  prefs: []
  type: TYPE_NORMAL
- en: Since Cassandra is an LSM-tree database, it does not update datafiles once they
    are written to persistent storage. Compaction and repair are tasks that run in
    the background as separate threads. Compaction helps Cassandra stay performant
    by consolidating SSTables written at different times, ignoring obsolete and deleted
    values. Repair is the process of comparing stored values across nodes to ensure
    consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Each node in a Cassandra cluster implements all of these capabilities and consumes
    equivalent compute and storage resources. This makes it difficult to scale compute
    and storage independently and can lead to situations where a cluster is overprovisioned
    in compute or storage resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2021, DataStax published a paper entitled [“DataStax Astra DB: Designing
    a Serverless Cloud-Native Database-as-a-Service”](https://oreil.ly/yHSxz) that
    describes a different approach. *Astra DB* is a version of Cassandra that has
    been refactored into microservices to allow more fine-grained scalability and
    to take advantage of the benefits of Kubernetes. In fact, Astra DB is not only
    Kubernetes native; it is essentially a Kubernetes-only database. [Figure 7-4](#astra_db_architecture)
    shows the Astra DB architecture at a high level, broken into a control plane,
    data plane, and supporting infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Astra DB architecture](assets/mcdk_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Astra DB architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s do a quick overview of the layers in this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB control plane
  prefs: []
  type: TYPE_NORMAL
- en: The control plane is responsible for provisioning Kubernetes clusters in various
    cloud provider regions. It also provisions Astra DB clusters within those Kubernetes
    clusters and provides the APIs that allow clients to create and manage databases,
    either through the Astra DB web application, or programmatically through the DevOps
    API. Jim Dickinson’s blog post [“How We Built the DataStax Astra DB Control Plane”](https://oreil.ly/jhU2Q)
    describes the architecture of the control plane and how it was migrated to be
    Kubernetes native.
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB data plane
  prefs: []
  type: TYPE_NORMAL
- en: The data plane is where the actual Astra DB databases run. The data plane consists
    of multiple microservices which together provide the capabilities that would have
    been a part of a single monolithic Cassandra node. Each database is deployed in
    a Kubernetes cluster in a dedicated Namespace and may be shared across multiple
    tenants, as described in more detail later on.
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: Each Kubernetes cluster also contains a set of infrastructure components that
    are shared across the Astra DB databases in that cluster, including etcd, Prometheus,
    and Grafana. etcd is used to store metadata, including the assignment of tenants
    to databases and database schema for each tenant. It also stores information about
    the cluster topology, replacing the role of gossip in the traditional Cassandra
    architecture. Prometheus and Grafana are deployed in a similar way as described
    in other architectures in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s dig more into a few of the microservices in the data plane:'
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB Operator
  prefs: []
  type: TYPE_NORMAL
- en: The Astra DB Operator manages the Kubernetes resources required for each database
    instance as described by a DBInstallation custom resource, as shown in [Figure 7-5](#astra_db_cluster_in_kubernetes).
    Similar to the Cass Operator project we discussed in [“Managing Cassandra in Kubernetes
    with Cass Operator”](ch06.html#managing_cassandra_in_kubernetes_with_c), the Astra
    DB Operator automates many of the operational tasks associated with managing a
    Cassandra cluster that would typically be performed by human operators using *nodetool*.
  prefs: []
  type: TYPE_NORMAL
- en: Coordination Service
  prefs: []
  type: TYPE_NORMAL
- en: The Coordination Service is responsible for handling application queries including
    reads, writes, and schema management. Each Coordination Service is an instance
    of Stargate (as discussed in [“Enabling Developer Productivity with Stargate APIs”](ch06.html#enabling_developer_productivity_with_st)
    that exposes endpoints for CQL and other APIs, with an Astra DB–specific plug-in
    that enables it to route requests intelligently to Data Service instances to actually
    store and retrieve data. Factoring this compute-intensive routing functionality
    into its own microservice enables it to be scaled up or down based on query traffic,
    independent of the volume of data being managed.
  prefs: []
  type: TYPE_NORMAL
- en: Data Service
  prefs: []
  type: TYPE_NORMAL
- en: Each Data Service instance is responsible for managing a subset of the data
    for each assigned tenant based on its position in the Cassandra token ring. The
    Data Service takes a tiered approach to data storage, maintaining in-memory data
    structures such as memtables, using local disk for caching, commit logs and indexes,
    and object storage for longer-term persistence of SSTables. The usage of object
    storage is one of the key differentiators of Astra DB from other databases we’ve
    examined so far, and we’ll examine other benefits of this approach throughout
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: Compaction Service
  prefs: []
  type: TYPE_NORMAL
- en: The Compaction Service is responsible for performing maintenance tasks including
    compaction and repair on SSTables in object storage. Compaction and repair are
    compute-intensive tasks that experienced Cassandra operators have historically
    scheduled for off-peak hours to limit their impact on cluster performance. In
    Astra DB, these tasks can be performed at any time the need arises without impacting
    query performance. The work is handled by a pool of Compaction Service instances
    which can scale up or down independently to generate repaired, compacted SSTables
    which are immediately accessible to Data Services.
  prefs: []
  type: TYPE_NORMAL
- en: IAM Service
  prefs: []
  type: TYPE_NORMAL
- en: All incoming application requests are routed through the Identity and Access
    Management (IAM) Service, which uses a standard set of roles and permissions defined
    in the control plane. While Cassandra has long had a pluggable architecture for
    authentication and authorization, factoring this out into its own microservice
    allows for more flexibility and support for additional providers such as Okta.
  prefs: []
  type: TYPE_NORMAL
- en: The data plane includes additional services which have been omitted from [Figure 7-4](#astra_db_architecture)
    for simplicity, including a Commitlog Replayer Service for recovery of failed
    Data Service instances, and an Autoscaling Service which uses analytics and machine
    learning to recommend to the operator when to scale the number of instances of
    each service up or down.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-5](#astra_db_cluster_in_kubernetes) shows what a typical DBInstallation
    looks like in terms of Kubernetes resources. Let’s walk through a few typical
    interactions focusing on individual instances of key services to demonstrate how
    each resource plays its part.'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes Ingress is configured for each cluster to manage incoming requests
    from client applications (1) and route requests to Coordinator Services by the
    tenant using a Kubernetes Service (2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Astra DB cluster in Kubernetes](assets/mcdk_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Astra DB cluster in Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Coordinator Service is a stateless service managed by a Deployment (3) which
    delegates authentication and authorization checks on each call to the IAM Service
    (4).
  prefs: []
  type: TYPE_NORMAL
- en: Authorized requests are then routed to one or more Data Services based on the
    tenant, again using a Kubernetes Service (5).
  prefs: []
  type: TYPE_NORMAL
- en: Data Services are managed using StatefulSets (6), which are used to assign each
    instance to a local PersistentVolume used for managing intermediate datafiles
    such as the commit log, which is populated immediately on writes. When possible,
    reads are served directly from in-memory data structures.
  prefs: []
  type: TYPE_NORMAL
- en: As is typical for Cassandra and other LSM tree storage engines, the Data Service
    occasionally writes SSTable files out to a persistent store (7). For Astra DB,
    that persistent store is an external object store managed by the cloud provider
    for high availability. A separate object storage bucket is used per tenant to
    ensure data privacy.
  prefs: []
  type: TYPE_NORMAL
- en: The Compaction Service can perform compaction and repair on SSTables in the
    object store asynchronously (8), with no impact to write and read queries.
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB also supports multiregion database clusters, which by definition span
    multiple Kubernetes clusters. Coordinator and Data Services are deployed across
    Datacenters (cloud regions) and racks (availability zones) using an approach similar
    to that described for K8ssandra in [“Deploying Multicluster Applications in Kubernetes”](ch06.html#deploying_multicluster_applications_in).
  prefs: []
  type: TYPE_NORMAL
- en: Astra DB’s microservice architecture allows it to make more optimal use of compute
    and storage resources and isolate compute-intensive operations, leading to overall
    cost savings to operate Cassandra clusters in the cloud. These cost savings are
    extended by the addition of multitenant features that allow each cluster to be
    shared across multiple tenants. The [Astra DB whitepaper](https://oreil.ly/Zq0yc)
    describes a technique called *shuffle sharding* which is used to match each tenant
    to a subset of the available Coordinator and Data Services, effectively creating
    a separate Cassandra token ring per tenant. As the population of tenants in an
    Astra DB instance changes, this topology can be easily updated to rebalance load
    without downtime, and larger tenants can be configured to use their own dedicated
    databases (DBInstallations). This approach minimizes cost while meeting SLAs for
    performance and availability.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve focused on the architecture Astra DB uses to provide
    a multitenant, serverless Cassandra that embodies both cloud native and Kubernetes
    native principles using a completely different style of deployment. This continues
    the tradition of the Amazon Dynamo and Google BigTable papers in generating public
    discussion around novel database architectures. In addition, several open source
    projects mentioned in this book including Cass Operator, K8ssandra, and Stargate
    trace their origins to Astra DB. A lot of innovation is going on in areas such
    as the core database, control plane, change data capture, streaming integration,
    data migration, and more, so look for more open source contributions and architecture
    proposals from this team in the future.
  prefs: []
  type: TYPE_NORMAL
- en: What to Look for in a Kubernetes Native Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After everything you’ve learned in the past few chapters about what it takes
    to deploy and manage various databases on Kubernetes, we are in a great position
    to define what you should look for in a Kubernetes native database.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following our cloud native data principles, the following are a few areas that
    should be considered basic requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum leverage of Kubernetes APIs
  prefs: []
  type: TYPE_NORMAL
- en: The database should be as tightly integrated with Kubernetes APIs as possible
    (for example, using PersistentVolumes for both local and remote storage, using
    Services for routing rather than maintaining lists of IPs of other nodes, and
    so on). Kubernetes extension points described in [Chapter 5](ch05.html#automating_database_management_on_kuber)
    should be used to supplement built-in Kubernetes functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In some areas, the existing Kubernetes APIs may not provide the exact behavior
    required for a given database or other application, as demonstrated by the creation
    of alternate StatefulSet implementations by the Vitess and TiDB projects. In these
    cases, every effort should be made to donate improvements back to the Kubernetes
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Automated, declarative management via operators
  prefs: []
  type: TYPE_NORMAL
- en: Databases should be deployed and managed on Kubernetes using operators and custom
    resources. Operators should serve as the primary control plane elements for managing
    databases. While it’s arguably helpful to have command-line tools or `kubectl`
    extensions that allow DBAs to intervene manually to optimize database performance
    and fix issues, these are ultimately functions that should be performed by an
    operator as it achieves the higher levels of maturity discussed in [Chapter 5](ch05.html#automating_database_management_on_kuber).
  prefs: []
  type: TYPE_NORMAL
- en: The goal should be that all required changes to a database can be accomplished
    by updating the desired state in a custom resource and letting the operator handle
    the rest. We’ll be in a great place when we can configure a database in terms
    of service-level objectives such as latency, throughput, availability, and cost
    per unit. Operators can determine how many database nodes are needed, what compute
    and storage tiers to use, when to perform backups, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Observable through standard APIs
  prefs: []
  type: TYPE_NORMAL
- en: We’re beginning to see common expectations for observability for data infrastructure
    on Kubernetes in terms of the familiar triad of metrics, logs, and tracing. The
    Prometheus-Grafana stack is somewhat of a de facto standard for metrics collection
    and visualization, with exposure of metrics from database services using the Prometheus
    format as a minimum criteria. Projects providing Prometheus integration should
    be flexible enough to provide their own dedicated stack, or push metrics to an
    existing installation shared with other applications.
  prefs: []
  type: TYPE_NORMAL
- en: Logs from all database application containers should be pushed to standard output
    (stdout) using sidecars if necessary—so they can be collected by log aggregation
    services. While it may take longer to see adoption for tracing, the ability to
    follow individual client requests through application calls down into the database
    tier through APIs such as OpenTracing will be an extremely powerful debugging
    tool for future cloud native applications.
  prefs: []
  type: TYPE_NORMAL
- en: Secure by default
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes project itself provides a great example of what it means to be
    secure by default—for example, by exposing access to ports on Pods and containers
    only when specifically enabled, and by providing primitives like Secrets that
    we can use to protect access to login credentials or sensitive configuration data.
  prefs: []
  type: TYPE_NORMAL
- en: Databases and other infrastructure need to make use of these tools and adopt
    industry standards and best practices for zero trust (including changing default
    administrator credentials), limiting exposure of application and management APIs.
    Exposed APIs should prefer encrypted protocols such as HTTPS. Data stored in PersistentVolumes
    should be encrypted, whether this encryption is performed by the application,
    the database, or the StorageClass provider. Audit logs should be provided as part
    of application logging, especially with respect to actions that configure user
    access.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, a Kubernetes native database is sympathetic to the way that Kubernetes
    works. It maximizes reuse of Kubernetes built-in capabilities instead of bringing
    along its own set of duplicative supporting infrastructure. The experience of
    using a Kubernetes native database is therefore very much like using Kubernetes
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Kubernetes Native
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As these basic requirements and more advanced expectations for what it means
    to be Kubernetes native solidify, what comes next? We’re starting to see common
    patterns within projects deploying databases on Kubernetes that could point to
    where things are headed in the future. These are admittedly a bit fuzzier, but
    let’s try to bring a couple of them into focus.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability through multidimensional architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed the repetition of several terms throughout the past few
    chapters such as *multicluster*, *multitenancy*, *microservices*, and *serverless*.
    A common thread uniting these terms is that they represent architectural approaches
    to scalability, as shown in [Figure 7-6](#architectural_approaches_for_scaling_in).
  prefs: []
  type: TYPE_NORMAL
- en: '![Architectural approaches for scaling in multiple dimensions](assets/mcdk_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Architectural approaches for scaling in multiple dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider how each of these approaches provides an independent axis for scalability.
    The visualization in [Figure 7-6](#architectural_approaches_for_scaling_in) depicts
    the impact of your application as a three-dimensional surface that grows as you
    scale along each axis:'
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures break the various functions of a database into independently
    scalable services. The serverless approach builds on this, encouraging the isolation
    of persistent state to a limited number of stateful services or even external
    services as much as possible. Kubernetes storage APIs in the PersistentVolume
    subsystem make it possible to leverage both local and networked storage options.
    These trends allow a true separation of compute and storage, and scale these resources
    independently.
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster
  prefs: []
  type: TYPE_NORMAL
- en: '*Multicluster* refers to the ability to scale an application across multiple
    Kubernetes clusters. Along with related terms like *multiregion*, *multi-datacenter*,
    and *multicloud*, this implies expanding the geographic footprint of the capabilities
    provided across potentially heterogeneous environments. This distribution of capability
    has positive implications for meeting users where they are with minimum latency,
    cloud provider cost optimization, and disaster recovery. As we discussed in [Chapter 6](ch06.html#integrating_data_infrastructure_in_a_ku),
    Kubernetes has historically not been as strong in its support for cross-cluster
    networking and service discovery. It will be interesting to track how databases
    and other applications take advantage of expected advances in Kubernetes federation
    in the coming years.'
  prefs: []
  type: TYPE_NORMAL
- en: Multitenancy
  prefs: []
  type: TYPE_NORMAL
- en: This is the ability to share infrastructure between multiple users to achieve
    the most efficient use of resources. As the public cloud providers have demonstrated
    in their IaaS offerings, a multitenant approach can be very effective at providing
    users a low-cost, low-risk access to infrastructure for innovative new projects,
    and then providing additional resources as these applications grow. Adopting a
    multitenant approach for data infrastructure has great potentiial value as well,
    so long as security guarantees are properly met and there is a seamless transition
    path to dedicated infrastructure for high-volume users before they become “noisy
    neighbors.” At this point in time, Kubernetes does not provide explicit support
    for multitenancy, although Namespaces can be a useful tool for providing dedicated
    resources for specific users.
  prefs: []
  type: TYPE_NORMAL
- en: While you may not have immediate need for all three of these axes of scalability
    for applications or data infrastructure you’re building, consider how growing
    in each of them can enhance the overall value you’re offering the world.
  prefs: []
  type: TYPE_NORMAL
- en: Community-focused innovation through open source and cloud services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another pattern you may have noticed in our narrative is the continual innovation
    loop between open source database projects and DBaaS offerings. PingCAP took the
    open source MySQL and ClickHouse databases, created a database service leveraging
    Kubernetes to help it manage the databases at scale, and then released open source
    projects including TiDB and TiFlash. DataStax took open source Cassandra, factored
    it into microservices, added an API layer, and deployed it on Kubernetes for its
    Astra DB, and has created multiple open source projects including Cass Operator,
    K8ssandra, and Stargate. In the spirit of Dynamo, BigTable, Calvin and other papers,
    these companies have open source architectures as well.
  prefs: []
  type: TYPE_NORMAL
- en: This innovation loop mirrors that of the larger Kubernetes community, in which
    the major cloud providers and storage vendors have helped drive the maturation
    of the core Kubernetes control plane and PersistentVolume subsystem, respectively.
    It’s interesting to observe that the highest momentum and fastest cycle time occurs
    within innovation loops that center around cloud services, rather than around
    the classic open core model focused on enterprise versions of open source projects.
  prefs: []
  type: TYPE_NORMAL
- en: As a software vendor, providing a cloud service allows you to iterate and evaluate
    new architectures and features more quickly. Flowing these innovations back to
    open source allows you to grow adoption by supporting a flexible consumption model.
    Both “run it yourself” and “rent it from us” become legitimate deployment options
    for your customers, with the ability to flex between approaches for different
    use cases. Customers gain confidence in the overall maturity and security of your
    technology, knowing that the open source version they can inspect and contribute
    to is largely the same as what you are running in your DBaaS.
  prefs: []
  type: TYPE_NORMAL
- en: 'A final side effect of these innovation trends is an implicit pull toward proven
    architectures and components. Consider these examples:'
  prefs: []
  type: TYPE_NORMAL
- en: etcd is used as a metadata store across multiple projects we’ve examined in
    this book, including Vitess and Astra DB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TiDB leverages the architecture of F1, implemented the Raft consensus protocol,
    and extended the ClickHouse columnar store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Astra DB leverages both the PersistentVolume subsystem and S3-compliant object
    storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of inventing new technologies to solve problems like metadata management
    and distributed transactions, these projects are investing their innovation in
    new features, developer experience, and the scalability axes we’ve examined in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve taken a deep look at TiDB and Astra DB to search out
    what makes them Kubernetes native. What was the point of this exercise? Our hope
    is that this analysis provides a deeper understanding to help consumers ask more
    insightful questions about the data infrastructure they are consuming, and to
    help those building data infrastructure and ecosystems to create technology that
    meets those expectations. We believe that data infrastructure that is not only
    cloud native but also Kubernetes native will lead to the best outcomes for everyone
    in terms of performance, availability, and cost.
  prefs: []
  type: TYPE_NORMAL
