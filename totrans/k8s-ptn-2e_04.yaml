- en: Chapter 2\. Predictable Demands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The foundation of successful application deployment, management, and coexistence
    on a shared cloud environment is dependent on identifying and declaring the application
    resource requirements and runtime dependencies. This *Predictable Demands* pattern
    indicates how you should declare application requirements, whether they are hard
    runtime dependencies or resource requirements. Declaring your requirements is
    essential for Kubernetes to find the right place for your application within the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes can manage applications written in different programming languages
    as long as the application can be run in a container. However, different languages
    have different resource requirements. Typically, a compiled language runs faster
    and often requires less memory compared to just-in-time runtimes or interpreted
    languages. Considering that many modern programming languages in the same category
    have similar resource requirements, from a resource consumption point of view,
    more important aspects are the domain, the business logic of an application, and
    the actual implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: Besides resource requirements, application runtimes also have dependencies on
    platform-managed capabilities like data storage or application configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowing the runtime requirements for a container is important mainly for two
    reasons. First, with all the runtime dependencies defined and resource demands
    envisaged, Kubernetes can make intelligent decisions about where to place a container
    on the cluster for the most efficient hardware utilization. In an environment
    with shared resources among a large number of processes with different priorities,
    the only way to ensure a successful coexistence is to know the demands of every
    process in advance. However, intelligent placement is only one side of the coin.
  prefs: []
  type: TYPE_NORMAL
- en: Container resource profiles are also essential for capacity planning. Based
    on the particular service demands and the total number of services, we can do
    some capacity planning for different environments and come up with the most cost-effective
    host profiles to satisfy the entire cluster demand. Service resource profiles
    and capacity planning go hand in hand for successful cluster management in the
    long term.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into resource profiles, let’s look at declaring runtime dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime Dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common runtime dependencies is file storage for saving application
    state. Container filesystems are ephemeral and are lost when a container is shut
    down. Kubernetes offers volume as a Pod-level storage utility that survives container
    restarts.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward type of volume is `emptyDir`, which lives as long as
    the Pod lives. When the Pod is removed, its content is also lost. The volume needs
    to be backed by another kind of storage mechanism to survive Pod restarts. If
    your application needs to read or write files to such long-lived storage, you
    must declare that dependency explicitly in the container definition using volumes,
    as shown in [Example 2-1](#ex-dependency-pv).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. Dependency on a PersistentVolume
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_predictable_demands_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Dependency of a PersistentVolumeClaim (PVC) to be present and bound.
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler evaluates the kind of volume a Pod requires, which affects where
    the Pod gets placed. If the Pod needs a volume that is not provided by any node
    on the cluster, the Pod is not scheduled at all. Volumes are an example of a runtime
    dependency that affects what kind of infrastructure a Pod can run and whether
    the Pod can be scheduled at all.
  prefs: []
  type: TYPE_NORMAL
- en: A similar dependency happens when you ask Kubernetes to expose a container port
    on a specific port on the host system through `hostPort`. The usage of a `hostPort`
    creates another runtime dependency on the nodes and limits where a Pod can be
    scheduled. `hostPort` reserves the port on each node in the cluster and is limited
    to a maximum of one Pod scheduled per node. Because of port conflicts, you can
    scale to as many Pods as there are nodes in the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Configurations are another type of dependency. Almost every application needs
    some configuration information, and the recommended solution offered by Kubernetes
    is through ConfigMaps. Your services need to have a strategy for consuming settings—either
    through environment variables or the filesystem. In either case, this introduces
    a runtime dependency of your container to the named ConfigMaps. If not all of
    the expected ConfigMaps are created, the containers are scheduled on a node, but
    they do not start up.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to ConfigMaps, Secrets offer a slightly more secure way of distributing
    environment-specific configurations to a container. The way to consume a Secret
    is the same as it is for ConfigMaps, and using a Secret introduces the same kind
    of dependency from a container to a namespace.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps and Secrets are explained in more detail in [Chapter 20, “Configuration
    Resource”](ch20.html#ConfigurationResource), and [Example 2-2](#ex-dependency-configmap)
    shows how these resources are used as runtime dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. Dependency on a ConfigMap
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_predictable_demands_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Mandatory dependency on the ConfigMap `random-generator-config`.
  prefs: []
  type: TYPE_NORMAL
- en: While the creation of ConfigMap and Secret objects are simple deployment tasks
    we have to perform, cluster nodes provide storage and port numbers. Some of these
    dependencies limit where a Pod gets scheduled (if anywhere at all), and other
    dependencies may prevent the Pod from starting up. When designing your containerized
    applications with such dependencies, always consider the runtime constraints they
    will create later.
  prefs: []
  type: TYPE_NORMAL
- en: Resource Profiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Specifying container dependencies such as ConfigMap, Secret, and volumes is
    straightforward. We need some more thinking and experimentation for figuring out
    the resource requirements of a container. Compute resources in the context of
    Kubernetes are defined as something that can be requested by, allocated to, and
    consumed from a container. The resources are categorized as *compressible* (i.e.,
    can be throttled, such as CPU or network bandwidth) and *incompressible* (i.e.,
    cannot be throttled, such as memory).
  prefs: []
  type: TYPE_NORMAL
- en: Making the distinction between compressible and incompressible resources is
    important. If your containers consume too many compressible resources such as
    CPU, they are throttled, but if they use too many incompressible resources (such
    as memory), they are killed (as there is no other way to ask an application to
    release allocated memory).
  prefs: []
  type: TYPE_NORMAL
- en: Based on the nature and the implementation details of your application, you
    have to specify the minimum amount of resources that are needed (called `requests`)
    and the maximum amount it can grow up to (the `limits`). Every container definition
    can specify the amount of CPU and memory it needs in the form of a request and
    limit. At a high level, the concept of `requests`/`limits` is similar to soft/hard
    limits. For example, similarly, we define heap size for a Java application by
    using the `-Xms` and `-Xmx` command-line options.
  prefs: []
  type: TYPE_NORMAL
- en: The `requests` amount (but not `limits`) is used by the scheduler when placing
    Pods to nodes. For a given Pod, the scheduler considers only nodes that still
    have enough capacity to accommodate the Pod and all of its containers by summing
    up the requested resource amounts. In that sense, the `requests` field of each
    container affects where a Pod can be scheduled or not. [Example 2-3](#ex-resource-limits)
    shows how such limits are specified for a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. Resource limits
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_predictable_demands_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Initial resource request for CPU and memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_predictable_demands_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Upper limit until we want our application to grow at max. We don’t specify CPU
    limits by intention.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following types of resources can be used as keys in the `requests` and
    `limits` specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '`memory`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This type is for the heap memory demands of your application, including volumes
    of type `emptyDir` with the configuration `medium: Memory`. Memory resources are
    incompressible, so containers that exceed their configured memory limit will trigger
    the Pod to be evicted; i.e., it gets deleted and recreated potentially on a different
    node.'
  prefs: []
  type: TYPE_NORMAL
- en: '`cpu`'
  prefs: []
  type: TYPE_NORMAL
- en: The `cpu` type is used to specify the range of needed CPU cycles for your application.
    However, it is a compressible resource, which means that in an overcommit situation
    for a node, all assigned CPU slots of all running containers are throttled relative
    to their specified requests. Therefore, it is highly recommended that you set
    `requests` for the CPU resource but *no* `limits` so that they can benefit from
    all excess CPU resources that otherwise would be wasted.
  prefs: []
  type: TYPE_NORMAL
- en: '`ephemeral-storage`'
  prefs: []
  type: TYPE_NORMAL
- en: Every node has some filesystem space dedicated for ephemeral storage that holds
    logs and writable container layers. `emptyDir` volumes that are not stored in
    a memory filesystem also use ephemeral storage. With this request and limit type,
    you can specify the application’s minimal and maximal needs. `ephemeral-storage`
    resources are not compressible and will cause a Pod to be evicted from the node
    if it uses more storage than specified in its `limit`.
  prefs: []
  type: TYPE_NORMAL
- en: '`hugepage-<size>`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Huge pages* are large, contiguous pre-allocated pages of memory that can be
    mounted as volumes. Depending on your Kubernetes node configuration, several sizes
    of huge pages are available, like 2 MB and 1 GB pages. You can specify a request
    and limit for how many of a certain type of huge pages you want to consume (e.g.,
    `hugepages-1Gi: 2Gi` for requesting two 1 GB huge pages). Huge pages can’t be
    overcommitted, so the request and limit must be the same.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on whether you specify the `requests`, the `limits`, or both, the
    platform offers three types of Quality of Service (QoS):'
  prefs: []
  type: TYPE_NORMAL
- en: Best-Effort
  prefs: []
  type: TYPE_NORMAL
- en: Pods that do not have any requests and limits set for its containers have a
    QoS of *Best-Effort*. Such a *Best-Effort* Pod is considered the lowest priority
    and is most likely killed first when the node where the Pod is placed runs out
    of incompressible resources.
  prefs: []
  type: TYPE_NORMAL
- en: Burstable
  prefs: []
  type: TYPE_NORMAL
- en: A Pod that defines an unequal amount for `requests` and `limits` values (and
    `limits` is larger than `requests`, as expected) are tagged as *Burstable*. Such
    a Pod has minimal resource guarantees but is also willing to consume more resources
    up to its `limit` when available. When the node is under incompressible resource
    pressure, these Pods are likely to be killed if no *Best-Effort* Pods remain.
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteed
  prefs: []
  type: TYPE_NORMAL
- en: A Pod that has an equal amount of `request` and `limit` resources belongs to
    the *Guaranteed* QoS category. These are the highest-priority Pods and are guaranteed
    not to be killed before *Best-Effort* and *Burstable* Pods. This QoS mode is the
    best option for your application’s memory resources, as it entails the least surprise
    and avoids out-of-memory triggered evictions.
  prefs: []
  type: TYPE_NORMAL
- en: So the resource characteristics you define or omit for the containers have a
    direct impact on its QoS and define the relative importance of the Pod in the
    event of resource starvation. Define your Pod resource requirements with this
    consequence in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Priority
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explained how container resource declarations also define Pods’ QoS and affect
    the order in which the Kubelet kills the container in a Pod in case of resource
    starvation. Two other related concepts are Pod priority and preemption. *Pod priority*
    allows you to indicate the importance of a Pod relative to other Pods, which affects
    the order in which Pods are scheduled. Let’s see that in action in [Example 2-4](#ex-predictable-demands-pod-priority).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. Pod priority
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_predictable_demands_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the priority class object.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_predictable_demands_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The priority value of the object.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_predictable_demands_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '`globalDefault` set to `true` is used for Pods that do not specify a `priorityClassName`.
    Only one PriorityClass can have `globalDefault` set to `true`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_predictable_demands_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The priority class to use with this Pod, as defined in PriorityClass resource.
  prefs: []
  type: TYPE_NORMAL
- en: We created a PriorityClass, a non-namespaced object for defining an integer-based
    priority. Our PriorityClass is named `high-priority` and has a priority of 1,000\.
    Now we can assign this priority to Pods by its name as `priorityClassName:` `high-priority`.
    PriorityClass is a mechanism for indicating the importance of Pods relative to
    one another, where the higher value indicates more important Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Pod priority affects the order in which the scheduler places Pods on nodes.
    First, the priority admission controller uses the `priorityClassName` field to
    populate the priority value for new Pods. When multiple Pods are waiting to be
    placed, the scheduler sorts the queue of pending Pods by highest priority first.
    Any pending Pod is picked before any other pending Pod with lower priority in
    the scheduling queue, and if there are no constraints preventing it from scheduling,
    the Pod gets scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Here comes the critical part. If there are no nodes with enough capacity to
    place a Pod, the scheduler can preempt (remove) lower-priority Pods from nodes
    to free up resources and place Pods with higher priority. As a result, the higher-priority
    Pod might be scheduled sooner than Pods with a lower priority if all other scheduling
    requirements are met. This algorithm effectively enables cluster administrators
    to control which Pods are more critical workloads and place them first by allowing
    the scheduler to evict Pods with lower priority to make room on a worker node
    for higher-priority Pods. If a Pod cannot be scheduled, the scheduler continues
    with the placement of other lower-priority Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you want your Pod to be scheduled with a particular priority but don’t
    want to evict any existing Pods. In that case, you can mark a PriorityClass with
    the field `preemptionPolicy: Never`. Pods assigned to this priority class will
    not trigger any eviction of running Pods but will still get scheduled according
    to their priority value.'
  prefs: []
  type: TYPE_NORMAL
- en: Pod QoS (discussed previously) and Pod priority are two orthogonal features
    that are not connected and have only a little overlap. QoS is used primarily by
    the Kubelet to preserve node stability when available compute resources are low.
    The Kubelet first considers QoS and then the PriorityClass of Pods before eviction.
    On the other hand, the scheduler eviction logic ignores the QoS of Pods entirely
    when choosing preemption targets. The scheduler attempts to pick a set of Pods
    with the lowest priority possible that satisfies the needs of higher-priority
    Pods waiting to be placed.
  prefs: []
  type: TYPE_NORMAL
- en: When Pods have a priority specified, it can have an undesired effect on other
    Pods that are evicted. For example, while a Pod’s graceful termination policies
    are respected, the PodDisruptionBudget as discussed in [Chapter 10, “Singleton
    Service”](ch10.html#SingletonService), is not guaranteed, which could break a
    lower-priority clustered application that relies on a quorum of Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Another concern is a malicious or uninformed user who creates Pods with the
    highest possible priority and evicts all other Pods. To prevent that, ResourceQuota
    has been extended to support PriorityClass, and higher-priority numbers are reserved
    for critical system-Pods that should not usually be preempted or evicted.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Pod priorities should be used with caution because user-specified
    numerical priorities that guide the scheduler and Kubelet about which Pods to
    place or to kill are subject to gaming by users. Any change could affect many
    Pods and could prevent the platform from delivering predictable service-level
    agreements.
  prefs: []
  type: TYPE_NORMAL
- en: Project Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is a self-service platform that enables developers to run applications
    as they see suitable on the designated isolated environments. However, working
    in a shared multitenanted platform also requires the presence of specific boundaries
    and control units to prevent some users from consuming all the platform’s resources.
    One such tool is ResourceQuota, which provides constraints for limiting the aggregated
    resource consumption in a namespace. With ResourceQuotas, the cluster administrators
    can limit the total sum of computing resources (CPU, memory) and storage consumed.
    It can also limit the total number of objects (such as ConfigMaps, Secrets, Pods,
    or Services) created in a namespace. [Example 2-5](#ex-predictable-demands-resourcequota)
    shows an instance that limits the usage of certain resources. See the official
    Kubernetes documentation on [Resource Quotas](https://oreil.ly/TLRMe) for the
    full list of supported resources for which you can restrict usage with ResourceQuotas.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. Definition of resource constraints
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_predictable_demands_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Namespace to which resource constraints are applied.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_predictable_demands_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Allow four active Pods in this namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_predictable_demands_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The sum of all memory limits of all Pods in this namespace must not be more
    than 5 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Another helpful tool in this area is LimitRange, which allows you to set resource
    usage limits for each type of resource. In addition to specifying the minimum
    and maximum permitted amounts for different resource types and the default values
    for these resources, it also allows you to control the ratio between the `requests`
    and `limits`, also known as the *overcommit level*. [Example 2-6](#ex-predictable-demands-limitrange)
    shows a LimitRange and the possible configuration options.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-6\. Definition of allowed and default resource usage limits
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_predictable_demands_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum values for requests and limits.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_predictable_demands_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum values for requests and limits.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_predictable_demands_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Default values for limits when no limits are specified.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_predictable_demands_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Default values for requests when no requests are specified.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_predictable_demands_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum ratio limit/request, used to specify the allowed overcommit level. Here,
    the memory limit must not be larger than twice the memory request, and the CPU
    limit can be as high as four times the CPU request.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_predictable_demands_CO6-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Type can be `Container`, `Pod`, (for all containers combined), or `PersistentVolumeClaim`
    (to specify the range for a request persistent volume).
  prefs: []
  type: TYPE_NORMAL
- en: LimitRanges help control the container resource profiles so that no containers
    require more resources than a cluster node can provide. LimitRanges can also prevent
    cluster users from creating containers that consume many resources, making the
    nodes not allocatable for other containers. Considering that the `requests` (and
    not `limits`) are the primary container characteristic the scheduler uses for
    placing, LimitRequestRatio allows you to control the amount of difference between
    the `requests` and `limits` of containers. A big combined gap between `requests`
    and `limits` increases the chances of overcommitting on the node and may degrade
    application performance when many containers simultaneously require more resources
    than initially requested.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that other shared node-level resources such as process IDs (PIDs)
    can be exhausted before hitting any resource limits. Kubernetes allows you to
    reserve a number of node PIDs for the system use and ensure that they are never
    exhausted by user workloads. Similarly, Pod PID limits allow a cluster administrator
    to limit the number of processes running in a Pod. We are not reviewing these
    in details here as they are set as Kubelet configurations options by cluster administrators
    and are not used by application developers.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering that containers may have different resource profiles in different
    environments, and a varied number of instances, it is evident that capacity planning
    for a multipurpose environment is not straightforward. For example, for best hardware
    utilization, on a nonproduction cluster, you may have mainly *Best-Effort* and
    *Burstable* containers. In such a dynamic environment, many containers are starting
    up and shutting down at the same time, and even if a container gets killed by
    the platform during resource starvation, it is not fatal. On the production cluster,
    where we want things to be more stable and predictable, the containers may be
    mainly of the *Guaranteed* type, and some may be *Burstable*. If a container gets
    killed, that is most likely a sign that the capacity of the cluster should be
    increased.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-1](#table-demands-service-example) presents a few services with CPU
    and memory demands.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Capacity planning example
  prefs: []
  type: TYPE_NORMAL
- en: '| Pod | CPU request | Memory request | Memory limit | Instances |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A | 500 m | 500 Mi | 500 Mi | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 250 m | 250 Mi | 1000 Mi | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 500 m | 1000 Mi | 2000 Mi | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 500 m | 500 Mi | 500 Mi | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Total** | **4000 m** | **5000 Mi** | **8500 Mi** | **9** |'
  prefs: []
  type: TYPE_TB
- en: Of course, in a real-life scenario, the more likely reason you are using a platform
    such as Kubernetes is that there are many more services to manage, some of which
    are about to retire, and some of which are still in the design and development
    phase. Even if it is a continually moving target, based on a similar approach
    as described previously, we can calculate the total amount of resources needed
    for all the services per environment.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that in the different environments, there are different numbers
    of containers, and you may even need to leave some room for autoscaling, build
    jobs, infrastructure containers, and more. Based on this information and the infrastructure
    provider, you can choose the most cost-effective compute instances that provide
    the required resources.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers are useful not only for process isolation and as a packaging format.
    With identified resource profiles, they are also the building blocks for successful
    capacity planning. Perform some early tests to discover the resource needs for
    each container, and use that information as a base for future capacity planning
    and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can help you here with the *Vertical Pod Autoscaler* (VPA), which
    monitors the resource consumption of your Pod over time and gives a recommendation
    for requests and limits. The VPA is described in detail in [“Vertical Pod Autoscaling”](ch29.html#elasticscale-vertical).
  prefs: []
  type: TYPE_NORMAL
- en: However, more importantly, resource profiles are the way an application communicates
    with Kubernetes to assist in scheduling and managing decisions. If your application
    doesn’t provide any `requests` or `limits`, all Kubernetes can do is treat your
    containers as opaque boxes that are dropped when the cluster gets full. So it
    is more or less mandatory for every application to think about and provide these
    resource declarations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to size our applications, in [Chapter 3, “Declarative
    Deployment”](ch03.html#DeclarativeDeployment), you will learn multiple strategies
    to install and update our applications on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: More Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Predictable Demands Example](https://oreil.ly/HYIqJ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Configure a Pod to Use a ConfigMap](https://oreil.ly/c54Gh)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kubernetes Best Practices: Resource Requests and Limits](https://oreil.ly/8bKD5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Resource Management for Pods and Containers](https://oreil.ly/a37eO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Manage HugePages](https://oreil.ly/RXQD1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Configure Default Memory Requests and Limits for a Namespace](https://oreil.ly/ozlU1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Node-Pressure Eviction](https://oreil.ly/fxRvs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pod Priority and Preemption](https://oreil.ly/FpUoH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Configure Quality of Service for Pods](https://oreil.ly/x07OT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Resource Quality of Service in Kubernetes](https://oreil.ly/yORlL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Resource Quotas](https://oreil.ly/rFSLa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Limit Ranges](https://oreil.ly/1bXfO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Process ID Limits and Reservations](https://oreil.ly/lkmMK)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[For the Love of God, Stop Using CPU Limits on Kubernetes](https://oreil.ly/Yk-Ag)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Everyone Should Know About Kubernetes Memory Limits](https://oreil.ly/cdJkP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
