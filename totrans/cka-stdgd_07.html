<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Troubleshooting"><div class="chapter" id="troubleshooting">
<h1><span class="label">Chapter 7. </span>Troubleshooting</h1>


<p><a data-type="indexterm" data-primary="troubleshooting" id="tro_ch"/>Establishing a Kubernetes cluster is one thing. Making sure that the cluster stays operational is another. As a Kubernetes administrator, you are continuously confronted with making sure that the cluster stays functional. Therefore, your troubleshooting skills must be sharp so that you can come up with strategies for identifying the root cause of an issue and fixing it.</p>

<p>Of all the domains covered by the exam, the section “Troubleshooting” has the highest weight for the overall score, so it’s important to understand failure scenarios and learn how to fix them. This chapter will address how to monitor and troubleshoot applications in different constellations. Furthermore, we’ll discuss failures that may arise for cluster components due to misconfiguration or error conditions.</p>

<p>At a high level, this chapter covers the following concepts:</p>

<ul>
<li>
<p>Evaluating logging options</p>
</li>
<li>
<p>Monitoring applications</p>
</li>
<li>
<p>Accessing container logs</p>
</li>
<li>
<p>Troubleshooting application failures</p>
</li>
<li>
<p>Troubleshooting cluster failures</p>
</li>
</ul>






<section data-type="sect1" data-pdf-bookmark="Evaluating Cluster and Node Logging"><div class="sect1" id="idm45322716763296">
<h1>Evaluating Cluster and Node Logging</h1>

<p><a data-type="indexterm" data-primary="cluster logging" id="idm45322716761952"/><a data-type="indexterm" data-primary="node logging" id="idm45322716761248"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="evaluating cluster logging" id="idm45322716760576"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="evaluating node logging" id="idm45322716759568"/>A real-world Kubernetes cluster manages hundreds or even thousands of Pods. For every Pod, you have at least a single container running a process. Each process can produce log output to the standard output or standard error streams. It’s imperative to capture the log output to proficiently determine the root cause of an application error. Moreover, cluster components produce logs for diagnostic purposes.</p>

<p>As you can see, Kubernetes’ logging mechanism is crucial for tracking down errors and monitoring cluster components and applications. Kubernetes can be configured to log on the cluster or the node level. The implementation approaches and their potential trade-offs may differ from one another.</p>








<section data-type="sect2" data-pdf-bookmark="Cluster Logging"><div class="sect2" id="idm45322716757680">
<h2>Cluster Logging</h2>

<p>Kubernetes doesn’t provide a native solution for cluster-level logging, but you can choose from the following three options to fulfill the requirements:</p>

<ul>
<li>
<p>Instantiating a node-level logging agent that runs on each of the cluster nodes</p>
</li>
<li>
<p>Configuring a sidecar container responsible for handling the application logs</p>
</li>
<li>
<p>Pushing the logs directly to a logging backend from the application logic</p>
</li>
</ul>

<p>The following discussion explains the benefits and drawbacks for each approach. For a detailed discussion, see the <a href="https://oreil.ly/rJjfV">Kubernetes documentation</a>.</p>










<section data-type="sect3" data-pdf-bookmark="Using a node logging agent"><div class="sect3" id="idm45322716751776">
<h3>Using a node logging agent</h3>

<p><a data-type="indexterm" data-primary="logging agent" id="idm45322716750560"/>The logging agent is a dedicated tool that publishes the logs to a backend. A backend can be an external logging service outside of the cluster. <a data-type="xref" href="#cluster-logging-agent">Figure 7-1</a> visualizes the logging architecture.</p>

<figure><div id="cluster-logging-agent" class="figure"><div class="border-box"><img src="Images/ckas_0701.png" alt="ckas 0701" width="1391" height="518"/></div><h6><span class="label">Figure 7-1. </span>Cluster-level logging with an agent</h6></div></figure>

<p>The benefit of this approach is that the application doesn’t require any changes to the code or the Pod configuration to support collecting logs. Agents should be run as a DaemonSet.</p>
</div></section>













<section data-type="sect3" class="less_space pagebreak-before" data-pdf-bookmark="Using a sidecar container"><div class="sect3" id="idm45322716745968">
<h3>Using a sidecar container</h3>

<p><a data-type="indexterm" data-primary="containers" data-secondary="sidecar" id="idm45322716744288"/><a data-type="indexterm" data-primary="sidecar container" id="idm45322716743312"/>A Pod can be configured to run another sidecar container alongside the main application container. The sidecar container streams standard output and error produced by the application and redirects the streams to a different location (e.g., a logging backend or a volume mounted to the container). <a data-type="xref" href="#cluster-logging-sidecar">Figure 7-2</a> shows the logging setup of a Pod that incorporates a streaming sidecar.</p>

<figure><div id="cluster-logging-sidecar" class="figure"><div class="border-box"><img src="Images/ckas_0702.png" alt="ckas 0702" width="1391" height="518"/></div><h6><span class="label">Figure 7-2. </span>Cluster-level logging with a sidecar container</h6></div></figure>

<p>This approach has the benefit of being able to easily separate different streams (e.g., separating error from info log entries).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Pushing directly to logging backend"><div class="sect3" id="idm45322716739040">
<h3>Pushing directly to logging backend</h3>

<p>This approach pushes the responsibility onto the application without adding a middleman. <a data-type="xref" href="#cluster-logging-direct">Figure 7-3</a> shows the logging setup.</p>

<figure><div id="cluster-logging-direct" class="figure"><div class="border-box"><img src="Images/ckas_0703.png" alt="ckas 0703" width="785" height="280"/></div><h6><span class="label">Figure 7-3. </span>Cluster-level logging by directly pushing to the backend</h6></div></figure>

<p>While architecturally less complex, any change to the logging backend will require a change to the application code and therefore a new deployment.</p>
</div></section>



</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Node Logging"><div class="sect2" id="idm45322716733808">
<h2>Node Logging</h2>

<p><a data-type="indexterm" data-primary="node logging" id="idm45322716732144"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="evaluating node logging" id="idm45322716731440"/>Node logging comes with the implication that the log files will be stored on the cluster node. The container runtime (e.g., Docker Engine) redirects standard output and error streams to the storage of the node with the help of the configured logging driver.</p>

<p><a data-type="indexterm" data-primary="logrotate" id="idm45322716729840"/>To avoid filling up the node storage with logging content, log rotation should be implemented. Log rotation is an automated process for compressing, moving, deleting, and/or archiving log data that grow beyond a certain threshold. The Linux tool <a href="https://oreil.ly/DE7XB">logrotate</a> is one way to configure log rotation for a Kubernetes cluster. <a data-type="xref" href="#node-logging">Figure 7-4</a> visualizes the node-level architecture.</p>

<figure><div id="node-logging" class="figure"><div class="border-box"><img src="Images/ckas_0704.png" alt="ckas 0704" width="1096" height="280"/></div><h6><span class="label">Figure 7-4. </span>Node-level logging</h6></div></figure>

<p>When you run <code>kubectl logs</code>, the kubelet receives the request, reads directly from the log file on the node, and returns the content to the client. The <code>kubectl logs</code> command returns only the latest log content, not the log entries that have already been archived.</p>

<p>The cluster components kube-scheduler and kube-proxy run in a container. Therefore, the log handling is the same as for any other application container. For system components that do not run in the container (e.g., the kubelet and the container runtime), logs will be written to journald if systemd is available. If systemd is not available, system components write their log files to the directory <code>/var/log</code> with the file extension <code>.log</code>.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Monitoring Cluster Components and Applications"><div class="sect1" id="idm45322716722240">
<h1>Monitoring Cluster Components and Applications</h1>

<p><a data-type="indexterm" data-primary="applications" data-secondary="monitoring for clusters" id="idm45322716720672"/><a data-type="indexterm" data-primary="clusters" data-secondary="monitoring components and applications" id="idm45322716719696"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="monitoring cluster components/applications" id="idm45322716718736"/>Deploying software to a Kubernetes cluster is only the start of operating an application long-term. Developers and administrators alike need to understand resource consumption patterns and behaviors of their applications with the goal of providing a scalable and reliable service.</p>

<p><a data-type="indexterm" data-primary="Prometheus" id="idm45322716717008"/><a data-type="indexterm" data-primary="Datadog" id="idm45322716716304"/>In the Kubernetes world, monitoring tools like Prometheus and Datadog help with collecting, processing, and visualizing the information over time. The exam does not expect you to be familiar with commercial monitoring, logging, tracing, and aggregation tools; however, it is helpful to gain a rough understanding of the underlying Kubernetes infrastructure responsible for collecting usage metrics. The following list shows examples of typical metrics:</p>

<ul>
<li>
<p>Number of nodes in the cluster</p>
</li>
<li>
<p>Health status of nodes</p>
</li>
<li>
<p>Node performance metrics such as CPU, memory, disk space, network</p>
</li>
<li>
<p>Pod-level performance metrics such as CPU and memory consumption</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="metrics server" id="idm45322716711152"/>This responsibility falls into the hands of the <a href="https://oreil.ly/OycST">metrics server</a>, a cluster-wide aggregator of resource usage data. As shown in <a data-type="xref" href="#metrics-server">Figure 7-5</a>, kubelets running on nodes collect the metrics and send them to the metrics server.</p>

<figure><div id="metrics-server" class="figure"><div class="border-box"><img src="Images/ckas_0705.png" alt="ckas 0705" width="1131" height="635"/></div><h6><span class="label">Figure 7-5. </span>Data collection for the metrics server</h6></div></figure>

<p>The metrics server stores data in memory and does not persist data over time. If you are looking for a solution that keeps historical data, then you need to look into commercial options. Refer to the documentation for more information on its installation process. If you’re using Minikube as your practice environment, <a href="https://oreil.ly/NanXK">enabling the metrics-server add-on</a> is straightforward using the following command:</p>
<pre data-type="programlisting">
<strong>$ minikube addons enable metrics-server</strong>
The 'metrics-server' addon is enabled
</pre>

<p>You can now query for metrics of cluster nodes and Pods with the <code>top</code> command:</p>

<pre data-type="programlisting"><strong>$ kubectl top nodes</strong>
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
minikube   283m         14%    1262Mi          32%
<strong>$ kubectl top pod frontend</strong>
NAME       CPU(cores)   MEMORY(bytes)
frontend   0m           2Mi</pre>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Troubleshooting Application Failures"><div class="sect1" id="idm45322716701616">
<h1>Troubleshooting Application Failures</h1>

<p><a data-type="indexterm" data-primary="applications" data-secondary="troubleshooting failure of" id="app_tro"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="application failures" id="tro_app"/>When operating an application in a production Kubernetes cluster, it’s almost inevitable that you’ll come across failure situations. It’s your responsibility as an administrator (potentially working closely with the application developer) to troubleshoot issues with deployed Kubernetes objects.</p>

<p>In this section, we’re going to take a look at debugging strategies that can help with identifying the root cause of an issue so that you can take action and correct the failure appropriately. For more information, reference the <a href="https://oreil.ly/4pxVS">Kubernetes documentation</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Troubleshooting Pods"><div class="sect2" id="idm45322716696096">
<h2>Troubleshooting Pods</h2>

<p><a data-type="indexterm" data-primary="Pods" data-secondary="troubleshooting" id="idm45322716694720"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="Pods" id="idm45322716693744"/>In most cases, creating a Pod is no issue. You simply emit the <code>run</code>, <code>create</code>, or <code>apply</code> commands to instantiate the Pod. If the YAML manifest is formed properly, Kubernetes accepts your request, so the assumption is that everything works as expected. To verify the correct behavior, the first thing you’ll want to do is to check the high-level runtime information of the Pod. The operation could involve other Kubernetes objects like a Deployment responsible for rolling out multiple replicas of a Pod.</p>










<section data-type="sect3" data-pdf-bookmark="Retrieving high-level information"><div class="sect3" id="idm45322716691184">
<h3>Retrieving high-level information</h3>

<p><a data-type="indexterm" data-primary="high-level information, retrieving" id="idm45322716689792"/><a data-type="indexterm" data-primary="commands" data-secondary="kubectl get all" id="idm45322716688656"/><a data-type="indexterm" data-primary="commands" data-secondary="kubectl get pods" id="idm45322716687744"/><a data-type="indexterm" data-primary="kubectl get all command" id="idm45322716686800"/><a data-type="indexterm" data-primary="kubectl get pods command" id="idm45322716686128"/>To retrieve the information, run either the <code>kubectl get pods</code> command for just the Pods running in the namespace or the <code>kubectl get all</code> command to retrieve the most prominent object types in the namespace (which includes Deployments). You will want to take a look at the columns <code>READY</code>, <code>STATUS</code>, and <code>RESTARTS</code>. In the optimal case, the number of ready containers matches the number of containers expected to be created by the Pod. For a single-container Pod, the <code>READY</code> column would say 1/1. The status should say <code>Running</code> to indicate that the Pod entered the proper lifecycle state. Be aware that it’s totally possible that a Pod renders a <code>Running</code> state, but the application isn’t actually working properly. If the number of restarts is greater than 0, then you might want to check the logic of the liveness probe (if defined) and identify the reason why a restart was necessary.</p>

<p>The following Pod observes the status <code>ErrImagePull</code> and makes 0/1 containers available to incoming traffic. In short, this Pod has a problem:</p>

<pre data-type="programlisting"><strong>$ kubectl get pods</strong>
NAME                  READY   STATUS         RESTARTS   AGE
pod/misbehaving-pod   0/1     ErrImagePull   0          2s</pre>

<p>After working with Kubernetes for a while, you’ll automatically recognize common error conditions. <a data-type="xref" href="#common_pod_error_statuses">Table 7-1</a> lists some of those error statuses and explains how to fix them.</p>
<table id="common_pod_error_statuses">
<caption><span class="label">Table 7-1. </span>Common Pod error statuses</caption>
<thead>
<tr>
<th>Status</th>
<th>Root cause</th>
<th>Potential fix</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><code>ImagePullBackOff</code> or 
<span class="keep-together"><code>ErrImagePull</code></span></p></td>
<td><p>Image could not be pulled from registry.</p></td>
<td><p>Check correct image name, check that image name exists in registry, verify network access from node to registry, ensure proper authentication.</p></td>
</tr>
<tr>
<td><p><code>CrashLoopBackOff</code></p></td>
<td><p>Application or command run in container crashes.</p></td>
<td><p>Check command executed in container, ensure that image can properly execute (e.g., by creating a container with Docker).</p></td>
</tr>
<tr>
<td><p><code>CreateContainerConfigError</code></p></td>
<td><p>ConfigMap or Secret referenced by container cannot be found.</p></td>
<td><p>Check correct name of the configuration object, verify the existence of the configuration object in the namespace.</p></td>
</tr>
</tbody>
</table>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Inspecting events"><div class="sect3" id="idm45322716665168">
<h3>Inspecting events</h3>

<p><a data-type="indexterm" data-primary="events, inspecting" id="idm45322716663824"/><a data-type="indexterm" data-primary="kubectl describe pod command" id="idm45322716663120"/><a data-type="indexterm" data-primary="commands" data-secondary="kubectl describe pod" id="idm45322716662384"/>It’s totally possible that you’ll not encounter any of those error statuses. But there’s still a chance of the Pod having a configuration issue. You can retrieve detailed information about the Pod and its events using the <code>kubectl describe pod</code> command to inspect its events. The following output belongs to a Pod that tries to mount a Secret that doesn’t exist. Instead of rendering a specific error message, the Pod gets stuck with the status <code>ContainerCreating</code>:</p>

<pre data-type="programlisting"><strong>$ kubectl get pods</strong>
NAME         READY   STATUS              RESTARTS   AGE
secret-pod   0/1     ContainerCreating   0          4m57s
<strong>$ kubectl describe pod secret-pod</strong>
...
Events:
  Type     Reason       Age                   From               Message
  ----     ------       ----                  ----               -------
  Normal   Scheduled    &lt;unknown&gt;             default-scheduler \
  Successfully assigned default/secret-pod to minikube
  Warning  FailedMount  3m15s                 kubelet, minikube  Unable to \
  attach or mount volumes: unmounted volumes=[mysecret], unattached \
  volumes=[default-token-bf8rh mysecret]: timed out waiting for the condition
  Warning  FailedMount  68s (x10 over 5m18s)  kubelet, minikube  \
  MountVolume.SetUp failed for volume "mysecret" : secret "mysecret" not found
  Warning  FailedMount  61s                   kubelet, minikube  Unable to \
  attach or mount volumes: unmounted volumes=[mysecret], unattached \
  volumes=[mysecret default-token-bf8rh]: timed out waiting for the condition</pre>

<p><a data-type="indexterm" data-primary="commands" data-secondary="kubectl get events" id="idm45322716657344"/><a data-type="indexterm" data-primary="kubectl get events command" id="idm45322716656368"/>Another helpful command is <code>kubectl get events</code>. The output of the command lists the events across all Pods for a given namespace. You can use additional command-line options to further filter and sort events:</p>

<pre data-type="programlisting"><strong>$ kubectl get events</strong>
LAST SEEN   TYPE      REASON             OBJECT                MESSAGE
3m14s       Warning   BackOff            pod/custom-cmd        Back-off \
restarting failed container
2s          Warning   FailedNeedsStart   cronjob/google-ping   Cannot \
determine if job needs to be started: too many missed start time (&gt; 100). \
Set or decrease .spec.startingDeadlineSeconds or check clock skew</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Inspecting logs"><div class="sect3" id="idm45322716653888">
<h3>Inspecting logs</h3>

<p><a data-type="indexterm" data-primary="logs, inspecting" id="idm45322716652512"/>When debugging a Pod, the next level of details can be retrieved by downloading and inspecting its logs. You may or may not find additional information that points to the root cause of a misbehaving Pod. It’s definitely worth a look. The YAML manifest shown in <a data-type="xref" href="#pod_failing_command">Example 7-1</a> defines a Pod running a shell command.</p>
<div id="pod_failing_command" data-type="example">
<h5><span class="label">Example 7-1. </span>A Pod running a failing shell command</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">incorrect-cmd-pod</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">test-container</code><code class="w"/>
<code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">busybox</code><code class="w"/>
<code class="w">    </code><code class="nt">command</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"/bin/sh"</code><code class="p-Indicator">,</code><code class="w"> </code><code class="s">"-c"</code><code class="p-Indicator">,</code><code class="w"> </code><code class="s">"unknown"</code><code class="p-Indicator">]</code><code class="w"/></pre></div>

<p>After creating the object, the Pod fails with the status <code>CrashLoopBackOff</code>. Running the <code>logs</code> command reveals that the command run in the container has an issue:</p>

<pre data-type="programlisting"><strong>$ kubectl create -f crash-loop-backoff.yaml</strong>
pod/incorrect-cmd-pod created
<strong>$ kubectl get pods incorrect-cmd-pod</strong>
NAME                READY   STATUS              RESTARTS   AGE
incorrect-cmd-pod   0/1     CrashLoopBackOff    5          3m20s
<strong>$ kubectl logs incorrect-cmd-pod</strong>
/bin/sh: unknown: not found</pre>

<p><a data-type="indexterm" data-primary="logs command" id="idm45322716609696"/><a data-type="indexterm" data-primary="commands" data-secondary="logs" id="idm45322716609104"/>The <code>logs</code> command provides two helpful options I’d like to mention here. The option <code>-f</code> streams the logs, meaning you’ll see new log entries as they’re being produced in real time. The option <code>--previous</code> gets the logs from the previous instantiation of a container, which is helpful if the container has been restarted.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Opening an Interactive Shell"><div class="sect2" id="idm45322716606400">
<h2>Opening an Interactive Shell</h2>

<p><a data-type="indexterm" data-primary="interactive shells, opening" id="idm45322716604992"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="opening interactive shells" id="idm45322716604320"/>If any of the previous commands do not point you to the root cause of the failing Pod, it’s time to open an interactive shell to a container. As an application developer, you’ll probably know best what behavior to expect from the application at runtime. Ensure that the correct configuration has been created and inspect the running processes by using the Unix or Windows utility tools, depending on the image run in the container.</p>

<p>Say you encounter a situation where a Pod seems to work properly on the surface, as shown in <a data-type="xref" href="#pod_current_date">Example 7-2</a>.</p>
<div id="pod_current_date" data-type="example">
<h5><span class="label">Example 7-2. </span>A Pod periodically writing the current date to a file</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">failing-pod</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">args</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">/bin/sh</code><code class="w"/>
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">-c</code><code class="w"/>
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">while true; do echo $(date) &gt;&gt; ~/tmp/curr-date.txt; sleep \</code><code class="w"/>
<code class="w">      </code><code class="l-Scalar-Plain">5; done;</code><code class="w"/>
<code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">busybox</code><code class="w"/>
<code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">failing-pod</code><code class="w"/></pre></div>

<p>After creating the Pod, you check the status. It says <code>Running</code>; however, when making a request to the application, the endpoint reports an error. Next, you check the logs. The log output renders an error message that points to a nonexistent directory. Apparently, the directory hasn’t been set up correctly but is needed by the application:</p>

<pre data-type="programlisting"><strong>$ kubectl create -f failing-pod.yaml</strong>
pod/failing-pod created
<strong>$ kubectl get pods failing-pod</strong>
NAME          READY   STATUS    RESTARTS   AGE
failing-pod   1/1     Running   0          5s
<strong>$ kubectl logs failing-pod</strong>
/bin/sh: can't create /root/tmp/curr-date.txt: nonexistent directory</pre>

<p>The <code>exec</code> command opens an interactive shell to further investigate the issue. Below, we’re using the Unix tools <code>mkdir</code>, <code>cd</code>, and <code>ls</code> inside of the running container to fix the problem. Obviously, the better mitigation strategy is to create the directory from the application or provide an instruction in the Dockerfile:</p>
<pre data-type="programlisting">
<strong>$ kubectl exec failing-pod -it -- /bin/sh</strong>
# mkdir -p ~/tmp
# cd ~/tmp
# ls -l
total 4
-rw-r--r-- 1 root root 112 May  9 23:52 curr-date.txt
</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Troubleshooting Services"><div class="sect2" id="idm45322716475280">
<h2>Troubleshooting Services</h2>

<p><a data-type="indexterm" data-primary="services" data-secondary="troubleshooting" id="idm45322716473744"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="services" id="idm45322716472768"/>A Service provides a unified network interface for Pods. For full coverage on networking aspects in Kubernetes, see <a data-type="xref" href="ch05.xhtml#services_networking">Chapter 5</a>. Here, I want to point out troubleshooting techniques for this primitive.</p>

<p>In case you can’t reach the Pods that should map to the Service, start by ensuring that the label selector matches with the assigned labels of the Pods. You can query the information by describing the Service and then render the labels of the available Pods with the option <code>--show-labels</code>. The following example does not have matching labels and therefore wouldn’t apply to any of the Pods running in the namespace:</p>

<pre data-type="programlisting"><strong>$ kubectl describe service myservice</strong>
...
Selector:          app=myapp
...
<strong>$ kubectl get pods --show-labels</strong>
NAME                     READY   STATUS    RESTARTS   AGE     LABELS
myapp-68bf896d89-qfhlv   1/1     Running   0          7m39s   app=hello
myapp-68bf896d89-tzt55   1/1     Running   0          7m37s   app=world</pre>

<p>Alternatively, you can also query the endpoints of the Service instance. Say you expected three Pods to be selected by a matching label but only two have been exposed by the Service. You’ll want to look at the label selection criteria:</p>

<pre data-type="programlisting"><strong>$ kubectl get endpoints myservice</strong>
NAME        ENDPOINTS                     AGE
myservice   172.17.0.5:80,172.17.0.6:80   9m31s</pre>

<p>A common source of confusion is the type of a Service. By default, the Service type is <code>ClusterIP</code>, which means that a Pod can be reached through the Service only if queried from the same node inside of the cluster. First, check the Service type. If you think that <code>ClusterIP</code> is the proper type you wanted to assign, open an interactive shell from a temporary Pod inside the cluster and run a <code>curl</code> or <code>wget</code> command:</p>

<pre data-type="programlisting"><strong>$ kubectl get services</strong>
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
myservice    ClusterIP   10.99.155.165   &lt;none&gt;        80/TCP    15m
<strong>$ kubectl run tmp --image=busybox -it --rm -- wget -O- 10.99.155.165:80</strong>
...</pre>

<p><a data-type="indexterm" data-primary="" data-startref="app_tro" id="idm45322716441408"/><a data-type="indexterm" data-primary="" data-startref="tro_app" id="idm45322716440560"/>Finally, check if the port mapping from the target port of the Service to the container port of the Pod is configured correctly. Both ports need to match or the network traffic wouldn’t be routed properly:</p>

<pre data-type="programlisting"><strong>$ kubectl get service myapp -o yaml | grep targetPort:</strong>
    targetPort: 80
<strong>$ kubectl get pods myapp-68bf896d89-qfhlv -o yaml | grep containerPort:</strong>
    - containerPort: 80</pre>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Troubleshooting Cluster Failures"><div class="sect1" id="idm45322716437792">
<h1>Troubleshooting Cluster Failures</h1>

<p><a data-type="indexterm" data-primary="troubleshooting" data-secondary="cluster failures" id="tro_cf"/><a data-type="indexterm" data-primary="clusters" data-secondary="troubleshooting failure of" id="clu_tro"/>There are many influencing factors that can render a Kubernetes cluster faulty on the component level. It’s a good idea to list the nodes available in the cluster to identify potential issues:</p>

<pre data-type="programlisting"><strong>$ kubectl get nodes</strong>
NAME           STATUS   ROLES                  AGE     VERSION
minikube       Ready    control-plane,master   3m18s   v1.22.3
minikube-m02   Ready    &lt;none&gt;                 2m51s   v1.22.3
minikube-m03   Ready    &lt;none&gt;                 2m24s   v1.22.3</pre>

<p>The output will give you a lay of the land. You can easily identify the responsibility of each node from the <code>ROLES</code> column, the Kubernetes version used, and the current health status.</p>

<p>There are a couple of things to look out for when identifying issues at a high level:</p>

<ul>
<li>
<p>Is the health status for the node anything other than “Ready”?</p>
</li>
<li>
<p>Does the version of a node deviate from the version of other nodes?</p>
</li>
</ul>

<p>In the following sections you can find individual sections on troubleshooting control plane nodes versus worker nodes.</p>








<section data-type="sect2" data-pdf-bookmark="Troubleshooting Control Plane Nodes"><div class="sect2" id="idm45322716428416">
<h2>Troubleshooting Control Plane Nodes</h2>

<p><a data-type="indexterm" data-primary="control plane nodes" data-secondary="troubleshooting" id="idm45322716427248"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="control plane nodes" id="idm45322716426272"/>Control plane nodes are the critical components for keeping a cluster operational. As described in <a data-type="xref" href="ch02.xhtml#managing_ha_cluster">“Managing a Highly Available Cluster”</a>, a cluster can consist of more than one control plane node to ensure a high degree of uptime. Detecting that one of the control plane nodes is faulty should be treated with extreme urgency to avoid compromising high-availability characteristics. For more information on troubleshooting techniques and root-cause analysis, reference the <a href="https://oreil.ly/KWeTt">Kubernetes 
<span class="keep-together">documentation</span></a>.</p>










<section data-type="sect3" data-pdf-bookmark="Rendering cluster information"><div class="sect3" id="idm45322716422912">
<h3>Rendering cluster information</h3>

<p><a data-type="indexterm" data-primary="clusters" data-secondary="rendering information for" id="idm45322716421520"/><a data-type="indexterm" data-primary="kubectl cluster-info command" id="idm45322716420480"/><a data-type="indexterm" data-primary="commands" data-secondary="kubectl cluster-info" id="idm45322716419792"/>To further diagnose issues on the control plane node, run the command <code>kubectl cluster-info</code>. As you can see in the following output, the command renders the addresses of the control plane and other cluster services:</p>

<pre data-type="programlisting"><strong>$ kubectl cluster-info</strong>
Kubernetes control plane is running at https://192.168.64.21:8443
CoreDNS is running at https://192.168.64.21:8443/api/v1/namespaces/ \
kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use <em>kubectl cluster-info dump</em>.</pre>

<p><a data-type="indexterm" data-primary="dump command" id="idm45322716416272"/><a data-type="indexterm" data-primary="commands" data-secondary="dump" id="idm45322716415568"/>For a detailed view of the cluster logs, append the <code>dump</code> subcommand. Due to the pages and pages of log messages, we won’t render the output in this book. Parse through the message to see if you can find any errors:</p>

<pre data-type="programlisting"><strong>$ kubectl cluster-info dump</strong></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Inspecting control plane components"><div class="sect3" id="idm45322716412960">
<h3>Inspecting control plane components</h3>

<p>Among those <a href="https://oreil.ly/IZR8Z">components available on the control plane node</a> are the following:<a data-type="indexterm" data-primary="kube-apiserver" id="idm45322716410688"/><a data-type="indexterm" data-primary="etcd" data-secondary="about" id="idm45322716409984"/><a data-type="indexterm" data-primary="kube-scheduler" id="idm45322716409040"/><a data-type="indexterm" data-primary="kube-controller-manager" id="idm45322716408368"/><a data-type="indexterm" data-primary="cloud-controller-manager" id="idm45322716407696"/></p>

<ul>
<li>
<p>kube-apiserver: Exposes the Kubernetes API used by clients like <code>kubectl</code> for managing objects.</p>
</li>
<li>
<p>etcd: A key-value store for storing the cluster data.</p>
</li>
<li>
<p>kube-scheduler: Selects nodes for Pods that have been scheduled but not created.</p>
</li>
<li>
<p>kube-controller-manager: Runs controller processes (e.g., the job controller responsible for Job object execution).</p>
</li>
<li>
<p>cloud-controller-manager: Links cloud provider–specific API to the Kubernetes cluster. This controller is not available in on-premise cluster installations of Kubernetes.</p>
</li>
</ul>

<p>To discover those components and their status, list the Pods available in the namespace <code>kube-system</code>. Here, you can find the list of control-plane components on Minikube:</p>

<pre data-type="programlisting"><strong>$ kubectl get pods -n kube-system</strong>
NAME                               READY   STATUS    RESTARTS      AGE
etcd-minikube                      1/1     Running   1 (11d ago)   29d
kube-apiserver-minikube            1/1     Running   1 (11d ago)   29d
kube-controller-manager-minikube   1/1     Running   1 (11d ago)   29d
kube-scheduler-minikube            1/1     Running   1 (11d ago)   29d
...</pre>

<p>Any status that does not show “Running” should be inspected further. You can retrieve the logs for control-plane component Pods in the same fashion you do for any other Pod, using the <code>logs</code> command. The following command downloads the logs for the kube-apiserver component:</p>

<pre data-type="programlisting"><strong>$ kubectl logs kube-apiserver-minikube -n kube-system</strong></pre>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Troubleshooting Worker Nodes"><div class="sect2" id="idm45322716397232">
<h2>Troubleshooting Worker Nodes</h2>

<p><a data-type="indexterm" data-primary="troubleshooting" data-secondary="worker nodes" id="tro_wn"/><a data-type="indexterm" data-primary="worker nodes" data-secondary="troubleshooting" id="wn_tro"/>Worker nodes are responsible for managing the workload. Make sure you have a sufficient number of worker nodes available to distribute the load. For a deeper discussion on how to join worker nodes to a cluster, see <a data-type="xref" href="ch02.xhtml#cluster_architecture_installation_configuration">Chapter 2</a>.</p>

<p><a data-type="indexterm" data-primary="commands" data-secondary="get nodes" id="idm45322716391760"/><a data-type="indexterm" data-primary="get nodes command" id="idm45322716390784"/>Any of the nodes available in a cluster can transition into an error state. It’s your job as a Kubernetes administrator to identify those situations and fix them in a timely manner. When listing the nodes of a cluster, you may see that a worker node is not in the “Ready” state, which is a good indicator that it’s not available to handle the workload. In the output of the <code>get nodes</code> command, you can see that the node named <code>worker-1</code> is in the “NotReady” state:</p>

<pre data-type="programlisting"><strong>$ kubectl get nodes</strong>
NAME                STATUS     ROLES                  AGE     VERSION
k8s-control-plane   Ready      control-plane,master   4d20h   v1.22.3
worker-1            NotReady   &lt;none&gt;                 4d20h   v1.22.3
worker-2            Ready      &lt;none&gt;                 4d20h   v1.22.3</pre>

<p>The “NotReady” state means that the node is unused and will accumulate operational costs without actually scheduling workload. There might be a variety of reasons why the node entered this state. The following list shows the most common reasons:<a data-type="indexterm" data-primary="kube-proxy" id="idm45322716387216"/></p>

<ul>
<li>
<p>Insufficient resources: The node may be low on memory or disk space.</p>
</li>
<li>
<p>Issues with the kubelet process: The process may have crashed or stopped on the node. Therefore, it cannot communicate with the API server running on any of the control plane nodes anymore.</p>
</li>
<li>
<p>Issues with kube-proxy: The Pod running kube-proxy is responsible for network communication from within the cluster and from the outside. The Pod transitioned into a nonfunctional state.</p>
</li>
</ul>

<p>SSH into the relevant worker node(s) and start your investigation.</p>










<section data-type="sect3" data-pdf-bookmark="Checking available resources"><div class="sect3" id="idm45322716382880">
<h3>Checking available resources</h3>

<p>A good way to identify the root cause of an unavailable worker node is to look at its details. The <code>describe node</code> command renders the section labeled “Conditions”:</p>

<pre data-type="programlisting"><strong>$ kubectl describe node worker-1</strong>
....
Conditions:
  Type                 Status  LastHeartbeatTime               \
    LastTransitionTime                Reason                       Message
  ----                 ------  -----------------               \
    ------------------                ------                       -------
  NetworkUnavailable   False   Thu, 20 Jan 2022 18:12:13 +0000 \
    Thu, 20 Jan 2022 18:12:13 +0000   CalicoIsUp               \
        Calico is running on this node
  MemoryPressure       False   Tue, 25 Jan 2022 15:59:18 +0000 \
    Thu, 20 Jan 2022 18:11:47 +0000   KubeletHasSufficientMemory \
      kubelet has sufficient memory available
  DiskPressure         False   Tue, 25 Jan 2022 15:59:18 +0000 \
    Thu, 20 Jan 2022 18:11:47 +0000   KubeletHasNoDiskPressure \
        kubelet has no disk pressure
  PIDPressure          False   Tue, 25 Jan 2022 15:59:18 +0000 \
    Thu, 20 Jan 2022 18:11:47 +0000   KubeletHasSufficientPID  \
        kubelet has sufficient PID available
  Ready                True    Tue, 25 Jan 2022 15:59:18 +0000 \
    Thu, 20 Jan 2022 18:12:07 +0000   KubeletReady             \
        kubelet is posting ready status. AppArmor enabled
...</pre>

<p>The table contains information about the resources available to the node, as well as an indication of other services like networking. See if any of the resource types render the status <code>True</code> or <code>Unknown</code>, which means that there’s an issue with the particular resource. You can further troubleshoot unavailable resources with a system-level command.</p>

<p><a data-type="indexterm" data-primary="top command" id="idm45322716376816"/><a data-type="indexterm" data-primary="commands" data-secondary="top" id="idm45322716376112"/>To check on memory and the number of processes running, use the <code>top</code> command:</p>

<pre data-type="programlisting"><strong>$ top</strong>
top - 18:45:09 up 1 day,  2:21,  1 user,  load average: 0.13, 0.13, 0.15
Tasks: 116 total,   3 running,  70 sleeping,   0 stopped,   0 zombie
%Cpu(s):  1.5 us,  0.8 sy,  0.0 ni, 97.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  1008552 total,   134660 free,   264604 used,   609288 buff/cache
KiB Swap:        0 total,        0 free,        0 used.   611248 avail Mem
...</pre>

<p><a data-type="indexterm" data-primary="df command" id="idm45322716373280"/><a data-type="indexterm" data-primary="commands" data-secondary="df" id="idm45322716372576"/>To check on the available disk space, use the command <code>df</code>:</p>

<pre data-type="programlisting"><strong>$ df -h</strong>
Filesystem      Size  Used Avail Use% Mounted on
udev            480M     0  480M   0% /dev
tmpfs            99M  1.0M   98M   2% /run
/dev/sda1        39G  2.7G   37G   7% /
tmpfs           493M     0  493M   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           493M     0  493M   0% /sys/fs/cgroup
vagrant         1.9T  252G  1.6T  14% /vagrant
tmpfs            99M     0   99M   0% /run/user/1000</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Checking the kubelet process"><div class="sect3" id="idm45322716369920">
<h3>Checking the kubelet process</h3>

<p><a data-type="indexterm" data-primary="kubelet process, checking" id="idm45322716368576"/><a data-type="indexterm" data-primary="describe node command" id="idm45322716367632"/><a data-type="indexterm" data-primary="commands" data-secondary="describe node" id="idm45322716366960"/><a data-type="indexterm" data-primary="systemctl command" id="idm45322716366016"/><a data-type="indexterm" data-primary="commands" data-secondary="systemctl" id="idm45322716365344"/>Some conditions rendered by the <code>describe node</code> command mention the kubelet process. If you look at the <code>Message</code> column, you might get an idea if the kubelet process is running properly. To troubleshoot a misbehaving kubelet process, run the following <code>systemctl</code> command:</p>

<pre data-type="programlisting"><strong>$ systemctl status kubelet</strong>
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; \
   vendor preset: enabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Thu 2022-01-20 18:11:41 UTC; 5 days ago
     Docs: https://kubernetes.io/docs/home/
 Main PID: 6537 (kubelet)
    Tasks: 15 (limit: 1151)
   CGroup: /system.slice/kubelet.service
           └─6537 /usr/bin/kubelet \
           --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \
           --kubeconfig=/etc/kubernetes/kubelet.conf \
           --config=/var/lib/kubelet/config.yaml --network-lines 1-10/10</pre>

<p>The most important information in the output is the value of the <code>Active</code> attribute. If it says something other than “active (running),” then you will need to dig deeper. Use <code>journalctl</code> to take a look at the log files of the process:</p>

<pre data-type="programlisting"><strong>$ journalctl -u kubelet.service</strong>
-- Logs begin at Thu 2022-01-20 18:10:41 UTC, end at
Tue 2022-01-25 18:44:05 UTC. --
Jan 20 18:11:31 worker-1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Jan 20 18:11:31 worker-1 systemd[1]: kubelet.service: Current command vanished \
from the unit file, execution of the command list won't be resumed.
Jan 20 18:11:31 worker-1 systemd[1]: Stopping kubelet: The Kubernetes
Node Agent...
Jan 20 18:11:31 worker-1 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Jan 20 18:11:31 worker-1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
....</pre>

<p>You will want to restart the process once you have identified the issue in the logs and fixed it:</p>

<pre data-type="programlisting"><strong>$ systemctl restart kubelet</strong></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Checking the certificate"><div class="sect3" id="idm45322716357904">
<h3>Checking the certificate</h3>

<p><a data-type="indexterm" data-primary="certificates, checking" id="idm45322716356752"/>Sometimes, the certificate used by the kubelet can expire. Make sure that the values for the attributes <code>Issuer</code> and <code>Not After</code> are correct:</p>

<pre data-type="programlisting"><strong>$ openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text</strong>
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 2 (0x2)
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN = worker-1-ca@1642702301
        Validity
            Not Before: Jan 20 17:11:41 2022 GMT
            Not After : Jan 20 17:11:41 2023 GMT
        Subject: CN = worker-1@1642702301
        ...</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Checking the kube-proxy Pod"><div class="sect3" id="idm45322716353696">
<h3>Checking the kube-proxy Pod</h3>

<p><a data-type="indexterm" data-primary="kube-proxy" id="idm45322716352256"/><a data-type="indexterm" data-primary="Pods" data-secondary="kube-proxy" id="idm45322716351552"/>The kube-proxy components run in a set of dedicated Pods in the namespace <code>kube-system</code>. You can clearly identify the Pods by their naming prefix <code>kube-proxy</code> and the appended hash. Verify if any of the Pods states a different status than “Running.” Each of the kube-proxy Pods runs on a dedicated worker node. You can add the <code>-o wide</code> option to render the node the Pod is running on in a new column:</p>

<pre data-type="programlisting" class="less_space pagebreak-before"><strong>$ kubectl get pods -n kube-system</strong>
NAME               READY   STATUS    RESTARTS   AGE
...
kube-proxy-csrww   1/1     Running   0          4d22h
kube-proxy-fjd48   1/1     Running   0          4d22h
kube-proxy-tvf52   1/1     Running   0          4d22h</pre>

<p>Take a look at the event log for kube-proxy Pods that seem to have an issue. The following command describes the Pod named <code>kube-proxy-csrww</code>. In addition, you might find more information in the event log of the corresponding DaemonSet:</p>

<pre data-type="programlisting"><strong>$ kubectl describe pod kube-proxy-csrww -n kube-system
$ kubectl describe daemonset kube-proxy -n kube-system</strong></pre>

<p><a data-type="indexterm" data-primary="" data-startref="tro_cf" id="idm45322716345072"/><a data-type="indexterm" data-primary="" data-startref="clu_tro" id="idm45322716344096"/><a data-type="indexterm" data-primary="" data-startref="tro_wn" id="idm45322716343152"/><a data-type="indexterm" data-primary="" data-startref="wn_tro" id="idm45322716342208"/>The logs may come in handy as well. You will be able to check the logs only for the kube-proxy Pod that runs on the specific worker node:</p>

<pre data-type="programlisting"><strong>$ kubectl describe pod kube-proxy-csrww -n kube-system | grep Node:</strong>
Node:                 worker-1/10.0.2.15
<strong>$ kubectl logs kube-proxy-csrww -n kube-system</strong></pre>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45322716396640">
<h1>Summary</h1>

<p><a data-type="indexterm" data-primary="troubleshooting" data-secondary="about" id="idm45322716338320"/>As a Kubernetes administrator, you need to be capable of identifying and fixing application and cluster component issues.</p>

<p>Logging is essential for tracing application flows and capturing potential error messages. You can configure logging on a cluster level and a node level, each of which comes with its own benefits and potential drawbacks. Depending on the log capturing approach, aspects such as log rotation and logging backend service may be incorporated. On the Pod level, you can directly ask for the application logs using the <code>kubectl logs</code> command. Use the command-line option <code>-c</code> to target a specific container in a multi-container setup.</p>

<p>Kubernetes’ native monitoring service, the metrics server, can be installed on the cluster to collect and aggregate Pod and node resource utilization data. Nodes send metrics via the kubelet to the centralized metrics server. End users can use the <code>kubectl top</code> command to render those metrics as a means to identify excessive resource usage.</p>

<p>Misconfiguration of workload and networking objects can lead to applications mishaps. You need to be familiar with the strategies relevant for diagnosing root causes and how fix them. A cluster may also develop error states leading to a variety of operational issues. You need to know which cluster components and processes run on control plane nodes and worker nodes. We discussed strategies for tackling different failure situations.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exam Essentials"><div class="sect1" id="idm45322716334096">
<h1>Exam Essentials</h1>
<dl>
<dt><a data-type="indexterm" data-primary="troubleshooting" data-secondary="exam essentials" id="idm45322716332128"/><a data-type="indexterm" data-primary="exam essentials" data-secondary="troubleshooting" id="idm45322716331120"/>Understand logging configuration on a theoretical level</dt>
<dd>
<p>Kubernetes logging is not a built-in capability. You have to configure logging proactively on the cluster or the node level. Compare the different approaches and their potential trade-offs.</p>
</dd>
<dt>Make accessing container logs your daily bread and butter</dt>
<dd>
<p>Accessing container logs is straightforward. Simply use the <code>logs</code> command. Practice the use of all relevant command-line options. The option <code>-c</code> targets a specific container. The option does not have to be used explicitly for single-container Pods. The option <code>-f</code> tails the log entries if you want to see live processing in an application. The <code>-p</code> option can be used for accessing logs if the container needed to be restarted, but you still want to take a look at the previous container logs.</p>
</dd>
<dt>Install and use the metrics server</dt>
<dd>
<p>The metrics server isn’t installed on a Kubernetes cluster by default. Go through the motions of installing the service. For the exam, you can assume that the metrics server is already available. Use the <code>top</code> command for Pods and nodes to identify resource consumption.</p>
</dd>
<dt>Know how to troubleshoot applications</dt>
<dd>
<p>Applications running in a Pod can easily break due to misconfiguration. Think of possible scenarios that can occur and try to model them proactively to represent a failure situation. Then using the commands <code>get</code>, <code>logs</code>, and <code>exec</code>, get to the bottom of the issue and fix it. Try to dream up obscure scenarios to become more comfortable with finding and fixing application issues for different resource types.</p>
</dd>
<dt>Know how to troubleshoot clusters</dt>
<dd>
<p>Control-plane and worker nodes can become unresponsive or dysfunctional for a variety of reasons. Administrators need to take care of their cluster’s health in order to keep it operational and scalable. Try to emulate error scenarios that may occur and apply the discussed troubleshooting techniques to identify and fix the underlying root cause.</p>
</dd>
</dl>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Sample Exercises"><div class="sect1" id="idm45322716318960">
<h1>Sample Exercises</h1>

<p><a data-type="indexterm" data-primary="sample exercises" data-secondary="troubleshooting" id="idm45322716317824"/><a data-type="indexterm" data-primary="troubleshooting" data-secondary="sample exercises" id="idm45322716316848"/><a data-type="indexterm" data-primary="" data-startref="tro_ch" id="idm45322716315904"/>Solutions to these exercises are available in the <a data-type="xref" href="app01.xhtml#appendix-a">Appendix</a>.</p>
<ol>
<li>
<p>You are supposed to implement cluster-level logging with a sidecar container. Create a multi-container Pod named <code>multi</code>. The main application container named <code>nginx</code> should use the image <code>nginx:1.21.6</code>. The sidecar container named <code>streaming</code> uses the image <code>busybox:1.35.0</code> and the arguments <code>/bin/sh</code>, <code>-c</code>, and <code>'tail -n+1 -f /var/log/nginx/access.log'</code>.</p>
</li>
<li>
<p>Define a volume of type <code>emptyDir</code> for the Pod and mount it to the path <code>/var/log/nginx</code> for both containers.</p>
</li>
<li>
<p>Access the endpoint of the nginx service a couple of times using a <code>wget</code> or <code>curl</code> command. Inspect the logs of the sidecar container.</p>
</li>
<li>
<p>Create two Pods named <code>stress-1</code> and <code>stress-2</code>. Define a container that uses the image <code>polinux/stress:1.0.4</code> with the command <code>stress</code> and the arguments <code>/bin/sh</code>, <code>-c</code>, and <code>'stress --vm 1 --vm-bytes $(shuf -i 20-200 -n 1)M --vm-hang 1'</code>. Set the container memory resource limits and requests to 250Mi.</p>
</li>
<li>
<p>Use the data available through the metrics server to identify which of the Pods, <code>stress-1</code> or <code>stress-2</code>, consumes the most memory. Write the name of the Pod to the file <code>max-memory.txt</code>.</p>
</li>
<li>
<p>Navigate to the directory <code>app-a/ch07/troubleshooting-pod</code> of the checked-out GitHub repository <a href="https://oreil.ly/jUIq8"><em>bmuschko/cka-study-guide</em></a>. Follow the instructions in the file <a href="https://oreil.ly/j9lTr"><code>instructions.md</code></a> for troubleshooting a faulty Pod setup.</p>
</li>
<li>
<p>Navigate to the directory <code>app-a/ch07/troubleshooting-deployment</code> of the checked-out GitHub repository <a href="https://oreil.ly/jUIq8"><em>bmuschko/cka-study-guide</em></a>. Follow the instructions in the file <code>instructions.md</code> for troubleshooting a faulty Deployment setup.</p>
</li>
<li>
<p>Navigate to the directory <code>app-a/ch07/troubleshooting-service</code> of the checked-out GitHub repository <a href="https://oreil.ly/jUIq8"><em>bmuschko/cka-study-guide</em></a>. Follow the instructions in the file <a href="https://oreil.ly/Z9kou"><code>instructions.md</code></a> for troubleshooting a faulty Service setup.</p>
</li>
<li>
<p>Navigate to the directory <code>app-a/ch07/troubleshooting-control-plane-node</code> of the checked-out GitHub repository <a href="https://oreil.ly/jUIq8"><em>bmuschko/cka-study-guide</em></a>. Follow the instructions in the file <a href="https://oreil.ly/lGygu"><code>instructions.md</code></a> for troubleshooting a faulty control plane node setup.</p>

<p><em>Prerequisite:</em> This exercise requires the installation of the tools <a href="https://oreil.ly/sasln">Vagrant</a> and <a href="https://oreil.ly/9Cvg9">
<span class="keep-together">VirtualBox</span></a>.</p>
</li>
<li>
<p>Navigate to the directory <code>app-a/ch07/troubleshooting-worker-node</code> of the checked-out GitHub repository <a href="https://oreil.ly/jUIq8"><em>bmuschko/cka-study-guide</em></a>. Follow the instructions in the file <a href="https://oreil.ly/Kyh58"><code>instructions.md</code></a> for troubleshooting a faulty worker node setup.</p>

<p><em>Prerequisite:</em> This exercise requires the installation of the tools <a href="https://oreil.ly/sasln">Vagrant</a> and <a href="https://oreil.ly/9Cvg9">
<span class="keep-together">VirtualBox</span></a>.</p>
</li>

</ol>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45322716313216">
<h5>Interactive Exam Practice</h5>
<p>Get more hands-on training and test your CKA exam readiness by working through our interactive CKA labs. Each step of the lab must be completed correctly before you can move to the next step. If you get stuck, you can view the solution and learn how to complete the step.</p>

<p>The following labs cover material from this chapter:</p>

<ul>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492099178">Setting Up Cluster-Level Logging with a Sidecar</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492099185">Using the Metrics Server</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492099192">Troubleshooting a Deployment</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492099208">Troubleshooting a Service</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492099215">Troubleshooting a Control Plane Node</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492099222">Troubleshooting a Worker Node</a></p>
</li>
<li>
<p><a href="https://learning.oreilly.com/scenarios/-/9781492099239">Troubleshooting Networking</a></p>
</li>
</ul>
</div></aside>
</div></section>







</div></section></div></body></html>