- en: Chapter 8\. eBPF for Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you saw in [Chapter 1](ch01.html#what_is_ebpf_and_why_is_it_importantque),
    the dynamic nature of eBPF allows us to customize the behavior of the kernel.
    In the world of networking, there is a huge range of desirable behavior that depends
    on the application. For example, a telecommunications operator might have to interface
    with telco-specific protocols like SRv6; a Kubernetes environment might need to
    be integrated with legacy applications; dedicated hardware load balancers can
    be replaced with XDP programs running on commodity hardware. eBPF allows programmers
    to build networking features to meet specific needs, without having to force them
    on all upstream kernel users.
  prefs: []
  type: TYPE_NORMAL
- en: Network tools based on eBPF are now widely used and have proven to be effective
    at prolific scale. The CNCF’s [Cilium project](http://cilium.io), for example,
    uses eBPF as a platform for Kubernetes networking, standalone load balancing,
    and much more, and it’s used by cloud native adopters in every conceivable industry
    vertical.^([1](ch08.html#ch08fn1)) Meta has been using eBPF at a vast scale—every
    packet to and from Facebook since 2017 has been through an XDP program. Another
    public and hyper-scaled example is Cloudflare’s use of eBPF for DDoS (distributed
    denial-of-service) protection.
  prefs: []
  type: TYPE_NORMAL
- en: These are complex, production-ready solutions, and their details are far beyond
    the scope of this book, but by reading the examples in this chapter you can get
    a feel for how eBPF networking solutions like these are built.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The code examples for this chapter are in the *chapter8* directory of the repository
    at [*github.com/lizrice/learning-ebpf*](https://github.com/lizrice/learning-ebpf).
  prefs: []
  type: TYPE_NORMAL
- en: Packet Drops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several network security features that involve dropping certain incoming
    packets and allowing others. These features include firewalling, DDoS protection,
    and mitigating packet-of-death vulnerabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Firewalling involves deciding on a per-packet basis whether to allow a packet,
    based on the source and destination IP addresses and/or port numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DDoS protection adds some complexity, perhaps keeping track of the rate at which
    packets are arriving from a particular source and/or detecting certain characteristics
    of the packet contents to determine that an attacker or set of attackers is trying
    to flood the interface with traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A packet-of-death vulnerability is a class of kernel vulnerability in which
    the kernel fails to safely process a packet crafted in a particular way. An attacker
    who sends packets with this particular format can exploit the vulnerability, which
    could potentially cause the kernel to crash. Traditionally, when a kernel vulnerability
    like this is found, it requires installing a new kernel with the fix, which in
    turn requires machine downtime. But an eBPF program that detects and drops these
    malicious packets can be installed dynamically, instantly protecting that host
    without affecting any applications running on the machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision-making algorithms for features like these are beyond the scope
    of this book, but let’s explore how eBPF programs attached to the XDP hook on
    a network interface drop certain packets, which is the basis for implementing
    these use cases.
  prefs: []
  type: TYPE_NORMAL
- en: XDP Program Return Codes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An XDP program is triggered by the arrival of a network packet. The program
    examines the packet, and when it’s done, the return code gives a *verdict* that
    indicates what to do next with that packet:'
  prefs: []
  type: TYPE_NORMAL
- en: '`XDP_PASS` indicates that the packet should be sent to the network stack in
    the normal way (as it would have done if there were no XDP program).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XDP_DROP` causes the packet to be discarded immediately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XDP_TX` sends the packet back out of the same interface it arrived on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XDP_REDIRECT` is used to send it to a different network interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XDP_ABORTED` results in the packet being dropped, but its use implies an error
    case or something unexpected, rather than a “normal” decision to discard a packet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For some use cases (like firewalling), the XDP program simply has to decide
    between passing the packet on or dropping it. An outline for an XDP program that
    decides whether to drop packets looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: An XDP program can also manipulate the packet contents, but I’ll come to that
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: XDP programs get triggered whenever an inbound network packet arrives on the
    interface to which it is attached. The `ctx` parameter is a pointer to an `xdp_md`
    structure, which holds metadata about the incoming packet. Let’s see how you can
    use this structure to examine the packet’s contents in order to reach a verdict.
  prefs: []
  type: TYPE_NORMAL
- en: XDP Packet Parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s the definition of the `xdp_md` structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Don’t be fooled by the `__u32` type for the first three fields, as they are
    really pointers. The `data` field indicates the location in memory where the packet
    starts, and `data_end` shows where it ends. As you saw in [Chapter 6](ch06.html#the_ebpf_verifier),
    to pass the eBPF verifier you will have to explicitly check that any reads or
    writes to the packet’s contents are within the range `data` to `data_end`.
  prefs: []
  type: TYPE_NORMAL
- en: There is also an area in memory ahead of the packet, between `data_meta` and
    `data`, for storing metadata about this packet. This can be used for coordination
    between multiple eBPF programs that might process the same packet at various places
    on its journey through the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the basics of parsing a network packet, there is an XDP program
    called `ping()` in the example code, which will simply generate a line of trace
    whenever it detects a ping (ICMP) packet. Here’s the code for that program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see this program in action by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `make` in the *chapter8* directory. This doesn’t just build the code; it
    also attaches the XDP program to the loopback interface (called `lo`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `ping localhost` in one terminal window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In another terminal window, watch the output generated in the trace pipe by
    running `cat /sys/kernel/tracing/trace_pipe`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should see two lines of trace being generated approximately every second,
    and they should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There are two lines of trace per second because the loopback interface is receiving
    both the ping requests and the ping responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily modify this code to drop ping packets by adding a line of code
    to return `XDP_DROP` when the protocol matches, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you try this, you’ll see that output resembling the following is only generated
    in the trace output once per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The loopback interface receives a ping request, and the XDP program drops it,
    so the request never gets far enough through the network stack to elicit a response.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the work in this XDP program is being done in a function called `lookup_protocol()`
    that determines the Layer 4 protocol type. It’s just an example, not a production-quality
    implementation of parsing a network packet! But it’s sufficient to give you an
    idea of how parsing in eBPF works.
  prefs: []
  type: TYPE_NORMAL
- en: The network packet that has been received consists of a string of bytes that
    are laid out as shown in [Figure 8-1](#layout_of_an_ip_network_packetcomma_sta).
  prefs: []
  type: TYPE_NORMAL
- en: '![Layout of an IP network packet, starting with an Ethernet header, followed
    by an IP header, and then the Layer 4 data](assets/lebp_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Layout of an IP network packet, starting with an Ethernet header,
    followed by an IP header, and then the Layer 4 data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `lookup_protocol()` function takes the `ctx` structure that holds information
    about where this network packet is in memory and returns the protocol type that
    it finds in the IP header. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#code_id_8_1)'
  prefs: []
  type: TYPE_NORMAL
- en: The local variables `data` and `data_end` point to the start and end of the
    network packet.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#code_id_8_2)'
  prefs: []
  type: TYPE_NORMAL
- en: The network packet should start with an Ethernet header.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#code_id_8_3)'
  prefs: []
  type: TYPE_NORMAL
- en: But you can’t simply assume this network packet is big enough to hold that Ethernet
    header! The verifier requires that you check this explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#code_id_8_4)'
  prefs: []
  type: TYPE_NORMAL
- en: The Ethernet header contains a 2-byte field that tells us the Layer 3 protocol.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#code_id_8_5)'
  prefs: []
  type: TYPE_NORMAL
- en: If the protocol type indicates that it’s an IP packet, the IP header immediately
    follows the Ethernet header.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#code_id_8_6)'
  prefs: []
  type: TYPE_NORMAL
- en: You can’t just assume there’s enough room for that IP header in the network
    packet. Again the verifier requires that you check explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#code_id_8_7)'
  prefs: []
  type: TYPE_NORMAL
- en: The IP header contains the protocol byte the function will return to its caller.
  prefs: []
  type: TYPE_NORMAL
- en: The `bpf_ntohs()` function used by this program ensures that the two bytes are
    in the order expected on this host. Network protocols are big-endian, but most
    processors are little-endian, meaning they hold multibyte values in a different
    order. This function converts (if necessary) from network ordering to host ordering.
    You should use this function whenever you extract a value from a field in a network
    packet that’s more than one byte long.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simple example here shows how just a few lines of eBPF code can have a
    dramatic impact on networking functionality. It’s not hard to imagine how more
    complex rules about which packets to pass and which packets to drop could result
    in the features I described at the start of this section: firewalling, DDoS protection,
    and packet-of-death vulnerability mitigation. Now let’s consider how even more
    functionality can be provided given the power to modify network packets within
    eBPF programs.'
  prefs: []
  type: TYPE_NORMAL
- en: Load Balancing and Forwarding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XDP programs aren’t limited to inspecting the contents of a packet. They can
    also modify the packet’s contents. Let’s consider what’s involved if you want
    to build a simple load balancer that takes packets sent to a given IP address
    and fans those requests to a number of backends that can fulfill the request.
  prefs: []
  type: TYPE_NORMAL
- en: There’s an example of this in the GitHub repo.^([2](ch08.html#ch08fn2)) The
    setup here is a set of containers that run on the same host. There’s a client,
    a load balancer, and two backends, each running in their own container. As illustrated
    in [Figure 8-2](#example_load_balancer_setup), the load balancer receives traffic
    from the client and forwards it to one of the two backend containers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Example load balancer setup](assets/lebp_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Example load balancer setup
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The load balancing function is implemented as an XDP program attached to the
    load balancer’s eth0 network interface. The return code from this program is `XDP_TX`,
    indicating that the packet should be sent back out of the interface it came in
    on. But before that happens, the program has to update the address information
    in the packet headers.
  prefs: []
  type: TYPE_NORMAL
- en: Although I think it’s useful as a learning exercise, this example code is very,
    very far from being production ready; for example, it uses hard-coded addresses
    that assume the exact setup of IP addresses shown in [Figure 8-2](#example_load_balancer_setup).
    It assumes that the only TCP traffic it will ever receive is requests from the
    client or responses to the client. It also cheats by taking advantage of the way
    Docker sets up virtual MAC addresses, using each container’s IP address as the
    last four bytes of the MAC address for the virtual Ethernet interface for each
    container. That virtual Ethernet interface is called eth0 from the perspective
    of the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the XDP program from the example load balancer code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#code_id_8_8)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of this function is practically the same as in the previous
    example: it locates the Ethernet header and then the IP header in the packet.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#code_id_8_9)'
  prefs: []
  type: TYPE_NORMAL
- en: This time it will process only TCP packets, passing anything else it receives
    on up the stack as if nothing had happened.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#code_id_8_10)'
  prefs: []
  type: TYPE_NORMAL
- en: Here the source IP address is checked. If this packet didn’t come from the client,
    I will assume it is a response going to the client.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#code_id_8_11)'
  prefs: []
  type: TYPE_NORMAL
- en: This code generates a pseudorandom choice between backends A and B.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#code_id_8_12)'
  prefs: []
  type: TYPE_NORMAL
- en: The destination IP and MAC addresses are updated to match whichever backend
    was chosen…
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#code_id_8_13)'
  prefs: []
  type: TYPE_NORMAL
- en: …or if this is a response from a backend (which is the assumption here if it
    didn’t come from a client), the destination IP and MAC addresses are updated to
    match the client.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#code_id_8_14)'
  prefs: []
  type: TYPE_NORMAL
- en: Wherever this packet is going, the source addresses need to be updated so that
    it looks as though the packet originated from the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#code_id_8_15)'
  prefs: []
  type: TYPE_NORMAL
- en: The IP header includes a checksum calculated over its contents, and since the
    source and destination IP addresses have both been updated, the checksum also
    needs to be recalculated and replaced in this packet.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since this is a book on eBPF and not networking, I haven’t delved into details
    such as why the IP and MAC addresses need to be updated or what happens if they
    aren’t. If you’re interested, I cover this some more in my [YouTube video of the
    eBPF Summit talk](https://oreil.ly/mQxtT) where I originally wrote this example
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like the previous example, the Makefile includes instructions to not only
    build the code but also use `bpftool` to load and attach the XDP program to the
    interface, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This `make` instruction needs to be run *inside* the load balancer container
    so that eth0 corresponds to its virtual Ethernet interface. This leads to an interesting
    point: an eBPF program is loaded into the kernel, of which there is only one;
    yet the attachment point may be within a particular network namespace and visible
    only within that network namespace.^([3](ch08.html#ch08fn3))'
  prefs: []
  type: TYPE_NORMAL
- en: XDP Offloading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea for XDP originated from a conversation speculating how useful it would
    be if you could run eBPF programs on a network card to make decisions about individual
    packets before they even reach the kernel’s networking stack.^([4](ch08.html#ch08fn4))
    There are some network interface cards that support this full *XDP offload* capability
    where they can indeed run eBPF programs on inbound packets on their own processor.
    This is illustrated in [Figure 8-3](#network_interface_cards_that_support_xd).
  prefs: []
  type: TYPE_NORMAL
- en: '![Network interface cards that support XDP offload can process, drop, and retransmit
    packets without any work required from the host CPU](assets/lebp_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Network interface cards that support XDP offload can process, drop,
    and retransmit packets without any work required from the host CPU
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This means a packet that gets dropped or redirected back out of the same physical
    interface—like the packet drop and load balancing examples earlier in this chapter—is
    never seen by the host’s kernel, and no CPU cycles on the host machine are ever
    spent processing them, as all the work is done on the network card.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the physical network interface card doesn’t support full XDP offload,
    many NIC drivers support XDP hooks, which minimizes the memory copying required
    for an eBPF program to process a packet.^([5](ch08.html#ch08fn5))
  prefs: []
  type: TYPE_NORMAL
- en: This can result in significant performance benefits and allows functionality
    like load balancing to run very efficiently on commodity hardware.^([6](ch08.html#ch08fn6))
  prefs: []
  type: TYPE_NORMAL
- en: You’ve seen how XDP can be used to process inbound network packets, accessing
    them as soon as possible as they arrive on a machine. eBPF can also be used to
    process traffic at other points in the network stack, in whatever direction it
    is flowing. Let’s move on and think about eBPF programs attached within the TC
    subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: Traffic Control (TC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I mentioned traffic control in the previous chapter. By the time a network packet
    reaches this point it will be in kernel memory in the form of an [`sk_buff`](https://oreil.ly/TKDCF).
    This is a data structure that’s used throughout the kernel’s network stack. eBPF
    programs attached within the TC subsystem receive a pointer to the `sk_buff` structure
    as the context parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You might be wondering why XDP programs don’t also use this same structure for
    their context. The answer is that the XDP hook happens before the network data
    reaches the network stack and before the `sk_buff` structure has been set up.
  prefs: []
  type: TYPE_NORMAL
- en: The TC subsystem is intended to regulate how network traffic is scheduled. For
    example, you might want to limit the bandwidth available to each application so
    that they all get a fair chance. But when you’re looking at scheduling individual
    packets, *bandwidth* isn’t a terribly meaningful term, as it’s used for the average
    amount of data being sent or received. A given application might be very bursty,
    or another application might be very sensitive to network latency, so TC gives
    much finer control over the way packets are handled and prioritized.^([7](ch08.html#ch08fn7))
  prefs: []
  type: TYPE_NORMAL
- en: eBPF programs were introduced here to give custom control over the algorithms
    used within TC. But with the power to manipulate, drop, or redirect packets, eBPF
    programs attached within TC can also be used as the building blocks for complex
    network behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'A given piece of network data in the stack flows in one of two directions:
    *ingress* (inbound from the network interface) or *egress* (outbound toward the
    network interface). eBPF programs can be attached in either direction and will
    affect traffic only in that direction. Unlike XDP, it’s possible to attach multiple
    eBPF programs that will be processed in sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional traffic control is split into *classifiers*, which classify packets
    based on some rule, and separate *actions*, which are taken based on the output
    from a classifier and determine what to do with a packet. There can be a series
    of classifiers, all defined as part of a *qdisc* or queuing discipline.
  prefs: []
  type: TYPE_NORMAL
- en: 'eBPF programs are attached as a classifier, but they can also determine what
    action to take within the same program. The action is indicated by the program’s
    return code (whose values are defined in *linux/pkt_cls.h*):'
  prefs: []
  type: TYPE_NORMAL
- en: '`TC_ACT_SHOT` tells the kernel to drop the packet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TC_ACT_UNSPEC` behaves as if the eBPF program hadn’t been run on this packet
    (so it would be passed to the next classifier in the sequence, if there is one).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TC_ACT_OK` tells the kernel to pass the packet to the next layer in the stack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TC_ACT_REDIRECT` sends the packet to the ingress or egress path of a different
    network device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at a few simple examples of programs that can be attached
    within TC. The first simply generates a line of trace and then tells the kernel
    to drop the packet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s consider how to drop only a subset of packets. This example drops
    ICMP (ping) request packets and is very similar to the XDP example you saw earlier
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `sk_buff` structure has pointers to the start and end of the packet data,
    very much like the `xdp_md` structure, and packet parsing proceeds in very much
    the same way. Again, to pass verification you have to explicitly check that any
    access to data is within the range between `data` and `data_end`.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why you would want to implement something like this at
    the TC layer when you have already seen the same kind of functionality implemented
    with XDP. One good reason is that you can use TC programs for egress traffic,
    where XDP can only process ingress traffic. Another is that because XDP is triggered
    as soon as the packet arrives, there is no `sk_buff` kernel data structure related
    to the packet at that point. If the eBPF program is interested in or wants to
    manipulate the `sk_buff` the kernel creates for this packet, the TC attachment
    point is suitable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To better understand the differences between XDP and TC eBPF programs, read
    the “Program Types” section in the [BPF and XDP Reference Guide](https://oreil.ly/MWAJL)
    from the Cilium project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s consider an example that doesn’t just drop certain packets. This
    example identifies a ping request being received and responds with a ping response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#code_id_8_16)'
  prefs: []
  type: TYPE_NORMAL
- en: The `is_icmp_ping_request()` function parses the packet and checks not only
    that it’s an ICMP message, but also that it’s an echo (ping) request.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#code_id_8_17)'
  prefs: []
  type: TYPE_NORMAL
- en: Since this function is going to send a response to the sender, the source and
    destination addresses need to be swapped. (You can read the example code if you
    want to see the nitty-gritty details of this, which also includes updating the
    IP header checksum.)
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#code_id_8_18)'
  prefs: []
  type: TYPE_NORMAL
- en: This is converted to an echo response by changing the type field in the ICMP
    header.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#code_id_8_19)'
  prefs: []
  type: TYPE_NORMAL
- en: This helper function sends a clone of the packet back through the interface
    (`skb->ifindex`) on which it was received.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#code_id_8_20)'
  prefs: []
  type: TYPE_NORMAL
- en: Since the helper function cloned the packet before sending out the response,
    the original packet should be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: In normal circumstances, a ping request would be handled later by the kernel’s
    network stack, but this small example demonstrates how network functionality more
    generally can be replaced by an eBPF implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Lots of networking capabilities today are handled by user space services, but
    where they can be replaced by eBPF programs, it’s likely to be great for performance.
    A packet that’s processed within the kernel doesn’t have to complete its journey
    through the rest of the stack; there is no need for it to transition to user space
    for processing, and the response doesn’t require a transition back into the kernel.
    What’s more, the two could run in parallel—an eBPF program can return `TC_ACT_OK`
    for any packet that requires complex processing that it can’t handle so that it
    gets passed up to the user space service as normal.
  prefs: []
  type: TYPE_NORMAL
- en: For me, this is an important aspect of implementing network functionality in
    eBPF. As the eBPF platform develops (e.g., more recent kernels allowing programs
    of one million instructions), it’s possible to implement increasingly complex
    aspects of networking in the kernel. The parts that are not yet implemented in
    eBPF can still be handled either by the traditional stack within the kernel or
    in user space. Over time, more and more features can be moved from user space
    into the kernel, with the flexibility and dynamic nature of eBPF meaning you won’t
    have to wait for them to be part of the kernel distribution itself. You can load
    eBPF implementations immediately, just as I discussed in [Chapter 1](ch01.html#what_is_ebpf_and_why_is_it_importantque).
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll return to the implementation of networking features in [“eBPF and Kubernetes
    Networking”](#ebpf_and_kubernetes_networking). But first, let’s consider another
    use case that eBPF enables: inspecting the decrypted contents of encrypted traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: Packet Encryption and Decryption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If an application uses encryption to secure data it sends or receives, there
    will be a point before it’s encrypted or after it’s decrypted where the data is
    in the clear. Recall that eBPF can attach programs pretty much anywhere on a machine,
    so if you can hook into a point where data is being passed and isn’t yet encrypted,
    or just after it has been decrypted, that would allow your eBPF program to observe
    that data in the clear. There’s no need to supply any certificates to decrypt
    the traffic, as you would in a traditional SSL inspection tool.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases an application will encrypt data using a library like OpenSSL
    or BoringSSL that lives in user space. In this case the traffic will already be
    encrypted by the time it reaches the socket, which is the user space/kernel boundary
    for network traffic. If you want to trace out this data in its unencrypted form,
    you can use an eBPF program attached to the right place in the user space code.
  prefs: []
  type: TYPE_NORMAL
- en: User Space SSL Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common way to trace out the decrypted content of encrypted packets is to
    hook into calls made to user space libraries like OpenSSL or BoringSSL. An application
    using OpenSSL sends data to be encrypted by making a call to a function called
    `SSL_write()` and retrieves cleartext data that was received over the network
    in encrypted form using `SSL_read()`. Hooking eBPF programs into these functions
    with uprobes allows an application to observe the data *from any application that
    uses this shared library* in the clear, before it is encrypted or after it has
    been decrypted. And there is no need for any keys, because those are already being
    provided by the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a fairly straightforward example called [openssl-tracer in the Pixie
    project](https://oreil.ly/puDp9),^([8](ch08.html#ch08fn8)) within which the eBPF
    programs are in a file called *openssl_tracer_bpf_funcs.c*. Here’s the part of
    that code that sends data to user space, using a perf buffer (similar to examples
    you have seen earlier in this book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can see that data from `buf` gets read into an `event` structure using the
    helper function `bpf_probe_read()`, and then that `event` structure is submitted
    to a perf buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this data is being sent to user space, it’s reasonable to assume this must
    be the data in unencrypted format. So where is this buffer of data obtained? You
    can work that out by seeing where the `process_SSL_data()` function is called.
    It’s called in two places: one for data being read and one for data being written.
    [Figure 8-4](#ebpf_programs_are_hooked_to_uprobes_at_) illustrates what is happening
    in the case of reading data that arrives on this machine in encrypted form.'
  prefs: []
  type: TYPE_NORMAL
- en: When you’re reading data, you supply a pointer to a buffer to `SSL_read()`,
    and when the function returns, that buffer will contain the unencrypted data.
    Much like kprobes, the input parameters to a function—including that buffer pointer—are
    only available to a uprobe attached to the entry point, as the registers they’re
    held in might well get overwritten during the function’s execution. But the data
    won’t be available in the buffer until the function exits, when you can read it
    using a uretprobe.
  prefs: []
  type: TYPE_NORMAL
- en: '![eBPF programs are hooked to uprobes at the entry to and exit from SSL_read()
    so that the unencrypted data can be read from the buffer pointer](assets/lebp_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. eBPF programs are hooked to uprobes at the entry to and exit from
    `SSL_read()` so that the unencrypted data can be read from the buffer pointer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So this example follows a common pattern for kprobes and uprobes, illustrated
    in [Figure 8-4](#ebpf_programs_are_hooked_to_uprobes_at_), where the entry probe
    temporarily stores input parameters using a map, from which the exit probe can
    retrieve them. Let’s look at the code that does this, starting with the eBPF program
    attached to the start of `SSL_read()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#code_id_8_21)'
  prefs: []
  type: TYPE_NORMAL
- en: As described in the comment for this function, the buffer pointer is the second
    parameter passed into the `SSL_read()` function to which this probe will be attached.
    The `PT_REGS_PARM2` macro gets this parameter from the context.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#code_id_8_22)'
  prefs: []
  type: TYPE_NORMAL
- en: The buffer pointer is stored in a hash map, for which the key is the current
    process and thread ID, obtained at the start of the function using the helper
    `bpf_get_current_pid_tgif()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the corresponding program for the exit probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#code_id_8_23)'
  prefs: []
  type: TYPE_NORMAL
- en: Having looked up the current process and thread ID, use this as the key to retrieve
    the buffer pointer from the hash map.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#code_id_8_24)'
  prefs: []
  type: TYPE_NORMAL
- en: If this isn’t a null pointer, call `process_SSL_data()`, which is the function
    you saw earlier that sends the data from that buffer to user space using the perf
    buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#code_id_8_25)'
  prefs: []
  type: TYPE_NORMAL
- en: Clean up the entry in the hash map, since every entry call should be paired
    with an exit.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows how to trace out the cleartext version of encrypted data
    that gets sent and received by a user space application. The tracing itself is
    attached to a user space library, and there’s no guarantee that every application
    will use a given SSL library. The BCC project includes a utility called [*sslsniff*](https://oreil.ly/tFT9p)
    that also supports GnuTLS and NSS. But if someone’s application uses some other
    encryption library (or even, heaven forbid, they chose to “roll their own crypto”),
    the uprobes simply won’t have the right places to hook to and these tracing tools
    won’t work.
  prefs: []
  type: TYPE_NORMAL
- en: There are even more common reasons why this uprobe-based approach might not
    be successful. Unlike the kernel (of which there is only one per [virtual] machine),
    there can be multiple copies of user space library code. If you’re using containers,
    each one is likely to have its own set of all library dependencies. You can hook
    into uprobes in these libraries, but you’d have to identify the right copy for
    the particular container you want to trace. Another possibility is that rather
    than using a shared, dynamically linked library, an application might be statically
    linked so that it’s a single standalone executable.
  prefs: []
  type: TYPE_NORMAL
- en: eBPF and Kubernetes Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although this book isn’t about Kubernetes, eBPF is so widely used for Kubernetes
    networking that it’s a great illustration of using the platform to customize the
    networking stack.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes environments, applications are deployed in *pods*. Each pod is
    a group of one or more containers that share kernel namespaces and cgroups, isolating
    pods from each other and from the host machine they are running on.
  prefs: []
  type: TYPE_NORMAL
- en: In particular (for the purposes of this chapter), a pod typically has its own
    network namespace and its own IP address.^([9](ch08.html#ch08fn9)) This means
    the kernel has a set of network stack structures for that namespace, separated
    from the host’s and from other pods. As shown in [Figure 8-5](#network_path_in_kubernetes),
    the pod is connected to the host by a virtual Ethernet connection, and it is allocated
    its own IP address.
  prefs: []
  type: TYPE_NORMAL
- en: '![Network path in Kubernetes](assets/lebp_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Network path in Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see from [Figure 8-5](#network_path_in_kubernetes) that a packet coming
    from outside the machine destined for an application pod has to travel through
    the network stack on the host, across the virtual Ethernet connection, and into
    the pod’s network namespace, and then it has to traverse the network stack again
    to reach the application.
  prefs: []
  type: TYPE_NORMAL
- en: Those two network stacks are running in the same kernel, so the packet is really
    running through the same processing twice. The more code a network packet has
    to pass through, the higher the latency, so if it’s possible to shorten the network
    path, that will likely bring about performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: An eBPF-based networking solution like Cilium can hook into the network stack
    to override the kernel’s native networking behavior, as shown in [Figure 8-6](#bypassing_iptables_and_conntrack_proces).
  prefs: []
  type: TYPE_NORMAL
- en: '![Bypassing iptables and conntrack processing with eBPF](assets/lebp_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Bypassing iptables and conntrack processing with eBPF
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In particular, eBPF enables replacing iptables and conntrack with a more efficient
    solution for managing network rules and connection tracking. Let’s discuss why
    this results in a significant performance improvement in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding iptables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes has a component called kube-proxy that implements load balancing
    behavior, allowing multiple pods to fulfill requests to a service. This has been
    implemented using iptables rules.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes offers users the choice of which networking solution to use through
    the use of the Container Network Interface (CNI). Some CNI plug-ins use iptables
    rules to implement L3/L4 network policy in Kubernetes; that is, the iptables rules
    indicate whether to drop a packet because it doesn’t meet the network policy.
  prefs: []
  type: TYPE_NORMAL
- en: Although iptables was effective for traditional (precontainer) networking, it
    has some weaknesses when it’s used in Kubernetes. In this environment, pods—and
    their IP addresses—come and go dynamically, and each time a pod is added or removed,
    the iptables rules have to be rewritten in their entirety, and this impacts performance
    at scale. (A [talk](https://oreil.ly/BO0-8) by Haibin Xie and Quinton Hoole at
    KubeCon in 2017 described how making a single rule update to iptables rules for
    20,000 services could take five hours.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Updates to iptables aren’t the only performance issues: looking up a rule requires
    a linear search through the table, which is an O(n) operation, growing linearly
    with the number of rules.'
  prefs: []
  type: TYPE_NORMAL
- en: Cilium uses eBPF hash table maps to store network policy rules, connection tracking,
    and load balancer lookup tables, which can replace iptables for kube-proxy. Both
    looking up an entry in a hash table and inserting a new one are approximately
    O(1) operations, which means they scale much, much better.
  prefs: []
  type: TYPE_NORMAL
- en: You can read about the benchmarked performance improvements this achieves on
    the Cilium [blog](https://oreil.ly/9NV99). In the same post you’ll see that Calico,
    another CNI that has an eBPF option, also achieves better performance when you
    pick its eBPF implementation over iptables. eBPF offers the most performant mechanisms
    for scalable, dynamic Kubernetes deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Coordinated Network Programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A complex networking implementation like Cilium can’t be written as a single
    eBPF program. As shown in [Figure 8-7](#cilium_consists_of_multiple_coordinated),
    it provides several different eBPF programs that are hooked into different parts
    of the kernel and its network stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cilium consists of multiple coordinated eBPF programs that hook into different
    points in the kernel](assets/lebp_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Cilium consists of multiple coordinated eBPF programs that hook
    into different points in the kernel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a general principle, Cilium intercepts traffic as soon as it can in order
    to shorten the processing path for each packet. Messages flowing out from an application
    pod are intercepted at the socket layer, as close to the application as possible.
    Inbound packets from the external network are intercepted using XDP. But what
    about the additional attachment points?
  prefs: []
  type: TYPE_NORMAL
- en: Cilium supports different networking modes that suit different environments.
    A full description of this is beyond the scope of this book (you can find more
    information at [Cilium.io](https://cilium.io)), but I’ll give a brief overview
    here so that you can see why there are so many different eBPF programs!
  prefs: []
  type: TYPE_NORMAL
- en: There is a simple, flat networking mode, in which Cilium allocates IP addresses
    for all the pods in a cluster from the same CIDR and directly routes traffic between
    them. There are also a couple of different tunneling modes, in which traffic intended
    for a pod on a different node gets encapsulated in a message addressed to that
    destination node’s IP address and decapsulated on that destination node for the
    final hop into the pod. Different eBPF programs get invoked to handle traffic
    depending on whether a packet is destined for a local container, the local host,
    another host on this network, or a tunnel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Figure 8-7](#cilium_consists_of_multiple_coordinated) you can see multiple
    TC programs that handle traffic to and from different devices. These devices represent
    the possible different real and virtual network interfaces where a packet might
    be flowing:'
  prefs: []
  type: TYPE_NORMAL
- en: The interface to a pod’s network (one end of the virtual Ethernet connection
    between the pod and the host)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interface to a network tunnel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interface to a physical network device on the host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The host’s own network interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you’re interested in learning more about how packets flow through Cilium,
    Arthur Chiao wrote this detailed and interesting blog post: [“Life of a Packet
    in Cilium: Discovering the Pod-to-Service Traffic Path and BPF Processing Logics”](https://oreil.ly/toxsM).'
  prefs: []
  type: TYPE_NORMAL
- en: The different eBPF programs attached at these various points in the kernel communicate
    using eBFP maps and using the metadata that can be attached to network packets
    as they flow through the stack (which I mentioned when I discussed accessing network
    packets in the XDP example). These programs don’t just route packets to their
    destination; they’re also used to drop packets—just like you saw in earlier examples—based
    on network policies.
  prefs: []
  type: TYPE_NORMAL
- en: Network Policy Enforcement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You saw at the start of this chapter how eBPF programs can drop packets, and
    that means they simply won’t reach their destination. This is the basis of network
    policy enforcement, and conceptually it’s essentially the same whether we are
    thinking about “traditional” or cloud native firewalling. A policy determines
    whether a packet should be dropped or not, based on information about its source
    and/or destination.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional environments, IP addresses are assigned to a particular server
    for a long period of time, but in Kubernetes, IP addresses come and go dynamically,
    and the address assigned today for a particular application pod might very well
    be reused for a completely different application tomorrow. This is why traditional
    firewalling isn’t terribly effective in cloud native environments. It would be
    impractical to redefine firewall rules manually every time IP addresses change.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Kubernetes supports the concept of a NetworkPolicy resource, which
    defines firewalling rules based on the labels applied to particular pods rather
    than based on their IP address. Although the resource type is native to Kubernetes,
    it’s not implemented by Kubernetes itself. Instead, this functionality is delegated
    to whatever CNI plug-in you’re using. If you choose a CNI that doesn’t support
    NetworkPolicy resources, any rules you might configure are simply ignored. On
    the flip side, CNIs are free to configure custom resources that allow for more
    sophisticated network policy configurations than the native Kubernetes definition
    allows. For example, Cilium supports features like DNS-based network policy rules,
    so you can define whether traffic is or isn’t allowed not based on an IP address
    but based on the DNS name (e.g., “*example.com*”). You can also define policies
    for various Layer 7 protocols, for example, allowing or denying traffic for HTTP
    GET calls but not for POST calls to a particular URL.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Isovalent’s free hands-on lab [“Getting Started with Cilium”](https://oreil.ly/afdeh)
    walks you through defining network policies at Layers 3/4 and Layer 7\. Another
    very useful resource is the Network Policy Editor at [*networkpolicy.io*](http://networkpolicy.io),
    which visually presents the effects of a network policy.
  prefs: []
  type: TYPE_NORMAL
- en: As I discussed earlier in this chapter, it’s possible to use iptables rules
    to drop traffic, and that’s an approach some CNIs have taken to implement Kubernetes
    NetworkPolicy rules. Cilium uses eBPF programs to drop traffic that doesn’t match
    the set of rules currently in place. Having seen examples of dropping packets
    earlier in this chapter, I hope you have a rough mental model for how this would
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Cilium uses Kubernetes identities to determine whether a given network policy
    rule applies. In the same way labels define which pods are part of a service in
    Kubernetes, labels also define Cilium’s security identity for the pod. eBPF hash
    tables, indexed by these service identities, make for very efficient rule lookups.
  prefs: []
  type: TYPE_NORMAL
- en: Encrypted Connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many organizations have requirements to protect their deployments and their
    users’ data by encrypting traffic between applications. This can be achieved by
    writing code in each application to ensure that it sets up secure connections,
    typically using mutual Traffic Layer Security (mTLS) underpinning an HTTP or gRPC
    connection. Setting up these connections requires first establishing the identities
    of the apps at either end of the connection (which is usually achieved by exchanging
    certificates) and then encrypting the data that flows between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes, it’s possible to offload the requirement from the application,
    either to a service mesh layer or to the underlying network itself. A full discussion
    of service mesh is beyond the scope of this book, but you might be interested
    in a piece I wrote on the new stack: [“How eBPF Streamlines the Service Mesh”](https://oreil.ly/5ayvF).
    Let’s concentrate here on the network layer and how eBPF makes it possible to
    push the encryption requirement into the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: The simplest option to ensure that traffic is encrypted within a Kubernetes
    cluster is to use *transparent encryption*. It’s called “transparent” because
    it takes place entirely at the network layer and it’s extremely lightweight from
    an operational point of view. The applications themselves don’t need to be aware
    of the encryption at all, and they don’t need to set up HTTPS connections; nor
    does this approach require any additional infrastructure components running under
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: There are two in-kernel encryption protocols in common usage, IPsec and WireGuard^((R)),
    and they’re both supported in Kubernetes networking by Cilium and Calico CNIs.
    It’s beyond the scope of this book to discuss the differences between these two
    protocols, but the key point is that they set up a secure tunnel between two machines.
    The CNI can choose to connect the eBPF endpoint for a pod via this secure tunnel.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is a nice write-up on the [Cilium blog](https://oreil.ly/xjpGP) of how
    Cilium uses WireGuard^((R)) as well as IPsec to provide encrypted traffic between
    nodes. The post also gives a brief overview of the performance characteristics
    of both.
  prefs: []
  type: TYPE_NORMAL
- en: The secure tunnel is set up using the identities of the nodes at either end.
    These identities are managed by Kubernetes anyway, so the administrative burden
    for an operator is minimal. For many purposes this is sufficient as it ensures
    that all network traffic in a cluster is encrypted. Transparent encryption can
    also be used unmodified with NetworkPolicy that uses Kubernetes identities to
    manage whether traffic can flow between different endpoints in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Some organizations operate a multitenant environment where there’s a need for
    strong multitenant boundaries and where it’s essential to use certificates to
    identify every application endpoint. Handling this within every application is
    a significant burden, so it’s something that more recently has been offloaded
    to a service mesh layer, but this requires a whole extra set of components to
    be deployed, causing additional resource consumption, latency, and operational
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: eBPF is now enabling a [new approach](https://oreil.ly/DSnLZ) that builds on
    transparent encryption but uses TLS for the initial certificate exchange and endpoint
    authentication so that the identities can represent individual applications rather
    than the nodes they are running on, as depicted in [Figure 8-8](#transparent_encryption_between_authenti).
  prefs: []
  type: TYPE_NORMAL
- en: '![Transparent encryption between authenticated application identities](assets/lebp_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Transparent encryption between authenticated application identities
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the authentication step has taken place, IPsec or WireGuard^((R)) within
    the kernel is used to encrypt the traffic that flows between those applications.
    This has a number of advantages. It allows third-party certificate and identity
    management tools like cert-manager or SPIFFE/SPIRE to handle the identity part,
    and the network takes care of encryption so that it’s all entirely transparent
    to the application. Cilium supports NetworkPolicy definitions that specify endpoints
    by their SPIFFE ID rather than just by their Kubernetes labels. And perhaps most
    importantly, this approach can be used with any protocol that travels in IP packets.
    That’s a big step up from mTLS, which works only for TCP-based connections.
  prefs: []
  type: TYPE_NORMAL
- en: There’s not enough room in this book to dive deep into all the internals of
    Cilium, but I hope this section helped you understand how eBPF is a powerful platform
    for building complex networking functionality like a fully featured Kubernetes
    CNI.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter you saw eBPF programs attached at a variety of different points
    in the network stack. I showed examples of basic packet processing, and I hope
    these gave you an indication of how eBPF can create powerful networking features.
    You also saw some real-life examples of these networking features, including load
    balancing, firewalling, security mitigation, and Kubernetes networking.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises and Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some ways to learn more about the range of networking use cases for
    eBPF:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the example XDP program `ping()` so that it generates different trace
    messages for ping responses and ping requests. The ICMP header immediately follows
    the IP header in the network packet (just like the IP header follows the Ethernet
    header). You’ll likely want to use `struct icmphdr` from *linux/icmp.h* and look
    at whether the type field shows `ICMP_ECHO` or `ICMP_ECHOREPLY`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to dive further into XDP programming, I recommend the xdp-project’s
    [xdp-tutorial](https://oreil.ly/UmJMF).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use [sslsniff](https://oreil.ly/Zuww7) from the BCC project to view the contents
    of encrypted traffic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore Cilium by using tutorials and labs linked to from the [Cilium website](https://cilium.io/get-started).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the editor at [*networkpolicy.io*](https://networkpolicy.io) to visualize
    the effect of network policies in a Kubernetes deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([1](ch08.html#ch08fn1-marker)) At the time of this writing, around 100 organizations
    have publicly announced their use of Cilium in its [*USERS.md* file](https://oreil.ly/PC7-G),
    though this number is growing quickly. Cilium has also been adopted by AWS, Google,
    and Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.html#ch08fn2-marker)) This example is based on a talk I gave at eBPF
    Summit 2021 called [“A Load Balancer from scratch”](https://oreil.ly/mQxtT). Build
    an eBPF load balancer in just over 15 minutes!
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.html#ch08fn3-marker)) If you want to explore this, try [CTF Challenge
    3 from eBPF Summit 2022](https://oreil.ly/YIh_t). I won’t give spoilers here in
    the book, but you can see the solution in [a walkthrough given by Duffie Cooley
    and me here](https://oreil.ly/_51rC).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.html#ch08fn4-marker)) See Daniel Borkmann’s presentation [“Little
    Helper Minions for Scaling Microservices”](https://oreil.ly/_8ZuF) that includes
    a history of eBPF, where he tells this anecdote.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.html#ch08fn5-marker)) Cilium maintains a [list of drivers that support
    XDP](https://oreil.ly/wCMjB) within the [BPF and XDP Reference Guide](https://oreil.ly/eB7vL).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.html#ch08fn6-marker)) Ceznam shared data about the performance boost
    its team saw when experimenting with an eBPF-based load balancer in [this blog
    post](https://oreil.ly/0cbCx).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.html#ch08fn7-marker)) For a more complete overview of TC and its
    concepts, I recommend Quentin Monnet’s post [“Understanding tc “direct action”
    mode for BPF”](https://oreil.ly/7gU2A).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch08.html#ch08fn8-marker)) There is also a blog post that accompanies
    this example at [*https://blog.px.dev/ebpf-openssl-tracing*](https://blog.px.dev/ebpf-openssl-tracing).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch08.html#ch08fn9-marker)) It’s possible for pods to be run in the host’s
    network namespace so that they share the IP address of the host, but this isn’t
    usually done unless there’s a good reason for an application running in the pod
    to require it.
  prefs: []
  type: TYPE_NORMAL
