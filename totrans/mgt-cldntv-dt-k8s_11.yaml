- en: Chapter 10\. Machine Learning and Other Emerging Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we covered traditional data infrastructure including databases,
    streaming platforms, and analytic engines with a focus on Kubernetes. Now it’s
    time to start looking beyond, exploring the projects and communities that are
    beginning to make cloud native their destination, especially concerning AI and
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any time multiple arrows start pointing in the same direction, it’s worth paying
    attention. The directional arrows in data infrastructure all point to an overall
    macro trend of convergence on Kubernetes, supported by several interrelated trends:'
  prefs: []
  type: TYPE_NORMAL
- en: Common stacks are emerging for managing compute-intensive AI/ML workloads, including
    those that leverage specific hardware such as GPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common data formats are helping to promote the efficient movement of data across
    compute, network, and storage resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object storage is becoming a common persistence layer for data infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will look at several emerging technologies that embody these
    trends, the use cases they enable, and how they contribute to helping you further
    manage the precious resources of compute, network, and storage. We have chosen
    a few projects that touch on different aspects of ML and using data—this is by
    no means an exhaustive survey of every technology in use today. We hear directly
    from the engineers working on each project and provide some details on how they
    fit into a cloud native data stack. You are highly encouraged to continue your
    journey into your interests beyond what is presented here. Follow your curiosity
    and contribute to the communities supporting new use cases in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The Cloud Native AI/ML Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in [Chapter 9](ch09.html#data_analytics_on_kubernetes), analytics,
    AI, and ML on Kubernetes is a topic worthy of more detailed examination. If you
    aren’t familiar with this specialty in the world of data, it’s an exciting domain
    that enhances our ability to produce real-time, data-driven decisions at scale.
    While many of the core algorithms have existed for decades, the nature of this
    work has been changing rapidly over the past few years. Data science as a profession
    has traditionally been relegated to the back office, where volumes of historical
    data were gleaned for insight to find meaning and predict the future. Data scientists
    rarely had any direct involvement with end-user applications, and their work was
    disconnected from user-facing applications.
  prefs: []
  type: TYPE_NORMAL
- en: This began to change with the emergence of the data engineer role. Data engineers
    build the processing engines and pipelines to productionalize data science and
    break down silos between disciplines. As is typical for emerging fields in data
    infrastructure, the largest, most vocal organizations set the tempo for data engineering,
    and their tools and methods have become the mainstream.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real-time nature of data in applications can’t be left just to databases
    and streaming platforms. Products built by data scientists must be closer to the
    end user to maximize their effectiveness in applications. Many organizations have
    recognized this as both a problem and an opportunity: how can we make data science
    another near-real-time component of application deployments? True to form, when
    faced with a challenge, the community has risen to the occasion to build new projects
    and create new disciplines. As a result, a new category of data infrastructure
    on Kubernetes is emerging alongside the traditional categories of persistence,
    streaming, and analytics. This new stack consists of tools that support the real-time
    serving of data specific to AI and ML.'
  prefs: []
  type: TYPE_NORMAL
- en: AI/ML Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are new to the field of AI/ML, it’s easy to become overwhelmed by the
    terminology. Before we look at a few cloud native technologies that solve problems
    in the AI stack, let’s spend some time understanding the new terms and concepts
    that are critical to understanding this specialty. If you are familiar with AI/ML,
    you can safely skip to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s briefly review some common terms used in AI/ML. These frequently
    appear in descriptions of projects and features, and you’ll need to understand
    them to select the right tools and apply them effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: The basic computational building block of ML is the algorithm. Algorithms are
    expressed in code as a set of instructions to analyze data. Common algorithms
    include linear regression, decision trees, k-means, and random forest. Data scientists
    spend their time working with algorithms to gain insights from data. When the
    procedures and parameters are right, the final, repeatable form is output into
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs: []
  type: TYPE_NORMAL
- en: 'ML aims to build systems that mimic the way humans learn so that they can answer
    questions based on provided data without explicit programming. Example questions
    include identifying whether two objects are similar, the likelihood of occurrence
    of a particular event, or choosing the best option given multiple candidates.
    The answering system for these questions is described in a mathematical model
    (or simply *model* for short). A model acts as a function machine: data that describes
    a question goes in, and new data that represents an answer comes out.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature
  prefs: []
  type: TYPE_NORMAL
- en: Features are the portions of a more extensive data set relevant to a specific
    use case. Features are used both to train models and to provide input to models
    in production. For example, if you wanted to predict the weather, you might select
    time, location, and temperature from a much larger data set, ignoring other data
    such as air quality. *Feature selection* is the process of determining what data
    you’ll use, which can be an iterative process. When you hear the word *feature*,
    you can easily translate that to *data*.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs: []
  type: TYPE_NORMAL
- en: A model consists of an algorithm plus data (features) that apply that algorithm
    to a particular domain. To train a model, training data is passed through the
    algorithm to help refine the output to match the expected answer based on the
    data presented. This training data contains the same features that will be used
    in production, with the key difference that the expected answer is known. For
    example, given historical temperatures, do the parameters used in the model predict
    what actually happened? Training is the most resource-intensive phase of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Flow
  prefs: []
  type: TYPE_NORMAL
- en: Flow is a shorthand term for *workflow*. An ML workflow describes the steps
    required to build a working model. The flow generally includes data collection,
    preprocessing and cleaning, feature engineering, model training, validation, and
    performance testing. These are typically fully automated processes.
  prefs: []
  type: TYPE_NORMAL
- en: Vector
  prefs: []
  type: TYPE_NORMAL
- en: The classic mathematical definition of a vector is a quantity that indicates
    direction and magnitude. ML models are mathematical formulas that use numerical
    data. Since not all source data is represented as numbers, normalizing input data
    into vector representations is the key to using general-purpose algorithms in
    ML. Images and text are examples of data that can be vector encoded in the preprocessing
    step of the flow.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs: []
  type: TYPE_NORMAL
- en: Prediction is the step of using the created model to produce a likely answer
    based on input data. For example, we might ask the expected temperature for a
    given location, date, and time by using a weather model. The question being answered
    takes the form “What will happen?”
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference models look for reasons by reversing the relationship of input and
    output data. Given an answer, what features contributed to arriving at this answer?
    Here’s another weather example: based on rainfall, what are the most associated
    temperatures and barometric pressures? The question being answered is “How did
    this happen?”'
  prefs: []
  type: TYPE_NORMAL
- en: Drift
  prefs: []
  type: TYPE_NORMAL
- en: Models are trained with snapshots of data from a point in time. Drift is a condition
    that occurs as the model loses accuracy due to conditions that have changed over
    time or are no longer relevant based on the original training data. When drift
    happens in a model, the solution is to refine the model with updated training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs: []
  type: TYPE_NORMAL
- en: 'Models are only as good as the algorithms used and how those algorithms are
    trained. Bias can be introduced at several points: in the algorithm itself, sample
    data that contains user prejudice or faulty measurement, or exclusion of data.
    In any case, the goal of ML is to be as accurate as possible, and bias is an accuracy
    measurement. Detecting bias in data is a complex problem and is easier to address
    early through good data governance and process rigor.'
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the key concepts that will help you understand the rest of
    this section. For a more complete introduction, consider [*Introduction to Machine
    Learning with Python*](https://oreil.ly/6ii4d) (O’Reilly) by Andreas C. Müller
    and Sarah Guido, or one of the many quality online courses available from your
    favorite learning platform.
  prefs: []
  type: TYPE_NORMAL
- en: Defining an AI/ML Stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given these definitions, we can describe the elements of a cloud native AI stack
    and the purposes such a stack can serve. Emerging disciplines and communities
    have similar implementations with minor variations as various teams innovate to
    solve their own specific needs. We can identify some common patterns by looking
    at organizations that use AI/ML in production at scale and the trends around Kubernetes
    adoption. [Figure 10-1](#common_elements_of_a_cloud_native_aisol) shows some of
    the typical elements found in architectures currently in production. Without being
    prescriptive, we’ll use this as an example of the types of tools in the stack
    and how they might fit together to serve the real-time components of AI/ML.
  prefs: []
  type: TYPE_NORMAL
- en: '![Common elements of a cloud native AI/ML stack](assets/mcdk_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Common elements of a cloud native AI/ML stack
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The goal of a cloud native AI/ML stack should be to get the insights produced
    by AI/ML as close to your users as possible, which means shortening the distance
    between backend analytic processes and using their output in frontend production
    systems. Data exploration happens using algorithms provided in libraries such
    as [scikit-learn](https://scikit-learn.org), [PyTorch](https://pytorch.org), [TensorFlow](https://www.tensorflow.org),
    and [XGBoost](https://xgboost.readthedocs.io) using data stored in databases or
    static files. Python is the most commonly used language with ML libraries. The
    systems we discussed in [Chapter 9](ch09.html#data_analytics_on_kubernetes), including
    Apache Spark, Dask, and Ray, are used to scale up the processing required to use
    Python libraries to build models. [Kubeflow](https://www.kubeflow.org) and similar
    tools allow data engineers to create ML workflows for model generation. The workflows
    output a model file to object storage, providing the bridge between the backend
    processes and frontend production use.
  prefs: []
  type: TYPE_NORMAL
- en: Models are meant to be used, and this is the role of real-time model serving
    tools such as [KServe](https://github.com/kserve/kserve), [Seldon](https://www.seldon.io),
    and [BentoML](https://www.bentoml.com). These tools perform predictions on behalf
    of applications using existing models from object storage and feature stores such
    as [Feast](https://feast.dev). Feature stores perform full lifecycle management
    of feature data, storing new feature data in an online database such as Cassandra,
    training, and serving features to models.
  prefs: []
  type: TYPE_NORMAL
- en: Vector similarity search engines are a new but familiar addition to the real-time
    serving stack for applications. While traditional search engines such as [Apache
    Solr](https://solr.apache.org) provide convenient APIs for text searching, including
    fuzzy matching, vector similarity search is a more powerful algorithm, helping
    to answer the question “What is like the thing I currently have?” To do this,
    it uses relationships in the data instead of just the terms in your search query.
    Vector similarity supports many formats, including text, video, audio, and anything
    else that can be analyzed into a vector. Many open source tools implement vector
    similarity search, including [Milvus](https://milvus.io), [Weaviate](https://weaviate.io),
    [Qdrant](https://qdrant.tech), [Vald](https://vald.vdaas.org), and [Vearch](https://github.com/vearch/vearch).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine a few of the tools that support frontend ML usage by applications
    in more detail and learn how they are deployed in Kubernetes: KServe, Feast, and
    Milvus.'
  prefs: []
  type: TYPE_NORMAL
- en: Real-Time Model Serving with KServe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The “last mile” problem of real-time access to analytic products in AI/ML is
    one that Kubernetes is well poised to solve. Consider the architecture of modern
    web applications: HTTP servers that seem to simply serve a web page often have
    much more complexity behind them. The reality is that application logic and data
    infrastructure are combined to hide the complexity. Much like the HTTP server
    that listens for requests and serves a web page, a model server hides the complexity
    of loading and executing models. It focuses on the developer experience after
    the data science is done.'
  prefs: []
  type: TYPE_NORMAL
- en: '*KServe* is a Kubernetes native model server that makes it easy to provide
    prediction capabilities to applications in production environments. Let’s learn
    more about the origins and functionality of KServe from one of the project founders.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-2](#deploying_kserve_in_kubernetes) shows how KServe is deployed
    on Kubernetes. The control plane consists of the KServe controller, which manages
    custom resources known as InferenceServices. Each InferenceService instance contains
    two microservices, a `Transformer` Service and a `Predictor` Service, each consisting
    of a Deployment and a Service. The Knative framework is used for request processing,
    treating these as serverless microservices that can scale to zero when they are
    not being used for maximum efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying KServe in Kubernetes](assets/mcdk_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Deploying KServe in Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `Transformer` Service provides the endpoint for prediction requests from
    client applications. It also implements a three-stage process of preprocessing,
    prediction, and post-processing:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs: []
  type: TYPE_NORMAL
- en: The `Transformer` Service converts the incoming data into a usable form for
    the model. For example, you may have a model that predicts whether a hot dog is
    in a picture. The `Transformer` Service will convert an incoming picture to a
    vector before passing it to the inference service. During preprocessing, the `Transformer`
    Service also loads feature data from a feature store such as Feast.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs: []
  type: TYPE_NORMAL
- en: The `Transformer` Service delegates the work of prediction to the `Predictor`
    Service, which is responsible for loading the model from object storage and executing
    it using the provided feature data.
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing
  prefs: []
  type: TYPE_NORMAL
- en: The `Transformer` Service receives the prediction result and performs any needed
    post-processing to prepare the response to the client application.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with traditional web serving, you can see the helpful analog
    that model serving creates. Instead of serving HTML pages, KServe covers the modern
    application needs for serving AI/ML workloads. As a Kuberentes native project,
    it fits seamlessly into your cloud native datacenter and application stack.
  prefs: []
  type: TYPE_NORMAL
- en: Full Lifecycle Feature Management with Feast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lifecycle management is a common theme in any data architecture, encompassing
    how data is added, updated, and deleted over time. Feature stores serve a helpful
    coordination role by managing the lifecycle of features used by ML models from
    discovery to their use in production systems, eliminating the versioning and coordination
    issues that can arise when different teams are involved. How did Feast come to
    exist?
  prefs: []
  type: TYPE_NORMAL
- en: As Willem noted, the deployment of Feast on Kubernetes is at a basic state of
    maturity. As no operator or custom resources are defined, you install Feast using
    a Helm chart. [Figure 10-3](#deploying_feast_in_kubernetes) shows a sample installation
    using the [example documented on the Feast website](https://oreil.ly/GYNjR), which
    consists of the feature server and other supporting services.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Feast in Kubernetes](assets/mcdk_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Deploying Feast in Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s examine these components and how they interact. Data scientists identify
    features from existing data sources in a process called *feature engineering*
    and create features using an interface exposed by the feature server (as defined
    in [“Bridging ML Models and Data with Feast”](#bridging_machine_learning_models_and_da)).
    The user can either provide feature data at the time of creating the feature or
    can connect to various backend services so that the data can be updated continuously.
    Feast can consume data published to Kafka topics, or through Kubernetes Jobs that
    pull data from an external source such as a data warehouse. The feature data is
    stored in an online database such as Redis or Cassandra so that it can be easily
    served to production applications. ZooKeeper is used to coordinate metadata and
    service discovery. The Helm chart also supports the ability to deploy Grafana
    for visualization of metrics. This may sound familiar to you, because the reuse
    of common building blocks like Redis, ZooKeeper, and Grafana is a pattern we’ve
    seen used in several other examples in this book.
  prefs: []
  type: TYPE_NORMAL
- en: When model serving tools like KServe are asked to make predictions, they use
    the features stored in Feast as a record of truth. Any updated training by data
    scientists is done using the same feature store, eliminating the need for multiple
    sources of data. The `Transformation` Service provides an optional capability
    to generate new features on demand by performing transformations on existing feature
    data.
  prefs: []
  type: TYPE_NORMAL
- en: KServe and Feast are often used together to create a complete real-time model
    serving stack. Feast performs the dynamic part of feature management, working
    with online and offline data storage as new features arrive through streaming
    and data warehouses. KServe handles the dynamic provisioning for the model serving
    by using the serverless capabilities of Knative. This means that when not in use,
    KServe can scale to zero and react when new requests arrive, saving valuable resources
    in your Kubernetes-based AI/ML stack by using only what you need.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Similarity Search with Milvus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve looked at tools that enable you to use ML models and features
    in production systems, let’s switch gears and look at a different type of AI/ML
    tool: vector similarity search (VSS). As discussed in [“AI/ML Definitions”](#aisolidusml_definitions),
    a vector is a number object representing direction and magnitude from an origin
    in vector space. VSS is an application of vector mathematics in ML. The k-nearest
    neighbors (KNN) algorithm is a way to find how “close” two things are next to
    each other. This algorithm has many variations, but all rely on expressing data
    as a vector. The data to be searched is vectorized using a CPU-intensive KNN-type
    algorithm; typically, this is more of a backend process. VSS servers can then
    index the vector data for less CPU-intensive searching and provide a query mechanism
    that allows end users to provide a vector and find things that are close to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Milvus* is one of many servers designed around the emerging field of VSS.
    Let’s learn how Milvus came to exist and why it’s a great fit for Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: As Xiaofan mentions, Milvus supports both standalone and clustered deployments,
    using the four layers described. Both models are supported in Kubernetes via Helm,
    with the clustered deployment shown in [Figure 10-4](#deploying_milvus_in_kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: The access layer contains the proxy Service, which uses a Kubernetes LoadBalancer
    to route requests from client applications. The services in the coordination layer
    handle incoming search and index queries, routing them to the core server components
    in the worker layer that handle queries and manage data storage and indexing.
    The data nodes manage persistence via files in object storage. The message storage
    uses Apache Pulsar or Apache Kafka to store the stream of incoming data that is
    then passed to data nodes.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Milvus is designed to be Kubernetes native, with a horizontally
    scalable architecture that makes it well poised to scale up to massive data sets
    including billions or even trillions of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Milvus in Kubernetes](assets/mcdk_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Deploying Milvus in Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Efficient Data Movement with Apache Arrow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have explored an AI/ML Kubernetes stack that helps you manage compute
    resources more efficiently, you might be wondering what can be done with network
    resources. The “fallacies of distributed computing” we discussed in [“Embrace
    Distributed Computing”](ch01.html#embrace_distributed_computing) include two important
    points: the fallacies of believing that bandwidth is infinite and that transport
    cost is zero. Even when compute and storage resources seem much more finite, it’s
    easy to forget how easily you can run out of bandwidth. The deeper you get into
    deploying your data infrastructure into Kubernetes, the more likely it is you
    will find out. Early adopters of Apache Hadoop often shared that as their clusters
    grew, their network switches needed to be replaced with the best that could be
    purchased at the time. Just consider what it takes to sort 10 terabytes of data.
    How about 1 petabyte? You get the idea.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Apache Arrow* is a project that addresses the problem of bandwidth utilization
    by providing a more efficient format. This actually isn’t an unknown approach
    in the history of computer science. IBM introduced [Extended Binary Coded Decimal
    Interchange Code (EBCDIC)](https://oreil.ly/MI488) character encoding to create
    efficiency for the preferred transport of the time: the punch card. Arrow attacks
    the problem of efficiency from the ground up in order to avoid the endless upgrading
    to add more resources, proving that the solution to a control problem is never
    “add more power.” Let’s hear from some experts to learn how this works.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Arrow-enabled projects enables you to share data efficiently, reducing
    your resource usage across compute, network, and storage. Example usage of Arrow
    with Spark is shown in [Figure 10-5](#moving_data_with_apache_arrow).
  prefs: []
  type: TYPE_NORMAL
- en: '![Moving data with Apache Arrow](assets/mcdk_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Moving data with Apache Arrow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Parquet datafiles containing Arrow-formatted data persisted to object storage
    can be easily loaded without a deserialization step (1). The data can then be
    analyzed by a Spark application (2), including loading directly into a GPU for
    processing where available. The same efficiency level is maintained when passing
    data between Worker Nodes using Arrow Flight (3). The Arrow record batch is sent
    without any intermediate memory copying or serialization, and the receiver can
    reconstruct the Arrow record without memory copy or deserialization. The efficient
    relationship between the remote processes eliminates two things: processing overhead
    for sending data and the efficient Arrow record format that eliminates wasted
    bandwidth.'
  prefs: []
  type: TYPE_NORMAL
- en: At the scale common in Spark applications, the effect on network latency and
    bandwidth can add up quickly. The network transport savings really keep your data
    moving, even when volumes reach into terabytes and petabytes. [Research](https://oreil.ly/rve9i)
    performed by Tanveer Ahmad at TU Delft showed a 20 to 30 times efficiency gain
    using Arrow Flight to move large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: Versioned Object Storage with lakeFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object storage is becoming the standard for cloud native data persistence. It
    lowers the complexity for services but also points to a different way of thinking
    about data mutability. Instead of opening a file and providing random access,
    file storage is precomputed, written once, and read many times. Instead of updating
    a datafile, you write a new one, but how do you distinguish which datafiles are
    current? For this reason, object storage presents issues with disk space management.
    Since there is no concept of managing an entire filesystem, each file is an object
    in a virtually infinite resource.
  prefs: []
  type: TYPE_NORMAL
- en: Object storage APIs are fairly basic with few frills, but data teams need more
    than just the basics for their use cases. [lakeFS](https://lakefs.io) and [Nessie](https://projectnessie.org)
    are two projects trying to make object storage a better fit for emerging workloads
    on Kubernetes. Let’s examine how lakeFS extends the functionality of object storage
    for cloud native applications.
  prefs: []
  type: TYPE_NORMAL
- en: Using lakeFS in Kubernetes is a great fit because of its stateless design and
    declarative deployment. A Helm deployment consists of [configuring](https://oreil.ly/GhZMB)
    the lakeFS service, which then serves as a communication gateway to and from other
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Communications into the server emulate S3 object storage, enabling interaction
    with any data store that supports the S3 API. Incoming communication is bound
    as a ClusterIP to serve HTTP traffic across one or more stateless lakeFS server
    Pods managed by a Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: lakeFS uses PostgreSQL to manage metadata, so users can either provide the endpoint
    for a running system, as shown in [Figure 10-6](#deploying_lakefs_in_kubernetes),
    or lakeFS can run an embedded PostgreSQL server inside the lakeFS Pod for its
    exclusive use. PostgreSQL is the state management for the stateless lakeFS servers
    when deployed as a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying lakeFS in Kubernetes](assets/mcdk_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Deploying lakeFS in Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The most important connection is to the object storage endpoints that will store
    the actual data. When users persist data to lakeFS, the actual datafile will pass
    through to the backend object storage, and versioning metadata is stored in PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: The additional outbound connection is for providing orchestration with other
    ML infrastructure. Webhooks allow for triggers on action that alert downstream
    systems when something such as a commit is issued. These triggers serve as a key
    ingredient to automated ML workflows and other applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, the pipeline of new and exciting ways to work with data in Kubernetes
    extends well into the future. New projects are addressing the challenges of advanced
    data workloads according to the cloud native principles of elasticity, scalability,
    and self-healing.
  prefs: []
  type: TYPE_NORMAL
- en: These tools give you the ability to manage the critical resources of compute,
    network and storage. You can better manage compute-intensive workloads such as
    AI/ML with KServe for the delivery, Feast for model management, and Milvus to
    operationalize new search methods. Network resources are ruled by the simple laws
    of volume and speed, and at the volumes of data we can create, every little bit
    helps. Apache Arrow reduces this volume by creating a common reference frame across
    applications. Unifying around object storage provides further efficiencies, with
    tools like lakeFS making object storage easier to consume in ways that are sympathetic
    to application data storage needs.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’ve examined data infrastructure on Kubernetes from mature
    areas like storage all the way out to cutting-edge projects for managing AI/ML
    artifacts such as models and features. Now it’s time to take all the knowledge
    you’ve gained so far and plan to put it into practice.
  prefs: []
  type: TYPE_NORMAL
