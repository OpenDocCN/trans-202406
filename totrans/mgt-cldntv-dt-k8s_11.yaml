- en: Chapter 10\. Machine Learning and Other Emerging Use Cases
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 机器学习和其他新兴用例
- en: In previous chapters, we covered traditional data infrastructure including databases,
    streaming platforms, and analytic engines with a focus on Kubernetes. Now it’s
    time to start looking beyond, exploring the projects and communities that are
    beginning to make cloud native their destination, especially concerning AI and
    ML.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们涵盖了传统数据基础设施，包括数据库、流处理平台和分析引擎，重点放在 Kubernetes 上。现在是时候超越这些，探索正在开始将云原生作为目的地的项目和社区，特别是涉及
    AI 和 ML 的项目。
- en: 'Any time multiple arrows start pointing in the same direction, it’s worth paying
    attention. The directional arrows in data infrastructure all point to an overall
    macro trend of convergence on Kubernetes, supported by several interrelated trends:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 每当多个箭头开始指向同一个方向时，都值得关注。数据基础设施中的方向箭头全部指向 Kubernetes 的整体宏观趋势收敛，由几个相互关联的趋势支持：
- en: Common stacks are emerging for managing compute-intensive AI/ML workloads, including
    those that leverage specific hardware such as GPUs.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为管理计算密集型 AI/ML 工作负载正在出现常见堆栈，包括利用特定硬件如 GPU 的堆栈。
- en: Common data formats are helping to promote the efficient movement of data across
    compute, network, and storage resources.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的数据格式有助于促进数据在计算、网络和存储资源之间的高效移动。
- en: Object storage is becoming a common persistence layer for data infrastructure.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象存储正变成数据基础设施的常见持久层。
- en: In this chapter, we will look at several emerging technologies that embody these
    trends, the use cases they enable, and how they contribute to helping you further
    manage the precious resources of compute, network, and storage. We have chosen
    a few projects that touch on different aspects of ML and using data—this is by
    no means an exhaustive survey of every technology in use today. We hear directly
    from the engineers working on each project and provide some details on how they
    fit into a cloud native data stack. You are highly encouraged to continue your
    journey into your interests beyond what is presented here. Follow your curiosity
    and contribute to the communities supporting new use cases in Kubernetes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨几种体现这些趋势的新兴技术，它们所支持的使用案例，以及它们如何有助于帮助您进一步管理计算、网络和存储等宝贵资源。我们选择了几个涉及机器学习和数据使用不同方面的项目——这绝不是对今天每种使用的技术的详尽调查。我们直接听取每个项目工程师的意见，并提供关于它们如何融入云原生数据堆栈的一些细节。强烈建议您继续深入您感兴趣的领域，超出此处呈现的内容。跟随您的好奇心，并为支持
    Kubernetes 中新使用案例的社区做出贡献。
- en: The Cloud Native AI/ML Stack
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云原生 AI/ML 堆栈
- en: As discussed in [Chapter 9](ch09.html#data_analytics_on_kubernetes), analytics,
    AI, and ML on Kubernetes is a topic worthy of more detailed examination. If you
    aren’t familiar with this specialty in the world of data, it’s an exciting domain
    that enhances our ability to produce real-time, data-driven decisions at scale.
    While many of the core algorithms have existed for decades, the nature of this
    work has been changing rapidly over the past few years. Data science as a profession
    has traditionally been relegated to the back office, where volumes of historical
    data were gleaned for insight to find meaning and predict the future. Data scientists
    rarely had any direct involvement with end-user applications, and their work was
    disconnected from user-facing applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如第9章所讨论的（[第9章](ch09.html#data_analytics_on_kubernetes)），在 Kubernetes 上进行的数据分析、人工智能（AI）和机器学习（ML）是一个值得更详细研究的主题。如果你对这个在数据世界中的专业不熟悉，它是一个令人兴奋的领域，增强了我们以规模化方式生成实时数据驱动决策的能力。虽然许多核心算法已存在几十年，但这项工作的性质在过去几年中发生了迅速变化。作为一个职业，数据科学传统上被局限在后台，历史数据的大量获取用于洞察发现意义并预测未来。数据科学家很少直接参与最终用户应用程序，他们的工作与用户界面应用程序是分离的。
- en: This began to change with the emergence of the data engineer role. Data engineers
    build the processing engines and pipelines to productionalize data science and
    break down silos between disciplines. As is typical for emerging fields in data
    infrastructure, the largest, most vocal organizations set the tempo for data engineering,
    and their tools and methods have become the mainstream.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据工程师角色的出现，情况开始改变。数据工程师建立处理引擎和管道，以生产化数据科学，并打破学科之间的壁垒。作为数据基础设施新兴领域的典型情况，最大、最有发言权的组织设定了数据工程的节奏，他们的工具和方法已成为主流。
- en: 'The real-time nature of data in applications can’t be left just to databases
    and streaming platforms. Products built by data scientists must be closer to the
    end user to maximize their effectiveness in applications. Many organizations have
    recognized this as both a problem and an opportunity: how can we make data science
    another near-real-time component of application deployments? True to form, when
    faced with a challenge, the community has risen to the occasion to build new projects
    and create new disciplines. As a result, a new category of data infrastructure
    on Kubernetes is emerging alongside the traditional categories of persistence,
    streaming, and analytics. This new stack consists of tools that support the real-time
    serving of data specific to AI and ML.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序中数据的实时特性不能仅仅依赖于数据库和流处理平台。由数据科学家构建的产品必须更接近最终用户，以最大化其在应用程序中的效果。许多组织已经意识到这既是一个问题也是一个机会：我们如何使数据科学成为应用部署的另一个近实时组件？忠于自己的形式，在面对挑战时，社区已经应对，并建立了新的项目并创造了新的学科。因此，与传统的持久性、流处理和分析类别并行出现了基于Kubernetes的新型数据基础设施类别。这个新堆栈包括支持AI和ML特定实时数据服务的工具。
- en: AI/ML Definitions
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI/ML 定义
- en: If you are new to the field of AI/ML, it’s easy to become overwhelmed by the
    terminology. Before we look at a few cloud native technologies that solve problems
    in the AI stack, let’s spend some time understanding the new terms and concepts
    that are critical to understanding this specialty. If you are familiar with AI/ML,
    you can safely skip to the next section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是AI/ML领域的新手，可能会因术语而感到不知所措。在我们看一些云原生技术解决AI堆栈中的问题之前，让我们花些时间了解这个专业领域的新术语和概念。如果您对AI/ML熟悉，可以安全地跳到下一节。
- en: 'First, let’s briefly review some common terms used in AI/ML. These frequently
    appear in descriptions of projects and features, and you’ll need to understand
    them to select the right tools and apply them effectively:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们简要回顾一些AI/ML中常见的术语。这些术语经常出现在项目和功能的描述中，您需要理解它们以选择正确的工具并有效地应用它们：
- en: Algorithm
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 算法
- en: The basic computational building block of ML is the algorithm. Algorithms are
    expressed in code as a set of instructions to analyze data. Common algorithms
    include linear regression, decision trees, k-means, and random forest. Data scientists
    spend their time working with algorithms to gain insights from data. When the
    procedures and parameters are right, the final, repeatable form is output into
    models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ML的基本计算构建块是算法。算法以代码形式表达为一组分析数据的指令。常见的算法包括线性回归、决策树、K均值和随机森林。数据科学家花费时间与算法一起工作，以从数据中获得洞见。当过程和参数正确时，最终的、可重复的形式输出为模型。
- en: Model
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: 'ML aims to build systems that mimic the way humans learn so that they can answer
    questions based on provided data without explicit programming. Example questions
    include identifying whether two objects are similar, the likelihood of occurrence
    of a particular event, or choosing the best option given multiple candidates.
    The answering system for these questions is described in a mathematical model
    (or simply *model* for short). A model acts as a function machine: data that describes
    a question goes in, and new data that represents an answer comes out.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ML旨在构建模仿人类学习方式的系统，以便根据提供的数据回答问题，而无需明确编程。示例问题包括识别两个对象是否相似，特定事件发生的可能性，或在多个候选项中选择最佳选项。用于这些问题的答案系统描述为数学模型（简称*模型*）。模型充当函数机器的角色：描述问题的数据输入，输出表示答案的新数据。
- en: Feature
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征
- en: Features are the portions of a more extensive data set relevant to a specific
    use case. Features are used both to train models and to provide input to models
    in production. For example, if you wanted to predict the weather, you might select
    time, location, and temperature from a much larger data set, ignoring other data
    such as air quality. *Feature selection* is the process of determining what data
    you’ll use, which can be an iterative process. When you hear the word *feature*,
    you can easily translate that to *data*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 特征是与特定用例相关的更大数据集的部分。特征既用于训练模型，也用于在生产中为模型提供输入。例如，如果您想要预测天气，可能会从更大的数据集中选择时间、地点和温度，忽略其他数据如空气质量。*特征选择*是确定要使用的数据的过程，这可能是一个迭代的过程。当您听到*特征*这个词时，您可以轻松地将其翻译为*数据*。
- en: Training
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: A model consists of an algorithm plus data (features) that apply that algorithm
    to a particular domain. To train a model, training data is passed through the
    algorithm to help refine the output to match the expected answer based on the
    data presented. This training data contains the same features that will be used
    in production, with the key difference that the expected answer is known. For
    example, given historical temperatures, do the parameters used in the model predict
    what actually happened? Training is the most resource-intensive phase of ML.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 模型由一个算法和应用该算法到特定领域的数据（特征）组成。为了训练一个模型，训练数据通过算法以帮助调整输出，以匹配基于所呈现数据的预期答案。这些训练数据包含相同的特征，这些特征将在生产中使用，唯一的区别在于已知期望的答案。例如，给定历史温度，模型中使用的参数是否预测了实际发生的情况？训练是机器学习中资源消耗最大的阶段。
- en: Flow
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 流
- en: Flow is a shorthand term for *workflow*. An ML workflow describes the steps
    required to build a working model. The flow generally includes data collection,
    preprocessing and cleaning, feature engineering, model training, validation, and
    performance testing. These are typically fully automated processes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 流是*工作流程*的简称。一个机器学习工作流描述了构建工作模型所需的步骤。流通常包括数据收集、预处理和清理、特征工程、模型训练、验证和性能测试。这些通常是全自动化的过程。
- en: Vector
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 向量
- en: The classic mathematical definition of a vector is a quantity that indicates
    direction and magnitude. ML models are mathematical formulas that use numerical
    data. Since not all source data is represented as numbers, normalizing input data
    into vector representations is the key to using general-purpose algorithms in
    ML. Images and text are examples of data that can be vector encoded in the preprocessing
    step of the flow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的经典数学定义是指示方向和大小的数量。机器学习模型是使用数值数据的数学公式。由于并非所有源数据都表示为数字，将输入数据归一化为向量表示是在流程预处理步骤中使用通用算法的关键。图像和文本是可以在流程的预处理步骤中进行向量编码的数据示例。
- en: Prediction
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 预测
- en: Prediction is the step of using the created model to produce a likely answer
    based on input data. For example, we might ask the expected temperature for a
    given location, date, and time by using a weather model. The question being answered
    takes the form “What will happen?”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是使用创建的模型根据输入数据生成可能的答案的步骤。例如，我们可以通过使用天气模型询问给定位置、日期和时间的预期温度。所回答的问题采取的形式是“将会发生什么？”
- en: Inference
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 推断
- en: 'Inference models look for reasons by reversing the relationship of input and
    output data. Given an answer, what features contributed to arriving at this answer?
    Here’s another weather example: based on rainfall, what are the most associated
    temperatures and barometric pressures? The question being answered is “How did
    this happen?”'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 推断模型通过颠倒输入和输出数据的关系来寻找原因。给定一个答案，是哪些特征导致了这个答案？这里有另一个天气的例子：基于降雨量，最相关的温度和气压是多少？所回答的问题是“这是如何发生的？”
- en: Drift
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移
- en: Models are trained with snapshots of data from a point in time. Drift is a condition
    that occurs as the model loses accuracy due to conditions that have changed over
    time or are no longer relevant based on the original training data. When drift
    happens in a model, the solution is to refine the model with updated training
    data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是用来自某一时间点的数据快照训练的。漂移是一个条件，因为模型由于时间变化或不再与原始训练数据相关而失去准确性。当模型发生漂移时，解决方案是通过更新的训练数据来完善模型。
- en: Bias
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见
- en: 'Models are only as good as the algorithms used and how those algorithms are
    trained. Bias can be introduced at several points: in the algorithm itself, sample
    data that contains user prejudice or faulty measurement, or exclusion of data.
    In any case, the goal of ML is to be as accurate as possible, and bias is an accuracy
    measurement. Detecting bias in data is a complex problem and is easier to address
    early through good data governance and process rigor.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的好坏取决于所使用的算法以及这些算法的训练方式。偏见可以在几个方面引入：在算法本身、包含用户偏见或错误测量的样本数据中，或者数据的排除中。无论如何，机器学习的目标是尽可能准确，而偏见是一种准确性的衡量。检测数据中的偏见是一个复杂的问题，通过良好的数据治理和严格的流程更容易在早期解决。
- en: These are some of the key concepts that will help you understand the rest of
    this section. For a more complete introduction, consider [*Introduction to Machine
    Learning with Python*](https://oreil.ly/6ii4d) (O’Reilly) by Andreas C. Müller
    and Sarah Guido, or one of the many quality online courses available from your
    favorite learning platform.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是帮助您理解本节其余内容的一些关键概念。要进行更全面的介绍，请考虑阅读 Andreas C. Müller 和 Sarah Guido 的 [*Python机器学习入门*](https://oreil.ly/6ii4d)（O'Reilly）或您喜欢的在线学习平台上的众多高质量课程之一。
- en: Defining an AI/ML Stack
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 AI/ML 栈
- en: Given these definitions, we can describe the elements of a cloud native AI stack
    and the purposes such a stack can serve. Emerging disciplines and communities
    have similar implementations with minor variations as various teams innovate to
    solve their own specific needs. We can identify some common patterns by looking
    at organizations that use AI/ML in production at scale and the trends around Kubernetes
    adoption. [Figure 10-1](#common_elements_of_a_cloud_native_aisol) shows some of
    the typical elements found in architectures currently in production. Without being
    prescriptive, we’ll use this as an example of the types of tools in the stack
    and how they might fit together to serve the real-time components of AI/ML.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了这些概念之后，我们可以描述云原生 AI 栈的元素及其可以发挥的作用。新兴的学科和社区通过各种团队创新以解决自己的特定需求，实施具有轻微差异的相似实现。通过查看在生产规模上使用
    AI/ML 的组织以及围绕 Kubernetes 采用的趋势，我们可以识别一些常见模式。[图 10-1](#common_elements_of_a_cloud_native_aisol)
    展示了当前在生产中体系结构中发现的一些典型元素。在不指定特定解决方案的情况下，我们将其作为 AI/ML 实时组件类型工具及其如何组合以服务的示例。
- en: '![Common elements of a cloud native AI/ML stack](assets/mcdk_1001.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![云原生 AI/ML 栈的常见元素](assets/mcdk_1001.png)'
- en: Figure 10-1\. Common elements of a cloud native AI/ML stack
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 云原生 AI/ML 栈的常见元素
- en: The goal of a cloud native AI/ML stack should be to get the insights produced
    by AI/ML as close to your users as possible, which means shortening the distance
    between backend analytic processes and using their output in frontend production
    systems. Data exploration happens using algorithms provided in libraries such
    as [scikit-learn](https://scikit-learn.org), [PyTorch](https://pytorch.org), [TensorFlow](https://www.tensorflow.org),
    and [XGBoost](https://xgboost.readthedocs.io) using data stored in databases or
    static files. Python is the most commonly used language with ML libraries. The
    systems we discussed in [Chapter 9](ch09.html#data_analytics_on_kubernetes), including
    Apache Spark, Dask, and Ray, are used to scale up the processing required to use
    Python libraries to build models. [Kubeflow](https://www.kubeflow.org) and similar
    tools allow data engineers to create ML workflows for model generation. The workflows
    output a model file to object storage, providing the bridge between the backend
    processes and frontend production use.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生 AI/ML 栈的目标应该是尽可能将 AI/ML 生成的洞见靠近您的用户，这意味着缩短后端分析处理与前端生产系统中使用其输出之间的距离。数据探索使用诸如
    [scikit-learn](https://scikit-learn.org)、[PyTorch](https://pytorch.org)、[TensorFlow](https://www.tensorflow.org)
    和 [XGBoost](https://xgboost.readthedocs.io) 等库提供的算法，使用存储在数据库或静态文件中的数据。Python 是最常用的具有
    ML 库的语言。我们在 [第 9 章](ch09.html#data_analytics_on_kubernetes) 中讨论的系统，包括 Apache Spark、Dask
    和 Ray，用于扩展处理所需的 Python 库以构建模型。[Kubeflow](https://www.kubeflow.org) 和类似工具允许数据工程师创建用于模型生成的
    ML 工作流程。工作流程将模型文件输出到对象存储，提供了后端处理与前端生产使用之间的桥梁。
- en: Models are meant to be used, and this is the role of real-time model serving
    tools such as [KServe](https://github.com/kserve/kserve), [Seldon](https://www.seldon.io),
    and [BentoML](https://www.bentoml.com). These tools perform predictions on behalf
    of applications using existing models from object storage and feature stores such
    as [Feast](https://feast.dev). Feature stores perform full lifecycle management
    of feature data, storing new feature data in an online database such as Cassandra,
    training, and serving features to models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的用途在于使用，这是实时模型服务工具如 [KServe](https://github.com/kserve/kserve)、[Seldon](https://www.seldon.io)
    和 [BentoML](https://www.bentoml.com) 所扮演的角色。这些工具代表应用程序使用来自对象存储和特征存储（例如 [Feast](https://feast.dev)）中现有模型进行预测。特征存储执行特征数据的完整生命周期管理，将新特征数据存储在在线数据库（如
    Cassandra）中，进行训练，并将特征提供给模型。
- en: Vector similarity search engines are a new but familiar addition to the real-time
    serving stack for applications. While traditional search engines such as [Apache
    Solr](https://solr.apache.org) provide convenient APIs for text searching, including
    fuzzy matching, vector similarity search is a more powerful algorithm, helping
    to answer the question “What is like the thing I currently have?” To do this,
    it uses relationships in the data instead of just the terms in your search query.
    Vector similarity supports many formats, including text, video, audio, and anything
    else that can be analyzed into a vector. Many open source tools implement vector
    similarity search, including [Milvus](https://milvus.io), [Weaviate](https://weaviate.io),
    [Qdrant](https://qdrant.tech), [Vald](https://vald.vdaas.org), and [Vearch](https://github.com/vearch/vearch).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 向量相似性搜索引擎是实时服务堆栈的一个新但熟悉的补充。虽然传统搜索引擎（如[Apache Solr](https://solr.apache.org)）为文本搜索提供了便捷的
    API，包括模糊匹配，但向量相似性搜索是一种更强大的算法，帮助回答“与我当前拥有的东西相似的是什么？”这一问题。为此，它利用数据中的关系，而不仅仅是搜索查询中的术语。向量相似性支持多种格式，包括文本、视频、音频以及任何可以分析为向量的内容。许多开源工具实现了向量相似性搜索，包括
    [Milvus](https://milvus.io)、[Weaviate](https://weaviate.io)、[Qdrant](https://qdrant.tech)、[Vald](https://vald.vdaas.org)
    和 [Vearch](https://github.com/vearch/vearch)。
- en: 'Let’s examine a few of the tools that support frontend ML usage by applications
    in more detail and learn how they are deployed in Kubernetes: KServe, Feast, and
    Milvus.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细研究一下支持应用程序前端 ML 使用的几个工具，并了解它们在 Kubernetes 中的部署情况：KServe、Feast 和 Milvus。
- en: Real-Time Model Serving with KServe
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 KServe 进行实时模型服务
- en: 'The “last mile” problem of real-time access to analytic products in AI/ML is
    one that Kubernetes is well poised to solve. Consider the architecture of modern
    web applications: HTTP servers that seem to simply serve a web page often have
    much more complexity behind them. The reality is that application logic and data
    infrastructure are combined to hide the complexity. Much like the HTTP server
    that listens for requests and serves a web page, a model server hides the complexity
    of loading and executing models. It focuses on the developer experience after
    the data science is done.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AI/ML 中实时访问分析产品的“最后一英里”问题是 Kubernetes 很可能解决的问题。考虑现代 Web 应用程序的架构：看似简单提供 Web 页面的
    HTTP 服务器背后往往有更复杂的逻辑。事实上，应用逻辑和数据基础设施结合在一起，以隐藏复杂性。就像监听请求并提供 Web 页面的 HTTP 服务器一样，模型服务器隐藏了加载和执行模型的复杂性，专注于数据科学完成后的开发者体验。
- en: '*KServe* is a Kubernetes native model server that makes it easy to provide
    prediction capabilities to applications in production environments. Let’s learn
    more about the origins and functionality of KServe from one of the project founders.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*KServe* 是一个原生于 Kubernetes 的模型服务器，可以轻松为生产环境中的应用程序提供预测能力。让我们从项目创始人之一那里更多地了解
    KServe 的起源和功能。'
- en: '[Figure 10-2](#deploying_kserve_in_kubernetes) shows how KServe is deployed
    on Kubernetes. The control plane consists of the KServe controller, which manages
    custom resources known as InferenceServices. Each InferenceService instance contains
    two microservices, a `Transformer` Service and a `Predictor` Service, each consisting
    of a Deployment and a Service. The Knative framework is used for request processing,
    treating these as serverless microservices that can scale to zero when they are
    not being used for maximum efficiency.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-2](#deploying_kserve_in_kubernetes) 展示了 KServe 在 Kubernetes 上的部署方式。控制平面由
    KServe 控制器组成，管理称为推理服务的自定义资源。每个推理服务实例包含两个微服务，即 `Transformer` 服务和 `Predictor` 服务，每个都包含一个
    Deployment 和一个 Service。Knative 框架用于请求处理，将它们视为无服务器微服务，在不使用时可以缩放到零，以实现最大效率。'
- en: '![Deploying KServe in Kubernetes](assets/mcdk_1002.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![在 Kubernetes 中部署 KServe](assets/mcdk_1002.png)'
- en: Figure 10-2\. Deploying KServe in Kubernetes
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 在 Kubernetes 中部署 KServe
- en: 'The `Transformer` Service provides the endpoint for prediction requests from
    client applications. It also implements a three-stage process of preprocessing,
    prediction, and post-processing:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer` 服务为来自客户端应用程序的预测请求提供端点。它还实现了预处理、预测和后处理的三阶段流程：'
- en: Preprocessing
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理
- en: The `Transformer` Service converts the incoming data into a usable form for
    the model. For example, you may have a model that predicts whether a hot dog is
    in a picture. The `Transformer` Service will convert an incoming picture to a
    vector before passing it to the inference service. During preprocessing, the `Transformer`
    Service also loads feature data from a feature store such as Feast.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer`服务将传入的数据转换为模型可用的形式。例如，您可能有一个模型来预测图片中是否有热狗。在将图片传递给推理服务之前，`Transformer`服务将其转换为向量。在预处理过程中，`Transformer`服务还从诸如Feast之类的特征存储加载特征数据。'
- en: Prediction
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 预测
- en: The `Transformer` Service delegates the work of prediction to the `Predictor`
    Service, which is responsible for loading the model from object storage and executing
    it using the provided feature data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer`服务委托预测工作给`Predictor`服务，后者负责从对象存储加载模型并使用提供的特征数据执行它。'
- en: Post-processing
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理
- en: The `Transformer` Service receives the prediction result and performs any needed
    post-processing to prepare the response to the client application.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer`服务接收预测结果并执行任何必要的后处理以准备响应给客户端应用程序。'
- en: If you are familiar with traditional web serving, you can see the helpful analog
    that model serving creates. Instead of serving HTML pages, KServe covers the modern
    application needs for serving AI/ML workloads. As a Kuberentes native project,
    it fits seamlessly into your cloud native datacenter and application stack.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉传统的Web服务，您会看到模型服务创建的有用类比。KServe不仅提供HTML页面服务，还涵盖了为提供AI/ML工作负载而生的现代应用需求。作为Kubernetes原生项目，它可以无缝集成到您的云原生数据中心和应用堆栈中。
- en: Full Lifecycle Feature Management with Feast
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Feast进行完整的生命周期特征管理
- en: Lifecycle management is a common theme in any data architecture, encompassing
    how data is added, updated, and deleted over time. Feature stores serve a helpful
    coordination role by managing the lifecycle of features used by ML models from
    discovery to their use in production systems, eliminating the versioning and coordination
    issues that can arise when different teams are involved. How did Feast come to
    exist?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 生命周期管理是任何数据架构中的常见主题，涵盖数据随时间的添加、更新和删除。特征存储通过管理从发现到在生产系统中使用的ML模型特征的生命周期，起到协调作用，消除不同团队参与时可能出现的版本控制和协调问题。Feast是如何产生的？
- en: As Willem noted, the deployment of Feast on Kubernetes is at a basic state of
    maturity. As no operator or custom resources are defined, you install Feast using
    a Helm chart. [Figure 10-3](#deploying_feast_in_kubernetes) shows a sample installation
    using the [example documented on the Feast website](https://oreil.ly/GYNjR), which
    consists of the feature server and other supporting services.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Willem所指出的，将Feast部署到Kubernetes的基本状态是成熟的。由于没有定义运算符或自定义资源，您可以使用Helm图表安装Feast。[图10-3](#deploying_feast_in_kubernetes)展示了使用[Feast网站上记录的示例](https://oreil.ly/GYNjR)，包括特征服务器和其他支持服务的样本安装。
- en: '![Deploying Feast in Kubernetes](assets/mcdk_1003.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![在Kubernetes中部署Feast](assets/mcdk_1003.png)'
- en: Figure 10-3\. Deploying Feast in Kubernetes
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3. 在Kubernetes中部署Feast
- en: Let’s examine these components and how they interact. Data scientists identify
    features from existing data sources in a process called *feature engineering*
    and create features using an interface exposed by the feature server (as defined
    in [“Bridging ML Models and Data with Feast”](#bridging_machine_learning_models_and_da)).
    The user can either provide feature data at the time of creating the feature or
    can connect to various backend services so that the data can be updated continuously.
    Feast can consume data published to Kafka topics, or through Kubernetes Jobs that
    pull data from an external source such as a data warehouse. The feature data is
    stored in an online database such as Redis or Cassandra so that it can be easily
    served to production applications. ZooKeeper is used to coordinate metadata and
    service discovery. The Helm chart also supports the ability to deploy Grafana
    for visualization of metrics. This may sound familiar to you, because the reuse
    of common building blocks like Redis, ZooKeeper, and Grafana is a pattern we’ve
    seen used in several other examples in this book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来研究这些组件及其如何互动。数据科学家从现有数据源中识别特征，这个过程称为*特征工程*，并使用由特征服务器公开的接口创建特征（如在[“用Feast桥接ML模型和数据”](#bridging_machine_learning_models_and_da)中定义）。用户可以在创建特征时提供特征数据，也可以连接到各种后端服务，以便数据可以持续更新。Feast可以消费发布到Kafka主题的数据，或通过Kubernetes作业从数据仓库拉取数据。特征数据存储在在线数据库（如Redis或Cassandra）中，以便可以轻松地提供给生产应用程序。ZooKeeper用于协调元数据和服务发现。Helm图表还支持部署Grafana来可视化指标。这可能听起来很熟悉，因为像Redis、ZooKeeper和Grafana这样的常见构建块的重复使用是我们在本书中几个其他示例中看到的模式。
- en: When model serving tools like KServe are asked to make predictions, they use
    the features stored in Feast as a record of truth. Any updated training by data
    scientists is done using the same feature store, eliminating the need for multiple
    sources of data. The `Transformation` Service provides an optional capability
    to generate new features on demand by performing transformations on existing feature
    data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当像KServe这样的模型服务工具被要求进行预测时，它们使用Feast中存储的特征作为真实记录。数据科学家通过相同的特征存储进行更新训练，消除了多源数据的需求。`Transformation`服务提供了一个可选的能力，通过对现有特征数据执行转换来按需生成新特征。
- en: KServe and Feast are often used together to create a complete real-time model
    serving stack. Feast performs the dynamic part of feature management, working
    with online and offline data storage as new features arrive through streaming
    and data warehouses. KServe handles the dynamic provisioning for the model serving
    by using the serverless capabilities of Knative. This means that when not in use,
    KServe can scale to zero and react when new requests arrive, saving valuable resources
    in your Kubernetes-based AI/ML stack by using only what you need.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: KServe和Feast通常一起使用，以创建完整的实时模型服务堆栈。Feast处理特征管理的动态部分，与在线和离线数据存储一起工作，随着新特征通过流式处理和数据仓库到达。KServe使用Knative的无服务器能力处理模型服务的动态配置。这意味着当不使用时，KServe可以自动缩放到零，并在有新请求到达时作出反应，通过仅使用所需资源来节省在基于Kubernetes的AI/ML堆栈中的宝贵资源。
- en: Vector Similarity Search with Milvus
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Milvus的向量相似性搜索
- en: 'Now that we’ve looked at tools that enable you to use ML models and features
    in production systems, let’s switch gears and look at a different type of AI/ML
    tool: vector similarity search (VSS). As discussed in [“AI/ML Definitions”](#aisolidusml_definitions),
    a vector is a number object representing direction and magnitude from an origin
    in vector space. VSS is an application of vector mathematics in ML. The k-nearest
    neighbors (KNN) algorithm is a way to find how “close” two things are next to
    each other. This algorithm has many variations, but all rely on expressing data
    as a vector. The data to be searched is vectorized using a CPU-intensive KNN-type
    algorithm; typically, this is more of a backend process. VSS servers can then
    index the vector data for less CPU-intensive searching and provide a query mechanism
    that allows end users to provide a vector and find things that are close to it.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了用于在生产系统中使用 ML 模型和特性的工具，让我们换个角度，看看另一种类型的 AI/ML 工具：向量相似性搜索（VSS）。正如在 [“AI/ML
    定义”](#aisolidusml_definitions) 中讨论的那样，向量是在向量空间中从原点表示方向和大小的数值对象。VSS 是在 ML 中应用向量数学的一种应用。k-最近邻（KNN）算法是一种找出两个相邻物体“接近程度”的方法。此算法有许多变体，但都依赖于将数据表达为向量。要搜索的数据使用
    CPU 密集型的 KNN 类型算法进行向量化处理；通常这是一个后端处理过程。VSS 服务器然后可以为向量数据建立索引，以提供 less CPU-intensive
    的搜索查询机制，允许最终用户提供一个向量并找到与之接近的内容。
- en: '*Milvus* is one of many servers designed around the emerging field of VSS.
    Let’s learn how Milvus came to exist and why it’s a great fit for Kubernetes.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*Milvus* 是围绕新兴领域 VSS 设计的众多服务器之一。让我们了解一下 Milvus 的由来及其为 Kubernetes 所适配的原因。'
- en: As Xiaofan mentions, Milvus supports both standalone and clustered deployments,
    using the four layers described. Both models are supported in Kubernetes via Helm,
    with the clustered deployment shown in [Figure 10-4](#deploying_milvus_in_kubernetes).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Xiaofan 所述，Milvus 支持独立部署和集群部署，使用所描述的四个层次。这两种模式在 Kubernetes 中都通过 Helm 支持，集群部署如
    [图 10-4](#deploying_milvus_in_kubernetes) 所示。
- en: The access layer contains the proxy Service, which uses a Kubernetes LoadBalancer
    to route requests from client applications. The services in the coordination layer
    handle incoming search and index queries, routing them to the core server components
    in the worker layer that handle queries and manage data storage and indexing.
    The data nodes manage persistence via files in object storage. The message storage
    uses Apache Pulsar or Apache Kafka to store the stream of incoming data that is
    then passed to data nodes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 访问层包含代理服务，该服务使用 Kubernetes LoadBalancer 从客户端应用程序路由请求。协调层中的服务处理传入的搜索和索引查询，并将其路由到工作层中的核心服务器组件，这些组件处理查询并管理数据存储和索引。数据节点通过对象存储中的文件管理持久性。消息存储使用
    Apache Pulsar 或 Apache Kafka 存储传入数据流，然后将其传递给数据节点。
- en: As you can see, Milvus is designed to be Kubernetes native, with a horizontally
    scalable architecture that makes it well poised to scale up to massive data sets
    including billions or even trillions of vectors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，Milvus 设计为 Kubernetes 原生，具有水平可扩展的架构，非常适合处理包括数十亿甚至数万亿向量在内的大规模数据集。
- en: '![Deploying Milvus in Kubernetes](assets/mcdk_1004.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![在 Kubernetes 中部署 Milvus](assets/mcdk_1004.png)'
- en: Figure 10-4\. Deploying Milvus in Kubernetes
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 在 Kubernetes 中部署 Milvus
- en: Efficient Data Movement with Apache Arrow
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Arrow 进行高效数据移动
- en: 'Now that we have explored an AI/ML Kubernetes stack that helps you manage compute
    resources more efficiently, you might be wondering what can be done with network
    resources. The “fallacies of distributed computing” we discussed in [“Embrace
    Distributed Computing”](ch01.html#embrace_distributed_computing) include two important
    points: the fallacies of believing that bandwidth is infinite and that transport
    cost is zero. Even when compute and storage resources seem much more finite, it’s
    easy to forget how easily you can run out of bandwidth. The deeper you get into
    deploying your data infrastructure into Kubernetes, the more likely it is you
    will find out. Early adopters of Apache Hadoop often shared that as their clusters
    grew, their network switches needed to be replaced with the best that could be
    purchased at the time. Just consider what it takes to sort 10 terabytes of data.
    How about 1 petabyte? You get the idea.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了一个AI/ML Kubernetes堆栈，帮助您更有效地管理计算资源，您可能想知道可以如何处理网络资源。我们在[“拥抱分布式计算”](ch01.html#embrace_distributed_computing)中讨论的“分布式计算谬论”包括两个重要点：误以为带宽是无限的，以及传输成本为零。即使计算和存储资源看起来更有限，很容易忽视带宽会很容易用完。当您深入部署数据基础设施到Kubernetes时，您越可能发现这一点。早期采用Apache
    Hadoop的用户经常分享，随着他们集群的增长，他们的网络交换机需要用当时最好的交换机替换。只需考虑对10TB数据进行排序需要多少资源？1PB呢？您明白了吧。
- en: '*Apache Arrow* is a project that addresses the problem of bandwidth utilization
    by providing a more efficient format. This actually isn’t an unknown approach
    in the history of computer science. IBM introduced [Extended Binary Coded Decimal
    Interchange Code (EBCDIC)](https://oreil.ly/MI488) character encoding to create
    efficiency for the preferred transport of the time: the punch card. Arrow attacks
    the problem of efficiency from the ground up in order to avoid the endless upgrading
    to add more resources, proving that the solution to a control problem is never
    “add more power.” Let’s hear from some experts to learn how this works.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apache Arrow* 是一个项目，通过提供更高效的格式来解决带宽利用问题。实际上，在计算机科学的历史上并不陌生。IBM引入了[扩展二进制编码的十进制互换码（EBCDIC）](https://oreil.ly/MI488)字符编码，以提供当时首选的传输方式——打孔卡片的效率。Arrow从根本上解决了效率问题，避免了不断增加资源的无止境升级，证明了控制问题的解决方案永远不是“增加更多的动力”。让我们听听一些专家的意见，了解这是如何运作的。'
- en: Using Arrow-enabled projects enables you to share data efficiently, reducing
    your resource usage across compute, network, and storage. Example usage of Arrow
    with Spark is shown in [Figure 10-5](#moving_data_with_apache_arrow).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Arrow启用的项目使您能够高效共享数据，减少计算、网络和存储资源的使用。Arrow与Spark的示例用法显示在[图 10-5](#moving_data_with_apache_arrow)中。
- en: '![Moving data with Apache Arrow](assets/mcdk_1005.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![使用Apache Arrow移动数据](assets/mcdk_1005.png)'
- en: Figure 10-5\. Moving data with Apache Arrow
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 使用Apache Arrow移动数据
- en: 'Parquet datafiles containing Arrow-formatted data persisted to object storage
    can be easily loaded without a deserialization step (1). The data can then be
    analyzed by a Spark application (2), including loading directly into a GPU for
    processing where available. The same efficiency level is maintained when passing
    data between Worker Nodes using Arrow Flight (3). The Arrow record batch is sent
    without any intermediate memory copying or serialization, and the receiver can
    reconstruct the Arrow record without memory copy or deserialization. The efficient
    relationship between the remote processes eliminates two things: processing overhead
    for sending data and the efficient Arrow record format that eliminates wasted
    bandwidth.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在对象存储中的Parquet数据文件，其中包含Arrow格式化的数据，可以在不需要反序列化步骤的情况下轻松加载（1）。然后，数据可以通过Spark应用程序进行分析（2），包括直接加载到支持的GPU进行处理。通过Arrow
    Flight（3）在工作节点之间传递数据时，可以保持同样的效率水平。Arrow记录批量被发送，无需任何中间内存复制或序列化，接收方可以在没有内存复制或反序列化的情况下重建Arrow记录。远程进程之间的高效关系消除了两个问题：发送数据的处理开销以及高效的Arrow记录格式，消除了带宽的浪费。
- en: At the scale common in Spark applications, the effect on network latency and
    bandwidth can add up quickly. The network transport savings really keep your data
    moving, even when volumes reach into terabytes and petabytes. [Research](https://oreil.ly/rve9i)
    performed by Tanveer Ahmad at TU Delft showed a 20 to 30 times efficiency gain
    using Arrow Flight to move large volumes of data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark应用程序中常见的规模下，网络延迟和带宽的影响可以迅速累积。网络传输节省确保数据流畅移动，即使数据量达到了TB和PB级别。在TU Delft的Tanveer
    Ahmad进行的[研究](https://oreil.ly/rve9i)表明，使用Arrow Flight移动大容量数据可实现20至30倍的效率提升。
- en: Versioned Object Storage with lakeFS
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带lakeFS的版本化对象存储
- en: Object storage is becoming the standard for cloud native data persistence. It
    lowers the complexity for services but also points to a different way of thinking
    about data mutability. Instead of opening a file and providing random access,
    file storage is precomputed, written once, and read many times. Instead of updating
    a datafile, you write a new one, but how do you distinguish which datafiles are
    current? For this reason, object storage presents issues with disk space management.
    Since there is no concept of managing an entire filesystem, each file is an object
    in a virtually infinite resource.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储正成为云原生数据持久化的标准。它降低了服务的复杂性，同时也指向了一种不同的数据可变性思维方式。与随机访问的文件存储不同，对象存储是预计算的，一次写入，多次读取。而不是更新数据文件，你会写入一个新文件，但如何区分哪些数据文件是当前的？因此，对象存储在磁盘空间管理方面存在问题。由于没有管理整个文件系统的概念，每个文件都是一个在虚拟无限资源中的对象。
- en: Object storage APIs are fairly basic with few frills, but data teams need more
    than just the basics for their use cases. [lakeFS](https://lakefs.io) and [Nessie](https://projectnessie.org)
    are two projects trying to make object storage a better fit for emerging workloads
    on Kubernetes. Let’s examine how lakeFS extends the functionality of object storage
    for cloud native applications.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储API相对基础，功能简单，但数据团队需要更多仅仅基础功能以适应他们的使用场景。[lakeFS](https://lakefs.io)和[Nessie](https://projectnessie.org)是两个项目，旨在使对象存储更适合Kubernetes上新兴工作负载。让我们来看看lakeFS如何扩展云原生应用的对象存储功能。
- en: Using lakeFS in Kubernetes is a great fit because of its stateless design and
    declarative deployment. A Helm deployment consists of [configuring](https://oreil.ly/GhZMB)
    the lakeFS service, which then serves as a communication gateway to and from other
    services.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中使用lakeFS非常合适，因为它具有无状态设计和声明式部署。Helm部署包括[配置](https://oreil.ly/GhZMB)lakeFS服务，该服务随后作为与其他服务之间的通信网关。
- en: Communications into the server emulate S3 object storage, enabling interaction
    with any data store that supports the S3 API. Incoming communication is bound
    as a ClusterIP to serve HTTP traffic across one or more stateless lakeFS server
    Pods managed by a Deployment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与服务器的通信模拟S3对象存储，允许与支持S3 API的任何数据存储交互。入站通信绑定为ClusterIP，用于通过Deployment管理的一个或多个无状态lakeFS服务器Pod提供HTTP流量服务。
- en: lakeFS uses PostgreSQL to manage metadata, so users can either provide the endpoint
    for a running system, as shown in [Figure 10-6](#deploying_lakefs_in_kubernetes),
    or lakeFS can run an embedded PostgreSQL server inside the lakeFS Pod for its
    exclusive use. PostgreSQL is the state management for the stateless lakeFS servers
    when deployed as a cluster.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: lakeFS使用PostgreSQL管理元数据，因此用户可以提供正在运行系统的端点，如[图10-6](#deploying_lakefs_in_kubernetes)所示，或者lakeFS可以在lakeFS
    Pod内运行一个嵌入式PostgreSQL服务器供其独占使用。当部署为集群时，PostgreSQL是无状态lakeFS服务器的状态管理工具。
- en: '![Deploying lakeFS in Kubernetes](assets/mcdk_1006.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![在Kubernetes中部署lakeFS](assets/mcdk_1006.png)'
- en: Figure 10-6\. Deploying lakeFS in Kubernetes
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 在Kubernetes中部署lakeFS
- en: The most important connection is to the object storage endpoints that will store
    the actual data. When users persist data to lakeFS, the actual datafile will pass
    through to the backend object storage, and versioning metadata is stored in PostgreSQL.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的连接是到将存储实际数据的对象存储端点。当用户将数据持久化到lakeFS时，实际数据文件将通过到后端对象存储，并且版本化元数据存储在PostgreSQL中。
- en: The additional outbound connection is for providing orchestration with other
    ML infrastructure. Webhooks allow for triggers on action that alert downstream
    systems when something such as a commit is issued. These triggers serve as a key
    ingredient to automated ML workflows and other applications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的出站连接用于与其他ML基础设施进行协作。Webhook允许在发生操作（如提交）时触发，从而警报下游系统。这些触发器是自动化ML工作流和其他应用的关键组成部分。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: As you can see, the pipeline of new and exciting ways to work with data in Kubernetes
    extends well into the future. New projects are addressing the challenges of advanced
    data workloads according to the cloud native principles of elasticity, scalability,
    and self-healing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，使用 Kubernetes 处理数据的新方法的流水线在未来可延续。新项目正在根据云原生的弹性、可伸缩性和自愈性原则解决先进数据工作负载的挑战。
- en: These tools give you the ability to manage the critical resources of compute,
    network and storage. You can better manage compute-intensive workloads such as
    AI/ML with KServe for the delivery, Feast for model management, and Milvus to
    operationalize new search methods. Network resources are ruled by the simple laws
    of volume and speed, and at the volumes of data we can create, every little bit
    helps. Apache Arrow reduces this volume by creating a common reference frame across
    applications. Unifying around object storage provides further efficiencies, with
    tools like lakeFS making object storage easier to consume in ways that are sympathetic
    to application data storage needs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具赋予你管理计算、网络和存储这些关键资源的能力。你可以通过 KServe 进行计算密集型工作负载的管理，例如 AI/ML 的交付，利用 Feast
    进行模型管理，并使用 Milvus 将新的搜索方法实现操作化。网络资源受到容量和速度的简单法则的支配，在我们可以创建的数据量级别上，每一点进展都有助于减少数据量。Apache
    Arrow 通过在应用程序间创建一个共同的参考框架来降低这一数据量。围绕对象存储的统一提供了进一步的效率，像 lakeFS 这样的工具使对象存储更易于以符合应用程序数据存储需求的方式使用。
- en: At this point, we’ve examined data infrastructure on Kubernetes from mature
    areas like storage all the way out to cutting-edge projects for managing AI/ML
    artifacts such as models and features. Now it’s time to take all the knowledge
    you’ve gained so far and plan to put it into practice.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从像存储这样成熟领域一直到管理 AI/ML 人工智能/机器学习构件的前沿项目，审视了在 Kubernetes 上的数据基础设施。现在是时候将迄今为止获得的所有知识应用于实践。
