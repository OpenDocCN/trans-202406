<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Kubernetes Networking Abstractions"><div class="chapter" id="kubernetes_networking_abstractions">
<h1><span class="label">Chapter 5. </span>Kubernetes Networking Abstractions</h1>


<p>Previously, we covered a swath of networking fundamentals and how traffic in Kubernetes gets from A to B. In
this chapter, we will <a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="overview of" id="ch5_term1"/>discuss networking abstractions in Kubernetes, primarily service discovery and load balancing.
Most notably, this is the chapter on services and ingresses. Both resources are notoriously complex, due to the large
number of options, as they attempt to solve numerous use cases. They are the most visible part of the Kubernetes
network stack, as they define basic network characteristics of workloads on Kubernetes. This is where developers
interact with the networking stack for their applications deployed on Kubernetes.</p>

<p>This chapter will cover fundamental examples of Kubernetes networking abstractions and the details on\f how they work.
To follow along, you will need the following tools:</p>

<ul>
<li>
<p>Docker</p>
</li>
<li>
<p>KIND</p>
</li>
<li>
<p>Linkerd</p>
</li>
</ul>

<p>You will need to be <a data-type="indexterm" data-primary="kubectl commands" data-secondary="guide to" id="idm46219936932808"/><a data-type="indexterm" data-primary="netns command" id="idm46219936931800"/>familiar with the <code>kubectl exec</code> and <code>Docker exec</code> commands. If you are not, our code repo will
have any and all the commands we discuss, so don’t worry too much. We will also make use of <code>ip</code> and <code>netns</code> from
Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.xhtml#linux_networking">2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.xhtml#container_networking_basics">3</a>. Note that most of these tools are for debugging and showing implementation details; you will not
necessarily need them during normal operations.</p>

<p>Docker, <a data-type="indexterm" data-primary="KIND (Kubernetes in Docker) cluster" id="idm46219936926200"/><a data-type="indexterm" data-primary="YAML configuration file" id="idm46219936925400"/>KIND, and Linkerd installs are available on their respective sites, and we’ve provided more information in the
book’s code repository as well.</p>
<div data-type="tip"><h6>Tip</h6>
<p><code>kubectl</code> is a key tool in this chapter’s examples, and it’s the standard for operators to interact with clusters and their networks.
You should be familiar with the <code>kubectl create</code>, <code>apply</code>, <code>get</code>, 
<span class="keep-together"><code>delete</code></span>, and <code>exec</code> commands.
Learn more in the <a href="https://oreil.ly/H8bTU">Kubernetes documentation</a> or run <code>kubectl [command] --help</code>.</p>
</div>

<p>This chapter will explore these Kubernetes networking abstractions:</p>

<ul>
<li>
<p>StatefulSets</p>
</li>
<li>
<p>Endpoints</p>

<ul>
<li>
<p>Endpoint slices</p>
</li>
</ul>
</li>
<li>
<p>Services</p>

<ul>
<li>
<p>NodePort</p>
</li>
<li>
<p>Cluster</p>
</li>
<li>
<p>Headless</p>
</li>
<li>
<p>External</p>
</li>
<li>
<p>LoadBalancer</p>
</li>
</ul>
</li>
<li>
<p>Ingress</p>

<ul>
<li>
<p>Ingress controller</p>
</li>
<li>
<p>Ingress rules</p>
</li>
</ul>
</li>
<li>
<p>Service meshes</p>

<ul>
<li>
<p>Linkerd</p>
</li>
</ul>
</li>
</ul>

<p>To explore these abstractions, we will deploy the examples to our Kubernetes cluster with the following steps:</p>
<ol>
<li>
<p>Deploy a KIND cluster with ingress enabled.</p>
</li>
<li>
<p>Explore StatefulSets.</p>
</li>
<li>
<p>Deploy Kubernetes services.</p>
</li>
<li>
<p>Deploy an ingress controller.</p>
</li>
<li>
<p>Deploy a Linkerd service mesh.</p>
</li>

</ol>

<p>These abstractions are at the heart of what the Kubernetes API provides to developers and administrators to
programmatically control the flow of communications into and out of the cluster. Understanding and mastering how to deploy
these abstractions is crucial for the success of any workload inside a cluster. After working through these examples, you
will understand which abstractions to use in certain situations for your applications.</p>

<p>With the KIND cluster configuration YAML, we can use KIND to create that cluster with the command in the next section. If this is
the first time running it, it will take some time to download all the Docker images for the working and control plane
Docker images.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The following examples assume that you still have the local KIND cluster running from the previous chapter, along with
the Golang web server and the <code>dnsutils</code> images for <a data-type="indexterm" data-startref="ch5_term1" id="idm46219936865416"/>testing.</p>
</div>






<section data-type="sect1" data-pdf-bookmark="StatefulSets"><div class="sect1" id="statefulsets">
<h1>StatefulSets</h1>

<p>StatefulSets are a <a data-type="indexterm" data-primary="StatefulSets, Kubernetes" id="ch5_term2"/><a data-type="indexterm" data-primary="deployment resource, Kubernetes" data-secondary="versus StatefulSets" data-secondary-sortas="StatefulSets" id="idm46219936861048"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="StatefulSets" id="ch5_term3"/>workload abstraction in Kubernetes to manage pods like you would a deployment. Unlike a
deployment, StatefulSets add the following features for applications that require them:</p>

<ul>
<li>
<p>Stable, unique network identifiers</p>
</li>
<li>
<p>Stable, persistent storage</p>
</li>
<li>
<p>Ordered, graceful deployment and scaling</p>
</li>
<li>
<p>Ordered, automated rolling updates</p>
</li>
</ul>

<p>The deployment resource is better suited for applications that do not have these requirements
(for example, a service that stores data in an external database).</p>

<p>Our database <a data-type="indexterm" data-primary="database, Postgres" id="idm46219936853640"/><a data-type="indexterm" data-primary="Postgres database" id="idm46219936852904"/><a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="StatefulSets on" id="idm46219936852232"/>for the Golang minimal web server uses a StatefulSet. The database has a service, a ConfigMap for the
Postgres username, a password, a test database name, and a StatefulSet for the containers running Postgres.</p>

<p>Let’s deploy it now:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f database.yaml
service/postgres created
configmap/postgres-config created
statefulset.apps/postgres created</pre>

<p>Let’s examine the <a data-type="indexterm" data-primary="DNS (Domain Name System)" data-secondary="StatefulSets and" id="idm46219936826664"/>DNS and network ramifications of using a StatefulSet.</p>

<p>To test DNS <a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="with StatefulSets" data-secondary-sortas="StatefulSets" id="idm46219936825304"/><a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="image" id="idm46219936848856"/>inside the cluster, we can use the <code>dnsutils</code> image; this image is <code>gcr
.io/kubernetes-e2e-test-images/dnsutils:1.3</code> and is used for Kubernetes testing:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f dnsutils.yaml

pod/dnsutils created

kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
dnsutils   1/1     Running   <code class="m">0</code>          9s</pre>

<p>With the replica configured with two pods, we see the StatefulSet deploy <code>postgres-0</code> and <code>postgres-1</code>, in that order, a
feature of StatefulSets with IP address 10.244.1.3 and 10.244.2.3, respectively:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP           NODE
dnsutils     1/1     Running   <code class="m">0</code>          15m   10.244.3.2   kind-worker3
postgres-0   1/1     Running   <code class="m">0</code>          15m   10.244.1.3   kind-worker2
postgres-1   1/1     Running   <code class="m">0</code>          14m   10.244.2.3   kind-worker</pre>

<p>Here is the name of our <a data-type="indexterm" data-primary="headless services, Kubernetes" id="idm46219936831992"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="headless" id="idm46219936831384"/>headless service, Postgres, that the client can use for queries to return the endpoint IP
addresses:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get svc postgres
NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<code class="o">(</code>S<code class="o">)</code>    AGE
postgres   ClusterIP                    &lt;none&gt;        5432/TCP   23m</pre>

<p>Using our <code>dnsutils</code> image, we can see that the DNS names for the StatefulSets will return those IP addresses along
with the cluster IP address of the Postgres service:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>dnsutils -- host postgres-0.postgres.default.svc.cluster.local.
postgres-0.postgres.default.svc.cluster.local has address 10.244.1.3

kubectl <code class="nb">exec </code>dnsutils -- host postgres-1.postgres.default.svc.cluster.local.
postgres-1.postgres.default.svc.cluster.local has address 10.244.2.3

kubectl <code class="nb">exec </code>dnsutils -- host postgres
postgres.default.svc.cluster.local has address 10.105.214.153</pre>

<p>StatefulSets attempt to mimic a fixed group of persistent machines. As a generic solution for stateful workloads,
specific behavior may be frustrating in specific use cases.</p>

<p>A common problem that users encounter is an update requiring manual intervention to fix when using <code>.spec
.updateStrategy.type: RollingUpdate</code>, and <code>.spec.podManagementPolicy: OrderedReady</code>, both of which are default settings.
With these settings, a user must manually intervene if an updated pod never becomes ready.</p>

<p>Also, StatefulSets require a service, preferably headless, to be responsible for the network identity of the pods, and
end users are responsible for creating this service.</p>

<p>Statefulsets have many configuration options, and many third-party alternatives exist (both generic stateful workload
controllers and software-specific workload 
<span class="keep-together">controllers).</span></p>

<p>StatefulSets offer functionality for a specific use case in Kubernetes. They should not be used for everyday
application deployments. <a data-type="indexterm" data-startref="ch4_term2" id="idm46219936801528"/><a data-type="indexterm" data-startref="ch4_term3" id="idm46219936778840"/>Later in this section, we will discuss more appropriate networking abstractions for
run-of-the-mill deployments.</p>

<p>In our next section, we will explore endpoints and endpoint slices, the backbone of Kubernetes services.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Endpoints"><div class="sect1" id="idm46219936863352">
<h1>Endpoints</h1>

<p>Endpoints help <a data-type="indexterm" data-primary="endpoints, Kubernetes" id="ch5_term4"/><a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="as service discovery tool" data-secondary-sortas="service discovery tool" id="ch5_term5"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="endpoints" id="ch5_term6"/>identify what pods are running for the service it powers. Endpoints are created and managed by
services. We will discuss services on their own later, to avoid covering too many new things at once. For now, let’s
just say that a service contains a standard label selector (introduced in <a data-type="xref" href="ch04.xhtml#kubernetes_networking_introduction">Chapter 4</a>), which defines which pods are
in the endpoints.</p>

<p>In <a data-type="xref" href="#img-endpoints">Figure 5-1</a>, we can see traffic being directed to an endpoint on node 2, pod 5.</p>

<figure><div id="img-endpoints" class="figure">
<img src="Images/neku_0501.png" alt="Kubernetes Endpoints" width="1231" height="960"/>
<h6><span class="label">Figure 5-1. </span>Endpoints in a service</h6>
</div></figure>

<p>Let’s discuss how this endpoint is created and maintained in the cluster.</p>

<p>Each endpoint <a data-type="indexterm" data-primary="ports" data-secondary="endpoints and" id="idm46219936746968"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="of endpoints" data-secondary-sortas="endpoints" id="idm46219936729448"/><a data-type="indexterm" data-primary="pods" data-secondary="and endpoints/endpointslices" data-secondary-sortas="endpoints/endpointslices" id="ch5_term7"/>contains a list of ports (which apply to all pods)
and two lists of addresses: ready and unready:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Endpoints</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">labels</code><code class="p">:</code>
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-endpoints</code>
<code class="nt">subsets</code><code class="p">:</code>
<code class="p-Indicator">-</code> <code class="nt">addresses</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">ip</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.0.1</code>
<code class="p-Indicator">-</code> <code class="nt">notReadyAddresses</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">ip</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.0.2</code>
  <code class="nt">ports</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
    <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code></pre>

<p>Addresses are <a data-type="indexterm" data-primary="addresses, endpoints and" id="idm46219936626392"/>listed in <code>.addresses</code> if they are passing pod readiness checks.
Addresses are listed in <code>.notReadyAddresses</code> if they are not.
This makes endpoints a <em>service discovery</em> tool,
where you can <a data-type="indexterm" data-primary="Endpoints objects" id="idm46219936667496"/>watch an <code>Endpoints</code> object to see the health and addresses of all pods:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get endpoints clusterip-service
NAME                ENDPOINTS
clusterip-service   10.244.1.5:8080,10.244.2.7:8080,10.244.2.8:8080 + <code class="m">1</code> more...</pre>

<p>We can get a better view of all the addresses with <code>kubectl describe</code>:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl describe endpoints clusterip-service
Name:         clusterip-service
Namespace:    default
Labels:       <code class="nv">app</code><code class="o">=</code>app
Annotations:  endpoints.kubernetes.io/last-change-trigger-time:
2021-01-30T18:51:36Z
Subsets:
  Addresses:          10.244.1.5,10.244.2.7,10.244.2.8,10.244.3.9
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    &lt;<code class="nb">unset</code>&gt;  <code class="m">8080</code>  TCP

Events:
  Type     Reason                  Age   From                 Message
  ----     ------                  ----  ----                 -------</pre>

<p>Let’s remove the app label and see how Kubernetes responds. In a separate terminal, run this command. This will allow
us to see changes to the pods in real time:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods -w</pre>

<p>In another separate terminal, let’s do the same thing with endpoints:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get endpoints -w</pre>

<p>We now need to get a pod name to remove from the <code>Endpoints</code> object:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl get pods -l <code class="nv">app</code><code class="o">=</code>app -o wide
NAME                  READY   STATUS   RESTARTS  AGE  IP           NODE
app-5586fc9d77-7frts  1/1     Running  <code class="m">0</code>         19m  10.244.1.5   kind-worker2
app-5586fc9d77-mxhgw  1/1     Running  <code class="m">0</code>         19m  10.244.3.9   kind-worker3
app-5586fc9d77-qpxwk  1/1     Running  <code class="m">0</code>         20m  10.244.2.7   kind-worker
app-5586fc9d77-tpz8q  1/1     Running  <code class="m">0</code>         19m  10.244.2.8   kind-worker</pre>

<p>With <code>kubectl label</code>, we can alter the pod’s <code>app-5586fc9d77-7frts</code> <code>app=app</code> label:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl label pod app-5586fc9d77-7frts <code class="nv">app</code><code class="o">=</code>nope --overwrite
pod/app-5586fc9d77-7frts labeled</pre>

<p>Both watch commands on endpoints and pods will see some changes for the same reason: removal of the label on the pod.
The <a data-type="indexterm" data-primary="deployment resource, Kubernetes" data-secondary="endpoint controller and" id="idm46219936519912"/>endpoint controller will notice a change to the pods with the label <code>app=app</code> and so did the deployment controller.
So Kubernetes did what Kubernetes does: it made the real state reflect the desired state:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods -w
NAME                   READY   STATUS             RESTARTS   AGE
app-5586fc9d77-7frts   1/1     Running            <code class="m">0</code>          21m
app-5586fc9d77-mxhgw   1/1     Running            <code class="m">0</code>          21m
app-5586fc9d77-qpxwk   1/1     Running            <code class="m">0</code>          22m
app-5586fc9d77-tpz8q   1/1     Running            <code class="m">0</code>          21m
dnsutils               1/1     Running            <code class="m">3</code>          3h1m
postgres-0             1/1     Running            <code class="m">0</code>          3h
postgres-1             1/1     Running            <code class="m">0</code>          3h
app-5586fc9d77-7frts   1/1     Running            <code class="m">0</code>          22m
app-5586fc9d77-7frts   1/1     Running            <code class="m">0</code>          22m
app-5586fc9d77-6dcg2   0/1     Pending            <code class="m">0</code>          0s
app-5586fc9d77-6dcg2   0/1     Pending            <code class="m">0</code>          0s
app-5586fc9d77-6dcg2   0/1     ContainerCreating  <code class="m">0</code>          0s
app-5586fc9d77-6dcg2   0/1     Running            <code class="m">0</code>          2s
app-5586fc9d77-6dcg2   1/1     Running            <code class="m">0</code>          7s</pre>

<p>The deployment has four pods, but our relabeled pod still exists: <code>app-5586fc9d77-7frts</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
app-5586fc9d77-6dcg2   1/1     Running   <code class="m">0</code>          4m51s
app-5586fc9d77-7frts   1/1     Running   <code class="m">0</code>          27m
app-5586fc9d77-mxhgw   1/1     Running   <code class="m">0</code>          27m
app-5586fc9d77-qpxwk   1/1     Running   <code class="m">0</code>          28m
app-5586fc9d77-tpz8q   1/1     Running   <code class="m">0</code>          27m
dnsutils               1/1     Running   <code class="m">3</code>          3h6m
postgres-0             1/1     Running   <code class="m">0</code>          3h6m
postgres-1             1/1     Running   <code class="m">0</code>          3h6m</pre>

<p>The pod <code>app-5586fc9d77-6dcg2</code> now is part of the deployment and endpoint object with IP address <code>10.244.1.6</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods app-5586fc9d77-6dcg2 -o wide
NAME                  READY  STATUS   RESTARTS  AGE   IP           NODE
app-5586fc9d77-6dcg2  1/1    Running  <code class="m">0</code>         3m6s  10.244.1.6   kind-worker2</pre>

<p>As always, we can see the full picture of details with <code>kubectl describe</code>:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl describe endpoints clusterip-service
Name:         clusterip-service
Namespace:    default
Labels:       <code class="nv">app</code><code class="o">=</code>app
Annotations:  endpoints.kubernetes.io/last-change-trigger-time:
2021-01-30T19:14:23Z
Subsets:
  Addresses:          10.244.1.6,10.244.2.7,10.244.2.8,10.244.3.9
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    &lt;<code class="nb">unset</code>&gt;  <code class="m">8080</code>  TCP

Events:
  Type     Reason                  Age   From                 Message
  ----     ------                  ----  ----                 -------</pre>

<p>For large deployments, that <a data-type="indexterm" data-primary="Endpoints objects" id="idm46219936416232"/>endpoint object can become very large, so much so that it can actually slow down changes
in the cluster. To solve that issue, the Kubernetes maintainers have come up with <a data-type="indexterm" data-startref="ch5_term4" id="idm46219936415304"/><a data-type="indexterm" data-startref="ch5_term5" id="idm46219936414632"/><a data-type="indexterm" data-startref="ch5_term6" id="idm46219936446104"/>endpoint slices.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Endpoint Slices"><div class="sect1" id="idm46219936776488">
<h1>Endpoint Slices</h1>

<p>You may be asking, how are they <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="versus endpointslices" data-secondary-sortas="endpointslices" id="ch5_term8"/><a data-type="indexterm" data-primary="endpointslices, Kubernetes" data-secondary="versus EndPoints" data-secondary-sortas="EndPoints" id="ch5_term9"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="endpointslices" id="ch5_term10"/><a data-type="indexterm" data-primary="endpointslices, Kubernetes" id="ch5_term11"/>different from endpoints? This is where we <em>really</em> start to get into the weeds of
Kubernetes networking.</p>

<p>In a typical cluster, Kubernetes <a data-type="indexterm" data-primary="Kubernetes" data-secondary="routing tools of" id="idm46219936460744"/><a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="endpoints/endpointslices and" id="idm46219936459768"/>runs <code>kube-proxy</code> on every node. <code>kube-proxy</code> is responsible for the per-node portions
of making services work, by handling routing and <em>outbound</em> load balancing to all the pods in a service. To do that,
<code>kube-proxy</code> watches all endpoints in the cluster so it knows all applicable pods that all services should route to.</p>

<p>Now, imagine <a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="bottleneck with" id="idm46219936456280"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="issues with" id="idm46219936455272"/>we have a <em>big</em> cluster, with thousands of nodes, and tens of thousands of pods. That means thousands of
kube-proxies are watching endpoints. When an address changes in an <code>Endpoints</code> object (say, from a rolling update,
scale up, eviction, health-check failure, or any number of reasons), the updated <code>Endpoints</code> object is pushed to all
listening kube-proxies. It is made worse by the number of pods, since more pods means larger <code>Endpoints</code> objects,
and more frequent changes. This eventually becomes a strain on <code>etcd</code>, the Kubernetes API server, and the network itself.
Kubernetes scaling limits are complex and depend on specific criteria, but endpoints watching is a common problem in
clusters that have thousands of nodes. Anecdotally, many Kubernetes users consider endpoint watches to be the
ultimate bottleneck of cluster size.</p>

<p>This problem is a function of <code>kube-proxy</code>’s design  and the expectation that any pod should be immediately able to
route to any service with no notice. <a data-type="indexterm" data-primary="endpointslices, Kubernetes" data-secondary="with kube-proxy" data-secondary-sortas="kube-proxy" id="idm46219936371976"/>Endpoint slices are an approach that allows <code>kube-proxy</code>’s fundamental design to
continue, while drastically reducing the watch bottleneck in large clusters where large services are used.</p>

<p>Endpoint slices <a data-type="indexterm" data-primary="Endpoints objects" id="idm46219936369528"/>have similar contents to <code>Endpoints</code> objects but also include an array of endpoints:</p>

<pre data-type="programlisting" data-code-language="yaml" class="less_space pagebreak-before"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">discovery.k8s.io/v1beta1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">EndpointSlice</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-slice-1</code>
  <code class="nt">labels</code><code class="p">:</code>
    <code class="nt">kubernetes.io/service-name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo</code>
<code class="nt">addressType</code><code class="p">:</code> <code class="l-Scalar-Plain">IPv4</code>
<code class="nt">ports</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">http</code>
    <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>
    <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>
<code class="nt">endpoints</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">addresses</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="s">"10.0.0.1"</code>
    <code class="nt">conditions</code><code class="p">:</code>
      <code class="nt">ready</code><code class="p">:</code> <code class="l-Scalar-Plain">true</code></pre>

<p>The meaningful difference between endpoints and endpoint slices is not the schema, but how Kubernetes treats them.
With “regular” endpoints, a Kubernetes service creates one endpoint for all pods in the service.
A service creates <em>multiple</em> endpoint slices, each containing a <em>subset</em> of pods; <a data-type="xref" href="#img-endpointslice">Figure 5-2</a> depicts this subset.
The union of all endpoint slices for a service contains all pods in the service.
This way, an IP address change (due to a new pod, a deleted pod,
or a pod’s health changing) will result in a much smaller data transfer to watchers.
Because Kubernetes doesn’t have a transactional API,
the same address may appear temporarily in multiple slices.
Any code consuming endpoint slices (such as <code>kube-proxy</code>) must be able to account for this.</p>

<p>The maximum <a data-type="indexterm" data-primary="maxendpoints-per-slice kube-controller-manager" id="idm46219936300936"/>number of addresses in an endpoint slice is set using the <code>--max-endpoints-per-slice</code>
<code>kube-controller-manager</code> flag.
The current default is 100, and the maximum is 1000.
The endpoint slice controller attempts to fill existing endpoint slices before creating new ones,
but does not rebalance endpoint slice.</p>

<p>The endpoint <a data-type="indexterm" data-primary="mirroring of endpoints/endpointslices" id="idm46219936298616"/>slice controller mirrors endpoints to endpoint slice, to allow systems to continue writing endpoints
while treating endpoint slice as the source of truth. The exact future of this behavior, and endpoints in general,
has not been finalized (however, as a v1 resource, endpoints would be sunset with substantial notice).
There are four <a data-type="indexterm" data-primary="Endpoints objects" id="idm46219936297336"/>exceptions that will prevent mirroring:</p>

<ul>
<li>
<p>There is no corresponding service.</p>
</li>
<li>
<p>The corresponding service resource selects pods.</p>
</li>
<li>
<p>The <code>Endpoints</code> object has the label <code>endpointslice.kubernetes.io/skip-mirror: true</code>.</p>
</li>
<li>
<p>The <code>Endpoints</code> object has the annotation <code> control-⁠⁠plane.alpha.kubernetes​​.io/leader</code>.</p>
</li>
</ul>

<figure><div id="img-endpointslice" class="figure">
<img src="Images/neku_0502.png" alt="EndpointsVSliice" width="1011" height="887"/>
<h6><span class="label">Figure 5-2. </span><code>Endpoints</code> versus <code>EndpointSlice</code> objects</h6>
</div></figure>

<p>You can fetch all endpoint slices for a specific service
by fetching endpoint slices filtered to the desired name in <code>.metadata.labels."kubernetes.io/service-name"</code>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Endpoint slices <a data-type="indexterm" data-primary="Kubernetes" data-secondary="beta resources of" id="idm46219936285912"/>have been in beta state since Kubernetes 1.17.
This is still the case in Kubernetes 1.20, the current version at the time of writing.
Beta resources typically don’t see major changes, and eventually graduate to stable APIs,
but that is not guaranteed.
If you directly use endpoint slices,
be aware that a future Kubernetes release may make a breaking change without much warning,
or the behaviors described here may change.</p>
</div>

<p>Let’s see some endpoints running in the cluster with <code>kubectl get endpointslice</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get endpointslice
NAME                      ADDRESSTYPE   PORTS   ENDPOINTS
clusterip-service-l2n9q   IPv4          <code class="m">8080</code>    10.244.2.7,10.244.2.8,10.244.1.5
+ <code class="m">1</code> more...</pre>

<p class="pagebreak-before">If we want more detail about the endpoint slices <code>clusterip-service-l2n9q</code>, we can use <code>kubectl describe</code> on it:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl describe endpointslice clusterip-service-l2n9q
Name:         clusterip-service-l2n9q
Namespace:    default
Labels:
endpointslice.kubernetes.io/managed-by<code class="o">=</code>endpointslice-controller.k8s.io
kubernetes.io/service-name<code class="o">=</code>clusterip-service
Annotations:  endpoints.kubernetes.io/last-change-trigger-time:
2021-01-30T18:51:36Z
AddressType:  IPv4
Ports:
  Name     Port  Protocol
  ----     ----  --------
  &lt;<code class="nb">unset</code>&gt;  <code class="m">8080</code>  TCP
Endpoints:
  - Addresses:  10.244.2.7
    Conditions:
      Ready:    <code class="nb">true</code>
<code class="nb">    </code>Hostname:   &lt;<code class="nb">unset</code>&gt;
    TargetRef:  Pod/app-5586fc9d77-qpxwk
    Topology:   kubernetes.io/hostname<code class="o">=</code>kind-worker
  - Addresses:  10.244.2.8
    Conditions:
      Ready:    <code class="nb">true</code>
<code class="nb">    </code>Hostname:   &lt;<code class="nb">unset</code>&gt;
    TargetRef:  Pod/app-5586fc9d77-tpz8q
    Topology:   kubernetes.io/hostname<code class="o">=</code>kind-worker
  - Addresses:  10.244.1.5
    Conditions:
      Ready:    <code class="nb">true</code>
<code class="nb">    </code>Hostname:   &lt;<code class="nb">unset</code>&gt;
    TargetRef:  Pod/app-5586fc9d77-7frts
    Topology:   kubernetes.io/hostname<code class="o">=</code>kind-worker2
  - Addresses:  10.244.3.9
    Conditions:
      Ready:    <code class="nb">true</code>
<code class="nb">    </code>Hostname:   &lt;<code class="nb">unset</code>&gt;
    TargetRef:  Pod/app-5586fc9d77-mxhgw
    Topology:   kubernetes.io/hostname<code class="o">=</code>kind-worker3
Events:         &lt;none&gt;</pre>

<p class="pagebreak-before">In the output, we see the <a data-type="indexterm" data-primary="TargetRef and endpoint slice" id="idm46219936199336"/>pod powering the endpoint slice from <code>TargetRef</code>. The <code>Topology</code> information gives us the hostname
of the worker node that the pod is deployed to. Most <a data-type="indexterm" data-primary="addresses output, Kubernetes" id="idm46219936197688"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="of endpoints" data-secondary-sortas="endpoints" id="idm46219936196968"/>importantly, the <code>Addresses</code> returns the IP address of the
endpoint object.</p>

<p>Endpoints and endpoint slices are important to understand because they identify the pods responsible for the services,
no matter the type deployed. <a data-type="indexterm" data-startref="ch5_term7" id="idm46219936186408"/><a data-type="indexterm" data-startref="ch5_term8" id="idm46219936185704"/><a data-type="indexterm" data-startref="ch5_term9" id="idm46219936185032"/><a data-type="indexterm" data-startref="ch5_term10" id="idm46219936184360"/><a data-type="indexterm" data-startref="ch5_term11" id="idm46219936183688"/>Later in the chapter, we’ll review how to use endpoints and labels for troubleshooting. Next,
we will investigate all the Kubernetes service types.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Kubernetes Services"><div class="sect1" id="idm46219936444392">
<h1>Kubernetes Services</h1>

<p>A service in Kubernetes is a <a data-type="indexterm" data-primary="load balancing" data-secondary="with Kubernetes services" data-secondary-sortas="Kubernetes services" id="idm46219936181368"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="services overview" id="ch5_term12"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="overview of" id="ch5_term13"/><a data-type="indexterm" data-primary="services, Kubernetes" id="ch5_term14"/>load balancing abstraction within a cluster.  There are four types of services,
specified by the <code>.spec.Type</code> field. Each type offers a different form of load balancing or discovery, which we will
cover individually. The four types are: ClusterIP, NodePort, LoadBalancer, and ExternalName.</p>

<p>Services use a <a data-type="indexterm" data-primary="pod selector" id="idm46219936175528"/><a data-type="indexterm" data-primary="pods" data-secondary="and services" data-secondary-sortas="services" id="ch5_term15"/>standard pod selector to match pods.
The service includes all matching pods.
Services create an endpoint (or endpoint slice) to handle pod discovery:</p>

<pre data-type="programlisting" data-code-language="bash">apiVersion: v1
kind: Service
metadata:
  name: demo-service
spec:
  selector:
    app: demo</pre>

<p>We will use the Golang minimal web server for all the service examples. We have added functionality to the
application to display the host and pod IP addresses in the REST request.</p>

<p><a data-type="xref" href="#img-pod-connection">Figure 5-3</a> outlines our pod networking status as a single pod in a cluster. The networking objects we are about
to explore will expose our app pods outside the cluster in some instances and in others allow us to scale our
application to meet demand. <a data-type="indexterm" data-primary="namespaces, network" data-secondary="pods sharing of" id="idm46219936141112"/><a data-type="indexterm" data-primary="namespaces, network" data-secondary="pause container for" id="idm46219936140168"/><a data-type="indexterm" data-primary="pause container, Kubernetes" id="idm46219936139224"/>Recall from Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.xhtml#container_networking_basics">3</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#kubernetes_networking_introduction">4</a> that containers running inside pods share a network
namespace. In addition, there is also a pause container that is created for each pod. The pause container manages the namespaces for the pod.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The pause container is the parent container for all running  containers in the pod. It holds and shares all the namespaces for the pod. You can <a data-type="indexterm" data-primary="Lewis, Ian" id="idm46219936166792"/>read more about the pause container in Ian Lewis’ <a href="https://oreil.ly/n51eq">blog post</a>.</p>
</div>

<figure><div id="img-pod-connection" class="figure">
<img src="Images/neku_0503.png" alt="Pod on Host" width="663" height="894"/>
<h6><span class="label">Figure 5-3. </span>Pod on host</h6>
</div></figure>

<p>Before we deploy the services, we must first deploy the web server that the services will be routing traffic to, if
we have not already:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl apply -f web.yaml
deployment.apps/app created

kubectl get pods -o wide
NAME                  READY  STATUS    RESTARTS  AGE  IP           NODE
app-9cc7d9df8-ffsm6   1/1    Running   <code class="m">0</code>         49s  10.244.1.4   kind-worker2
dnsutils              1/1    Running   <code class="m">0</code>         49m  10.244.3.2   kind-worker3
postgres-0            1/1    Running   <code class="m">0</code>         48m  10.244.1.3   kind-worker2
postgres-1            1/1    Running   <code class="m">0</code>         48m  10.244.2.3   kind-worker</pre>

<p>Let’s look <a data-type="indexterm" data-startref="ch5_term12" id="idm46219936157512"/><a data-type="indexterm" data-startref="ch5_term13" id="idm46219936156904"/><a data-type="indexterm" data-startref="ch5_term15" id="idm46219936155800"/>at each type of service starting with NodePort.</p>








<section data-type="sect2" data-pdf-bookmark="NodePort"><div class="sect2" id="idm46219936154872">
<h2>NodePort</h2>

<p>A NodePort <a data-type="indexterm" data-primary="NodePort services, Kubernetes" id="ch5_term16"/><a data-type="indexterm" data-primary="ports" data-secondary="with NodePort Service" data-secondary-sortas="NodePort Service" id="ch5_term17"/><a data-type="indexterm" data-primary="routing" data-secondary="with NodePort Service" data-secondary-sortas="NodePort Service" id="ch5_term18"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="NodePort" id="ch5_term58"/>service provides a simple way for external software,  such as a load balancer, to route traffic to the pods. The software only needs to be aware of node IP addresses, and the service’s port(s). A NodePort service exposes a fixed port
on all nodes, which routes to applicable pods. A NodePort service uses the <code>.spec.ports.[].nodePort</code> field to specify
the port to open on all nodes, for the corresponding port on pods:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-service</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">NodePort</code>
  <code class="nt">selector</code><code class="p">:</code>
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">demo</code>
  <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>
      <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>
      <code class="nt">nodePort</code><code class="p">:</code> <code class="l-Scalar-Plain">30000</code></pre>

<p>The <code>nodePort</code> field can be left blank, in which case Kubernetes  automatically selects a unique port.
The <code>--service-node-port-range</code> flag in 
<span class="keep-together"><code>kube-controller-manager</code></span> sets the valid range for ports, 30000–32767.
Manually specified ports must be within this range.</p>

<p>Using a NodePort <a data-type="indexterm" data-primary="routing" data-secondary="for external traffic" data-secondary-sortas="external traffic" id="ch5_term19"/>service, external users can connect to the nodeport on any node and be routed to a pod on a node
that has a pod backing that service; <a data-type="xref" href="#img-node-port-traffic">Figure 5-4</a> demonstrates this. The service directs traffic to node 3, and
<code>iptables</code> rules forward the traffic to node 2 hosting the pod. This is a bit inefficient, as a typical connection will
be routed to a pod on another node.</p>

<figure><div id="img-node-port-traffic" class="figure">
<img src="Images/neku_0504.png" alt="Node Port Traffic Flow" width="1140" height="968"/>
<h6><span class="label">Figure 5-4. </span>NodePort traffic flow</h6>
</div></figure>

<p><a data-type="xref" href="#img-node-port-traffic">Figure 5-4</a> requires us to discuss an <a data-type="indexterm" data-primary="externalTrafficPolicy routing" id="idm46219936048824"/>attribute of services, externalTrafficPolicy. ExternalTrafficPolicy indicates
how a service will <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="local versus cluster routing to" id="idm46219936047944"/>route external traffic to either node-local or cluster-wide endpoints. <code>Local</code> preserves the client
source IP and avoids a second hop for LoadBalancer and NodePort type services but risks potentially imbalanced
traffic spreading. Cluster obscures the client source IP and may cause a second hop to another node but should
have good overall load-spreading. A <a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="iptable routing rules of" id="idm46219936046216"/><code>Cluster</code> value means that for each worker node, the <code>kube-proxy iptable</code> rules are
set up to route the traffic to the pods backing the service anywhere in the cluster, just like we have shown in
<a data-type="xref" href="#img-node-port-traffic">Figure 5-4</a>.</p>

<p>A <code>Local</code> value means the <code>kube-proxy iptable</code> rules are set up only on the worker nodes with relevant pods running to
route the traffic local to the worker node. Using <code>Local</code> also allows application developers to preserve the source IP
of the user request. If <a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="endpoints/endpointslices and" id="idm46219936041416"/>you set externalTrafficPolicy to the value <code>Local</code>, <code>kube-proxy</code> will proxy requests only to
node-local endpoints and will not forward traffic to other nodes. If there are no local endpoints, packets sent to
the node are dropped.</p>

<p>Let’s scale up the deployment of our web app for some more testing:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl scale deployment app --replicas 4
deployment.apps/app scaled

 kubectl get pods -l <code class="nv">app</code><code class="o">=</code>app -o wide
NAME                  READY   STATUS      IP           NODE
app-9cc7d9df8-9d5t8   1/1     Running     10.244.2.4   kind-worker
app-9cc7d9df8-ffsm6   1/1     Running     10.244.1.4   kind-worker2
app-9cc7d9df8-srxk5   1/1     Running     10.244.3.4   kind-worker3
app-9cc7d9df8-zrnvb   1/1     Running     10.244.3.5   kind-worker3</pre>

<p>With four pods running, we will have one pod at every node in the cluster:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl get pods -o wide -l <code class="nv">app</code><code class="o">=</code>app
NAME                   READY   STATUS     IP           NODE
app-5586fc9d77-7frts   1/1     Running    10.244.1.5   kind-worker2
app-5586fc9d77-mxhgw   1/1     Running    10.244.3.9   kind-worker3
app-5586fc9d77-qpxwk   1/1     Running    10.244.2.7   kind-worker
app-5586fc9d77-tpz8q   1/1     Running    10.244.2.8   kind-worker</pre>

<p>Now let’s deploy our NodePort service:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f services-nodeport.yaml
service/nodeport-service created

kubectl describe svc nodeport-service
Name:                     nodeport-service
Namespace:                default
Labels:                   &lt;none&gt;
Annotations:              Selector:  <code class="nv">app</code><code class="o">=</code>app
Type:                     NodePort
IP:                       10.101.85.57
Port:                     <code class="nb">echo  </code>8080/TCP
TargetPort:               8080/TCP
NodePort:                 <code class="nb">echo  </code>30040/TCP
Endpoints:                10.244.1.5:8080,10.244.2.7:8080,10.244.2.8:8080
+ <code class="m">1</code> more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;</pre>

<p>To test the <a data-type="indexterm" data-primary="nodes" data-secondary="worker nodes as" id="idm46219936032936"/><a data-type="indexterm" data-primary="worker nodes, Kubernetes" id="idm46219936032280"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with NodePort service" data-secondary-sortas="NodePort service" id="idm46219936031640"/>NodePort service, we must retrieve the IP address of a worker node:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get nodes -o wide
NAME                 STATUS   ROLES   INTERNAL-IP OS-IMAGE
kind-control-plane   Ready    master  172.18.0.5  Ubuntu 19.10
kind-worker          Ready    &lt;none&gt;  172.18.0.3  Ubuntu 19.10
kind-worker2         Ready    &lt;none&gt;  172.18.0.4  Ubuntu 19.10
kind-worker3         Ready    &lt;none&gt;  172.18.0.2  Ubuntu 19.10</pre>

<p>Communication external to the cluster will use a <code>NodePort</code> value of 30040 opened on each worker and the node worker’s IP address.</p>

<p>We can see that our pods are reachable on each host in the cluster:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec</code> -it dnsutils -- wget -q -O-  172.18.0.5:30040/host
NODE: kind-worker2, POD IP:10.244.1.5

kubectl <code class="nb">exec</code> -it dnsutils -- wget -q -O-  172.18.0.3:30040/host
NODE: kind-worker, POD IP:10.244.2.8

kubectl <code class="nb">exec</code> -it dnsutils -- wget -q -O-  172.18.0.4:30040/host
NODE: kind-worker2, POD IP:10.244.1.5</pre>

<p>It’s important to consider the limitations as well. A NodePort deployment will fail if it cannot allocate the
requested port. Also, ports must be tracked across all applications using a NodePort service.
Using manually selected ports raises the issue of port collisions (especially when applying a workload to multiple
clusters, which may not have the same NodePorts free).</p>

<p>Another downside of using the NodePort service type is that the load balancer
or client software must be aware of the node IP addresses.
A static configuration (e.g., an operator manually copying node IP addresses) may become too outdated over time (especially on a cloud provider) as IP addresses change or nodes are replaced.
A reliable system automatically populates node IP addresses, either by watching which
machines have been allocated to the cluster
or by listing nodes from the Kubernetes API itself.</p>

<p>NodePorts are the earliest form of services. We will see that other service types use NodePorts as a base structure
in their architecture. NodePorts should not be used by themselves, as clients would need to know the IP addresses of
hosts and the node for connection requests. We will see how NodePorts are used to enable load balancers later in the
chapter when we discuss cloud networks.</p>

<p>Next up <a data-type="indexterm" data-startref="ch5_term16" id="idm46219935907192"/><a data-type="indexterm" data-startref="ch5_term17" id="idm46219935906632"/><a data-type="indexterm" data-startref="ch5_term18" id="idm46219935905960"/><a data-type="indexterm" data-startref="ch5_term19" id="idm46219935938008"/><a data-type="indexterm" data-startref="ch5_term58" id="idm46219935937336"/>is the default type for services, ClusterIP.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="ClusterIP"><div class="sect2" id="idm46219936154616">
<h2>ClusterIP</h2>

<p>The IP addresses of pods <a data-type="indexterm" data-primary="pods" data-secondary="and services" data-secondary-sortas="services" id="ch5_term23"/><a data-type="indexterm" data-primary="ClusterIP service, Kubernetes" id="ch5_term20"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="ClusterIP" id="ch5_term21"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with ClusterIP service" data-secondary-sortas="ClusterIP service" id="ch5_term22"/>share the life cycle of the pod and thus are not reliable for clients to use for
requests. Services help overcome this pod networking design. A <a data-type="indexterm" data-primary="load balancing" data-secondary="with Kubernetes services" data-secondary-sortas="Kubernetes services" id="idm46219935885768"/>ClusterIP service provides an internal load
balancer with a single IP address that maps to all matching (and ready) pods.</p>

<p>The service’s IP address must be within the CIDR set in <code>service-cluster-ip-range</code>, in the API server.
You can specify a valid IP address manually, or leave <code>.spec.clusterIP</code> unset to have one assigned automatically.
The ClusterIP service address is a virtual IP address that is  routable only internally.</p>

<p><code>kube-proxy</code> is responsible <a data-type="indexterm" data-primary="kube-proxy, Kubernetes" data-secondary="with ClusterIP" data-secondary-sortas="ClusterIP" id="idm46219935882232"/>for making the ClusterIP service address route to all applicable pods. In “normal” configurations, <code>kube-proxy</code> performs L4 load balancing,
which may not be sufficient. For example, older pods may see more load, due to accumulating more long-lived
connections from clients. Or, a few clients making many requests may cause the load to be distributed unevenly.</p>

<p>A particular use case example for ClusterIP is when a workload requires a load balancer within the same cluster.</p>

<p>In <a data-type="xref" href="#img-clusterip">Figure 5-5</a>, we can see a ClusterIP service deployed. The service name is App with a selector, or App=App1.
There are two pods powering this service. Pod 1 and Pod 5 match the selector for the service.</p>

<figure><div id="img-clusterip" class="figure">
<img src="Images/neku_0505.png" alt="Cluster IP" width="1140" height="960"/>
<h6><span class="label">Figure 5-5. </span>Cluster IP example service</h6>
</div></figure>

<p>Let’s dig into an example on the command line with our KIND cluster.</p>

<p>We will deploy a <a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="with ClusterIP service example" data-secondary-sortas="ClusterIP service example" id="idm46219935982504"/>ClusterIP service for use with our Golang web server:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f service-clusterip.yaml
service/clusterip-service created

kubectl describe svc clusterip-service
Name:              clusterip-service
Namespace:         default
Labels:            <code class="nv">app</code><code class="o">=</code>app
Annotations:       Selector:  <code class="nv">app</code><code class="o">=</code>app
Type:              ClusterIP
IP:                10.98.252.195
Port:              &lt;<code class="nb">unset</code>&gt;  80/TCP
TargetPort:        8080/TCP
Endpoints:         &lt;none&gt;
Session Affinity:  None
Events:            &lt;none&gt;</pre>

<p>The ClusterIP service name is resolvable in the network:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>dnsutils -- host clusterip-service
clusterip-service.default.svc.cluster.local has address 10.98.252.195</pre>

<p>Now we can reach the host API endpoint with the Cluster IP address <code>10.98.252.195</code>, with the service name <code>clusterip-service</code>; or directly with the pod IP address <code>10.244.1.4</code> and port 8080:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>dnsutils -- wget -q -O- clusterip-service/host
NODE: kind-worker2, POD IP:10.244.1.4

kubectl <code class="nb">exec </code>dnsutils -- wget -q -O- 10.98.252.195/host
NODE: kind-worker2, POD IP:10.244.1.4

kubectl <code class="nb">exec </code>dnsutils -- wget -q -O- 10.244.1.4:8080/host
NODE: kind-worker2, POD IP:10.244.1.4</pre>

<p>The ClusterIP <a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="default service for" id="idm46219935833112"/>service is the default type for services. With that default status, it is warranted that we should
explore what the ClusterIP service abstracted for us. If you recall from Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.xhtml#linux_networking">2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.xhtml#container_networking_basics">3</a>, this list is similar to
what is set up with the Docker network, but <a data-type="indexterm" data-primary="iptables, Linux" data-secondary="with ClusterIP service" data-secondary-sortas="ClusterIP service" id="idm46219935857704"/>we now also have <code>iptables</code> for the service across all nodes:</p>

<ul>
<li>
<p>View the VETH pair and match with the pod.</p>
</li>
<li>
<p>View the network namespace and match with the pod.</p>
</li>
<li>
<p>Verify the PIDs on the node and match the pods.</p>
</li>
<li>
<p>Match services with <code>iptables</code> rules.</p>
</li>
</ul>

<p>To explore this, we <a data-type="indexterm" data-primary="nodes" data-secondary="worker nodes as" id="ch5_term24"/><a data-type="indexterm" data-primary="worker nodes, Kubernetes" id="ch5_term25"/>need to know what worker node the pod is deployed to, and that is <code>kind-worker2</code>:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">
kubectl get pods -o wide --field-selector spec.nodeName<code class="o">=</code>kind-worker2 -l <code class="nv">app</code><code class="o">=</code>app
NAME                  READY   STATUS    RESTARTS   AGE     IP           NODE
app-9cc7d9df8-ffsm6   1/1     Running   <code class="m">0</code>          7m23s   10.244.1.4   kind-worker2
</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The container IDs and names will be different for you.</p>
</div>

<p>Since we are <a data-type="indexterm" data-primary="KIND (Kubernetes in Docker) cluster" id="idm46219935793192"/>using KIND, we can use <code>docker ps</code> and <code>docker exec</code> to get information out of the running worker node
<code>kind-worker-2</code>:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">

docker ps
CONTAINER ID  COMMAND                  PORTS                      NAMES
df6df0736958  <code class="s2">"/usr/local/bin/entr…"</code>                               kind-worker2
e242f11d2d00  <code class="s2">"/usr/local/bin/entr…"</code>                               kind-worker
a76b32f37c0e  <code class="s2">"/usr/local/bin/entr…"</code>                               kind-worker3
07ccb63d870f  <code class="s2">"/usr/local/bin/entr…"</code>   0.0.0.0:80-&gt;80/tcp,         kind-control-plane
                                       0.0.0.0:443-&gt;443/tcp,
                                       127.0.0.1:52321-&gt;6443/tcp
</pre>

<p>The <code>kind-worker2</code> container ID is <code>df6df0736958</code>; KIND was <em>kind</em> enough to label each container with names, so we can
reference each worker node with its name <code>kind-worker2</code>:</p>

<p>Let’s see the IP address and route table information of our pod, <code>app-9cc7d9df8-ffsm6</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>app-9cc7d9df8-ffsm6 ip r
default via 10.244.1.1 dev eth0
10.244.1.0/24 via 10.244.1.1 dev eth0 src 10.244.1.4
10.244.1.1 dev eth0 scope link src 10.244.1.4</pre>

<p>Our pod’s IP address is <code>10.244.1.4</code> running on interface <code>eth0@if5</code> with <code>10.244.1.1</code> as its default route. That
matches interface 5 on the pod <code>veth45d1f3e8@if5</code>:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">
kubectl <code class="nb">exec </code>app-9cc7d9df8-ffsm6 ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue state UNKNOWN group default
qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: tunl0@NONE: &lt;NOARP&gt; mtu <code class="m">1480</code> qdisc noop state DOWN
group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
3: ip6tnl0@NONE: &lt;NOARP&gt; mtu <code class="m">1452</code> qdisc noop state DOWN group default qlen 1000
    link/tunnel6 :: brd ::
5: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc noqueue state UP
group default
    link/ether 3e:57:42:6e:cd:45 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.1.4/24 brd 10.244.1.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::3c57:42ff:fe6e:cd45/64 scope link
       valid_lft forever preferred_lft forever
</pre>

<p>Let’s check <a data-type="indexterm" data-primary="namespaces, network" data-secondary="with ClusterIP service" data-secondary-sortas="ClusterIP service" id="idm46219935731992"/>the network namespace as well, from the <code>node ip a</code> output:</p>

<pre data-type="programlisting" data-code-language="bash"> docker <code class="nb">exec</code> -it kind-worker2 ip a
&lt;trimmerd&gt;
5: veth45d1f3e8@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc noqueue
state UP group default
    link/ether 3e:39:16:38:3f:23 brd &lt;&gt;
    link-netns cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4
    inet 10.244.1.1/32 brd 10.244.1.1 scope global veth45d1f3e8
       valid_lft forever preferred_lft forever</pre>

<p><code>netns list</code> confirms <a data-type="indexterm" data-primary="netns list" id="idm46219935693096"/>that the network namespaces match our pods, interface to the host interface,
<code>cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4</code>:</p>

<pre data-type="programlisting" data-code-language="bash">docker <code class="nb">exec</code> -it kind-worker2 /usr/sbin/ip netns list
cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4 <code class="o">(</code>id: 2<code class="o">)</code>
cni-c18c44cb-6c3e-c48d-b783-e7850d40e01c <code class="o">(</code>id: 1<code class="o">)</code></pre>

<p>Let’s see what processes run inside that network namespace. For that we will use <code>docker exec</code>
to run commands inside the node <code>kind-worker2</code> hosting the pod and its network namespace:</p>

<pre data-type="programlisting" data-code-language="bash"> docker <code class="nb">exec</code> -it kind-worker2 /usr/sbin/ip netns pid
 cni-ec37f6e4-a1b5-9bc9-b324-59d612edb4d4
4687
4737</pre>

<p>Now we can <code>grep</code> for <a data-type="indexterm" data-primary="process id" id="idm46219935637272"/>each process ID and inspect what they are doing:</p>

<pre data-type="programlisting" data-code-language="bash">docker <code class="nb">exec</code> -it kind-worker2 ps aux <code class="p">|</code> grep 4687
root      <code class="m">4687</code>  0.0  0.0    <code class="m">968</code>     <code class="m">4</code> ?        Ss   17:00   0:00 /pause

docker <code class="nb">exec</code> -it kind-worker2 ps aux <code class="p">|</code> grep 4737
root      <code class="m">4737</code>  0.0  0.0 <code class="m">708376</code>  <code class="m">6368</code> ?        Ssl  17:00   0:00 /opt/web-server</pre>

<p><code>4737</code> is the process ID of our web server container running on  <code>kind-worker2</code>.</p>

<p><code>4687</code> is our <a data-type="indexterm" data-primary="pause container, Kubernetes" id="idm46219935611096"/>pause container holding onto all our namespaces.</p>

<p>Now let’s see <a data-type="indexterm" data-primary="iptables, Linux" data-secondary="chains of" id="ch5_term26"/><a data-type="indexterm" data-primary="iptables, Linux" data-secondary="with ClusterIP service" data-secondary-sortas="ClusterIP service" id="ch5_term27"/>what will happen to the <code>iptables</code> on the worker node:</p>

<pre data-type="programlisting" data-code-language="bash">docker <code class="nb">exec</code> -it kind-worker2 iptables -L
Chain INPUT <code class="o">(</code>policy ACCEPT<code class="o">)</code>
target                  prot opt <code class="nb">source     </code>destination
/* kubernetes service portals */
KUBE-SERVICES           all  --  anywhere   anywhere    ctstate NEW
/* kubernetes externally-visible service portals */
KUBE-EXTERNAL-SERVICES  all  --  anywhere   anywhere    ctstate NEW
KUBE-FIREWALL           all  --  anywhere   anywhere

Chain FORWARD <code class="o">(</code>policy ACCEPT<code class="o">)</code>
target        prot opt <code class="nb">source     </code>destination
/* kubernetes forwarding rules */
KUBE-FORWARD  all  --  anywhere   anywhere
/* kubernetes service portals */
KUBE-SERVICES all  --  anywhere   anywhere             ctstate NEW

Chain OUTPUT <code class="o">(</code>policy ACCEPT<code class="o">)</code>
target          prot opt <code class="nb">source               </code>destination
/* kubernetes service portals */
KUBE-SERVICES   all  --  anywhere             anywhere             ctstate NEW
KUBE-FIREWALL   all  --  anywhere             anywhere

Chain KUBE-EXTERNAL-SERVICES <code class="o">(</code><code class="m">1</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination

Chain KUBE-FIREWALL <code class="o">(</code><code class="m">2</code> references<code class="o">)</code>
target     prot opt <code class="nb">source    </code>destination
/* kubernetes firewall <code class="k">for</code> dropping marked packets */
DROP       all  --  anywhere  anywhere   mark match 0x8000/0x8000

Chain KUBE-FORWARD <code class="o">(</code><code class="m">1</code> references<code class="o">)</code>
target  prot opt <code class="nb">source    </code>destination
DROP    all  --  anywhere  anywhere    ctstate INVALID
/*kubernetes forwarding rules*/
ACCEPT  all  --  anywhere  anywhere     mark match 0x4000/0x4000
/*kubernetes forwarding conntrack pod <code class="nb">source </code>rule*/
ACCEPT  all  --  anywhere  anywhere     ctstate RELATED,ESTABLISHED
/*kubernetes forwarding conntrack pod destination rule*/
ACCEPT  all  --  anywhere  anywhere     ctstate RELATED,ESTABLISHED

Chain KUBE-KUBELET-CANARY <code class="o">(</code><code class="m">0</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination

Chain KUBE-PROXY-CANARY <code class="o">(</code><code class="m">0</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination

Chain KUBE-SERVICES <code class="o">(</code><code class="m">3</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination</pre>

<p>That is a <a data-type="indexterm" data-startref="ch5_term24" id="idm46219935667688"/><a data-type="indexterm" data-startref="ch5_term25" id="idm46219935520712"/>lot of tables being managed by Kubernetes.</p>

<p>We can dive a little deeper to examine the <code>iptables</code> responsible for the services we deployed.
Let’s retrieve the IP address of the <code>clusterip-service</code> deployed.
We need this to find the matching <code>iptables</code> rules:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get svc clusterip-service
NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<code class="o">(</code>S<code class="o">)</code>    AGE
clusterip-service   ClusterIP   10.98.252.195    &lt;none&gt;        80/TCP     57m</pre>

<p>Now use the clusterIP of the service, <code>10.98.252.195</code>, to find our <code>iptables</code> rule:</p>

<pre data-type="programlisting" data-code-language="bash">docker <code class="nb">exec</code> -it  kind-worker2 iptables -L -t nat <code class="p">|</code> grep 10.98.252.195
/* default/clusterip-service: cluster IP */
KUBE-MARK-MASQ  tcp  -- !10.244.0.0/16        10.98.252.195 tcp dpt:80
/* default/clusterip-service: cluster IP */
KUBE-SVC-V7R3EVKW3DT43QQM  tcp  --  anywhere  10.98.252.195 tcp dpt:80</pre>

<p>List all the rules on the chain <code>KUBE-SVC-V7R3EVKW3DT43QQM</code>:</p>

<pre data-type="programlisting" data-code-language="bash">docker <code class="nb">exec</code> -it  kind-worker2 iptables -t nat -L KUBE-SVC-V7R3EVKW3DT43QQM
Chain KUBE-SVC-V7R3EVKW3DT43QQM <code class="o">(</code><code class="m">1</code> references<code class="o">)</code>
target     prot opt <code class="nb">source               </code>destination
/* default/clusterip-service: */
KUBE-SEP-THJR2P3Q4C2QAEPT  all  --  anywhere             anywhere</pre>

<p>The <code>KUBE-SEP-</code> will contain the endpoints for the services, <code>KUBE-SEP-THJR2P3Q4C2QAEPT</code>.</p>

<p>Now we can see what the rules for this chain are in <code>iptables</code>:</p>

<pre data-type="programlisting" data-code-language="bash">docker <code class="nb">exec</code> -it kind-worker2 iptables -L KUBE-SEP-THJR2P3Q4C2QAEPT -t nat
Chain KUBE-SEP-THJR2P3Q4C2QAEPT <code class="o">(</code><code class="m">1</code> references<code class="o">)</code>
target          prot opt <code class="nb">source          </code>destination
/* default/clusterip-service: */
KUBE-MARK-MASQ  all  --  10.244.1.4      anywhere
/* default/clusterip-service: */
DNAT            tcp  --  anywhere        anywhere    tcp to:10.244.1.4:8080</pre>

<p><code>10.244.1.4:8080</code> is one of the <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="ClusterIP and" id="idm46219935488696"/>service endpoints, aka a pod backing the service, which is confirmed with the output
of <code>kubectl get ep clusterip-service</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get ep clusterip-service
NAME                ENDPOINTS                         AGE
clusterip-service   10.244.1.4:8080                   62m

kubectl describe ep clusterip-service
Name:         clusterip-service
Namespace:    default
Labels:       <code class="nv">app</code><code class="o">=</code>app
Annotations:  &lt;none&gt;
Subsets:
  Addresses:          10.244.1.4
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    &lt;<code class="nb">unset</code>&gt;  <code class="m">8080</code>  TCP

Events:  &lt;none&gt;</pre>

<p>Now, let’s <a data-type="indexterm" data-startref="ch5_term23" id="idm46219935371768"/><a data-type="indexterm" data-startref="ch5_term26" id="idm46219935371160"/><a data-type="indexterm" data-startref="ch5_term27" id="idm46219935370520"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="issues with" id="idm46219935369848"/><a data-type="indexterm" data-primary="routing" data-secondary="for internal traffic" data-secondary-sortas="internal traffic" id="idm46219935368904"/>explore the limitations of the ClusterIP service. The ClusterIP service is for internal traffic to the cluster, and it
suffers the same issues as endpoints do. As the service size grows, updates to it will slow.
In <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a>, we discussed how to mitigate that by using IPVS over <code>iptables</code> as the proxy mode for <code>kube-proxy</code>.
We will discuss later in this chapter how to get traffic into the cluster using ingress and the other service type
LoadBalancer.</p>

<p>ClusterIP is the default type of service, but there are several other specific types of services such as headless and
ExternalName. ExternalName is a specific type of services that helps with reaching services outside the cluster.
We briefly touched on headless services with StatefulSets, but let’s review those services in depth now.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Headless"><div class="sect2" id="idm46219935934984">
<h2>Headless</h2>

<p>A headless service isn’t a <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="headless services with" id="ch5_term28"/><a data-type="indexterm" data-primary="headless services, Kubernetes" id="ch5_term29"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="headless" id="ch5_term30"/>formal type of service (i.e., there is no <code>.spec.type: Headless</code>).
A headless service is a service with <code>.spec.clusterIP: "None"</code>.
This is distinct from merely <em>not setting</em> a cluster IP address,
which makes Kubernetes automatically assign a cluster IP address.</p>

<p>When ClusterIP is set to None, the service does not support any load balancing functionality.
Instead, it <a data-type="indexterm" data-primary="DNS (Domain Name System)" data-secondary="headless services with" id="idm46219935348760"/>only provisions an <code>Endpoints</code> object
and points the service DNS record at all pods that are selected and ready.</p>

<p>A headless service provides a generic way to watch endpoints,
without needing to interact with the Kubernetes API.
Fetching DNS records is much simpler than integrating with the Kubernetes API,
and it may not be possible with third-party 
<span class="keep-together">software.</span></p>

<p>Headless services allow developers to deploy multiple copies of a pod in a deployment. <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="of endpoints" data-secondary-sortas="endpoints" id="idm46219935345624"/>Instead of a single IP
address returned, like with the ClusterIP service, all the IP addresses of the endpoint are returned in the query. It then is up to the client to pick which one to use. To see this in action, let’s scale up the deployment of our web app:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl scale deployment app --replicas 4
deployment.apps/app scaled

 kubectl get pods -l <code class="nv">app</code><code class="o">=</code>app -o wide
NAME                  READY   STATUS      IP           NODE
app-9cc7d9df8-9d5t8   1/1     Running     10.244.2.4   kind-worker
app-9cc7d9df8-ffsm6   1/1     Running     10.244.1.4   kind-worker2
app-9cc7d9df8-srxk5   1/1     Running     10.244.3.4   kind-worker3
app-9cc7d9df8-zrnvb   1/1     Running     10.244.3.5   kind-worker3</pre>

<p>Now let’s deploy the headless service:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f service-headless.yml
service/headless-service created</pre>

<p>The DNS query will return all four of the pod IP addresses. <a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="image" id="idm46219935336024"/>Using our <code>dnsutils</code> image, we can verify that is the case:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>dnsutils -- host -v -t a headless-service
Trying <code class="s2">"headless-service.default.svc.cluster.local"</code>
<code class="p">;;</code> -&gt;&gt;HEADER<code class="s">&lt;&lt;- opco</code>de: QUERY, status: NOERROR, id: 45294
<code class="p">;;</code> flags: qr aa rd<code class="p">;</code> QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 0

<code class="p">;;</code> QUESTION SECTION:
<code class="p">;</code>headless-service.default.svc.cluster.local. IN A

<code class="p">;;</code> ANSWER SECTION:
headless-service.default.svc.cluster.local. <code class="m">30</code> IN A 10.244.2.4
headless-service.default.svc.cluster.local. <code class="m">30</code> IN A 10.244.3.5
headless-service.default.svc.cluster.local. <code class="m">30</code> IN A 10.244.1.4
headless-service.default.svc.cluster.local. <code class="m">30</code> IN A 10.244.3.4

Received <code class="m">292</code> bytes from 10.96.0.10#53 in <code class="m">0</code> ms</pre>

<p>The IP addresses returned from the query also match the endpoints for the service. Using <code>kubectl describe</code> for the endpoint  confirms that:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl describe endpoints headless-service
Name:         headless-service
Namespace:    default
Labels:       service.kubernetes.io/headless
Annotations:  endpoints.kubernetes.io/last-change-trigger-time:
2021-01-30T18:16:09Z
Subsets:
  Addresses:          10.244.1.4,10.244.2.4,10.244.3.4,10.244.3.5
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    &lt;<code class="nb">unset</code>&gt;  <code class="m">8080</code>  TCP

Events:  &lt;none&gt;</pre>

<p>Headless has a specific use case and is not typically used for deployments. As we mentioned in <a data-type="xref" href="#statefulsets">“StatefulSets”</a>, if developers need to let the client decide which endpoint to use, headless is the appropriate type of service to deploy. Two examples of headless services are clustered databases and applications that have client-side load-balancing logic built into the <a data-type="indexterm" data-startref="ch5_term20" id="idm46219935308664"/><a data-type="indexterm" data-startref="ch5_term21" id="idm46219935308056"/><a data-type="indexterm" data-startref="ch5_term22" id="idm46219935213048"/><a data-type="indexterm" data-startref="ch5_term28" id="idm46219935212376"/><a data-type="indexterm" data-startref="ch5_term29" id="idm46219935211704"/><a data-type="indexterm" data-startref="ch5_term30" id="idm46219935211032"/>code.</p>

<p>Our next example is ExternalName, which aids in migrations of services external to the cluster. It also offers other DNS advantages inside cluster DNS.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="ExternalName Service"><div class="sect2" id="idm46219935934360">
<h2>ExternalName Service</h2>

<p>ExternalName is a <a data-type="indexterm" data-primary="cluster networking" data-secondary="ExternalName service in" id="ch5_term31"/><a data-type="indexterm" data-primary="DNS (Domain Name System)" data-secondary="ExternalService with" id="ch5_term32"/><a data-type="indexterm" data-primary="ExternalName Service, Kubernetes" id="ch5_term33"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="ExternalName Service" id="ch5_term34"/>special type of service that does not have selectors and uses DNS names instead.</p>

<p>When looking up the host <code>ext-service.default.svc.cluster.local</code>,
the cluster DNS service returns a CNAME record of <code>database.mycompany.com</code>:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">ext-service</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">ExternalName</code>
  <code class="nt">externalName</code><code class="p">:</code> <code class="l-Scalar-Plain">database.mycompany.com</code></pre>

<p>If developers are migrating an application into Kubernetes but its dependencies are staying external to the cluster, ExternalName service allows them to define a DNS record internal to the cluster no matter where the service actually runs.</p>

<p>DNS will try the search as shown in the following example:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl <code class="nb">exec</code> -it dnsutils -- host -v -t a github.com
Trying <code class="s2">"github.com.default.svc.cluster.local"</code>
Trying <code class="s2">"github.com.svc.cluster.local"</code>
Trying <code class="s2">"github.com.cluster.local"</code>
Trying <code class="s2">"github.com"</code>
<code class="p">;;</code> -&gt;&gt;HEADER<code class="s">&lt;&lt;- opco</code>de: QUERY, status: NOERROR, id: 55908
<code class="p">;;</code> flags: qr rd ra<code class="p">;</code> QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

<code class="p">;;</code> QUESTION SECTION:
<code class="p">;</code>github.com.                    IN      A

<code class="p">;;</code> ANSWER SECTION:
github.com.             <code class="m">30</code>      IN      A       140.82.112.3

Received <code class="m">54</code> bytes from 10.96.0.10#53 in <code class="m">18</code> ms</pre>

<p>As an example, the ExternalName service allows developers to map a service to a DNS name.</p>

<p>Now if we deploy the external service like so:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f service-external.yml
service/external-service created</pre>

<p>The A record for github.com is returned from the <code>external-service</code> query:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec</code> -it dnsutils -- host -v -t a external-service
Trying <code class="s2">"external-service.default.svc.cluster.local"</code>
<code class="p">;;</code> -&gt;&gt;HEADER<code class="s">&lt;&lt;- opco</code>de: QUERY, status: NOERROR, id: 11252
<code class="p">;;</code> flags: qr aa rd<code class="p">;</code> QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0

<code class="p">;;</code> QUESTION SECTION:
<code class="p">;</code>external-service.default.svc.cluster.local. IN A

<code class="p">;;</code> ANSWER SECTION:
external-service.default.svc.cluster.local. <code class="m">24</code> IN CNAME github.com.
github.com.             <code class="m">24</code>      IN      A       140.82.112.3

Received <code class="m">152</code> bytes from 10.96.0.10#53 in <code class="m">0</code> ms</pre>

<p>The CNAME for the external service returns github.com:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec</code> -it dnsutils -- host -v -t cname external-service
Trying <code class="s2">"external-service.default.svc.cluster.local"</code>
<code class="p">;;</code> -&gt;&gt;HEADER<code class="s">&lt;&lt;- opco</code>de: QUERY, status: NOERROR, id: 36874
<code class="p">;;</code> flags: qr aa rd<code class="p">;</code> QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

<code class="p">;;</code> QUESTION SECTION:
<code class="p">;</code>external-service.default.svc.cluster.local. IN CNAME

<code class="p">;;</code> ANSWER SECTION:
external-service.default.svc.cluster.local. <code class="m">30</code> IN CNAME github.com.

Received <code class="m">126</code> bytes from 10.96.0.10#53 in <code class="m">0</code> ms</pre>

<p>Sending traffic to a <a data-type="indexterm" data-primary="headless services, Kubernetes" id="idm46219935121400"/><a data-type="indexterm" data-primary="DNS (Domain Name System)" data-secondary="headless services with" id="idm46219935120792"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="headless" id="idm46219935119944"/>headless service via a DNS record is possible but inadvisable. DNS is a notoriously poor way to load balance,
as software takes very different (and often simple or unintuitive)
approaches to A or AAAA DNS records that return multiple IP addresses. For example, it is common for software to always choose the first IP address in the response and/or cache and reuse the same IP address indefinitely. If you need to be able to send traffic to the service’s DNS address, consider a (standard) ClusterIP or LoadBalancer service.</p>

<p>The “correct” way to use a headless service is to query the service’s A/AAAA DNS record and use that data in a server-side or client-side <a data-type="indexterm" data-startref="ch5_term31" id="idm46219935068920"/><a data-type="indexterm" data-startref="ch5_term32" id="idm46219935068216"/><a data-type="indexterm" data-startref="ch5_term33" id="idm46219935067544"/><a data-type="indexterm" data-startref="ch5_term34" id="idm46219935066872"/>load balancer.</p>

<p>Most of the services we have been discussing are for internal traffic management for the cluster network.
In our <a data-type="indexterm" data-startref="ch5_term14" id="idm46219935065528"/>next sections, will be reviewing how to route requests into the cluster
with service type LoadBalancer and ingress.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="LoadBalancer"><div class="sect2" id="idm46219935256664">
<h2>LoadBalancer</h2>

<p>LoadBalancer service <a data-type="indexterm" data-primary="load balancing" data-secondary="with Kubernetes services" data-secondary-sortas="Kubernetes services" id="ch5_term35"/><a data-type="indexterm" data-primary="LoadBalancer services, Kubernetes" id="ch5_term36"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="LoadBalancer" id="ch5_term37"/>exposes services external to the cluster network. They combine the NodePort service behavior with an external integration, such as a cloud provider’s load balancer.
Notably, <a data-type="indexterm" data-primary="Transport layer, TCP/IP (L4)" data-secondary="LoadBalancer services in" id="idm46219935029416"/>LoadBalancer services handle L4 traffic (unlike ingress, which handles L7 traffic), so they will work for any TCP or UDP service, provided the load balancer selected supports L4 traffic.</p>

<p>Configuration and <a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="LoadBalancer services and" id="idm46219935027720"/>load balancer options are extremely dependent on the cloud provider.
For example, some will support <code>.spec.loadBalancerIP</code> (with varying setup required), and some will ignore it:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-service</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">selector</code><code class="p">:</code>
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">demo</code>
  <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>
      <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>
      <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
  <code class="nt">clusterIP</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.5.1</code>
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">LoadBalancer</code></pre>

<p>Once the load balancer has been provisioned, its IP address will be written to
<code>.status.loadBalancer.ingress.ip</code>.</p>

<p>LoadBalancer services are useful for exposing TCP or UDP services to the outside world. Traffic will <a data-type="indexterm" data-primary="port 80" id="idm46219934971256"/>come into the
load balancer on its public IP address and TCP port 80, defined by <code>spec.ports[*].port</code>  and routed to the
cluster IP address, <code>10.0.5.1</code>, and then to container target port 8080, <code>spec.ports[*].targetPort</code>. Not shown in the
example is the <code>.spec.ports[*].nodePort</code>; if not specified, Kubernetes will pick one for the service.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The service’s <code>spec.ports[*].targetPort</code> must match your pod’s container applications
<code>spec.container[*].ports.containerPort</code>, along with the protocol. It’s like missing a semicolon in Kubernetes networking
otherwise.</p>
</div>

<p>In <a data-type="xref" href="#loadbalancer">Figure 5-6</a>, we can see how a LoadBalancer type builds on the other service types. The cloud load balancer will determine
how to distribute traffic; we will discuss that in depth in the next chapter.</p>

<figure><div id="loadbalancer" class="figure">
<img src="Images/neku_0506.png" alt="LoadBalancer service" width="1443" height="705"/>
<h6><span class="label">Figure 5-6. </span>LoadBalancer service</h6>
</div></figure>

<p>Let’s continue to extend our <a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="with LoadBalance service example" data-secondary-sortas="LoadBalance service example" id="ch5_term38"/>Golang web server example with a LoadBalancer service.</p>

<p>Since we are <a data-type="indexterm" data-primary="load balancing" data-secondary="with MetalLB example" data-secondary-sortas="MetalLB example" id="ch5_term39"/><a data-type="indexterm" data-primary="MetalLB project" id="ch5_term41"/>running on our local machine and not in a service provider like AWS, GCP, or Azure, we can use MetalLB as
an example for our LoadBalancer service. The MetalLB project aims to allow users to deploy bare-metal load balancers for
their clusters.</p>

<p>This example has <a data-type="indexterm" data-primary="KIND (Kubernetes in Docker) cluster" id="idm46219934957400"/>been modified from the <a href="https://oreil.ly/h8xIt">KIND example deployment</a>.</p>

<p>Our first step is to deploy a separate namespace for MetalLB:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f mlb-ns.yaml
namespace/metallb-system created</pre>

<p>MetalLB members also require a secret for joining the LoadBalancer cluster; let’s deploy one now for them to use in our cluster:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl create secret generic -n metallb-system memberlist
--from-literal<code class="o">=</code><code class="nv">secretkey</code><code class="o">=</code><code class="s2">"</code><code class="k">$(</code>openssl rand -base64 128<code class="k">)</code><code class="s2">"</code>
secret/memberlist created</pre>

<p>Now we can deploy MetalLB!</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl apply -f ./metallb.yaml
podsecuritypolicy.policy/controller created
podsecuritypolicy.policy/speaker created
serviceaccount/controller created
serviceaccount/speaker created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
role.rbac.authorization.k8s.io/config-watcher created
role.rbac.authorization.k8s.io/pod-lister created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/config-watcher created
rolebinding.rbac.authorization.k8s.io/pod-lister created
daemonset.apps/speaker created
deployment.apps/controller created</pre>

<p>As you can see, it deploys many objects, and now we wait for the deployment to finish. We can monitor the deployment of
resources with the <code>--watch</code> option in the <code>metallb-system</code> namespace:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods -n metallb-system --watch
NAME                          READY   STATUS              RESTARTS   AGE
controller-5df88bd85d-mvgqn   0/1     ContainerCreating   <code class="m">0</code>          10s
speaker-5knqb                 1/1     Running             <code class="m">0</code>          10s
speaker-k79c9                 1/1     Running             <code class="m">0</code>          10s
speaker-pfs2p                 1/1     Running             <code class="m">0</code>          10s
speaker-sl7fd                 1/1     Running             <code class="m">0</code>          10s
controller-5df88bd85d-mvgqn   1/1     Running             <code class="m">0</code>          12s</pre>

<p>To complete the configuration, we <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="for LoadBalancer service" data-secondary-sortas="LoadBalancer service" id="ch5_term40"/>need to provide MetalLB with a range of IP addresses it controls.
This range has to be on the Docker KIND network:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">
docker network inspect -f <code class="s1">'{{.IPAM.Config}}'</code> kind
<code class="o">[{</code>172.18.0.0/16  172.18.0.1 map<code class="o">[]}</code> <code class="o">{</code>fc00:f853:ccd:e793::/64  fc00:f853:ccd:e793::1 map<code class="o">[]}]</code>
</pre>

<p><code>172.18.0.0/16</code> is our Docker network running locally.</p>

<p>We want our LoadBalancer IP range to come from this subclass. We
can configure MetalLB, for instance,
to use <code>172.18.255.200</code> to <code>172.18.255.250</code> by creating the ConfigMap.</p>

<p>The ConfigMap would look like this:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">ConfigMap</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">metallb-system</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">config</code>
<code class="nt">data</code><code class="p">:</code>
  <code class="nt">config</code><code class="p">:</code> <code class="p-Indicator">|</code>
    <code class="no">address-pools:</code>
    <code class="no">- name: default</code>
      <code class="no">protocol: layer2</code>
      <code class="no">addresses:</code>
      <code class="no">- 172.18.255.200-172.18.255.250</code></pre>

<p>Let’s deploy it so we can use MetalLB:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f ./metallb-configmap.yaml</pre>

<p>Now we deploy a load balancer for our web app:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f services-loadbalancer.yaml
service/loadbalancer-service created</pre>

<p>For fun let’s <a data-type="indexterm" data-primary="Kubernetes" data-secondary="scaling with" id="idm46219934702264"/>scale the web app deployment to 10, if you have the resources for it:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl scale deployment app --replicas 10

 kubectl get pods -o wide
NAME                  READY  STATUS    RESTARTS  AGE   IP           NODE
app-7bdb9ffd6c-b5x7m  2/2    Running   <code class="m">0</code>         26s   10.244.3.15  kind-worker
app-7bdb9ffd6c-bqtf8  2/2    Running   <code class="m">0</code>         26s   10.244.2.13  kind-worker2
app-7bdb9ffd6c-fb9sf  2/2    Running   <code class="m">0</code>         26s   10.244.3.14  kind-worker
app-7bdb9ffd6c-hrt7b  2/2    Running   <code class="m">0</code>         26s   10.244.2.7   kind-worker2
app-7bdb9ffd6c-l2794  2/2    Running   <code class="m">0</code>         26s   10.244.2.9   kind-worker2
app-7bdb9ffd6c-l4cfx  2/2    Running   <code class="m">0</code>         26s   10.244.3.11  kind-worker2
app-7bdb9ffd6c-rr4kn  2/2    Running   <code class="m">0</code>         23m   10.244.3.10  kind-worker
app-7bdb9ffd6c-s4k92  2/2    Running   <code class="m">0</code>         26s   10.244.3.13  kind-worker
app-7bdb9ffd6c-shmdt  2/2    Running   <code class="m">0</code>         26s   10.244.1.12  kind-worker3
app-7bdb9ffd6c-v87f9  2/2    Running   <code class="m">0</code>         26s   10.244.1.11  kind-worker3
app2-658bcd97bd-4n888 1/1    Running   <code class="m">0</code>         35m   10.244.2.6   kind-worker3
app2-658bcd97bd-mnpkp 1/1    Running   <code class="m">0</code>         35m   10.244.3.7   kind-worker
app2-658bcd97bd-w2qkl 1/1    Running   <code class="m">0</code>         35m   10.244.3.8   kind-worker
dnsutils              1/1    Running   <code class="m">1</code>         75m   10.244.1.2   kind-worker3
postgres-0            1/1    Running   <code class="m">0</code>         75m   10.244.1.4   kind-worker3
postgres-1            1/1    Running   <code class="m">0</code>         75m   10.244.3.4   kind-worker</pre>

<p>Now we can test the provisioned load balancer.</p>

<p>With more replicas deployed for our app behind the load balancer, we need the external IP of the load balancer,
<code>172.18.255.200</code>:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">
kubectl get svc loadbalancer-service
NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP
PORT<code class="o">(</code>S<code class="o">)</code>        AGE
loadbalancer-service   LoadBalancer   10.99.24.220   172.18.255.200
80:31276/TCP   52s


kubectl get svc/loadbalancer-service -o<code class="o">=</code><code class="nv">jsonpath</code><code class="o">=</code><code class="s1">'{.status.loadBalancer.ingress[0].ip}'</code>
172.18.255.200
</pre>

<p>Since <a data-type="indexterm" data-primary="Docker container technology" data-secondary="images from" id="idm46219934657016"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="KIND network on" id="idm46219934656168"/>Docker for Mac or Windows does not expose the KIND network to the host, we cannot directly reach the
<code>172.18.255.200</code> LoadBalancer IP on the Docker private network.</p>

<p>We can simulate it by attaching a Docker container to the KIND network and cURLing the load balancer as a workaround.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you would like to read more about this issue, there is a great <a href="https://oreil.ly/6rTKJ">blog post</a>.</p>
</div>

<p>We will use <a data-type="indexterm" data-primary="netshoot image, Docker" id="idm46219934663672"/><a data-type="indexterm" data-primary="nicolaka/netshoot image, Docker" id="idm46219934662936"/>another great networking Docker image called <code>nicolaka/netshoot</code> to run locally,
attach to the KIND Docker network, and send requests to our MetalLB load balancer.</p>

<p>If we run it several times, we can see the load balancer is doing its job of routing traffic to different pods:</p>

<pre data-type="programlisting" data-code-language="bash">docker run --network kind -a stdin -a stdout -i -t nicolaka/netshoot
curl 172.18.255.200/host
NODE: kind-worker, POD IP:10.244.2.7

docker run --network kind -a stdin -a stdout -i -t nicolaka/netshoot
curl 172.18.255.200/host
NODE: kind-worker, POD IP:10.244.2.9

docker run --network kind -a stdin -a stdout -i -t nicolaka/netshoot
curl 172.18.255.200/host
NODE: kind-worker3, POD IP:10.244.3.11

docker run --network kind -a stdin -a stdout -i -t nicolaka/netshoot
curl 172.18.255.200/host
NODE: kind-worker2, POD IP:10.244.1.6

docker run --network kind -a stdin -a stdout -i -t nicolaka/netshoot
curl 172.18.255.200/host
NODE: kind-worker, POD IP:10.244.2.9</pre>

<p>With each new request, the metalLB service is sending requests to different pods. LoadBalancer, like other services, uses
selectors and labels for the pods, and we can see that in the <code>kubectl describe endpoints loadbalancer-service</code>. The pod IP
addresses match our results from the cURL commands:</p>

<pre data-type="programlisting" data-code-language="bash"> kubectl describe endpoints loadbalancer-service
Name:         loadbalancer-service
Namespace:    default
Labels:       <code class="nv">app</code><code class="o">=</code>app
Annotations:  endpoints.kubernetes.io/last-change-trigger-time:
2021-01-30T19:59:57Z
Subsets:
  Addresses:
  10.244.1.6,
  10.244.1.7,
  10.244.1.8,
  10.244.2.10,
  10.244.2.7,
  10.244.2.8,
  10.244.2.9,
  10.244.3.11,
  10.244.3.12,
  10.244.3.9
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name          Port  Protocol
    ----          ----  --------
    service-port  <code class="m">8080</code>  TCP

Events:  &lt;none&gt;</pre>

<p>It is important to <a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="LoadBalancer services and" id="idm46219934646856"/>remember that LoadBalancer services require specific integrations and will not work without cloud
provider support, or manually installed software such as MetalLB.</p>

<p>They are <a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="load balancers for" id="idm46219934645096"/>not (normally) L7 load balancers, and therefore cannot intelligently handle HTTP(S) requests. There is a one-to-one mapping of load balancer to workload, which means that all requests sent to that load balancer must be handled
by the same <a data-type="indexterm" data-startref="ch5_term35" id="idm46219934652200"/><a data-type="indexterm" data-startref="ch5_term36" id="idm46219934651528"/><a data-type="indexterm" data-startref="ch5_term37" id="idm46219934650856"/><a data-type="indexterm" data-startref="ch5_term38" id="idm46219934650184"/><a data-type="indexterm" data-startref="ch5_term39" id="idm46219934649512"/><a data-type="indexterm" data-startref="ch5_term40" id="idm46219934567048"/><a data-type="indexterm" data-startref="ch5_term41" id="idm46219934566376"/>workload.</p>
<div data-type="tip"><h6>Tip</h6>
<p>While it’s not a network service, it is <a data-type="indexterm" data-primary="Horizontal Pod Autoscaler service" id="idm46219934564696"/>important to mention the Horizontal Pod Autoscaler service, which that will scale pods in a replication controller, deployment, ReplicaSet, or StatefulSet based on CPU utilization.</p>
</div>

<p>We can <a data-type="indexterm" data-primary="scaling" id="idm46219934563192"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="scaling with" id="idm46219934562456"/>scale our application to the demands of the users, with no need for configuration changes on anyone’s part.
Kubernetes and the LoadBalancer service take care of all of that for developers, systems, and network administrators.</p>

<p>We will see in the next chapter how we can take that even further using cloud services for autoscaling.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Services Conclusion"><div class="sect2" id="idm46219935034136">
<h2>Services Conclusion</h2>

<p>Here are some <a data-type="indexterm" data-primary="debugging" id="idm46219934617608"/><a data-type="indexterm" data-primary="troubleshooting" id="idm46219934616872"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="troubleshooting tips for" id="idm46219934616200"/>troubleshooting tips if issues arise with the endpoints or services:</p>

<ul>
<li>
<p>Removing the <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="troubleshooting tips for" id="idm46219934613912"/>label on the pod allows it to continue to run while also updating the endpoint and service.
The endpoint controller will remove that unlabeled pod from the endpoint objects, and the deployment
will deploy another pod; this will allow you to troubleshoot issues with that specific unlabeled pod
but not adversely affect the service to end customers.
I’ve used this one countless times during development, and we did so in the previous section’s examples.</p>
</li>
<li>
<p>There are <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="with probes for health of pods" data-secondary-sortas="probes for health of pods" id="idm46219934611496"/>two probes that communicate the pod’s health to the Kubelet and the rest of the Kubernetes environment.</p>
</li>
<li>
<p>It is also <a data-type="indexterm" data-primary="YAML configuration file" id="idm46219934596792"/>easy to mess up the YAML manifest, so make sure to compare ports on the service and pods and make sure they match.</p>
</li>
<li>
<p>We discussed <a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="NetworkPolicy overview and" id="idm46219934594968"/><a data-type="indexterm" data-primary="NetworkPolicies, Kubernetes" data-secondary="overview of" id="idm46219934593992"/>network policies in <a data-type="xref" href="ch03.xhtml#container_networking_basics">Chapter 3</a>, which can also stop pods from communicating with each other and services. If your cluster network is using network policies, ensure that they are set up appropriately for application traffic flow.</p>
</li>
<li>
<p>Also remember to use <a data-type="indexterm" data-primary="netshoot image, Docker" id="idm46219934591064"/><a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="as debugging tool" data-secondary-sortas="debugging tool" id="idm46219934590328"/><a data-type="indexterm" data-primary="cluster networking" data-secondary="diagnostic tools for" id="idm46219934589112"/>diagnostic tools like the <code>dnsutils</code> pod; the <code>netshoot</code> pods on the cluster network are helpful
debugging tools.</p>
</li>
<li>
<p>If endpoints are taking too long to come up in the cluster,
there are several options that can be configured on the Kubelet to control how fast it responds to change in the Kubernetes environment:</p>
<div class="openblock"><dl>
<dt><code>--kube-api-qps</code></dt>
<dd>
<p>Sets the query-per-second rate the Kubelet will use
when communicating with the Kubernetes API server; the default is 5.</p>
</dd>
<dt><code>--kube-api-burst</code></dt>
<dd>
<p>Temporarily allows API queries to burst to this number; the default is 10.</p>
</dd>
<dt><code>--iptables-sync-period</code></dt>
<dd>
<p>This is the <a data-type="indexterm" data-primary="iptables-sync-period setting" id="idm46219934540104"/><a data-type="indexterm" data-primary="iptables, Linux" data-secondary="rules of" id="idm46219934539400"/>maximum interval of how often <code>iptables</code> rules are refreshed (e.g., 5s, 1m, 2h22m). This must be greater than 0; the default is 30s.</p>
</dd>
<dt><code>--ipvs-sync-period duration</code></dt>
<dd>
<p>This is the <a data-type="indexterm" data-primary="ipvs-sync-period duration setting" id="idm46219934536376"/><a data-type="indexterm" data-primary="IPVS (IP Virtual Server), Linux" data-secondary="response time with" id="idm46219934535576"/>maximum interval of how often IPVS rules are refreshed.
This must be greater than 0; the efault is 30s.</p>
</dd>
</dl>
</div>

</li>
<li>
<p>Increasing these options for larger clusters is recommended, but also remember this increases the resources on both
the Kubelet and the API server, so keep that in mind.</p>
</li>
</ul>

<p>These tips can help alleviate issues and are good to be aware of as the number of services and pods grow in the cluster.</p>

<p>The various <a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="services overview" data-seealso="services, Kubernetes" id="idm46219934531992"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="overview of" id="idm46219934530744"/>types of services exemplify how powerful the network  abstractions are in Kubernetes. We have dug deep into how these work for each layer of the tool chain. Developers looking to deploy applications to Kubernetes now have the knowledge to pick and choose which services are right for their use cases. No longer will network administrators have to manually update load balancers with IP addresses, with Kubernetes managing that for them.</p>

<p>We have just scratched the surface of what is possible with services. With each new version of Kubernetes, there are
options to tune and configurations to run services. Test each service for your use cases and ensure you are using
the appropriate services to optimize your
applications on the Kubernetes network.</p>

<p>The LoadBalancer <a data-type="indexterm" data-primary="LoadBalancer services, Kubernetes" id="idm46219934528088"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="LoadBalancer" id="idm46219934527384"/><a data-type="indexterm" data-primary="HTTP" data-secondary="load balancing" id="idm46219934526440"/><a data-type="indexterm" data-primary="HTTP" data-secondary="services for" id="idm46219934525496"/><a data-type="indexterm" data-primary="load balancing" data-secondary="HTTP" id="idm46219934524552"/><a data-type="indexterm" data-primary="load balancing" data-secondary="external" id="idm46219934523608"/>service type is the only one that allows for traffic into the cluster, exposing HTTP(S) services
behind a load balancer for external users to connect to. Ingresses support path-based routing, which allows different HTTP paths to be served by different services. The next section will discuss ingress and how it is an alternative to managing connectivity into the cluster resources.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Ingress"><div class="sect1" id="idm46219936182472">
<h1>Ingress</h1>

<p>Ingress is a <a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="ingress and" id="idm46219934520904"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="overview of" id="idm46219934519896"/><a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="ingress" id="ch5_term42"/><a data-type="indexterm" data-primary="load balancing" data-secondary="ingress for" id="ch5_term43"/><a data-type="indexterm" data-primary="routing" data-secondary="with ingress" data-secondary-sortas="ingress" id="ch5_term44"/>Kubernetes-specific L7 (HTTP) load balancer, which is accessible externally, contrasting with L4
ClusterIP service, which is internal to the cluster. This is the typical choice for exposing an HTTP(S) workload to
external users. An ingress can be a single entry point into an API or a microservice-based architecture. Traffic can
be routed to services based on HTTP information in the request.  Ingress is a configuration spec (with multiple
implementations) for routing HTTP traffic to Kubernetes services. <a data-type="xref" href="#img-ingress">Figure 5-7</a> outlines the ingress components.</p>

<figure><div id="img-ingress" class="figure">
<img src="Images/neku_0507.png" alt="Ingress" width="1440" height="705"/>
<h6><span class="label">Figure 5-7. </span>Ingress architecture</h6>
</div></figure>

<p>To manage traffic in a cluster with ingress, there are two components required: the controller and
rules. The controller manages ingress pods, and the rules deployed define how the traffic is routed.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Ingress Controllers and Rules"><div class="sect1" id="idm46219934510744">
<h1>Ingress Controllers and Rules</h1>

<p>We call ingress <a data-type="indexterm" data-primary="internal load balancer controllers" id="ch5_term45"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="controllers" id="ch5_term46"/>implementations ingress <em>controllers</em>.
In Kubernetes, a controller is software that is responsible for managing a typical resource type
and making reality match the desired state.</p>

<p>There are two general kinds of controllers: external load balancer controllers and internal load balancer
controllers. <a data-type="indexterm" data-primary="external load balancer controllers" id="idm46219934505208"/>External load balancer controllers create a load balancer that exists “outside” the cluster, such as a
cloud provider product. Internal load balancer controllers deploy a load balancer that runs within the cluster and
do not directly solve the problem of routing consumers to the load balancer. There are a myriad of ways that cluster
administrators run internal load balancers, such as running the load balancer on a subset of special nodes, and
routing traffic somehow to those nodes. The primary motivation for choosing an internal load balancer is cost
reduction. An internal load balancer for ingress can route traffic for multiple ingress objects, whereas an external
load balancer controller typically needs one load balancer per ingress. As most <a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="LoadBalancer services and" id="idm46219934503544"/>cloud providers charge by load
balancer, it is cheaper to support a single cloud load balancer that does fan-out within the cluster, than
many cloud load balancers. Note that this incurs operational overhead and increased latency and compute costs, so be
sure the money you’re saving is worth it. Many companies have a bad habit of optimizing on inconsequential cloud
spend line items.</p>

<p>Let’s look at the spec for an ingress controller. Like LoadBalancer services, most of the spec is universal, but
various ingress controllers have different features and accept different configs. We’ll start with the basics:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">basic-ingress</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">rules</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">http</code><code class="p">:</code>
      <code class="nt">paths</code><code class="p">:</code>
      <code class="c1"># Send all /demo requests to demo-service.</code>
      <code class="p-Indicator">-</code> <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/demo</code>
        <code class="nt">pathType</code><code class="p">:</code> <code class="l-Scalar-Plain">Prefix</code>
        <code class="nt">backend</code><code class="p">:</code>
          <code class="nt">service</code><code class="p">:</code>
            <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-service</code>
            <code class="nt">port</code><code class="p">:</code>
              <code class="nt">number</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>
  <code class="c1"># Send all other requests to main-service.</code>
  <code class="nt">defaultBackend</code><code class="p">:</code>
    <code class="nt">service</code><code class="p">:</code>
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">main-service</code>
      <code class="nt">port</code><code class="p">:</code>
        <code class="nt">number</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>

<p>The previous example is representative of a typical ingress. It sends traffic to <code>/demo</code> to one service and all other
traffic to another. <a data-type="indexterm" data-primary="backend destinations" data-secondary="ingress rules and" id="idm46219934498248"/>Ingresses have a “default backend” where requests are routed if no rule matches. This can be
configured in many ingress controllers in the controller configuration itself (e.g., a generic 404 page), and many
support the <code>.spec.defaultBackend</code> field. Ingresses support <a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="path types with" id="idm46219934442728"/>multiple ways to specify a path. There are currently three:</p>
<dl>
<dt>Exact</dt>
<dd>
<p>Matches the specific path and only the given path (including trailing <code>/</code> or lack thereof).</p>
</dd>
<dt>Prefix</dt>
<dd>
<p>Matches all paths that start with the given path.</p>
</dd>
<dt>ImplementationSpecific</dt>
<dd>
<p>Allows for custom semantics from the current ingress controller.</p>
</dd>
</dl>

<p>When a request matches multiple paths, the most specific match is chosen.
For example, if there are rules for <code>/first</code> and <code>/first/second</code>,
any request starting with <code>/first/second</code> will go to the backend for <code>/first/second</code>.
If a path matches an exact path and a prefix path, the request will go to the backend for the exact rule.</p>

<p class="pagebreak-before">Ingresses can also use hostnames in rules:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">multi-host-ingress</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">rules</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">a.example.com</code>
    <code class="nt">http</code><code class="p">:</code>
      <code class="nt">paths</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="nt">pathType</code><code class="p">:</code> <code class="l-Scalar-Plain">Prefix</code>
        <code class="nt">path</code><code class="p">:</code> <code class="s">"/"</code>
        <code class="nt">backend</code><code class="p">:</code>
          <code class="nt">service</code><code class="p">:</code>
            <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">service-a</code>
            <code class="nt">port</code><code class="p">:</code>
              <code class="nt">number</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">b.example.com</code>
    <code class="nt">http</code><code class="p">:</code>
      <code class="nt">paths</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="nt">pathType</code><code class="p">:</code> <code class="l-Scalar-Plain">Prefix</code>
        <code class="nt">path</code><code class="p">:</code> <code class="s">"/"</code>
        <code class="nt">backend</code><code class="p">:</code>
          <code class="nt">service</code><code class="p">:</code>
            <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">service-b</code>
            <code class="nt">port</code><code class="p">:</code>
              <code class="nt">number</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>

<p>In this example, we serve traffic to <code>a.example.com</code> from one service and traffic to <code>b.example.com</code> from another.
This is comparable to virtual hosts in web servers. You may want to use host rules to use a single load balancer and IP
to serve multiple unique domains.</p>

<p>Ingresses have <a data-type="indexterm" data-primary="TLS (Transport Layer Security)" id="idm46219934321992"/>basic TLS support:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-ingress-secure</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">tls</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">hosts</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">https-example.com</code>
    <code class="nt">secretName</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-tls</code>
  <code class="nt">rules</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">https-example.com</code>
    <code class="nt">http</code><code class="p">:</code>
      <code class="nt">paths</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>
        <code class="nt">pathType</code><code class="p">:</code> <code class="l-Scalar-Plain">Prefix</code>
        <code class="nt">backend</code><code class="p">:</code>
          <code class="nt">service</code><code class="p">:</code>
            <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-service</code>
            <code class="nt">port</code><code class="p">:</code>
              <code class="nt">number</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>

<p>The TLS config references a <a data-type="indexterm" data-primary="secret, Kubernetes" id="idm46219934318792"/><a data-type="indexterm" data-primary="certificates and keys" data-secondary="with ingress" data-secondary-sortas="ingress" id="idm46219934318184"/>Kubernetes secret by name, in <code>.spec.tls.[*].secretName</code>. Ingress controllers expect the
TLS certificate and key to be provided in <code>.data."tls.crt"</code> and <code>.data."tls.key"</code> respectively, as shown here:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Secret</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">demo-tls</code>
<code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">kubernetes.io/tls</code>
<code class="nt">data</code><code class="p">:</code>
  <code class="nt">tls.crt</code><code class="p">:</code> <code class="l-Scalar-Plain">cert, encoded in base64</code>
  <code class="nt">tls.key</code><code class="p">:</code> <code class="l-Scalar-Plain">key, encoded in base64</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you don’t need to manage traditionally issued certificates by hand, <a data-type="indexterm" data-primary="cert-manager" id="idm46219934270728"/>you can use <a href="https://oreil.ly/qkN0h">cert-manager</a> to automatically fetch and update certs.</p>
</div>

<p>We mentioned earlier that ingress is simply a spec, and drastically different implementations exist. It’s <a data-type="indexterm" data-primary="class, ingress selection of" id="idm46219934268600"/><a data-type="indexterm" data-primary="IngressClass settings" id="idm46219934267928"/>possible to use multiple ingress controllers in a single cluster, using <code>IngressClass</code> settings. An ingress class represents an ingress controller, and therefore a specific ingress implementation.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Annotations in <a data-type="indexterm" data-primary="annotations, Kubernetes" id="idm46219934284584"/>Kubernetes must be strings. Because <code>true</code> and <code>false</code> have distinct nonstring meanings, you cannot set an annotation to <code>true</code> or <code>false</code> without quotes. <code>"true"</code> and <code>"false"</code> are both valid.
This <a data-type="indexterm" data-primary="Kubernetes" data-secondary="issues with" id="idm46219934281128"/>is a <a href="https://oreil.ly/76uSI">long-running bug</a>, which is often encountered when setting a default priority class.</p>
</div>

<p><code>IngressClass</code> was introduced in Kubernetes 1.18.
Prior to 1.18, annotating ingresses with <code>kubernetes.io/ingress.class</code> was a common convention but relied on all installed ingress controllers to support it. Ingresses can pick an ingress class by setting the class’s name in <code>.spec.ingressClassName</code>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If more than one ingress class is set as default, Kubernetes will not allow you to create an ingress with no ingress class or remove the ingress class from an existing ingress. You can use admission control to prevent multiple ingress classes from being marked as default.</p>
</div>

<p>Ingress only supports HTTP(S) requests, which is insufficient if your service uses a different protocol (e.g., most
databases use their own protocols). Some <a data-type="indexterm" data-primary="NGINX ingress controller" id="idm46219934141704"/>ingress controllers, such as the NGINX ingress controller, do support TCP
and UDP, but this is not the norm.</p>

<p>Now on to deploying an ingress controller so we can add ingress rules to our Golang web server example.</p>

<p>When we deployed our KIND cluster, we had to add several options to allow us to deploy an <a data-type="indexterm" data-primary="extraPortMappings for ingress controller" id="idm46219934139752"/>ingress controller:</p>

<ul>
<li>
<p>extraPortMappings allow the local host to make requests to the ingress controller over ports 80/443.</p>
</li>
<li>
<p>Node-labels only <a data-type="indexterm" data-primary="nodes" data-secondary="ingress restrictions and" id="idm46219934136920"/>allow the ingress controller to run on a specific node(s) matching the label selector.</p>
</li>
</ul>

<p>There are many <a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="controller decision guide for" id="idm46219934134904"/>options to choose from with ingress controllers. The Kubernetes system does not start or have a default
controller like it does with other pieces. The Kubernetes community does support AWS, GCE, and Nginx ingress
controllers. <a data-type="xref" href="#brief_list_of_ingress_controller_options">Table 5-1</a> outlines several options for <a data-type="indexterm" data-primary="HAProxy ingress" id="idm46219934132712"/><a data-type="indexterm" data-primary="Istio Ingress" id="idm46219934132040"/><a data-type="indexterm" data-primary="Kong ingress controller for Kubernetes" id="idm46219934131368"/><a data-type="indexterm" data-primary="NGINX ingress controller" id="ch5_term48"/><a data-type="indexterm" data-primary="Traefik Kubernetes ingress" id="idm46219934129672"/><a data-type="indexterm" data-primary="Community ingress Nginx" id="ch5_term47"/>ingress.</p>
<table id="brief_list_of_ingress_controller_options">
<caption><span class="label">Table 5-1. </span>Brief list of ingress controller options</caption>
<thead>
<tr>
<th>Name</th>
<th>Commercial support</th>
<th>Engine</th>
<th>Protocol support</th>
<th>SSL termination</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Ambassador ingress controller</p></td>
<td><p>Yes</p></td>
<td><p>Envoy</p></td>
<td><p>gRPC, HTTP/2, WebSockets</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Community ingress Nginx</p></td>
<td><p>No</p></td>
<td><p>NGINX</p></td>
<td><p>gRPC, HTTP/2, WebSockets</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>NGINX Inc. ingress</p></td>
<td><p>Yes</p></td>
<td><p>NGINX</p></td>
<td><p>HTTP, Websocket, gRPC</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>HAProxy ingress</p></td>
<td><p>Yes</p></td>
<td><p>HAProxy</p></td>
<td><p>gRPC, HTTP/2, WebSockets</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Istio Ingress</p></td>
<td><p>No</p></td>
<td><p>Envoy</p></td>
<td><p>HTTP, HTTPS, gRPC, HTTP/2</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Kong ingress controller for Kubernetes</p></td>
<td><p>Yes</p></td>
<td><p>Lua on top of Nginx</p></td>
<td><p>gRPC, HTTP/2</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Traefik Kubernetes ingress</p></td>
<td><p>Yes</p></td>
<td><p>Traefik</p></td>
<td><p>HTTP/2, gRPC, and WebSockets</p></td>
<td><p>Yes</p></td>
</tr>
</tbody>
</table>

<p>Some things to consider when deciding on the ingress for your clusters:</p>

<ul>
<li>
<p>Protocol support: Do you need more than TCP/UDP, for <a data-type="indexterm" data-primary="gRPC service" id="idm46219934086072"/>example gRPC integration or WebSocket?</p>
</li>
<li>
<p>Commercial support: Do you need commercial support?</p>
</li>
<li>
<p>Advanced features: Are JWT/oAuth2 authentication or circuit breakers requirements for your
applications?</p>
</li>
</ul>

<ul class="less_space pagebreak-before">
<li>
<p>API gateway features: Do you need some API gateway functionalities such as rate-limiting?</p>
</li>
<li>
<p>Traffic distribution: Does your application require support for specialized traffic distribution like canary A/B testing or mirroring?</p>
</li>
</ul>

<p>For our example, we have chosen to use the Community version of the NGINX ingress controller.</p>
<div data-type="tip"><h6>Tip</h6>
<p>For more ingress controllers to choose from, <a href="https://oreil.ly/Lzn5q">kubernetes.io</a> maintains a list.</p>
</div>

<p>Let’s deploy the NGINX ingress controller into our KIND cluster:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f ingress.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
configmap/ingress-nginx-controller created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
service/ingress-nginx-controller-admission created
service/ingress-nginx-controller created
deployment.apps/ingress-nginx-controller created
validatingwebhookconfiguration.admissionregistration.k8s.io/
ingress-nginx-admission created
serviceaccount/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created</pre>

<p>As with all deployments, we must wait for the controller to be ready before we can use it. With the following command, we can
verify if our ingress controller is ready for use:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">wait</code> --namespace ingress-nginx <code class="se">\</code>
&gt;   --for<code class="o">=</code><code class="nv">condition</code><code class="o">=</code>ready pod <code class="se">\</code>
&gt;   --selector<code class="o">=</code>app.kubernetes.io/component<code class="o">=</code>controller <code class="se">\</code>
&gt;   --timeout<code class="o">=</code>90s
pod/ingress-nginx-controller-76b5f89575-zps4k condition met</pre>

<p>The controller is deployed to the cluster, and <a data-type="indexterm" data-startref="ch5_term45" id="idm46219934042664"/><a data-type="indexterm" data-startref="ch5_term46" id="idm46219934041928"/>now we’re ready to write ingress rules for our application.</p>










<section data-type="sect3" data-pdf-bookmark="Deploy ingress rules"><div class="sect3" id="idm46219934041032">
<h3>Deploy ingress rules</h3>

<p>Our YAML <a data-type="indexterm" data-primary="YAML configuration file" id="idm46219934039336"/><a data-type="indexterm" data-primary="rules" data-secondary="ingress" id="ch5_term49"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="rules for" id="ch5_term50"/>manifest defines several ingress rules to use with our Golang web server example:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f ingress-rule.yaml
ingress.extensions/ingress-resource created

kubectl get ingress
NAME               CLASS    HOSTS   ADDRESS   PORTS   AGE
ingress-resource   &lt;none&gt;   *                 <code class="m">80</code>      4s</pre>

<p>With <code>describe</code> we can <a data-type="indexterm" data-primary="backend destinations" data-secondary="ingress rules and" id="idm46219934061176"/>see all the backends that map to the ClusterIP service and the pods:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl describe ingress
Name:             ingress-resource
Namespace:        default
Address:
Default backend:  default-http-backend:80 <code class="o">(</code>&lt;error:
endpoints <code class="s2">"default-http-backend"</code> not found&gt;<code class="o">)</code>
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /host  clusterip-service:8080 <code class="o">(</code>
10.244.1.6:8080,10.244.1.7:8080,10.244.1.8:8080<code class="o">)</code>
Annotations:  kubernetes.io/ingress.class: nginx
Events:
  Type    Reason  Age   From                      Message
  ----    ------  ----  ----                      -------
  Normal  Sync    17s   nginx-ingress-controller  Scheduled <code class="k">for</code> sync</pre>

<p>Our ingress rule is only for the <code>/host</code> route and will route requests to our 
<span class="keep-together"><code>clusterip-service:8080</code></span> service.</p>

<p>We can <a data-type="indexterm" data-primary="cURL/curl tool" data-secondary="for testing ingresses" data-secondary-sortas="testing ingresses" id="idm46219934016216"/><a data-type="indexterm" data-primary="cURL/curl tool" data-secondary="localhost command with" id="idm46219934014936"/>test that with cURL to http://localhost/host:</p>

<pre data-type="programlisting" data-code-language="bash">curl localhost/host
NODE: kind-worker2, POD IP:10.244.1.6
curl localhost/healthz</pre>

<p>Now we can see how powerful ingresses are; let’s <a data-type="indexterm" data-primary="ClusterIP service, Kubernetes" id="idm46219933996104"/><a data-type="indexterm" data-primary="services, Kubernetes" data-secondary="ClusterIP" id="idm46219933995496"/>deploy a second deployment and ClusterIP service.</p>

<p>Our new deployment and service will be used to answer the requests for <code>/data</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f ingress-example-2.yaml
deployment.apps/app2 created
service/clusterip-service-2 configured
ingress.extensions/ingress-resource-2 configured</pre>

<p class="pagebreak-before">Now both the <code>/host</code> and <code>/data</code> work but are going to separate services:</p>

<pre data-type="programlisting" data-code-language="bash">curl localhost/host
NODE: kind-worker2, POD IP:10.244.1.6

curl localhost/data
Database Connected</pre>

<p>Since ingress <a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="ingress and" id="idm46219933893368"/>works on layer 7, there are many more options to route traffic with, such as host header and URI path.</p>

<p>For more <a data-type="indexterm" data-startref="ch5_term42" id="idm46219933891768"/><a data-type="indexterm" data-startref="ch5_term43" id="idm46219933891032"/><a data-type="indexterm" data-startref="ch5_term44" id="idm46219933890360"/><a data-type="indexterm" data-startref="ch5_term47" id="idm46219933914328"/><a data-type="indexterm" data-startref="ch5_term48" id="idm46219933913656"/><a data-type="indexterm" data-startref="ch5_term49" id="idm46219933912984"/><a data-type="indexterm" data-startref="ch5_term50" id="idm46219933912312"/>advanced traffic routing and release patterns, a service mesh is required to be deployed
in the cluster network. Let’s dig into that next.</p>
</div></section>



</div></section>













<section data-type="sect1" data-pdf-bookmark="Service Meshes"><div class="sect1" id="servicemeshes">
<h1>Service Meshes</h1>

<p>A new cluster with the default options has some limitations. So, let’s get an understanding for what those limitations
are and how a <a data-type="indexterm" data-primary="Kubernetes networking abstractions" data-secondary="service meshes" id="ch5_term51"/><a data-type="indexterm" data-primary="service meshes, Kubernetes" data-secondary="functionality of" id="ch5_term52"/>service mesh can resolve some of those limitations. A <em>service mesh</em> <a data-type="indexterm" data-primary="service-to-service communication" id="idm46219933961112"/><a data-type="indexterm" data-primary="communication" data-secondary="service-to-service" id="idm46219933960408"/>is an API-driven infrastructure
layer for handling service-to-service communication.</p>

<p>From a security point of view, all traffic inside the cluster is unencrypted between pods, and each
application team that runs a service must configure monitoring separately for each service. We have discussed the
service types, but we have not discussed how to update deployments of pods for them. Service meshes support more than
the basic deployment type; they support rolling updates and re-creations, like Canary does. From a developer’s perspective, injecting faults
into the network is useful, but also not directly supported in default Kubernetes network deployments. With <a data-type="indexterm" data-primary="circuit breaking, service meshes for" id="idm46219933958360"/>service
meshes, developers can add fault testing, and instead of just killing pods, you can use service meshes to inject
delays—again, each application would have to build in fault testing or circuit breaking.</p>

<p>There are several pieces of functionality that a service mesh enhances or provides in a default Kubernetes cluster
network:</p>
<dl>
<dt>Service Discovery</dt>
<dd>
<p>Instead of <a data-type="indexterm" data-primary="service discovery tools" id="idm46219933954920"/>relying on DNS for service discovery, the service mesh manages service discovery,
and removes the need for it to be implemented in each individual application.</p>
</dd>
<dt>Load Balancing</dt>
<dd>
<p>The service mesh <a data-type="indexterm" data-primary="load balancing" data-secondary="with service mesh" data-secondary-sortas="service mesh" id="idm46219933952600"/>adds more advanced load balancing algorithms such as least request, consistent
hashing, and zone aware.</p>
</dd>
<dt>Communication Resiliency</dt>
<dd>
<p>The service mesh <a data-type="indexterm" data-primary="communication" data-secondary="resiliency of" id="idm46219933949832"/>can increase communication resilience for applications by not having to
implement retries, timeouts, circuit breaking,  or rate limiting in application code.</p>
</dd>
<dt>Security</dt>
<dd>
<div class="openblock">
<p>A service mesh can <a data-type="indexterm" data-primary="security" data-secondary="with service meshes" data-secondary-sortas="service meshes" id="idm46219933872424"/>provide the folllowing:
* End-to-end encryption <a data-type="indexterm" data-primary="end-to-end encryption" id="idm46219933871208"/>with mTLS between services
* Authorization policies, which <a data-type="indexterm" data-primary="authorization policies, service mesh" id="idm46219933870472"/>authorize what services can communicate with each other, not just at the layer 3 and 4 levels like in Kubernetes network polices.</p>
</div>

</dd>
<dt>Observability</dt>
<dd>
<p>Service meshes <a data-type="indexterm" data-primary="observability, service meshes for" id="idm46219933868104"/>add in observability by enriching the layer 7 metrics and adding tracing and alerting.</p>
</dd>
<dt>Routing Control</dt>
<dd>
<p>Traffic <a data-type="indexterm" data-primary="routing" data-secondary="service mesh control of" id="idm46219933865992"/>shifting and mirroring in the cluster.</p>
</dd>
<dt>API</dt>
<dd>
<p>All of this <a data-type="indexterm" data-primary="API, Kubernetes" id="idm46219933863576"/>can be controlled via an API provided by the service mesh 
<span class="keep-together">implementation.</span></p>
</dd>
</dl>

<p>Let’s walk through several components of a service mesh in <a data-type="xref" href="#img-service-mesh">Figure 5-8</a>.</p>

<figure><div id="img-service-mesh" class="figure">
<img src="Images/neku_0508.png" alt="Service mesh Components" width="973" height="654"/>
<h6><span class="label">Figure 5-8. </span>Service mesh components</h6>
</div></figure>

<p>Traffic is handled <a data-type="indexterm" data-primary="service meshes, Kubernetes" data-secondary="components of" id="idm46219933858296"/>differently depending on the component or destination of traffic. Traffic into and out of the cluster
is managed by the gateways. <a data-type="indexterm" data-primary="mTLS (mutual Transport Layer Security) for encryption" id="idm46219933857048"/>Traffic between the frontend, backend, and user service is all encrypted with Mutual
TLS (mTLS) and is handled by the service mesh. All the traffic to the frontend, backend, and user pods in the service mesh is
proxied by the sidecar proxy deployed within the pods. Even if the <a data-type="indexterm" data-primary="control plane, Kubernetes" data-secondary="with service meshes" data-secondary-sortas="service meshes" id="idm46219933855864"/>control plane is down and updates cannot be made to the mesh, the service and application traffic are not <a data-type="indexterm" data-startref="ch5_term52" id="idm46219933854424"/>affected.</p>

<p>There are several <a data-type="indexterm" data-primary="service meshes, Kubernetes" data-secondary="options with" id="idm46219933853208"/>options to use when deploying a service mesh; here are highlights of just a few:</p>

<ul>
<li>
<p>Istio</p>

<ul>
<li>
<p>Uses a Go <a data-type="indexterm" data-primary="Istio service mesh" id="idm46219933849656"/>control plane with an Envoy proxy.</p>
</li>
<li>
<p>This is a Kubernetes-native solution that was initially released by Lyft.</p>
</li>
</ul>
</li>
<li>
<p>Consul</p>

<ul>
<li>
<p>Uses HashiCorp <a data-type="indexterm" data-primary="Consul service mesh" id="idm46219933845656"/>Consul as the control plane.</p>
</li>
<li>
<p>Consul Connect uses an agent installed on every node as a DaemonSet, which communicates
with the Envoy sidecar proxies that handle routing and forwarding of traffic.</p>
</li>
</ul>
</li>
<li>
<p>AWS App Mesh</p>

<ul>
<li>
<p>Is an <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="App Mesh service mesh in" id="idm46219933841528"/>AWS-managed solution that implements its own control plane.</p>
</li>
<li>
<p>Does not have mTLS or traffic policy.</p>
</li>
<li>
<p>Uses the Envoy proxy for the data plane.</p>
</li>
</ul>
</li>
<li>
<p>Linkerd</p>

<ul>
<li>
<p>Also uses Go <a data-type="indexterm" data-primary="Linkerd service mesh" id="ch5_term55"/>for the control plane with the Linkerd proxy.</p>
</li>
<li>
<p>No traffic shifting and no distributed tracing.</p>
</li>
<li>
<p>Is a Kubernetes-only solution, which results in fewer moving pieces and means that Linkerd has less complexity overall.</p>
</li>
</ul>
</li>
</ul>

<p>It is our opinion that the best use case for a service mesh is <a data-type="indexterm" data-primary="mTLS (mutual Transport Layer Security) for encryption" id="idm46219933832312"/>mTLS between services. Other higher-level <a data-type="indexterm" data-primary="circuit breaking, service meshes for" id="idm46219933831304"/><a data-type="indexterm" data-primary="fault testing, service meshes for" id="idm46219933830568"/><a data-type="indexterm" data-primary="developers" id="idm46219933829880"/>use cases for developers include circuit breaking and fault testing for APIs. For <a data-type="indexterm" data-primary="network administrators" id="idm46219933829080"/>network administrators, advanced routing policies and algorithms can be deployed with service meshes.</p>

<p>Let’s look at a service mesh example. The first <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Linkerd" data-secondary-sortas="Linkerd" id="ch5_term53"/><a data-type="indexterm" data-primary="Linkerd CLI" id="ch5_term54"/>thing you need to do if you haven’t already is <a href="https://oreil.ly/jVaPm">install the Linkerd CLI</a>.</p>

<p>Your choices <a data-type="indexterm" data-primary="cURL/curl tool" data-secondary="for service meshes" data-secondary-sortas="service meshes" id="idm46219933823944"/>are cURL, bash, or brew if you’re on a Mac:</p>

<pre data-type="programlisting" data-code-language="bash">curl -sL https://run.linkerd.io/install <code class="p">|</code> sh

OR

brew install linkerd

linkerd version
Client version: stable-2.9.2
Server version: unavailable</pre>

<p>This preflight checklist will verify that our cluster can run Linkerd:</p>

<pre data-type="programlisting" data-code-language="bash"> linkerd check --pre
kubernetes-api
--------------
√ can initialize the client
√ can query the Kubernetes API

kubernetes-version
------------------
√ is running the minimum Kubernetes API version
√ is running the minimum kubectl version

pre-kubernetes-setup
--------------------
√ control plane namespace does not already exist
√ can create non-namespaced resources
√ can create ServiceAccounts
√ can create Services
√ can create Deployments
√ can create CronJobs
√ can create ConfigMaps
√ can create Secrets
√ can <code class="nb">read </code>Secrets
√ can <code class="nb">read </code>extension-apiserver-authentication configmap
√ no clock skew detected

pre-kubernetes-capability
-------------------------
√ has NET_ADMIN capability
√ has NET_RAW capability

linkerd-version
---------------
√ can determine the latest version
√ cli is up-to-date

Status check results are √</pre>

<p>The Linkerd CLI tool can install Linkerd for us <a data-type="indexterm" data-primary="KIND (Kubernetes in Docker) cluster" id="idm46219933813576"/>onto our KIND cluster:</p>

<pre data-type="programlisting" data-code-language="bash">linkerd install <code class="p">|</code> kubectl apply -f -
namespace/linkerd created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-identity created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-identity created
serviceaccount/linkerd-identity created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-controller created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-controller created
serviceaccount/linkerd-controller created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-destination created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-destination created
serviceaccount/linkerd-destination created
role.rbac.authorization.k8s.io/linkerd-heartbeat created
rolebinding.rbac.authorization.k8s.io/linkerd-heartbeat created
serviceaccount/linkerd-heartbeat created
role.rbac.authorization.k8s.io/linkerd-web created
rolebinding.rbac.authorization.k8s.io/linkerd-web created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-web-check created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-web-check created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-web-admin created
serviceaccount/linkerd-web created
customresourcedefinition.apiextensions.k8s.io/serviceprofiles.linkerd.io created
customresourcedefinition.apiextensions.k8s.io/trafficsplits.split.smi-spec.io
created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-proxy-injector created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-proxy-injector
created
serviceaccount/linkerd-proxy-injector created
secret/linkerd-proxy-injector-k8s-tls created
mutatingwebhookconfiguration.admissionregistration.k8s.io
 /linkerd-proxy-injector-webhook-config created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-sp-validator created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-sp-validator
created
serviceaccount/linkerd-sp-validator created
secret/linkerd-sp-validator-k8s-tls created
validatingwebhookconfiguration.admissionregistration.k8s.io
 /linkerd-sp-validator-webhook-config created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-tap created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-tap-admin created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-tap created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-tap-auth-delegator
created
serviceaccount/linkerd-tap created
rolebinding.rbac.authorization.k8s.io/linkerd-linkerd-tap-auth-reader created
secret/linkerd-tap-k8s-tls created
apiservice.apiregistration.k8s.io/v1alpha1.tap.linkerd.io created
podsecuritypolicy.policy/linkerd-linkerd-control-plane created
role.rbac.authorization.k8s.io/linkerd-psp created
rolebinding.rbac.authorization.k8s.io/linkerd-psp created
configmap/linkerd-config created
secret/linkerd-identity-issuer created
service/linkerd-identity created
service/linkerd-identity-headless created
deployment.apps/linkerd-identity created
service/linkerd-controller-api created
deployment.apps/linkerd-controller created
service/linkerd-dst created
service/linkerd-dst-headless created
deployment.apps/linkerd-destination created
cronjob.batch/linkerd-heartbeat created
service/linkerd-web created
deployment.apps/linkerd-web created
deployment.apps/linkerd-proxy-injector created
service/linkerd-proxy-injector created
service/linkerd-sp-validator created
deployment.apps/linkerd-sp-validator created
service/linkerd-tap created
deployment.apps/linkerd-tap created
serviceaccount/linkerd-grafana created
configmap/linkerd-grafana-config created
service/linkerd-grafana created
deployment.apps/linkerd-grafana created
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-prometheus created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-prometheus created
serviceaccount/linkerd-prometheus created
configmap/linkerd-prometheus-config created
service/linkerd-prometheus created
deployment.apps/linkerd-prometheus created
secret/linkerd-config-overrides created</pre>

<p>As with the ingress controller and MetalLB, we can see that a lot of components are installed in our cluster.</p>

<p>Linkerd can validate the installation with the <code>linkerd check</code> command.</p>

<p>It will validate a plethora of checks for the Linkerd install, included but not limited to the Kubernetes API version,
controllers, pods, and configs to run Linkerd, as well as all the services, versions, and APIs needed to run <a data-type="indexterm" data-primary="kubernetes-version" id="idm46219933796504"/><a data-type="indexterm" data-primary="control-plane-version" id="idm46219933795800"/>Linkerd:</p>

<pre data-type="programlisting" data-code-language="bash">linkerd check
kubernetes-api
--------------
√ can initialize the client
√ can query the Kubernetes API

kubernetes-version
------------------
√ is running the minimum Kubernetes API version
√ is running the minimum kubectl version

linkerd-existence
-----------------
√ <code class="s1">'linkerd-config'</code> config map exists
√ heartbeat ServiceAccount exists
√ control plane replica sets are ready
√ no unschedulable pods
√ controller pod is running
√ can initialize the client
√ can query the control plane API

linkerd-config
--------------
√ control plane Namespace exists
√ control plane ClusterRoles exist
√ control plane ClusterRoleBindings exist
√ control plane ServiceAccounts exist
√ control plane CustomResourceDefinitions exist
√ control plane MutatingWebhookConfigurations exist
√ control plane ValidatingWebhookConfigurations exist
√ control plane PodSecurityPolicies exist

linkerd-identity
----------------
√ certificate config is valid
√ trust anchors are using supported crypto algorithm
√ trust anchors are within their validity period
√ trust anchors are valid <code class="k">for</code> at least <code class="m">60</code> days
√ issuer cert is using supported crypto algorithm
√ issuer cert is within its validity period
√ issuer cert is valid <code class="k">for</code> at least <code class="m">60</code> days
√ issuer cert is issued by the trust anchor

linkerd-webhooks-and-apisvc-tls
-------------------------------
√ tap API server has valid cert
√ tap API server cert is valid <code class="k">for</code> at least <code class="m">60</code> days
√ proxy-injector webhook has valid cert
√ proxy-injector cert is valid <code class="k">for</code> at least <code class="m">60</code> days
√ sp-validator webhook has valid cert
√ sp-validator cert is valid <code class="k">for</code> at least <code class="m">60</code> days

linkerd-api
-----------
√ control plane pods are ready
√ control plane self-check
√ <code class="o">[</code>kubernetes<code class="o">]</code> control plane can talk to Kubernetes
√ <code class="o">[</code>prometheus<code class="o">]</code> control plane can talk to Prometheus
√ tap api service is running

linkerd-version
---------------
√ can determine the latest version
√ cli is up-to-date

control-plane-version
---------------------
√ control plane is up-to-date
√ control plane and cli versions match

linkerd-prometheus
------------------
√ prometheus add-on service account exists
√ prometheus add-on config map exists
√ prometheus pod is running

linkerd-grafana
---------------
√ grafana add-on service account exists
√ grafana add-on config map exists
√ grafana pod is running

Status check results are √</pre>

<p>Now that <a data-type="indexterm" data-startref="ch5_term53" id="idm46219933733608"/><a data-type="indexterm" data-startref="ch5_term54" id="idm46219933732968"/>everything looks good with our install of Linkerd, we can add our application to the service mesh:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl -n linkerd get deploy
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
linkerd-controller       1/1     <code class="m">1</code>            <code class="m">1</code>           3m17s
linkerd-destination      1/1     <code class="m">1</code>            <code class="m">1</code>           3m17s
linkerd-grafana          1/1     <code class="m">1</code>            <code class="m">1</code>           3m16s
linkerd-identity         1/1     <code class="m">1</code>            <code class="m">1</code>           3m17s
linkerd-prometheus       1/1     <code class="m">1</code>            <code class="m">1</code>           3m16s
linkerd-proxy-injector   1/1     <code class="m">1</code>            <code class="m">1</code>           3m17s
linkerd-sp-validator     1/1     <code class="m">1</code>            <code class="m">1</code>           3m17s
linkerd-tap              1/1     <code class="m">1</code>            <code class="m">1</code>           3m17s
linkerd-web              1/1     <code class="m">1</code>            <code class="m">1</code>           3m17s</pre>

<p>Let’s pull up the <a data-type="indexterm" data-primary="Linkerd Dashboard" id="ch5_term57"/>Linkerd console to investigate what we have just deployed. We can start the console with <code>linkerd dashboard &amp;</code>.</p>

<p>This will proxy the console to our local machine available at <code>http://localhost:50750</code>:</p>

<pre data-type="programlisting" data-code-language="bash">linkerd viz install <code class="p">|</code> kubectl apply -f -
linkerd viz dashboard
Linkerd dashboard available at:
http://localhost:50750
Grafana dashboard available at:
http://localhost:50750/grafana
Opening Linkerd dashboard in the default browser</pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you’re having issues with reaching the dashboard, you can run <code>linkerd viz check</code> and find more help in the Linkerd 
<span class="keep-together"><a href="https://oreil.ly/MqgAp">documentation</a>.</span></p>
</div>

<p>We can see all our deployed objects from the previous exercises in <a data-type="xref" href="#linkerd-dashboards">Figure 5-9</a>.</p>

<p>Our ClusterIP service is not part of the Linkerd service mesh. We will need to use the proxy injector to add our service
to the mesh. It accomplishes this by watching for a specific annotation that can be added either with Linkerd <code>inject</code> or
by hand to the pod’s spec.</p>

<figure><div id="linkerd-dashboards" class="figure">
<img src="Images/neku_0509.png" alt="Linkderd Dashboard" width="1731" height="1205"/>
<h6><span class="label">Figure 5-9. </span>Linkerd dashboard</h6>
</div></figure>

<p>Let’s remove some older exercises’ resources for clarity:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl delete -f ingress-example-2.yaml
deployment.apps <code class="s2">"app2"</code> deleted
service <code class="s2">"clusterip-service-2"</code> deleted
ingress.extensions <code class="s2">"ingress-resource-2"</code> deleted

kubectl delete pods app-5586fc9d77-7frts
pod <code class="s2">"app-5586fc9d77-7frts"</code> deleted

kubectl delete -f ingress-rule.yaml
ingress.extensions <code class="s2">"ingress-resource"</code> deleted</pre>

<p>We can use the <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Linkerd" data-secondary-sortas="Linkerd" id="idm46219933697432"/><a data-type="indexterm" data-primary="annotations, Kubernetes" id="idm46219933696312"/>Linkerd CLI to inject the proper annotations into our deployment spec, so that will become part of the
mesh.</p>

<p>We first need to get our application manifest, <code>cat web.yaml</code>, and use Linkerd to inject the annotations, <code>linkerd
inject -</code>, then apply them back to the Kubernetes API with <code>kubectl apply -f -</code>:</p>

<pre data-type="programlisting" data-code-language="bash">cat web.yaml <code class="p">|</code> linkerd inject - <code class="p">|</code> kubectl apply -f -

deployment <code class="s2">"app"</code> injected

deployment.apps/app configured</pre>

<p>If we describe our app deployment, we can see that Linkerd has injected new annotations for us,
<code>Annotations:  linkerd.io/inject: enabled</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl describe deployment app
Name:                   app
Namespace:              default
CreationTimestamp:      Sat, <code class="m">30</code> Jan <code class="m">2021</code> 13:48:47 -0500
Labels:                 &lt;none&gt;
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               <code class="nv">app</code><code class="o">=</code>app
Replicas:               <code class="m">1</code> desired <code class="p">|</code> <code class="m">1</code> updated <code class="p">|</code> <code class="m">1</code> total <code class="p">|</code> <code class="m">1</code> available <code class="p">|</code>
<code class="m">0</code> unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:       <code class="nv">app</code><code class="o">=</code>app
  Annotations:  linkerd.io/inject: enabled
  Containers:
   go-web:
    Image:      strongjz/go-web:v0.0.6
    Port:       8080/TCP
    Host Port:  0/TCP
    Liveness:   http-get http://:8080/healthz <code class="nv">delay</code><code class="o">=</code>5s <code class="nv">timeout</code><code class="o">=</code>1s <code class="nv">period</code><code class="o">=</code>5s
    Readiness:  http-get http://:8080/ <code class="nv">delay</code><code class="o">=</code>5s <code class="nv">timeout</code><code class="o">=</code>1s <code class="nv">period</code><code class="o">=</code>5s
    Environment:
      MY_NODE_NAME:             <code class="o">(</code>v1:spec.nodeName<code class="o">)</code>
      MY_POD_NAME:              <code class="o">(</code>v1:metadata.name<code class="o">)</code>
      MY_POD_NAMESPACE:         <code class="o">(</code>v1:metadata.namespace<code class="o">)</code>
      MY_POD_IP:                <code class="o">(</code>v1:status.podIP<code class="o">)</code>
      MY_POD_SERVICE_ACCOUNT:   <code class="o">(</code>v1:spec.serviceAccountName<code class="o">)</code>
      DB_HOST:                 postgres
      DB_USER:                 postgres
      DB_PASSWORD:             mysecretpassword
      DB_PORT:                 5432
    Mounts:                    &lt;none&gt;
  Volumes:                     &lt;none&gt;
Conditions:
 Type           Status  Reason
 ----           ------  ------
 Available      True    MinimumReplicasAvailable
 Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   app-78dfbb4854 <code class="o">(</code>1/1 replicas created<code class="o">)</code>
Events:
 Type   Reason            Age   From                   Message
 ----   ------            ----  ----                   -------
 Normal ScalingReplicaSet 4m4s  deployment-controller  Scaled down app-5586fc9d77
 Normal ScalingReplicaSet 4m4s  deployment-controller  Scaled up app-78dfbb4854
 Normal Injected          4m4s  linkerd-proxy-injector Linkerd sidecar injected
 Normal ScalingReplicaSet 3m54s deployment-controller  Scaled app-5586fc9d77</pre>

<p>If we navigate to the app in the dashboard, we can see that our deployment is part of the Linkerd service mesh now, as
shown in <a data-type="xref" href="#app-dashboards">Figure 5-10</a>.</p>

<figure><div id="app-dashboards" class="figure">
<img src="Images/neku_0510.png" alt="App Linkderd Dashboard" width="1669" height="841"/>
<h6><span class="label">Figure 5-10. </span>Web app deployment linkerd dashboard</h6>
</div></figure>

<p>The CLI can <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Linkerd" data-secondary-sortas="Linkerd" id="idm46219933497608"/><a data-type="indexterm" data-startref="ch5_term57" id="idm46219933496312"/>also display our stats for us:</p>

<pre data-type="programlisting" data-code-language="bash">linkerd stat deployments -n default
NAME  MESHED  SUCCESS    RPS LATENCY_P50 LATENCY_P95 LATENCY_P99 TCP_CONN
app      1/1  100.00% 0.4rps         1ms         1ms         1ms              1</pre>

<p>Again, let’s <a data-type="indexterm" data-primary="scaling" id="idm46219933493832"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="scaling with" id="idm46219933493336"/>scale up our deployment:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl scale deploy app --replicas 10
deployment.apps/app scaled</pre>

<p>In <a data-type="xref" href="#app-stats">Figure 5-11</a>, we navigate to the web browser and open <a href="https://oreil.ly/qQx9T">this link</a> so we can watch the stats in real time. Select the
default namespaces, and in Resources select our deployment/app. Then click “start for the web” to start displaying the metrics.</p>

<p>In a separate terminal let’s <a data-type="indexterm" data-primary="netshoot image, Docker" id="idm46219933490328"/><a data-type="indexterm" data-primary="nicolaka/netshoot image, Docker" id="idm46219933489624"/>use the <code>netshoot</code> image, but this time running inside our KIND cluster:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash
If you don<code class="err">'</code>t see a <code class="nb">command </code>prompt, try pressing enter.
bash-5.0#</pre>

<figure><div id="app-stats" class="figure">
<img src="Images/neku_0511.png" alt="App Stats Dashboard" width="1191" height="420"/>
<h6><span class="label">Figure 5-11. </span>Web app dashboard</h6>
</div></figure>

<p>Let’s send a few hundred queries and see the stats:</p>

<pre data-type="programlisting" data-code-language="bash">bash-5.0#for i in <code class="sb">`</code>seq <code class="m">1</code> 100<code class="sb">`</code><code class="p">;</code>
<code class="k">do</code> curl http://clusterip-service/host <code class="o">&amp;&amp;</code> sleep 2<code class="p">;</code>
<code class="k">done</code></pre>

<p>In our terminal we <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="with probes for health of pods" data-secondary-sortas="probes for health of pods" id="idm46219933445880"/><a data-type="indexterm" data-primary="health checks/probes" data-secondary="on pods by Kubelet" data-secondary-sortas="pods by Kubelet" id="idm46219933445016"/>can see all the liveness and readiness probes as well as our <code>/host</code> requests.</p>

<p><code>tmp-shell</code> is our <code>netshoot</code> bash terminal with our <code>for</code> loop running.</p>

<p><code>10.244.2.1</code>, <code>10.244.3.1</code>, and <code>10.244.2.1</code> are the Kubelets of the hosts running our probes for us:</p>

<pre data-type="programlisting" data-code-language="bash">linkerd viz stat deploy
NAME   MESHED   SUCCESS     RPS  LATENCY_P50  LATENCY_P95  LATENCY_P99  TCP_CONN
app       1/1   100.00%  0.7rps          1ms          1ms          1ms         3</pre>

<p>Our example showed the observability functionality for a service mesh only. Linkerd, Istio,
and the like have <a data-type="indexterm" data-primary="network administrators" id="idm46219933476840"/><a data-type="indexterm" data-primary="developers" id="idm46219933476280"/>many more options available for developers and network administrators to control,
monitor, and troubleshoot services running inside their cluster network. As with the ingress controller,
there are many options and features available. It is up to you and your teams to decide what
functionality and features are important for your <a data-type="indexterm" data-startref="ch5_term51" id="idm46219933475192"/><a data-type="indexterm" data-startref="ch5_term55" id="idm46219933386424"/>networks.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm46219933964248">
<h1>Conclusion</h1>

<p>The Kubernetes <a data-type="indexterm" data-primary="cluster networking" data-seealso="Kubernetes networking abstractions" id="idm46219933385336"/>networking world is feature rich with many options for teams to deploy, test, and manage with their Kubernetes cluster. Each new addition will add complexity and overhead to the cluster operations. We have given developers, network administrators, and system administrators a view into the abstractions that Kubernetes offers.</p>

<p>From internal traffic to external traffic to the cluster, teams  must choose what abstractions work best for their workloads. This is no small task, and now you are armed with the knowledge to begin those discussions.</p>

<p>In our next chapter, we take our Kubernetes services and network learnings to the cloud! We will explore the network
services offered by each cloud provider and how they are integrated into their Kubernetes managed service offering.</p>
</div></section>







</div></section></div></body></html>