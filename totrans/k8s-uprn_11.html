<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 11. DaemonSets" data-type="chapter" epub:type="chapter"><div class="chapter" id="daemon_sets">
<h1><span class="label">Chapter 11. </span>DaemonSets</h1>
<p>Deployments and ReplicaSets are generally about creating a service (such as a web server) with multiple replicas for redundancy. <a data-primary="DaemonSets" data-type="indexterm" id="ix_DaeS"/>But that is not the only reason to replicate a set of Pods within a cluster. Another reason is to schedule a single Pod on every node
within the cluster. Generally, the motivation for replicating a Pod to every node is to land some sort of agent or daemon on each node, and the
Kubernetes object for achieving this is the DaemonSet.</p>
<p>A DaemonSet ensures that a copy of a Pod is running across a set of nodes in a Kubernetes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. DaemonSets share similar functionality with ReplicaSets; both create Pods that are expected to be long-running services and ensure that the desired state and the observed state of the cluster match.</p>
<p>Given the similarities between DaemonSets and ReplicaSets, it’s important to understand when to use one over the other. ReplicaSets should be used when your application is completely decoupled from the node and you can run multiple copies on a given node without special consideration. DaemonSets should be used when a single copy of your application must run on all or a subset of the nodes in the cluster.</p>
<p>You should generally not use scheduling restrictions or other parameters to ensure that Pods do not colocate on the same node. If you find yourself wanting a single Pod per node, then a DaemonSet is the correct Kubernetes resource to use. Likewise, if you find yourself building a homogeneous replicated service to serve user traffic, then a ReplicaSet is probably the right Kubernetes resource to use.</p>
<p>You can use labels to run DaemonSet Pods on specific nodes; for example, you may want to run specialized intrusion-detection software on nodes that are exposed to
the edge network.</p>
<p>You can also use DaemonSets to install software on nodes in a cloud-based cluster. <a data-primary="cloud" data-secondary="using DaemonSets to install software on nodes in a cloud-based cluster" data-type="indexterm" id="idm45664076677840"/>For many cloud services, an upgrade or scaling of a cluster can delete and/or re-create new virtual machines. <a data-primary="infrastructure" data-secondary="immutable" data-type="indexterm" id="idm45664076676560"/><a data-primary="immutable infrastructure" data-type="indexterm" id="idm45664076675616"/>This dynamic <em>immutable infrastructure</em> approach can cause problems if you want (or are required by central IT) to have specific software on every node. To ensure that specific software is installed on every machine despite upgrades and scale events, a DaemonSet is the right approach. You can even mount the host filesystem and run scripts that install RPM/DEB packages onto the host operating system. In this way, you can have a cloud native cluster that still meets the enterprise requirements of your IT department.</p>
<section data-pdf-bookmark="DaemonSet Scheduler" data-type="sect1"><div class="sect1" id="idm45664076673712">
<h1>DaemonSet Scheduler</h1>
<p>By default, a DaemonSet will create a copy of a Pod on every node unless a node selector is used, which will limit eligible nodes to those with a matching set of labels.<a data-primary="DaemonSets" data-secondary="scheduler" data-type="indexterm" id="idm45664076672048"/><a data-primary="scheduler" data-secondary="DaemonSet" data-type="indexterm" id="idm45664076671072"/> DaemonSets determine which node a Pod will run on at Pod creation time by specifying the <code>nodeName</code> field in the Pod spec. As a result, Pods created by DaemonSets are ignored by the Kubernetes scheduler.</p>
<p>Like ReplicaSets, DaemonSets are managed by a reconciliation control loop that measures the desired state (a Pod is present on all nodes) with the observed state (is the Pod present on a particular node?). <a data-primary="reconciliation loops" data-type="indexterm" id="idm45664076668736"/>Given this information, the DaemonSet controller creates a Pod on each node that doesn’t currently have a matching Pod.</p>
<p>If a new node is added to the cluster, then the DaemonSet controller notices that it is missing a Pod and adds the Pod to the new node.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>DaemonSets and ReplicaSets are a great demonstration of the value of decoupled architecture.<a data-primary="decoupled architectures" data-secondary="DaemonSets and ReplicaSets in Kubernetes" data-type="indexterm" id="idm45664076666112"/><a data-primary="DaemonSets" data-secondary="demonstrating value of decoupled architecture in Kubernetes" data-type="indexterm" id="idm45664076665040"/> It might seem that the right design would be for a ReplicaSet to own the Pods it manages, and for Pods to be subresources of a ReplicaSet. Likewise, the Pods managed by a DaemonSet would be subresources of that DaemonSet.  However, this kind of encapsulation would require that tools for dealing with Pods be written twice: once for DaemonSets and once for ReplicaSets.  Instead, Kubernetes uses a decoupled approach where Pods are top-level objects. This means that every tool you have learned for introspecting Pods in the context of ReplicaSets (e.g., <code>kubectl logs &lt;<em>pod-name</em>&gt;</code>) is equally applicable to Pods created by DaemonSets.</p>
</div>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Creating DaemonSets" data-type="sect1"><div class="sect1" id="idm45664076662640">
<h1>Creating DaemonSets</h1>
<p>DaemonSets are created by submitting a DaemonSet configuration to the Kubernetes API server.<a data-primary="fluentd tool" data-secondary="creating logging agent on every node in target cluster" data-type="indexterm" id="idm45664076660576"/><a data-primary="DaemonSets" data-secondary="creating" data-type="indexterm" id="ix_DaeScr"/><a data-primary="nodes" data-secondary="DaemonSet creating fluentd agent on each node in cluster" data-type="indexterm" id="ix_nodeDS"/> The DaemonSet in <a data-type="xref" href="#example0901">Example 11-1</a> will create a <code>fluentd</code> logging agent on every node in the target cluster.</p>
<div data-type="example" id="example0901">
<h5><span class="label">Example 11-1. </span>fluentd.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">DaemonSet</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluentd</code><code class="w"/>
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluentd</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluentd</code><code class="w"/>
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluentd</code><code class="w"/>
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluentd</code><code class="w"/>
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">fluent/fluentd:v0.14.10</code><code class="w"/>
<code class="w">        </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>
<code class="w">          </code><code class="nt">limits</code><code class="p">:</code><code class="w"/>
<code class="w">            </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">200Mi</code><code class="w"/>
<code class="w">          </code><code class="nt">requests</code><code class="p">:</code><code class="w"/>
<code class="w">            </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">100m</code><code class="w"/>
<code class="w">            </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">200Mi</code><code class="w"/>
<code class="w">        </code><code class="nt">volumeMounts</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">varlog</code><code class="w"/>
<code class="w">          </code><code class="nt">mountPath</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/var/log</code><code class="w"/>
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">varlibdockercontainers</code><code class="w"/>
<code class="w">          </code><code class="nt">mountPath</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/var/lib/docker/containers</code><code class="w"/>
<code class="w">          </code><code class="nt">readOnly</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">true</code><code class="w"/>
<code class="w">      </code><code class="nt">terminationGracePeriodSeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">30</code><code class="w"/>
<code class="w">      </code><code class="nt">volumes</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">varlog</code><code class="w"/>
<code class="w">        </code><code class="nt">hostPath</code><code class="p">:</code><code class="w"/>
<code class="w">          </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/var/log</code><code class="w"/>
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">varlibdockercontainers</code><code class="w"/>
<code class="w">        </code><code class="nt">hostPath</code><code class="p">:</code><code class="w"/>
<code class="w">          </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/var/lib/docker/containers</code><code class="w"/></pre></div>
<p>DaemonSets require a unique name across all DaemonSets in a given Kubernetes namespace. Each DaemonSet must include a Pod template spec, which will be used to create Pods as needed. This is where the similarities between ReplicaSets and DaemonSets end. Unlike ReplicaSets, DaemonSets will create Pods on every node in the cluster by default unless a node selector is used.</p>
<p>Once you have a valid DaemonSet configuration in place, you can use the <code>kubectl apply</code> command to submit the DaemonSet to the Kubernetes API.<a data-primary="kubectl tool" data-secondary="commands" data-tertiary="apply -f" data-type="indexterm" id="idm45664076409632"/> In this section, we will create a DaemonSet to ensure the <code>fluentd</code> HTTP server is running on every node in our cluster:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f fluentd.yaml</strong>
daemonset.apps/fluentd created</pre>
<p>Once the <code>fluentd</code> DaemonSet has been successfully submitted to the Kubernetes API, you can query its current <a data-primary="kubectl tool" data-secondary="commands" data-tertiary="describe daemonset" data-type="indexterm" id="idm45664076439920"/>state using the <code>kubectl describe</code> command:</p>
<pre data-type="programlisting">$ <strong>kubectl describe daemonset fluentd</strong>
Name:           fluentd
Selector:       app=fluentd
Node-Selector:  &lt;none&gt;
Labels:         app=fluentd
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 3
Current Number of Nodes Scheduled: 3
Number of Nodes Scheduled with Up-to-date Pods: 3
Number of Nodes Scheduled with Available Pods: 3
Number of Nodes Misscheduled: 0
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
...</pre>
<p>This output indicates a <code>fluentd</code> Pod was successfully deployed to all three nodes in our cluster.<a data-primary="kubectl tool" data-secondary="commands" data-tertiary="get pods" data-type="indexterm" id="idm45664076435936"/> We can verify this using the <code>kubectl get pods</code> command with the <code>-o</code> flag to print the nodes where each <code>fluentd</code> Pod was assigned:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods -l app=fluentd -o wide</strong>
NAME            READY   STATUS    RESTARTS   AGE   IP             NODE
fluentd-1q6c6   1/1     Running   0          13m   10.240.0.101   k0-default...
fluentd-mwi7h   1/1     Running   0          13m   10.240.0.80    k0-default...
fluentd-zr6l7   1/1     Running   0          13m   10.240.0.44    k0-default...</pre>
<p>With the <code>fluentd</code> DaemonSet in place, adding a new node to the cluster will result in a <code>fluentd</code> Pod being deployed to that node automatically:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods -l app=fluentd -o wide</strong>
NAME            READY   STATUS    RESTARTS   AGE   IP             NODE
fluentd-1q6c6   1/1     Running   0          13m   10.240.0.101   k0-default...
fluentd-mwi7h   1/1     Running   0          13m   10.240.0.80    k0-default...
fluentd-oipmq   1/1     Running   0          43s   10.240.0.96    k0-default...
fluentd-zr6l7   1/1     Running   0          13m   10.240.0.44    k0-default...</pre>
<p>This is exactly the behavior you want when managing logging daemons and other cluster-wide services. No action was required from our end; this is how the Kubernetes DaemonSet controller reconciles its observed state with our desired state.<a data-primary="nodes" data-secondary="DaemonSet creating fluentd agent on each node in cluster" data-startref="ix_nodeDS" data-type="indexterm" id="idm45664076428928"/><a data-primary="DaemonSets" data-secondary="creating" data-startref="ix_DaeScr" data-type="indexterm" id="idm45664076427712"/></p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Limiting DaemonSets to Specific Nodes" data-type="sect1"><div class="sect1" id="idm45664076662048">
<h1>Limiting DaemonSets to Specific Nodes</h1>
<p>The most common use case for DaemonSets is to run a Pod across every node in a Kubernetes cluster. <a data-primary="DaemonSets" data-secondary="limiting to particular nodes" data-type="indexterm" id="idm45664076424608"/><a data-primary="nodes" data-secondary="limiting DaemonSets to particular nodes" data-type="indexterm" id="idm45664076423568"/>However, there are some cases where you want to deploy a Pod to only a subset of nodes. For example, maybe you have a workload that requires a GPU or access to fast storage only available on a subset of nodes in your cluster. In cases like these, node labels can be used to tag specific nodes that meet workload <span class="keep-together">requirements</span>.</p>
<section data-pdf-bookmark="Adding Labels to Nodes" data-type="sect2"><div class="sect2" id="idm45664076421696">
<h2>Adding Labels to Nodes</h2>
<p>The first step in limiting DaemonSets to specific nodes is to add the desired set of labels to a subset of nodes.<a data-primary="DaemonSets" data-secondary="limiting to particular nodes" data-tertiary="adding labels to nodes" data-type="indexterm" id="idm45664076419968"/><a data-primary="nodes" data-secondary="limiting DaemonSets to particular nodes" data-tertiary="adding labels to nodes" data-type="indexterm" id="idm45664076367744"/><a data-primary="labels" data-secondary="adding to nodes" data-type="indexterm" id="idm45664076366624"/><a data-primary="kubectl tool" data-secondary="commands" data-tertiary="label" data-type="indexterm" id="idm45664076365680"/> This can be achieved using the <code>kubectl label</code> command.</p>
<p>The following command adds the <code>ssd=true</code> label to a single node:</p>
<pre data-type="programlisting">$ <strong>kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true</strong>
node/k0-default-pool-35609c18-z7tb labeled</pre>
<p>Just like with other Kubernetes resources, listing nodes without a label selector returns all nodes in the cluster:</p>
<pre data-type="programlisting">$ <strong>kubectl get nodes</strong>
NAME                            STATUS   ROLES    AGE   VERSION
k0-default-pool-35609c18-0xnl   Ready    agent    23m   v1.21.1
k0-default-pool-35609c18-pol3   Ready    agent    1d    v1.21.1
k0-default-pool-35609c18-ydae   Ready    agent    1d    v1.21.1
k0-default-pool-35609c18-z7tb   Ready    agent    1d    v1.21.1</pre>
<p>Using a label selector, we can filter nodes based on labels.<a data-primary="labels" data-secondary="adding to nodes" data-tertiary="filtering nodes with label selector" data-type="indexterm" id="idm45664076359568"/><a data-primary="kubectl tool" data-secondary="commands" data-tertiary="get nodes --selector" data-type="indexterm" id="idm45664076358304"/> To list only the nodes that have the <code>ssd</code> label set to <code>true</code>, use the <code>kubectl get nodes</code> command with the <code class="keep-together">--selector</code> flag:</p>
<pre data-type="programlisting">$ <strong>kubectl get nodes --selector ssd=true</strong>
NAME                            STATUS   ROLES   AGE   VERSION
k0-default-pool-35609c18-z7tb   Ready    agent   1d    v1.21.1</pre>
</div></section>
<section data-pdf-bookmark="Node Selectors" data-type="sect2"><div class="sect2" id="idm45664076353792">
<h2>Node Selectors</h2>
<p>Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. <a data-primary="DaemonSets" data-secondary="limiting to particular nodes" data-tertiary="node selectors" data-type="indexterm" id="idm45664076352240"/><a data-primary="Pods" data-secondary="node selectors limiting nodes Pod can run on" data-type="indexterm" id="idm45664076350928"/><a data-primary="nodes" data-secondary="limiting DaemonSets to particular nodes" data-tertiary="node selectors" data-type="indexterm" id="idm45664076350080"/><a data-primary="NGINX" data-secondary="limiting to nodes with label set to ssd=true" data-type="indexterm" id="idm45664076348992"/>Node selectors are defined as part of the Pod spec when creating a DaemonSet. The DaemonSet configuration in <a data-type="xref" href="#example0902">Example 11-2</a> limits NGINX to running only on nodes with the <code>ssd=true</code> label set.</p>
<div class="pagebreak-before less_space" data-type="example" id="example0902">
<h5><span class="label">Example 11-2. </span>nginx-fast-storage.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="s">"DaemonSet"</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>
<code class="w">    </code><code class="nt">ssd</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-fast-storage</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>
<code class="w">      </code><code class="nt">ssd</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>
<code class="w">        </code><code class="nt">ssd</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">nodeSelector</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="nt">ssd</code><code class="p">:</code><code class="w"> </code><code class="s">"true"</code><code class="w"/>
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>
<code class="w">          </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:1.10.0</code><code class="w"/></pre></div>
<p>Let’s see what<a data-primary="nginx-fast-storage DaemonSet, submitting to Kubernetes API" data-type="indexterm" id="idm45664076240400"/> happens when we submit the <code>nginx-fast-storage</code> DaemonSet to the Kubernetes API:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f nginx-fast-storage.yaml</strong>
daemonset.apps/nginx-fast-storage created</pre>
<p>Since there is only one node with the <code>ssd=true</code> label, the <code>nginx-fast-storage</code> Pod will only run on that node:</p>
<pre data-type="programlisting">$ <strong>kubectl get pods -l app=nginx -o wide</strong>
NAME                       READY   STATUS    RESTARTS   AGE   IP            NODE
nginx-fast-storage-7b90t   1/1     Running   0          44s   10.240.0.48   ...</pre>
<p>Adding the <code>ssd=true</code> label to additional nodes will cause the <code>nginx-fast-storage</code> Pod to be deployed on those nodes. The inverse is also true: if a required label is removed from a node, the Pod will be removed by the DaemonSet controller.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Removing labels from a node that are required by a DaemonSet’s node selector will cause the Pod being managed by that DaemonSet to be removed from the node.</p>
</div>
</div></section>
</div></section>
<section data-pdf-bookmark="Updating a DaemonSet" data-type="sect1"><div class="sect1" id="idm45664076283392">
<h1>Updating a DaemonSet</h1>
<p>DaemonSets are great for deploying services across an entire cluster, but what about upgrades?<a data-primary="DaemonSets" data-secondary="updating" data-type="indexterm" id="idm45664076281600"/> Prior to Kubernetes 1.6, the only way to update Pods managed by a DaemonSet was to update the DaemonSet and then manually delete each Pod that was managed by the DaemonSet so that it would be re-created with the new configuration. With the release of Kubernetes 1.6, DaemonSets gained an equivalent to the Deployment object that manages a ReplicaSet rollout inside the cluster.</p>
<p>DaemonSets can be rolled out using the same <code>RollingUpdate</code> strategy that Deployments use.<a data-primary="RollingUpdate strategy" data-secondary="configuring for DaemonSet updates" data-type="indexterm" id="idm45664076279472"/><a data-primary="strategies" data-secondary="RollingUpdate, configuring for DaemonSets" data-type="indexterm" id="idm45664076278528"/> You can configure the update strategy using the <code>spec.update​Strat⁠egy.type</code> field, which should have the value <code>RollingUpdate</code>. When a DaemonSet has an update strategy of <code class="keep-together">RollingUpdate</code>, any change to the <code>spec.template</code> field (or subfields) in the DaemonSet will initiate a rolling update.</p>
<p>As with rolling updates of Deployments (see <a data-type="xref" href="ch10.xhtml#deployments_chapter">Chapter 10</a>), the <code>RollingUpdate</code> strategy gradually updates members of a DaemonSet until all of the Pods are running the new configuration. There are two parameters that control the rolling update of a <span class="keep-together">DaemonSet</span>:</p>
<dl>
<dt><code>spec.minReadySeconds</code></dt>
<dd>
<p>Determines how long a Pod must be “ready” before the rolling update proceeds to upgrade subsequent Pods</p>
</dd>
<dt><code>spec.updateStrategy.rollingUpdate.maxUnavailable</code></dt>
<dd>
<p>Indicates how many Pods may be simultaneously updated by the rolling update</p>
</dd>
</dl>
<p>You will likely want to set <code>spec.minReadySeconds</code> to a reasonably long value, for example 30–60 seconds, to ensure that your Pod is truly healthy before the rollout proceeds.</p>
<p>The setting for <code>spec.updateStrategy.rollingUpdate.maxUnavailable</code> is more likely to be application-dependent.<a data-primary="minReadySeconds parameter" data-type="indexterm" id="idm45664076165216"/> Setting it to <code>1</code> is a safe, general-purpose strategy, but it also takes a while to complete the rollout (number of nodes × <code>minReady​Sec⁠onds</code>). Increasing the maximum unavailability will make your rollout move faster, but increases the “blast radius” of a failed rollout. The characteristics of your application and cluster environment dictate the relative values of speed versus safety.<a data-primary="maxUnavailable parameter" data-type="indexterm" id="idm45664076163136"/> A good approach might be to set <code>maxUnavailable</code> to <code>1</code> and only increase it if users or administrators complain about DaemonSet rollout speed.<a data-primary="kubectl tool" data-secondary="commands" data-tertiary="rollout status" data-type="indexterm" id="idm45664076161344"/><a data-primary="rollouts" data-secondary="monitoring status of" data-type="indexterm" id="idm45664076160096"/></p>
<p>Once a rolling update has started, you can use the <code>kubectl rollout</code> commands to see the current status of a DaemonSet rollout. For example, <code>kubectl rollout status daemonSets my-daemon-set</code> will show
the current rollout status of a DaemonSet named <code>my-daemon-set</code>.</p>
</div></section>
<section data-pdf-bookmark="Deleting a DaemonSet" data-type="sect1"><div class="sect1" id="idm45664076157424">
<h1>Deleting a DaemonSet</h1>
<p>Deleting a DaemonSet using the <code>kubectl delete</code> command is pretty straightfoward.<a data-primary="DaemonSets" data-secondary="deleting" data-type="indexterm" id="idm45664076155632"/><a data-primary="kubectl tool" data-secondary="commands" data-tertiary="delete" data-type="indexterm" id="idm45664076154656"/> Just be sure to supply the correct name of the DaemonSet you would like to delete:</p>
<pre data-type="programlisting">$ <strong>kubectl delete -f fluentd.yaml</strong></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. Set the <code>--cascade</code> flag to <code>false</code> to ensure only the DaemonSet is deleted and not the Pods.</p>
</div>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45664076149856">
<h1>Summary</h1>
<p>DaemonSets provide an easy-to-use abstraction for running a set of Pods on every node in a Kubernetes cluster, or, if the case requires it, on a subset of nodes based on labels. The DaemonSet provides its own controller and scheduler to ensure key services like monitoring agents are always up and running on the right nodes in your cluster.</p>
<p>For some applications, you simply want to schedule a certain number of replicas; you don’t really care where they run as long as they have sufficient resources and distribution to operate reliably. However, there is a different class of applications, like agents and monitoring applications, that needs to be present on every machine in a cluster to function properly. These DaemonSets aren’t really traditional serving applications, but rather add additional capabilities and features to the Kubernetes cluster itself. Because the DaemonSet is an active declarative object managed by a controller, it makes it easy to declare your intent that an agent run on every machine without explicitly placing it on every machine. This is especially useful in the context of an autoscaled Kubernetes cluster where nodes may constantly be coming and going without user intervention. In such cases, the DaemonSet automatically adds the proper agents to each node as the autoscaler adds the node to the cluster.<a data-primary="DaemonSets" data-startref="ix_DaeS" data-type="indexterm" id="idm45664076147728"/></p>
</div></section>
</div></section></div></body></html>