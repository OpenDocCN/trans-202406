<html><head></head><body><section data-pdf-bookmark="Chapter 14. Running Machine Learning in Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="running_machine_learning_in_kubernetes">&#13;
<h1><span class="label">Chapter 14. </span>Running Machine Learning in Kubernetes</h1>&#13;
&#13;
&#13;
<p>The age of microservices, distributed systems, and the cloud has&#13;
provided the perfect environmental conditions for the democratization of&#13;
machine learning models and tooling. Infrastructure at scale has now&#13;
become commoditized, and the tooling around the machine learning&#13;
ecosystem is maturing. Kubernetes is one of the&#13;
&#13;
<span class="keep-together">platforms that</span> has become increasingly popular among developers, data scientists,&#13;
and the wider open source community as the perfect environment to&#13;
enable the machine learning workflow and life cycle. Large machine learning models like <a href="https://oreil.ly/sGzRc">GPT-4</a> and <a href="https://oreil.ly/zTWNx">DALL·E</a> have brought machine learning into the spotlight and organizations like <a href="https://oreil.ly/bCXwF">OpenAI</a> have been very public about their use of Kubernetes to support these models. In this chapter, we&#13;
will cover why Kubernetes is a great platform for machine learning and&#13;
provide best practices for both cluster administrators and data&#13;
scientists alike on how to get the most out of Kubernetes when running&#13;
machine learning workloads. Specifically, we focus on<a data-primary="deep learning" data-see="machine learning" data-type="indexterm" id="id938"/> deep&#13;
learning rather than traditional machine learning because deep learning has&#13;
quickly become the area of innovation on platforms like Kubernetes.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Why Is Kubernetes Great for Machine Learning?" data-type="sect1"><div class="sect1" id="id230">&#13;
<h1>Why Is Kubernetes Great for Machine Learning?</h1>&#13;
&#13;
<p>Kubernetes <a data-primary="machine learning" data-secondary="advantages of Kubernetes" data-type="indexterm" id="machine-learn-advantage"/>has quickly become the home for rapid innovation in deep&#13;
learning. The confluence of tooling and libraries such as <a href="https://oreil.ly/nzHaG">TensorFlow</a>&#13;
makes this technology more accessible to a large audience of data&#13;
scientists. What makes Kubernetes such a great place to run your deep&#13;
learning workloads? Let’s cover what Kubernetes provides:</p>&#13;
<dl class="less_space pagebreak-before">&#13;
<dt>Ubiquitous</dt>&#13;
<dd>&#13;
<p>Kubernetes is everywhere. All the major public clouds support it, and there are distributions for private clouds and infrastructure. Basing ecosystem tooling on a platform like Kubernetes allows users to run their deep learning workloads <span class="keep-together">anywhere.</span></p>&#13;
</dd>&#13;
<dt>Scalable</dt>&#13;
<dd>&#13;
<p>Deep learning workflows typically need access to large amounts of computing power to efficiently train machine learning models. Kubernetes ships with native autoscaling capabilities that make it easy for data scientists to achieve and fine-tune the level of scale they need to train their models.</p>&#13;
</dd>&#13;
<dt>Extensible</dt>&#13;
<dd>&#13;
<p>Efficiently training a machine learning model typically requires access to specialized hardware. Kubernetes allows cluster administrators to quickly and easily expose new types of hardware to the scheduler without having to change the Kubernetes source code. It also allows custom resources and controllers to be seamlessly integrated into the Kubernetes API to support specialized workflows, such as hyperparameter tuning.</p>&#13;
</dd>&#13;
<dt>Self-service</dt>&#13;
<dd>&#13;
<p>Data scientists can use Kubernetes to perform self-service machine learning workflows on demand, without needing specialized knowledge of Kubernetes itself.</p>&#13;
</dd>&#13;
<dt>Portable</dt>&#13;
<dd>&#13;
<p>Machine learning models can be run anywhere, provided that the tooling is based on the Kubernetes API. This allows machine learning workloads to be portable across Kubernetes <a data-primary="machine learning" data-secondary="advantages of Kubernetes" data-startref="machine-learn-advantage" data-type="indexterm" id="id939"/>providers.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Machine Learning Workflow" data-type="sect1"><div class="sect1" id="id117">&#13;
<h1>Machine Learning Workflow</h1>&#13;
&#13;
<p>To <a data-primary="machine learning" data-secondary="workflow" data-type="indexterm" id="machine-learn-workflow"/>effectively understand the needs of deep learning, you must&#13;
understand the complete machine learning workflow. <a data-type="xref" href="#machine_learning_development_workflow">Figure 14-1</a> represents a&#13;
simplified workflow.</p>&#13;
&#13;
<figure><div class="figure" id="machine_learning_development_workflow">&#13;
<img alt="Machine learning development workflow" src="assets/kbp2_1401.png"/>&#13;
<h6><span class="label">Figure 14-1. </span>Machine learning development workflow</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">As you can see, the workflow has the following phases:</p>&#13;
<dl>&#13;
<dt>Dataset preparation</dt>&#13;
<dd>&#13;
<p>    This phase<a data-primary="dataset preparation phase (machine learning)" data-type="indexterm" id="id940"/> includes the storage, indexing,&#13;
cataloging, and metadata associated with the dataset used to&#13;
train the model. For the purposes of this book, we consider only the storage aspect. Datasets vary in size, from hundreds of&#13;
<span class="keep-together">megabytes</span> to hundreds of terabytes, and even petabytes, and need to be provided to&#13;
the model in order for the model to be trained. You must consider&#13;
storage that provides the appropriate properties to meet these needs.&#13;
Typically, large-scale block and object stores are required and must be&#13;
accessible via Kubernetes-native storage abstractions or directly&#13;
accessible APIs.</p>&#13;
</dd>&#13;
<dt>Model development</dt>&#13;
<dd>&#13;
<p>    In <a data-primary="algorithm development phase (machine learning)" data-type="indexterm" id="id941"/>this phase, data&#13;
scientists write, share, and collaborate on machine learning algorithms.&#13;
Open source tools like JupyterHub are easy to install on Kubernetes because&#13;
they typically function like any other workload.</p>&#13;
</dd>&#13;
<dt>Training</dt>&#13;
<dd>&#13;
<p>For <a data-primary="training phase (machine learning)" data-type="indexterm" id="training-machine-learning"/>a model to use the dataset to learn how to perform the tasks it’s designed to perform, it must be trained. The resulting artifact&#13;
of the training process is usually a checkpoint of the trained model state. The training process is&#13;
the piece that takes advantage of all the capabilities of Kubernetes at the&#13;
same time. Scheduling, access to specialized hardware, dataset volume&#13;
management, scaling, and networking will all be exercised in unison to complete this task. We cover more of the specifics of the&#13;
training phase in the next section.</p>&#13;
</dd>&#13;
<dt>Serving</dt>&#13;
<dd>&#13;
<p>    This is <a data-primary="serving phase (machine learning)" data-type="indexterm" id="id942"/>the process of making the trained model accessible to&#13;
service requests from clients so that it can make an inference based on the data&#13;
supplied from the client. For example, if you have an image-recognition&#13;
model that’s been trained to detect dogs and cats, a client might&#13;
submit a picture of a dog, and the model should be able to determine whether it&#13;
is a dog, with a certain level of<a data-primary="machine learning" data-secondary="workflow" data-startref="machine-learn-workflow" data-type="indexterm" id="id943"/> accuracy.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Machine Learning for Kubernetes Cluster Admins" data-type="sect1"><div class="sect1" id="id369">&#13;
<h1>Machine Learning for Kubernetes Cluster Admins</h1>&#13;
&#13;
<p>There are a few topics to consider before&#13;
running machine learning workloads on your Kubernetes cluster. This section is&#13;
specifically targeted to cluster administrators. The largest&#13;
challenge you will face as a cluster administrator responsible&#13;
for a team of data scientists is understanding the terminology. There&#13;
are myriad new terms that you must become familiar with over time,&#13;
but rest assured, you can do it. Let’s look at the main&#13;
problem areas you’ll need to address when preparing a cluster&#13;
for machine learning workloads.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Model Training on Kubernetes" data-type="sect2"><div class="sect2" id="id118">&#13;
<h2>Model Training on Kubernetes</h2>&#13;
&#13;
<p>Training <a data-primary="machine learning" data-secondary="model training" data-type="indexterm" id="machine-learn-model-train"/><a data-primary="model training" data-type="indexterm" id="model-train"/>machine learning models on Kubernetes requires conventional CPUs and graphics processing units (GPUs). Typically,&#13;
the more resources you apply, the faster the training will be completed.&#13;
In most cases, model training can be achieved on a single machine that has the required resources.&#13;
Many cloud providers offer <span class="keep-together">multi-GPU</span> virtual machine (VM) types, so&#13;
we recommend scaling VMs vertically to four to eight GPUs before looking&#13;
into distributed training. Data scientists use a technique known as <em>hyperparameter tuning</em> when <a data-primary="hyperparameter tuning" data-type="indexterm" id="id944"/>training models. A hyperparameter is simply a parameter that has a set value before the training process begins. Hyperparameter tuning is the process of finding the optimal set of&#13;
hyperparameters for model training. The technique involves running many of the same training jobs with a&#13;
different set of hyperparameters.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Training your first model on Kubernetes" data-type="sect3"><div class="sect3" id="id119">&#13;
<h3>Training your first model on Kubernetes</h3>&#13;
&#13;
<p>In this example, you are going to use the MNIST dataset to train an image-classification model. The MNIST dataset is publicly available and commonly used for image classification.</p>&#13;
&#13;
<p>To train the model, you need GPUs. Let’s confirm that your Kubernetes&#13;
cluster has GPUs available. The following command shows how many GPUs are&#13;
available in a Kubernetes cluster. From the output we can see that this cluster has four GPUs available:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">$<code class="w"> </code>kubectl<code class="w"> </code>get<code class="w"> </code>nodes<code class="w"> </code>-o<code class="w"> </code>yaml<code class="w"> </code><code class="p">|</code><code class="w"> </code>grep<code class="w"> </code>-i<code class="w"> </code>nvidia.com/gpu<code class="w"/>&#13;
<code class="w">      </code>nvidia.com/gpu:<code class="w"> </code><code class="s2">"1"</code><code class="w"/>&#13;
<code class="w">      </code>nvidia.com/gpu:<code class="w"> </code><code class="s2">"1"</code><code class="w"/>&#13;
<code class="w">      </code>nvidia.com/gpu:<code class="w"> </code><code class="s2">"1"</code><code class="w"/>&#13;
<code class="w">      </code>nvidia.com/gpu:<code class="w"> </code><code class="s2">"1"</code><code class="w"/></pre>&#13;
&#13;
<p>Given that training is a batch workload, to run your training you’re going to use the <code>Job</code> kind in Kubernetes. You will run your training for 500 steps and use a single GPU. Create a file&#13;
called <em>mnist-demo.yaml</em> using the following manifest, and save it to your filesystem:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">batch/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Job</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mnist-demo</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mnist-demo</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mnist-demo</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mnist-demo</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">lachlanevenson/tf-mnist:gpu</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">args</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"--max_steps"</code><code class="p-Indicator">,</code><code class="w"> </code><code class="s">"500"</code><code class="p-Indicator">]</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">imagePullPolicy</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">IfNotPresent</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">limits</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">           </code><code class="nt">nvidia.com/gpu</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">restartPolicy</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">OnFailure</code><code class="w"/></pre>&#13;
&#13;
<p>Now, create this resource on your Kubernetes cluster:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">$<code class="w"> </code>kubectl<code class="w"> </code>create<code class="w"> </code>-f<code class="w"> </code>mnist-demo.yaml<code class="w"/>&#13;
job.batch/mnist-demo<code class="w"> </code>created<code class="w"/></pre>&#13;
&#13;
<p>Check the status of the job you just created:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">$<code class="w"> </code>kubectl<code class="w"> </code>get<code class="w"> </code><code class="nb">jobs</code><code class="w"/>&#13;
NAME<code class="w">         </code>COMPLETIONS<code class="w">   </code>DURATION<code class="w">   </code>AGE<code class="w"/>&#13;
mnist-demo<code class="w">   </code><code class="m">1</code>/1<code class="w">           </code>31s<code class="w">        </code>49s<code class="w"/></pre>&#13;
&#13;
<p>If you look at the pods, you should see the training job running:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">$<code class="w"> </code>kubectl<code class="w"> </code>get<code class="w"> </code>pods<code class="w"/>&#13;
NAME<code class="w">               </code>READY<code class="w">   </code>STATUS<code class="w">    </code>RESTARTS<code class="w">   </code>AGE<code class="w"/>&#13;
mnist-demo-8lqrn<code class="w">   </code><code class="m">1</code>/1<code class="w">     </code>Running<code class="w">   </code><code class="m">0</code><code class="w">          </code>63s<code class="w"/></pre>&#13;
&#13;
<p>Looking at the pod logs, you can see the training happening:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">$<code class="w"> </code>$<code class="w"> </code>kubectl<code class="w"> </code>logs<code class="w"> </code>mnist-demo-8lqrn<code class="w"/>&#13;
<code class="m">2023</code>-02-10<code class="w"> </code><code class="m">23</code>:14:42.007518:<code class="w"> </code>I<code class="w"/>&#13;
<code class="w">  </code>tensorflow/core/platform/cpu_feature_guard.cc:137<code class="o">]</code><code class="w"> </code>Your<code class="w"> </code>CPU<code class="w"> </code>supports<code class="w"/>&#13;
<code class="w">    </code>instructions<code class="w"> </code>that<code class="w"> </code>this<code class="w"> </code>TensorFlow<code class="w"> </code>binary<code class="w"> </code>was<code class="w"> </code>not<code class="w"> </code>compiled<code class="w"> </code>to<code class="w"/>&#13;
<code class="w">      </code>use:<code class="w"> </code>SSE4.1<code class="w"> </code>SSE4.2<code class="w"> </code>AVX<code class="w"> </code>AVX2<code class="w"> </code>FMA<code class="w"/>&#13;
<code class="m">2023</code>-02-10<code class="w"> </code><code class="m">23</code>:14:42.205555:<code class="w"> </code>I<code class="w"/>&#13;
<code class="w">  </code>tensorflow/core/common_runtime/gpu/gpu_device.cc:1030<code class="o">]</code><code class="w"> </code>Found<code class="w"> </code>device<code class="w"> </code><code class="m">0</code><code class="w"> </code>with<code class="w"/>&#13;
<code class="w">         </code>properties:<code class="w"/>&#13;
name:<code class="w"> </code>Tesla<code class="w"> </code>K80<code class="w"> </code>major:<code class="w"> </code><code class="m">3</code><code class="w"> </code>minor:<code class="w"> </code><code class="m">7</code><code class="w"> </code>memoryClockRate<code class="o">(</code>GHz<code class="o">)</code>:<code class="w"> </code><code class="m">0</code>.8235<code class="w"/>&#13;
pciBusID:<code class="w"> </code><code class="m">0001</code>:00:00.0<code class="w"/>&#13;
totalMemory:<code class="w"> </code><code class="m">11</code>.17GiB<code class="w"> </code>freeMemory:<code class="w"> </code><code class="m">11</code>.12GiB<code class="w"/>&#13;
<code class="m">2023</code>-02-10<code class="w"> </code><code class="m">23</code>:14:42.205596:<code class="w"> </code>I<code class="w"/>&#13;
<code class="w">  </code>tensorflow/core/common_runtime/gpu/gpu_device.cc:1120<code class="o">]</code><code class="w"> </code>Creating<code class="w"> </code>TensorFlow<code class="w"/>&#13;
<code class="w">        </code>device<code class="w"> </code><code class="o">(</code>/device:GPU:0<code class="o">)</code><code class="w"> </code>-&gt;<code class="w"> </code><code class="o">(</code>device:<code class="w"> </code><code class="m">0</code>,<code class="w"> </code>name:<code class="w"> </code>Tesla<code class="w"> </code>K80,<code class="w"> </code>pci<code class="w"> </code>bus<code class="w"/>&#13;
<code class="w">          </code>id:<code class="w"> </code><code class="m">0001</code>:00:00.0,<code class="w"> </code>compute<code class="w"> </code>capability:<code class="w"> </code><code class="m">3</code>.7<code class="o">)</code><code class="w"/>&#13;
<code class="m">2023</code>-02-10<code class="w"> </code><code class="m">23</code>:14:46.848342:<code class="w"> </code>I<code class="w"/>&#13;
<code class="w">  </code>tensorflow/stream_executor/dso_loader.cc:139<code class="o">]</code><code class="w"> </code>successfully<code class="w"> </code>opened<code class="w"> </code>CUDA<code class="w"> </code>library<code class="w"/>&#13;
<code class="w">        </code>libcupti.so.8.0<code class="w"> </code>locally<code class="w"/>&#13;
Successfully<code class="w"> </code>downloaded<code class="w"> </code>train-images-idx3-ubyte.gz<code class="w"> </code><code class="m">9912422</code><code class="w"> </code>bytes.<code class="w"/>&#13;
Extracting<code class="w"> </code>/tmp/tensorflow/input_data/train-images-idx3-ubyte.gz<code class="w"/>&#13;
Successfully<code class="w"> </code>downloaded<code class="w"> </code>train-labels-idx1-ubyte.gz<code class="w"> </code><code class="m">28881</code><code class="w"> </code>bytes.<code class="w"/>&#13;
Extracting<code class="w"> </code>/tmp/tensorflow/input_data/train-labels-idx1-ubyte.gz<code class="w"/>&#13;
Successfully<code class="w"> </code>downloaded<code class="w"> </code>t10k-images-idx3-ubyte.gz<code class="w"> </code><code class="m">1648877</code><code class="w"> </code>bytes.<code class="w"/>&#13;
Extracting<code class="w"> </code>/tmp/tensorflow/input_data/t10k-images-idx3-ubyte.gz<code class="w"/>&#13;
Successfully<code class="w"> </code>downloaded<code class="w"> </code>t10k-labels-idx1-ubyte.gz<code class="w"> </code><code class="m">4542</code><code class="w"> </code>bytes.<code class="w"/>&#13;
Extracting<code class="w"> </code>/tmp/tensorflow/input_data/t10k-labels-idx1-ubyte.gz<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">0</code>:<code class="w"> </code><code class="m">0</code>.0886<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">10</code>:<code class="w"> </code><code class="m">0</code>.7094<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">20</code>:<code class="w"> </code><code class="m">0</code>.8354<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">30</code>:<code class="w"> </code><code class="m">0</code>.8667<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">40</code>:<code class="w"> </code><code class="m">0</code>.8833<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">50</code>:<code class="w"> </code><code class="m">0</code>.8902<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">60</code>:<code class="w"> </code><code class="m">0</code>.897<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">70</code>:<code class="w"> </code><code class="m">0</code>.9062<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">80</code>:<code class="w"> </code><code class="m">0</code>.9057<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">90</code>:<code class="w"> </code><code class="m">0</code>.906<code class="w"/>&#13;
Adding<code class="w"> </code>run<code class="w"> </code>metadata<code class="w"> </code><code class="k">for</code><code class="w"> </code><code class="m">99</code><code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">100</code>:<code class="w"> </code><code class="m">0</code>.9163<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">110</code>:<code class="w"> </code><code class="m">0</code>.9203<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">120</code>:<code class="w"> </code><code class="m">0</code>.9168<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">130</code>:<code class="w"> </code><code class="m">0</code>.9215<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">140</code>:<code class="w"> </code><code class="m">0</code>.9241<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">150</code>:<code class="w"> </code><code class="m">0</code>.9251<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">160</code>:<code class="w"> </code><code class="m">0</code>.9286<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">170</code>:<code class="w"> </code><code class="m">0</code>.9288<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">180</code>:<code class="w"> </code><code class="m">0</code>.9274<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">190</code>:<code class="w"> </code><code class="m">0</code>.9337<code class="w"/>&#13;
Adding<code class="w"> </code>run<code class="w"> </code>metadata<code class="w"> </code><code class="k">for</code><code class="w"> </code><code class="m">199</code><code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">200</code>:<code class="w"> </code><code class="m">0</code>.9361<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">210</code>:<code class="w"> </code><code class="m">0</code>.9369<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">220</code>:<code class="w"> </code><code class="m">0</code>.9365<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">230</code>:<code class="w"> </code><code class="m">0</code>.9328<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">240</code>:<code class="w"> </code><code class="m">0</code>.9409<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">250</code>:<code class="w"> </code><code class="m">0</code>.9428<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">260</code>:<code class="w"> </code><code class="m">0</code>.9408<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">270</code>:<code class="w"> </code><code class="m">0</code>.9432<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">280</code>:<code class="w"> </code><code class="m">0</code>.9438<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">290</code>:<code class="w"> </code><code class="m">0</code>.9433<code class="w"/>&#13;
Adding<code class="w"> </code>run<code class="w"> </code>metadata<code class="w"> </code><code class="k">for</code><code class="w"> </code><code class="m">299</code><code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">300</code>:<code class="w"> </code><code class="m">0</code>.9446<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">310</code>:<code class="w"> </code><code class="m">0</code>.9466<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">320</code>:<code class="w"> </code><code class="m">0</code>.9468<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">330</code>:<code class="w"> </code><code class="m">0</code>.9463<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">340</code>:<code class="w"> </code><code class="m">0</code>.9464<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">350</code>:<code class="w"> </code><code class="m">0</code>.9489<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">360</code>:<code class="w"> </code><code class="m">0</code>.9506<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">370</code>:<code class="w"> </code><code class="m">0</code>.9489<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">380</code>:<code class="w"> </code><code class="m">0</code>.9484<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">390</code>:<code class="w"> </code><code class="m">0</code>.9494<code class="w"/>&#13;
Adding<code class="w"> </code>run<code class="w"> </code>metadata<code class="w"> </code><code class="k">for</code><code class="w"> </code><code class="m">399</code><code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">400</code>:<code class="w"> </code><code class="m">0</code>.9513<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">410</code>:<code class="w"> </code><code class="m">0</code>.9474<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">420</code>:<code class="w"> </code><code class="m">0</code>.9499<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">430</code>:<code class="w"> </code><code class="m">0</code>.9462<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">440</code>:<code class="w"> </code><code class="m">0</code>.952<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">450</code>:<code class="w"> </code><code class="m">0</code>.952<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">460</code>:<code class="w"> </code><code class="m">0</code>.9487<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">470</code>:<code class="w"> </code><code class="m">0</code>.9569<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">480</code>:<code class="w"> </code><code class="m">0</code>.9547<code class="w"/>&#13;
Accuracy<code class="w"> </code>at<code class="w"> </code>step<code class="w"> </code><code class="m">490</code>:<code class="w"> </code><code class="m">0</code>.9516<code class="w"/>&#13;
Adding<code class="w"> </code>run<code class="w"> </code>metadata<code class="w"> </code><code class="k">for</code><code class="w"> </code><code class="m">499</code><code class="w"/></pre>&#13;
&#13;
<p>Finally, you can see that the training has completed by looking at the job status:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">$<code class="w"> </code>kubectl<code class="w"> </code>get<code class="w"> </code><code class="nb">jobs</code><code class="w"/>&#13;
NAME<code class="w">         </code>COMPLETIONS<code class="w">   </code>DURATION<code class="w">   </code>AGE<code class="w"/>&#13;
mnist-demo<code class="w">   </code><code class="m">1</code>/1<code class="w">           </code>31s<code class="w">        </code>2m19s<code class="w"/></pre>&#13;
&#13;
<p>To clean up the training job, simply run the following command:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">$<code class="w"> </code>kubectl<code class="w"> </code>delete<code class="w"> </code>-f<code class="w"> </code>mnist-demo.yaml<code class="w"/>&#13;
job.batch<code class="w"> </code><code class="s2">"mnist-demo"</code><code class="w"> </code>deleted<code class="w"/></pre>&#13;
&#13;
<p>Congratulations! You just ran your first model training job<a data-primary="machine learning" data-secondary="model training" data-startref="machine-learn-model-train" data-type="indexterm" id="id945"/><a data-primary="model training" data-startref="model-train" data-type="indexterm" id="id946"/> on Kubernetes.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Distributed Training on Kubernetes" data-type="sect2"><div class="sect2" id="id120">&#13;
<h2>Distributed Training on Kubernetes</h2>&#13;
&#13;
<p>Distributed training<a data-primary="machine learning" data-secondary="distributed training" data-type="indexterm" id="id947"/><a data-primary="distributed training" data-type="indexterm" id="id948"/> is still in its infancy and is&#13;
difficult to optimize. Running a training job that requires&#13;
eight GPUs will almost always be faster to train on a single eight-GPU machine compared to&#13;
two machines with four GPUs each. The only time you should resort to using distributed&#13;
training is when the model doesn’t fit on the biggest machine available.&#13;
If you are certain you must run distributed training, it is important&#13;
to understand the architecture. <a data-type="xref" href="#distributed_tensorflow_architecture">Figure 14-2</a> depicts the distributed&#13;
TensorFlow architecture, and you can see how the model and the<a data-primary="training phase (machine learning)" data-startref="training-machine-learning" data-type="indexterm" id="id949"/> parameters&#13;
are <span class="keep-together">distributed.</span></p>&#13;
&#13;
<figure><div class="figure" id="distributed_tensorflow_architecture">&#13;
<img alt="Distributed TensorFlow architecture" src="assets/kbp2_1402.png"/>&#13;
<h6><span class="label">Figure 14-2. </span>Distributed TensorFlow architecture</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Constraints" data-type="sect2"><div class="sect2" id="id121">&#13;
<h2>Resource Constraints</h2>&#13;
&#13;
<p>Machine learning<a data-primary="machine learning" data-secondary="resource constraints" data-type="indexterm" id="id950"/><a data-primary="resource constraints in machine learning" data-type="indexterm" id="id951"/><a data-primary="constraints" data-secondary="in machine learning" data-secondary-sortas="machine learning" data-type="indexterm" id="id952"/> workloads demand very specific configurations across&#13;
all aspects of your cluster. The training phases are most certainly the&#13;
most resource intensive. It’s also important to note, as we mentioned a moment ago, that machine&#13;
learning algorithm training is almost always a batch-style workload. Specifically, it&#13;
will have a start time and a finish time. The finish time of a training&#13;
run depends on how quickly you can meet the resource requirements of the&#13;
model training. This means that scaling is almost certainly a quicker&#13;
way to finish training jobs faster, but scaling has its own set of&#13;
bottlenecks.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Specialized Hardware" data-type="sect2"><div class="sect2" id="id122">&#13;
<h2>Specialized Hardware</h2>&#13;
&#13;
<p>Training <a data-primary="machine learning" data-secondary="specialized hardware for" data-type="indexterm" id="machine-learn-hardware"/><a data-primary="specialized hardware (machine learning)" data-type="indexterm" id="special-hardware"/><a data-primary="hardware for machine learning" data-type="indexterm" id="hardware-machine-learn"/>and serving a model is almost always more efficient on&#13;
specialized hardware. A typical example of such specialized hardware&#13;
would be commodity GPUs. Kubernetes allows you to access GPUs&#13;
via device plug-ins that make the GPU resource known to the Kubernetes&#13;
scheduler and therefore able to be scheduled. A device plug-in framework facilitates this capability, which means that vendors do not need&#13;
to modify the core Kubernetes code to implement their specific device.&#13;
These device plug-ins typically run on each node as DaemonSets, which are&#13;
processes that are responsible for advertising these specific resources&#13;
to the Kubernetes API. Let’s look at the&#13;
<a href="https://oreil.ly/RgKuz">NVIDIA device plug-in for Kubernetes</a>, which enables access to NVIDIA GPUs. After they’re running, you can create a pod as follows, and Kubernetes will ensure that it is&#13;
scheduled to a node that has these resources <span class="keep-together">available:</span></p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pod</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">gpu-pod</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">digits-container</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nvidia/digits:6.0</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">limits</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">nvidia.com/gpu</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2</code><code class="w"> </code><code class="c1"># requesting 2 GPUs</code><code class="w"/></pre>&#13;
&#13;
<p>Device plug-ins are not limited to GPUs; you can use them wherever specialized hardware is needed—for example, Field Programmable Gate Arrays (FPGAs) or InfiniBand.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scheduling idiosyncrasies" data-type="sect3"><div class="sect3" id="id123">&#13;
<h3>Scheduling idiosyncrasies</h3>&#13;
&#13;
<p>It’s important <a data-primary="scheduling" data-secondary="machine learning" data-type="indexterm" id="id953"/>to note that Kubernetes cannot make decisions about&#13;
resources that it does not have knowledge about. One of the things you&#13;
might notice is that the GPUs are not running at capacity when you are&#13;
training. You are therefore not achieving the level of&#13;
utilization you would like to see. Let’s consider the previous example; it exposes only the number of GPU cores and omits the number of threads&#13;
that can be run per core. It also doesn’t expose which bus the GPU core&#13;
is on, so that jobs that need access to one another or to the same memory might&#13;
be colocated on the same Kubernetes nodes. All these considerations might be addressed by device plug-ins in the future but for now might leave you&#13;
wondering why you cannot get 100% utilization on that beefy GPU you just&#13;
purchased. It’s also worth mentioning that you cannot request fractions of&#13;
GPUs (for example, 0.1), which means that even if the specific GPU supports running multiple&#13;
threads concurrently, you will not be able to utilize that <a data-primary="machine learning" data-secondary="specialized hardware for" data-startref="machine-learn-hardware" data-type="indexterm" id="id954"/><a data-primary="specialized hardware (machine learning)" data-startref="special-hardware" data-type="indexterm" id="id955"/><a data-primary="hardware for machine learning" data-startref="hardware-machine-learn" data-type="indexterm" id="id956"/>capacity.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Libraries, Drivers, and Kernel Modules" data-type="sect2"><div class="sect2" id="id124">&#13;
<h2>Libraries, Drivers, and Kernel Modules</h2>&#13;
&#13;
<p>To access <a data-primary="machine learning" data-secondary="libraries/drivers/kernel modules" data-type="indexterm" id="machine-learn-library"/><a data-primary="libraries for machine learning" data-type="indexterm" id="library-machine-learn"/><a data-primary="drivers for machine learning" data-type="indexterm" id="driver-machine-learn"/><a data-primary="kernel modules for machine learning" data-type="indexterm" id="kernel-machine-learn"/>specialized hardware, you typically need purpose-built libraries, drivers,&#13;
and kernel modules. You will need to ensure that&#13;
these are mounted into the container runtime so that they are available&#13;
to the tooling running in the container. You might ask, “Why don’t I just&#13;
add these to the container image itself?” The answer is simple: the tools&#13;
need to match the version on the underlying host and must be configured&#13;
appropriately for that specific system. Container runtimes&#13;
such as <a href="https://oreil.ly/Re0Ef">NVIDIA Docker</a> remove the burden of having to map host volumes into each container. In&#13;
lieu of having a purpose-built container runtime, you might be able to&#13;
build an admission webhook that provides the same functionality. It’s&#13;
also important to consider that you might need privileged containers to access some specialized hardware, which affects the cluster security&#13;
profile. The installation of the associated libraries, drivers, and kernel&#13;
modules might also be facilitated by Kubernetes device plug-ins. Many device plug-ins&#13;
run checks on each machine to confirm that all installations have been completed before&#13;
they advertise the schedulable GPU resources to the Kubernetes scheduler.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Storage" data-type="sect2"><div class="sect2" id="id231">&#13;
<h2>Storage</h2>&#13;
&#13;
<p>Storage<a data-primary="machine learning" data-secondary="storage" data-type="indexterm" id="id957"/><a data-primary="storage" data-secondary="for machine learning" data-secondary-sortas="machine learning" data-type="indexterm" id="id958"/> is one of the most critical aspects of the machine learning&#13;
workflow. You need to consider storage because it directly affects&#13;
the following pieces of the machine learning workflow:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Dataset storage and distribution among nodes during training</p>&#13;
</li>&#13;
<li>&#13;
<p>Checkpoints and saving models</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space" data-pdf-bookmark="Dataset storage and distribution among nodes during training" data-type="sect3"><div class="sect3" id="id125">&#13;
<h3>Dataset storage and distribution among nodes during training</h3>&#13;
&#13;
<p>During training, <a data-primary="datasets for machine learning" data-type="indexterm" id="id959"/>the dataset must be retrievable by every node.&#13;
The storage needs are read-only, and, typically, the faster the disk, the&#13;
better. The type of disk that’s providing the storage is almost&#13;
completely dependent on the size of the dataset. Datasets of hundreds of&#13;
megabytes or gigabytes might be perfect for block storage, but datasets&#13;
that are several or hundreds of terabytes in size might be better suited&#13;
to object storage. Depending on the size and location of the disks that&#13;
hold the datasets, there might be a performance hit on your networking.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Checkpoints and saving models" data-type="sect3"><div class="sect3" id="id126">&#13;
<h3>Checkpoints and saving models</h3>&#13;
&#13;
<p>Checkpoints are<a data-primary="checkpoints" data-type="indexterm" id="id960"/><a data-primary="saving machine learning models" data-type="indexterm" id="id961"/> created as a model is being trained, and saving models&#13;
allows you to use them for serving. In both cases, you need storage&#13;
attached to each of the nodes to store this data. The data is&#13;
typically stored under a single directory, and each node is&#13;
writing to a specific checkpoint or save file. Most tools expect the&#13;
checkpoint and save data to be in a single location and require&#13;
<code>ReadWriteMany</code>. <code>ReadWriteMany</code> simply means that the volume can be&#13;
mounted as read-write by many nodes. When using Kubernetes PersistentVolumes, you will need to determine the best storage platform for your&#13;
needs. The Kubernetes documentation keeps a&#13;
<a href="https://oreil.ly/aMjGd">list</a>&#13;
of volume plug-ins that support <code>ReadWriteMany</code>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Networking" data-type="sect2"><div class="sect2" id="id232">&#13;
<h2>Networking</h2>&#13;
&#13;
<p>The <a data-primary="machine learning" data-secondary="networking" data-type="indexterm" id="id962"/><a data-primary="networking" data-secondary="for machine learning" data-secondary-sortas="machine learning" data-type="indexterm" id="id963"/>training phase of the machine learning workflow has a large impact on the network (specifically, when running distributed training).&#13;
If we consider TensorFlow’s distributed architecture, two discrete phases create a lot of network traffic: variable distribution from each of the parameter servers to&#13;
each of the nodes, and the application of gradients from each node back to the parameter server (refer back to <a data-type="xref" href="#distributed_tensorflow_architecture">Figure 14-2</a>). The time it takes for this exchange to happen directly affects the time it takes to train a model. So, it’s a simple game of the faster, the better (within reason, of&#13;
course). With most public clouds and servers today supporting 1-Gbps, 10-Gbps, and&#13;
sometimes 40-Gbps network interface cards, generally network bandwidth is a concern only at lower bandwidths. You might also consider InfiniBand if you need high network bandwidth.</p>&#13;
&#13;
<p>While raw network bandwidth is more often than not a limiting factor,&#13;
in some instances the problem is getting the data onto the wire from the&#13;
kernel in the first place. Some open source projects&#13;
take advantage of Remote Direct Memory Access (RDMA) to further accelerate&#13;
network traffic without the need to modify your nodes or&#13;
application code. RDMA allows computers in a network to exchange data in&#13;
main memory without using the processor, cache, or operating system of&#13;
either computer.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Specialized Protocols" data-type="sect2"><div class="sect2" id="id127">&#13;
<h2>Specialized Protocols</h2>&#13;
&#13;
<p>Other<a data-primary="machine learning" data-secondary="specialized protocols for" data-type="indexterm" id="id964"/><a data-primary="specialized protocols (machine learning)" data-type="indexterm" id="id965"/> specialized protocols you can consider when using&#13;
machine learning on Kubernetes are often vendor&#13;
specific, but they all seek to address distributed training scaling&#13;
issues by removing areas of the architecture that quickly become&#13;
bottlenecks. For example, parameter servers. These protocols often allow&#13;
the direct exchange of information between GPUs on multiple nodes&#13;
without the need to involve the node CPU and OS. Here are a&#13;
couple you might want to look into to more efficiently scale&#13;
your distributed training:</p>&#13;
<dl>&#13;
<dt>Message Passing Interface (MPI)</dt>&#13;
<dd>&#13;
<p>A <a data-primary="Message Passing Interface (MPI)" data-type="indexterm" id="id966"/>standardized portable API for the transfer of data between distributed processes</p>&#13;
</dd>&#13;
<dt>NVIDIA Collective Communications Library (NCCL)</dt>&#13;
<dd>&#13;
<p>A library of topology-aware multi-GPU <a data-primary="NVIDIA Collective Communications Library (NCCL)" data-type="indexterm" id="id967"/>communication primitives</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Scientist Concerns" data-type="sect1"><div class="sect1" id="id128">&#13;
<h1>Data Scientist Concerns</h1>&#13;
&#13;
<p>Earlier in<a data-primary="machine learning" data-secondary="data scientist tools" data-type="indexterm" id="machine-learn-data-science-tools"/><a data-primary="data scientist tools" data-type="indexterm" id="data-science-tools"/> the chapter, we shared considerations you need to make in&#13;
order to be able to run machine learning workloads on your Kubernetes&#13;
cluster. But what about the data scientist? Here we cover&#13;
some popular tools that make it easy for data scientists to utilize&#13;
Kubernetes for machine learning without having to be a Kubernetes&#13;
expert:</p>&#13;
<dl>&#13;
<dt><a href="https://oreil.ly/UVxjM">Kubeflow</a></dt>&#13;
<dd>&#13;
<p>A<a data-primary="Kubeflow" data-type="indexterm" id="id968"/> machine learning toolkit for Kubernetes, it is native to Kubernetes and ships with several tools necessary to complete the machine learning workflow. Tools such as Jupyter Notebooks, pipelines, and Kubernetes-native controllers make it simple and easy for data scientists to get the most out of Kubernetes as a platform for machine learning.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/NZ7Nj">Polyaxon</a></dt>&#13;
<dd>&#13;
<p>A tool for <a data-primary="Polyaxon" data-type="indexterm" id="id969"/>managing machine learning workflows that supports many popular libraries and runs on any Kubernetes cluster. Polyaxon has both commercial and open source offerings.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/CivM_">Pachyderm</a></dt>&#13;
<dd>&#13;
<p>An <a data-primary="Pachyderm" data-type="indexterm" id="id970"/>enterprise-ready data science platform with a rich suite of tools for dataset preparation, life cycle, and versioning, along with the ability to build machine learning pipelines. Pachyderm has a commercial offering you can deploy to any Kubernetes cluster.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Machine Learning on Kubernetes Best Practices" data-type="sect1"><div class="sect1" id="id129">&#13;
<h1>Machine Learning on Kubernetes Best Practices</h1>&#13;
&#13;
<p>To <a data-primary="machine learning" data-secondary="best practices" data-type="indexterm" id="machine-learn-best-practice"/><a data-primary="best practices" data-secondary="machine learning" data-type="indexterm" id="best-practice-machine-learn"/>achieve optimal performance for your machine learning workloads, consider the following best practices:</p>&#13;
<dl>&#13;
<dt>Smart scheduling and autoscaling</dt>&#13;
<dd>&#13;
<p>Given that<a data-primary="autoscaling" data-type="indexterm" id="id971"/> most stages of the machine learning workflow are batch by&#13;
nature, we recommend you utilize a Cluster Autoscaler. GPU-enabled&#13;
hardware is costly, and you certainly do not want to be paying for it&#13;
when it’s not in use. We recommend batching jobs to run at specific&#13;
times using either taints and tolerations or via a time-specific&#13;
Cluster Autoscaler. That way, the cluster can scale to the needs of the machine&#13;
learning workloads when needed, and not a moment sooner. Regarding taints and tolerations,&#13;
upstream convention is to taint the node with the extended resource as the key. For example,&#13;
a node with NVIDIA GPUs should be tainted as follows: <code>Key: nvidia.com/gpu, Effect: NoSchedule</code>.&#13;
Using this method means you can also utilize the <code>ExtendedResourceToleration</code> admission controller, which will <span class="keep-together">automatically</span> add the appropriate tolerations for such taints&#13;
to pods requesting extended resources so that the users don’t need to add them manually.</p>&#13;
</dd>&#13;
<dt>The truth is that model training is a delicate balance</dt>&#13;
<dd>&#13;
<p>Allowing things to move faster in one area often leads to bottlenecks&#13;
in others. It’s an endeavor of constant observation and tuning. As a&#13;
general rule, we recommend you try to make the GPU become&#13;
the bottleneck because it is the most costly resource.&#13;
Keep your GPUs saturated. Be prepared to always be on the lookout&#13;
for bottlenecks, and set up your monitoring to track the GPU, CPU,&#13;
network, and storage utilization.</p>&#13;
</dd>&#13;
<dt>Mixed workload clusters</dt>&#13;
<dd>&#13;
<p>Clusters that <a data-primary="mixed workload clusters" data-type="indexterm" id="id972"/><a data-primary="clusters" data-secondary="mixed workload" data-type="indexterm" id="id973"/>are used to run the day-to-day business services might also&#13;
be used for machine learning. Given the high performance&#13;
requirements of machine learning workloads, we recommend using a separate&#13;
node pool that’s tainted to accept only machine learning workloads.&#13;
This will help protect the rest of the cluster from any impact from the machine learning workloads running on the machine learning node pool. Furthermore, you should consider multiple GPU-enabled&#13;
node pools, each with different performance characteristics to suit the workload types.&#13;
We also recommend enabling node autoscaling on the&#13;
machine learning node pool(s). Use mixed mode clusters only after you have a&#13;
solid understanding of the performance impact that your machine learning&#13;
workloads have on your cluster.</p>&#13;
</dd>&#13;
<dt>Achieving linear scaling with distributed training</dt>&#13;
<dd>&#13;
<p>This is<a data-primary="distributed training" data-type="indexterm" id="id974"/><a data-primary="linear scaling" data-type="indexterm" id="id975"/><a data-primary="scaling" data-secondary="linear" data-type="indexterm" id="id976"/> the holy grail of distributed model training. Most libraries&#13;
unfortunately don’t scale linearly when distributed. A lot of work is being done to make scaling better, but it’s important to&#13;
understand the costs because this isn’t as simple as throwing more&#13;
hardware at the problem. In our experience, it’s almost always the model&#13;
itself and not the infrastructure supporting it that is the source of&#13;
the bottleneck. It is, however, important to review the utilization of the&#13;
GPU, CPU, network, and storage before pointing fingers at the model. Open source tools such as&#13;
<a href="https://oreil.ly/3NMtg">Horovod</a> seek to improve distributed&#13;
training frameworks and provide better <a data-primary="machine learning" data-secondary="best practices" data-startref="machine-learn-best-practice" data-type="indexterm" id="id977"/><a data-primary="best practices" data-secondary="machine learning" data-startref="best-practice-machine-learn" data-type="indexterm" id="id978"/>model scaling.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id370">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>We’ve covered a lot of ground in this chapter and hopefully have provided valuable insight into why Kubernetes is a great platform for&#13;
machine learning, especially deep learning, and the considerations you&#13;
need to be aware of before deploying your first machine learning&#13;
workload. If you exercise the recommendations in this chapter, you will&#13;
be well equipped to build and maintain a Kubernetes cluster for these&#13;
specialized workloads.</p>&#13;
</div></section>&#13;
</div></section></body></html>