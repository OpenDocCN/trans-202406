<html><head></head><body><section data-pdf-bookmark="Chapter 29. Elastic Scale" data-type="chapter" epub:type="chapter"><div class="chapter" id="ElasticScale">&#13;
<h1><span class="label">Chapter 29. </span>Elastic Scale</h1>&#13;
&#13;
&#13;
<p>The<a data-primary="Elastic Scale" data-type="indexterm" id="elstcscl29"/> <em>Elastic Scale</em> pattern covers application scaling in multiple dimensions: horizontal scaling by adapting the number of Pod replicas, vertical scaling by adapting resource requirements for Pods, and scaling the cluster itself by changing the number of cluster nodes. While all of these actions can be performed manually, in this chapter we explore how Kubernetes can perform scaling based on load automatically.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect1"><div class="sect1" id="idm45902081926144">&#13;
<h1>Problem</h1>&#13;
&#13;
<p>Kubernetes<a data-primary="problems" data-secondary="scaling, automatic" data-type="indexterm" id="idm45902081924784"/> automates the orchestration and management of distributed applications composed of a large number of immutable containers by maintaining their declaratively expressed desired state. However, with the seasonal nature of many workloads that often change over time, it is not an easy task to figure out how the desired state should look. Accurately identifying how many resources a container will require and how many replicas a service will need at a given time to meet service-level agreements takes time and effort. Luckily, Kubernetes makes it easy to alter the resources of a container, the desired replicas for a service, or the number of nodes in the cluster. Such changes can happen either manually, or given specific rules, can be performed in a fully automated manner.</p>&#13;
&#13;
<p>Kubernetes not only can preserve a fixed Pod and cluster setup but can also monitor external load and capacity-related events, analyze the current state, and scale itself for the desired performance. This kind of observation is a way for Kubernetes to adapt and gain antifragile traits based on actual usage metrics rather than anticipated factors. Let’s explore the different ways we can achieve such behavior and how to combine the various scaling methods for an even greater experience.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect1"><div class="sect1" id="idm45902081923008">&#13;
<h1>Solution</h1>&#13;
&#13;
<p>There are two main approaches to scaling any application: horizontal and vertical. <em>Horizontally</em> in the Kubernetes world equates to creating more replicas of a Pod. <em>Vertically</em> scaling implies giving more resources to running containers managed by Pods. While it may seem straightforward on paper, creating an application configuration for autoscaling on a shared cloud platform without affecting other services and the cluster itself requires significant trial and error. As always, Kubernetes provides a variety of features and techniques to find the best setup for our applications, and we explore them briefly here.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Manual Horizontal Scaling" data-type="sect2"><div class="sect2" id="idm45902081920544">&#13;
<h2>Manual Horizontal Scaling</h2>&#13;
&#13;
<p>The<a data-primary="scaling" data-secondary="manual horizontal scaling" data-type="indexterm" id="idm45902081919248"/> manual scaling approach, as the name suggests, is based on a human operator issuing commands to Kubernetes. This approach can be used in the absence of autoscaling or for gradual discovery and tuning of the optimal configuration of an application matching the slow-changing load over long periods. An advantage of the manual approach is that it also allows anticipatory rather than reactive-only changes: knowing the seasonality and the expected application load, you can scale it out in advance, rather than reacting to an already-increased load through autoscaling, for example. We can perform manual scaling in two styles.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Imperative scaling" data-type="sect3"><div class="sect3" id="idm45902081918016">&#13;
<h3>Imperative scaling</h3>&#13;
&#13;
<p>A<a data-primary="imperative scaling" data-type="indexterm" id="idm45902081916720"/> controller such as ReplicaSet is responsible for making sure a specific number of Pod instances are always up and running. Thus, scaling a Pod is as trivially simple as changing the number of desired replicas. Given a Deployment named <code>random-generator</code>, scaling it to four instances can be done in one command, as shown in <a data-type="xref" href="#ex-elastic-scale-horizontal-imperative">Example 29-1</a>.</p>&#13;
<div data-type="example" id="ex-elastic-scale-horizontal-imperative">&#13;
<h5><span class="label">Example 29-1. </span>Scaling a Deployment’s replicas on the command line</h5>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>scale<code class="w"> </code>random-generator<code class="w"> </code>--replicas<code class="o">=</code><code class="m">4</code><code class="w"/></pre></div>&#13;
&#13;
<p>After such a change, the<a data-primary="ReplicaSet" data-secondary="scaling" data-type="indexterm" id="RSscaling29"/> ReplicaSet could either create additional Pods to scale up or, if there are more Pods than desired, delete them to scale down.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Declarative scaling" data-type="sect3"><div class="sect3" id="idm45902081910032">&#13;
<h3>Declarative scaling</h3>&#13;
&#13;
<p>While<a data-primary="declarative scaling" data-type="indexterm" id="idm45902081908528"/> using the scale command is trivially simple and good for quick reactions to emergencies, it does not preserve this configuration outside the cluster. Typically, all Kubernetes applications would have their resource definitions stored in a source control system that also includes the number of replicas. Recreating the ReplicaSet from its original definition would change the number of replicas back to its previous number. To avoid such a configuration drift and to introduce operational processes for backporting changes, it is a better practice to change the desired number of replicas declaratively in the ReplicaSet or some other definition and apply the changes to Kubernetes, as shown in <a data-type="xref" href="#ex-deployment-declaratively">Example 29-2</a>.</p>&#13;
<div data-type="example" id="ex-deployment-declaratively">&#13;
<h5><span class="label">Example 29-2. </span>Using a Deployment for declaratively setting the number of replicas</h5>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>apply<code class="w"> </code>-f<code class="w"> </code>random-generator-deployment.yaml<code class="w"/></pre></div>&#13;
&#13;
<p>We can scale resources managing multiple Pods such as ReplicaSets, Deployments, and StatefulSets. Notice the asymmetric behavior in scaling a StatefulSet with &#13;
<span class="keep-together">persistent</span> storage. As described in<a data-primary="Stateful Service" data-secondary="Elastic Scale" data-type="indexterm" id="idm45902081884432"/><a data-primary="Stateful Service" data-type="indexterm" id="idm45902081883584"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch12.html#StatefulService">Chapter 12, “Stateful Service”</a>, if the StatefulSet has a <code>.spec.volumeClaimTemplates</code> element, it will create PVCs while scaling, but it won’t delete them when scaling down to preserve the storage from deletion.<a data-primary="" data-startref="RSscaling29" data-type="indexterm" id="idm45902081862736"/></p>&#13;
&#13;
<p>Another Kubernetes resource that can be scaled but follows a different naming convention is the Job resource, which we described in<a data-primary="Batch Job" data-secondary="Elastic Scale" data-type="indexterm" id="idm45902081861184"/><a data-primary="Batch Job" data-type="indexterm" id="idm45902081860208"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch07.html#BatchJob">Chapter 7, “Batch Job”</a>. A Job can be scaled to execute multiple instances of the same Pod at the same time by changing the<a data-primary="Job parallelism" data-type="indexterm" id="idm45902081858064"/> <code>.spec.parallelism</code> field rather than <code>.spec.replicas</code>. However, the semantic effect is the same: increased capacity with more processing units that act as a single logical unit.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>For describing resource fields, we use a JSON path notation. For example, <code>.spec.replicas</code> points to the <code>replicas</code> field of the resource’s <code>spec</code> section.</p>&#13;
</div>&#13;
&#13;
<p>Both manual scaling styles (imperative and declarative) expect a human to observe or anticipate a change in the application load, make a decision on how much to scale, and apply it to the cluster. They have the same effect, but they are not suitable for dynamic workload patterns that change often and require continuous adaptation. Next, let’s see how we can automate scaling decisions themselves.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Horizontal Pod Autoscaling" data-type="sect2"><div class="sect2" id="elasticscale-horizontal">&#13;
<h2>Horizontal Pod Autoscaling</h2>&#13;
&#13;
<p>Many<a data-primary="scaling" data-secondary="horizontal Pod autoscaling" data-type="indexterm" id="idm45902081835376"/> workloads have a dynamic nature that varies over time and makes it hard to have a fixed scaling configuration. But cloud native technologies such as Kubernetes enable you to create applications that adapt to changing loads. Autoscaling in &#13;
<span class="keep-together">Kubernetes</span> allows us to define a varying application capacity that is not fixed but instead ensures just enough capacity to handle a different load. The most straightforward approach to achieving such behavior is by using a HorizontalPodAutoscaler (HPA) to horizontally scale the number of Pods. HPA is an intrinsic part of Kubernetes and does not require any extra installation steps.&#13;
One important limitation of the HPA is that it can’t scale down to zero Pods so that no resources are consumed at all if nobody is using the deployed workload.&#13;
Luckily, Kubernetes add-ons offer scale-to-zero and transform Kubernetes into a true serverless platform. Knative<a data-primary="Knative" data-secondary="horizontal pod autoscaling" data-type="indexterm" id="idm45902081833776"/> and KEDA are the most prominent of such Kubernetes extensions. We will have a look at both in <a data-type="xref" href="#elastic-scale-knative">“Knative”</a> and <a data-type="xref" href="#elastic-scale-keda">“KEDA”</a>, but let’s first see how Kubernetes offers horizontal autoscaling out of the box.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes HorizontalPodAutoscaler" data-type="sect3"><div class="sect3" id="idm45902081831056">&#13;
<h3>Kubernetes HorizontalPodAutoscaler</h3>&#13;
&#13;
<p>The HPA is best explained with an example. An HPA for the <code>random-generator</code> Deployment can be created with the command in <a data-type="xref" href="#ex-elastic-scale-kubectl-hpa">Example 29-3</a>. For the HPA to have any effect, it is important that the Deployment declare a <code>.spec.resources.requests</code> limit for the CPU as described in<a data-primary="Predictable Demands" data-type="indexterm" id="idm45902081827312"/><a data-primary="Predictable Demands" data-secondary="Elastic Scale" data-type="indexterm" id="idm45902081826640"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch02.html#PredictableDemands">Chapter 2, “Predictable Demands”</a>. Another requirement is enabling the metrics server, which is a cluster-wide aggregator of resource usage data.<a data-primary="kubectl" data-secondary="horizontal Pod autoscaling" data-type="indexterm" id="idm45902081824208"/></p>&#13;
<div data-type="example" id="ex-elastic-scale-kubectl-hpa">&#13;
<h5><span class="label">Example 29-3. </span>Create HPA definition on the command line</h5>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>autoscale<code class="w"> </code>deployment<code class="w"> </code>random-generator<code class="w"> </code>--cpu-percent<code class="o">=</code><code class="m">50</code><code class="w"> </code>--min<code class="o">=</code><code class="m">1</code><code class="w"> </code>--max<code class="o">=</code><code class="m">5</code><code class="w"/></pre></div>&#13;
&#13;
<p>The preceding command will create the HPA definition shown in <a data-type="xref" href="#ex-elastic-scale-hpa">Example 29-4</a>.</p>&#13;
<div data-type="example" id="ex-elastic-scale-hpa">&#13;
<h5><span class="label">Example 29-4. </span>HPA definition</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">autoscaling/v2</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">HorizontalPodAutoscaler</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">minReplicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w">               </code><a class="co" href="#callout_elastic_scale_CO1-1" id="co_elastic_scale_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">maxReplicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">5</code><code class="w">               </code><a class="co" href="#callout_elastic_scale_CO1-2" id="co_elastic_scale_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">scaleTargetRef</code><code class="p">:</code><code class="w">              </code><a class="co" href="#callout_elastic_scale_CO1-3" id="co_elastic_scale_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">metrics</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">resource</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cpu</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">target</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">averageUtilization</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">50</code><code class="w"> </code><a class="co" href="#callout_elastic_scale_CO1-4" id="co_elastic_scale_CO1-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Utilization</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Resource</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_elastic_scale_CO1-1" id="callout_elastic_scale_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Minimum number of Pods that should always run.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO1-2" id="callout_elastic_scale_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Maximum number of Pods until the HPA can scale up.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO1-3" id="callout_elastic_scale_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Reference to the object that should be associated with this HPA.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO1-4" id="callout_elastic_scale_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Desired CPU usage as a percentage of the Pods’ requested CPU resource. For example, when the Pods have a <code>.spec.resources.requests.cpu</code> of 200m, a scale-up happens when on average more than 100m CPU (= 50%) is utilized.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>This<a data-primary="ReplicaSet" data-secondary="Elastic Scale" data-type="indexterm" id="RSelastic29"/> definition instructs the HPA controller to keep between one and five Pod instances to retain an average Pod CPU usage of around 50% of the specified CPU resource limit in the Pod’s <code>.spec.resources.requests</code> declaration. While it is possible to apply such an HPA to any resource that supports the <code>scale</code> subresource such as Deployments, ReplicaSets, and StatefulSets, you must consider the side effects. Deployments create new ReplicaSets during updates but without copying over any HPA definitions. If you apply an HPA to a ReplicaSet managed by a Deployment, it is not copied over to new ReplicaSets and will be lost. A better technique is to apply the HPA to the higher-level Deployment abstraction, which preserves and applies the HPA to the new ReplicaSet versions.</p>&#13;
&#13;
<p>Now, let’s see how an HPA can replace a human operator to ensure autoscaling. At a high level, the HPA controller performs the following steps continuously:</p>&#13;
<ol>&#13;
<li>&#13;
<p>It retrieves metrics about the Pods that are subject to scaling according to the HPA definition. Metrics are not read directly from the Pods but from the Kubernetes Metrics APIs that serve aggregated metrics (and even custom and external metrics if configured to do so). Pod-level resource metrics are obtained from the Metrics API, and all other metrics are retrieved from the Custom Metrics API of Kubernetes.</p>&#13;
</li>&#13;
<li>&#13;
<p>It calculates the required number of replicas based on the current metric value and targeting the desired metric value. Here is a simplified version of the &#13;
<span class="keep-together">formula:</span></p>&#13;
</li>&#13;
&#13;
</ol>&#13;
<div data-type="equation">&#13;
<math alttext="d e s i r e d upper R e p l i c a s equals left ceiling c u r r e n t upper R e p l i c a s times StartFraction c u r r e n t upper M e t r i c upper V a l u e Over d e s i r e d upper M e t r i c upper V a l u e EndFraction right ceiling" display="block">&#13;
  <mrow>&#13;
    <mi>d</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>d</mi>&#13;
    <mi>R</mi>&#13;
    <mi>e</mi>&#13;
    <mi>p</mi>&#13;
    <mi>l</mi>&#13;
    <mi>i</mi>&#13;
    <mi>c</mi>&#13;
    <mi>a</mi>&#13;
    <mi>s</mi>&#13;
    <mo>=</mo>&#13;
    <mo>⌈</mo>&#13;
    <mi>c</mi>&#13;
    <mi>u</mi>&#13;
    <mi>r</mi>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>n</mi>&#13;
    <mi>t</mi>&#13;
    <mi>R</mi>&#13;
    <mi>e</mi>&#13;
    <mi>p</mi>&#13;
    <mi>l</mi>&#13;
    <mi>i</mi>&#13;
    <mi>c</mi>&#13;
    <mi>a</mi>&#13;
    <mi>s</mi>&#13;
    <mo>×</mo>&#13;
    <mfrac><mrow><mi>c</mi><mi>u</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>M</mi><mi>e</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>c</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow> <mrow><mi>d</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>M</mi><mi>e</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>c</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow></mfrac>&#13;
    <mo>⌉</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>For example, if there is a single Pod with a current CPU usage metric value of 90% of the specified CPU resource request value,<sup><a data-type="noteref" href="ch29.html#idm45902081598912" id="idm45902081598912-marker">1</a></sup> and the desired value is 50%, the number of replicas will be doubled, as <math alttext="left ceiling 1 times StartFraction 90 Over 50 EndFraction right ceiling equals 2">&#13;
  <mrow>&#13;
    <mo>⌈</mo>&#13;
    <mn>1</mn>&#13;
    <mo>×</mo>&#13;
    <mfrac><mn>90</mn> <mn>50</mn></mfrac>&#13;
    <mo>⌉</mo>&#13;
    <mo>=</mo>&#13;
    <mn>2</mn>&#13;
  </mrow>&#13;
</math>. The actual implementation is more complicated as it has to consider multiple running Pod instances, cover multiple metric types, and account for many corner cases and fluctuating values as well. If multiple metrics are specified, for example, then the HPA evaluates each metric separately and proposes a value that is the largest of all. After all the calculations, the final output is a single-integer number representing the number of desired replicas that keep the measured value below the desired<a data-primary="targetThresholds value" data-type="indexterm" id="idm45902081592768"/><a data-primary="thresholds value" data-type="indexterm" id="idm45902081592064"/> threshold value.</p>&#13;
&#13;
<p>The <code>replicas</code> field of the autoscaled resource will be updated with this calculated number, and other controllers do their bit of work in achieving and keeping the new desired state.&#13;
<a data-type="xref" href="#img-elastic-scale-hpa">Figure 29-1</a> shows how the HPA works: monitoring metrics and changing declared replicas accordingly.</p>&#13;
&#13;
<figure class="width-70"><div class="figure" id="img-elastic-scale-hpa">&#13;
<img alt="Horizontal Pod autoscaling mechanism" src="assets/kup2_2901.png"/>&#13;
<h6><span class="label">Figure 29-1. </span>Horizontal Pod autoscaling mechanism</h6>&#13;
</div></figure>&#13;
&#13;
<p>Autoscaling is an area of Kubernetes with many low-level details, and each one can have a significant impact on the overall behavior of autoscaling. As such, it is beyond the scope of this book to cover all the details, but <a data-type="xref" href="#elastic-scale-more-information">“More Information”</a> provides the latest up-to-date information on the subject.</p>&#13;
&#13;
<p>Broadly, there are the following metric types:</p>&#13;
<dl>&#13;
<dt>Standard metrics</dt>&#13;
<dd>&#13;
<p>These metrics are declared with <code>.spec.metrics.resource[].type</code> equal to <code>Resource</code> and represent resource usage metrics such as CPU and memory. They are generic and available for any container on any cluster under the same name. You can specify them as a percentage, as we did in the preceding example, or as an absolute value. In both cases, the values are based on the guaranteed resource amount, which are the container resource <code>requests</code> values and not the <code>limits</code> values. These are the easiest-to-use metric types generally provided by the metrics server component, which can be launched as cluster add-ons.</p>&#13;
</dd>&#13;
<dt>Custom metrics</dt>&#13;
<dd>&#13;
<p>These metrics with <code>.spec.metrics.resource[].type</code> equal to <code>Object</code> or <code>Pod</code> require a more advanced cluster-monitoring setup, which can vary from cluster to cluster. A custom metric with the Pod type, as the name suggests, describes a Pod-specific metric, whereas the Object type can describe any other object. The custom metrics are served in an aggregated API Server under the &#13;
<span class="keep-together"><code>custom.metrics.k8s.io</code></span> API path and are provided by different metrics adapters,  such  as  Prometheus,  Datadog,  Microsoft  Azure,  or  Google  Stackdriver.</p>&#13;
</dd>&#13;
<dt>External metrics</dt>&#13;
<dd>&#13;
<p>This category is for metrics that describe resources that are not a part of the Kubernetes cluster. For example, you may have a Pod that consumes messages from a cloud-based queueing service. In such a scenario, you’ll want to scale the number of consumer Pods based on the queue depth. Such a metric would be populated by an external metrics plugin similar to custom metrics. Only one external metrics endpoint can be hooked into the Kubernetes API server. For using metrics from many different external systems, an extra aggregation layer like KEDA is required (see <a data-type="xref" href="#elastic-scale-keda">“KEDA”</a>).<a data-primary="" data-startref="RSelastic29" data-type="indexterm" id="idm45902081576400"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Getting autoscaling right is not easy and involves a little experimenting and tuning. The following are a few of the main areas to consider when setting up an HPA:</p>&#13;
<dl>&#13;
<dt>Metric selection</dt>&#13;
<dd>&#13;
<p>Probably one of the most critical decisions around autoscaling is which metrics to use. For an HPA to be useful, there must be a direct correlation between the metric value and the number of Pod replicas. For example, if the chosen metric is of the Queries-per-Second kind (such as HTTP requests per second), increasing the number of Pods causes the average number of queries to go down as the queries are dispatched to more Pods. The same is true if the metric is CPU usage, as there is a direct correlation between the query rate and CPU usage (an increased number of queries would result in increased CPU usage). For other metrics such as memory consumption, that is not the case. The issue with memory is that if a service consumes a certain amount of memory, starting more Pod instances most likely will not result in a memory decrease unless the application is clustered and aware of the other instances and has mechanisms to distribute and release its memory. If the memory is not released and reflected in the metrics, the HPA would create more and more Pods in an effort to decrease it, until it reaches the upper replica threshold, which is probably not the desired behavior. So choose a metric that is directly (preferably linearly) correlated to the number of Pods.</p>&#13;
</dd>&#13;
<dt>Preventing thrashing</dt>&#13;
<dd>&#13;
<p>The HPA applies various techniques to avoid rapid execution of conflicting decisions that can lead to a fluctuating number of replicas when the load is not stable. For example, during scale-up, the HPA disregards high CPU usage samples when a Pod is initializing, ensuring a smoothing reaction to increasing load. During scale-down, to avoid scaling down in response to a short dip in usage, the controller considers all scale recommendations during a configurable time window and chooses the highest recommendation from within the window. All this makes the HPA more stable when dealing with random metric fluctuations.</p>&#13;
</dd>&#13;
<dt>Delayed reaction</dt>&#13;
<dd>&#13;
<p>Triggering a scaling action based on a metric value is a multistep process involving multiple Kubernetes components. First, it is the cAdvisor (container advisor) agent that collects metrics at regular intervals for the<a data-primary="Kubelet" data-secondary="metrics collection" data-type="indexterm" id="idm45902081569008"/> Kubelet. Then the metrics server collects metrics from the Kubelet at regular intervals. The HPA controller loop also runs periodically and analyzes the collected metrics. The HPA scaling formula introduces some delayed reaction to prevent fluctuations/thrashing (as explained in the previous point). All this activity accumulates into a delay between the cause and the scaling reaction. Tuning these parameters by introducing more delay makes the HPA less responsive, but reducing the delays increases the load on the platform and increases thrashing. Configuring Kubernetes to balance resources and performance is an ongoing learning process.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Tuning the autoscale algorithm for the HPA in Kubernetes can be complex. To help with this, Kubernetes provides the <code>.spec.behavior</code> field in the HPA specification. This field allows you to customize the behavior of the HPA when scaling the number of replicas in a Deployment.</p>&#13;
&#13;
<p>For each scaling direction (up or down), you can use the <code>.spec.behavior</code> field to specify the following parameters:</p>&#13;
<dl>&#13;
<dt><code>policies</code></dt>&#13;
<dd>&#13;
<p>These describe the maximum number of replicas to scale in a given period.</p>&#13;
</dd>&#13;
<dt><code>stabilizationWindowSeconds</code></dt>&#13;
<dd>&#13;
<p>This specifies when the HPA will not make any further scaling decisions. Setting this field can help to prevent thrashing effects, where the HPA rapidly scales the number of replicas up and down.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#ex-elastic-scale-hpa-behavior">Example 29-5</a> shows how the behavior can be configured.&#13;
All behavior parameters can also be configured on the CLI with<a data-primary="kubectl" data-secondary="scaling" data-type="indexterm" id="idm45902081561184"/> <code>kubectl autoscale</code>.</p>&#13;
<div data-type="example" id="ex-elastic-scale-hpa-behavior">&#13;
<h5><span class="label">Example 29-5. </span>Configuration of the autoscaling algorithm</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">autoscaling/v2</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">HorizontalPodAutoscaler</code><code class="w">&#13;
</code><code class="p">...</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p">...</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">behavior</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">scaleDown</code><code class="p">:</code><code class="w">                        </code><a class="co" href="#callout_elastic_scale_CO2-1" id="co_elastic_scale_CO2-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="nt">stabilizationWindowSeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">300</code><code class="w"> </code><a class="co" href="#callout_elastic_scale_CO2-2" id="co_elastic_scale_CO2-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="nt">policies</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Percent</code><code class="w">                 </code><a class="co" href="#callout_elastic_scale_CO2-3" id="co_elastic_scale_CO2-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">value</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">periodSeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">60</code><code class="w">&#13;
&#13;
</code><code class="w">    </code><code class="nt">scaleUp</code><code class="p">:</code><code class="w">                          </code><a class="co" href="#callout_elastic_scale_CO2-4" id="co_elastic_scale_CO2-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="nt">policies</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Pods</code><code class="w">                    </code><a class="co" href="#callout_elastic_scale_CO2-5" id="co_elastic_scale_CO2-5"><img alt="5" src="assets/5.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">value</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">4</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">periodSeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">15</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_elastic_scale_CO2-1" id="callout_elastic_scale_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Scaling behavior when scaling down.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO2-2" id="callout_elastic_scale_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>A 5-minute minimum window for down-scaling decisions to prevent flapping.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO2-3" id="callout_elastic_scale_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Scale down at most 10% of the current replicas in one minute.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO2-4" id="callout_elastic_scale_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Scaling behavior when scaling up.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO2-5" id="callout_elastic_scale_CO2-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Scale up at most four Pods within 15 seconds.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Please refer to the Kubernetes documentation on <a href="https://oreil.ly/gQAa9">configuring the scaling behavior</a> for all the details and usage examples.</p>&#13;
&#13;
<p>While the HPA is very powerful and covers the basic needs for autoscaling, it lacks one crucial feature: scale-to-zero<a data-primary="scaling" data-secondary="scale-to-zero" data-type="indexterm" id="idm45902081411136"/> for stopping all Pods of an application if it is not used. That’s important so that it does not cause any costs based on memory, CPU, or network usage. However, scaling to zero is not so hard; the tricky part is waking up again and scaling to at least one Pod by a trigger, like an incoming HTTP request or an event to process.</p>&#13;
&#13;
<p>The following two sections introduce the two most prominent Kubernetes-based add-ons for enabling scale-to-zero: Knative and KEDA.&#13;
It is essential to understand that Knative and KEDA are not alternative but complementary solutions. Both projects cover different use cases and can ideally be used together. As we will see, Knative specializes in stateless HTTP applications and offers an autoscaling algorithm that goes beyond the capabilities of the HPA. On the other hand, KEDA is a pull-based approach that can be triggered by many different sources, like messages in a Kafka topic or IBM MQ queue.</p>&#13;
&#13;
<p>Let’s have a closer look at Knative and KEDA.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Knative" data-type="sect3"><div class="sect3" id="elastic-scale-knative">&#13;
<h3>Knative</h3>&#13;
&#13;
<p>Knative<a data-primary="Knative" data-secondary="scale-to-zero" data-type="indexterm" id="KNzero29"/> is a CNCF project initiated by Google in 2018, with broad industry support from vendors like IBM, VMware, and Red Hat.&#13;
This Kubernetes add-on consists of three parts:</p>&#13;
<dl>&#13;
<dt>Knative Serving</dt>&#13;
<dd>&#13;
<p>This<a data-primary="Knative" data-secondary="serving" data-type="indexterm" id="idm45902081403744"/> is a simplified application deployment model with sophisticated autoscaling and traffic-splitting capabilities, including scale-to-zero.<a data-primary="scaling" data-secondary="Knative Serving" data-type="indexterm" id="idm45902081402608"/></p>&#13;
</dd>&#13;
<dt>Knative Eventing</dt>&#13;
<dd>&#13;
<p>This<a data-primary="Knative" data-secondary="eventing" data-type="indexterm" id="idm45902081400384"/> provides everything needed to create an Event Mesh to connect event sources that produce CloudEvents<sup><a data-type="noteref" href="ch29.html#idm45902081399136" id="idm45902081399136-marker">2</a></sup> with a sink that consumes these events. Those sinks are typically Knative Serving services.</p>&#13;
</dd>&#13;
<dt>Knative Functions</dt>&#13;
<dd>&#13;
<p>This<a data-primary="Knative" data-secondary="functions" data-type="indexterm" id="idm45902081396976"/> is for scaffolding and building Knative Serving services from source code. It supports various programming languages and offers an AWS Lambda-like programming model.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In this section, we will focus on Knative Serving and its autoscaler for an application that uses HTTP to offer its services.&#13;
For those workloads, CPU and memory are metrics that only indirectly correlate to actual usage.&#13;
A much better metric is the number of <em>concurrent requests</em> per Pod—i.e., requests that are processed in parallel.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Another HTTP-based metric that Knative can use is <em>requests per second</em> (rps). Still, this metric does not say anything about the costs of a single request, so concurrent requests are typically the much better metric to use, as they capture the frequency of requests and the duration of those requests. You can select the scale metric individually for each application or as a global default.</p>&#13;
</div>&#13;
&#13;
<p>Basing the autoscaling decision on concurrent requests gives a much better correlation to the latency of HTTP request processing than scaling based on CPU or memory consumption can provide.</p>&#13;
&#13;
<p>Historically, Knative used to be implemented as a custom metric adapter for the HPA in Kubernetes. However, it later developed its own implementation in order to have more flexibility in influencing the scaling algorithm and to avoid the bottleneck of being able to register only a single custom metric adapter in a Kubernetes cluster.</p>&#13;
&#13;
<p>While Knative still supports using the HPA for scaling based on memory or CPU usage, it now focuses on using its own autoscaling implementation, called the Knative Pod Autoscaler (KPA). This allows Knative to have more control over the scaling algorithm and to better optimize it for the needs of the application.</p>&#13;
&#13;
<p>The architecture of the KPA is shown in <a data-type="xref" href="#img-elastic-scale-knative-architecture">Figure 29-2</a>.</p>&#13;
&#13;
<figure class="width-80"><div class="figure" id="img-elastic-scale-knative-architecture">&#13;
<img alt="Knative Pod Autoscaler components" src="assets/kup2_2902.png"/>&#13;
<h6><span class="label">Figure 29-2. </span>Knative Pod Autoscaler</h6>&#13;
</div></figure>&#13;
&#13;
<p>Three components are playing together for autoscaling a service:</p>&#13;
<dl>&#13;
<dt>Activator</dt>&#13;
<dd>&#13;
<p>This is a proxy in front of the application that is always available, even when the application is scaled down to zero Pods. When the application is scaled down to zero, and a first request comes in, the request gets buffered, and the application is scaled up to at least one Pod. It’s important to note that during a <em>cold start</em>, all incoming requests will be buffered to ensure that no requests are lost.</p>&#13;
</dd>&#13;
<dt>Queue proxy</dt>&#13;
<dd>&#13;
<p>The queue proxy is an ambassador sidecar described in <a data-type="xref" href="ch18.html#Ambassador">Chapter 18</a> that is injected into the application’s Pod by the Knative controller. It intercepts the request path for collecting metrics relevant to autoscaling, like concurrent requests.</p>&#13;
</dd>&#13;
<dt>Autoscaler</dt>&#13;
<dd>&#13;
<p>This<a data-primary="ReplicaSet" data-secondary="scaling" data-type="indexterm" id="idm45902081382240"/> is a service running in the background that is responsible for the scaling decision based on the data it gets from the activator and queue-proxy. The autoscaler is the one that sets the replica count in the application’s ReplicaSet.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The KPA algorithm can be configured in many ways to optimize the autoscaling behavior for any workload and traffic shape.&#13;
<a data-type="xref" href="#table-elastic-scale-knative-parameters">Table 29-1</a> shows some of the configuration options for tuning the KPA for individual services via annotations.&#13;
Similar configuration options also exist for global defaults that are stored in a ConfigMap.&#13;
You can find the full set of all autoscaling configuration options in the <a href="https://oreil.ly/m09BV">Knative documentation</a>.&#13;
This documentation has more details about the Knative scaling algorithm, like dealing with bursty workloads by scaling up more aggressively when the increase in concurrent requests is over a threshold.</p>&#13;
<table id="table-elastic-scale-knative-parameters">&#13;
<caption><span class="label">Table 29-1. </span>Important Knative scaling parameters. <code>autoscaling.knative.dev/</code>, the common annotation prefix, has been omitted.</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Annotation</th>&#13;
<th>Description</th>&#13;
<th>Default</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p><code>target</code></p></td>&#13;
<td><p>Number of simultaneous requests that can be processed by each replica. This is a soft limit and might be temporarily exceeded in case of a traffic burst. <code>.spec.concurrencyLimit</code> is used as a hard limit that can’t be crossed.</p></td>&#13;
<td><p>100</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>target-utilization-percentage</code></p></td>&#13;
<td><p>Start creating new replicas if this fraction of the concurrency limit has been reached.</p></td>&#13;
<td><p>70</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>min-scale</code></p></td>&#13;
<td><p>Minimum number of replicas to keep. If set to a value greater than zero, the application will never scale down to zero.</p></td>&#13;
<td><p>0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>max-scale</code></p></td>&#13;
<td><p>Upper bound for the number of replicas; zero means unlimited scaling.</p></td>&#13;
<td><p>0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>activation-scale</code></p></td>&#13;
<td><p>How many replicas to create when scaling up from zero.</p></td>&#13;
<td><p>1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>scale-down-delay</code></p></td>&#13;
<td><p>How long scale-down conditions must hold before scaling down. Useful for keeping replicas warm before scaling zero in order to avoid cold start time.</p></td>&#13;
<td><p>0s</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>window</code></p></td>&#13;
<td><p>Length of the time window over which metrics are averaged to provide the input for scaling decisions.</p></td>&#13;
<td><p>60s</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p><a data-type="xref" href="#ex-elastic-scale-kservice">Example 29-6</a> shows a Knative service that deploys an example application.&#13;
It looks similar to a Kubernetes Deployment. However, behind the scenes, the Knative operator creates the Kubernetes resources needed to expose your application as a web service, i.e., a ReplicaSet, Kubernetes Service, and Ingress for exposing the application to the outside of your cluster.</p>&#13;
<div data-type="example" id="ex-elastic-scale-kservice">&#13;
<h5><span class="label">Example 29-6. </span>Knative service</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">serving.knative.dev/v1</code><code class="w">         </code><a class="co" href="#callout_elastic_scale_CO3-1" id="co_elastic_scale_CO3-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">annotations</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">autoscaling.knative.dev/target</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">80</code><code class="s">"</code><code class="w">   </code><a class="co" href="#callout_elastic_scale_CO3-2" id="co_elastic_scale_CO3-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">autoscaling.knative.dev/window</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">120s</code><code class="s">"</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">k8spatterns/random</code><code class="w">          </code><a class="co" href="#callout_elastic_scale_CO3-3" id="co_elastic_scale_CO3-3"><img alt="3" src="assets/3.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_elastic_scale_CO3-1" id="callout_elastic_scale_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Knative also uses Service for the resource name but with the API group <code>serving.knative.dev</code>, which is different from a Kubernetes Service from the <code>core</code> API group.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO3-2" id="callout_elastic_scale_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Options for tuning the autoscaling algorithm. See <a data-type="xref" href="#table-elastic-scale-knative-parameters">Table 29-1</a> for the available options.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO3-3" id="callout_elastic_scale_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The only mandatory argument for a Knative Service is a reference to a container image.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>We only briefly touch on Knative here. There is much more that can help you in operating the Knative autoscaler. Please check out the <a href="https://knative.dev">online documentation</a> for more features of Knative Serving, like<a data-primary="Knative" data-secondary="traffic split" data-type="indexterm" id="idm45902081246256"/> traffic splitting for the complex rollout scenarios we described in <a data-type="xref" data-xrefstyle="chap-num-title" href="ch03.html#DeclarativeDeployment">Chapter 3, “Declarative Deployment”</a>.&#13;
Also, if you are following an event-driven architecture (EDA) paradigm for your applications, Knative Eventing and Knative Functions have a lot to offer.<a data-primary="" data-startref="KNzero29" data-type="indexterm" id="idm45902081244016"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="KEDA" data-type="sect3"><div class="sect3" id="elastic-scale-keda">&#13;
<h3>KEDA</h3>&#13;
&#13;
<p>Kubernetes Event-Driven Autoscaling (KEDA)<a data-primary="KEDA (Kubernetes Event-Driven Autoscaling)" data-type="indexterm" id="KEDA29"/><a data-primary="Kubernetes Event-Driven Autoscaling (KEDA)" data-type="indexterm" id="kubevent29"/><a data-primary="scaling" data-secondary="Kubernetes Event-Driven Autoscaling (KEDA)" data-type="indexterm" id="scalkeda29"/> is the other important Kubernetes-based autoscaling platform that supports scale-to-zero but has a different scope than Knative.&#13;
While Knative supports autoscaling based on HTTP traffic, KEDA is a pull-based approach that scales based on external metrics from different systems.&#13;
Knative and KEDA play very well together, and there is only a little overlap,<sup><a data-type="noteref" href="ch29.html#idm45902081237056" id="idm45902081237056-marker">3</a></sup> so nothing prevents you from using both add-ons together.</p>&#13;
&#13;
<p>So, what is KEDA?&#13;
KEDA is a CNCF project that Microsoft and Red Hat created in 2019 and consists of the following components:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The KEDA Operator reconciles a ScaledObject custom resource that connects the scaled target (e.g., a Deployment or StatefulSet) with an autoscale trigger that connects to an external system via a so-called <em>scaler</em>. It is also responsible for configuring the HPA with the external metrics service provided by KEDA.</p>&#13;
</li>&#13;
<li>&#13;
<p>KEDA’s metrics service is registered as an APIService resource in the Kubernetes API aggregation layer so that the HPA can use it as an external metrics service.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-type="xref" href="#img-elastic-scale-keda-architecture">Figure 29-3</a> illustrates the relationship between the KEDA Operator, metrics service, and the Kubernetes HPA.</p>&#13;
&#13;
<figure><div class="figure" id="img-elastic-scale-keda-architecture">&#13;
<img alt="KEDA operator and metrics server" src="assets/kup2_2903.png"/>&#13;
<h6><span class="label">Figure 29-3. </span>KEDA autoscaling components</h6>&#13;
</div></figure>&#13;
&#13;
<p>While Knative is a complete solution that completely replaces HPA for a consumption-based autoscaling, KEDA is a hybrid solution.&#13;
KEDA’s autoscaling algorithm distinguishes between two scenarios:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Activation by scaling from zero replicas to one (<em>0 ↔ 1</em>): This action is performed by the KEDA operator itself when it detects that a used scaler’s metric exceeds a certain threshold.</p>&#13;
</li>&#13;
<li>&#13;
<p>Scaling up and down when running (<em>1 ↔ n</em>): When the workload is already active, the HPA takes over and scales based on the external metric that KEDA offers.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The central element for KEDA is the custom resource ScaledObject, provided by the user to configure KEDA-based autoscaling and playing a similar role as the HorizontalPodAutoscaler resource.&#13;
As soon as the KEDA operator detects a new instance of ScaledObject, it automatically creates a HorizontalPodAutoscaler resource that uses the KEDA metrics service as an external metrics provider and the scaling parameters.</p>&#13;
&#13;
<p><a data-type="xref" href="#ex-elastic-scale-scaledobject">Example 29-7</a> shows how you can scale a Deployment based on the number of messages in an<a data-primary="Apache Kafka" data-type="indexterm" id="idm45902081223968"/><a data-primary="Kafka" data-type="indexterm" id="idm45902081223296"/> Apache Kafka topic.</p>&#13;
<div class="less_space pagebreak-before" data-type="example" id="ex-elastic-scale-scaledobject">&#13;
<h5><span class="label">Example 29-7. </span>ScaledObject definition</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">keda.sh/v1alpha1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ScaledObject</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kafka-scaledobject</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">scaleTargetRef</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kafka-consumer</code><code class="w">                           </code><a class="co" href="#callout_elastic_scale_CO4-1" id="co_elastic_scale_CO4-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">pollingInterval</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">30</code><code class="w">                              </code><a class="co" href="#callout_elastic_scale_CO4-2" id="co_elastic_scale_CO4-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">triggers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kafka</code><code class="w">                                  </code><a class="co" href="#callout_elastic_scale_CO4-3" id="co_elastic_scale_CO4-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">      </code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">bootstrapServers</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bootstrap.kafka.svc:9092</code><code class="w"> </code><a class="co" href="#callout_elastic_scale_CO4-4" id="co_elastic_scale_CO4-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="w">        </code><code class="nt">consumerGroup</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">my-group</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">topic</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">my-topic</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_elastic_scale_CO4-1" id="callout_elastic_scale_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Reference to a Deployment with the name <code>kafka-consumer</code> that should be autoscaled. You can also specify other scalable workloads here; Deployment is the default.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO4-2" id="callout_elastic_scale_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>In the action phase (scale from zero), poll every 30 seconds for the metric value. In this example, it is the number of messages in a Kafka topic.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO4-3" id="callout_elastic_scale_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Select the Apache Kafka scaler.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO4-4" id="callout_elastic_scale_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Configuration options for the Apache Kafka scaler—i.e., how to connect to the Kafka cluster and which topic to monitor.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>KEDA provides many out-of-the-box scalers that can be selected to connect to external systems for the autoscaling stimulus.&#13;
You can obtain the complete list of directly supported scalers from the <a href="https://oreil.ly/rkJKU">KEDA home page</a>.&#13;
In addition, you can easily integrate custom scalers by providing an external service that communicates with KEDA over a gRPC-based API.</p>&#13;
&#13;
<p>KEDA is a great autoscaling solution when you need to scale based on work items held in external systems, like message queues that your application consumes.&#13;
To some degree, this pattern shares some of the characteristics of <a data-type="xref" data-xrefstyle="chap-num-title" href="ch07.html#BatchJob">Chapter 7, “Batch Job”</a>: the workload runs only when work is done and does not consume any resources when idle.&#13;
Both can be scaled up for parallel processing of the work items.&#13;
The difference here is that a KEDA ScaledObject does the up-scale automatically, whereas for a Kubernetes <code>Job</code>, you must manually determine the<a data-primary="Job parallelism" data-type="indexterm" id="idm45902081091248"/> parallelism parameters.&#13;
With KEDA, you can also automatically trigger Kubernetes Jobs based on the availability of external workloads.&#13;
The ScaledJob custom resource is precisely for this purpose so that instead of scaling up replicas from 0 to 1, a Job resource is started in case a scaler’s activation threshold is met.&#13;
Note that the <code>parallelism</code> field in the Job is still fixed, but the autoscaling happens on the Job resource level itself (i.e., Job resources themselves play the role of replicas).</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45902081089904">&#13;
<h1>Push Versus Pull Horizontal Autoscalers</h1>&#13;
<p>Kubernetes knows about two main types of horizontal autoscalers: push autoscalers and pull autoscalers.</p>&#13;
&#13;
<p><em>Push autoscalers</em> operate by actively pushing metrics to the autoscaler, which then uses those metrics to decide how to scale. This technique is often used when the metrics have been directly generated by a system closely integrated with the autoscaler. For example, in <a data-primary="Knative" data-secondary="horizontal pod autoscaling" data-type="indexterm" id="idm45902081087392"/>Knative, the Activator pushes the metrics about concurrent requests to the Autoscaler component, as illustrated in <a data-type="xref" href="#img-elastic-scale-knative-architecture">Figure 29-2</a>.</p>&#13;
&#13;
<p><em>Pull autoscalers</em> operate by actively pulling metrics from the application or external sources. Pulling is often used when the metrics are not directly accessible to the autoscaler or when the metrics are stored in an external system. KEDA, for example, is a pull autoscaler that scales deployments based on, for example, the number of events or messages in a queue. <a data-type="xref" href="#img-elastic-scale-keda-architecture">Figure 29-3</a> shows how KEDA uses a custom Kubernetes controller to pull metrics about the number of events and then uses those metrics to determine whether to scale up or down.</p>&#13;
&#13;
<p>Push autoscalers are often used for applications that receive data, like from HTTP endpoints. In contrast, pull autoscalers are suitable for applications that actively retrieve their workload, such as pulling from a message queue.</p>&#13;
</div></aside>&#13;
&#13;
<p><a data-type="xref" href="#table-elasticscale-comparison">Table 29-2</a> summarizes the unique features<a data-primary="Knative" data-secondary="versus HPA and KEDA" data-secondary-sortas="HPA and KEDA" data-type="indexterm" id="idm45902081082080"/> and differences between HPA, Knative, and KEDA.</p>&#13;
<table id="table-elasticscale-comparison">&#13;
<caption><span class="label">Table 29-2. </span>Horizontal autoscaling on Kubernetes</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>HPA</th>&#13;
<th>Knative</th>&#13;
<th>KEDA</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Scale metrics</p></td>&#13;
<td><p>Resource usage</p></td>&#13;
<td><p>HTTP requests</p></td>&#13;
<td><p>External metrics like message queue backlog</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Scale-to-zero</p></td>&#13;
<td><p>No</p></td>&#13;
<td><p>Yes</p></td>&#13;
<td><p>Yes</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Type</p></td>&#13;
<td><p>Pull</p></td>&#13;
<td><p>Push</p></td>&#13;
<td><p>Pull</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Typical use cases</p></td>&#13;
<td><p>Stable traffic web applications, Batch processing</p></td>&#13;
<td><p>Serverless applications with rapid scaling, serverless functions</p></td>&#13;
<td><p>Message-driven microservices</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Now that we have seen all the possibilities for scaling horizontally with HPA, Knative, and KEDA, let’s look at a completely different kind of scaling that does not alter the number of parallel-running replicas but lets your application grow and shrink.<a data-primary="" data-startref="scalkeda29" data-type="indexterm" id="idm45902081064784"/><a data-primary="" data-startref="KEDA29" data-type="indexterm" id="idm45902081063808"/><a data-primary="" data-startref="kubevent29" data-type="indexterm" id="idm45902081062864"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vertical Pod Autoscaling" data-type="sect2"><div class="sect2" id="elasticscale-vertical">&#13;
<h2>Vertical Pod Autoscaling</h2>&#13;
&#13;
<p>Horizontal scaling<a data-primary="scaling" data-secondary="vertical Pod autoscaling" data-type="indexterm" id="idm45902081059776"/><a data-primary="vertical Pod autoscaling" data-type="indexterm" id="idm45902081058704"/> is preferred over vertical scaling because it is less disruptive, especially for stateless services. That is not the case for stateful services, where vertical scaling may be preferred. Other scenarios where vertical scaling is useful include tuning the resource needs of a service based on actual load patterns. We’ve discussed why identifying the correct number of Pod replicas might be difficult and even impossible when the load changes over time. Vertical scaling also has these kinds of challenges in identifying the correct <code>requests</code> and <code>limits</code> for a container. The Kubernetes Vertical Pod Autoscaler (VPA) aims to address these challenges by automating the process of adjusting and allocating resources based on real-world usage feedback.</p>&#13;
&#13;
<p>As we saw in<a data-primary="Predictable Demands" data-type="indexterm" id="idm45902081056640"/><a data-primary="Predictable Demands" data-secondary="Elastic Scale" data-type="indexterm" id="idm45902081055520"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch02.html#PredictableDemands">Chapter 2, “Predictable Demands”</a>, every container in a Pod can  specify its CPU and memory <code>requests</code>, which influences where the Pods will be scheduled. In a sense, the resource <code>requests</code> and <code>limits</code> of a Pod form a contract between the Pod and the<a data-primary="scheduler" data-secondary="requests amount" data-type="indexterm" id="idm45902081051872"/> scheduler, which causes a certain amount of resources to be guaranteed or prevents the Pod from being scheduled. Setting the memory <code>requests</code> too low can cause nodes to be more tightly packed, which in turn can lead to out-of-memory errors or workload eviction due to memory pressure. If the CPU <code>limits</code> are too low, CPU starvation and underperforming workloads can occur. On the other hand, specifying resource <code>requests</code> that are too high allocates unnecessary capacity, leading to wasted resources. It is important to set resource <code>requests</code> as accurately as possible since they impact the cluster utilization and the effectiveness of horizontal scaling. Let’s see how VPA helps address this.</p>&#13;
&#13;
<p>On a cluster with VPA and the metrics server installed, we can use a VPA definition to demonstrate vertical autoscaling of Pods, as in <a data-type="xref" href="#ex-elastic-scale-vpa">Example 29-8</a>.</p>&#13;
<div data-type="example" id="ex-elastic-scale-vpa">&#13;
<h5><span class="label">Example 29-8. </span>VPA</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">autoscaling.k8s.io/v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">VerticalPodAutoscaler</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator-vpa</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">targetRef</code><code class="p">:</code><code class="w">            </code><a class="co" href="#callout_elastic_scale_CO5-1" id="co_elastic_scale_CO5-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">    </code><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">updatePolicy</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">updateMode</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">Off</code><code class="s">"</code><code class="w">   </code><a class="co" href="#callout_elastic_scale_CO5-2" id="co_elastic_scale_CO5-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_elastic_scale_CO5-1" id="callout_elastic_scale_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Reference to the higher-level resource that holds the selector to identify the Pods to manage.</p></dd>&#13;
<dt><a class="co" href="#co_elastic_scale_CO5-2" id="callout_elastic_scale_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The update policy for how VPA will apply changes.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>A VPA definition has the following main parts:</p>&#13;
<dl>&#13;
<dt>Target reference</dt>&#13;
<dd>&#13;
<p>The target reference points to a higher-level resource that controls Pods, like a Deployment or a StatefulSet. From this resource, the VPA looks up the label selector<a data-primary="label selectors" data-secondary="vertical Pod autoscaling" data-type="indexterm" id="idm45902080987904"/> for identifying the Pods it should handle. If the reference points to a resource that does not contain such a selector, then it will report an error in the VPA status section.</p>&#13;
</dd>&#13;
<dt>Update policy</dt>&#13;
<dd>&#13;
<p>The update policy controls how the VPA applies changes. The <code>Initial</code> mode allows you to assign resource requests only during Pod creation time and not later. The default <code>Auto</code> mode allows resource assignment to Pods at creation time, but additionally, it can update Pods during their lifetimes, by evicting and rescheduling the Pod. The value <code>Off</code> disables automatic changes to Pods but allows you to suggest resource values. This is a kind of dry run for discovering the right size of a container without applying it directly.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>A VPA definition can also have a resource policy that influences how the VPA computes the recommended resources (e.g., by setting per-container lower and upper resource boundaries).</p>&#13;
&#13;
<p>Depending on which <code>.spec.updatePolicy.updateMode</code> is configured, the VPA involves different system components. All three VPA components—recommender, admission plugin, and updater—are decoupled and independent and can be replaced with alternative implementations. The module with the intelligence to produce recommendations is the recommender, which is inspired by Google’s Borg system. The implementation analyzes the actual resource usage of a container under load for a certain period (by default, eight days), produces a histogram, and chooses a high-percentile value for that period. In addition to metrics, it also considers resource and specifically memory-related Pod events such as evictions and <code>OutOfMemory</code> events.</p>&#13;
&#13;
<p>In our example, we chose <code>.spec.updatePolicy.updateMode</code> equals <code>Off</code>, but there are two other options to choose from, each with a different level of potential disruption on the scaled Pods. Let’s see how different values for <code>updateMode</code> work, starting from nondisruptive to a more disruptive order:</p>&#13;
<dl>&#13;
<dt>Off</dt>&#13;
<dd>&#13;
<p>The VPA recommender gathers Pod metrics and events and then produces recommendations. The VPA recommendations are always stored in the <code>status</code> section of the VPA resource. However, this is as far as the <code>Off</code> mode goes. It analyzes and produces recommendations, but it does not apply them to the Pods. This mode is useful for getting insight on the Pod resource consumption without introducing any changes and causing disruption. That decision is left for the user to make if desired.</p>&#13;
</dd>&#13;
<dt>Initial</dt>&#13;
<dd>&#13;
<p>In this mode, the VPA goes one step further. In addition to the activities performed by the recommender component, it also activates the VPA admission Controller, which applies the recommendations to newly created Pods only. For example, if a Pod is scaled manually, updated by a Deployment, or evicted and restarted for whatever reason, the Pod’s resource request values are updated by the VPA Admission Controller.</p>&#13;
&#13;
<p>This controller is a <em>mutating admission Webhook</em> that overrides the <code>requests</code> of new matching Pods that are associated with the VPA resource. This mode does not restart a running Pod, but it is still partially disruptive because it changes the resource request of newly created Pods. This in turn can affect where a new Pod is scheduled. What’s more, it is possible that after applying the recommended resource requests, the Pod is scheduled to a different node, which can have unexpected consequences. Or worse, the Pod might not be scheduled to any node if there is not enough capacity on the cluster.</p>&#13;
</dd>&#13;
<dt>Recreate and Auto</dt>&#13;
<dd>&#13;
<p>In addition to the recommendation creation and its application for newly created Pods, as described previously, in this mode, the VPA also activates its updated component. The <code>Recreate</code> update mode forcibly evicts and restarts all Pods in the deployment to apply the VPA’s recommendations, while the <code>Auto</code> update mode is supposed to support in-place updates of resource limits without restarting Pods in a future version of Kubernetes. As of 2023, <code>Auto</code> behaves the same as <code>Recreate</code>, so both update modes can be disruptive and may lead to the unexpected scheduling issues that have been described earlier.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Kubernetes is designed to manage immutable containers with immutable Pod <code>spec</code> definitions, as seen in <a data-type="xref" href="#img-elastic-scale-vpa">Figure 29-4</a>. While this simplifies horizontal scaling, it introduces challenges for vertical scaling, such as requiring Pod deletion and recreation, which can impact scheduling and cause service disruptions. This is true even when the Pod is scaling down and wants to release already-allocated resources with no disruption.</p>&#13;
&#13;
<p>Another concern is the coexistence of VPA and HPA because these autoscalers are not currently aware of each other, which can lead to unwanted behavior. For example, if an HPA is using resource metrics such as CPU and memory, and the VPA is also influencing the same values, you may end up with horizontally scaled Pods that are also vertically scaled (hence double scaling).</p>&#13;
&#13;
<p>We can’t go into more details here. Although it is still evolving, it is worth keeping an eye on the VPA as it is a feature that has the potential to significantly improve resource consumption.</p>&#13;
&#13;
<figure><div class="figure" id="img-elastic-scale-vpa">&#13;
<img alt="Vertical Pod autoscaling mechanism" src="assets/kup2_2904.png"/>&#13;
<h6><span class="label">Figure 29-4. </span>Vertical Pod autoscaling mechanism</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Autoscaling" data-type="sect2"><div class="sect2" id="idm45902080928672">&#13;
<h2>Cluster Autoscaling</h2>&#13;
&#13;
<p>The<a data-primary="scaling" data-secondary="cluster autoscaling" data-type="indexterm" id="idm45902080927008"/><a data-primary="cluster autoscaling" data-type="indexterm" id="idm45902080926000"/> patterns in this book primarily use Kubernetes primitives and resources targeted at developers using a Kubernetes cluster that’s already set up, which is usually an operational task. Since it is a topic related to the elasticity and scaling of workloads, we will briefly cover the Kubernetes Cluster Autoscaler (CA) here.</p>&#13;
&#13;
<p>One of the tenets of cloud computing is pay-as-you-go resource consumption. We can consume cloud services when needed, and only as much as needed. CA can interact with cloud providers where Kubernetes is running and request additional<a data-primary="nodes" data-secondary="scaling up" data-type="indexterm" id="idm45902080924816"/> nodes during peak times or shut down idle nodes during other times, reducing infrastructure costs. While the HPA and VPA perform Pod-level scaling and ensure service-capacity elasticity within a cluster, the CA provides node scalability to ensure cluster-capacity elasticity.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45902080923584">&#13;
<h1>Cluster API</h1>&#13;
<p>All<a data-primary="Cluster API" data-type="indexterm" id="idm45902080922288"/> major cloud providers support Kubernetes CA. However, to make this happen, plugins have been written by cloud providers, leading to vendor locking and inconsistent CA support. Luckily, the Cluster API Kubernetes project aims to provide APIs for cluster creation, configuration, and management. All major public and private cloud providers like AWS, IBM Cloud, Azure, GCE, vSphere, and OpenStack support this initiative. This also allows CA to be used in on-premises Kubernetes installations. The heart of the Cluster API is a machine controller running in the background, for which several independent implementations like the Kubermatic machine-controller or the machine-api-operator by Red Hat OpenShift already exist. It is worth keeping an eye on the Cluster API as it may become the backbone for any cluster autoscaling in the future.</p>&#13;
</div></aside>&#13;
&#13;
<p>CA is a Kubernetes add-on that has to be turned on and configured with a minimum and maximum number of nodes. It can function only when the Kubernetes cluster is running on a cloud-computing infrastructure where nodes can be provisioned and decommissioned on demand and that has support for Kubernetes CA, such as AWS, IBM Cloud Kubernetes Service, Microsoft Azure, or Google Compute Engine.</p>&#13;
&#13;
<p>A CA primarily performs two operations: it add new nodes to a cluster or removes nodes from a cluster. Let’s see how these actions are performed:</p>&#13;
<dl>&#13;
<dt>Adding a new node (scale-up)</dt>&#13;
<dd>&#13;
<p>If you have an application with a variable load (busy times during the day, weekend, or holiday season and much less load during other times), you need varying capacity to meet these demands. You could buy fixed capacity from a cloud provider to cover the peak times, but paying for it during less busy periods reduces the benefits of cloud computing. This is where CA becomes truly useful.</p>&#13;
&#13;
<p>When a Pod is scaled horizontally or vertically, either manually or through HPA or VPA, the replicas have to be assigned to nodes with enough capacity to satisfy the requested CPU and memory. If no node in the cluster has enough capacity to satisfy all of the Pod’s requirements, the Pod is marked as <em>unschedulable</em> and remains in the waiting state until such a node is found. CA monitors for such Pods to see whether adding a new node would satisfy the needs of the Pods. If the answer is yes, it resizes the cluster and accommodates the waiting Pods.</p>&#13;
&#13;
<p>CA cannot expand the cluster by a random node—it has to choose a node from the available node groups the cluster is running on.<sup><a data-type="noteref" href="ch29.html#idm45902080917024" id="idm45902080917024-marker">4</a></sup> It assumes that all the machines in a node group have the same capacity and the same labels, and that they run the same Pods specified by local manifest files or<a data-primary="DaemonSets" data-secondary="cluster autoscaling" data-type="indexterm" id="idm45902080916336"/> DaemonSets. This assumption is necessary for CA to estimate how much extra Pod capacity a new node will add to the cluster.</p>&#13;
&#13;
<p>If multiple node groups are satisfying the needs of the waiting Pods, CA can be configured to choose a node group by different strategies called <em>expanders</em>. An expander can expand a node group with an additional node by prioritizing least cost or least resource waste, accommodating most Pods, or just randomly. At the end of a successful node selection, a new machine should be provisioned by the cloud provider in a few minutes and registered in the API Server as a new Kubernetes node ready to host the waiting Pods.</p>&#13;
</dd>&#13;
<dt>Removing a node (scale-down)</dt>&#13;
<dd>&#13;
<p>Scaling<a data-primary="nodes" data-secondary="scaling down" data-type="indexterm" id="idm45902080912832"/> down Pods or nodes without service disruption is always more involved and requires many checks. CA performs scale-down if there is no need to scale up and a node is identified as unneeded. A node is qualified for scale-down if it satisfies the following main conditions:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>More than half of its capacity is unused—that is, the sum of all requested CPU and the memory of all Pods on the node is less than 50% of the node-allocatable resource capacity.</p>&#13;
</li>&#13;
<li>&#13;
<p>All movable Pods on the node (Pods that are not run locally by manifest files or Pods created by<a data-primary="DaemonSets" data-secondary="scaling down" data-type="indexterm" id="idm45902080909600"/> DaemonSets) can be placed on other nodes. To prove that, CA performs a scheduling simulation and identifies the future location of every Pod that would be evicted. The final location of the Pods is still determined by the scheduler and can be different, but the simulation ensures there is spare capacity for the Pods.</p>&#13;
</li>&#13;
<li>&#13;
<p>There are no other reasons to prevent node deletion, such as a node being excluded from scaling down through annotations.</p>&#13;
</li>&#13;
<li>&#13;
<p>There are no Pods that cannot be moved, such as Pods with a PodDisruptionBudget that cannot be satisfied, Pods with local storage, Pods with annotations preventing eviction, Pods created without a controller, or system Pods.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>All of these checks are performed to ensure no Pod is deleted that cannot be started on a different node. If all of the preceding conditions are true for a while (the default is 10 minutes), the node qualifies for deletion. The node is deleted by marking it as unschedulable and moving all Pods from it to other nodes.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#img-elastic-scale-ca">Figure 29-5</a> summarizes how the CA interacts with cloud providers and Kubernetes for scaling out cluster nodes.</p>&#13;
&#13;
<figure><div class="figure" id="img-elastic-scale-ca">&#13;
<img alt="Cluster autoscaling mechanism" src="assets/kup2_2905.png"/>&#13;
<h6><span class="label">Figure 29-5. </span>Cluster autoscaling mechanism</h6>&#13;
</div></figure>&#13;
&#13;
<p>As<a data-primary="Pods" data-secondary="automatic scaling based on loads" data-type="indexterm" id="idm45902080902080"/> you’ve probably figured out by now, scaling Pods and nodes are decoupled but complementary procedures. An HPA or VPA can analyze usage metrics and events, and scale Pods. If the cluster capacity is insufficient, the CA kicks in and increases the capacity. The CA is also helpful when irregularities occur in the cluster load due to batch Jobs, recurring tasks, continuous integration tests, or other peak tasks that require a temporary increase in the capacity. It can increase and reduce capacity and provide significant savings on cloud infrastructure costs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scaling Levels" data-type="sect2"><div class="sect2" id="idm45902080900624">&#13;
<h2>Scaling Levels</h2>&#13;
&#13;
<p>In this chapter, we explored various techniques for scaling deployed workloads to meet their changing resource needs. While a human operator can manually perform most of the activities listed here, that doesn’t align with the cloud native mindset. To enable large-scale distributed system management, automating repetitive activities is a must. The preferred approach is to automate scaling and enable human operators to focus on tasks that a Kubernetes<a data-primary="Operator" data-type="indexterm" id="idm45902080898976"/><a data-primary="Operator" data-secondary="Elastic Scale" data-type="indexterm" id="idm45902080898272"/> Operator cannot automate yet.</p>&#13;
&#13;
<p>Let’s<a data-primary="scaling" data-secondary="application scaling levels" data-type="indexterm" id="idm45902080896816"/> review all of the scaling techniques, from the more granular to the more coarse-grained order, as shown in <a data-type="xref" href="#img-elastic-scale-levels">Figure 29-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="img-elastic-scale-levels">&#13;
<img alt="Application scaling levels" src="assets/kup2_2906.png"/>&#13;
<h6><span class="label">Figure 29-6. </span>Application-scaling levels</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Application tuning" data-type="sect3"><div class="sect3" id="idm45902080892544">&#13;
<h3>Application tuning</h3>&#13;
&#13;
<p>At the most granular level, there is an application tuning technique we didn’t cover in this chapter, as it is not a Kubernetes-related activity. However, the very first action you can take is to tune the application running in the container to best use the allocated resources. This activity is not performed every time a service is scaled, but it must be performed initially before hitting production. For example, for Java runtimes, that is right-sizing thread pools for best use of the available CPU shares the container is getting, then tuning the different memory regions such as heap, nonheap, and thread stack sizes. Adjusting these values is typically performed through configuration changes rather than code changes.</p>&#13;
&#13;
<p>Container-native applications use start scripts that can calculate good default values for thread counts, and memory sizes for the application based on the allocated container resources rather than the shared full-node capacity. Using such scripts is an excellent first step. You can also go one step further and use techniques and libraries such as the Netflix Adaptive Concurrency Limits library, where the application can dynamically calculate its concurrency limits by self-profiling and adapting. This is a kind of in-app autoscaling that removes the need for manually tuning services.</p>&#13;
&#13;
<p>Tuning applications can cause regressions similar to a code change and must be followed by a degree of testing. For example, changing the heap size of an application can cause it to be killed with an <code>OutOfMemory</code> error, and horizontal scaling won’t be able to help. On the other hand, scaling Pods vertically or horizontally, or provisioning more nodes, will not be as effective if your application is not consuming the resources allocated for the container properly. So tuning for scale at this level can impact all other scaling methods and can be disruptive, but it must be performed at least once for optimal application behavior.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vertical Pod autoscaling" data-type="sect3"><div class="sect3" id="idm45902080889712">&#13;
<h3>Vertical Pod autoscaling</h3>&#13;
&#13;
<p>Assuming the application is consuming the container resources effectively, the next step is setting the right resource requests and limits in the containers. Earlier, we explored how VPA can automate the process of discovering and applying optimal values driven by real consumption. A significant concern here is that Kubernetes requires Pods to be deleted and created from scratch, which leaves the potential for short or unexpected periods of service disruption. Allocating more resources to a resource-starved container may make the Pod unschedulable and increase the load on other instances even more. Increasing container resources may also require application tuning to best use the increased resources.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Horizontal Pod autoscaling" data-type="sect3"><div class="sect3" id="idm45902080888112">&#13;
<h3>Horizontal Pod autoscaling</h3>&#13;
&#13;
<p>The preceding two techniques are a form of vertical scaling; we hope to get better performance from existing Pods by tuning them but without changing their count. The following two techniques are a form of horizontal scaling: we don’t touch the Pod specification, but we change the Pod and node count. This approach reduces the chances of introducing any regression and disruption and allows more straightforward automation. HPA, <a data-primary="Knative" data-secondary="horizontal pod autoscaling" data-type="indexterm" id="idm45902080886768"/>Knative, and KEDA are the most popular forms of horizontal scaling. Initially, HPA provided minimal functionality through CPU and memory metrics support only. Now it uses custom and external metrics for more advanced scaling use cases that allow scaling based on metrics that have an improved cost correlation.</p>&#13;
&#13;
<p>Assuming that you have performed the preceding two methods once for identifying good values for the application setup itself and determined the resource consumption of the container, from there on, you can enable HPA and have the application adapt to shifting resource needs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster autoscaling" data-type="sect3"><div class="sect3" id="idm45902080884960">&#13;
<h3>Cluster autoscaling</h3>&#13;
&#13;
<p>The scaling techniques described in HPA and VPA provide elasticity within the boundary of the cluster capacity only. You can apply them only if there is enough room within the Kubernetes cluster. CA introduces flexibility at the cluster capacity level. CA is complementary to the other scaling methods but is also completely decoupled. It doesn’t care about the reason for extra capacity demand, or why there is unused capacity, or whether it is a human operator or an autoscaler that is changing the workload profiles. CA can extend the cluster to ensure demanded capacity or shrink it to spare some resources.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect1"><div class="sect1" id="idm45902080883376">&#13;
<h1>Discussion</h1>&#13;
&#13;
<p>Elasticity and the different scaling techniques are an area of Kubernetes that is still actively evolving. The VPA, for example, is still experimental. Also, with the popularization of the serverless programming model, scaling to zero and quick scaling have become a priority. Knative and KEDA are Kubernetes add-ons that exactly address this need to provide the foundation for scale-to-zero, as we briefly described in <a data-type="xref" href="#elastic-scale-knative">“Knative”</a> and <a data-type="xref" href="#elastic-scale-keda">“KEDA”</a>. Those projects are progressing quickly and are introducing very exciting new cloud native primitives. We are watching this space closely and recommend you keep an eye on Knative and KEDA too.</p>&#13;
&#13;
<p>Given a desired state specification of a distributed system, Kubernetes can create and maintain it. It also makes it reliable and resilient to failures, by continuously monitoring and self-healing and ensuring its current state matches the desired one. While a resilient and reliable system is good enough for many applications today, Kubernetes goes a step further. A small but properly configured Kubernetes system would not break under a heavy load but instead would scale the Pods and nodes. So in the face of these external stressors, the system would get bigger and stronger rather than weaker and more brittle, giving Kubernetes antifragile capabilities.<a data-primary="" data-startref="elstcscl29" data-type="indexterm" id="idm45902080879568"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="More Information" data-type="sect1"><div class="sect1" id="elastic-scale-more-information">&#13;
<h1>More Information</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/PTUws">Elastic Scale Example</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/x2DJI">Rightsize Your Pods with Vertical Pod Autoscaling</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/_nRvf">Kubernetes Autoscaling 101</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/_hg2J">Horizontal Pod Autoscaling</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/n1C4o">HPA Algorithm Details</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/4BN1z">Horizontal Pod Autoscaler Walk-Through</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/8W7WM">Knative</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/dt15f">Knative Autoscaling</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/-f2di">Knative: Serving Your Serverless Services</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://keda.sh">KEDA</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/0Q4g4">Application Autoscaling Made Easy with Kubernetes Event-Driven Autoscaling (KEDA)</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/lIDRK">Kubernetes Metrics API and Clients</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/GowW1">Vertical Pod Autoscaling</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/bhuVj">Configuring Vertical Pod Autoscaling</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/8LUZT">Vertical Pod Autoscaler Proposal</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/Hk5Xc">Vertical Pod Autoscaler GitHub Repo</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/eKb8G">Kubernetes VPA: Guide to Kubernetes Autoscaling</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/inobt">Cluster Autoscaler</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/oq_FS">Performance Under Load: Adaptive Concurrency Limits at Netflix</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/YmgkB">Cluster Autoscaler FAQ</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/pw4aC">Cluster API</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/OvJrT">Kubermatic Machine-Controller</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/W2o6v">OpenShift Machine API Operator</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/RH7fI">Adaptive Concurrency Limits Library (Java)</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/f0TyP">Knative Tutorial</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45902081598912"><sup><a href="ch29.html#idm45902081598912-marker">1</a></sup> For multiple running Pods, the average CPU utilization is used as <em>currentMetricValue</em>.</p><p data-type="footnote" id="idm45902081399136"><sup><a href="ch29.html#idm45902081399136-marker">2</a></sup> CloudEvents is a CNCF standard that describes the format and metadata for events in a cloud context.</p><p data-type="footnote" id="idm45902081237056"><sup><a href="ch29.html#idm45902081237056-marker">3</a></sup> KEDA initially did not support HTTP-triggered autoscaling, and although there is now a <a href="https://oreil.ly/DyvZK">KEDA HTTP add-on</a> it is still in its infancy (in 2023), requires a complex setup, and would need to catch up quite a bit to reach the maturity of the KPA that is included out of the box in Knative.</p><p data-type="footnote" id="idm45902080917024"><sup><a href="ch29.html#idm45902080917024-marker">4</a></sup> Node groups is not an intrinsic Kubernetes concept (i.e., there is no NodeGroup resource) but is used as an abstraction in the CA and Cluster APIs to describe nodes that share certain characteristics.</p></div></div></section></body></html>