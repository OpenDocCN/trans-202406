<html><head></head><body><section data-pdf-bookmark="Chapter 14. Continuous Deployment in Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="continuous">&#13;
<h1><span class="label">Chapter 14. </span>Continuous Deployment in Kubernetes</h1>&#13;
&#13;
<blockquote class="epigraph">&#13;
<p>Tao does not do, but nothing is not done.</p>&#13;
<p data-type="attribution">Lao Tzu</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="continuous deployment (CD)" data-type="indexterm" id="ix_14-continuous-adoc0"/><a data-primary="Kubernetes" data-secondary="continuous deployment" data-type="indexterm" id="ix_14-continuous-adoc1"/>In this chapter, we’ll look at a key DevOps principle—<em>continuous integration</em> and <em>continuous deployment</em> (CI/CD) and see how we can achieve this in a cloud native, Kubernetes-based environment. We outline some of the options for setting up continuous deployment pipelines to work with Kubernetes, and show you a fully worked example using Google’s Cloud Build. We will also cover the concept of GitOps and walk through how to automatically deploy to Kubernetes using a GitOps tool called Flux.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Is Continuous Deployment?" data-type="sect1"><div class="sect1" id="idm45979375457312">&#13;
<h1>What Is Continuous Deployment?</h1>&#13;
&#13;
<p><a data-primary="continuous deployment (CD)" data-secondary="introduction to" data-type="indexterm" id="idm45979375455968"/><em>Continuous deployment</em> (CD) is the automatic deployment of successful builds to production. Like the test suite, deployment should also be managed centrally and automated. Developers should be able to deploy new versions by either pushing a button, or merging a merge request, or pushing a Git release tag.</p>&#13;
&#13;
<p>CD is often associated with <em>continuous integration</em> (CI): the automatic integration and testing of developers’ changes against the mainline branch. The idea is that if you’re making changes on a branch that would break the build when merged to the mainline, continuous integration will let you know that right away, rather than waiting until you finish your branch and do the final merge. The combination of continuous integration and deployment is often referred to as <em>CI/CD</em>.</p>&#13;
&#13;
<p>The machinery of continuous deployment is often referred to as a <em>pipeline</em>: a series of automated actions that take code from the developer’s workstation to production, via a sequence of test and acceptance stages.</p>&#13;
&#13;
<p>A typical pipeline for containerized applications might look like the following:</p>&#13;
<ol>&#13;
<li>&#13;
<p>A developer pushes their code changes to the repository.</p>&#13;
</li>&#13;
<li>&#13;
<p>The build system automatically builds the current version of the code and runs tests.</p>&#13;
</li>&#13;
<li>&#13;
<p>If all tests pass, the container image will be published into the central container registry.</p>&#13;
</li>&#13;
<li>&#13;
<p>The newly built container is deployed automatically to a staging environment.</p>&#13;
</li>&#13;
<li>&#13;
<p>The staging environment undergoes some automated and/or manual acceptance tests.</p>&#13;
</li>&#13;
<li>&#13;
<p>The verified container image is deployed to production.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>A key point is that the artifact that is tested and deployed through your various environments is not the <em>source code</em>, but the <em>container</em>. There are many ways for errors to creep in between source code and a running binary, and testing the container instead of the code can help catch a lot of these.</p>&#13;
&#13;
<p>The great benefit of CD is <em>no surprises in production</em>; nothing gets deployed unless the exact binary image has already been successfully tested in staging.</p>&#13;
&#13;
<p>You can see a detailed example of a CD pipeline like this in <a data-type="xref" href="#cloudbuild-example">“A CI/CD Pipeline with Cloud Build”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Which CD Tool Should I Use?" data-type="sect1"><div class="sect1" id="idm45979375441792">&#13;
<h1>Which CD Tool Should I Use?</h1>&#13;
&#13;
<p><a data-primary="continuous deployment (CD)" data-secondary="tools for" data-type="indexterm" id="ix_14-continuous-adoc2"/>As usual, the problem is not a shortage of available tools, but the sheer range of choices. There are several new CI/CD tools designed specifically for cloud native applications, and long-established traditional build tools such as Jenkins also now have plug-ins to allow them to work with Kubernetes and containers.</p>&#13;
&#13;
<p>As a result, if you are already doing CI/CD, you probably don’t need to switch to a whole new system. If you are migrating existing applications to Kubernetes, you can likely do this with a few small tweaks to your existing pipelines.</p>&#13;
&#13;
<p>In the next section, we will briefly cover some of the popular hosted and self-hosted options for CI/CD tools. We certainly won’t be able to cover them all, but here is a quick list that should get you started in your search.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hosted CI/CD Tools" data-type="sect1"><div class="sect1" id="idm45979375437632">&#13;
<h1>Hosted CI/CD Tools</h1>&#13;
&#13;
<p>If you are looking for an out-of-the-box solution for your CI/CD pipelines where you do not need to maintain the underlying infrastructure, then you should consider using a hosted offering. The main cloud providers all offer CI/CD tools that integrate well within their ecosystems, so it would be worth first exploring the tools that are already part of your cloud account.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Azure Pipelines" data-type="sect2"><div class="sect2" id="idm45979375436128">&#13;
<h2>Azure Pipelines</h2>&#13;
&#13;
<p><a data-primary="Azure Pipelines" data-type="indexterm" id="idm45979375434928"/>Microsoft’s Azure DevOps service (formerly known as Visual Studio Team Services) includes a continuous delivery pipeline facility, called <a href="https://oreil.ly/pVbMb">Azure Pipelines</a>, similar to Google Cloud Build.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Google Cloud Build" data-type="sect2"><div class="sect2" id="idm45979375432928">&#13;
<h2>Google Cloud Build</h2>&#13;
&#13;
<p><a data-primary="Google Cloud Build" data-type="indexterm" id="idm45979375431728"/><a data-primary="Google Cloud Build" data-secondary="benefits of" data-type="indexterm" id="idm45979375431024"/>If you run your infrastructure on Google Cloud Platform, then you should look at <a href="https://cloud.google.com/build">Cloud Build</a>. It runs containers as the various build steps and the configuration YAML for the pipeline lives in your code repository.</p>&#13;
&#13;
<p>You can configure Cloud Build to watch your Git repository. When a preset condition is triggered, such as pushing to a certain branch or tag, Cloud Build will run your specified pipeline, such as building a new container image, running your test suite, publishing the image, and perhaps deploying the new version to Kubernetes.</p>&#13;
&#13;
<p>For a complete working example of a CD pipeline in Cloud Build, see <a data-type="xref" href="#cloudbuild-example">“A CI/CD Pipeline with Cloud Build”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Codefresh" data-type="sect2"><div class="sect2" id="idm45979375427232">&#13;
<h2>Codefresh</h2>&#13;
&#13;
<p><a data-primary="Codefresh" data-type="indexterm" id="idm45979375426032"/><a href="https://codefresh.io">Codefresh</a> is a managed service for testing and deploying applications to Kubernetes. One interesting feature is the ability to deploy temporary staging environments for every feature branch.</p>&#13;
&#13;
<p>Using containers, Codefresh can build, test, and deploy on-demand environments, and then you can configure how you would like to deploy your containers into various environments in your clusters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GitHub Actions" data-type="sect2"><div class="sect2" id="idm45979375424000">&#13;
<h2>GitHub Actions</h2>&#13;
&#13;
<p><a data-primary="GitHub Actions" data-type="indexterm" id="idm45979375422800"/><a href="https://oreil.ly/eeWSL">GitHub Actions</a> is integrated into the popular hosted Git repository site. Actions are shared using GitHub repos, making it very easy to mix and match and share build tools across different applications. Azure has published a <a href="https://oreil.ly/GcHam">popular GitHub Action</a> for deploying to Kubernetes clusters.</p>&#13;
&#13;
<p>GitHub also offers the option to run GitHub Action runners locally on your own servers to keep your builds inside of your network.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GitLab CI" data-type="sect2"><div class="sect2" id="idm45979375419856">&#13;
<h2>GitLab CI</h2>&#13;
&#13;
<p><a data-primary="GitLab CI" data-type="indexterm" id="idm45979375418656"/>GitLab is a popular alternative to GitHub for hosting Git repositories. You can use their hosted offering, or you can run GitLab yourself on your own infrastructure. It comes with a powerful built-in CI/CD tool, <a href="https://oreil.ly/e5f11">GitLab CI</a>, that can be used for testing and deploying your code. If you are already using GitLab, it makes sense to look at GitLab CI for implementing your continuous deployment pipeline.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Self-Hosted CI/CD Tools" data-type="sect1"><div class="sect1" id="idm45979375416688">&#13;
<h1>Self-Hosted CI/CD Tools</h1>&#13;
&#13;
<p>If you would rather own more of the underlying infrastructure for your pipeline, then there are also several good options for CI/CD tools that you can run wherever you like. Some of these tools have been around long before Kubernetes, and some have been developed specifically for Kubernetes-based CI/CD pipelines.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Jenkins" data-type="sect2"><div class="sect2" id="idm45979375415056">&#13;
<h2>Jenkins</h2>&#13;
&#13;
<p><a data-primary="Jenkins" data-type="indexterm" id="idm45979375413856"/><a href="https://jenkins.io">Jenkins</a> is a very widely adopted CI/CD tool and has been around for years. It has plug-ins for just about everything you could want to use in a workflow, including Docker, <code>kubectl</code>, and Helm. There is also a newer dedicated project for running Jenkins in Kubernetes called <a href="https://jenkins-x.io">JenkinsX</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Drone" data-type="sect2"><div class="sect2" id="idm45979375411024">&#13;
<h2>Drone</h2>&#13;
&#13;
<p><a data-primary="Drone" data-type="indexterm" id="idm45979375409792"/><a href="https://oreil.ly/qtXoR">Drone</a> is a tool built with, and for, containers. It is simple and lightweight, with the pipeline defined by a single YAML file. Since each build step consists of running a container, it means that anything you can run in a container you can run on Drone.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tekton" data-type="sect2"><div class="sect2" id="idm45979375407904">&#13;
<h2>Tekton</h2>&#13;
&#13;
<p><a data-primary="Tekton" data-type="indexterm" id="idm45979375406704"/><a href="https://tekton.dev">Tekton</a> introduces an interesting concept where CI/CD components actually consist of Kubernetes CRDs. You can therefore construct your build, test, and deployment steps using native Kubernetes resources, and manage the pipeline the same way you manage anything else in your Kubernetes clusters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Concourse" data-type="sect2"><div class="sect2" id="idm45979375404784">&#13;
<h2>Concourse</h2>&#13;
&#13;
<p><a data-primary="Concourse" data-type="indexterm" id="idm45979375403584"/><a href="https://concourse-ci.org">Concourse</a> is an open source CD tool written in Go. It also adopts the declarative pipeline approach, much like Drone and Cloud Build, using a YAML file to define and execute build steps. Concourse provides an <a href="https://oreil.ly/UCvPz">official Helm chart</a> to deploy it on Kubernetes, making it easy to get a containerized pipeline up and running quickly.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Spinnaker" data-type="sect2"><div class="sect2" id="idm45979375400928">&#13;
<h2>Spinnaker</h2>&#13;
&#13;
<p><a data-primary="Spinnaker" data-type="indexterm" id="idm45979375399728"/><a href="https://spinnaker.io">Spinnaker</a> is very powerful and flexible, but can be a little daunting at first glance. Developed originally by Netflix, it excels at large-scale and complex deployments, such as blue/green deployments (see <a data-type="xref" href="ch13.html#bluegreen">“Blue/Green Deployments”</a>). There is a free ebook about Spinnaker, titled <a href="https://oreil.ly/hkb64"><em>Continuous Delivery with Spinnaker</em></a> (O’Reilly), that should give you some idea whether Spinnaker fits your needs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Argo" data-type="sect2"><div class="sect2" id="idm45979375396368">&#13;
<h2>Argo</h2>&#13;
&#13;
<p><a data-primary="Argo" data-type="indexterm" id="idm45979375394880"/><a href="https://oreil.ly/8FmGn">Argo CD</a> is a GitOps tool similar to Flux (see <a data-type="xref" href="#gitops">“GitOps”</a>) that automates deployments by syncing what is running in Kubernetes with manifests stored in a central Git repo. Rather than “pushing” changes via <code>kubectl</code> or <code>helm</code>, Argo continuously “pulls” in changes from the Git repo and applies them from within the cluster. Argo also offers a <a href="https://argoproj.github.io">popular pipeline tool</a> for running any sort of pipeline workflows, not necessarily just for CI/CD.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Keel" data-type="sect2"><div class="sect2" id="idm45979375390656">&#13;
<h2>Keel</h2>&#13;
&#13;
<p><a data-primary="Keel" data-type="indexterm" id="idm45979375389456"/><a href="https://keel.sh">Keel</a> is not a full end-to-end CI/CD tool but is solely concerned with deploying new container images when they are published into a container registry. It can be configured to respond to webhooks, send and receive Slack messages, and wait for approvals before deploying to a new environment. If you already have a CI process that works well for you but just need a way to automate the CD part, then Keel may be worth evaluating.<a data-startref="ix_14-continuous-adoc2" data-type="indexterm" id="idm45979375388000"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="A CI/CD Pipeline with Cloud Build" data-type="sect1"><div class="sect1" id="cloudbuild-example">&#13;
<h1>A CI/CD Pipeline with Cloud Build</h1>&#13;
&#13;
<p><a data-primary="continuous deployment (CD)" data-secondary="Cloud Build pipeline" data-type="indexterm" id="ix_14-continuous-adoc3"/><a data-primary="Google Cloud Build" data-secondary="CI/CD pipeline example" data-type="indexterm" id="ix_14-continuous-adoc4"/>Now that you know the general principles of CI/CD, and have learned about some of the tooling options, let’s look at a complete, end-to-end example of a demo pipeline.</p>&#13;
&#13;
<p>The idea is not that you should necessarily use exactly the same tools and configuration as we have here; rather, we hope you’ll get a sense of how everything fits together, and can adapt some parts of this example to suit your own environment.</p>&#13;
&#13;
<p>In this example, we’ll be using GitHub, Google Kubernetes Engine (GKE) clusters, and Google Cloud Build, but we don’t rely on any specific features of those products. You can replicate this kind of pipeline using whatever tools you prefer.</p>&#13;
&#13;
<p>If you’d like to work through this example using your own GCP account, please bear in mind that it uses some billable resources. You’ll want to delete and clean up any test cloud resources afterward to make sure you don’t get charged unexpectedly.</p>&#13;
&#13;
<p>If you’d rather try out a CI/CD example locally without using any Google Cloud resources, skip down to <a data-type="xref" href="#gitops">“GitOps”</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Setting Up Google Cloud and GKE" data-type="sect2"><div class="sect2" id="idm45979375378672">&#13;
<h2>Setting Up Google Cloud and GKE</h2>&#13;
&#13;
<p>If you are signing up for a new Google Cloud account for the first time, you’ll be eligible for some free credits, which should enable you to run a Kubernetes cluster, and other cloud resources, without being billed for a few months. However, you should definitely monitor your usage when trying out any cloud service to make sure that you aren’t accruing any unexpected charges. You can find out more about the free-tier offering and create an account at the <a href="https://cloud.google.com/free">Google Cloud Platform site</a>.</p>&#13;
&#13;
<p>Once you are signed up and logged into your own Google Cloud project, create a GKE cluster following <a href="https://oreil.ly/zcLVd">these instructions</a>. An Autopilot cluster will be fine for this example, and choose a region that is close to your location. You’ll also need to enable the <a href="https://oreil.ly/O8ZNV">Cloud Build</a> and <a href="https://oreil.ly/b86m9">Artifact Registry</a> APIs in your new project, as we’ll be using those services along with GKE.</p>&#13;
&#13;
<p>Next we’ll walk you through the following steps to prepare for creating the pipeline:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Fork the demo repository into your own personal GitHub account.</p>&#13;
</li>&#13;
<li>&#13;
<p>Create a container repository in Artifact Registry.</p>&#13;
</li>&#13;
<li>&#13;
<p>Authenticate Cloud Build to use Artifact Registry and GKE.</p>&#13;
</li>&#13;
<li>&#13;
<p>Create a Cloud Build trigger for building and testing on a push to any Git branch.</p>&#13;
</li>&#13;
<li>&#13;
<p>Create a trigger for deploying to GKE based on Git tags.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Forking the Demo Repository" data-type="sect2"><div class="sect2" id="fork-demo-repo">&#13;
<h2>Forking the Demo Repository</h2>&#13;
&#13;
<p>Using your GitHub account, use the GitHub interface to fork the <a href="https://oreil.ly/LAI8f">demo repo</a>. If you are unfamiliar with a repo fork, you can learn more about it in the <a href="https://oreil.ly/S37E1">GitHub docs</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Create Artifact Registry Container Repository" data-type="sect2"><div class="sect2" id="idm45979375364352">&#13;
<h2>Create Artifact Registry Container Repository</h2>&#13;
&#13;
<p>GCP offers a private artifact repository tool called Artifact Registry that can store Docker containers, Python packages, npm packages, and other types of artifacts. We will use this for hosting the demo container image that we will build.</p>&#13;
&#13;
<p>Browse to the Artifact Registry page in the <a href="https://oreil.ly/aIZet">Google Cloud web console</a> and create a new Docker repository called <code>demo</code> following <a href="https://oreil.ly/sBY0O">these instructions</a>. Create it in the same Google Cloud region where you created your GKE cluster.</p>&#13;
&#13;
<p>You will also need to authorize the Cloud Build service account to have permission to make changes to your Kubernetes Engine cluster. Under the IAM section in GCP, grant the service account for Cloud Build—the <em>Kubernetes Engine Developer</em> and <em>Artifact Registry Repository Administrator</em>—IAM roles in your project following <a href="https://oreil.ly/tMtAR">the instructions in the GCP docs</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Configuring Cloud Build" data-type="sect2"><div class="sect2" id="idm45979375357792">&#13;
<h2>Configuring Cloud Build</h2>&#13;
&#13;
<p>Now let’s look at the steps in our build pipeline. In many modern CI/CD platforms, each step of a pipeline consists of running a container. The build steps are defined using a YAML file that lives in your Git repo. Using containers for each step means that you can easily package, version, and share common tools and scripts between different pipelines.</p>&#13;
&#13;
<p>Inside the demo repository, there is a directory called <em>hello-cloudbuild-v2</em>. Inside that directory, you will find the <em>cloudbuild.yaml</em> file that defines our Cloud Build pipeline.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Notice the <em>-v2</em> in the directory name for <em>hello-cloudbuild-v2</em>. There are some changes here for the second edition of the book that do not match the first edition. In order to avoid breaking any of the examples used in the first edition, we are using a completely different directory for this example.</p>&#13;
</div>&#13;
&#13;
<p>Let’s look at each of the build steps in this file in turn.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Building the Test Container" data-type="sect2"><div class="sect2" id="idm45979375351616">&#13;
<h2>Building the Test Container</h2>&#13;
&#13;
<p>Here’s the first step:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">build-test-image</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">dir</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hello-cloudbuild-v2</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">gcr.io/cloud-builders/docker</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">entrypoint</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bash</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">args</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">-c</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="p-Indicator">|</code><code class="w"/>&#13;
<code class="w">      </code><code class="no">docker image build --target build --tag demo:test .</code><code class="w"/></pre>&#13;
&#13;
<p>Like all Cloud Build steps, this consists of a set of YAML key-value pairs:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>id</code> gives a human-friendly label to the build step.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>dir</code> specifies the subdirectory of the Git repo to work in.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>name</code> identifies the container to run for this step.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>entrypoint</code> specifies the command to run in the container, if not the default.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>args</code> gives the necessary arguments to the entrypoint command. (We’re using a little trick here with <code>bash -c |</code> to keep our <code>args</code> together on a single line, just to make it easier to read.)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The purpose of this first step is to build a container that we can use to run our application’s tests. Since we are using a multistage build (see <a data-type="xref" href="ch02.html#multistagedockerfile">“Understanding Dockerfiles”</a>), we want to build only the first stage for now. So we are using the <code>--target build</code> argument, which tells Docker to only build the part in the Dockerfile under <code>FROM golang:1.17-alpine AS build</code> and stop before moving on to the next step.</p>&#13;
&#13;
<p>This means that the resulting container will still have Go installed, along with any of the packages or files used in the step labeled <code>...AS build</code>, which means that we can use this image to run the test suite. It is often the case that you need packages in your container for running tests that you do not want to be in your final production images. Here we are essentially building a throwaway container that is only used for running the test suite, and is discarded afterwards.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Running the Tests" data-type="sect2"><div class="sect2" id="idm45979375319632">&#13;
<h2>Running the Tests</h2>&#13;
&#13;
<p>Here’s the next step in our <code>cloudbuild.yaml</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">run-tests</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">dir</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hello-cloudbuild-v2</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">gcr.io/cloud-builders/docker</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">entrypoint</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bash</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">args</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">-c</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="p-Indicator">|</code><code class="w"/>&#13;
<code class="w">      </code><code class="no">docker container run demo:test go test</code><code class="w"/></pre>&#13;
&#13;
<p>Since we tagged our throwaway container as <code>demo:test</code>, that temporary image will still be available for the rest of this build inside Cloud Build. This step will run the <code>go test</code> command against that container. If any tests fail, this step will fail and the build will exit. Otherwise, it will continue on to the next step.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Building the Application Container" data-type="sect2"><div class="sect2" id="idm45979375257264">&#13;
<h2>Building the Application Container</h2>&#13;
&#13;
<p>Here we run <code>docker build</code> again, but without the <code>--target</code> flag so that we run the entire multistage build, ending up with the final application container:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">build-app</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">dir</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hello-cloudbuild-v2</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">gcr.io/cloud-builders/docker</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">args</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">docker</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">build</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">--tag</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">${_REGION}-docker.pkg.dev/$PROJECT_ID/demo/demo:$COMMIT_SHA</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">.</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Substitution Variables" data-type="sect2"><div class="sect2" id="idm45979375168464">&#13;
<h2>Substitution Variables</h2>&#13;
&#13;
<p><a data-primary="substitutions, in Cloud Build" data-type="indexterm" id="idm45979375111840"/>In order to make Cloud Build pipeline files reusable and flexible we use variables, or what Cloud Build calls <em>substitutions</em>. Anything that begins with a <code>$</code> will be substituted when the pipeline runs. For example, <code>$PROJECT_ID</code> will interpolate as the Google Cloud Project where a particular build is running, and <code>$COMMIT_SHA</code> is the specific Git commit SHA that triggered this build. User-defined substitutions in Cloud Build must begin with an underscore character (<code>_</code>) and use only uppercase letters and numbers. We will use the <code>${_REGION}</code> substitution variable below when we create the build trigger.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Git SHA Tags" data-type="sect2"><div class="sect2" id="gitshatags">&#13;
<h2>Git SHA Tags</h2>&#13;
&#13;
<p><a data-primary="Git" data-secondary="SHA tags" data-type="indexterm" id="idm45979375148608"/><a data-primary="SHA tags" data-type="indexterm" id="idm45979375147632"/>You may ask why we are using <code>$COMMIT_SHA</code> for our container image tag. In Git, every commit has a unique identifier, called a SHA (named for the Secure Hash Algorithm that generates it). A SHA is a long string of hex digits, like <code>5ba6bfd64a31eb4013ccaba27d95cddd15d50ba3</code>.</p>&#13;
&#13;
<p>If you use this SHA to tag your image, it provides a link to the exact Git commit that generated it—which is also a complete snapshot of the code that is in the container. The nice thing about tagging build artifacts with the originating Git SHA is that you can build and test lots of feature branches simultaneously, without any conflicts.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Validating the Kubernetes Manifests" data-type="sect2"><div class="sect2" id="idm45979375145008">&#13;
<h2>Validating the Kubernetes Manifests</h2>&#13;
&#13;
<p>At this point in the pipeline, we have built a new container that has passed tests and is ready to deploy. But before we do, we’d also like to do a quick check to make sure that our Kubernetes manifests are valid. <a data-primary="Helm" data-secondary="commands" data-tertiary="template" data-type="indexterm" id="idm45979375143248"/>In this final step of the build, we’ll run <code>helm template</code> to generate the rendered version of our Helm chart, and <a data-primary="kubeval" data-type="indexterm" id="idm45979375141488"/>then pipe that to the <code>kubeval</code> tool (see <a data-type="xref" href="ch12.html#kubeval">“kubeval”</a>) to check for any issues:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubeval</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">dir</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hello-cloudbuild-v2</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cloudnatived/helm-cloudbuilder</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">entrypoint</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bash</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">args</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">-c</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="p-Indicator">|</code><code class="w"/>&#13;
<code class="w">      </code><code class="no">helm template ./k8s/demo/ | kubeval</code><code class="w"/></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Note that we’re using our own Helm container image here (<code>cloudnatived/helm-cloudbuilder</code>), which contains <code>helm</code> and <code>kubeval</code>, but you could also create and use your own “builder images,” containing any additional build or testing tools that you use. Just remember that it’s important to keep your builder images small and lean (see <a data-type="xref" href="ch02.html#minimalcontainers-intro">“Minimal Container Images”</a>). When you’re running tens or hundreds of builds a day, the increased pull time of large containers can really add up.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Publishing the Image" data-type="sect2"><div class="sect2" id="idm45979375064672">&#13;
<h2>Publishing the Image</h2>&#13;
&#13;
<p>Assuming each step in the pipeline completes successfully, Cloud Build can then publish the resulting container image to the Artifact Registry repository you created earlier. To specify which images from the build that you want to publish, list them under <code>images</code> in the Cloud Build file:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">images</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">${_REGION}-docker.pkg.dev/$PROJECT_ID/demo/demo:$COMMIT_SHA</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating the First Build Trigger" data-type="sect2"><div class="sect2" id="create-build-trigger">&#13;
<h2>Creating the First Build Trigger</h2>&#13;
&#13;
<p><a data-primary="triggers, in Cloud Build" data-type="indexterm" id="idm45979375018000"/>Now that you’ve seen how the pipeline works, let’s create the build triggers in Google Cloud that will actually execute the pipeline, based on our specified conditions. A Cloud Build trigger specifies a Git repo to watch, a condition on which to activate (such as pushing to a particular branch or tag), and a pipeline file to execute.</p>&#13;
&#13;
<p>Go ahead and create a new Cloud Build trigger now. Log in to your Google Cloud project and browse to the <a href="https://oreil.ly/LLuWt">Cloud Build triggers page</a>.</p>&#13;
&#13;
<p>Click the Add Trigger button to make a new build trigger, and select GitHub as the source repository. You’ll be asked to grant permission for Google Cloud to access your GitHub repo. Select <code>YOUR_GITHUB_USERNAME/demo</code> and Google Cloud will link to your forked copy of the demo repository. You can name the trigger whatever you like.</p>&#13;
&#13;
<p>Under the Branch section, select <code>.*</code> so that it will match any branch.</p>&#13;
&#13;
<p>Under the Configuration section, choose the Cloud Build configuration file and set the location to <em>hello-cloudbuild-v2/cloudbuild.yaml</em>, which is where the file lives in the demo repo.</p>&#13;
&#13;
<p>Finally, we need to create some substitution variables so that we can reuse this same <em>cloudbuild.yaml</em> file for different builds.</p>&#13;
&#13;
<p>For this example, you’ll need to add the following substitution variable to your trigger:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>_REGION</code> should be the GCP region where you deployed your Artifact Registry and GKE cluster, such as <code>us-central1</code>, or <code>southamerica-east1</code>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Click the Create trigger button when you are done. You’re now ready to test the trigger and see what happens!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Testing the Trigger" data-type="sect2"><div class="sect2" id="idm45979374986720">&#13;
<h2>Testing the Trigger</h2>&#13;
&#13;
<p>Go ahead and make a change to your forked copy of the demo repository. Edit both <em>main.go</em> and <em>main_test.go</em>, replacing <code>Hello</code> with <code>Hola</code>, or whatever you like, and save both files (we’ll use <code>sed</code> in the example below). You can also run the tests locally, if you have Golang installed, to make sure that the test suite still passes. When ready, commit and push the changes to your forked copy of the Git repo:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong>cd hello-cloudbuild-v2</strong>&#13;
<strong>sed -i <em>s/Hello/Hola/g</em> main.go</strong>&#13;
<strong>sed -i <em>s/Hello/Hola/g</em> main_test.go</strong>&#13;
<strong>go test</strong>&#13;
PASS&#13;
ok  	github.com/cloudnativedevops/demo/hello-cloudbuild	0.011s&#13;
<strong>git commit -am "Update greeting"</strong>&#13;
<strong>git push</strong>&#13;
...</pre>&#13;
&#13;
<p>If you look in the <a href="https://oreil.ly/YAc9a">Cloud Build web UI</a>, you will see the list of recent builds in your project. You should see one at the top of the list for the current change you just pushed. It may be still running, or it may have already finished.</p>&#13;
&#13;
<p>Hopefully you will see a green check indicating that all steps passed. If not, check the log output in the build and see what failed.</p>&#13;
&#13;
<p>Assuming it passed, a container should have been published into your private Google Artifact Registry tagged with the Git commit SHA of your change.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploying from a CI/CD Pipeline" data-type="sect2"><div class="sect2" id="idm45979374976384">&#13;
<h2>Deploying from a CI/CD Pipeline</h2>&#13;
&#13;
<p>Now that you can trigger a build with a Git push, run tests, and publish the final container to the registry, you are ready to deploy that container to Kubernetes.</p>&#13;
&#13;
<p>For this example we will imagine there are two environments, one for <code>production</code>, and one for <code>staging</code>, and we will deploy them into separate namespaces: <code>staging-demo</code> and <code>production-demo</code>. Both will run in the same GKE cluster (although you would probably want to use separate clusters for your real applications).</p>&#13;
&#13;
<p>To keep things simple, we’re going to use the Git tags <code>production</code> and <code>staging</code> for triggering deployments to each environment. You may have your own process for managing versions, such as using semantic version (<a href="https://semver.org">SemVer</a>), release tags, or automatically deploying to a staging environment whenever the main or trunk branch is updated. Feel free to adapt these examples to your own situation.</p>&#13;
&#13;
<p>We will configure Cloud Build to deploy to staging when the <code>staging</code> Git tag is pushed to the repo, and to production when the <code>production</code> tag is pushed. This requires a new pipeline that uses a different Cloud Build YAML file, <em>cloudbuild-deploy.yaml</em>. Let’s take a look at the steps that are in our deploy pipepline:</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Getting credentials for the Kubernetes cluster" data-type="sect3"><div class="sect3" id="idm45979374968336">&#13;
<h3>Getting credentials for the Kubernetes cluster</h3>&#13;
&#13;
<p>To deploy to Kubernetes with Cloud Build, the build will need a working <code>KUBECONFIG</code>, which we can get with <code>kubectl</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">get-kube-config</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">dir</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hello-cloudbuild-v2</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">gcr.io/cloud-builders/kubectl</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">env</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">CLOUDSDK_CORE_PROJECT=$PROJECT_ID</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">CLOUDSDK_COMPUTE_REGION=${_REGION}</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">CLOUDSDK_CONTAINER_CLUSTER=${_CLOUDSDK_CONTAINER_CLUSTER}</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">KUBECONFIG=/workspace/.kube/config</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">args</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">cluster-info</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploying to the cluster" data-type="sect3"><div class="sect3" id="idm45979374880976">&#13;
<h3>Deploying to the cluster</h3>&#13;
&#13;
<p>Once the build is authenticated, it can run Helm to actually upgrade (or install) the application in the cluster:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">id</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">deploy</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">dir</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hello-cloudbuild-v2</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cloudnatived/helm-cloudbuilder</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">env</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">KUBECONFIG=/workspace/.kube/config</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">args</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">helm</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">upgrade</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">--create-namespace</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">--install</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">${TAG_NAME}-demo</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">--namespace=${TAG_NAME}-demo</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">--values</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">k8s/demo/${TAG_NAME}-values.yaml</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">--set</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">container.image=${_REGION}-docker.pkg.dev/$PROJECT_ID/demo/demo</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">--set</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">container.tag=${COMMIT_SHA}</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">./k8s/demo</code><code class="w"/></pre>&#13;
&#13;
<p>We are passing a few additional flags to the <code>helm upgrade</code> command:</p>&#13;
<dl>&#13;
<dt><code>namespace</code></dt>&#13;
<dd>&#13;
<p>The namespace where the application should be deployed</p>&#13;
</dd>&#13;
<dt><code>values</code></dt>&#13;
<dd>&#13;
<p>The Helm values file to use for this environment</p>&#13;
</dd>&#13;
<dt><code>set container.image</code></dt>&#13;
<dd>&#13;
<p>Sets the container name to deploy</p>&#13;
</dd>&#13;
<dt><code>set container.tag</code></dt>&#13;
<dd>&#13;
<p>Deploys the image with this specific tag (the originating Git SHA)</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating a Deploy Trigger" data-type="sect2"><div class="sect2" id="idm45979374976080">&#13;
<h2>Creating a Deploy Trigger</h2>&#13;
&#13;
<p>Now let’s create a new Cloud Build trigger for deploying to our imaginary <code>staging</code> environment.</p>&#13;
&#13;
<p id="create-staging-trigger">Create a new trigger in the Cloud Build web UI just as you did in <a data-type="xref" href="#create-build-trigger">“Creating the First Build Trigger”</a>. The repo will be the same, but this time configure it to trigger when a <em>tag</em> is pushed instead of a branch, and set the tag name to match <em>staging</em>.</p>&#13;
&#13;
<p>Also, instead of using the <em>cloudbuild.yaml</em> file, for this build we will use <code>*hello-cloudbuild-v2/cloudbuild-deploy.yaml*</code>.</p>&#13;
&#13;
<p>In the <code>Substitution variables</code> section, we’ll set some values that are specific to the deploy builds:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>_REGION</code> will be the same as you used in the first trigger. It should match the GCP availability region where you created your GKE cluster and Artifact Registry repo.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>_CLOUDSDK_CONTAINER_CLUSTER</code> is the name of your GKE cluster.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Using these variables here means that we can use the same YAML file for deploying both staging and production, even if those environments were in separate clusters, or in separate GCP projects.</p>&#13;
&#13;
<p>Once you have created the trigger for the <code>staging</code> tag, go ahead and try it out by pushing a <code>staging</code> tag to the repo:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">git tag -f staging</code></strong><code class="go">&#13;
</code><strong><code class="go">git push -f origin refs/tags/staging</code></strong><code class="go">&#13;
</code><code class="go">Total 0 (delta 0), reused 0 (delta 0)&#13;
</code><code class="go">To github.com:domingusj/demo.git&#13;
</code><code class="go"> * [new tag]         staging -&gt; staging</code></pre>&#13;
&#13;
<p>As before, you can watch the <a href="https://oreil.ly/UQmGq">build progress</a> in the Cloud Build UI. If all goes as planned, Cloud Build should successfully authenticate to your GKE cluster and deploy the staging version of your application into the <code>staging-demo</code> namespace. You can verify this by checking the <a href="https://oreil.ly/LbLjU">GKE dashboard</a>  (or use <code>helm status</code>).</p>&#13;
&#13;
<p>Finally, follow the same steps to create a separate Cloud Build trigger that deploys to production on a push to the <code>production</code> tag. If all goes well, you’ll have another copy of the app running in a new <code>production-demo</code> namespace. Again, in this example we deployed both environments to the same GKE cluster but for real applications you would likely want to keep these separate.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Adapting the Example Pipeline" data-type="sect2"><div class="sect2" id="idm45979374823552">&#13;
<h2>Adapting the Example Pipeline</h2>&#13;
&#13;
<p>When you are done trying out the demo pipeline you will want to delete any of the GCP resources you created for testing, including the GKE cluster, the <code>demo</code> Artifact Registry repository, and your Cloud Build triggers.</p>&#13;
&#13;
<p>We hope this example demonstrates the key concepts of a CI/CD pipeline. If you’re using Cloud Build, you can use these examples as a starting point for setting up your own pipelines. If you’re using other tools, we hope you can easily adapt the patterns we’ve shown here to work in your own environment. Automating the build, test, and deployment steps for your applications will greatly improve the experience of creating and deploying software for everyone involved.<a data-startref="ix_14-continuous-adoc4" data-type="indexterm" id="idm45979374716704"/><a data-startref="ix_14-continuous-adoc3" data-type="indexterm" id="idm45979374716000"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GitOps" data-type="sect1"><div class="sect1" id="gitops">&#13;
<h1>GitOps</h1>&#13;
&#13;
<p><a data-primary="GitOps" data-type="indexterm" id="idm45979374693392"/>As we mentioned in <a data-type="xref" href="ch01.html#infra-as-code">“Infrastructure as Code”</a>, an integral part of the industry’s shift toward DevOps was the need to manage infrastructure by means of code and source control. “GitOps” is a newer term that seems to mean something slightly different depending on who you ask. But a high-level GitOps involves using source control (Git being one of the more popular source control tools) to track and manage infrastructure in an automated way. Imagine the Kubernetes reconciliation loop but applied in a broader sense, where any and all infrastructure is configured and deployed solely by pushing changes to a Git repo. A number of existing CI/CD tools have rebranded themselves as being “GitOps” tools and we expect this concept to grow and evolve rapidly throughout the software industry in the coming years.</p>&#13;
&#13;
<p>In this section, we will use a tool called Flux to automatically deploy the demo application to a local Kubernetes cluster by pushing changes to a GitHub repository.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Flux" data-type="sect2"><div class="sect2" id="idm45979374691184">&#13;
<h2>Flux</h2>&#13;
&#13;
<p><a data-primary="Flux" data-type="indexterm" id="ix_14-continuous-adoc5"/>Weaveworks (the creators of <a href="https://eksctl.io"><code>eksctl</code></a>) may have been the first to coin the <a href="https://oreil.ly/Bjd3D">GitOps term</a>. They have also built one of the more popular GitOps tools called <a href="https://oreil.ly/Eoqs6">Flux</a>. It can be used to automatically deploy changes to a Kubernetes cluster by polling a Git repo, watching for any changes, and automatically applying the changes from the Flux Pods running inside of the cluster. Let’s try out an example to see how it works.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Set Up Flux" data-type="sect3"><div class="sect3" id="idm45979374685920">&#13;
<h3>Set Up Flux</h3>&#13;
&#13;
<p><a data-primary="Flux" data-secondary="setting up" data-type="indexterm" id="idm45979374684368"/><code>flux</code> is the CLI tool used for interacting with Flux and can also be used to install the Flux components in Kubernetes. Follow the <a href="https://oreil.ly/erpNZ">installation instructions</a> for your operating system and point your <code>kubectl</code> at your test Kubernetes cluster.</p>&#13;
&#13;
<p>You will need to create a GitHub personal access token so that Flux can talk securely to GitHub. You can generate one in the Settings page of your GitHub profile under the Developer Settings section. It will need the <code>repo</code> permission and you should decide if you want to have it automatically expire on a schedule, or set it to never expire. For this example either is fine, but in a real production system you should always have a process to rotate any credentials on a regular basis.</p>&#13;
&#13;
<p>Follow the <a href="https://oreil.ly/amjIN">GitHub instructions</a> to generate a personal access token, export it into your environment along with your username, and check to see if Flux is ready to install using the <code>flux check</code> command:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong>export GITHUB_TOKEN=<em>YOUR_PERSONAL_ACCESS_TOKEN</em></strong>&#13;
<strong>export GITHUB_USER=<em>YOUR_GITHUB_USERNAME</em></strong>&#13;
<strong>flux check --pre</strong>&#13;
checking prerequisites&#13;
Kubernetes 1.21.4 &gt;=1.19.0-0&#13;
prerequisites checks passed&#13;
...</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Install Flux" data-type="sect3"><div class="sect3" id="idm45979374675760">&#13;
<h3>Install Flux</h3>&#13;
&#13;
<p><a data-primary="Flux" data-secondary="installing" data-type="indexterm" id="idm45979374674560"/>Assuming the check passes, you are ready to install Flux! As part of the process it will use your personal access token to automatically create a new GitHub repo in your account and then use that repo for managing your cluster going forward:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">flux bootstrap github \</code></strong><code class="go">&#13;
</code><code class="go">  </code><strong><code class="go">--owner=$GITHUB_USER \</code></strong><code class="go">&#13;
</code><code class="go">  </code><strong><code class="go">--repository=flux-demo \</code></strong><code class="go">&#13;
</code><code class="go">  </code><strong><code class="go">--branch=main \</code></strong><code class="go">&#13;
</code><code class="go">  </code><strong><code class="go">--path=./clusters/demo-cluster \</code></strong><code class="go">&#13;
</code><code class="go">  </code><strong><code class="go">--personal</code></strong><code class="go">&#13;
</code><code class="go">connecting to github.com&#13;
</code><code class="go">...&#13;
</code><code class="go">cloned repository&#13;
</code><code class="go">...&#13;
</code><code class="go">all components are healthy</code></pre>&#13;
&#13;
<p>Among other things, you should see that Flux successfully connected to your repo. You can browse to your new repo and see that inside of this repo Flux created a directory called <em>clusters/demo-cluster/flux-system</em> containing all of the Flux Kubernetes manifests that are now running in your cluster in the new <code>flux-system</code> namespace.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Create a New Deployment Using Flux" data-type="sect3"><div class="sect3" id="idm45979374651024">&#13;
<h3>Create a New Deployment Using Flux</h3>&#13;
&#13;
<p><a data-primary="Flux" data-secondary="creating new deployment with" data-type="indexterm" id="idm45979374649536"/>Now let’s use Flux to automatically deploy a new namespace and deployment to your cluster. In proper GitOps fashion we will do this only by pushing changes to the Git repo. You’ll need to clone the new repo that Flux created, which means you will need to set up your credentials with GitHub in order to push new commits. If you have not done this yet, you can follow <a href="https://oreil.ly/UVcoA">these instructions from GitHub</a>. Once you have your repo cloned, make a new directory alongside <code>flux-system</code> for our new <code>flux-demo</code> Deployment:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">git clone git@github.com:_$GITHUB_USER_/flux-demo.git</code></strong><code class="go">&#13;
</code><strong><code class="go">cd flux-demo</code></strong><code class="go">&#13;
</code><strong><code class="go">mkdir clusters/demo-cluster/flux-demo</code></strong><code class="go">&#13;
</code><strong><code class="go">cd clusters/demo-cluster/flux-demo</code></strong></pre>&#13;
&#13;
<p>Next we will generate the YAML needed for a new Namespace and Deployment called <code>flux-demo</code> using <code>kubectl</code> and the <code>--dry-run</code> flag. After saving those to new manifest files, we will commit and push them to the repo:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl create namespace flux-demo -o yaml --dry-run=client &gt; namespace.yaml</code></strong><code class="go">&#13;
</code><strong><code class="go">kubectl create deployment flux-demo -n flux-demo \</code></strong><code class="go">&#13;
</code><strong><code class="go">-o yaml --dry-run=client --image=cloudnatived/demo:hello &gt; deployment.yaml</code></strong><code class="go">&#13;
</code><strong><code class="go">git add namespace.yaml deployment.yaml</code></strong><code class="go">&#13;
</code><strong><code class="go">git commit -m "Create flux-demo Deployment"</code></strong><code class="go">&#13;
</code><strong><code class="go">git push</code></strong></pre>&#13;
&#13;
<p>Since Flux is regularly polling the Git repo and watching for any changes, it will automatically create and deploy your new <code>flux-demo</code> manifests when it detects the new files:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods -n flux-demo</code></strong><code class="go">&#13;
</code><code class="go">NAME                        READY   STATUS    RESTARTS   AGE&#13;
</code><code class="go">flux-demo-c77cc8d6f-zfzql   1/1     Running   0          39s</code></pre>&#13;
&#13;
<p>In addition to plain Kubernetes manifests, Flux can manage Helm releases, and manifests that use kustomize. You can also configure Flux to poll your container registry and automatically deploy new images. It will then make a new Git commit back to the repo, tracking the image version that it deployed, keeping your Git repo in sync with what is actually running in the cluster.</p>&#13;
&#13;
<p>Also, just like the Kubernetes reconciliation loop, Flux continuously monitors for any manual changes to the resources that it manages. It will attempt to keep the cluster in sync with what is in the Git repo. This way any manual changes to anything managed with Flux should automatically be rolled back, allowing you to better trust your Git repo as the ultimate source, or truth, for what should be running in your cluster.</p>&#13;
&#13;
<p>This is one of the main goals of GitOps: that you should be able to manage Kubernetes automatically using code that is tracked in Git. Pushing changes to this repo is the only way you should ever make changes to your clusters when using Flux. Using Git to manage your infrastructure means that you will have a record of all changes in your commit history, and also that all changes can be peer-reviewed as part of your team’s merge-request process.<a data-startref="ix_14-continuous-adoc5" data-type="indexterm" id="idm45979374551296"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45979374650432">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Setting up a continuous deployment pipeline for your applications allows you to deploy software consistently, reliably, and quickly. Ideally, developers should be able to push code to the source control repository and all of the build, test, and deploy phases happen automatically in a centralized pipeline.</p>&#13;
&#13;
<p>Because there are so many options for CI/CD software and techniques, we can’t give you a single recipe that’ll work for everybody. Instead, we’ve aimed to show you how and why CD is beneficial, and give you a few important things to think about when you come to implement it in your own organization:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Deciding which CI/CD tools to use is an important process when building a new pipeline. All of the tools we mention throughout this book could likely be incorporated into almost any existing environment.</p>&#13;
</li>&#13;
<li>&#13;
<p>Jenkins, GitHub Actions, GitLab, Drone, Cloud Build, and Spinnaker are just a few of the popular CI/CD tools that work well with Kubernetes.</p>&#13;
</li>&#13;
<li>&#13;
<p>Defining the build pipeline steps with code allows you to track and modify these steps alongside application code.</p>&#13;
</li>&#13;
<li>&#13;
<p>Containers enable developers to promote build artifacts up through environments, such as testing, staging, and eventually production, ideally without having to rebuild a new container.</p>&#13;
</li>&#13;
<li>&#13;
<p>Our example pipeline using Cloud Build should be easily adaptable for other tools and types of applications. The overall build, test, and deploy steps are largely the same in any CI/CD pipeline, regardless of the tools used or type of software.</p>&#13;
</li>&#13;
<li>&#13;
<p>GitOps is a newer term used when talking about CI/CD pipelines. The main idea is that deployments and infrastructure changes should be managed using code that is tracked in source control (Git).</p>&#13;
</li>&#13;
<li>&#13;
<p>Flux and Argo have popular GitOps tools that can automatically apply changes to your clusters whenever you push code changes to a Git repo.<a data-startref="ix_14-continuous-adoc1" data-type="indexterm" id="idm45979374525664"/><a data-startref="ix_14-continuous-adoc0" data-type="indexterm" id="idm45979374524960"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>