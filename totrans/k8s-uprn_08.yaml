- en: Chapter 8\. HTTP Load Balancing with Ingress
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章：使用 Ingress 进行 HTTP 负载均衡
- en: A critical part of any application is getting network traffic to and from that
    application. As described in [Chapter 7](ch07.xhtml#service_discovery), Kubernetes
    has a set of capabilities to enable services to be exposed outside of the cluster.
    For many users and simple use cases, these capabilities are sufficient.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 任何应用程序的关键部分是将网络流量发送和接收到该应用程序。正如在[第 7 章](ch07.xhtml#service_discovery)中描述的那样，Kubernetes
    具有一套能力，使得服务能够在集群外部暴露。对于许多用户和简单的用例，这些能力是足够的。
- en: 'But the Service object operates at Layer 4 (according to the OSI model).^([1](ch08.xhtml#idm45664078055792))
    This means that it only forwards TCP and UDP connections and doesn’t look inside
    of those connections. Because of this, hosting many applications on a cluster
    uses many different exposed services. In the case where these services are `type:
    NodePort`, you’ll have to have clients connect to a unique port per service. In
    the case where these services are `type: LoadBalancer`, you’ll be allocating (often
    expensive or scarce) cloud resources for each service. But for HTTP (Layer 7)-based
    services, we can do better.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '但是 Service 对象在 OSI 模型中操作于第 4 层。^([1](ch08.xhtml#idm45664078055792)) 这意味着它仅转发
    TCP 和 UDP 连接，并且不检查这些连接的内部内容。因此，在集群上托管多个应用程序时，会使用许多不同的暴露服务。对于那些`type: NodePort`的服务，您将不得不为每个服务连接到唯一端口。而对于那些`type:
    LoadBalancer`的服务，您将为每个服务分配（通常昂贵或稀缺的）云资源。但是对于基于 HTTP（第 7 层）的服务，我们可以做得更好。'
- en: When solving a similar problem in non-Kubernetes situations, users often turn
    to the idea of “virtual hosting.” This is a mechanism to host many HTTP sites
    on a single IP address. Typically, the user uses a load balancer or reverse proxy
    to accept incoming connections on HTTP (80) and HTTPS (443) ports. That program
    then parses the HTTP connection and, based on the `Host` header and the URL path
    that is requested, proxies the HTTP call to some other program. In this way, that
    load balancer or reverse proxy directs traffic for decoding and directing incoming
    connections to the right “upstream” server.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当在非 Kubernetes 环境中解决类似问题时，用户通常会转向“虚拟主机”这一概念。这是一种在单个 IP 地址上托管多个 HTTP 站点的机制。通常，用户使用负载均衡器或反向代理接受
    HTTP（80）和 HTTPS（443）端口的传入连接。该程序然后解析 HTTP 连接，并基于`Host`头和请求的 URL 路径，将 HTTP 调用代理到其他程序。通过这种方式，负载均衡器或反向代理将流量引导到正确的“上游”服务器进行解码和连接引导。
- en: Kubernetes calls its HTTP-based load-balancing system *Ingress*. Ingress is
    a Kubernetes-native way to implement the “virtual hosting” pattern we just discussed.
    One of the more complex aspects of the pattern is that the user has to manage
    the load balancer configuration file. In a dynamic environment and as the set
    of virtual hosts expands, this can be very complex. The Kubernetes Ingress system
    works to simplify this by (a) standardizing that configuration, (b) moving it
    to a standard Kubernetes object, and (c) merging multiple Ingress objects into
    a single config for the load balancer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 将其基于 HTTP 的负载均衡系统称为*Ingress*。Ingress 是 Kubernetes 本地的一种实现“虚拟主机”模式的方式，正如我们刚刚讨论的那样。该模式的更复杂部分之一是用户必须管理负载均衡器配置文件。在动态环境中，并且随着虚拟主机集合的扩展，这可能会非常复杂。Kubernetes
    Ingress 系统通过（a）标准化该配置，（b）将其移到标准 Kubernetes 对象，并且（c）将多个 Ingress 对象合并为负载均衡器的单一配置，来简化这一过程。
- en: 'The typical software base implementation looks something like what is depicted
    in [Figure 8-1](#figingressflow). The Ingress controller is a software system
    made up of two parts. The first is the Ingress proxy, which is exposed outside
    the cluster using a service of `type: LoadBalancer`. This proxy sends requests
    to “upstream” servers. The other component is the Ingress reconciler, or operator.
    The Ingress operator is responsible for reading and monitoring Ingress objects
    in the Kubernetes API and reconfiguring the Ingress proxy to route traffic as
    specified in the Ingress resource. There are many different Ingress implementations.
    In some, these two components are combined in a single container; in others, they
    are distinct components that are deployed separately in the Kubernetes cluster.
    In [Figure 8-1](#figingressflow), we introduce one example of an Ingress controller.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/kur3_0801.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. The typical software Ingress controller configuration
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ingress Spec Versus Ingress Controllers
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While conceptually simple, at an implementation level, Ingress is very different
    from pretty much every other regular resource object in Kubernetes. Specifically,
    it is split into a common resource specification and a controller implementation.
    There is no “standard” Ingress controller that is built into Kubernetes, so the
    user must install one of many optional implementations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Users can create and modify Ingress objects just like every other object. But,
    by default, there is no code running to actually act on those objects. It is up
    to the users (or the distribution they are using) to install and manage an outside
    controller. In this way, the controller is pluggable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of reasons that Ingress ended up like this. First of all,
    there is no one single HTTP load balancer that can be used universally. In addition
    to many software load balancers (both open source and proprietary), there are
    also load-balancing capabilities provided by cloud providers (e.g., ELB on AWS),
    and hardware-based load balancers. The second reason is that the Ingress object
    was added to Kubernetes before any of the common extensibility capabilities were
    added (see [Chapter 17](ch17.xhtml#extending_kubernetes)). As Ingress progresses,
    it is likely that it will evolve to use these mechanisms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Installing Contour
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there are many available Ingress controllers, for the examples here, we
    use an Ingress controller called Contour. This is a controller built to configure
    the open source (and CNCF project) load balancer called Envoy. Envoy is built
    to be dynamically configured via an API. The Contour Ingress controller takes
    care of translating the Ingress objects into something that Envoy can understand.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The [Contour project](https://oreil.ly/5IHmq) was created by Heptio in collaboration
    with real-world customers and is used in production settings but is now an independent
    open source project.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install Contour with a simple one-line invocation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that this requires execution by a user who has `cluster-admin` permissions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'This one line works for most configurations. It creates a namespace called
    `projectcontour`. Inside of that namespace it creates a deployment (with two replicas)
    and an external-facing service of `type: LoadBalancer`. In addition, it sets up
    the correct permissions via a service account and installs a CustomResourceDefinition
    (see [Chapter 17](ch17.xhtml#extending_kubernetes)) for some extended capabilities
    discussed in [“The Future of Ingress”](#future_of_ingress).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Because it is a global install, you need to ensure that you have wide admin
    permissions on the cluster you are installing into. After you install it, you
    can fetch the external address of Contour via:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Look at the `EXTERNAL-IP` column. This can be either an IP address (for GCP
    and Azure) or a hostname (for AWS). Other clouds and environments may differ.
    If your Kubernetes cluster doesn’t support services of `type: LoadBalancer`, you’ll
    have to change the YAML for installing Contour to use `type: NodePort` and route
    traffic to machines on the cluster via a mechanism that works in your configuration.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using `minikube`, you probably won’t have anything listed for `EXTERNAL-IP`.
    To fix this, you need to open a separate terminal window and run `minikube tunnel`.
    This configures networking routes such that you have unique IP addresses assigned
    to every service of `type: LoadBalancer`.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Configuring DNS
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make Ingress work well, you need to configure DNS entries to the external
    address for your load balancer. You can map multiple hostnames to a single external
    endpoint and the Ingress controller will direct incoming requests to the appropriate
    upstream service based on that hostname.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, we assume that you have a domain called `example.com`. You
    need to configure two DNS entries: `alpaca.example.com` and `bandicoot.example.com`.
    If you have an IP address for your external load balancer, you’ll want to create
    A records. If you have a hostname, you’ll want to configure CNAME records.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The [ExternalDNS project](https://oreil.ly/ILdEj) is a cluster add-on that you
    can use to manage DNS records for you. ExternalDNS monitors your Kubernetes cluster
    and synchronizes IP addresses for Kubernetes Service resources with an external
    DNS provider. ExternalDNS supports a wide variety of DNS providers including traditional
    domain registrars as well as public cloud providers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a Local hosts File
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you don’t have a domain or if you are using a local solution such as `minikube`,
    you can set up a local configuration by editing your */etc/hosts* file to add
    an IP address. You need admin/root privileges on your workstation. The location
    of the file may differ on your platform, and making it take effect may require
    extra steps. For example, on Windows the file is usually at *C:\Windows\System32\drivers\etc\hosts*,
    and for recent versions of macOS, you need to run `sudo killall -HUP mDNSResponder`
    after changing the file.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the file to add a line like the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For `<*ip-address*>`, fill in the external IP address for Contour. If all you
    have is a hostname (like from AWS), you can get an IP address (that may change
    in the future) by executing `host -t a *<address>*`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget to undo these changes when you are done!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Using Ingress
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have an Ingress controller configured, let’s put it through its
    paces. First, we’ll create a few upstream (also sometimes referred to as “backend”)
    services to play with by executing the following commands:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Simplest Usage
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest way to use Ingress is to have it just blindly pass everything that
    it sees through to an upstream service. There is limited support for imperative
    commands to work with Ingress in `kubectl`, so we’ll start with a YAML file (see
    [Example 8-1](#simple-ingress-ex)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-1\. simple-ingress.yaml
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create this Ingress with `kubectl apply`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can verify that it was set up correctly using `kubectl get` and `kubectl
    describe`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This sets things up so that *any* HTTP request that hits the Ingress controller
    is forwarded on to the `alpaca` service. You can now access the `alpaca` instance
    of `kuard` on any of the raw IPs/CNAMEs of the service; in this case, either `alpaca.example.com`
    or `bandicoot.example.com`. This doesn’t, at this point, add much value over a
    simple service of `type: LoadBalancer`. The following sections experiment with
    more complex configurations.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Using Hostnames
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Things start to get interesting when we direct traffic based on properties of
    the request. The most common example of this is to have the Ingress system look
    at the HTTP host header (which is set to the DNS domain in the original URL) and
    direct traffic based on that header. Let’s add *another* Ingress object for directing
    traffic to the `alpaca` service for any traffic directed to `alpaca.example.com`
    (see [Example 8-2](#host-ingress)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-2\. host-ingress.yaml
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create this Ingress with `kubectl apply`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can verify that things are set up correctly as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are a couple of confusing things here. First, there is a reference to
    the `default-http-backend`. This is a convention that only some Ingress controllers
    use to handle requests that aren’t handled in any other way. These controllers
    send those requests to a service called `default-http-backend` in the `kube-system`
    namespace. This convention is surfaced client-side in `kubectl`. Next, there are
    no endpoints listed for the `alpaca` backend service. This is a bug in `kubectl`
    that is fixed in Kubernetes v1.14.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, you should now be able to address the `alpaca` service via *http://alpaca.example.com*.
    If instead you reach the service endpoint via other methods, you should get the
    default service.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Using Paths
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next interesting scenario is to direct traffic based on not just the hostname,
    but also the path in the HTTP request. We can do this easily by specifying a path
    in the `paths` entry (see [Example 8-3](#path-ingress)). In this example, we direct
    everything coming into *http://bandicoot.example.com* to the `bandicoot` service,
    but we also send *http://bandicoot.example.com/a* to the `alpaca` service. This
    type of scenario can be used to host multiple services on different paths of a
    single domain.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-3\. path-ingress.yaml
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When there are multiple paths on the same host listed in the Ingress system,
    the longest prefix matches. So, in this example, traffic starting with `/a/` is
    forwarded to the `alpaca` service, while all other traffic (starting with `/`)
    is directed to the `bandicoot` service.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: As requests get proxied to the upstream service, the path remains unmodified.
    That means a request to `bandicoot.example.com/a/` shows up to the upstream server
    that is configured for that request hostname and path. The upstream service needs
    to be ready to serve traffic on that subpath. In this case, `kuard` has special
    code for testing, where it responds on the root path (`/`) along with a predefined
    set of subpaths (`/a/`, `/b/`, and `/c/`).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Cleanup
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To clean up, execute the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Advanced Ingress Topics and Gotchas
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ingress supports some other fancy features. The level of support for these features
    differs based on the Ingress controller implementation, and two controllers may
    implement a feature in slightly different ways.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Many of the extended features are exposed via annotations on the Ingress object.
    Be careful; these annotations can be hard to validate and are easy to get wrong.
    Many of these annotations apply to the entire Ingress object and so can be more
    general than you might like. To scope the annotations down, you can always split
    a single Ingress object into multiple Ingress objects. The Ingress controller
    should read them and merge them together.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Running Multiple Ingress Controllers
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are multiple Ingress controller implementations, and you may want to run
    multiple Ingress controllers on a single cluster. To solve this case, the IngressClass
    resource exists so that an Ingress resource can request a particular implementation.
    When you create an Ingress resource, you use the `spec.ingressClassName` field
    to specify the specific Ingress resource.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Kubernetes prior to version 1.18, the `IngressClassName` field did not exist
    and the `kubernetes.io/ingress.class` annotation was used instead. While this
    is still supported by many controllers, it is recommended that people move away
    from the annotation as it will likely be deprecated by controllers in the future.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: If the `spec.ingressClassName` annotation is missing, a default Ingress controller
    is used. It is specified by adding the `ingressclass.kubernetes.io/is-default-class`
    annotation to the correct IngressClass resource.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Ingress Objects
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you specify multiple Ingress objects, the Ingress controllers should read
    them all and try to merge them into a coherent configuration. However, if you
    specify duplicate and conflicting configurations, the behavior is undefined. It
    is likely that different Ingress controllers will behave differently. Even a single
    implementation may do different things depending on nonobvious factors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Ingress and Namespaces
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ingress interacts with namespaces in some nonobvious ways. First, due to an
    abundance of security caution, an Ingress object can refer to only an upstream
    service in the same namespace. This means that you can’t use an Ingress object
    to point a subpath to a service in another namespace.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: However, multiple Ingress objects in different namespaces can specify subpaths
    for the same host. These Ingress objects are then merged to come up with the final
    config for the Ingress controller.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: This cross-namespace behavior means that coordinating Ingress globally across
    the cluster is necessary. If not coordinated carefully, an Ingress object in one
    namespace could cause problems (and undefined behavior) in other namespaces.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Typically there are no restrictions built into the Ingress controller around
    which namespaces are allowed to specify which hostnames and paths. Advanced users
    may try to enforce a policy for this using a custom admission controller. There
    are also evolutions of Ingress described in [“The Future of Ingress”](#future_of_ingress)
    that address this problem.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Path Rewriting
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some Ingress controller implementations support, optionally, doing path rewriting.
    This can be used to modify the path in the HTTP request as it gets proxied. This
    is usually specified by an annotation on the Ingress object and applies to all
    requests that are specified by that object. For example, if we were using the
    NGINX Ingress controller, we could specify an annotation of `nginx.ingress​.kuber⁠netes.io/rewrite-target:
    /`. This can sometimes make upstream services work on a subpath even if they weren’t
    built to do so.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple implementations that not only implement path rewriting, but
    also support regular expressions when specifying the path. For example, the NGINX
    controller allows regular expressions to capture parts of the path and then use
    that captured content when doing rewriting. How this is done (and what variant
    of regular expressions is used) is implementation-specific.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Path rewriting isn’t a silver bullet, though, and can often lead to bugs. Many
    web applications assume that they can link within themselves using absolute paths.
    In that case, the app in question may be hosted on `/subpath` but have requests
    show up to it on `/`. It may then send a user to `/app-path`. There is then the
    question of whether that is an “internal” link for the app (in which case it should
    instead be `/subpath/app-path`) or a link to some other app. For this reason,
    it is probably best to avoid subpaths for any complicated applications if you
    can help it.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Serving TLS
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When serving websites, it is becoming increasingly necessary to do so securely
    using TLS and HTTPS. Ingress supports this (as do most Ingress controllers).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: First, users need to specify a Secret with their TLS certificate and keys⁠—something
    like what is outlined in [Example 8-4](#tls-secret). You can also create a Secret
    imperatively with `kubectl create secret tls <*secret-name*> --cert <*certificate-pem-file*>
    --key <*private-key-pem-file*>`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-4\. tls-secret.yaml
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once you have the certificate uploaded, you can reference it in an Ingress object.
    This specifies a list of certificates along with the hostnames that those certificates
    should be used for (see [Example 8-5](#tls-ingress)). Again, if multiple Ingress
    objects specify certificates for the same hostname, the behavior is undefined.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-5\. tls-ingress.yaml
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Uploading and managing TLS secrets can be difficult. In addition, certificates
    can often come at a significant cost. To help solve this problem, there is a nonprofit
    called [“Let’s Encrypt”](https://letsencrypt.org) running a free Certificate Authority
    that is API-driven. Since it is API-driven, it is possible to set up a Kubernetes
    cluster that automatically fetches and installs TLS certificates for you. It can
    be tricky to set up, but when working, it’s very simple to use. The missing piece
    is an open source project called [cert-manager](https://cert-manager.io) created
    by Jetstack, a UK startup, onboarded to the CNCF. The *cert-manager.io* website
    or [GitHub repository](https://oreil.ly/S0PU4) has details on how to install cert-manager
    and get started.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Alternate Ingress Implementations
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many different implementations of Ingress controllers, each building
    on the base Ingress object with unique features. It is a vibrant ecosystem.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: First, each cloud provider has an Ingress implementation that exposes the specific
    cloud-based L7 load balancer for that cloud. Instead of configuring a software
    load balancer running in a Pod, these controllers take Ingress objects and use
    them to configure, via an API, the cloud-based load balancers. This reduces the
    load on the cluster and the management burden for the operators, but can often
    come at a cost.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，每个云提供商都有一个 Ingress 实现，用于暴露特定的基于云的 L7 负载均衡器。与配置在 Pod 中运行的软件负载均衡器不同，这些控制器接收
    Ingress 对象，并通过 API 配置基于云的负载均衡器。这减少了集群的负载和操作员的管理负担，但通常会带来一定的成本。
- en: The most popular generic Ingress controller is probably the open source [NGINX
    Ingress controller](https://oreil.ly/EstHX). Be aware that there is also a commercial
    controller based on the proprietary NGINX Plus. The open source controller essentially
    reads Ingress objects and merges them into an NGINX configuration file. It then
    signals to the NGINX process to restart with the new configuration (while responsibly
    serving existing in-flight connections). The open source NGINX controller has
    an enormous number of features and options exposed via [annotations](https://oreil.ly/V8nM7).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的通用 Ingress 控制器可能是开源的 [NGINX Ingress 控制器](https://oreil.ly/EstHX)。请注意，还有一个基于专有
    NGINX Plus 的商业控制器。开源控制器基本上会读取 Ingress 对象，并将它们合并到一个 NGINX 配置文件中。然后，它向 NGINX 进程发出信号，以使用新配置重新启动（同时负责处理正在进行中的连接）。开源
    NGINX 控制器具有大量通过 [annotations](https://oreil.ly/V8nM7) 公开的功能和选项。
- en: '[Emissary](https://oreil.ly/5HDun) and [Gloo](https://oreil.ly/rZDlX) are two
    other Envoy-based Ingress controllers that are focused on being API gateways.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[Emissary](https://oreil.ly/5HDun) 和 [Gloo](https://oreil.ly/rZDlX) 是另外两个基于
    Envoy 的 Ingress 控制器，专注于成为 API 网关。'
- en: '[Traefik](https://traefik.io) is a reverse proxy implemented in Go that also
    can function as an Ingress controller. It has a set of features and dashboards
    that are very developer-friendly.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[Traefik](https://traefik.io) 是一个用 Go 实现的反向代理，也可以作为 Ingress 控制器运行。它具有一系列非常适合开发者的功能和仪表板。'
- en: This just scratches the surface. The Ingress ecosystem is very active, and there
    are many new projects and commercial offerings that build on the humble Ingress
    object in unique ways.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是冰山一角。Ingress 生态系统非常活跃，有许多新项目和商业产品以独特的方式构建在谦逊的 Ingress 对象之上。
- en: The Future of Ingress
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress 的未来
- en: As you have seen, the Ingress object provides a very useful abstraction for
    configuring L7 load balancers⁠—but it hasn’t scaled to all the features that users
    want and various implementations are looking to offer. Many of the features in
    Ingress are underdefined. Implementations can surface these features in different
    ways, reducing the portability of configurations between implementations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，Ingress 对象提供了一个非常有用的抽象来配置 L7 负载均衡器，但它还没有扩展到用户需要的所有功能，并且各种实现正在寻求提供。许多
    Ingress 中的功能定义不清晰。实现可以以不同的方式展示这些功能，从而降低配置在不同实现之间的可移植性。
- en: Another problem is that it is easy to misconfigure Ingress. The way that multiple
    objects compbine opens the door for conflicts that are resolved differently by
    different implementations. In addition, the way that these are merged across namespaces
    breaks the idea of namespace isolation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，容易配置错误的 Ingress。多个对象的组合方式为不同实现提供了解决冲突的机会。此外，这些对象在命名空间之间的合并方式破坏了命名空间隔离的理念。
- en: Ingress was also created before the idea of a service mesh (exemplified by projects
    such as Istio and Linkerd) was well known. The intersection of Ingress and service
    meshes is still being defined. Service meshes are covered in greater detail in
    [Chapter 15](ch15.xhtml#service_mesh).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 的创建是在服务网格的概念（例如 Istio 和 Linkerd 这样的项目）被广泛认知之前。Ingress 和服务网格的交集仍在定义中。服务网格在[第
    15 章](ch15.xhtml#service_mesh)中有更详细的介绍。
- en: The future of HTTP load balancing for Kubernetes looks to be the *Gateway* API,
    which is in the midst of development by the Kubernetes special interest group
    (SIG) dedicated to networking. The Gateway API project is intended to develop
    a more modern API for routing in Kubernetes. Though it is more focused on HTTP
    balancing, Gateway also includes resources for controlling Layer 4 (TCP) balancing.
    The Gateway APIs are still very much under development, so it is strongly recommended
    that people stick to the existing Ingress and Service resources that are currently
    present in Kubernetes. The current state of the [Gateway API can be found online](https://oreil.ly/zhlil).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的 HTTP 负载均衡未来看起来将是 *Gateway* API，这是由专注于网络的 Kubernetes 特别兴趣小组（SIG）正在开发中。Gateway
    API 项目旨在为 Kubernetes 中的路由开发一个更现代化的 API。虽然它更专注于 HTTP 负载均衡，Gateway 也包括用于控制第四层（TCP）负载均衡的资源。Gateway
    API 目前仍在积极开发中，因此强烈建议大家继续使用目前在 Kubernetes 中存在的 Ingress 和 Service 资源。关于 [Gateway
    API 的当前状态可以在网上找到](https://oreil.ly/zhlil)。
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Ingress is a unique system in Kubernetes. It is simply a schema, and the implementations
    of a controller for that schema must be installed and managed separately. But
    it is also a critical system for exposing services to users in a practical and
    cost-efficient way. As Kubernetes continues to mature, expect to see Ingress become
    more and more relevant.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 是 Kubernetes 中独特的系统。它只是一个模式，该模式的控制器实现必须单独安装和管理。但它也是一种将服务以实用和经济高效的方式暴露给用户的关键系统。随着
    Kubernetes 的不断成熟，预计 Ingress 将变得越来越重要。
- en: ^([1](ch08.xhtml#idm45664078055792-marker)) The [Open Systems Interconnection
    (OSI) model](https://oreil.ly/czfCd) is a standard way to describe how different
    networking layers build on each other. TCP and UDP are considered to be Layer
    4, while HTTP is Layer 7.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.xhtml#idm45664078055792-marker)) [开放系统互联模型（OSI 模型）](https://oreil.ly/czfCd)
    是描述不同网络层如何构建在一起的标准方式。TCP 和 UDP 被认为是第四层，而 HTTP 是第七层。
