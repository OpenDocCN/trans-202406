<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Container Runtime Isolation"><div class="chapter" id="ch-container-runtime-isolation">
<h1><span class="label">Chapter 3. </span>Container Runtime Isolation</h1>


<p>Linux has evolved sandboxing and isolation techniques beyond simple virtual machines
(VMs) that strengthen it from current and future vulnerabilities.
Sometimes these sandboxes are called <em>micro VMs</em>.</p>

<p>These sandboxes <a data-type="indexterm" data-primary="sandboxes" data-seealso="virtualization" id="idm45302816697552"/>combine parts of all previous container and VM approaches.
You would use them to protect sensitive workloads and data, as they focus on
rapid deployment and high performance on shared infrastructure.</p>

<p>In this chapter we’ll discuss different types of micro VMs that use virtual machines and containers together, to protect your running Linux kernel and userspace. The generic term <em>sandboxing</em> is used to cover the entire spectrum: each tool in
this chapter combines software and hardware <a data-type="indexterm" data-primary="virtualization" data-seealso="sandboxes" id="idm45302816695104"/>virtualization of technologies and uses Linux’s Kernel Virtual Machine (KVM), <a data-type="indexterm" data-primary="Kernel Virtual Machine (KVM)" id="idm45302816694000"/><a data-type="indexterm" data-primary="KVM (Kernel Virtual Machine)" id="idm45302816693312"/>which is widely used to power VMs in
public cloud services, including Amazon Web Services and Google Cloud.</p>

<p>You run a lot of workloads at BCTL, and you should remember that while these
techniques may also protect against Kubernetes mistakes, all of your web-facing
software and infrastructure is a more obvious place to defend first.
Zero-days and container breakouts are rare in comparison to simple security-sensitive misconfigurations.</p>

<p>Hardened runtimes are newer, and have fewer generally less dangerous CVEs than
the kernel or more established container runtimes, so we’ll focus less on historical
breakouts and more on the history of micro VM design and rationale.</p>






<section data-type="sect1" data-pdf-bookmark="Defaults"><div class="sect1" id="idm45302816690832">
<h1>Defaults</h1>

<p><code>kubeadm</code> installs <a data-type="indexterm" data-primary="kubeadm" data-secondary="runc" id="idm45302816688736"/><a data-type="indexterm" data-primary="runc" data-secondary="management" id="idm45302816687760"/>Kubernetes with <code>runc</code> as its <a data-type="indexterm" data-primary="container runtimes" data-secondary="runc" id="contrun_runc"/>container runtime, using <code>cri-o</code> or 
<span class="keep-together"><code>containerd</code></span> to manage it. The
old <code>dockershim</code> way<a data-type="indexterm" data-primary="dockershim" id="idm45302816683056"/> of running <code>runc</code> was removed in Kubernetes v1.20, so although Kubernetes doesn’t use Docker
any more, the <code>runc</code> container runtime that Docker is built on continues to run containers for us. <a data-type="xref" href="#runtime-sandboxing-k8s-cris">Figure 3-1</a> shows three ways Kubernetes can consume the <code>runc</code> container runtime: CRI-O, <code>containerd</code>, and Docker.</p>

<figure><div id="runtime-sandboxing-k8s-cris" class="figure">
<img src="Images/haku_0301.png" alt="haku 0301" width="734" height="697"/>
<h6><span class="label">Figure 3-1. </span>Kubernetes container runtime interfaces</h6>
</div></figure>

<p>We’ll<a data-type="indexterm" data-primary="interfaces, container runtimes" id="idm45302816677296"/> get into container runtimes in a lot of detail later on in this<a data-type="indexterm" data-primary="container runtimes" data-secondary="runc" data-startref="contrun_runc" id="idm45302816676368"/> chapter.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Threat Model"><div class="sect1" id="idm45302816674768">
<h1>Threat Model</h1>

<p>You have two main reasons for <a data-type="indexterm" data-primary="workloads" data-secondary="isolating, rationale" id="idm45302816673104"/><a data-type="indexterm" data-primary="pods" data-secondary="isolating, rationale" id="idm45302816672128"/>isolating a workload or pod—it may have access to sensitive information and data, or it
may be untrusted and potentially hostile to other users of the system:</p>

<ul>
<li>
<p>A <em>sensitive</em> workload is one whose data or code is too important to permit unauthorized access to. This may include fraud<a data-type="indexterm" data-primary="workloads" data-secondary="sensitive" id="idm45302816669184"/>
detection systems, pricing engines, high-frequency trading algorithms, personally identifiable information (PII),
financial records, passwords that may be reused in other systems, machine learning models, or an organization’s “secret
sauce.” Sensitive workloads are precious.</p>
</li>
<li>
<p><em>Untrusted</em> workloads <a data-type="indexterm" data-primary="workloads" data-secondary="untrusted" id="wrklds_untrust"/><a data-type="indexterm" data-primary="untrusted workloads" id="idm45302816665376"/>are those that may be dangerous to run. They may allow high-risk
user input or run external software.</p>
</li>
</ul>

<p>Examples of potentially untrusted workloads include:</p>

<ul>
<li>
<p>VM workloads on a cloud provider’s <a data-type="indexterm" data-primary="hypervisors" data-secondary="untrusted workloads" id="idm45302816662656"/><a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="untrusted workloads" id="idm45302816661680"/><a data-type="indexterm" data-primary="virtual machines (VMs)" data-see="VMs (virtual machines)" id="idm45302816660736"/>hypervisor</p>
</li>
<li>
<p>CI/CD infrastructure <a data-type="indexterm" data-primary="CI/CD (Continuous Integration, Continuous Delivery)" data-secondary="untrusted workloads" id="idm45302816658800"/><a data-type="indexterm" data-primary="Continuous Integration, Continuous Delivery (CI/CD)" data-see="CI/CD (Continuous Integration, Continuous Delivery)" id="idm45302816657696"/>subject to build-time supply chain attacks</p>
</li>
<li>
<p>Transcoding <a data-type="indexterm" data-primary="transcoding, untrusted workloads" id="idm45302816655728"/>of complex files with potential parser errors</p>
</li>
</ul>

<p>Untrusted workloads may also<a data-type="indexterm" data-primary="software" data-secondary="untrusted workloads" id="idm45302816654096"/><a data-type="indexterm" data-primary="security" data-secondary="CVEs" data-tertiary="zero-day" id="idm45302816653120"/><a data-type="indexterm" data-primary="CVEs (Common Vulnerabilities and Exposures)" data-secondary="zero-day, software and" id="idm45302816651904"/><a data-type="indexterm" data-primary="zero-day vulnerabilities" data-secondary="software with CVEs" id="idm45302816650864"/> include software with published or suspected zero-day Common Vulnerabilities and Exposures
(CVEs)—if no patch is available and the workload is business-critical, isolating it further may decrease the potential
impact of the vulnerability if<a data-type="indexterm" data-primary="workloads" data-secondary="untrusted" data-startref="wrklds_untrust" id="idm45302816649472"/> exploited.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The threat to a host running untrusted workloads <a data-type="indexterm" data-primary="untrusted workloads, sandboxing" id="idm45302816647120"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="untrusted workloads" id="idm45302816646400"/>is the workload, or process, itself. By sandboxing a process and
removing the system APIs available to it, the attack surface presented by the host to the process is decreased. Even if
that process is compromised, the risk to the host is less.</p>
</div>

<p>BCTL allows users to upload files to import data and shipping manifests, so you have a risk that threat actors will try
to upload badly formatted or malicious files to try to force exploitable software errors. The pods <a data-type="indexterm" data-primary="batch workloads, sandboxing" id="idm45302816644320"/>that run the batch
transformation and processing workloads are a good candidate for sandboxing, as they are processing untrusted inputs as
shown in <a data-type="xref" href="#runtime-sandboxing-malicious-batch-workload">Figure 3-2</a>.</p>

<figure><div id="runtime-sandboxing-malicious-batch-workload" class="figure">
<img src="Images/haku_0302.png" alt="haku 0302" width="1439" height="658"/>
<h6><span class="label">Figure 3-2. </span>Sandboxing a risky batch workload</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Any data supplied to an<a data-type="indexterm" data-primary="user data, sanitized" id="idm45302816639584"/> application by users can be considered untrusted, however most input will be sanitized in some way (for example, validating against an integer or string type). Complex files like PDFs or videos cannot be sanitized in this way, and rely upon the encoding libraries to be secure, which they sometimes are not. Bugs in this type are often “escapable” like CVE-X or ImageTragick.</p>
</div>

<p class="pagebreak-before">Your threat model may include:</p>

<ul>
<li>
<p>An untrusted<a data-type="indexterm" data-primary="threat modeling" data-secondary="untrusted workloads" id="idm45302816636240"/> user input triggers a bug in a workload that an attacker uses to execute malicious code</p>
</li>
<li>
<p>A sensitive application is compromised and the attacker tries to exfiltrate data</p>
</li>
<li>
<p>A malicious user on a compromised node attempts to read memory of other processes on the host</p>
</li>
<li>
<p>New sandboxing code is less well tested, and may contain exploitable bugs</p>
</li>
<li>
<p>A container image build pulls malicious dependencies and code from unauthenticated external sources that may contain malware</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Existing container runtimes come <a data-type="indexterm" data-primary="container runtimes" data-secondary="default hardening" id="idm45302816629952"/><a data-type="indexterm" data-primary="Docker" data-secondary="container runtimes, default hardening" id="idm45302816628976"/>with some hardening by default, and Docker uses default <code>seccomp</code> and AppArmor profiles
that drop a large number of unused system calls. These are not enabled by default in Kubernetes and must be enforced
with admission control or PodSecurityPolicy. The <code>SeccompDefault=true</code> kubelet feature gate in v1.22 restores this container runtime default behavior.</p>
</div>

<p>Now that we have an idea of the dangers to your systems, let’s take a step back. We’ll look at virtualization: what it
is, why we use containers, and how to combine the best bits of containers and VMs.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Containers, Virtual Machines, and Sandboxes"><div class="sect1" id="idm45302816674144">
<h1>Containers, Virtual Machines, and Sandboxes</h1>

<p>A major difference between a <a data-type="indexterm" data-primary="containers" data-secondary="compared to VMs" id="idm45302816624416"/><a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="compared to containers" id="idm45302816623440"/>container and a VM is that containers exist on a shared host kernel. VMs boot a kernel
every time they start, use hardware-assisted virtualization, and have a more secure but traditionally slower
runtime.</p>

<p>A common perception is that containers are optimized for speed and portability, and virtual machines sacrifice these
features for more robust isolation from malicious behavior and higher fault tolerance.</p>

<p>This perception is not entirely true. Both technologies share a lot of common code pathways in the kernel itself.
Containers and virtual machines have evolved like co-orbiting stars, never fully able to escape each other’s gravity.
Container runtimes are a form of kernel virtualization. The OCI (<a href="https://oreil.ly/RCCWR">Open Container
Initiative</a>) container <a data-type="indexterm" data-primary="OCI (Open Container Initiative)" id="idm45302816620112"/><a data-type="indexterm" data-primary="Open Container Initiative (OCI)" data-see="OCI (Open Container Initiative)" id="idm45302816619344"/><a data-type="indexterm" data-primary="containers" data-secondary="OCI" id="idm45302816618368"/>image specifications have become the standardized atomic unit of container deployment.</p>

<p>Next-generation sandboxes <a data-type="indexterm" data-primary="sandboxes" data-secondary="techniques" id="sndbx_technqs"/>combine container and virtualization techniques (see <a data-type="xref" href="#runtime-comparison">Figure 3-3</a>) to reduce
workloads’ access to the kernel. They do this by by emulating kernel functionality in userspace or the isolated guest
environment, thus reducing the host’s <a data-type="indexterm" data-primary="attack surface" data-secondary="sandboxes" id="idm45302816614256"/>attack surface to the process inside the sandbox. Well-defined interfaces can help
to reduce complexity, minimizing the opportunity for untested code paths. And, by integrating the sandboxes with
<code>containerd</code>, they <a data-type="indexterm" data-primary="containerd" data-secondary="sandboxes" id="idm45302816612592"/>are also able to interact with OCI images and with a software proxy (“shim”) to connect two different
interfaces, which can be used with orchestrators like Kubernetes.</p>

<figure><div id="runtime-comparison" class="figure">
<img src="Images/haku_0303.png" alt="image" width="1153" height="580"/>
<h6><span class="label">Figure 3-3. </span>Comparison of container isolation approaches; source: Christian Bargmann and Marina Tropmann-Frick’s <a href="https://oreil.ly/4slD4">container isolation paper</a></h6>
</div></figure>

<p>These sandboxing techniques are especially relevant to public cloud providers, for which multitenancy<a data-type="indexterm" data-primary="multitenancy" data-secondary="sandboxes" id="idm45302816607904"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="multitenancy" id="idm45302816606928"/> and bin packing is highly lucrative. Aggressively multitenanted systems such as Google Cloud Functions<a data-type="indexterm" data-primary="Google Cloud" data-secondary="Functions, sandboxes" id="idm45302816605744"/> and AWS Lambda are <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Lambda, sandboxes" id="idm45302816604672"/><a data-type="indexterm" data-primary="Lambda (AWS)" id="idm45302816603680"/><a data-type="indexterm" data-primary="Amazon Web Services (AWS)" data-see="AWS (Amazon Web Services)" id="idm45302816603008"/>running “untrusted code as a service,” and this isolation software is born from cloud vendor security requirements to isolate serverless runtimes from other tenants. Multitenancy will be discussed in depth in the<a data-type="indexterm" data-primary="sandboxes" data-secondary="techniques" data-startref="sndbx_technqs" id="idm45302816601664"/> next chapter.</p>

<p>Cloud providers <a data-type="indexterm" data-primary="cloud providers" data-secondary="containers and" id="idm45302816600032"/>use virtual machines as the atomic unit of compute, but they may also wrap the root virtual machine process in container-like technologies. Customers then use the virtual machine to run containers—virtualized inception.</p>

<p>Traditional virtualization emulates a physical hardware architecture in software. Micro <a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="micro" id="idm45302816598160"/><a data-type="indexterm" data-primary="micro VMs" id="idm45302816597184"/>VMs emulate as small an API as possible, removing features like I/O devices and even system calls to ensure least privilege. However, they are still running the same Linux kernel code to perform low-level program operations such as memory mapping and opening sockets—just with additional security abstractions to create a secure by default runtime. So even though VMs are not sharing as much of the kernel as containers do, some system calls must still be executed by the host kernel.</p>

<p>Software abstractions<a data-type="indexterm" data-primary="software abstractions" id="idm45302816595504"/> require CPU time to execute, and so virtualization must always be a balance of security and performance. It is possible to add enough layers of abstraction and indirection that a process is considered “highly secure,” but it is unlikely that this ultimate security will result in a viable user experience. Unikernels<a data-type="indexterm" data-primary="unikernels" id="idm45302816594304"/> go in the other direction, tracing a program’s execution and then removing almost all kernel functionality except what the program has used. Observability and debuggability are perhaps the reasons that unikernels have not seen widespread adoption.</p>

<p>To understand the trade-offs and compromises inherent in each approach, it is important to grok a comparison of virtualization types. Virtualization has existed for a long time and has many variations.</p>








<section data-type="sect2" data-pdf-bookmark="How Virtual Machines Work"><div class="sect2" id="idm45302816592496">
<h2>How Virtual Machines Work</h2>

<p>Although virtual <a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="overview" id="VM_oview"/>machines and associated technologies have existed since the late 1950s, a lack of hardware support in the 1990s led to their temporary demise. During this time “process virtual machines” became more popular, especially the Java virtual machine (JVM). In this chapter we are exclusively referring to system virtual machines: a <a data-type="indexterm" data-primary="system virtual machines" id="idm45302816589104"/>form of virtualization not tied to a specific programming language. Examples include KVM/QEMU, VMware, Xen, VirtualBox, etc.</p>

<p>Virtual machine research began in the 1960s to facilitate sharing large, expensive physical machines between multiple users and processes (see <a data-type="xref" href="#runtime-virtualisation-family-tree">Figure 3-4</a>). To share a physical host safely, some level of isolation must be enforced between tenants—and in case of hostile tenants, there should be much less access to the underlying system.</p>

<figure><div id="runtime-virtualisation-family-tree" class="figure">
<img src="Images/haku_0304.png" alt="Container abstractions" width="1413" height="823"/>
<h6><span class="label">Figure 3-4. </span>Family tree of virtualization; source: <a href="https://oreil.ly/7OLfk">“The Ideal Versus the Real”</a></h6>
</div></figure>

<p class="pagebreak-before">This is performed in hardware (the CPU), software (in the kernel, and userspace), or from cooperation between both layers, and allows many users to share the same large physical hardware. This innovation became the driving technology behind <a data-type="indexterm" data-primary="public cloud" id="idm45302816583088"/>public cloud adoption: safe sharing and isolation for processes, memory, and the resources they require from the physical host machine.</p>

<p>The host machine is split into smaller isolated compute units, traditionally referred to as <a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="guests" id="idm45302816581392"/><a data-type="indexterm" data-primary="guests (VMs)" id="idm45302816580416"/>guests (see <a data-type="xref" href="#runtime-virtualisation-high-level">Figure 3-5</a>). These guests interact with a virtualized layer above the physical host’s CPU and devices. That layer intercepts system calls to handle them itself: either by proxying them to the host kernel, or handling the request itself—doing the kernel’s job where possible. Full <a data-type="indexterm" data-primary="full virtualization" id="idm45302816578480"/>virtualization (e.g., VMware) emulates hardware and boots a full kernel inside the guest. Operating-system–level virtualization<a data-type="indexterm" data-primary="operating-system–level virtualization" id="idm45302816577536"/> (e.g., a container) emulates the host’s kernel (i.e., using namespace, <code>cgroups</code>, capabilities, and <code>seccomp</code>) so it can start a containerized process directly on the host kernel. Processes in containers share many of the kernel pathways and security mechanisms that processes in VMs execute.</p>

<figure><div id="runtime-virtualisation-high-level" class="figure">
<img src="Images/haku_0305.png" alt="image" width="1313" height="486"/>
<h6><span class="label">Figure 3-5. </span>Server virtualization; source: <a href="https://oreil.ly/oNBFf">“The Ideal Versus the Real”</a></h6>
</div></figure>

<p>To boot a <a data-type="indexterm" data-primary="kernels, booting" id="idm45302816572800"/>kernel, a guest operating system will require access to a subset of the host machine’s functionality,
including BIOS routines, devices and peripherals (e.g., keyboard, graphical/console access, storage, and networking), an
interrupt controller and an interval timer, a source of entropy (for random number seeds), and the memory address space
that it will run in.</p>

<p>Inside each guest virtual machine is an environment in which processes (or workloads) can run. The virtual machine
itself is owned by a privileged parent process that manages its setup and interaction with the host, known as a <em>virtual
machine monitor</em> or VMM <a data-type="indexterm" data-primary="VMMs (virtual machine monitors)" id="idm45302816570512"/><a data-type="indexterm" data-primary="virtual machine monitors (VMMs)" id="idm45302816569712"/>(as in <a data-type="xref" href="#runtime-virtualisation-vmm">Figure 3-6</a>). This has also been known as a <a data-type="indexterm" data-primary="hypervisors" id="idm45302816568048"/>hypervisor, but the
distinction is blurred with more recent approaches so the original term VMM is preferred.</p>

<figure><div id="runtime-virtualisation-vmm" class="figure">
<img src="Images/haku_0306.png" alt="image" width="213" height="317"/>
<h6><span class="label">Figure 3-6. </span>A virtual machine manager</h6>
</div></figure>

<p>Linux has a built-in virtual machine manager <a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="management, KVM" id="idm45302816564416"/><a data-type="indexterm" data-primary="KVM (Kernel Virtual Machine)" id="idm45302816563440"/><a data-type="indexterm" data-primary="Kernel Virtual Machine (KVM)" id="idm45302816562752"/>called KVM that allows a host kernel to run virtual machines. Along with <a data-type="indexterm" data-primary="QEMU" id="idm45302816561936"/><a data-type="indexterm" data-primary="memory management" id="idm45302816561264"/><a data-type="indexterm" data-primary="device emulation" id="idm45302816560592"/>QEMU, which emulates physical devices and provides memory management to the guest (and can run by itself if necessary), an operating system can run fully emulated by the guest OS and by QEMU (as contrasted with the Xen hypervisor in <a data-type="xref" href="#runtime-virtualisation-kvm-vs-xen-vs-qemu">Figure 3-7</a>). This emulation narrows the interface between the VM and the host kernel and reduces the amount of kernel code the process inside the VM can reach directly. This provides a greater level of isolation from unknown kernel vulnerabilities.</p>

<figure><div id="runtime-virtualisation-kvm-vs-xen-vs-qemu" class="figure">
<img src="Images/haku_0307.png" alt="image" width="1439" height="658"/>
<h6><span class="label">Figure 3-7. </span>KVM contrasted with Xen and QEMU; source: <a href="https://oreil.ly/k1bJ1">What Is the Difference Between KVM and QEMU</a></h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Despite many decades of effort, “in practice no virtual machine is completely equivalent to its
real machine counterpart” (<a href="https://oreil.ly/oNBFf">“The Ideal Versus the Real”</a>). This is due to the complexities of emulating
hardware, and hopefully decreases the chance that we’re living in a simulation.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Benefits of Virtualization"><div class="sect2" id="idm45302816591872">
<h2>Benefits of Virtualization</h2>

<p>Like all things <a data-type="indexterm" data-primary="virtualization" data-secondary="benefits" id="idm45302816552336"/>we try to secure, virtualization must balance performance with security: decreasing the risk of running your workloads using the minimum possible number of extra checks at runtime. For containers, a shared host kernel is an avenue of potential container escape—the Linux kernel has a long heritage and monolithic codebase.</p>

<p>Linux is mainly written in the C language, which has classes of<a data-type="indexterm" data-primary="vulnerabilities" data-secondary="C programming language" id="idm45302816550480"/><a data-type="indexterm" data-primary="C (programming language)" data-secondary="vulnerabilities" id="idm45302816549424"/><a data-type="indexterm" data-primary="memory management" data-secondary="vulnerabilities" id="idm45302816548464"/><a data-type="indexterm" data-primary="range checking, vulnerabilities" id="idm45302816547520"/> memory management and range checking vulnerabilities that have proven notoriously difficult to entirely eradicate. Many applications have experienced these exploitable bugs when subjected to fuzzers. This risk means we want to keep hostile code away from trusted interfaces in case they have zero-day vulnerabilities. This is a pretty serious defensive stance—it’s about reducing any window of opportunity for an attacker that has access to zero-day Linux vulnerabilities.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Google’s <a href="https://oreil.ly/8LAkV">OSS-Fuzz</a> was <a data-type="indexterm" data-primary="OSS-Fuzz" id="idm45302816544512"/>born from the swirling maelstrom around the Heartbleed OpenSSL bug,
which may have been raging in the wild for up to two years. Critical, internet-bolstering projects like OpenSSL are
poorly funded and much goodwill exists in the open source community, so finding these bugs before they are
exploited is a vital step in securing critical software.</p>
</div>

<p>The sandboxing model defends <a data-type="indexterm" data-primary="sandboxes" data-secondary="zero-day vulnerabilities" id="idm45302816542768"/>against zero-days by abstractions. It moves processes away from the Linux system call interface to reduce the opportunities to exploit it, using an assortment of containers and capabilities, LSMs and kernel modules, hardware and software virtualization, and dedicated drivers. Most recent sandboxes use a type-safe language<a data-type="indexterm" data-primary="type-safe languages, sandboxes" id="idm45302816541264"/> like Golang or Rust, which makes their memory management safer than software programmed in C (which requires manual
and potentially error-prone memory management).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="What’s Wrong with Containers?"><div class="sect2" id="idm45302816540144">
<h2>What’s Wrong with Containers?</h2>

<p>Let’s further define <a data-type="indexterm" data-primary="containers" data-secondary="host kernel interaction" id="cont_hkinteraction"/><a data-type="indexterm" data-primary="host kernels, container interaction" id="hk_contint"/>what we mean by containers by looking at how they interact with the host kernel, as shown in <a data-type="xref" href="#runtime-host-kernel-boundary">Figure 3-8</a>.</p>

<p>Containers talk directly to the host kernel, but the layers of LSMs, capabilities, and namespaces ensure they do not have
full host kernel access. Conversely, instead of sharing one kernel, VMs use a guest kernel (a dedicated kernel running in a
hypervisor). This means if the VM’s guest kernel is compromised, more work is required to break out of the hypervisor
and into the host.</p>

<figure><div id="runtime-host-kernel-boundary" class="figure">
<img src="Images/haku_0308.png" alt="Host kernel boundary" width="1039" height="636"/>
<h6><span class="label">Figure 3-8. </span>Host kernel boundary</h6>
</div></figure>

<p>Containers are created by a low-level container runtime, and as users we talk to the high-level container runtime that
controls it.</p>

<p>The diagram in <a data-type="xref" href="#runtime-docker-podman-crio">Figure 3-9</a> shows the high-level interfaces, with the container managers on the left. Then Kubernetes, Docker, and Podman interact with their respective libraries and runtimes. These perform useful container management features including pushing and pulling container images, managing storage and network interfaces, and interacting with the low-level container runtime.</p>

<figure><div id="runtime-docker-podman-crio" class="figure">
<img src="Images/haku_0309.png" alt="Container abstractions" width="1442" height="629"/>
<h6><span class="label">Figure 3-9. </span>Container abstractions; source: <a href="https://oreil.ly/2Mx7n">“What’s up with CRI-O, Kata Containers and Podman?”</a></h6>
</div></figure>

<p>In the middle column of <a data-type="xref" href="#runtime-docker-podman-crio">Figure 3-9</a> are the container runtimes that your Kubernetes cluster interacts with, while in the right column are the low-level runtimes responsible for starting and managing the container.</p>

<p>That low-level container runtime is directly responsible for starting and managing containers, interfacing with the kernel to create the namespaces and configuration, and finally starting the process in the container. It is also responsible for handling your process inside the container, and getting its system calls to the host kernel at <a data-type="indexterm" data-primary="containers" data-secondary="host kernel interaction" data-startref="cont_hkinteraction" id="idm45302816525488"/><a data-type="indexterm" data-primary="host kernels, container interaction" data-startref="hk_contint" id="idm45302816524240"/>runtime.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="User Namespace Vulnerabilities"><div class="sect2" id="idm45302816523024">
<h2>User Namespace Vulnerabilities</h2>

<p>Linux was written <a data-type="indexterm" data-primary="namespaces" data-secondary="user, vulnerabilities" id="namesp_usr_vuln"/><a data-type="indexterm" data-primary="user namespaces" data-secondary="vulnerabilities" id="usrnamsp_vuln"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="user namespaces" id="vuln_usrnamesp"/>with a core assumption: that the root user is always in the host namespace. This assumption held true while there were no other namespaces. But this changed with the introduction of user namespaces (the last major kernel namespace to be completed): developing user namespaces required many code changes to code concerning the root user.</p>

<p>User namespaces allow you to <a data-type="indexterm" data-primary="users, ID mapping" id="idm45302816516496"/>map users inside a container to other users on the host, so ID 0 (root) inside the container can create files on a volume that from within the container look to be root-owned. But when you inspect the same volume from the host, they show up as owned by the user root was mapped to (e.g., user ID 1000, or 110000, as shown in <a data-type="xref" href="#runtime-user-ns-remapping">Figure 3-10</a>). User namespaces are not enabled in Kubernetes, although work is underway to support them.</p>

<figure><div id="runtime-user-ns-remapping" class="figure">
<img src="Images/haku_0310.png" alt="User namespace user id remapping" width="650" height="504"/>
<h6><span class="label">Figure 3-10. </span>User namespace user ID remapping</h6>
</div></figure>

<p>Everything in Linux is a file, and files are owned by users. This makes user namespaces wide-reaching and complex, and they have been a source of <a data-type="indexterm" data-primary="privilege escalation" data-secondary="user namespaces" id="idm45302816511808"/><a data-type="indexterm" data-primary="user namespaces" data-secondary="privilege escalation" id="idm45302816510832"/>privilege escalation bugs in previous versions of Linux:</p>
<dl>
<dt><a href="https://oreil.ly/5UHB1">CVE-2013-1858</a> (user namespace &amp; CLONE_FS)</dt>
<dd>
<p>The clone system-call<a data-type="indexterm" data-primary="clone system-call" id="idm45302816507440"/>
implementation in the Linux kernel before 3.8.3 does not properly handle a combination of the <code>CLONE_NEWUSER</code> and
<code>CLONE_FS</code> flags, which allows local users to gain privileges by calling <code>chroot</code> and leveraging the sharing of the /
directory between a parent process and a child process.</p>
</dd>
<dt><a href="https://oreil.ly/iRKjY">CVE-2014-4014</a> (user namespace &amp; chmod)</dt>
<dd>
<p>The capabilities <a data-type="indexterm" data-primary="capabilities" data-secondary="privilege escalation" id="idm45302816503104"/>implementation in
the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local
users to bypass intended <code>chmod</code> restrictions by first creating a user namespace, as demonstrated by setting the <code>setgid</code>
bit on a file with group ownership of <code>root</code>.</p>
</dd>
<dt><a href="https://oreil.ly/uCaNj">CVE-2015-1328</a> (user namespace &amp; OverlayFS (Ubuntu only))</dt>
<dd>
<p>The <code>overlayfs</code> implementation in<a data-type="indexterm" data-primary="overlayfs, privilege escalation" id="idm45302816498096"/> the Linux kernel package before 3.19.0-21.21 in Ubuntu versions until 15.04 did not properly check  permissions for file creation in the upper filesystem directory, which allowed local users to obtain root access by  leveraging a configuration in which <code>overlayfs</code> is permitted in an arbitrary mount namespace.</p>
</dd>
<dt><a href="https://oreil.ly/8YIWz">CVE-2018-18955</a> (user namespace &amp; complex ID mapping)</dt>
<dd>
<p>In the Linux kernel 4.15.x through 4.19.x before 4.19.2, <code>map_write()</code> <em>in kernel/user_namespace.c</em> allows <a data-type="indexterm" data-primary="map_write(), privilege escalation" id="idm45302816493440"/>privilege escalation because it mishandles nested user namespaces with more than 5 <code>UID</code> or <code>GID</code> ranges. A user who has <code>CAP_SYS_ADMIN</code> in an affected user namespace can bypass access controls on resources outside the namespace, as demonstrated by reading <em>/etc/shadow</em>. This occurs because an ID transformation takes place properly for the namespaced-to-kernel direction but not for the
kernel-to-namespaced direction.</p>
</dd>
</dl>

<p>Containers are not inherently “insecure,” but as we saw in <a data-type="xref" href="ch02.xhtml#ch-pod-level-resources">Chapter 2</a>, they can leak some information
about a host, and a root-owned container runtime is a potential exploitation path for a hostile process or container
image.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Operations such as creating network adapters in the host network namespace, and mounting host disks, are historically root-only, which has made rootless<a data-type="indexterm" data-primary="container builds" data-secondary="rootless" id="contbld_rootless"/> containers harder to implement. Rootfull <a data-type="indexterm" data-primary="container runtimes" data-secondary="rootfull" id="idm45302816486480"/><a data-type="indexterm" data-primary="rootfull container runtimes" id="idm45302816485536"/>container runtimes were the only viable option for the first decade of popularized container use.</p>

<p>Exploits that<a data-type="indexterm" data-primary="exploits" data-secondary="rootfull containers" id="idm45302816484048"/><a data-type="indexterm" data-primary="CVEs (Common Vulnerabilities and Exposures)" data-secondary="rootfull containers" id="idm45302816483072"/><a data-type="indexterm" data-primary="security" data-secondary="CVEs" data-tertiary="rootfull containers" id="idm45302816482000"/> have abused this rootfulness include <a href="https://oreil.ly/ZZyRQ">CVE-2019-5736</a>, replacing the <code>runc</code> binary from inside a container via <em>/proc/self/exe</em>, and <a href="https://oreil.ly/DSKFf">CVE-2019-14271</a>, attacking the host from inside a container responding to <code>docker cp</code>.</p>
</div>

<p>Underlying concerns about a root-owned daemon can be assuaged by running rootless <a data-type="indexterm" data-primary="containers" data-secondary="rootless" id="idm45302816477504"/><a data-type="indexterm" data-primary="rootless containers" id="idm45302816476528"/>containers in “unprivileged user
namespaces” mode: creating containers using a nonroot user, within their own user namespace. This is supported in
Docker 20.0X and Podman.</p>

<p class="pagebreak-before"><em>Rootless</em> means the low-level container runtime process that creates the container is owned by an unprivileged user,
and so <a data-type="indexterm" data-primary="container breakouts" data-secondary="rootless containers" id="idm45302816474336"/>container breakout via the process tree only escapes to a nonroot user, nullifying some potential attacks.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Rootless containers introduce a hopefully less dangerous risk—user namespaces have historically been a rich source of
vulnerabilities. The answer to whether it is riskier to run root-owned daemon or user namespaces isn’t clear-cut, although any reduction of root privileges is likely to be the more effective <a data-type="indexterm" data-primary="security boundaries" data-secondary="rootless containers" id="idm45302816471792"/>security boundary.
There have been more high-profile breakouts from root-owned Docker, but this may well be down to adoption and widespread use.</p>

<p>Rootless containers (without a root-owned daemon) provide a security boundary as compared to those with root-owned
daemons. When code owned by the host’s root user is compromised by a malicious process, it can potentially read and
write other users’ files, attack the network and its traffic, or install malware to the host.</p>
</div>

<p>The mapping of user identifiers (UIDs)<a data-type="indexterm" data-primary="user identifiers (UIDs), mapping" id="idm45302816469248"/><a data-type="indexterm" data-primary="UIDs (user identifiers), mapping" id="idm45302816468480"/> in the guest to actual users on the host depends on the user mappings of the host user namespace, container user namespace, and rootless runtime, as shown in <a data-type="xref" href="#runtime-userns-and-rootles">Figure 3-11</a>.</p>

<figure><div id="runtime-userns-and-rootles" class="figure">
<img src="Images/haku_0311.png" alt="User mapping for Rootless and User Namespace containers" width="957" height="796"/>
<h6><span class="label">Figure 3-11. </span>Container abstractions; source: <a href="https://oreil.ly/B2KzQ">“Experimenting with Rootless Docker”</a></h6>
</div></figure>

<p>User namespaces allow nonroot users to pretend to be the host’s root user. The “root-in-userns” user can have a “fake” UID 0 and permission to create new namespaces (mount, net, uts, ipc), change the container’s hostname, and mount points.</p>

<p>This allows root-in-userns, which is unprivileged in the host namespace, to create new containers. To achieve this, additional work must be done: network connections into the host network namespace can only be created by the host’s root. For rootless containers, an unprivileged <em>slirp4netns</em> networking device (guarded by <code>seccomp</code>) is used to create a virtual network device.</p>

<p>Unfortunately, mounting remote filesystems becomes difficult when the remote system, e.g., NFS home directories, does not understand the host’s user namespaces.</p>

<p>In the <a href="https://oreil.ly/YjwLF">rootless Podman guide</a>, <a href="https://oreil.ly/QzBhv">Dan Walsh</a> says:</p>
<blockquote>
<p>If you have a normal process creating files on an NFS share and not taking advantage of user-namespaced capabilities, everything works fine. The problem comes in when the root process inside the container needs to do something on the NFS share that requires special capability access. In that case, the remote kernel will not know about the capability and will most likely deny access.</p></blockquote>

<p>While rootless Podman<a data-type="indexterm" data-primary="container builds" data-secondary="rootless" data-startref="contbld_rootless" id="idm45302816457456"/><a data-type="indexterm" data-primary="rootless Podman" data-secondary="configuration" id="idm45302816456208"/><a data-type="indexterm" data-primary="Podman" data-secondary="rootless" id="idm45302816455232"/> has SELinux support (and dynamic profile support via <a href="https://oreil.ly/AuSMF">udica</a>), rootless<a data-type="indexterm" data-primary="rootless Docker, configuration" id="idm45302816453440"/><a data-type="indexterm" data-primary="Docker" data-secondary="rootless" id="idm45302816452688"/> Docker does not yet support AppArmor and, for both runtimes, CRIU (Checkpoint/Restore In Userspace, a feature to freeze running applications) is 
<span class="keep-together">disabled.</span></p>

<p>Both rootless runtimes require configuration for some networking features: <code>CAP_NET_BIND_SERVICE</code> is required by the
kernel to bind to ports below 1024 (historically considered a privileged boundary), and ping is not supported for users
with high UIDs if the ID is not in <em>/proc/sys/net/ipv4/ping_group_range</em> (although this can be changed by host root).
Host networking is not permitted (as it breaks the network isolation), <code>cgroups</code> v2 are functional but only when running
under <code>systemd</code>, and <code>cgroup</code> v1 is not supported by either rootless implementation. There are more details in the docs for
<a href="https://oreil.ly/3SWtT">shortcomings of rootless Podman</a>.</p>

<p>Docker and Podman share <a data-type="indexterm" data-primary="performance" data-secondary="Podman" id="idm45302816446896"/><a data-type="indexterm" data-primary="Podman" data-secondary="performance" id="idm45302816445888"/><a data-type="indexterm" data-primary="performance" data-secondary="Docker" id="idm45302816444944"/><a data-type="indexterm" data-primary="Docker" data-secondary="performance" id="idm45302816444000"/>similar performance and features as both use <code>runc</code>, although Docker has an established networking model that doesn’t support host networking in rootless mode, whereas Podman reuses Kubernetes’ Container Network Interface (CNI)) plug-ins for greater networking deployment flexibility.</p>

<p>Rootless containers <a data-type="indexterm" data-primary="rootless containers" data-secondary="privilege escalation" id="idm45302816441808"/>decrease the risk of running your container images. Rootlessness prevents an exploit escalating to root via many host interactions (although some use of <code>SETUID</code> and <code>SETGID</code> binaries is often needed by software aiming to avoid running processes as root).</p>

<p>While rootless containers protect the host from the container, it may still be possible to read some data from the host, although an adversary will find this a lot less useful. Root capabilities are needed to interact with potential privilege escalation points including <em>/proc</em>, host devices, and the kernel interface, among others.</p>

<p>Throughout these layers of abstraction, <a data-type="indexterm" data-primary="system calls" data-secondary="C programming language and" id="idm45302816437904"/><a data-type="indexterm" data-primary="C (programming language)" data-secondary="system calls" id="idm45302816436912"/>system calls are still ultimately handled by software written in potentially unsafe C. Is the rootless runtime’s exposure to C-based system calls in the Linux kernel really that bad? Well, the C language powers the internet (and world?) and has done so for decades, but its lack of memory management leads to the same critical bugs occurring over and over again. When the kernel, OpenSSL, and other critical software are written in C, we just want to move everything as far away from trusted kernel space as possible.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a href="https://oreil.ly/yyD5o">Whitesource suggests</a> that C <a data-type="indexterm" data-primary="C (programming language)" data-secondary="vulnerabilities" id="idm45302816433664"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="C programming language" id="idm45302816432640"/>has accounted for 47% of all reported vulnerabilities in the last 10 years. This may largely be due to its proliferation and longevity, but highlights the inherent risk.</p>
</div>

<p>While “trimmed-down” kernels exist (like unikernels and rump kernels), many traditional and legacy <a data-type="indexterm" data-primary="applications" data-secondary="porting onto containers" id="idm45302816430752"/><a data-type="indexterm" data-primary="containers" data-secondary="porting applications" id="idm45302816429776"/>applications are
portable onto a container runtime without code modifications. To achieve this feat for a unikernel would require the
application to be ported to the new reduced kernel. Containerizing an application is a generally frictionless developer
experience, which has contributed to the success of <a data-type="indexterm" data-primary="namespaces" data-secondary="user, vulnerabilities" data-startref="namesp_usr_vuln" id="idm45302816428384"/><a data-type="indexterm" data-primary="user namespaces" data-secondary="vulnerabilities" data-startref="usrnamsp_vuln" id="idm45302816427168"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="user namespaces" data-startref="vuln_usrnamesp" id="idm45302816425952"/>containers.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Sandboxing"><div class="sect1" id="idm45302816625424">
<h1>Sandboxing</h1>

<p>If a process can<a data-type="indexterm" data-primary="sandboxes" data-secondary="overview" id="sandbx_oview"/> exploit the kernel, it can take over the system the kernel is running. This is a risk that adversaries
like Captian Hashjack will attempt to exploit, and so cloud providers and hardware vendors have been pioneering
different approaches to moving away from Linux system call interaction for the guest.</p>

<p>Linux <a data-type="indexterm" data-primary="containers" data-secondary="isolation" id="idm45302816421056"/>containers are a lightweight form of isolation as they allow workloads to use kernel APIs directly, minimizing the
layers of abstraction. Sandboxes take a variety of other approaches, and generally use container techniques as well.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Linux’s Kernel Virtual Machine (KVM) is a <a data-type="indexterm" data-primary="KVM (Kernel Virtual Machine)" id="idm45302816418640"/><a data-type="indexterm" data-primary="Kernel Virtual Machine (KVM)" id="idm45302816417968"/><a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="compared to containers" id="idm45302816417280"/><a data-type="indexterm" data-primary="containers" data-secondary="compared to VMs" id="idm45302816416336"/>module that allows the kernel to run a nested version of itself as a hypervisor. It uses the processor’s
hardware virtualization commands and allows each “guest” to run a full Linux or Windows operating system in the virtual
machine with private, virtualized hardware. A virtual machine differs from a container as the guest’s processes are running on their
own kernel: container processes always share the host 
<span class="keep-together">kernel.</span></p>
</div>

<p>Sandboxes combine the best of virtualization and container isolation to optimize for specific use cases.</p>

<p>gVisor<a data-type="indexterm" data-primary="gVisor" data-secondary="overview" id="idm45302816413216"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="gVisor" id="sbox_gV"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="Firecracker" id="sbox_Fcracker"/><a data-type="indexterm" data-primary="Firecracker" data-secondary="overview" id="idm45302816409776"/> and Firecracker (written in Golang and Rust, respectively) both operate on the premise that their statically typed
system call proxying (between the workload/guest process and the host kernel) is more secure for consumption by
untrusted workloads than the Linux kernel itself, and that performance is not significantly impacted.</p>

<p>gVisor starts a KVM or operates in <code>ptrace</code> mode (using a debug <code>ptrace</code> system call to
monitor and control its guest), and inside starts a userspace kernel, which proxies system calls down to the host using
a “sentry” process. This trusted process reimplements 237 Linux system calls and only needs 53 host system calls to
operate. It is constrained to that list of system calls by <code>seccomp</code>. It also starts a companion “filesystem interaction”
side process called Gofer to prevent a compromised sentry process interacting with the host’s filesystem, and finally
implements its own userspace networking stack to isolate it from
bugs in the Linux TCP/IP stack.</p>

<p>Firecracker, on the other hand, while also using KVM, starts a stripped-down device emulator instead of implementing the heavyweight QEMU process to emulate devices (as traditional Linux virtual machines do). This reduces the host’s attack surface and removes unnecessary code, requiring 36 system calls itself to function.</p>

<p>And finally, at the other end of the diagram in <a data-type="xref" href="#runtime-virtualisation-spectrum">Figure 3-12</a>, KVM/QEMU VMs <a data-type="indexterm" data-primary="VMs (virtual machines)" data-secondary="hardware emulation" id="idm45302816403936"/><a data-type="indexterm" data-primary="KVM (Kernel Virtual Machine)" id="idm45302816402960"/><a data-type="indexterm" data-primary="Kernel Virtual Machine (KVM)" id="idm45302816402272"/><a data-type="indexterm" data-primary="QEMU" data-secondary="hardware emulation" id="idm45302816401584"/>emulate hardware and so provide a
guest kernel and full device emulation, which increases startup times and memory footprint.</p>

<figure><div id="runtime-virtualisation-spectrum" class="figure">
<img src="Images/haku_0312.png" alt="image" width="1003" height="202"/>
<h6><span class="label">Figure 3-12. </span>Spectrum of isolation</h6>
</div></figure>

<p>Virtualization provides better hardware isolation through CPU integration, but is slower to start and run due to the
abstraction layer between the guest and the underlying host.</p>

<p>Containers are lightweight and suitably secure for most workloads. They run in production for multinational organizations around the world. But high-sensitivity workloads <a data-type="indexterm" data-primary="workloads" data-secondary="risk categories" id="idm45302816397168"/>and data need greater isolation. You can categorize workloads by risk:</p>

<ul>
<li>
<p>Does this application access a sensitive or high-value asset?</p>
</li>
<li>
<p>Is this application able to receive untrusted traffic or input?</p>
</li>
<li>
<p>Have there been vulnerabilities or bugs in this application before?</p>
</li>
</ul>

<p class="pagebreak-before">If the answer to any of those is yes, you may want to consider a next-generation
sandboxing technology to further isolate workloads.</p>

<p>gVisor, Firecracker, and Kata Containers all take different approaches to virtual machine isolation, while sharing the
aim of challenging the perception of slow startup time and high memory overhead.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Kata Containers is a <a data-type="indexterm" data-primary="Kata Containers" id="idm45302816390272"/><a data-type="indexterm" data-primary="container runtimes" data-secondary="Kata Containers" id="idm45302816389536"/>container runtime that starts a VM and runs a container inside. It is widely compatible and
can run <code>firecracker</code> as a guest.</p>
</div>

<p><a data-type="xref" href="#runtime-virtualisation-comparison">Table 3-1</a> compares these sandboxes and some key features.</p>
<table id="runtime-virtualisation-comparison" style="width: 100%">
<caption><span class="label">Table 3-1. </span>Comparison of sandbox features; source: <a href="https://oreil.ly/vpKaB">“Making Containers More Isolated: An Overview of Sandboxed Container Technologies”</a></caption>
<thead>
<tr>
<th/>
<th>Supported container platforms</th>
<th>Dedicated guest kernel</th>
<th>Support different guest kernels</th>
<th>Open source</th>
<th>Hot-plug</th>
<th>Direct access to HW</th>
<th>Required hypervisors</th>
<th>Backed by</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>gVisor</p></td>
<td><p>Docker, K8s</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>None</p></td>
<td><p>Google</p></td>
</tr>
<tr>
<td><p>Firecracker</p></td>
<td><p>Docker</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>KVM</p></td>
<td><p>Amazon</p></td>
</tr>
<tr>
<td><p>Kata</p></td>
<td><p>Docker, K8s</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>KVM or Xen</p></td>
<td><p>OpenStack</p></td>
</tr>
</tbody>
</table>

<p>Each sandbox combines virtual machine and container technologies: some VMM process, a Linux kernel within the
virtual machine, a Linux userspace in which to run the process once the kernel has booted, and some mix of
kernel-based isolation (that is, container-style namespaces, <code>cgroups</code>, or <code>seccomp</code>) either within the VM, around the VMM,
or some combination thereof.</p>

<p>Let’s have a closer look at each<a data-type="indexterm" data-primary="sandboxes" data-secondary="gVisor" data-startref="sbox_gV" id="idm45302816359728"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="Firecracker" data-startref="sbox_Fcracker" id="idm45302816358480"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="overview" data-startref="sandbx_oview" id="idm45302816357264"/> one.</p>








<section data-type="sect2" data-pdf-bookmark="gVisor"><div class="sect2" id="idm45302816355792">
<h2>gVisor</h2>

<p>Google’s gVisor<a data-type="indexterm" data-primary="gVisor" data-secondary="running" id="gV_run"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="gVisor" data-tertiary="running" id="sbox_gV_run"/> was originally built to allow untrusted, customer-supplied workloads to run in AppEngine on Borg, Google’s internal orchestrator and the progenitor to Kubernetes. It now protects Google Cloud <a data-type="indexterm" data-primary="Google Cloud" data-secondary="gVisor" id="idm45302816350704"/>products: App Engine standard environment, Cloud Functions, Cloud ML Engine, and
Cloud Run, and it has been modified to run in GKE. It has the best Docker and Kubernetes integrations from among
this chapter’s sandboxing technologies.</p>
<div data-type="note" epub:type="note" class="less_space pagebreak-before"><h6>Note</h6>
<p>To run the examples, the <a data-type="indexterm" data-primary="gVisor" data-secondary="runtime binary" id="idm45302816348160"/><a data-type="indexterm" data-primary="runtime binaries, gVisor" id="idm45302816347184"/>gVisor runtime binary <a href="https://oreil.ly/Tj3hX">must be installed</a> on the host or worker node.</p>
</div>

<p>Docker <a data-type="indexterm" data-primary="Docker" data-secondary="gVisor, starting" id="idm45302816344960"/>supports pluggable container runtimes, and a simple <code>docker run -it --runtime=runsc</code> starts a gVisor sandboxed OCI container. Let’s have a look at what’s in <em>/proc</em> in a vanilla gVisor <a data-type="indexterm" data-primary="gVisor" data-secondary="containers, compared to runc" id="idm45302816342992"/><a data-type="indexterm" data-primary="containers" data-secondary="gVisor compared to runc" id="idm45302816341920"/><a data-type="indexterm" data-primary="runc" data-secondary="containers, compared to gVisor" id="idm45302816340976"/>container to compare it with standard <code>runc</code>:</p>

<pre data-type="programlisting" data-code-language="bash">user@host:~ <code class="o">[</code>0<code class="o">]</code><code class="nv">$ </code>docker run -it --runtime<code class="o">=</code>runsc sublimino/hack <code class="se">\</code>
  ls -lasp /proc/1

total 0
<code class="m">0</code> dr-xr-xr-x <code class="m">1</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 ./
<code class="m">0</code> dr-xr-xr-x <code class="m">2</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 ../
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 auxv
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 cmdline
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 comm
<code class="m">0</code> lrwxrwxrwx <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 cwd -&gt; /root
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 environ
<code class="m">0</code> lrwxrwxrwx <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 exe -&gt; /usr/bin/coreutils
<code class="m">0</code> dr-x------ <code class="m">1</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 fd/
<code class="m">0</code> dr-x------ <code class="m">1</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 fdinfo/
<code class="m">0</code> -rw-r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 gid_map
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 io
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 maps
<code class="m">0</code> -r-------- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 mem
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 mountinfo
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 mounts
<code class="m">0</code> dr-xr-xr-x <code class="m">1</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 net/
<code class="m">0</code> dr-x--x--x <code class="m">1</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 ns/
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 oom_score
<code class="m">0</code> -rw-r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 oom_score_adj
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 smaps
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 stat
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 statm
<code class="m">0</code> -r--r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 status
<code class="m">0</code> dr-xr-xr-x <code class="m">3</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 task/
<code class="m">0</code> -rw-r--r-- <code class="m">0</code> root root <code class="m">0</code> May <code class="m">23</code> 16:22 uid_map</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Removing special files from this directory prevents a hostile process from accessing the relevant feature
in the underlying host  
<span class="keep-together">kernel.</span></p>
</div>

<p>There are far fewer entries in <em>/proc</em> than in a <code>runc</code> container, as this diff shows:</p>

<pre data-type="programlisting" data-code-language="bash">user@host:~ <code class="o">[</code>0<code class="o">]</code><code class="nv">$ </code>diff -u <code class="se">\</code>
  &lt;<code class="o">(</code>docker run -t sublimino/hack ls -1 /proc/1<code class="o">)</code> <code class="se">\</code>
  &lt;<code class="o">(</code>docker run -t --runtime<code class="o">=</code>runsc sublimino/hack ls -1 /proc/1<code class="o">)</code>

-arch_status
-attr
-autogroup
 auxv
-cgroup
-clear_refs
 cmdline
 comm
-coredump_filter
-cpu_resctrl_groups
-cpuset
 cwd
 environ
 exe
@@ -16,39 +8,17 @@
 fdinfo
 gid_map
 io
-limits
-loginuid
-map_files
 maps
 mem
 mountinfo
 mounts
-mountstats
 net
 ns
-numa_maps
-oom_adj
 oom_score
 oom_score_adj
-pagemap
-patch_state
-personality
-projid_map
-root
-sched
-schedstat
-sessionid
-setgroups
 smaps
-smaps_rollup
-stack
 stat
 statm
 status
-syscall
 task
-timens_offsets
-timers
-timerslack_ns
 uid_map
-wchan</pre>

<p>The sentry <a data-type="indexterm" data-primary="Sentry process, gVisor" id="Senproc_gV"/>process that <a href="https://oreil.ly/MRraT">simulates the Linux system call interface</a> reimplements over 235 of the ~350 possible system calls in Linux 5.3.11. This shows you a “masked” view of the <em>/proc</em> and <em>/dev</em> virtual filesystems. These filesystems have historically leaked the container abstraction by sharing information from the host (memory, devices, processes, etc.) so are an area of special concern.</p>

<p>Let’s look at system devices under <em>/dev</em> in gVisor and <code>runc</code>:</p>

<pre data-type="programlisting" data-code-language="bash">user@host:~ <code class="o">[</code>0<code class="o">]</code><code class="nv">$ </code>diff -u <code class="se">\</code>
  &lt;<code class="o">(</code>docker run -t sublimino/hack ls -1p /dev<code class="o">)</code> <code class="se">\</code>
  &lt;<code class="o">(</code>docker run -t --runtime<code class="o">=</code>runsc sublimino/hack ls -1p /dev<code class="o">)</code>

-console
-core
 fd
 full
 mqueue/
+net/
 null
 ptmx
 pts/</pre>

<p>We can see that <a data-type="indexterm" data-primary="runsc, gVisor" id="idm45302816139568"/>the <code>runsc</code> gVisor runtime drops the <code>console</code> and <code>core</code> devices, but includes a <code>/dev/net/tun</code> device (under the <em>net/</em> directory) for its <code>netstack</code> networking stack, which also runs inside Sentry. Netstack can be bypassed for direct host network access (at the cost of some isolation), or host networking disabled entirely for fully host-isolated networking (depending on the CNI or other network configured within the sandbox).</p>

<p>Apart from these giveaways, gVisor is kind enough to identify itself at boot time, which you can see in a container with
<code>dmesg</code>:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>docker run --runtime<code class="o">=</code>runsc sublimino/hack dmesg
<code class="o">[</code>   0.000000<code class="o">]</code> Starting gVisor...
<code class="o">[</code>   0.340005<code class="o">]</code> Feeding the init monster...
<code class="o">[</code>   0.539162<code class="o">]</code> Committing treasure map to memory...
<code class="o">[</code>   0.688276<code class="o">]</code> Searching <code class="k">for</code> socket adapter...
<code class="o">[</code>   0.759369<code class="o">]</code> Checking naughty and nice process list...
<code class="o">[</code>   0.901809<code class="o">]</code> Rewriting operating system in Javascript...
<code class="o">[</code>   1.384894<code class="o">]</code> Daemonizing children...
<code class="o">[</code>   1.439736<code class="o">]</code> Granting licence to <code class="nb">kill</code><code class="o">(</code>2<code class="o">)</code>...
<code class="o">[</code>   1.794506<code class="o">]</code> Creating process schedule...
<code class="o">[</code>   1.917512<code class="o">]</code> Creating bureaucratic processes...
<code class="o">[</code>   2.083647<code class="o">]</code> Checking naughty and nice process list...
<code class="o">[</code>   2.131183<code class="o">]</code> Ready!</pre>

<p>Notably this is not the real time it takes to start the container, and the quirky messages are randomized—don’t
rely on them for automation. If we <code>time</code> the process we can see it start faster than it claims:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code><code class="nb">time </code>docker run --runtime<code class="o">=</code>runsc sublimino/hack dmesg
<code class="o">[</code>   0.000000<code class="o">]</code> Starting gVisor...
<code class="o">[</code>   0.599179<code class="o">]</code> Mounting deweydecimalfs...
<code class="o">[</code>   0.764608<code class="o">]</code> Consulting tar man page...
<code class="o">[</code>   0.821558<code class="o">]</code> Verifying that no non-zero bytes made their way into /dev/zero...
<code class="o">[</code>   0.892079<code class="o">]</code> Synthesizing system calls...
<code class="o">[</code>   1.381226<code class="o">]</code> Preparing <code class="k">for</code> the zombie uprising...
<code class="o">[</code>   1.521717<code class="o">]</code> Adversarially training Redcode AI...
<code class="o">[</code>   1.717601<code class="o">]</code> Conjuring /dev/null black hole...
<code class="o">[</code>   2.161358<code class="o">]</code> Accelerating teletypewriter to <code class="m">9600</code> baud...
<code class="o">[</code>   2.423051<code class="o">]</code> Checking naughty and nice process list...
<code class="o">[</code>   2.437441<code class="o">]</code> Generating random numbers by fair dice roll...
<code class="o">[</code>   2.855270<code class="o">]</code> Ready!

real    0m0.852s
user    0m0.021s
sys     0m0.016s</pre>

<p>Unless an application running in a sandbox explicitly checks for these features of the environment, it will be unaware that it is in a sandbox. Your application makes the same system calls as it would to a normal Linux kernel, but the Sentry process intercepts the system calls as shown in <a data-type="xref" href="#runtime-gvisor-boundaries">Figure 3-13</a>.</p>

<figure><div id="runtime-gvisor-boundaries" class="figure">
<img src="Images/haku_0313.png" alt="image" width="611" height="457"/>
<h6><span class="label">Figure 3-13. </span>gVisor container components and privilege boundaries</h6>
</div></figure>

<p>Sentry prevents the application interacting directly with the host kernel, and has a <code>seccomp</code> profile that limits its possible host system calls. This helps prevent escalation in case a tenant breaks into Sentry and attempts to attack the host kernel.</p>

<p>Implementing a userspace kernel is a Herculean undertaking and does not cover every system call. This means some applications are not able to run in <a data-type="indexterm" data-primary="gVisor" data-secondary="limitations" id="idm45302815951440"/>gvisor, although in practice this doesn’t happen very often and there are millions of workloads running on GCP under gVisor.</p>

<p>The Sentry has a side process called <a data-type="indexterm" data-primary="Gofer process, gVisor" id="idm45302815949872"/>Gofer. It handles disks and devices, which are historically common VM attack vectors. Separating out these responsibilities increases your resistance to compromise; if Sentry has an exploitable bug, it can’t be used to attack the host’s devices directly because they’re all proxied through Gofer.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>gVisor is written in <a href="https://Golang.org">Go</a> to avoid security pitfalls that can plague kernels. Go <a data-type="indexterm" data-primary="Go (programming language)" data-secondary="gVisor" id="idm45302815947024"/>is strongly typed, with built-in bounds checks, no uninitialized variables, no use-after-free bugs, no stack overflow bugs, and a built-in race detector. However, using Go has its challenges, and the runtime often introduces a little performance 
<span class="keep-together">overhead.</span></p>
</div>

<p>However, this comes at the cost of some reduced application compatibility and a high per-system-call overhead. Of course, not all applications make a lot of system calls, so this depends on usage.</p>

<p>Application system calls are redirected to Sentry by a Platform Syscall Switcher, <a data-type="indexterm" data-primary="Platform Syscall Switcher, gVisor" id="idm45302815943952"/>which intercepts the application when it tries to make system calls to the kernel. Sentry then makes the required <a data-type="indexterm" data-primary="system calls" data-secondary="gVisor" id="idm45302815942976"/>system calls to the host for the containerized process, as shown in <a data-type="xref" href="#runtime-gvisor-privilege">Figure 3-14</a>. This proxying prevents the application from directly controlling system calls.</p>

<figure class="width-85"><div id="runtime-gvisor-privilege" class="figure">
<img src="Images/haku_0314.png" alt="image" width="1441" height="1074"/>
<h6><span class="label">Figure 3-14. </span>gVisor container components and privilege levels</h6>
</div></figure>

<p>Sentry sits in a loop waiting for a system call to be generated by the application, as shown in <a data-type="xref" href="#runtime-gvisor-sentry-loop">Figure 3-15</a>.</p>

<figure><div id="runtime-gvisor-sentry-loop" class="figure">
<img src="Images/haku_0315.png" alt="image" width="694" height="820"/>
<h6><span class="label">Figure 3-15. </span>gVisor sentry pseudocode; source: <a href="https://oreil.ly/s1DjO">Resource Sharing</a></h6>
</div></figure>

<p>It captures the system call with <code>ptrace</code>, handles it, and returns a response to the process (often without making the expected system call to the host). This simple model protects the underlying kernel from any direct interaction with the process inside the container.</p>

<p>The decreasing number of permitted calls shown in <a data-type="xref" href="#runtime-gvisor-syscall-hierarchy">Figure 3-16</a> limits the exploitable interface of the underlying host kernel to 68 system calls, while the containerized application process believes it has access to all ~350 kernel <a data-type="indexterm" data-primary="gVisor" data-secondary="running" data-startref="gV_run" id="idm45302815932000"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="gVisor" data-tertiary="running" data-startref="sbox_gV_run" id="idm45302815930784"/>calls.</p>

<p>The Platform Syscall Switcher, gVisor’s system call <a data-type="indexterm" data-primary="Sentry process, gVisor" data-startref="Senproc_gV" id="idm45302815928464"/>interceptor, has two modes: <code>ptrace</code> and KVM. The <code>ptrace</code> (“process trace”) system call <a data-type="indexterm" data-primary="ptrace system call, gVisor" id="idm45302815926528"/>provides a mechanism for a parent process to observe and modify another process’s behavior. <code>PTRACE_SYSEMU</code> forces <a data-type="indexterm" data-primary="PTRACE_SYSEMU" id="idm45302815925136"/>the traced process to stop on entry to the next syscall, and gVisor is able to respond to it or proxy the request to the host kernel, going via Gofer if I/O is required.</p>

<figure class="width-90"><div id="runtime-gvisor-syscall-hierarchy" class="figure">
<img src="Images/haku_0316.png" alt="image" width="1411" height="993"/>
<h6><span class="label">Figure 3-16. </span>gVisor system call hierarchy</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Firecracker"><div class="sect2" id="idm45302816354848">
<h2>Firecracker</h2>

<p>Firecracker  <a data-type="indexterm" data-primary="Firecracker" data-secondary="running" id="fcracker_run"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="Firecracker" data-tertiary="running" id="sbox_fcracker_run"/>is a virtual machine monitor (VMM) that boots a dedicated VM for its guest using KVM. Instead of using KVM’s traditional device emulation pairing with QEMU, Firecracker implements its own memory management <a data-type="indexterm" data-primary="memory management" data-secondary="Firecracker and" id="idm45302815917648"/>and <a data-type="indexterm" data-primary="device emulation" data-secondary="Firecracker and" id="idm45302815916576"/>device emulation. It has no BIOS (instead implementing Linux Boot Protocol), no PCI support, and stripped down, simple, virtualized devices with a single network device, a block I/O device, timer, clock, serial console, and keyboard device that only simulates Ctrl-Alt-Del to reset the VM, as shown in <a data-type="xref" href="#runtime-firecracker-1">Figure 3-17</a>.</p>

<figure><div id="runtime-firecracker-1" class="figure">
<img src="Images/haku_0317.png" alt="image" width="470" height="469"/>
<h6><span class="label">Figure 3-17. </span>Firecracker and KVM interaction; source: <a href="https://oreil.ly/s1DjO">Resource Sharing</a></h6>
</div></figure>

<p>The Firecracker VMM process that starts the guest virtual machine is in turn started by a <em>jailer</em> process. The jailer <a data-type="indexterm" data-primary="jailer process, Firecracker" id="idm45302815910816"/>configures the security configuration of the VMM sandbox (GID and UID assignment, network namespaces, create chroot, create <code>cgroups</code>), then terminates and passes control to Firecracker, where <code>seccomp</code> is enforced around the KVM guest kernel and userspace that it boots.</p>

<p>Instead of using a second process for I/O like gVisor, Firecracker uses the KVM’s virtio drivers to proxy from the
guest’s Firecracker process to the host kernel, via the VMM (shown in <a data-type="xref" href="#runtime-firecracker-2">Figure 3-18</a>). When the Firecracker
VM image starts, it boots into protected mode in the guest kernel, never running in its real mode.</p>

<figure><div id="runtime-firecracker-2" class="figure">
<img src="Images/haku_0318.png" alt="image" width="1216" height="560"/>
<h6><span class="label">Figure 3-18. </span>Firecracker sandboxing the guest kernel from the host</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>Firecracker is compatible with Kubernetes and OCI using the <a href="https://oreil.ly/rRswg">firecracker-containerd shim</a>.</p>
</div>

<p>Firecracker invokes far less host kernel code than traditional LXC or gVisor once it has started, although they all touch similar amounts of kernel code to start their 
<span class="keep-together">sandboxes.</span></p>

<p>Performance improvements <a data-type="indexterm" data-primary="performance" data-secondary="Firecracker" id="idm45302815901888"/>are gained from an isolated memory stack, and lazily flushing data to the page cache instead of disk to increase filesystem performance. It supports arbitrary Linux binaries but does not support generic Linux kernels. It was created for AWS’s Lambda service, forked from Google’s ChromeOS VMM, <a data-type="indexterm" data-primary="crosvm" id="idm45302815900464"/>crosvm:</p>
<blockquote>
<p>What makes crosvm unique is a focus on safety within the programming language and a sandbox around the virtual devices to protect the kernel from attack in case of an exploit in the devices.</p>
<p data-type="attribution"><a href="https://oreil.ly/dbaZ5">Chrome OS Virtual Machine Monitor</a></p>
</blockquote>

<p>Firecracker is a statically linked Rust binary that is compatible with Kata Containers,
<a href="https://oreil.ly/lUQ4Y">Weave Ignite</a>, <a href="https://oreil.ly/zn0Nc">firekube</a>, and
<a href="https://oreil.ly/pluqR">firecracker-containerd</a>. It provides soft allocation (not
allocating memory until it’s actually used) for more aggressive “bin packing,” and so greater resource <a data-type="indexterm" data-primary="Firecracker" data-secondary="running" data-startref="fcracker_run" id="idm45302815894640"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="Firecracker" data-tertiary="running" data-startref="sbox_fcracker_run" id="idm45302815893392"/>utilization.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Kata Containers"><div class="sect2" id="idm45302815921776">
<h2>Kata Containers</h2>

<p>Finally, Kata Containers <a data-type="indexterm" data-primary="Kata Containers, running" id="KatCont_run"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="Kata Containers" data-tertiary="running" id="sbox_KatCont_run"/>consists of lightweight VMs containing a container engine. They are highly optimized for running
containers. They are also the oldest, and most mature, of the recent sandboxes. Compatibility is wide, with support for most container orchestrators.</p>

<p>Grown from a combination of Intel Clear Containers and Hyper.sh RunV,
Kata Containers (<a data-type="xref" href="#runtime-kata-1">Figure 3-19</a>) wraps containers with a dedicated KVM
virtual machine  and device emulation from a pluggable backend: QEMU,
QEMU-lite, NEMU (a custom stripped-down QEMU), or Firecracker.
It is an OCI runtime and so supports 
<span class="keep-together">Kubernetes.</span></p>

<figure><div id="runtime-kata-1" class="figure">
<img src="Images/haku_0319.png" alt="image" width="989" height="652"/>
<h6><span class="label">Figure 3-19. </span>Kata Containers architecture</h6>
</div></figure>

<p>The Kata Containers runtime launches each container on a guest Linux kernel. Each Linux system is on its own hardware-isolated VM, as you can see in <a data-type="xref" href="#runtime-kata-2">Figure 3-20</a>.</p>

<p>The <code>kata-runtime</code> process is the VMM, and the interface to the OCI runtime. <code>kata-proxy</code> handles I/O for the <code>kata-agent</code>
(and therefore the application) using KVM’s <code>virtio-serial</code>, and multiplexes a command channel over the same connection.</p>

<p><code>kata-shim</code> is the interface to the container engine, handling container lifecycles, signals, and logs.</p>

<figure><div id="runtime-kata-2" class="figure">
<img src="Images/haku_0320.png" alt="image" width="1196" height="731"/>
<h6><span class="label">Figure 3-20. </span>Kata Containers components</h6>
</div></figure>

<p>The guest is started using KVM and either QEMU or Firecracker. The project has forked QEMU twice to experiment with lightweight start times and has reimplemented a number of features back into QEMU, which is now preferred to NEMU (the most recent fork).</p>

<p>Inside the VM, QEMU boots an optimized kernel, and <code>systemd</code> starts the <code>kata-agent</code> process. <code>kata-agent</code>, which uses <code>libcontainer</code> and so shares a lot of code with <code>runc</code>, manages the containers running inside the VM.</p>

<p>Networking is provided by integrating with CNI (or Docker’s CNM), and a network namespace is created for each VM. Because of its networking model, the host network can’t be joined.</p>

<p>SELinux and AppArmor are not currently implemented, <a data-type="indexterm" data-primary="Kata Containers, running" data-startref="KatCont_run" id="idm45302815872160"/><a data-type="indexterm" data-primary="sandboxes" data-secondary="Kata Containers" data-tertiary="running" data-startref="sbox_KatCont_run" id="idm45302815871168"/>and some OCI inconsistencies <a href="https://oreil.ly/VUz84">limit the Docker integration</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="rust-vmm"><div class="sect2" id="idm45302815891184">
<h2>rust-vmm</h2>

<p>Many new VMM technologies<a data-type="indexterm" data-primary="rust-vmm toolkit, overview" id="rustvmm_oview"/> have some Rustlang components. So is Rust any good?</p>

<p>It is similar to Golang in that it is memory safe (memory model, virtio, etc.) but it is built atop a memory ownership model, which avoids whole classes of bugs including use after free, double free, and dangling pointer issues.</p>

<p>It has safe and simple concurrency and no garbage collector (which may incur some virtualization overhead and latency), instead using build-time analysis to find segmentation faults and memory issues.</p>

<p><a href="https://oreil.ly/vs5f7">rust-vmm</a> is a development toolkit for new VMMs as shown in <a data-type="xref" href="#runtime-rust-vmm-1">Figure 3-21</a>. It is a collection of building blocks (Rust packages, or “crates”) comprised of virtualization components. These are well tested (and therefore better secured) and provide a simple, clean interface. For example, the <code>vm-memory</code> crate is a guest memory abstraction, providing a guest address, memory regions, and guest shared memory.</p>

<figure><div id="runtime-rust-vmm-1" class="figure">
<img src="Images/haku_0321.png" alt="image" width="1282" height="401"/>
<h6><span class="label">Figure 3-21. </span>Kata Containers components; source: <a href="https://oreil.ly/s1DjO">Resource Sharing</a></h6>
</div></figure>

<p>The project was birthed from ChromeOS’s <code>cross-vm</code> (<code>crosvm</code>), which was forked by Firecracker and subsequently abstracted into “hypervisor from scratch” Rust crates. This approach will enable the development of a plug-and-play hypervisor 
<span class="keep-together">architecture.</span></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>To see how a runtime is <a data-type="indexterm" data-primary="rust-vmm toolkit, overview" data-startref="rustvmm_oview" id="idm45302815856448"/>built, you can check out <a href="https://oreil.ly/z4PmV">Youki</a>. It’s an experimental container
runtime written in Rust that implements the <code>runc</code> <a href="https://oreil.ly/MBWS0">runtime-spec</a>.</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Risks of Sandboxing"><div class="sect1" id="idm45302816423696">
<h1>Risks of Sandboxing</h1>

<p>The degree of <a data-type="indexterm" data-primary="sandboxes" data-secondary="risks" id="sbox_risks"/>access and privilege that a guest process has to host features, or virtualized versions of them, impacts the attack surface available to an attacker in control of the guest process.</p>

<p>This new tranche of sandbox technologies is under active development. It’s code, and like all new code, is at risk of exploitable bugs. This is a fact of software, however, and is infinitely better than no new software at all!</p>

<p>It may be that these sandboxes are not yet a target for attackers. The level of innovation and baseline knowledge to contribute means the barrier to entry is set high. Captain Hashjack is likely to prioritize easier targets.</p>

<p>From an administrator’s perspective, modifying or debugging applications within the sandbox becomes slightly more difficult, similar to the difference between bare metal and containerized processes. These difficulties are not insurmountable but require administrator familiarization with the underlying runtime.</p>

<p>It is still possible to run
<a href="https://oreil.ly/xxRnE">privileged sandboxes</a>
that have elevated capabilities within the guest. And although the risks are fewer than for privileged containers, users
should be aware that any reduction of isolation increases the risk of running the process inside the <a data-type="indexterm" data-primary="sandboxes" data-secondary="risks" data-startref="sbox_risks" id="idm45302815847152"/>sandbox.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Kubernetes Runtime Class"><div class="sect1" id="idm45302815845648">
<h1>Kubernetes Runtime Class</h1>

<p>Kubernetes and <a data-type="indexterm" data-primary="RuntimeClass, overview" id="idm45302815844176"/>Docker support running multiple container runtimes simultaneously;
in Kubernetes,
<a href="https://oreil.ly/dRHzA">Runtime Class</a>
is stable from v1.20 on. This means a Kubernetes worker node can host pods running
under different Container Runtime Interfaces (CRIs), which greatly enhances workload separation.</p>

<p>With <code>spec.template.spec.runtimeClassName</code> you can target a sandbox for a
Kubernetes workload via CRI.</p>

<p>Docker is able to run any OCI-compliant runtime (e.g., <code>runc</code>, <code>runsc</code>), but the
Kubernetes <code>kubelet</code> uses<a data-type="indexterm" data-primary="CRI (Container Runtime Interfaces), RuntimeClass and" id="idm45302815839712"/><a data-type="indexterm" data-primary="Container Runtime Interfaces (CRI), RuntimeClass and" id="idm45302815838880"/> CRI. While Kubernetes has not yet distinguished between
types of sandboxes, we can still set node affinity and toleration so pods are scheduled
on to nodes that have the relevant sandbox technology installed.</p>

<p>To use a new CRI runtime in Kubernetes, create a non-namespaced <code>RuntimeClass</code>:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">node.k8s.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">RuntimeClass</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">gvisor</code>  <code class="c1"># The name the RuntimeClass will be referenced by</code>
  <code class="c1"># RuntimeClass is a non-namespaced resource</code>
<code class="nt">handler</code><code class="p">:</code> <code class="l-Scalar-Plain">gvisor</code>  <code class="c1"># The name of the corresponding CRI configuration</code></pre>

<p>Then reference the CRI runtime class in the pod definition:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-gvisor-pod</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">runtimeClassName</code><code class="p">:</code> <code class="l-Scalar-Plain">gvisor</code>
  <code class="c1"># ...</code></pre>

<p>This has started a new pod using <code>gvisor</code>. Remember that <code>runsc</code> (gVisor’s runtime component) must be installed
 on the node that the pod is scheduled on.</p>
</div></section>













<section data-type="sect1" class="pagebreak-before" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45302815776368">
<h1>Conclusion</h1>

<p>Generally sandboxes are more secure, and containers are less complex.</p>

<p>When running sensitive or untrusted workloads, you want to narrow the interface between a sandboxed process and the host. There are trade-offs—debugging a rogue process becomes much harder, and traditional tracing tools may not have good 
<span class="keep-together">compatibility.</span></p>

<p>There is a general, minor performance overhead for sandboxes over containers (~50–200ms startup), which may be negligible for some workloads, and benchmarking is strongly encouraged. Options may also be limited by platform or nested virtualization options.</p>

<p>As next-generation runtimes have focused on stripping down legacy compatibility, they are very small and very fast to
start up (compared to traditional VMs)—not as fast as LXC or <code>runc</code>, but fast enough for FaaS providers to offer
aggressive scale rates.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Traditional container runtimes <a data-type="indexterm" data-primary="sandboxes" data-secondary="start times" id="idm45302815770608"/><a data-type="indexterm" data-primary="performance" data-secondary="sandboxes, start times" id="idm45302815769632"/>like LXC and <code>runc</code> are faster to start as they run a process on an existing kernel.
Sandboxes need to configure their own guest kernel, which leads to slightly longer start times.</p>
</div>

<p>Managed services are easiest to adopt, with gVisor in GKE and Firecracker <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Fargate" id="idm45302815767424"/><a data-type="indexterm" data-primary="Fargate (AWS)" id="idm45302815741440"/>in AWS Fargate.
Both of them, and Kata, will run anywhere virtualization is supported, and the future is
bright with the <code>rust-vmm</code> library promising many more runtimes to keep valuable workloads safe.</p>

<p>Segregating the most sensitive workloads on dedicated nodes in sandboxes gives your systems the greatest resistance to practical compromise.</p>
</div></section>







</div></section></div></body></html>