<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Appendix. Building Your Own Kubernetes Cluster" data-type="appendix" epub:type="appendix"><div class="appendix" id="rpi_cluster">
<h1><span class="label">Appendix. </span>Building Your Own Kubernetes Cluster</h1>
<p>While Kubernetes is often experienced through the virtual world of public cloud computing, where the closest you get to your cluster is a web browser or a terminal, it can be a very rewarding experience to physically build a Kubernetes cluster on bare metal.<a data-primary="Raspberry Pi" data-secondary="building Kubernetes cluster on" data-type="indexterm" id="ix_RaspPi"/><a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-type="indexterm" id="ix_clsbld"/> Likewise, nothing compares to physically pulling the power or network on a node and watching how Kubernetes reacts to heal your application to convince you of its utility.</p>
<p>Building your own cluster might seem like both a challenging and an expensive effort, but fortunately it is neither. The ability to purchase low-cost, system-on-chip computer boards, as well as a great deal of work by the community to make Kubernetes easier to install, means that it is possible to build a small Kubernetes cluster in a few hours.</p>
<p>In the following instructions, we focus on building a cluster of Raspberry Pi machines, but with slight adaptations, the same instructions could be made to work with a variety of different single-board machines or any other computers
you may have around.</p>
<section data-pdf-bookmark="Parts List" data-type="sect1"><div class="sect1" id="idm45664067046512">
<h1>Parts List</h1>
<p>The first thing you need to do is assemble the pieces for your cluster. In all the examples here, we assume a four-node cluster.<a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="parts list" data-type="indexterm" id="idm45664067044864"/> You could build a cluster of three nodes, or even a cluster of a hundred nodes if you wanted to, but four is a pretty good number. To start, you’ll need to purchase (or scrounge) the various pieces needed to build the cluster.</p>
<p class="pagebreak-before less_space">Here is the shopping list, with some approximate prices as of the time of 
<span class="keep-together">writing</span>:</p>
<ol>
<li>
<p>Four Raspberry Pi 4 machines with at least 2 GB of memory—$180</p>
</li>
<li>
<p>Four SDHC memory cards, at least 8 GB (buy high-quality ones!)—$30–50</p>
</li>
<li>
<p>Four 12-inch Cat. 6 Ethernet cables—$10</p>
</li>
<li>
<p>Four 12-inch USB-A to USB-C cables—$10</p>
</li>
<li>
<p>One 5-port 10/100 fast Ethernet switch—$10</p>
</li>
<li>
<p>One 5-port USB charger—$25</p>
</li>
<li>
<p>One Raspberry Pi stackable case capable of holding four Pis—$40 (or build your own)</p>
</li>
<li>
<p>One USB-to-barrel plug for powering the Ethernet switch (optional)—$5</p>
</li>
</ol>
<p>The total for the cluster comes to about $300, which you can drop down to $200 by building a three-node cluster and skipping the case and the USB power cable for the switch (though the case and the cable really clean up the whole cluster).</p>
<p>One other note on memory cards: do not scrimp here. Low-end memory cards behave unpredictably and make your cluster really unstable. If you want to save some money, buy a smaller, high-quality card. High-quality 8 GB cards can be had for around $7 each online.</p>
<p>Once you have your parts, you’re ready to move on to building the cluster.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>These instructions also assume that you have a device capable of flashing an SDHC card. If you do not, you will need to purchase a USB memory card reader/writer.</p>
</div>
</div></section>
<section data-pdf-bookmark="Flashing Images" data-type="sect1"><div class="sect1" id="idm45664067030640">
<h1>Flashing Images</h1>
<p>The default Ubuntu 20.04 image supports Raspberry Pi 4 and also is a common
operating system used by many Kubernetes clusters.<a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="flashing images" data-type="indexterm" id="idm45664067029168"/> The easiest way to install that is using the Raspberry Pi Imager provided by the <a href="https://oreil.ly/4s8Wa">Raspberry Pi project</a>:</p>
<ul>
<li>
<p><a href="https://oreil.ly/g7Lzw">macOS</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Y7CD3">Windows</a></p>
</li>
<li>
<p><a href="https://oreil.ly/u4YvC">Linux</a></p>
</li>
</ul>
<p>Use the imager to write the Ubuntu 20.04 image onto each of your memory cards. Ubuntu may not be the default image choice in the imager, but you can select it as an option.</p>
</div></section>
<section data-pdf-bookmark="First Boot" data-type="sect1"><div class="sect1" id="idm45664067021552">
<h1>First Boot</h1>
<p>The first thing to do is to boot just your API server node. Assemble your cluster, and decide which is going to be the API server node. Insert the memory card, plug the board into an HDMI output, and plug a keyboard into the USB port.<a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="first boot" data-type="indexterm" id="idm45664067019632"/></p>
<p>Next, attach the power to boot the board.</p>
<p>Log in at the prompt using the username <strong><code>ubuntu</code></strong> and the password <strong><code>ubuntu</code></strong>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The very first thing you should do with your Raspberry Pi (or any new device) is to change the default password. The default password for every type of install everywhere is well known by people who will misbehave given a default login to a system. This makes the internet less safe for everyone. Please change your default <span class="keep-together">passwords</span>!</p>
</div>
<p>Repeat these steps for each of the nodes in your cluster.</p>
<section data-pdf-bookmark="Setting Up Networking" data-type="sect2"><div class="sect2" id="idm45664067013888">
<h2>Setting Up Networking</h2>
<p>The next step is to set up networking on the API server.<a data-primary="networking" data-secondary="setting up for Kubernetes cluster" data-type="indexterm" id="idm45664067011168"/><a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="setting up networking" data-type="indexterm" id="idm45664067010096"/>
Setting up networking for a Kubernetes cluster can be complicated. In the
following example, we are setting up a network where a single machine is
attached to the internet using wireless networking; this machine is also
connected to a cluster network over wired Ethernet and provides a DHCP (Dynamic Host Configuration Protocol)
server to provide a network address to the remaining nodes in the cluster.
An illustration of this network is shown here:</p>
<figure><div class="figure">
<img alt="kur3 aain01" height="490" src="assets/kur3_aain01.png" width="1214"/>
<h6/>
</div></figure>
<p>Decide which of your boards will host the API server and <code>etcd</code>. It’s often easiest to remember which one this is by making it the top or bottom node in your stack, but some sort of label also works.</p>
<p>To do this, edit the file <em>/etc/netplan/50-cloud-init.yaml</em>. If this file doesn’t exist, you can create it. The contents of the file should look like:</p>
<pre data-type="programlisting">network:
    version: 2
    ethernets:
        eth0:
            dhcp4: false
            dhcp6: false
            addresses:
            - '10.0.0.1/24'
            optional: true
    wifis:
        wlan0:
            access-points:
                &lt;your-ssid-here&gt;:
                    password: '&lt;your-password-here&gt;'
            dhcp4: true
            optional: true</pre>
<p>This sets the main Ethernet interface to have the statically allocated address 10.0.0.1 and sets up the WiFi interface to connect to your local WiFi. You should then run <code>sudo netplan apply</code> to pick up these new changes.</p>
<p>Reboot the machine to claim the 10.0.0.1 address. You can validate that this
is set correctly by running <code>ip addr</code> and looking at the address for the
<code>eth0</code> interface. Also validate that the connection to the internet works correctly.</p>
<p>Next, we’re going to install DHCP on this API server so it will allocate addresses to
the worker nodes. Run:</p>
<pre data-type="programlisting">$ <strong>apt-get install isc-dhcp-server</strong></pre>
<p>Then configure the DHCP server as follows (<em>/etc/dhcp/dhcpd.conf</em>):</p>
<pre data-type="programlisting"># Set a domain name, can basically be anything
option domain-name "cluster.home";

# Use Google DNS by default, you can substitute ISP-supplied values here
option domain-name-servers 8.8.8.8, 8.8.4.4;

# We'll use 10.0.0.X for our subnet
subnet 10.0.0.0 netmask 255.255.255.0 {
    range 10.0.0.1 10.0.0.10;
    option subnet-mask 255.255.255.0;
    option broadcast-address 10.0.0.255;
    option routers 10.0.0.1;
}
default-lease-time 600;
max-lease-time 7200;
authoritative;</pre>
<p>You may also need to edit <em>/etc/default/isc-dhcp-server</em> to set the
<code>INTERFACES</code> environment variable to <code>eth0</code>. Restart the DHCP server with <code>sudo systemctl restart isc-dhcp-server</code>. Now your machine should be handing out IP addresses. You can test this by hooking up a second machine to the switch via Ethernet. This
second machine should get the address 10.0.0.2 from the DHCP server.</p>
<p>Remember to edit the <em>/etc/hostname</em> file to rename this machine to <code>node-1</code>. To help Kubernetes do its networking, you also need to set up <code>iptables</code> so
that it can see bridged network traffic. Create a file at <em>/etc/modules-load.d/k8s.conf</em> that just contains <code>br_netfilter</code>. This will
load the <code>br_netfilter</code> module into your kernel.</p>
<p>Next you need to enable some <code>systemctl</code> settings for network bridging and
address translation (NAT) so that Kubernetes networking will work, and your
nodes can reach the public internet. Create a file named <em>/etc/sysctl.d/k8s.conf</em> and add:</p>
<pre data-type="programlisting">net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-iptables=1</pre>
<p>Then edit <em>/etc/rc.local</em> (or the equivalent) and add <code>iptables</code> rules for forwarding from <code>eth0</code> to <code>wlan0</code> (and back):</p>
<pre data-type="programlisting">iptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE
iptables -A FORWARD -i wlan0 -o eth0 -m state \
  --state RELATED,ESTABLISHED -j ACCEPT
iptables -A FORWARD -i eth0 -o wlan0 -j ACCEPT</pre>
<p>At this point, the basic networking setup should be complete. Plug in and power up the remaining two boards (you should see them assigned the addresses 10.0.0.3 and 10.0.0.4). Edit the <em>/etc/hostname</em> file on each machine to name them <code>node-2</code> and <code>node-3</code>, respectively.</p>
<p>Validate this by first looking at <em>/var/lib/dhcp/dhcpd.leases</em>, and then SSH to the nodes (remember again to change the default password first thing). Validate that the nodes can connect to the external internet.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45664066983472">
<h5>Extra Credit</h5>
<p>There are a couple of extra steps you can take that will make it easier
to manage your cluster. The first is to edit <em>/etc/hosts</em> on each machine to map the names to the right addresses. On each machine, add:</p>
<pre data-type="programlisting">...
10.0.0.1 kubernetes
10.0.0.2 node-1
10.0.0.3 node-2
10.0.0.4 node-3
...</pre>
<p>Now you can use those names when connecting to those machines.</p>
<p>The second is to set up passwordless SSH access. To do this, run <code>ssh-keygen</code> and then copy the <em>$HOME/.ssh/id_rsa.pub</em> file into <em>/home/ubuntu/.ssh/authorized_keys</em> on <code>node-1</code>, <code>node-2</code>, and <code>node-3</code>.</p>
</div></aside>
</div></section>
<section data-pdf-bookmark="Installing a Container Runtime" data-type="sect2"><div class="sect2" id="idm45664066976800">
<h2>Installing a Container Runtime</h2>
<p>Before you can install Kubernetes, you need to install a container runtime.
There are several possible runtimes you can use, but the most broadly adopted is <code>containerd</code> from Docker.<a data-primary="containerd" data-type="indexterm" id="idm45664066974416"/><a data-primary="containers" data-secondary="installing container runtime" data-type="indexterm" id="idm45664066973680"/><a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="installing container runtime" data-type="indexterm" id="idm45664066972672"/> <code>containerd</code> is provided by the standard Ubuntu
package manager, but its version tends to lag a little bit. It’s a little
more work, but we recommend installing it from the Docker project itself.</p>
<p>The first step is to set up Docker as <a data-primary="Docker" data-secondary="installing Docker package repository" data-type="indexterm" id="idm45664066970256"/>a repository for installing packages
on your system:</p>
<pre data-type="programlisting"># Add some prerequisites
sudo apt-get install ca-certificates curl gnupg lsb-release

# Install Docker's signing key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor \
-o /usr/share/keyrings/docker-archive-keyring.gpg</pre>
<p>As a final step, create the file <em>/etc/apt/sources.list.d/docker.list</em> with
the following 
<span class="keep-together">contents</span>:</p>
<pre data-type="programlisting">deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] \
https://download.docker.com/linux/ubuntu   focal stable</pre>
<p>Now that you have installed the Docker package repository, you can install
<code>containerd.io</code> by running the following command. It is important to install
<code>containerd.io</code>, not <code>containerd</code>, to get the Docker package instead of the
default Ubuntu package:</p>
<pre data-type="programlisting">sudo apt-get update; sudo apt-get install containerd.io</pre>
<p>At this point, <code>containerd</code> is installed, but you need to configure it since
the configuration supplied by the package won’t work with Kubernetes:</p>
<pre data-type="programlisting">containerd config default &gt; config.toml
sudo mv config.toml /etc/containerd/config.toml

# Restart to pick up the config
sudo systemctl restart containerd</pre>
<p>Now that you have a container runtime installed, you can move on to installing
Kubernetes itself.</p>
</div></section>
<section data-pdf-bookmark="Installing Kubernetes" data-type="sect2"><div class="sect2" id="idm45664066976208">
<h2>Installing Kubernetes</h2>
<p>At this point you should have all nodes up with IP addresses
and capable of accessing the internet.<a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="installing Kubernetes" data-type="indexterm" id="idm45664066958960"/> Now it’s time to install
Kubernetes on all of the nodes. Using SSH, run the following commands on all nodes to install the <code>kubelet</code> and <code>kubeadm</code> tools.</p>
<p>First, add the encryption key for the packages:</p>
<pre data-type="programlisting"># <strong>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg \
| sudo apt-key add -</strong></pre>
<p>Then add the repository to your list of repositories:</p>
<pre data-type="programlisting"># <strong>echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" \
  | sudo tee /etc/apt/sources.list.d/kubernetes.list</strong></pre>
<p>Finally, update and install the Kubernetes tools. This will also update
all packages on your system for good measure:</p>
<pre data-type="programlisting"># <strong>sudo apt-get update</strong>
$ <strong>sudo apt-get upgrade</strong>
$ <strong>sudo apt-get install -y kubelet kubeadm kubectl kubernetes-cni</strong></pre>
</div></section>
<section data-pdf-bookmark="Setting Up the Cluster" data-type="sect2"><div class="sect2" id="idm45664066950768">
<h2>Setting Up the Cluster</h2>
<p>On the API <a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="setting up the cluster" data-type="indexterm" id="idm45664066949216"/>server node (the one running DHCP and connected to the internet), run:</p>
<pre data-type="programlisting">$ <strong>sudo kubeadm init --pod-network-cidr 10.244.0.0/16 \
        --apiserver-advertise-address 10.0.0.1 \
        --apiserver-cert-extra-sans kubernetes.cluster.home</strong></pre>
<p>Note that you are advertising your internal-facing IP address, not your external address.</p>
<p>Eventually, this will print out a command for joining nodes to your cluster. It will look something like:</p>
<pre data-type="programlisting">$ <strong>kubeadm join --token=<em>&lt;token&gt;</em> 10.0.0.1</strong></pre>
<p>SSH onto each of the worker nodes in your cluster and run that command.</p>
<p>When all of that is done, you should be able to run this command and see your working cluster:</p>
<pre data-type="programlisting">$ <strong>kubectl get nodes</strong></pre>
</div></section>
<section data-pdf-bookmark="Setting Up Cluster Networking" data-type="sect2"><div class="sect2" id="idm45664066941856">
<h2>Setting Up Cluster Networking</h2>
<p>You have your node-level networking set up, but you still need to set up the Pod-to-Pod networking. <a data-primary="networking" data-secondary="setting up for Kubernetes cluster" data-type="indexterm" id="idm45664066940112"/><a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-tertiary="setting up cluster networking" data-type="indexterm" id="idm45664066939072"/>Since all of the nodes in your cluster are running on the same physical Ethernet network, you can simply set up the correct routing rules in the host kernels.</p>
<p>The easiest way to manage this is to use the <a href="https://oreil.ly/ltaOv">Flannel tool</a> created by CoreOS and now supported by the <a href="https://oreil.ly/RHfMH">Flannel project</a>.
Flannel supports a number of different routing modes; we will use the <code>host-gw</code> mode. You can download an example configuration from the <a href="https://github.com/coreos/flannel">Flannel project page</a>:</p>
<pre data-type="programlisting">$ <strong>curl https://oreil.ly/kube-flannelyml \
  &gt; kube-flannel.yaml</strong></pre>
<p>The default configuration that Flannel supplies uses <code>vxlan</code> mode instead. To fix this, open up that configuration file in your favorite editor; replace <code>vxlan</code> with <code>host-gw</code>.</p>
<p>You can also do this with the <code>sed</code> tool in place:</p>
<pre data-type="programlisting">$ <strong>curl https://oreil.ly/kube-flannelyml \
  | sed "s/vxlan/host-gw/g" \
  &gt; kube-flannel.yaml</strong></pre>
<p>Once you have your updated <em>kube-flannel.yaml</em> file, you can create the Flannel networking setup with:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f kube-flannel.yaml</strong></pre>
<p>This will create two objects, a ConfigMap used to configure Flannel
and a DaemonSet that runs the actual Flannel daemon. You can inspect these with:</p>
<pre data-type="programlisting">$ <strong>kubectl describe --namespace=kube-system configmaps/kube-flannel-cfg</strong>
$ <strong>kubectl describe --namespace=kube-system daemonsets/kube-flannel-ds</strong></pre>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45664067021056">
<h1>Summary</h1>
<p>At this point, you should have a working Kubernetes cluster operating on your
Raspberry Pis. This can be great for exploring Kubernetes. Schedule some jobs,
open up the UI, and try breaking your cluster by rebooting machines or
disconnecting the network.<a data-primary="Raspberry Pi" data-secondary="building Kubernetes cluster on" data-startref="ix_RaspPi" data-type="indexterm" id="idm45664066924240"/><a data-primary="clusters" data-secondary="building your own Kubernetes cluster" data-startref="ix_clsbld" data-type="indexterm" id="idm45664066922928"/></p>
</div></section>
</div></section></div></body></html>