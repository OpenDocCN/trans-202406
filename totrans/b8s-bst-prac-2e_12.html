<html><head></head><body><section data-pdf-bookmark="Chapter 12. Managing Multiple Clusters" data-type="chapter" epub:type="chapter"><div class="chapter" id="managing_multiple_clusters">&#13;
<h1><span class="label">Chapter 12. </span>Managing Multiple Clusters</h1>&#13;
&#13;
&#13;
<p>In this chapter, we<a data-primary="clusters" data-secondary="multiple" data-see="multiple clusters" data-type="indexterm" id="id853"/> discuss best practices for managing multiple&#13;
Kubernetes clusters. We dive into the details of the differences between&#13;
multicluster management and federation, tools to manage multiple&#13;
clusters, and operational patterns for managing multiple clusters.</p>&#13;
&#13;
<p>You might wonder why you would need multiple Kubernetes&#13;
clusters. Kubernetes was built to consolidate many&#13;
workloads to a single cluster, correct? This is true, but there are scenarios that might require multiple clusters,&#13;
such as workloads across regions, concerns of blast radius, regulatory&#13;
compliance, and specialized workloads.</p>&#13;
&#13;
<p>We discuss these scenarios and explore the tools and techniques for&#13;
managing multiple clusters in Kubernetes.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Why Multiple Clusters?" data-type="sect1"><div class="sect1" id="id101">&#13;
<h1>Why Multiple Clusters?</h1>&#13;
&#13;
<p>When <a data-primary="multiple clusters" data-secondary="reasons for using" data-type="indexterm" id="multiple-cluster-reasons"/>adopting Kubernetes, you will likely have more than one cluster, and&#13;
you might even start with more than one cluster to break out production&#13;
from staging, user acceptance testing (UAT), or development. Kubernetes provides some&#13;
multitenancy features with namespaces, which are a logical way to break up&#13;
a cluster into smaller logical constructs. Namespaces allow you to&#13;
define Role-Based Access Control (RBAC), quotas, pod security policies, and&#13;
network policies to allow separation of workloads. This is a great way&#13;
to separate multiple teams and projects, but there are other&#13;
concerns that might require you to build a multicluster architecture.&#13;
Concerns to think about when deciding to use multicluster&#13;
versus a single-cluster architecture:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Blast radius</p>&#13;
</li>&#13;
<li>&#13;
<p>Compliance</p>&#13;
</li>&#13;
<li>&#13;
<p>Security</p>&#13;
</li>&#13;
<li>&#13;
<p>Hard multitenancy</p>&#13;
</li>&#13;
<li>&#13;
<p>Regional-based workloads</p>&#13;
</li>&#13;
<li>&#13;
<p>Specialized workloads</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>When thinking through your architecture, <em>blast radius</em> should come front&#13;
and center. This is one of the main concerns that we see with users designing&#13;
for multicluster architectures. With microservice architectures we&#13;
employ circuit breakers, retries, bulkheads, and rate limiting to constrain the extent&#13;
of damage to our systems. You should design the same into your&#13;
infrastructure layer, and multiple clusters can help with preventing the&#13;
impact of cascading failures due to software issues. For example, if you&#13;
have one cluster that serves 500 applications and you have a&#13;
platform issue, it takes out 100% of the 500 applications. If you&#13;
had a platform layer issue with five clusters serving those 500&#13;
applications, you affect only 20% of the applications. The downside&#13;
to this is that now you need to manage five clusters, and your consolidation&#13;
ratios will not be as good as with a single cluster. Dan Woods wrote a great <a href="https://oreil.ly/YnGUD">article</a> about an actual cascading failure in a production Kubernetes&#13;
environment. It is a great example of why you will&#13;
want to consider multicluster architectures for larger <span class="keep-together">environments.</span></p>&#13;
&#13;
<p><em>Compliance</em> is <a data-primary="compliance with multiple clusters" data-type="indexterm" id="id854"/>another area of concern for multicluster design because&#13;
there are special considerations for Payment Card Industry (PCI), Health Insurance Portability and Accountability (HIPAA), and other workloads. It’s&#13;
not that Kubernetes doesn’t provide some multitenant features, but&#13;
these workloads might be easier to manage if they are segregated from&#13;
general purpose workloads. These compliant workloads might have specific&#13;
requirements with respect to security hardening, nonshared components, or&#13;
dedicated workload requirements. It’s just much easier to separate these&#13;
workloads than to have to treat the cluster in such a specialized&#13;
fashion.</p>&#13;
&#13;
<p><em>Security</em> in <a data-primary="security" data-secondary="multiple clusters" data-type="indexterm" id="id855"/>large Kubernetes clusters can become difficult to manage. As you start onboarding more and more teams to a Kubernetes cluster, each team may have different security requirements, and it can become very difficult to meet those needs in a large multitenant cluster. Even just managing RBAC, network policies, and pod security policies can become difficult at scale in a single cluster. A small change to a network policy can inadvertently open up security risk to other users of the cluster. With multiple clusters you can limit the security impact with a misconfiguration. If you decide that a larger Kubernetes cluster fits your requirements, then ensure that you have a very good operational process for making security changes and that you understand the blast radius of making a change to RBAC, network policy, and pod security policies.</p>&#13;
&#13;
<p>Kubernetes doesn’t provide <em>hard multitenancy</em> because<a data-primary="hard multitenancy" data-type="indexterm" id="id856"/><a data-primary="multitenancy, hard" data-type="indexterm" id="id857"/> it shares the&#13;
same API boundary with all workloads running within the cluster. With&#13;
namespacing this gives us good soft multitenancy, but not enough to&#13;
protect against hostile workloads within the cluster. Hard multitenancy&#13;
is not a requirement for a lot of users; they trust the workloads that&#13;
will be running within the cluster. Hard multitenancy is typically a&#13;
requirement if you are a cloud provider, hosting software as a service (SaaS)-based software, or&#13;
hosting untrusted workloads with untrusted user control.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The Kubernetes project does address hard multitenancy concerns with Virtual Clusters,<a data-primary="Virtual Clusters" data-type="indexterm" id="id858"/> outside the scope of the book. Find more information on the <a href="https://oreil.ly/KlFlK">project’s GitHub</a>.</p>&#13;
</div>&#13;
&#13;
<p>When running<a data-primary="regional distribution" data-type="indexterm" id="id859"/> workloads that need to serve traffic from in-region&#13;
endpoints, your design will include multiple clusters that are based per&#13;
region. When you have a globally distributed application, it becomes a&#13;
requirement at that point to run multiple clusters. When you have&#13;
workloads that need to be <em>regionally distributed</em>, it’s a great use case&#13;
for cluster federation of multiple clusters, which we dig into&#13;
further later in this chapter.</p>&#13;
&#13;
<p><em>Specialized workloads</em>, such <a data-primary="specialized workloads" data-type="indexterm" id="id860"/>as high-performance computing (HPC), machine learning (ML), and grid computing, also need&#13;
to be addressed in the multicluster architecture. These types of&#13;
specialized workloads might require specific types of hardware, have&#13;
unique performance profiles, and have specialized users of the clusters.&#13;
We’ve seen this use case to be less prevalent in the design decision because&#13;
having multiple Kubernetes node pools can help address specialized&#13;
hardware and performance profiles. When you need a very large&#13;
cluster for an HPC or machine learning workload, you should consider just dedicating clusters for these <span class="keep-together">workloads.</span></p>&#13;
&#13;
<p>With multicluster, you get isolation for “free,” but it also has design&#13;
concerns that you need to address at<a data-primary="multiple clusters" data-secondary="reasons for using" data-startref="multiple-cluster-reasons" data-type="indexterm" id="id861"/> the outset.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multicluster Design Concerns" data-type="sect1"><div class="sect1" id="id102">&#13;
<h1>Multicluster Design Concerns</h1>&#13;
&#13;
<p>When choosing<a data-primary="multiple clusters" data-secondary="designing" data-type="indexterm" id="multiple-cluster-design"/><a data-primary="designing" data-secondary="multiple clusters" data-type="indexterm" id="design-multiple-cluster"/> a multicluster design there are some challenges that you’ll&#13;
run into. Some of these challenges might deter you from attempting a&#13;
multicluster design given that the design might overcomplicate your&#13;
architecture. Some of the common challenges we find users running into&#13;
are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Data replication</p>&#13;
</li>&#13;
<li>&#13;
<p>Service discovery</p>&#13;
</li>&#13;
<li>&#13;
<p>Network routing</p>&#13;
</li>&#13;
<li>&#13;
<p>Operational management</p>&#13;
</li>&#13;
<li>&#13;
<p>Continuous deployment</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><em>Data replication</em> and <a data-primary="data replication" data-type="indexterm" id="id862"/>consistency have always been the crux of deploying workloads across geographical regions and multiple clusters. When&#13;
running these services, you need to decide what runs where and develop a replication strategy. Most databases have built-in tools to&#13;
perform the replication, but you need to design the application to be able to handle the replication strategy. For NoSQL-type database&#13;
services this can be easier because they can handle scaling across&#13;
multiple instances, but you still need to ensure that your application can&#13;
handle eventual consistency across geographic regions or at least the&#13;
latency across regions. Some cloud services, such as Google Cloud Spanner&#13;
and Microsoft Azure CosmosDB, have built database services to help with the&#13;
complications of handling data across multiple geographic regions.</p>&#13;
&#13;
<p>Each Kubernetes cluster deploys its<a data-primary="service discovery" data-type="indexterm" id="id863"/> own <em>service discovery</em> registry,&#13;
and registries are not synchronized across multiple clusters. This complicates&#13;
applications being able to easily identify and discover one another.&#13;
Tools such as HashiCorp’s Consul can transparently synchronize services&#13;
from multiple clusters and even services that reside outside of&#13;
Kubernetes. Other tools like Istio, Linkerd, and Cilium are building on multiple cluster architectures to extend service&#13;
discovery between <span class="keep-together">clusters.</span></p>&#13;
&#13;
<p>Kubernetes makes networking from within the cluster very easy, as it’s a&#13;
flat network and avoids using network address translation (NAT). If you need&#13;
to route traffic in and out of the cluster, this becomes more&#13;
complicated. Ingress into the cluster is implemented as a 1:1 mapping of&#13;
ingress to the cluster because it doesn’t support multicluster topologies&#13;
with the Ingress resource. You’ll also need to consider the egress&#13;
traffic between clusters and how to route that traffic. When your&#13;
applications reside within a single cluster this is easy, but when introducing multicluster, you need to think about the latency of extra&#13;
hops for services that have application dependencies in another cluster.&#13;
For applications that have tightly coupled dependencies, you should&#13;
consider running these services within the same cluster to remove&#13;
latency and extra complexity.</p>&#13;
&#13;
<p>One of the biggest overheads to managing multiclusters is the&#13;
<em>operational management</em>. Instead<a data-primary="operational management of multiple clusters" data-type="indexterm" id="id864"/> of one or a couple of clusters to&#13;
manage and keep consistent, you might now have many clusters to manage in&#13;
your environment. One of the most important aspects to managing multiclusters is ensuring that you have good automation practices in place because&#13;
this will help to reduce the operational burden. When automating your&#13;
clusters, you need to take into account the infrastructure deployment and&#13;
managing add-on features to your clusters. For managing the&#13;
infrastructure, using a tool like HashiCorp’s Terraform can help with&#13;
deploying and managing a consistent state across your fleet of clusters.</p>&#13;
&#13;
<p>Using<a data-primary="Infrastructure as Code (IaC) tools" data-type="indexterm" id="id865"/><a data-primary="IaC (Infrastructure as Code) tools" data-type="indexterm" id="id866"/> an <em>Infrastructure as Code</em> (IaC) tool like Terraform will give you the&#13;
benefit of providing a reproducible way to deploy your&#13;
clusters. On the other hand, you also need to be able to consistently&#13;
manage add-ons to the cluster, such as monitoring, logging, ingress,&#13;
security, and other tools. Security is another important aspect of&#13;
operational management, and you must be able to maintain security policies,&#13;
RBAC, and network policies across clusters. Later&#13;
in this chapter, we dive deeper into the topic of maintaining&#13;
consistent clusters with automation.</p>&#13;
&#13;
<p>With<a data-primary="CD (continuous delivery)" data-secondary="multiple clusters" data-type="indexterm" id="id867"/> multiple clusters and continuous delivery (CD), you now need to deal&#13;
with multiple Kubernetes API endpoints versus a single API endpoint. This&#13;
can cause challenges in the distribution of applications. You can easily&#13;
manage multiple pipelines, but suppose that you have a hundred&#13;
different pipelines to manage, which can make application distribution&#13;
very difficult. With this in mind, you need to look at different approaches to&#13;
managing this situation. We take a look at solutions to help manage this&#13;
later in the <a data-primary="multiple clusters" data-secondary="designing" data-startref="multiple-cluster-design" data-type="indexterm" id="id868"/><a data-primary="designing" data-secondary="multiple clusters" data-startref="design-multiple-cluster" data-type="indexterm" id="id869"/>chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Managing Multiple Cluster Deployments" data-type="sect1"><div class="sect1" id="id103">&#13;
<h1>Managing Multiple Cluster Deployments</h1>&#13;
&#13;
<p>One of <a data-primary="multiple clusters" data-secondary="managing" data-tertiary="deployments" data-type="indexterm" id="multiple-cluster-manage-deploy"/><a data-primary="deployments" data-secondary="multiple clusters, managing" data-type="indexterm" id="deploy-multiple-cluster"/>the first steps you want to take when managing multicluster&#13;
deployments is to use an IaC tool like Terraform&#13;
to set up deployments. Other deployment tools, such as kubespray, kops, or other cloud&#13;
provider–specific tools, are all valid choices, but, most importantly, use a&#13;
tool that allows you to source control your cluster deployment for&#13;
repeatability.</p>&#13;
&#13;
<p>Automation is key to successfully managing multiple clusters in your&#13;
environment. You might not have everything automated on day one, but you should&#13;
make it a priority to automate all aspects of your cluster deployments&#13;
and operations.</p>&#13;
&#13;
<p>An interesting project<a data-primary="Cluster API" data-type="indexterm" id="id870"/> is the <a href="https://oreil.ly/edzIa">Kubernetes Cluster API</a>, a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. It provides optional additive functionality on top of core Kubernetes. The Cluster API provides a cluster-level configuration declared through a common API, which will give you the ability to easily automate and build tooling around cluster automation. The Cluster API is still in its early stages, but it’s a project to keep an eye on.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deployment and Management Patterns" data-type="sect1"><div class="sect1" id="id104">&#13;
<h1>Deployment and Management Patterns</h1>&#13;
&#13;
<p>Kubernetes operators <a data-primary="infrastructure as Software" data-type="indexterm" id="id871"/>were introduced as an implementation of the <em>Infrastructure as Software</em> concept. Using them allows you to abstract the deployment of applications and services in a Kubernetes cluster. For example, suppose that you want to standardize on Prometheus for monitoring your Kubernetes clusters. You would need to create and manage various objects (deployment, service, ingress, etc.) for each cluster and team. You would also need to maintain the fundamental configurations of Prometheus, such as versions, persistence, retention policies, and replicas. As you can imagine, the maintenance of such a solution could be difficult across a large number of clusters and teams.</p>&#13;
&#13;
<p>Instead of dealing with so many objects and configurations, you could install the <code>prometheus-operator</code>. This extends the Kubernetes API, exposing multiple new object kinds called <code>Prometheus</code>, <code>ServiceMonitor</code>, <code>PrometheusRule</code>, and <code>AlertManager</code>, which allow you to specify all the details of a Prometheus deployment using just a few objects. You can use the <code>kubectl</code> tool to manage such objects, just as it manages any other Kubernetes API object.</p>&#13;
&#13;
<p><a data-type="xref" href="#figure13-1">Figure 12-1</a> shows the architecture of the <code>prometheus-operator</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure13-1">&#13;
<img alt="prometheus-operator architecture" src="assets/kbp2_1201.png"/>&#13;
<h6><span class="label">Figure 12-1. </span><code>prometheus-operator</code> architecture</h6>&#13;
</div></figure>&#13;
&#13;
<p>Utilizing <a data-primary="Operator pattern" data-type="indexterm" id="operator-pattern"/>the <em>Operator</em> pattern for automating key operational tasks can&#13;
help improve your overall cluster management capabilities. The Operator&#13;
pattern was introduced by the CoreOS team in 2016 with the etcd operator&#13;
and <code>prometheus-operator</code>. The Operator pattern builds on two concepts:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Custom resource definitions</p>&#13;
</li>&#13;
<li>&#13;
<p>Custom controllers</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><em>Custom resource definitions</em> (CRDs) are<a data-primary="CRDs (custom resource definitions)" data-secondary="definition of" data-type="indexterm" id="id872"/> objects that allow you to extend the&#13;
Kubernetes API, based on your own API that you define.</p>&#13;
&#13;
<p><em>Custom controllers</em> are<a data-primary="custom controllers" data-type="indexterm" id="id873"/> built on the core Kubernetes concepts of&#13;
resources and controllers. Custom controllers allow you to build your own logic by watching events from Kubernetes API objects such as namespaces, Deployments, pods, or your own CRD. With custom controllers, you can build your CRDs in a declarative way. If you consider how the Kubernetes Deployment controller works in a reconciliation loop to always maintain the state of the Deployment object to maintain its declarative state, this brings the same advantages of controllers to your CRDs.</p>&#13;
&#13;
<p>When utilizing the Operator pattern, you can build in automation to&#13;
operational tasks that need to be performed on operational tooling in&#13;
multiclusters. Let’s take the following <a href="https://oreil.ly/9WvJQ">Elasticsearch operator</a> as an&#13;
example. The Elasticsearch&#13;
operator can perform the following operations:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Replicas for master, client, and data nodes</p>&#13;
</li>&#13;
<li>&#13;
<p>Zones for highly available deployments</p>&#13;
</li>&#13;
<li>&#13;
<p>Volume sizes for master and data nodes</p>&#13;
</li>&#13;
<li>&#13;
<p>Resizing of cluster</p>&#13;
</li>&#13;
<li>&#13;
<p>Snapshot for backups of the Elasticsearch cluster</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As you can see, the operator provides automation for many tasks that you&#13;
would need to perform when managing Elasticsearch, such as automating&#13;
snapshots for backup and resizing the cluster. The beauty of this is that you&#13;
manage everything through familiar Kubernetes objects.</p>&#13;
&#13;
<p>Think about how you can take advantage of different operators like the&#13;
<code>prometheus-operator</code> in your environment and also how you can build your&#13;
own custom operator to offload common <a data-primary="multiple clusters" data-secondary="managing" data-startref="multiple-cluster-manage-deploy" data-tertiary="deployments" data-type="indexterm" id="id874"/><a data-primary="deployments" data-secondary="multiple clusters, managing" data-startref="deploy-multiple-cluster" data-type="indexterm" id="id875"/><a data-primary="Operator pattern" data-startref="operator-pattern" data-type="indexterm" id="id876"/>operational tasks.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The GitOps Approach to Managing Clusters" data-type="sect1"><div class="sect1" id="id227">&#13;
<h1>The GitOps Approach to Managing Clusters</h1>&#13;
&#13;
<p><em>GitOps</em> was <a data-primary="multiple clusters" data-secondary="managing" data-tertiary="GitOps" data-type="indexterm" id="multiple-cluster-manage-gitops"/><a data-primary="GitOps" data-secondary="multiple cluster management" data-type="indexterm" id="gitops-multiple-cluster-manage"/>popularized by the folks at Weaveworks, and the idea and&#13;
fundamentals were based on their experience of running Kubernetes in&#13;
production. GitOps takes the concepts of the software&#13;
development life cycle and applies them to operations. With GitOps, your&#13;
Git repository becomes your source of truth, and your cluster is&#13;
synchronized to the configured Git repository. For example, if you&#13;
update a Kubernetes Deployment manifest, those configuration changes are&#13;
automatically reflected in the cluster state.</p>&#13;
&#13;
<p>By using this method, you can make it easier to maintain multiclusters&#13;
that are consistent and avoid configuration drift across the fleet.&#13;
GitOps allows you to declaratively describe your clusters for&#13;
multiple environments and drives to maintain that state for the cluster.&#13;
The practice of GitOps can apply to both application delivery and&#13;
operations, but in this chapter, we focus on using it to manage&#13;
clusters and operational tooling.</p>&#13;
&#13;
<p>Weaveworks Flux was one of the first tools to enable the GitOps approach, and it’s the tool we will use throughout the rest of the chapter. There are many&#13;
new tools that have been released into the cloud native ecosystem that are&#13;
worth a look, such as Argo CD, from the folks at Intuit, which has also&#13;
been widely adopted for the GitOps approach.</p>&#13;
&#13;
<p>We’ll get into a deeper dive of utilizing a GitOps model in <a data-type="xref" href="ch18.html#gitops">Chapter 18</a>, but the following provides a quick glance at the benefit of utilizing GitOps for cluster management.</p>&#13;
&#13;
<p><a data-type="xref" href="#gitops_workflow">Figure 12-2</a> presents a representation of a GitOps workflow.</p>&#13;
&#13;
<figure><div class="figure" id="gitops_workflow">&#13;
<img alt="GitOps workflow" src="assets/kbp2_1202.png"/>&#13;
<h6><span class="label">Figure 12-2. </span>GitOps workflow</h6>&#13;
</div></figure>&#13;
&#13;
<p>So, let’s get Flux set up in your cluster and get a repository synchronized to&#13;
the <span class="keep-together">cluster:</span></p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">git<code class="w"> </code>clone<code class="w"> </code>https://github.com/weaveworks/flux<code class="w"/>&#13;
<code class="nb">cd</code><code class="w"> </code>flux<code class="w"/></pre>&#13;
&#13;
<p>You now need to change the Deployment manifest to&#13;
configure it with your forked repo from <a data-type="xref" href="ch05.html#continuous_integration_testing_and_deployment">Chapter 5</a>. Modify the following line in the Deployment file to match your forked GitHub repository:</p>&#13;
<pre>vim deploy/flux-deployment.yaml</pre>&#13;
&#13;
<p>Modify the following line with your Git repository:</p>&#13;
<pre>--git-url=git@github.com:weaveworks/flux-get-started&#13;
  (ex. --git-url=git@github.com:your_repo/kbp )</pre>&#13;
&#13;
<p>Now, go ahead and deploy Flux to your cluster:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>apply<code class="w"> </code>-f<code class="w"> </code>deploy<code class="w"/></pre>&#13;
&#13;
<p>When Flux installs, it creates an SSH key so that it can authenticate with the&#13;
Git repository. Use the Flux command-line tool to retrieve the SSH&#13;
key so that you can configure access to your forked repository; first, you need to install&#13;
<code>fluxctl</code>.</p>&#13;
&#13;
<p>For macOS:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">brew<code class="w"> </code>install<code class="w"> </code>fluxctl<code class="w"/></pre>&#13;
&#13;
<p>For Linux Snap Packages:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">snap<code class="w"> </code>install<code class="w"> </code>fluxctl<code class="w"/></pre>&#13;
&#13;
<p>For all other packages, you can find the <a href="https://oreil.ly/4TAx5">latest binaries here</a>:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">fluxctl<code class="w"> </code>identity<code class="w"/></pre>&#13;
&#13;
<p>Open GitHub, navigate to your fork, go to Setting &gt; “Deploy keys,”&#13;
click “Add deploy key,” give it a Title, select the “Allow write access” checkbox,&#13;
paste the Flux public key, and then click “Add key.” See the <a href="https://oreil.ly/Oet57">GitHub documentation</a> for&#13;
more information on how to manage deploy keys.</p>&#13;
&#13;
<p>Now if you view the Flux logs, you should see that it is synchronizing&#13;
with your GitHub repository:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>-n<code class="w"> </code>default<code class="w"> </code>logs<code class="w"> </code>deployment/flux<code class="w"> </code>-f<code class="w"/></pre>&#13;
&#13;
<p>After you see that it’s synchronizing with your GitHub repository, you should see&#13;
that the Elasticsearch, Prometheus, Redis, and frontend pods are created:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>get<code class="w"> </code>pods<code class="w"> </code>-w<code class="w"/></pre>&#13;
&#13;
<p>With this example complete, you should be able to see how easy it is for you to synchronize your GitHub repository state with your Kubernetes cluster. This&#13;
makes managing the multiple operational tools in your cluster much&#13;
easier, because multiple clusters can synchronize with a single repository and&#13;
you avoid the situation of having snowflake <a data-primary="multiple clusters" data-secondary="managing" data-startref="multiple-cluster-manage-gitops" data-tertiary="GitOps" data-type="indexterm" id="id877"/><a data-primary="GitOps" data-secondary="multiple cluster management" data-startref="gitops-multiple-cluster-manage" data-type="indexterm" id="id878"/>clusters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multicluster Management Tools" data-type="sect1"><div class="sect1" id="id105">&#13;
<h1>Multicluster Management Tools</h1>&#13;
&#13;
<p>When <a data-primary="multiple clusters" data-secondary="managing" data-tertiary="tools for" data-type="indexterm" id="multiple-cluster-manage-tools"/>working with multiple clusters, using <code>kubectl</code> can immediately become confusing because you need to set different contexts to manage&#13;
the different clusters. Two tools that you will want to install right away&#13;
when dealing with multiple clusters are <em>kubectx</em> and <em>kubens</em>, which allow you to easily change between multiple contexts and namespaces.</p>&#13;
&#13;
<p>When you need a full-fledged multicluster management tool, there are a&#13;
few within the Kubernetes ecosystem to look at for managing multiple&#13;
clusters. Following is a summary of some of the more popular tools:</p>&#13;
<dl>&#13;
<dt><a href="https://oreil.ly/8qGNh">Rancher</a></dt>&#13;
<dd>&#13;
<p>Rancher <a data-primary="Rancher" data-type="indexterm" id="id879"/>centrally manages multiple Kubernetes clusters in a centrally managed UI. It monitors, manages, backs up, and restores Kubernetes&#13;
clusters across on-premises, cloud, and hosted Kubernetes setups. It also has&#13;
tools for controlling applications deployed across multiple clusters and&#13;
provides operational tooling.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/HUv5k">Open Cluster Management</a> (OCM)</dt>&#13;
<dd>&#13;
<p>OCM is<a data-primary="OCM (Open Cluster Management)" data-type="indexterm" id="id880"/> a community-driven project focused on multicluster and multicloud scenarios for Kubernetes apps. It provides cluster registration, workload distribution, and dynamic placement of policies and workloads.</p>&#13;
</dd>&#13;
<dt><a href="https://oreil.ly/fElD5">Gardener</a></dt>&#13;
<dd>&#13;
<p>Gardener <a data-primary="Gardener" data-type="indexterm" id="id881"/>takes a different approach to multicluster management in that it&#13;
utilizes Kubernetes primitives to provide Kubernetes as a Service to&#13;
your end users. It provides support for all major cloud vendors and was&#13;
developed by the folks at SAP. This solution is geared to users who are building a Kubernetes as a Service <a data-primary="multiple clusters" data-secondary="managing" data-startref="multiple-cluster-manage-tools" data-tertiary="tools for" data-type="indexterm" id="id882"/>offering.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Federation" data-type="sect1"><div class="sect1" id="id106">&#13;
<h1>Kubernetes Federation</h1>&#13;
&#13;
<p>Kubernetes first<a data-primary="multiple clusters" data-secondary="Federation" data-type="indexterm" id="id883"/><a data-primary="Federation" data-type="indexterm" id="id884"/> introduced Federation v1 in Kubernetes 1.3, and it has&#13;
since been deprecated in lieu of Federation v2. Federation v1 set&#13;
out to help with the distribution of applications to multiple clusters.&#13;
Federation v1 was built utilizing the Kubernetes API and heavily&#13;
relied on Kubernetes annotations, which imposed some problems in its&#13;
design. The design was tightly coupled to the core Kubernetes API, which&#13;
made Federation v1 quite monolithic. At the time, the design&#13;
decisions were probably not bad choices, but they were built on the&#13;
primitives that were available. The introduction of Kubernetes CRDs allowed a different way of thinking about how Federation could be designed.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Managing Multiple Clusters Best Practices" data-type="sect1"><div class="sect1" id="id228">&#13;
<h1>Managing Multiple Clusters Best Practices</h1>&#13;
&#13;
<p>Consider the<a data-primary="multiple clusters" data-secondary="managing" data-tertiary="best practices" data-type="indexterm" id="multiple-cluster-manage-best-practice"/><a data-primary="best practices" data-secondary="multiple cluster management" data-type="indexterm" id="best-practice-multiple-cluster"/> following best practices when managing multiple Kubernetes clusters:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Limit the blast radius of your clusters to ensure cascading failures&#13;
don’t have a bigger impact on your applications.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you have regulatory concerns such as PCI, HIPPA, or HiTrust, think about utilizing multiclusters to ease the complexity of mixing&#13;
these workloads with general workloads.</p>&#13;
</li>&#13;
<li>&#13;
<p>If hard multitenancy is a business requirement, workloads should be deployed to a dedicated cluster.</p>&#13;
</li>&#13;
<li>&#13;
<p>If multiple regions are needed for your applications, utilize a Global&#13;
Load Balancer to manage traffic between clusters.</p>&#13;
</li>&#13;
<li>&#13;
<p>You can break out specialized workloads such as HPC into their own&#13;
individual clusters to ensure that the specialized needs for the workloads&#13;
are met.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you’re deploying workloads that will be spread across multiple regional&#13;
datacenters, first ensure there is a data replication strategy for the&#13;
workload. Multiple clusters across regions can be easy, but replicating&#13;
data across regions can be complicated, so ensure there is a sound&#13;
strategy to handle asynchronous and synchronous workloads.</p>&#13;
</li>&#13;
<li>&#13;
<p>Utilize Kubernetes operators like the <code>prometheus-operator</code> or Elasticsearch operator to handle automated operational tasks.</p>&#13;
</li>&#13;
<li>&#13;
<p>When designing your multicluster strategy, also consider how you will&#13;
implement service discovery and networking between clusters. Service mesh tools&#13;
like HashiCorp’s Consul or Istio can help with networking across&#13;
clusters.</p>&#13;
</li>&#13;
<li>&#13;
<p>Be sure that your CD strategy can handle&#13;
multiple rollouts between regions or multiple clusters.</p>&#13;
</li>&#13;
<li>&#13;
<p>Investigate utilizing a GitOps approach to managing multiple cluster&#13;
operational components to ensure consistency between all clusters in&#13;
your fleet. The GitOps approach doesn’t work for everyone’s&#13;
environment, but you should at least investigate it to ease the operational&#13;
burden of multicluster <a data-primary="multiple clusters" data-secondary="managing" data-startref="multiple-cluster-manage-best-practice" data-tertiary="best practices" data-type="indexterm" id="id885"/><a data-primary="best practices" data-secondary="multiple cluster management" data-startref="best-practice-multiple-cluster" data-type="indexterm" id="id886"/>environments.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id367">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we discussed different strategies for managing multiple Kubernetes&#13;
clusters. It’s important&#13;
to think about your needs at the outset and whether those needs match&#13;
a multicluster topology. The first scenario to think about is whether you truly need <em>hard</em> multitenancy because this will automatically require&#13;
a multicluster strategy. If you don’t, consider your compliance&#13;
needs and whether you have the operational capacity to consume the overhead&#13;
of multicluster architectures. Finally, if you’re going with more, smaller&#13;
clusters, ensure that you automate their delivery and management to reduce the operational burden.</p>&#13;
</div></section>&#13;
</div></section></body></html>