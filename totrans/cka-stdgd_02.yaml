- en: Chapter 2\. Cluster Architecture, Installation, and Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the name of the chapter, the first section of the curriculum refers
    to typical tasks you’d expect of a Kubernetes administrator. Those tasks include
    understanding the architectural components of a Kubernetes cluster, setting up
    a cluster from scratch, and maintaining a cluster going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, this section also covers the security aspects of a cluster, more
    specifically role-based access control (RBAC). You are expected to understand
    how to map permissions for operations to API resources for a set of users or processes.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, you will understand the tools and procedures for
    installing and maintaining a Kubernetes cluster. Moreover, you’ll know how to
    configure RBAC for representative, real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, this chapter covers the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding RBAC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing of a cluster with `kubeadm`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading a version of a Kubernetes cluster with `kubeadm`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing up and restoring etcd with `etcdctl`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding a highly available Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Role-Based Access Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes you need to be authenticated before you are allowed to make a
    request to an API resource. A cluster administrator usually has access to all
    resources and operations. The easiest way to operate a cluster is to provide everyone
    with an admin account. While “admin access for everyone” sounds fantastic as you
    grow your business, it comes with a considerable amount of risk. Users may accidentally
    delete a Secret Kubernetes object, which likely breaks one or many applications
    and therefore has a tremendous impact on end users. As you can imagine, this approach
    is not a good idea for production environments that run mission-critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: As with other production systems, only certain users should have full access,
    whereas the majority of users have read-only access (and potentially access to
    mutate the system) depending on the role. For example, application developers
    do not need to manage cluster nodes. They only need to tend to the objects required
    to run and configure their application.
  prefs: []
  type: TYPE_NORMAL
- en: RBAC defines policies for users, groups, and processes by allowing or disallowing
    access to manage API resources. Enabling and configuring RBAC is mandatory for
    any organization with a strong emphasis on security. For the exam, you need to
    understand the involved RBAC API resource types and how to create and configure
    them in different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: RBAC High-Level Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RBAC helps with implementing a variety of use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a system for users with different roles to access a set of Kubernetes
    resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling processes running in a Pod and the operations they can perform via
    the Kubernetes API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting the visibility of certain resources per namespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RBAC consists of three key building blocks, as shown in [Figure 2-1](#rbac_key_building_blocks).
    Together, they connect API primitives and their allowed operations to the so-called
    subject, which is a user, a group, or a ServiceAccount.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list breaks down the responsibilities by terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: Subject
  prefs: []
  type: TYPE_NORMAL
- en: The user or process that wants to access a resource
  prefs: []
  type: TYPE_NORMAL
- en: Resource
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes API resource type (e.g., a Deployment or node)
  prefs: []
  type: TYPE_NORMAL
- en: Verb
  prefs: []
  type: TYPE_NORMAL
- en: The operation that can be executed on the resource (e.g., creating a Pod or
    deleting a Service)
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0201](Images/ckas_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. RBAC key building blocks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Creating a Subject
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of RBAC, you can use a user account, service account, or a group
    as a subject. Users and groups are not stored in etcd, the Kubernetes database,
    and are meant for processes running outside of the cluster. Service accounts exists
    as objects in Kubernetes and are used by processes running inside of the cluster.
    In this section, you’ll learn how to create them.
  prefs: []
  type: TYPE_NORMAL
- en: User accounts and groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes does not represent a user as with an API resource. The user is meant
    to be managed by the administrator of a Kubernetes cluster, which then distributes
    the credentials of the account to the real person or to be used by an external
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Calls to the API server with a user need to be authenticated. Kubernetes offers
    a variety of authentication methods for those API requests. [Table 2-1](#authentication_strategies_for_managing_rbac_subjects)
    shows different ways of authenticating RBAC subjects.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Authentication strategies for managing RBAC subjects
  prefs: []
  type: TYPE_NORMAL
- en: '| Authentication strategy | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X.509 client certificate | Uses an OpenSSL client certificate to authenticate
    |'
  prefs: []
  type: TYPE_TB
- en: '| Basic authentication | Uses username and password to authenticate |'
  prefs: []
  type: TYPE_TB
- en: '| Bearer tokens | Uses OpenID (a flavor of OAuth2) or webhooks as a way to
    authenticate |'
  prefs: []
  type: TYPE_TB
- en: 'To keep matters simple, the following steps demonstrate the creation of a user
    that uses an OpenSSL client certificate to authenticate. Those actions have to
    be performed with the cluster-admin Role object. During the exam, you will not
    have to create a user yourself. You can assume that the relevant setup has been
    performed for you. Therefore, you will not need to memorize the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log into the Kubernetes control plane node and create a temporary directory
    that will hold the generated keys. Navigate into the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a private key using the `openssl` executable. Provide an expressive
    file name, such as `<username>.key`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a certificate sign request (CSR) in a file with the extension `.csr`.
    You need to provide the private key from the previous step. The `-subj` option
    provides the username (CN) and the group (O). The following command uses the username
    `johndoe` and the group named `cka-study-guide`. To avoid assigning the user to
    a group, leave off the /O component of the assignment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, sign the CSR with the Kubernetes cluster certificate authority (CA).
    The CA can usually be found in the directory `/etc/kubernetes/pki` and needs to
    contain the files `ca.crt` and `ca.key`. We are going to use minikube here, which
    stores those files in the directory pass:[<code>~/.minikube</code>. The following
    command signs the CSR and makes it valid for 364 days:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the user in Kubernetes by setting a user entry in kubeconfig for `johndoe`.
    Point to the CRT and key file. Set a context entry in kubeconfig for `johndoe`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To switch to the user, use the context named `johndoe-context`. You can check
    the current context using the command `config current-context`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: ServiceAccount
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A user represents a real person who commonly interacts with the Kubernetes cluster
    using the `kubectl` executable or the UI dashboard. Some service applications
    like [Helm](https://helm.sh) running inside of a Pod need to interact with the
    Kubernetes cluster by making requests to the API server via RESTful HTTP calls.
    For example, a Helm chart would define multiple Kubernetes objects required for
    a business application. Kubernetes uses a ServiceAccount to authenticate the Helm
    service process with the API server through an authentication token. This ServiceAccount
    can be assigned to a Pod and mapped to RBAC rules.
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes cluster already comes with a ServiceAccount, the `default` ServiceAccount
    that lives in the `default` namespace. Any Pod that doesn’t explicitly assign
    a ServiceAccount uses the `default` ServiceAccount.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a custom ServiceAccount imperatively, run the `create serviceaccount`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The declarative way to create a ServiceAccount looks very straightforward. You
    simply provide the appropriate `kind` and a name, as shown in [Example 2-1](#a_yaml_manifest_defining_a_serviceaccount).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. A YAML manifest defining a ServiceAccount
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Listing ServiceAccounts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Listing the ServiceAccounts can be achieved with the `get serviceaccounts`
    command. As you can see in the following output, the `default` namespace lists
    the `default` ServiceAccount and the custom ServiceAccount we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Rendering ServiceAccount Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Upon object creation, the API server creates a Secret holding the API token
    and assigns it to the ServiceAccount. The Secret and token names use the ServiceAccount
    name as a prefix. You can discover the details of a ServiceAccount using the `describe
    serviceaccount` command, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Consequently, you should be able to find a Secret object for the `default`
    and the `build-bot` ServiceAccount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Assigning a ServiceAccount to a Pod
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a ServiceAccount to take effect, it needs to be assigned to a Pod running
    the application intended to make API calls. Upon Pod creation, you can use the
    command-line option `--serviceaccount` in conjunction with the `run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can directly assign the ServiceAccount in the YAML manifest
    of a Pod, Deployment, Job, or CronJob using the field `serviceAccountName`. [Example 2-2](#a_yaml_manifest_assigning_a_serviceaccount_to_a_pod)
    shows the definition of a ServiceAccount to a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. A YAML manifest assigning a ServiceAccount to a Pod
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Understanding RBAC API Primitives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With those key concepts in mind, let’s take a look at the Kubernetes API primitives
    that implement the RBAC functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: Role
  prefs: []
  type: TYPE_NORMAL
- en: The Role API primitive declares the API resources and their operations this
    rule should operate on. For example, you may want to say “allow listing and deleting
    of Pods,” or you may express “allow watching the logs of Pods,” or even both with
    the same Role. Any operation that is not spelled out explicitly is disallowed
    as soon as it is bound to the subject.
  prefs: []
  type: TYPE_NORMAL
- en: RoleBinding
  prefs: []
  type: TYPE_NORMAL
- en: The RoleBinding API primitive *binds* the Role object to the subject(s). It
    is the glue for making the rules active. For example, you may want to say “bind
    the Role that permits updating Services to the user John Doe.”
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-2](#rbac_primitives) shows the relationship between the involved
    API primitives. Keep in mind that the image renders only a selected list of API
    resource types and operations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0202](Images/ckas_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. RBAC primitives
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following sections demonstrate the namespace-wide usage of Roles and RoleBindings,
    but the same operations and attributes apply to cluster-wide Roles and RoleBindings,
    discussed in [“Namespace-wide and Cluster-wide RBAC”](#cluster-wide-rbac).
  prefs: []
  type: TYPE_NORMAL
- en: Default User-Facing Roles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes defines a set of default Roles. You can assign them to a subject
    via a RoleBinding or define your own, custom Roles depending on your needs. [Table 2-2](#default_user_facing_roles)
    describes the default user-facing Roles.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Default User-Facing Roles
  prefs: []
  type: TYPE_NORMAL
- en: '| Default ClusterRole | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| cluster-admin | Allows read and write access to resources across all namespaces.
    |'
  prefs: []
  type: TYPE_TB
- en: '| admin | Allows read and write access to resources in namespace including
    Roles and RoleBindings. |'
  prefs: []
  type: TYPE_TB
- en: '| edit | Allows read and write access to resources in namespace except Roles
    and RoleBindings. Provides access to Secrets. |'
  prefs: []
  type: TYPE_TB
- en: '| view | Allows read-only access to resources in namespace except Roles, RoleBindings,
    and Secrets. |'
  prefs: []
  type: TYPE_TB
- en: To define new Roles and RoleBindings, you will have to use a context that allows
    for creating or modifying them, that is, cluster-admin or admin.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Roles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Roles can be created imperatively with the `create role` command. The most
    important options for the command are `--verb` for defining the verbs aka operations,
    and `--resource` for declaring a list of API resources. The following command
    creates a new Role for the resources Pod, Deployment, and Service with the verbs
    `list`, `get`, and `watch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Declaring multiple verbs and resources for a single imperative `create role`
    command can be declared as a comma-separated list for the corresponding command-line
    option or as multiple arguments. For example, `--verb=list,get,watch` and `--verb=list
    --verb=get --verb=watch` carry the same instructions. You may also use the wildcard
    “*” to refer to all verbs or resources.
  prefs: []
  type: TYPE_NORMAL
- en: The command-line option `--resource-name` spells out one or many object names
    that the policy rules should apply to. A name of a Pod could be `nginx` and listed
    here with its name. Providing a list of resource names is optional. If no names
    have been provided, then the provided rules apply to all objects of a resource
    type.
  prefs: []
  type: TYPE_NORMAL
- en: The declarative approach can become a little lengthy. As you can see in [Example 2-3](#a_yaml_manifest_defining_a_role),
    the section `rules` lists the resources and verbs. Resources with an API group,
    like Deployments that use the API version `apps/v1`, need to explicitly declare
    it under the attribute `apiGroups`. All other resources (e.g., Pods and Services),
    simply use an empty string as their API version doesn’t contain a group. Be aware
    that the imperative command for creating a Role automatically determines the API
    group.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. A YAML manifest defining a Role
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Listing Roles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the Role has been created, its object can be listed. The list of Roles
    renders only the name and the creation timestamp. Each of the listed roles does
    not give away any of its details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Rendering Role Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can inspect the details of a Role using the `describe` command. The output
    renders a table that maps a resource to its permitted verbs. This cluster has
    no resources created, so the list of resource names in the following console output
    is empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Creating RoleBindings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The imperative command creating a RoleBinding object is `create rolebinding`.
    To bind a Role to the RoleBinding, use the `--role` command-line option. The subject
    type can be assigned by declaring the options `--user`, `--group`, or `--serviceaccount`.
    The following command creates the RoleBinding with the name `read-only-binding`
    to the user called `johndoe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Example 2-4](#a_yaml_manifest_defining_a_rolebinding) shows a YAML manifest
    representing the RoleBinding. You can see from the structure that a role can be
    mapped to one or many subjects. The data type is an array indicated by the dash
    character under the attribute `subjects`. At this time, only the user `johndoe`
    has been assigned.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. A YAML manifest defining a RoleBinding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Listing RoleBindings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important information the list of RoleBindings gives away is the associated
    Role. The following command shows that the RoleBinding `read-only-binding` has
    been mapped to the Role `read-only`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The output does not provide an indication of the subjects. You will need to
    render the details of the object for more information, as described in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Rendering RoleBinding Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RoleBindings can be inspected using the `describe` command. The output renders
    a table of subjects and the assigned role. The following example renders the descriptive
    representation of the RoleBinding named `read-only-binding`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Seeing the RBAC Rules in Effect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see how Kubernetes enforces the RBAC rules for the scenario we set up
    so far. First, we’ll create a new Deployment with the `cluster-admin` credentials.
    In Minikube, this user is assigned to the context `minikube`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’ll switch the context for the user `johndoe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that the user `johndoe` is permitted to list deployments. We’ll verify
    that by using the `get deployments` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The RBAC rules only allow listing Deployments, Pods, and Services. The following
    command tries to list the ReplicaSets, which results in an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'A similar behavior can be observed when trying to use other verbs than `list`,
    `get`, or `watch`. The following command tries to delete a Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'At any given time, you can check a user’s permissions with the `auth can-i`
    command. The command gives you the option to list all permissions or check a specific
    permission:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Namespace-wide and Cluster-wide RBAC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Roles and RoleBindings apply to a particular namespace. You will have to specify
    the namespace at the time of creating both objects. Sometimes, a set of Roles
    and Rolebindings needs to apply to multiple namespaces or even the whole cluster.
    For a cluster-wide definition, Kubernetes offers the API resource types ClusterRole
    and ClusterRoleBinding. The configuration elements are effectively the same. The
    only difference is the value of the `kind` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: To define a cluster-wide Role, use the imperative subcommand `clusterrole` or
    the kind `ClusterRole` in the YAML manifest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To define a cluster-wide RoleBinding, use the imperative subcommand `clusterrolebinding`
    or the kind `ClusterRoleBinding` in the YAML manifest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating RBAC Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Existing ClusterRoles can be aggregated to avoid having to redefine a new, composed
    set of rules that likely leads to duplication of instructions. For example, say
    you wanted to combine a user-facing role with a custom Role. An aggregated ClusterRule
    can merge rules via label selection without having to copy-paste the existing
    rules into one.
  prefs: []
  type: TYPE_NORMAL
- en: Say we defined two ClusterRoles shown in Examples [2-5](#a_yaml_manifest_defining_a_clusterrole_for_listing_pods)
    and [2-6](#a_yaml_manifest_defining_a_clusterrole_for_deleting_services). The
    ClusterRole `list-pods` allows for listing Pods and the ClusterRole `delete-services`
    allows for deleting Services.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. A YAML manifest defining a ClusterRole for listing Pods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Example 2-6\. A YAML manifest defining a ClusterRole for deleting Services
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: To aggregate those rules, ClusterRoles can specify an `aggregationRule`. This
    attribute describes the label selection rules. [Example 2-7](#a_yaml_manifest_defining_a_clusterrole_with_aggregated_fules)
    shows an aggregated ClusterRole defined by an array of `matchLabels` criteria.
    The ClusterRole does not add its own rules as indicated by `rules:` `[]`; however,
    there’s no limiting factor that would disallow it.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-7\. A YAML manifest defining a ClusterRole with aggregated rules
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify the proper aggregation behavior of the ClusterRole by describing
    the object. You can see in the following output that both ClusterRoles, `list-pods`
    and `delete-services`, have been taken into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: For more information on ClusterRole label selection rules, see the [official
    documentation](https://oreil.ly/J6k3m). The page also explains how to aggregate
    the default user-facing ClusterRoles.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Managing a Kubernetes Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When thinking about the typical tasks of a Kubernetes administrator, I am sure
    that at least one of the following bread-and-butter activities comes to mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping a control plane node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrapping worker nodes and joining them to the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading a cluster to a newer version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The low-level command-line tool for performing cluster bootstrapping operations
    is called `kubeadm`. It is not meant for provisioning the underlying infrastructure.
    That’s the purpose of infrastructure automation tools like Ansible and Terraform.
    To install `kubeadm`, follow the [installation instructions](https://oreil.ly/gKq4m)
    in the official Kubernetes documentation.
  prefs: []
  type: TYPE_NORMAL
- en: While not explicitly stated in the CKA frequently asked questions (FAQ) page,
    you can assume that the `kubeadm` executable has been preinstalled for you. The
    following sections describe the processes for creating and managing a Kubernetes
    cluster on a high level and will use `kubeadm` heavily. For more detailed information,
    see the step-by-step Kubernetes reference documentation I will point out for each
    of the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Installing a Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most basic topology of a Kubernetes cluster consists of a single node that
    acts as the control plane and the worker node at the same time. By default, many
    developer-centric Kubernetes installations like minikube or Docker Desktop start
    with this configuration. While a single-node cluster may be a good option for
    a Kubernetes playground, it is not a good foundation for scalability and high-availability
    reasons. At the very least, you will want to create a cluster with a single control
    plane and one or many nodes handling the workload.
  prefs: []
  type: TYPE_NORMAL
- en: This section explains how to install a cluster with a single control plane and
    one worker node. You can repeat the worker node installation process to add more
    worker nodes to the cluster. You can find a full description of the [installation
    steps](https://oreil.ly/8visY) in the official Kubernetes documentation. [Figure 2-3](#cluster_installation_process)
    illustrates the installation process.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0203](Images/ckas_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Process for a cluster installation process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Initializing the Control Plane Node
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start by initializing the control plane on the control plane node. The control
    plane is the machine responsible for hosting the API server, etcd, and other components
    important to managing the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open an interactive shell to the control plane node using the `ssh` command.
    The following command targets the control plane node named `kube-control-plane`
    running Ubuntu 18.04.5 LTS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the control plane using the `kubeadm init` command. You will need
    to add the following two command-line options: provide the IP addresses for the
    Pod network with the option `--pod-network-cidr`. With the option `--apiserver-advertise-address`,
    you can declare the IP address the API Server will advertise to listen on.'
  prefs: []
  type: TYPE_NORMAL
- en: The console output renders a `kubeadm join` command. Keep that command around
    for later. It is important for joining worker nodes to the cluster in a later
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving the join command for worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can always retrieve the `join` command by running `kubeadm token create
    --print-join-command` on the control plane node should you lose it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command uses `172.18.0.0/16` for the Classless Inter-Domain Routing
    (CIDR) and IP address `10.8.8.10` for the API server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After the `init` command has finished, run the necessary commands from the
    console output to start the cluster as nonroot user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You must deploy a [Container Network Interface (CNI) plugin](https://oreil.ly/t6eJ7)
    so that Pods can communicate with each other. You can pick from a wide range of
    networking plugins listed in the [Kubernetes documentation](https://oreil.ly/1Y7MF).
    Popular plugins include Flannel, Calico, and Weave Net. Sometimes you will see
    the term “add-ons” in the documentation, which is synonymous with plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CKA exam will most likely ask you to install a specific add-on. Most of
    the installation instructions live on external web pages, not permitted to be
    used during the exam. Make sure that you search for the relevant instructions
    in the official Kubernetes documentation. For example, you can find the installation
    instructions for Weave Net [here](https://oreil.ly/86YpI). The following command
    installs the Weave Net objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the control plane node indicates the “Ready” status using the command
    `kubectl get nodes`. It might take a couple of seconds before the node transitions
    from the “NotReady” status to the “Ready” status. You have an issue with your
    node installation in case the status transition does not occur. Refer to [Chapter 7](ch07.xhtml#troubleshooting)
    for debugging strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the control plane node using the `exit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Joining the Worker Nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Worker nodes are responsible for handling the workload scheduled by the control
    plane. Examples of workloads are Pods, Deployments, Jobs, and CronJobs. To add
    a worker node to the cluster so that it can be used, you will have to run a couple
    of commands, as described next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open an interactive shell to the worker node using the `ssh` command. The following
    command targets the worker node named `kube-worker-1` running Ubuntu 18.04.5 LTS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `kubeadm join` command provided by the `kubeadm init` console output
    on the control plane node. The following command shows an example. Remember that
    the token and SHA256 hash will be different for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You won’t be able to run the `kubectl get nodes` command from the worker node
    without copying the administrator kubeconfig file from the control plane node.
    Follow the [instructions](https://oreil.ly/AIM8a) in the Kubernetes documentation
    to do so or log back into the control plane node. Here, we are just going to log
    back into the control plane node. You should see that the worker node has joined
    the cluster and is in a “Ready” status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You can repeat the process for any other worker node you want to add to the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Managing a Highly Available Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Single control plane clusters are easy to install; however, they present an
    issue when the node is lost. Once the control plane node becomes unavailable,
    any ReplicaSet running on a worker node cannot re-create a Pod due to the inability
    to talk back to the scheduler running on a control plane node. Moreover, clusters
    cannot be accessed externally anymore (e.g., via `kubectl`), as the API server
    cannot be reached.
  prefs: []
  type: TYPE_NORMAL
- en: High-availability (HA) clusters help with scalability and redundancy. For the
    exam, you will need to have a basic understanding about configuring them and their
    implications. Given the complexity of standing up an HA cluster, it’s unlikely
    that you’ll be asked to perform the steps during the exam. For a full discussion
    on setting up HA clusters, see the [relevant page](https://oreil.ly/17ZDL) in
    the Kubernetes documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The *stacked etcd topology* involves creating two or more control plane nodes
    where etcd is colocated on the node. [Figure 2-4](#stacked_etcd_topology) shows
    a representation of the topology with three control plane nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0204](Images/ckas_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Stacked etcd topology with three control plane nodes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each of the control plane nodes hosts the API server, the scheduler, and the
    controller manager. Worker nodes communicate with the API server through a load
    balancer. It is recommended to operate this cluster topology with a minimum of
    three control plane nodes for redundancy reasons due to the tight coupling of
    etcd to the control plane node. By default, `kubeadm` will create an etcd instance
    when joining a control plane node to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The *external etcd node* topology separates etcd from the control plane node
    by running it on a dedicated machine. [Figure 2-5](#external_etcd_node_topology)
    shows a setup with three control plane nodes, each of which run etcd on a different
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0205](Images/ckas_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. External etcd node topology
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar to the stacked etcd topology, each control plane node hosts the API
    server, the scheduler, and the controller manager. The worker nodes communicate
    with them through a load balancer. The main difference here is that the etcd instances
    run on a separate host. This topology decouples etcd from other control plane
    functionality and therefore has less of an impact on redundancy when a control
    plane node is lost. As you can see in the illustration, this topology requires
    twice as many hosts as the stacked etcd topology.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading a Cluster Version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over time, you will want to upgrade the Kubernetes version of an existing cluster
    to pick up bug fixes and new features. The upgrade process has to be performed
    in a controlled manner to avoid the disruption of workload currently in execution
    and to prevent the corruption of cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to upgrade from a minor version to a next higher one (e.g.,
    from 1.18.0 to 1.19.0), or from a patch version to a higher one (e.g., from 1.18.0
    to 1.18.3). Abstain from jumping up multiple minor versions to avoid unexpected
    side effects. You can find a full description of the [upgrade steps](https://oreil.ly/2dCfk)
    in the official Kubernetes documentation. [Figure 2-6](#cluster_version_upgrade)
    illustrates the upgrade process.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0206](Images/ckas_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Process for a cluster version upgrade
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Upgrading control plane nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As explained earlier, a Kubernetes cluster may employ one or many control plane
    nodes to better support high-availability and scalability concerns. When upgrading
    a cluster version, this change needs to happen for control plane nodes one at
    a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pick one of the control plane nodes that contains the kubeconfig file (located
    at `/etc/kubernetes/admin.conf`), and open an interactive shell to the control
    plane node using the `ssh` command. The following command targets the control
    plane node named `kube-control-plane` running Ubuntu 18.04.5 LTS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'First, check the nodes and their Kubernetes versions. In this setup, all nodes
    run on version 1.18.0\. We are dealing with only a single control plane node and
    a single worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Start by upgrading the `kubeadm` version. Identify the version you’d like to
    upgrade to. On Ubuntu machines, you can use the following `apt-get` command. The
    version format usually includes a patch version (e.g., `1.20.7-00`). Check the
    Kubernetes documentation if your machine is running a different operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Upgrade `kubeadm` to a target version. Say you’d want to upgrade to version
    `1.19.0-00`. The following series of commands installs `kubeadm` with that specific
    version and checks the currently installed version to verify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Check which versions are available to upgrade to and validate whether your
    current cluster is upgradable. You can see in the output of the following command
    that we could upgrade to version `1.19.12`. For now, we’ll stick with `1.19.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As described in the console output, we’ll start the upgrade for the control
    plane. The process may take a couple of minutes. You may have to upgrade the CNI
    plugin as well. Follow the provider instructions for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Drain the control plane node by evicting the workload. Any new workload won’t
    be schedulable on the node until uncordoned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Upgrade the kubelet and the `kubectl` tool to the same version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the kubelet process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Reenable the control plane node back so that the new workload can become schedulable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The control plane nodes should now show the usage of Kubernetes 1.19.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the control plane node using the `exit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Upgrading worker nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pick one of the worker nodes, and open an interactive shell to the node using
    the `ssh` command. The following command targets the worker node named `kube-worker-1`
    running Ubuntu 18.04.5 LTS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Upgrade `kubeadm` to a target version. This is the same command you used for
    the control plane node, as explained earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Upgrade the kubelet configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Drain the worker node by evicting the workload. Any new workload won’t be schedulable
    on the node until uncordoned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Upgrade the kubelet and the `kubectl` tool with the same command used for the
    control plane node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the kubelet process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Reenable the worker node so that the new workload can become schedulable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing the nodes should now show version 1.19.0 for the worker node. You won’t
    be able to run `kubectl get nodes` from the worker node without copying the administrator
    kubeconfig file from the control plane node. Follow the [instructions](https://oreil.ly/NGHaQ)
    in the Kubernetes documentation to do so or log back into the control plane node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the worker node using the `exit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Backing Up and Restoring etcd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes stores both the declared and observed states of the cluster in the
    distributed etcd key-value store. It’s important to have a backup plan in place
    that can help you with restoring the data in case of data corruption. Backing
    up the data should happen periodically in short time frames to avoid losing as
    little historical data as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The backup process stores the ectd data in a so-called snapshot file. This snapshot
    file can be used to restore the etcd data at any given time. You can encrypt the
    snapshot file to protect sensitive information. The tool `etcdctl` is central
    to the backup and restore procedure.
  prefs: []
  type: TYPE_NORMAL
- en: As an administrator, you will need to understand how to use the tool for both
    operations. You may need to install `etcdctl` if it is not available on the control
    plane node yet. You can find [installation instructions](https://oreil.ly/CrI28)
    in the etcd GitHub repository. [Figure 2-7](#backing_up_restoring_etcd) visualizes
    the etcd backup and restoration process.
  prefs: []
  type: TYPE_NORMAL
- en: '![ckas 0207](Images/ckas_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Process for a backing up and restoring etcd
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on your cluster topology, your cluster may consist of one or many
    etcd instances. Refer to the section “High-Availability Cluster Setup” for more
    information on how to set it up. The following sections explain a single-node
    etcd cluster setup. You can find [additional instructions](https://oreil.ly/PvS5u)
    on the backup and restoration process for multinode etcd clusters in the official
    Kubernetes documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Backing Up etcd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open an interactive shell to the machine hosting etcd using the `ssh` command.
    The following command targets the control plane node named `kube-control-plane`
    running Ubuntu 18.04.5 LTS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the installed version of `etcdctl` to verify that the tool has been installed.
    On this node, the version is 3.4.14:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Etcd is deployed as a Pod in the `kube-system` namespace. Inspect the version
    by describing the Pod. In the following output, you will find that the version
    is 3.4.13-0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The same `describe` command reveals the configuration of the etcd service.
    Look for the value of the option `--listen-client-urls` for the endpoint URL.
    In the following output, the host is `localhost`, and the port is `2379`. The
    server certificate is located at `/etc/kubernetes/pki/etcd/server.crt` defined
    by the option `--cert-file`. The CA certificate can be found at `/etc/kubernetes/pki/etcd/ca.crt`
    specified by the option `--trusted-ca-file`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `etcdctl` command to create the backup with version 3 of the tool.
    For a good starting point, copy the command from the [official Kubernetes documentation](https://oreil.ly/LuM2P).
    Provide the mandatory command-line options `--cacert`, `--cert`, and `--key`.
    The option `--endpoints` is not needed as we are running the command on the same
    server as etcd. After running the command, the file `/tmp/etcd-backup.db` has
    been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the node using the `exit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Restoring etcd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You created a backup of etcd and stored it in a safe space. There’s nothing
    else to do at this time. Effectively, it’s your insurance policy that becomes
    relevant when disaster strikes. In the case of a disaster scenario, the data in
    etcd gets corrupted or the machine managing etcd experiences a physical storage
    failure. That’s the time when you want to pull out the etcd backup for restoration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore etcd from the backup, use the `etcdctl snapshot restore` command.
    At a minimum, provide the `--data-dir` command-line option. Here, we are using
    the data directory `/tmp/from-backup`. After running the command, you should be
    able to find the restored backup in the directory `/var/lib/from-backup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the YAML manifest of the etcd Pod, which can be found at `/etc/kubernetes/manifests/etcd.yaml`.
    Change the value of the attribute `spec.volumes.hostPath` with the name `etcd-data`
    from the original value `/var/lib/etcd` to `/var/lib/from-backup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The `etcd-kube-control-plane` Pod will be re-created, and it points to the
    restored backup directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: In case the Pod doesn’t transition into the “Running” status, try to delete
    it manually with the command `kubectl delete pod etcd-kube-control-plane -n kube-system`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exit the node using the `exit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Production-ready Kubernetes clusters should employ security policies to control
    which users and what processes can manage objects. Role-based access control (RBAC)
    defines those rules. RBAC introduces specific API resources that map subjects
    to the operations allowed for particular objects. Rules can be defined on a namespace
    or cluster level using the API resource types Role, ClusterRole, RoleBinding,
    and ClusterRoleBinding. To avoid duplication of rules, ClusterRoles can be aggregated
    with the help of label selection.
  prefs: []
  type: TYPE_NORMAL
- en: As a Kubernetes administrator, you need to be familiar with typical tasks involving
    the management of the cluster nodes. The primary tool for installing new nodes
    and upgrading a node version is `kubeadm`. The cluster topology of such a cluster
    can vary. For optimal results with redundancy and scalability, consider configuring
    the cluster with a high-availability setup that uses three or more control plane
    nodes and dedicated etcd hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Backing up the etcd database should be performed as a periodic process to prevent
    the loss of crucial data in the event of a node or storage corruption. You can
    use the tool `etcdctl` to back up and restore etcd from the control plane node
    or via an API endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Know how to define RBAC rules
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining RBAC rules involves a couple of moving parts: the subject defined
    by users, groups, and ServiceAccounts; the RBAC-specific API resources on the
    namespace and cluster level; and, finally, the verbs that allow the corresponding
    operations on the Kubernetes objects. Practice the creation of subjects, and how
    to tie them together to form the desired access rules. Ensure that you verify
    the correct behavior with different constellations.'
  prefs: []
  type: TYPE_NORMAL
- en: Know how to create and manage a Kubernetes cluster
  prefs: []
  type: TYPE_NORMAL
- en: Installing new cluster nodes and upgrading the version of an existing cluster
    node are typical tasks performed by a Kubernetes administrator. You do not need
    to memorize all the steps involved. The documentation provides a step-by-step,
    easy-to-follow manual for those operations. For upgrading a cluster version, it
    is recommended to jump up by a single minor version or multiple patch versions
    before tackling the next higher version. High-availability clusters help with
    redundancy and scalability. For the exam, you will need to understand the different
    HA topologies though it’s unlikely that you’ll have to configure one of them as
    the process would involve a suite of different hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Practice backing up and restoring etcd
  prefs: []
  type: TYPE_NORMAL
- en: The process for etcd disaster recovery is not as well documented as you’d expect.
    Practice the backup and a restoration process hands-on a couple of times to get
    the hang of it. Remember to point the control plane node(s) to the restored snapshot
    file to recover the data.
  prefs: []
  type: TYPE_NORMAL
- en: Sample Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solutions to these exercises are available in the [Appendix](app01.xhtml#appendix-a).
  prefs: []
  type: TYPE_NORMAL
- en: Create the ServiceAccount named `api-access` in a new namespace called `apps`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a ClusterRole with the name `api-clusterrole`, and create a ClusterRoleBinding
    named `api-clusterrolebinding`. Map the ServiceAccount from the previous step
    to the API resources `pods` with the operations `watch`, `list`, and `get`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Pod named `operator` with the image `nginx:1.21.1` in the namespace
    `apps`. Expose the container port 80\. Assign the ServiceAccount `api-access`
    to the Pod. Create another Pod named `disposable` with the image `nginx:1.21.1`
    in the namespace `rm`. Do not assign the ServiceAccount to the Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open an interactive shell to the Pod named `operator`. Use the command-line
    tool `curl` to make an API call to list the Pods in the namespace `rm`. What response
    do you expect? Use the command-line tool `curl` to make an API call to delete
    the Pod `disposable` in the namespace `rm`. Does the response differ from the
    first call? You can find information about how to interact with Pods using the
    API via HTTP in the [reference guide](https://oreil.ly/SZls9).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the directory *app-a/ch02/upgrade-version* of the checked-out GitHub
    repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Start up the
    VMs running the cluster using the command `vagrant up`. Upgrade all nodes of the
    cluster from Kubernetes 1.20.4 to 1.21.2\. The cluster consists of a single control
    plane node named `k8s-control-plane`, and three worker nodes named `worker-1`,
    `worker-2`, and `worker-3`. Once done, shut down the cluster using `vagrant destroy
    -f`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prerequisite:* This exercise requires the installation of the tools [Vagrant](https://oreil.ly/sasln)
    and [VirtualBox](https://oreil.ly/9Cvg9).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the directory *app-a/ch02/backup-restore-etcd* of the checked-out
    GitHub repository [*bmuschko/cka-study-guide*](https://oreil.ly/jUIq8). Start
    up the VMs running the cluster using the command `vagrant up`. The cluster consists
    of a single control plane node named `k8s-control-plane` and two worker nodes
    named `worker-1` and `worker-2`. The `etcdctl` tool has been preinstalled on the
    node `k8s-control-plane`. Back up etcd to the snapshot file `/opt/etcd.bak`. Restore
    etcd from the snapshot file. Use the data directory `/var/bak`. Once done, shut
    down the cluster using `vagrant destroy -f`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prerequisite:* This exercise requires the installation of the tools [Vagrant](https://oreil.ly/sasln)
    and [VirtualBox](https://oreil.ly/9Cvg9).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
