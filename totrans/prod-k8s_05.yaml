- en: Chapter 4\. Container Storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 容器存储
- en: While Kubernetes cut its teeth in the world of stateless workloads, running
    stateful services has become increasingly common. Even complex stateful workloads
    such as databases and message queues are finding their way to Kubernetes clusters.
    To support these workloads, Kubernetes needs to provide storage capabilities beyond
    ephemeral options. Namely, systems that can provide increased resilience and availability
    in the face of various events such as an application crashing or a workload being
    rescheduled to a different host.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Kubernetes 最初是在无状态工作负载领域发展起来的，但运行有状态服务变得越来越普遍。甚至像数据库和消息队列这样复杂的有状态工作负载也在 Kubernetes
    集群中找到了应用的方式。为了支持这些工作负载，Kubernetes 需要提供超越临时选项的存储能力。也就是说，系统需要在应用崩溃或工作负载被重新调度到不同主机时提供增强的韧性和可用性。
- en: In this chapter we are going to explore how our platform can offer storage services
    to applications. We’ll start by covering key concerns of application persistence
    and storage system expectations before moving on to address the storage primitives
    available in Kubernetes. As we get into more advanced storage needs, we will look
    to the [Container Storage Interface (CSI)](https://kubernetes-csi.github.io/docs),
    which enables our integration with various storage providers. Lastly, we’ll explore
    using a CSI plug-in to provide self-service storage to our applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨我们的平台如何为应用程序提供存储服务。我们将首先讨论应用程序持久性和存储系统期望的关键问题，然后再深入讨论 Kubernetes 中可用的存储原语。随着我们探讨更高级的存储需求，我们将看到
    [容器存储接口 (CSI)](https://kubernetes-csi.github.io/docs) 的使用，它使我们能够与各种存储提供者进行集成。最后，我们将探讨使用
    CSI 插件为我们的应用程序提供自助存储。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Storage is a vast subject in itself. Our intentions are to give you just enough
    detail to make informed decisions about the storage you may offer to workloads.
    If storage is not your background, we highly recommend going over these concepts
    with your infrastructure/storage team. Kubernetes does not negate the need for
    storage expertise in your organization!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 存储本身就是一个广阔的主题。我们的意图是为您提供足够的细节，以便您能够为工作负载做出明智的存储决策。如果存储不是您的背景，强烈建议与您的基础设施/存储团队共同讨论这些概念。Kubernetes
    不能取代您组织中对存储专业知识的需求！
- en: Storage Considerations
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储考虑事项
- en: Before getting into Kubernetes storage patterns and options, we should take
    a step back and analyze some key considerations around potential storage needs.
    At an infrastructure and application level, it is important to think through the
    following requirements.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究 Kubernetes 存储模式和选项之前，我们应该退后一步，分析潜在的存储需求周围的一些关键考虑因素。在基础设施和应用程序级别，思考以下需求是非常重要的。
- en: Access modes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问模式
- en: Volume expansion
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷扩展
- en: Dynamic provisioning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态供给
- en: Backup and recovery
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和恢复
- en: Block, file, and object storage
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块、文件和对象存储
- en: Ephemeral data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临时数据
- en: Choosing a provider
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择提供者
- en: Access Modes
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问模式
- en: 'There are three access modes that can be supported for applications:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以支持三种访问模式：
- en: ReadWriteOnce (RWO)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ReadWriteOnce (RWO)
- en: A single Pod can read and write to the volume.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 单个 Pod 可以对卷进行读写操作。
- en: ReadOnlyMany (ROX)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ReadOnlyMany (ROX)
- en: Multiple Pods can read the volume.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多个 Pod 可以读取卷。
- en: ReadWriteMany (RWX)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ReadWriteMany (RWX)
- en: Multiple Pods can read and write to the volume.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 多个 Pod 可以对卷进行读写操作。
- en: For cloud native applications, RWO is by far the most common pattern. When leveraging
    common providers such [Amazon Elastic Block Storage (EBS)](https://aws.amazon.com/ebs)
    or [Azure Disk Storage](https://oreil.ly/wAtBg), you are limited to RWO because
    the disk may only be attached to one node. While this limitation may seem problematic,
    most cloud native applications work best with this kind of storage, where the
    volume is exclusively theirs and offers high-performance read/write.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于云原生应用程序，RWO 模式是最常见的模式。当利用常见的提供者如 [Amazon Elastic Block Storage (EBS)](https://aws.amazon.com/ebs)
    或 [Azure Disk Storage](https://oreil.ly/wAtBg) 时，你只能选择 RWO，因为磁盘只能附加到一个节点。虽然这种限制可能看起来有问题，但大多数云原生应用程序在这种存储环境中表现最佳，因为卷是独占的，提供高性能的读写能力。
- en: Many times, we find legacy applications that have a requirement for RWX. Often,
    they are built to assume access to a [network file system (NFS)](https://oreil.ly/OrsBR).
    When services need to share state, there are often more elegant solutions than
    sharing data over NFS; for example, the use of message queues or databases. Additionally,
    should an application wish to share data, it’s typically best to expose this over
    an API, rather than grant access to its file system. This makes many use cases
    for RWX, at times, questionable. Unless NFS is the correct design choice, platform
    teams may be confronted with the tough choice of whether to offer RWX-compatible
    storage or request their developers re-architect applications. Should the call
    be made that supporting ROX or RWX is required, there are several providers that
    can be integrated with, such as [Amazon Elastic File System (EFS)](https://aws.amazon.com/efs)
    and [Azure File Share](https://oreil.ly/u6HiQ).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常发现遗留应用程序需要 RWX 的需求。通常，它们被设计为假定可以访问[网络文件系统（NFS）](https://oreil.ly/OrsBR)。当服务需要共享状态时，通常有比通过
    NFS 共享数据更优雅的解决方案；例如，使用消息队列或数据库。此外，如果应用程序希望共享数据，通常最好通过 API 公开此数据，而不是授予访问其文件系统的权限。这使得许多情况下对于
    RWX 的使用都显得可疑。除非 NFS 是正确的设计选择，否则平台团队可能面临是否提供兼容 RWX 的存储或要求其开发人员重新架构应用程序的艰难选择。如果决定支持
    ROX 或 RWX 是必需的，那么可以集成几个提供者，如[亚马逊弹性文件系统（EFS）](https://aws.amazon.com/efs)和[Azure
    文件共享](https://oreil.ly/u6HiQ)。
- en: Volume Expansion
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷扩展
- en: 'Over time, an application may begin to fill up its volume. This can pose a
    challenge since replacing the volume with a larger one would require migration
    of data. One solution to this is supporting volume expansion. From the perspective
    of a container orchestrator such as Kubernetes, this involves a few steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，应用程序可能会开始填满其卷。这可能会带来挑战，因为用更大的卷替换卷将需要数据迁移。其中一个解决方案是支持卷扩展。从像 Kubernetes
    这样的容器编排器的角度来看，这涉及到几个步骤：
- en: Request additional storage from the orchestrator (e.g., via a PersistentVolumeClaim).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从编排器（例如，通过 PersistentVolumeClaim）请求额外的存储。
- en: Expand the size of the volume via the storage provider.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过存储提供者扩展卷的大小。
- en: Expand the filesystem to make use of the larger volume.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展文件系统以利用更大的卷。
- en: Once complete, the Pod will have access to the additional space. This feature
    is contingent on our choice of storage backend and whether the integration in
    Kubernetes can facilitate the preceding steps. We will explore an example of volume
    expansion later in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，Pod 将能够访问额外的空间。这个功能取决于我们选择的存储后端以及 Kubernetes 中的集成是否能够促成前面的步骤。本章稍后我们将探讨一个卷扩展的示例。
- en: Volume Provisioning
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷配置
- en: 'There are two provisioning models available to you: dynamic and static provisioning.
    Static provisioning assumes volumes are created on nodes for Kubernetes to consume.
    Dynamic provisioning is when a driver runs within the cluster and can satisfy
    storage requests of workloads by talking to a storage provider. Out of these two
    models, dynamic provisioning, when possible, is preferred. Often, the choice between
    the two is a matter of whether your underlying storage system has a compatible
    driver for Kubernetes. We’ll dive into these drivers later in the chapter.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择两种配置模型：动态配置和静态配置。静态配置假定卷是在节点上为 Kubernetes 创建的。动态配置是指在集群内运行驱动程序，并可以通过与存储提供者通信来满足工作负载的存储请求。在这两种模型中，如果可能的话，动态配置是首选。通常，选择两者之间的一个取决于您的底层存储系统是否有兼容
    Kubernetes 的驱动程序。我们稍后会深入讨论这些驱动程序。
- en: Backup and Recovery
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 备份与恢复
- en: Backup is one of the most complex aspects of storage, especially when automated
    restores are a requirement. In general terms, a backup is a copy of data that
    is stored for use in case of data loss. Typically, we balance backup strategies
    with the availability guarantees of our storage systems. For example, while backups
    are always important, they are less critical when our storage system has a replication
    guarantee where loss of hardware will *not* result in loss of data. Another consideration
    is that applications may require different procedures to facilitate backup and
    restores. The idea that we can take a backup of an entire cluster and restore
    it at any time is typically a naive outlook, or at minimum, one that requires
    mountains of engineering effort to achieve.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 备份是存储的最复杂方面之一，尤其是自动恢复是一个要求。一般来说，备份是数据的副本，用于在数据丢失时使用。通常，我们根据存储系统的可用性保证平衡备份策略。例如，尽管备份始终重要，但在我们的存储系统具有复制保证的情况下，它们的重要性较低，硬件丢失*不*会导致数据丢失。另一个考虑因素是应用程序可能需要不同的程序来促进备份和恢复。通常，我们可以随时备份整个集群并进行恢复的想法是一个天真的观点，或者至少需要大量的工程努力才能实现。
- en: Deciding who should be responsible for backup and recovery of applications can
    be one of the most challenging debates within an organization. Arguably, offering
    restore features as a platform service can be a “nice to have.” However, it can
    tear at the seams when we get into application-specific complexity—for example,
    when an app cannot restart and needs actions to take place that are known only
    to developers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决定由谁负责应用程序的备份和恢复可能是组织内最具挑战性的讨论之一。可以说，将恢复功能作为平台服务提供可能是一个“好处”。然而，当我们涉及到应用程序特定的复杂性时，这可能会引发问题，例如当一个应用程序无法重新启动并且需要进行只有开发人员知道的操作时。
- en: One of the most popular backup solutions for both Kubernetes state and application
    state is [Project Velero](https://velero.io). Velero can back up Kubernetes objects
    should you have a desire to migrate or restore them across clusters. Additionally,
    Velero supports the scheduling of volume snapshots. As we dive deeper into volume
    snapshotting in this chapter, we’ll learn that the ability to schedule and manage
    snapshots is *not* taken care of for us. More so, we are often given the snapshotting
    primitives but need to define an orchestration flow around them. Lastly, Velero
    supports backup and restore hooks. These enable us to run commands in the container
    before performing backup or recovery. For example, some applications may require
    stopping traffic or triggering a flush before a backup should be taken. This is
    made possible using hooks in Velero.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[Project Velero](https://velero.io) 是 Kubernetes 状态和应用状态的最受欢迎的备份解决方案之一。如果您希望在集群之间迁移或恢复它们，Velero
    可以备份 Kubernetes 对象。此外，Velero 支持调度卷快照。在本章节深入讨论卷快照时，我们会了解到，安排和管理快照的能力并*不*是自动完成的。更进一步，我们通常提供快照基元，但需要定义围绕它们的编排流程。最后，Velero
    支持备份和恢复钩子。这些钩子允许我们在执行备份或恢复之前在容器中运行命令。例如，某些应用可能需要在进行备份之前停止流量或触发刷新。Velero 通过钩子使这成为可能。'
- en: Block Devices and File and Object Storage
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块设备和文件与对象存储
- en: The storage types our applications expect are key to selecting the appropriate
    underlying storage and Kubernetes integration. The most common storage type used
    by applications is file storage. File storage is a block device with a filesystem
    on top. This enables applications to write to files in the way we are familiar
    with on any operating system.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序期望的存储类型对选择适当的底层存储和 Kubernetes 集成至关重要。应用程序使用的最常见的存储类型是文件存储。文件存储是一个带有文件系统的块设备。这使得应用程序可以按照我们在任何操作系统上熟悉的方式写入文件。
- en: Underlying a filesystem is a block device. Rather than establishing a filesystem
    on top, we can offer the device such that applications may communicate directly
    with raw block. Filesystems inherently add overhead to writing data. In modern
    software development, it’s pretty rare to be concerned about filesystem overhead.
    However, if your use case warrants direct interaction with raw block devices,
    this is something certain storage systems can support.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件系统的底层是一个块设备。与在其上建立文件系统不同，我们可以提供设备，以便应用程序可以直接与原始块通信。文件系统在写入数据时固有地增加了开销。在现代软件开发中，我们很少会担心文件系统的开销。但是，如果您的用例需要直接与原始块设备进行交互，某些存储系统可以支持这一点。
- en: The final storage type is object storage. Object storage deviates from files
    in the sense that there is not the conventional hierarchy. Object storage enables
    developers to take unstructured data, give it a unique identifier, add some metadata
    around it, and store it. Cloud-provider object stores such as [Amazon S3](https://aws.amazon.com/s3)
    have become popular locations for organizations to host images, binaries, and
    more. This popularity has been accelerated by its fully featured web API and access
    control. Object stores are *most commonly* interacted with from the application
    itself, where the application uses a library to authenticate and interact with
    the provider. Since there is less standardization around interfaces for interaction
    with object stores, it is less common to see them integrated as platform services
    that applications can interact with transparently.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种存储类型是对象存储。对象存储在文件的意义上有所偏离，因为它没有传统的层次结构。对象存储使开发人员能够获取非结构化数据，为其添加唯一标识符，添加一些元数据，并存储它。云提供商的对象存储，例如[Amazon
    S3](https://aws.amazon.com/s3)，已成为组织托管图像、二进制文件等的流行位置。这种流行度得益于其功能齐全的 Web API 和访问控制。对象存储通常由应用程序本身进行交互，应用程序使用库进行身份验证和与提供程序的交互。由于对象存储的交互接口标准化程度较低，因此不常见将其集成为应用程序可以透明交互的平台服务。
- en: Ephemeral Data
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 短暂数据
- en: While storage may imply a level of persistence that is beyond the life cycle
    of a Pod, there are valid use cases for supporting ephemeral data usage. By default,
    containers that write to their own filesystem will utilize ephemeral storage.
    If the container were to restart, this storage would be lost. The [emptyDir](https://oreil.ly/86zjA)
    volume type is available for ephemeral storage that is resilient to restarts.
    Not only is this resilient to container restarts, but it can be used to share
    files between containers in the same Pod.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存储可能意味着超出 Pod 生命周期的持久性水平，但支持短暂数据使用的有效用例是存在的。默认情况下，写入自己文件系统的容器将利用短暂存储。如果容器重新启动，这些存储将丢失。[emptyDir](https://oreil.ly/86zjA)
    卷类型用于能够抵御重新启动的短暂存储。这不仅对容器重新启动具有韧性，而且可以用于在同一 Pod 中的容器之间共享文件。
- en: The biggest risk with ephemeral data is ensuring your Pods don’t consume too
    much of the host’s storage capacity. While numbers like 4Gi per Pod might not
    seem like much, consider a node can run hundreds, in some cases thousands, of
    Pods. Kubernetes supports the ability to limit the cumulative amount of ephemeral
    storage available to Pods in a Namespace. Configuration of these concerns are
    covered in [Chapter 12](ch12.html#multi_tenancy_chapter).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 短暂数据的最大风险是确保您的 Pod 不会占用主机存储容量过多。尽管每个 Pod 的 4Gi 数字可能看起来并不多，但考虑一个节点可以运行数百个，有时甚至数千个
    Pod。Kubernetes 支持限制命名空间中的 Pod 可用短暂存储的累计量。这些问题的配置在[第 12 章](ch12.html#multi_tenancy_chapter)中有详细介绍。
- en: Choosing a Storage Provider
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择存储提供者
- en: There is no shortage of storage providers available to you. Options span from
    storage solutions you might manage yourself such as Ceph to fully managed systems
    like Google Persistent Disk or Amazon Elastic Block Store. The variance in options
    is far beyond the scope of this book. However, we do recommend understanding the
    capabilities of storage systems along with which of those capabilities are easily
    integrated with Kubernetes. This will surface perspective on how well one solution
    may satisfy your application requirements relative to another. Additionally, in
    the case you may be managing your own storage system, consider using something
    you have operational experience with when possible. Introducing Kubernetes alongside
    a new storage system adds a lot of new operational complexity to your organization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可供选择的存储提供者众多。选项从您可能自行管理的存储解决方案，例如 Ceph，到像 Google Persistent Disk 或 Amazon Elastic
    Block Store 这样的完全托管系统。选项的差异远远超出了本书的范围。然而，我们建议了解存储系统的能力以及这些能力与 Kubernetes 容易集成的情况。这将使您能够评估一种解决方案相对于另一种解决方案在满足应用程序需求方面的表现。此外，在可能的情况下，如果您正在管理自己的存储系统，请考虑使用您已经具有操作经验的内容。将
    Kubernetes 引入新的存储系统将为您的组织增加大量新的运营复杂性。
- en: Kubernetes Storage Primitives
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 存储基元
- en: Out of the box, Kubernetes provides multiple primitives to support workload
    storage. These primitives provide the building blocks we will utilize to offer
    sophisticated storage solutions. In this section, we are going to cover PersistentVolumes,
    PersistentVolumeClaims, and StorageClasses using an example of allocating fast
    pre-provisioned storage to containers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了多个原语来支持工作负载存储。这些原语为我们提供了构建复杂存储解决方案所需的基础。在本节中，我们将使用一个例子来介绍 PersistentVolumes、PersistentVolumeClaims
    和 StorageClasses，将快速预配置的存储分配给容器。
- en: Persistent Volumes and Claims
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久卷和声明
- en: 'Volumes and claims live at the foundation of storage in Kubernetes. These are
    exposed using the [PersistentVolume](https://oreil.ly/7_OAz) and [PersistentVolumeClaim](https://oreil.ly/PKtAr)
    APIs. The PersistentVolume resource represents a storage volume known to Kubernetes.
    Let’s assume an administrator has prepared a node to offer 30Gi of fast, on-host,
    storage. Let’s also assume the administrator has provisioned this storage at */mnt/fast-disk/pod-0*.
    To represent this volume in Kubernetes, the administrator can then create a PersistentVolume
    object:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，卷和声明是存储的基础。这些通过 [PersistentVolume](https://oreil.ly/7_OAz) 和
    [PersistentVolumeClaim](https://oreil.ly/PKtAr) API 公开。PersistentVolume 资源表示 Kubernetes
    中已知的存储卷。假设管理员已经准备好一个节点，提供了 30Gi 的快速本地存储在 */mnt/fast-disk/pod-0*。为了在 Kubernetes
    中表示这个卷，管理员可以创建一个 PersistentVolume 对象：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_container_storage_CO1-1)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO1-1)'
- en: The amount of storage available in this volume. Used to determine whether a
    claim can bind to this volume.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此卷中可用的存储量。用于确定声明是否能绑定到此卷。
- en: '[![2](assets/2.png)](#co_container_storage_CO1-2)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO1-2)'
- en: Specifies whether the volume is a [block device](https://oreil.ly/mrHwE) or
    filesystem.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 指定卷是 [块设备](https://oreil.ly/mrHwE) 还是文件系统。
- en: '[![3](assets/3.png)](#co_container_storage_CO1-3)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_container_storage_CO1-3)'
- en: Specifies the access mode of the volume. Includes `ReadWriteOnce`, `ReadMany`,
    and `ReadWriteMany`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 指定卷的访问模式。包括 `ReadWriteOnce`、`ReadMany` 和 `ReadWriteMany`。
- en: '[![4](assets/4.png)](#co_container_storage_CO1-4)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_container_storage_CO1-4)'
- en: Associates this volume with a storage class. Used to pair an eventual claim
    to this volume.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将此卷与存储类关联，用于将最终的声明与此卷配对。
- en: '[![5](assets/5.png)](#co_container_storage_CO1-5)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_container_storage_CO1-5)'
- en: Identifies which node this volume should be associated with.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 标识应该将此卷关联到哪个节点。
- en: 'As you can see, the PersistentVolume contains details around the implementation
    of the volume. To provide one more layer of abstraction, a PersistentVolumeClaim
    is introduced, which binds to an appropriate volume based on its request. Most
    commonly, this will be defined by the application team, added to their Namespace,
    and referenced from their Pod:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，PersistentVolume 包含有关卷实现的详细信息。为了提供更高一层的抽象，引入了 PersistentVolumeClaim，它根据其请求绑定到适当的卷。通常情况下，这将由应用团队定义，并添加到他们的
    Namespace，并从他们的 Pod 引用：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_container_storage_CO2-1)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO2-1)'
- en: Checks for a volume that is of the class `local-storage` with the access mode
    `ReadWriteOnce`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 检查是否有一个类为 `local-storage`，访问模式为 `ReadWriteOnce` 的卷。
- en: '[![2](assets/2.png)](#co_container_storage_CO2-2)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO2-2)'
- en: Binds to a volume with >= `30Gi` of storage.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 绑定到一个具有 >= `30Gi` 存储量的卷。
- en: '[![3](assets/3.png)](#co_container_storage_CO2-3)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_container_storage_CO2-3)'
- en: Declares this Pod a consumer of the PersistentVolumeClaim.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 声明此 Pod 作为 PersistentVolumeClaim 的消费者。
- en: Based on the PersistentVolume’s `nodeAffinity` settings, the Pod will be automatically
    scheduled on the host where this volume is available. There is no additional affinity
    configuration required from the developer.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 PersistentVolume 的 `nodeAffinity` 设置，Pod 将自动调度到具有此卷的主机上。开发者无需额外配置亲和性。
- en: This process has demonstrated a very manual flow for how administrators could
    make this storage available to developers. We refer to this as static provisioning.
    With proper automation this could be a viable way to expose fast disk on hosts
    to Pods. For example, the [Local Persistence Volume Static Provisioner](https://oreil.ly/YiQ0G)
    can be deployed to the cluster to detect preallocated storage and expose it, automatically,
    as PersistentVolumes. It also provides some life cycle management capabilities
    such as deleting data upon destruction of the PersistentVolumeClaim.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程展示了管理员如何将这些存储提供给开发人员的非常手动的流程。我们称之为静态配置。通过适当的自动化，这可以成为向 Pod 暴露快速磁盘的可行方式。例如，可以部署[本地持久卷静态提供程序](https://oreil.ly/YiQ0G)到集群中，以检测预分配的存储并自动将其暴露为持久卷。它还提供一些生命周期管理功能，例如在销毁持久卷索赔时删除数据。
- en: Warning
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: There are multiple ways to achieve local storage that can lead you into a bad
    practice. For example, it can seem compelling to allow developers to use [hostPath](https://oreil.ly/PAU8Y)
    rather than needing to preprovision a local storage. `hostPath` enables you to
    specify a path on the host to bind to rather than having to use a PersistentVolume
    and PersistentVolumeClaim. This can be a huge security risk as it enables developers
    to bind to directories on the host, which can have a negative impact on the host
    and other Pods. If you desire to provide developers ephemeral storage that can
    withstand a Pod restart but not the Pod being deleted or moved to a different
    node, you can use [EmptyDir](https://oreil.ly/mPwBg). This will allocate storage
    in the filesystem managed by Kube and be transparent to the Pod.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以实现本地存储，这可能导致不良实践。例如，允许开发人员使用[hostPath](https://oreil.ly/PAU8Y)似乎很有吸引力，而不需要预先配置本地存储。`hostPath`
    允许您指定绑定到主机的路径，而不必使用 PersistentVolume 和 PersistentVolumeClaim。这可能是一个巨大的安全风险，因为它允许开发人员绑定到主机上的目录，这可能会对主机和其他
    Pod 产生负面影响。如果您希望为开发人员提供能够经受 Pod 重启但不能承受 Pod 被删除或移动到不同节点的临时存储，可以使用[EmptyDir](https://oreil.ly/mPwBg)。这将在由
    Kube 管理的文件系统中分配存储，并对 Pod 透明。
- en: Storage Classes
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储类
- en: 'In many environments, expecting nodes to be prepared ahead of time with disks
    and volumes is unrealistic. These cases often warrant dynamic provisioning, where
    volumes can be made available based on the needs of our claims. To facilitate
    this model, we can make classes of storage available to our developers. These
    are defined using the [StorageClass](https://oreil.ly/MoG_T) API. Assuming your
    cluster runs in AWS and you want to offer EBS volumes to Pods dynamically, the
    following StorageClass can be added:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多环境中，期望节点提前准备好磁盘和卷是不现实的。这些情况通常需要动态配置，可以根据我们索赔的需要提供卷。为了促进这种模型，我们可以向开发人员提供存储类。这些使用[StorageClass](https://oreil.ly/MoG_T)
    API 定义。假设您的集群在 AWS 上运行，并且想要动态地向 Pod 提供 EBS 卷，可以添加以下 StorageClass：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_container_storage_CO3-1)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO3-1)'
- en: The name of the StorageClass that can be referenced from claims.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从索赔引用的 StorageClass 名称。
- en: '[![2](assets/2.png)](#co_container_storage_CO3-2)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO3-2)'
- en: Sets this StorageClass as the default. If a claim does not specify a class,
    this will be used.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将此 StorageClass 设置为默认值。如果索赔未指定类，则将使用此值。
- en: '[![3](assets/3.png)](#co_container_storage_CO3-3)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_container_storage_CO3-3)'
- en: Uses the `aws-ebs` provisioner to create the volumes based on claims.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`aws-ebs`提供程序根据索赔创建卷。
- en: '[![4](assets/4.png)](#co_container_storage_CO3-4)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_container_storage_CO3-4)'
- en: Provider-specific configuration for how to provision volumes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 用于配置特定于提供程序的卷配置。
- en: You can offer a variety of storage options to developers by making multiple
    StorageClasses available. This includes supporting more than one provider in a
    single cluster—for example, running Ceph alongside VMware vSAN. Alternatively,
    you may offer different tiers of storage via the same provider. An example would
    be offering cheaper storage alongside more expensive options. Unfortunately, Kubernetes
    lacks granular controls to limit what classes developers can request. Control
    can be implemented as validating admission control, which is covered in [Chapter 8](ch08.html#chapter8).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供多个 StorageClass，您可以向开发人员提供各种存储选项。这包括在单个集群中支持多个提供者，例如同时运行 Ceph 和 VMware vSAN。或者，您可以通过同一提供者提供不同层次的存储。例如，提供更便宜的存储选项以及更昂贵的选项。不幸的是，Kubernetes
    缺乏细粒度的控制来限制开发人员可以请求的类别。可以实现控制作为验证入场控制，这在[第 8 章](ch08.html#chapter8)中有详细介绍。
- en: Kubernetes offers a wide variety of providers including AWS EBS, Glusterfs,
    GCE PD, Ceph RBD, and many more. Historically, these providers were implemented
    in-tree. This means storage providers needed to implement their logic in the core
    Kubernetes project. This code would then get shipped in the relevant Kubernetes
    control plane components.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供各种各样的提供者，包括 AWS EBS、Glusterfs、GCE PD、Ceph RBD 等。从历史上看，这些提供者是在内核中实现的。这意味着存储提供者需要在核心
    Kubernetes 项目中实现其逻辑。然后，此代码将与相关的 Kubernetes 控制平面组件一起发布。
- en: There were several downsides to this model. For one, the storage provider could
    not be managed out of band. All changes to the provider needed to be tied to a
    Kubernetes release. Also, every Kubernetes deployment shipped with unnecessary
    code. For example, clusters running AWS still had the provider code for interacting
    with GCE PDs. It quickly became apparent there was high value in externalizing
    these provider integrations and deprecating the in-tree functionality. [FlexVolume
    drivers](https://oreil.ly/YnnCq) were an out-of-tree implementation specification
    that initially aimed to solve this problem. However, FlexVolumes have been put
    into maintenance mode in favor of our next topic, the Container Storage Interface
    (CSI).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型存在几个缺点。首先，存储提供者无法独立管理。对提供者的所有更改都必须与 Kubernetes 发布版本绑定。此外，每个 Kubernetes 部署都会包含不必要的代码。例如，运行
    AWS 的集群仍包含用于与 GCE PDs 交互的提供者代码。很快就显而易见，将这些提供者集成外部化并弃用内核功能具有高度价值。[FlexVolume 驱动程序](https://oreil.ly/YnnCq)是最初旨在解决这一问题的一种基于外部实现规范。然而，FlexVolumes
    已进入维护模式，以支持我们接下来要讨论的容器存储接口（CSI）。
- en: The Container Storage Interface (CSI)
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器存储接口（CSI）
- en: The Container Storage Interface is the answer to how we provide block and file
    storage to our workloads. The implementations of CSI are referred to as drivers,
    which have the operational knowledge for talking to storage providers. These providers
    span from cloud systems such as [Google Persistent Disks](https://cloud.google.com/persistent-disk)
    to storage systems (such as [Ceph](https://ceph.io)) deployed and managed by you.
    The drivers are implemented by storage providers in projects that live out-of-tree.
    They can be entirely managed out of band from the cluster they are deployed within.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 容器存储接口是我们如何为工作负载提供块和文件存储的答案。CSI 的实现被称为驱动程序，这些驱动程序具有与存储提供者通信的操作知识。这些提供者涵盖从诸如[Google
    持久磁盘](https://cloud.google.com/persistent-disk)等云系统到由您部署和管理的存储系统（例如[Ceph](https://ceph.io)）。这些驱动程序由存储提供者在项目中实现，这些项目独立于集群之外完全管理。
- en: At a high level, CSI implementations feature a controller plug-in and a node
    plug-in. CSI driver developers have a lot of flexibility in how they implement
    these components. Typically, implementations bundle the controller and node plug-ins
    in the same binary and enable either mode via an environment variable such as
    `X_CSI_MODE`. The only expectations are that the driver registers with the kubelet
    and the endpoints in the CSI specification are implemented.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，CSI 的实现包括控制器插件和节点插件。CSI 驱动程序开发人员在实现这些组件时具有很大的灵活性。通常，实现会将控制器和节点插件捆绑在同一个二进制文件中，并通过环境变量（例如
    `X_CSI_MODE`）启用任一模式。唯一的期望是驱动程序在 kubelet 中注册，并实现 CSI 规范中的端点。
- en: The controller service is responsible for managing the creation and deletion
    of volumes in the storage provider. This functionality extends into (optional)
    features such as taking volume snapshots and expanding volumes. The node service
    is responsible for preparing volumes to be consumed by Pods on the node. Often
    this means setting up the mounts and reporting information about volumes on the
    node. Both the node and controller service also implement identity services that
    report plug-in info, capabilities, and whether the plug-in is healthy. With this
    in mind, [Figure 4-1](#cluster_running_a_csi_plugin_the_driver) represents a cluster
    architecture with these components deployed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器服务负责管理存储提供程序中卷的创建和删除。此功能扩展到（可选）功能，例如获取卷快照和扩展卷。节点服务负责准备卷以供节点上的 Pod 使用。通常意味着设置挂载点并报告有关节点上卷的信息。节点和控制器服务还实现了报告插件信息、能力以及插件是否健康的身份服务。因此，[图
    4-1](#cluster_running_a_csi_plugin_the_driver) 描述了部署了这些组件的集群架构。
- en: '![prku 0401](assets/prku_0401.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0401](assets/prku_0401.png)'
- en: Figure 4-1\. Cluster running a CSI plug-in. The driver runs in a node and controller
    mode. The controller is typically run as a Deployment. The node service is deployed
    as a DaemonSet, which places a Pod on each host.
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 集群运行 CSI 插件。驱动程序以节点和控制器模式运行。控制器通常作为 Deployment 运行。节点服务部署为 DaemonSet，在每个主机上放置一个
    Pod。
- en: Let’s take a deeper look at these two components, the controller and the node.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解这两个组件，即控制器和节点。
- en: CSI Controller
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSI 控制器
- en: 'The CSI Controller service provides APIs for managing volumes in a persistent
    storage system. The Kubernetes control plane *does not* interact with the CSI
    Controller service directly. Instead, controllers maintained by the Kubernetes
    storage community react to Kubernetes events and translate them into CSI instructions,
    such as CreateVolumeRequest when a new PersistentVolumeClaim is created. Because
    the CSI Controller service exposes its APIs over UNIX sockets, the controllers
    are usually deployed as sidecars alongside the CSI Controller service. There are
    multiple external controllers, each with different behavior:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: CSI 控制器服务提供管理持久存储系统中卷的 API。Kubernetes 控制平面 *不直接* 与 CSI 控制器服务交互。相反，由 Kubernetes
    存储社区维护的控制器会对 Kubernetes 事件做出反应，并将其转换为 CSI 指令，例如在创建新的 PersistentVolumeClaim 时的
    CreateVolumeRequest。由于 CSI 控制器服务通过 UNIX sockets 公开其 API，控制器通常作为 CSI 控制器服务的 sidecar
    部署。有多个外部控制器，每个控制器具有不同的行为：
- en: external-provisioner
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: external-provisioner
- en: When PersistentVolumeClaims are created, this requests a volume be created from
    the CSI driver. Once the volume is created in the storage provider, this provisioner
    creates a PersistentVolume object in Kubernetes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 PersistentVolumeClaims 时，请求从 CSI 驱动程序创建卷。一旦在存储提供程序中创建卷，此提供程序将在 Kubernetes
    中创建 PersistentVolume 对象。
- en: external-attacher
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: external-attacher
- en: Watches the VolumeAttachment objects, which declare that a volume should be
    attached or detached from a node. Sends the attach or detach request to the CSI
    driver.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 监视 VolumeAttachment 对象，声明应将卷附加到节点或从节点分离。向 CSI 驱动程序发送附加或分离请求。
- en: external-resizer
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: external-resizer
- en: Detects storage-size changes in PersistentVolumeClaims. Sends requests for expansion
    to the CSI driver.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 检测 PersistentVolumeClaims 中存储大小的变化。向 CSI 驱动程序发送扩展请求。
- en: external-snapshotter
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: external-snapshotter
- en: When VolumeSnapshotContent objects are created, snapshot requests are sent to
    the driver.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 VolumeSnapshotContent 对象时，会向驱动程序发送快照请求。
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When implementing CSI plug-ins, developers are not required to use the aforementioned
    controllers. However, their use is encouraged to prevent duplication of logic
    in every CSI plug-in.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现 CSI 插件时，开发者并非必须使用上述控制器。但是，鼓励使用这些控制器，以防止在每个 CSI 插件中重复逻辑。
- en: CSI Node
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSI 节点
- en: 'The Node plug-in typically runs the same driver code as the controller plug-in.
    However, running in the “node mode” means it is focused on tasks such as mounting
    attached volumes, establishing their filesystem, and mounting volumes to Pods.
    Requests for these behaviors is done via the kubelet. Along with the driver, the
    following sidecars are often included in the Pod:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 节点插件通常运行与控制器插件相同的驱动程序代码。但是，“节点模式”中的运行意味着专注于任务，例如挂载附加的卷、建立其文件系统以及将卷挂载到 Pod。这些行为的请求通过
    kubelet 完成。除了驱动程序外，通常还在 Pod 中包括以下 sidecars：
- en: node-driver-registrar
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: node-driver-registrar
- en: Sends a [registration request](https://oreil.ly/kmkJh) to the kubelet to make
    it aware of the CSI driver.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 发送 [注册请求](https://oreil.ly/kmkJh) 到 kubelet 以使其意识到 CSI 驱动程序。
- en: liveness-probe
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 活动探针
- en: Reports the health of the CSI driver.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 报告 CSI 驱动程序的健康状况。
- en: Implementing Storage as a Service
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施存储即服务
- en: We have now covered key considerations for application storage, storage primitives
    available in Kubernetes, and driver integration using the CSI. Now it’s time to
    bring these ideas together and look at an implementation that offers developers
    storage as a service. We want to provide a declarative way to request storage
    and make it available to workloads. We also prefer to do this dynamically, not
    requiring an administrator to preprovision and attach volumes. Rather, we’d like
    to achieve this on demand based on the needs of workloads.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了应用程序存储的关键考虑因素，Kubernetes 中可用的存储原语以及使用 CSI 进行驱动程序集成。现在是时候将这些想法结合起来，看一看提供开发人员存储即服务的实现。我们希望以声明方式提供请求存储并使其可用于工作负载的方法。我们还希望动态实现这一点，不需要管理员预先配置和附加卷。相反，我们希望根据工作负载的需求随需提供。
- en: In order to get started with this implementation, we’ll use Amazon Web Services
    (AWS). This example integrates with AWS’s [elastic block](https://oreil.ly/I4VVw)
    storage system. If your choice in provider differs, the majority of this content
    will still be relevant! We are simply using this provider as a concrete example
    of how all the pieces fit together.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始这个实现，我们将使用亚马逊网络服务（AWS）。此示例集成了 AWS 的 [弹性块](https://oreil.ly/I4VVw) 存储系统。如果您选择的提供商不同，这些内容的大部分仍然相关！我们只是使用这个提供商作为所有部分如何组合在一起的具体示例。
- en: Next we are going to dive into installation of the integration/driver, exposing
    storage options to developers, consuming the storage with workloads, resizing
    volumes, and taking volume snapshots.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将深入探讨集成/驱动程序的安装，向开发人员公开存储选项，使用工作负载消耗存储空间，调整卷大小以及获取卷快照。
- en: Installation
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'Installation is a fairly straightforward process consisting of two key steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程相对直接，主要包括两个关键步骤：
- en: Configure access to the provider.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置访问提供商。
- en: Deploy the driver components to the cluster.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将驱动程序组件部署到集群中。
- en: 'The provider, in this case AWS, will require the driver to identify itself,
    ensuring it has appropriate access. In this case, we have three options available
    to us. One is to update the [instance profile](https://oreil.ly/fGWYd) of the
    Kubernetes nodes. This will prevent us from worrying about credentials at the
    Kubernetes level but will provide universal privileges to workloads that can reach
    the AWS API. The second and likely most secure option is to introduce an identity
    service that can provide IAM permissions to specific workloads. A project that
    is an example of this is [kiam](https://github.com/uswitch/kiam). This approach
    is covered in [Chapter 10](ch10.html#chapter10). Lastly, you can add credentials
    in a secret that gets mounted into the CSI driver. In this model, the secret would
    look as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，供应商 AWS 将要求驱动程序标识自身，确保其具有适当的访问权限。在这种情况下，我们有三个可选项可供选择。其中一种方法是更新 [实例配置文件](https://oreil.ly/fGWYd)
    的 Kubernetes 节点。这将使我们不必担心 Kubernetes 级别的凭证，但将为能够访问 AWS API 的工作负载提供通用权限。第二种，也可能是最安全的选择是引入一个身份服务，可以为特定工作负载提供
    IAM 权限。一个示例项目是 [kiam](https://github.com/uswitch/kiam)。这种方法在 [第 10 章](ch10.html#chapter10)
    中有详细讲解。最后，您可以在挂载到 CSI 驱动程序的密钥中添加凭证。在这种模型中，密钥将如下所示：
- en: '[PRE3]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Warning
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This account will have access to manipulating an underlying storage system.
    Access to this secret should be carefully managed. See [Chapter 7](ch07.html#chapter7)
    for more information.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 该帐户将有权访问操作底层存储系统。应仔细管理对此密钥的访问。有关更多信息，请参阅 [第 7 章](ch07.html#chapter7)。
- en: 'With this configuration in place, the CSI components may be installed. First,
    the controller is installed as a Deployment. When running multiple replicas, it
    will use leader-election to determine which instance should be active. Then, the
    node plug-in is installed, which comes in the form of a DaemonSet running a Pod
    on every node. Once initialized, the instances of the node plug-in will register
    with their kubelets. The kubelet will then report the CSI-enabled node by creating
    a CSINode object for every Kubernetes node. The output of a three-node cluster
    is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这样的配置，CSI组件可以安装。首先，控制器作为一个Deployment安装。在运行多个副本时，它将使用领导选举确定哪个实例应处于活动状态。然后安装节点插件，它作为DaemonSet形式在每个节点上运行一个Pod。初始化后，节点插件的实例将向其kubelet注册。kubelet然后通过为每个Kubernetes节点创建一个CSINode对象来报告启用CSI的节点。一个三节点集群的输出如下：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we can see, there are three nodes listed with one driver registered on each
    node. Examining the YAML of one CSINode exposes the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，列出了三个节点，每个节点上注册了一个驱动程序。检查一个CSINode的YAML文件揭示了以下内容：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_container_storage_CO4-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO4-1)'
- en: The maximum number of volumes allowed on this node.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最大允许在此节点上的卷数。
- en: '[![2](assets/2.png)](#co_container_storage_CO4-2)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO4-2)'
- en: When a node is picked for a workload, this value will be passed in the CreateVolumeRequest
    so that the driver knows *where* to create the volume. This is important for storage
    systems where nodes in the cluster won’t have access to the same storage. For
    example, in AWS, when a Pod is scheduled in an availability zone, the Volume must
    be created in the same zone.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择节点用于工作负载时，此值将传递给CreateVolumeRequest，以便驱动程序知道*在哪里*创建卷。对于存储系统而言，集群中的节点无法访问相同的存储是很重要的。例如，在AWS中，当Pod被调度到可用区时，卷必须在相同的区域创建。
- en: 'Additionally, the driver is officially registered with the cluster. The details
    can be found in the CSIDriver object:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，驱动程序已在集群中正式注册。详细信息可以在CSIDriver对象中找到：
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_container_storage_CO5-1)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO5-1)'
- en: The name of the provider representing this driver. This name will be bound to
    class(es) of storage we offer to platform users.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 代表此驱动程序的提供者名称。此名称将绑定到我们为平台用户提供的存储类。
- en: '[![2](assets/2.png)](#co_container_storage_CO5-2)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO5-2)'
- en: Specifies that an attach operation must be completed before volumes are mounted.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 指定必须在挂载卷之前完成附加操作。
- en: '[![3](assets/3.png)](#co_container_storage_CO5-3)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_container_storage_CO5-3)'
- en: Does not need to pass Pod metadata in as context when setting up a mount.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置挂载时，不需要将Pod元数据传递为上下文。
- en: '[![4](assets/4.png)](#co_container_storage_CO5-4)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_container_storage_CO5-4)'
- en: The default model for provisioning persistent volumes. [Inline support](https://oreil.ly/Z_pDY)
    can be enabled by setting this option to `Ephemeral`. In the ephemeral mode, the
    storage is expected to last only as long as the Pod.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 永久卷的默认模型。[内联支持](https://oreil.ly/Z_pDY)可以通过将此选项设置为`Ephemeral`来启用。在短暂模式中，存储预期只持续与Pod一样长的时间。
- en: The settings and objects we have explored so far are artifacts of our bootstrapping
    process. The CSIDriver object makes for easier discovery of driver details and
    was included in the driver’s deployment bundle. The CSINode objects are managed
    by the kubelet. A generic registrar sidecar is included in the node plug-in Pod
    and gets details from the CSI driver and registers the driver with the kubelet.
    The kubelet then reports up the quantity of CSI drivers available on each host.
    [Figure 4-2](#csidriver_object_is_deployed_and_part_of_the_bundle_while_the_node_plugin_registers_with_the_kubelet)
    demonstrates this bootstrapping process.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们探讨的设置和对象都是我们引导过程的产物。CSIDriver对象使得可以更轻松地发现驱动程序的详细信息，并且包含在驱动程序的部署包中。CSINode对象由kubelet管理。通用的注册器旁路进程包含在节点插件Pod中，并从CSI驱动程序获取详细信息，并将驱动程序注册到kubelet中。然后kubelet报告每个主机上可用的CSI驱动程序数量。[Figure 4-2](#csidriver_object_is_deployed_and_part_of_the_bundle_while_the_node_plugin_registers_with_the_kubelet)展示了这个引导过程。
- en: '![prku 0402](assets/prku_0402.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0402](assets/prku_0402.png)'
- en: Figure 4-2\. CSIDriver object is deployed and part of the bundle while the node
    plug-in registers with the kubelet. This in turn creates/manages the CSINode objects.
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. CSIDriver对象已部署并包含在包中，同时节点插件与kubelet注册。这反过来创建/管理CSINode对象。
- en: Exposing Storage Options
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 暴露存储选项
- en: In order to provide storage options to developers, we need to create StorageClasses.
    For this scenario we’ll assume there are two types of storage we’d like to expose.The
    first option is to expose cheap disk that can be used for workload persistence
    needs. Many times, applications don’t need an SSD as they are just persisting
    some files that do not require quick read/write. As such, the cheap disk (HDD)
    will be the default option. Then we’d like to offer faster SSD with a custom [IOPS](https://oreil.ly/qXMcQ)
    per gigabyte configured. [Table 4-1](#table_4-1) shows our offerings; prices reflect
    AWS costs at the time of this writing.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为开发者提供存储选项，我们需要创建StorageClasses。对于这种情况，我们假设有两种类型的存储我们想要暴露。第一个选项是提供便宜的磁盘，可以用于工作负载持久性需求。许多时候，应用程序不需要SSD，因为它们只是持久化一些不需要快速读写的文件。因此，便宜的磁盘（HDD）将是默认选项。然后，我们希望提供更快的SSD，每GB配置一个定制的[IOPS](https://oreil.ly/qXMcQ)。[Table 4-1](#table_4-1)展示了我们的提供；价格反映了本文撰写时的AWS成本。
- en: Table 4-1\. Storage offerings
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Table 4-1\. 存储提供
- en: '| Offering name | Storage type | Max throughput per volume | AWS cost |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 提供名称 | 存储类型 | 每卷最大吞吐量 | AWS成本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| default-block | HDD (optimized) | 40–90 MB/s | $0.045 per GB per month |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| default-block | HDD（优化） | 40–90 MB/s | $0.045每GB每月 |'
- en: '| performance-block | SSD (io1) | ~1000 MB/s | $0.125 per GB per month + $0.065
    per provisioned IOPS per month |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| performance-block | SSD（io1） | ~1000 MB/s | $0.125每GB每月 + $0.065每预配的IOPS每月
    |'
- en: In order to create these offerings, we’ll create a storage class for each. Inside
    each storage class is a `parameters` field. This is where we can configure settings
    that satisfy the features in [Table 4-1](#table_4-1).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这些产品，我们将为每个创建一个存储类。在每个存储类内部有一个`parameters`字段。这是我们可以配置以满足[Table 4-1](#table_4-1)中功能的设置的地方。
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_container_storage_CO6-1)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO6-1)'
- en: This is the name of the storage offering we are providing to platform users.
    It will be referenced from PeristentVolumeClaims.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们为平台用户提供的存储提供的名称。它将被从PeristentVolumeClaims引用。
- en: '[![2](assets/2.png)](#co_container_storage_CO6-2)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO6-2)'
- en: This sets the offering as the default. If a PersistentVolumeClaim is created
    *without* a StorageClass specified, `default-block` will be used.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这将设置提供作为默认选项。如果创建PersistentVolumeClaim时未指定StorageClass，则将使用`default-block`。
- en: '[![3](assets/3.png)](#co_container_storage_CO6-3)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_container_storage_CO6-3)'
- en: Mapping to which CSI driver should be executed.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 映射到应执行哪个CSI驱动程序。
- en: '[![4](assets/4.png)](#co_container_storage_CO6-4)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_container_storage_CO6-4)'
- en: Allow expansion of the volume size via changes to a PersistentVolumeClaim.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 允许通过更改PersistentVolumeClaim来扩展卷大小。
- en: '[![5](assets/5.png)](#co_container_storage_CO6-5)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_container_storage_CO6-5)'
- en: Do not provision the volume until a Pod consumes the PersistentVolumeClaim.
    This will ensure the volume is created in the appropriate availability zone of
    the scheduled Pod. It also prevents orphaned PVCs from creating volumes in AWS,
    which you will be billed for.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pod消耗PersistentVolumeClaim之前不要预留卷。这将确保在安排的Pod的适当可用性区域中创建卷。它还防止了孤立的PVC在AWS中创建卷，这些卷将会被计费。
- en: '[![6](assets/6.png)](#co_container_storage_CO6-6)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_container_storage_CO6-6)'
- en: Specifies what type of storage the driver should acquire to satisfy claims.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 指定驱动程序应获取什么类型的存储以满足要求。
- en: '[![7](assets/7.png)](#co_container_storage_CO6-7)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_container_storage_CO6-7)'
- en: Second class, tuned to high-performance SSD.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类，调整为高性能SSD。
- en: Consuming Storage
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消费存储
- en: 'With the preceding pieces in place, we are now ready for users to consume these
    different classes of storage. We will start by looking at the developer experience
    of requesting storage. Then we’ll walk through the internals of how it is satisfied.
    To start off, let’s see what a developer gets when listing available StorageClasses:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分准备好之后，我们现在可以让用户使用这些不同类别的存储了。我们将首先查看开发人员请求存储时的体验。然后我们将详细介绍如何满足这些请求。首先，让我们看看当列出可用的StorageClasses时开发者会得到什么：
- en: '[PRE8]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Warning
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: By enabling developers to create PVCs, we will be allowing them to reference
    *any* StorageClass. If this is problematic, you may wish to consider implementing
    Validating Admission control to assess whether requests are appropriate. This
    topic is covered in [Chapter 8](ch08.html#chapter8).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过允许开发人员创建PVCs，我们将允许他们引用*任何*StorageClass。如果这是一个问题，您可能希望考虑实施验证入场控制来评估请求是否合适。这个主题在[Chapter 8](ch08.html#chapter8)中有详细介绍。
- en: 'Let’s assume the developer wants to make a cheaper HDD and more performant
    SSD available for an application. In this case, two PersistentVolumeClaims are
    created. We’ll refer to these as `pvc0` and `pvc1`, respectively:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 假设开发者想要为一个应用程序提供更便宜的 HDD 和更高性能的 SSD。在这种情况下，将创建两个 PersistentVolumeClaims。我们将分别称之为`pvc0`和`pvc1`：
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_container_storage_CO7-1)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO7-1)'
- en: This will use the default storage class (`default-block`) and assume other defaults
    such as RWO and filesystem storage type.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用默认存储类(`default-block`)，并假设其他默认设置如 RWO 和文件系统存储类型。
- en: '[![2](assets/2.png)](#co_container_storage_CO7-2)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO7-2)'
- en: Ensure `performance-block` is requested to the driver rather than `default-block`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 确保向驱动程序请求`performance-block`而不是`default-block`。
- en: Based on the StorageClass settings, these two will exhibit different provisioning
    behaviors. The performant storage (from `pvc1`) is created as an unattached volume
    in AWS. This volume can be attached quickly and is ready to use. The default storage
    (from `pv0`) will sit in a `Pending` state where the cluster waits until a Pod
    consumes the PVC to provision storage in AWS. While this will require more work
    to provision when a Pod finally consumes the claim, you will not be billed for
    the unused storage! The relationship between the claim in Kubernetes and volume
    in AWS can be seen in [Figure 4-3](#pv1_is_provisioned_as_a_volume_in_aws).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 StorageClass 的设置，这两个将表现出不同的供应行为。高性能存储（来自`pvc1`）在 AWS 中被创建为一个未附加的卷。这种卷可以快速附加并且随时可用。默认存储（来自`pv0`）将处于`Pending`状态，集群会等待直到一个
    Pod 使用 PVC 来在 AWS 中配置存储。虽然当一个 Pod 最终使用声明时，这将需要更多的工作来配置，但您不会因为未使用的存储而被计费！Kubernetes
    中声明与 AWS 中卷之间的关系可以在 [图 4-3](#pv1_is_provisioned_as_a_volume_in_aws) 中看到。
- en: '![prku 0403](assets/prku_0403.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0403](assets/prku_0403.png)'
- en: Figure 4-3\. `pv1` is provisioned as a volume in AWS, and the CSIVolumeName
    is propagated for ease of correlation. `pv0` will not have a respective volume
    created until a Pod references it.
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. `pv1` 在 AWS 中被配置为一个卷，并且 CSIVolumeName 被传播以便于关联。直到一个 Pod 引用它之前，`pv0`
    将不会创建相应的卷。
- en: Now let’s assume the developer creates two Pods. One Pod references `pv0` while
    the other references `pv1`. Once each Pod is scheduled on a Node, the volume will
    be attached to that node for consumption. For `pv0`, before this can occur the
    volume will also be created in AWS. With the Pods scheduled and volumes attached,
    a filesystem is established and the storage is mounted into the container. Because
    these are persistent volumes, we have now introduced a model where even if the
    Pod is rescheduled to another node, the volume can come with it. The end-to-end
    flow for how we’ve satisfied the self-service storage request is shown in [Figure 4-4](#end_to_end_flow_of_the_driver_and_kubernetes).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设开发者创建了两个 Pod。一个 Pod 引用`pv0`，而另一个引用`pv1`。一旦每个 Pod 被调度到一个节点上，卷将附加到该节点以供使用。对于`pv0`，在此之前还需要在
    AWS 中创建卷。当 Pod 被调度并且卷被附加后，文件系统就建立起来，存储就被挂载到容器中。由于这些是持久卷，我们现在引入了一种模式，即使 Pod 被重新调度到另一个节点，卷也可以随之移动。我们已经在
    [图 4-4](#end_to_end_flow_of_the_driver_and_kubernetes) 中展示了我们如何满足自服务存储请求的端到端流程。
- en: '![prku 0404](assets/prku_0404.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0404](assets/prku_0404.png)'
- en: Figure 4-4\. End-to-end flow of the driver and Kubernetes working together to
    satisfy the storage request.
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 驱动程序和 Kubernetes 协作满足存储请求的端到端流程。
- en: Note
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Events are particularly helpful in debugging storage interaction with CSI. Because
    provisioning, attaching, and mounting are all happening in order to satisfy a
    PVC, you should view events on these objects as different components report what
    they have done. `kubectl describe -n $NAMESPACE pvc $PVC_NAME` is an easy way
    to view these events.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 事件在调试与 CSI 的存储交互中特别有帮助。因为为了满足 PVC，需要按顺序进行供应、附加和挂载，您应该查看这些对象上的事件，以便不同组件报告它们所做的工作。使用`kubectl
    describe -n $NAMESPACE pvc $PVC_NAME`可以轻松查看这些事件。
- en: Resizing
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整大小
- en: Resizing is a supported feature in the `aws-ebs-csi-driver`. In most CSI implementations,
    the `external-resizer` controller is used to detect changes in PersistentVolumeClaim
    objects. When a size change is detected, it is forwarded to the driver, which
    will expand the volume. In this case, the driver running in the controller plug-in
    will facilitate expansion with the AWS EBS API.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 调整大小是`aws-ebs-csi-driver`中支持的特性。在大多数 CSI 实现中，`external-resizer`控制器用于检测持久卷声明对象的更改。当检测到大小变化时，它会转发给驱动程序，驱动程序将扩展卷。在这种情况下，运行在控制器插件中的驱动程序将利用
    AWS EBS API 进行扩展。
- en: 'Once the volume is expanded in EBS, the new space is *not* immediately usable
    to the container. This is because the filesystem still occupies only the original
    space. In order for the filesystem to expand, we’ll need to wait for the node
    plug-in’s driver instance to expand the filesystem. This can all be done *without*
    terminating the Pod. The filesystem expansion can be seen in the following logs
    from the node plug-in’s CSI driver:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 EBS 卷后，新空间*不*立即可用于容器。这是因为文件系统仍然仅占用原始空间。为了扩展文件系统，我们需要等待节点插件驱动实例扩展文件系统。所有这些操作都可以在*不*终止
    Pod 的情况下完成。文件系统的扩展可以在节点插件的 CSI 驱动程序的以下日志中看到：
- en: '[PRE10]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Warning
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Kubernetes does not support downsizing a PVC’s size field. Unless the CSI-driver
    provides a workaround for this, you may not be able to downsize without re-creating
    a volume. Keep this in mind when growing volumes.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 不支持缩小 PVC 的 size 字段。除非 CSI 驱动程序提供此类解决方法，否则您可能无法在不重新创建卷的情况下缩小卷的大小。在扩展卷时，请牢记这一点。
- en: Snapshots
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快照
- en: To facilitate periodic backups of volume data used by containers, snapshot functionality
    is available. The functionality is often broken into two controllers, which are
    responsible for two different CRDs. The CRDs include VolumeSnapshot and VolumeContentSnapshot.
    At a high-level, the VolumeSnapshot is responsible for the life cycle of volumes.
    Based on these objects, VolumeContentSnapshots are managed by the external-snapshotter
    controller. This controller is typically run as a sidecar in the CSI’s controller
    plug-in and forwards requests to the driver.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便容器使用的卷数据的定期备份，提供了快照功能。功能通常分为两个控制器，分别负责两个不同的 CRD。CRD 包括 VolumeSnapshot 和
    VolumeContentSnapshot。在高层次上，VolumeSnapshot 负责卷的生命周期。基于这些对象，VolumeContentSnapshot
    由 external-snapshotter 控制器管理。该控制器通常作为 CSI 控制器插件的 sidecar 运行，并将请求转发给驱动程序。
- en: Note
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of this writing, these objects are implemented as CRDs and not core
    Kubernetes API objects. This requires the CSI driver or Kubernetes distribution
    to deploy the CRD definitions ahead of time.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，这些对象实现为 CRD，而不是核心 Kubernetes API 对象。这要求 CSI 驱动程序或 Kubernetes 发行版预先部署
    CRD 定义。
- en: 'Similar to offering storage via StorageClasses, snapshotting is offered by
    introducing a Snapshot class. The following YAML represents this class:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过 StorageClasses 提供存储类似，通过引入 Snapshot 类提供快照功能。以下 YAML 表示了此类：
- en: '[PRE11]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_container_storage_CO8-1)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO8-1)'
- en: Which driver to delegate snapshot request to.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 委托快照请求的驱动程序。
- en: '[![2](assets/2.png)](#co_container_storage_CO8-2)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO8-2)'
- en: Whether the VolumeSnapshotContent should be deleted when the VolumeSnapshot
    is deleted. In effect, the actual volume could be deleted (depending on support
    from the provider).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当删除 VolumeSnapshot 时是否应删除 VolumeSnapshotContent。实际上，可能会删除实际卷（具体取决于供应商的支持）。
- en: 'In the Namespace of the application and PersistentVolumeClaim, a VolumeSnapshot
    may be created. An example is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序的命名空间和 PersistentVolumeClaim 中，可能会创建一个 VolumeSnapshot。示例如下：
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_container_storage_CO9-1)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO9-1)'
- en: Specifies the class, which informs the driver to use.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 指定要使用的类别，以通知驱动程序使用。
- en: '[![2](assets/2.png)](#co_container_storage_CO9-2)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_container_storage_CO9-2)'
- en: Specifies the volume claim, which informs the volume to snapshot.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 指定要进行快照的卷索赔。
- en: The existence of this object will inform the need to create a VolumeSnapshotContent
    object. This object has a scope of cluster-wide. The detection of a VolumeSnapshotContent
    object will cause a request to create a snapshot and the driver will satisfy this
    by communicating with AWS EBS. Once satisfied, the VolumeSnapshot will report
    ReadyToUse. [Figure 4-5](#the_various_objects_and_their_relations_that_make_up_the_snapshot_flow)
    demonstrates the relationship between the various objects.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此对象的存在将通知需要创建 VolumeSnapshotContent 对象。此对象的作用域为整个集群。检测到 VolumeSnapshotContent
    对象将导致请求创建快照，并通过与 AWS EBS 通信来满足驱动程序的请求。满足后，VolumeSnapshot 将报告 ReadyToUse。 [图 4-5](#the_various_objects_and_their_relations_that_make_up_the_snapshot_flow)
    展示了各种对象之间的关系。
- en: '![prku 0405](assets/prku_0405.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0405](assets/prku_0405.png)'
- en: Figure 4-5\. The various objects and their relations that make up the snapshot
    flow.
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 组成快照流的各种对象及其关系。
- en: 'With a snapshot in place, we can explore a scenario of data loss. Whether the
    original volume was accidentally deleted, had a failure, or was removed due to
    an accidental deletion of a PersistentVolumeClaim, we can reestablish the data.
    To do this, a new PersistentVolumeClaim is created with the `spec.dataSource`
    specified. `dataSource` supports referencing a VolumeSnapshot that can populate
    data into the new claim. The following manifest recovers from the previously created
    snapshot:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 有了快照后，我们可以探索数据丢失的场景。无论原始卷是因为意外删除、故障或因PersistentVolumeClaim的意外删除而移除，我们都可以恢复数据。为此，创建一个带有指定`spec.dataSource`的新PersistentVolumeClaim。`dataSource`支持引用VolumeSnapshot，可以将数据填充到新声明中。以下清单从先前创建的快照中恢复：
- en: '[PRE13]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_container_storage_CO10-1)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_container_storage_CO10-1)'
- en: The VolumeSnapshot instance that references the EBS snapshot to replenish the
    new PVC.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: VolumeSnapshot实例引用EBS快照以填充新的PVC。
- en: Once the Pod is re-created to reference this new claim, the last snapshotted
    state will return to the container! Now we have access to all the primitives for
    creating a robust backup and recovery solution. Solutions could range from scheduling
    snapshots via a CronJob, writing a custom controller, or using tools such as [Velero](https://velero.io)
    to back up Kubernetes objects along with data volumes on a schedule.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦重新创建Pod以引用这个新声明，最后一个快照状态将返回到容器中！现在我们可以访问所有用于创建强大的备份和恢复解决方案的基本元素。解决方案可以从通过CronJob调度快照、编写自定义控制器，到使用工具如[Velero](https://velero.io)定期备份Kubernetes对象及数据卷。
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve explored a variety of container storage topics. First,
    we want to have a deep understanding of application requirements to best inform
    our technical decision. Then we want to ensure that our underlying storage provider
    can satisfy these needs and that we have the operational expertise (when required)
    to operate them. Lastly, we should establish an integration between the orchestrator
    and the storage system, ensuring developers can get the storage they need without
    being proficient in an underlying storage system.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了各种容器存储主题。首先，我们希望深入了解应用程序的需求，以便最好地指导我们的技术决策。然后，我们希望确保我们的底层存储提供商能够满足这些需求，并且我们有操作经验（在需要时）。最后，我们应确立编排器与存储系统之间的集成，确保开发人员可以获取所需的存储，而无需精通底层存储系统。
