- en: Chapter 9\. Networking, Network Security, and Service Mesh
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 网络、网络安全和服务网格
- en: Kubernetes is effectively a manager of distributed systems across a cluster
    of connected systems. This immediately puts critical importance on how the connected
    systems communicate with one another, and networking is the key to this. Understanding
    how Kubernetes facilitates communication among the distributed services it manages
    is important for the effective application of interservice communication.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有效地是跨连接系统集群的分布式系统管理器。这立即将系统连接的方式如何与其它系统通信置于至关重要的位置，而网络是这一切的关键。了解
    Kubernetes 如何促进其管理的分布式服务之间的通信对于有效地应用服务间通信非常重要。
- en: This chapter focuses on the principles that Kubernetes places on the network
    and best practices around applying these concepts in different situations. With
    any discussion of networking, security is usually brought along for the ride.
    The traditional models of network security boundaries being controlled at the
    network layer are not absent in this new world of distributed systems in Kubernetes,
    but how they are implemented and the capabilities offered change slightly. Kubernetes
    brings along a native API for network security policies that will sound eerily
    similar to firewall rules of old.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍 Kubernetes 在网络上的原则，并围绕在不同情况下应用这些概念的最佳实践展开讨论。任何关于网络的讨论通常都会带来安全性的讨论。传统的网络安全边界模型在
    Kubernetes 中的分布式系统的新世界中并未消失，但它们的实施方式和提供的功能略有变化。Kubernetes 带来了一种原生的网络安全策略 API，听起来令人毛骨悚然地类似于旧时防火墙规则。
- en: The last section of this chapter delves into the new and scary world of service
    meshes. The term “scary” is used in jest, but it is quite the Wild West when it
    comes to service mesh technology in Kubernetes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一节深入探讨了服务网格的新世界，术语“恐怖”是开玩笑用的，但在 Kubernetes 的服务网格技术中确实是相当不可预测的领域。
- en: Kubernetes Network Principles
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络原则
- en: 'Understanding how Kubernetes uses the underlying network to facilitate communication
    among services is critical to understanding how to effectively plan application
    architectures. Usually, networking topics start to give most people major headaches.
    We are going to keep this rather simple because this is more of a best practice
    guidance than a lesson on container networking. Luckily for us, Kubernetes has
    laid down some rules of the road for networking that give us a start. The rules
    outline how communication is expected to behave between different components.
    Let’s take a closer look at each of these rules:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 如何利用底层网络来促进服务之间的通信对于有效规划应用程序架构至关重要。通常，网络主题开始让大多数人头痛起来。我们将保持简单，因为这更多是最佳实践指导，而不是容器网络的课程。幸运的是，Kubernetes
    已经为网络制定了一些规则，为我们提供了一个起点。这些规则概述了不同组件之间预期的通信行为。让我们更详细地看看每个规则：
- en: Container-to-container communication in the same pod
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 同一 Pod 中容器之间的通信
- en: All containers in the same pod share the same network space. This effectively
    allows localhost communication between the containers. It also means that containers
    in the same pod need to expose different ports. This is done using the power of
    Linux namespaces and Docker networking to allow these containers to be on the
    same local network through the use of a paused container in every pod that does
    nothing but host the networking for the pod. [Figure 9-1](#figure91) shows how
    Container A can communicate directly with Container B using localhost and the
    port number that the container is listening on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 同一 Pod 中的所有容器共享相同的网络空间。这有效地允许容器之间通过 localhost 进行通信。这也意味着同一 Pod 中的容器需要暴露不同的端口。通过
    Linux 命名空间和 Docker 网络的能力，通过在每个 Pod 中运行一个什么也不做的暂停容器来托管 Pod 的网络，从而使这些容器可以位于相同的本地网络。[图
    9-1](#figure91) 展示了容器 A 如何直接使用 localhost 和容器正在监听的端口号与容器 B 进行通信。
- en: '![kbp2 0901](assets/kbp2_0901.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0901](assets/kbp2_0901.png)'
- en: Figure 9-1\. Intrapod communication between containers
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1 两个容器之间的 Pod 内通信
- en: Pod-to-pod communication
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 与 Pod 的通信
- en: All pods need to communicate with one another without any network address translation
    (NAT). This means that the pod’s IP address that is seen by the receiving pod
    is the sender’s actual IP address. This is handled in different ways, depending
    on the network plug-in used, which we discuss in more detail later in the chapter.
    This rule is true between pods on the same node and pods that are on different
    nodes in the same cluster. This also extends to the node being able to communicate
    directly to the pod with no NAT involved. This allows host-based agents or system
    daemons to communicate to the pods as needed. [Figure 9-2](#figure92) is a representation
    of the communication processes between pods in the same node and pods in different
    nodes of the cluster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的 pod 都需要在没有任何网络地址转换（NAT）的情况下相互通信。这意味着接收 pod 看到的 pod IP 地址就是发送 pod 的实际 IP
    地址。这是通过使用不同的网络插件来处理的，具体在本章后面会详细讨论。这条规则适用于同一节点上的 pod，以及同一集群中不同节点上的 pod。这也扩展到节点能够直接与
    pod 进行通信，无需涉及 NAT。这允许基于主机的代理或系统守护程序根据需要与 pod 进行通信。[图 9-2](#figure92) 是表示同一节点内
    pod 和集群中不同节点上 pod 之间通信过程的图示。
- en: '![kbp2 0902](assets/kbp2_0902.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0902](assets/kbp2_0902.png)'
- en: Figure 9-2\. Pod-to-pod communication intra- and internode
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. Pod 对 pod 的节点内和节点间通信
- en: Service-to-pod communication
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 服务对 pod 的通信
- en: Services in Kubernetes represent a durable IP address and port that is found
    on each node that will forward all traffic to the endpoints that are mapped to
    the service. Over the different iterations of Kubernetes, the method in favor
    of enabling this has changed, but the two main methods are via the use of iptables
    or the newer IP Virtual Server (IPVS). Some cloud providers and more advanced
    implementations allow for a new eBPF-based dataplane. Most implementations today
    use the iptables implementation to enable a pseudo–Layer 4 load balancer on each
    node. [Figure 9-3](#figure93) is a visual representation of how the service is
    tied to the pods via label selectors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的服务代表了在每个节点上找到的持久 IP 地址和端口，将所有流量转发到映射到服务的端点。在 Kubernetes 的不同迭代中，启用这一功能的方法有所改变，但主要方法有通过使用
    iptables 或更新的 IP Virtual Server（IPVS）。一些云提供商和更高级的实现允许基于新的 eBPF 数据平面。今天大多数实现使用
    iptables 实现在每个节点上启用伪 Layer 4 负载均衡器。[图 9-3](#figure93) 是显示服务如何通过标签选择器与 pod 关联的视觉表示。
- en: '![kbp2 0903](assets/kbp2_0903.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0903](assets/kbp2_0903.png)'
- en: Figure 9-3\. Service-to-pod communication
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 服务对 pod 的通信
- en: Network Plug-ins
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络插件
- en: Early on, the Special Interest Group (SIG) guided the networking standards to
    more of a pluggable architecture, opening the door for numerous third-party networking
    projects, which in many cases injected value-added capabilities into Kubernetes
    workloads. These network plug-ins come in two flavors. The most basic is called
    Kubenet and is the default plug-in provided by Kubernetes natively. The second
    type of plug-in follows the Container Network Interface (CNI) specification, which
    is a generic plug-in network solution for containers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，特别兴趣组（SIG）引导网络标准向更具可插拔架构的方向发展，为 Kubernetes 工作负载打开了许多第三方网络项目的大门，在许多情况下，这些项目为
    Kubernetes 工作负载注入了增值能力。这些网络插件分为两种类型。最基本的称为 Kubenet，是 Kubernetes 本地提供的默认插件。第二种类型的插件遵循容器网络接口（CNI）规范，这是容器的通用插件网络解决方案。
- en: Kubenet
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubenet
- en: Kubenet is the most basic network plug-in that comes out of the box in Kubernetes.
    It is the simplest of the plug-ins and provides a Linux bridge, `cbr0`, that’s
    a virtual Ethernet pair for the pods connected to it. The pod then gets an IP
    address from a Classless Inter-Domain Routing (CIDR) range that is distributed
    across the nodes of the cluster. There is also an IP masquerade flag that should
    be set to allow traffic destined to IPs outside the pod CIDR range to be masqueraded.
    This obeys the rules of pod-to-pod communication because only traffic destined
    outside the pod CIDR undergoes network address translation (NAT). After the packet
    leaves a node to go to another node, some kind of routing is put in place to facilitate
    the process to forward the traffic to the correct node.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet 是 Kubernetes 中随箱即出的最基本的网络插件。它是最简单的插件，为连接到其上的 pod 提供了一个 Linux 桥接器 `cbr0`，这是一个虚拟以太网对。然后，pod
    从分布在集群节点上的无类域间路由（CIDR）范围获取 IP 地址。还有一个 IP 伪装标志，应设置为允许发送到 pod CIDR 范围外 IP 的流量进行伪装。这遵守了
    pod 对 pod 通信的规则，因为只有发送到 pod CIDR 范围外的流量才会经过网络地址转换（NAT）。当数据包离开一个节点去往另一个节点时，会放置某种路由来促进将流量正确转发到相应的节点的过程。
- en: Kubenet Best Practices
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubenet最佳实践
- en: Kubenet allows for a simple network stack and does not consume precious IP addresses
    on already crowded networks. This is especially true of cloud networks that are
    extended to on-premises datacenters.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubenet允许简单的网络堆栈，并且不会在已经拥挤的网络上消耗宝贵的IP地址。这对扩展到本地数据中心的云网络尤其重要。
- en: Ensure that the pod CIDR range is large enough to handle the potential size
    of the cluster and the pods in each cluster. The default pods per node set in
    kubelet is 110, but you can adjust this.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保Pod CIDR范围足够大，以处理集群的潜在大小和每个集群中的Pod。kubelet中设置的默认每节点Pod数为110，但您可以进行调整。
- en: Understand and plan accordingly for the route rules to properly allow traffic
    to find pods in the proper nodes. In cloud providers, this is usually automated,
    but on-premises or edge cases will require automation and solid network management.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保根据路由规则进行适当的规划，以便流量能够正确找到节点中的Pod。在云提供商中，这通常是自动化的，但在本地或边缘情况下将需要自动化和可靠的网络管理。
- en: The CNI Plug-in
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI插件
- en: The CNI plug-in has basic requirements set aside by the specification. These
    specifications dictate the interfaces and minimal API actions that the CNI offers
    and how it will interface with the container runtime that is used in the cluster.
    The network management components are defined by the CNI, but they all must include
    some type of IP address management and minimally allow for the addition and deletion
    of a container to a network. The full original specification originally derived
    from the `rkt` networking proposal is [available on GitHub](https://oreil.ly/wGvF7).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CNI插件通过规范设定了基本要求。这些规范规定了CNI提供的接口和最低API操作，以及它如何与集群中使用的容器运行时接口。网络管理组件由CNI定义，但它们都必须包括某种类型的IP地址管理，并最少允许将容器添加到网络并删除容器。最初源自`rkt`网络提案的完整原始规范可在[GitHub上获得](https://oreil.ly/wGvF7)。
- en: The Core CNI project provides libraries that you can use to write plug-ins that
    provide the basic requirements and can call other plug-ins to perform various
    functions. This adaptability led to numerous CNI plug-ins that you can use in
    container networking from cloud providers, like the Microsoft Azure native CNI
    and the Amazon Web Services (AWS) VPC CNI plug-in, as well as plug-ins from traditional
    network providers such as Nuage CNI, Juniper Networks Contrail/Tunsten Fabric,
    and VMware NSX.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 核心CNI项目提供了库，您可以使用这些库编写插件，提供基本要求，并调用其他插件执行各种功能。这种适应性导致了许多CNI插件，您可以在云提供商的容器网络中使用，如微软Azure本机CNI和亚马逊Web服务（AWS）VPC
    CNI插件，以及来自传统网络提供商的插件，如Nuage CNI、Juniper Networks Contrail/Tunsten Fabric和VMware
    NSX。
- en: CNI Best Practices
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI最佳实践
- en: 'Networking is a critical component of a functioning Kubernetes environment.
    The interaction between the virtual components within Kubernetes and the physical
    network environment should be carefully designed to ensure dependable application
    communication:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是运行良好的Kubernetes环境的关键组成部分。Kubernetes内的虚拟组件与物理网络环境之间的交互应经过精心设计，以确保可靠的应用程序通信：
- en: Evaluate the feature set needed to accomplish the overall networking goals of
    the infrastructure. Some CNI plug-ins provide native high availability, multicloud
    connectivity, Kubernetes network policy support, and various other features.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估完成基础设施的整体网络目标所需的功能集。一些CNI插件提供本地高可用性、多云连接、Kubernetes网络策略支持以及其他各种功能。
- en: If you are running clusters via public cloud providers, verify that any CNI
    plug-ins that are not native to the cloud provider’s Software-Defined Network
    (SDN) are actually supported.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果通过公共云提供商运行集群，请验证是否支持云提供商的SDN中不是本地的任何CNI插件。
- en: 'Verify that any network security tools, network observability, and management
    tools are compatible with the CNI plug-in of choice. If not, research which tools
    can replace the existing ones. It is important to not lose either observability
    or security capabilities because the needs will be expanded when moving to a large-scale
    distributed system such as Kubernetes. You can add tools like Weaveworks Weave
    Scope, Dynatrace, and Sysdig to any Kubernetes environment, and each offers its
    own benefits. If you’re running in a cloud provider’s managed service, such as
    Azure AKS, Google GCE, or AWS EKS, look for native tools like Azure Container
    Insights and Network Watcher, Google Logging and Monitoring, and AWS CloudWatch.
    Whatever tool you use, it should provide insight into the network stack and the
    Four Golden signals, made popular by the amazing Google SRE team and Rob Ewashuck:
    Latency, Traffic, Errors, and Saturation.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证任何网络安全工具、网络可观察性工具和管理工具是否与所选CNI插件兼容。如果不兼容，研究可以替换现有工具的替代工具。当转移到诸如Kubernetes之类的大规模分布式系统时，不要失去可观察性或安全性能力是非常重要的。您可以将Weaveworks
    Weave Scope、Dynatrace和Sysdig等工具添加到任何Kubernetes环境中，每个工具都提供其独特的优势。如果您在云提供商的托管服务中运行，例如Azure
    AKS、Google GCE或AWS EKS，请寻找像Azure Container Insights和Network Watcher、Google Logging和Monitoring、以及AWS
    CloudWatch等本地工具。无论您使用哪种工具，它都应提供对网络堆栈和由Google SRE团队和Rob Ewashuck流行的四个黄金信号（延迟、流量、错误和饱和度）的洞察。
- en: If you’re using CNIs that do not provide an overlay network separate from the
    SDN space, ensure that you have proper network address space to handle node IPs,
    pod IPs, internal load balancers, and overhead for cluster upgrade and scale out
    processes.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您正在使用不提供与SDN空间分开的覆盖网络的CNI，请确保您有适当的网络地址空间来处理节点IP、Pod IP、内部负载均衡器以及集群升级和扩展过程中的开销。
- en: Services in Kubernetes
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的服务
- en: When pods are deployed into a Kubernetes cluster, because of the basic rules
    of Kubernetes networking and the network plug-in used to facilitate these rules,
    pods can directly communicate only with other pods within the same cluster. Some
    CNI plug-ins give the pods IPs on the same network space as the nodes, so technically,
    after the IP of a pod is known, it can be accessed directly from outside the cluster.
    This, however, is not an efficient way to access services being served by a pod,
    because of the ephemeral nature of pods in Kubernetes. Imagine that you have a
    function or system that needs to access an API that is running in a pod in Kubernetes.
    For a while, that might work with no issue, but at some point there might be a
    voluntary or involuntary disruption that will cause that pod to disappear. Kubernetes
    will potentially create a replacement pod with a new name and IP address, so naturally
    there needs to be some mechanism to find the replacement pod. This is where the
    service API comes to the rescue.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod部署到Kubernetes集群中时，由于Kubernetes网络的基本规则及用于促进这些规则的网络插件，Pod只能直接与同一集群中的其他Pod进行通信。某些CNI插件在与节点相同的网络空间上为Pod提供IP，因此技术上，一旦知道了Pod的IP，就可以直接从集群外访问它。然而，由于Kubernetes中Pod的瞬时性质，这并不是访问由Pod提供的服务的有效方式。想象一下，您有一个需要访问运行在Kubernetes
    Pod中的API的函数或系统。一段时间内，这可能会毫无问题地运行，但在某个时刻可能会出现自愿或非自愿的中断，导致该Pod消失。Kubernetes可能会创建一个新的Pod来替换原来的Pod，并分配新的名称和IP地址，因此自然需要某种机制来找到替换的Pod。这就是服务API出马的地方。
- en: The service API allows for a durable IP and port to be assigned within the Kubernetes
    cluster and automatically mapped to the proper pods as endpoints to the service.
    This magic happens through the iptables or IPVS on Linux nodes to create a mapping
    of the assigned service IP and port to the endpoint’s or pod’s actual IPs. The
    controller that manages this is called the `kube-proxy` service, which actually
    runs on each node in the cluster. It is responsible for manipulating the iptables
    rules on each node.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 服务API允许在Kubernetes集群内分配持久的IP和端口，并自动映射到服务的正确Pod端点。这一魔法通过Linux节点上的iptables或IPVS实现，以创建将分配的服务IP和端口映射到端点或Pod实际IP的映射。负责管理这一过程的控制器称为`kube-proxy`服务，它实际上在集群中的每个节点上运行。它负责在每个节点上操作iptables规则。
- en: When a service object is defined, the type of service needs to be defined. The
    service type will dictate whether the endpoints are exposed only within the cluster
    or outside of the cluster. We will briefly discuss four basic service types in
    the following sections.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义服务对象时，需要定义服务的类型。服务类型将决定端点是仅在集群内部暴露还是在集群外部暴露。我们将在以下部分简要讨论四种基本的服务类型。
- en: Service Type ClusterIP
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClusterIP 服务类型
- en: 'ClusterIP is the default service type if one is not declared in the specification.
    ClusterIP means that the service is assigned an IP from a designated service CIDR
    range. This IP is as long-lasting as the service object, so it provides an IP
    and port and protocol mapping to backend pods using the selector field; however,
    as we will see, there are cases for which you can have no selector. The declaration
    of the service also provides for a Domain Name System (DNS) name for the service.
    This facilitates service discovery within the cluster and allows for workloads
    to easily communicate with other services within the cluster by using DNS lookup
    based on the service name. As an example, if you have the service definition shown
    in the following example and need to access that service from another pod inside
    the cluster via an HTTP call, the call can simply use http://web1-svc if the client
    is in the same namespace as the service:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在规范中未声明服务类型，则 ClusterIP 是默认的服务类型。ClusterIP 意味着服务将被分配一个指定服务 CIDR 范围内的 IP。此
    IP 与服务对象一样持久，因此它为后端 Pod 提供 IP、端口和协议映射，使用选择器字段；然而，正如我们将看到的，有时您可以没有选择器的情况。服务的声明还为服务提供了一个域名系统（DNS）名称。这在集群内部促进了服务发现，并允许工作负载通过基于服务名称的
    DNS 查找轻松与集群中的其他服务通信。例如，如果您有如下示例中所示的服务定义，并且需要通过 HTTP 调用从集群内的另一个 Pod 访问该服务，则调用可以简单地使用
    http://web1-svc（如果客户端与服务在同一命名空间中）：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If it is required to find services in other namespaces, the DNS pattern would
    be `*<service_name>.<namespace_name>*.svc.cluster.local`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要在其他命名空间中查找服务，则 DNS 模式将是 `*<service_name>.<namespace_name>*.svc.cluster.local`。
- en: If no selector is given in a service definition, the endpoints can be explicitly
    defined for the service by using an endpoint API definition. This will basically
    add an IP and port as a specific endpoint to a service instead of relying on the
    selector attribute to automatically update the endpoints from the pods that are
    in scope by the selector match. This can be useful in a few scenarios in which
    you have a specific database that is not in a cluster that is to be used for testing,
    but you will change the service later to a Kubernetes-deployed database. This
    is sometimes called a *headless service* because it is not managed by `kube-proxy`
    as other services are, but you can directly manage the endpoints, as shown in
    [Figure 9-4](#figure94).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在服务定义中没有给出选择器，则可以通过使用端点 API 定义为服务显式定义服务的端点，而不是依赖于选择器属性从符合选择器匹配的 Pod 中自动更新端点。在某些场景中，这可能非常有用，例如您有一个特定的数据库用于测试，而不是集群中的数据库，但稍后将该服务更改为
    Kubernetes 部署的数据库。这有时被称为 *无头服务*，因为它不像其他服务一样由 `kube-proxy` 管理，但您可以直接管理端点，如 [图 9-4](#figure94)
    所示。
- en: '![kbp2 0904](assets/kbp2_0904.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0904](assets/kbp2_0904.png)'
- en: Figure 9-4\. ClusterIP-pod and service visualization
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. ClusterIP-Pod 和服务可视化
- en: Service Type NodePort
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodePort 服务类型
- en: The NodePort service type assigns a high-level port on each node of the cluster
    to the service IP and port on each node. The high-level NodePorts fall within
    the 30,000 through 32,767 ranges and can either be statically assigned or explicitly
    defined in the service specification. NodePorts are typically used for on-premises
    clusters or bespoke solutions that do not offer automatic load-balancing configuration.
    To directly access the service from outside the cluster, use NodeIP:NodePort,
    as depicted in [Figure 9-5](#figure95).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 服务类型为集群中每个节点分配一个高级端口到每个节点的服务 IP 和端口。高级 NodePort 位于 30,000 到 32,767
    范围内，并且可以静态分配或在服务规范中明确定义。NodePorts 通常用于本地集群或不提供自动负载均衡配置的定制解决方案。要从集群外部直接访问服务，请使用
    NodeIP:NodePort，如 [图 9-5](#figure95) 所示。
- en: '![kbp2 0905](assets/kbp2_0905.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0905](assets/kbp2_0905.png)'
- en: Figure 9-5\. NodePort–pod, service and host network visualization
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. NodePort-Pod、服务和主机网络可视化
- en: Service Type ExternalName
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部名称服务类型
- en: 'The ExternalName service type is seldom used in practice, but it can be helpful
    for passing cluster-durable DNS names to external DNS named services. A common
    example is an external database service from a cloud provider that has a unique
    DNS supplied by the cloud provider, such as `mymongodb.documents.azure.com`. Technically,
    this can be added very easily to a pod specification using an `Environment` variable,
    as discussed in [Chapter 6](ch06.html#versioning_releases_and_rollouts). However,
    it might be more advantageous to use a more generic name in the cluster, such
    as `prod-mongodb`, which enables the change of the actual database it points to
    by just changing the service specification instead of having to recycle the pods
    because the `Environment` variable has changed:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Service Type LoadBalancer
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LoadBalancer is a very special service type because it enables automation with
    cloud providers and other programmable cloud infrastructure services. The `LoadBalancer`
    type is a single method to ensure the deployment of the load-balancing mechanism
    that the infrastructure provider of the Kubernetes cluster supplies. This means
    that in most cases, `LoadBalancer` will work roughly the same way in AWS, Azure,
    GCE, OpenStack, and others. This entry will usually create a public-facing load-balanced
    service; however, each cloud provider has some specific annotations that enable
    other features, such as internal-only load balancers, AWS ELB configuration parameters,
    and so on. You can also define the actual load-balancer IP to use and the source
    ranges to allow within the service specification, as seen in the code sample that
    follows and the visual representation in [Figure 9-6](#figure96):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![kbp2 0906](assets/kbp2_0906.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. LoadBalancer–pod, service, node, and cloud provider network visualization
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ingress and Ingress Controllers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although not technically a service type in Kubernetes, the Ingress specification
    is an important concept for ingress to workloads in Kubernetes. Services, as defined
    by the Service API, allow for a basic level of Layer 3/4 load balancing. The reality
    is that many of the stateless services that are deployed in Kubernetes require
    a high level of traffic management and usually require application-level control:
    more specifically, HTTP protocol management.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The Ingress API is basically an HTTP-level router that allows for host- and
    path-based rules to direct to specific backend services. Imagine a website hosted
    on www.evillgenius.com and two different paths that are hosted on that site, */registration*
    and */labaccess*, that are served by two different services hosted in Kubernetes,
    `reg-svc` and `labaccess-svc`. You can define an ingress rule to ensure that requests
    to www.evillgenius.com/registration are forwarded to the `reg-svc` service and
    the correct endpoint pods, and, similarly, that requests to www.evillgenius.com/labaccess
    are forwarded to the correct endpoints of the `labaccess-svc` service. The Ingress
    API also permits host-based routing to allow for different hosts on a single ingress.
    An additional feature is the ability to declare a Kubernetes secret that holds
    the certificate information for Transport Layer Security (TLS) termination on
    port 443\. When a path is not specified, there is usually a default backend that
    can be used to give a better user experience than the standard 404 error.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'The details around the specific TLS and default backend configuration are actually
    handled by what is known as the Ingress controller. The Ingress controller is
    decoupled from the Ingress API and allows for operators to deploy an Ingress controller
    of choice, such as NGINX, Traefik, HAProxy, and others. An Ingress controller,
    as the name suggests, is a controller just like any Kubernetes controller, but
    it’s not part of the system and is instead a third-party controller that understands
    the Kubernetes Ingress API for dynamic configuration. The most common implementation
    of an Ingress controller is NGINX because it is partly maintained by the Kubernetes
    project; however, there are numerous examples of both open source and commercial
    Ingress controllers:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Gateway API
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Ingress API had some challenges over the years that it was in beta and
    following its v1 promotion. These challenges have led to other network services
    offering different abstractions through the use of Custom Resource Definitions
    and controllers to create their own APIs that fill some of the gaps Ingress has
    had. Some of the most common challenges with the Ingress API have been:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The lack of expressiveness in the definition as it represents the lowest common
    denominator for the capabilities of the particular Ingress implementation.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A general lack of extensibility in the architecture. Vendors have used countless
    annotations to expose specific implementation capabilities; however, this has
    some limitations.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of vendor-specific annotations has removed some of the portability promised
    by the API. An annotation to expose a capability in an NGINX-based Ingress controller
    may be different or expressed differently from a Kong-based controller implementation.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no formal way to do multi-tenancy with the current Ingress API, and
    DevOps teams have to create very tight controls to prevent path conflicts between
    Ingress definitions that could impact other tenants in the same cluster.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前的入口 API 没有正式的多租户处理方式，DevOps 团队必须创建非常严格的控制措施，以防止入口定义之间的路径冲突，这可能会影响同一集群中的其他租户。
- en: Introduced in 2019, the Gateway API is currently managed as a project by the
    SIG Network team under the Kubernetes Project. The Gateway API does not intend
    to replace the Ingress API as it primarily targets exposing HTTP applications
    with a declarative syntax. This API exposes a more general API for proxying for
    many types of protocols, and fits a more role-based management process because
    it models more closely the infrastructure components in the environment.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 网关 API 自 2019 年推出以来，目前由 Kubernetes 项目下的 SIG Network 团队管理。网关 API 并不打算取代 Ingress
    API，因为它主要用于以声明性语法暴露 HTTP 应用程序。此 API 提供了一个更通用的代理多种协议的 API，并适合更加基于角色的管理流程，因为它更紧密地模拟环境中的基础设施组件。
- en: The role-based paradigm, as shown in [Figure 9-7](#figure97), is important in
    answering some of the shortcomings of the existing Ingress API. The separate components
    allow for infrastructure providers, such as cloud providers and proxy ISVs, to
    define the infrastructure and platform operators to define through policy what
    infrastructure can be used. Developers can then worry about how they want to expose
    their services within the constraints they are given. [Figure 9-8](#figure98)
    shows a practical example of how Gateway API structure abstracts the infrastructure
    services and capabilities away from the developer and allows them to focus on
    their specific service needs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于角色的范式，如[图 9-7](#figure97)所示，对于解决现有入口 API 的一些缺陷非常重要。独立的组件允许基础设施提供者（例如云提供商和代理
    ISV）定义基础设施，平台操作员通过策略定义可以使用什么基础设施。开发人员可以根据所给的约束条件考虑如何公开他们的服务。[图 9-8](#figure98)展示了网关
    API 结构如何将基础设施服务和功能抽象出来，使开发人员可以专注于其特定的服务需求。
- en: '![kbp2 0907](assets/kbp2_0907.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0907](assets/kbp2_0907.png)'
- en: Figure 9-7\. Gateway API structure
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. 网关 API 结构
- en: '![kbp2 0908](assets/kbp2_0908.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0908](assets/kbp2_0908.png)'
- en: Figure 9-8\. Gateway API structure, continued
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-8\. 网关 API 结构，续
- en: The specification is very promising, and many of the leading providers of proxies
    and services meshes, as well as cloud providers, have begun to implement the Gateway
    API into their stack. Google’s GKE, Acnodeal EPIC, Contour, Apache APISIX, and
    others have begun to offer limited preview or alpha support. As of this writing,
    the API itself is in beta for the GatewayClass, Gateway, and HTTPRoute resources,
    and others are in Alpha support. Unlike the Ingress API, this is a custom resource
    that can be added to any cluster and therefore does not follow the Kubernetes
    alpha or beta release process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这一规范非常有前景，许多主要的代理和服务网格提供商，以及云提供商，已经开始将网关 API 集成到其堆栈中。Google 的 GKE、Acnodeal EPIC、Contour、Apache
    APISIX 等已经开始提供有限的预览或 alpha 支持。截至目前，API 本身在 GatewayClass、Gateway 和 HTTPRoute 资源上处于
    beta 阶段，其他资源则处于 alpha 支持阶段。与 Ingress API 不同，这是一个自定义资源，可以添加到任何集群中，因此不遵循 Kubernetes
    的 alpha 或 beta 发布流程。
- en: Services and Ingress Controllers Best Practices
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务和入口控制器的最佳实践
- en: 'Creating a complex virtual network environment with interconnected applications
    requires careful planning. Effectively managing how the different services of
    the application communicate with one another and the outside world requires constant
    attention as the application changes. These best practices will help make management
    easier:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个复杂的虚拟网络环境，其中应用程序彼此相互连接，需要仔细规划。有效地管理应用程序不同服务之间以及与外部世界的通信方式，需要随着应用程序变化而持续关注。以下是一些管理最佳实践：
- en: Limit the number of services that need to be accessed from outside the cluster.
    Ideally, most services will be ClusterIP, and only external-facing services will
    be exposed externally to the cluster.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制需要从集群外部访问的服务数量。理想情况下，大多数服务将是 ClusterIP，只有外部服务才会暴露给集群外部。
- en: If the services that need to be exposed are primarily HTTP/HTTPS-based services,
    it is best to use an Ingress API and Ingress controller to route traffic to backing
    services with TLS termination. Depending on the type of Ingress controller used,
    features such as rate limiting, header rewrites, OAuth authentication, observability,
    and other services can be made available without having to build them into the
    applications themselves.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose an Ingress controller that has the needed functionality for secure ingress
    of your web-based workloads. Standardize on one and use it across the enterprise
    because many of the specific configuration annotations vary between implementations
    and prevent the deployment code from being portable across enterprise Kubernetes
    implementations.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate cloud service provider–specific Ingress controller options to move
    the infrastructure management and load of the ingress out of the cluster, but
    still allow for Kubernetes API configuration.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When serving mostly APIs externally, evaluate API-specific Ingress controllers,
    such as Kong or Ambassador, that have more fine-tuning for API-based workloads.
    Although NGINX, Traefik, and others might offer some API tuning, it will not be
    as fine-grained as specific API proxy systems.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When deploying Ingress controllers as pod-based workloads in Kubernetes, ensure
    that the deployments are designed for high availability and aggregate performance
    throughput. Use metrics observability to properly scale the ingress, but include
    enough cushion to prevent client disruptions while the workload scales.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network Security Policy
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NetworkPolicy API built into Kubernetes allows for network-level ingress
    and egress access control defined with your workload. Network policies allow you
    to control how groups of pods are allowed to communicate with one another and
    with other endpoints. If you want to dig deeper into the NetworkPolicy specification,
    it might sound confusing, especially given that it is defined as a Kubernetes
    API, but it requires a network plug-in that supports the NetworkPolicy API.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Network policies have a simple YAML structure that can look complicated, but
    if you think of it as a simple East-West traffic firewall, it might help you to
    understand it a little better. Each policy specification has `podSelector`, `ingress`,
    `egress`, and `policyType` fields. The only required field is `podSelector`, which
    follows the same convention as any Kubernetes selector with a `matchLabels`. You
    can create multiple NetworkPolicy definitions that can target the same pods, and
    the effect is additive. Because NetworkPolicy objects are namespaced objects,
    if no selector is given for a `podSelector`, all pods in the namespace fall into
    the scope of the policy. If any ingress or egress rules are defined, this creates
    an allow list for what can ingress or egress from the pod. There is an important
    distinction here: if a pod falls into the scope of a policy because of a selector
    match, all traffic, unless explicitly defined in an ingress or egress rule, is
    blocked. This little, nuanced detail means that if a pod does not fall into any
    policy because of a selector match, all ingress and egress is allowed to the pod.
    This was done on purpose to allow for ease of deploying new workloads into Kubernetes
    without any blockers.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The `ingress` and `egress` fields are basically a list of rules based on source
    or destination and can be specific CIDR ranges, `podSelector`s, or `namespaceSelector`s.
    If you leave the ingress field empty, it is like a deny-all inbound. Similarly,
    if you leave the egress empty, it is deny-all outbound. Port and protocol lists
    are also supported to further tighten down the type of communications allowed.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The `policyTypes` field specifies to which network policy rule types the policy
    object is associated. If the field is not present, it will just look at the `ingress`
    and `egress` lists fields. The difference again is that you must explicitly call
    out egress in `policyTypes` and also have an egress rule list for this policy
    to work. Ingress is assumed, and defining it explicitly is not needed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a prototypical example of a three-tier application deployed to a
    single namespace where the tiers are labeled as `tier: "web"`, `tier: "db"`, and
    `tier: "api"`. If you want to ensure that traffic is properly limited to each
    tier, create a NetworkPolicy manifest like the following.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Default deny rule:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Web layer network policy:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'API layer network policy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Database layer network policy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Network Policy Best Practices
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Securing network traffic in an enterprise system was once the domain of physical
    hardware devices with complex networking rule sets. Now, with Kubernetes network
    policy, a more application-centric approach can be taken to segment and control
    the traffic of the applications hosted in Kubernetes. Some common best practices
    apply no matter which policy plug-in is used:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Start off slow and focus on traffic ingress to pods. Complicating matters with
    ingress and egress rules can make network tracing a nightmare. As soon as traffic
    is flowing as expected, you can begin to look at egress rules to further control
    flow to sensitive workloads. The specification also favors ingress because it
    defaults many options even if nothing is entered into the ingress rules list.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the network plug-in used either has some of its own interface to
    the NetworkPolicy API or supports other well-known plug-ins. Example plug-ins
    include Calico, Cilium, Kube-router, Romana, and Weave Net.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the network team is used to having a “default-deny” policy in place, create
    a network policy such as the following for each namespace in the cluster that
    will contain workloads to be protected. This ensures that even if another network
    policy is deleted, no pods are accidentally “exposed”:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If pods need to be accessed from the internet, use a label to explicitly apply
    a network policy that allows ingress. Be aware of the entire flow in case the
    actual IP that a packet is coming from is not the internet but rather the internal
    IP of a load balancer, firewall, or other network device. For example, to allow
    traffic from all (including external) sources for pods having the `allow-internet=true`
    label, do this:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Try to align application workloads to single namespaces for ease of creating
    rules because the rules themselves are namespace specific. If cross-namespace
    communication is needed, try to be as explicit as possible and perhaps use specific
    labels to identify the flow pattern:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Have a test bed namespace that has fewer restrictive policies, if any at all,
    to allow time to investigate the correct traffic patterns needed.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service Meshes
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is easy to imagine a single cluster hosting hundreds of services that load-balance
    across thousands of endpoints that communicate with one another, access external
    resources, and are potentially being accessed from external sources. This can
    be quite daunting when trying to manage, secure, observe, and trace all the connections
    among these services, especially with the dynamic nature of the endpoints coming
    and going from the overall system. The concept of a *service mesh*, which is not
    unique to Kubernetes, allows for control over how these services are connected
    and secured with a dedicated date plane and control plane. Service meshes all
    have different capabilities, but usually they all offer some of the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing of traffic with potentially fine-grained traffic-shaping policies
    that are distributed across the mesh.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery of services that are members of the mesh, which might include
    services within a cluster or in another cluster, or an outside system that is
    a member of the mesh.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability of the traffic and services, including tracing across the distributed
    services using tracing systems like Jaeger or Zipkin that follow the OpenTracing
    standards.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security of the traffic in the mesh using mutual authentication. In some cases,
    not only pod-to-pod or East-West traffic is secured, but an Ingress controller
    is also provided that offers North-South security and control.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resiliency, health, and failure-prevention capabilities that allow for patterns
    such as circuit breaker, retries, deadlines, and so on.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key here is that all these features are integrated into the applications
    that take part in the mesh with little or no application changes. How can all
    these amazing features come for free? Sidecar proxies are usually the way this
    is done. The majority of service meshes available today inject a proxy that is
    part of the data plane into each pod that is a member of the mesh. This allows
    for policies and security to be synchronized across the mesh by the control-plane
    components. This hides the network details from the container that holds the workload
    and leaves it to the proxy to handle the complexity of the distributed network.
    In the application’s perspective, it only communicates via localhost to its proxy.
    In many cases, the control plane and data plane might be different technologies
    but complementary to each other.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the first service mesh that comes to mind is Istio, a project
    by Google, Lyft, and IBM that uses Envoy as its data-plane proxy and uses proprietary
    control-plane components Mixer, Pilot, Galley, and Citadel. Other service meshes
    offer varying levels of capabilities, such as Linkerd2, which uses its own data-plane
    proxy built using Rust. HashiCorp has recently added more Kubernetes-centric service
    mesh capabilities to Consul, which allows you to choose between Consul’s own proxy
    or Envoy, and offers commercial support for its service mesh.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The topic of service meshes in Kubernetes is a fluid one—if not overly emotional
    in many social media tech circles—so a detailed explanation of each mesh has no
    value here. We would be remiss if we did not mention the promising efforts led
    by Microsoft, Linkerd, HashiCorp, Solo.io, Kinvolk, and Weaveworks around the
    Service Mesh Interface (SMI). The SMI hopes to set a standard interface for basic
    feature sets that are expected of all service meshes. The specification as of
    this writing covers traffic policy such as identity and transport-level encryption,
    traffic telemetry that captures key metrics between services in the mesh, and
    traffic management to allow for traffic shifting and weighting between different
    services. This project hopes to take some of the variability out of the service
    meshes yet allow for service mesh vendors to extend and build value-added capabilities
    into their products to differentiate themselves.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Service Mesh Best Practices
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The service mesh community continues to grow every day, and as more enterprises
    help define their needs, the service mesh ecosystem will change dramatically.
    These best practices are, as of this writing, based on common problems that service
    meshes try to solve today:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Rate the importance of the key features service meshes offer and determine which
    current offerings provide the most important features with the least amount of
    overhead. Overhead here means both human technical debt and infrastructure resource
    debt. If all that is really required is mutual TLS between certain pods, would
    it be easier to perhaps find a CNI that offers that capability integrated into
    the plug-in?
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the need for a cross-system mesh, such as multicloud or hybrid scenarios,
    a key requirement? Not all service meshes offer this capability, and if they do,
    it is a complicated process that often introduces fragility into the environment.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of the service mesh offerings are open source community-based projects,
    and if the team that will be managing the environment is new to service meshes,
    commercially supported offerings might be a better option. Some companies are
    beginning to offer commercially supported and managed service meshes based on
    Istio, which can be helpful because it is almost universally agreed upon that
    Istio is a complicated system to manage.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to application management, one of the most important things that
    Kubernetes provides is the ability to link different pieces of your application.
    In this chapter, we looked at the details of how Kubernetes works, including how
    pods get their IP addresses through CNI plug-ins, how those IPs are grouped to
    form services, and how more application or Layer 7 routing can be implemented
    via Ingress resources (which in turn use services). You also saw how to limit
    traffic and secure your network using networking policies, and, finally, how service
    mesh technologies are transforming the ways in which people connect and monitor
    the connections between their services. In addition to setting up your application
    to run and be deployed reliably, setting up the networking for your application
    is a crucial piece of using Kubernetes successfully. Understanding how Kubernetes
    approaches networking and how that intersects optimally with your application
    is critical to its ultimate success.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
