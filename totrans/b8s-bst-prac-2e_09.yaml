- en: Chapter 9\. Networking, Network Security, and Service Mesh
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 网络、网络安全和服务网格
- en: Kubernetes is effectively a manager of distributed systems across a cluster
    of connected systems. This immediately puts critical importance on how the connected
    systems communicate with one another, and networking is the key to this. Understanding
    how Kubernetes facilitates communication among the distributed services it manages
    is important for the effective application of interservice communication.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有效地是跨连接系统集群的分布式系统管理器。这立即将系统连接的方式如何与其它系统通信置于至关重要的位置，而网络是这一切的关键。了解
    Kubernetes 如何促进其管理的分布式服务之间的通信对于有效地应用服务间通信非常重要。
- en: This chapter focuses on the principles that Kubernetes places on the network
    and best practices around applying these concepts in different situations. With
    any discussion of networking, security is usually brought along for the ride.
    The traditional models of network security boundaries being controlled at the
    network layer are not absent in this new world of distributed systems in Kubernetes,
    but how they are implemented and the capabilities offered change slightly. Kubernetes
    brings along a native API for network security policies that will sound eerily
    similar to firewall rules of old.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍 Kubernetes 在网络上的原则，并围绕在不同情况下应用这些概念的最佳实践展开讨论。任何关于网络的讨论通常都会带来安全性的讨论。传统的网络安全边界模型在
    Kubernetes 中的分布式系统的新世界中并未消失，但它们的实施方式和提供的功能略有变化。Kubernetes 带来了一种原生的网络安全策略 API，听起来令人毛骨悚然地类似于旧时防火墙规则。
- en: The last section of this chapter delves into the new and scary world of service
    meshes. The term “scary” is used in jest, but it is quite the Wild West when it
    comes to service mesh technology in Kubernetes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一节深入探讨了服务网格的新世界，术语“恐怖”是开玩笑用的，但在 Kubernetes 的服务网格技术中确实是相当不可预测的领域。
- en: Kubernetes Network Principles
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络原则
- en: 'Understanding how Kubernetes uses the underlying network to facilitate communication
    among services is critical to understanding how to effectively plan application
    architectures. Usually, networking topics start to give most people major headaches.
    We are going to keep this rather simple because this is more of a best practice
    guidance than a lesson on container networking. Luckily for us, Kubernetes has
    laid down some rules of the road for networking that give us a start. The rules
    outline how communication is expected to behave between different components.
    Let’s take a closer look at each of these rules:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 如何利用底层网络来促进服务之间的通信对于有效规划应用程序架构至关重要。通常，网络主题开始让大多数人头痛起来。我们将保持简单，因为这更多是最佳实践指导，而不是容器网络的课程。幸运的是，Kubernetes
    已经为网络制定了一些规则，为我们提供了一个起点。这些规则概述了不同组件之间预期的通信行为。让我们更详细地看看每个规则：
- en: Container-to-container communication in the same pod
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 同一 Pod 中容器之间的通信
- en: All containers in the same pod share the same network space. This effectively
    allows localhost communication between the containers. It also means that containers
    in the same pod need to expose different ports. This is done using the power of
    Linux namespaces and Docker networking to allow these containers to be on the
    same local network through the use of a paused container in every pod that does
    nothing but host the networking for the pod. [Figure 9-1](#figure91) shows how
    Container A can communicate directly with Container B using localhost and the
    port number that the container is listening on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 同一 Pod 中的所有容器共享相同的网络空间。这有效地允许容器之间通过 localhost 进行通信。这也意味着同一 Pod 中的容器需要暴露不同的端口。通过
    Linux 命名空间和 Docker 网络的能力，通过在每个 Pod 中运行一个什么也不做的暂停容器来托管 Pod 的网络，从而使这些容器可以位于相同的本地网络。[图
    9-1](#figure91) 展示了容器 A 如何直接使用 localhost 和容器正在监听的端口号与容器 B 进行通信。
- en: '![kbp2 0901](assets/kbp2_0901.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0901](assets/kbp2_0901.png)'
- en: Figure 9-1\. Intrapod communication between containers
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1 两个容器之间的 Pod 内通信
- en: Pod-to-pod communication
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 与 Pod 的通信
- en: All pods need to communicate with one another without any network address translation
    (NAT). This means that the pod’s IP address that is seen by the receiving pod
    is the sender’s actual IP address. This is handled in different ways, depending
    on the network plug-in used, which we discuss in more detail later in the chapter.
    This rule is true between pods on the same node and pods that are on different
    nodes in the same cluster. This also extends to the node being able to communicate
    directly to the pod with no NAT involved. This allows host-based agents or system
    daemons to communicate to the pods as needed. [Figure 9-2](#figure92) is a representation
    of the communication processes between pods in the same node and pods in different
    nodes of the cluster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的 pod 都需要在没有任何网络地址转换（NAT）的情况下相互通信。这意味着接收 pod 看到的 pod IP 地址就是发送 pod 的实际 IP
    地址。这是通过使用不同的网络插件来处理的，具体在本章后面会详细讨论。这条规则适用于同一节点上的 pod，以及同一集群中不同节点上的 pod。这也扩展到节点能够直接与
    pod 进行通信，无需涉及 NAT。这允许基于主机的代理或系统守护程序根据需要与 pod 进行通信。[图 9-2](#figure92) 是表示同一节点内
    pod 和集群中不同节点上 pod 之间通信过程的图示。
- en: '![kbp2 0902](assets/kbp2_0902.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0902](assets/kbp2_0902.png)'
- en: Figure 9-2\. Pod-to-pod communication intra- and internode
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. Pod 对 pod 的节点内和节点间通信
- en: Service-to-pod communication
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 服务对 pod 的通信
- en: Services in Kubernetes represent a durable IP address and port that is found
    on each node that will forward all traffic to the endpoints that are mapped to
    the service. Over the different iterations of Kubernetes, the method in favor
    of enabling this has changed, but the two main methods are via the use of iptables
    or the newer IP Virtual Server (IPVS). Some cloud providers and more advanced
    implementations allow for a new eBPF-based dataplane. Most implementations today
    use the iptables implementation to enable a pseudo–Layer 4 load balancer on each
    node. [Figure 9-3](#figure93) is a visual representation of how the service is
    tied to the pods via label selectors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的服务代表了在每个节点上找到的持久 IP 地址和端口，将所有流量转发到映射到服务的端点。在 Kubernetes 的不同迭代中，启用这一功能的方法有所改变，但主要方法有通过使用
    iptables 或更新的 IP Virtual Server（IPVS）。一些云提供商和更高级的实现允许基于新的 eBPF 数据平面。今天大多数实现使用
    iptables 实现在每个节点上启用伪 Layer 4 负载均衡器。[图 9-3](#figure93) 是显示服务如何通过标签选择器与 pod 关联的视觉表示。
- en: '![kbp2 0903](assets/kbp2_0903.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0903](assets/kbp2_0903.png)'
- en: Figure 9-3\. Service-to-pod communication
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 服务对 pod 的通信
- en: Network Plug-ins
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络插件
- en: Early on, the Special Interest Group (SIG) guided the networking standards to
    more of a pluggable architecture, opening the door for numerous third-party networking
    projects, which in many cases injected value-added capabilities into Kubernetes
    workloads. These network plug-ins come in two flavors. The most basic is called
    Kubenet and is the default plug-in provided by Kubernetes natively. The second
    type of plug-in follows the Container Network Interface (CNI) specification, which
    is a generic plug-in network solution for containers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，特别兴趣组（SIG）引导网络标准向更具可插拔架构的方向发展，为 Kubernetes 工作负载打开了许多第三方网络项目的大门，在许多情况下，这些项目为
    Kubernetes 工作负载注入了增值能力。这些网络插件分为两种类型。最基本的称为 Kubenet，是 Kubernetes 本地提供的默认插件。第二种类型的插件遵循容器网络接口（CNI）规范，这是容器的通用插件网络解决方案。
- en: Kubenet
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubenet
- en: Kubenet is the most basic network plug-in that comes out of the box in Kubernetes.
    It is the simplest of the plug-ins and provides a Linux bridge, `cbr0`, that’s
    a virtual Ethernet pair for the pods connected to it. The pod then gets an IP
    address from a Classless Inter-Domain Routing (CIDR) range that is distributed
    across the nodes of the cluster. There is also an IP masquerade flag that should
    be set to allow traffic destined to IPs outside the pod CIDR range to be masqueraded.
    This obeys the rules of pod-to-pod communication because only traffic destined
    outside the pod CIDR undergoes network address translation (NAT). After the packet
    leaves a node to go to another node, some kind of routing is put in place to facilitate
    the process to forward the traffic to the correct node.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet 是 Kubernetes 中随箱即出的最基本的网络插件。它是最简单的插件，为连接到其上的 pod 提供了一个 Linux 桥接器 `cbr0`，这是一个虚拟以太网对。然后，pod
    从分布在集群节点上的无类域间路由（CIDR）范围获取 IP 地址。还有一个 IP 伪装标志，应设置为允许发送到 pod CIDR 范围外 IP 的流量进行伪装。这遵守了
    pod 对 pod 通信的规则，因为只有发送到 pod CIDR 范围外的流量才会经过网络地址转换（NAT）。当数据包离开一个节点去往另一个节点时，会放置某种路由来促进将流量正确转发到相应的节点的过程。
- en: Kubenet Best Practices
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubenet最佳实践
- en: Kubenet allows for a simple network stack and does not consume precious IP addresses
    on already crowded networks. This is especially true of cloud networks that are
    extended to on-premises datacenters.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubenet允许简单的网络堆栈，并且不会在已经拥挤的网络上消耗宝贵的IP地址。这对扩展到本地数据中心的云网络尤其重要。
- en: Ensure that the pod CIDR range is large enough to handle the potential size
    of the cluster and the pods in each cluster. The default pods per node set in
    kubelet is 110, but you can adjust this.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保Pod CIDR范围足够大，以处理集群的潜在大小和每个集群中的Pod。kubelet中设置的默认每节点Pod数为110，但您可以进行调整。
- en: Understand and plan accordingly for the route rules to properly allow traffic
    to find pods in the proper nodes. In cloud providers, this is usually automated,
    but on-premises or edge cases will require automation and solid network management.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保根据路由规则进行适当的规划，以便流量能够正确找到节点中的Pod。在云提供商中，这通常是自动化的，但在本地或边缘情况下将需要自动化和可靠的网络管理。
- en: The CNI Plug-in
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI插件
- en: The CNI plug-in has basic requirements set aside by the specification. These
    specifications dictate the interfaces and minimal API actions that the CNI offers
    and how it will interface with the container runtime that is used in the cluster.
    The network management components are defined by the CNI, but they all must include
    some type of IP address management and minimally allow for the addition and deletion
    of a container to a network. The full original specification originally derived
    from the `rkt` networking proposal is [available on GitHub](https://oreil.ly/wGvF7).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CNI插件通过规范设定了基本要求。这些规范规定了CNI提供的接口和最低API操作，以及它如何与集群中使用的容器运行时接口。网络管理组件由CNI定义，但它们都必须包括某种类型的IP地址管理，并最少允许将容器添加到网络并删除容器。最初源自`rkt`网络提案的完整原始规范可在[GitHub上获得](https://oreil.ly/wGvF7)。
- en: The Core CNI project provides libraries that you can use to write plug-ins that
    provide the basic requirements and can call other plug-ins to perform various
    functions. This adaptability led to numerous CNI plug-ins that you can use in
    container networking from cloud providers, like the Microsoft Azure native CNI
    and the Amazon Web Services (AWS) VPC CNI plug-in, as well as plug-ins from traditional
    network providers such as Nuage CNI, Juniper Networks Contrail/Tunsten Fabric,
    and VMware NSX.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 核心CNI项目提供了库，您可以使用这些库编写插件，提供基本要求，并调用其他插件执行各种功能。这种适应性导致了许多CNI插件，您可以在云提供商的容器网络中使用，如微软Azure本机CNI和亚马逊Web服务（AWS）VPC
    CNI插件，以及来自传统网络提供商的插件，如Nuage CNI、Juniper Networks Contrail/Tunsten Fabric和VMware
    NSX。
- en: CNI Best Practices
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI最佳实践
- en: 'Networking is a critical component of a functioning Kubernetes environment.
    The interaction between the virtual components within Kubernetes and the physical
    network environment should be carefully designed to ensure dependable application
    communication:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是运行良好的Kubernetes环境的关键组成部分。Kubernetes内的虚拟组件与物理网络环境之间的交互应经过精心设计，以确保可靠的应用程序通信：
- en: Evaluate the feature set needed to accomplish the overall networking goals of
    the infrastructure. Some CNI plug-ins provide native high availability, multicloud
    connectivity, Kubernetes network policy support, and various other features.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估完成基础设施的整体网络目标所需的功能集。一些CNI插件提供本地高可用性、多云连接、Kubernetes网络策略支持以及其他各种功能。
- en: If you are running clusters via public cloud providers, verify that any CNI
    plug-ins that are not native to the cloud provider’s Software-Defined Network
    (SDN) are actually supported.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果通过公共云提供商运行集群，请验证是否支持云提供商的SDN中不是本地的任何CNI插件。
- en: 'Verify that any network security tools, network observability, and management
    tools are compatible with the CNI plug-in of choice. If not, research which tools
    can replace the existing ones. It is important to not lose either observability
    or security capabilities because the needs will be expanded when moving to a large-scale
    distributed system such as Kubernetes. You can add tools like Weaveworks Weave
    Scope, Dynatrace, and Sysdig to any Kubernetes environment, and each offers its
    own benefits. If you’re running in a cloud provider’s managed service, such as
    Azure AKS, Google GCE, or AWS EKS, look for native tools like Azure Container
    Insights and Network Watcher, Google Logging and Monitoring, and AWS CloudWatch.
    Whatever tool you use, it should provide insight into the network stack and the
    Four Golden signals, made popular by the amazing Google SRE team and Rob Ewashuck:
    Latency, Traffic, Errors, and Saturation.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证任何网络安全工具、网络可观察性工具和管理工具是否与所选CNI插件兼容。如果不兼容，研究可以替换现有工具的替代工具。当转移到诸如Kubernetes之类的大规模分布式系统时，不要失去可观察性或安全性能力是非常重要的。您可以将Weaveworks
    Weave Scope、Dynatrace和Sysdig等工具添加到任何Kubernetes环境中，每个工具都提供其独特的优势。如果您在云提供商的托管服务中运行，例如Azure
    AKS、Google GCE或AWS EKS，请寻找像Azure Container Insights和Network Watcher、Google Logging和Monitoring、以及AWS
    CloudWatch等本地工具。无论您使用哪种工具，它都应提供对网络堆栈和由Google SRE团队和Rob Ewashuck流行的四个黄金信号（延迟、流量、错误和饱和度）的洞察。
- en: If you’re using CNIs that do not provide an overlay network separate from the
    SDN space, ensure that you have proper network address space to handle node IPs,
    pod IPs, internal load balancers, and overhead for cluster upgrade and scale out
    processes.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您正在使用不提供与SDN空间分开的覆盖网络的CNI，请确保您有适当的网络地址空间来处理节点IP、Pod IP、内部负载均衡器以及集群升级和扩展过程中的开销。
- en: Services in Kubernetes
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的服务
- en: When pods are deployed into a Kubernetes cluster, because of the basic rules
    of Kubernetes networking and the network plug-in used to facilitate these rules,
    pods can directly communicate only with other pods within the same cluster. Some
    CNI plug-ins give the pods IPs on the same network space as the nodes, so technically,
    after the IP of a pod is known, it can be accessed directly from outside the cluster.
    This, however, is not an efficient way to access services being served by a pod,
    because of the ephemeral nature of pods in Kubernetes. Imagine that you have a
    function or system that needs to access an API that is running in a pod in Kubernetes.
    For a while, that might work with no issue, but at some point there might be a
    voluntary or involuntary disruption that will cause that pod to disappear. Kubernetes
    will potentially create a replacement pod with a new name and IP address, so naturally
    there needs to be some mechanism to find the replacement pod. This is where the
    service API comes to the rescue.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod部署到Kubernetes集群中时，由于Kubernetes网络的基本规则及用于促进这些规则的网络插件，Pod只能直接与同一集群中的其他Pod进行通信。某些CNI插件在与节点相同的网络空间上为Pod提供IP，因此技术上，一旦知道了Pod的IP，就可以直接从集群外访问它。然而，由于Kubernetes中Pod的瞬时性质，这并不是访问由Pod提供的服务的有效方式。想象一下，您有一个需要访问运行在Kubernetes
    Pod中的API的函数或系统。一段时间内，这可能会毫无问题地运行，但在某个时刻可能会出现自愿或非自愿的中断，导致该Pod消失。Kubernetes可能会创建一个新的Pod来替换原来的Pod，并分配新的名称和IP地址，因此自然需要某种机制来找到替换的Pod。这就是服务API出马的地方。
- en: The service API allows for a durable IP and port to be assigned within the Kubernetes
    cluster and automatically mapped to the proper pods as endpoints to the service.
    This magic happens through the iptables or IPVS on Linux nodes to create a mapping
    of the assigned service IP and port to the endpoint’s or pod’s actual IPs. The
    controller that manages this is called the `kube-proxy` service, which actually
    runs on each node in the cluster. It is responsible for manipulating the iptables
    rules on each node.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 服务API允许在Kubernetes集群内分配持久的IP和端口，并自动映射到服务的正确Pod端点。这一魔法通过Linux节点上的iptables或IPVS实现，以创建将分配的服务IP和端口映射到端点或Pod实际IP的映射。负责管理这一过程的控制器称为`kube-proxy`服务，它实际上在集群中的每个节点上运行。它负责在每个节点上操作iptables规则。
- en: When a service object is defined, the type of service needs to be defined. The
    service type will dictate whether the endpoints are exposed only within the cluster
    or outside of the cluster. We will briefly discuss four basic service types in
    the following sections.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义服务对象时，需要定义服务的类型。服务类型将决定端点是仅在集群内部暴露还是在集群外部暴露。我们将在以下部分简要讨论四种基本的服务类型。
- en: Service Type ClusterIP
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClusterIP 服务类型
- en: 'ClusterIP is the default service type if one is not declared in the specification.
    ClusterIP means that the service is assigned an IP from a designated service CIDR
    range. This IP is as long-lasting as the service object, so it provides an IP
    and port and protocol mapping to backend pods using the selector field; however,
    as we will see, there are cases for which you can have no selector. The declaration
    of the service also provides for a Domain Name System (DNS) name for the service.
    This facilitates service discovery within the cluster and allows for workloads
    to easily communicate with other services within the cluster by using DNS lookup
    based on the service name. As an example, if you have the service definition shown
    in the following example and need to access that service from another pod inside
    the cluster via an HTTP call, the call can simply use http://web1-svc if the client
    is in the same namespace as the service:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在规范中未声明服务类型，则 ClusterIP 是默认的服务类型。ClusterIP 意味着服务将被分配一个指定服务 CIDR 范围内的 IP。此
    IP 与服务对象一样持久，因此它为后端 Pod 提供 IP、端口和协议映射，使用选择器字段；然而，正如我们将看到的，有时您可以没有选择器的情况。服务的声明还为服务提供了一个域名系统（DNS）名称。这在集群内部促进了服务发现，并允许工作负载通过基于服务名称的
    DNS 查找轻松与集群中的其他服务通信。例如，如果您有如下示例中所示的服务定义，并且需要通过 HTTP 调用从集群内的另一个 Pod 访问该服务，则调用可以简单地使用
    http://web1-svc（如果客户端与服务在同一命名空间中）：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If it is required to find services in other namespaces, the DNS pattern would
    be `*<service_name>.<namespace_name>*.svc.cluster.local`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要在其他命名空间中查找服务，则 DNS 模式将是 `*<service_name>.<namespace_name>*.svc.cluster.local`。
- en: If no selector is given in a service definition, the endpoints can be explicitly
    defined for the service by using an endpoint API definition. This will basically
    add an IP and port as a specific endpoint to a service instead of relying on the
    selector attribute to automatically update the endpoints from the pods that are
    in scope by the selector match. This can be useful in a few scenarios in which
    you have a specific database that is not in a cluster that is to be used for testing,
    but you will change the service later to a Kubernetes-deployed database. This
    is sometimes called a *headless service* because it is not managed by `kube-proxy`
    as other services are, but you can directly manage the endpoints, as shown in
    [Figure 9-4](#figure94).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在服务定义中没有给出选择器，则可以通过使用端点 API 定义为服务显式定义服务的端点，而不是依赖于选择器属性从符合选择器匹配的 Pod 中自动更新端点。在某些场景中，这可能非常有用，例如您有一个特定的数据库用于测试，而不是集群中的数据库，但稍后将该服务更改为
    Kubernetes 部署的数据库。这有时被称为 *无头服务*，因为它不像其他服务一样由 `kube-proxy` 管理，但您可以直接管理端点，如 [图 9-4](#figure94)
    所示。
- en: '![kbp2 0904](assets/kbp2_0904.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0904](assets/kbp2_0904.png)'
- en: Figure 9-4\. ClusterIP-pod and service visualization
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. ClusterIP-Pod 和服务可视化
- en: Service Type NodePort
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodePort 服务类型
- en: The NodePort service type assigns a high-level port on each node of the cluster
    to the service IP and port on each node. The high-level NodePorts fall within
    the 30,000 through 32,767 ranges and can either be statically assigned or explicitly
    defined in the service specification. NodePorts are typically used for on-premises
    clusters or bespoke solutions that do not offer automatic load-balancing configuration.
    To directly access the service from outside the cluster, use NodeIP:NodePort,
    as depicted in [Figure 9-5](#figure95).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 服务类型为集群中每个节点分配一个高级端口到每个节点的服务 IP 和端口。高级 NodePort 位于 30,000 到 32,767
    范围内，并且可以静态分配或在服务规范中明确定义。NodePorts 通常用于本地集群或不提供自动负载均衡配置的定制解决方案。要从集群外部直接访问服务，请使用
    NodeIP:NodePort，如 [图 9-5](#figure95) 所示。
- en: '![kbp2 0905](assets/kbp2_0905.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0905](assets/kbp2_0905.png)'
- en: Figure 9-5\. NodePort–pod, service and host network visualization
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. NodePort-Pod、服务和主机网络可视化
- en: Service Type ExternalName
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部名称服务类型
- en: 'The ExternalName service type is seldom used in practice, but it can be helpful
    for passing cluster-durable DNS names to external DNS named services. A common
    example is an external database service from a cloud provider that has a unique
    DNS supplied by the cloud provider, such as `mymongodb.documents.azure.com`. Technically,
    this can be added very easily to a pod specification using an `Environment` variable,
    as discussed in [Chapter 6](ch06.html#versioning_releases_and_rollouts). However,
    it might be more advantageous to use a more generic name in the cluster, such
    as `prod-mongodb`, which enables the change of the actual database it points to
    by just changing the service specification instead of having to recycle the pods
    because the `Environment` variable has changed:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName 服务类型在实践中很少使用，但对于将集群持久性 DNS 名称传递给外部 DNS 命名服务可能有所帮助。一个常见的例子是来自云提供商的外部数据库服务，云提供商提供唯一
    DNS，例如 `mymongodb.documents.azure.com`。从技术上讲，可以非常容易地通过在 Pod 规范中使用 `Environment`
    变量来添加它，如 [第6章](ch06.html#versioning_releases_and_rollouts) 中所讨论的那样。然而，使用更通用的集群名称，如
    `prod-mongodb`，可能更有利，因为这样只需更改服务规范即可改变它指向的实际数据库，而无需因为 `Environment` 变量的更改而重新启动
    Pod：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Service Type LoadBalancer
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Service Type LoadBalancer
- en: 'LoadBalancer is a very special service type because it enables automation with
    cloud providers and other programmable cloud infrastructure services. The `LoadBalancer`
    type is a single method to ensure the deployment of the load-balancing mechanism
    that the infrastructure provider of the Kubernetes cluster supplies. This means
    that in most cases, `LoadBalancer` will work roughly the same way in AWS, Azure,
    GCE, OpenStack, and others. This entry will usually create a public-facing load-balanced
    service; however, each cloud provider has some specific annotations that enable
    other features, such as internal-only load balancers, AWS ELB configuration parameters,
    and so on. You can also define the actual load-balancer IP to use and the source
    ranges to allow within the service specification, as seen in the code sample that
    follows and the visual representation in [Figure 9-6](#figure96):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer 是一个非常特殊的服务类型，因为它可以与云提供商和其他可编程云基础设施服务自动化集成。`LoadBalancer` 类型是确保部署由
    Kubernetes 集群的基础设施提供商提供的负载均衡机制的单一方法。这意味着在大多数情况下，`LoadBalancer` 在 AWS、Azure、GCE、OpenStack
    等环境中的工作方式大致相同。这个入口通常会创建一个公共面向外部的负载均衡服务；然而，每个云提供商都有一些特定的注释，以启用其他功能，如仅内部的负载均衡器、AWS
    ELB 配置参数等。您还可以定义要使用的实际负载均衡器 IP 和在服务规范中允许的源范围，如接下来的代码示例和在 [图 9-6](#figure96) 中的可视化表示：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![kbp2 0906](assets/kbp2_0906.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0906](assets/kbp2_0906.png)'
- en: Figure 9-6\. LoadBalancer–pod, service, node, and cloud provider network visualization
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-6\. 负载均衡器- Pod、服务、节点和云提供商网络可视化
- en: Ingress and Ingress Controllers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress 和 Ingress 控制器
- en: 'Although not technically a service type in Kubernetes, the Ingress specification
    is an important concept for ingress to workloads in Kubernetes. Services, as defined
    by the Service API, allow for a basic level of Layer 3/4 load balancing. The reality
    is that many of the stateless services that are deployed in Kubernetes require
    a high level of traffic management and usually require application-level control:
    more specifically, HTTP protocol management.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 Kubernetes 中技术上不是一个服务类型，但 Ingress 规范对于 Kubernetes 中的工作负载入口非常重要。服务根据服务 API
    的定义，允许基本的第3/4层负载均衡。事实上，许多部署在 Kubernetes 中的无状态服务需要高级别的流量管理，并且通常需要应用级别的控制，尤其是 HTTP
    协议管理。
- en: The Ingress API is basically an HTTP-level router that allows for host- and
    path-based rules to direct to specific backend services. Imagine a website hosted
    on www.evillgenius.com and two different paths that are hosted on that site, */registration*
    and */labaccess*, that are served by two different services hosted in Kubernetes,
    `reg-svc` and `labaccess-svc`. You can define an ingress rule to ensure that requests
    to www.evillgenius.com/registration are forwarded to the `reg-svc` service and
    the correct endpoint pods, and, similarly, that requests to www.evillgenius.com/labaccess
    are forwarded to the correct endpoints of the `labaccess-svc` service. The Ingress
    API also permits host-based routing to allow for different hosts on a single ingress.
    An additional feature is the ability to declare a Kubernetes secret that holds
    the certificate information for Transport Layer Security (TLS) termination on
    port 443\. When a path is not specified, there is usually a default backend that
    can be used to give a better user experience than the standard 404 error.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 入口 API 基本上是一个 HTTP 级别的路由器，允许基于主机和路径的规则将请求定向到特定的后端服务。想象一个托管在 www.evillgenius.com
    上的网站，并且该站点上托管有两个不同路径，*/registration* 和 */labaccess*，由 Kubernetes 中托管的两个不同服务 `reg-svc`
    和 `labaccess-svc` 提供。您可以定义一个入口规则，以确保将对 www.evillgenius.com/registration 的请求转发到
    `reg-svc` 服务和正确的端点 pod，同样地，将对 www.evillgenius.com/labaccess 的请求转发到 `labaccess-svc`
    服务的正确端点。入口 API 还允许基于主机的路由，以允许在单个入口上使用不同的主机。另一个功能是声明一个 Kubernetes 密钥，其中包含用于在端口
    443 上终止传输层安全性（TLS）的证书信息。当未指定路径时，通常会有一个默认后端可以用于提供比标准 404 错误更好的用户体验。
- en: 'The details around the specific TLS and default backend configuration are actually
    handled by what is known as the Ingress controller. The Ingress controller is
    decoupled from the Ingress API and allows for operators to deploy an Ingress controller
    of choice, such as NGINX, Traefik, HAProxy, and others. An Ingress controller,
    as the name suggests, is a controller just like any Kubernetes controller, but
    it’s not part of the system and is instead a third-party controller that understands
    the Kubernetes Ingress API for dynamic configuration. The most common implementation
    of an Ingress controller is NGINX because it is partly maintained by the Kubernetes
    project; however, there are numerous examples of both open source and commercial
    Ingress controllers:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特定的 TLS 和默认后端配置的详细信息实际上由称为入口控制器的东西处理。入口控制器与入口 API 解耦，允许运维人员部署他们选择的入口控制器，如
    NGINX、Traefik、HAProxy 等。入口控制器就像任何 Kubernetes 控制器一样，但它不是系统的一部分，而是一个理解 Kubernetes
    入口 API 的第三方控制器，用于动态配置。入口控制器的最常见实现是 NGINX，因为它部分由 Kubernetes 项目维护；然而，也有许多开源和商业入口控制器的例子：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Gateway API
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网关 API
- en: 'The Ingress API had some challenges over the years that it was in beta and
    following its v1 promotion. These challenges have led to other network services
    offering different abstractions through the use of Custom Resource Definitions
    and controllers to create their own APIs that fill some of the gaps Ingress has
    had. Some of the most common challenges with the Ingress API have been:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 入口 API 在它处于 beta 阶段和升级到 v1 之后的几年里遇到了一些挑战。这些挑战导致了其他网络服务通过使用自定义资源定义和控制器来创建自己的
    API，填补了入口 API 存在的一些空白。入口 API 遇到的一些最常见挑战包括：
- en: The lack of expressiveness in the definition as it represents the lowest common
    denominator for the capabilities of the particular Ingress implementation.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义的表达能力不足，因为它代表特定入口实现的能力的最低公分母。
- en: A general lack of extensibility in the architecture. Vendors have used countless
    annotations to expose specific implementation capabilities; however, this has
    some limitations.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构中普遍存在的扩展性不足。供应商已经使用了无数的注解来暴露特定的实现能力；然而，这也有一些局限性。
- en: The use of vendor-specific annotations has removed some of the portability promised
    by the API. An annotation to expose a capability in an NGINX-based Ingress controller
    may be different or expressed differently from a Kong-based controller implementation.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定供应商的注解删除了 API 所承诺的部分可移植性。用于在基于 NGINX 的入口控制器中公开功能的注解可能与基于 Kong 的控制器实现有所不同或表达不同。
- en: There is no formal way to do multi-tenancy with the current Ingress API, and
    DevOps teams have to create very tight controls to prevent path conflicts between
    Ingress definitions that could impact other tenants in the same cluster.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前的入口 API 没有正式的多租户处理方式，DevOps 团队必须创建非常严格的控制措施，以防止入口定义之间的路径冲突，这可能会影响同一集群中的其他租户。
- en: Introduced in 2019, the Gateway API is currently managed as a project by the
    SIG Network team under the Kubernetes Project. The Gateway API does not intend
    to replace the Ingress API as it primarily targets exposing HTTP applications
    with a declarative syntax. This API exposes a more general API for proxying for
    many types of protocols, and fits a more role-based management process because
    it models more closely the infrastructure components in the environment.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 网关 API 自 2019 年推出以来，目前由 Kubernetes 项目下的 SIG Network 团队管理。网关 API 并不打算取代 Ingress
    API，因为它主要用于以声明性语法暴露 HTTP 应用程序。此 API 提供了一个更通用的代理多种协议的 API，并适合更加基于角色的管理流程，因为它更紧密地模拟环境中的基础设施组件。
- en: The role-based paradigm, as shown in [Figure 9-7](#figure97), is important in
    answering some of the shortcomings of the existing Ingress API. The separate components
    allow for infrastructure providers, such as cloud providers and proxy ISVs, to
    define the infrastructure and platform operators to define through policy what
    infrastructure can be used. Developers can then worry about how they want to expose
    their services within the constraints they are given. [Figure 9-8](#figure98)
    shows a practical example of how Gateway API structure abstracts the infrastructure
    services and capabilities away from the developer and allows them to focus on
    their specific service needs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于角色的范式，如[图 9-7](#figure97)所示，对于解决现有入口 API 的一些缺陷非常重要。独立的组件允许基础设施提供者（例如云提供商和代理
    ISV）定义基础设施，平台操作员通过策略定义可以使用什么基础设施。开发人员可以根据所给的约束条件考虑如何公开他们的服务。[图 9-8](#figure98)展示了网关
    API 结构如何将基础设施服务和功能抽象出来，使开发人员可以专注于其特定的服务需求。
- en: '![kbp2 0907](assets/kbp2_0907.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0907](assets/kbp2_0907.png)'
- en: Figure 9-7\. Gateway API structure
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. 网关 API 结构
- en: '![kbp2 0908](assets/kbp2_0908.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![kbp2 0908](assets/kbp2_0908.png)'
- en: Figure 9-8\. Gateway API structure, continued
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-8\. 网关 API 结构，续
- en: The specification is very promising, and many of the leading providers of proxies
    and services meshes, as well as cloud providers, have begun to implement the Gateway
    API into their stack. Google’s GKE, Acnodeal EPIC, Contour, Apache APISIX, and
    others have begun to offer limited preview or alpha support. As of this writing,
    the API itself is in beta for the GatewayClass, Gateway, and HTTPRoute resources,
    and others are in Alpha support. Unlike the Ingress API, this is a custom resource
    that can be added to any cluster and therefore does not follow the Kubernetes
    alpha or beta release process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这一规范非常有前景，许多主要的代理和服务网格提供商，以及云提供商，已经开始将网关 API 集成到其堆栈中。Google 的 GKE、Acnodeal EPIC、Contour、Apache
    APISIX 等已经开始提供有限的预览或 alpha 支持。截至目前，API 本身在 GatewayClass、Gateway 和 HTTPRoute 资源上处于
    beta 阶段，其他资源则处于 alpha 支持阶段。与 Ingress API 不同，这是一个自定义资源，可以添加到任何集群中，因此不遵循 Kubernetes
    的 alpha 或 beta 发布流程。
- en: Services and Ingress Controllers Best Practices
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务和入口控制器的最佳实践
- en: 'Creating a complex virtual network environment with interconnected applications
    requires careful planning. Effectively managing how the different services of
    the application communicate with one another and the outside world requires constant
    attention as the application changes. These best practices will help make management
    easier:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个复杂的虚拟网络环境，其中应用程序彼此相互连接，需要仔细规划。有效地管理应用程序不同服务之间以及与外部世界的通信方式，需要随着应用程序变化而持续关注。以下是一些管理最佳实践：
- en: Limit the number of services that need to be accessed from outside the cluster.
    Ideally, most services will be ClusterIP, and only external-facing services will
    be exposed externally to the cluster.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制需要从集群外部访问的服务数量。理想情况下，大多数服务将是 ClusterIP，只有外部服务才会暴露给集群外部。
- en: If the services that need to be exposed are primarily HTTP/HTTPS-based services,
    it is best to use an Ingress API and Ingress controller to route traffic to backing
    services with TLS termination. Depending on the type of Ingress controller used,
    features such as rate limiting, header rewrites, OAuth authentication, observability,
    and other services can be made available without having to build them into the
    applications themselves.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要暴露的服务主要是基于HTTP/HTTPS的服务，最好使用Ingress API和Ingress控制器来路由流量到支持TLS终止的后端服务。根据使用的Ingress控制器类型，诸如速率限制、头部重写、OAuth认证、可观察性以及其他服务等功能可以提供，而无需将它们构建到应用程序中。
- en: Choose an Ingress controller that has the needed functionality for secure ingress
    of your web-based workloads. Standardize on one and use it across the enterprise
    because many of the specific configuration annotations vary between implementations
    and prevent the deployment code from being portable across enterprise Kubernetes
    implementations.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个具有所需安全入口功能的Ingress控制器来服务您的基于Web的工作负载。在企业中标准化一个控制器并在全企业范围内使用它，因为许多特定的配置注解在不同实现之间会有所不同，并阻止部署代码在企业Kubernetes实施之间的可移植性。
- en: Evaluate cloud service provider–specific Ingress controller options to move
    the infrastructure management and load of the ingress out of the cluster, but
    still allow for Kubernetes API configuration.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估特定云服务提供商的Ingress控制器选项，以将入口的基础设施管理和负载移出集群，但仍允许通过Kubernetes API进行配置。
- en: When serving mostly APIs externally, evaluate API-specific Ingress controllers,
    such as Kong or Ambassador, that have more fine-tuning for API-based workloads.
    Although NGINX, Traefik, and others might offer some API tuning, it will not be
    as fine-grained as specific API proxy systems.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当主要外部提供API时，评估特定于API的Ingress控制器，如Kong或Ambassador，这些控制器对于基于API的工作负载具有更精细的调优能力。尽管NGINX、Traefik等可能提供了一些API调优，但不如专用的API代理系统精细。
- en: When deploying Ingress controllers as pod-based workloads in Kubernetes, ensure
    that the deployments are designed for high availability and aggregate performance
    throughput. Use metrics observability to properly scale the ingress, but include
    enough cushion to prevent client disruptions while the workload scales.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中将Ingress控制器部署为基于Pod的工作负载时，确保部署设计具有高可用性和聚合性能吞吐量。使用指标观测性来正确地调整Ingress的规模，但要包括足够的余量以防止工作负载扩展时客户端中断。
- en: Network Security Policy
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络安全策略
- en: The NetworkPolicy API built into Kubernetes allows for network-level ingress
    and egress access control defined with your workload. Network policies allow you
    to control how groups of pods are allowed to communicate with one another and
    with other endpoints. If you want to dig deeper into the NetworkPolicy specification,
    it might sound confusing, especially given that it is defined as a Kubernetes
    API, but it requires a network plug-in that supports the NetworkPolicy API.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes内置的NetworkPolicy API允许定义与工作负载相关的网络级入口和出口访问控制。网络策略允许您控制一组Pod如何与彼此以及其他端点通信。如果您希望深入了解NetworkPolicy规范，可能会感到困惑，特别是因为它被定义为Kubernetes
    API，但需要一个支持NetworkPolicy API的网络插件。
- en: 'Network policies have a simple YAML structure that can look complicated, but
    if you think of it as a simple East-West traffic firewall, it might help you to
    understand it a little better. Each policy specification has `podSelector`, `ingress`,
    `egress`, and `policyType` fields. The only required field is `podSelector`, which
    follows the same convention as any Kubernetes selector with a `matchLabels`. You
    can create multiple NetworkPolicy definitions that can target the same pods, and
    the effect is additive. Because NetworkPolicy objects are namespaced objects,
    if no selector is given for a `podSelector`, all pods in the namespace fall into
    the scope of the policy. If any ingress or egress rules are defined, this creates
    an allow list for what can ingress or egress from the pod. There is an important
    distinction here: if a pod falls into the scope of a policy because of a selector
    match, all traffic, unless explicitly defined in an ingress or egress rule, is
    blocked. This little, nuanced detail means that if a pod does not fall into any
    policy because of a selector match, all ingress and egress is allowed to the pod.
    This was done on purpose to allow for ease of deploying new workloads into Kubernetes
    without any blockers.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略具有简单的 YAML 结构，看起来可能复杂，但如果将其视为简单的东西流量防火墙，可能会帮助您更好地理解。每个策略规范都有 `podSelector`、`ingress`、`egress`
    和 `policyType` 字段。唯一必需的字段是 `podSelector`，其遵循与任何 Kubernetes 选择器相同的约定，并具有 `matchLabels`。您可以创建多个
    NetworkPolicy 定义，这些定义可以针对相同的 pods，其效果是累加的。因为 NetworkPolicy 对象是命名空间对象，如果没有为 `podSelector`
    给出选择器，那么命名空间中的所有 pods 都属于策略的范围。如果定义了任何 ingress 或 egress 规则，则这会创建一个允许列表，指定可以从 pod
    进入或离开的内容。这里有一个重要的区别：如果一个 pod 因为选择器匹配而落入策略的范围内，所有流量（除非在 ingress 或 egress 规则中明确定义）都会被阻止。这个微小而微妙的细节意味着，如果一个
    pod 因为选择器匹配而不落入任何策略，那么所有的 ingress 和 egress 都允许到该 pod。这是有意为之，以便在 Kubernetes 中轻松部署新的工作负载而无需任何阻碍。
- en: The `ingress` and `egress` fields are basically a list of rules based on source
    or destination and can be specific CIDR ranges, `podSelector`s, or `namespaceSelector`s.
    If you leave the ingress field empty, it is like a deny-all inbound. Similarly,
    if you leave the egress empty, it is deny-all outbound. Port and protocol lists
    are also supported to further tighten down the type of communications allowed.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`ingress` 和 `egress` 字段基本上是基于源或目的地的规则列表，可以是特定的 CIDR 范围，`podSelector` 或 `namespaceSelector`。如果将
    ingress 字段留空，则等同于拒绝所有入站流量。类似地，如果将 egress 字段留空，则等同于拒绝所有出站流量。还支持端口和协议列表，以进一步限制允许的通信类型。'
- en: The `policyTypes` field specifies to which network policy rule types the policy
    object is associated. If the field is not present, it will just look at the `ingress`
    and `egress` lists fields. The difference again is that you must explicitly call
    out egress in `policyTypes` and also have an egress rule list for this policy
    to work. Ingress is assumed, and defining it explicitly is not needed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`policyTypes` 字段指定策略对象关联的网络策略规则类型。如果该字段不存在，则只会查看 `ingress` 和 `egress` 列表字段。再次区分的是，您必须在
    `policyTypes` 中明确调用出 egress，并且还必须为此策略定义一个 egress 规则列表才能使其工作。Ingress 是默认的，不需要显式定义。'
- en: 'Let’s use a prototypical example of a three-tier application deployed to a
    single namespace where the tiers are labeled as `tier: "web"`, `tier: "db"`, and
    `tier: "api"`. If you want to ensure that traffic is properly limited to each
    tier, create a NetworkPolicy manifest like the following.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们以部署到单个命名空间的三层应用程序的原型示例为例，其中各层被标记为 `tier: "web"`、`tier: "db"` 和 `tier: "api"`。如果您希望确保流量正确限制到每个层级，请创建如下的
    NetworkPolicy 清单。'
- en: 'Default deny rule:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 默认拒绝规则：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Web layer network policy:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Web 层网络策略：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'API layer network policy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: API 层网络策略：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Database layer network policy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库层网络策略：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Network Policy Best Practices
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络策略最佳实践
- en: 'Securing network traffic in an enterprise system was once the domain of physical
    hardware devices with complex networking rule sets. Now, with Kubernetes network
    policy, a more application-centric approach can be taken to segment and control
    the traffic of the applications hosted in Kubernetes. Some common best practices
    apply no matter which policy plug-in is used:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业系统中保护网络流量曾经是复杂网络规则集的物理硬件设备的领域。现在，通过 Kubernetes 网络策略，可以采用更具应用中心化的方法来分段和控制托管在
    Kubernetes 中的应用程序的流量。一些常见的最佳实践适用于任何使用的策略插件：
- en: Start off slow and focus on traffic ingress to pods. Complicating matters with
    ingress and egress rules can make network tracing a nightmare. As soon as traffic
    is flowing as expected, you can begin to look at egress rules to further control
    flow to sensitive workloads. The specification also favors ingress because it
    defaults many options even if nothing is entered into the ingress rules list.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从流量入口到 pod 开始缓慢进行，并侧重于此。使用入口和出口规则会使网络跟踪变得非常复杂。一旦流量按预期流动，可以开始查看出口规则，以进一步控制流向敏感工作负载。规范也偏向于入口，因为即使在入口规则列表中没有输入任何内容，也会默认许多选项。
- en: Ensure that the network plug-in used either has some of its own interface to
    the NetworkPolicy API or supports other well-known plug-ins. Example plug-ins
    include Calico, Cilium, Kube-router, Romana, and Weave Net.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保所使用的网络插件要么具有自己的接口与 NetworkPolicy API 连接，要么支持其他众所周知的插件。例如插件包括 Calico、Cilium、Kube-router、Romana
    和 Weave Net。
- en: 'If the network team is used to having a “default-deny” policy in place, create
    a network policy such as the following for each namespace in the cluster that
    will contain workloads to be protected. This ensures that even if another network
    policy is deleted, no pods are accidentally “exposed”:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果网络团队习惯于使用“默认拒绝”策略，请为集群中每个包含要保护的工作负载的命名空间创建以下网络策略。这样即使删除了另一个网络策略，也不会意外“暴露”任何
    pod：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If pods need to be accessed from the internet, use a label to explicitly apply
    a network policy that allows ingress. Be aware of the entire flow in case the
    actual IP that a packet is coming from is not the internet but rather the internal
    IP of a load balancer, firewall, or other network device. For example, to allow
    traffic from all (including external) sources for pods having the `allow-internet=true`
    label, do this:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要从互联网访问 pod，请使用标签明确应用允许入口的网络策略。在实际 IP 不是来自互联网而是负载均衡器、防火墙或其他网络设备的内部 IP 的情况下，要注意整个流程。例如，为允许所有（包括外部）来源的
    pod 执行`allow-internet=true`标签的流量，执行以下操作：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Try to align application workloads to single namespaces for ease of creating
    rules because the rules themselves are namespace specific. If cross-namespace
    communication is needed, try to be as explicit as possible and perhaps use specific
    labels to identify the flow pattern:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量将应用工作负载对齐到单个命名空间，以便更容易创建规则，因为规则本身是命名空间特定的。如果需要跨命名空间通信，请尽可能明确，并可能使用特定标签来识别流模式：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Have a test bed namespace that has fewer restrictive policies, if any at all,
    to allow time to investigate the correct traffic patterns needed.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试命名空间中设置较少限制的策略（如果有的话），以便有时间调查所需的正确流量模式。
- en: Service Meshes
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格
- en: 'It is easy to imagine a single cluster hosting hundreds of services that load-balance
    across thousands of endpoints that communicate with one another, access external
    resources, and are potentially being accessed from external sources. This can
    be quite daunting when trying to manage, secure, observe, and trace all the connections
    among these services, especially with the dynamic nature of the endpoints coming
    and going from the overall system. The concept of a *service mesh*, which is not
    unique to Kubernetes, allows for control over how these services are connected
    and secured with a dedicated date plane and control plane. Service meshes all
    have different capabilities, but usually they all offer some of the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想象一个单一集群托管数百个服务，这些服务在数千个端点之间进行负载平衡，彼此通信，访问外部资源，并且可能被外部来源访问。在试图管理、安全地保护、观察和跟踪所有这些服务之间的连接时，这可能非常令人畏惧，特别是由于端点在整个系统中的动态性质。*服务网格*的概念（不仅限于
    Kubernetes）允许控制这些服务如何通过专用数据平面和控制平面连接和安全保护。服务网格通常具有不同的功能，但通常都提供以下一些功能：
- en: Load balancing of traffic with potentially fine-grained traffic-shaping policies
    that are distributed across the mesh.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可能是细粒度流量整形策略的负载均衡，这些策略分布在整个网格中。
- en: Service discovery of services that are members of the mesh, which might include
    services within a cluster or in another cluster, or an outside system that is
    a member of the mesh.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现网格成员服务，这可能包括集群内或另一个集群中的服务，或者是网格成员的外部系统。
- en: Observability of the traffic and services, including tracing across the distributed
    services using tracing systems like Jaeger or Zipkin that follow the OpenTracing
    standards.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察流量和服务的可观察性，包括使用像 Jaeger 或 Zipkin 这样遵循 OpenTracing 标准的跨分布式服务的追踪系统。
- en: Security of the traffic in the mesh using mutual authentication. In some cases,
    not only pod-to-pod or East-West traffic is secured, but an Ingress controller
    is also provided that offers North-South security and control.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格中流量的安全性使用相互认证来保障。在某些情况下，不仅仅是 Pod 与 Pod 或东西向流量被保护，还提供了一个 Ingress 控制器，提供南北向的安全性和控制。
- en: Resiliency, health, and failure-prevention capabilities that allow for patterns
    such as circuit breaker, retries, deadlines, and so on.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供弹性、健康和故障预防能力，允许诸如熔断器、重试、期限等模式。
- en: The key here is that all these features are integrated into the applications
    that take part in the mesh with little or no application changes. How can all
    these amazing features come for free? Sidecar proxies are usually the way this
    is done. The majority of service meshes available today inject a proxy that is
    part of the data plane into each pod that is a member of the mesh. This allows
    for policies and security to be synchronized across the mesh by the control-plane
    components. This hides the network details from the container that holds the workload
    and leaves it to the proxy to handle the complexity of the distributed network.
    In the application’s perspective, it only communicates via localhost to its proxy.
    In many cases, the control plane and data plane might be different technologies
    but complementary to each other.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是所有这些功能都集成到参与网格的应用程序中，几乎不需要或根本不需要应用程序更改。所有这些令人惊叹的功能如何免费获得？通常是通过 Sidecar
    代理完成的。今天大多数可用的服务网格在每个成员 pod 中注入一个数据平面的代理，以便由控制平面组件在整个网格中同步策略和安全性。这隐藏了容纳工作负载的容器的网络细节，并将其留给代理处理分布式网络的复杂性。从应用程序的角度来看，它仅通过
    localhost 与其代理进行通信。在许多情况下，控制平面和数据平面可能是不同的技术，但彼此互补。
- en: In many cases, the first service mesh that comes to mind is Istio, a project
    by Google, Lyft, and IBM that uses Envoy as its data-plane proxy and uses proprietary
    control-plane components Mixer, Pilot, Galley, and Citadel. Other service meshes
    offer varying levels of capabilities, such as Linkerd2, which uses its own data-plane
    proxy built using Rust. HashiCorp has recently added more Kubernetes-centric service
    mesh capabilities to Consul, which allows you to choose between Consul’s own proxy
    or Envoy, and offers commercial support for its service mesh.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，首先想到的服务网格是 Istio，这是一个由 Google、Lyft 和 IBM 合作的项目，使用 Envoy 作为其数据平面代理，并使用专有的控制平面组件
    Mixer、Pilot、Galley 和 Citadel。其他服务网格提供不同级别的功能，比如使用 Rust 构建自己的数据平面代理的 Linkerd2。HashiCorp
    最近在 Consul 中增加了更多面向 Kubernetes 的服务网格能力，允许您选择 Consul 自己的代理或 Envoy，并为其服务网格提供商业支持。
- en: The topic of service meshes in Kubernetes is a fluid one—if not overly emotional
    in many social media tech circles—so a detailed explanation of each mesh has no
    value here. We would be remiss if we did not mention the promising efforts led
    by Microsoft, Linkerd, HashiCorp, Solo.io, Kinvolk, and Weaveworks around the
    Service Mesh Interface (SMI). The SMI hopes to set a standard interface for basic
    feature sets that are expected of all service meshes. The specification as of
    this writing covers traffic policy such as identity and transport-level encryption,
    traffic telemetry that captures key metrics between services in the mesh, and
    traffic management to allow for traffic shifting and weighting between different
    services. This project hopes to take some of the variability out of the service
    meshes yet allow for service mesh vendors to extend and build value-added capabilities
    into their products to differentiate themselves.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的服务网格主题是一个流动的话题，如果不是在许多社交媒体技术圈中过于情绪化，那么详细解释每个网格在这里没有价值。如果不提到由 Microsoft、Linkerd、HashiCorp、Solo.io、Kinvolk
    和 Weaveworks 领导的有希望的努力，我们将不会尽责。服务网格接口（SMI）希望为所有服务网格期望的基本功能集设定一个标准接口。截至本文撰写时，该规范涵盖流量策略，如身份和传输级加密、捕获网格内服务之间关键指标的流量遥测以及允许在不同服务之间进行流量转移和加权的流量管理。该项目希望消除服务网格的某些变化性，同时允许服务网格供应商扩展和构建增值功能以区分自己的产品。
- en: Service Mesh Best Practices
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格最佳实践
- en: 'The service mesh community continues to grow every day, and as more enterprises
    help define their needs, the service mesh ecosystem will change dramatically.
    These best practices are, as of this writing, based on common problems that service
    meshes try to solve today:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格社区每天都在不断增长，随着更多企业帮助定义他们的需求，服务网格生态系统将发生显著变化。本文所述的最佳实践基于服务网格今天试图解决的常见问题：
- en: Rate the importance of the key features service meshes offer and determine which
    current offerings provide the most important features with the least amount of
    overhead. Overhead here means both human technical debt and infrastructure resource
    debt. If all that is really required is mutual TLS between certain pods, would
    it be easier to perhaps find a CNI that offers that capability integrated into
    the plug-in?
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估服务网格提供的关键功能的重要性，并确定哪些当前提供的方案在人力技术债务和基础设施资源债务方面提供了最重要的功能。如果确实需要的只是某些 Pod 之间的互联
    TLS，也许找到一个集成了这种功能的 CNI 插件会更容易。
- en: Is the need for a cross-system mesh, such as multicloud or hybrid scenarios,
    a key requirement? Not all service meshes offer this capability, and if they do,
    it is a complicated process that often introduces fragility into the environment.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨系统网格的需求，如多云或混合场景，是否是一个关键需求？并非所有服务网格都提供此功能，即使提供，也往往会在环境中引入脆弱性。
- en: Many of the service mesh offerings are open source community-based projects,
    and if the team that will be managing the environment is new to service meshes,
    commercially supported offerings might be a better option. Some companies are
    beginning to offer commercially supported and managed service meshes based on
    Istio, which can be helpful because it is almost universally agreed upon that
    Istio is a complicated system to manage.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多服务网格方案都是基于开源社区的项目，如果管理环境的团队对服务网格还不熟悉，商业支持的方案可能是一个更好的选择。一些公司开始提供基于 Istio 的商业支持和托管服务网格，这可能非常有帮助，因为几乎普遍认为
    Istio 是一个复杂的系统需要管理。
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In addition to application management, one of the most important things that
    Kubernetes provides is the ability to link different pieces of your application.
    In this chapter, we looked at the details of how Kubernetes works, including how
    pods get their IP addresses through CNI plug-ins, how those IPs are grouped to
    form services, and how more application or Layer 7 routing can be implemented
    via Ingress resources (which in turn use services). You also saw how to limit
    traffic and secure your network using networking policies, and, finally, how service
    mesh technologies are transforming the ways in which people connect and monitor
    the connections between their services. In addition to setting up your application
    to run and be deployed reliably, setting up the networking for your application
    is a crucial piece of using Kubernetes successfully. Understanding how Kubernetes
    approaches networking and how that intersects optimally with your application
    is critical to its ultimate success.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了应用程序管理外，Kubernetes 提供的最重要的功能之一是能够链接应用程序的不同部分。在本章中，我们详细介绍了 Kubernetes 的工作原理，包括如何通过
    CNI 插件为 Pod 分配 IP 地址，如何将这些 IP 地址分组形成服务，以及如何通过 Ingress 资源实现更多的应用程序或第 7 层路由（这些资源反过来使用服务）。您还看到了如何使用网络策略限制流量和保护网络安全，最后，看到了服务网格技术如何改变人们连接和监控服务之间连接方式的方式。除了设置应用程序以可靠地运行和部署外，为应用程序设置网络是成功使用
    Kubernetes 的关键部分。了解 Kubernetes 如何处理网络以及这如何与您的应用程序最佳交集是其最终成功的关键。
