<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Kubernetes and Cloud Networking"><div class="chapter" id="kubernetes_and_cloud_networking">
<h1><span class="label">Chapter 6. </span>Kubernetes and Cloud Networking</h1>


<p>The use of the cloud and its service offerings has grown tremendously: 77% of enterprises are using the public cloud
in some capacity, and 81% can innovate more quickly with the public cloud than on-premise. With the popularity and
innovation available in the cloud, it follows that running Kubernetes in the cloud is a logical step. Each major cloud
provider has its own managed service offering for Kubernetes using its cloud network services.</p>

<p>In this chapter, we’ll explore the network services offered by the major cloud providers AWS, Azure, and GCP with a
focus on how they affect the networking needed to run a Kubernetes cluster inside that specific cloud. All the <a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="CNIs for" id="idm46219933379960"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="cloud networking with" id="idm46219933379016"/>providers also have a CNI project that makes running a Kubernetes cluster smoother from an integration perspective with their cloud network APIs, so an exploration of the CNIs is warranted. After reading this chapter, administrators will  understand how cloud providers implement their managed Kubernetes on top of their cloud network
services.</p>






<section data-type="sect1" data-pdf-bookmark="Amazon Web Services"><div class="sect1" id="idm46219933377480">
<h1>Amazon Web Services</h1>

<p>Amazon Web <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="overview of" id="idm46219933375944"/>Services (AWS) has grown its cloud service offerings from Simple Queue Service (SQS) and Simple Storage
Service (S3) to well over 200 services.  Gartner Research positions AWS in the Leaders quadrant of its 2020 Magic
Quadrant for Cloud Infrastructure &amp; Platform Services. Many services are built atop of other foundational services.
For example, Lambda uses S3 for code storage and DynamoDB for metadata. AWS CodeCommit uses S3 for code storage.
EC2, S3, and CloudWatch are integrated into the Amazon Elastic MapReduce service, creating a managed data platform.
The AWS networking services are no different. Advanced services such as peering and endpoints use building blocks from
core networking fundamentals. Understanding those fundamentals, which enable AWS to build a
comprehensive Kubernetes service, is needed for administrators and developers.</p>








<section data-type="sect2" data-pdf-bookmark="AWS Network Services"><div class="sect2" id="idm46219933357544">
<h2>AWS Network Services</h2>

<p>AWS has many <a data-type="indexterm" data-primary="AWS networking" id="ch6_term1"/>services that allow users to extend and secure their cloud networks. <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="eksctl in" id="idm46219933354488"/>Amazon Elastic Kubernetes Service
(EKS) makes extensive use of those network components available in the AWS cloud. We will discuss the basics of
AWS networking components and how they are related to deploying an EKS cluster network. This section will also discuss
several other open source tools that make managing a cluster and application deployments simple. The <a data-type="indexterm" data-primary="eksctl tool, AWS" id="idm46219933353064"/><a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="eksctl as" id="idm46219933352392"/>first is <code>eksctl</code>,
a CLI tool that deploys and manages EKS clusters. As we have seen from previous chapters, there are many components
needed to run a cluster, and that is also true on the AWS network. <code>eksctl</code> will deploy all the components in AWS for
cluster and network administrators. Then, we will discuss the AWS VPC CNI, which allows the cluster to use native AWS
services to scale pods and manage their IP address space. Finally, we will examine the <a data-type="indexterm" data-primary="AWS networking" data-secondary="AWS ALB ingress controller in" id="idm46219933349992"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with AWS ALB" data-secondary-sortas="AWS ALB" id="idm46219933349000"/>AWS Application Load Balancer
ingress controller, which automates, manages, and simplifies deployments of application load balancers and ingresses
for developers running applications on the AWS network.</p>










<section data-type="sect3" data-pdf-bookmark="Virtual private cloud"><div class="sect3" id="idm46219933347432">
<h3>Virtual private cloud</h3>

<p>The basis of the <a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with AWS" data-secondary-sortas="AWS" id="idm46219933329624"/>AWS network is the virtual private cloud (VPC). A majority of AWS resources will work inside the VPC.
VPC networking is an isolated virtual network defined by administrators for only their account and its resources. <a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="for AWS network services" data-secondary-sortas="AWS network services" id="idm46219933328104"/>In
<a data-type="xref" href="#VPC">Figure 6-1</a>, we can see a VPC defined with a single CIDR of <code>192.168.0.0/16</code>. All resources inside the VPC will use that
range for private IP addresses. AWS is constantly enhancing its service offerings; now, network administrators can use
multiple nonoverlapping CIDRs in a VPC. The pod IP addresses will also come from VPC CIDR and host IP addressing; more on that in <a data-type="xref" href="#awsvpccni">“AWS VPC CNI”</a>. A VPC is set up per AWS region; you can have multiple VPCs per region, but a VPC is defined in only one.</p>

<figure class="width-50"><div id="VPC" class="figure">
<img src="Images/neku_0601.png" alt="neku 0601" width="1254" height="448"/>
<h6><span class="label">Figure 6-1. </span>AWS virtual private cloud</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Region and availability zones"><div class="sect3" id="region">
<h3>Region and availability zones</h3>

<p>Resources are <a data-type="indexterm" data-primary="AWS networking" data-secondary="with AZs and geographic regions" data-secondary-sortas="AZs and geographic regions" id="idm46219933320152"/><a data-type="indexterm" data-primary="regions in VPC networking" data-secondary="with AWS" data-secondary-sortas="AWS" id="idm46219933318792"/>defined by boundaries in AWS, such as global, region, or availability zone. AWS networking comprises multiple
regions; each <a data-type="indexterm" data-primary="availability zones (AZs)" id="idm46219933317288"/><a data-type="indexterm" data-primary="AZs (availability zones)" id="idm46219933316600"/>AWS region consists of multiple isolated and physically separate availability zones (AZs) within a
geographic area. An AZ can contain multiple data centers, as shown in <a data-type="xref" href="#aws-regiom">Figure 6-2</a>. Some regions can contain six
AZs, while newer regions could contain only two. Each AZ is directly connected to the
others but is isolated from the failures of another AZ. This design is important to understand for multiple reasons:
high availability, load balancing, and subnets are all affected. In one region a load balancer will route traffic
over multiple AZs, which have separate subnets and thus enable HA for applications.</p>

<figure><div id="aws-regiom" class="figure">
<img src="Images/neku_0602.png" alt="neku 0602" width="1216" height="554"/>
<h6><span class="label">Figure 6-2. </span>AWS region network layout</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>An up-to-date list of AWS regions and AZs is available in the  
<span class="keep-together"><a href="https://oreil.ly/gppRp">documentation</a>.</span></p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Subnet"><div class="sect3" id="idm46219933310024">
<h3>Subnet</h3>

<p>A VPC is compromised of <a data-type="indexterm" data-primary="AWS networking" data-secondary="subnets in" id="ch6_term2"/><a data-type="indexterm" data-primary="subnets" data-secondary="in AWS" data-secondary-sortas="AWS" id="ch6_term3"/><a data-type="indexterm" data-primary="routing" data-secondary="in cloud networking" data-secondary-sortas="cloud networking" id="ch6_term4"/><a data-type="indexterm" data-primary="routing" data-secondary="for external traffic" data-secondary-sortas="external traffic" id="ch6_term5"/><a data-type="indexterm" data-primary="routing" data-secondary="for internal traffic" data-secondary-sortas="internal traffic" id="ch6_term6"/><a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="for AWS network services" data-secondary-sortas="AWS network services" id="idm46219933301320"/>multiple subnets from the CIDR range and deployed to a single AZ. Applications that
require high availability should run in multiple AZs and be load balanced with any one of the load balancers
available, as discussed in <a data-type="xref" href="#region">“Region and availability zones”</a>.</p>

<p>A subnet is <a data-type="indexterm" data-primary="private subnets" id="idm46219933298136"/><a data-type="indexterm" data-primary="public subnets" id="idm46219933297432"/><a data-type="indexterm" data-primary="subnets" data-secondary="private" id="idm46219933296728"/><a data-type="indexterm" data-primary="subnets" data-secondary="public" id="idm46219933295784"/>public if the routing table has a <a data-type="indexterm" data-primary="IGW (internet gateway), AWS" id="idm46219933294712"/><a data-type="indexterm" data-primary="internet gateway (IGW), AWS" id="idm46219933294024"/>route to an internet gateway. In <a data-type="xref" href="#Private-subnet">Figure 6-3</a>, there are three public and private subnets. Private subnets have no direct route to the internet. These subnets are for
internal network traffic, such as databases. The size of your VPC CIDR range and the number of public and private subnets
are a design consideration when deploying your network architecture. Recent improvements to VPC like allowing
multiple CIDR ranges help lessen the ramification of poor design choices, since now network engineers can simply add
another CIDR range to a provisioned VPC.</p>

<figure><div id="Private-subnet" class="figure">
<img src="Images/neku_0603.png" alt="Subnet" width="1342" height="481"/>
<h6><span class="label">Figure 6-3. </span>VPC subnets</h6>
</div></figure>

<p>Let’s discuss those components that help define if a subnet is public or private.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Routing tables"><div class="sect3" id="idm46219933289528">
<h3>Routing tables</h3>

<p>Each subnet <a data-type="indexterm" data-primary="AWS networking" data-secondary="route tables in" id="ch6_term7"/><a data-type="indexterm" data-primary="route tables" id="ch6_term8"/>has exactly one route table associated with it. If one is not explicitly associated with it, the main
route table is the default one. Network connectivity issues can manifest here; developers deploying applications
inside a VPC must know to manipulate route tables to ensure traffic flows where it’s intended.</p>

<p>The following are rules for the <a data-type="indexterm" data-primary="main route tables, VPC" id="idm46219933284808"/>main route table:</p>

<ul>
<li>
<p>The main route table cannot be deleted.</p>
</li>
<li>
<p>A gateway route table cannot be set as the main.</p>
</li>
<li>
<p>The main route table can be replaced with a custom route table.</p>
</li>
<li>
<p>Admins can add, remove, and modify routes in the main route table.</p>
</li>
<li>
<p>The local route is the most specific.</p>
</li>
<li>
<p>Subnets can explicitly associate with the main route table.</p>
</li>
</ul>

<p>There are route tables with specific goals in mind; here is a list of them and a description of how they are
different:</p>
<dl>
<dt>Main route table</dt>
<dd>
<p>This route table automatically controls routing for all subnets that are not explicitly
associated with any other route table.</p>
</dd>
<dt>Custom route table</dt>
<dd>
<p>A route table <a data-type="indexterm" data-primary="custom route table, VPC" id="idm46219933274312"/>network engineers create and customize for specific application traffic flow.</p>
</dd>
<dt>Edge association</dt>
<dd>
<p>A routing table <a data-type="indexterm" data-primary="edge association, VPC" id="idm46219933272168"/>to route inbound VPC traffic to an edge appliance.</p>
</dd>
<dt>Subnet route table</dt>
<dd>
<p>A route table <a data-type="indexterm" data-primary="subnet route table, VPC" id="idm46219933269960"/>that’s associated with a subnet.</p>
</dd>
<dt>Gateway route table</dt>
<dd>
<p>A route table <a data-type="indexterm" data-primary="gateway route table, VPC" id="idm46219933267816"/><a data-type="indexterm" data-primary="IGW (internet gateway), AWS" id="idm46219933267064"/><a data-type="indexterm" data-primary="internet gateway (IGW), AWS" id="idm46219933266376"/>that’s associated with an internet gateway or virtual private gateway.</p>
</dd>
</dl>

<p>Each route table has several components that determine its responsibilities:</p>
<dl>
<dt>Route table association</dt>
<dd>
<p>The association between a route table and a subnet, internet
gateway, or virtual private gateway.</p>
</dd>
<dt>Rules</dt>
<dd>
<p>A list of routing entries that define the table; each rule has a destination, target, status, and
propagated flag.</p>
</dd>
<dt>Destination</dt>
<dd>
<p>The range of IP addresses where you want traffic to go (destination CIDR).</p>
</dd>
<dt>Target</dt>
<dd>
<p>The gateway, network interface, or connection through which to send the destination traffic; for example,
an internet gateway.</p>
</dd>
<dt>Status</dt>
<dd>
<p>The state of a route in the route table: active or blackhole. The blackhole state indicates that the
route’s target isn’t available.</p>
</dd>
<dt>Propagation</dt>
<dd>
<p>Route propagation allows a virtual private gateway to automatically propagate routes to the route
tables. This flag lets you know if it was added via propagation.</p>
</dd>
<dt>Local route</dt>
<dd>
<p>A default <a data-type="indexterm" data-primary="local route, VPC" id="idm46219933254600"/>route for communication within the VPC.</p>
</dd>
</dl>

<p>In <a data-type="xref" href="#route-table_6">Figure 6-4</a>, there are two routes in the route table. Any traffic destined for <code>11.0.0.0/16</code> stays on the local network
inside the VPC. All <a data-type="indexterm" data-primary="public subnets" id="idm46219933251800"/><a data-type="indexterm" data-primary="subnets" data-secondary="public" id="idm46219933251096"/>other traffic, <code>0.0.0.0/0</code>, goes to the internet gateway, <code>igw-f43c4690</code>, making it a public subnet.</p>

<figure><div id="route-table_6" class="figure">
<img src="Images/neku_0604.png" alt="Route" width="594" height="158"/>
<h6><span class="label">Figure 6-4. </span>Route table</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Elastic network interface"><div class="sect3" id="idm46219933288936">
<h3>Elastic network interface</h3>

<p>An elastic <a data-type="indexterm" data-startref="ch6_term2" id="idm46219933245656"/><a data-type="indexterm" data-startref="ch6_term3" id="idm46219933244952"/><a data-type="indexterm" data-startref="ch6_term4" id="idm46219933244248"/><a data-type="indexterm" data-startref="ch6_term5" id="idm46219933243576"/><a data-type="indexterm" data-startref="ch6_term6" id="idm46219933242904"/><a data-type="indexterm" data-startref="ch6_term7" id="idm46219933242232"/><a data-type="indexterm" data-startref="ch6_term8" id="idm46219933241560"/><a data-type="indexterm" data-primary="AWS networking" data-secondary="ENIs and EIPs in" id="ch6_term9"/><a data-type="indexterm" data-primary="ENI (elastic network interface)" id="ch6_term10"/>network interface (ENI) is a logical networking component in a VPC that is equivalent to a virtual network
card. ENIs contain an IP address, for the instance, and they are elastic in the sense that they can be associated and
disassociated to an instance while retaining its properties.</p>

<p>ENIs have these <a data-type="indexterm" data-primary="IPv4" data-secondary="addresses" id="idm46219933238072"/><a data-type="indexterm" data-primary="IPv4" data-secondary="and ENIs" data-secondary-sortas="ENIs" id="idm46219933237096"/><a data-type="indexterm" data-primary="IPv6" data-secondary="addresses" id="idm46219933235848"/><a data-type="indexterm" data-primary="IPv6" data-secondary="and ENIs" data-secondary-sortas="ENIs" id="idm46219933234904"/>properties:</p>

<ul>
<li>
<p>Primary private IPv4 address</p>
</li>
<li>
<p>Secondary private IPv4 addresses</p>
</li>
<li>
<p>One elastic IP (EIP) address per private IPv4 address</p>
</li>
<li>
<p>One public IPv4 address, which can be auto-assigned to the network interface for <code>eth0</code> when you launch an instance</p>
</li>
<li>
<p>One or more IPv6 addresses</p>
</li>
<li>
<p>One or more security groups</p>
</li>
<li>
<p>MAC address</p>
</li>
<li>
<p>Source/destination check flag</p>
</li>
<li>
<p>Description</p>
</li>
</ul>

<p>A common <a data-type="indexterm" data-primary="corporate networks with ENIs" id="idm46219933224024"/>use case for ENIs is the creation of management networks that are accessible only from a corporate network.
<a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with AWS" data-secondary-sortas="AWS" id="idm46219933222968"/>AWS services like Amazon WorkSpaces use ENIs to allow access to the customer VPC and the AWS-managed VPC. Lambda can
reach resources, like databases, inside a VPC by provisioning and attaching to an ENI.</p>

<p>Later in the section we will see how the AWS VPC CNI uses and manages ENIs along with IP addresses for pods.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Elastic IP address"><div class="sect3" id="idm46219933220712">
<h3>Elastic IP address</h3>

<p>An EIP <a data-type="indexterm" data-primary="EIP (elastic IP addresses)" id="idm46219933219208"/><a data-type="indexterm" data-primary="elastic IP addresses (EIP)" id="idm46219933218504"/>address is a static public IPv4 address used for dynamic network addressing in the AWS cloud. An
EIP is associated with any instance or network interface in any VPC. With an EIP, application developers can mask
an instance’s failures by remapping the address to another instance.</p>

<p>An EIP address is a property of an ENI and is associated with an instance by updating the ENI attached to the
instance. The advantage of associating an EIP with the ENI rather than directly to the instance is that all the
network interface attributes move from one instance to another in a single step.</p>

<p class="pagebreak-before">The following rules apply:</p>

<ul>
<li>
<p>An EIP address can be associated with either a single instance or a network interface at a time.</p>
</li>
<li>
<p>An EIP address can migrate from one instance or network interface to another.</p>
</li>
<li>
<p>There is a (soft) limit of five EIP addresses.</p>
</li>
<li>
<p>IPv6 is not supported.</p>
</li>
</ul>

<p>Services like NAT and internet gateway use EIPs for consistency between the AZ. Other gateway services like a
bastion can benefit from using an EIP. <a data-type="indexterm" data-primary="AWS networking" data-secondary="subnets in" id="ch6_term11"/><a data-type="indexterm" data-primary="subnets" data-secondary="in AWS" data-secondary-sortas="AWS" id="ch6_term12"/>Subnets can automatically assign public IP addresses to EC2 instances, but that
address could change; using an EIP would prevent <a data-type="indexterm" data-startref="ch6_term9" id="idm46219933208248"/><a data-type="indexterm" data-startref="ch6_term10" id="idm46219933207576"/>that.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Security controls"><div class="sect3" id="idm46219933206648">
<h3>Security controls</h3>

<p>There are two <a data-type="indexterm" data-primary="AWS networking" data-secondary="security controls in" id="ch6_term13"/><a data-type="indexterm" data-primary="security" data-secondary="in AWS networking" data-secondary-sortas="AWS networking" id="ch6_term14"/>fundamental security controls within AWS networking: security groups and network access control lists
(NACLs). In our experience, lots of issues arise from misconfigured security groups and NACLs. Developers and <a data-type="indexterm" data-primary="developers" id="idm46219933202200"/><a data-type="indexterm" data-primary="network engineers" id="idm46219933201528"/>network
engineers need to understand the differences between the two and the impacts of changes on them.</p>












<section data-type="sect4" data-pdf-bookmark="Security groups"><div class="sect4" id="idm46219933200472">
<h4>Security groups</h4>

<p>Security groups <a data-type="indexterm" data-primary="Security Groups, AWS" id="idm46219933199000"/>operate at the instance or network interface level and act as a firewall for those devices associated with them. A security group is a group of network devices that require common network access to each other and other devices on the network. <a data-type="indexterm" data-primary="availability zones (AZs)" id="idm46219933197880"/><a data-type="indexterm" data-primary="AZs (availability zones)" id="idm46219933197192"/><a data-type="indexterm" data-primary="AWS networking" data-secondary="with AZs and geographic regions" data-secondary-sortas="AZs and geographic regions" id="idm46219933196504"/><a data-type="indexterm" data-primary="regions in VPC networking" data-secondary="with AWS" data-secondary-sortas="AWS" id="idm46219933195256"/>In <a data-type="xref" href="#sec-group">Figure 6-5</a> ,we can see that security works across AZs. Security groups have two tables, for inbound and outbound traffic flow. Security groups are stateful, so if traffic is allowed on the inbound flow, the outgoing traffic is allowed. Each security group has a list of rules that define the filter for traffic. Each rule is evaluated before a forwarding decision is made.</p>

<figure><div id="sec-group" class="figure">
<img src="Images/neku_0605.png" alt="Security Group" width="1342" height="541"/>
<h6><span class="label">Figure 6-5. </span>Security group</h6>
</div></figure>

<p class="pagebreak-before">The following is a list of components of security group rules:</p>
<dl>
<dt>Source/destination</dt>
<dd>
<p>Source (inbound rules) or destination (outbound rules) of the traffic inspected:</p>

<ul>
<li>
<p>Individual or range of IPv4 or IPv6 addresses</p>
</li>
<li>
<p>Another security group</p>
</li>
<li>
<p>Other ENIs, gateways, or interfaces</p>
</li>
</ul>
</dd>
<dt>Protocol</dt>
<dd>
<p>Which layer 4 protocol being filtered, 6 (TCP), 17 (UDP), and 1 (ICMP)</p>
</dd>
<dt>Port range</dt>
<dd>
<p>Specific ports for the protocol being filtered</p>
</dd>
<dt>Description</dt>
<dd>
<p>User-defined field to inform others of the intent of the security group</p>
</dd>
</dl>

<p>Security groups are similar to the Kubernetes network policies we discussed in earlier chapters. They are a
fundamental network technology and should always be used to secure your instances in the AWS VPC. EKS deploys several
security groups for communication between the AWS-managed data plane and your worker nodes.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Network access control lists"><div class="sect4" id="idm46219933179656">
<h4>Network access control lists</h4>

<p>Network access <a data-type="indexterm" data-primary="network engineers" id="idm46219933178024"/><a data-type="indexterm" data-primary="NACL (network access control lists)" id="ch6_term15"/><a data-type="indexterm" data-primary="network access control lists (NACL)" id="ch6_term16"/>control lists operate similarly to how they do in other firewalls so that network engineers will be
familiar with them. In <a data-type="xref" href="#NACL">Figure 6-6</a>, you can see each subnet has a default NACL associated with it and is bounded to an
AZ, unlike the security group. Filter rules must be defined explicitly in both directions. The default rules are quite
permissive, allowing all traffic in both directions. Users can define their own NACLs to use with a subnet for an
added security layer if the security group is too open. By default, custom NACLs deny all traffic, and therefore add rules when
deployed; otherwise, instances will lose connectivity.</p>

<p>Here are <a data-type="indexterm" data-startref="ch6_term11" id="idm46219933173400"/><a data-type="indexterm" data-startref="ch6_term12" id="idm46219933172664"/>the components of an NACL:</p>
<dl>
<dt>Rule number</dt>
<dd>
<p>Rules are evaluated starting with the lowest numbered rule.</p>
</dd>
<dt>Type</dt>
<dd>
<p>The type of traffic, such as SSH or HTTP.</p>
</dd>
<dt>Protocol</dt>
<dd>
<p>Any protocol that has a standard protocol number: TCP/UDP or ALL.</p>
</dd>
<dt>Port range</dt>
<dd>
<p>The listening port or port range for the traffic. For example, 80 for HTTP traffic.</p>
</dd>
<dt>Source</dt>
<dd>
<p>Inbound rules only; the CIDR range source of the traffic.</p>
</dd>
<dt>Destination</dt>
<dd>
<p>Outbound rules only; the destination for the traffic.</p>
</dd>
<dt>Allow/Deny</dt>
<dd>
<p>Whether to allow or deny the specified traffic.</p>
</dd>
</dl>

<figure><div id="NACL" class="figure">
<img src="Images/neku_0606.png" alt="NACL" width="1342" height="575"/>
<h6><span class="label">Figure 6-6. </span>NACL</h6>
</div></figure>

<p>NACLs add an extra layer of security for subnets that may protect from lack or misconfiguration of security groups.</p>

<p><a data-type="xref" href="#security_and_nacl_comparison_table">Table 6-1</a> summarizes the <a data-type="indexterm" data-primary="Security Groups, AWS" id="idm46219933158088"/>fundamental differences between security groups and network ACLs.</p>
<table id="security_and_nacl_comparison_table">
<caption><span class="label">Table 6-1. </span>Security and NACL comparison table</caption>
<thead>
<tr>
<th>Security group</th>
<th>Network ACL</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Operates at the instance level.</p></td>
<td><p>Operates at the subnet level.</p></td>
</tr>
<tr>
<td><p>Supports allow rules only.</p></td>
<td><p>Supports allow rules and deny rules.</p></td>
</tr>
<tr>
<td><p>Stateful: Return traffic is automatically allowed, regardless of any rules.</p></td>
<td><p>Stateless: Return traffic must be explicitly allowed by rules.</p></td>
</tr>
<tr>
<td><p>All rules are evaluated before a forwarding decision is made.</p></td>
<td><p>Rules are processed in order, starting with the lowest numbered rule.</p></td>
</tr>
<tr>
<td><p>Applies to an instance or network interface.</p></td>
<td><p>All rules apply to all instances in the subnets that it’s associated with.</p></td>
</tr>
</tbody>
</table>

<p class="pagebreak-before">It is crucial to <a data-type="indexterm" data-primary="network engineers" id="idm46219933144744"/><a data-type="indexterm" data-primary="developers" id="idm46219933144008"/>understand the differences between NACL and security groups. Network connectivity issues often arise
due to a security group not allowing traffic on a specific port or someone not adding an outbound rule on an NACL. When
troubleshooting issues with AWS networking, developers and network engineers alike should add checking these components
to their troubleshooting <a data-type="indexterm" data-startref="ch6_term13" id="idm46219933142824"/><a data-type="indexterm" data-startref="ch6_term14" id="idm46219933142152"/><a data-type="indexterm" data-startref="ch6_term15" id="idm46219933141480"/><a data-type="indexterm" data-startref="ch6_term16" id="idm46219933140808"/>list.</p>

<p>All the <a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with AWS" data-secondary-sortas="AWS" id="ch6_term19"/>components we have discussed thus far manage traffic flow inside the VPC. The following services
manage traffic into the VPC from client requests and ultimately to applications running inside a Kubernetes cluster:
network address translation devices, internet gateway, and load balancers. Let’s dig into those a little more.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Network address translation devices"><div class="sect3" id="idm46219933137384">
<h3>Network address translation devices</h3>

<p>Network address <a data-type="indexterm" data-primary="AWS networking" data-secondary="NAT devices in" id="idm46219933135976"/><a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="with AWS network services" data-secondary-sortas="AWS network services" id="idm46219933134968"/>translation (NAT) devices are used when instances inside a VPC require internet connectivity, but
network connections should not be made directly to instances. Examples of instances that should run behind a NAT
device are database instances or other middleware needed to run applications.</p>

<p>In AWS, network engineers have several options for running NAT devices. They can manage their own NAT
devices deployed as EC2 instances or <a data-type="indexterm" data-primary="AWS Managed Service NAT gateway (NAT GW)" id="idm46219933132744"/><a data-type="indexterm" data-primary="NAT GW (AWS Managed Service NAT gateway)" id="idm46219933131944"/>use the AWS Managed Service NAT gateway (NAT GW). Both <a data-type="indexterm" data-primary="public subnets" id="idm46219933131112"/><a data-type="indexterm" data-primary="subnets" data-secondary="public" id="idm46219933130440"/>require public subnets
deployed in multiple AZs for high availability and EIP. A restriction of a NAT GW is that the IP address of it cannot
change after you deploy it. Also, that IP address will be the source IP address used to communicate with the internet
gateway.</p>

<p>In the VPC <a data-type="indexterm" data-primary="route tables" id="idm46219933128472"/><a data-type="indexterm" data-primary="AWS networking" data-secondary="route tables in" id="idm46219933127768"/>route table in <a data-type="xref" href="#Nat-Int-Routing-Diagram">Figure 6-7</a>, we can see how the two route tables exist to establish a connection to the
internet. The <a data-type="indexterm" data-primary="main route tables, VPC" id="idm46219933125752"/>main route table has two rules, a local route for the inter-VPC and a route for <code>0.0.0.0/0</code> with a target of
the NAT GW ID. The <a data-type="indexterm" data-primary="private subnets" id="idm46219933124568"/><a data-type="indexterm" data-primary="subnets" data-secondary="private" id="idm46219933123864"/>private subnet’s database servers will route traffic to the internet via that NAT GW rule in
their route tables.</p>

<p>Pods and instances in EKS will need to egress the VPC, so a NAT device must be deployed. Your choice of NAT device
will depend on the operational overhead, cost, or availability requirements for your network <a data-type="indexterm" data-startref="ch6_term17" id="idm46219933122184"/><a data-type="indexterm" data-startref="ch6_term18" id="idm46219933121480"/>design.</p>

<figure><div id="Nat-Int-Routing-Diagram" class="figure">
<img src="Images/neku_0607.png" alt="net-int" width="1413" height="1663"/>
<h6><span class="label">Figure 6-7. </span>VPC routing diagram</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Internet gateway"><div class="sect3" id="idm46219933118648">
<h3>Internet gateway</h3>

<p>The internet <a data-type="indexterm" data-primary="AWS networking" data-secondary="internet gateway in" id="idm46219933117240"/><a data-type="indexterm" data-primary="internet gateway (IGW), AWS" id="idm46219933116232"/>gateway is an AWS-managed service and device in the VPC network that allows connectivity to the internet
for all devices in the VPC. Here are the steps to ensure access to or from the internet in a VPC:</p>
<ol class="less_space pagebreak-before">
<li>
<p>Deploy and attach an IGW to the VPC.</p>
</li>
<li>
<p>Define a route in the subnet’s route table that directs internet-bound traffic to the IGW.</p>
</li>
<li>
<p>Verify NACLs and security group rules allow the traffic to flow to and from instances.</p>
</li>

</ol>

<p>All of this is shown in the VPC routing from <a data-type="xref" href="#Nat-Int-Routing-Diagram">Figure 6-7</a>. We see the IGW deploy for the VPC, a <a data-type="indexterm" data-primary="custom route table, VPC" id="idm46219933110312"/>custom route table
setup that routes all traffic, <code>0.0.0.0/0</code>, to the IGW. The web instances have an IPv4 internet routable address,
<code>198.51.100.1-3</code>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Elastic load balancers"><div class="sect3" id="idm46219933108264">
<h3>Elastic load balancers</h3>

<p>Now that traffic flows from the internet and clients can request access to applications running inside a VPC, we
will <a data-type="indexterm" data-startref="ch6_term19" id="idm46219933106440"/>need to scale and distribute the load for requests. <a data-type="indexterm" data-primary="AWS networking" data-secondary="elastic load balancer in" id="ch6_term20"/><a data-type="indexterm" data-primary="elastic load balancers" id="ch6_term21"/><a data-type="indexterm" data-primary="load balancing" data-secondary="elastic for AWS" id="ch6_term22"/>AWS has several options for developers, depending on the
type of application load and network traffic requirements needed.</p>

<p>The elastic load balancer has four options:</p>
<dl>
<dt>Classic</dt>
<dd>
<p>A classic <a data-type="indexterm" data-primary="classic load balancers" id="idm46219933099704"/>load balancer provides fundamental load balancing of EC2 instances. It operates at the request and
the connection level. Classic load balancers are limited in functionality and are not to be used with
containers.</p>
</dd>
<dt>Application</dt>
<dd>
<p>Application load <a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="with HTTP" data-secondary-sortas="HTTP" id="idm46219933097336"/><a data-type="indexterm" data-primary="AWS ALB (application load balancer) ingress controller" data-secondary="overview of" id="idm46219933096056"/><a data-type="indexterm" data-primary="AWS networking" data-secondary="AWS ALB ingress controller in" id="idm46219933095016"/><a data-type="indexterm" data-primary="HTTP" data-secondary="application load balancer and" id="idm46219933094104"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with AWS ALB" data-secondary-sortas="AWS ALB" id="idm46219933093144"/>balancers are layer 7 aware. Traffic routing is made with request-specific
information like HTTP headers or HTTP paths. The application load balancer is used with the application load balancer
controller. The ALB controller allows devs to automate the deployment and ALB without using the console or API, instead just
a few YAML lines.</p>
</dd>
<dt>Network</dt>
<dd>
<p>The network <a data-type="indexterm" data-primary="network load balancers" id="idm46219933090168"/><a data-type="indexterm" data-primary="Transport layer, TCP/IP (L4)" data-secondary="load balancing and" id="idm46219933089432"/>load balancer operates at layer 4. Traffic can be routed based on incoming TCP/UDP ports to
individual hosts running services on that port. The network load balancer also allows admins to deploy then with an EIP, a
feature unique to the network load balancer.</p>
</dd>
<dt>Gateway</dt>
<dd>
<p>The gateway <a data-type="indexterm" data-primary="gateway load balancers" id="idm46219933086792"/>load balancer manages traffic for appliances at the VPC level. Such network devices
like deep packet inspection or proxies can be used with a gateway load balancer. The gateway load balancer is added here
to complete the AWS service offering but is not used within the EKS ecosystem.</p>
</dd>
</dl>

<p class="pagebreak-before">AWS load balancers have several attributes that are important to understand when working with not only containers but
other workloads inside the VPC:</p>
<dl>
<dt>Rule</dt>
<dd>
<p>(ALB only) The <a data-type="indexterm" data-primary="rules" data-secondary="AWS load balancer" id="idm46219933082824"/>rules that you define for your listener determine how the load balancer routes all requests to
the targets in the target groups.</p>
</dd>
<dt>Listener</dt>
<dd>
<p>Checks for requests from clients. <a data-type="indexterm" data-primary="HTTP" data-secondary="load balancing" id="idm46219933080264"/><a data-type="indexterm" data-primary="load balancing" data-secondary="HTTP" id="idm46219933079288"/>They support HTTP and HTTPS on ports 1–65535.</p>
</dd>
<dt>Target</dt>
<dd>
<p>An EC2 instance, IP address, pods, or lambda running application code.</p>
</dd>
<dt>Target Group</dt>
<dd>
<p>Used to route requests to a registered target.</p>
</dd>
<dt>Health Check</dt>
<dd>
<p>Test to <a data-type="indexterm" data-primary="health checks/probes" data-secondary="for load balancers" data-secondary-sortas="load balancers" id="idm46219933074168"/>ensure targets are still able to accept client requests.</p>
</dd>
</dl>

<p>Each of these <a data-type="indexterm" data-primary="listeners, AWS load balancer" id="idm46219933072376"/><a data-type="indexterm" data-primary="requests, AWS load balancer rules for" id="idm46219933071624"/>components of an ALB is outlined in <a data-type="xref" href="#loadbalancer_6">Figure 6-8</a>. When a request comes into the load balancer, a listener is
continually checking for requests that match the protocol and port defined for it. Each listener has a set of rules that
define where to direct the request. The rule will have an action type to determine how to handle the request:</p>
<dl>
<dt>authenticate-cognito</dt>
<dd>
<p>(HTTPS listeners) Use <a data-type="indexterm" data-primary="authentication" data-secondary="for AWS load balancers" data-secondary-sortas="AWS load balancers" id="idm46219933067848"/>Amazon Cognito to authenticate users.</p>
</dd>
<dt>authenticate-oidc</dt>
<dd>
<p>(HTTPS listeners) Use an identity provider that is compliant with OpenID Connect to authenticate users.</p>
</dd>
<dt>fixed-response</dt>
<dd>
<p>Returns a custom HTTP response.</p>
</dd>
<dt>forward</dt>
<dd>
<p>Forward requests to the specified target groups.</p>
</dd>
<dt>redirect</dt>
<dd>
<p>Redirect requests from one URL to another.</p>
</dd>
</dl>

<p>The action with the lowest order value is performed first. Each rule must include exactly one of the following
actions: forward, redirect, or fixed-response. In <a data-type="xref" href="#loadbalancer_6">Figure 6-8</a>, we have <a data-type="indexterm" data-primary="targets/target groups, AWS" id="idm46219933059544"/>target groups, which will be the recipient of
our forward rules. Each target in the target group will have health checks so the load balancer will know which
instances are healthy and ready to receive requests.</p>

<figure><div id="loadbalancer_6" class="figure">
<img src="Images/neku_0608.png" alt="loadbalancer" width="1297" height="476"/>
<h6><span class="label">Figure 6-8. </span>Load balancer components</h6>
</div></figure>

<p>Now that we have a basic understanding of how AWS structures its networking components, we can begin to see how EKS
leverages these components to the network and secure the managed Kubernetes cluster and <a data-type="indexterm" data-startref="ch6_term1" id="idm46219933055896"/><a data-type="indexterm" data-startref="ch6_term20" id="idm46219933055192"/><a data-type="indexterm" data-startref="ch6_term21" id="idm46219933054520"/><a data-type="indexterm" data-startref="ch6_term22" id="idm46219933053848"/>network.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Amazon Elastic Kubernetes Service"><div class="sect2" id="idm46219933357080">
<h2>Amazon Elastic Kubernetes Service</h2>

<p>Amazon Elastic <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" id="ch6_term23"/><a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="overview of" id="idm46219933050552"/>Kubernetes Service (EKS) is AWS’s managed Kubernetes service. It <a data-type="indexterm" data-primary="network administrators" id="idm46219933049480"/><a data-type="indexterm" data-primary="cluster administrators" data-secondary="AWS EKS and" id="idm46219933048728"/><a data-type="indexterm" data-primary="developers" id="idm46219933047784"/>allows developers, cluster administrators,
and network administrators to <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="scaling with" id="idm46219933046984"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="scaling with" id="idm46219933046024"/><a data-type="indexterm" data-primary="scaling" id="idm46219933045080"/>quickly deploy a production-scale Kubernetes cluster. Using the scaling nature
of the cloud and AWS network services, with one API request, many services are deployed, including all the
components we reviewed in the previous sections.</p>

<p>How does EKS accomplish this? Like with any new service AWS releases, EKS has gotten significantly more feature-rich and
easier to use. EKS now supports on-prem deploys with EKS Anywhere, serverless with EKS Fargate, and even Windows
nodes. EKS <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="eksctl as" id="idm46219933043512"/><a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="eksctl in" id="idm46219933042568"/><a data-type="indexterm" data-primary="eksctl tool, AWS" id="idm46219933041608"/>clusters can be deployed traditionally with the AWS CLI or console. <code>eksctl</code> is a command-line tool
developed by Weaveworks, and it is by far the easiest way to date to deploy all the components needed to run EKS. Our
next section will detail the requirements to run an EKS cluster and how <code>eksctl</code> accomplishes this for cluster admins
and devs.</p>

<p>Let’s discuss the components of EKS cluster networking.</p>










<section data-type="sect3" data-pdf-bookmark="EKS nodes"><div class="sect3" id="idm46219933039096">
<h3>EKS nodes</h3>

<p>Workers nodes <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="nodes in" id="idm46219933037512"/><a data-type="indexterm" data-primary="nodes" data-secondary="in EKS" data-secondary-sortas="EKS" id="idm46219933036488"/>in EKS come in three flavors: EKS-managed node groups, self-managed nodes, and AWS Fargate.  The choice
for the administrator is how much control and operational overhead they would like to accrue.</p>
<dl>
<dt>Managed node group</dt>
<dd>
<p>Amazon EKS <a data-type="indexterm" data-primary="managed node groups, Amazon EKS" id="idm46219933033240"/><a data-type="indexterm" data-primary="EC2 instances, Amazon EKS" id="idm46219933032536"/><a data-type="indexterm" data-primary="Auto Scaling groups, Amazon EKS" id="idm46219933031848"/>managed node groups create and manage EC2 instances for you. All managed nodes are provisioned as
part of an  EC2 Auto Scaling group that’s managed by Amazon EKS as well. All resources including  EC2 instances and
Auto Scaling groups run within your AWS account. A managed-node group’s Auto Scaling group spans all the subnets that
you specify when you create the group.</p>
</dd>
<dt>Self-managed node group</dt>
<dd>
<p>Amazon EKS <a data-type="indexterm" data-primary="self-managed node groups, Amazon EKS" id="idm46219933029368"/>nodes run in your AWS account and connect to your cluster’s control plane via the API
endpoint. You deploy nodes into a node group. A node group is a collection of EC2 instances that are
deployed in an EC2 Auto Scaling group. All instances in a node group must do the following:</p>

<ul>
<li>
<p>Be the same instance type</p>
</li>
<li>
<p>Be running the same Amazon Machine Image</p>
</li>
<li>
<p>Use the same Amazon EKS node IAM role</p>
</li>
</ul>
</dd>
<dt>Fargate</dt>
<dd>
<p>Amazon EKS integrates Kubernetes <a data-type="indexterm" data-primary="Fargate, AWS" id="idm46219933023512"/>with AWS Fargate by using controllers that are built by AWS using the upstream,
extensible model provided by Kubernetes. Each pod running on Fargate has its own isolation boundary and does not
share the underlying kernel, CPU, memory, or elastic network interface with another pod. You also
cannot use security groups for pods with pods running on Fargate.</p>
</dd>
</dl>

<p>The instance <a data-type="indexterm" data-primary="cluster networking" data-secondary="in cloud" data-secondary-sortas="cloud" id="idm46219933021800"/>type also affects the cluster network. In EKS the number of pods that can run on the nodes is <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219933020312"/>defined
by the number of IP addresses that instance can run. We discuss this further in <a data-type="xref" href="#awsvpccni">“AWS VPC CNI”</a> and <a data-type="xref" href="#eksctl">“eksctl”</a>.</p>

<p>Nodes must be able to communicate with the Kubernetes control plane and other AWS services. The IP address space is
crucial to run an EKS cluster. Nodes, pods, and <a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="for Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219933016584"/><a data-type="indexterm" data-primary="NAT GW (AWS Managed Service NAT gateway)" id="idm46219933015240"/><a data-type="indexterm" data-primary="AWS Managed Service NAT gateway (NAT GW)" id="idm46219933014536"/>all other services will use the VPC CIDR address ranges for
components. The EKS VPC requires a NAT gateway for private subnets and that those subnets be tagged for use with EKS:</p>
<pre>Key – kubernetes.io/cluster/&lt;cluster-name&gt;
Value – shared</pre>

<p>The placement of each node will determine the network “mode” that EKS operates; this has design considerations for
your subnets and Kubernetes API traffic routing.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="EKS mode"><div class="sect3" id="idm46219933012408">
<h3>EKS mode</h3>

<p><a data-type="xref" href="#eks-comms">Figure 6-9</a> outlines EKS components. The <a data-type="indexterm" data-primary="control plane, Kubernetes" data-secondary="of Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219933009816"/><a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with AWS" data-secondary-sortas="AWS" id="ch6_term26"/>Amazon EKS control plane creates up to four cross-account elastic network
interfaces in your VPC for each cluster. EKS uses two VPCs, one for the Kubernetes control plane, including the
Kubernetes API masters, API loadbalancer, and etcd depending on the networking model; the <a data-type="indexterm" data-primary="customer VPC, Amazon EKS" id="ch6_term24"/>other is the customer VPC
where the EKS worker nodes run your pods. As <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="with Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219933005624"/>part of the boot process for the EC2 instance, the Kubelet is started. The node’s Kubelet reaches out to the Kubernetes cluster endpoint to register the node. It connects either to
the public 
<span class="keep-together">endpoint</span> outside the VPC or to the private endpoint within the VPC. <code>kubectl</code> commands reach out to the API
endpoint in the EKS VPC. <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="communication path of" id="idm46219933002840"/>End users reach applications running in the customer VPC.</p>

<figure><div id="eks-comms" class="figure">
<img src="Images/neku_0609.png" alt="eks-comms" width="1144" height="734"/>
<h6><span class="label">Figure 6-9. </span>EKS communication path</h6>
</div></figure>

<p>There are three ways to configure cluster control traffic and the Kubernetes API endpoint for EKS, depending on where
the control and data planes of the Kubernetes components run.</p>

<p>The networking <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="public/private network modes in" id="ch6_term25"/>modes are as follows:</p>
<dl>
<dt>Public-only</dt>
<dd>
<p>Everything runs in a <a data-type="indexterm" data-primary="public subnets" id="idm46219932995560"/><a data-type="indexterm" data-primary="subnets" data-secondary="public" id="idm46219932994824"/>public subnet, including worker nodes.</p>
</dd>
<dt>Private-only</dt>
<dd>
<p>Runs solely in a <a data-type="indexterm" data-primary="private subnets" id="idm46219932992472"/><a data-type="indexterm" data-primary="subnets" data-secondary="private" id="idm46219932991736"/>private subnet, and Kubernetes cannot create internet-facing load balancers.</p>
</dd>
<dt>Mixed</dt>
<dd>
<p>Combo of public and private.</p>
</dd>
</dl>

<p class="pagebreak-before">The public <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="with Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932988568"/>endpoint is the default option; it is public because the load balancer for the API endpoint is on a
public subnet, as shown in <a data-type="xref" href="#eks-public">Figure 6-10</a>. Kubernetes API requests that originate from within the cluster’s VPC, like when
the worker node reaches out to the control plane, leave the customer VPC, but not the Amazon network. <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="security with" id="idm46219932986008"/><a data-type="indexterm" data-primary="security" data-secondary="with public connectivity" data-secondary-sortas="public connectivity" id="idm46219932985048"/><a data-type="indexterm" data-primary="security" data-secondary="in Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932983960"/>One security
concern to consider when using a public endpoint is that the API endpoints are on a public subnet and reachable on
the internet.</p>

<figure><div id="eks-public" class="figure">
<img src="Images/neku_0610.png" alt="eks-public" width="1234" height="858"/>
<h6><span class="label">Figure 6-10. </span>EKS public-only network mode</h6>
</div></figure>

<p><a data-type="xref" href="#eks-priv">Figure 6-11</a> shows the private endpoint mode; all traffic to your cluster API must come from within
your cluster’s VPC. There’s no internet access to your API server; any <code>kubectl</code> commands must come from within the
VPC or a connected network. The cluster’s API endpoint is resolved by public DNS to a private IP address in
the VPC.</p>

<figure><div id="eks-priv" class="figure">
<img src="Images/neku_0611.png" alt="eks-priv" width="1310" height="1013"/>
<h6><span class="label">Figure 6-11. </span>EKS private-only network mode</h6>
</div></figure>

<p>When both public and private endpoints are enabled, <a data-type="indexterm" data-primary="control plane, Kubernetes" data-secondary="of Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932976184"/>any Kubernetes API requests from within the VPC communicate to the
control plane by the EKS-managed ENIs within the customer VPC, as demonstrated in <a data-type="xref" href="#eks-combo">Figure 6-12</a>. The cluster API <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="security with" id="idm46219932973784"/><a data-type="indexterm" data-primary="security" data-secondary="with public connectivity" data-secondary-sortas="public connectivity" id="idm46219932972792"/><a data-type="indexterm" data-primary="security" data-secondary="in Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932971560"/>is
still accessible from the internet, but it can be limited using security groups and NACLs.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Please see the <a href="https://oreil.ly/mW7ii">AWS documentation</a> for more ways to deploy an EKS.</p>
</div>

<p>Determining what <a data-type="indexterm" data-primary="cluster administrators" data-secondary="VPC decisions by" id="idm46219932967768"/>mode to operate in is a critical decision administrators will make. It will affect the
application traffic, the routing for load balancers, and the security of the cluster. There are many other requirements
when deploying a cluster in EKS as <a data-type="indexterm" data-startref="ch6_term24" id="idm46219932966376"/><a data-type="indexterm" data-startref="ch6_term25" id="idm46219932965704"/>well. <code>eksctl</code> is one tool to help manage all those requirements. But how does <code>eksctl</code>
accomplish that?</p>

<figure><div id="eks-combo" class="figure">
<img src="Images/neku_0612.png" alt="eks-combo" width="1234" height="1061"/>
<h6><span class="label">Figure 6-12. </span>EKS public and private network mode</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="eksctl"><div class="sect3" id="eksctl">
<h3>eksctl</h3>

<p><code>eksctl</code> is a command-line <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="eksctl in" id="ch6_term27"/><a data-type="indexterm" data-primary="eksctl tool, AWS" id="ch6_term28"/>tool developed by Weaveworks, and it is by far the easiest way to deploy all the components needed to run EKS.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>All the information about <code>eksctl</code> is available on its <a href="https://eksctl.io">website</a>.</p>
</div>

<p><code>eksctl</code> defaults to creating a cluster with the following default parameters:</p>

<ul>
<li>
<p>An autogenerated cluster name</p>
</li>
<li>
<p>Two m5.large worker nodes</p>
</li>
<li>
<p>Use of the official AWS EKS AMI</p>
</li>
<li>
<p>Us-west-2 default AWS region</p>
</li>
<li>
<p>A dedicated VPC</p>
</li>
</ul>

<p>A dedicated <a data-type="indexterm" data-primary="AWS networking" data-secondary="subnets in" id="idm46219932948536"/><a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="for Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932947560"/><a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="for AWS network services" data-secondary-sortas="AWS network services" id="idm46219932946216"/><a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="nodes in" id="idm46219932944952"/><a data-type="indexterm" data-primary="IGW (internet gateway), AWS" id="idm46219932943992"/><a data-type="indexterm" data-primary="internet gateway (IGW), AWS" id="idm46219932943304"/><a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="with Amazon eksctl" data-secondary-sortas="Amazon eksctl" id="idm46219932942616"/><a data-type="indexterm" data-primary="nodes" data-secondary="in EKS" data-secondary-sortas="EKS" id="idm46219932941384"/><a data-type="indexterm" data-primary="private subnets" id="idm46219932940168"/><a data-type="indexterm" data-primary="subnets" data-secondary="in AWS" data-secondary-sortas="AWS" id="idm46219932939496"/><a data-type="indexterm" data-primary="subnets" data-secondary="private" id="idm46219932938280"/>VPC with 192.168.0.0/16 CIDR range, <code>eksctl</code> will create by default 8 /19 subnets: three private, three
public, and two reserved subnets. <code>eksctl</code> will also deploy a NAT GW that allows for communication of nodes placed in
private subnets and an internet gateway to enable access for needed container images and communication  to the Amazon
S3 and Amazon ECR APIs.</p>

<p>Two security <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="security with" id="idm46219932935768"/><a data-type="indexterm" data-primary="security" data-secondary="in Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932934744"/><a data-type="indexterm" data-primary="ingress inter node group SG" id="idm46219932933528"/><a data-type="indexterm" data-primary="control plane, Kubernetes" data-secondary="of Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932932840"/>groups are set up for the EKS cluster:</p>
<dl>
<dt>Ingress inter node group SG</dt>
<dd>
<p>Allows nodes to communicate with each other  on all ports</p>
</dd>
<dt>Control plane security group</dt>
<dd>
<p>Allows communication between the control plane and worker node groups</p>
</dd>
</dl>

<p>Node groups in <a data-type="indexterm" data-primary="subnets" data-secondary="public" id="idm46219932927848"/><a data-type="indexterm" data-primary="public subnets" id="idm46219932926840"/>public subnets will have SSH disabled. <a data-type="indexterm" data-primary="EC2 instances, Amazon EKS" id="idm46219932925992"/>EC2 instances in the initial node group get a public IP and can be
accessed on high-level ports.</p>

<p>One node group <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932924632"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="maximum pods and" id="idm46219932923336"/><a data-type="indexterm" data-primary="pods" data-secondary="and Amazon eksctl" data-secondary-sortas="Amazon eksctl" id="idm46219932922376"/><a data-type="indexterm" data-primary="pods" data-secondary="maximum number of" id="idm46219932921160"/>containing two m5.large nodes is the default for <code>eksctl</code>. But how many pods can that node run? AWS has
a formula based on the node type and the number of interfaces and IP addresses it can support. That formula is as follows:</p>

<pre data-type="programlisting">(Number of network interfaces for the instance type ×
(the number of IP addresses per network interface - 1)) + 2</pre>

<p>Using the preceding formula and the default instance size on <code>eksctl</code>, an m5.large can support a maximum of 29 pods.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>System pods count toward the maximum pods. The CNI plugin and <code>kube-proxy</code> pods run on every node
in a cluster, so you’re only able to deploy 27 additional pods to an m5.large instance.
CoreDNS runs on nodes in the cluster, which further decrements the maximum number of pods a node can run.</p>
</div>

<p>Teams running <a data-type="indexterm" data-primary="cluster administrators" data-secondary="VPC decisions by" id="idm46219932915432"/><a data-type="indexterm" data-primary="network engineers" id="idm46219932914424"/>clusters must decide on cluster sizing and instance types to ensure no deployment issues with
hitting node and IP limitations. Pods will sit in the “waiting” state if there are no nodes available with the pod’s
IP address. Scaling events for the EKS node groups can also hit EC2 instance type limits and cause cascading
issues.</p>

<p>All of these networking options are configurable via the <code>eksctl</code> config file.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><code>eksctl</code> VPC options are available in the <a href="https://oreil.ly/m2Nqc">eksctl documentation</a>.</p>
</div>

<p>We have <a data-type="indexterm" data-startref="ch6_term27" id="idm46219932909576"/><a data-type="indexterm" data-startref="ch6_term28" id="idm46219932908872"/>discussed how the size node is important for pod IP addressing and the number of them we can run. Once the
node is deployed, <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="AWS VPC CNI in" id="ch6_term29"/><a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="CNIs for" id="ch6_term31"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="AWS VPC CNI as" id="ch6_term32"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="cloud networking with" id="ch6_term33"/><a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="AWS VPC CNI in" id="ch6_term34"/><a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="nodes in" id="ch6_term35"/><a data-type="indexterm" data-primary="nodes" data-secondary="in EKS" data-secondary-sortas="EKS" id="ch6_term36"/>the AWS VPC CNI manages pod IP addressing for nodes. Let’s dive into the inner workings of the CNI.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="AWS VPC CNI"><div class="sect3" id="awsvpccni">
<h3>AWS VPC CNI</h3>

<p>AWS has its <a data-type="indexterm" data-primary="open source projects" id="idm46219932897064"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI plug-ins for" id="idm46219932896328"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubernetes plugins with" id="idm46219932895384"/>open source implementation of a CNI. AWS VPC CNI for the Kubernetes plugin offers high throughput and
availability, low latency, and minimal network jitter on the AWS network. Network engineers can apply existing AWS
VPC networking and security best practices for building Kubernetes clusters on  AWS. It includes using native AWS
services like VPC flow logs, VPC routing policies, and security groups for network traffic isolation.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The open source for AWS VPC CNI is on <a href="https://oreil.ly/akwqx">GitHub</a>.</p>
</div>

<p>There are two components to the AWS VPC CNI:</p>
<dl>
<dt>CNI plugin</dt>
<dd>
<p>The CNI plugin is responsible for wiring up the host’s and pod’s network stack when called. It also
configures the interfaces and virtual Ethernet pairs.</p>
</dd>
<dt>ipamd</dt>
<dd>
<p>A long-running <a data-type="indexterm" data-primary="IPAMD, AWS VPC CNI" id="idm46219932888600"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932887864"/>node-local IPAM daemon is responsible for maintaining a
warm pool of available IP addresses and assigning an IP address to a pod.</p>
</dd>
</dl>

<p><a data-type="xref" href="#aws-VPC-cni">Figure 6-13</a> demonstrates what the VPC CNI will do for nodes. A <a data-type="indexterm" data-primary="customer VPC, Amazon EKS" id="idm46219932884680"/>customer VPC with a subnet 10.200.1.0/24 in AWS
gives us 250 usable addresses in this subnet. There are two nodes in our cluster. In EKS, the managed nodes run
with the AWS CNI as a daemon set. In our example, each node has  only one pod running, with a secondary <a data-type="indexterm" data-primary="ENI (elastic network interface)" data-seealso="EKS (Amazon Elastic Kubernetes Service)" id="idm46219932883624"/>IP address on
the ENI, <code>10.200.1.6</code> and <code>10.200.1.8</code>, for each pod. When a worker node first joins the cluster, there is only one ENI
and all its addresses in the ENI. When pod three gets scheduled to node 1, ipamd assigns the IP address to
the ENI for that pod. In this case, <code>10.200.1.7</code> is the same thing on node 2 with pod 4.</p>

<p>When a <a data-type="indexterm" data-primary="nodes" data-secondary="worker nodes as" id="idm46219932880648"/><a data-type="indexterm" data-primary="worker nodes, Kubernetes" id="idm46219932879640"/>worker node first joins the cluster, there is only one ENI and all of its addresses in the ENI. Without
any configuration, <code>ipamd</code> always tries to keep one extra ENI. When several pods running on the node exceeds the number
of addresses on a single ENI, the CNI backend starts allocating a new ENI. The CNI plugin works by <a data-type="indexterm" data-primary="EC2 instances, Amazon EKS" id="idm46219932878136"/>allocating multiple
ENIs to EC2 instances and then attaches secondary IP addresses to these ENIs. This plugin allows the CNI to allocate
as many IPs per instance as 
<span class="keep-together">possible.</span></p>

<figure><div id="aws-VPC-cni" class="figure">
<img src="Images/neku_0613.png" alt="neku 0613" width="1343" height="1073"/>
<h6><span class="label">Figure 6-13. </span>AWS VPC CNI example</h6>
</div></figure>

<p>The AWS VPC CNI is highly configurable. This list includes just a few options:</p>
<dl>
<dt>AWS_VPC_CNI_NODE_PORT_SUPPORT</dt>
<dd>
<p>Specifies whether <a data-type="indexterm" data-primary="NodePort services, Kubernetes" id="idm46219932872312"/>NodePort services are enabled on a worker node’s primary network
interface. This requires additional <code>iptables</code> rules and that the kernel’s reverse path filter on the primary interface
is set to loose.</p>
</dd>
<dt>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG</dt>
<dd>
<p>Worker nodes can <a data-type="indexterm" data-primary="public subnets" id="idm46219932869448"/><a data-type="indexterm" data-primary="private subnets" id="idm46219932868712"/><a data-type="indexterm" data-primary="subnets" data-secondary="private" id="idm46219932868040"/><a data-type="indexterm" data-primary="subnets" data-secondary="public" id="idm46219932867096"/>be configured in public subnets, so you need to configure
pods to be deployed in private subnets, or if pods’ security requirement needs are different from others running on the
node, setting this to <code>true</code> will enable that.</p>
</dd>
<dt>AWS_VPC_ENI_MTU</dt>
<dd>
<p>Default: 9001. Used to <a data-type="indexterm" data-primary="maximum transmission unit (MTU)" id="idm46219932864072"/><a data-type="indexterm" data-primary="MTU (maximum transmission unit)" id="idm46219932863272"/><a data-type="indexterm" data-primary="packets" data-secondary="MTUs of" id="idm46219932862584"/>configure the MTU size for attached ENIs. The valid range is
from 576 to 9001.</p>
</dd>
<dt>WARM_ENI_TARGET</dt>
<dd>
<p>Specifies the <a data-type="indexterm" data-primary="IPAMD, AWS VPC CNI" id="idm46219932860232"/>number of free elastic network interfaces (and all of their available IP addresses)
that the <code>ipamd</code> daemon should attempt to keep available for pod assignment on the node. By default, <code>ipamd</code> attempts to
keep one elastic network interface and all of its IP addresses available for pod assignment. The number of IP addresses
per network interface varies by instance type.</p>
</dd>
<dt>AWS_VPC_K8S_CNI_EXTERNALSNAT</dt>
<dd>
<p>Specifies <a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="with AWS VPN CNI" data-secondary-sortas="AWS VPN CNI" id="idm46219932856792"/><a data-type="indexterm" data-primary="SNAT (source NAT)" id="idm46219932855448"/>whether an external NAT gateway should be used to provide SNAT of secondary
ENI IP addresses. If set to <code>true</code>, the SNAT <code>iptables</code> rule and external VPC IP rule are not applied, and these rules are
removed if they have already been applied. Disable SNAT if you need to allow inbound communication to your pods from
external VPNs, direct connections, and external VPCs, and your pods do not need to access the internet directly via
an internet gateway.</p>
</dd>
</dl>

<p>For example, if your pods with a private IP address need to communicate with others’ private IP address spaces, you
enable <code>AWS_VPC_K8S_CNI_EXTERNALSNAT</code> by using this command:</p>

<pre data-type="programlisting">kubectl set env daemonset
-n kube-system aws-node AWS_VPC_K8S_CNI_EXTERNALSNAT=true</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>All the information <a data-type="indexterm" data-primary="EKS Pod Networking, information for" id="idm46219932850712"/>for EKS pod networking can be found in the <a href="https://oreil.ly/RAVVY">EKS documentation</a>.</p>
</div>

<p>The AWS VPC CNI allows for maximum control over the networking options on EKS in the AWS network.</p>

<p>There is <a data-type="indexterm" data-startref="ch6_term26" id="idm46219932847672"/><a data-type="indexterm" data-startref="ch6_term29" id="idm46219932846968"/><a data-type="indexterm" data-startref="ch6_term31" id="idm46219932846264"/><a data-type="indexterm" data-startref="ch6_term32" id="idm46219932845592"/><a data-type="indexterm" data-startref="ch6_term33" id="idm46219932844920"/><a data-type="indexterm" data-startref="ch6_term34" id="idm46219932844248"/><a data-type="indexterm" data-startref="ch6_term35" id="idm46219932843576"/><a data-type="indexterm" data-startref="ch6_term36" id="idm46219932842904"/><a data-type="indexterm" data-primary="AWS ALB (application load balancer) ingress controller" data-secondary="overview of" id="ch6_term37"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="with AWS ALB" data-secondary-sortas="AWS ALB" id="ch6_term38"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="controllers" id="ch6_term39"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with AWS ALB" data-secondary-sortas="AWS ALB" id="ch6_term40"/><a data-type="indexterm" data-primary="AWS networking" data-secondary="AWS ALB ingress controller in" id="ch6_term41"/>also the AWS ALB ingress controller that makes managing and deploying applications on the
AWS cloud network smooth and automated. Let’s dig into that next.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="AWS ALB ingress controller"><div class="sect3" id="idm46219932898312">
<h3>AWS ALB ingress controller</h3>

<p>Let’s walk through the example in <a data-type="xref" href="#alb">Figure 6-14</a> of how the AWS ALB works with Kubernetes. For a review of what an
ingress controller is, please check out <a data-type="xref" href="ch05.xhtml#kubernetes_networking_abstractions">Chapter 5</a>.</p>

<p>Let’s discuss all the moving parts of ALB Ingress controller:</p>
<ol>
<li>
<p>The ALB ingress controller watches for ingress events from the API server. When requirements are met,
it will start the creation process of an ALB.</p>
</li>
<li>
<p>An ALB is created in AWS for the new ingress resource. Those resources can be internal or external to the cluster.</p>
</li>
<li>
<p>Target groups <a data-type="indexterm" data-primary="targets/target groups, AWS" id="idm46219932828344"/>are created in AWS for each unique Kubernetes service described in the ingress resource.</p>
</li>
<li>
<p>Listeners are <a data-type="indexterm" data-primary="listeners, AWS load balancer" id="idm46219932826568"/><a data-type="indexterm" data-primary="NodePort services, Kubernetes" id="idm46219932825816"/>created for every port detailed in your ingress resource annotations. Default ports
for HTTP and HTTPS traffic are set up if not specified. NodePort
services for each service create the node ports that are used for our health checks.</p>
</li>
<li>
<p>Rules are <a data-type="indexterm" data-primary="rules" data-secondary="AWS load balancer" id="idm46219932823912"/>created for each path specified in your ingress resource. This ensures traffic to a specific
path is routed to the correct Kubernetes service.</p>
</li>

</ol>

<figure><div id="alb" class="figure">
<img src="Images/neku_0614.png" alt="ALB" width="1060" height="934"/>
<h6><span class="label">Figure 6-14. </span>AWS ALB example</h6>
</div></figure>

<p>How traffic reaches nodes and pods is affected by one of two modes the ALB can run:</p>
<dl>
<dt>Instance mode</dt>
<dd>
<p>Ingress <a data-type="indexterm" data-primary="instance mode, AWS ALB" id="idm46219932818536"/>traffic starts at the ALB and reaches the Kubernetes nodes through each service’s NodePort.
This means that services referenced from ingress resources must be exposed by <code>type:NodePort</code> to be reached by the ALB.</p>
</dd>
<dt>IP mode</dt>
<dd>
<p>Ingress <a data-type="indexterm" data-primary="IP mode, AWS ALB" id="idm46219932815688"/>traffic starts at the ALB and reaches directly to the Kubernetes pods. CNIs must support a directly
accessible pod IP address via secondary IP addresses on ENI.</p>
</dd>
</dl>

<p>The AWS ALB ingress controller allows developers to manage their network needs like their application components.
There is no need for other tool sets in the pipeline.</p>

<p>The AWS networking components are tightly integrated with EKS. <a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="scaling with" id="idm46219932813192"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="scaling with" id="idm46219932812120"/><a data-type="indexterm" data-primary="scaling" id="idm46219932811176"/>Understanding the basic options of how they work is
fundamental for all those looking to scale their applications on Kubernetes on AWS using EKS. The <a data-type="indexterm" data-primary="subnets" data-secondary="cluster size and size of" id="idm46219932810216"/>size of your
subnets, the placements of the nodes in those subnets, and of course the size of nodes will affect how large of a network
of pods and services you can run on the AWS network. Using a managed service such as EKS, with open source tools like
<code>eksctl</code>, will greatly reduce the operational overhead of running an AWS Kubernetes <a data-type="indexterm" data-startref="ch6_term37" id="idm46219932808472"/><a data-type="indexterm" data-startref="ch6_term38" id="idm46219932807768"/><a data-type="indexterm" data-startref="ch6_term39" id="idm46219932807096"/><a data-type="indexterm" data-startref="ch6_term40" id="idm46219932806424"/><a data-type="indexterm" data-primary="AWS ALB (application load balancer) ingress controller" data-secondary="example deployment of" id="ch6_term42"/><a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="with AWS EKS cluster example" data-secondary-sortas="AWS EKS cluster example" id="ch6_term43"/><a data-type="indexterm" data-primary="EKS (Amazon Elastic Kubernetes Service)" data-secondary="example deployment with" id="ch6_term44"/>cluster.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Deploying an Application on an AWS EKS Cluster"><div class="sect2" id="idm46219932801640">
<h2>Deploying an Application on an AWS EKS Cluster</h2>

<p>Let’s walk through deploying an EKS cluster to manage our Golang web server:</p>
<ol>
<li>
<p>Deploy the EKS cluster.</p>
</li>
<li>
<p>Deploy the web server Application and LoadBalancer.</p>
</li>
<li>
<p>Verify.</p>
</li>
<li>
<p>Deploy ALB Ingress Controller and Verify.</p>
</li>
<li>
<p>Clean up.</p>
</li>

</ol>










<section data-type="sect3" data-pdf-bookmark="Deploy EKS cluster"><div class="sect3" id="idm46219932795016">
<h3>Deploy EKS cluster</h3>

<p>Let’s deploy an EKS cluster, with the current and latest version EKS supports, 1.20:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nb">export </code><code class="nv">CLUSTER_NAME</code><code class="o">=</code>eks-demo
eksctl create cluster -N <code class="m">3</code> --name <code class="si">${</code><code class="nv">CLUSTER_NAME</code><code class="si">}</code> --version<code class="o">=</code>1.20
eksctl version 0.54.0
using region us-west-2
setting availability zones to <code class="o">[</code>us-west-2b us-west-2a us-west-2c<code class="o">]</code>
subnets <code class="k">for</code> us-west-2b - public:192.168.0.0/19 private:192.168.96.0/19
subnets <code class="k">for</code> us-west-2a - public:192.168.32.0/19 private:192.168.128.0/19
subnets <code class="k">for</code> us-west-2c - public:192.168.64.0/19 private:192.168.160.0/19
nodegroup <code class="s2">"ng-90b7a9a5"</code> will use <code class="s2">"ami-0a1abe779ecfc6a3e"</code> <code class="o">[</code>AmazonLinux2/1.20<code class="o">]</code>
using Kubernetes version 1.20
creating EKS cluster <code class="s2">"eks-demo"</code> in <code class="s2">"us-west-2"</code> region with un-managed nodes
will create <code class="m">2</code> separate CloudFormation stacks <code class="k">for</code> cluster itself and the initial
nodegroup
<code class="k">if</code> you encounter any issues, check CloudFormation console or try
<code class="s1">'eksctl utils describe-stacks --region=us-west-2 --cluster=eks-demo'</code>
CloudWatch logging will not be enabled <code class="k">for</code> cluster <code class="s2">"eks-demo"</code> in <code class="s2">"us-west-2"</code>
you can <code class="nb">enable </code>it with
<code class="s1">'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE</code>
<code class="s1">(e.g. all)} --region=us-west-2 --cluster=eks-demo'</code>
Kubernetes API endpoint access will use default of
<code class="o">{</code><code class="nv">publicAccess</code><code class="o">=</code><code class="nb">true</code>, <code class="nv">privateAccess</code><code class="o">=</code><code class="nb">false</code><code class="o">}</code> <code class="k">for</code> cluster <code class="s2">"eks-demo"</code> in <code class="s2">"us-west-2"</code>
<code class="m">2</code> sequential tasks: <code class="o">{</code> create cluster control plane <code class="s2">"eks-demo"</code>,
<code class="m">3</code> sequential sub-tasks: <code class="o">{</code> <code class="nb">wait </code><code class="k">for</code> control plane to become ready, <code class="m">1</code> task:
<code class="o">{</code> create addons <code class="o">}</code>, create nodegroup <code class="s2">"ng-90b7a9a5"</code> <code class="o">}</code> <code class="o">}</code>
building cluster stack <code class="s2">"eksctl-eks-demo-cluster"</code>
deploying stack <code class="s2">"eksctl-eks-demo-cluster"</code>
waiting <code class="k">for</code> CloudFormation stack <code class="s2">"eksctl-eks-demo-cluster"</code>
&lt;truncate&gt;
building nodegroup stack <code class="s2">"eksctl-eks-demo-nodegroup-ng-90b7a9a5"</code>
--nodes-min<code class="o">=</code><code class="m">3</code> was <code class="nb">set </code>automatically <code class="k">for</code> nodegroup ng-90b7a9a5
deploying stack <code class="s2">"eksctl-eks-demo-nodegroup-ng-90b7a9a5"</code>
waiting <code class="k">for</code> CloudFormation stack <code class="s2">"eksctl-eks-demo-nodegroup-ng-90b7a9a5"</code>
&lt;truncated&gt;
waiting <code class="k">for</code> the control plane availability...
saved kubeconfig as <code class="s2">"/Users/strongjz/.kube/config"</code>
no tasks
all EKS cluster resources <code class="k">for</code> <code class="s2">"eks-demo"</code> have been created
adding identity
<code class="s2">"arn:aws:iam::1234567890:role/</code>
<code class="s2">eksctl-eks-demo-nodegroup-ng-9-NodeInstanceRole-TLKVDDVTW2TZ"</code> to auth ConfigMap
nodegroup <code class="s2">"ng-90b7a9a5"</code> has <code class="m">0</code> node<code class="o">(</code>s<code class="o">)</code>
waiting <code class="k">for</code> at least <code class="m">3</code> node<code class="o">(</code>s<code class="o">)</code> to become ready in <code class="s2">"ng-90b7a9a5"</code>
nodegroup <code class="s2">"ng-90b7a9a5"</code> has <code class="m">3</code> node<code class="o">(</code>s<code class="o">)</code>
node <code class="s2">"ip-192-168-31-17.us-west-2.compute.internal"</code> is ready
node <code class="s2">"ip-192-168-58-247.us-west-2.compute.internal"</code> is ready
node <code class="s2">"ip-192-168-85-104.us-west-2.compute.internal"</code> is ready
kubectl <code class="nb">command </code>should work with <code class="s2">"/Users/strongjz/.kube/config"</code>,
try <code class="s1">'kubectl get nodes'</code>
EKS cluster <code class="s2">"eks-demo"</code> in <code class="s2">"us-west-2"</code> region is ready</pre>

<p>In the output we can see that EKS creating a nodegroup, <code>eksctl-eks-demo-nodegroup-ng-90b7a9a5</code>, with three nodes:</p>

<pre data-type="programlisting">ip-192-168-31-17.us-west-2.compute.internal
ip-192-168-58-247.us-west-2.compute.internal
ip-192-168-85-104.us-west-2.compute.internal</pre>

<p>They are all inside a VPC with <a data-type="indexterm" data-primary="AWS networking" data-secondary="subnets in" id="idm46219932787176"/><a data-type="indexterm" data-primary="private subnets" id="idm46219932678680"/><a data-type="indexterm" data-primary="public subnets" id="idm46219932678008"/><a data-type="indexterm" data-primary="subnets" data-secondary="in AWS" data-secondary-sortas="AWS" id="idm46219932677336"/><a data-type="indexterm" data-primary="subnets" data-secondary="public" id="idm46219932676120"/><a data-type="indexterm" data-primary="subnets" data-secondary="private" id="idm46219932675176"/>three public and three private subnets across three AZs:</p>

<pre data-type="programlisting">public:192.168.0.0/19 private:192.168.96.0/19
public:192.168.32.0/19 private:192.168.128.0/19
public:192.168.64.0/19 private:192.168.160.0/19</pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>We used the <a data-type="indexterm" data-primary="eksctl tool, AWS" id="idm46219932672280"/>default settings of eksctl, and it deployed the k8s API as a public endpoint, <code>{publicAccess=true,
privateAccess=false}</code>.</p>
</div>

<p>Now we can deploy our Golang web application in the cluster and expose it with a LoadBalancer service.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Deploy test application"><div class="sect3" id="idm46219932670184">
<h3>Deploy test application</h3>

<p>You can deploy <a data-type="indexterm" data-primary="database, Postgres" id="idm46219932668200"/><a data-type="indexterm" data-primary="Postgres database" id="idm46219932667464"/><a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="with AWS EKS deployment" data-secondary-sortas="AWS EKS deployment" id="ch6_term45"/>applications individually or all together. <em>dnsutils.yml</em> is our <code>dnsutils</code> testing pod, <em>database.yml</em> is the
Postgres database for pod connectivity testing, <em>web.yml</em> is the Golang web server and the LoadBalancer service:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f dnsutils.yml,database.yml,web.yml</pre>

<p>Let’s run a <code>kubectl get pods</code> to see if all the pods are running fine:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods -o wide
NAME                   READY   STATUS    IP               NODE
app-6bf97c555d-5mzfb   1/1     Running   192.168.15.108   ip-192-168-0-94
app-6bf97c555d-76fgm   1/1     Running   192.168.52.42    ip-192-168-63-151
app-6bf97c555d-gw4k9   1/1     Running   192.168.88.61    ip-192-168-91-46
dnsutils               1/1     Running   192.168.57.174   ip-192-168-63-151
postgres-0             1/1     Running   192.168.70.170   ip-192-168-91-46</pre>

<p>Now check on <a data-type="indexterm" data-primary="LoadBalancer services, Kubernetes" id="idm46219932656792"/>the LoadBalancer service:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">

kubectl get svc clusterip-service
NAME                TYPE           CLUSTER-IP
EXTERNAL-IP                                                              PORT<code class="o">(</code>S<code class="o">)</code>        AGE
clusterip-service   LoadBalancer   10.100.159.28
a76d1c69125e543e5b67c899f5e45284-593302470.us-west-2.elb.amazonaws.com   80:32671/TCP   29m
</pre>

<p>The service has <a data-type="indexterm" data-primary="endpoints, Kubernetes" data-secondary="with Amazon EKS" data-secondary-sortas="Amazon EKS" id="idm46219932643448"/>endpoints as well:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">
kubectl get endpoints clusterip-service
NAME                ENDPOINTS                                                   AGE
clusterip-service   192.168.15.108:8080,192.168.52.42:8080,192.168.88.61:8080   58m
</pre>

<p>We should <a data-type="indexterm" data-primary="ClusterIP service, Kubernetes" id="idm46219932645688"/><a data-type="indexterm" data-primary="clusterip-service command, AWS EKS" id="idm46219932645080"/>verify the application is reachable inside the cluster, with the ClusterIP and port, <code>10.100.159.28:8080</code>; service name and port, <code>clusterip-service:80</code>; and finally pod IP and port, <code>192.168.15.108:8080</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 10.100.159.28:80/data
Database Connected

kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 10.100.159.28:80/host
NODE: ip-192-168-63-151.us-west-2.compute.internal, POD IP:192.168.52.42

kubectl <code class="nb">exec </code>dnsutils -- wget -qO- clusterip-service:80/host
NODE: ip-192-168-91-46.us-west-2.compute.internal, POD IP:192.168.88.61

kubectl <code class="nb">exec </code>dnsutils -- wget -qO- clusterip-service:80/data
Database Connected

kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 192.168.15.108:8080/data
Database Connected

kubectl <code class="nb">exec </code>dnsutils -- wget -qO- 192.168.15.108:8080/host
NODE: ip-192-168-0-94.us-west-2.compute.internal, POD IP:192.168.15.108</pre>

<p>The database port is reachable from <code>dnsutils</code>, with the pod IP and port <code>192.168.70.170:5432</code>, and the service name and port - <code>postgres:5432</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>dnsutils -- nc -z -vv -w <code class="m">5</code> 192.168.70.170 5432
192.168.70.170 <code class="o">(</code>192.168.70.170:5432<code class="o">)</code> open
sent 0, rcvd 0

kubectl <code class="nb">exec </code>dnsutils -- nc -z -vv -w <code class="m">5</code> postgres 5432
postgres <code class="o">(</code>10.100.106.134:5432<code class="o">)</code> open
sent 0, rcvd 0</pre>

<p>The application inside the cluster is up and running. Let’s test it from external to the cluster.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Verify LoadBalancer services for Golang web server"><div class="sect3" id="idm46219932669528">
<h3>Verify LoadBalancer services for Golang web server</h3>

<p><code>kubectl</code> will return all the information we will need to test, the ClusterIP, the external IP, and all the ports:</p>
<pre class="small no-indent" data-type="programlisting" data-code-language="bash">

kubectl get svc clusterip-service
NAME                TYPE           CLUSTER-IP
EXTERNAL-IP                                                              PORT<code class="o">(</code>S<code class="o">)</code>        AGE
clusterip-service   LoadBalancer   10.100.159.28
a76d1c69125e543e5b67c899f5e45284-593302470.us-west-2.elb.amazonaws.com   80:32671/TCP   29m
</pre>

<p>Using the external IP of the load balancer:</p>

<pre data-type="programlisting" data-code-language="bash">wget -qO-
a76d1c69125e543e5b67c899f5e45284-593302470.us-west-2.elb.amazonaws.com/data
Database Connected</pre>

<p>Let’s test the <a data-type="indexterm" data-primary="backend destinations" data-secondary="requests to" id="idm46219932650216"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with AWS ALB" data-secondary-sortas="AWS ALB" id="idm46219932649368"/>load balancer and make multiple requests to our backends:</p>

<pre data-type="programlisting" data-code-language="bash">wget -qO-
a76d1c69125e543e5b67c899f5e45284-593302470.us-west-2.elb.amazonaws.com/host
NODE: ip-192-168-63-151.us-west-2.compute.internal, POD IP:192.168.52.42

wget -qO-
a76d1c69125e543e5b67c899f5e45284-593302470.us-west-2.elb.amazonaws.com/host
NODE: ip-192-168-91-46.us-west-2.compute.internal, POD IP:192.168.88.61

wget -qO-
a76d1c69125e543e5b67c899f5e45284-593302470.us-west-2.elb.amazonaws.com/host
NODE: ip-192-168-0-94.us-west-2.compute.internal, POD IP:192.168.15.108

wget -qO-
a76d1c69125e543e5b67c899f5e45284-593302470.us-west-2.elb.amazonaws.com/host
NODE: ip-192-168-0-94.us-west-2.compute.internal, POD IP:192.168.15.108</pre>

<p><code>kubectl get pods -o wide</code> again will <a data-type="indexterm" data-primary="pods" data-secondary="and AWS ALB" data-secondary-sortas="AWS ALB" id="idm46219932440920"/>verify our pod information matches the loadbalancer requests:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get pods -o wide
NAME                  READY  STATUS    IP              NODE
app-6bf97c555d-5mzfb  1/1    Running   192.168.15.108  ip-192-168-0-94
app-6bf97c555d-76fgm  1/1    Running   192.168.52.42   ip-192-168-63-151
app-6bf97c555d-gw4k9  1/1    Running   192.168.88.61   ip-192-168-91-46
dnsutils              1/1    Running   192.168.57.174  ip-192-168-63-151
postgres-0            1/1    Running   192.168.70.170  ip-192-168-91-46</pre>

<p>We can also <a data-type="indexterm" data-primary="nodeports, dnsutils for checking" id="idm46219934198920"/>check the nodeport, since <code>dnsutils</code> is running inside our VPC, on an EC2 instance; it can do a DNS lookup on
the private host, <code>ip-192-168-0-94.us-west-2.compute.internal</code>, and the <code>kubectl get service</code> command gave us the
node port, 32671:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl <code class="nb">exec </code>dnsutils -- wget -qO-
ip-192-168-0-94.us-west-2.compute.internal:32671/host
NODE: ip-192-168-0-94.us-west-2.compute.internal, POD IP:192.168.15.108</pre>

<p>Everything <a data-type="indexterm" data-startref="ch6_term45" id="idm46219932446856"/>seems to running just fine externally and locally in our cluster.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Deploy ALB ingress and verify"><div class="sect3" id="idm46219932484952">
<h3>Deploy ALB ingress and verify</h3>

<p>For some <a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="with AWS ALB" data-secondary-sortas="AWS ALB" id="idm46219932372680"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="controllers" id="idm46219932371432"/><a data-type="indexterm" data-primary="AWS Account ID" id="idm46219932370520"/>sections of the deployment, we will need to know the AWS account ID we are deploying. Let’s put that into
an environment variable. To get your account ID, you can run the following:</p>

<pre data-type="programlisting" data-code-language="bash">aws sts get-caller-identity
<code class="o">{</code>
    <code class="s2">"UserId"</code>: <code class="s2">"AIDA2RZMTHAQTEUI3Z537"</code>,
    <code class="s2">"Account"</code>: <code class="s2">"1234567890"</code>,
    <code class="s2">"Arn"</code>: <code class="s2">"arn:aws:iam::1234567890:user/eks"</code>
<code class="o">}</code>

<code class="nb">export </code><code class="nv">ACCOUNT_ID</code><code class="o">=</code>1234567890</pre>

<p>If it is not set up for the cluster already, we <a data-type="indexterm" data-primary="OIDC provider" id="idm46219932398840"/><a data-type="indexterm" data-primary="IAM permissions" id="idm46219932398232"/><a data-type="indexterm" data-primary="SA (ServiceAccount)" id="idm46219932397624"/><a data-type="indexterm" data-primary="ServiceAccount (SA)" id="idm46219932396952"/>will have to set up an OIDC provider with the cluster.</p>

<p>This step is needed to give IAM permissions to a pod running in the cluster using the IAM for SA:</p>

<pre data-type="programlisting" data-code-language="bash">eksctl utils associate-iam-oidc-provider <code class="se">\</code>
--region <code class="si">${</code><code class="nv">AWS_REGION</code><code class="si">}</code> <code class="se">\</code>
--cluster <code class="si">${</code><code class="nv">CLUSTER_NAME</code><code class="si">}</code>  <code class="se">\</code>
--approve</pre>

<p>For the SA role, we will need to create an IAM policy to determine the permissions for the ALB controller in AWS:</p>

<pre data-type="programlisting" data-code-language="bash">aws iam create-policy <code class="se">\</code>
--policy-name AWSLoadBalancerControllerIAMPolicy <code class="se">\</code>
--policy-document iam_policy.json</pre>

<p>Now we need to create the SA and attach it to the IAM role we created:</p>

<pre data-type="programlisting" data-code-language="bash">eksctl create iamserviceaccount <code class="se">\</code>
&gt; --cluster <code class="si">${</code><code class="nv">CLUSTER_NAME</code><code class="si">}</code> <code class="se">\</code>
&gt; --namespace kube-system <code class="se">\</code>
&gt; --name aws-load-balancer-controller <code class="se">\</code>
&gt; --attach-policy-arn
arn:aws:iam::<code class="si">${</code><code class="nv">ACCOUNT_ID</code><code class="si">}</code>:policy/AWSLoadBalancerControllerIAMPolicy <code class="se">\</code>
&gt; --override-existing-serviceaccounts <code class="se">\</code>
&gt; --approve
eksctl version 0.54.0
using region us-west-2
<code class="m">1</code> iamserviceaccount <code class="o">(</code>kube-system/aws-load-balancer-controller<code class="o">)</code> was included
<code class="o">(</code>based on the include/exclude rules<code class="o">)</code>
metadata of serviceaccounts that exist in Kubernetes will be updated,
as --override-existing-serviceaccounts was <code class="nb">set</code>
<code class="m">1</code> task: <code class="o">{</code> <code class="m">2</code> sequential sub-tasks: <code class="o">{</code> create IAM role <code class="k">for</code> serviceaccount
<code class="s2">"kube-system/aws-load-balancer-controller"</code>, create serviceaccount
<code class="s2">"kube-system/aws-load-balancer-controller"</code> <code class="o">}</code> <code class="o">}</code>
building iamserviceaccount stack
deploying stack
waiting <code class="k">for</code> CloudFormation stack
waiting <code class="k">for</code> CloudFormation stack
waiting <code class="k">for</code> CloudFormation stack
created serviceaccount <code class="s2">"kube-system/aws-load-balancer-controller"</code></pre>

<p>We can see all the details of the SA with the following:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl get sa aws-load-balancer-controller -n kube-system -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
annotations:
eks.amazonaws.com/role-arn:
arn:aws:iam::1234567890:role/eksctl-eks-demo-addon-iamserviceaccount-Role1
creationTimestamp: <code class="s2">"2021-06-27T18:40:06Z"</code>
labels:
app.kubernetes.io/managed-by: eksctl
name: aws-load-balancer-controller
namespace: kube-system
resourceVersion: <code class="s2">"16133"</code>
uid: 30281eb5-8edf-4840-bc94-f214c1102e4f
secrets:
- name: aws-load-balancer-controller-token-dtq48</pre>

<p>The <code>TargetGroupBinding</code> CRD <a data-type="indexterm" data-primary="CRDs (custom resource definitions)" id="idm46219932210712"/><a data-type="indexterm" data-primary="custom resource definitions (CRDs)" id="idm46219932210104"/><a data-type="indexterm" data-primary="TargetGroupBinding CRD" id="idm46219932209464"/><a data-type="indexterm" data-primary="targets/target groups, AWS" id="idm46219932208792"/>allows the controller to bind a Kubernetes
service endpoint to an AWS <code>TargetGroup</code>:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f crd.yml
customresourcedefinition.apiextensions.k8s.io/ingressclassparams.elbv2.k8s.aws
configured
customresourcedefinition.apiextensions.k8s.io/targetgroupbindings.elbv2.k8s.aws
configured</pre>

<p>Now we’re ready to the <a data-type="indexterm" data-primary="Helm deployment" id="idm46219932199784"/>deploy the ALB controller with Helm.</p>

<p>Set the version environment to deploy:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nb">export </code><code class="nv">ALB_LB_VERSION</code><code class="o">=</code><code class="s2">"v2.2.0"</code></pre>

<p>Now deploy it, add the <code>eks</code> Helm repo, <a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with AWS" data-secondary-sortas="AWS" id="idm46219932195704"/>get the VPC ID the cluster is running in, and finally deploy via Helm.</p>

<pre data-type="programlisting" data-code-language="bash">helm repo add eks https://aws.github.io/eks-charts

<code class="nb">export </code><code class="nv">VPC_ID</code><code class="o">=</code><code class="k">$(</code>aws eks describe-cluster <code class="se">\</code>
--name <code class="si">${</code><code class="nv">CLUSTER_NAME</code><code class="si">}</code> <code class="se">\</code>
--query <code class="s2">"cluster.resourcesVpcConfig.vpcId"</code> <code class="se">\</code>
--output text<code class="k">)</code>

helm upgrade -i aws-load-balancer-controller <code class="se">\</code>
eks/aws-load-balancer-controller <code class="se">\</code>
-n kube-system <code class="se">\</code>
--set <code class="nv">clusterName</code><code class="o">=</code><code class="si">${</code><code class="nv">CLUSTER_NAME</code><code class="si">}</code> <code class="se">\</code>
--set serviceAccount.create<code class="o">=</code><code class="nb">false</code> <code class="se">\</code>
--set serviceAccount.name<code class="o">=</code>aws-load-balancer-controller <code class="se">\</code>
--set image.tag<code class="o">=</code><code class="s2">"</code><code class="si">${</code><code class="nv">ALB_LB_VERSION</code><code class="si">}</code><code class="s2">"</code> <code class="se">\</code>
--set <code class="nv">region</code><code class="o">=</code><code class="si">${</code><code class="nv">AWS_REGION</code><code class="si">}</code> <code class="se">\</code>
--set <code class="nv">vpcId</code><code class="o">=</code><code class="si">${</code><code class="nv">VPC_ID</code><code class="si">}</code>

Release <code class="s2">"aws-load-balancer-controller"</code> has been upgraded. Happy Helming!
NAME: aws-load-balancer-controller
LAST DEPLOYED: Sun Jun <code class="m">27</code> 14:43:06 2021
NAMESPACE: kube-system
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
AWS Load Balancer controller installed!</pre>

<p>We can watch the deploy logs here:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl logs -n kube-system -f deploy/aws-load-balancer-controller</pre>

<p>Now to deploy our ingress with ALB:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl apply -f alb-rules.yml
ingress.networking.k8s.io/app configured</pre>

<p>With the <code>kubectl describe ing app</code> output, we can see the ALB has been deployed.</p>

<p>We can also see the ALB public DNS address, the rules for the instances, and the endpoints backing the service.</p>

<pre data-type="programlisting" data-code-language="bash">kubectl describe ing app
Name:             app
Namespace:        default
Address:
k8s-default-app-d5e5a26be4-2128411681.us-west-2.elb.amazonaws.com
Default backend:  default-http-backend:80
<code class="o">(</code>&lt;error: endpoints <code class="s2">"default-http-backend"</code> not found&gt;<code class="o">)</code>
Rules:
Host        Path  Backends
  ----        ----  --------
*
          /data   clusterip-service:80 <code class="o">(</code>192.168.3.221:8080,
192.168.44.165:8080,
192.168.89.224:8080<code class="o">)</code>
          /host   clusterip-service:80 <code class="o">(</code>192.168.3.221:8080,
192.168.44.165:8080,
192.168.89.224:8080<code class="o">)</code>
Annotations:  alb.ingress.kubernetes.io/scheme: internet-facing
kubernetes.io/ingress.class: alb
Events:
Type     Reason                  Age                     From
Message
----     ------                  ----                    ----
-------
Normal   SuccessfullyReconciled  4m33s <code class="o">(</code>x2 over 5m58s<code class="o">)</code>   ingress
Successfully reconciled</pre>

<p>It’s time to test our ALB!</p>

<pre data-type="programlisting" data-code-language="bash">wget -qO- k8s-default-app-d5e5a26be4-2128411681.us-west-2.elb.amazonaws.com/data
Database Connected

wget -qO- k8s-default-app-d5e5a26be4-2128411681.us-west-2.elb.amazonaws.com/host
NODE: ip-192-168-63-151.us-west-2.compute.internal, POD IP:192.168.44.165</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Cleanup"><div class="sect3" id="idm46219932445208">
<h3>Cleanup</h3>

<p>Once you are done working with EKS and testing, make sure to delete the applications pods and the service to ensure
that everything is deleted:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl delete -f dnsutils.yml,database.yml,web.yml</pre>

<p>Clean up the ALB:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl delete -f alb-rules.yml</pre>

<p>Remove the <a data-type="indexterm" data-primary="IAM permissions" id="idm46219932018632"/>IAM policy for ALB controller:</p>

<pre data-type="programlisting" data-code-language="bash">aws iam  delete-policy
--policy-arn arn:aws:iam::<code class="si">${</code><code class="nv">ACCOUNT_ID</code><code class="si">}</code>:policy/AWSLoadBalancerControllerIAMPolicy</pre>

<p>Verify there are no leftover EBS volumes from the PVCs for test application. Delete any EBS volumes found for the
PVC’s for the Postgres test database:</p>

<pre data-type="programlisting" data-code-language="bash">aws ec2 describe-volumes --filters
<code class="nv">Name</code><code class="o">=</code>tag:kubernetes.io/created-for/pv/name,Values<code class="o">=</code>*
--query <code class="s2">"Volumes[].{ID:VolumeId}"</code></pre>

<p>Verify there are no <a data-type="indexterm" data-primary="load balancing" data-secondary="with AWS ALB" data-secondary-sortas="AWS ALB" id="idm46219932013736"/>load balancers running, ALB or otherwise:</p>

<pre data-type="programlisting" data-code-language="bash">aws elbv2 describe-load-balancers --query <code class="s2">"LoadBalancers[].LoadBalancerArn"</code></pre>

<pre data-type="programlisting" data-code-language="bash">aws elb describe-load-balancers --query <code class="s2">"LoadBalancerDescriptions[].DNSName"</code></pre>

<p>Let’s make sure we delete the cluster, so you don’t get charged for a cluster doing nothing:</p>

<pre data-type="programlisting" data-code-language="bash">eksctl delete cluster --name <code class="si">${</code><code class="nv">CLUSTER_NAME</code><code class="si">}</code></pre>

<p>We deployed a service load balancer that will for each service deploy a classical ELB into AWS. The ALB controller allows developers to use ingress with ALB or NLBs to expose the application externally. If we <a data-type="indexterm" data-primary="backend destinations" data-secondary="ingress rules and" id="idm46219931965672"/>were to scale our application to multiple backend services, the ingress allows us to use one load balancer and route based on layer 7
information.</p>

<p>In the next <a data-type="indexterm" data-startref="ch6_term23" id="idm46219931839768"/><a data-type="indexterm" data-startref="ch6_term41" id="idm46219931839064"/><a data-type="indexterm" data-startref="ch6_term42" id="idm46219931838392"/><a data-type="indexterm" data-startref="ch6_term43" id="idm46219931837720"/><a data-type="indexterm" data-startref="ch6_term44" id="idm46219931889384"/>section, we will explore GCP in the same manner we just did for AWS.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Google Compute Cloud (GCP)"><div class="sect1" id="idm46219931952248">
<h1>Google Compute Cloud (GCP)</h1>

<p>In 2008, Google announced <a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="with Google Compute Cloud (GCP)" data-secondary-sortas="Google Compute Cloud (GCP)" id="ch6_term49"/><a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" id="ch6_term50"/>App Engine, a platform as a service to deploy Java, Python, Ruby, and Go applications.
Like its competitors, GCP has extended its service offerings. Cloud providers work to distinguish their
offerings, so no two products are ever the same. Nonetheless, many products do have a lot in common. For instance,
GCP Compute Engine is an infrastructure as a service to run virtual machines. The GCP network consists of 25 cloud
regions, 76 zones, and 144 network edge locations. Utilizing both the scale of the GCP network and Compute
Engine, GCP has released Google Kubernetes Engine, its container as a service platform.</p>








<section data-type="sect2" data-pdf-bookmark="GCP Network Services"><div class="sect2" id="idm46219931817320">
<h2>GCP Network Services</h2>

<p>Managed and unmanaged Kubernetes clusters on GCP share the same networking principles. <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="network services in" id="ch6_term48"/><a data-type="indexterm" data-primary="GCE (Google Compute Engine) instances" id="idm46219931797800"/><a data-type="indexterm" data-primary="Google Compute Engine (GCE) instances" id="idm46219931797112"/>Nodes in either managed or
unmanaged clusters run as Google Compute Engine instances. <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="VPCs in" id="ch6_term46"/><a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with GCP" data-secondary-sortas="GCP" id="ch6_term47"/>Networks in GCP are VPC networks. GCP VPC networks, like in AWS, contain functionality for IP management, routing, firewalling, and peering.</p>

<p>The GCP network <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="network tiers in" id="idm46219931862680"/><a data-type="indexterm" data-primary="network engineers" id="idm46219931861656"/><a data-type="indexterm" data-primary="performance, improving" id="idm46219931860984"/>is divided into tiers for customers to choose from; there are premium and standard tiers. They differ in
performance, routing, and functionality, so network engineers must decide which is suitable for their workloads. The
premium tier is the highest performance for your workloads. All the traffic between the internet and instances in
the VPC network is routed within Google’s network as far as possible. If your services need global availability, you
should use premium. Make sure to remember that the premium tier is the default unless you make configuration changes.</p>

<p>The standard tier is a cost-optimized tier where traffic between the internet and VMs in the VPC network is routed over
the internet in general. Network engineers should pick this tier for services that are going to be hosted entirely
within a region. The standard tier cannot guarantee performance as it is subject to the same performance that all
workloads share on the internet.</p>

<p>The GCP network <a data-type="indexterm" data-primary="global resources, GCP" id="idm46219931858392"/>differs from the other providers by having what is called <em>global</em> resources. Global because users can
access them in any zone within the same project. These resources include such things as VPC, firewalls, and their
routes.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>See the <a href="https://oreil.ly/mzgG2">GCP documentation</a> for a more comprehensive overview of the network tiers.</p>
</div>










<section data-type="sect3" data-pdf-bookmark="Regions and zones"><div class="sect3" id="idm46219931777176">
<h3>Regions and zones</h3>

<p>Regions are <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="regions in" id="ch6_term51"/><a data-type="indexterm" data-primary="regions in VPC networking" data-secondary="with GCP" data-secondary-sortas="GCP" id="ch6_term54"/>independent geographic areas that contain multiple zones. Regional resources offer redundancy by being
deployed across multiple zones for that region. Zones are deployment areas for resources within a region. One zone is
typically a data center within a region, and administrators should consider them a single fault domain. In
fault-tolerant application deployments, the best practice is to deploy applications across multiple zones within a
region, and for high availability, you should deploy applications across various regions. If a zone becomes
unavailable, all the zone resources will be unavailable  until owners restore services.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Virtual private cloud"><div class="sect3" id="idm46219931772072">
<h3>Virtual private cloud</h3>

<p>A VPC is a virtual network that provides connectivity for resources within a GCP project. Like accounts and
subscriptions, projects can contain multiple VPC networks, and by default, new projects start with a default
auto-mode VPC network that also includes one subnet in each region. Custom-mode VPC networks can contain no subnets.
As stated earlier, VPC networks are global resources and are not associated with any particular region or zone.</p>

<p>A VPC network <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="subnets in" id="ch6_term52"/><a data-type="indexterm" data-primary="subnets" data-secondary="in GCP" data-secondary-sortas="GCP" id="ch6_term53"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Google GCP" data-secondary-sortas="Google GCP" id="ch6_term55"/>contains one or more regional subnets. <a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="for Google GCP" data-secondary-sortas="Google GCP" id="idm46219931765304"/>Subnets have a region, CIDR, and globally unique name. You can
use any CIDR for a subnet, including one that overlaps with another private address space. The specific choice of
subnet CIDR impacts which IP addresses you can reach and which networks you can peer.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Google creates a “default” VPC network,
with randomly generated subnets for each region.
Some <a data-type="indexterm" data-primary="peering, VPC network" id="idm46219931762504"/>subnets may overlap with another VPC’s subnet
(such as the default VPC network in another Google Cloud project),
which will prevent peering.</p>
</div>

<p>VPC networks support peering and shared VPC configuration. Peering a VPC network allows the VPC in one project to
route to the VPC in another, placing them on the same L3 network. You cannot peer with any overlapping VPC network,
as some IP addresses exist in both networks. A shared VPC allows another project to use specific subnets, such as
creating machines that are part of that subnet. The  <a href="https://oreil.ly/98Wav">VPC documentation</a> has more information.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Peering VPC networks is standard, as organizations often assign different teams, applications, or components to their
project in Google Cloud. Peering has upsides for access control, quota, and reporting. Some admins may also create
multiple VPC networks within a project for similar reasons.</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Subnet"><div class="sect3" id="idm46219931758296">
<h3>Subnet</h3>

<p>Subnets are portions within a VPC network with one primary IP range with the ability to have zero or more
secondary ranges. Subnets are regional resources, and each subnet defines a range of IP addresses. A region can have
more than one subnet. There are two modes of subnet formulation when you create them: auto or custom. When you create
an auto-mode VPC network, one subnet from each region is automatically created within it using predefined IP ranges.
When you define a custom-mode VPC network, GCP does not provision any subnets, giving administrators control over the
ranges. Custom-mode VPC networks are suited for enterprises and production environments for network engineers to use.</p>

<p>Google Cloud <a data-type="indexterm" data-primary="routing" data-secondary="for external traffic" data-secondary-sortas="external traffic" id="idm46219931755848"/><a data-type="indexterm" data-primary="routing" data-secondary="for internal traffic" data-secondary-sortas="internal traffic" id="idm46219931754568"/><a data-type="indexterm" data-primary="routing" data-secondary="in cloud networking" data-secondary-sortas="cloud networking" id="idm46219931753352"/>allows you to “reserve” static IP addresses for internal and external IP addresses. Users can utilize
reserved IP addresses for GCE instances, load balancers, and other products beyond our scope. Reserved internal IP
addresses have a name and can be generated automatically or assigned manually. Reserving an internal static IP
address prevents it from being randomly automatically assigned while not in use.</p>

<p>Reserving external IP addresses is similar; although you can request an automatically assigned IP address, you
cannot choose what IP address to reserve. Because you are reserving a globally routable IP address, charges apply in
some circumstances. You cannot secure an external IP address that you were assigned automatically as an ephemeral IP
<a data-type="indexterm" data-startref="ch6_term51" id="idm46219931750824"/><a data-type="indexterm" data-startref="ch6_term52" id="idm46219931750120"/><a data-type="indexterm" data-startref="ch6_term53" id="idm46219931749448"/><a data-type="indexterm" data-startref="ch6_term54" id="idm46219931748776"/><a data-type="indexterm" data-startref="ch6_term55" id="idm46219931748104"/>address.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Routes and firewall rules"><div class="sect3" id="idm46219931747048">
<h3>Routes and firewall rules</h3>

<p>When deploying a VPC, you <a data-type="indexterm" data-primary="firewalls" data-secondary="rules with GCP/GKE for" id="idm46219931745704"/>can use firewall rules to allow or deny connections to and from your application instances
based on the rules you deploy. Each firewall rule can apply to ingress or egress connections, but not both. The
instance level is where GCP enforces rules, but the configuration pairs with the VPC network, and you cannot share
firewall rules among VPC networks, peered networks included. VPC firewall rules are stateful, so when a TCP session
starts, firewall rules allow bidirectional traffic similar to an AWS <a data-type="indexterm" data-startref="ch6_term46" id="idm46219931744088"/><a data-type="indexterm" data-startref="ch6_term47" id="idm46219931743416"/>security group.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Cloud load balancing"><div class="sect3" id="idm46219931742360">
<h3>Cloud load balancing</h3>

<p>Google Cloud Load Balancer (GCLB) <a data-type="indexterm" data-primary="GCLB (Google Cloud Load Balancer)" id="ch6_term56"/><a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="load balancing in" id="ch6_term57"/><a data-type="indexterm" data-primary="Google Cloud Load Balancer (GCLB)" id="ch6_term58"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with GCLB/GKE" data-secondary-sortas="GCLB/GKE" id="ch6_term59"/>offers a fully distributed, high-performance, scalable load balancing service
across GCP, with various load balancer options. With GCLB, you get a single Anycast IP that fronts all your backend
instances across the globe, including multiregion failover. In addition, software-defined load balancing services
enable you to apply load balancing to your HTTP(S), TCP/SSL, and UDP traffic. You can also terminate your SSL traffic
with an SSL proxy and HTTPS load balancing. Internal load balancing enables you to build highly available internal
services for your internal instances without requiring any load balancers to be exposed to the internet.</p>

<p>The vast majority of <a data-type="indexterm" data-primary="LoadBalancer services, Kubernetes" id="idm46219931734920"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="with GCP" data-secondary-sortas="GCP" id="idm46219931734168"/>GCP users make use of GCP’s load balancers with Kubernetes ingress. GCP has internal-facing and
external-facing load balancers, with L4 and L7 support. GKE clusters default to creating a GCP load balancer for
ingresses and <code>type: LoadBalancer</code> services.</p>

<p>To expose applications outside a GKE cluster, GKE provides a built-in GKE ingress controller and GKE service
controller, which deploys a Google Cloud load balancer on behalf of GKE users. GKE provides three different load
balancers to control access and spread incoming traffic across your cluster as evenly as possible. You can configure
one service to use multiple types of load balancers simultaneously:</p>
<dl>
<dt>External load balancers</dt>
<dd>
<p>Manage traffic <a data-type="indexterm" data-primary="routing" data-secondary="for external traffic" data-secondary-sortas="external traffic" id="idm46219931729592"/><a data-type="indexterm" data-primary="routing" data-secondary="in cloud networking" data-secondary-sortas="cloud networking" id="idm46219931728312"/><a data-type="indexterm" data-primary="forwarding" data-secondary="load balancers and" id="idm46219931727096"/><a data-type="indexterm" data-primary="load balancing" data-secondary="external" id="idm46219931726152"/>from outside the cluster and outside the VPC network. External load balancers use forwarding
rules associated with the Google Cloud network to route traffic to a Kubernetes node.</p>
</dd>
<dt>Internal load balancers</dt>
<dd>
<p>Manage <a data-type="indexterm" data-primary="routing" data-secondary="for internal traffic" data-secondary-sortas="internal traffic" id="idm46219931723608"/><a data-type="indexterm" data-primary="load balancing" data-secondary="internal" id="idm46219931722328"/>traffic coming from within the same VPC network. Like external load balancers,
internal ones use forwarding rules associated with the Google Cloud network to route traffic to a Kubernetes node.</p>
</dd>
<dt>HTTP load balancers</dt>
<dd>
<p>Specialized <a data-type="indexterm" data-primary="HTTP" data-secondary="load balancing" id="idm46219931719768"/><a data-type="indexterm" data-primary="load balancing" data-secondary="HTTP" id="idm46219931718760"/>external load balancers used for HTTP traffic. They use an ingress resource rather
than a forwarding rule to route traffic to a Kubernetes node.</p>
</dd>
</dl>

<p>When you create an ingress object, the GKE ingress controller configures a Google Cloud HTTP(S) load balancer
according to the ingress manifest and the associated Kubernetes service rules manifest. The client sends a request to
the load balancer. The <a data-type="indexterm" data-primary="NodePort services, Kubernetes" id="idm46219931716744"/><a data-type="indexterm" data-primary="nodes" data-secondary="in GCP/GKE" data-secondary-sortas="GCP/GKE" id="ch6_term60"/>load balancer is a proxy; it chooses a node and forwards the request to that node’s
NodeIP:NodePort combination. The node uses its <code>iptables</code> NAT table to select a pod. As we learned in earlier chapters,
<code>kube-proxy</code> manages the <code>iptables</code> rules on that node.</p>

<p>When an ingress <a data-type="indexterm" data-primary="load balancing" data-secondary="ingress for" id="idm46219931712616"/><a data-type="indexterm" data-primary="pods" data-secondary="and GCP/GKE/GCE" data-secondary-sortas="GCP/GKE/GCE" id="idm46219931711608"/>creates a load balancer, the load balancer is “pod aware” instead of routing to all nodes (and
relying on the service to route requests to a pod), and the load balancer routes to individual pods.
It does this by tracking the underlying <code>Endpoints</code>/<code>EndpointSlice</code> object (as covered in <a data-type="xref" href="ch05.xhtml#kubernetes_networking_abstractions">Chapter 5</a>) and using individual
pod IP addresses as target addresses.</p>

<p>Cluster administrators <a data-type="indexterm" data-primary="ingress-Nginx" id="idm46219931707960"/>can use an in-cluster ingress provider, such as ingress-Nginx or Contour. A load
balancer points to applicable nodes running the ingress proxy in such a setup, which routes requests to the
applicable pods from there. This setup is cheaper for clusters that have many ingresses but incurs performance overhead.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="GCE instances"><div class="sect3" id="idm46219931706648">
<h3>GCE instances</h3>

<p>GCE instances <a data-type="indexterm" data-primary="GCE (Google Compute Engine) instances" id="idm46219931705288"/><a data-type="indexterm" data-primary="Google Compute Engine (GCE) instances" id="idm46219931704680"/><a data-type="indexterm" data-primary="interfaces, network" id="idm46219931704024"/><a data-type="indexterm" data-primary="network interfaces" id="idm46219931703352"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Google GCP" data-secondary-sortas="Google GCP" id="idm46219931702680"/>have one or more network interfaces. A network interface has a network and subnetwork, a private IP
address, and a public IP address. The private IP address must be part of the subnetwork. Private IP addresses can
be automatic and ephemeral, custom and ephemeral, or static. External IP addresses can be automatic and ephemeral, or
static. You can add more network interfaces to a GCE instance. Additional network interfaces don’t need to be in the
same VPC network. For example, you may have an instance that bridges two VPCs with varying levels of security. <a data-type="indexterm" data-startref="ch6_term48" id="idm46219931700744"/><a data-type="indexterm" data-startref="ch6_term56" id="idm46219931700072"/><a data-type="indexterm" data-startref="ch6_term57" id="idm46219931699400"/><a data-type="indexterm" data-startref="ch6_term58" id="idm46219931698728"/><a data-type="indexterm" data-startref="ch6_term59" id="idm46219931698056"/>Let’s
discuss how GKE uses these instances and manages the network services that empower GKE.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="GKE"><div class="sect2" id="idm46219931816728">
<h2>GKE</h2>

<p>Google Kubernetes <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="Kubernetes Engine (GKE) in" id="ch6_term61"/><a data-type="indexterm" data-primary="GKE (Google Kubernetes Engine)" id="ch6_term62"/><a data-type="indexterm" data-primary="Google Kubernetes Engine (GKE)" id="ch6_term63"/>Engine (GKE) is Google’s managed Kubernetes service. GKE runs a hidden control plane, which cannot
be directly viewed or accessed. You can only access specific control plane configurations and the Kubernetes API.</p>

<p>GKE exposes broad cluster config around things like machine types and cluster scaling. It <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="maximum pods and" id="idm46219931691496"/><a data-type="indexterm" data-primary="pods" data-secondary="maximum number of" id="idm46219931690552"/><a data-type="indexterm" data-primary="pods" data-secondary="and GCP/GKE/GCE" data-secondary-sortas="GCP/GKE/GCE" id="idm46219931689608"/>reveals only some
network-related settings. At the time of writing, NetworkPolicy support (via Calico), max pods per node (<code>maxPods</code> in
the kubelet, <code>--node-CIDR-mask-size</code> in <code>kube-controller-manager</code>), and the pod address range (<code>--cluster-CIDR</code> in
<code>kube-controller-manager</code>) are the customizable options. It is not possible to directly set
<code>apiserver/kube-controller-manager</code> flags.</p>

<p>GKE supports public and private clusters. Private clusters don’t issue public IP addresses to nodes, which means
nodes are accessible only within your private network. Private clusters also allow you to restrict access to the
Kubernetes API to specific IP addresses. <a data-type="indexterm" data-primary="node pools" id="idm46219931684888"/>GKE runs worker nodes using automatically managed GCE instances by creating
creates <em>node pools</em>.</p>










<section data-type="sect3" data-pdf-bookmark="GCP GKE nodes"><div class="sect3" id="idm46219931683416">
<h3>GCP GKE nodes</h3>

<p>Networking for GKE nodes is comparable to networking for self-managed Kubernetes clusters on GKE. GKE clusters
define <em>node pools,</em> which are a set of nodes with an identical configuration. This configuration contains
GCE-specific settings as well as general Kubernetes settings. Node pools define (virtual) machine type, autoscaling,
and the GCE service account. You can also set custom taints and labels per node pool.</p>

<p>A cluster <a data-type="indexterm" data-primary="cluster networking" data-secondary="in cloud" data-secondary-sortas="cloud" id="idm46219931680136"/><a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="VPCs in" id="idm46219931678472"/><a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with GCP" data-secondary-sortas="GCP" id="idm46219931677560"/>exists on exactly one VPC network. Individual nodes can have their 
<span class="keep-together">network</span> tags for crafting specific
firewall rules. Any GKE cluster running 1.16 or later will have a <code>kube-proxy</code> DaemonSet so that all new nodes in the
cluster will automatically have the <code>kube-proxy</code> start. The <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="subnets in" id="idm46219931674520"/><a data-type="indexterm" data-primary="subnets" data-secondary="cluster size and size of" id="idm46219931673496"/><a data-type="indexterm" data-primary="subnets" data-secondary="in GCP" data-secondary-sortas="GCP" id="idm46219931672536"/><a data-type="indexterm" data-primary="subnet mask (netmask)" id="idm46219931671320"/><a data-type="indexterm" data-primary="netmask (subnet mask)" id="idm46219931670648"/>size of the subnet allows will affect the size of the
cluster. So, pay attention to the size of that when you deploy clusters that scale. There is a formula you can use to
calculate the maximum number of nodes, <code>N</code>, that a given netmask can support. Use <code>S</code> for the netmask size, whose valid
range is between 8 and 29:</p>
<pre>N = 2(32 -S) - 4</pre>

<p>Calculate the size of the netmask, <code>S</code>, required to support a maximum of <code>N</code> nodes:</p>
<pre>S = 32 - ⌈log2(N + 4)⌉</pre>

<p><a data-type="xref" href="#cluster_node_scale_with_subnet_size">Table 6-2</a> also outlines cluster node and how it scales with subnet size.</p>
<table id="cluster_node_scale_with_subnet_size">
<caption><span class="label">Table 6-2. </span>Cluster node scale with subnet size</caption>
<thead>
<tr>
<th>Subnet primary IP range</th>
<th>Maximum nodes</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>/29</p></td>
<td><p>Minimum size for a subnet’s primary IP range: 4 nodes</p></td>
</tr>
<tr>
<td><p>/28</p></td>
<td><p>12 nodes</p></td>
</tr>
<tr>
<td><p>/27</p></td>
<td><p>28 nodes</p></td>
</tr>
<tr>
<td><p>/26</p></td>
<td><p>60 nodes</p></td>
</tr>
<tr>
<td><p>/25</p></td>
<td><p>124 nodes</p></td>
</tr>
<tr>
<td><p>/24</p></td>
<td><p>252 nodes</p></td>
</tr>
<tr>
<td><p>/23</p></td>
<td><p>508 nodes</p></td>
</tr>
<tr>
<td><p>/22</p></td>
<td><p>1,020 nodes</p></td>
</tr>
<tr>
<td><p>/21</p></td>
<td><p>2,044 nodes</p></td>
</tr>
<tr>
<td><p>/20</p></td>
<td><p>The default size of a subnet’s primary IP range in auto mode networks: 4,092 nodes</p></td>
</tr>
<tr>
<td><p>/19</p></td>
<td><p>8,188 nodes</p></td>
</tr>
<tr>
<td><p>/8</p></td>
<td><p>Maximum size for a subnet’s primary IP range: 16,777,212 nodes</p></td>
</tr>
</tbody>
</table>

<p>If you<a data-type="indexterm" data-startref="ch6_term60" id="idm46219931642136"/> use GKE’s CNI, one end of the veth pair is attached to the pod in its namespace and connects the other side to the Linux bridge device cbr0.1, exactly how we outlined it in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.xhtml#linux_networking">2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.xhtml#container_networking_basics">3</a>.</p>

<p>Clusters span <a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="regions in" id="idm46219931637944"/><a data-type="indexterm" data-primary="regions in VPC networking" data-secondary="with GCP" data-secondary-sortas="GCP" id="idm46219931636904"/>either the zone or region boundary; zonal clusters have only a single control plane. Regional
clusters have multiple replicas of the control plane. Also, when you deploy clusters, there are <a data-type="indexterm" data-primary="routes-based cluster" id="idm46219931635352"/><a data-type="indexterm" data-primary="cluster modes, GKE" id="ch6_term64"/><a data-type="indexterm" data-primary="container-native cluster, GKE" id="ch6_term65"/><a data-type="indexterm" data-primary="VPC-native cluster, GKE" id="ch6_term66"/>two cluster modes with
GKE: VPC-native and routes based. A cluster that uses alias IP address ranges is considered a VPC-native cluster. A cluster that uses custom
static routes in a VPC network is called a <em>routes-based cluster</em>. <a data-type="xref" href="#cluster_mode_with_cluster_creation_method">Table 6-3</a> outlines how the creation method
maps with the cluster mode.</p>
<table id="cluster_mode_with_cluster_creation_method">
<caption><span class="label">Table 6-3. </span>Cluster mode with cluster creation method</caption>
<thead>
<tr>
<th>Cluster creation method</th>
<th>Cluster network mode</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Google Cloud Console</p></td>
<td><p>VPC-native</p></td>
</tr>
<tr>
<td><p>REST API</p></td>
<td><p>Routes-based</p></td>
</tr>
<tr>
<td><p>gcloud v256.0.0 and higher or v250.0.0 and lower</p></td>
<td><p>Routes-based</p></td>
</tr>
<tr>
<td><p>gcloud v251.0.0–255.0.0</p></td>
<td><p>VPC-native</p></td>
</tr>
</tbody>
</table>

<p>When using VPC-native, administrators can also <a data-type="indexterm" data-primary="NEG (network endpoint groups)" id="idm46219931619832"/><a data-type="indexterm" data-primary="network endpoint groups (NEG)" id="idm46219931619112"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with NEG" data-secondary-sortas="NEG" id="idm46219931618424"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with GCLB/GKE" data-secondary-sortas="GCLB/GKE" id="idm46219931617208"/>take advantage of network endpoint groups (NEG), which represent a
group of backends served by a load balancer. NEGs are lists of IP addresses managed by an NEG controller and are used
by Google Cloud load balancers. IP addresses in an NEG can be primary or secondary IP addresses of a VM, which means
they can be pod IPs. This <a data-type="indexterm" data-primary="pods" data-secondary="and GCP/GKE/GCE" data-secondary-sortas="GCP/GKE/GCE" id="idm46219931615512"/>enables container-native load balancing that sends traffic directly to pods from a Google
Cloud load balancer.</p>

<p>VPC-native clusters have several benefits:</p>

<ul>
<li>
<p>Pod IP addresses are natively routable inside the cluster’s VPC network.</p>
</li>
<li>
<p>Pod IP addresses are reserved in network before pod creation.</p>
</li>
<li>
<p>Pod IP address ranges are dependent on custom static routes.</p>
</li>
<li>
<p>Firewall rules <a data-type="indexterm" data-primary="firewalls" data-secondary="rules with GCP/GKE for" id="idm46219931609560"/><a data-type="indexterm" data-primary="GCP (Google Compute Cloud)" data-secondary="firewall rules in" id="idm46219931608552"/>apply to just pod IP address ranges instead of any IP address on the cluster’s nodes.</p>
</li>
<li>
<p>GCP cloud network connectivity to on-premise extends to pod IP address ranges.</p>
</li>
</ul>

<p><a data-type="xref" href="#neg">Figure 6-15</a> shows the mapping of GKE to GCE components.</p>

<figure><div id="neg" class="figure">
<img src="Images/neku_0615.png" alt="NEG" width="947" height="418"/>
<h6><span class="label">Figure 6-15. </span>NEG to GCE components</h6>
</div></figure>

<p>Here is a list of improvements that NEGs bring to the GKE network:</p>
<dl>
<dt>Improved network performance</dt>
<dd>
<p>The container-native <a data-type="indexterm" data-primary="performance, improving" id="idm46219931601208"/>load balancer talks directly with the pods, and connections have fewer network hops; both
latency and throughput are improved.</p>
</dd>
<dt>Increased visibility</dt>
<dd>
<p>With <a data-type="indexterm" data-primary="HTTP" data-secondary="load balancing" id="idm46219931598920"/><a data-type="indexterm" data-primary="load balancing" data-secondary="HTTP" id="idm46219931597912"/><a data-type="indexterm" data-primary="latency, routing" id="idm46219931596968"/><a data-type="indexterm" data-primary="routing" data-secondary="latency in" id="idm46219931596296"/>container-native load balancing, you have visibility into the latency from the HTTP
load balancer to the pods. The latency from the HTTP load balancer to each pod is visible, which was aggregated with
node IP-based container-native load balancing. This increased visibility makes troubleshooting your services at the NEG
level easier.</p>
</dd>
<dt>Support for advanced load balancing</dt>
<dd>
<p>Container-native load balancing offers native support in GKE for several HTTP load-balancing
features, such as integration with Google Cloud services like Google Cloud Armor, Cloud CDN, and Identity-Aware Proxy.
It also features load-balancing algorithms for accurate traffic distribution.</p>
</dd>
</dl>

<p>Like most managed Kubernetes offerings from major providers,
GKE is tightly integrated with Google Cloud offerings.
Although much of the software driving GKE is opaque, it <a data-type="indexterm" data-primary="GCE (Google Compute Engine) instances" id="idm46219931592616"/>uses standard resources such as GCE instances
that can be inspected and debugged like any other GCP resources.
If you really need to manage your own clusters, you will lose out on some functionality,
such as container-aware load balancing.</p>

<p>It’s worth noting that GCP does not yet support IPv6, unlike AWS and Azure.</p>

<p>Finally, <a data-type="indexterm" data-startref="ch6_term49" id="idm46219931590728"/><a data-type="indexterm" data-startref="ch6_term50" id="idm46219931589992"/><a data-type="indexterm" data-startref="ch6_term61" id="idm46219931589320"/><a data-type="indexterm" data-startref="ch6_term62" id="idm46219931588648"/><a data-type="indexterm" data-startref="ch6_term63" id="idm46219931587976"/><a data-type="indexterm" data-startref="ch6_term64" id="idm46219931587304"/><a data-type="indexterm" data-startref="ch6_term65" id="idm46219931586632"/><a data-type="indexterm" data-startref="ch6_term66" id="idm46219931585960"/>we’ll look at Kubernetes networking on Azure.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Azure"><div class="sect1" id="idm46219931682312">
<h1>Azure</h1>

<p>Microsoft Azure, <a data-type="indexterm" data-primary="Azure" data-secondary="overview of" id="ch6_term67"/><a data-type="indexterm" data-primary="Azure" id="ch6_term68"/>like other cloud providers, offers an assortment of enterprise-ready network solutions and services.
Before we can discuss how Azure AKS networking works, <a data-type="indexterm" data-primary="Azure" data-secondary="deployment models for" id="idm46219931581368"/>we should discuss Azure deployment models. Azure has gone
through some significant iterations and improvements over the years, resulting in two different deployment models
that can encounter Azure. These models differ in how resources are deployed and managed and may impact how users
leverage the resources.</p>

<p>The first deployment model was the classic deployment model. This model was the initial deployment and management
method for Azure.
All resources existed independently of each other, and you could not logically group them. This was cumbersome; users
had to create, update, and delete each component of a solution, leading to errors, missed resources, and additional
time, effort, and cost. Finally, these resources could not even be tagged for easy searching, adding to the
difficulty of the solution.</p>

<p>In 2014, Microsoft <a data-type="indexterm" data-primary="ARM (Azure Resource Manager)" id="idm46219931578664"/><a data-type="indexterm" data-primary="Azure Resource Manager (ARM)" id="idm46219931577960"/><a data-type="indexterm" data-primary="Resource Manager deployment, Azure" id="idm46219931577272"/>introduced the Azure Resource Manager as the second model. This new model is the recommended model
from Microsoft, with the recommendation going so far as to say that you should redeploy your resources using the
Azure Resource Manager (ARM). The primary change with this model was the introduction of the resource group. Resource
groups are a logical grouping of resources that allows for tracking, tagging, and configuring the resources as a group
rather than 
<span class="keep-together">individually.</span></p>

<p>Now that we understand the basics of how resources are deployed and managed in Azure, we can discuss the Azure network service
offerings and how they interact with the Azure Kubernetes Service (AKS) and non-Azure Kubernetes offerings.</p>








<section data-type="sect2" data-pdf-bookmark="Azure Networking Services"><div class="sect2" id="idm46219931574648">
<h2>Azure Networking Services</h2>

<p>The core of <a data-type="indexterm" data-primary="Azure" data-secondary="Vnet networking in" id="ch6_term73"/><a data-type="indexterm" data-primary="Azure networking services (Vnet)" id="ch6_term74"/>Azure networking services is the virtual network, also known as an Azure Vnet. The Vnet establishes an
isolated virtual network infrastructure to connect your deployed Azure resources such as virtual machines and AKS
clusters. Through additional resources, Vnets connect your deployed resources to the public internet as well as your
on-premise infrastructure. Unless the configuration is changed, all Azure Vnets can communicate with the internet
through a default route.</p>

<p>In <a data-type="xref" href="#Vnet">Figure 6-16</a>, an Azure Vnet <a data-type="indexterm" data-primary="CIDR (Classless Inter-Domain Routing) ranges" data-secondary="for Azure Vnet" data-secondary-sortas="Azure Vnet" id="idm46219931568840"/>has a single CIDR of <code>192.168.0.0/16</code>. Vnets, like other Azure resources, require a
subscription to place the Vnet into a resource group for the Vnet. The security of the Vnet can be configured
while some options, such as IAM permissions, are inherited from the resource group and the subscription. The Vnet is
confined to a specified region. Multiple Vnets can exist within a single region, but a Vnet can exist within only one region.</p>

<figure><div id="Vnet" class="figure">
<img src="Images/neku_0616.png" alt="Vnet" width="951" height="422"/>
<h6><span class="label">Figure 6-16. </span>Azure Vnet</h6>
</div></figure>










<section data-type="sect3" data-pdf-bookmark="Azure backbone infrastructure"><div class="sect3" id="idm46219931564440">
<h3>Azure backbone infrastructure</h3>

<p>Microsoft Azure <a data-type="indexterm" data-startref="ch6_term67" id="idm46219931562936"/><a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="regions of" id="ch6_term69"/><a data-type="indexterm" data-primary="regions in VPC networking" data-secondary="with Azure" data-secondary-sortas="Azure" id="ch6_term70"/>leverages a globally dispersed network of data centers and zones. The foundation of this dispersal is
the Azure region, which comprises a set of data centers within a latency-defined area, connected by a low-latency,
dedicated network infrastructure. A region can contain any number of data centers that meet these criteria, but two
to three are often present per region. Any <a data-type="indexterm" data-primary="Azure Geography" id="idm46219931558888"/>area of the world containing at least one Azure region is known as Azure geography.</p>

<p>Availability <a data-type="indexterm" data-primary="availability zones (AZs)" id="ch6_term71"/><a data-type="indexterm" data-primary="AZs (availability zones)" id="ch6_term72"/>zones further divide a region. Availability zones are physical locations that can consist of one or more
data centers maintained by independent power, 
<span class="keep-together">cooling,</span> and networking infrastructure. The relationship of a region to
its availability zones is architected so that a single availability zone failure cannot bring down an entire region
of services. Each availability zone in a region is connected to the other availability zones in the region but not
dependent on the different zones. Availability zones allow Azure to offer 99.99% uptime for supported services. A
region can consist of multiple availability zones, as shown in <a data-type="xref" href="#Region">Figure 6-17</a>, which can, in turn, consist of numerous data
centers.</p>

<figure><div id="Region" class="figure">
<img src="Images/neku_0617.png" alt="Region" width="857" height="618"/>
<h6><span class="label">Figure 6-17. </span>Region</h6>
</div></figure>

<p>Since a Vnet is within a region and regions are divided into availability zones, Vnets are also available across the
availability zones of the region they are deployed. As shown in <a data-type="xref" href="#VnetWithAzs">Figure 6-18</a>, it is a best practice when deploying infrastructure for high
availability to leverage multiple availability zones for redundancy. Availability zones allow Azure to offer 99.99%
uptime for supported services. Azure allows for the use of load balancers for networking across redundant systems
such as these.</p>

<figure><div id="VnetWithAzs" class="figure">
<img src="Images/neku_0618.png" alt="Vnet With availability zones" width="1082" height="487"/>
<h6><span class="label">Figure 6-18. </span>Vnet with availability zones</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The Azure <a href="https://oreil.ly/Pv0iq">documentation</a> has an up-to-date list of Azure geographies, regions, and availability zones.</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Subnets"><div class="sect3" id="idm46219931545576">
<h3>Subnets</h3>

<p>Resource IPs are <a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="subnets in" id="idm46219931544248"/><a data-type="indexterm" data-primary="subnets" data-secondary="in Azure" data-secondary-sortas="Azure" id="idm46219931543224"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Azure" data-secondary-sortas="Azure" id="idm46219931542008"/>not assigned directly from the Vnet. Instead, subnets divide and define a Vnet. The subnets receive
their address space from the Vnet. Then, private IPs are allocated to provisioned resources within each subnet. This
is where the IP addressing AKS clusters and pods will come. Like Vnets, Azure subnets span availability zones, as depicted in <a data-type="xref" href="#SubnetsAcrossAzs">Figure 6-19</a>.</p>

<figure><div id="SubnetsAcrossAzs" class="figure">
<img src="Images/neku_0619.png" alt="Subnets Across availability zones" width="1082" height="654"/>
<h6><span class="label">Figure 6-19. </span>Subnets across availability zones</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Route tables"><div class="sect3" id="idm46219931537160">
<h3>Route tables</h3>

<p>As mentioned <a data-type="indexterm" data-startref="ch6_term69" id="idm46219931535496"/><a data-type="indexterm" data-startref="ch6_term70" id="idm46219931534760"/><a data-type="indexterm" data-startref="ch6_term71" id="idm46219931534088"/><a data-type="indexterm" data-startref="ch6_term72" id="idm46219931533416"/>in previous sections, a route table governs subnet communication or an array of directions on where to send network traffic. Each newly provisioned subnet comes equipped with a default route table populated with some default system routes. This route cannot be deleted or changed. The system routes include a route to the Vnet the subnet is defined within, routes for <code>10.0.0.0/8</code> and <code>192.168.0.0/16</code> that are by default set to go nowhere, and most importantly a default route to the
internet. The default route to the internet allows any newly provisioned resource with an Azure IP to communicate out
to the internet by default. This default route is an essential difference between Azure and some other cloud service
providers and requires adequate security measures to protect each Azure Vnet.</p>

<p><a data-type="xref" href="#Route_Table">Figure 6-20</a> shows a <a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="route tables in" id="ch6_term75"/><a data-type="indexterm" data-primary="route tables" id="ch6_term76"/><a data-type="indexterm" data-primary="routing" data-secondary="in cloud networking" data-secondary-sortas="cloud networking" id="ch6_term77"/>standard route table for a newly provisioned AKS setup. There are routes for the agent pools with
their CIDRs as well as their next-hop IP. The next-hop IP is the route the table has defined for the path, and the
next-hop type is set for a virtual appliance, which would be the load balancer in this case. What is not present are
those default system routes. The default routes are still in the configuration, just not viewable in the route table.
Understanding <a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="security with" id="idm46219931525512"/><a data-type="indexterm" data-primary="security" data-secondary="for Azure" data-secondary-sortas="Azure" id="idm46219931524552"/>Azure’s default networking behavior is critical from a security perspective and from troubleshooting and
planning 
<span class="keep-together">perspectives.</span></p>

<figure><div id="Route_Table" class="figure">
<img src="Images/neku_0620.png" alt="AKS Route Table" width="2820" height="753"/>
<h6><span class="label">Figure 6-20. </span>Route table</h6>
</div></figure>

<p>Some system routes, known as optional default routes, <a data-type="indexterm" data-primary="peering, VPC network" id="idm46219931520072"/>affect only if the capabilities, such as Vnet peering, are
enabled. Vnet peering allows Vnets anywhere globally to establish a private connection across the Azure global
infrastructure backbone to communicate.</p>

<p>Custom routes <a data-type="indexterm" data-primary="custom routes, Azure" id="idm46219931518632"/><a data-type="indexterm" data-primary="user-defined routes, Azure" id="idm46219931517896"/>can also populate route tables, which the Border Gateway Protocol either creates if leveraged or uses
user-defined routes. User-defined routes are essential because they allow the network administrators to define routes
beyond what Azure establishes by default, such as proxies or firewall routes. Custom routes also impact the system
default routes. While you cannot alter the default routes, a customer route with a higher priority can overrule it.
An example of this is to use a user-defined route to send traffic bound for the internet to a next-hop of a virtual
firewall appliance rather than the internet directly. <a data-type="xref" href="#Route_Table_with_Custom_Route">Figure 6-21</a> defines a custom route called Google with a
next-hop type of internet. As long as the priorities are set up correctly, this custom route will send that traffic
out the default system route for the internet, even if another rule redirects the remaining internet traffic.</p>

<figure><div id="Route_Table_with_Custom_Route" class="figure">
<img src="Images/neku_0621.png" alt="AKS Route Table with Custom Route" width="2820" height="786"/>
<h6><span class="label">Figure 6-21. </span>Route table with custom route</h6>
</div></figure>

<p>Route tables can <a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="subnets in" id="idm46219931512952"/><a data-type="indexterm" data-primary="subnets" data-secondary="in Azure" data-secondary-sortas="Azure" id="idm46219931511880"/>also be created on their own and then used to configure a subnet. This is useful for maintaining a
single route table for multiple subnets, especially when there are many user-defined routes involved. A subnet can
have only one route table associated with it, but a route table can be associated with multiple subnets. The rules of
configuring a user-created route table and a route table created as part of the subnet’s default creation are the same. They have the same default system routes and will update with the same optional default routes as they come into
effect.</p>

<p>While most routes within a route table will use an IP range as the source address, <a data-type="indexterm" data-primary="service tags, Azure" id="idm46219931509560"/>Azure has begun to introduce the concept
of using service tags for sources. A service tag is a phrase that represents a collection of service IPs within the
Azure backend, such as SQL.EastUs, which is a service tag that describes the IP address range for the Microsoft SQL
Platform service offering in the eastern US. With this feature, it could be possible to define a route from one Azure
service, such as AzureDevOps, as the source, and another service, such as Azure AppService, as the destination
without knowing the IP ranges for <a data-type="indexterm" data-startref="ch6_term75" id="idm46219931508168"/><a data-type="indexterm" data-startref="ch6_term76" id="idm46219931507496"/><a data-type="indexterm" data-startref="ch6_term77" id="idm46219931506824"/>either.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <a href="https://oreil.ly/CDedn">Azure documentation</a> has a list of available service tags.</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Public and private IPs"><div class="sect3" id="idm46219931536536">
<h3>Public and private IPs</h3>

<p>Azure allocates <a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="public/private IPs in" id="idm46219931502808"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Azure" data-secondary-sortas="Azure" id="ch6_term78"/>IP addresses as independent resources themselves, which means that a user can create a public IP or
private IP without attaching it to anything. These IP addresses can be named and built in a resource group that
allows for future allocation. This is a <a data-type="indexterm" data-primary="Kubernetes" data-secondary="scaling with" id="idm46219931499832"/><a data-type="indexterm" data-primary="scaling" id="idm46219931498888"/>crucial step when preparing for AKS cluster scaling as you want to make sure
that enough private IP addresses have been reserved for the possible pods if you decide to leverage Azure CNI for networking.
Azure CNI will be discussed in a later section.</p>

<p>IP address resources, both public and private, are also defined as either dynamic or static. A static IP address is reserved
to not change, while a dynamic IP address can change if it is not allocated to a resource, such as a virtual machine or AKS pod.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Network security groups"><div class="sect3" id="idm46219931496904">
<h3>Network security groups</h3>

<p>NSGs are used to <a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="security with" id="ch6_term79"/><a data-type="indexterm" data-primary="filtering" data-secondary="with NSGs" id="ch6_term80"/><a data-type="indexterm" data-primary="network security groups (NSGs)" id="ch6_term81"/><a data-type="indexterm" data-primary="NSGs (network security groups)" id="ch6_term82"/>configure Vnets, subnets, and network interface cards (NICs) with inbound and outbound security rules. The rules filter traffic and determine whether the traffic will be allowed to proceed or be dropped. NSG rules are flexible to filter traffic based on source and destination IP addresses, network
ports, and network protocols. An NSG rule can use one or multiple of these filter items and can apply many NSGs.</p>

<p>An NSG rule can have any of the following components to define its filtering:</p>
<dl>
<dt>Priority</dt>
<dd>
<p>This is a number between 100 and 4096. The lowest numbers are evaluated first, and the first match is the
rule that is used. Once a match is found, no further rules are evaluated.</p>
</dd>
<dt>Source/destination</dt>
<dd>
<p>Source (inbound rules) or destination (outbound rules) of the traffic inspected. The
source/destination can be any of the following:</p>

<ul>
<li>
<p>Individual IP address</p>
</li>
<li>
<p>CIDR block (i.e., 10.2.0.0/24)</p>
</li>
<li>
<p>Microsoft Azure service tag</p>
</li>
<li>
<p>Application security groups</p>
</li>
</ul>
</dd>
<dt>Protocol</dt>
<dd>
<p>TCP, UDP, ICMP, ESP, AH, or Any.</p>
</dd>
<dt>Direction</dt>
<dd>
<p>The rule for inbound or outbound traffic.</p>
</dd>
<dt>Port range</dt>
<dd>
<p>Single ports or ranges can be specified here.</p>
</dd>
<dt>Action</dt>
<dd>
<p>Allow or deny the traffic.</p>
</dd>
</dl>

<p><a data-type="xref" href="#Azure_NSG">Figure 6-22</a> shows an example of an NSG.</p>

<figure><div id="Azure_NSG" class="figure">
<img src="Images/neku_0622.png" alt="Azure NSG" width="2282" height="682"/>
<h6><span class="label">Figure 6-22. </span>Azure NSG</h6>
</div></figure>

<p>There are some considerations to keep in mind when configuring Azure network security groups. First, two or more
rules cannot exist with the same priority and direction. The priority or direction can match as long as the other
does not. Second, <a data-type="indexterm" data-primary="ARM (Azure Resource Manager)" id="idm46219931473112"/><a data-type="indexterm" data-primary="Azure Resource Manager (ARM)" id="idm46219931472344"/><a data-type="indexterm" data-primary="Resource Manager deployment, Azure" id="idm46219931471656"/><a data-type="indexterm" data-primary="Azure" data-secondary="deployment models for" id="idm46219931470968"/>port ranges can be used only in the Resource Manager deployment model, not the classic deployment
model. This limitation also applies to IP address ranges and service tags for the source/destination. Third, when
specifying the IP address for an Azure resource as the source/destination, if the resource has both a public and
private IP address, use the private IP address. Azure performs the translation from public to private IP addressing
outside this process, and the private IP address will be the right choice at the time of processing.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Communication outside the virtual network"><div class="sect3" id="idm46219931469208">
<h3>Communication outside the virtual network</h3>

<p>The concepts described so far have mainly pertained to Azure networking within a single Vnet. This type of
communication is vital in Azure networking but far from the only type. <a data-type="indexterm" data-primary="Azure" data-secondary="communications external to" id="idm46219931467544"/><a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="communications external to" id="idm46219931466552"/>Most Azure implementations will require
communication outside the virtual network to other networks, including, but not limited to, on-premise networks,
other Azure virtual networks, and the internet. These communication paths require many of the same considerations as
the internal networking processes and use many of the same resources, with a few differences. This section will expand
on some of those differences.</p>

<p>Vnet <a data-type="indexterm" data-primary="peering, VPC network" id="idm46219931464632"/><a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="regions of" id="idm46219931463896"/><a data-type="indexterm" data-primary="regions in VPC networking" data-secondary="with Azure" data-secondary-sortas="Azure" id="idm46219931462936"/>peering can connect Vnets in different regions using global virtual network peering, but there are constraints
with certain services such as load balancers.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For a list of these constraints, see the Azure <a href="https://oreil.ly/wnaEi">documentation</a>.</p>
</div>

<p>Communication outside of Azure to the internet uses a different set of resources. <a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="public/private IPs in" id="idm46219931459096"/><a data-type="indexterm" data-primary="routing" data-secondary="for internal traffic" data-secondary-sortas="internal traffic" id="idm46219931458104"/><a data-type="indexterm" data-primary="routing" data-secondary="for external traffic" data-secondary-sortas="external traffic" id="idm46219931456888"/>Public IPs, as discussed earlier,
can be created and assigned to a resource in Azure. The resource uses its private IP address for all networking
internal to Azure. When the traffic from the resource needs to exit the internal networks to the internet, Azure
translates the private IP address into the resource’s assigned public IP. At this point, the traffic can leave to the
internet. Incoming traffic bound for the public IP address of an Azure resource translates to the resource’s assigned
private IP address at the Vnet boundary, and the private IP is used from then on for the rest of the traffic’s trip
to its destination. This traffic path is why all subnet rules for things like NSGs are defined using private IP <a data-type="indexterm" data-startref="ch6_term79" id="idm46219931454792"/><a data-type="indexterm" data-startref="ch6_term80" id="idm46219931454120"/><a data-type="indexterm" data-startref="ch6_term81" id="idm46219931453448"/><a data-type="indexterm" data-startref="ch6_term82" id="idm46219931452776"/>addresses.</p>

<p>NAT can <a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="with Azure" data-secondary-sortas="Azure" id="idm46219931451560"/><a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="subnets in" id="idm46219931450264"/><a data-type="indexterm" data-primary="subnets" data-secondary="in Azure" data-secondary-sortas="Azure" id="idm46219931449304"/>also be configured on a subnet. If configured, resources on a subnet with NAT
enabled do not need a public IP address to communicate with the internet. NAT is enabled on a subnet to allow
outbound-only internet traffic with a public IP from a pool of provisioned public IP addresses. NAT will enable
resources to route traffic to the internet for requests such as updates or installs and return with the requested
traffic but prevents the resources from being accessible on the internet. It is important to note that, when
configured, NAT takes priority over all other outbound rules and replaces the default internet destination for the
subnet. NAT also uses port address <a data-type="indexterm" data-startref="ch6_term73" id="idm46219931447272"/><a data-type="indexterm" data-startref="ch6_term74" id="idm46219931446600"/><a data-type="indexterm" data-startref="ch6_term78" id="idm46219931445928"/>translation (PAT) by default.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Azure load balancer"><div class="sect3" id="idm46219931445000">
<h3>Azure load balancer</h3>

<p>Now that you have a method of communicating outside the network and communication to flow back into the Vnet, a
way to keep those lines of communication available is needed. <a data-type="indexterm" data-primary="Azure" data-secondary="load balancing in" id="ch6_term84"/><a data-type="indexterm" data-primary="load balancing" data-secondary="with Azure" data-secondary-sortas="Azure" id="ch6_term85"/>Azure load balancers are often used to accomplish this
by distributing traffic across backend pools of resources rather than a single resource to handle the request. There
are two primary load balancer types in Azure: the standard load balancer and the application gateway.</p>

<p>Azure standard <a data-type="indexterm" data-primary="standard load balancers, Azure" id="idm46219931439640"/><a data-type="indexterm" data-primary="Transport layer, TCP/IP (L4)" data-secondary="load balancing and" id="idm46219931438888"/><a data-type="indexterm" data-primary="NSGs (network security groups)" id="idm46219931437928"/><a data-type="indexterm" data-primary="network security groups (NSGs)" id="idm46219931437240"/>load balancers are layer 4 systems that distribute incoming traffic based on layer 4 protocols
such as TCP and UDP, meaning traffic is routed based on IP address and port. These load balancers filter incoming
traffic from the internet, but they can also load balance traffic from one Azure resource to a set of other Azure
resources. The standard load balancer uses a zero-trust network model. This model requires an NSG to “open” traffic
to be inspected by the load balancer. If the attached NSG does not permit the traffic, the load balancer will not
attempt to route it.</p>

<p>Azure application <a data-type="indexterm" data-primary="AGIC (application gateway ingress controller), Azure" id="idm46219931435496"/><a data-type="indexterm" data-primary="application gateway ingress controller (AGIC), Azure" id="idm46219931434760"/><a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="with HTTP" data-secondary-sortas="HTTP" id="idm46219931434008"/><a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="load balancers for" id="idm46219931432792"/><a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="application gateway ingress controller in" id="idm46219931431848"/><a data-type="indexterm" data-primary="host headers (URI)" id="idm46219931430856"/><a data-type="indexterm" data-primary="URI (host headers)" id="idm46219931430184"/><a data-type="indexterm" data-primary="HTTP" data-secondary="requests, filtering of" id="idm46219931429512"/>gateways are similar to standard load balancers in that they distribute incoming traffic but
differently in that they do so at layer 7. This allows for the inspection of incoming HTTP requests to filter based on
URI or host headers. Application gateways can also be used as web application firewalls to further secure and filter
traffic. Additionally, the application gateway can also be used as the ingress controller for AKS clusters.</p>

<p>Load balancers, whether standard or application gateways, have some basic concepts that sound be considered:</p>
<dl>
<dt>Frontend IP address</dt>
<dd>
<p>Either <a data-type="indexterm" data-primary="frontend IP address" id="idm46219931425752"/>public or private depending on the use, this is the IP address used to target the load
balancer and, by extension, the backend resources it is balancing.</p>
</dd>
<dt>SKU</dt>
<dd>
<p>Like other Azure <a data-type="indexterm" data-primary="SKU, Azure load balancing" id="idm46219931423432"/>resources, this defines the “type” of the load balancer and, therefore, the different
configuration options available.</p>
</dd>
<dt>Backend pool</dt>
<dd>
<p>This is the <a data-type="indexterm" data-primary="backend destinations" data-secondary="for load balancing" data-secondary-sortas="load balancing" id="ch6_term83"/>collection of resources that the load balancer is distributing traffic to, such as a
collection
of virtual machines or the pods within an AKS cluster.</p>
</dd>
</dl>
<dl class="less_space pagebreak-before">
<dt>Health probes</dt>
<dd>
<div class="openblock">
<p>These are methods <a data-type="indexterm" data-primary="health checks/probes" data-secondary="for load balancers" data-secondary-sortas="load balancers" id="idm46219931416712"/>used by the load balancer to ensure the backend resource is available for traffic,
such as a health endpoint that returns an OK status:</p>
<dl>
<dt>Listener</dt>
<dd>
<p>A configuration that tells the load balancer what type of traffic to expect, such as HTTP requests.</p>
</dd>
<dt>Rules</dt>
<dd>
<p>Determines how to route the incoming traffic for that listener.</p>
</dd>
</dl>
</div>

</dd>
</dl>

<p><a data-type="xref" href="#Azure_Load_Balancer_Components">Figure 6-23</a> illustrates some of these primary components within the Azure load balancer architecture. Traffic comes
into the load balancer and is compared to the listeners to determine if the load balancer balances the traffic. Then
the traffic is evaluated against the rules and finally sent on to the backend pool. Backend pool
resources with appropriately responding health probes will process the traffic.</p>

<figure><div id="Azure_Load_Balancer_Components" class="figure">
<img src="Images/neku_0623.png" alt="Azure Load Balancer Components" width="1370" height="737"/>
<h6><span class="label">Figure 6-23. </span>Azure load balancer components</h6>
</div></figure>

<p><a data-type="xref" href="#AKS_Load_Balancing">Figure 6-24</a> shows how AKS would use the load balancer.</p>

<p>Now that <a data-type="indexterm" data-startref="ch6_term83" id="idm46219931406328"/><a data-type="indexterm" data-startref="ch6_term84" id="idm46219931405592"/><a data-type="indexterm" data-startref="ch6_term85" id="idm46219931404920"/>we have a basic knowledge of the Azure network, we can discuss how Azure uses these constructs in its
managed Kubernetes offering, Azure Kubernetes Service.</p>

<figure><div id="AKS_Load_Balancing" class="figure">
<img src="Images/neku_0624.png" alt="AKS Load Balancing" width="896" height="571"/>
<h6><span class="label">Figure 6-24. </span>AKS load balancing</h6>
</div></figure>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Azure Kubernetes Service"><div class="sect2" id="idm46219931574152">
<h2>Azure Kubernetes Service</h2>

<p>Like other cloud providers, <a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="overview of" id="ch6_term86"/><a data-type="indexterm" data-primary="Azure" data-secondary="AKS (Azure Kubernetes Service) in" id="ch6_term87"/><a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" id="ch6_term88"/>Microsoft understood the need to leverage the power of Kubernetes and therefore
introduced the Azure Kubernetes Service as the Azure Kubernetes offering. AKS is a hosted service offering from Azure and therefore handles a large portion of the overhead of
managing Kubernetes. Azure handles components such as health monitoring and maintenance, leaving more time for
development and operations engineers to leverage the scalability and power of Kubernetes for their solutions.</p>

<p>AKS can have <a data-type="indexterm" data-primary="cluster networking" data-secondary="in cloud" data-secondary-sortas="cloud" id="idm46219931396024"/>clusters created and managed using the Azure CLI, Azure PowerShell, the Azure Portal, and other
template-based deployment options such as ARM templates and HashiCorp’s Terraform. With AKS, Azure manages the
Kubernetes masters so that the user only needs to handle the node agents. This allows Azure to offer the core of AKS
as a free service where the only payment required is for the agent nodes and peripheral services such as storage and
networking.</p>

<p>The Azure Portal allows for easy management and configuration of the AKS environment. <a data-type="xref" href="#Azure_Portal_AKS_Overview">Figure 6-25</a> shows the overview
page of a newly provisioned AKS environment. On this page, you can see information and links to many of the crucial
integrations and properties. The cluster’s resource group, DNS address, Kubernetes version, networking type, and a
link to the node pools are visible in the Essentials section.</p>

<p><a data-type="xref" href="#Azure_Portal_AKS_Properties">Figure 6-26</a> zooms in on the Properties section of the overview page, where users can find additional information and links to corresponding components. Most of the data is the same as the information in the Essentials section.
However, the various subnet CIDRs for the AKS environment components can be viewed here for things such as the Docker
bridge and the pod subnet.</p>

<figure><div id="Azure_Portal_AKS_Overview" class="figure">
<img src="Images/neku_0625.png" alt="Azure Portal AKS Overview" width="2206" height="1087"/>
<h6><span class="label">Figure 6-25. </span>Azure Portal AKS overview</h6>
</div></figure>

<figure><div id="Azure_Portal_AKS_Properties" class="figure">
<img src="Images/neku_0626.png" alt="Azure Portal AKS Properties" width="2039" height="751"/>
<h6><span class="label">Figure 6-26. </span>Azure Portal AKS properties</h6>
</div></figure>

<p>Kubernetes <a data-type="indexterm" data-primary="pods" data-secondary="and Azure/AKS" data-secondary-sortas="Azure/AKS" id="ch6_term90"/>pods created within AKS are attached to virtual networks and can access network resources through
abstraction. The kube-proxy on each AKS node creates this abstraction, and this component allows for inbound and
outbound traffic. Additionally, AKS seeks to make Kubernetes management even more streamlined by simplifying how to
roll changes to virtual network changes. Network services in AKS are autoconfigured when specific changes occur. For
example, opening a network port to a pod will also trigger relevant changes to the attached NSGs to open those ports.</p>

<p>By default, <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="with Azure" data-secondary-sortas="Azure" id="ch6_term89"/>AKS will create an Azure DNS record that has a public IP. However, the default network rules prevent
public access. The private mode can create the cluster to use no public IPs and block public access for only
internal use of the cluster. This mode will cause the cluster access to be available only from within the Vnet. By
default, the standard SKU will create an AKS load balancer. This configuration can be changed during deployment if
deploying via the CLI. Resources not included in the cluster are made in a separate, auto-generated resource group.</p>

<p>When leveraging the <a data-type="indexterm" data-primary="kubenet networking model" id="idm46219931381320"/>kubenet networking model for AKS, the following <a data-type="indexterm" data-primary="nodes" data-secondary="in AKS" data-secondary-sortas="AKS" id="idm46219931380376"/><a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="with Azure" data-secondary-sortas="Azure" id="idm46219931379160"/>rules are true:</p>

<ul>
<li>
<p>Nodes receive an IP address from the Azure virtual network subnet.</p>
</li>
<li>
<p>Pods receive an IP address from a logically different address space than the nodes.</p>
</li>
<li>
<p>The source IP address of the traffic switches to the node’s primary address.</p>
</li>
<li>
<p>NAT is configured for the pods to reach Azure resources on the Vnet.</p>
</li>
</ul>

<p>It is important to note that only the nodes receive a routable IP; the pods do not.</p>

<p>While kubenet is an easy way to administer Kubernetes networking within the Azure Kubernetes Service, it is not
the only way. <a data-type="indexterm" data-startref="ch6_term86" id="idm46219931372760"/>Like other cloud providers, Azure also allows for the use the CNI when managing Kubernetes infrastructure.
Let’s discuss CNI in the next section.</p>










<section data-type="sect3" data-pdf-bookmark="Azure CNI"><div class="sect3" id="idm46219931371640">
<h3>Azure CNI</h3>

<p>Microsoft <a data-type="indexterm" data-primary="Azure" data-secondary="CNI for" id="idm46219931370168"/><a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="CNI for" id="idm46219931369160"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="for Azure" data-secondary-sortas="Azure" id="idm46219931368200"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="maximum pods and" id="idm46219931366968"/><a data-type="indexterm" data-primary="pods" data-secondary="maximum number of" id="idm46219931366008"/><a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="CNIs for" id="idm46219931365064"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="cloud networking with" id="idm46219931364104"/>has provided its own CNI plugin for Azure and AKS, Azure CNI. The first
significant difference between this and kubenet is that the pods receive routable IP information and can be accessed
directly. This difference places additional importance on the need for IP address space planning. Each node has a
maximum number of pods it can use, and many IP addresses are reserved for that use.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>More information can be found on the Azure
Container Networking <a href="https://oreil.ly/G2zyC">GitHub</a>.</p>
</div>

<p>With Azure CNI, traffic inside the Vnet is no longer NAT’d to the node’s IP address but to the pod’s IP itself, as
illustrated in <a data-type="xref" href="#Azure_CNI">Figure 6-27</a>. Outside traffic, such as to the internet, is still NAT’d to the node’s IP address. Azure
CNI still performs the backend IP address management and routing for these items, though, as all resources on the
same Azure Vnet can communicate with each other by default.</p>

<p>The Azure CNI <a data-type="indexterm" data-startref="ch6_term89" id="idm46219931358536"/>can also be used for Kubernetes deployments outside AKS. While there is additional work to be done on
the cluster that Azure would typically handle, this allows you to leverage Azure networking and other resources while
maintaining more control over the customarily managed aspects of Kubernetes under AKS.</p>

<figure><div id="Azure_CNI" class="figure">
<img src="Images/neku_0627.png" alt="Azure CNI" width="755" height="759"/>
<h6><span class="label">Figure 6-27. </span>Azure CNI</h6>
</div></figure>

<p>Azure CNI also provides the added benefit of allowing for the separation of duties while maintaining the AKS
infrastructure. The Azure CNI creates the networking resources in a separate resource group. Being in a different
resource group allows for more control over permissions at the resource group level within the Azure Resource
Management deployment model. Different teams can access some components of AKS, such as the networking, without
needing access to others, such as the application deployments.</p>

<p>Azure CNI is not the only way to leverage additional Azure services to enhance your Kubernetes network infrastructure.
The next section will discuss the use of an Azure application gateway as a means of controlling ingress into your
Kubernetes cluster.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Application gateway ingress controller"><div class="sect3" id="idm46219931353624">
<h3>Application gateway ingress controller</h3>

<p>Azure allows <a data-type="indexterm" data-primary="AGIC (application gateway ingress controller), Azure" id="idm46219931352152"/><a data-type="indexterm" data-primary="application gateway ingress controller (AGIC), Azure" id="idm46219931351320"/><a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="application gateway ingress controller in" id="idm46219931350616"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="with Azure AKS" data-secondary-sortas="Azure AKS" id="idm46219931349624"/><a data-type="indexterm" data-primary="ingress, Kubernetes-specific" data-secondary="controllers" id="idm46219931348392"/>for the deployment of an application gateway inside the AKS cluster deployment to serve as the application gateway ingress controller (AGIC). This deployment model eliminates the need for maintaining a secondary
load balancer outside the AKS infrastructure, thereby reducing maintenance overhead and error points. AGIC deploys
its pods in the cluster. It then monitors other aspects of the cluster for configuration changes. When a change is
detected, AGIC updates the Azure Resource Manager template that configures the load balancer and
then applies the updated configuration. <a data-type="xref" href="#Azure_AGIC">Figure 6-28</a> illustrates this.</p>

<figure><div id="Azure_AGIC" class="figure">
<img src="Images/neku_0628.png" alt="Azure Application Gateway Ingress Controller" width="1390" height="696"/>
<h6><span class="label">Figure 6-28. </span>Azure AGIC</h6>
</div></figure>

<p>There are <a data-type="indexterm" data-startref="ch6_term90" id="idm46219931343464"/>AKS SKU limitations for the use of the AGIC, only supporting Standard_v2 and WAF_v2, but those SKUs also
have autoscaling capabilities. Use cases for using such a form of ingress, such as the need for <a data-type="indexterm" data-primary="Kubernetes" data-secondary="scaling with" id="idm46219931342376"/><a data-type="indexterm" data-primary="scaling" id="idm46219931341432"/>high scalability,
have the potential for the AKS environment to scale. Microsoft supports the use of both <a data-type="indexterm" data-primary="Helm deployment" id="idm46219931340504"/>Helm and the AKS add-on as
deployment options for the AGIC. These are the critical differences between the two options:</p>

<ul>
<li>
<p>Helm deployment values cannot be edited when using the AKS add-on.</p>
</li>
<li>
<p>Helm supports Prohibited Target configuration. An AGIC can configure the application gateway to target only the AKS
instances without impacting other backend components.</p>
</li>
<li>
<p>The AKS add-on, as a managed service, will be automatically updated to its current and more secure versions. Helm
deployments will need manual updating.</p>
</li>
</ul>

<p>Even though <a data-type="indexterm" data-primary="Application layer (L7)" data-secondary="ingress and" id="idm46219931335704"/>AGIC is configured as the Kubernetes ingress resource, it still carries the full benefit of the cluster’s
standard layer 7 application gateway. Application gateway services such as TLS termination, URL routing, and the
web application firewall capability are all configurable for the cluster as part of the AGIC.</p>

<p>While many Kubernetes and networking fundamentals are universal across cloud providers, Azure offers its own spin on Kubernetes
networking through its enterprise-focused resource design and management. Whether you have a need for a single cluster using basic
settings and kubenet or a large-scale deployment with advanced networking through the use of deployed load balancers and application
gateways, Microsoft’s Azure Kubernetes Service can be leveraged to deliver a reliable, managed Kubernetes infrastructure.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Deploying an Application to Azure Kubernetes Service"><div class="sect2" id="idm46219931401416">
<h2>Deploying an Application to Azure Kubernetes Service</h2>

<p>Standing up an Azure Kubernetes Service cluster is one of the basic skills needed to begin exploring AKS networking.
This <a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="example deployment with" id="ch6_term91"/>section will go through the steps of standing up a sample cluster and deploying the Golang web server example
from <a data-type="xref" href="ch01.xhtml#networking_introduction">Chapter 1</a> to that cluster. We will be using a combination of the Azure Portal, the Azure CLI, and <code>kubectl</code> to
perform these actions.</p>

<p>Before we begin with the cluster deployment and configuration, we should discuss the <a data-type="indexterm" data-primary="ACR (Azure Container Registry)" id="idm46219931328424"/><a data-type="indexterm" data-primary="Azure Container Registry (ACR)" id="idm46219931327656"/><a data-type="indexterm" data-primary="images, container" data-secondary="in Azure" data-secondary-sortas="Azure" id="idm46219931326968"/>Azure Container Registry (ACR).
The ACR is where you store container images in Azure. For this example, we will use the ACR as the location
for the container image we will be deploying. To import an image to the ACR, you will need to have the image locally
available on your computer. Once you have the image available, we have to prep it for the ACR.</p>

<p>First, <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Docker" data-secondary-sortas="Docker" id="idm46219931324872"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="images from" id="idm46219931323240"/>identify the ACR repository you want to store the image in and log in from the Docker CLI with <code>docker login
&lt;acr_repository&gt;.azurecr.io</code>. For this example, we will use the ACR repository <code>tjbakstestcr</code>, so the command would
be <code>docker login tjbakstestcr.azurecr.io</code>. Next, tag the local image you wish to import to the ACR with
<code>&lt;acr_repository&gt;.azurecr.io\&lt;imagetag&gt;</code>. For this example, we will use an image currently tagged <code>aksdemo</code>. Therefore,
the tag would be <code>tjbakstestcr.azure.io/aksdemo</code>. To tag the image, use the command <code>docker tag &lt;local_image_tag&gt;
&lt;acr_image_tag&gt;</code>. This example would use the command <code>docker tag aksdemo tjbakstestcr.azure.io/aksdem</code>. Finally,
we push the image to the ACR with <code>docker push tjbakstestcr.azure.io/aksdem</code>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can find additional information on Docker and the Azure Container Registry in the official <a href="https://oreil.ly/5swhT">documentation</a>.</p>
</div>

<p>Once the image is in the ACR, the <a data-type="indexterm" data-primary="service principal, Azure" id="idm46219931315480"/><a data-type="indexterm" data-primary="Azure Active Directory service principals" id="idm46219931314712"/>final prerequisite is to set up a service principal. This is easier to set up before
you begin, but you can do this during the AKS cluster creation. An Azure service principal is a representation of an
Azure Active Directory Application object. Service principals are generally used to interact with Azure through
application automation. We will be using a service principal to allow the AKS cluster to pull the <code>aksdemo</code> image from
the ACR. The service principal needs to have access to the ACR repository that you store the image in. You will need to
record the client ID and secret of the service principal you want to use.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can find additional information on Azure Active Directory service principals in the <a href="https://oreil.ly/pnZTw">documentation</a>.</p>
</div>

<p>Now that we have our image in the ACR and our service principal client ID and secret, we can begin deploying the AKS cluster.</p>










<section data-type="sect3" data-pdf-bookmark="Deploying an Azure Kubernetes Service cluster"><div class="sect3" id="idm46219931310120">
<h3>Deploying an Azure Kubernetes Service cluster</h3>

<p>The time has <a data-type="indexterm" data-primary="cluster networking" data-secondary="in cloud" data-secondary-sortas="cloud" id="ch6_term92"/><a data-type="indexterm" data-primary="Kubernetes cluster, deploying" id="ch6_term93"/>come to deploy our cluster. We are going to start in the Azure Portal. Go to <a href="https://oreil.ly/Wx4Ny"><em>portal.azure.com</em></a> to log in.
Once logged in, you should see a dashboard with a search bar at the top that will be used to locate services. From the
search bar, we will be typing <strong>kubernetes</strong> and selecting the Kubernetes Service option from the drop-down menu, which is
outlined in <a data-type="xref" href="#Azure_Kubernetes_Search">Figure 6-29</a>.</p>

<figure><div id="Azure_Kubernetes_Search" class="figure">
<img src="Images/neku_0629.png" alt="Azure Kubernetes Search" width="1992" height="692"/>
<h6><span class="label">Figure 6-29. </span>Azure Kubernetes search</h6>
</div></figure>

<p>Now we are on the Azure Kubernetes Services blade. Deployed AKS clusters are viewed from this screen using
filters and queries. This is also the screen for creating new AKS clusters. Near the top of the screen, <a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="create page in" id="ch6_term94"/><a data-type="indexterm" data-primary="create page, AKS" id="ch6_term95"/>we are going
to select Create as shown in <a data-type="xref" href="#Azure_Kubernetes_Create">Figure 6-30</a>. This will cause a drop-down menu to appear, where we will select “Create a
Kubernetes cluster.”</p>

<figure><div id="Azure_Kubernetes_Create" class="figure">
<img src="Images/neku_0630.png" alt="Azure Kubernetes Create" width="1948" height="506"/>
<h6><span class="label">Figure 6-30. </span>Creating an Azure Kubernetes cluster</h6>
</div></figure>

<p>Next we will define the properties of the AKS cluster from the “Create Kubernetes cluster” screen. First, we will
populate the Project Details section by selecting the subscription that the cluster will be deployed to. There is a
drop-down menu that allows for easier searching and selection. For this example, we are using the <code>tjb_azure_test_2</code> subscription,
but any subscription can work as long as you have access to it. Next, we have to define the resource group we will use
to group the AKS cluster. This can be an existing resource group or a new one can be created. For this example, we will
create a new resource group named <code>go-web</code>.</p>

<p>After the Project Details section is complete, we move on to the Cluster Details section. Here, we will define the
name of the cluster, which will be “go-web” for this example. The <a data-type="indexterm" data-primary="availability zones (AZs)" id="idm46219931292728"/><a data-type="indexterm" data-primary="AZs (availability zones)" id="idm46219931291960"/><a data-type="indexterm" data-primary="Azure networking services (Vnet)" data-secondary="regions of" id="idm46219931291272"/><a data-type="indexterm" data-primary="regions in VPC networking" data-secondary="with Azure" data-secondary-sortas="Azure" id="idm46219931290312"/>region, availability zones, and Kubernetes version
fields are also defined in this section and will have predefined defaults that can be changed. For this example, however,
we will use the default “(US) West 2” region with no availability zones and the default Kubernetes version of 1.19.11.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Not all Azure regions have availability zones that can be selected. If availability zones are part of the AKS architecture
that is being deployed, the appropriate regions should be considered. You can find more information on AKS regions in the availability zones <a href="https://oreil.ly/enxii">documentation</a>.</p>
</div>

<p>Finally, we will <a data-type="indexterm" data-primary="nodes" data-secondary="in AKS" data-secondary-sortas="AKS" id="idm46219931286024"/><a data-type="indexterm" data-primary="node pools" id="idm46219931284744"/>complete the Primary Node Pool section of the “Create Kubernetes cluster” screen by selecting the
node size and node count. For this example, we are going to keep the default node size of DS2 v2 and the default node
count of 3. While most virtual machines, sizes are available for use within AKS, there are some restrictions. <a data-type="xref" href="#Azure_Kubernetes_Create_Page">Figure 6-31</a>
shows the options we have selected filled in.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can find more information on AKS restrictions, including restricted node sizes, in the <a href="https://oreil.ly/A4bHq">documentation</a>.</p>
</div>

<p>Click the “Next: Node pools” button <a data-type="indexterm" data-startref="ch6_term94" id="idm46219931280296"/><a data-type="indexterm" data-startref="ch6_term95" id="idm46219931279592"/>to move to the Node Pools tab. This page allows for the configuration of additional
node pools for the AKS cluster. For this example, we are going to leave the defaults on this page and <a data-type="indexterm" data-primary="authentication" data-secondary="for Azure Kubernetes" data-secondary-sortas="Azure Kubernetes" id="idm46219931278584"/><a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="authentication page in" id="idm46219931277368"/>move on to the
Authentication page by clicking the “Next: Authentication” button at the bottom of the screen.</p>

<figure><div id="Azure_Kubernetes_Create_Page" class="figure">
<img src="Images/neku_0631.png" alt="Azure Kubernetes Create Page" width="1596" height="1364"/>
<h6><span class="label">Figure 6-31. </span>Azure Kubernetes create page</h6>
</div></figure>

<p><a data-type="xref" href="#Azure_Kubernetes_Authentication_Page">Figure 6-32</a> shows the Authentication page, where we will define the authentication method that the AKS cluster will use
to connect to attached Azure services such as the ACR we discussed previously in this chapter. “System-Assigned Managed Identity”
is the default authentication method, but we are going to select the “Service principal” radio button.</p>

<p>If you did not create a <a data-type="indexterm" data-primary="Azure Active Directory service principals" id="idm46219931272248"/><a data-type="indexterm" data-primary="service principal, Azure" id="idm46219931271448"/>service principal at the beginning of this section, you can create a new one here. If you create a
service principal at this stage, you will have to go back and grant that service principal permissions to access the ACR.
However, since we will use a previously created service principal, we are going to click the “Configure service principal”
link and enter the client ID and secret.</p>

<figure><div id="Azure_Kubernetes_Authentication_Page" class="figure">
<img src="Images/neku_0632.png" alt="Azure Kubernetes Authentication Page" width="1638" height="1592"/>
<h6><span class="label">Figure 6-32. </span>Azure Kubernetes Authentication page</h6>
</div></figure>

<p>The remaining configurations will remain at the defaults at this time. To complete the AKS cluster creation, we are going to click
the “Review + create” button. This <a data-type="indexterm" data-primary="Azure Kubernetes Service (AKS)" data-secondary="validation page in" id="idm46219931267608"/><a data-type="indexterm" data-primary="validation, Azure Kubernetes page for" id="idm46219931266568"/>will take us to the validation page. As shown in <a data-type="xref" href="#Azure_Kubernetes_Validation_Page">Figure 6-33</a>, if everything is
defined appropriately, the validation will return a “Validation Passed” message at the top of the screen. If
something is misconfigured, a “Validation Failed” message will be there instead. As long as validation passes, we
will review the settings and click Create.</p>

<figure><div id="Azure_Kubernetes_Validation_Page" class="figure">
<img src="Images/neku_0633.png" alt="Azure Kubernetes Validation Page" width="1478" height="1573"/>
<h6><span class="label">Figure 6-33. </span>Azure Kubernetes validation page</h6>
</div></figure>

<p>You can view the deployment status from the notification bell on the top of the Azure screen. <a data-type="xref" href="#Azure_Kubernetes_Deployment_Progress">Figure 6-34</a> shows our example
deployment in progress. This page has information that can be used to troubleshoot with Microsoft should an issue arise
such as the deployment name, start time, and correlation ID.</p>

<p>Our example deployed completely without issue, as shown in <a data-type="xref" href="#Azure_Kubernetes_Deployment_Complete">Figure 6-35</a>. Now that the AKS cluster is deployed, we need
to connect to it and configure it for use with our example web server.</p>

<figure><div id="Azure_Kubernetes_Deployment_Progress" class="figure">
<img src="Images/neku_0634.png" alt="Azure Kubernetes Deployment Progress" width="2242" height="939"/>
<h6><span class="label">Figure 6-34. </span>Azure Kubernetes deployment progress</h6>
</div></figure>

<figure><div id="Azure_Kubernetes_Deployment_Complete" class="figure">
<img src="Images/neku_0635.png" alt="Azure Kubernetes Deployment Complete" width="2240" height="1133"/>
<h6><span class="label">Figure 6-35. </span>Azure Kubernetes deployment complete</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Connecting to and configuring AKS"><div class="sect3" id="idm46219931309176">
<h3>Connecting to and configuring AKS</h3>

<p>We will now shift to working with the example <code>go-web</code> AKS cluster from the 
<span class="keep-together">command</span> line. To manage AKS clusters from
the command line, we will primarily use the <code>kubectl</code> command. <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Azure" data-secondary-sortas="Azure" id="idm46219931252472"/><a data-type="indexterm" data-primary="kubectl commands" data-secondary="Azure installation of" id="idm46219931251128"/>Azure CLI has a simple command, <code>az aks install-cli</code>, to
install the <code>kubectl</code> program for use. Before we can use <code>kubectl</code>, though, we need to gain access to the cluster. The command
<code>az aks get-credentials</code> 
<span class="keep-together"><code>--resource-group</code></span> <code>&lt;resource_group_name&gt; --name &lt;aks_cluster_name&gt;</code> is used to gain access to the
AKS cluster. For our example, we will use <code>az aks get-credentials --resource-group go-web --name go-web</code> to access our
<code>go-web</code> cluster in the <code>go-web</code> resource group.</p>

<p>Next we will <a data-type="indexterm" data-primary="ACR (Azure Container Registry)" id="idm46219931245224"/><a data-type="indexterm" data-primary="Azure Container Registry (ACR)" id="idm46219931244472"/>attach the Azure container registry that has our <code>aksdemo</code> image. 
<span class="keep-together">The command</span> <code>az aks update -n</code> <code>&lt;aks_cluster_name&gt;
-g</code> <code>&lt;clus⁠⁠ter_resource_​​group_name&gt;</code> <code>--attach-acr &lt;acr_repo_name&gt;</code> will attach a named ACR repo to an existing AKS cluster.
For our example, we will use the command <code>az aks update -n tjbakstest -g tjbakstest --attach-acr tjbakstestcr</code>. Our example
runs for a few moments and then produces the output shown in <a data-type="xref" href="#EX6">Example 6-1</a>.</p>
<div id="EX6" data-type="example">
<h5><span class="label">Example 6-1. </span>AttachACR output</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="o">{</code>- Finished ..
  <code class="s2">"aadProfile"</code>: null,
  <code class="s2">"addonProfiles"</code>: <code class="o">{</code>
    <code class="s2">"azurepolicy"</code>: <code class="o">{</code>
      <code class="s2">"config"</code>: null,
      <code class="s2">"enabled"</code>: <code class="nb">false</code>,
      <code class="s2">"identity"</code>: null
    <code class="o">}</code>,
    <code class="s2">"httpApplicationRouting"</code>: <code class="o">{</code>
      <code class="s2">"config"</code>: null,
      <code class="s2">"enabled"</code>: <code class="nb">false</code>,
      <code class="s2">"identity"</code>: null
    <code class="o">}</code>,
    <code class="s2">"omsAgent"</code>: <code class="o">{</code>
      <code class="s2">"config"</code>: <code class="o">{</code>
        <code class="s2">"logAnalyticsWorkspaceResourceID"</code>:
        <code class="s2">"/subscriptions/7a0e265a-c0e4-4081-8d76-aafbca9db45e/</code>
<code class="s2">        resourcegroups/defaultresourcegroup-wus2/providers/</code>
<code class="s2">        microsoft.operationalinsights/</code>
<code class="s2">        workspaces/defaultworkspace-7a0e265a-c0e4-4081-8d76-aafbca9db45e-wus2"</code>
      <code class="o">}</code>,
      <code class="s2">"enabled"</code>: <code class="nb">true</code>,
      <code class="s2">"identity"</code>: null
    <code class="o">}</code>
  <code class="o">}</code>,
  <code class="s2">"agentPoolProfiles"</code>: <code class="o">[</code>
    <code class="o">{</code>
      <code class="s2">"availabilityZones"</code>: null,
      <code class="s2">"count"</code>: 3,
      <code class="s2">"enableAutoScaling"</code>: <code class="nb">false</code>,
      <code class="s2">"enableNodePublicIp"</code>: null,
      <code class="s2">"maxCount"</code>: null,
      <code class="s2">"maxPods"</code>: 110,
      <code class="s2">"minCount"</code>: null,
      <code class="s2">"mode"</code>: <code class="s2">"System"</code>,
      <code class="s2">"name"</code>: <code class="s2">"agentpool"</code>,
      <code class="s2">"nodeImageVersion"</code>: <code class="s2">"AKSUbuntu-1804gen2containerd-2021.06.02"</code>,
      <code class="s2">"nodeLabels"</code>: <code class="o">{}</code>,
      <code class="s2">"nodeTaints"</code>: null,
      <code class="s2">"orchestratorVersion"</code>: <code class="s2">"1.19.11"</code>,
      <code class="s2">"osDiskSizeGb"</code>: 128,
      <code class="s2">"osDiskType"</code>: <code class="s2">"Managed"</code>,
      <code class="s2">"osType"</code>: <code class="s2">"Linux"</code>,
      <code class="s2">"powerState"</code>: <code class="o">{</code>
        <code class="s2">"code"</code>: <code class="s2">"Running"</code>
      <code class="o">}</code>,
      <code class="s2">"provisioningState"</code>: <code class="s2">"Succeeded"</code>,
      <code class="s2">"proximityPlacementGroupId"</code>: null,
      <code class="s2">"scaleSetEvictionPolicy"</code>: null,
      <code class="s2">"scaleSetPriority"</code>: null,
      <code class="s2">"spotMaxPrice"</code>: null,
      <code class="s2">"tags"</code>: null,
      <code class="s2">"type"</code>: <code class="s2">"VirtualMachineScaleSets"</code>,
      <code class="s2">"upgradeSettings"</code>: null,
      <code class="s2">"vmSize"</code>: <code class="s2">"Standard_DS2_v2"</code>,
      <code class="s2">"vnetSubnetId"</code>: null
    <code class="o">}</code>
  <code class="o">]</code>,
  <code class="s2">"apiServerAccessProfile"</code>: <code class="o">{</code>
    <code class="s2">"authorizedIpRanges"</code>: null,
    <code class="s2">"enablePrivateCluster"</code>: <code class="nb">false</code>
  <code class="o">}</code>,
  <code class="s2">"autoScalerProfile"</code>: null,
  <code class="s2">"diskEncryptionSetId"</code>: null,
  <code class="s2">"dnsPrefix"</code>: <code class="s2">"go-web-dns"</code>,
  <code class="s2">"enablePodSecurityPolicy"</code>: null,
  <code class="s2">"enableRbac"</code>: <code class="nb">true</code>,
  <code class="s2">"fqdn"</code>: <code class="s2">"go-web-dns-a59354e4.hcp.westus.azmk8s.io"</code>,
  <code class="s2">"id"</code>:
  <code class="s2">"/subscriptions/7a0e265a-c0e4-4081-8d76-aafbca9db45e/</code>
<code class="s2">  resourcegroups/go-web/providers/Microsoft.ContainerService/managedClusters/go-web"</code>,
  <code class="s2">"identity"</code>: null,
  <code class="s2">"identityProfile"</code>: null,
  <code class="s2">"kubernetesVersion"</code>: <code class="s2">"1.19.11"</code>,
  <code class="s2">"linuxProfile"</code>: null,
  <code class="s2">"location"</code>: <code class="s2">"westus"</code>,
  <code class="s2">"maxAgentPools"</code>: 100,
  <code class="s2">"name"</code>: <code class="s2">"go-web"</code>,
  <code class="s2">"networkProfile"</code>: <code class="o">{</code>
    <code class="s2">"dnsServiceIp"</code>: <code class="s2">"10.0.0.10"</code>,
    <code class="s2">"dockerBridgeCidr"</code>: <code class="s2">"172.17.0.1/16"</code>,
    <code class="s2">"loadBalancerProfile"</code>: <code class="o">{</code>
      <code class="s2">"allocatedOutboundPorts"</code>: null,
      <code class="s2">"effectiveOutboundIps"</code>: <code class="o">[</code>
        <code class="o">{</code>
          <code class="s2">"id"</code>:
          <code class="s2">"/subscriptions/7a0e265a-c0e4-4081-8d76-aafbca9db45e/</code>
<code class="s2">          resourceGroups/MC_go-web_go-web_westus/providers/Microsoft.Network/</code>
<code class="s2">          publicIPAddresses/eb67f61d-7370-4a38-a237-a95e9393b294"</code>,
          <code class="s2">"resourceGroup"</code>: <code class="s2">"MC_go-web_go-web_westus"</code>
        <code class="o">}</code>
      <code class="o">]</code>,
      <code class="s2">"idleTimeoutInMinutes"</code>: null,
      <code class="s2">"managedOutboundIps"</code>: <code class="o">{</code>
        <code class="s2">"count"</code>: 1
      <code class="o">}</code>,
      <code class="s2">"outboundIpPrefixes"</code>: null,
      <code class="s2">"outboundIps"</code>: null
    <code class="o">}</code>,
    <code class="s2">"loadBalancerSku"</code>: <code class="s2">"Standard"</code>,
    <code class="s2">"networkMode"</code>: null,
    <code class="s2">"networkPlugin"</code>: <code class="s2">"kubenet"</code>,
    <code class="s2">"networkPolicy"</code>: null,
    <code class="s2">"outboundType"</code>: <code class="s2">"loadBalancer"</code>,
    <code class="s2">"podCidr"</code>: <code class="s2">"10.244.0.0/16"</code>,
    <code class="s2">"serviceCidr"</code>: <code class="s2">"10.0.0.0/16"</code>
  <code class="o">}</code>,
  <code class="s2">"nodeResourceGroup"</code>: <code class="s2">"MC_go-web_go-web_westus"</code>,
  <code class="s2">"powerState"</code>: <code class="o">{</code>
    <code class="s2">"code"</code>: <code class="s2">"Running"</code>
  <code class="o">}</code>,
  <code class="s2">"privateFqdn"</code>: null,
  <code class="s2">"provisioningState"</code>: <code class="s2">"Succeeded"</code>,
  <code class="s2">"resourceGroup"</code>: <code class="s2">"go-web"</code>,
  <code class="s2">"servicePrincipalProfile"</code>: <code class="o">{</code>
    <code class="s2">"clientId"</code>: <code class="s2">"bbd3ac10-5c0c-4084-a1b8-39dd1097ec1c"</code>,
    <code class="s2">"secret"</code>: null
  <code class="o">}</code>,
  <code class="s2">"sku"</code>: <code class="o">{</code>
    <code class="s2">"name"</code>: <code class="s2">"Basic"</code>,
    <code class="s2">"tier"</code>: <code class="s2">"Free"</code>
  <code class="o">}</code>,
  <code class="s2">"tags"</code>: <code class="o">{</code>
    <code class="s2">"createdby"</code>: <code class="s2">"tjb"</code>
  <code class="o">}</code>,
  <code class="s2">"type"</code>: <code class="s2">"Microsoft.ContainerService/ManagedClusters"</code>,
  <code class="s2">"windowsProfile"</code>: null
<code class="o">}</code></pre></div>

<p>This output is the CLI representation of the AKS cluster information. This means that the attachment was successful.
Now that we have access to the AKS cluster and the ACR is attached, we can deploy the example Go web server to the
AKS cluster.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Deploying the Go web server"><div class="sect3" id="idm46219931227400">
<h3>Deploying the Go web server</h3>

<p>We are going <a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="with AKS cluster example" data-secondary-sortas="AKS cluster example" id="idm46219931225560"/><a data-type="indexterm" data-primary="YAML configuration file" id="idm46219931224296"/>to deploy the Golang code shown in <a data-type="xref" href="#kubernetes_podspec_for_golang_minimal_webserver">Example 6-2</a>. As mentioned earlier in this
chapter, <a data-type="indexterm" data-primary="ACR (Azure Container Registry)" id="idm46219930971704"/><a data-type="indexterm" data-primary="Azure Container Registry (ACR)" id="idm46219930970968"/>this code has been built into a Docker image and now is stored in the ACR in the <code>tjbakstestcr</code> repository. We will be using the following deployment YAML file to deploy the application.</p>
<div id="kubernetes_podspec_for_golang_minimal_webserver" data-type="example">
<h5><span class="label">Example 6-2. </span>Kubernetes Podspec for Golang minimal webserver</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">labels</code><code class="p">:</code>
    <code class="nt">test</code><code class="p">:</code> <code class="l-Scalar-Plain">liveness</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web</code>
    <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">go-web:v0.0.1</code>
    <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">containerPort</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
    <code class="nt">livenessProbe</code><code class="p">:</code>
      <code class="nt">httpGet</code><code class="p">:</code>
        <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/healthz</code>
        <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
      <code class="nt">initialDelaySeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>
      <code class="nt">periodSeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>
    <code class="nt">readinessProbe</code><code class="p">:</code>
      <code class="nt">httpGet</code><code class="p">:</code>
        <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>
        <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8080</code>
      <code class="nt">initialDelaySeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>
      <code class="nt">periodSeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code></pre></div>

<p>Breaking down this YAML file, we see that we are creating two AKS resources: a deployment and a service. The deployment is
configured for the creation of a container named <code>go-web</code> and a container port 8080. The deployment also references the

<span class="keep-together"><code>aksdemo</code></span> ACR image with the line <code>image: tjbakstestcr.azurecr.io/aksdemo</code> as the image that will be deployed to the container.
The service is also configured with the name go-web. The YAML specifies the service is a load balancer listening on port
8080 and targeting the <code>go-web</code> app.</p>

<p>Now we need to publish the application to the AKS cluster. The command <code>kubectl apply -f &lt;yaml_file_name&gt;.yaml</code> will
publish the application to the cluster. We will see from the output that two things are created: <code>deployment.apps/go-web</code>
and 
<span class="keep-together"><code>service/go-web</code>.</span> When we run the command <code>kubectl get pods</code>, we can see an output like that shown here:</p>

<pre id="getpods" data-type="programlisting" data-code-language="bash">○ → kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
go-web-574dd4c94d-2z5lp   1/1     Running   <code class="m">0</code>          5h29m</pre>

<p>Now that the application is deployed, we will connect to it to verify it is up and running. When a default AKS cluster
is stood up, a load balancer is deployed with it with a public IP address. We could go through the portal and locate
that load balancer and public IP address, but <code>kubectl</code> offers an easier path. The command <code>kubectl get</code> [.keep-together]#<code>service</code> <code>go-web</code>
produces this output:</p>

<pre id="getservice" data-type="programlisting" data-code-language="bash">○ → kubectl get service go-web
NAME     TYPE           CLUSTER-IP   EXTERNAL-IP    PORT<code class="o">(</code>S<code class="o">)</code>          AGE
go-web   LoadBalancer   10.0.3.75    13.88.96.117   8080:31728/TCP   21h</pre>

<p>In this output, we see the external IP address of 13.88.96.117. Therefore, if everything deployed correctly, <a data-type="indexterm" data-primary="cURL/curl tool" data-secondary="client requests with" id="idm46219930905480"/>we should
be able to cURL 13.88.96.117 at port 8080 with the command <code>curl 13.88.96.117:8080</code>. As we can see from this output,
we have a successful deployment:</p>

<pre id="curlrequest" data-type="programlisting" data-code-language="bash">○ → curl 13.88.96.117:8080 -vvv
*   Trying 13.88.96.117...
* TCP_NODELAY <code class="nb">set</code>
* Connected to 13.88.96.117 <code class="o">(</code>13.88.96.117<code class="o">)</code> port <code class="m">8080</code> <code class="o">(</code><code class="c">#0)</code>
&gt; GET / HTTP/1.1
&gt; Host: 13.88.96.117:8080
&gt; User-Agent: curl/7.64.1
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 <code class="m">200</code> OK
&lt; Date: Fri, <code class="m">25</code> Jun <code class="m">2021</code> 20:12:48 GMT
&lt; Content-Length: 5
&lt; Content-Type: text/plain<code class="p">;</code> <code class="nv">charset</code><code class="o">=</code>utf-8
&lt;
* Connection <code class="c">#0 to host 13.88.96.117 left intact</code>
Hello* Closing connection 0</pre>

<p>Going to a web browser and navigating to http://13.88.96.117:8080 will also be available, as shown in <a data-type="xref" href="#Azure_Kubernetes_Hello_App">Figure 6-36</a>.</p>

<figure><div id="Azure_Kubernetes_Hello_App" class="figure">
<img src="Images/neku_0636.png" alt="Azure Kubernetes Hello App" width="900" height="163"/>
<h6><span class="label">Figure 6-36. </span>Azure Kubernetes Hello app</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="AKS conclusion"><div class="sect3" id="idm46219931226808">
<h3>AKS conclusion</h3>

<p>In this section, we deployed an example Golang web server to an Azure Kubernetes Service cluster. We used the Azure Portal,
the <code>az cli</code>, and <code>kubectl</code> to deploy and configure the cluster and then deploy the application. We leveraged the Azure container
registry to host our web server image. We also used a YAML file to deploy the application and tested it with cURL and web <a data-type="indexterm" data-startref="ch6_term68" id="idm46219930860280"/><a data-type="indexterm" data-startref="ch6_term87" id="idm46219930891480"/><a data-type="indexterm" data-startref="ch6_term88" id="idm46219930890808"/><a data-type="indexterm" data-startref="ch6_term91" id="idm46219930890136"/><a data-type="indexterm" data-startref="ch6_term92" id="idm46219930889464"/><a data-type="indexterm" data-startref="ch6_term93" id="idm46219930888792"/>browsing.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm46219931332728">
<h1>Conclusion</h1>

<p>Each cloud <a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="summary of options for" id="idm46219930886456"/>provider has its nuanced differences when it comes to network services provided for Kubernetes clusters.
<a data-type="xref" href="#cloud_network_and_kubernetes_summary">Table 6-4</a> highlights some of those differences.  There are lots of factors to choose from when picking a cloud
service provider, and even more when selecting the managed Kubernetes platform to run. Our aim in this chapter was to
educate administrators and developers on the choices you will have to make when managing workloads on Kubernetes.</p>
<table id="cloud_network_and_kubernetes_summary">
<caption><span class="label">Table 6-4. </span>Cloud network and Kubernetes summary</caption>
<thead>
<tr>
<th/>
<th>AWS</th>
<th>Azure</th>
<th>GCP</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Virtual network</p></td>
<td><p>VPC</p></td>
<td><p>Vnet</p></td>
<td><p>VPC</p></td>
</tr>
<tr>
<td><p>Network scope</p></td>
<td><p>Region</p></td>
<td><p>Region</p></td>
<td><p>Global</p></td>
</tr>
<tr>
<td><p>Subnet boundary</p></td>
<td><p>Zone</p></td>
<td><p>Region</p></td>
<td><p>Region</p></td>
</tr>
<tr>
<td><p>Routing scope</p></td>
<td><p>Subnet</p></td>
<td><p>Subnet</p></td>
<td><p>VPC</p></td>
</tr>
<tr>
<td><p>Security controls</p></td>
<td><p>NACL/SecGroups</p></td>
<td><p>Network security groups/Application SecGroup</p></td>
<td><p>Firewall</p></td>
</tr>
<tr>
<td><p>IPv6</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p>Kubernetes managed</p></td>
<td><p>eks</p></td>
<td><p>aks</p></td>
<td><p>gke</p></td>
</tr>
<tr>
<td><p>ingress</p></td>
<td><p>AWS ALB controller</p></td>
<td><p>Nginx-Ingress</p></td>
<td><p>GKE ingress controller</p></td>
</tr>
<tr>
<td><p>Cloud custom CNI</p></td>
<td><p>AWS VPC CNI</p></td>
<td><p>Azure CNI</p></td>
<td><p>GKE CNI</p></td>
</tr>
<tr>
<td><p>Load Balancer support</p></td>
<td><p>ALB L7, L4 w/NLB, and Nginx</p></td>
<td><p>L4 Azure Load Balancer, L7 w/Nginx</p></td>
<td><p>L7, HTTP(S)</p></td>
</tr>
<tr>
<td><p>Network policies</p></td>
<td><p>Yes (Calico/Cilium)</p></td>
<td><p>Yes (Calico/Cilium)</p></td>
<td><p>Yes (Calico/Cilium)</p></td>
</tr>
</tbody>
</table>

<p>We have covered many layers, from the OSI foundation to running networks in the cloud for our clusters. Cluster
administrators, network engineers, and developers alike have many decisions to make, such as the subnet size, the CNI to choose,
and the load balancer type, to name a few. Understanding all of those and how they will affect the cluster network
was the basis for this book. <a data-type="indexterm" data-primary="Layer 3" data-see="Network (Internet) layer, TCP/IP (L3)" id="idm46219930690696"/><a data-type="indexterm" data-primary="TCP/IP" data-secondary="Network (Internet) layer" data-see="Network (Internet) layer, TCP/IP (L3)" id="idm46219930688984"/><a data-type="indexterm" data-primary="communication" data-secondary="pod-to-pod restrictions of" data-see="NetworkPolicies, Kubernetes" id="idm46219930687736"/><a data-type="indexterm" data-primary="pods" data-secondary="restriction of communication between" data-see="NetworkPolicies, Kubernetes" id="idm46219930686488"/><a data-type="indexterm" data-primary="routing" data-secondary="restrictions on" data-see="NetworkPolicies, Kubernetes" id="idm46219930685240"/><a data-type="indexterm" data-primary="syntax layer" data-see="Presentation layer" id="idm46219930684008"/><a data-type="indexterm" data-primary="Transmission Control Protocol" data-see="TCP (Transmission Control Protocol)" id="idm46219930683064"/><a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="in Transport layer" data-see="Transport layer, TCP/IP (L4)" data-secondary-sortas="Transport layer" id="idm46219930682088"/><a data-type="indexterm" data-primary="L4" data-see="Transport layer, TCP/IP (L4)" id="idm46219930680568"/><a data-type="indexterm" data-primary="Layer 4" data-see="Transport layer, TCP/IP (L4)" id="idm46219930679608"/><a data-type="indexterm" data-primary="TCP/IP" data-secondary="Transport layer" data-see="Transport layer, TCP/IP (L4)" id="idm46219930678648"/><a data-type="indexterm" data-primary="User Datagram textProtocol" data-see="UDP (User Datagram textProtocol)" id="idm46219930677416"/><a data-type="indexterm" data-primary="routing" data-secondary="CIDR ranges for" data-see="CIDR (Classless Inter-Domain Routing) ranges" id="idm46219930676440"/><a data-type="indexterm" data-primary="containers" data-secondary="namespaces and" data-see="namespaces, network" id="idm46219930675128"/>This is just the beginning of your journey for managing your clusters at scale. We have
managed to cover only the networking options available for managing Kubernetes clusters. Storage, compute, and even
how to deploy workloads onto those clusters are decisions you will have to make now. The <a data-type="indexterm" data-primary="Hacking Kubernetes (O'Reilly book)" id="idm46219930673480"/><a data-type="indexterm" data-primary="Production Kubernetes (O'Reilly book)" id="idm46219930672792"/>O’Reilly library has an extensive
number of books to help, such as <a href="https://oreil.ly/Xx12u"><em>Production Kubernetes</em></a> (Rosso et al.), where you learn what the path to production looks like when using
Kubernetes, and <a href="https://oreil.ly/FcU8C"><em>Hacking Kubernetes</em></a> (Martin and Hausenblas), on how to harden Kubernetes and how to review Kubernetes clusters for
security weaknesses.</p>

<p>We hope this guide has helped make those networking choices easy for you. We were inspired to see what the Kubernetes
community has done and are excited to see what you build on top of the abstractions Kubernetes <a data-type="indexterm" data-primary="L7" data-see="Application layer (L7)" id="idm46219930669976"/><a data-type="indexterm" data-primary="Layer 7" data-see="Application layer (L7)" id="idm46219930668488"/><a data-type="indexterm" data-primary="security" data-secondary="attacker vulnerabilities" data-see="attacker strategies" id="idm46219930667544"/><a data-type="indexterm" data-primary="Amazon Web Services (AWS)" data-see="AWS (Amazon Web Services)" id="idm46219930666312"/><a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="with AWS" data-see="AWS (Amazon Web Services)" data-secondary-sortas="AWS" id="idm46219930665336"/><a data-type="indexterm" data-primary="ALB" data-see="AWS ALB (application load balancer) ingress controller" id="idm46219930663816"/><a data-type="indexterm" data-primary="application load balancers (ALB)" data-see="AWS ALB (application load balancer) ingress controller" id="idm46219930662840"/><a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="AWS networking in" data-see="AWS networking" id="idm46219930661848"/><a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="with Azure" data-see="Azure" data-secondary-sortas="Azure" id="idm46219930660616"/><a data-type="indexterm" data-primary="AKS" data-see="Azure Kubernetes Service (AKS)" id="idm46219930659112"/><a data-type="indexterm" data-primary="Vnet" data-see="Azure networking services (Vnet)" id="idm46219930658152"/><a data-type="indexterm" data-primary="security" data-secondary="certificates" data-see="certificates and keys" id="idm46219930657192"/><a data-type="indexterm" data-primary="Classless Inter-Domain Routing (CIDR)" data-see="CIDR (Classless Inter-Domain Routing) ranges" id="idm46219930655976"/><a data-type="indexterm" data-primary="command-line interface (CLI)" data-see="CLI (client/command-line interface)" id="idm46219930654984"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="in cloud" data-see="cloud networking and Kubernetes" data-secondary-sortas="cloud" id="idm46219930654008"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="as cluster" data-see="cluster networking" data-secondary-sortas="cluster" id="idm46219930652504"/><a data-type="indexterm" data-primary="HTTP" data-secondary="cURL and" data-see="cURL/curl tool" id="idm46219930651016"/><a data-type="indexterm" data-primary="containers" data-secondary="Docker" data-see="Docker networking model" id="idm46219930649800"/><a data-type="indexterm" data-primary="Amazon Elastic Kubernetes Service (EKS)" data-see="EKS (Amazon Elastic Kubernetes Service)" id="idm46219930648584"/><a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="EKS in" data-see="EKS (Amazon Elastic Kubernetes Service)" id="idm46219930647608"/><a data-type="indexterm" data-primary="elastic network interface (ENI)" data-see="ENI (elastic network interface)" id="idm46219930646360"/><a data-type="indexterm" data-primary="Google Compute Cloud (GCP)" data-see="GCP (Google Compute Cloud)" id="idm46219930645384"/><a data-type="indexterm" data-primary="minimal web server (Go)" data-see="Golang (Go) web server" id="idm46219930644408"/><a data-type="indexterm" data-primary="web server (Go)" data-see="Golang (Go) web server" id="idm46219930643464"/><a data-type="indexterm" data-primary="Internet Control Message Protocol (ICMP)" data-see="ICMP (Internet Control Message Protocol)" id="idm46219930642520"/>provides for you.<a data-type="indexterm" data-primary="Internet Protocol (IP)" data-see="IP (Internet Protocol)" id="idm46219930641624"/><a data-type="indexterm" data-primary="addresses, IP" data-see="IP (Internet Protocol) addresses" id="idm46219930640776"/><a data-type="indexterm" data-primary="Kubernetes in Docker" data-see="KIND (Kubernetes in Docker) cluster" id="idm46219930639928"/><a data-type="indexterm" data-primary="Data Link layer" data-see="Link layer (L2)" id="idm46219930639080"/><a data-type="indexterm" data-primary="L2" data-see="Link layer (L2)" id="idm46219930638232"/><a data-type="indexterm" data-primary="Linux networking" data-secondary="routing" data-see="Linux, routing in" id="idm46219930637320"/><a data-type="indexterm" data-primary="routing" data-secondary="in Linux" data-see="Linux, routing in" data-secondary-sortas="Linux" id="idm46219930636104"/><a data-type="indexterm" data-primary="Media Access Control" data-see="MAC (Media Access Control) sublayer" id="idm46219930634616"/><a data-type="indexterm" data-primary="network namespaces" data-see="namespaces, network" id="idm46219930633656"/><a data-type="indexterm" data-primary="network address translation" data-see="NAT (network address translation)" id="idm46219930632712"/><a data-type="indexterm" data-primary="Internet (IP) layer" data-see="Network (Internet) layer, TCP/IP (L3)" id="idm46219930631736"/><a data-type="indexterm" data-primary="L3" data-see="Network (Internet) layer, TCP/IP (L3)" id="idm46219930630776"/></p>
</div></section>







</div></section></div></body></html>