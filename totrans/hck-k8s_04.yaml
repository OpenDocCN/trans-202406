- en: Chapter 3\. Container Runtime Isolation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux has evolved sandboxing and isolation techniques beyond simple virtual
    machines (VMs) that strengthen it from current and future vulnerabilities. Sometimes
    these sandboxes are called *micro VMs*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: These sandboxes combine parts of all previous container and VM approaches. You
    would use them to protect sensitive workloads and data, as they focus on rapid
    deployment and high performance on shared infrastructure.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we’ll discuss different types of micro VMs that use virtual
    machines and containers together, to protect your running Linux kernel and userspace.
    The generic term *sandboxing* is used to cover the entire spectrum: each tool
    in this chapter combines software and hardware virtualization of technologies
    and uses Linux’s Kernel Virtual Machine (KVM), which is widely used to power VMs
    in public cloud services, including Amazon Web Services and Google Cloud.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: You run a lot of workloads at BCTL, and you should remember that while these
    techniques may also protect against Kubernetes mistakes, all of your web-facing
    software and infrastructure is a more obvious place to defend first. Zero-days
    and container breakouts are rare in comparison to simple security-sensitive misconfigurations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Hardened runtimes are newer, and have fewer generally less dangerous CVEs than
    the kernel or more established container runtimes, so we’ll focus less on historical
    breakouts and more on the history of micro VM design and rationale.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Defaults
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`kubeadm` installs Kubernetes with `runc` as its container runtime, using `cri-o`
    or `containerd` to manage it. The old `dockershim` way of running `runc` was removed
    in Kubernetes v1.20, so although Kubernetes doesn’t use Docker any more, the `runc`
    container runtime that Docker is built on continues to run containers for us.
    [Figure 3-1](#runtime-sandboxing-k8s-cris) shows three ways Kubernetes can consume
    the `runc` container runtime: CRI-O, `containerd`, and Docker.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![haku 0301](Images/haku_0301.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Kubernetes container runtime interfaces
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ll get into container runtimes in a lot of detail later on in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Threat Model
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have two main reasons for isolating a workload or pod—it may have access
    to sensitive information and data, or it may be untrusted and potentially hostile
    to other users of the system:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A *sensitive* workload is one whose data or code is too important to permit
    unauthorized access to. This may include fraud detection systems, pricing engines,
    high-frequency trading algorithms, personally identifiable information (PII),
    financial records, passwords that may be reused in other systems, machine learning
    models, or an organization’s “secret sauce.” Sensitive workloads are precious.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Untrusted* workloads are those that may be dangerous to run. They may allow
    high-risk user input or run external software.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of potentially untrusted workloads include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: VM workloads on a cloud provider’s hypervisor
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CI/CD infrastructure subject to build-time supply chain attacks
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcoding of complex files with potential parser errors
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Untrusted workloads may also include software with published or suspected zero-day
    Common Vulnerabilities and Exposures (CVEs)—if no patch is available and the workload
    is business-critical, isolating it further may decrease the potential impact of
    the vulnerability if exploited.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The threat to a host running untrusted workloads is the workload, or process,
    itself. By sandboxing a process and removing the system APIs available to it,
    the attack surface presented by the host to the process is decreased. Even if
    that process is compromised, the risk to the host is less.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: BCTL allows users to upload files to import data and shipping manifests, so
    you have a risk that threat actors will try to upload badly formatted or malicious
    files to try to force exploitable software errors. The pods that run the batch
    transformation and processing workloads are a good candidate for sandboxing, as
    they are processing untrusted inputs as shown in [Figure 3-2](#runtime-sandboxing-malicious-batch-workload).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![haku 0302](Images/haku_0302.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Sandboxing a risky batch workload
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Any data supplied to an application by users can be considered untrusted, however
    most input will be sanitized in some way (for example, validating against an integer
    or string type). Complex files like PDFs or videos cannot be sanitized in this
    way, and rely upon the encoding libraries to be secure, which they sometimes are
    not. Bugs in this type are often “escapable” like CVE-X or ImageTragick.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Your threat model may include:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: An untrusted user input triggers a bug in a workload that an attacker uses to
    execute malicious code
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sensitive application is compromised and the attacker tries to exfiltrate
    data
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A malicious user on a compromised node attempts to read memory of other processes
    on the host
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New sandboxing code is less well tested, and may contain exploitable bugs
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A container image build pulls malicious dependencies and code from unauthenticated
    external sources that may contain malware
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Existing container runtimes come with some hardening by default, and Docker
    uses default `seccomp` and AppArmor profiles that drop a large number of unused
    system calls. These are not enabled by default in Kubernetes and must be enforced
    with admission control or PodSecurityPolicy. The `SeccompDefault=true` kubelet
    feature gate in v1.22 restores this container runtime default behavior.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an idea of the dangers to your systems, let’s take a step
    back. We’ll look at virtualization: what it is, why we use containers, and how
    to combine the best bits of containers and VMs.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Containers, Virtual Machines, and Sandboxes
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A major difference between a container and a VM is that containers exist on
    a shared host kernel. VMs boot a kernel every time they start, use hardware-assisted
    virtualization, and have a more secure but traditionally slower runtime.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: A common perception is that containers are optimized for speed and portability,
    and virtual machines sacrifice these features for more robust isolation from malicious
    behavior and higher fault tolerance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: This perception is not entirely true. Both technologies share a lot of common
    code pathways in the kernel itself. Containers and virtual machines have evolved
    like co-orbiting stars, never fully able to escape each other’s gravity. Container
    runtimes are a form of kernel virtualization. The OCI ([Open Container Initiative](https://oreil.ly/RCCWR))
    container image specifications have become the standardized atomic unit of container
    deployment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Next-generation sandboxes combine container and virtualization techniques (see
    [Figure 3-3](#runtime-comparison)) to reduce workloads’ access to the kernel.
    They do this by by emulating kernel functionality in userspace or the isolated
    guest environment, thus reducing the host’s attack surface to the process inside
    the sandbox. Well-defined interfaces can help to reduce complexity, minimizing
    the opportunity for untested code paths. And, by integrating the sandboxes with
    `containerd`, they are also able to interact with OCI images and with a software
    proxy (“shim”) to connect two different interfaces, which can be used with orchestrators
    like Kubernetes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0303.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-3\. Comparison of container isolation approaches; source: Christian
    Bargmann and Marina Tropmann-Frick’s [container isolation paper](https://oreil.ly/4slD4)'
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These sandboxing techniques are especially relevant to public cloud providers,
    for which multitenancy and bin packing is highly lucrative. Aggressively multitenanted
    systems such as Google Cloud Functions and AWS Lambda are running “untrusted code
    as a service,” and this isolation software is born from cloud vendor security
    requirements to isolate serverless runtimes from other tenants. Multitenancy will
    be discussed in depth in the next chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Cloud providers use virtual machines as the atomic unit of compute, but they
    may also wrap the root virtual machine process in container-like technologies.
    Customers then use the virtual machine to run containers—virtualized inception.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Traditional virtualization emulates a physical hardware architecture in software.
    Micro VMs emulate as small an API as possible, removing features like I/O devices
    and even system calls to ensure least privilege. However, they are still running
    the same Linux kernel code to perform low-level program operations such as memory
    mapping and opening sockets—just with additional security abstractions to create
    a secure by default runtime. So even though VMs are not sharing as much of the
    kernel as containers do, some system calls must still be executed by the host
    kernel.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Software abstractions require CPU time to execute, and so virtualization must
    always be a balance of security and performance. It is possible to add enough
    layers of abstraction and indirection that a process is considered “highly secure,”
    but it is unlikely that this ultimate security will result in a viable user experience.
    Unikernels go in the other direction, tracing a program’s execution and then removing
    almost all kernel functionality except what the program has used. Observability
    and debuggability are perhaps the reasons that unikernels have not seen widespread
    adoption.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: To understand the trade-offs and compromises inherent in each approach, it is
    important to grok a comparison of virtualization types. Virtualization has existed
    for a long time and has many variations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: How Virtual Machines Work
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although virtual machines and associated technologies have existed since the
    late 1950s, a lack of hardware support in the 1990s led to their temporary demise.
    During this time “process virtual machines” became more popular, especially the
    Java virtual machine (JVM). In this chapter we are exclusively referring to system
    virtual machines: a form of virtualization not tied to a specific programming
    language. Examples include KVM/QEMU, VMware, Xen, VirtualBox, etc.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machine research began in the 1960s to facilitate sharing large, expensive
    physical machines between multiple users and processes (see [Figure 3-4](#runtime-virtualisation-family-tree)).
    To share a physical host safely, some level of isolation must be enforced between
    tenants—and in case of hostile tenants, there should be much less access to the
    underlying system.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![Container abstractions](Images/haku_0304.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4\. Family tree of virtualization; source: [“The Ideal Versus the
    Real”](https://oreil.ly/7OLfk)'
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is performed in hardware (the CPU), software (in the kernel, and userspace),
    or from cooperation between both layers, and allows many users to share the same
    large physical hardware. This innovation became the driving technology behind
    public cloud adoption: safe sharing and isolation for processes, memory, and the
    resources they require from the physical host machine.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The host machine is split into smaller isolated compute units, traditionally
    referred to as guests (see [Figure 3-5](#runtime-virtualisation-high-level)).
    These guests interact with a virtualized layer above the physical host’s CPU and
    devices. That layer intercepts system calls to handle them itself: either by proxying
    them to the host kernel, or handling the request itself—doing the kernel’s job
    where possible. Full virtualization (e.g., VMware) emulates hardware and boots
    a full kernel inside the guest. Operating-system–level virtualization (e.g., a
    container) emulates the host’s kernel (i.e., using namespace, `cgroups`, capabilities,
    and `seccomp`) so it can start a containerized process directly on the host kernel.
    Processes in containers share many of the kernel pathways and security mechanisms
    that processes in VMs execute.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0305.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-5\. Server virtualization; source: [“The Ideal Versus the Real”](https://oreil.ly/oNBFf)'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To boot a kernel, a guest operating system will require access to a subset of
    the host machine’s functionality, including BIOS routines, devices and peripherals
    (e.g., keyboard, graphical/console access, storage, and networking), an interrupt
    controller and an interval timer, a source of entropy (for random number seeds),
    and the memory address space that it will run in.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Inside each guest virtual machine is an environment in which processes (or workloads)
    can run. The virtual machine itself is owned by a privileged parent process that
    manages its setup and interaction with the host, known as a *virtual machine monitor*
    or VMM (as in [Figure 3-6](#runtime-virtualisation-vmm)). This has also been known
    as a hypervisor, but the distinction is blurred with more recent approaches so
    the original term VMM is preferred.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0306.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. A virtual machine manager
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linux has a built-in virtual machine manager called KVM that allows a host kernel
    to run virtual machines. Along with QEMU, which emulates physical devices and
    provides memory management to the guest (and can run by itself if necessary),
    an operating system can run fully emulated by the guest OS and by QEMU (as contrasted
    with the Xen hypervisor in [Figure 3-7](#runtime-virtualisation-kvm-vs-xen-vs-qemu)).
    This emulation narrows the interface between the VM and the host kernel and reduces
    the amount of kernel code the process inside the VM can reach directly. This provides
    a greater level of isolation from unknown kernel vulnerabilities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0307.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-7\. KVM contrasted with Xen and QEMU; source: [What Is the Difference
    Between KVM and QEMU](https://oreil.ly/k1bJ1)'
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Despite many decades of effort, “in practice no virtual machine is completely
    equivalent to its real machine counterpart” ([“The Ideal Versus the Real”](https://oreil.ly/oNBFf)).
    This is due to the complexities of emulating hardware, and hopefully decreases
    the chance that we’re living in a simulation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Virtualization
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like all things we try to secure, virtualization must balance performance with
    security: decreasing the risk of running your workloads using the minimum possible
    number of extra checks at runtime. For containers, a shared host kernel is an
    avenue of potential container escape—the Linux kernel has a long heritage and
    monolithic codebase.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Linux is mainly written in the C language, which has classes of memory management
    and range checking vulnerabilities that have proven notoriously difficult to entirely
    eradicate. Many applications have experienced these exploitable bugs when subjected
    to fuzzers. This risk means we want to keep hostile code away from trusted interfaces
    in case they have zero-day vulnerabilities. This is a pretty serious defensive
    stance—it’s about reducing any window of opportunity for an attacker that has
    access to zero-day Linux vulnerabilities.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Google’s [OSS-Fuzz](https://oreil.ly/8LAkV) was born from the swirling maelstrom
    around the Heartbleed OpenSSL bug, which may have been raging in the wild for
    up to two years. Critical, internet-bolstering projects like OpenSSL are poorly
    funded and much goodwill exists in the open source community, so finding these
    bugs before they are exploited is a vital step in securing critical software.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The sandboxing model defends against zero-days by abstractions. It moves processes
    away from the Linux system call interface to reduce the opportunities to exploit
    it, using an assortment of containers and capabilities, LSMs and kernel modules,
    hardware and software virtualization, and dedicated drivers. Most recent sandboxes
    use a type-safe language like Golang or Rust, which makes their memory management
    safer than software programmed in C (which requires manual and potentially error-prone
    memory management).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: What’s Wrong with Containers?
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s further define what we mean by containers by looking at how they interact
    with the host kernel, as shown in [Figure 3-8](#runtime-host-kernel-boundary).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Containers talk directly to the host kernel, but the layers of LSMs, capabilities,
    and namespaces ensure they do not have full host kernel access. Conversely, instead
    of sharing one kernel, VMs use a guest kernel (a dedicated kernel running in a
    hypervisor). This means if the VM’s guest kernel is compromised, more work is
    required to break out of the hypervisor and into the host.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Host kernel boundary](Images/haku_0308.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Host kernel boundary
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Containers are created by a low-level container runtime, and as users we talk
    to the high-level container runtime that controls it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The diagram in [Figure 3-9](#runtime-docker-podman-crio) shows the high-level
    interfaces, with the container managers on the left. Then Kubernetes, Docker,
    and Podman interact with their respective libraries and runtimes. These perform
    useful container management features including pushing and pulling container images,
    managing storage and network interfaces, and interacting with the low-level container
    runtime.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![Container abstractions](Images/haku_0309.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-9\. Container abstractions; source: [“What’s up with CRI-O, Kata Containers
    and Podman?”](https://oreil.ly/2Mx7n)'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the middle column of [Figure 3-9](#runtime-docker-podman-crio) are the container
    runtimes that your Kubernetes cluster interacts with, while in the right column
    are the low-level runtimes responsible for starting and managing the container.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: That low-level container runtime is directly responsible for starting and managing
    containers, interfacing with the kernel to create the namespaces and configuration,
    and finally starting the process in the container. It is also responsible for
    handling your process inside the container, and getting its system calls to the
    host kernel at runtime.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: User Namespace Vulnerabilities
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linux was written with a core assumption: that the root user is always in the
    host namespace. This assumption held true while there were no other namespaces.
    But this changed with the introduction of user namespaces (the last major kernel
    namespace to be completed): developing user namespaces required many code changes
    to code concerning the root user.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: User namespaces allow you to map users inside a container to other users on
    the host, so ID 0 (root) inside the container can create files on a volume that
    from within the container look to be root-owned. But when you inspect the same
    volume from the host, they show up as owned by the user root was mapped to (e.g.,
    user ID 1000, or 110000, as shown in [Figure 3-10](#runtime-user-ns-remapping)).
    User namespaces are not enabled in Kubernetes, although work is underway to support
    them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![User namespace user id remapping](Images/haku_0310.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. User namespace user ID remapping
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Everything in Linux is a file, and files are owned by users. This makes user
    namespaces wide-reaching and complex, and they have been a source of privilege
    escalation bugs in previous versions of Linux:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[CVE-2013-1858](https://oreil.ly/5UHB1) (user namespace & CLONE_FS)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The clone system-call implementation in the Linux kernel before 3.8.3 does not
    properly handle a combination of the `CLONE_NEWUSER` and `CLONE_FS` flags, which
    allows local users to gain privileges by calling `chroot` and leveraging the sharing
    of the / directory between a parent process and a child process.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[CVE-2014-4014](https://oreil.ly/iRKjY) (user namespace & chmod)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The capabilities implementation in the Linux kernel before 3.14.8 does not properly
    consider that namespaces are inapplicable to inodes, which allows local users
    to bypass intended `chmod` restrictions by first creating a user namespace, as
    demonstrated by setting the `setgid` bit on a file with group ownership of `root`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[CVE-2015-1328](https://oreil.ly/uCaNj) (user namespace & OverlayFS (Ubuntu
    only))'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The `overlayfs` implementation in the Linux kernel package before 3.19.0-21.21
    in Ubuntu versions until 15.04 did not properly check permissions for file creation
    in the upper filesystem directory, which allowed local users to obtain root access
    by leveraging a configuration in which `overlayfs` is permitted in an arbitrary
    mount namespace.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[CVE-2018-18955](https://oreil.ly/8YIWz) (user namespace & complex ID mapping)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: In the Linux kernel 4.15.x through 4.19.x before 4.19.2, `map_write()` *in kernel/user_namespace.c*
    allows privilege escalation because it mishandles nested user namespaces with
    more than 5 `UID` or `GID` ranges. A user who has `CAP_SYS_ADMIN` in an affected
    user namespace can bypass access controls on resources outside the namespace,
    as demonstrated by reading */etc/shadow*. This occurs because an ID transformation
    takes place properly for the namespaced-to-kernel direction but not for the kernel-to-namespaced
    direction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Containers are not inherently “insecure,” but as we saw in [Chapter 2](ch02.xhtml#ch-pod-level-resources),
    they can leak some information about a host, and a root-owned container runtime
    is a potential exploitation path for a hostile process or container image.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Operations such as creating network adapters in the host network namespace,
    and mounting host disks, are historically root-only, which has made rootless containers
    harder to implement. Rootfull container runtimes were the only viable option for
    the first decade of popularized container use.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Exploits that have abused this rootfulness include [CVE-2019-5736](https://oreil.ly/ZZyRQ),
    replacing the `runc` binary from inside a container via */proc/self/exe*, and
    [CVE-2019-14271](https://oreil.ly/DSKFf), attacking the host from inside a container
    responding to `docker cp`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Underlying concerns about a root-owned daemon can be assuaged by running rootless
    containers in “unprivileged user namespaces” mode: creating containers using a
    nonroot user, within their own user namespace. This is supported in Docker 20.0X
    and Podman.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '*Rootless* means the low-level container runtime process that creates the container
    is owned by an unprivileged user, and so container breakout via the process tree
    only escapes to a nonroot user, nullifying some potential attacks.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Rootless containers introduce a hopefully less dangerous risk—user namespaces
    have historically been a rich source of vulnerabilities. The answer to whether
    it is riskier to run root-owned daemon or user namespaces isn’t clear-cut, although
    any reduction of root privileges is likely to be the more effective security boundary.
    There have been more high-profile breakouts from root-owned Docker, but this may
    well be down to adoption and widespread use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Rootless containers (without a root-owned daemon) provide a security boundary
    as compared to those with root-owned daemons. When code owned by the host’s root
    user is compromised by a malicious process, it can potentially read and write
    other users’ files, attack the network and its traffic, or install malware to
    the host.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The mapping of user identifiers (UIDs) in the guest to actual users on the host
    depends on the user mappings of the host user namespace, container user namespace,
    and rootless runtime, as shown in [Figure 3-11](#runtime-userns-and-rootles).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![User mapping for Rootless and User Namespace containers](Images/haku_0311.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-11\. Container abstractions; source: [“Experimenting with Rootless
    Docker”](https://oreil.ly/B2KzQ)'
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: User namespaces allow nonroot users to pretend to be the host’s root user. The
    “root-in-userns” user can have a “fake” UID 0 and permission to create new namespaces
    (mount, net, uts, ipc), change the container’s hostname, and mount points.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows root-in-userns, which is unprivileged in the host namespace, to
    create new containers. To achieve this, additional work must be done: network
    connections into the host network namespace can only be created by the host’s
    root. For rootless containers, an unprivileged *slirp4netns* networking device
    (guarded by `seccomp`) is used to create a virtual network device.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, mounting remote filesystems becomes difficult when the remote
    system, e.g., NFS home directories, does not understand the host’s user namespaces.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'In the [rootless Podman guide](https://oreil.ly/YjwLF), [Dan Walsh](https://oreil.ly/QzBhv)
    says:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: If you have a normal process creating files on an NFS share and not taking advantage
    of user-namespaced capabilities, everything works fine. The problem comes in when
    the root process inside the container needs to do something on the NFS share that
    requires special capability access. In that case, the remote kernel will not know
    about the capability and will most likely deny access.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While rootless Podman has SELinux support (and dynamic profile support via [udica](https://oreil.ly/AuSMF)),
    rootless Docker does not yet support AppArmor and, for both runtimes, CRIU (Checkpoint/Restore
    In Userspace, a feature to freeze running applications) is disabled.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Both rootless runtimes require configuration for some networking features:
    `CAP_NET_BIND_SERVICE` is required by the kernel to bind to ports below 1024 (historically
    considered a privileged boundary), and ping is not supported for users with high
    UIDs if the ID is not in */proc/sys/net/ipv4/ping_group_range* (although this
    can be changed by host root). Host networking is not permitted (as it breaks the
    network isolation), `cgroups` v2 are functional but only when running under `systemd`,
    and `cgroup` v1 is not supported by either rootless implementation. There are
    more details in the docs for [shortcomings of rootless Podman](https://oreil.ly/3SWtT).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Docker and Podman share similar performance and features as both use `runc`,
    although Docker has an established networking model that doesn’t support host
    networking in rootless mode, whereas Podman reuses Kubernetes’ Container Network
    Interface (CNI)) plug-ins for greater networking deployment flexibility.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Rootless containers decrease the risk of running your container images. Rootlessness
    prevents an exploit escalating to root via many host interactions (although some
    use of `SETUID` and `SETGID` binaries is often needed by software aiming to avoid
    running processes as root).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: While rootless containers protect the host from the container, it may still
    be possible to read some data from the host, although an adversary will find this
    a lot less useful. Root capabilities are needed to interact with potential privilege
    escalation points including */proc*, host devices, and the kernel interface, among
    others.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Throughout these layers of abstraction, system calls are still ultimately handled
    by software written in potentially unsafe C. Is the rootless runtime’s exposure
    to C-based system calls in the Linux kernel really that bad? Well, the C language
    powers the internet (and world?) and has done so for decades, but its lack of
    memory management leads to the same critical bugs occurring over and over again.
    When the kernel, OpenSSL, and other critical software are written in C, we just
    want to move everything as far away from trusted kernel space as possible.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Whitesource suggests](https://oreil.ly/yyD5o) that C has accounted for 47%
    of all reported vulnerabilities in the last 10 years. This may largely be due
    to its proliferation and longevity, but highlights the inherent risk.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: While “trimmed-down” kernels exist (like unikernels and rump kernels), many
    traditional and legacy applications are portable onto a container runtime without
    code modifications. To achieve this feat for a unikernel would require the application
    to be ported to the new reduced kernel. Containerizing an application is a generally
    frictionless developer experience, which has contributed to the success of containers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Sandboxing
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a process can exploit the kernel, it can take over the system the kernel
    is running. This is a risk that adversaries like Captian Hashjack will attempt
    to exploit, and so cloud providers and hardware vendors have been pioneering different
    approaches to moving away from Linux system call interaction for the guest.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Linux containers are a lightweight form of isolation as they allow workloads
    to use kernel APIs directly, minimizing the layers of abstraction. Sandboxes take
    a variety of other approaches, and generally use container techniques as well.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Linux’s Kernel Virtual Machine (KVM) is a module that allows the kernel to
    run a nested version of itself as a hypervisor. It uses the processor’s hardware
    virtualization commands and allows each “guest” to run a full Linux or Windows
    operating system in the virtual machine with private, virtualized hardware. A
    virtual machine differs from a container as the guest’s processes are running
    on their own kernel: container processes always share the host kernel.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Sandboxes combine the best of virtualization and container isolation to optimize
    for specific use cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: gVisor and Firecracker (written in Golang and Rust, respectively) both operate
    on the premise that their statically typed system call proxying (between the workload/guest
    process and the host kernel) is more secure for consumption by untrusted workloads
    than the Linux kernel itself, and that performance is not significantly impacted.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: gVisor starts a KVM or operates in `ptrace` mode (using a debug `ptrace` system
    call to monitor and control its guest), and inside starts a userspace kernel,
    which proxies system calls down to the host using a “sentry” process. This trusted
    process reimplements 237 Linux system calls and only needs 53 host system calls
    to operate. It is constrained to that list of system calls by `seccomp`. It also
    starts a companion “filesystem interaction” side process called Gofer to prevent
    a compromised sentry process interacting with the host’s filesystem, and finally
    implements its own userspace networking stack to isolate it from bugs in the Linux
    TCP/IP stack.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Firecracker, on the other hand, while also using KVM, starts a stripped-down
    device emulator instead of implementing the heavyweight QEMU process to emulate
    devices (as traditional Linux virtual machines do). This reduces the host’s attack
    surface and removes unnecessary code, requiring 36 system calls itself to function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: And finally, at the other end of the diagram in [Figure 3-12](#runtime-virtualisation-spectrum),
    KVM/QEMU VMs emulate hardware and so provide a guest kernel and full device emulation,
    which increases startup times and memory footprint.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0312.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Spectrum of isolation
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Virtualization provides better hardware isolation through CPU integration, but
    is slower to start and run due to the abstraction layer between the guest and
    the underlying host.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers are lightweight and suitably secure for most workloads. They run
    in production for multinational organizations around the world. But high-sensitivity
    workloads and data need greater isolation. You can categorize workloads by risk:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Does this application access a sensitive or high-value asset?
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this application able to receive untrusted traffic or input?
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have there been vulnerabilities or bugs in this application before?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the answer to any of those is yes, you may want to consider a next-generation
    sandboxing technology to further isolate workloads.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: gVisor, Firecracker, and Kata Containers all take different approaches to virtual
    machine isolation, while sharing the aim of challenging the perception of slow
    startup time and high memory overhead.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kata Containers is a container runtime that starts a VM and runs a container
    inside. It is widely compatible and can run `firecracker` as a guest.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-1](#runtime-virtualisation-comparison) compares these sandboxes and
    some key features.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3-1\. Comparison of sandbox features; source: [“Making Containers More
    Isolated: An Overview of Sandboxed Container Technologies”](https://oreil.ly/vpKaB)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Supported container platforms | Dedicated guest kernel | Support different
    guest kernels | Open source | Hot-plug | Direct access to HW | Required hypervisors
    | Backed by |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| gVisor | Docker, K8s | Yes | No | Yes | No | No | None | Google |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Firecracker | Docker | Yes | Yes | Yes | No | No | KVM | Amazon |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| Kata | Docker, K8s | Yes | Yes | Yes | Yes | Yes | KVM or Xen | OpenStack
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: 'Each sandbox combines virtual machine and container technologies: some VMM
    process, a Linux kernel within the virtual machine, a Linux userspace in which
    to run the process once the kernel has booted, and some mix of kernel-based isolation
    (that is, container-style namespaces, `cgroups`, or `seccomp`) either within the
    VM, around the VMM, or some combination thereof.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a closer look at each one.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: gVisor
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Google’s gVisor was originally built to allow untrusted, customer-supplied
    workloads to run in AppEngine on Borg, Google’s internal orchestrator and the
    progenitor to Kubernetes. It now protects Google Cloud products: App Engine standard
    environment, Cloud Functions, Cloud ML Engine, and Cloud Run, and it has been
    modified to run in GKE. It has the best Docker and Kubernetes integrations from
    among this chapter’s sandboxing technologies.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To run the examples, the gVisor runtime binary [must be installed](https://oreil.ly/Tj3hX)
    on the host or worker node.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker supports pluggable container runtimes, and a simple `docker run -it
    --runtime=runsc` starts a gVisor sandboxed OCI container. Let’s have a look at
    what’s in */proc* in a vanilla gVisor container to compare it with standard `runc`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Removing special files from this directory prevents a hostile process from accessing
    the relevant feature in the underlying host kernel.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'There are far fewer entries in */proc* than in a `runc` container, as this
    diff shows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The sentry process that [simulates the Linux system call interface](https://oreil.ly/MRraT)
    reimplements over 235 of the ~350 possible system calls in Linux 5.3.11\. This
    shows you a “masked” view of the */proc* and */dev* virtual filesystems. These
    filesystems have historically leaked the container abstraction by sharing information
    from the host (memory, devices, processes, etc.) so are an area of special concern.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at system devices under */dev* in gVisor and `runc`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see that the `runsc` gVisor runtime drops the `console` and `core` devices,
    but includes a `/dev/net/tun` device (under the *net/* directory) for its `netstack`
    networking stack, which also runs inside Sentry. Netstack can be bypassed for
    direct host network access (at the cost of some isolation), or host networking
    disabled entirely for fully host-isolated networking (depending on the CNI or
    other network configured within the sandbox).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from these giveaways, gVisor is kind enough to identify itself at boot
    time, which you can see in a container with `dmesg`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Notably this is not the real time it takes to start the container, and the
    quirky messages are randomized—don’t rely on them for automation. If we `time`
    the process we can see it start faster than it claims:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Unless an application running in a sandbox explicitly checks for these features
    of the environment, it will be unaware that it is in a sandbox. Your application
    makes the same system calls as it would to a normal Linux kernel, but the Sentry
    process intercepts the system calls as shown in [Figure 3-13](#runtime-gvisor-boundaries).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0313.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. gVisor container components and privilege boundaries
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sentry prevents the application interacting directly with the host kernel, and
    has a `seccomp` profile that limits its possible host system calls. This helps
    prevent escalation in case a tenant breaks into Sentry and attempts to attack
    the host kernel.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a userspace kernel is a Herculean undertaking and does not cover
    every system call. This means some applications are not able to run in gvisor,
    although in practice this doesn’t happen very often and there are millions of
    workloads running on GCP under gVisor.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The Sentry has a side process called Gofer. It handles disks and devices, which
    are historically common VM attack vectors. Separating out these responsibilities
    increases your resistance to compromise; if Sentry has an exploitable bug, it
    can’t be used to attack the host’s devices directly because they’re all proxied
    through Gofer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: gVisor is written in [Go](https://Golang.org) to avoid security pitfalls that
    can plague kernels. Go is strongly typed, with built-in bounds checks, no uninitialized
    variables, no use-after-free bugs, no stack overflow bugs, and a built-in race
    detector. However, using Go has its challenges, and the runtime often introduces
    a little performance overhead.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: However, this comes at the cost of some reduced application compatibility and
    a high per-system-call overhead. Of course, not all applications make a lot of
    system calls, so this depends on usage.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Application system calls are redirected to Sentry by a Platform Syscall Switcher,
    which intercepts the application when it tries to make system calls to the kernel.
    Sentry then makes the required system calls to the host for the containerized
    process, as shown in [Figure 3-14](#runtime-gvisor-privilege). This proxying prevents
    the application from directly controlling system calls.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0314.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. gVisor container components and privilege levels
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sentry sits in a loop waiting for a system call to be generated by the application,
    as shown in [Figure 3-15](#runtime-gvisor-sentry-loop).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0315.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-15\. gVisor sentry pseudocode; source: [Resource Sharing](https://oreil.ly/s1DjO)'
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It captures the system call with `ptrace`, handles it, and returns a response
    to the process (often without making the expected system call to the host). This
    simple model protects the underlying kernel from any direct interaction with the
    process inside the container.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The decreasing number of permitted calls shown in [Figure 3-16](#runtime-gvisor-syscall-hierarchy)
    limits the exploitable interface of the underlying host kernel to 68 system calls,
    while the containerized application process believes it has access to all ~350
    kernel calls.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The Platform Syscall Switcher, gVisor’s system call interceptor, has two modes:
    `ptrace` and KVM. The `ptrace` (“process trace”) system call provides a mechanism
    for a parent process to observe and modify another process’s behavior. `PTRACE_SYSEMU`
    forces the traced process to stop on entry to the next syscall, and gVisor is
    able to respond to it or proxy the request to the host kernel, going via Gofer
    if I/O is required.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0316.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. gVisor system call hierarchy
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Firecracker
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firecracker is a virtual machine monitor (VMM) that boots a dedicated VM for
    its guest using KVM. Instead of using KVM’s traditional device emulation pairing
    with QEMU, Firecracker implements its own memory management and device emulation.
    It has no BIOS (instead implementing Linux Boot Protocol), no PCI support, and
    stripped down, simple, virtualized devices with a single network device, a block
    I/O device, timer, clock, serial console, and keyboard device that only simulates
    Ctrl-Alt-Del to reset the VM, as shown in [Figure 3-17](#runtime-firecracker-1).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0317.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-17\. Firecracker and KVM interaction; source: [Resource Sharing](https://oreil.ly/s1DjO)'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Firecracker VMM process that starts the guest virtual machine is in turn
    started by a *jailer* process. The jailer configures the security configuration
    of the VMM sandbox (GID and UID assignment, network namespaces, create chroot,
    create `cgroups`), then terminates and passes control to Firecracker, where `seccomp`
    is enforced around the KVM guest kernel and userspace that it boots.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a second process for I/O like gVisor, Firecracker uses the
    KVM’s virtio drivers to proxy from the guest’s Firecracker process to the host
    kernel, via the VMM (shown in [Figure 3-18](#runtime-firecracker-2)). When the
    Firecracker VM image starts, it boots into protected mode in the guest kernel,
    never running in its real mode.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0318.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. Firecracker sandboxing the guest kernel from the host
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Firecracker is compatible with Kubernetes and OCI using the [firecracker-containerd
    shim](https://oreil.ly/rRswg).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Firecracker invokes far less host kernel code than traditional LXC or gVisor
    once it has started, although they all touch similar amounts of kernel code to
    start their sandboxes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance improvements are gained from an isolated memory stack, and lazily
    flushing data to the page cache instead of disk to increase filesystem performance.
    It supports arbitrary Linux binaries but does not support generic Linux kernels.
    It was created for AWS’s Lambda service, forked from Google’s ChromeOS VMM, crosvm:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: What makes crosvm unique is a focus on safety within the programming language
    and a sandbox around the virtual devices to protect the kernel from attack in
    case of an exploit in the devices.
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Chrome OS Virtual Machine Monitor](https://oreil.ly/dbaZ5)'
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Firecracker is a statically linked Rust binary that is compatible with Kata
    Containers, [Weave Ignite](https://oreil.ly/lUQ4Y), [firekube](https://oreil.ly/zn0Nc),
    and [firecracker-containerd](https://oreil.ly/pluqR). It provides soft allocation
    (not allocating memory until it’s actually used) for more aggressive “bin packing,”
    and so greater resource utilization.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Kata Containers
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, Kata Containers consists of lightweight VMs containing a container
    engine. They are highly optimized for running containers. They are also the oldest,
    and most mature, of the recent sandboxes. Compatibility is wide, with support
    for most container orchestrators.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Grown from a combination of Intel Clear Containers and Hyper.sh RunV, Kata
    Containers ([Figure 3-19](#runtime-kata-1)) wraps containers with a dedicated
    KVM virtual machine and device emulation from a pluggable backend: QEMU, QEMU-lite,
    NEMU (a custom stripped-down QEMU), or Firecracker. It is an OCI runtime and so
    supports Kubernetes.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0319.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 3-19\. Kata Containers architecture
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Kata Containers runtime launches each container on a guest Linux kernel.
    Each Linux system is on its own hardware-isolated VM, as you can see in [Figure 3-20](#runtime-kata-2).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The `kata-runtime` process is the VMM, and the interface to the OCI runtime.
    `kata-proxy` handles I/O for the `kata-agent` (and therefore the application)
    using KVM’s `virtio-serial`, and multiplexes a command channel over the same connection.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '`kata-shim` is the interface to the container engine, handling container lifecycles,
    signals, and logs.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0320.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: Figure 3-20\. Kata Containers components
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The guest is started using KVM and either QEMU or Firecracker. The project has
    forked QEMU twice to experiment with lightweight start times and has reimplemented
    a number of features back into QEMU, which is now preferred to NEMU (the most
    recent fork).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Inside the VM, QEMU boots an optimized kernel, and `systemd` starts the `kata-agent`
    process. `kata-agent`, which uses `libcontainer` and so shares a lot of code with
    `runc`, manages the containers running inside the VM.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Networking is provided by integrating with CNI (or Docker’s CNM), and a network
    namespace is created for each VM. Because of its networking model, the host network
    can’t be joined.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: SELinux and AppArmor are not currently implemented, and some OCI inconsistencies
    [limit the Docker integration](https://oreil.ly/VUz84).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: rust-vmm
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many new VMM technologies have some Rustlang components. So is Rust any good?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: It is similar to Golang in that it is memory safe (memory model, virtio, etc.)
    but it is built atop a memory ownership model, which avoids whole classes of bugs
    including use after free, double free, and dangling pointer issues.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: It has safe and simple concurrency and no garbage collector (which may incur
    some virtualization overhead and latency), instead using build-time analysis to
    find segmentation faults and memory issues.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[rust-vmm](https://oreil.ly/vs5f7) is a development toolkit for new VMMs as
    shown in [Figure 3-21](#runtime-rust-vmm-1). It is a collection of building blocks
    (Rust packages, or “crates”) comprised of virtualization components. These are
    well tested (and therefore better secured) and provide a simple, clean interface.
    For example, the `vm-memory` crate is a guest memory abstraction, providing a
    guest address, memory regions, and guest shared memory.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/haku_0321.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-21\. Kata Containers components; source: [Resource Sharing](https://oreil.ly/s1DjO)'
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The project was birthed from ChromeOS’s `cross-vm` (`crosvm`), which was forked
    by Firecracker and subsequently abstracted into “hypervisor from scratch” Rust
    crates. This approach will enable the development of a plug-and-play hypervisor
    architecture.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To see how a runtime is built, you can check out [Youki](https://oreil.ly/z4PmV).
    It’s an experimental container runtime written in Rust that implements the `runc`
    [runtime-spec](https://oreil.ly/MBWS0).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Risks of Sandboxing
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The degree of access and privilege that a guest process has to host features,
    or virtualized versions of them, impacts the attack surface available to an attacker
    in control of the guest process.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: This new tranche of sandbox technologies is under active development. It’s code,
    and like all new code, is at risk of exploitable bugs. This is a fact of software,
    however, and is infinitely better than no new software at all!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: It may be that these sandboxes are not yet a target for attackers. The level
    of innovation and baseline knowledge to contribute means the barrier to entry
    is set high. Captain Hashjack is likely to prioritize easier targets.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: From an administrator’s perspective, modifying or debugging applications within
    the sandbox becomes slightly more difficult, similar to the difference between
    bare metal and containerized processes. These difficulties are not insurmountable
    but require administrator familiarization with the underlying runtime.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: It is still possible to run [privileged sandboxes](https://oreil.ly/xxRnE) that
    have elevated capabilities within the guest. And although the risks are fewer
    than for privileged containers, users should be aware that any reduction of isolation
    increases the risk of running the process inside the sandbox.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Runtime Class
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes and Docker support running multiple container runtimes simultaneously;
    in Kubernetes, [Runtime Class](https://oreil.ly/dRHzA) is stable from v1.20 on.
    This means a Kubernetes worker node can host pods running under different Container
    Runtime Interfaces (CRIs), which greatly enhances workload separation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: With `spec.template.spec.runtimeClassName` you can target a sandbox for a Kubernetes
    workload via CRI.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Docker is able to run any OCI-compliant runtime (e.g., `runc`, `runsc`), but
    the Kubernetes `kubelet` uses CRI. While Kubernetes has not yet distinguished
    between types of sandboxes, we can still set node affinity and toleration so pods
    are scheduled on to nodes that have the relevant sandbox technology installed.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a new CRI runtime in Kubernetes, create a non-namespaced `RuntimeClass`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then reference the CRI runtime class in the pod definition:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This has started a new pod using `gvisor`. Remember that `runsc` (gVisor’s runtime
    component) must be installed on the node that the pod is scheduled on.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally sandboxes are more secure, and containers are less complex.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: When running sensitive or untrusted workloads, you want to narrow the interface
    between a sandboxed process and the host. There are trade-offs—debugging a rogue
    process becomes much harder, and traditional tracing tools may not have good compatibility.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: There is a general, minor performance overhead for sandboxes over containers
    (~50–200ms startup), which may be negligible for some workloads, and benchmarking
    is strongly encouraged. Options may also be limited by platform or nested virtualization
    options.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: As next-generation runtimes have focused on stripping down legacy compatibility,
    they are very small and very fast to start up (compared to traditional VMs)—not
    as fast as LXC or `runc`, but fast enough for FaaS providers to offer aggressive
    scale rates.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Traditional container runtimes like LXC and `runc` are faster to start as they
    run a process on an existing kernel. Sandboxes need to configure their own guest
    kernel, which leads to slightly longer start times.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Managed services are easiest to adopt, with gVisor in GKE and Firecracker in
    AWS Fargate. Both of them, and Kata, will run anywhere virtualization is supported,
    and the future is bright with the `rust-vmm` library promising many more runtimes
    to keep valuable workloads safe.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Segregating the most sensitive workloads on dedicated nodes in sandboxes gives
    your systems the greatest resistance to practical compromise.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
