- en: Chapter 10\. Setting Up a Replica Set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter introduces MongoDB’s high-availability system: replica sets. It
    covers:'
  prefs: []
  type: TYPE_NORMAL
- en: What replica sets are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up a replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What configuration options are available for replica set members
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the first chapter, we’ve been using a standalone server, a single *mongod*
    server. It’s an easy way to get started but a dangerous way to run in production.
    What if your server crashes or becomes unavailable? Your database will be unavailable
    for at least a little while. If there are problems with the hardware, you might
    have to move your data to another machine. In the worst case, disk or network
    issues could leave you with corrupt or inaccessible data.
  prefs: []
  type: TYPE_NORMAL
- en: Replication is a way of keeping identical copies of your data on multiple servers
    and is recommended for all production deployments. Replication keeps your application
    running and your data safe, even if something happens to one or more of your servers.
  prefs: []
  type: TYPE_NORMAL
- en: With MongoDB, you set up replication by creating a *replica set*. A replica
    set is a group of servers with one *primary*, the server taking writes, and multiple
    *secondaries*, servers that keep copies of the primary’s data. If the primary
    crashes, the secondaries can elect a new primary from amongst themselves.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using replication and a server goes down, you can still access your
    data from the other servers in the set. If the data on a server is damaged or
    inaccessible, you can make a new copy of the data from one of the other members
    of the set.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces replica sets and covers how to set up replication on
    your system. If you are less interested in replication mechanics and simply want
    to create a replica set for testing/development or production, use MongoDB’s cloud
    solution, [MongoDB Atlas](https://atlas.mongodb.com). It’s easy to use and provides
    a free-tier option for experimentation. Alternatively, to manage MongoDB clusters
    in your own infrastructure, you can use [Ops Manager](https://oreil.ly/-X6yp).
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a Replica Set, Part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll show you how to set up a three-node replica set on a
    single machine so you can start experimenting with replica set mechanics. This
    is the type of setup that you might script just to get a replica set up and running
    and then poke at it with administrative commands in the *mongo* shell or simulate
    network partitions or server failures to better understand how MongoDB handles
    high availability and disaster recovery. In production, you should always use
    a replica set and allocate a dedicated host to each member to avoid resource contention
    and provide isolation against server failure. To provide further resilience, you
    should also use the [DNS Seedlist Connection format](https://oreil.ly/cCORE) to
    specify how your applications connect to your replica set. The advantage to using
    DNS is that servers hosting your MongoDB replica set members can be changed in
    rotation without needing to reconfigure the clients (specifically, their connection
    strings).
  prefs: []
  type: TYPE_NORMAL
- en: Given the variety of virtualization and cloud options available, it is nearly
    as easy to bring up a test replica set with each member on a dedicated host. We’ve
    provided a Vagrant script to allow you to experiment with this option.^([1](ch10.xhtml#idm45882356792632))
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with our test replica set, let’s first create separate data
    directories for each node. On Linux or macOS, run the following command in the
    terminal to create the three directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will create the directories *~/data/rs1*, *~/data/rs2*, and *~/data/rs3*
    (`~` identifies your home directory).
  prefs: []
  type: TYPE_NORMAL
- en: 'On Windows, to create these directories, run the following in the Command Prompt
    (cmd) or PowerShell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, on Linux or macOS, run each of the following commands in a separate terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On Windows, run each of the following commands in its own Command Prompt or
    PowerShell window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once you’ve started them, you should have three separate *mongod* processes
    running.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, the principles we will walk through in the rest of this chapter
    apply to replica sets used in production deployments where each *mongod* has a
    dedicated host. However, there are additional details pertaining to securing replica
    sets that we address in [Chapter 19](ch19.xhtml#chapter-data-admin); we’ll touch
    on those just briefly here as a preview.
  prefs: []
  type: TYPE_NORMAL
- en: Networking Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every member of a set must be able to make connections to every other member
    of the set (including itself). If you get errors about members not being able
    to reach other members that you know are running, you may have to change your
    network configuration to allow connections between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The processes you’ve launched can just as easily be running on separate servers.
    However, with the release of MongoDB 3.6, *mongod* binds to *localhost* (127.0.0.1)
    only by default. In order for each member of replica set to communicate with the
    others, you must also bind to an IP address that is reachable by other members.
    If we were running a *mongod* instance on a server with a network interface having
    an IP address of 198.51.100.1 and we wanted to run it as a member of replica set
    with each member on different servers, we could specify the command-line parameter
    `--bind_ip` or use `bind_ip` in the configuration file for this instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We would make similar modifications to launch the other *mongod*s as well in
    this case, regardless of whether we’re running on Linux, macOS, or Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Security Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you bind to IP addresses other than *localhost*, when configuring a replica
    set, you should enable authorization controls and specify an authentication mechanism.
    In addition, it is a good idea to encrypt data on disk and communication among
    replica set members and between the set and clients. We’ll go into more detail
    on securing replica sets in [Chapter 19](ch19.xhtml#chapter-data-admin).
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a Replica Set, Part 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Returning to our example, with the work we’ve done so far, each *mongod* does
    not yet know that the others exist. To tell them about one another, we need to
    create a configuration that lists each of the members and send this configuration
    to one of our *mongod* processes. It will take care of propagating the configuration
    to the other members.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a fourth terminal, Windows Command Prompt, or PowerShell window, launch
    a *mongo* shell that connects to one of the running *mongod* instances. You can
    do this by typing the following command. With this command, we’ll connect to the
    *mongod* running on port 27017:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in the *mongo* shell, create a configuration document and pass this to
    the `rs.initiate()` helper to initiate a replica set. This will initiate a replica
    set containing three members and propagate the configuration to the rest of the
    *mongod*s so that a replica set is formed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are several important parts of a replica set configuration document. The
    config’s `"_id"` is the name of the replica set that you passed in on the command
    line (in this example, `"mdbDefGuide"`). Make sure that this name matches exactly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of the document is an array of members of the set. Each of these
    needs two fields: an `"_id"` that is an integer and unique among the replica set
    members, and a hostname.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are using *localhost* as a hostname for the members in this set.
    This is for example purposes only. In later chapters where we discuss securing
    replica sets, we’ll look at configurations that are more appropriate for production
    deployments. MongoDB allows all-*localhost* replica sets for testing locally but
    will protest if you try to mix *localhost* and non-*localhost* servers in a config.
  prefs: []
  type: TYPE_NORMAL
- en: This config document is your replica set configuration. The member running on
    *localhost:27017* will parse the configuration and send messages to the other
    members, alerting them of the new configuration. Once they have all loaded the
    configuration, they will elect a primary and start handling reads and writes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unfortunately, you cannot convert a standalone server to a replica set without
    some downtime for restarting it and initializing the set. Thus, even if you only
    have one server to start out with, you may want to configure it as a one-member
    replica set. That way, if you want to add more members later, you can do so without
    downtime.
  prefs: []
  type: TYPE_NORMAL
- en: If you are starting a brand-new set, you can send the configuration to any member
    in the set. If you are starting with data on one of the members, you must send
    the configuration to the member with data. You cannot initiate a replica set with
    data on more than one member.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once initiated, you should have a fully functional replica set. The replica
    set should elect a primary. You can view the status of a replica set using `rs.status()`.
    The output from `rs.status()` tells you quite a bit about the replica set, including
    a number of things we’ve not yet covered, but don’t worry, we’ll get there! For
    now, take a look at the `members` array. Note that all three of our *mongod* instances
    are listed in this array and that one of them, in this case the *mongod* running
    on port 27017, has been elected primary. The other two are secondaries. If you
    try this for yourself you will certainly have different values for `"date"` and
    the several `Timestamp` values in this output, but you might also find that a
    different *mongod* was elected primary (that’s totally fine):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Observing Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If your replica set elected the *mongod* on port 27017 as primary, then the
    *mongo* shell used to initiate the replica set is currently connected to the primary.
    You should see the prompt change to something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that we are connected to the primary of the replica set having
    the `"_id"` `"mdbDefGuide"`. To simplify and for the sake of clarity, we’ll abbreviate
    the *mongo* shell prompt to just `>` throughout the replication examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your replica set elected a different node primary, quit the shell and connect
    to the primary by specifying the correct port number in the command line, as we
    did when launching the *mongo* shell earlier. For example, if your set’s primary
    is on port 27018, connect using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you’re connected to the primary, try doing some writes and see what
    happens. First, insert 1,000 documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now check one of the secondaries and verify that it has a copy of all of these
    documents. You could do this by quitting the shell and connecting using the port
    number of one of the secondaries, but it’s easy to acquire a connection to one
    of the secondaries by instantiating a connection object using the `Mongo` constructor
    within the shell you’re already running.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, use your connection to the *test* database on the primary to run the
    `isMaster` command. This will show you the status of the replica set, in a much
    more concise form than `rs.status()`. It is also a convenient means of determining
    which member is primary when writing application code or scripting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If at any point an election is called and the *mongod* you’re connected to
    becomes a secondary, you can use the `isMaster` command to determine which member
    has become primary. The output here tells us that *localhost:27018* and *localhost:27019*
    are both secondaries, so we can use either for our purposes. Let’s instantiate
    a connection to *localhost:27019*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we attempt to do a read on the collection that has been replicated
    to the secondary, we’ll get an error. Let’s attempt to do a `find` on this collection
    and then review the error and why we get it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Secondaries may fall behind the primary (or *lag*) and not have the most current
    writes, so secondaries will refuse read requests by default to prevent applications
    from accidentally reading stale data. Thus, if you attempt to query a secondary,
    you’ll get an error stating that it’s not the primary. This is to protect your
    application from accidentally connecting to a secondary and reading stale data.
    To allow queries on the secondary, we can set an “I’m okay with reading from secondaries”
    flag, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that `slaveOk` is set on the *connection* (`secondaryConn`), not the database
    (`secondaryDB`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you’re all set to read from this member. Query it normally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can see that all of our documents are there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, try to write to a secondary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the secondary does not accept the write. A secondary will only
    perform writes that it gets through replication, not from clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one other interesting feature that you should try out: automatic failover.
    If the primary goes down, one of the secondaries will automatically be elected
    primary. To test this, stop the primary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see some error messages generated when you run this command because
    the *mongod* running on port 27017 (the member we’re connected to) will terminate
    and the shell we’re using will lose its connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This isn’t a problem. It won’t cause the shell to crash. Go ahead and run `isMaster`
    on the secondary to see who has become the new primary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from `isMaster` should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that the primary has switched to 27018\. Your primary may be the other
    server; whichever secondary noticed that the primary was down first will be elected.
    Now you can send writes to the new primary.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`isMaster` is a very old command, predating replica sets to when MongoDB only
    supported master/slave replication. Thus, it does not use the replica set terminology
    consistently: it still calls the primary a “master.” You can generally think of
    “master” as equivalent to “primary” and “slave” as equivalent to “secondary.”'
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and bring back up the server we had running at *localhost:27017*. You
    simply need to find the command-line interface from which you launched it. You’ll
    see some messages indicating that it terminated. Just run it again using the same
    command you used to launch it originally.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You just set up, used, and even poked a little at a replica
    set to force a shutdown and an election for a new primary.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few key concepts to remember:'
  prefs: []
  type: TYPE_NORMAL
- en: Clients can send a primary all the same operations they could send a standalone
    server (reads, writes, commands, index builds, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clients cannot write to secondaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clients, by default, cannot read from secondaries. You can enable this by explicitly
    setting an “I know I’m reading from a secondary” setting on the connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing Your Replica Set Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Replica set configurations can be changed at any time: members can be added,
    removed, or modified. There are shell helpers for some common operations. For
    example, to add a new member to the set, you can use `rs.add`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can remove members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check that a reconfiguration succeeded by running `rs.config()` in
    the shell. It will print the current configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Each time you change the configuration, the `"version"` field will increase.
    It starts at version 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also modify existing members, not just add and remove them. To make
    modifications, create the configuration document that you want in the shell and
    call `rs.reconfig()`. For example, suppose we have a configuration such as the
    one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Someone accidentally added member 0 by IP address, instead of its hostname.
    To change that, first we load the current configuration in the shell and then
    we change the relevant fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the config document is correct, we need to send it to the database
    using the `rs.reconfig()` helper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`rs.reconfig()` is often more useful than `rs.add()` and `rs.remove()` for
    complex operations, such as modifying members’ configurations or adding/removing
    multiple members at once. You can use it to make any legal configuration change
    you need: simply create the config document that represents your desired configuration
    and pass it to `rs.reconfig()`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to Design a Set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To plan out your set, there are certain concepts that you must be familiar
    with. The next chapter goes into more detail about these, but the most important
    is that replica sets are all about majorities: you need a majority of members
    to elect a primary, a primary can only stay primary as long as it can reach a
    majority, and a write is safe when it’s been replicated to a majority. This majority
    is defined to be “more than half of all members in the set,” as shown in [Table 10-1](#table9-1).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. What is a majority?
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of members in the set | Majority of the set |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 4 |'
  prefs: []
  type: TYPE_TB
- en: Note that it doesn’t matter how many members are down or unavailable; majority
    is based on the set’s configuration.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that we have a five-member set and three members go down,
    as shown in [Figure 10-1](#repl141). There are still two members up. These two
    members cannot reach a majority of the set (at least three members), so they cannot
    elect a primary. If one of them were primary, it would step down as soon as it
    noticed that it could not reach a majority. After a few seconds, your set would
    consist of two secondaries and three unreachable members.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. With a minority of the set available, all members will be secondaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Many users find this frustrating: why can’t the two remaining members elect
    a primary? The problem is that it’s possible that the other three members didn’t
    actually go down, and that it was instead the network that went down, as shown
    in [Figure 10-2](#repl142). In this case, the three members on the left will elect
    a primary, since they can reach a majority of the set (three members out of five).
    In the case of a network partition, we do not want both sides of the partition
    to elect a primary, because then the set would have two primaries. Both primaries
    would be writing to the database, and the datasets would diverge. Requiring a
    majority to elect or stay a primary is a neat way of avoiding ending up with more
    than one primary.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. For the members, a network partition looks identical to servers
    on the other side of the partition going down
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to configure your set in such a way that you’ll usually be able
    to have one primary. For example, in the five-member set described here, if members
    1, 2, and 3 are in one data center and members 4 and 5 are in another, there should
    almost always be a majority available in the first data center (it’s more likely
    to have a network break between data centers than within them).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of common configurations that are recommended:'
  prefs: []
  type: TYPE_NORMAL
- en: A majority of the set in one data center, as in [Figure 10-2](#repl142). This
    is a good design if you have a primary data center where you always want your
    replica set’s primary to be located. So long as your primary data center is healthy,
    you will have a primary. However, if that data center becomes unavailable, your
    secondary data center will not be able to elect a new primary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An equal number of servers in each data center, plus a tie-breaking server in
    a third location. This is a good design if your data centers are “equal” in preference,
    since generally servers from either data center will be able to see a majority
    of the set. However, it involves having three separate locations for servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More complex requirements might require different configurations, but you should
    keep in mind how your set will acquire a majority under adverse conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these complexities would disappear if MongoDB supported having more
    than one primary. However, this would bring its own host of complexities. With
    two primaries, you would have to handle conflicting writes (e.g., if someone updates
    a document on one primary and someone deletes it on another primary). There are
    two popular ways of handling conflicts in systems that support multiple writers:
    manual reconciliation or having the system arbitrarily pick a “winner.” Neither
    of these options is a very easy model for developers to code against, seeing as
    you can’t be sure that the data you’ve written won’t change out from under you.
    Thus, MongoDB chose to only support having a single primary. This makes development
    easier but can result in periods when the replica set is read-only.'
  prefs: []
  type: TYPE_NORMAL
- en: How Elections Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a secondary cannot reach a primary, it will contact all the other members
    and request that it be elected primary. These other members do several sanity
    checks: Can they reach a primary that the member seeking election cannot? Is the
    member seeking election up to date with replication? Is there any member with
    a higher priority available that should be elected instead?'
  prefs: []
  type: TYPE_NORMAL
- en: In version 3.2, MongoDB introduced version 1 of the replication protocol. Protocol
    version 1 is based on the RAFT consensus protocol developed by Diego Ongaro and
    John Ousterhout at Stanford University. It is best described as RAFT-like and
    is tailored to include a number of replication concepts that are specific to MongoDB,
    such as arbiters, priority, nonvoting members, write concern, etc. Protocol version
    1 provided the foundation for new features such as a shorter failover time and
    greatly reduces the time to detect false primary situations. It also prevents
    double voting through the use of term IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: RAFT is a consensus algorithm that is broken into relatively independent subproblems.
    Consensus is the process through which multiple servers or processes agree on
    values. RAFT ensures consensus such that the same series of commands produces
    the same series of results and arrives at the same series of states across the
    members of a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Replica set members send heartbeats (pings) to each other every two seconds.
    If a heartbeat does not return from a member within 10 seconds, the other members
    mark the delinquent member as inaccessible. The election algorithm will make a
    “best-effort” attempt to have the secondary with the highest priority available
    call an election. Member priority affects both the timing and the outcome of elections;
    secondaries with higher priority call elections relatively sooner than secondaries
    with lower priority, and are also more likely to win. However, a lower-priority
    instance can be elected as primary for brief periods, even if a higher-priority
    secondary is available. Replica set members continue to call elections until the
    highest-priority member available becomes primary.
  prefs: []
  type: TYPE_NORMAL
- en: To be elected primary, a member must be up to date with replication, as far
    as the members it can reach know. All replicated operations are strictly ordered
    by an ascending identifier, so the candidate must have operations later than or
    equal to those of any member it can reach.
  prefs: []
  type: TYPE_NORMAL
- en: Member Configuration Options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The replica sets we have set up so far have been fairly uniform in that every
    member has the same configuration as every other member. However, there are many
    situations when you don’t want members to be identical: you might want one member
    to preferentially be primary or make a member invisible to clients so that no
    read requests can be routed to it. These and many other configuration options
    can be specified in the member subdocuments of the replica set configuration.
    This section outlines the member options that you can set.'
  prefs: []
  type: TYPE_NORMAL
- en: Priority
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Priority is an indication of how strongly this member “wants” to become primary.
    Its value can range from `0` to `100`, and the default is `1`. Setting `"priority"`
    to `0` has a special meaning: members with a priority of `0` can never become
    primary. These are called *passive* members.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The highest-priority member will always be elected primary (so long as it can
    reach a majority of the set and has the most up-to-date data). For example, suppose
    you add a member with a priority of `1.5` to the set, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Assuming the other members of the set have priority `1`, once *server-4* caught
    up with the rest of the set, the current primary would automatically step down
    and *server-4* would elect itself. If *server-4* was, for some reason, unable
    to catch up, the current primary would stay primary. Setting priorities will never
    cause your set to go primary-less. It will also never cause a member that is behind
    to become primary (until it has caught up).
  prefs: []
  type: TYPE_NORMAL
- en: 'The absolute value of `"priority"` only matters in relation to whether it is
    greater or less than the other priorities in the set: members with priorities
    of `100`, `1`, and `1` will behave the same way as members of another set with
    priorities `2`, `1`, and `1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Members
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clients do not route requests to hidden members, and hidden members are not
    preferred as replication sources (although they will be used if more desirable
    sources are not available). Thus, many people will hide less powerful or backup
    servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose you had a set that looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To hide *server-3*, you could add the `hidden: true` field to its configuration.
    A member must have a priority of `0` to be hidden (you can’t have a hidden primary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now running `isMaster` will show:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`rs.status()` and `rs.config()` will still show the member; it only disappears
    from `isMaster`. When clients connect to a replica set, they call `isMaster` to
    determine the members of the set. Thus, hidden members will never be used for
    read requests.'
  prefs: []
  type: TYPE_NORMAL
- en: To unhide a member, change the `hidden` option to `false` or remove the option
    entirely.
  prefs: []
  type: TYPE_NORMAL
- en: Election Arbiters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A two-member set has clear disadvantages for majority requirements. However,
    many people with small deployments do not want to keep three copies of their data,
    feeling that two is enough and that keeping a third copy is not worth the administrative,
    operational, and financial costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For these deployments, MongoDB supports a special type of member called an
    *arbiter*, whose only purpose is to participate in elections. Arbiters hold no
    data and aren’t used by clients: they just provide a majority for two-member sets.
    In general, deployments without arbiters are preferable.'
  prefs: []
  type: TYPE_NORMAL
- en: As arbiters don’t have any of the traditional responsibilities of a *mongod*
    server, you can run an arbiter as a lightweight process on a wimpier server than
    you’d generally use for MongoDB. It’s often a good idea, if possible, to run an
    arbiter in a separate failure domain from the other members, so that it has an
    “outside perspective” on the set, as described in the deployment recommendations
    in [“How to Design a Set”](#repl-setup-section4).
  prefs: []
  type: TYPE_NORMAL
- en: 'You start up an arbiter in the same way that you start a normal *mongod*, using
    the ``--replSet *`name`*`` option and an empty data directory. You can add it
    to the set using the `rs.addArb()` helper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Equivalently, you can specify the `"arbiterOnly"` option in the member configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'An arbiter, once added to the set, is an arbiter forever: you cannot reconfigure
    an arbiter to become a nonarbiter, or vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: One other thing that arbiters are good for is breaking ties in larger clusters.
    If you have an even number of nodes, you may have half the nodes vote for one
    member and half for another. An arbiter can cast the deciding vote. There are
    a few things to keep in mind when using arbiters, though; we’ll look at these
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Use at most one arbiter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that, in both of the use cases just described, you need *at most* one arbiter.
    You do not need an arbiter if you have an odd number of nodes. A common misconception
    seems to be that you should add extra arbiters “just in case.” However, it doesn’t
    help elections go any faster or provide any additional data safety to add extra
    arbiters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have a three-member set. Two members are required to elect a primary.
    If you add an arbiter, you’ll have a four-member set, so three members will be
    required to choose a primary. Thus, your set is potentially less stable: instead
    of requiring 67% of your set to be up, you’re now requiring 75%.'
  prefs: []
  type: TYPE_NORMAL
- en: Having extra members can also make elections take longer. If you have an even
    number of nodes because you added an arbiter, your arbiters can cause ties, not
    prevent them.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to using an arbiter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have a choice between a data node and an arbiter, choose a data node.
    Using an arbiter instead of a data node in a small set can make some operational
    tasks more difficult. For example, suppose you are running a replica set with
    two “normal” members and one arbiter, and one of the data-holding members goes
    down. If that member is well and truly dead (the data is unrecoverable), you will
    have to get a copy of the data from the current primary to the new server you’ll
    be using as a secondary. Copying data can put a lot of stress on a server, and
    thus slow down your application. (Generally, copying a few gigabytes to a new
    server is trivial but more than a hundred starts becoming impractical.)
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, if you have three data-holding members, there’s more “breathing
    room” if a server completely dies. You can use the remaining secondary to bootstrap
    a new server instead of depending on your primary.
  prefs: []
  type: TYPE_NORMAL
- en: In the two-member-plus-arbiter scenario, the primary is the last remaining good
    copy of your data *and* the one trying to handle load from your application while
    you’re trying to get another copy of your data online.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if possible, use an odd number of “normal” members instead of an arbiter.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In three-member replica sets with a primary-secondary-arbiter (PSA) architecture
    or sharded clusters with a three-member PSA shard, there is a known issue with
    cache pressure increasing if either of the two data-bearing nodes are down and
    the `"majority"` read concern is enabled. Ideally, you should replace the arbiter
    with a data-bearing member for these deployments. Alternatively, to prevent storage
    cache pressure the [`"majority"` read concern can be disabled](https://oreil.ly/p6nUm)
    on each of the *mongod* instances in the deployment or shards.
  prefs: []
  type: TYPE_NORMAL
- en: Building Indexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes a secondary does not need to have the same (or any) indexes that
    exist on the primary. If you are using a secondary only for backup data or offline
    batch jobs, you might want to specify `"buildIndexes" : false` in the member’s
    configuration. This option prevents the secondary from building any indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a permanent setting: members that have `"buildIndexes" : false` specified
    can never be reconfigured to be “normal” index-building members again. If you
    want to change a non-index-building member to an index-building one, you must
    remove it from the set, delete all of its data, add it to the set again, and allow
    it to resync from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: As with hidden members, this option requires the member’s priority to be `0`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.xhtml#idm45882356792632-marker)) See [*https://github.com/mongodb-the-definitive-guide-3e/mongodb-the-definitive-guide-3e*](https://github.com/mongodb-the-definitive-guide-3e/mongodb-the-definitive-guide-3e).
  prefs: []
  type: TYPE_NORMAL
