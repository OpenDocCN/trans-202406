- en: Chapter 12\. Managing Multiple Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discuss best practices for managing multiple Kubernetes
    clusters. We dive into the details of the differences between multicluster management
    and federation, tools to manage multiple clusters, and operational patterns for
    managing multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder why you would need multiple Kubernetes clusters. Kubernetes
    was built to consolidate many workloads to a single cluster, correct? This is
    true, but there are scenarios that might require multiple clusters, such as workloads
    across regions, concerns of blast radius, regulatory compliance, and specialized
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: We discuss these scenarios and explore the tools and techniques for managing
    multiple clusters in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Why Multiple Clusters?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When adopting Kubernetes, you will likely have more than one cluster, and you
    might even start with more than one cluster to break out production from staging,
    user acceptance testing (UAT), or development. Kubernetes provides some multitenancy
    features with namespaces, which are a logical way to break up a cluster into smaller
    logical constructs. Namespaces allow you to define Role-Based Access Control (RBAC),
    quotas, pod security policies, and network policies to allow separation of workloads.
    This is a great way to separate multiple teams and projects, but there are other
    concerns that might require you to build a multicluster architecture. Concerns
    to think about when deciding to use multicluster versus a single-cluster architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Blast radius
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard multitenancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regional-based workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When thinking through your architecture, *blast radius* should come front and
    center. This is one of the main concerns that we see with users designing for
    multicluster architectures. With microservice architectures we employ circuit
    breakers, retries, bulkheads, and rate limiting to constrain the extent of damage
    to our systems. You should design the same into your infrastructure layer, and
    multiple clusters can help with preventing the impact of cascading failures due
    to software issues. For example, if you have one cluster that serves 500 applications
    and you have a platform issue, it takes out 100% of the 500 applications. If you
    had a platform layer issue with five clusters serving those 500 applications,
    you affect only 20% of the applications. The downside to this is that now you
    need to manage five clusters, and your consolidation ratios will not be as good
    as with a single cluster. Dan Woods wrote a great [article](https://oreil.ly/YnGUD)
    about an actual cascading failure in a production Kubernetes environment. It is
    a great example of why you will want to consider multicluster architectures for
    larger environments.
  prefs: []
  type: TYPE_NORMAL
- en: '*Compliance* is another area of concern for multicluster design because there
    are special considerations for Payment Card Industry (PCI), Health Insurance Portability
    and Accountability (HIPAA), and other workloads. It’s not that Kubernetes doesn’t
    provide some multitenant features, but these workloads might be easier to manage
    if they are segregated from general purpose workloads. These compliant workloads
    might have specific requirements with respect to security hardening, nonshared
    components, or dedicated workload requirements. It’s just much easier to separate
    these workloads than to have to treat the cluster in such a specialized fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Security* in large Kubernetes clusters can become difficult to manage. As
    you start onboarding more and more teams to a Kubernetes cluster, each team may
    have different security requirements, and it can become very difficult to meet
    those needs in a large multitenant cluster. Even just managing RBAC, network policies,
    and pod security policies can become difficult at scale in a single cluster. A
    small change to a network policy can inadvertently open up security risk to other
    users of the cluster. With multiple clusters you can limit the security impact
    with a misconfiguration. If you decide that a larger Kubernetes cluster fits your
    requirements, then ensure that you have a very good operational process for making
    security changes and that you understand the blast radius of making a change to
    RBAC, network policy, and pod security policies.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes doesn’t provide *hard multitenancy* because it shares the same API
    boundary with all workloads running within the cluster. With namespacing this
    gives us good soft multitenancy, but not enough to protect against hostile workloads
    within the cluster. Hard multitenancy is not a requirement for a lot of users;
    they trust the workloads that will be running within the cluster. Hard multitenancy
    is typically a requirement if you are a cloud provider, hosting software as a
    service (SaaS)-based software, or hosting untrusted workloads with untrusted user
    control.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Kubernetes project does address hard multitenancy concerns with Virtual
    Clusters, outside the scope of the book. Find more information on the [project’s
    GitHub](https://oreil.ly/KlFlK).
  prefs: []
  type: TYPE_NORMAL
- en: When running workloads that need to serve traffic from in-region endpoints,
    your design will include multiple clusters that are based per region. When you
    have a globally distributed application, it becomes a requirement at that point
    to run multiple clusters. When you have workloads that need to be *regionally
    distributed*, it’s a great use case for cluster federation of multiple clusters,
    which we dig into further later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '*Specialized workloads*, such as high-performance computing (HPC), machine
    learning (ML), and grid computing, also need to be addressed in the multicluster
    architecture. These types of specialized workloads might require specific types
    of hardware, have unique performance profiles, and have specialized users of the
    clusters. We’ve seen this use case to be less prevalent in the design decision
    because having multiple Kubernetes node pools can help address specialized hardware
    and performance profiles. When you need a very large cluster for an HPC or machine
    learning workload, you should consider just dedicating clusters for these workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: With multicluster, you get isolation for “free,” but it also has design concerns
    that you need to address at the outset.
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster Design Concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When choosing a multicluster design there are some challenges that you’ll run
    into. Some of these challenges might deter you from attempting a multicluster
    design given that the design might overcomplicate your architecture. Some of the
    common challenges we find users running into are:'
  prefs: []
  type: TYPE_NORMAL
- en: Data replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network routing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data replication* and consistency have always been the crux of deploying workloads
    across geographical regions and multiple clusters. When running these services,
    you need to decide what runs where and develop a replication strategy. Most databases
    have built-in tools to perform the replication, but you need to design the application
    to be able to handle the replication strategy. For NoSQL-type database services
    this can be easier because they can handle scaling across multiple instances,
    but you still need to ensure that your application can handle eventual consistency
    across geographic regions or at least the latency across regions. Some cloud services,
    such as Google Cloud Spanner and Microsoft Azure CosmosDB, have built database
    services to help with the complications of handling data across multiple geographic
    regions.'
  prefs: []
  type: TYPE_NORMAL
- en: Each Kubernetes cluster deploys its own *service discovery* registry, and registries
    are not synchronized across multiple clusters. This complicates applications being
    able to easily identify and discover one another. Tools such as HashiCorp’s Consul
    can transparently synchronize services from multiple clusters and even services
    that reside outside of Kubernetes. Other tools like Istio, Linkerd, and Cilium
    are building on multiple cluster architectures to extend service discovery between
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes makes networking from within the cluster very easy, as it’s a flat
    network and avoids using network address translation (NAT). If you need to route
    traffic in and out of the cluster, this becomes more complicated. Ingress into
    the cluster is implemented as a 1:1 mapping of ingress to the cluster because
    it doesn’t support multicluster topologies with the Ingress resource. You’ll also
    need to consider the egress traffic between clusters and how to route that traffic.
    When your applications reside within a single cluster this is easy, but when introducing
    multicluster, you need to think about the latency of extra hops for services that
    have application dependencies in another cluster. For applications that have tightly
    coupled dependencies, you should consider running these services within the same
    cluster to remove latency and extra complexity.
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest overheads to managing multiclusters is the *operational management*.
    Instead of one or a couple of clusters to manage and keep consistent, you might
    now have many clusters to manage in your environment. One of the most important
    aspects to managing multiclusters is ensuring that you have good automation practices
    in place because this will help to reduce the operational burden. When automating
    your clusters, you need to take into account the infrastructure deployment and
    managing add-on features to your clusters. For managing the infrastructure, using
    a tool like HashiCorp’s Terraform can help with deploying and managing a consistent
    state across your fleet of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Using an *Infrastructure as Code* (IaC) tool like Terraform will give you the
    benefit of providing a reproducible way to deploy your clusters. On the other
    hand, you also need to be able to consistently manage add-ons to the cluster,
    such as monitoring, logging, ingress, security, and other tools. Security is another
    important aspect of operational management, and you must be able to maintain security
    policies, RBAC, and network policies across clusters. Later in this chapter, we
    dive deeper into the topic of maintaining consistent clusters with automation.
  prefs: []
  type: TYPE_NORMAL
- en: With multiple clusters and continuous delivery (CD), you now need to deal with
    multiple Kubernetes API endpoints versus a single API endpoint. This can cause
    challenges in the distribution of applications. You can easily manage multiple
    pipelines, but suppose that you have a hundred different pipelines to manage,
    which can make application distribution very difficult. With this in mind, you
    need to look at different approaches to managing this situation. We take a look
    at solutions to help manage this later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Multiple Cluster Deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first steps you want to take when managing multicluster deployments
    is to use an IaC tool like Terraform to set up deployments. Other deployment tools,
    such as kubespray, kops, or other cloud provider–specific tools, are all valid
    choices, but, most importantly, use a tool that allows you to source control your
    cluster deployment for repeatability.
  prefs: []
  type: TYPE_NORMAL
- en: Automation is key to successfully managing multiple clusters in your environment.
    You might not have everything automated on day one, but you should make it a priority
    to automate all aspects of your cluster deployments and operations.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting project is the [Kubernetes Cluster API](https://oreil.ly/edzIa),
    a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation,
    configuration, and management. It provides optional additive functionality on
    top of core Kubernetes. The Cluster API provides a cluster-level configuration
    declared through a common API, which will give you the ability to easily automate
    and build tooling around cluster automation. The Cluster API is still in its early
    stages, but it’s a project to keep an eye on.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment and Management Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes operators were introduced as an implementation of the *Infrastructure
    as Software* concept. Using them allows you to abstract the deployment of applications
    and services in a Kubernetes cluster. For example, suppose that you want to standardize
    on Prometheus for monitoring your Kubernetes clusters. You would need to create
    and manage various objects (deployment, service, ingress, etc.) for each cluster
    and team. You would also need to maintain the fundamental configurations of Prometheus,
    such as versions, persistence, retention policies, and replicas. As you can imagine,
    the maintenance of such a solution could be difficult across a large number of
    clusters and teams.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of dealing with so many objects and configurations, you could install
    the `prometheus-operator`. This extends the Kubernetes API, exposing multiple
    new object kinds called `Prometheus`, `ServiceMonitor`, `PrometheusRule`, and
    `AlertManager`, which allow you to specify all the details of a Prometheus deployment
    using just a few objects. You can use the `kubectl` tool to manage such objects,
    just as it manages any other Kubernetes API object.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-1](#figure13-1) shows the architecture of the `prometheus-operator`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![prometheus-operator architecture](assets/kbp2_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. `prometheus-operator` architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Utilizing the *Operator* pattern for automating key operational tasks can help
    improve your overall cluster management capabilities. The Operator pattern was
    introduced by the CoreOS team in 2016 with the etcd operator and `prometheus-operator`.
    The Operator pattern builds on two concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom resource definitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom controllers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Custom resource definitions* (CRDs) are objects that allow you to extend the
    Kubernetes API, based on your own API that you define.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Custom controllers* are built on the core Kubernetes concepts of resources
    and controllers. Custom controllers allow you to build your own logic by watching
    events from Kubernetes API objects such as namespaces, Deployments, pods, or your
    own CRD. With custom controllers, you can build your CRDs in a declarative way.
    If you consider how the Kubernetes Deployment controller works in a reconciliation
    loop to always maintain the state of the Deployment object to maintain its declarative
    state, this brings the same advantages of controllers to your CRDs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When utilizing the Operator pattern, you can build in automation to operational
    tasks that need to be performed on operational tooling in multiclusters. Let’s
    take the following [Elasticsearch operator](https://oreil.ly/9WvJQ) as an example.
    The Elasticsearch operator can perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Replicas for master, client, and data nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zones for highly available deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volume sizes for master and data nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resizing of cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snapshot for backups of the Elasticsearch cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the operator provides automation for many tasks that you would
    need to perform when managing Elasticsearch, such as automating snapshots for
    backup and resizing the cluster. The beauty of this is that you manage everything
    through familiar Kubernetes objects.
  prefs: []
  type: TYPE_NORMAL
- en: Think about how you can take advantage of different operators like the `prometheus-operator`
    in your environment and also how you can build your own custom operator to offload
    common operational tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The GitOps Approach to Managing Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*GitOps* was popularized by the folks at Weaveworks, and the idea and fundamentals
    were based on their experience of running Kubernetes in production. GitOps takes
    the concepts of the software development life cycle and applies them to operations.
    With GitOps, your Git repository becomes your source of truth, and your cluster
    is synchronized to the configured Git repository. For example, if you update a
    Kubernetes Deployment manifest, those configuration changes are automatically
    reflected in the cluster state.'
  prefs: []
  type: TYPE_NORMAL
- en: By using this method, you can make it easier to maintain multiclusters that
    are consistent and avoid configuration drift across the fleet. GitOps allows you
    to declaratively describe your clusters for multiple environments and drives to
    maintain that state for the cluster. The practice of GitOps can apply to both
    application delivery and operations, but in this chapter, we focus on using it
    to manage clusters and operational tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Weaveworks Flux was one of the first tools to enable the GitOps approach, and
    it’s the tool we will use throughout the rest of the chapter. There are many new
    tools that have been released into the cloud native ecosystem that are worth a
    look, such as Argo CD, from the folks at Intuit, which has also been widely adopted
    for the GitOps approach.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll get into a deeper dive of utilizing a GitOps model in [Chapter 18](ch18.html#gitops),
    but the following provides a quick glance at the benefit of utilizing GitOps for
    cluster management.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-2](#gitops_workflow) presents a representation of a GitOps workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![GitOps workflow](assets/kbp2_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. GitOps workflow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, let’s get Flux set up in your cluster and get a repository synchronized
    to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You now need to change the Deployment manifest to configure it with your forked
    repo from [Chapter 5](ch05.html#continuous_integration_testing_and_deployment).
    Modify the following line in the Deployment file to match your forked GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify the following line with your Git repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, go ahead and deploy Flux to your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When Flux installs, it creates an SSH key so that it can authenticate with the
    Git repository. Use the Flux command-line tool to retrieve the SSH key so that
    you can configure access to your forked repository; first, you need to install
    `fluxctl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For Linux Snap Packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For all other packages, you can find the [latest binaries here](https://oreil.ly/4TAx5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Open GitHub, navigate to your fork, go to Setting > “Deploy keys,” click “Add
    deploy key,” give it a Title, select the “Allow write access” checkbox, paste
    the Flux public key, and then click “Add key.” See the [GitHub documentation](https://oreil.ly/Oet57)
    for more information on how to manage deploy keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you view the Flux logs, you should see that it is synchronizing with
    your GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After you see that it’s synchronizing with your GitHub repository, you should
    see that the Elasticsearch, Prometheus, Redis, and frontend pods are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With this example complete, you should be able to see how easy it is for you
    to synchronize your GitHub repository state with your Kubernetes cluster. This
    makes managing the multiple operational tools in your cluster much easier, because
    multiple clusters can synchronize with a single repository and you avoid the situation
    of having snowflake clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Multicluster Management Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with multiple clusters, using `kubectl` can immediately become
    confusing because you need to set different contexts to manage the different clusters.
    Two tools that you will want to install right away when dealing with multiple
    clusters are *kubectx* and *kubens*, which allow you to easily change between
    multiple contexts and namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you need a full-fledged multicluster management tool, there are a few
    within the Kubernetes ecosystem to look at for managing multiple clusters. Following
    is a summary of some of the more popular tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Rancher](https://oreil.ly/8qGNh)'
  prefs: []
  type: TYPE_NORMAL
- en: Rancher centrally manages multiple Kubernetes clusters in a centrally managed
    UI. It monitors, manages, backs up, and restores Kubernetes clusters across on-premises,
    cloud, and hosted Kubernetes setups. It also has tools for controlling applications
    deployed across multiple clusters and provides operational tooling.
  prefs: []
  type: TYPE_NORMAL
- en: '[Open Cluster Management](https://oreil.ly/HUv5k) (OCM)'
  prefs: []
  type: TYPE_NORMAL
- en: OCM is a community-driven project focused on multicluster and multicloud scenarios
    for Kubernetes apps. It provides cluster registration, workload distribution,
    and dynamic placement of policies and workloads.
  prefs: []
  type: TYPE_NORMAL
- en: '[Gardener](https://oreil.ly/fElD5)'
  prefs: []
  type: TYPE_NORMAL
- en: Gardener takes a different approach to multicluster management in that it utilizes
    Kubernetes primitives to provide Kubernetes as a Service to your end users. It
    provides support for all major cloud vendors and was developed by the folks at
    SAP. This solution is geared to users who are building a Kubernetes as a Service
    offering.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes first introduced Federation v1 in Kubernetes 1.3, and it has since
    been deprecated in lieu of Federation v2\. Federation v1 set out to help with
    the distribution of applications to multiple clusters. Federation v1 was built
    utilizing the Kubernetes API and heavily relied on Kubernetes annotations, which
    imposed some problems in its design. The design was tightly coupled to the core
    Kubernetes API, which made Federation v1 quite monolithic. At the time, the design
    decisions were probably not bad choices, but they were built on the primitives
    that were available. The introduction of Kubernetes CRDs allowed a different way
    of thinking about how Federation could be designed.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Multiple Clusters Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following best practices when managing multiple Kubernetes clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Limit the blast radius of your clusters to ensure cascading failures don’t have
    a bigger impact on your applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have regulatory concerns such as PCI, HIPPA, or HiTrust, think about
    utilizing multiclusters to ease the complexity of mixing these workloads with
    general workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If hard multitenancy is a business requirement, workloads should be deployed
    to a dedicated cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If multiple regions are needed for your applications, utilize a Global Load
    Balancer to manage traffic between clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can break out specialized workloads such as HPC into their own individual
    clusters to ensure that the specialized needs for the workloads are met.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re deploying workloads that will be spread across multiple regional datacenters,
    first ensure there is a data replication strategy for the workload. Multiple clusters
    across regions can be easy, but replicating data across regions can be complicated,
    so ensure there is a sound strategy to handle asynchronous and synchronous workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize Kubernetes operators like the `prometheus-operator` or Elasticsearch
    operator to handle automated operational tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When designing your multicluster strategy, also consider how you will implement
    service discovery and networking between clusters. Service mesh tools like HashiCorp’s
    Consul or Istio can help with networking across clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure that your CD strategy can handle multiple rollouts between regions or
    multiple clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigate utilizing a GitOps approach to managing multiple cluster operational
    components to ensure consistency between all clusters in your fleet. The GitOps
    approach doesn’t work for everyone’s environment, but you should at least investigate
    it to ease the operational burden of multicluster environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed different strategies for managing multiple Kubernetes
    clusters. It’s important to think about your needs at the outset and whether those
    needs match a multicluster topology. The first scenario to think about is whether
    you truly need *hard* multitenancy because this will automatically require a multicluster
    strategy. If you don’t, consider your compliance needs and whether you have the
    operational capacity to consume the overhead of multicluster architectures. Finally,
    if you’re going with more, smaller clusters, ensure that you automate their delivery
    and management to reduce the operational burden.
  prefs: []
  type: TYPE_NORMAL
