- en: Chapter 30\. Image Builder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a general-purpose orchestration engine, suitable not only for
    running applications but also for building container images. The *Image Builder*
    pattern explains why it makes sense to build the container images within the cluster
    and what techniques exist today for creating images within Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the patterns in this book so far have been about operating applications
    on Kubernetes. You’ve learned how to develop and prepare applications to be good
    cloud native citizens. However, what about *building* the application itself?
    The classic approach is to build container images outside the cluster, push them
    to a registry, and refer to them in the Kubernetes Deployment descriptors. However,
    building within the cluster has several advantages.
  prefs: []
  type: TYPE_NORMAL
- en: If your company policies allow, having only one cluster for everything is advantageous.
    Building and running applications in one place can considerably reduce maintenance
    costs. It also simplifies capacity planning and reduces platform resource overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, continuous integration (CI) systems like Jenkins are used to build
    images. Building with a CI system is a scheduling problem for efficiently finding
    free computing resources for build jobs. At the heart of Kubernetes is a highly
    sophisticated scheduler that is a perfect fit for this kind of scheduling challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Once we move to continuous delivery (CD), where we transition from *building*
    images to *running* containers, if the build happens within the same cluster,
    both phases share the same infrastructure and ease transition. For example, let’s
    assume that a new security vulnerability is discovered in a base image used for
    all applications. As soon as your team has fixed this issue, you have to rebuild
    all the application images that depend on this base image and update your running
    applications with the new image. When implementing this *Image Builder* pattern,
    the cluster knows both—the build of an image and its deployment—and can automatically
    do a redeployment if a base image changes. In [“OpenShift Build”](#image-builder-openshift),
    we’ll see how OpenShift implements such automation.
  prefs: []
  type: TYPE_NORMAL
- en: Having seen the benefits of building images on the platform, let’s look at what
    techniques exist for creating images in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of 2023, a whole zoo of in-cluster container image-build techniques exists.
    While all target the same goal of building images, each tool adds a twist, making
    it unique and suitable for specific situations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 30-1](#img-image-builder-levels) contains the essential image-building
    techniques as of 2023 for building container images within a Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![kup2 3001](assets/kup2_3001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 30-1\. Container image builds within Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This chapter contains a brief overview of most of these techniques. You can
    find more details about these tools by following the links in [“More Information”](#image-builder-more-information).
    Please note that while many of the tools described here are matured and used in
    production projects, there are no guarantees that some of those projects still
    exist when you read these lines. Before using one, you should check whether the
    project is still alive and supported.
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorizing these tools is not straightforward as they are partly overlapping
    or dependent on one another. Each of these tools has a unique focus, but for in-cluster
    builds, we can identify these high-level categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Container image builder
  prefs: []
  type: TYPE_NORMAL
- en: These tools create container images within the cluster. There is some overlap
    of these tools, and they vary, but all of them can run without privileged access.
    You can also run these tools outside the cluster as CLI programs. The sole purpose
    of these builders is to create a container image, but they don’t care about application
    redeployments.
  prefs: []
  type: TYPE_NORMAL
- en: Build orchestration
  prefs: []
  type: TYPE_NORMAL
- en: These tools operate on a higher level of abstraction and eventually trigger
    the container image builder for creating images. They also support build-related
    tasks like updating the deployment descriptors after the image has been built.
    CI/CD systems, as described previously, are typical examples of orchestrators.
  prefs: []
  type: TYPE_NORMAL
- en: Container Image Builder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the essential prerequisites for building images from within a cluster
    is creating images without having privileged access to the node host. Various
    tools exists that fulfill this prerequisite, and they can be roughly categorized
    according to how the container image is specified and built.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile-Based builders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following builders are based on the well-known Dockerfile format for defining
    the build instructions. All of them are compatible on a Dockerfile level, and
    they either work completely without talking to a background daemon or talk via
    a REST API remotely with a build process that is running in a nonprivileged mode:'
  prefs: []
  type: TYPE_NORMAL
- en: Buildah and Podman
  prefs: []
  type: TYPE_NORMAL
- en: Buildah and its sister Podman are potent tools for building OCI-compliant images
    without a Docker daemon. They create images locally within the container before
    pushing them to an image registry. Buildah and Podman overlap in functionality,
    with Buildah focusing on building container images (though Podman can also create
    container images by wrapping the Buildah API). The difference is shaped more clearly
    in this [README](https://oreil.ly/kSgHk).
  prefs: []
  type: TYPE_NORMAL
- en: Kaniko
  prefs: []
  type: TYPE_NORMAL
- en: Kaniko is one backbone of the Google Cloud Build service and is deliberately
    targeted for running as a build container in Kubernetes. Within the build container,
    Kaniko still runs with UID 0, but the Pod holding the container itself is nonprivileged.
    This requirement prevents the usage of Kaniko in clusters that disallow running
    as a root user in a container, like in OpenShift. We see Kaniko in action in [“Build
    Pod”](#image-builder-build-pod).
  prefs: []
  type: TYPE_NORMAL
- en: BuildKit
  prefs: []
  type: TYPE_NORMAL
- en: Docker extracted its build engine into a separate project, BuildKit, which can
    be used independently of Docker. It inherits from Docker its client-server architecture
    with a BuildKit daemon running in the background, waiting for build jobs. Usually,
    this daemon runs directly in the container that triggers the build, but it can
    also run in a Kubernetes cluster to allow distributed rootless builds. BuildKit
    introduces a Low-Level Build (LLB) definition format supported by multiple frontends.
    LLB allows complex build graphs and can be used for arbitrary complex build definitions.
    BuildKit also supports features that go beyond the original Dockerfile specification.
    In addition to Dockerfiles, BuildKit can use other frontends to define the container
    image’s content via LLB.
  prefs: []
  type: TYPE_NORMAL
- en: Multilanguage builders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many developers care only that their application gets packaged as container
    images and not so much about how this is done. To cover this use case, multilanguage
    builders exist to support many programming platforms. They detect an existing
    project, like a Spring Boot application or generic Python build, and select an
    opinionated image build flow accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '*Buildpacks* have been around since 2012 and were initially introduced by Heroku
    to allow you to push developer’s code directly to their platform. Cloud Foundry
    picked up that idea and created a fork of buildpacks that eventually led to the
    infamous `cf push` idiom that many considered the gold standard of Platform as
    a Service (PaaS). In 2018, the different forks of Buildpacks united under the
    umbrella of the CNCF and are now known as *Cloud Native Buildpacks* (CNB). Besides
    individual buildpacks for different programming languages, CNB introduce a lifecycle
    for transforming source code to executable container images.'
  prefs: []
  type: TYPE_NORMAL
- en: The lifecycle can roughly be divided into three main phases:^([1](ch30.html#idm45902080790064))
  prefs: []
  type: TYPE_NORMAL
- en: In the *detect* phase, CNB iterate over a list of configured buildpacks. Each
    buildpack can decide whether it fits for the given source code. For example, a
    Java-based buildpack will raise its hand when it detects a Maven *pom.xml*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All buildpacks that survived the detect phase will be called in the *build*
    phase to provide their part for the final, possibly compiled artifact. For example,
    a buildpack for a Node.js application calls `npm install` to fetch all required
    dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last step in the CNB lifecycle is an *export* to the final OCI image that
    gets pushed to a registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNB target two personas. The primary audience includes *Developers* who want
    to deploy their code onto Kubernetes or any other container-based platform. The
    other is *Buildpack Authors*, who create individual buildpacks and group them
    into so-called *builders*. You can choose from a list of prefactored buildpacks
    and builders or create your own for you and your team. Developers can then pick
    up those buildpacks by referencing them when running the CNB lifecycle on their
    source code. Several tools are available for executing this lifecycle; you’ll
    find a complete list at the [Cloud Native Buildpacks site](https://oreil.ly/B07Et).
  prefs: []
  type: TYPE_NORMAL
- en: 'For using CNB within a Kubernetes cluster, the following tasks are helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pack` is a CLI command to configure and execute the CNB lifecycle locally.
    It requires access to an OCI container runtime engine like Docker or Podman to
    run Builder images that hold the list of buildpacks to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CI steps like Tekton build tasks or GitHub actions that call the lifecycle directly
    from a configured Builder image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kpack` comes with an Operator that allows you to configure and run buildpacks
    within a Kubernetes cluster. All the core concepts of CNB, like Builder or Buildpacks,
    are reflected directly as CustomResourceDefinitions. `kpack` is not yet part of
    the CNB project itself, but as of 2023 is about to be absorbed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many other platforms and projects have adopted CNB as their build platform of
    choice. For example, Knative Functions use CNB under the hood to transform Function
    code to container images before they get deployed as Knative services.
  prefs: []
  type: TYPE_NORMAL
- en: '*OpenShift’s Source-to-Image* (S2I) is another opinionated building method
    with builder images. S2I takes you directly from your application’s source code
    to executable container images. We will look closely at S2I in [“OpenShift Build”](#image-builder-openshift).'
  prefs: []
  type: TYPE_NORMAL
- en: Specialized builders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, specialized builders with an opinionated way of creating images exist
    for specific situations. While their scope is narrow, their strong opinion allows
    for a highly optimized build flow that increases flexibility and decreases build
    times. All these builders perform a rootless build. They create the container
    image without running arbitrary commands as with a Dockerfile `RUN` directive.
    They create the image layers locally with the application artifacts and push them
    directly to a container image registry:'
  prefs: []
  type: TYPE_NORMAL
- en: Jib
  prefs: []
  type: TYPE_NORMAL
- en: Jib is a pure Java library and build extension that integrates nicely with Java
    build tools like Maven or Gradle. It creates separate image layers directly for
    the Java build artifacts, its dependencies, and other static resources to optimize
    image rebuild times. Like the other builders, it speaks directly with a container
    image registry for the resulting images.
  prefs: []
  type: TYPE_NORMAL
- en: ko
  prefs: []
  type: TYPE_NORMAL
- en: For creating images from Golang sources, ko is a great tool. It can directly
    create images from remote Git repositories and update Pod specifications to point
    to the image after it has been built and pushed to a registry.
  prefs: []
  type: TYPE_NORMAL
- en: Apko
  prefs: []
  type: TYPE_NORMAL
- en: Apko is a unique builder that uses Alpine’s Apk packages as building blocks
    instead of Dockerfile scripts. This strategy allows for the easy reuse of building
    blocks when creating multiple similar images.
  prefs: []
  type: TYPE_NORMAL
- en: This list is only a selection of the many specialized build techniques. All
    of them have a very narrow scope of what they can build. The advantage of this
    opinionated approach is that they can optimize build time and image size because
    they know precisely about the domain in which they operate and can make strong
    assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen some ways to build container images, let’s jump one abstraction
    level higher and see how we can embed the actual build in a broader context.
  prefs: []
  type: TYPE_NORMAL
- en: Build Orchestrators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Build orchestrators are CI and CD platforms like Tekton, Argo CD, or Flux. Those
    platforms cover your application’s entire automated management lifecycle, including
    building, testing, releasing, deploying, security scanning, and much more. There
    are excellent books that cover those platforms and bring it all together, so we
    won’t go into the details here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to general-purpose CI and CD platforms, we can use more specialized
    orchestrators to create container images:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift builds
  prefs: []
  type: TYPE_NORMAL
- en: One of the oldest and most mature ways of building images in a Kubernetes cluster
    is the *OpenShift build* subsystem. It allows you to build images in several ways.
    We take a closer look at the OpenShift way of building images in [“OpenShift Build”](#image-builder-openshift).
  prefs: []
  type: TYPE_NORMAL
- en: kbld
  prefs: []
  type: TYPE_NORMAL
- en: 'kbld is part of Carvel, a toolset for building, configuring, and deploying
    on Kubernetes. kbld is responsible for building containers with one of the builder
    technologies we described in [“Container Image Builder”](#image-builder-builders)
    and updating resource descriptors with a reference to the images that have been
    built. The technique for updating the YAML files is very similar to how ko works:
    kbld looks for `image` fields and sets their values to the coordinates of the
    freshly built image.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Job
  prefs: []
  type: TYPE_NORMAL
- en: You can also use standard Kubernetes Jobs for triggering builds with any of
    the image builders from [“Container Image Builder”](#image-builder-builders).
    Jobs are described in detail in [Chapter 7, “Batch Job”](ch07.html#BatchJob).
    Such a Job wraps a build Pod specification for defining the runtime parts. The
    build Pod picks up the source code from a remote source repository and uses one
    of the in-cluster builders to create the appropriate image. We’ll see such a Pod
    in action in [“Build Pod”](#image-builder-build-pod).
  prefs: []
  type: TYPE_NORMAL
- en: Build Pod
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To carve out the essential ingredients of typical in-cluster builds, let’s start
    minimally and use a Kubernetes Pod for performing a complete build and deploy
    cycle. These build steps are illustrated in [Figure 30-2](#img-build-pod).
  prefs: []
  type: TYPE_NORMAL
- en: '![kup2 3002](assets/kup2_3002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 30-2\. In-cluster container image build with a build Pod
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following tasks are representative of all build orchestrators and cover
    all aspects of creating container images:'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the source code from a given remote Git repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a compiled language, perform a local build within the container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the application with one of the techniques described in [“Container Image
    Builder”](#image-builder-builders).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push the image to a remote image registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, update a deployment with the new image reference, which will trigger
    a redeployment of the application following the strategies described in [Chapter 3,
    “Declarative Deployment”](ch03.html#DeclarativeDeployment).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The build Pod in our example uses init containers as described in [Chapter 15,
    “Init Container”](ch15.html#InitContainer), to ensure that the build steps are
    running one after the other. In a real-world scenario, you would use a CI system
    like Tekton to specify and execute these tasks sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: The complete build Pod definition is shown in [Example 30-1](#ex-image-builder-build-pod).
  prefs: []
  type: TYPE_NORMAL
- en: Example 30-1\. Build Pod using Kaniko
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_builder_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Init container for fetching the source code from a remote Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_builder_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Volume in which to store the source code.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_builder_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Kaniko as build container, storing the created image as a reference in the shared
    workspace.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_builder_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Build is running unprivileged.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_image_builder_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Secret for pushing to Docker Hub registry mounted at a well-known path so that
    Kaniko can find it.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_image_builder_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Mounting shared workspace for getting the source code.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_image_builder_CO1-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Container for updating the deployment `random` with the image reference from
    the Kaniko build.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_image_builder_CO1-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Secret volume with the Docker Hub credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_image_builder_CO1-9)'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of a shared volume as an empty directory on the node’s local filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](assets/10.png)](#co_image_builder_CO1-10)'
  prefs: []
  type: TYPE_NORMAL
- en: ServiceAccount that is allowed to patch a Deployment resource.
  prefs: []
  type: TYPE_NORMAL
- en: '[![11](assets/11.png)](#co_image_builder_CO1-11)'
  prefs: []
  type: TYPE_NORMAL
- en: Never restart this Pod.
  prefs: []
  type: TYPE_NORMAL
- en: This example is quite involved, so let’s break it down into three main parts.
  prefs: []
  type: TYPE_NORMAL
- en: First, before being able to build a container image, the application code needs
    to be fetched. In most cases, the source code is picked up from a remote Git repository,
    but other techniques are available. For development purposes, it is convenient
    to get the source code from your local machine so that you don’t have to go over
    a remote source repository and mess up your commit history with triggering commits.
    Because the build happens within a cluster, that source code must be uploaded
    somehow to your build container. Another possibility is to distribute the source
    code packaged in a container image and distribute it via a container image registry.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 30-1](#ex-image-builder-build-pod), we use an init container to
    fetch the source code from our source Git repository and store it in a shared
    Pod volume `source` of type `emptyDir` so that it can later be picked up by the
    build process container.
  prefs: []
  type: TYPE_NORMAL
- en: Second, after the application code is retrieved, the actual build happens. In
    our example, we use [Kaniko](https://oreil.ly/SQeYa), which uses a regular Dockerfile
    and can run entirely unprivileged. We again use an init container to ensure that
    the build starts only after the source code has been fully fetched. The container
    image is created locally on disk, and we also configure Kaniko to push the resulting
    image to a remote Docker registry.
  prefs: []
  type: TYPE_NORMAL
- en: The credentials for pushing to the registry are picked up from a Kubernetes
    Secret. We describe Secrets in detail in [Chapter 20, “Configuration Resource”](ch20.html#ConfigurationResource).
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, for the particular case of authentication against a Docker registry,
    we have direct support from `kubectl` for creating such a secret that stores this
    configuration in a well-known format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For [Example 30-1](#ex-image-builder-build-pod), such a secret is mounted into
    the build container under a given path so that Kaniko can pick it up when pushing
    the created image. In [Chapter 25, “Secure Configuration”](ch25.html#SecureConfiguration),
    we explain how such a secret can be stored securely so that it can’t be forged.
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to update an existing Deployment with the newly created image.
    This task is now performed in the actual application container of the Pod.^([2](ch30.html#idm45902080225920))
    The referenced image is from our example repository and contains just a `kubectl`
    binary that patches the specified Deployment with the new image name with the
    following call, shown in [Example 30-2](#ex-image-builder-deploy).
  prefs: []
  type: TYPE_NORMAL
- en: Example 30-2\. Update image field in Deployment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_builder_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Pickup image name stored by the previous build step in the file */opt/image-name*.
    This file is provided as the first argument to this script.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_builder_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: JSON path to update the Pod spec with the new image reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_builder_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Patch the deployment given as the second argument (`random` in our example)
    and trigger a new rollout.
  prefs: []
  type: TYPE_NORMAL
- en: The Pod’s assigned ServiceAccount `build-pod` is set up so it can write to this
    Deployment. Assigning permissions to a ServiceAccount is described fully in [Chapter 26,
    “Access Control”](ch26.html#AccessControl). When the image reference is updated
    in the Deployment, a rollout as described in [Chapter 3, “Declarative Deployment”](ch03.html#DeclarativeDeployment),
    is performed.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the fully working setup in the book’s [example repository](https://oreil.ly/jVF6h).
    The build Pod is the simplest way to orchestrate an in-cluster build and redeployment.
    As mentioned, it is meant for illustrative purposes only.
  prefs: []
  type: TYPE_NORMAL
- en: For real-world use cases, you should use a CI/CD solution like Tekton or a whole
    build orchestration platform like OpenShift Build, which we describe now.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift Build
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Red Hat OpenShift is an enterprise distribution of Kubernetes. Besides supporting
    everything Kubernetes supports, it adds a few enterprise-related features like
    an integrated container image registry, single sign-on support, and a new user
    interface, and it also adds a native image building capability to Kubernetes.
    [OKD](https://www.okd.io) is the upstream open source community edition distribution
    that contains all the OpenShift features.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenShift build was the first cluster-integrated way of directly building images
    managed by Kubernetes. It supports multiple strategies for building images:'
  prefs: []
  type: TYPE_NORMAL
- en: Source-to-Image (S2I)
  prefs: []
  type: TYPE_NORMAL
- en: Takes the source code of an application and creates the runnable artifact with
    the help of a language-specific S2I builder image and then pushes the images to
    the integrated registry.
  prefs: []
  type: TYPE_NORMAL
- en: Docker builds
  prefs: []
  type: TYPE_NORMAL
- en: Use a Dockerfile plus a context directory and creates an image as a Docker daemon
    would do.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline builds
  prefs: []
  type: TYPE_NORMAL
- en: Map build-to-build jobs of an internally managed Tekton by allowing the user
    to configure a Tekton pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Custom builds
  prefs: []
  type: TYPE_NORMAL
- en: Give you full control over how you create your image. Within a custom build,
    you have to create the image on your own within the build container and push it
    to a registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input for doing the builds can come from different sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Git
  prefs: []
  type: TYPE_NORMAL
- en: Repository specified via a remote URL from where the source is fetched.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile
  prefs: []
  type: TYPE_NORMAL
- en: A Dockerfile that is directly stored as part of the build configuration resource.
  prefs: []
  type: TYPE_NORMAL
- en: Image
  prefs: []
  type: TYPE_NORMAL
- en: Another container image from which files are extracted for the current build.
    This source type allows for *chained builds*, as shown in [Example 30-4](#ex-image-builder-chained).
  prefs: []
  type: TYPE_NORMAL
- en: Secret
  prefs: []
  type: TYPE_NORMAL
- en: Resource for providing confidential information for the build.
  prefs: []
  type: TYPE_NORMAL
- en: Binary
  prefs: []
  type: TYPE_NORMAL
- en: Source to provide all input from the outside. This input has to be provided
    when starting the build.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of which input sources we can use in which way depends on the build
    strategy. *Binary* and *Git* are mutually exclusive source types. All other sources
    can be combined or used on a standalone basis. We will see later in [Example 30-3](#ex-image-builder-s2i)
    how this works.
  prefs: []
  type: TYPE_NORMAL
- en: All the build information is defined in a central resource object called BuildConfig.
    We can create this resource either by directly applying it to the cluster or by
    using the CLI tool `oc`, which is the OpenShift equivalent of `kubectl`. `oc`
    supports build-specific commands for defining and triggering a build.
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at BuildConfig, we need to understand two additional concepts
    specific to OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: An ImageStream is an OpenShift resource that references one or more container
    images. It is a bit similar to a Docker repository, which also contains multiple
    images with different tags. OpenShift maps an actual tagged image to an ImageStreamTag
    resource so that an ImageStream (repository) has a list of references to ImageStreamTags
    (tagged images). Why is this extra abstraction required? Because it allows OpenShift
    to emit events when an image is updated in the registry for an ImageStreamTag.
    Images are created during builds or when an image is pushed to the OpenShift internal
    registry. That way, the build or deployment controllers can listen to these events
    and trigger a new build or start a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To connect an ImageStream to a deployment, OpenShift uses the DeploymentConfig
    resource instead of the Kubernetes Deployment resource, which can only use container
    image references directly. However, you can still use vanilla Deployment resources
    in OpenShift with ImageStreams by adding some [OpenShift-specific annotations](https://oreil.ly/Tu9GA).
  prefs: []
  type: TYPE_NORMAL
- en: The other concept is a *trigger*, which we can consider as a kind of listener
    to events. One possible trigger is `imageChange`, which reacts to the event published
    because of an ImageStreamTag change. As a reaction, such a trigger can, for example,
    cause the rebuild of another image or redeployment of the Pods using this image.
    You can read more about triggers and the kinds of triggers available in addition
    to the `imageChange` trigger in the [OpenShift documentation](https://oreil.ly/J4qTQ).
  prefs: []
  type: TYPE_NORMAL
- en: Source-to-Image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s have a quick look at what an S2I builder image looks like. We won’t go
    into too many details here, but an S2I builder image is a standard container image
    that contains a set of S2I scripts. It is very similar to Cloud Native Buildpacks
    but with a much simpler lifecycle that knows two mandatory commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assemble`'
  prefs: []
  type: TYPE_NORMAL
- en: The script that gets called when the build starts. Its task is to take the source
    given by one of the configured inputs, compile it if necessary, and copy the final
    artifacts to the proper locations.
  prefs: []
  type: TYPE_NORMAL
- en: '`run`'
  prefs: []
  type: TYPE_NORMAL
- en: Used as an entry point for this image. OpenShift calls this script when it deploys
    the image. This run script uses the generated artifacts to deliver the application
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can also script to provide a usage message, saving the generated
    artifacts for so-called *incremental builds* that are accessible by the `assemble`
    script in a subsequent build run, or add some sanity checks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a closer look at an S2I build in [Figure 30-3](#img-image-builder-s2i).
    An S2I build has two ingredients: a builder image and a source input. Both are
    brought together by the S2I build system when a build is started—either because
    a trigger event was received or because we started it manually. When the build
    image has finished by, for example, compiling the source code, the container is
    committed to an image and pushed to the configured ImageStreamTag. This image
    contains the compiled and prepared artifacts, and the image’s `run` script is
    set as the entry point.'
  prefs: []
  type: TYPE_NORMAL
- en: '![kup2 3003](assets/kup2_3003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 30-3\. S2I build with Git source as input
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Example 30-3](#ex-image-builder-s2i) shows a simple Java S2I build with a
    Java S2I image. This build takes a source, the builder image, and produces an
    output image that is pushed to an ImageStreamTag. It can be started manually via
    `oc start-build` or automatically when the builder image changes.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 30-3\. S2I Build using a Java builder image
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_builder_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Reference to the source code to fetch; in this case, pick it up from GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_builder_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`sourceStrategy` switches to S2I mode, and the builder image is picked up directly
    from Docker Hub.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_builder_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The ImageStreamTag to update with the generated image. It’s the committed builder
    container after the `assemble` script has run.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_builder_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Rebuild automatically when the source code in the repository changes.
  prefs: []
  type: TYPE_NORMAL
- en: S2I is a robust mechanism for creating application images, and it is more secure
    than plain Docker builds because the build process is under full control of trusted
    builder images. However, this approach still has some drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: For complex applications, S2I can be slow, especially when the build needs to
    load many dependencies. Without any optimization, S2I loads all dependencies afresh
    for every build. In the case of a Java application built with Maven, there is
    no caching as when doing local builds. To avoid downloading half of the internet
    again and again, it is recommended that you set up a cluster-internal Maven repository
    that serves as a cache. The builder image then has to be configured to access
    this common repository instead of downloading the artifacts from remote repositories.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to decrease the build time is to use *incremental builds* with S2I,
    which allows you to reuse artifacts created or downloaded in a previous S2I build.
    However, a lot of data is copied over from the previously generated image to the
    current build container, and the performance benefits are typically not much better
    than using a cluster-local proxy that holds the dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Another drawback of S2I is that the generated image also contains the whole
    build environment.^([3](ch30.html#idm45902079913104)) This fact increases not
    only the size of the application image but also the surface for a potential attack,
    as builder tools can become vulnerable too.
  prefs: []
  type: TYPE_NORMAL
- en: To get rid of unneeded builder tools like Maven, OpenShift offers *chained builds*,
    which take the result of an S2I build and create a slim runtime image. We look
    at chained builds in [“Chained builds”](#image-builder-chained).
  prefs: []
  type: TYPE_NORMAL
- en: Docker builds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenShift also supports Docker builds directly within the cluster. Docker builds
    work by mounting the Docker daemon’s socket directly in the build container, which
    is then used for a `docker build`. The source for a Docker build is a Dockerfile
    and a directory holding the context. You can also use an `Image` source that refers
    an arbitrary image and from which files can be copied into the Docker build context
    directory. As mentioned in the next section, this technique, together with triggers,
    can be used for chained builds.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use a standard multistage Dockerfile to separate the
    build and runtime parts. Our [example repository](https://oreil.ly/mn4vg) contains
    a fully working multistage Docker build example that results in the same image
    as the chained build described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Chained builds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The mechanics of a chained build are shown in [Figure 30-4](#img-image-builder-chained).
    A chained build consists of an initial S2I build, which creates the runtime artifact
    such as a binary executable. This artifact is then picked up from the generated
    image by a second build, typically a Docker build.
  prefs: []
  type: TYPE_NORMAL
- en: '![kup2 3004](assets/kup2_3004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 30-4\. Chained build with S2I for compiling and Docker build for application
    image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Example 30-4](#ex-image-builder-chained) shows the setup of this second build
    config, which uses the JAR file generated in [Example 30-3](#ex-image-builder-s2i).
    The image that is eventually pushed to the ImageStream `random-generator-runtime`
    can be used in a DeploymentConfig to run the application.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The trigger used in [Example 30-4](#ex-image-builder-chained) monitors the result
    of the S2I build. This trigger causes a rebuild of this runtime image whenever
    we run an S2I build so that both ImageStreams are always in sync.
  prefs: []
  type: TYPE_NORMAL
- en: Example 30-4\. Docker build for creating the application image
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_builder_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Image source references the ImageStream that contains the result of the S2I
    build run and selects a directory within the image that contains the compiled
    JAR archive.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_builder_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile source for the Docker build that copies the JAR archive from the
    ImageStream generated by the S2I build.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_builder_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The `strategy` selects a Docker build.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_builder_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Rebuild automatically when the S2I result ImageStream changes—after a successful
    S2I run to compile the JAR archive.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_image_builder_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Register listener for image updates, and do a redeploy when a new image has
    been added to the ImageStream.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full example with installation instructions in our [example
    repository](https://oreil.ly/mn4vg).
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, OpenShift build, along with its most prominent S2I mode, is one
    of the oldest and most mature ways to safely build container images within an
    OpenShift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have seen two ways to build container images within a cluster. The plain
    build Pod illustrates the most crucial tasks that every build system needs to
    execute: fetching the source code, creating a runnable artifact from your source
    code, creating a container image containing the application’s artifacts, pushing
    this image to an image registry, and finally updating any deployments so that
    it picks up the newly created image from that registry. This example is not meant
    for direct production use as it contains too many manual steps that existing build
    orchestrators cover more effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenShift build system nicely demonstrates one of the main benefits of building
    and running an application in the same cluster. With OpenShift’s ImageStream triggers,
    you can connect multiple builds and redeploy your application if a build updates
    your application’s container image. Better integration between build and deployment
    is a step forward to the holy grail of CD. OpenShift builds with S2I are a proven
    and established technology, but S2I is usable only when using the OpenShift distribution
    of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The landscape of in-cluster build tools as of 2023 is rich and contains many
    exciting techniques that partly overlap. As a result, you can expect some consolidation,
    but new tooling will arise over time, so we’ll see more implementations of the
    *Image Builder* pattern emerge.
  prefs: []
  type: TYPE_NORMAL
- en: More Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Image Builder Example](https://oreil.ly/39C_l)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image Builders:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Buildah](https://oreil.ly/AY7ml)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaniko](https://oreil.ly/SQeYa)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Is BuildKit?](https://oreil.ly/N28Dn)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building Multi-Architecture Images with Buildpacks](https://oreil.ly/sBth1)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Jib](https://oreil.ly/jy9KH)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pack](https://oreil.ly/iILs7)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kpack](https://oreil.ly/LpGvB)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ko](https://oreil.ly/9hARS)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Apko: A Better Way to Build Containers?](https://oreil.ly/5227Q)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Build Orchestrators:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenShift Builds](https://oreil.ly/dIii_)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kbld](https://oreil.ly/Uako8)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multistage Build](https://oreil.ly/8-zKu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chaining S2I Builds](https://oreil.ly/3MPXZ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build Triggers Overview](https://oreil.ly/jcFx7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Source-to-Image Specification](https://oreil.ly/0B2cc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Incremental S2I Builds](https://oreil.ly/YbUen)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building Container Images in Kubernetes: It’s Been a Journey!](https://oreil.ly/0ijOJ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build Multi-Architecture Container Images Using Kubernetes](https://oreil.ly/0Neln)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best Practices for Running Buildah in a Container](https://oreil.ly/8E76m)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch30.html#idm45902080790064-marker)) CNB cover more phases. The entire
    lifecycle is explained on the [Buildpacks site](https://oreil.ly/8l69J).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch30.html#idm45902080225920-marker)) We could have also chosen an init
    container again here and used a no-op application container, but since the application
    containers start only after all init containers have been finished, it doesn’t
    matter much where we put the container. In all cases, the three specified containers
    run after one another.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch30.html#idm45902079913104-marker)) This is different from Cloud Native
    Buildpacks, which use a separate runtime image in its stack for carrying the final
    artifact.
  prefs: []
  type: TYPE_NORMAL
