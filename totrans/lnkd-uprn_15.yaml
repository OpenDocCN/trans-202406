- en: Chapter 15\. Debugging Linkerd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve come this far, you understand how valuable a tool Linkerd can be for
    a platform to provide. It secures your apps, provides powerful insights into those
    applications, and can account for underlying application and network issues by
    making your connections more reliable. In this chapter, we’re going to look at
    what to do when you need to troubleshoot Linkerd itself.
  prefs: []
  type: TYPE_NORMAL
- en: In all cases, the first step when diagnosing an issue with Linkerd is to use
    the Linkerd CLI’s built-in health checking tool by running `linkerd check`, as
    we discussed in more detail in [Chapter 6](ch06.html#LUAR_cli). `linkerd check`
    is a very quick way to pinpoint many of the most common issues with Linkerd installations;
    for example, it can immediately diagnose expired certificates, which is the most
    common issue that causes Linkerd outages in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing Data Plane Issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linkerd is somewhat famous for not requiring a ton of hands-on work with the
    data plane; however, it’s still useful to be able to do some basic troubleshooting.
    Many proxy issues end up involving fairly similar sets of solutions.
  prefs: []
  type: TYPE_NORMAL
- en: “Common” Linkerd Data Plane Failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Linkerd is generally a fault-tolerant service mesh, there are a few conditions
    that we see arise more than others. Knowing how to tackle these can be extremely
    helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Pods failing to start
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you run into a situation where injected Pods are failing to start, the first
    step will be to identify exactly where the failure is occurring. This is where
    the `kubectl describe pod` and `kubectl logs` commands can provide a lot of useful
    information.
  prefs: []
  type: TYPE_NORMAL
- en: It’s often most useful to start by describing a failing Pod, to get a sense
    of what Kubernetes believes has happened. For example, did the Linkerd init container
    fail? Did the Pod get killed because its probes never reported ready? This information
    can help you decide which containers to pull logs from, if you need the logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the failing containers belong to the application rather than to Linkerd,
    you’ll likely be best off looking at non-Linkerd-specific potential causes, like
    a failing common dependency, or perhaps Nodes running out of resources. If it’s
    Linkerd containers failing, though, the second step is to understand whether the
    failure is affecting all new Pods or only some new Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: Single Pod failure
  prefs: []
  type: TYPE_NORMAL
- en: If a single Pod is failing, you’ll want to look at the characteristics of that
    Pod. Which containers are failing to start? Is it the proxy, an init container,
    or the application itself? If other Pods are starting normally, it isn’t a systemic
    issue and you’ll need to dive into the particulars of the specific Pod.
  prefs: []
  type: TYPE_NORMAL
- en: All Pods are failing to start
  prefs: []
  type: TYPE_NORMAL
- en: If the failure is happening on all new Pods, you likely have a more serious
    systemic issue. As noted before, the most common reason that all Pods fail to
    start is a certificate issue. The `linkerd check` command will reveal this kind
    of issue immediately, which is why we recommend running it first.
  prefs: []
  type: TYPE_NORMAL
- en: Another possible—though uncommon—issue is that the Linkerd proxy injector isn’t
    running or is unhealthy. Note that when running Linkerd in HA mode, which (as
    discussed in [Chapter 14](ch14.html#LUAR_production_ready)) we recommend for production
    use, Kubernetes will refuse to start any new application Pods until the injector
    is healthy.
  prefs: []
  type: TYPE_NORMAL
- en: A subset of Pods is failing to start
  prefs: []
  type: TYPE_NORMAL
- en: 'If only some new Pods are failing to start, it’s time to begin isolating what
    common factors exist across those Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: Look at the output of `kubectl get pods -o wide` to see if they’ve been scheduled
    on the same Node or Nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Look at the `kubectl describe pod` output: are the init containers failing
    to complete successfully?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re using the Linkerd CNI plugin, you’ll want to check the state of the
    network validator, and you may need to restart the CNI container for that Node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re not using the Linkerd CNI plugin, take a look at the output of `kubectl
    logs` for the Linkerd init container. If it failed to complete successfully, try
    to see what is unique about the Node it’s running on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intermittent proxy errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Intermittent proxy errors can be one of the hardest issues to solve. Any intermittent
    problem with the proxy is necessarily difficult to catch and address. When running
    Linkerd in production, it’s important to build in monitoring to catch errors including:'
  prefs: []
  type: TYPE_NORMAL
- en: Permission denied events
  prefs: []
  type: TYPE_NORMAL
- en: These represent either a dangerous misconfiguration in your platform or a genuine
    threat to your environment. You’ll need to collect and analyze the logs from the
    Linkerd proxies in order to detect these events.
  prefs: []
  type: TYPE_NORMAL
- en: Protocol detection timeouts
  prefs: []
  type: TYPE_NORMAL
- en: Protocol detection, discussed in [Chapter 4](ch04.html#LUAR_meshing_workloads),
    is the process of Linkerd automatically identifying traffic between two Pods.
    It’s an important step that occurs before the proxy can begin sending and receiving
    traffic. On occasion, protocol detection can time out and then fall back to treating
    a connection as a standard TCP connection. That means the proxy will introduce
    an unnecessary 10-second delay, and then the connection will not benefit from
    Linkerd’s ability to do things like request-level routing.
  prefs: []
  type: TYPE_NORMAL
- en: Protocol detection timeout events usually indicate a port that should be marked
    as skipped or opaque (again, this is discussed in [Chapter 4](ch04.html#LUAR_meshing_workloads)).
    In particular, the proxy will never be able to correctly perform protocol detection
    for server-speaks-first protocols. If you’re using Linkerd’s policy resources,
    you have the ability to declare the protocol on a given port. This allows you
    to skip protocol detection entirely and will improve the overall availability,
    as well as security, of your applications.
  prefs: []
  type: TYPE_NORMAL
- en: A proxy may also be unable to handle protocol detection if it is overloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Out of memory events
  prefs: []
  type: TYPE_NORMAL
- en: An out of memory event for the proxy represents a resource allocation issue.
    It’s important to monitor, test, and manage the resource limits for the Linkerd
    proxy. It intercepts and manages the traffic going into and out of your applications,
    and managing its resources is a core responsibility of the platform team.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure you’re providing the proxy with the resources it needs to handle the
    traffic flowing through it, or you’re going to have a bad day.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP errors
  prefs: []
  type: TYPE_NORMAL
- en: In general, Linkerd will reuse persistent TCP connections between your Pods
    and will surface any application-level errors that occur, so if your applications
    have any kind of underlying configuration issues, you should see the errors that
    your application is actually generating.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are times that the Linkerd proxy will itself respond to a request
    with an immediate 502, 503, or 504, and it’s important to understand what can
    cause these:'
  prefs: []
  type: TYPE_NORMAL
- en: 502s
  prefs: []
  type: TYPE_NORMAL
- en: It’s not uncommon for new service mesh users to see an uptick in the occurrence
    of 502 errors when they begin adding applications to the service mesh. This is
    because whenever Linkerd sees a connection error between proxies, it will show
    up as a 502\. If you’re seeing a large number of 502 errors in your environment,
    consult the [Linkerd docs](https://oreil.ly/77p8P) for more troubleshooting steps
    you can take.
  prefs: []
  type: TYPE_NORMAL
- en: 503s and 504s
  prefs: []
  type: TYPE_NORMAL
- en: 503s and 504s show up when a Linkerd proxy finds that requests are overwhelming
    a workload’s ability to respond.
  prefs: []
  type: TYPE_NORMAL
- en: 'Internally, a Linkerd proxy maintains a queue of requests to dispatch. In normal
    operation, an incoming request spends almost no time in the queue: it is queued,
    and the proxy chooses an available endpoint for the request and then immediately
    dispatches it.'
  prefs: []
  type: TYPE_NORMAL
- en: However, suppose there’s a massive flood of incoming requests, far too many
    for the workload to handle. When the queue gets too long, Linkerd begins *load
    shedding*, where any new request that arrives gets an immediate 503 directly from
    the proxy. To stop load shedding, the incoming request rate needs to slow enough
    for the requests in the queue to get dispatched, allowing the queue to shrink.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the pool of endpoints is dynamic: a given endpoint can be removed from
    the pool by circuit breaking, for example. If the pool ever becomes completely
    empty—i.e., there are *no* available endpoints—then the queue enters a state called
    *failfast*. In failfast, all requests in the queue immediately get a 504 response,
    and then Linkerd shifts to load shedding, so new requests again get a 503.'
  prefs: []
  type: TYPE_NORMAL
- en: To get out of failfast, some backend has to become available again. If you got
    into failfast due to circuit breaking marking all your backends unavailable, the
    most likely way you’ll get out is that circuit breaking will allow a probe request
    to go through, it’ll succeed, and then the backend will be marked available again.
    At that point Linkerd can bring the workload out of failfast and things will start
    being handled normally again.
  prefs: []
  type: TYPE_NORMAL
- en: 503 Is Not the Same as 504!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note the different responses there! 504s happen *only* at the point where the
    load balancer enters failfast, while 503s indicate load shedding—which could be
    due to failfast, or could be due to too much traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Proxy Log Levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In normal operation, the Linkerd proxy does not log debugging information. If
    needed, you can change the Linkerd proxy’s log level without needing to restart
    the proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Debug Logging Can Be Expensive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you set too verbose a log level, the proxy will consume dramatically more
    resources, and its performance will degrade. Do not modify the proxy’s log level
    unless you need to, and be sure to reset the log level when you’re not actively
    debugging.
  prefs: []
  type: TYPE_NORMAL
- en: When you need to start debugging, you can turn on debug-level logging as shown
    in [Example 15-1](#EX-troubleshooting-loglevel-debug).
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-1\. Turning on debug logging
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you’ve finished debugging, be sure to turn *off* debug-level logging,
    as shown in [Example 15-2](#EX-troubleshooting-loglevel-normal). *Don’t leave
    the proxy running debug-level logging for longer than you need to*: it *will*
    impact performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 15-2\. Turning off debug logging
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Log levels were discussed in [“Accessing Linkerd Logs”](ch14.html#accessing_linkerd_logs_ch14);
    you can find more information about configuring the Linkerd proxy’s log level
    in the [official Linkerd docs](https://oreil.ly/GNv9I).
  prefs: []
  type: TYPE_NORMAL
- en: Debugging the Linkerd Control Plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linkerd’s control plane is broken down into the core control plane and its extensions
    (such as Linkerd Viz and Linkerd Multicluster). Since every component of the control
    plane uses a Linkerd proxy to communicate, you can take advantage of Linkerd’s
    observability for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Linkerd Control Plane and Availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The control plane is a part of all mesh operations, so its health is critical.
    As we mentioned at the start of this chapter, the fastest way to get a good overview
    of the health of the control plane—outside of paying for a managed service—is
    to run `linkerd check`.
  prefs: []
  type: TYPE_NORMAL
- en: '`linkerd check` will go through a series of detailed tests and will validate
    that there are no known misconfigurations. If there are issues, `linkerd check`
    will point you to documentation about how to fix the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Always Start with linkerd check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s hard to overstate how useful `linkerd check` is. Whenever you see anything
    that looks unusual about your mesh, *always* start with `linkerd check`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s also good to realize that `linkerd check` is deliberate about the order
    in which it runs its tests: it starts by running all the tests for the core control
    plane, then moves on to each extension in sequence. Within each section, `linkerd
    check` generally performs its tests in sequence, with each one needing to pass
    before the next is able to run. If a test fails, the section it’s in and its position
    within the section can itself give you a lot of information about where to start
    debugging.'
  prefs: []
  type: TYPE_NORMAL
- en: The Core Control Plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core control plane controls Pod creation, certificate management, and traffic
    routing between Pods. As discussed in [Chapter 2](ch02.html#LUAR_intro_to_linkerd),
    the core control plane consists of three main components: the proxy injector,
    the destination controller, and the identity controller. We’ll dive into failure
    modes for those components now and what to do about them.'
  prefs: []
  type: TYPE_NORMAL
- en: Use HA Mode in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any instance of Linkerd that is running in production must use high availability
    mode—no exceptions! Without HA mode, a single failure in the control plane can
    put the whole mesh at risk, which is not acceptable in production. You can read
    more about HA mode in [Chapter 14](ch14.html#LUAR_production_ready).
  prefs: []
  type: TYPE_NORMAL
- en: The identity controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any failure in the identity controller will impact the issuing and renewing
    of workload certificates. If a Pod can’t get a certificate at start time, the
    proxy will fail to start and it will log a message to that effect. On the other
    side, if a proxy’s certificate expires, it will begin emitting log messages that
    it failed to renew and will be unable to connect to any new Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of this writing we’re familiar with only two known failure modes for the
    identity controller:'
  prefs: []
  type: TYPE_NORMAL
- en: Expired certificates
  prefs: []
  type: TYPE_NORMAL
- en: When you install Linkerd, you provide the control plane with a trust anchor
    certificate and an identity issuer certificate, as discussed at length in [Chapter 7](ch07.html#LUAR_mtls_and_certs).
    If the identity issuer certificate expires, the identity controller will no longer
    be able to issue new certificates, which will cause a mesh-wide outage as individual
    proxies become unable to establish secure connections with one another.
  prefs: []
  type: TYPE_NORMAL
- en: Never Let Your Certificates Expire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Expired certificates will cause your entire mesh to grind to a halt, and they
    are *the* most common cause of production Linkerd outages. You *must* regularly
    monitor the health of your certificates.
  prefs: []
  type: TYPE_NORMAL
- en: Identity controller overload
  prefs: []
  type: TYPE_NORMAL
- en: Linkerd’s identity controller is accessed every time a new meshed Pod is created.
    As such, it’s possible—though difficult—to overwhelm the identity controller by
    creating a very large number of Pods. This will manifest as long delays in Pod
    creation and a large number of messages about certificate creation in the identity
    controller’s logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to deal with an overloaded identity controller is horizontal
    scaling: add more replicas to the `linkerd-identity` deployment in the `linkerd`
    namespace. You might also want to consider allowing it to request more CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: The destination controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linkerd’s destination controller is responsible for providing the individual
    proxies with their routing information as well as providing details about the
    effective policy for your environment. It is critical to the normal operations
    of the mesh, and any degradation should be considered a critical issue to be addressed
    immediately. There are two main areas to be aware of here:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs: []
  type: TYPE_NORMAL
- en: The destination controller’s memory usage scales linearly with the number of
    endpoints in your cluster. For most clusters, this results in the destination
    controller consuming a fairly consistent amount of memory over time. In turn,
    this means that if the destination controller gets OOMKilled by Kubernetes, it
    is very likely to reach its memory limit and get OOMKilled again every time it
    restarts. Therefore, it’s important to actively monitor and manage the memory
    limits on the destination controller.
  prefs: []
  type: TYPE_NORMAL
- en: Proxy cache
  prefs: []
  type: TYPE_NORMAL
- en: The Linkerd proxy maintains a cache of endpoints that can be reused by the proxy
    in the event that the destination controller is unavailable. By default, the proxy
    will cache its endpoint list for 5 seconds and maintain that cache as long as
    it gets reused in any given 5-second interval. You can configure that timeout
    with the `outboundDiscoveryCacheUnusedTimeout` property of the `linkerd-control-plane`
    Helm chart. Increasing the timeout will increase your overall resiliency in the
    face of a destination controller outage, particularly for services that see less
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The proxy injector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linkerd’s proxy injector is a mutating webhook that acts in response to Pod
    creation events. Of all the elements in the core control plane, the proxy injector
    is the one least likely to see issues, but it’s important to actively monitor
    its health:'
  prefs: []
  type: TYPE_NORMAL
- en: In HA mode, if the proxy injector crashes, new Pods will not be allowed to start
    until the proxy injector is back online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In non-HA mode, if the proxy injector crashes, new Pods won’t be injected into
    the mesh until it’s back online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use HA Mode in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be clear from the above description why it’s so important to use high
    availability mode in production. You can read more about HA mode in [Chapter 14](ch14.html#LUAR_production_ready).
  prefs: []
  type: TYPE_NORMAL
- en: Linkerd Extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No Linkerd extensions are in the critical path to mesh operations, but many
    users of Linkerd use the Linkerd Viz extension, in particular, to gather additional
    details about the operation of their services. In the event that you’re seeing
    aberrant behavior from Linkerd Viz, `linkerd check` will almost always show you
    what the failure is, and generally the best remediation is to upgrade or reinstall
    the extension.
  prefs: []
  type: TYPE_NORMAL
- en: The one exception to this “turn it off and on again” approach to troubleshooting
    Linkerd Viz is if you’re using its built-in Prometheus instance. As we discussed
    in [Chapter 14](ch14.html#LUAR_production_ready) and elsewhere, the built-in Prometheus
    instance stores metrics only in memory. This means that as you add more metrics
    data, it *will* periodically get OOMKilled by Kubernetes, and you will lose whatever
    data it had stored. This is a known limitation of the built-in Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Do Not Use the Built-in Prometheus in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be clear from the previous description that you *must not use the
    built-in Prometheus instance in production*. You can read more about this in [Chapter 14](ch14.html#LUAR_production_ready).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, Linkerd is a well-behaved, fault-tolerant mesh—especially in HA
    mode. However, as with all software, things can go wrong. After reading this chapter,
    you should have an idea of where to start looking when they do.
  prefs: []
  type: TYPE_NORMAL
