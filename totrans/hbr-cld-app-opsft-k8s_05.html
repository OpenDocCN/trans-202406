<html><head></head><body><section data-pdf-bookmark="Chapter 4. Single Cluster Availability" data-type="chapter" epub:type="chapter"><div class="chapter" id="single_cluster_availability">&#13;
<h1><span class="label">Chapter 4. </span>Single Cluster Availability</h1>&#13;
<p>A solid foundation for each individual cluster is critical to the <a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="about" data-type="indexterm" id="idm45358201123288"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="about" data-type="indexterm" id="idm45358201121256"/><a contenteditable="false" data-primary="single cluster design" data-secondary="about" data-type="indexterm" id="idm45358201119640"/><a contenteditable="false" data-primary="multicluster management" data-secondary="about single cluster design" data-type="indexterm" id="idm45358201118184"/><a contenteditable="false" data-primary="geographic distribution" data-secondary="single versus multiple clusters" data-type="indexterm" id="idm45358201116792"/><a contenteditable="false" data-primary="geographic distribution" data-secondary="stateful applications and databases" data-type="indexterm" id="idm45358201115400"/>availability of your applications and services. Even with these advanced distributed application capabilities, there are some systems for which multicluster isn’t an option. Most stateful applications and databases are not capable of geographic distribution and are very complicated to properly operate across multiple clusters. Many modern data storage solutions require low latency multizone solutions, making it less than optimal to break up the data clusters into higher latency environments. <a contenteditable="false" data-primary="StatefulSet API object" data-type="indexterm" id="idm45358201113368"/>It is also important to consider that many data platforms have <code>StatefulSet</code> solutions to help operate them, requiring a single-cluster architecture. <a contenteditable="false" data-primary="HA" data-see="high availability" data-type="indexterm" id="idm45358201111640"/><a contenteditable="false" data-primary="high availability (HA)" data-secondary="about single cluster design" data-type="indexterm" id="idm45358201110264"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="availability" data-type="indexterm" id="idm45358201108872"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="availability" data-type="indexterm" id="idm45358201107496"/>We think that having a strong foundation of what it means for a system to be <em>available</em> and <em>highly available</em> is critical to understanding the value and architecture of OpenShift and Kubernetes. Note that we’ll discuss how to take advantage of multicluster architectures for the ultimate in availability and flexibility in <a data-type="xref" href="ch05.html#continuous_delivery_across_clusters">Chapter 5</a>. This chapter should help prepare you with the knowledge required to make intelligent decisions about what your availability goals are and how to leverage Kubernetes and OpenShift to achieve them.</p>&#13;
<section data-pdf-bookmark="System Availability" data-type="sect1"><div class="sect1" id="system_availability">&#13;
<h1>System Availability</h1>&#13;
<p>In modern service and application delivery, we typically talk about <a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="about system availability" data-type="indexterm" id="idm45358201101800"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="system availability" data-type="indexterm" id="ch04-sysav"/><a contenteditable="false" data-primary="system availability" data-secondary="about" data-type="indexterm" id="idm45358201098200"/><a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="system availability" data-type="indexterm" id="ch04-sysav2"/><a contenteditable="false" data-primary="nines for SLOs" data-type="indexterm" id="idm45358201095160"/><a contenteditable="false" data-primary="SLOs" data-see="service-level objectives" data-type="indexterm" id="idm45358201094056"/><a contenteditable="false" data-primary="four nines as SLO" data-type="indexterm" id="idm45358201092664"/><a contenteditable="false" data-primary="five nines as SLO" data-type="indexterm" id="idm45358201091560"/><a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="nines for SLOs" data-type="indexterm" id="idm45358201090456"/>system availability in terms of what percentage of time an application or service is available and responding normally. A common standard in the technology service industry is to describe SLOs in terms of nines. We hear the terms <em>four nines</em> or <em>five nines</em> to indicate the percentage of time that the service is available. Four nines is 99.99%, and five nines is 99.999%. We’ll use these standards throughout this chapter. <a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="about" data-type="indexterm" id="idm45358201087640"/><a contenteditable="false" data-primary="service-level agreements (SLAs)" data-secondary="about SLOs within" data-type="indexterm" id="idm45358201086248"/><a contenteditable="false" data-primary="SLAs" data-see="service-level agreements" data-type="indexterm" id="idm45358201084856"/>This SLO is the agreement within the SLA that the service provider has with its stakeholder or customer. These SLOs are often measured on a monthly basis, or sometimes annually.</p>&#13;
<section data-pdf-bookmark="Measuring System Availability" data-type="sect2"><div class="sect2" id="measuring_system_availability">&#13;
<h2>Measuring System Availability</h2>&#13;
<p>In his day job, one of the authors, Jake, is responsible for the <a contenteditable="false" data-primary="system availability" data-secondary="measuring" data-type="indexterm" id="idm45358201080984"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="measuring system availability" data-type="indexterm" id="idm45358201079528"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="measuring system availability" data-type="indexterm" id="idm45358201077848"/><a contenteditable="false" data-primary="IBM Cloud" data-secondary="author management jobs" data-type="indexterm" id="idm45358201076184"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="management of IBM Cloud services" data-type="indexterm" id="idm45358201074808"/><a contenteditable="false" data-primary="site reliability engineering (SRE) team" data-type="indexterm" id="idm45358201073416"/><a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="management of IBM Cloud services" data-type="indexterm" id="idm45358201072296"/>delivery of <a href="https://oreil.ly/7SAB7">IBM Cloud Kubernetes Service</a> and <a href="https://oreil.ly/FMJGW">Red Hat OpenShift on IBM Cloud</a>. These are global services that provide managed Kubernetes and OpenShift solutions with the simplicity of an API-driven, user-friendly cloud experience. For these services, the site reliability engineering (SRE) team measures SLOs on a monthly basis as that coincides with the client billing cycle. It’s helpful to think about how much downtime is associated with these nines. We can see in <a data-type="xref" href="#availability_tablesemicolon_calculations">Table 4-1</a> that our disruption budget or allowable downtime can be very low once we get into higher levels of availability. When performing this analysis, your team needs to carefully consider what it is willing to commit to.<a contenteditable="false" data-primary="nines for SLOs" data-type="indexterm" id="idm45358201067768"/><a contenteditable="false" data-primary="four nines as SLO" data-type="indexterm" id="idm45358201066696"/><a contenteditable="false" data-primary="five nines as SLO" data-type="indexterm" id="idm45358201065592"/><a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="nines for SLOs" data-type="indexterm" id="idm45358201064488"/><a contenteditable="false" data-primary="metrics" data-secondary="nines for SLOs" data-type="indexterm" id="idm45358201063048"/></p>&#13;
<table class="border" id="availability_tablesemicolon_calculations">&#13;
<caption><span class="label">Table 4-1. </span>Availability table; calculations based on an average 730-hour month</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Availability percentage</th>&#13;
<th>Downtime per month</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>99% (two nines)</td>&#13;
<td>7 hours 18 minutes</td>&#13;
</tr>&#13;
<tr>&#13;
<td>99.5% (two and a half nines)</td>&#13;
<td>3 hours 39 minutes</td>&#13;
</tr>&#13;
<tr>&#13;
<td>99.9% (three nines)</td>&#13;
<td>43 minutes 48 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>99.95% (three and a half nines)</td>&#13;
<td>21 minutes 54 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>99.99% (four nines)</td>&#13;
<td>4 minutes 22.8 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>99.999% (five nines)</td>&#13;
<td>26.28 seconds</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p>There are two key performance indicators (KPIs) that a team will strive to <a contenteditable="false" data-primary="key performance indicators of system availability" data-type="indexterm" id="idm45358201049016"/><a contenteditable="false" data-primary="mean time to recovery (MTTR)" data-type="indexterm" id="idm45358201047880"/><a contenteditable="false" data-primary="MTTR (mean time to recovery)" data-type="indexterm" id="idm45358201046760"/><a contenteditable="false" data-primary="mean time between failures (MTBF)" data-type="indexterm" id="idm45358201045640"/><a contenteditable="false" data-primary="MTBF (mean time between failures)" data-type="indexterm" id="idm45358201044520"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="key performance indicators" data-type="indexterm" id="idm45358201043400"/><a contenteditable="false" data-primary="system availability" data-secondary="measuring" data-tertiary="key performance indicators" data-type="indexterm" id="idm45358201041720"/><a contenteditable="false" data-primary="system availability" data-secondary="key performance indicators" data-type="indexterm" id="idm45358201040056"/>improve in order to achieve an agreed-upon SLO. They are <em>mean time to recovery</em> (MTTR) and <em>mean time between failures</em> (MTBF). Together, these KPIs directly impact availability:</p>&#13;
<div data-type="equation">&#13;
<math mode="display">&#13;
 <mi mathvariant="italic">Availability</mi> <mo>=</mo> <mfrac><mrow><mi mathvariant="italic">MTBF</mi></mrow> <mrow><mi mathvariant="italic">MTBF</mi> <mo>+</mo> <mi mathvariant="italic">MTTR</mi></mrow></mfrac>&#13;
</math>&#13;
</div>&#13;
<p>It’s helpful to think about how we can arrive at our desired availability<a contenteditable="false" data-primary="nines for SLOs" data-secondary="four nines availability example" data-type="indexterm" id="idm45358201031560"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="four nines availability example" data-type="indexterm" id="idm45358201030120"/><a contenteditable="false" data-primary="system availability" data-secondary="measuring" data-tertiary="four nines availability example" data-type="indexterm" id="idm45358201028440"/><a contenteditable="false" data-primary="four nines as SLO" data-secondary="four nines availability example" data-type="indexterm" id="idm45358201026776"/><a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="nines for SLOs" data-tertiary="four nines availability example" data-type="indexterm" id="idm45358201025384"/> percentage. Let’s say we want to get to a 99.99% availability. In <a data-type="xref" href="#four_nines_left_parenthesisnineninedotni">Table 4-2</a>, we compare some different MTTR and MTBF numbers that can get us to this availability.</p>&#13;
<table class="border" id="four_nines_left_parenthesisnineninedotni">&#13;
<caption><span class="label">Table 4-2. </span>Four nines (99.99%) availability calculations</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>MTBF</th>&#13;
<th>MTTR</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>1 day</td>&#13;
<td>8.64 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1 week</td>&#13;
<td>60.48 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1 month</td>&#13;
<td>4 minutes 22.8 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1 year</td>&#13;
<td>52 minutes 33.6 seconds</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p>For those following along at home, this means that to get to <a contenteditable="false" data-primary="failure" data-secondary="four-nines availability" data-type="indexterm" id="idm45358201012840"/>four-nines availability in a system that will experience a failure once a day, you have to identify and repair that failure in under nine seconds. The engineering investment required to achieve platform stability and software quality to dramatically extend the MTBF is simply massive. For many customers, four nines is prohibitively expensive. Getting to that level of availability takes hardware, engineering, and a support staff. <a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="two nines availability example" data-type="indexterm" id="idm45358201010808"/><a contenteditable="false" data-primary="service-level objectives (SLOs)" data-secondary="nines for SLOs" data-tertiary="two nines availability example" data-type="indexterm" id="idm45358201009128"/><a contenteditable="false" data-primary="nines for SLOs" data-secondary="two nines availability example" data-type="indexterm" id="idm45358201007448"/><a contenteditable="false" data-primary="two nines as SLO" data-type="indexterm" id="idm45358201006056"/>For contrast, let’s take a look at a two nines comparison in <a data-type="xref" href="#two_nines_nineninepercent_availability_c">Table 4-3</a>.</p>&#13;
<table class="border" id="two_nines_nineninepercent_availability_c">&#13;
<caption><span class="label">Table 4-3. </span>Two nines 99% availability calculations</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>MTBF</th>&#13;
<th>MTTR</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>30 minutes</td>&#13;
<td>18.18 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1 hour</td>&#13;
<td>36.36 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1 day</td>&#13;
<td>14 minutes 32.4 seconds</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1 week</td>&#13;
<td>1 hour 41 minutes 48 seconds</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p>What are we to make of these numbers? Well, there are a few things to take away from this. First, if you have a target of four nines, then you will need an unbelievably reliable system with failures occurring on the order of once a year if you are going to have any hope of hitting the target. <a contenteditable="false" data-primary="failure" data-secondary="two nines availability" data-type="indexterm" id="idm45358200993832"/>Even at two nines, if you have a single failure a day, then you are going to need to identify and repair that failure in under 15 minutes. Second, we need to consider the likelihood of these failures. A failure once a day, you say? Who has a failure once a day? Well, consider this: the SLA for a single VM instance is typically around 90%, which means 73.05 hours of downtime per month. A single VM is going to be down on average three days per month. If your application depends on just 10 systems, then the odds are that at least one of those is going to be down every day.</p>&#13;
<p>Does this picture look grim and insurmountable? If not, then you are quite the optimist. If it is looking challenging, then you have recognized that hitting these failure rate numbers and recovery times with standard monitoring and recovery runbooks is unlikely. Enter the world of highly available systems.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="What Is a Highly Available System?" data-type="sect2"><div class="sect2" id="what_is_a_highly_available_systemquestio">&#13;
<h2>What Is a Highly Available System?</h2>&#13;
<p>In the simplest terms, <em>high availability (HA)</em> refers to a <a contenteditable="false" data-primary="high availability (HA)" data-secondary="single cluster designs" data-type="indexterm" id="ch04-hiav"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="high availability systems" data-type="indexterm" id="ch04-hiav2"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="high availability systems" data-type="indexterm" id="ch04-hiav3"/><a contenteditable="false" data-primary="high availability (HA)" data-secondary="about" data-type="indexterm" id="idm45358200982856"/><a contenteditable="false" data-primary="failure" data-secondary="high availability systems" data-type="indexterm" id="idm45358200981480"/>system that has the characteristic of not having any single point of failure. Any one “component” can sustain a failure and the system as a whole experiences no loss of availability. This attribute is sometimes referred to as having the ability to treat individual components as cattle rather than pets. <a contenteditable="false" data-primary="Baker, Bill" data-type="indexterm" id="idm45358200979640"/><a contenteditable="false" data-primary="McCance, Gavin" data-type="indexterm" id="idm45358200978536"/>This concept was first introduced by Bill Baker of Microsoft in his talk “Scaling SQL Server 2012.”<sup><a data-type="noteref" href="ch04.html#ch01fn26" id="ch01fn26-marker">1</a></sup> It was later popularized by Gavin McCance when talking about OpenStack at CERN.<sup><a data-type="noteref" href="ch04.html#ch01fn27" id="ch01fn27-marker">2</a></sup> As long as the overall herd is healthy, we don’t need to be concerned with the health of the individuals. We like to keep <em>all</em> of our pets healthy and in top shape, but it is rather time-consuming and expensive. If you have only a handful of pets, this can be sustainable. However, in modern cloud computing, we typically see a herd of hundreds or thousands. There simply are not enough SRE engineers on the planet to keep the entire herd healthy.</p>&#13;
<p>In a system that has a single point of failure, even the most cared-for components occasionally have a failure that we cannot prevent. Once discovered via monitoring, some time is always required to bring them back to health. Hopefully, our introduction to system availability convinces you of the challenges of MTBF and MTTR.</p>&#13;
<p>It is helpful for us to have a diagram of the simplest of highly available systems. <a contenteditable="false" data-primary="load balancing" data-secondary="high availability system at simplest" data-type="indexterm" id="idm45358200972008"/>A simple stateless web application with a load balancer is a great example, as shown in <a data-type="xref" href="#highly_available_stateless_web_applicati">Figure 4-1</a>.</p>&#13;
<figure><div class="figure" id="highly_available_stateless_web_applicati">&#13;
<img src="assets/hcok_0401.png"/>&#13;
<h6><span class="label">Figure 4-1. </span>Highly available stateless web application with load balancer</h6>&#13;
</div></figure>&#13;
<p>In this simplest of HA systems, if either one of these two web application instances fails, the load balancer can detect that failure and stop routing traffic to the failed instance. The actor doesn’t know about the individual instances; all of the actor’s interactions are with the load balancer. The load balancer is even capable of doing retries if a request to one of the instances fails or does not respond fast enough.</p>&#13;
<p>Now, obviously the load balancer itself could also fail, but load balancers generally have much higher availability numbers than your typical web application. <a contenteditable="false" data-primary="service-level agreements (SLAs)" data-secondary="cloud load balancers" data-type="indexterm" id="idm45358200966104"/><a contenteditable="false" data-primary="four nines as SLO" data-secondary="cloud load balancers" data-type="indexterm" id="idm45358200964712"/><a contenteditable="false" data-primary="load balancing" data-secondary="cloud load balancer SLA" data-type="indexterm" id="idm45358200963336"/>A typical cloud load balancer has an SLA of 99.99%, which would be the ceiling for the availability of any application running behind that load balancer. Where a cloud load balancer is not available, you can deploy them in pairs with virtual IPs and <em>Virtual Router Redundancy Protocol</em> (VRRP), which can move a single IP address between two load balancer instances. The public cloud providers can simplify all of this with their application and network load balancer services, which are capable of handling all of the HA aspects of the load balancer itself.</p>&#13;
<p>All done, right? Well, not quite; there are so many components and services to run in a modern application and service platform. In our experience, <a contenteditable="false" data-primary="microservices definition" data-secondary="numbers of microservices" data-type="indexterm" id="idm45358200960312"/>many large cloud services could consist of multiple groups of five to six microservices. Then add in the fact that your application or service might be globally distributed and/or replicated. <a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="author management jobs" data-type="indexterm" id="idm45358200958520"/><a contenteditable="false" data-primary="IBM Cloud" data-secondary="author management jobs" data-type="indexterm" id="idm45358200957144"/>Consider our own Red Hat OpenShift on IBM Cloud service availability in <a data-type="xref" href="#red_hat_openshift_on_ibm_cloud_locations">Figure 4-2</a>.</p>&#13;
<figure><div class="figure" id="red_hat_openshift_on_ibm_cloud_locations">&#13;
<img src="assets/hcok_0402.png"/>&#13;
<h6><span class="label">Figure 4-2. </span>Red Hat OpenShift on IBM Cloud locations</h6>&#13;
</div></figure>&#13;
<p>That’s six global multizone regions along with 14 other datacenters worldwide. Our team runs over 40 microservices in each of those multizone regions, and about half that in the single-zone datacenters. The numbers add up quickly. Imagine if we had to manually deploy each microservice and configure a load balancer for each one of them in each of these locations. We’d need an army of engineers just writing custom configuration data. Enter OpenShift and Kubernetes.<a contenteditable="false" data-primary="" data-startref="ch04-hiav" data-type="indexterm" id="idm45358200951864"/><a contenteditable="false" data-primary="" data-startref="ch04-hiav2" data-type="indexterm" id="idm45358200950488"/><a contenteditable="false" data-primary="" data-startref="ch04-hiav3" data-type="indexterm" id="idm45358200949112"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="OpenShift and Kubernetes Application and Service Availability" data-type="sect2"><div class="sect2" id="openshift_and_kubernetes_application_and">&#13;
<h2>OpenShift and Kubernetes Application and Service Availability</h2>&#13;
<p>Many components in Kubernetes and OpenShift contribute to the availability of the overall system. We’ll discuss how we keep the application healthy, how we deal with networking, and how we are able to address potential issues with compute nodes.</p>&#13;
<section data-pdf-bookmark="The application" data-type="sect3"><div class="sect3" id="the_application">&#13;
<h3>The application</h3>&#13;
<p>For our stateless application, the simple load balancer is the key <a contenteditable="false" data-primary="YAML files" data-secondary="spec section" data-tertiary="livenessProbe" data-type="indexterm" id="idm45358200943464"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="availability" data-tertiary="applications" data-type="indexterm" id="ch04-avlb"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="availability" data-tertiary="applications" data-type="indexterm" id="ch04-avlb2"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="applications in Kubernetes/OpenShift" data-type="indexterm" id="ch04-avlb3"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="applications in Kubernetes/OpenShift" data-type="indexterm" id="ch04-avlb4"/>to high availability. However, as we discussed, if we rely on manually deploying and configuring load balancers and applications, then keeping them up to date won’t be a whole lot of fun. In <a data-type="xref" href="ch02.html#getting_started_with_openshift_and_kuber">Chapter 2</a>, we discussed Kubernetes deployments. This elegant construct makes it simple to deploy multiple replicas of an application. <a contenteditable="false" data-primary="livenessProbe" data-type="indexterm" id="ch04-lvprb2"/><a contenteditable="false" data-primary="deployments" data-secondary="application deployments" data-tertiary="livenessProbe" data-type="indexterm" id="ch04-lvprb3"/><a contenteditable="false" data-primary="health checking" data-secondary="containers" data-tertiary="livenessProbe" data-type="indexterm" id="ch04-lvprb4"/><a contenteditable="false" data-primary="containers" data-secondary="about" data-tertiary="health checking livenessProbe" data-type="indexterm" id="ch04-lvprb"/><a contenteditable="false" data-primary="spec section in YAML files" data-secondary="livenessProbe" data-type="indexterm" id="idm45358200925192"/><a contenteditable="false" data-primary="YAML files" data-secondary="deployment" data-tertiary="livenessProbe" data-type="indexterm" id="idm45358200923800"/><a contenteditable="false" data-primary="kubelet" data-secondary="livenessProbe" data-type="indexterm" id="ch04-lvprb5"/><a contenteditable="false" data-primary="pods" data-secondary="status" data-tertiary="livenessProbe" data-type="indexterm" id="idm45358200920504"/>Let’s look at the <code>livenessProbe</code>, which is an advanced capability and one of the more critical features of the deployment-pod-container spec. The <code>livenessProbe</code> defines a probe that will be executed by the <code>kubelet</code> against the container to determine if the target container is healthy. If the <code>livenessProbe</code> fails to return success after the configured <code>failure​Th⁠reshold</code>, the <code>kubelet</code> will stop the container and attempt to restart it.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>To handle transient issues in a container’s ability to serve <a contenteditable="false" data-primary="containers" data-secondary="about" data-tertiary="readinessProbe" data-type="indexterm" id="idm45358200914904"/><a contenteditable="false" data-primary="readinessProbe" data-type="indexterm" id="idm45358200913080"/><a contenteditable="false" data-primary="health checking" data-secondary="containers" data-tertiary="readinessProbe" data-type="indexterm" id="idm45358200911976"/><a contenteditable="false" data-primary="pods" data-secondary="status" data-tertiary="readinessProbe" data-type="indexterm" id="idm45358200910328"/>requests, you may use a <code>readinessProbe</code>, which will determine the routing rules for Kubernetes services but won’t restart the container if it fails. If the <code>readinessProbe</code> fails, the state of the pod is marked <span class="keep-together"><code>NotReady</code></span> and the pod is removed from the service endpoint list. The probe continues to run and will set the state back to <code>Ready</code> upon success. This can be useful if a pod has an external dependency that may be temporarily unreachable. A restart of this pod won’t help, and thus using a <code>livenessProbe</code> to restart the pod has no benefit. See the <a href="https://oreil.ly/GqNnW">community docs</a> for more details on the available probes in Kubernetes and OpenShift.</p>&#13;
</div>&#13;
<p>The following example shows a simple deployment that includes a <code>livenessProbe</code> with common parameters:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx&#13;
  labels:&#13;
    app: webserver&#13;
spec:&#13;
  replicas: 3&#13;
  selector:&#13;
    matchLabels:&#13;
      app: webserver&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: webserver&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx:1.7.9&#13;
        ports:&#13;
          - containerPort: 80&#13;
        livenessProbe:&#13;
          httpGet:&#13;
            path: /&#13;
            port: 80&#13;
          initialDelaySeconds: 3&#13;
          periodSeconds: 3&#13;
          timeoutSeconds: 2&#13;
          failureThreshold: 2</pre>&#13;
<p>Let’s take a closer look at the options we have chosen in our example for <code>liveness​P⁠robe</code> and how they translate into real-world behaviors:</p>&#13;
<dl>&#13;
<dt><code>httpGet</code></dt>&#13;
<dd>The type of check that will be performed. You can also use <code>tcp</code> or <code>exec</code> commands that will execute to test if the container is healthy.</dd>&#13;
<dt><code>initialDelaySeconds</code></dt>&#13;
<dd>How long the <code>kubelet</code> will wait before attempting the first probe.</dd>&#13;
<dt><code>periodSeconds</code></dt>&#13;
<dd>How often the <code>kubelet</code> will test the probe.</dd>&#13;
<dt><code>timeoutSeconds</code></dt>&#13;
<dd>How long the <code>kubelet</code> client will wait for a response.</dd>&#13;
<dt><code>failureThreshold</code></dt>&#13;
<dd>The number of consecutive failures that the <code>kubelet</code> must run through before marking the probe as failed. In the case of our <code>livenessProbe</code>, the result is restarting this container.</dd>&#13;
</dl>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Be sure to thoroughly test your <code>livenessProbe</code> and the timing <a contenteditable="false" data-primary="debugging" data-secondary="livenessProbes" data-type="indexterm" id="idm45358200891032"/><a contenteditable="false" data-primary="errors" data-secondary="CrashLoopBackOff scenario" data-type="indexterm" id="idm45358200889624"/><a contenteditable="false" data-primary="initialDelaySeconds in livenessProbe" data-type="indexterm" id="idm45358200888232"/>settings. We have heard of numerous cases where a slow application startup results in a <code>CrashLoopBackOff</code> scenario where a pod never starts up successfully due to never having sufficient <code>initialDelaySeconds</code> to start completely. The <code>livenessProbe</code> never succeeds because the <code>kubelet</code> starts to check for liveness before the application is ready to start receiving requests. It can be painful for rolling updates to set very long initial delay settings, but it can improve stability of the system where there is a lot of variability in initial liveness times.</p>&#13;
</div>&#13;
<p>Together, we have all the makings of a traditional load balancer health check.<a contenteditable="false" data-primary="health checking" data-secondary="load balancer" data-type="indexterm" id="idm45358200884360"/><a contenteditable="false" data-primary="load balancing" data-secondary="health check for load balancer" data-type="indexterm" id="idm45358200882888"/><a contenteditable="false" data-primary="HAProxy" data-secondary="health checking" data-type="indexterm" id="idm45358200881496"/><a contenteditable="false" data-primary="load balancing" data-secondary="HAProxy for" data-type="indexterm" id="idm45358200880120"/> <span class="keep-together">HAProxy</span> is a popular traditional software load balancing solution. If we look at a standard HAProxy configuration, we’ll see many of the same critical health-checking components:</p>&#13;
<pre data-type="programlisting">frontend nginxfront&#13;
 bind 10.10.10.2:80&#13;
 mode tcp&#13;
 default_backend nginxback&#13;
 &#13;
backend nginxback&#13;
 mode tcp&#13;
 balance roundrobin&#13;
 option httpchk GET /&#13;
 timeout check 2s&#13;
 default-server inter 3s fall 2 &#13;
 server 10.10.10.10:80 check&#13;
 server 10.10.10.11:80 check&#13;
 server 10.10.10.12:80 check</pre>&#13;
<p>The interesting thing to note here is that we’re looking at the same configuration settings for a load balancer configuration in legacy infrastructure as we are for a deployment in Kubernetes. In fact, the actual check performed is identical. Both initiate an HTTP connection via port 80 and check for a 200 response code. However, the net resulting behavior is quite different. In the case of HAProxy, it just determines if network traffic should be sent to the backend server. HAProxy has no idea what that backend server is and has no ability to change its runtime state. In contrast, Kubernetes isn’t changing the network routing directly here. Kubernetes is only updating the state of each of the pods in the deployment (<code>Ready</code> versus <code>NotReady</code>), and it will restart that container if it fails its <code>livenessProbe</code>. Restarting the pod won’t fix all issues that are causing unresponsiveness, but it often can solve minor issues.</p>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p><code>livenessProbe</code> isn’t just great for ensuring that your containers are healthy and ready to receive traffic. It’s a fantastic programmable solution for the pragmatic operator who is tired of cleaning up Java Virtual Machine (JVM) leaking threads and memory to the point of becoming unresponsive. It’s a nice backup to the resource limits that we set in place in <a data-type="xref" href="ch03.html#advanced_resource_management">Chapter 3</a> in an attempt to limit these resource leaks.</p>&#13;
</div>&#13;
&#13;
<p>It’s worth noting that associating these health-checking configurations<a contenteditable="false" data-primary="health checking" data-secondary="runtime components, not networking" data-type="indexterm" id="idm45358200870760"/> with the runtime components rather than a networking feature of the system is a deliberate choice. In Kubernetes and OpenShift, the <code>livenessProbe</code> is performed by the <span class="keep-together"><code>kubelet</code></span> rather than by a load balancer. As we can see in <a data-type="xref" href="#kubelet_performing_livenessprobe_to_loca">Figure 4-3</a>, this probe is executed on every node where we find a pod from the deployment.</p>&#13;
<figure><div class="figure" id="kubelet_performing_livenessprobe_to_loca">&#13;
<img src="assets/hcok_0403.png"/>&#13;
<h6><span class="label">Figure 4-3. </span><code>kubelet</code> performing <code>livenessProbe</code> to local nginx pod</h6>&#13;
</div></figure>&#13;
<p>Every <code>kubelet</code> (one per worker node) is responsible for performing all<a contenteditable="false" data-primary="kubelet" data-secondary="one per worker node" data-type="indexterm" id="idm45358200862856"/><a contenteditable="false" data-primary="worker nodes" data-secondary="one kubelet per" data-type="indexterm" id="idm45358200861400"/> of the probes for all containers running on the <code>kubelet</code>’s node. This ensures a highly scalable solution that distributes the probe workload and maintains the state and availability of pods and containers across the entire fleet.<a contenteditable="false" data-primary="" data-startref="ch04-avlb" data-type="indexterm" id="idm45358200859288"/><a contenteditable="false" data-primary="" data-startref="ch04-avlb2" data-type="indexterm" id="idm45358200857912"/><a contenteditable="false" data-primary="" data-startref="ch04-avlb3" data-type="indexterm" id="idm45358200856536"/><a contenteditable="false" data-primary="" data-startref="ch04-avlb4" data-type="indexterm" id="idm45358200855160"/><a contenteditable="false" data-primary="" data-startref="ch04-lvprb" data-type="indexterm" id="idm45358200853784"/><a contenteditable="false" data-primary="" data-startref="ch04-lvprb2" data-type="indexterm" id="idm45358200852408"/><a contenteditable="false" data-primary="" data-startref="ch04-lvprb3" data-type="indexterm" id="idm45358200851032"/><a contenteditable="false" data-primary="" data-startref="ch04-lvprb4" data-type="indexterm" id="idm45358200849656"/><a contenteditable="false" data-primary="" data-startref="ch04-lvprb5" data-type="indexterm" id="idm45358200848280"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The infrastructure" data-type="sect3"><div class="sect3" id="the_infrastructure">&#13;
<h3>The infrastructure</h3>&#13;
<p>Where does the networking come from in Kubernetes? That’s the job<a contenteditable="false" data-primary="infrastructure" data-secondary="availability in Kubernetes/OpenShift" data-type="indexterm" id="ch04-avnt6"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="availability" data-tertiary="networking" data-type="indexterm" id="ch04-avnt"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="availability" data-tertiary="networking" data-type="indexterm" id="ch04-avnt2"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="networking on Kubernetes/OpenShift" data-type="indexterm" id="ch04-avnt3"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="networking on Kubernetes/OpenShift" data-type="indexterm" id="ch04-avnt4"/><a contenteditable="false" data-primary="networking availability" data-type="indexterm" id="ch04-avnt5"/> of the Kubernetes service. Here is where things get interesting. Let’s look at how traffic travels to our <code>nginx</code> pod. First, our service definition:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
 labels:&#13;
 app: webserver&#13;
 name: nginx-svc&#13;
spec:&#13;
 ports:&#13;
 - port: 80&#13;
 protocol: TCP&#13;
 targetPort: 8080&#13;
 selector:&#13;
 app: webserver&#13;
 type: ClusterIP&#13;
 clusterIP: 172.21.102.110</pre>&#13;
<p>This service definition tells Kubernetes that it should route any traffic in the cluster that is sent to <code>172.21.102.110</code> on port <code>80</code> to any pods that match the selector <code>app=webserver</code> on port <code>8080</code>. <a contenteditable="false" data-primary="scaling" data-secondary="networking availability in Kubernetes/OpenShift" data-type="indexterm" id="idm45358200830184"/><a contenteditable="false" data-primary="delete" data-secondary="pod" data-tertiary="availability of networking" data-type="indexterm" id="idm45358200590040"/>The advantage here is that as we scale our webserver application up and down or update it and the pods are replaced, the service keeps the endpoints up to date. A great example of endpoint recovery can be seen if we delete all the pods and let our deployment replace them automatically for us:</p>&#13;
<pre data-type="programlisting">$ <strong>kubectl get endpoints nginx-svc</strong>&#13;
NAME      ENDPOINTS                                          AGE&#13;
nginx-svc 172.30.157.70:80,172.30.168.134:80,172.30.86.21:80 3m21s&#13;
&#13;
$ <strong>kubectl delete pods --all</strong>&#13;
pod "nginx-65d4d9c4d4-cvmjm" deleted&#13;
pod "nginx-65d4d9c4d4-vgftl" deleted&#13;
pod "nginx-65d4d9c4d4-xgh49" deleted&#13;
&#13;
$ <strong>kubectl get endpoints nginx-svc</strong>&#13;
NAME      ENDPOINTS                                          AGE&#13;
nginx-svc 172.30.157.71:80,172.30.168.136:80,172.30.86.22:80 4m15s</pre>&#13;
<p>This automatic management of the endpoints for a service begins to show some of the power of Kubernetes versus traditional infrastructure. <a contenteditable="false" data-primary="HAProxy" data-secondary="Kubernetes maintaining" data-type="indexterm" id="idm45358200584872"/><a contenteditable="false" data-primary="load balancing" data-secondary="HAProxy for" data-tertiary="Kubernetes maintaining" data-type="indexterm" id="idm45358200583496"/><a contenteditable="false" data-primary="pods" data-secondary="status" data-tertiary="livenessProbe" data-type="indexterm" id="idm45358200581848"/><a contenteditable="false" data-primary="pods" data-secondary="status" data-tertiary="readinessProbe" data-type="indexterm" id="idm45358200580200"/><a contenteditable="false" data-primary="livenessProbe" data-type="indexterm" id="idm45358200578552"/><a contenteditable="false" data-primary="readinessProbe" data-type="indexterm" id="idm45358200577448"/>There’s no need to update server configurations in HAProxy. The Kubernetes service does all the work to keep our “load balancer” up to date. To assemble that list of endpoints, the Kubernetes service looks at all pods that are in the <code>Ready</code> state and matches the label selector from the service definition. If a container fails a probe or the node that the pod is hosted on is no longer healthy, then the pod is marked <code>NotReady</code> and the service will stop routing traffic to that pod. Here we again see the role of the <code>livenessProbe</code> (as well as the <code>readinessProbe</code>), which is not to directly affect routing of traffic, but rather to inform the state of the pod and thus the list of endpoints used for routing service <span class="keep-together">traffic</span>.</p>&#13;
<p>In a situation where we have multiple replicas of our application spread across multiple worker nodes,<a contenteditable="false" data-primary="replicas" data-secondary="availability despite worker node death" data-type="indexterm" id="idm45358200572664"/><a contenteditable="false" data-primary="worker nodes" data-secondary="availability despite worker node death" data-type="indexterm" id="idm45358200571224"/><a contenteditable="false" data-primary="failure" data-secondary="node failure" data-tertiary="availability despite" data-type="indexterm" id="idm45358200569832"/><a contenteditable="false" data-primary="node failure" data-secondary="availability despite" data-type="indexterm" id="idm45358200568184"/> we have a solution with the service where even if a worker node dies, we won’t lose any availability. Kubernetes is smart enough to recognize that the pods from that worker node may no longer be available and will update the service endpoint list across the full cluster to stop sending traffic to the pods on the failed node.</p>&#13;
<p>We’ve just discussed what happens when a compute node dies: the service itself is smart enough to route traffic around any pods on that node. In fact, the deployment will then also replace those pods from the failed node with new pods on another node after rescheduling them, which is very handy for dealing with compute failures. What about the need to provide a “load balancer” for the service? <a contenteditable="false" data-primary="kube-proxy" data-secondary="load balancing" data-type="indexterm" id="idm45358200565496"/><a contenteditable="false" data-primary="load balancing" data-secondary="kube-proxy providing" data-type="indexterm" id="idm45358200564120"/>Kubernetes takes a very different approach in this matter. It uses the kube-proxy component to provide the load-balancing capabilities. The interesting thing about kube-proxy is that it is a fully distributed component providing a load-balancing solution that blends the distributed nature of client-side load balancers and the frontend of a traditional load balancer. There is not a separate client-side load balancer for every client instance, but there is one per compute node.</p>&#13;
<p>Note that kube-proxy is responsible for translating service definitions and their endpoints<a contenteditable="false" data-primary="kube-proxy" data-secondary="iptables" data-type="indexterm" id="idm45358200561368"/><a contenteditable="false" data-primary="IP Virtual Server (IPVS)" data-type="indexterm" id="idm45358200559992"/><a contenteditable="false" data-primary="iptables" data-type="indexterm" id="idm45358200558920"/> into iptables<sup><a data-type="noteref" href="ch04.html#ch01fn28" id="ch01fn28-marker">3</a></sup> or IPVS<sup><a data-type="noteref" href="ch04.html#ch01fn29" id="ch01fn29-marker">4</a></sup> rules on each worker node to route the traffic appropriately. For simplicity, we do not include the iptables/IPVS rules themselves in our diagram; we represent them with kube-proxy, as shown in <a data-type="xref" href="#kube_proxy_routing_traffic_to_data_acces">Figure 4-4</a>.</p>&#13;
<figure><div class="figure" id="kube_proxy_routing_traffic_to_data_acces">&#13;
<img src="assets/hcok_0404.png"/>&#13;
<h6><span class="label">Figure 4-4. </span>kube-proxy routing traffic to data-access endpoints</h6>&#13;
</div></figure>&#13;
<p>In the example in <a data-type="xref" href="#kube_proxy_routing_traffic_to_data_acces">Figure 4-4</a>, note that the webserver (client) will route traffic to the local kube-proxy instance, and then kube-proxy sends traffic along to the data-access service pods running on the local and/or remote nodes. Thus, if any one of these nodes fails, then the client application and the kube-proxy that does the load balancing for that node die together. As a result, the kube-proxy is a key component that provides a simplified and distributed solution for high availability in the Kubernetes system.</p>&#13;
<p>We have now reviewed how Kubernetes has engineered a solution to maintain system uptime despite a failure of any single compute node, service instance, or “load balancer” (kube-proxy), thus allowing us to maintain the availability of our entire system automatically. No SRE intervention is required to identify or resolve the failure. Time and again, we see issues where a Kubernetes-hosted system does suffer some failure despite this. This is often the result of poorly written probes that are unable to account for some edge-case application failure. Kubernetes does an excellent job of providing the platform and guidelines for ultimate availability, but it is not magic. Users will need to bring well-engineered and properly configured applications and services to see this potential.</p>&#13;
<p>There are other networking solutions for load balancing as well. The kube-proxy is great, but as we have noted, it really works only when routing traffic from a client that lives in the cluster. <a contenteditable="false" data-primary="load balancing" data-secondary="requests from outside the cluster" data-type="indexterm" id="idm45358200547544"/>For routing client requests from outside the cluster, we’ll need a slightly different approach. Solutions such as an ingress controller, OpenShift Router, and Istio Ingress Gateway enable the same integration with the Kubernetes service concept but provide a way to route traffic from outside the cluster to the service and applications running inside the cluster. It is important to note that you will need to combine these Layer 7 routing solutions with an external Layer 4 load balancer in order to maintain proper availability. <a data-type="xref" href="#external_load_balancer_and_openshift_rou">Figure 4-5</a> <a contenteditable="false" data-primary="load balancing" data-secondary="routes of OpenShift" data-tertiary="external load balancer and" data-type="indexterm" id="idm45358200544424"/><a contenteditable="false" data-primary="routes of OpenShift" data-secondary="external load balancer and" data-type="indexterm" id="idm45358200542760"/><a contenteditable="false" data-primary="deployments" data-secondary="routes load balancing" data-type="indexterm" id="idm45358200541368"/>shows how an external load balancer is combined with an OpenShift Router to get a request from the actor outside the cluster to the web server that is running in the cluster. Note that when these network solutions are used, they do not route their traffic through kube-proxy; they do their own load balancing directly to the endpoints of the service.</p>&#13;
<figure><div class="figure" id="external_load_balancer_and_openshift_rou">&#13;
<img src="assets/hcok_0405.png"/>&#13;
<h6><span class="label">Figure 4-5. </span>External load balancer and OpenShift Router</h6>&#13;
</div></figure>&#13;
<p>All of the same rules and logic for redundancy and availability apply here as we have discussed thus far. The external load balancer is just like the HAProxy example we provided earlier in this chapter. There simply is no escaping the laws of networking. You still need a highly available solution to get traffic from the outside world into the cluster. The advantage here is that we don’t need to set up a separate load balancer for every application in our cluster. The OpenShift Router, ingress controller, and Istio Ingress Gateway can all serve many different applications in the cluster because they support Layer 7 routing, not just Layer 4. Layer 7 routers can inspect HTTP(S) headers to understand the target URL and use that information to send traffic to the correct service. It’s important to note that the OpenShift Router and ingress controller still rely on the Kubernetes service to do the liveness and readiness probe work to determine which pods the Router or ingress controller should send traffic to.</p>&#13;
<p>While these are more complex solutions, we simply need to have something that allows for routing external traffic to services hosted in our cluster. <a contenteditable="false" data-primary="NodePorts" data-secondary="documentation online" data-type="indexterm" id="idm45358200535448"/><a contenteditable="false" data-primary="load balancing" data-secondary="routes of OpenShift" data-tertiary="NodePort documentation online" data-type="indexterm" id="idm45358200534072"/>We have left discussion of the <code>NodePort</code> concept upon which the external load balancer solution is based to the reader to investigate from the <a href="https://oreil.ly/NxmdG">Kubernetes documentation</a>.<a contenteditable="false" data-primary="" data-startref="ch04-avnt" data-type="indexterm" id="idm45358200531064"/><a contenteditable="false" data-primary="" data-startref="ch04-avnt2" data-type="indexterm" id="idm45358200529688"/><a contenteditable="false" data-primary="" data-startref="ch04-avnt3" data-type="indexterm" id="idm45358200528312"/><a contenteditable="false" data-primary="" data-startref="ch04-avnt4" data-type="indexterm" id="idm45358200526936"/><a contenteditable="false" data-primary="" data-startref="ch04-avnt5" data-type="indexterm" id="idm45358200525560"/><a contenteditable="false" data-primary="" data-startref="ch04-avnt6" data-type="indexterm" id="idm45358200524184"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The result" data-type="sect3"><div class="sect3" id="the_result">&#13;
<h3>The result</h3>&#13;
<p>Looking back to our initial availability discussion, Kubernetes has supplied a mechanism to identify and repair an issue. <a contenteditable="false" data-primary="Kubernetes" data-secondary="availability" data-tertiary="mean time to recovery" data-type="indexterm" id="idm45358200520760"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="availability" data-tertiary="mean time to recovery" data-type="indexterm" id="idm45358200519112"/><a contenteditable="false" data-primary="livenessProbe" data-secondary="mean time to recovery" data-type="indexterm" id="idm45358200517464"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="MTTR on Kubernetes/OpenShift" data-type="indexterm" id="idm45358200516088"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="MTTR on Kubernetes/OpenShift" data-type="indexterm" id="idm45358200514504"/><a contenteditable="false" data-primary="mean time to recovery (MTTR)" data-type="indexterm" id="idm45358200512840"/><a contenteditable="false" data-primary="MTTR (mean time to recovery)" data-type="indexterm" id="idm45358200511720"/>Based on our application <code>livenessProbe</code> settings, it would take a maximum of six seconds to identify the problem and then some amount of time to reroute traffic (mitigating any failed requests), and then it would automatically repair the failed container by restarting it. That container-restart process in an efficient application can be quite speedy indeed, subsecond. Even conservatively at 10 seconds, that looks pretty good for our MTTR. We could be more aggressive with our settings and do a <code>livenessProbe</code> every two seconds, kick a restart on just a single failure, and get to a sub-five-second MTTR.</p>&#13;
<p>A quick note on <code>livenessProbe</code> and <code>readinessProbe</code> settings: remember that the <code>kubelet</code> is going to be making requests to every instance of these containers across the entire cluster. If you have hundreds of replicas running and they are all being hit every two seconds by the probes, then things could get very chatty in your logging or metrics solution. It may be a nonissue if your probes are hitting fairly light endpoints that are comparable to those that may be hit hundreds or thousands of times a second to handle user requests, but it’s worth noting all the same.</p>&#13;
<p>Now that we have some idea of the kind of MTTR we could achieve with Kubernetes for our application, let’s take a look at what this does for some of our availability calculations to determine MTBF for our system in <a data-type="xref" href="#mtbf_calculations_for_mttr">Table 4-4</a>.</p>&#13;
<table class="border" id="mtbf_calculations_for_mttr">&#13;
<caption><span class="label">Table 4-4. </span>MTBF calculations for MTTR</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>MTTR</th>&#13;
<th>MTBF for 99% availability</th>&#13;
<th>MTBF for 99.99% availability</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>5 seconds</td>&#13;
<td>8.26 minutes</td>&#13;
<td>13 hours 53 minutes</td>&#13;
</tr>&#13;
<tr>&#13;
<td>10 seconds</td>&#13;
<td>16.5 seconds</td>&#13;
<td>27 hours 46 minutes</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1 minute</td>&#13;
<td>99 seconds</td>&#13;
<td>6 days 22.65 hours</td>&#13;
</tr>&#13;
<tr>&#13;
<td>10 minutes</td>&#13;
<td>16 minutes 30 seconds</td>&#13;
<td>69 days 10.5 hours</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p>Looking at this data, we can start to see how with the power of just the simple Kubernetes deployment concept, we can get to some reasonable availability calculations that we could sustain.<a contenteditable="false" data-primary="" data-startref="ch04-sysav" data-type="indexterm" id="idm45358200493288"/><a contenteditable="false" data-primary="" data-startref="ch04-sysav2" data-type="indexterm" id="idm45358200491912"/></p>&#13;
<p>Done and dusted, right? Not so fast. Different failure modes can have a variety of different results for Kubernetes and OpenShift. We’ll take a look at these failures next.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Failure Modes" data-type="sect1"><div class="sect1" id="failure_modes">&#13;
<h1>Failure Modes</h1>&#13;
<p>You can experience a variety of different types of failures in your <a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="failure modes" data-type="indexterm" id="ch04-fail"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="failure modes" data-type="indexterm" id="ch04-fail2"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="availability" data-tertiary="failure modes" data-type="indexterm" id="ch04-fail3"/><a contenteditable="false" data-primary="OpenShift (Red Hat)" data-secondary="availability" data-tertiary="failure modes" data-type="indexterm" id="ch04-fail4"/>Kubernetes and OpenShift systems. We’ll cover many of these failures, how to identify them, what the application or service impact is, and how to prevent them.</p>&#13;
<section data-pdf-bookmark="Application Pod Failure" data-type="sect2"><div class="sect2" id="application_pod_failur">&#13;
<h2>Application Pod Failure</h2>&#13;
<p>A single application pod failure (<a data-type="xref" href="#application_pod_failure">Figure 4-6</a>) has the potential to result in almost no downtime, as based on our earlier discussion around probes and the <code>kubelet</code> ability to automatically restart failed pods.</p>&#13;
<figure><div class="figure" id="application_pod_failure">&#13;
<img src="assets/hcok_0406.png"/>&#13;
<h6><span class="label">Figure 4-6. </span>Application pod failure</h6>&#13;
</div></figure>&#13;
<p>It is worth noting that longer probe intervals can result in longer MTTR as that<a contenteditable="false" data-primary="pods" data-secondary="application pod failure" data-type="indexterm" id="idm45358200474520"/><a contenteditable="false" data-primary="application pod failure" data-type="indexterm" id="idm45358200473144"/><a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="application pod failure" data-type="indexterm" id="idm45358200472040"/><a contenteditable="false" data-primary="availability" data-secondary="single cluster availability" data-tertiary="application pod failure" data-type="indexterm" id="idm45358200470392"/><a contenteditable="false" data-primary="single cluster design" data-secondary="availability" data-tertiary="application pod failure" data-type="indexterm" id="idm45358200468680"/><a contenteditable="false" data-primary="readinessProbe" data-secondary="failure propagation time" data-type="indexterm" id="idm45358200467032"/> increases the time to detect the failure. It’s also interesting to note that if you are using <code>readinessProbes</code>, it can take some time for a readiness failure to propagate through the Kubernetes or OpenShift system. The total time for recovery can be:</p>&#13;
<pre data-type="programlisting">(readinessProbe Internal * failureThreshold) + iptables-min-sync-period</pre>&#13;
<p>The <code>iptables-min-sync-period</code> is a new configuration setting<a contenteditable="false" data-primary="kube-proxy" data-secondary="iptables" data-tertiary="configuration of update time" data-type="indexterm" id="idm45358200462968"/><a contenteditable="false" data-primary="iptables" data-secondary="configuration of update time" data-type="indexterm" id="idm45358200461288"/><a contenteditable="false" data-primary="IP Virtual Server (IPVS)" data-secondary="configuration of update time" data-type="indexterm" id="idm45358200459896"/> we have not discussed yet. This setting (or <code>ipvs-min-sync-period</code> in the case of IPVS mode) of kube-proxy determines the shortest period of time that can elapse between any two updates of the iptables rules for kube-proxy.<sup><a data-type="noteref" href="ch04.html#ch01fn30" id="ch01fn30-marker">5</a></sup> If one endpoint in the cluster is updated at t0 and then another endpoint update occurs five seconds later but the <code>min-sync</code> is set to 30 seconds, then kube-proxy (iptables/IPVS) will still continue to route traffic to potentially invalid endpoints. The default for this setting is one second and thus minimal impact. However, in very large clusters with huge numbers of services (or huge numbers of endpoints), this interval may need to be increased to ensure reasonable performance of the iptables/IPVS control plane. The reason for this is that as there are more and more services to manage, the size of the iptables or IPVS rule set can become massive, easily tens of thousands of rules. If the <code>iptables-min-sync-period</code> is not adjusted, then kube-proxy can become overwhelmed with constantly trying to update the rules. It’s important to remember this relationship. Cluster size can impact performance and responsiveness of the entire system.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Worker Node Failure" data-type="sect2"><div class="sect2" id="worker_node_failure-id00001">&#13;
<h2>Worker Node Failure</h2>&#13;
<p>One of the most common failures seen in any OpenShift or Kubernetes cluster<a contenteditable="false" data-primary="worker nodes" data-secondary="failure of worker node" data-type="indexterm" id="idm45358200450904"/><a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="worker node failure" data-type="indexterm" id="idm45358200449528"/><a contenteditable="false" data-primary="failure" data-secondary="node failure" data-tertiary="worker node failure" data-type="indexterm" id="idm45358200447880"/><a contenteditable="false" data-primary="node failure" data-secondary="worker node failure" data-type="indexterm" id="idm45358200446232"/> is the single worker node failure (<a data-type="xref" href="#worker_node_failure-id00002">Figure 4-7</a>). Single node faults can occur for a variety of reasons. These issues include disk failure, network failure, operating system failure, disk space exhaustion, memory exhaustion, and more. Let’s take a closer look at what takes place in the Kubernetes system when there is a node failure. The process to recover a single node will directly impact our availability calculations of the system.</p>&#13;
<figure><div class="figure" id="worker_node_failure-id00002">&#13;
<img src="assets/hcok_0407.png"/>&#13;
<h6><span class="label">Figure 4-7. </span>Worker node failure</h6>&#13;
</div></figure>&#13;
<p class="pagebreak-before less_space">The following are the time intervals that occur when responding to a worker node failure:</p>&#13;
<dl>&#13;
<dt>T0</dt>&#13;
<dd>Failure occurs on node.</dd>&#13;
<dt>T1 = T0 + <code>nodeLeaseDurationSeconds</code></dt>&#13;
<dd>Kubernetes and OpenShift now use a lease system to track the health of worker nodes. Each <code>kubelet</code> registers a lease object for itself with the control plane. If that lease expires, then the node is immediately marked as <code>NotReady</code>. As soon as the node is marked <code>NotReady</code>, then the service controller will denote that any pods on that node may have failed and any service endpoints associated with the pods on that node should be removed. This is how Kubernetes decides to stop routing traffic to pods on a failed node. The result is the same as the <code>liveness​P⁠robe</code> failing for the pod but affects all pods on the failed node. <code>nodeLease​Dura⁠tionSeconds</code> is configured on the <code>kubelet</code> and determines how long the lease will last. The default time is 40 seconds.</dd>&#13;
<dt>T2 = T1 + <code>iptables-min-sync-period</code></dt>&#13;
<dd>This is the additional time that may be required to update the fleet of kube-proxy configurations across the cluster. This behavior is identical to the behavior that was discussed earlier in our application pod failure scenario where any changes in the endpoint list for a service need to be synced to all of the kube-proxy instances across the cluster. Once the sync completes, then all traffic is routed properly, and any issues from the failed node will be resolved minus the capacity lost. The default minimum sync period is 30 seconds.</dd>&#13;
<dt>T3 = T2 + <code>pod-eviction-timeout</code></dt>&#13;
<dd>This setting is configured on the kube-controller-manager and determines how long the controller will wait after a node is marked <code>NotReady</code> before it will attempt to terminate pods on the <code>NotReady</code> node. If there are controllers managing the pods that are terminating, then typically they will start a new replacement pod, as is the case with the deployment controller logic. This does not affect routing of traffic to the pods on the bad node; that was addressed at T2. However, this will recover any diminished capacity associated with the pods from the failed node. The default timeout is five minutes.</dd>&#13;
</dl>&#13;
<p>As we can see, the impact of a node failure is on a similar order of magnitude as an application pod failure. Note that here we are calculating the worst-case scenario to get to T2 where service is recovered. Some node failures may be detected and reported faster, and as a result, the T1 time can be quicker as we don’t have to wait for the lease to expire for the node to be marked <code>NotReady</code>. Also remember that the <span class="keep-together">iptables</span> may sync faster; this is just the worst-case scenario.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Worker Zone Failure" data-type="sect2"><div class="sect2" id="worker_zone_failure-id00003">&#13;
<h2>Worker Zone Failure</h2>&#13;
<p>A zone failure is handled much the same as a worker node failure. In a total zone failure<a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="worker zone failure" data-type="indexterm" id="idm45358200426680"/><a contenteditable="false" data-primary="worker zone failure" data-type="indexterm" id="idm45358200425032"/><a contenteditable="false" data-primary="networking availability" data-secondary="worker zone failure" data-type="indexterm" id="idm45358200423928"/><a contenteditable="false" data-primary="zone failure" data-type="indexterm" id="idm45358200422552"/> (<a data-type="xref" href="#worker_zone_failure-id00004">Figure 4-8</a>), such as a network failure, we would typically expect all nodes to be marked as <code>NotReady</code> within a reasonably small window of time.</p>&#13;
<figure><div class="figure" id="worker_zone_failure-id00004">&#13;
<img src="assets/hcok_0408.png"/>&#13;
<h6><span class="label">Figure 4-8. </span>Worker zone failure</h6>&#13;
</div></figure>&#13;
<p>A zone failure can be “detected” by Kubernetes and OpenShift if the number of <a contenteditable="false" data-primary="unhealthy-zone-threshold percentage" data-type="indexterm" id="idm45358200417560"/><a contenteditable="false" data-primary="reliability documentation for zone failures" data-type="indexterm" id="idm45358200416104"/>nodes in a single zone that is failing exceeds the <code>unhealthy-zone-threshold</code> percentage.<sup><a data-type="noteref" href="ch04.html#ch01fn31" id="ch01fn31-marker">6</a></sup> If a zone failure is detected, there is no change to the service endpoint evaluation process. <a contenteditable="false" data-primary="evicting pods" data-secondary="zone failure" data-type="indexterm" id="idm45358200412456"/><a contenteditable="false" data-primary="worker nodes" data-secondary="pod eviction" data-tertiary="zone failure" data-type="indexterm" id="idm45358200411080"/>All pods on the failed nodes will be removed as endpoints for any associated services. The <code>unhealthy-zone-threshold</code> will change what happens to the pod eviction process. <a contenteditable="false" data-primary="large-cluster-size-threshold" data-type="indexterm" id="idm45358200408664"/>If the cluster is below the <code>large-cluster-size-threshold</code> (defined as a count of nodes), then all pod eviction in the zone will stop. If <code>unhealthy-zone-threshold</code> is above the large cluster size, then pod eviction starts, but nodes are processed at a much slower rate as defined by the <code>secondary-node-eviction-rate</code>. This is in an effort to prevent a storm of pod scheduling and allows the system some additional time to recover without “overreacting” to a temporary failure.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Control Plane Failure" data-type="sect2"><div class="sect2" id="control_plane_failure">&#13;
<h2>Control Plane Failure</h2>&#13;
<p>There are a variety of different control plane failure scenarios. There are network-related<a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="control plane failure" data-type="indexterm" id="ch04-ctlpl"/><a contenteditable="false" data-primary="control plane failure" data-type="indexterm" id="ch04-ctlpl2"/><a contenteditable="false" data-primary="networking availability" data-secondary="control plane failure" data-type="indexterm" id="ch04-ctlpl3"/><a contenteditable="false" data-primary="worker nodes" data-secondary="control plane failure" data-type="indexterm" id="ch04-ctlpl4"/> failures between worker nodes and the control plane, and there are control plane component failures. We’ll get into the details of the high availability features of the various control plane components later in this section. Before we get started with those details, it’s worth talking about what happens when there is any kind of total control plane failure. How does the cluster overall respond to a failure like the one shown in <a data-type="xref" href="#complete_control_plane_failure">Figure 4-9</a>?</p>&#13;
<figure><div class="figure" id="complete_control_plane_failure">&#13;
<img src="assets/hcok_0409.png"/>&#13;
<h6><span class="label">Figure 4-9. </span>Complete control plane failure</h6>&#13;
</div></figure>&#13;
<p>The good news is that all of the workloads running on the worker nodes are unaffected in this scenario. All of the kube-proxy configurations, ingress controllers, pods, <code>DaemonSets</code>, network policies, and so on continue to function exactly as they were at the time of the control plane failure. The bad news is that no changes or updates can be made to the system in this state. In addition, if a worker node fails during this scenario, then there is no detection or updating of the cluster to compensate for the failure. Fortunately, the likelihood of such a failure is low if the control plane is implemented properly. <a contenteditable="false" data-primary="high availability (HA)" data-secondary="control plane features" data-type="indexterm" id="idm45358200392472"/>We’ll cover some of the HA features of the control plane now.</p>&#13;
<p>Rather than keep everyone waiting, we’ll start with the most complex, potentially most challenging, and easily most critical component within the control plane. <a contenteditable="false" data-primary="etcd" data-secondary="about" data-type="indexterm" id="idm45358200390376"/>The component worthy of the title of “most critical” is etcd. Kubernetes and OpenShift use the open source key value store etcd as their persistent storage solution. The etcd datastore is where all objects in the system are persisted. All of the data and information related to the nodes, deployments, pods, services, and so on are stored in etcd.</p>&#13;
<section data-pdf-bookmark="etcd failures" data-type="sect3"><div class="sect3" id="etcd_failures">&#13;
<h3>etcd failures</h3>&#13;
<p>You can encounter many different issues with etcd. Some are easier to mitigate than others.<a contenteditable="false" data-primary="etcd" data-secondary="failure of" data-type="indexterm" id="idm45358200386200"/><a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="etcd failure" data-type="indexterm" id="idm45358200384824"/><a contenteditable="false" data-primary="load balancing" data-secondary="etcd built-in load balancing" data-type="indexterm" id="idm45358200383176"/> Connectivity failures from the kube-apiserver to etcd may be the most common. The modern etcd client has load balancing and resiliency built in. Note that if you have a traditional load balancer that sits in front of your etcd instance, then much of this does not apply, as you likely have a single endpoint registered for etcd. <a contenteditable="false" data-primary="kube-apiserver" data-secondary="--etcd-servers setting" data-secondary-sortas="etcd-servers setting" data-type="indexterm" id="idm45358200381304"/><a contenteditable="false" data-primary="etcd" data-secondary="--etcd-servers setting" data-secondary-sortas="etcd-servers setting" data-type="indexterm" id="idm45358200379656"/>Regardless, the kube-apiserver will use the <code>--etcd-servers</code> setting to determine what IP(s) and/or hostname(s) the etcd go client is configured with. <a contenteditable="false" data-primary="load balancing" data-secondary="etcd built-in load balancing" data-tertiary="documentation online" data-type="indexterm" id="idm45358200377352"/>There is excellent <a href="https://oreil.ly/TUc0U">documentation about the load balancing behavior</a> in the community. It’s worth noting that most modern Kubernetes and OpenShift versions are now using the clientv3-grpc1.23 variant in their client. This version of the client maintains active gRPC subconnections at all times to determine connectivity to the etcd server <span class="keep-together">endpoints</span>.</p>&#13;
<p>The failure recovery behavior of the etcd client enables the system to deal with communication failures between the kube-apiserver and etcd. Unfortunately, the etcd <span class="keep-together">client</span> cannot handle issues like performance degradation of a peer or network partitioning. <a contenteditable="false" data-primary="etcd" data-secondary="failure of" data-tertiary="readiness checks" data-type="indexterm" id="idm45358200372712"/><a contenteditable="false" data-primary="networking availability" data-secondary="control plane failure" data-tertiary="readiness checks" data-type="indexterm" id="idm45358200370968"/><a contenteditable="false" data-primary="control plane failure" data-secondary="readiness checks" data-type="indexterm" id="idm45358200369320"/><a contenteditable="false" data-primary="health checking" data-secondary="etcd client readiness checks" data-type="indexterm" id="idm45358200367944"/><a contenteditable="false" data-primary="metrics" data-secondary="etcd performance" data-type="indexterm" id="idm45358200366600"/>This can be mitigated to some extent with excellent readiness checking of the etcd peers. If your system allows for readiness checking (for example, etcd hosted on Kubernetes), you can configure readiness checks that can look for network partitioning or severe performance degradation using something like:</p>&#13;
<pre data-type="programlisting">etcdctl get --consistency=s testKey --endpoints=https://localhost:port</pre>&#13;
<p>Using this code will ensure that the local peer is able to properly perform a serialized read and is a stronger check to make sure that not only is this peer listening but also it can handle get requests. A put test can be used (which also ensures peer connectivity), but this can be a very expensive health check that introduces tons of key revisions and may adversely impact the etcd cluster. We are conducting an ongoing investigation to determine whether etcd performance metrics are a reliable indicator that a peer is suffering performance degradation and should be removed from the cluster. At this point, we can only recommend standard monitoring and SRE alerting to observe performance issues. There is not yet strong evidence that you can safely kill a peer because of performance issues and not potentially adversely impact overall availability of the cluster.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It would be poor form to have all of this discussion about improving availability of etcd without also mentioning how <a contenteditable="false" data-primary="etcd" data-secondary="failure of" data-tertiary="disaster-recovery process" data-type="indexterm" id="idm45358200361352"/><a contenteditable="false" data-primary="control plane failure" data-secondary="disaster-recovery process" data-type="indexterm" id="idm45358200359736"/><a contenteditable="false" data-primary="networking availability" data-secondary="control plane failure" data-tertiary="disaster-recovery process" data-type="indexterm" id="idm45358200358344"/><a contenteditable="false" data-primary="disaster-recovery process for etcd failure" data-type="indexterm" id="idm45358200356680"/><a contenteditable="false" data-primary="backups in disaster recovery" data-type="indexterm" id="idm45358200355480"/>critical it is to have a proper disaster-recovery process in place for etcd. Regular intervals of automated backups are a must-have. Ensure that you regularly test your disaster-recovery process for etcd. Here’s a bonus tip for etcd disaster recovery: even if quorum is broken for the etcd cluster, if etcd processes are still running, you can take a new backup from that peer even without quorum to ensure that you minimize data loss for any time delta from the last regularly scheduled backup of etcd. Always try to take a backup from a remaining member of etcd before restoring. <a contenteditable="false" data-primary="backups in disaster recovery" data-secondary="documentation online" data-type="indexterm" id="idm45358200353624"/><a contenteditable="false" data-primary="disaster-recovery process for etcd failure" data-secondary="documentation online for backup and restore" data-type="indexterm" id="idm45358200352232"/><a contenteditable="false" data-primary="control plane failure" data-secondary="disaster-recovery process" data-tertiary="documentation online" data-type="indexterm" id="idm45358200350792"/><a contenteditable="false" data-primary="etcd" data-secondary="failure of" data-tertiary="documentation online for backup and restore" data-type="indexterm" id="idm45358200349128"/>The <a href="https://oreil.ly/jCvAb">Kubernetes documentation</a> has some excellent info on etcd backup and restore options.</p>&#13;
</div>&#13;
<p>The most significant concern with etcd is maintaining quorum of the etcd cluster. etcd will break quorum any time less than a majority of the peers are healthy.<a contenteditable="false" data-primary="etcd" data-secondary="failure of" data-tertiary="quorum of etcd cluster" data-type="indexterm" id="idm45358200345960"/><a contenteditable="false" data-primary="control plane failure" data-secondary="quorum of etcd cluster" data-type="indexterm" id="idm45358200344312"/><a contenteditable="false" data-primary="networking availability" data-secondary="control plane failure" data-tertiary="quorum of etcd cluster" data-type="indexterm" id="idm45358200342936"/> If quorum breaks, then the entire Kubernetes/OpenShift control plane will fail and all control plane actions will begin to fail.  Increasing the number of peers will increase the number of peer failures that can be sustained. Note that having an even number of peers doesn’t increase fault tolerance and can harm the cluster’s ability to detect split brain-network partitions.<sup><a data-type="noteref" href="ch04.html#ch01fn32" id="ch01fn32-marker">7</a></sup></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="kube-apiserver failures" data-type="sect3"><div class="sect3" id="kube_apiserver_failures">&#13;
<h3>kube-apiserver failures</h3>&#13;
<p><a contenteditable="false" data-primary="kube-apiserver" data-secondary="about" data-type="indexterm" id="idm45358200337368"/><a contenteditable="false" data-primary="create, read, update, and delete (CRUD)" data-type="indexterm" id="idm45358200335992"/><a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="kube-apiserver failure" data-type="indexterm" id="idm45358200334872"/><a contenteditable="false" data-primary="kube-apiserver" data-secondary="failure of" data-type="indexterm" id="idm45358200333224"/>The kube-apiserver is responsible for handling all requests for <em>create, read, update, and delete</em> (CRUD) operations on objects in the Kubernetes or OpenShift cluster. This applies to end-user client applications as well as internal components of the cluster, such as worker nodes, kube-controller-manager, and kube-scheduler. All interactions with the etcd persistent store are handled through the kube-apiserver.</p>&#13;
<p>The kube-apiserver is an active/active scale-out application. It requires no leader election or other mechanism to manage the fleet of kube-apiserver instances. They are all running in an active mode. This attribute makes it straightforward to run the kube-apiserver in a highly available configuration. The cluster administrator may run as many instances of kube-apiserver as they wish, within reason, and place a load balancer in front of the apiserver. If properly configured, a single kube-apiserver failure will have no impact on cluster function. See <a data-type="xref" href="#kube_apiserver_single_failure_with_no_av">Figure 4-10</a> for an example.</p>&#13;
<figure><div class="figure" id="kube_apiserver_single_failure_with_no_av">&#13;
<img src="assets/hcok_0410.png"/>&#13;
<h6><span class="label">Figure 4-10. </span>kube-apiserver single failure with no availability impact</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
<section data-pdf-bookmark="kube-scheduler and kube-controller-manager failures" data-type="sect3"><div class="sect3" id="kube_scheduler_and_kube_controller_manag">&#13;
<h3>kube-scheduler and kube-controller-manager failures</h3>&#13;
<p>These two components have very similar availability features. They both run<a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="kube-scheduler failure" data-type="indexterm" id="idm45358200325160"/><a contenteditable="false" data-primary="kube-scheduler" data-secondary="failure of" data-type="indexterm" id="idm45358200323512"/><a contenteditable="false" data-primary="kube-controller-manager" data-secondary="failure of" data-type="indexterm" id="idm45358200322136"/><a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="kube-controller-manager failure" data-type="indexterm" id="idm45358200320760"/><a contenteditable="false" data-primary="leader election during failure" data-type="indexterm" id="idm45358200319096"/> with a single active instance at all times and use the same leader election features. They leverage the lease object to manage the leader of the cluster. If a follower instance of either component fails, there is no impact to the overall cluster functionality. If the leader fails, then for the <code>--leader-elect-lease-duration</code> there will be no leader. However, no component interacts with the scheduler or controller manager directly. These two components interact asynchronously on object state changes in the cluster.</p>&#13;
<p>Loss of leader for the kube-controller-manager can result in a delay of recovery from a node failure, as an example. If a pod fails for some reason, then the controller <span class="keep-together">manager</span> will not respond to update the endpoints of the associated service. So while a concurrent leader failure with other system failures can result in a prolonged MTTR for the system, it will not result in an extended failure or outage.</p>&#13;
<p>A failed kube-scheduler leader should have almost no impact on availability. While the leader is down awaiting a new election, no new pods will be scheduled to nodes. So while you may experience diminished capacity due to lack of scheduling of pods from a concurrent failed node, you’ll still have service and routing updates to the cluster at this time.<a contenteditable="false" data-primary="" data-startref="ch04-ctlpl" data-type="indexterm" id="idm45358200314600"/><a contenteditable="false" data-primary="" data-startref="ch04-ctlpl2" data-type="indexterm" id="idm45358200313224"/><a contenteditable="false" data-primary="" data-startref="ch04-ctlpl3" data-type="indexterm" id="idm45358200311848"/><a contenteditable="false" data-primary="" data-startref="ch04-ctlpl4" data-type="indexterm" id="idm45358200310472"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Network Failure" data-type="sect2"><div class="sect2" id="network_failure">&#13;
<h2>Network Failure</h2>&#13;
<p>Networking failures come in many different varieties. We could see failures<a contenteditable="false" data-primary="failure" data-secondary="failure modes" data-tertiary="network failure" data-type="indexterm" id="idm45358200306968"/><a contenteditable="false" data-primary="networking availability" data-secondary="network failure" data-type="indexterm" id="idm45358200305320"/> of routing-table updates that cause packets to be misrouted. In other scenarios, we might see load-balancer failures. There could be a network-switch failure. The list of potential issues is endless. To simplify our discussion of this topic, we will bundle network failures into two common flavors:<a contenteditable="false" data-primary="North-South network failure" data-type="indexterm" id="idm45358200303496"/><a contenteditable="false" data-primary="East-West network failure" data-type="indexterm" id="idm45358200302424"/> <em>North-South network failures</em> occur when traffic from outside our Kubernetes or OpenShift cluster can no longer enter or exit, and <em>East-West network failures</em> describe a situation where network traffic cannot travel between the nodes in our cluster.</p>&#13;
<section data-pdf-bookmark="North-South network failure" data-type="sect3"><div class="sect3" id="north_south_network_failure">&#13;
<h3>North-South network failure</h3>&#13;
<p>This failure scenario refers to the situation where the cluster itself may be healthy, control plane and data plane, but there may be issues routing traffic into the cluster from external sources. In <a data-type="xref" href="#north_south_networking_failure">Figure 4-11</a>, we have a situation where connectivity from the external load balancer to the worker node has failed.</p>&#13;
<figure><div class="figure" id="north_south_networking_failure">&#13;
<img src="assets/hcok_0411.png"/>&#13;
<h6><span class="label">Figure 4-11. </span>North-South networking failure</h6>&#13;
</div></figure>&#13;
<p>The result is that the load balancer health checks to any <code>NodePort</code>s <a contenteditable="false" data-primary="NodePorts" data-secondary="North-South networking failure" data-type="indexterm" id="idm45358200294232"/><a contenteditable="false" data-primary="load balancing" data-secondary="health checks by load balancer" data-tertiary="North-South networking failure" data-type="indexterm" id="idm45358200292760"/><a contenteditable="false" data-primary="mean time to recovery (MTTR)" data-secondary="North-South networking failure" data-type="indexterm" id="idm45358200291080"/>exposed from the connection to the failed worker node will fail, the load balancer will stop routing traffic to that worker node, and life goes on. In fact, the MTTR here will be quite good, as it will take only the time required for the external load balancer to fail the health check to stop routing traffic down the failed link, or potentially it would have zero downtime if connection time limits and retries are integrated into this load-balancer solution. Load balancers were born for this stuff.</p>&#13;
<p>If <em>all</em> connectivity between the load balancer and the worker nodes fails, then all access from outside the cluster will have failed. The result is real downtime for any service whose purpose is to be accessed from outside the cluster. The good news is that we can implement multizone clusters to mitigate the chances of such catastrophic failures. In <a data-type="xref" href="ch08.html#working_example_of_multicluster_applicat">Chapter 8</a>, we will discuss using multiple clusters with global DNS-based load balancing for further improvements to mitigating geographic network outages. In addition, these North-South failures only affect access to services from outside the cluster. Everything operating within the cluster may still function without any service interruption.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="East-West network failure" data-type="sect3"><div class="sect3" id="east_west_network_failure">&#13;
<h3>East-West network failure</h3>&#13;
<p>We may also encounter scenarios where nodes may lose cross-zone <a contenteditable="false" data-primary="East-West network failure" data-type="indexterm" id="idm45358200284424"/>connectivity or network partitioning. This is most frequently experienced in multizone clusters when there is a network failure between zones. In <a data-type="xref" href="#east_west_total_failure">Figure 4-12</a> we look at a full zone failure.</p>&#13;
<figure><div class="figure" id="east_west_total_failure">&#13;
<img src="assets/hcok_0412.png"/>&#13;
<h6><span class="label">Figure 4-12. </span>East-West total failure</h6>&#13;
</div></figure>&#13;
<p>In this scenario, we are faced with a “clean” zone failure where Zone C is fully partitioned. The worker node in the zone will continue to try and renew its lease to the control plane node in Zone C. If this were successful, it would lead to issues. However, the Zone C control plane will drop out of the cluster and stop responding to requests. Two things happen for this to occur. First, etcd in Zone C will begin to fail as it will not be able to contact etcd in other zones to participate in the quorum on the etcd cluster. As a result, the kube-apiserver in Zone C will also start to report back as unhealthy. The worker in Zone C won’t be able to renew its lease with the control plane. As a result, it will just sit tight and do nothing. The <code>kubelet</code> will restart any containers that fail. However, as we can see, there is no connectivity between the Zone C worker and other zones. Any application communication will begin to fail.</p>&#13;
<p>The good news is that all will be well in Zones A and B. <a contenteditable="false" data-primary="etcd" data-secondary="failure of" data-tertiary="quorum of etcd cluster" data-type="indexterm" id="idm45358200277960"/><a contenteditable="false" data-primary="control plane failure" data-secondary="quorum of etcd cluster" data-tertiary="East-West network failure" data-type="indexterm" id="idm45358200276232"/>The control plane etcd will retain quorum (electing a new leader in A or B if the leader was previously in C), and all control plane components will be healthy. In addition, the lease for the worker in Zone C will expire, and thus this node will be marked as <code>NotReady</code>. All Zone C service endpoints will be removed, and kube-proxy will be updated to stop routing traffic to pods in Zone C.</p>&#13;
<p>There are probably a number of scenarios that can leave the Kubernetes or OpenShift platform slightly out of sorts. We’re going to focus on one such scenario that does expose a weakness in the Kubernetes design. <a contenteditable="false" data-primary="worker nodes" data-secondary="East-West network failure between" data-type="indexterm" id="idm45358200272920"/><a contenteditable="false" data-primary="East-West network failure" data-secondary="between worker nodes" data-type="indexterm" id="idm45358200271528"/>This is a failure where East-West traffic only fails between worker nodes. It is not uncommon to provide some network isolation between the worker nodes and control plane, which can result in this inconsistency between control plane connectivity and worker node connectivity. We’ve got an example depicting this in <a data-type="xref" href="#east_west_data_plane_failure">Figure 4-13</a>.</p>&#13;
<figure><div class="figure" id="east_west_data_plane_failure">&#13;
<img src="assets/hcok_0413.png"/>&#13;
<h6><span class="label">Figure 4-13. </span>East-West data plane failure</h6>&#13;
</div></figure>&#13;
<p>The result here is quite interesting. All worker nodes are able to report back as <a contenteditable="false" data-primary="debugging" data-secondary="node failure" data-tertiary="East-West network failure" data-type="indexterm" id="idm45358200266520"/>healthy to the control plane. The worker nodes have no validation of communication between one another. This also includes kube-proxy, as it is not a traditional load balancer. Kube-proxy does not perform any health checking that can validate connectivity between workers. The result is that all pods, services, and endpoints remain fully intact. That sounds great, but the issue is that if any pod tries to route requests across the failed zone boundary, that request will fail and kube-proxy does not perform retries. This can be a hassle to detect and/or debug.</p>&#13;
<p>One approach to take is to include the addition of some cross-zone network validation components at the worker level. We also recommend monitoring and alerting for your application pods so that you can detect and alert.</p>&#13;
<p>There is another secret weapon in this battle. If you do not rely on kube-proxy, but rather a solution that includes things like circuit breakers and automatic retries for timeouts, then you can overcome these situations, and many others, with no modification to your applications or Kubernetes or OpenShift. <a contenteditable="false" data-primary="Istio service mesh project" data-secondary="Envoy in sidecar container" data-type="indexterm" id="idm45358200262392"/><a contenteditable="false" data-primary="Envoy in sidecar container" data-type="indexterm" id="idm45358200261032"/><a contenteditable="false" data-primary="load balancing" data-secondary="Envoy in sidecar container" data-type="indexterm" id="idm45358200259912"/>Service mesh solutions like <a href="https://istio.io">Istio</a> and <a href="https://oreil.ly/3drTt">Red Hat Service Mesh</a> introduce a full mesh of sidecar containers to every pod. These sidecar containers run Envoy, a small, lightweight, efficient load balancer that includes advanced circuit breaking, load balancing, and retry capabilities. In this type of service mesh configuration, each sidecar is smart enough to stop routing traffic to any endpoint IPs with which it fails to <span class="keep-together">communicate</span>.<a contenteditable="false" data-primary="" data-startref="ch04-fail" data-type="indexterm" id="idm45358200255912"/><a contenteditable="false" data-primary="" data-startref="ch04-fail2" data-type="indexterm" id="idm45358200254504"/><a contenteditable="false" data-primary="" data-startref="ch04-fail3" data-type="indexterm" id="idm45358200253128"/><a contenteditable="false" data-primary="" data-startref="ch04-fail4" data-type="indexterm" id="idm45358200251752"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00009">&#13;
<h1>Summary</h1>&#13;
<p>We now have a solid basis for understanding how we can measure and calculate availability of a system. In addition, we have a clear understanding of how Kubernetes provides the tools and architecture to implement a highly available application and service platform. It’s critical to understand how the system functions and what its failure modes are in order for development teams to properly architect their applications and define their Kubernetes and OpenShift resources. SRE and operations teams should familiarize themselves with these failure modes and plan monitoring and alerting accordingly.</p>&#13;
<p>With these Kubernetes tools in hand, we should be ready to build a properly architected Kubernetes or OpenShift cluster with highly available services. We also have the equations necessary to derive an SLO for our services based on the choices we have made. How far can we push our availability? Armed with the analysis and tools outlined in this chapter, you have the ability to make an informed decision about your SLOs and create an implementation that meets your goals.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch01fn26"><sup><a href="ch04.html#ch01fn26-marker">1</a></sup> Unfortunately, PASS has ceased operations, and the talk given in early 2012 is not available.</p><p data-type="footnote" id="ch01fn27"><sup><a href="ch04.html#ch01fn27-marker">2</a></sup> Gavin McCance, “CERN Data Centre Evolution,” talk presented at SCDC12: Supporting Science with Cloud Computing (November 19, 2012), <a href="https://oreil.ly/F1Iw9"><em class="hyperlink">https://oreil.ly/F1Iw9</em></a>.</p><p data-type="footnote" id="ch01fn28"><sup><a href="ch04.html#ch01fn28-marker">3</a></sup> The <a href="https://oreil.ly/6Pmwf">Wikipedia entry on iptables</a> provides more information.</p><p data-type="footnote" id="ch01fn29"><sup><a href="ch04.html#ch01fn29-marker">4</a></sup> The <a href="https://oreil.ly/LSYXT">Wikipedia entry on IP Virtual Server</a> provides more information.</p><p data-type="footnote" id="ch01fn30"><sup><a href="ch04.html#ch01fn30-marker">5</a></sup> See the <a contenteditable="false" data-primary="kube-proxy" data-secondary="documentation online" data-type="indexterm" id="idm45358200456872"/>Kubernetes documentation on <a href="https://oreil.ly/HReuF">kube-proxy</a> for more information.</p><p data-type="footnote" id="ch01fn31"><sup><a href="ch04.html#ch01fn31-marker">6</a></sup> See the Kubernetes documentation on <a href="https://oreil.ly/uzHtJ">Nodes Reliability</a> for more information.</p><p data-type="footnote" id="ch01fn32"><sup><a href="ch04.html#ch01fn32-marker">7</a></sup> For more information, see the <a href="https://oreil.ly/XrFaC">etcd FAQ</a>.</p></div></div></section></body></html>