- en: Chapter 6\. Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, you have learned about the fundamentals of cloud native
    applications—how to design, develop, and operate them as well as how to deal with
    data. To conclude, this chapter aims to provide a laundry list covering tips,
    proven techniques, and proven best practices to build and manage reactive cloud
    native applications.
  prefs: []
  type: TYPE_NORMAL
- en: Moving to Cloud Native
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml#fundamentals), you learned about the process that
    many customers follow when moving traditional applications to the cloud. There
    are many best practices and lessons learned that you should consider when moving
    an existing application into the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking Up the Monolith for the Right Reasons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '“Never change a running system” is a widely used statement in software development,
    and it is also applicable when you consider moving your application to the cloud.
    If your sole requirement is to move your application to the cloud, you can always
    consider moving it on Infrastructure as a Service (IaaS)—in fact, that should
    be your very first step. That said, there are benefits of redesigning your application
    to be cloud native, but you need to weigh the pros and cons. Following are some
    guidelines indicating that a redesign makes sense:'
  prefs: []
  type: TYPE_NORMAL
- en: Your codebase has grown to a point that it takes very long to release an updated
    version and thus you cannot react to new market or customer requirements quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of your applications have different scale requirements. A good example
    is a traditional three-tier application consisting of a frontend, business, and
    data tier. Only the frontend tier might experience heavy load in user requests,
    whereas the business and data tier are still comfortably handling the load. As
    mentioned in [Chapter 2](ch02.xhtml#fundamentals) and [Chapter 3](ch03.xhtml#designing_cloud-native_applications),
    cloud native applications allow you to scale services independently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better technology choices have emerged. There is constant innovation in the
    technology sector, and some new technologies might be better suited for parts
    of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After you have decided that you want to redesign your application, you need
    to consider many things. In the following sections, we provide a comprehensive
    look at these considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Decouple Simple Services First
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by breaking off components that provide simpler functionality because
    they usually do not have a lot of dependencies and, thus, are not deeply integrated
    within the monolith.
  prefs: []
  type: TYPE_NORMAL
- en: Learn to Operate on a Small Scale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the first service as a learning path for how to operate in a cloud native
    world. Starting with a simple service, you can focus on setting up automation
    to provision the infrastructure and the CI/CD pipeline so that you become familiar
    with the process of developing, deploying, and operating a cloud native service.
    Having a simple service and minimal infrastructure will allow you to learn, exercise,
    and improve your new process ahead of time, without substantial impact on the
    monolith and your end users.
  prefs: []
  type: TYPE_NORMAL
- en: Use an Anticorruption Layer Pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nothing is perfect, especially in the software development world, so you will
    eventually end up with a new service that makes calls back to the monolith. In
    this case, you might want to use the *Anticorruption Layer* pattern. This pattern
    is used to implement a facade or adapter between components that don’t share the
    same semantics. The purpose of the anticorruption layer is to translate the request
    from one component to another; for example, implementing protocol or schema translations.
  prefs: []
  type: TYPE_NORMAL
- en: To implement this, you design and create a new API in the monolith that makes
    calls through the anticorruption layer in the new service, as shown in [Figure 6-1](#anticorruption_layer_pattern).
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0601](Images/clna_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. *Anticorruption Layer* pattern
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a couple of considerations when you are using this approach. As [Figure 6-1](#anticorruption_layer_pattern)
    illustrates, the anticorruption layer is a service on its own, so you need to
    think about how to scale and operate the layer. Also, you need to think about
    whether you want to retire the anticorruption layer after the monolithic application
    has been fully moved into a cloud native application.
  prefs: []
  type: TYPE_NORMAL
- en: Use a Strangler Pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are decomposing your monolith to move to microservices and functions,
    you can use a gateway and a pattern such as a *Strangler* pattern. The idea behind
    the Strangler pattern is to use the gateway as a facade while you gradually move
    the backend monolith to a new architecture—either services, functions, or a combination
    of both. As you’re making progress breaking up the monolith and implementing those
    pieces of functionality as services or functions, you update the gateway to redirect
    requests to the new functionality, instead as shown in [Figure 6-2](#migrating_from_monolith_using_the_strang).
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0602](Images/clna_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Migrating from monolith using the *Strangler* pattern
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the Strangler pattern might not be suitable for the instance in which
    you can’t intercept the requests going to the backing monolith. The pattern also
    might not make sense if you have a smaller system, for which it’s easier and faster
    to replace the entire system, instead of gradually moving it.
  prefs: []
  type: TYPE_NORMAL
- en: The Anticorruption Layer and Strangler patterns have been proven many times
    as good approaches to move a monolithic legacy application to a cloud native application
    because both promote a gradual approach.
  prefs: []
  type: TYPE_NORMAL
- en: Come Up with a Data Migration Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a monolith, you are usually working with a centrally shared datastore where
    data is read from and written to by multiple places and services. To truly move
    to the cloud native architecture, you need to decouple data as well. Your data
    migration strategy might consist of multiple phases, especially if you can’t migrate
    everything at the same time. However, in most cases, you will need to do an incremental
    migration while keeping the entire system running. A gradual migration will probably
    involve writing data twice (to the new and old datastore) for a while. After you
    have data in both places and synchronized, you will need to modify where the data
    is being read from and then read everything from the new store. Finally, you should
    be able to stop writing data to the old store completely.
  prefs: []
  type: TYPE_NORMAL
- en: Rewrite Any Boilerplate Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monoliths will usually have large amounts of code that deals with the configuration,
    data caching, datastore access, and so on and is probably using older libraries
    and frameworks. When moving capabilities to a new service, you should rewrite
    this code. The best option is to throw away the old code and rewrite it from scratch
    instead of modifying the existing code and molding it so it fits the new service.
  prefs: []
  type: TYPE_NORMAL
- en: Reconsider Frameworks, Languages, Data Structures, and Datastores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Moving to microservices gives you an option to rethink the existing implementation.
    Are there new frameworks or languages that you could use to rewrite the current
    code that provide better features and functionalities for your scenarios? If it
    makes sense to rewrite the code, do it! Also, reconsider any data structures in
    the current code. Would they still make sense when moved to a service? You should
    also evaluate whether you want to use different datastores. [Chapter 4](ch04.xhtml#working_with_data)
    outlines what datastores are best suited for certain data structures and query
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Retire Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After you’ve created a new service and all the traffic is redirected to that
    service, you need to retire and remove the old code that resides in the monolith.
    Using this approach, you are shrinking the monolith and expanding your services.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring Resiliency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resiliency is the ability of a system to recover from failures and continue
    to function and serve requests. Resiliency is not about avoiding failures; instead,
    it is all about responding to failures in such a manner that avoids significant
    downtime or data loss.
  prefs: []
  type: TYPE_NORMAL
- en: Handle Transient Failures with Retries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Requests can fail due to multiple reasons such as network latency, dropped
    connections, or timeouts if downstream services are busy. You can avoid most of
    these failures if you retry the request. Retrying can also improve the stability
    of your application. However, before blindly retrying all requests, you need to
    implement a bit of logic that determines whether the request should be retried.
    If the failure is not transient or there is a likelihood that a retry won’t be
    successful, it is better for the component to cancel the request and respond with
    an appropriate error message. For example, retrying a failed login because of
    an incorrect password is futile and retries won’t help. If failure is due to a
    rare network issue, you can retry the request right away given that the same issue
    probably won’t persist. Finally, if the failure happens because the downstream
    service is busy or you are being rate limited, for example, you should retry after
    a delay. Here are some common strategies for delaying between retry operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Constant
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the same time between each attempt.
  prefs: []
  type: TYPE_NORMAL
- en: Linear
  prefs: []
  type: TYPE_NORMAL
- en: Incrementally increase the time between each retry. For example, you can start
    with one second, then three seconds, five seconds, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential back-off
  prefs: []
  type: TYPE_NORMAL
- en: Exponentially increase time between each retry. For example, start with 3 seconds,
    12 seconds, 30 seconds, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what type of failure you are dealing with, you can also immediately
    retry the operation once and then use one of the delay strategies mentioned in
    the preceding list. You can handle retries in the component’s source code by using
    the retry and transient failure logic provided by many of the service SDKs, or
    at the infrastructure layer if you are using a service mesh, such as Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Use a Finite Number of Retries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regardless of which retry strategy you’re using, always make sure to use a finite
    number of retries. Having an infinite number of retries will cause an unnecessary
    strain on the system.
  prefs: []
  type: TYPE_NORMAL
- en: Use Circuit Breakers for Nontransient Failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of a circuit breaker is to prevent components from doing operations
    that will likely fail and are not transient. Circuit breakers monitor the number
    of faults, and based on that information decide whether the request should continue
    or an error should be returned without even invoking the downstream service. If
    a circuit breaker trips, the number of failures has exceeded a predefined value,
    and the circuit breaker will automatically return errors for a preset time. After
    the preset time elapses, it will reset the failure count and allow requests to
    go through to the downstream service again. A well-known library that implements
    the circuit breaker pattern is Hystrix from Netflix. If you are using a service
    mesh like Istio or Envoy proxies, you can take advantage of the circuit breaker
    implementation in those solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Graceful Degradation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Services should degrade gracefully, so even if they fail, they still provide
    an acceptable user experience if it makes sense. For example, if you can’t retrieve
    the data, you could display a cached version of the data, and as soon as the data
    source recovers, you show the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: Use a Bulkhead Pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Bulkhead* pattern refers to isolating different parts of your system into
    groups in such a way that if one fails, the others will continue running unaffected.
    Grouping your services this way allows you isolate failures and continue serving
    requests even when there’s a failure.
  prefs: []
  type: TYPE_NORMAL
- en: Implement Health Checks and Readiness Checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implement a health check and a readiness check for every service you deploy.
    The platform can use these to determine whether the service is healthy and performing
    correctly as well as when the service is ready to start accepting requests. In
    Kubernetes, health checks are called *probes*. The liveness probe is used to determine
    when a container should be restarted, whereas the readiness probe determines whether
    a pod should start receiving traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The initial delay defines the number of seconds after the container has started
    before liveness or readiness probes are active, whereas the period defines how
    often the probe is performed. There are also additional settings such as success/failure
    threshold and timeouts that you can use to fine-tune the probes.
  prefs: []
  type: TYPE_NORMAL
- en: Define CPU and Memory Limits for Your Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should define CPU and memory limits to isolate resources and prevent certain
    services instances from consuming too many resources. In Kubernetes, you can achieve
    this by defining the memory and CPU limits within the pod definition.
  prefs: []
  type: TYPE_NORMAL
- en: Implement Rate Limiting and Throttling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You use rate limiting and throttling to limit the number of incoming or outgoing
    requests for a service. Implementing those can help you to keep your service responsive
    even in the case of a sudden spike in requests. Throttling, on the other hand,
    is often used for outgoing requests. Think about using it when you want to control
    the number of requests sent to an external service to minimize the costs or to
    make sure that your service does not look like the origin of a Denial-of-Service
    attack.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security in the cloud native world is based on the shared responsibility model.
    The cloud providers are not solely responsible for the security of their customers’
    solutions; instead, they share that responsibility with the customers. From an
    application perspective you should consider adopting the defense-in-depth concept,
    which is discussed in [Chapter 3](ch03.xhtml#designing_cloud-native_applications).
    The best practices listed in this section will help you to ensure security.
  prefs: []
  type: TYPE_NORMAL
- en: Treat Security Requirements the Same as Any Other Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having fully automated processes is in spirit of the cloud native development.
    To achieve this, all security requirements must be treated as any other requirement
    and be pushed through your development pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate Security in Your Designs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you’re planning and designing your cloud native solutions, you need to think
    about security and incorporate the security features in your design. As part of
    your design, you also should call out any additional security concerns that need
    to be addressed during component development.
  prefs: []
  type: TYPE_NORMAL
- en: Grant Least-Privileged Access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your services or functions need access to any resources, they should be granted
    specific permissions that have the least amount of access set to them. For example,
    if your service is reading only from the database, it does not need to use an
    account that has write permissions.
  prefs: []
  type: TYPE_NORMAL
- en: Use Separate Accounts/Subscriptions/Tenants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the terminology of your cloud provider, your cloud native system
    should use separate accounts, subscriptions, and/or tenants. At the very least,
    you will need a separate account for every environment you will be using; that
    way, you can ensure proper isolation between environments.
  prefs: []
  type: TYPE_NORMAL
- en: Securely Store All Secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Any secrets within your system, used either by your components or Continuous
    Integration/Continuous Development (CI/CD) pipeline, need to be encrypted and
    securely stored. It might sound like a no-brainer, but never store any secrets
    in plain text: always encrypt them. It’s always best to use existing and proven
    secret management systems that take care of these things for you. The simplest
    option is to use Kubernetes Secrets to store the secrets used by services within
    the cluster. Secrets are stored in etcd, a distributed key/value store. However,
    managed and centralized solutions have multiple advantages over Kubernetes secrets:
    everything is stored in a centralized location, you can define access control
    policies, secrets are encrypted, auditing support is provided, and more. Some
    examples of managed solutions are Microsoft Azure Key Vault, Amazon Secrets Manager,
    and HashiCorp Vault.'
  prefs: []
  type: TYPE_NORMAL
- en: Obfuscate Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any data your component uses needs to be properly obfuscated. For example, you
    never want to log any data classified as Personally Identifiable Information (PII)
    in plain text; if you need to log or store it, ensure that it’s either obfuscated
    (if logging it) or encrypted (if storing it).
  prefs: []
  type: TYPE_NORMAL
- en: Encrypt Data in Transit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encrypting data in transit protects your data if communications are intercepted
    while the data moves between components. To achieve this protection, you need
    to encrypt the data before transmitting it, authenticate the endpoints, and finally
    decrypt and verify the data after it reaches the endpoint. Transport Layer Security
    (TLS) is used to encrypt data in transit for transport security. If you are using
    a service mesh, TLS might already be implemented between the proxies in the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Use Federated Identity Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using an existing federated identity management service (Auth0, for example)
    to handle how users sign up, sign in, and sign out allows you to redirect users
    to a third-party page for authentication. Your component should delegate authentication
    and authorization whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: Use Role-Based Access Control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Role-Based Access Control (RBAC) has been around for a long time. RBAC is a
    control access mechanism around roles and privileges, and as you have learned,
    it can be a great asset to your defense-in-depth strategy because it allows you
    to provide fine-grained access to users to only the resources they need. Kubernetes
    RBAC, for example, controls permissions to the Kubernetes API. Using RBAC, you
    can allow or deny specific users from creating deployments or listing pods, and
    more. It’s a good practice to scope Kubernetes RBAC permissions by namespaces
    rather than cluster roles.
  prefs: []
  type: TYPE_NORMAL
- en: Isolate Kubernetes Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any pods running in a Kubernetes cluster are not isolated and can accept requests
    from any source. Defining a network policy on pods allows you to isolate pods
    and make them reject any connections that are not allowed by the policy. For example,
    if a component in your system is compromised, a network policy will prevent the
    malicious actor from communicating with services with which you don’t want them
    to communicate. Using a NetworkPolicy resource in Kubernetes, you can define a
    pod selector and detailed ingress and egress policies.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most modern applications have some need to store and work with data. A growing
    number of data storage and analytics services are available as cloud provider–managed
    services. Cloud native applications are designed to take full advantage of cloud
    provider–managed data systems and are designed to evolve to take advantage of
    a growing number of features. When working with data in the cloud, many of the
    standard data best practices still apply: have a disaster recovery plan, keep
    business logic out of the database, avoid overfetching or excessively chatty I/O,
    use data access implementations that prevent SQL injections attacks, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Use Managed Databases and Analytics Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever possible use a managed database. Provisioning a database on virtual
    machines (VMs) or in a Kubernetes cluster can often be a quick and easy task.
    Production databases that require backups and replicas can quickly increase the
    time and burden of operating data storage systems. By offloading the operational
    burden of deploying and managing a database, teams are able to focus more on development.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, a data storage technology might not be available as a managed
    service or it might be necessary to have access to some configurations that are
    not available in a managed version of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Use a Datastore That Best Fits Data Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing on-premises applications, architects would often try to avoid
    using multiple databases. Each database technology used would require database
    administrators with the skillset to deploy and manage the database, significantly
    increasing the operational costs of the application. The reduced operational costs
    of cloud-managed databases make it possible to use multiple different types of
    datastores to put data in a system best suited for the data type, read, and write
    requirements. Cloud native applications take full advantage of this, using multiple
    data storage technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Keep Data in Multiple Regions or Zones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Store production data for applications across multiple regions or zones. How
    the data is stored across the zones or regions will depend on the application’s
    availability requirements; for example, the data might be backups or a replicated
    database. If a cloud provider experiences a failure of a zone or region, the data
    can be available to be used for recovery or failover.
  prefs: []
  type: TYPE_NORMAL
- en: Use Data Partitioning and Replication for Scale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud native applications are designed to scale out as opposed to scale up.
    Scaling a database up is achieved by increasing the resources available to a database
    instance; for example, adding more cores or memory. This ultimately encounters
    a hard limit and can be costly. Scaling databases out is achieved through distributing
    the data across multiple instances of a database. The database is partitioned,
    or broken up, and stored in multiple databases.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid Overfetching and Chatty I/O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfetching is when an application requests data from a database but needs
    only a fraction of the data for the operation. For example, an application might
    display a list of orders with a simple summary but request the entire order and
    order details without needing it. A chatty application, on the other hand, makes
    a lot of small calls to complete an operation when a single request can be made
    to the database.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Put Business Logic in the Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Too many application scaling issues are the result of putting too much logic
    in the database. Databases made it easy to put business logic inside the database
    by supporting standard development languages, and it became convenient to perform
    these tasks in the database. This often introduces scaling issues because a database
    is commonly an expensive shared resource.
  prefs: []
  type: TYPE_NORMAL
- en: Test with Production-like Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create automation to anonymize production data that can be updated with new
    rules as the data changes. Applications should be tested with production-like
    data. Data is sometimes pulled from production systems, scrubbed, and loaded into
    test systems to provide production-like data. You should automate this process
    so that it is easy to update as the data changes.
  prefs: []
  type: TYPE_NORMAL
- en: Handle Transient Failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the resiliency section of this chapter, failures will happen.
    Expect failures when making calls to a database and be prepared to handle them.
    Many of the database client libraries support transient fault handling already.
    It’s important to understand whether they do and how it’s supported.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance indicates how well a system can execute an operation within a certain
    time frame, whereas scalability refers to how a system can handle load increase
    without impact on the performance. Predicting periods of increased activity to
    a system can be tough, so the components need to be able to scale out as needed
    to meet the increased demand and then scale down, after the demand decreases.
    The subsections that follow present some best practices to help you achieve optimal
    performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Design Stateless Services That Scale Out
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Services should be designed to scale out. Scaling out is an approach to increasing
    the scale of a service by adding more instances of a service. Scaling up is an
    approach to scaling a service by adding more resources like memory or cores, but
    this method generally has a hard limit. By designing a service to scale out and
    back in, you can scale the service to handle variations in the load without impacting
    the availability of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful applications are inherently difficult to scale and should be avoided.
    If stateful services are necessary, it’s generally best to separate the functionality
    from the application and use a partitioning strategy and managed services if they
    are available.
  prefs: []
  type: TYPE_NORMAL
- en: Use Platform Autoscaling Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When possible, use any autoscaling features that are built into the platform
    before implementing your own. Kubernetes offers Horizontal Pod Autoscaler (HPA).
    HPA scales the pods based on the CPU, memory, or custom metrics. You specify the
    metric (e.g., 85% of CPU or 16 GB of memory) and the minimum and maximum number
    of pod replicas. After the target metric is reached, Kubernetes automatically
    scales the pods. Similarly, cluster autoscaling scales the number of cluster nodes
    if pods can’t be scheduled. Cluster autoscaling uses the requested resources in
    the pod specification to determine whether nodes should be added.
  prefs: []
  type: TYPE_NORMAL
- en: Use Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Caching is a technique that can help improve the performance of your component
    by temporarily storing frequently used data in storage that’s close to the component.
    This improves the response time because the component does not need to go to the
    original source. The most basic type of cache is an in-memory store that is being
    used by a single process. If you have multiple instances of your component, each
    instance will have its own independent copy of the in-memory cache. This can cause
    consistency problems if data is not static because the different instances will
    have different versions of cached data. To solve this problem, you can use shared
    caching, which ensures that different component instances use the same cached
    data. In this case, cache is stored separately, usually in front of the database.
  prefs: []
  type: TYPE_NORMAL
- en: Use Partitioning to Scale Beyond Service Limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud services will often have some defined scale limits. It’s important to
    understand the scalability limits of each of the services used and how much they
    can be scaled up. If a single service is unable to scale to meet the application’s
    requirements, create multiple service instances and partition work across the
    instances. For example, if a managed gateway was capable of handling 80% of the
    application’s intended load, create another gateway and split the services across
    the gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much of the software development life cycle (SDLC) and general server architecture
    best practices are the same for serverless architectures. Given serverless is
    a different operating model, there are, however, some best practices specific
    to functions.
  prefs: []
  type: TYPE_NORMAL
- en: Write Single-Purpose Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Follow the single-responsibility principle and only write functions that have
    a single responsibility. This will make your functions easier to reason about,
    test, and, when the time comes, debug.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Chain Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, functions should push messages/data to a queue or a datastore to
    trigger any other functions if needed. Having one or more functions call other
    functions is often considered an antipattern that additionally increases your
    cost and makes the debugging more difficult. If your application requires the
    daisy-chaining of functions, you should consider using function offerings such
    as Azure Durable Functions or AWS Step Functions.
  prefs: []
  type: TYPE_NORMAL
- en: Keep Functions Light and Simple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each function should do just one thing and rely on only a minimal number of
    external libraries. Any extra and unnecessary code in the function makes the function
    bigger in size, and that affects the start time.
  prefs: []
  type: TYPE_NORMAL
- en: Make Functions Stateless
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Don’t save any data in your functions because new function instances usually
    run in their own isolated environment and don’t share anything with other functions
    or invocations of the same function.
  prefs: []
  type: TYPE_NORMAL
- en: Separate Function Entry Point from the Function Logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Functions will have an entry point invoked by the function framework. Framework-specific
    context is generally passed to the function entry point, along with invocation
    context. For example, if the function is invoked through an HTTP request like
    an API gateway, the context will contain HTTP-specific details. The entry-point
    method should separate these entry-point details from the rest of the code. This
    will improve manageability, testability, and portability of the functions.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid Long-Running Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most Function as a Service (FaaS) offerings have an upper limit for execution
    time per function. As a result, long-running functions can cause issues such as
    increased load times and timeouts. Whenever possible, refactor large functions
    into smaller ones that work together.
  prefs: []
  type: TYPE_NORMAL
- en: Use Queues for Cross-Function Communication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of passing information among one another, functions should use a queue
    to which to post the messages. Other functions can be triggered and executed based
    off the events that happen on that queue (item added, removed, updated, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DevOps practice provides the foundation necessary for organizations to make
    the best use of cloud technologies. Cloud native applications utilize DevOps principles
    and best practices that are detailed in [Chapter 5](ch05.xhtml#devops).
  prefs: []
  type: TYPE_NORMAL
- en: Deployments and Releases Are Separate Activities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to make a distinction between deployment and release. Deployment
    is the act of taking the built component and placing it within an environment—the
    component is fully configured and ready to go; however, there is no traffic being
    sent to it. As part of the component release, we begin to allow traffic to the
    deployed component. This separation allows you to do gradual releases, A/B testing,
    and canary deployments in a controlled manner.
  prefs: []
  type: TYPE_NORMAL
- en: Keep Deployments Small
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each component deployment should be a small event that can be performed by a
    single team in a short time. There is no general rule about how small a deployment
    should be and how much time it should take to deploy a component, because this
    is highly dependent on the component, your process, and the change to the component.
    A good approach is to be able to roll out a critical fix within a day.
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD Definition Lives with the Component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to store and version any CI/CD configuration and dependencies alongside
    the component. Each push to the component’s branch triggers the pipeline and executes
    jobs defined in the CI/CD configuration. To control component deployments to different
    environments (development, staging, production), you can use the Git branch names
    and configure your pipeline to deploy the master branch only to a production environment,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Consistent Application Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a consistently reliable and repeatable deployment process, you can minimize
    errors. Automate as many processes as possible and ensure that you have a rollback
    plan defined in case deployment fails.
  prefs: []
  type: TYPE_NORMAL
- en: Use Zero-Downtime Releases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To maximize the availability of your system during releases, consider using
    zero-downtime releases such as blue/green or canary. Using one of these approaches
    also allows you to quickly roll back the update in case of failures.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Modify Deployed Infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Infrastructure should be immutable. Modifying deployed infrastructure can quickly
    get out of hand, and keeping track of what changed can be complicated. If you
    need to update the infrastructure, redeploy it instead.
  prefs: []
  type: TYPE_NORMAL
- en: Use Containerized Build
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To avoid configuring build environments, package your build process into Docker
    containers. Consider using multiple images and containers for builds instead of
    creating a single, monolithic build image.
  prefs: []
  type: TYPE_NORMAL
- en: Describe Infrastructure Using Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Infrastructure should be described using either cloud provider’s declarative
    templates or a programming language or scripts that provision the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Use Namespaces to Organize Services in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every resource in a Kubernetes cluster belongs to a namespace. By default, newly
    created resources go into a namespace called *default*. For better organization
    of services, it is a good practice to use descriptive names and group services
    into bounded contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Isolate the Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a dedicated production cluster and physically separate the production cluster
    for your development, staging, or testing environments.
  prefs: []
  type: TYPE_NORMAL
- en: Separate Function Source Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each function must be independently versioned and have its own dependencies.
    If that’s not the case, you will end up with a monolith and a tightly coupled
    codebase.
  prefs: []
  type: TYPE_NORMAL
- en: Correlate Deployments with Commits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pick a branching strategy that allows you to correlate the deployments to specific
    commits in your branch and that also allows you to identify which version of the
    source code is deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Logging, Monitoring, and Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application and infrastructure logging can provide much more value than just
    root-cause analysis. A proper logging solution will provide valuable insights
    into applications and systems, and it’s often necessary for monitoring the health
    of an application and alerting operations of important events. As cloud applications
    become more distributed, logging and instrumentation become increasingly challenging
    and important.
  prefs: []
  type: TYPE_NORMAL
- en: Use a Unified Logging System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a unified logging system capable of capturing log messages across all services
    and levels of a system and store them in a centralized store. Whether you move
    all logs to a centralized store for analysis and search, or you leave them on
    the machine with the necessary tools in place to run a distributed query, it’s
    important that an engineer can find and analyze logs without having to go from
    one system to the next.
  prefs: []
  type: TYPE_NORMAL
- en: Use Correlation IDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Include a unique correlation ID (CID) that is passed through all services. If
    one of the services fails, the correlation ID is used to trace the request through
    the system and pinpoint where the failure occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Include Context with Log Entries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each log entry should contain additional context that can help when you are
    investigating issues. For example, include all exception handling, retry attempts,
    service name or ID, image version, binary version, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Common and Structured Logging Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decide on a common and structured logging format that all components will use.
    This will allow you to quickly search and parse the logs later on. Also, make
    sure you are using the same time zone information in all your components. In general,
    it is best to adhere to a common time format such as Coordinated Universal Time
    (UTC).
  prefs: []
  type: TYPE_NORMAL
- en: Tag Your Metrics Appropriately
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to using clear and unique metric names, make sure that you are storing
    any additional information, such as component name, environment, function name,
    region, and so forth, in the metric tags. With tags in place, you can create queries,
    dashboards, and reports using multiple dimensions (e.g., average latency across
    a specific region or across regions for a specific function).
  prefs: []
  type: TYPE_NORMAL
- en: Avoid Alert Fatigue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The sheer number of metrics makes it difficult to determine how to set up the
    alerting and what to alert on. If you are firing off too many alerts, eventually
    people will stop paying attention to them and no longer take them seriously. Also,
    investigating a bunch of alerts can become overwhelming and it could be the only
    thing your team is doing. It is important to classify alerts by severity: low,
    moderate, and high. The purpose of low-severity alerts is to potentially use them
    later, when doing root-cause analysis of a high-severity alert. You can use them
    to uncover certain patterns, but they do not require any immediate action when
    fired. A moderate-severity alert should either create a notification or open a
    ticket. These are the alerts you want to look at, but are not high priority and
    don’t need immediate action. They could represent a temporary condition (increase
    demand, for example) that eventually goes away. They also give you an early warning
    of a possible high-severity alert. Finally, high-severity alerts are the ones
    that will wake people up in the middle of the night and require immediate action.
    Recently, machine learning–based approaches to automatically triage issues and
    raise alerts are gaining in popularity, and the term AIOps has even been introduced.'
  prefs: []
  type: TYPE_NORMAL
- en: Define and Alert on Key Performance Indicators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud native systems will have a plethora of signals that are being emitted
    and monitored. You need to filter down those signals and determine which ones
    are the most important and valuable. These Key Performance Indicators (KPIs) give
    you insight into the health of your system. For example, one KPI is latency, which
    measures the time it takes to service a request. If you begin seeing latency increase
    or deviate from an acceptable range, it is probably time to issue an alert and
    have someone take a look at it. In addition to KPIs, you can use other signals
    and metrics to determine why something is failing.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Testing in Production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using continuous testing you can generate requests that are sent throughout
    the system and simulate real users. You can utilize this traffic to get test coverage
    for the components, discover potential issues, and test your monitoring and alerting.
    Following are some common continuous testing practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canary testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These practices are discussed in [Chapter 5](ch05.xhtml#devops).
  prefs: []
  type: TYPE_NORMAL
- en: Start with Basic Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you are always collecting traffic (how much demand is placed on
    the component), latency (the time it takes to service a request), and errors (rate
    of requests that fail) for each component in your system.
  prefs: []
  type: TYPE_NORMAL
- en: Service Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Service communication is an important part of cloud native applications. Whether
    it’s a client communicating with a backend, a service communicating with a database,
    or the individual services in a distributed architecture communicating with one
    another, these interactions are an important part of cloud native applications.
    Many different forms of communication are used depending on the requirements.
    The following subsections offer some best practices for service communication.
  prefs: []
  type: TYPE_NORMAL
- en: Design for Backward and Forward Compatibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With backward compatibility, you ensure that new functionality added to a service
    or component does not break any existing service. For example, in [Figure 6-3](#backward_compatibility-id1),
    Service A v1.0 works with Service B v1.0\. Backward compatibility means that the
    release of Service B v1.1 will not break the functionality of Service A.
  prefs: []
  type: TYPE_NORMAL
- en: '![clna 0603](Images/clna_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Backward compatibility
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To ensure backward compatibility, any new fields added to the API should be
    optional or have sensible defaults. Any existing fields should never be renamed,
    because that will break the backward compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Parallel change*, also known as the *Expand and Contract* pattern, can be
    used to safely introduce backward-incompatible changes. As an example, say a service
    owner would like to change a property or resource on an interface. The service
    owner will expand the interface with a new property or resource, and then after
    all consumers have had a chance to move the service interface, the previous property
    is removed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your system or components need to ensure rollback functionality, you will
    need to think about the forward compatibility as you’re making changes to your
    service. Forward compatibility means that your components are compatible with
    future versions. Your service should be able to accept “future” data and messaging
    formats and handle them appropriately. A good example of forward compatibility
    is HTML: when it encounters an unknown tag or attribute, it’s not going to fail;
    it will just skip it.'
  prefs: []
  type: TYPE_NORMAL
- en: Define Service Contracts That Do Not Leak Internal Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A service that exposes an API should define contracts and test against the contracts
    when releasing updates. For example, a REST-based service would generally define
    a contract in the OpenAPI format or as documentation, and consumers of the service
    would build to this contract. Updates to the service can be pushed, and as long
    as it doesn’t introduce any breaking changes to the API contract, these releases
    would not affect the consumer. Leaking internal implementations of a service can
    make it difficult to make changes and introduces coupling. Don’t assume a consumer
    is not using some piece of data exposed through the API.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Services that publish messages to a queue or a stream should also define a contract
    in the same way. The service publishing the events will generally own the contract.
  prefs: []
  type: TYPE_NORMAL
- en: Prefer Asynchronous Communication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use asynchronous communication whenever possible. It works well with distributed
    systems and decouples the execution of two or more services. A message bus or
    a stream is often used when implementing this approach, but you could use direct
    calls through something like gRPC as well. Both use a message bus as a channel.
  prefs: []
  type: TYPE_NORMAL
- en: Use Efficient Serialization Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed applications like those built using a microservices architecture
    rely more heavily on communications and messaging between services. The data serialization
    and deserialization can add a lot of overhead in service communication.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In one case, serialization and deserialization were found to account for nearly
    40% of the CPU utilization across all the services. Replacing the standard JSON
    serialization library with a custom one reduced this overhead to roughly 15% of
    overall CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Use efficient serialization formats like protocol buffers, commonly used in
    gRPC. Understand the trade-offs with the different serialization formats, because
    tooling and consumer requirements might not make this a feasible option. You can
    also use other techniques to reduce the need for serialization in some services
    by placing some of the data into headers. For example, if a service receives a
    request and operates on only a handful of fields in a large message payload before
    passing it to a downstream service, by putting these fields into headers the service
    does not need to deserialize or reserialize the payload. The service reads and
    writes headers and then simply passes the entire payload through to the downstream
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Use Queues or Streams to Handle Heavy Loads and Traffic Spikes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A queue or a stream between components acts as a buffer and stores the message
    until it is retrieved. Using a queue allows the components to process the messages
    at their own pace, regardless of the incoming volume or load. Consequently, this
    helps maximize the availability and scalability of your services.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Requests for Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Queues can be used for batching multiple requests and performing an action only
    once. For example, it is more efficient to write 1,000 batched entries into the
    database instead of one entry at a time 1,000 times.
  prefs: []
  type: TYPE_NORMAL
- en: Split Up Large Messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sending, receiving, and manipulating large messages requires more resources
    and can slow down your entire system. The *Claim-Check* pattern talks about splitting
    a large message into two parts. You store the entire message in an external service
    (database, for example) and send only the reference to the message. Any interested
    message receivers can use the reference to obtain the full message from the database.
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s possible to run most applications in a Docker container without very much
    effort. However, there are some potential pitfalls when running containers in
    production and streamlining the build, deployment, and monitoring. A number of
    best practices have been identified to help avoid the pitfalls and improve the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Store Images in a Trusted Registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any images running on the platform should come from the trusted container image
    registry. Kubernetes exposes a webhook (validating admission) that can be used
    to ensure pods can use images only from a trusted registry. If you’re using Google
    Cloud, you can take advantage of the binary authorization security measure that
    ensures only trusted images are deployed on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Utilize the Docker Build Cache
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the build cache when building Docker images can speed up the build process.
    All images are built up from layers, and each line in the Dockerfile contributes
    a layer to the final image. During the build, Docker will try to reuse a layer
    from a previous build instead of building it again. However, it can reuse only
    the cached layers if all previous build steps used it as well. To get the most
    out of the Docker build cache, put the commands that change more often (e.g.,
    adding the source code to the image, building the source code) at the end of the
    Dockerfile. That way, any preceding steps will be reused.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Run Containers in Privileged Mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running containers in privileged mode allows access to everything on the host.
    Use the security policy on the pod to prevent containers from running in privileged
    mode. If a container does for some reason require privileged mode to make changes
    to the host environment, consider separating that functionality from the container
    and into the infrastructure provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: Use Explicit Container Image Tags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Always tag your container images with specific tags that tightly link the container
    image to the code that is packaged in the image. To tag the images properly, you
    can either use a Git commit hash that uniquely identifies the version of the code
    (e.g., `1f7a7a472`) or use a semantic version (e.g., `1.0.1`). The tag `latest`
    is used as a default value if no tag is provided; however, because it’s not tightly
    linked to the specific version of the code, you should avoid using it. The latest
    tag should never be used in a production environment because it can cause inconsistent
    behavior that can be difficult to troubleshoot.
  prefs: []
  type: TYPE_NORMAL
- en: Keep Container Images Small
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to taking up less space in a container registry or the host system
    using the image to run a container, smaller images improve image push and pull
    performance. This in turn improves the performance when you start containers as
    part of deploying or scaling a service. The application and its dependencies will
    have some impact on the size of the image, but you can reduce most of the image
    size by using lean base images and ensuring that unnecessary files are not included
    in the image. For example, the alpine 3.9.4 image is only 3 MB, with the Debian
    stretch image at 45 MB, and the CentOS 7.6.1810 at 75 MB. The distributions generally
    offer a slim version that removes more from the base image that might not be needed
    by the application. Generally, there are two things to keep in mind for keeping
    images lean:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a lean base image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include only the files needed for the operation of the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the Container Builder pattern to create lean images by separating
    the images used to build the artifacts from the base image used to run the application.
    Docker’s multistage build is often used to implement this. You can create Docker
    build files that can start from different images used for executing the commands
    to build and test artifacts, and then define another base image as part of creating
    the image to run the application.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using a *.dockerignore* file can improve build speed by excluding files that
    are not needed in the Docker build.
  prefs: []
  type: TYPE_NORMAL
- en: Run One Application per Container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Always run a single application within a container. Containers were designed
    to run a single application, with the container having the same life cycle as
    the application running in the container. Running multiple applications within
    the same container makes it difficult to manage, and you might end up with a container
    in which one of the processes has crashed or is unresponsive.
  prefs: []
  type: TYPE_NORMAL
- en: Use Verified Images from Trusted Repositories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s a large and growing number of publicly available images that are helpful
    when working with containers. Docker repository tags are mutable, so it’s important
    to understand that the images can change. When using images in an external repository
    it’s best to copy or re-create them from the external repository into one managed
    by the organization. The organization’s repository is usually closer to the CI
    services, and this approach removes another service dependency that could impact
    build.
  prefs: []
  type: TYPE_NORMAL
- en: Use Vulnerability Scanning Tools on Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to be aware of any vulnerabilities that affect your images because
    this can compromise the security of your system. If a vulnerability is discovered,
    you need to rebuild the image with the patches and fixes included and then redeploy
    it. Some cloud providers offer vulnerability scanning with their image registry
    solutions, so make sure you are taking advantage of those features.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scan an image as often as possible because new cybersecurity vulnerabilities
    and exposures (CVE) are released daily.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Store Data in Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Containers are ephemeral—they can be stopped, destroyed, or replaced without
    any loss of data. If the service running in a container needs to store data, use
    a volume mount to save the data. The contents in a volume exist outside the life
    cycle of a container and a volume does not increase the size of a container. If
    the container requires temporary nonpersistent writes, use a tmpfs mount, which
    will improve performance by avoiding writes to a container’s writable layer.
  prefs: []
  type: TYPE_NORMAL
- en: Never Store Secrets or Configuration Inside an Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hardcoding any type of secrets within an image is something you want to avoid.
    If your container requires any secrets, define them within environment variables
    or as files, mounted to the container through a volume.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We could easily fill an entire book covering best practices for cloud native
    applications given the number of technologies involved. However, there are certain
    areas that have been coming up repeatedly in customer conversations, and this
    chapter has covered a collection of best practices, tips, and proven patterns
    for cloud native applications for those areas. You should have a better understanding
    of the factors you may want to consider.
  prefs: []
  type: TYPE_NORMAL
