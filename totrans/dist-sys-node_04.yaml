- en: Chapter 3\. Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running redundant copies of a service is important for at least two reasons.
  prefs: []
  type: TYPE_NORMAL
- en: The first reason is to achieve *high availability*. Consider that processes,
    and entire machines, occasionally crash. If only a single instance of a producer
    is running and that instance crashes, then consumers are unable to function until
    the crashed producer has been relaunched. With two or more running producer instances,
    a single downed instance won’t necessarily prevent a consumer from functioning.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason is that there’s only so much throughput that a given Node.js
    instance can handle. For example, depending on the hardware, the most basic Node.js
    “Hello World” service might have a throughput of around 40,000 requests per second
    (r/s). Once an application begins serializing and deserializing payloads or doing
    other CPU-intensive work, that throughput is going to drop by orders of magnitude.
    Offloading work to additional processes helps prevent a single process from getting
    overwhelmed.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few tools available for splitting up work. [“The Cluster Module”](#ch_scaling_sec_clustering)
    looks at a built-in module that makes it easy to run redundant copies of application
    code on the same server. [“Reverse Proxies with HAProxy”](#ch_scaling_sec_rp)
    runs multiple redundant copies of a service using an external tool—allowing them
    to run on different machines. Finally, [“SLA and Load Testing”](#ch_scaling_sec_bm)
    looks at how to understand the load that a service can handle by examining benchmarks,
    which can be used to determine the number of instances it should scale to.
  prefs: []
  type: TYPE_NORMAL
- en: The Cluster Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Node.js provides the `cluster` module to allow running multiple copies of a
    Node.js application on the same machine, dispatching incoming network messages
    to the copies. This module is similar to the `child_process` module, which provides
    a `fork()` method^([1](ch03.html#idm46291192440536)) for spawning Node.js sub
    processes; the main difference is the added mechanism for routing incoming requests.
  prefs: []
  type: TYPE_NORMAL
- en: The `cluster` module provides a simple API and is immediately accessible to
    any Node.js program. Because of this it’s often the knee-jerk solution when an
    application needs to scale to multiple instances. It’s become somewhat ubiquitous,
    with many open source Node.js application depending on it. Unfortunately, it’s
    also a bit of an antipattern, and is almost never the best tool to scale a process.
    Due to this ubiquity it’s necessary to understand how it works, even though you
    should avoid it more often than not.
  prefs: []
  type: TYPE_NORMAL
- en: The [documentation for `cluster`](https://nodejs.org/api/cluster.html) includes
    a single Node.js file that loads the `http` and `cluster` modules and has an `if`
    statement to see if the script is being run as the master, forking off some worker
    processes if true. Otherwise, if it’s not the master, it creates an HTTP service
    and begins listening. This example code is both a little dangerous and a little
    misleading.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reason the documentation code sample is dangerous is that it promotes loading
    a lot of potentially heavy and complicated modules within the parent process.
    The reason it’s misleading is that the example doesn’t make it obvious that multiple
    separate instances of the application are running and that things like global
    variables cannot be shared. For these reasons you’ll consider the modified example
    shown in [Example 3-1](#ex_cluster_master).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. *recipe-api/producer-http-basic-master.js*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `cluster` module is needed in the parent process.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Override the default application entry point of `__filename`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_scaling_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '`cluster.fork()` is called once for each time a worker needs to be created.
    This code produces two workers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_scaling_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Several events that `cluster` emits are listened to and logged.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_scaling_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Uncomment this to make workers difficult to kill.
  prefs: []
  type: TYPE_NORMAL
- en: The way `cluster` works is that the master process spawns worker processes in
    a special mode where a few things can happen. In this mode, when a worker attempts
    to listen on a port, it sends a message to the master. It’s actually the master
    that listens on the port. Incoming requests are then routed to the different worker
    processes. If any workers attempt to listen on the special port `0` (used for
    picking a random port), the master will listen once and each individual worker
    will receive requests from that same random port. A visualization of this master
    and worker relationship is provided in [Figure 3-1](#fig_cluster).
  prefs: []
  type: TYPE_NORMAL
- en: '![A Master Node.js process and two Worker Node.js processes](assets/dsnj_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Master-worker relationships with `cluster`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'No changes need to be made to basic stateless applications that serve as the
    worker—the *recipe-api/producer-http-basic.js* code will work just fine.^([2](ch03.html#idm46291192181256))
    Now it’s time to make a few requests to the server. This time, execute the *recipe-api/producer-http-basic-master.js*
    file instead of the *recipe-api/producer-http-basic.js* file. In the output you
    should see some messages resembling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now there are three running processes. This can be confirmed by running the
    following command, where `<PID>` is replaced with the process ID of the master
    process, in my case *7649*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'A truncated version of the output from this command when run on my Linux machine
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This provides a visualization of the parent process, displayed as `./master.js`,
    as well as the two child processes, displayed as `server.js`. It also displays
    some other interesting information if run on a Linux machine. Note that each of
    the three processes shows six additional child entries below them, each labelled
    as `{node}`, as well as their unique process IDs. These entries suggest multithreading
    in the underlying libuv layer. Note that if you run this on macOS, you will only
    see the three Node.js processes listed.
  prefs: []
  type: TYPE_NORMAL
- en: Request Dispatching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On macOS and Linux machines, the requests will be dispatched round-robin to
    the workers by default. On Windows, requests will be dispatched depending on which
    worker is perceived to be the least busy. You can make three successive requests
    directly to the *recipe-api* service and see this happening for yourself. With
    this example, requests are made directly to the *recipe-api*, since these changes
    won’t affect the *web-api* service. Run the following command three times in another
    terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the output you should see that the requests have been cycled between the
    two running worker instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may recall from [Example 3-1](#ex_cluster_master), some event listeners
    were created in the *recipe-api/master.js* file. So far the `listening` event
    has been triggered. This next step triggers the other two events. When you made
    the three HTTP requests, the PID values of the worker processes were displayed
    in the console. Go ahead and kill one of the processes to see what happens. Choose
    one of the PIDs and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In my case I ran `kill 7656`. The master process then has both the `disconnect`
    and the `exit` events fire, in that order. You should see output similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now go ahead and repeat the same three HTTP requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This time, each of the responses is coming from the same remaining worker process.
    If you then run the `kill` command with the remaining worker process, you’ll see
    that the `disconnect` and `exit` events are called and that the master process
    then quits.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that there’s a commented call to `cluster.fork()` inside of the `exit`
    event handler. Uncomment that line, start the master process again, and make some
    requests to get the PID values of the workers. Then, run the `kill` command to
    stop one of the workers. Notice that the worker process is then immediately started
    again by the master. In this case, the only way to permanently kill the children
    is to kill the master.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `cluster` module isn’t a magic bullet. In fact, it is often more of an antipattern.
    More often than not, another tool should be used to manage multiple copies of
    a Node.js process. Doing so usually helps with visibility into process crashes
    and allows you to easily scale instances. Sure, you could build in application
    support for scaling the number of workers up and down, but that’s better left
    to an outside tool. [Chapter 7](ch07.html#ch_kubernetes) looks into doing just
    that.
  prefs: []
  type: TYPE_NORMAL
- en: This module is mostly useful in situations where an application is bound by
    the CPU, not by I/O. This is in part due to JavaScript being single threaded,
    and also because libuv is so efficient at handling asynchronous events. It’s also
    fairly fast due to the way it passes incoming requests to a child process. In
    theory, this is faster than using a reverse proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Node.js applications can get complex. Processes often end up with dozens, if
    not hundreds, of modules that make outside connections, consume memory, or read
    configuration. Each one of these operations can expose another weakness in an
    application that can cause it to crash.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason it’s better to keep the master process as simple as possible.
    [Example 3-1](#ex_cluster_master) proves that there’s no reason for a master to
    load an HTTP framework or consume another database connection. Logic *could* be
    built into the master to restart failed workers, but the master itself can’t be
    restarted as easily.
  prefs: []
  type: TYPE_NORMAL
- en: Another caveat of the `cluster` module is that it essentially operates at Layer
    4, at the TCP/UDP level, and isn’t necessarily aware of Layer 7 protocols. Why
    might this matter? Well, with an incoming HTTP request being sent to a master
    and two workers, assuming the TCP connection closes after the request finishes,
    each subsequent request then gets dispatched to a different backend service. However,
    with gRPC over HTTP/2, those connections are intentionally left open for much
    longer. In these situations, future gRPC calls will not get dispatched to separate
    worker processes—they’ll be stuck with just one. When this happens, you’ll often
    see that one worker is doing most of the work and the whole purpose of clustering
    has been defeated.
  prefs: []
  type: TYPE_NORMAL
- en: This issue with sticky connections can be proved by adapting it to the code
    written previously in [“RPC with gRPC”](ch02.html#ch_protocols_sec_grpc). By leaving
    the producer and consumer code exactly the same, and by introducing the generic
    cluster master from [Example 3-1](#ex_cluster_master), the issue surfaces. Run
    the producer master and the consumer, and make several HTTP requests to the consumer,
    and the returned `producer_data.pid` value will always be the same. Then, stop
    and restart the consumer. This will cause the HTTP/2 connection to stop and start
    again. The round-robin routing of `cluster` will then route the consumer to the
    other worker. Make several HTTP requests to the consumer again, and the `producer_data.pid`
    values will now all point to the second worker.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason you shouldn’t always reach for the `cluster` module is that it
    won’t always make an application faster. In some situations it can simply consume
    more resources and have either no effect or a negative effect on the performance
    of the application. Consider, for example, an environment where a process is limited
    to a single CPU core. This can happen if you’re running on a VPS (Virtual Private
    Server, a fancy name for a dedicated virtual machine) such as a `t3.small` machine
    offered on AWS EC2\. It can also happen if a process is running inside of a container
    with CPU constraints, which can be configured when running an application within
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for a slowdown is this: when running a cluster with two workers,
    there are three single-threaded instances of JavaScript running. However, there
    is a single CPU core available to run each instance one at a time. This means
    the operating system has to do more work deciding which of the three processes
    runs at any given time. True, the master instance is mostly asleep, but the two
    workers will fight with each other for CPU cycles.'
  prefs: []
  type: TYPE_NORMAL
- en: Time to switch from theory to practice. First, create a new file for simulating
    a service that performs CPU-intensive work, making it a candidate to use with
    `cluster`. This service will simply calculate Fibonacci values based on an input
    number. [Example 3-2](#ex_fibonacci) is an illustration of such a service.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. *cluster-fibonacci.js*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The service has a single route, `/<limit>`, where `limit` is the number of iterations
    to count.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `fibonacci()` method does a lot of CPU-intensive math and blocks the event
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: The same [Example 3-1](#ex_cluster_master) code can be used for acting as the
    cluster master. Re-create the content from the cluster master example and place
    it in a *master-fibonacci.js* file next to *cluster-fibonacci.js*. Then, update
    it so that it’s loading *cluster-fibonacci.js*, instead of *producer-http-basic.js*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you’ll do is run a benchmark against a cluster of Fibonacci
    services. Execute the *master-fibonacci.js* file and then run a benchmarking command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will run the *Autocannon* benchmarking tool (covered in more detail in
    [“Introduction to Autocannon”](#ch_scaling_sec_bm_subsec_autocannon)) against
    the application. It will run over two connections, as fast as it can, for 10 seconds.
    Once the operation is complete you’ll get a table of statistics in response. For
    now you’ll only consider two values, and the values I received have been re-created
    in [Table 3-1](#table_fibonacci_cluster).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Fibonacci cluster with multiple cores
  prefs: []
  type: TYPE_NORMAL
- en: '| Statistic | Result |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Avg latency | 147.05ms |'
  prefs: []
  type: TYPE_TB
- en: '| Avg req/sec | 13.46 r/s |'
  prefs: []
  type: TYPE_TB
- en: Next, kill the *master-fibonacci.js* cluster master, then run just the *cluster-fibonacci.js*
    file directly. Then, run the exact same `autocannon` command that you ran before.
    Again, you’ll get some more results, and mine happen to look like [Table 3-2](#table_fibonacci_single).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-2\. Fibonacci single process
  prefs: []
  type: TYPE_NORMAL
- en: '| Statistic | Result |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Avg latency | 239.61ms |'
  prefs: []
  type: TYPE_TB
- en: '| Avg req/sec | 8.2 r/s |'
  prefs: []
  type: TYPE_TB
- en: In this situation, on my machine with multiple CPU cores, I can see that by
    running two instances of the CPU-intensive Fibonacci service, I’m able to increase
    throughput by about 40%. You should see something similar.
  prefs: []
  type: TYPE_NORMAL
- en: Next, assuming you have access to a Linux machine, you’ll simulate an environment
    that only has a single CPU instance available. This is done by using the `taskset`
    command to force processes to use a specific CPU core. This command doesn’t exist
    on macOS, but you can get the gist of it by reading along.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the *master-fibonacci.js* cluster master file again. Note that the output
    of the service includes the PID value of the master, as well as the two workers.
    Take note of these PID values, and in another terminal, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Finally, run the same `autocannon` command used throughout this section. Once
    it completes, more information will be provided to you. In my case, I received
    the results shown in [Table 3-3](#table_fibonacci_cluster_restricted).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-3\. Fibonacci cluster with single core
  prefs: []
  type: TYPE_NORMAL
- en: '| Statistic | Result |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Avg latency | 252.09ms |'
  prefs: []
  type: TYPE_TB
- en: '| Avg req/sec | 7.8 r/s |'
  prefs: []
  type: TYPE_TB
- en: In this case, I can see that using the `cluster` module, while having more worker
    threads than I have CPU cores, results in an application that runs slower than
    if I had only run a single instance of the process on my machine.
  prefs: []
  type: TYPE_NORMAL
- en: The greatest shortcoming of `cluster` is that it only dispatches incoming requests
    to processes running on the same machine. The next section looks at a tool that
    works when application code runs on multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: Reverse Proxies with HAProxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A reverse proxy is a tool that accepts a request from a client, forwards it
    to a server, takes the response from the server, and sends it back to the client.
    At first glance it may sound like such a tool merely adds an unnecessary network
    hop and increases network latency, but as you’ll see, it actually provides many
    useful features to a service stack. Reverse proxies often operate at either Layer
    4, such as TCP, or Layer 7, via HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: One of the features it provides is that of load balancing. A reverse proxy can
    accept an incoming request and forward it to one of several servers before replying
    with the response to the client. Again, this may sound like an additional hop
    for no reason, as a client could maintain a list of upstream servers and directly
    communicate with a specific server. However, consider the situation where an organization
    may have several different API servers running. An organization wouldn’t want
    to put the onus of choosing which API instance to use on a third-party consumer,
    like by exposing `api1.example.org` through `api9.example.org`. Instead, consumers
    should be able to use `api.example.org` and their requests should automatically
    get routed to an appropriate service. A diagram of this concept is shown in [Figure 3-2](#fig_reverse_proxy).
  prefs: []
  type: TYPE_NORMAL
- en: '![A request comes from the internet, passes through a reverse proxy, and gets
    sent to a Node.js application](assets/dsnj_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Reverse proxies intercept incoming network traffic
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are several different approaches a reverse proxy can take when choosing
    which backend service to route an incoming request to. Just like with the `cluster`
    module, the round-robin is usually the default behavior. Requests can also be
    dispatched based on which backend service is currently servicing the fewest requests.
    They can be dispatched randomly, or they can even be dispatched based on content
    of the initial request, such as a session ID stored in an HTTP URL or cookie (also
    known as a sticky session). And, perhaps most importantly, a reverse proxy can
    poll backend services to see which ones are healthy, refusing to dispatch requests
    to services that aren’t healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Other beneficial features include cleaning up or rejecting malformed HTTP requests
    (which can prevent bugs in the Node.js HTTP parser from being exploited), logging
    requests so that application code doesn’t have to, adding request timeouts, and
    performing gzip compression and TLS encryption. The benefits of a reverse proxy
    usually far outweigh the losses for all but the most performance-critical applications.
    Because of this you should almost always use some form of reverse proxy between
    your Node.js applications and the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to HAProxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HAProxy is a very performant open source reverse proxy that works with both
    Layer 4 and Layer 7 protocols. It’s written in C and is designed to be stable
    and use minimal resources, offloading as much processing as possible to the kernel.
    Like JavaScript, HAProxy is event driven and single threaded.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy is quite simple to setup. It can be deployed by shipping a single binary
    executable weighing in at about a dozen megabytes. Configuration can be done entirely
    using a single text file.
  prefs: []
  type: TYPE_NORMAL
- en: Before you start running HAProxy, you’ll first need to have it installed. A
    few suggestions for doing so are provided in [Appendix A](app01.html#appendix_install_haproxy).
    Otherwise, feel free to use your preferred software installation method to get
    a copy of HAProxy (at least v2) installed on your development machine.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy provides an optional web dashboard that displays statistics for a running
    HAProxy instance. Create an HAProxy configuration file, one that doesn’t yet perform
    any actual reverse proxying but instead just exposes the dashboard. Create a file
    named *haproxy/stats.cfg* in your project folder and add the content shown in
    [Example 3-3](#ex_haproxy_stats).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. *haproxy/stats.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `frontend` called `inbound`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Listen for HTTP traffic on port `:8000`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_scaling_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Enable the stats interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that file created, you’re now ready to execute HAProxy. Run the following
    command in a terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll get a few warnings printed in the console since the config file is a
    little too simple. These warnings will be fixed soon, but HAProxy will otherwise
    run just fine. Next, in a web browser, open the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: At this point you’ll be able to see some stats about the HAProxy instance. Of
    course, there isn’t anything interesting in there just yet. The only statistics
    displayed are for the single frontend. At this point you can refresh the page,
    and the bytes transferred count will increase because the dashboard also measures
    requests to itself.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy works by creating both *frontends*—ports that it listens on for incoming
    requests—and *backends*—upstream backend services identified by hosts and ports
    that it will forward requests to. The next section actually creates a backend
    to route incoming requests to.
  prefs: []
  type: TYPE_NORMAL
- en: Load Balancing and Health Checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section enables the load balancing features of HAProxy and also gets rid
    of those warnings in the [Example 3-3](#ex_haproxy_stats) configuration. Earlier
    you looked at the reasons why an organization should use a reverse proxy to intercept
    incoming traffic. In this section, you’ll configure HAProxy to do just that; it
    will act as a load balancer between external traffic and the *web-api* service,
    exposing a single host/port combination but ultimately serving up traffic from
    two service instances. [Figure 3-3](#fig_haproxy_loadbalance) provides a visual
    representation of this.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, no application changes need to be made to allow for load balancing
    with HAProxy. However, to better show off the capabilities of HAProxy, a feature
    called a *health check* will be added. A simple endpoint that responds with a
    200 status code will suffice for now. To do this, duplicate the *web-api/consumer-http-basic.js*
    file and add a new endpoint, as shown in [Example 3-4](#ex_health_endpoint). [“Health
    Checks”](ch04.html#ch_monitoring_sec_health) will look at building out a more
    accurate health check endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '![HAProxy load balancing requests to two web-api instances](assets/dsnj_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Load balancing with HAProxy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Example 3-4\. *web-api/consumer-http-healthendpoint.js* (truncated)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You’ll also need a new configuration file for HAProxy. Create a file named *haproxy/load-balance.cfg*
    and add the content from [Example 3-5](#ex_haproxy_loadbalance) to it.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-5\. *haproxy/load-balance.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `defaults` section configures multiple frontends.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Timeout values have been added, eliminating the HAProxy warnings.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_scaling_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: A frontend can route to multiple backends. In this case, only the *web-api*
    backend should be routed to.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_scaling_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The first backend, *web-api*, has been configured.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_scaling_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Health checks for this backend make a `GET /health` HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_scaling_CO4-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The *web-api* routes requests to two backends, and the `check` parameter enables
    health checking.
  prefs: []
  type: TYPE_NORMAL
- en: This configuration file instructs HAProxy to look for two *web-api* instances
    running on the current machine. To avoid a port collision, the application instances
    have been instructed to listen on ports `:3001` and `:3002`. The *inbound* frontend
    is configured to listen on port `:3000`, essentially allowing HAProxy to be a
    swap-in replacement for a regular running *web-api* instance.
  prefs: []
  type: TYPE_NORMAL
- en: Much like with the `cluster` module in [“The Cluster Module”](#ch_scaling_sec_clustering),
    requests are routed round-robin^([3](ch03.html#idm46291191422376)) between two
    separate Node.js processes. But now there is one fewer running Node.js process
    to maintain. As implied by the `host:port` combination, these processes don’t
    need to run on localhost for HAProxy to forward the requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you’ve created the config file and have a new endpoint, it’s time
    to run some processes. For this example, you’ll need to open five different terminal
    windows. Run the following four commands in four different terminal window, and
    run the fifth command several times in a fifth window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Notice that in the output for the `curl` command, `consumer_pid` cycles between
    two values as HAProxy routes requests round-robin between the two *web-api* instances.
    Also, notice that the `producer_pid` value stays the same since only a single
    *recipe-api* instance is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'This command order runs the dependent programs first. In this case the *recipe-api*
    instance is run first, then two *web-api* instances, followed by HAProxy. Once
    the HAProxy instance is running, you should notice something interesting in the
    *web-api* terminals: the *health check* message is being printed over and over,
    once every two seconds. This is because HAProxy has started performing health
    checks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up the HAProxy statistics page again^([4](ch03.html#idm46291191395832))
    by visiting [*http://localhost:3000/admin?stats*](http://localhost:3000/admin?stats).
    You should now see two sections in the output: one for the *inbound* frontend
    and one for the new *web-api* backend. In the *web-api* section, you should see
    the two different server instances listed. Both of them should have green backgrounds,
    signaling that their health checks are passing. A truncated version of the results
    I get is shown in [Table 3-4](#table_haproxy_stats).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-4\. Truncated HAProxy stats
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Sessions total | Bytes out | LastChk |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| web-api-1 | 6 | 2,262 | L7OK/200 in 1ms |'
  prefs: []
  type: TYPE_TB
- en: '| web-api-2 | 5 | 1,885 | L7OK/200 in 0ms |'
  prefs: []
  type: TYPE_TB
- en: '| Backend | 11 | 4,147 |  |'
  prefs: []
  type: TYPE_TB
- en: The final line, *Backend*, represents the totals for the columns above it. In
    this output, you can see that the requests are distributed essentially equally
    between the two instances. You can also see that the health checks are passing
    by examining the *LastChk* column. In this case both servers are passing the L7
    health check (HTTP) by returning a 200 status within 1ms.
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s time to have a little fun with this setup. First, switch to one of
    the terminals running a copy of *web-api*. Stop the process by pressing Ctrl +
    C. Then, switch back to the statistics webpage and refresh a few times. Depending
    on how quick you are, you should see one of the lines in the *web-api* section
    change from green to yellow to red. This is because HAProxy has determined the
    service is down since it’s no longer responding to health checks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that HAProxy has determined the service to be down, switch back to the fifth
    terminal screen and run a few more `curl` commands. Notice that you continuously
    get responses, albeit from the same *web-api* PID. Since HAProxy knows one of
    the services is down, it’s only going to route requests to the healthy instance.
  prefs: []
  type: TYPE_NORMAL
- en: Switch back to the terminal where you killed the *web-api* instance, start it
    again, and switch back to the stats page. Refresh a few times and notice how the
    status turns from red to yellow to green. Switch back to the `curl` terminal,
    run the command a few more times, and notice that HAProxy is now dispatching commands
    between both instances again.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, this setup seems to work pretty smoothly. You killed a service,
    and it stopped receiving traffic. Then, you brought it back, and the traffic resumed.
    But can you guess what the problem is?
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, in the console output from the running *web-api* instances, the health
    checks could be seen firing every two seconds. This means that there is a length
    of time for which a server can be down, but HAProxy isn’t aware of it yet. This
    means that there are periods of time that requests can still fail. To illustrate
    this, first restart the dead *web-api* instance, then pick one of the `consumer_pid`
    values from the output and replace the `CONSUMER_PID` in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: What this command does is kill a *web-api* process and then make two HTTP requests,
    all so quickly that HAProxy shouldn’t have enough time to know that something
    bad has happened. In the output, you should see that one of the commands has failed
    and that the other has succeeded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The health checks can be configured a little more than what’s been shown so
    far. Additional `flag value` pairs can be specified after the `check` flag present
    at the end of the `server` lines. For example, such a configuration might look
    like this: `server ... check inter 10s fall 4`. [Table 3-5](#table_haproxy_health)
    describes these flags and how they may be configured.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-5\. HAProxy health check flags
  prefs: []
  type: TYPE_NORMAL
- en: '| Flag | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `inter` | interval | 2s | Interval between checks |'
  prefs: []
  type: TYPE_TB
- en: '| `fastinter` | interval | `inter` | Interval when transitioning states |'
  prefs: []
  type: TYPE_TB
- en: '| `downinter` | interval | `inter` | Interval between checks when down |'
  prefs: []
  type: TYPE_TB
- en: '| `fall` | int | 3 | Consecutive healthy checks before being UP |'
  prefs: []
  type: TYPE_TB
- en: '| `rise` | int | 2 | Consecutive unhealthy checks before being DOWN |'
  prefs: []
  type: TYPE_TB
- en: Even though the health checks can be configured to run very aggressively, there
    still isn’t a perfect solution to the problem of detecting when a service is down;
    with this approach there is always a risk that requests will be sent to an unhealthy
    service. [“Idempotency and Messaging Resilience”](ch08.html#ch_resilience_sec_messaging)
    looks at a solution to this problem where clients are configured to retry failed
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compression can be configured easily with HAProxy by setting additional configuration
    flags on the particular backend containing content that HAProxy should compress.
    See [Example 3-6](#ex_haproxy_compression) for a demonstration of how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-6\. *haproxy/compression.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Prevent HAProxy from forwarding the `Accept-Encoding` header to the backend
    service.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This enables gzip compression; other algorithms are also available.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_scaling_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Compression is enabled depending on the `Content-Type` header.
  prefs: []
  type: TYPE_NORMAL
- en: This example specifically states that compression should only be enabled on
    responses that have a `Content-Type` header value of `application/json`, which
    is what the two services have been using, or `text/plain`, which can sometimes
    sneak through if an endpoint hasn’t been properly configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like in [Example 2-4](ch02.html#ex_node_gzip), where gzip compression
    was performed entirely in Node.js, HAProxy is also going to perform compression
    only when it knows the client supports it by checking the `Accept-Encoding` header.
    To confirm that HAProxy is compressing the responses, run the following commands
    in separate terminal windows (in this case you only need a single *web-api* running):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Performing gzip compression using HAProxy will be more performant than doing
    it within the Node.js process. [“HTTP compression”](#ch_scaling_sec_bm_subsec_gzip)
    will test the performance of this.
  prefs: []
  type: TYPE_NORMAL
- en: TLS Termination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performing TLS termination in a centralized location is convenient for many
    reasons. A big reason is that additional logic doesn’t need to be added to applications
    for updating certificates. Hunting down which instances have outdated certificates
    can also be avoided. A single team within an organization can handle all of the
    certificate generation. Applications also don’t have to incur additional CPU overhead.
  prefs: []
  type: TYPE_NORMAL
- en: That said, HAProxy will direct traffic to a single service in this example.
    The architecture for this looks like [Figure 3-4](#fig_haproxy_tls).
  prefs: []
  type: TYPE_NORMAL
- en: '![HAProxy performs TLS Termination, sending unencrypted HTTP traffic to backend
    services](assets/dsnj_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. HAProxy TLS termination
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TLS termination is rather straight-forward with HAProxy, and many of the same
    rules covered in [“HTTPS / TLS”](ch02.html#ch_protocols_sec_http_subsec_tls) still
    apply. For example, all the certificate generation and chain of trust concepts
    still apply, and these cert files adhere to well-understood standards. One difference
    is that in this section a *.pem* file is used, which is a file containing both
    the content of the *.cert* file and the *.key* files. [Example 3-7](#ex_generate_pem)
    is a modified version of a previous command. It generates the individual files
    and concatenates them together.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-7\. Generating a *.pem* file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Another HAProxy configuration script is now needed. [Example 3-8](#ex_haproxy_tls)
    modifies the *inbound* frontend to listen via HTTPS and to load the *combined.pem*
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-8\. *haproxy/tls.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `global` section configures global HAProxy settings.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `ssl` flag specifies that the frontend uses TLS, and the `crt` flag points
    to the *.pem* file.
  prefs: []
  type: TYPE_NORMAL
- en: The `global` section allows for global HAProxy configuration. In this case it
    sets the Diffie-Hellman key size parameter used by clients and prevents an HAProxy
    warning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you’ve configured HAProxy, go ahead and run it with this new configuration
    file and then send it some requests. Run the following commands in four separate
    terminal windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Since HAProxy is using a self-signed certificate, the `curl` command requires
    the `--insecure` flag again. With a real-world example, since the HTTPS traffic
    is public facing, you’d want to use a real certificate authority like *Let’s Encrypt*
    to generate certificates for you. Let’s Encrypt comes with a tool called *certbot*,
    which can be configured to automatically renew certificates before they expire,
    as well as reconfigure HAProxy on the fly to make use of the updated certificates.
    Configuring certbot is beyond the scope of this book, and there exists literature
    on how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other options that can be configured regarding TLS in HAProxy.
    It allows for specifying which cipher suites to use, TLS session cache sizes,
    and SNI (Server Name Indication). A single frontend can specify a port for both
    standard HTTP and HTTPS. HAProxy can redirect a user agent making an HTTP request
    to the equivalent HTTPS path.
  prefs: []
  type: TYPE_NORMAL
- en: Performing TLS termination using HAProxy may be more performant than doing it
    within the Node.js process. [“TLS termination”](#ch_scaling_sec_bm_subsec_tls)
    will test this claim.
  prefs: []
  type: TYPE_NORMAL
- en: Rate Limiting and Back Pressure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[“SLA and Load Testing”](#ch_scaling_sec_bm) looks at ways to determine how
    much load a Node.js service can handle. This section looks at ways of enforcing
    such a limit.'
  prefs: []
  type: TYPE_NORMAL
- en: A Node.js process, by default, will “handle” as many requests as it receives.
    For example, when creating a basic HTTP server with a callback when a request
    is received, those callbacks will keep getting scheduled by the event loop and
    called whenever possible. Sometimes, though, this can overwhelm a process. If
    the callback is doing a lot of blocking work, having too many of them scheduled
    will result in the process locking up. A bigger issue is memory consumption; every
    single queued callback comes with a new function context containing variables
    and references to the incoming request. Sometimes the best solution is to reduce
    the amount of concurrent connections being handled by a Node.js process at a given
    time.
  prefs: []
  type: TYPE_NORMAL
- en: One way to do this is to set the `maxConnections` property of an `http.Server`
    instance. By setting this value, the Node.js process will automatically drop any
    incoming connections that would increase the connection count to be greater than
    this limit.
  prefs: []
  type: TYPE_NORMAL
- en: Every popular Node.js HTTP framework on npm will either expose the `http.Server`
    instance it uses or provide a method for overriding the value. However, in this
    example, a basic HTTP server using the built-in `http` module is constructed.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new file and add the contents of [Example 3-9](#ex_node_maxconn) to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-9\. *low-connections.js*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This `setTimeout()` simulates slow asynchronous activity, like a database operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of incoming connections is set to 2.
  prefs: []
  type: TYPE_NORMAL
- en: This server simulates a slow application. Each incoming request takes 10 seconds
    to run before the response is received. This won’t simulate a process with heavy
    CPU usage, but it does simulate a request that is slow enough to possibly overwhelm
    Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, open four terminal windows. In the first one, run the *low-connections.js*
    service. In the other three, make the same HTTP request by using the `curl` command.
    You’ll need to run the `curl` commands within 10 seconds, so you might want to
    first paste the command three times and then execute them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming you ran the commands quick enough, the first two `curl` calls should
    run, albeit slowly, pausing for 10 seconds before finally writing the message
    `OK` to the terminal window. The third time it ran, however, the command should
    have written an error and would have closed immediately. On my machine, the `curl`
    command prints `curl: (56) Recv failure: Connection reset by peer`. Likewise,
    the server terminal window should *not* have written a message about the current
    number of connections.'
  prefs: []
  type: TYPE_NORMAL
- en: The `server.maxConnections` value sets a hard limit to the number of requests
    for this particular server instance, and Node.js will drop any connections above
    that limit.
  prefs: []
  type: TYPE_NORMAL
- en: This might sound a bit harsh! As a client consuming a service, a more ideal
    situation might instead be to have the server queue up the request. Luckily, HAProxy
    can be configured to do this on behalf of the application. Create a new HAProxy
    configuration file with the content from [Example 3-10](#ex_haproxy_maxconn).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-10\. *haproxy/backpressure.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_scaling_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Max connections can be configured globally. This includes incoming frontend
    and outgoing backend connections.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_scaling_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Force HAProxy to close HTTP connections to the backend.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_scaling_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Max connections can be specified per backend-service instance.
  prefs: []
  type: TYPE_NORMAL
- en: This example sets a global flag of `maxconn 8`. This means that between all
    frontends and backends combined, only eight connections can be running at the
    same time, including any calls to the admin interface. Usually you’ll want to
    set this to a conservative value, if you use it at all. More interestingly, however,
    is the `maxconn 2` flag attached to the specific backend instance. This will be
    the real limiting factor with this configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that `option httpclose` is set on the backend. This is to cause HAProxy
    to immediately close connections to the service. Having these connections remain
    open won’t necessarily slow down the service, but it’s required since the `server.maxConnections`
    value is still set to 2 in the application; with the connections left open, the
    server will drop new connections, even though the callbacks have finished firing
    with previous requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with the new configuration file, go ahead and run the same Node.js service,
    an instance of HAProxy using the configuration, and again, run multiple copies
    of the `curl` requests in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, you should see the first two `curl` commands successfully kicking off
    a log message on the server. However, this time the third `curl` command doesn’t
    immediately close. Instead, it’ll wait until one of the previous commands finishes
    and the connection closes. Once that happens, HAProxy becomes aware that it’s
    now free to send an additional request along, and the third request is sent through,
    causing the server to log another message about having two concurrent requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*Back pressure* results when a consuming service has its requests queued up,
    like what is now happening here. If the consumer fires requests serially, back
    pressure created on the producer’s side will cause the consumer to slow down.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually it’s fine to only enforce limits within the reverse proxy without having
    to also enforce limits in the application itself. However, depending on how your
    architecture is implemented, it could be that sources other than a single HAProxy
    instance are able to send requests to your services. In those cases it might make
    sense to set a higher limit within the Node.js process and then set a more conservative
    limit within the reverse proxy. For example, if you know your service will come
    to a standstill with 100 concurrent requests, perhaps set `server.maxConnections`
    to 90 and set `maxconn` to 80, adjusting margins depending on how dangerous you’re
    feeling.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to configure the maximum number of connections, it’s time
    to look at methods for determining how many connections a service can actually
    handle.
  prefs: []
  type: TYPE_NORMAL
- en: SLA and Load Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software as a service (SaaS) companies provide an online service to their users.
    The expectation of the modern user is that such services are available 24/7\.
    Just imagine how weird it would be if Facebook weren’t available on Fridays from
    2 P.M. to 3 P.M. Business-to-business (B2B) companies typically have even stricter
    requirements, often paired with contractual obligation. When an organization sells
    access to an API, there are often contractual provisions stating that the organization
    won’t make backwards-breaking changes without ample notice to upgrade, that a
    service will be available around the clock, and that requests will be served within
    a specified timespan.
  prefs: []
  type: TYPE_NORMAL
- en: Such contractual requirements are called a *Service Level Agreement (SLA)*.
    Sometimes companies make them available online, such as the [Amazon Compute Service
    Level Agreement](https://oreil.ly/ZYoE5) page. Sometimes they’re negotiated on
    a per-client basis. Sadly, often they do not exist at all, performance isn’t prioritized,
    and engineers don’t get to tackle such concerns until a customer complaint ticket
    arrives.
  prefs: []
  type: TYPE_NORMAL
- en: An SLA may contain more than one *Service Level Objective (SLO)*. These are
    individual promises in the SLA that an organization makes to a customer. They
    can include things like uptime requirements, API request latency, and failure
    rates. When it comes to measuring the real values that a service is achieving,
    those are called *Service Level Indicators (SLI)*. I like to think of the SLO
    as a numerator and the SLI as a denominator. An SLO might be that an API should
    respond in 100ms, and an SLI might be that the API does respond in 83ms.
  prefs: []
  type: TYPE_NORMAL
- en: This section looks at the importance of determining SLOs, not only for an organization
    but for individual services as well. It looks at ways to define an SLO and ways
    to measure a service’s performance by running one-off load tests (sometimes called
    a benchmark). Later, [“Metrics with Graphite, StatsD, and Grafana”](ch04.html#ch_monitoring_sec_metrics)
    looks at how to constantly monitor performance.
  prefs: []
  type: TYPE_NORMAL
- en: Before defining what an SLA should look like, you’ll first look at some performance
    characteristics and how they can be measured. To do this, you’ll load test some
    of the services you built previously. This should get you familiar with load testing
    tools and with what sort of throughput to expect in situations without business
    logic. Once you have that familiarity, measuring your own applications should
    be easier.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Autocannon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These load tests use *Autocannon*. There are plenty of alternatives, but this
    one is both easy to install (it’s a one-line npm command) and displays detailed
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Feel free to use whatever load-testing tool you’re most comfortable with. However,
    never compare the results of one tool with the results from another, as the results
    for the same service can vary greatly. Try to standardize on the same tool throughout
    your organization so that teams can consistently communicate about performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Autocannon is available as an npm package and it happens to provide a histogram
    of request statistics, which is a very important tool when measuring performance.
    Install it by running the following command (note that you might need to prefix
    it with `sudo` if you get permission errors):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Running a Baseline Load Test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These load tests will mostly run the applications that you’ve already created
    in the *examples/* folder. But first, you’ll get familiar with the Autocannon
    command and establish a baseline by load testing some very simple services. The
    first will be a vanilla Node.js HTTP server, and the next will be using a framework.
    In both, a simple string will be used as the reply.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be sure to disable any `console.log()` statements that run *within* a request
    handler. Although these statements provide an insignificant amount of delay in
    a production application doing real work, they significantly slow down many of
    the load tests in this section.
  prefs: []
  type: TYPE_NORMAL
- en: For this first example, create a new directory called *benchmark/* and create
    a file within it with the contents from [Example 3-11](#ex_bm_vanilla). This vanilla
    HTTP server will function as the most basic of load tests.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-11\. *benchmark/native-http.js*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Ideally, all of these tests would be run on an unused server with the same capabilities
    as a production server, but for the sake of learning, running it on your local
    development laptop is fine. Do keep in mind that the numbers you get locally will
    not reflect the numbers you would get in production!
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the service and, in another terminal window, run Autocannon to start the
    load test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This command uses three different flags. The `-d` flag stands for *duration*,
    and in this case it’s configured to run for 60 seconds. The `-c` flag represents
    the number of concurrent *connections*, and here it’s configured to use 10 connections.
    The `-l` flag tells Autocannon to display a detailed *latency* histogram. The
    URL to be tested is the final argument to the command. In this case Autocannon
    simply sends `GET` requests, but it can be configured to make `POST` requests
    and provide request bodies.
  prefs: []
  type: TYPE_NORMAL
- en: Tables [3-6](#table_bm_latency) through [3-8](#table_bm_latency_detailed) contain
    my results.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-6\. Autocannon request latency
  prefs: []
  type: TYPE_NORMAL
- en: '| Stat | 2.5% | 50% | 97.5% | 99% | Avg | Stdev | Max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Latency | 0ms | 0ms | 0ms | 0ms | 0.01ms | 0.08ms | 9.45ms |'
  prefs: []
  type: TYPE_TB
- en: The first table contains information about the latency, or how much time it
    takes to receive a response after a request has been sent. As you can see, Autocannon
    groups latency into four buckets. The *2.5%* bucket represents rather speedy requests,
    *50%* is the median, *97.5%* are the slower results, and *99%* are some of the
    slowest, with the *Max* column representing the slowest request. In this table,
    lower results are faster. The numbers so far are all so small that a decision
    can’t yet be made.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-7\. Autocannon request volume
  prefs: []
  type: TYPE_NORMAL
- en: '| Stat | 1% | 2.5% | 50% | 97.5% | Avg | Stdev | Min |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Req/Sec | 29,487 | 36,703 | 39,039 | 42,751 | 38,884.14 | 1,748.17 | 29,477
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bytes/Sec | 3.66 MB | 4.55 MB | 4.84 MB | 5.3 MB | 4.82 MB | 217 kB | 3.66
    MB |'
  prefs: []
  type: TYPE_TB
- en: The second table provides some different information, namely the requests per
    second that were sent to the server. In this table, higher numbers are better.
    The headings in this table correlate to their opposites in the previous table;
    the *1%* column correlates to the *99%* column, for example.
  prefs: []
  type: TYPE_NORMAL
- en: The numbers in this table are much more interesting. What they describe is that,
    on average, the server is able to handle 38,884 requests per second. But the average
    isn’t too useful, and is it not a number that engineers should rely on.
  prefs: []
  type: TYPE_NORMAL
- en: Consider that it’s often the case that one request from a user can result in
    several requests being sent to a given service. For example, if a user opens a
    web page that lists which ingredients they should stock up on based on their top
    10 recipes, that one request might then generate 10 requests to the recipe service.
    The slowness of the overall user request is then compounded by the slowness of
    the backend service requests. For this reason, it’s important to pick a higher
    percentile, like 95% or 99%, when reporting service speed. This is referred to
    as being the *top percentile* and is abbreviated as *TP95* or *TP99* when communicating
    throughput.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of these results, one can say the TP99 has a latency of 0ms, or
    a throughput of 29,487 requests per second.
  prefs: []
  type: TYPE_NORMAL
- en: The third table is the result of providing the `-l` flag, and contains more
    granular latency information.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-8\. Autocannon detailed latency results
  prefs: []
  type: TYPE_NORMAL
- en: '| Percentile | Latency | Percentile | Latency | Percentile | Latency |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.001% | 0ms | 10% | 0ms | 97.5% | 0ms |'
  prefs: []
  type: TYPE_TB
- en: '| 0.01% | 0ms | 25% | 0ms | 99% | 0ms |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1% | 0ms | 50% | 0ms | 99.9% | 1ms |'
  prefs: []
  type: TYPE_TB
- en: '| 1% | 0ms | 75% | 0ms | 99.99% | 2ms |'
  prefs: []
  type: TYPE_TB
- en: '| 2.5% | 0ms | 90% | 0ms | 99.999% | 3ms |'
  prefs: []
  type: TYPE_TB
- en: The second-to-last row explains that 99.99% of requests (four nines) will get
    a response within at least 2ms. The final row explains that 99.999% of requests
    will get a response within 3ms.
  prefs: []
  type: TYPE_NORMAL
- en: This information can then be graphed to better convey what’s going on, as shown
    in [Figure 3-5](#fig_bm_latency_graph).
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocannon Latency Results Graph](assets/dsnj_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Autocannon latency results graph
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Again, with these low numbers, the results aren’t that interesting yet.
  prefs: []
  type: TYPE_NORMAL
- en: Based on my results, I can determine that, assuming TP99, the absolute best
    throughput I can get from a Node.js service using this specific version of Node.js
    and this specific hardware is roughly 25,000 r/s (after some conservative rounding).
    It would then be silly to attempt to achieve anything higher than that value.
  prefs: []
  type: TYPE_NORMAL
- en: As it turns, out 25,000 r/s is actually pretty high, and you’ll very likely
    never end up in a situation where achieving such a throughput from a single application
    instance is a requirement. If your use-case does demand higher throughput, you’ll
    likely need to consider other languages like Rust or C++.
  prefs: []
  type: TYPE_NORMAL
- en: Reverse Proxy Concerns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously I claimed that performing certain actions, specifically gzip compression
    and TLS termination, within a reverse proxy is usually faster than performing
    them within a running Node.js process. Load tests can be used to see if these
    claims are true.
  prefs: []
  type: TYPE_NORMAL
- en: These tests run the client and the server on the same machine. To accurately
    load test your production application, you’ll need to test in a production setting.
    The intention here is to measure CPU impact, as the network traffic generated
    by Node.js and HAProxy should be equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a baseline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'But first, another baseline needs to be established, and an inevitable truth
    must be faced: introducing a reverse proxy must increase latency by at least a
    little bit. To prove this, use the same *benchmark/native-http.js* file from before.
    However, this time you’ll put minimally configured HAProxy in front of it. Create
    a configuration file with the content from [Example 3-12](#ex_haproxy_benchmark).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-12\. *haproxy/benchmark-basic.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the service in one terminal window and HAProxy in a second terminal window,
    and then run the same Autocannon load test in a third terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The results I get look like those in [Figure 3-6](#fig_bm_latency_haproxy_basic).
    The TP99 throughput is 19,967 r/s, a decrease of 32%, and the max request took
    28.6ms.
  prefs: []
  type: TYPE_NORMAL
- en: These results may seem high when compared to the previous results, but again,
    remember that the application isn’t doing much work. The TP99 latency for a request,
    both before and after adding HAProxy, is still less than 1ms. If a real service
    takes 100ms to respond, the addition of HAProxy has increased the response time
    by less than 1%.
  prefs: []
  type: TYPE_NORMAL
- en: '![HAProxy Latency](assets/dsnj_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. HAProxy latency
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: HTTP compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simple pass-through configuration file is required for the next two tests.
    This configuration will have HAProxy simply forward requests from the client to
    the server. The config file has a `mode tcp` line, which means HAProxy will essentially
    act as an L4 proxy and not inspect the HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: Having HAProxy ensures the benchmarks will test the effects of offloading processing
    from Node.js to HAProxy, not the effects of an additional network hop. Create
    an *haproxy/passthru.cfg* file with the contents from [Example 3-13](#ex_haproxy_passthru).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-13\. *haproxy/passthru.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now you can measure the cost of performing gzip compression. Compression versus
    no compression won’t be compared here. (If that were the goal, the tests would
    absolutely need to be on separate machines, since the gain is in reduced bandwidth.)
    Instead, the performance of performing compression in HAProxy versus Node.js is
    compared.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the same *server-gzip.js* file that was created in [Example 2-4](ch02.html#ex_node_gzip),
    though you’ll want to comment out the `console.log` calls. The same *haproxy/compression.cfg*
    file created in [Example 3-6](#ex_haproxy_compression) will also be used, as well
    as the *haproxy/passthru.cfg* file you just created from [Example 3-13](#ex_haproxy_passthru).
    For this test, you’ll need to stop HAProxy and restart it with a different configuration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here are the results when I ran the tests on my machine. [Figure 3-7](#fig_bm_latency_gzip_node)
    shows the results of running gzip with Node.js, and [Figure 3-8](#fig_bm_latency_gzip_haproxy)
    contains the results for HAProxy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Node.js gzip Compression Latency](assets/dsnj_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Node.js gzip compression latency
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This test shows that requests are served a bit faster using HAProxy for performing
    gzip compression than when using Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: '![HAProxy gzip Compression Latency](assets/dsnj_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. HAProxy gzip compression latency
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TLS termination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TLS absolutely has a negative impact on application performance^([5](ch03.html#idm46291190530232))
    (in an HTTP versus HTTPS sense). These tests just compare the performance impact
    of performing TLS termination within HAProxy instead of Node.js, not HTTP compared
    to HTTPS. The throughput numbers have been reproduced in the following since the
    tests run so fast that the latency listing graphs mostly contains zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, performing TLS termination within the Node.js process is tested. For
    this test use the same *recipe-api/producer-https-basic.js* file that you created
    in [Example 2-7](ch02.html#ex_node_server_https), commenting out any `console.log`
    statements from the request handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 3-9](#table_bm_tls_node) contains the results of running this load test
    on my machine.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-9\. Native Node.js TLS termination throughput
  prefs: []
  type: TYPE_NORMAL
- en: '| Stat | 1% | 2.5% | 50% | 97.5% | Avg | Stdev | Min |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Req/Sec | 7,263 | 11,991 | 13,231 | 18,655 | 13,580.7 | 1,833.58 | 7,263
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bytes/Sec | 2.75 MB | 4.53 MB | 5 MB | 7.05 MB | 5.13 MB | 693 kB | 2.75
    MB |'
  prefs: []
  type: TYPE_TB
- en: 'Next, to test HAProxy, make use of the *recipe-api/producer-http-basic.js*
    file created back in [Example 1-6](ch01.html#ex_producer) (again, comment out
    the `console.log` calls), as well as the *haproxy/tls.cfg* file from [Example 3-8](#ex_haproxy_tls):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 3-10](#table_bm_tls_haproxy) contains the results of running this load
    test on my machine.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-10\. HAProxy TLS termination throughput
  prefs: []
  type: TYPE_NORMAL
- en: '| Stat | 1% | 2.5% | 50% | 97.5% | Avg | Stdev | Min |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Req/Sec | 960 | 1,108 | 1,207 | 1,269 | 1,202.32 | 41.29 | 960 |'
  prefs: []
  type: TYPE_TB
- en: '| Bytes/Sec | 216 kB | 249 kB | 272 kB | 286 kB | 271 kB | 9.29 kB | 216 kB
    |'
  prefs: []
  type: TYPE_TB
- en: In this case, a massive penalty happens when having HAProxy perform the TLS
    termination instead of Node.js! However, take this with a grain of salt. The JSON
    payload being used so far is about 200 bytes long. With a larger payload, like
    those in excess of 20kb, HAProxy usually outperforms Node.js when doing TLS termination.
  prefs: []
  type: TYPE_NORMAL
- en: As with all benchmarks, it’s important to test your application in your environment.
    The services used in this book are quite simple; a “real” application, doing CPU-intensive
    work like template rendering, and sending documents with varying payload sizes
    will behave completely differently.
  prefs: []
  type: TYPE_NORMAL
- en: Protocol Concerns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now you’ll load test some of the previously covered protocols, namely JSON over
    HTTP, GraphQL, and gRPC. Since these approaches do change the payload contents,
    measuring their transmission over a network will be more important than in [“Reverse
    Proxy Concerns”](#ch_scaling_sec_bm_subsec_rp). Also, recall that protocols like
    gRPC are more likely to be used for cross-service traffic than for external traffic.
    For that reason, I’ll run these load tests on two different machines within the
    same cloud provider data center.
  prefs: []
  type: TYPE_NORMAL
- en: For these tests, your approach is going to be to cheat a little bit. Ideally,
    you’d build a client from scratch, one that would natively speak the protocol
    being tested and would measure the throughput. But since you already built the
    *web-api* clients that accept HTTP requests, you’ll simply point Autocannon at
    those so that you don’t need to build three new applications. This is visualized
    in [Figure 3-9](#fig_benchmark_cloud).
  prefs: []
  type: TYPE_NORMAL
- en: Since there’s an additional network hop, this approach can’t accurately measure
    performance, like X is Y% faster than Z, but it can rank their performance—as
    implemented in Node.js using these particular libraries—from fastest to slowest.
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocannon and web-api run on one VPS, while recipe-api runs on the other](assets/dsnj_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Benchmarking in the cloud
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you have access to a cloud provider and a few dollars to spare, feel free
    to spin up two new VPS instances and copy the *examples/* directory that you have
    so far to them. You should use machines with at least two CPU cores. This is particularly
    important on the client where Autocannon and *web-api* might compete for CPU access
    with a single core. Otherwise, you can also run the examples on your development
    machine, at which point you can omit the `TARGET` environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to replace `<RECIPE_API_IP>` with the IP address or hostname of the
    *recipe-api* service in each of the following examples.
  prefs: []
  type: TYPE_NORMAL
- en: JSON over HTTP benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This first load test will benchmark the *recipe-api/producer-http-basic.js*
    service created in [Example 1-6](ch01.html#ex_producer) by sending requests through
    the *web-api/consumer-http-basic.js* service created in [Example 1-7](ch01.html#ex_consumer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: My results for this benchmark appear in [Figure 3-10](#fig_benchmark_jsonhttp).
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of JSON over HTTP Results](assets/dsnj_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Benchmarking JSON over HTTP
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GraphQL benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This next load test will use the *recipe-api/producer-graphql.js* service created
    in [Example 2-11](ch02.html#ex_graphql_producer) by sending requests through the
    *web-api/consumer-graphql.js* service created in [Example 2-12](ch02.html#ex_graphql_consumer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: My results for this load test appear in [Figure 3-11](#fig_benchmark_graphql).
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of GraphQL Results](assets/dsnj_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Benchmarking GraphQL
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: gRPC benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This final load test will test the *recipe-api/producer-grpc.js* service created
    in [Example 2-14](ch02.html#ex_grpc_producer) by sending requests through the
    *web-api/consumer-grpc.js* service created in [Example 2-15](ch02.html#ex_grpc_consumer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: My results for this load test appear in [Figure 3-12](#fig_benchmark_grpc).
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of gRPC Results](assets/dsnj_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Benchmarking gRPC
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to these results, JSON over HTTP is typically the fastest, with GraphQL
    being the second fastest and gRPC being the third fastest. Again, these results
    will change for real-world applications, especially when dealing with more complex
    payloads or when servers are farther apart.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that `JSON.stringify()` is extremely optimized in V8,
    so any other serializer is going to have a hard time keeping up. GraphQL has its
    own parser for parsing query strings, which will add some additional latency versus
    a query represented purely using JSON. gRPC needs to do a bunch of `Buffer` work
    to serialize and deserialize objects into binary. This means gRPC should be faster
    in more static, compiled languages like C++ than in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Coming Up with SLOs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An SLO can cover many different aspects of a service. Some of these are business-related
    requirements, like the service will never double charge a customer for a single
    purchase. Other more generic SLOs are the topic of this section, like the service
    will have a TP99 latency of 200ms and will have an uptime of 99.9%.
  prefs: []
  type: TYPE_NORMAL
- en: Coming up with an SLO for latency can be tricky. For one thing, the time it
    will take for your application to serve a response might depend on the time it
    takes an upstream service to return its response. If you’re adopting the concept
    of an SLO for the first time, you’ll need upstream services to *also* come up
    with SLOs of their own. Otherwise, when their service latency jumps from 20ms
    to 40ms, who’s to know if they’re actually doing something wrong?
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to keep in mind is that your service will very likely receive
    more traffic during certain times of the day and certain days of the week, especially
    if traffic is governed by the interactions of people. For example, a backend service
    used by an online retailer will get more traffic on Mondays, in the evenings,
    and near holidays, whereas a service receiving periodic sensor data will always
    handle data at the same rate. Whatever SLOs you do decide on will need to hold
    true during times of peak traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Something that can make measuring performance difficult is the concept of the
    *noisy neighbor*. This is a problem that occurs when a service is running on a
    machine with other services and those other services end up consuming too many
    resources, such as CPU or bandwidth. This can cause your service to take more
    time to respond.
  prefs: []
  type: TYPE_NORMAL
- en: When first starting with an SLO, it’s useful to perform a load test on your
    service as a starting point. For example, [Figure 3-13](#fig_benchmark_radar)
    is the result of benchmarking a production application that I built. With this
    service, the TP99 has a latency of 57ms. To get it any faster would require performance
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to completely mimic production situations when load testing your service.
    For example, if a real consumer makes a request through a reverse proxy, then
    make sure your load tests also go through the same reverse proxy, instead of connecting
    directly to the service.
  prefs: []
  type: TYPE_NORMAL
- en: '![Benchmark of a Production Application](assets/dsnj_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. Benchmarking a production application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another thing to consider is what the consumers of your service are expecting.
    For example, if your service provides suggestions for an autocomplete form when
    a user types a query, having a response time of less than 100ms is vital. On the
    other hand, if your service triggers the creation of a bank loan, having a response
    time of 60s might also be acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: If a downstream service has a hard response time requirement and you’re not
    currently satisfying it, you’ll have to find a way to make your service more performant.
    You can try throwing more servers at the problem, but often you’ll need to get
    into the code and make things faster. Consider adding a performance test when
    code is being considered for merging. [“Automated Testing”](ch06.html#ch_deployments_sec_testing)
    discusses automated tests in further detail.
  prefs: []
  type: TYPE_NORMAL
- en: When you do determine a latency SLO, you’ll want to determine how many service
    instances to run. For example, you might have an SLO where the TP99 response time
    is 100ms. Perhaps a single server is able to perform at this level when handling
    500 requests per minute. However, when the traffic increases to 1,000 requests
    per minute, the TP99 drops to 150ms. In this situation, you’ll need to add a second
    service. Experiment with adding more services, and testing load at different rates,
    to understand how many services it takes to increase your traffic by two, three,
    or even ten times the amount.
  prefs: []
  type: TYPE_NORMAL
- en: Autocannon has the `-R` flag for specifying an exact number of requests per
    second. Use this to throw an exact rate of requests at your service. Once you
    do that, you can measure your application at different request rates and find
    out where it stops performing at the intended latency. Once that happens, add
    another service instance and test again. Using this method, you’ll know how many
    service instances are needed in order to satisfy the TP99 SLO based on different
    overall throughputs.
  prefs: []
  type: TYPE_NORMAL
- en: Using the *cluster-fibonacci.js* application created in [Example 3-2](#ex_fibonacci)
    as a guide, you’ll now attempt to measure just this. This application, with a
    Fibonacci limit of 10,000, is an attempt to simulate a real service. The TP99
    value you’ll want to maintain is 20ms. Create another HAProxy configuration file
    *haproxy/fibonacci.cfg* based on the content in [Example 3-14](#ex_haproxy_fibonacci).
    You’ll iterate on this file as you add new service instances.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-14\. *haproxy/fibonacci.cfg*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This application is a little too CPU heavy. Add a sleep statement to simulate
    a slow database connection, which should keep the event loop a little busier.
    Introduce a `sleep()` function like this one, causing requests to take at least
    10ms longer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run a single instance of *cluster-fibonacci.js*, as well as HAProxy,
    using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: My TP99 value is 18ms, which is below the 20ms SLO, so I know that one instance
    can handle traffic of at least 10 r/s. So, now double that value! Run the Autocannon
    command again by setting the `-R` flag to 20\. On my machine the value is now
    24ms, which is too high. Of course, your results will be different. Keep tweaking
    the requests per second value until you reach the 20ms TP99 SLO threshold. At
    this point you’ve discovered how many requests per second a single instance of
    your service can handle! Write that number down.
  prefs: []
  type: TYPE_NORMAL
- en: Next, uncomment the second-to-last line of the *haproxy/fibonacci.cfg* file.
    Also, run another instance of *cluster-fibonacci.js*, setting the `PORT` value
    to `5002`. Restart HAProxy to reload the modified config file. Then, run the Autocannon
    command again with increased traffic. Increase the requests per second until you
    reach the threshold again, and write down the value. Do it a third and final time.
    [Table 3-11](#table_fibonacci_sla) contains my results.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-11\. Fibonacci SLO
  prefs: []
  type: TYPE_NORMAL
- en: '| Instance count | 1 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Max r/s | 12 | 23 | 32 |'
  prefs: []
  type: TYPE_TB
- en: With this information I can deduce that if my service needs to run with 10 requests
    per second, then a single instance will allow me to honor my 20ms SLO for my consumers.
    If, however, the holiday season is coming and I know consumers are going to want
    to calculate the 5,000th Fibonacci sequence at a rate of 25 requests per second,
    then I’m going to need to run three instances.
  prefs: []
  type: TYPE_NORMAL
- en: If you work in an organization that doesn’t currently make any performance promises,
    I encourage you to measure your service’s performance and come up with an SLO
    using current performance as a starting point. Add that SLO to your project’s
    *README* and strive to improve it each quarter.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark results are useful for coming up with initial SLO values. To know
    whether or not your application actually achieves an SLO in production requires
    observing real production SLIs. The next chapter covers application observability,
    which can be used to measure SLIs.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm46291192440536-marker)) The `fork()` method name is inspired
    by the fork system call, though the two are technically unrelated.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#idm46291192181256-marker)) More advanced applications might
    have some race-conditions unearthed when running multiple copies.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm46291191422376-marker)) This backend has a `balance <algorithm>`
    directive implicitely set to `roundrobin`. It can be set to `leastconn` to route
    requests to the instance with the fewest connections, `source` to consistently
    route a client by IP to an instance, and several other algorithm options are also
    available.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#idm46291191395832-marker)) You’ll need to manually refresh it
    any time you want to see updated statistics; the page only displays a static snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.html#idm46291190530232-marker)) Regardless of performance, it’s necessary
    that services exposed to the internet are encrypted.
  prefs: []
  type: TYPE_NORMAL
