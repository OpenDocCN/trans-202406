- en: Chapter 20\. Policy and Governance for Kubernetes Clusters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第20章 Kubernetes 集群的政策和治理
- en: Throughout this book we have introduced many different Kubernetes resource types,
    each with a specific purpose. It doesn’t take long before the resources on a Kubernetes
    cluster go from several, for a single microservice application, to hundreds and
    thousands, for a complete distributed application. In the context of a production
    cluster it isn’t hard to imagine the challenges associated with managing thousands
    of resources.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们介绍了许多不同的 Kubernetes 资源类型，每种都有特定的用途。在 Kubernetes 集群中的资源数量从单一微服务应用的几个，迅速增加到完整分布式应用的几百甚至几千个，所需管理的资源数目也随之增加。在生产集群的上下文中，管理成千上万的资源所面临的挑战是显而易见的。
- en: In this chapter, we introduce the concepts of policy and governance. *Policy*
    is a set of constraints and conditions for how Kubernetes resources can be configured.
    *Governance* provides the ability to verify and enforce organizational policies
    for all resources deployed to a Kubernetes cluster, such as ensuring all resources
    use current best practices, comply with security policy, or adhere to company
    conventions. Whatever your case may be, your tooling needs to be flexible and
    scalable so that all resources defined on a cluster comply with your organization’s
    defined policies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了政策和治理的概念。*政策* 是一组约束和条件，规定 Kubernetes 资源的配置方式。*治理* 提供了验证和执行所有部署到 Kubernetes
    集群的资源的组织政策的能力，例如确保所有资源使用当前最佳实践，符合安全政策，或遵守公司惯例。无论你的情况如何，你的工具都需要灵活和可扩展，以便集群上定义的所有资源都符合你组织定义的政策。
- en: Why Policy and Governance Matter
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么政策和治理很重要
- en: There are many different types of policies in Kubernetes. For example, NetworkPolicy
    allows you to specify what network services and endpoints a Pod can connect to.
    PodSecurityPolicy enables fine-grained control over the security elements of a
    Pod. Both can be used to configure network or container runtimes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有许多不同类型的政策。例如，NetworkPolicy 允许你指定一个 Pod 可以连接到的网络服务和端点。PodSecurityPolicy
    使你能够对 Pod 的安全元素进行细粒度控制。两者都可以用于配置网络或容器运行时。
- en: However, you might want to enforce a policy before Kubernetes resources are
    even created. This is the problem that policy and governance solve. At this point,
    you might be thinking, “Isn’t this what role-based access control does?” However,
    as you’ll see in this chapter, RBAC isn’t granular enough to restrict specific
    fields within resources from being set.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可能希望在 Kubernetes 资源甚至被创建之前就强制执行一个政策。这就是政策和治理解决的问题。此时，你可能会想到，“这不是基于角色的访问控制（RBAC）所做的事情吗？”然而，正如你将在本章中看到的，RBAC
    的粒度不足以限制资源中特定字段的设置。
- en: 'Here are some common examples of policies that cluster administrators often
    configure:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些集群管理员常常配置的政策的常见例子：
- en: All containers *must* only come from a specific container registry.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有容器 *必须* 只来自特定的容器注册表。
- en: All Pods *must* be labeled with the department name and contact information.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 Pod *必须* 带有部门名称和联系信息的标签。
- en: All Pods *must* have both CPU and memory resource limits set.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 Pod *必须* 设置 CPU 和内存资源限制。
- en: All Ingress hostnames *must* be unique across a cluster.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 Ingress 主机名 *必须* 在集群中唯一。
- en: A certain service *must* not be made available on the internet.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某个服务 *不得* 对外开放。
- en: Containers *must* not listen on privileged ports.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器 *不得* 监听特权端口。
- en: Cluster administrators may also want to audit existing resources on a cluster,
    perform dry-run policy evaluations, or even mutate a resource based on a set of
    conditions—for example, applying labels to a Pod if they aren’t present.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 群集   集群管理员也可能想要审计集群上的现有资源，进行干运行政策评估，或基于一组条件修改资源——例如，如果不存在的话，对一个 Pod 应用标签。
- en: It’s very important for cluster administrators to be able to define policy and
    perform compliance audits without interfering with the developers’ ability to
    deploy applications to Kubernetes. If developers are creating noncompliant resources,
    you need a system to make sure they get the feedback and remediation they need
    to bring their work into compliance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理员能够定义政策并进行合规性审计，而不干扰开发人员将应用部署到 Kubernetes 的能力，这一点非常重要。如果开发人员创建了不合规的资源，你需要一个系统，确保他们得到反馈和修正，以使他们的工作符合规定。
- en: Let’s take a look at how to achieve policy and governance by leveraging core
    extensibility components of Kubernetes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何利用 Kubernetes 核心的扩展组件实现政策和治理。
- en: Admission Flow
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入驻流程
- en: To understand how policy and governance ensures resources are compliant before
    they are created in your Kubernetes cluster, you must first understand the request
    flow through the Kubernetes API server. [Figure 20-1](#fig1701) depicts the flow
    of an API request through the API server. Here, we’ll focus on mutating admission,
    validating admission, and webhooks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![kur3 2001](assets/kur3_2001.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Figure 20-1\. API request flow through the Kubernetes API server
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Admission controllers operate inline as an API request flows through the Kubernetes
    API server and are used to either mutate or validate the API request resource
    before it’s saved to storage. Mutating admission controllers allow the resource
    to be modified; validating admission controllers do not. There are many different
    types of admission controllers; this chapter focuses on admission webhooks, which
    are dynamically configurable. They allow a cluster administrator to configure
    an endpoint to which the API server can send requests for evaluation by creating
    either a MutatingWebhookConfiguration or a ValidatingWebhookConfiguration resource.
    The admission webhook will respond with an “admit” or “deny” directive to let
    the API server know whether to save the resource to storage.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Policy and Governance with Gatekeeper
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s dive into how to configure policies and ensure that Kubernetes resources
    are compliant. The Kubernetes project doesn’t provide any controllers that enable
    policy and governance, but there are open source solutions. Here, we will focus
    on an open source ecosystem project called [Gatekeeper](https://oreil.ly/u0deR).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Gatekeeper is a Kubernetes-native policy controller that evaluates resources
    based on defined policy and determines whether to allow a Kubernetes resource
    to be created or modified. These evaluations happen server-side as the API request
    flows through the Kubernetes API server, which means each cluster has a single
    point of processing. Processing the policy evaluations server-side means that
    you can install Gatekeeper on existing Kubernetes clusters without changing developer
    tooling, workflows, or continuous delivery pipelines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Gatekeeper uses *custom resource definitions* (CRDs) to define a new set of
    Kubernetes resources specific to configuring it, which allows cluster administrators
    to use familiar tools like `kubectl` to operate Gatekeeper. In addition, it provides
    real-time, meaningful feedback to the user on why a resource was denied and how
    to remediate the problem. These Gatekeeper-specific custom resources can be stored
    in source control and managed using GitOps workflows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Gatekeeper also performs *resource mutation* (resource modification based on
    defined conditions) and auditing. It is highly configurable and offers fine-grained
    control over what resources to evaluate and in which namespaces.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: What Is Open Policy Agent?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the core of Gatekeeper is [Open Policy Agent](https://oreil.ly/nbR5d), a
    cloud native open source policy engine that is extensible and allows policy to
    be portable across different applications. Open Policy Agent (OPA) is responsible
    for performing all policy evaluations and returning either an admit or deny. This
    gives Gatekeeper access to an ecosystem of policy tooling, such as, [Conftest](https://oreil.ly/ElWYE),
    which enables you to write policy tests and implement them in continuous integration
    pipelines before deployment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Open Policy Agent exclusively uses a native query language called [Rego](https://oreil.ly/Ar55f)
    for all policies. One of the core tenets of Gatekeeper is to abstract the inner
    workings of Rego from the cluster administrator and present a structured API in
    the form of a Kubernetes CRD to create and apply policy. This lets you share parameterized
    policies across organizations and the community. The Gatekeeper project maintains
    a policy library solely for this purpose (discussed later in this chapter).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Installing Gatekeeper
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start configuring policies, you’ll need to install Gatekeeper. Gatekeeper
    components run as Pods in the `gatekeeper-system` namespace and configure a webhook
    admission controller.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Do not install Gatekeeper on a Kubernetes cluster without first understanding
    how to safely create and disable policy. You should also review the installation
    YAML before installing Gatekeeper to ensure that you are comfortable with the
    resources it creates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install Gatekeeper using the Helm package manager:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gatekeeper installation requires cluster-admin permissions and is version specific.
    Please refer to the official documentation for the latest release of [Gatekeeper](https://oreil.ly/GvLHc).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the installation is complete, confirm that Gatekeeper is up and running:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can also review how the webhook is configured using this command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Under the `rules` section of the output above, we see that all resources are
    being sent to the webhook admission controller, running as a service named `gatekeeper-webhook-service`
    in the `gatekeeper-system` namespace. Only resources from namespaces that aren’t
    labeled `admission.gatekeeper.sh/ignore` will be considered for policy evaluation.
    Finally, the `failurePolicy` is set to `Ignore`, which means that this is a *fail
    open configuration*: if the Gatekeeper service doesn’t respond within the configured
    timeout of three seconds, the request will be admitted.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Policies
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have Gatekeeper installed, you can start configuring policies.
    We will first go through a canonical example and demonstrate how the cluster administrator
    creates policies. Then we’ll look at the developer experience when creating compliant
    and noncompliant resources. We will then expand on each step to gain a deeper
    understanding, and walk you through the process of creating a sample policy stating
    that container images can only come from one specific registry. This example is
    based on the [Gatekeeper policy library](https://oreil.ly/ikfZk).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: First, you’ll need to configure the policy we need to create a custom resource
    called a *constraint template*. This is usually done by a cluster administrator.
    The constraint template in [Example 20-1](#example1701) requires you to provide
    a list of container repositories as parameters that Kubernetes resources are allowed
    to use.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-1\. allowedrepos-constraint-template.yaml
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create the constraint template using the following command:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now you can create a constraint resource to put the policy into effect (again,
    playing the role of the cluster administrator). The constraint in [Example 20-2](#example1702)
    allows all containers with the prefix of `gcr.io/kuar-demo/` in the `default`
    namespace. The `enforcementAction` is set to “deny”: any noncompliant resources
    will be denied.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-2\. allowedrepos-constraint.yaml
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The next step is to create some Pods to test that the policy is indeed working.
    [Example 20-3](#example1703) creates a Pod using a container image, `gcr.io/kuar-demo/kuard-amd64:blue`,
    that complies with the constraint we defined in the previous step. Workload resource
    creation is typically performed by the developer responsible for operating the
    service or a continuous delivery pipeline.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-3\. compliant-pod.yaml
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: What happens if we create a noncompliant Pod? [Example 20-4](#example1704) creates
    a Pod using a container image, `nginx`, that is *not* compliant with the constraint
    we defined in the previous step. Workload resource creation would typically be
    performed by the developer or continuous delivery pipeline responsible for operating
    the service. Note the output in [Example 20-4](#example1704).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-4\. noncompliant-pod.yaml
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Example 20-4](#example1704) shows that an error is returned to the user with
    details on why the resource was not created and how to remediate the issue. Cluster
    administrators can configure the error message in the constraint template.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your constraint’s scope is Pods and you create a resource that generates
    Pods, such as ReplicaSets, Gatekeeper will return an error. However, it won’t
    be returned to you, the user, but to the controller trying to create the Pod.
    To see these error messages, look in the event log for the relevant resource.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Constraint Templates
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have walked through a canoncial example, take a closer look at the
    constraint template in [Example 20-1](#example1701), which takes a list of container
    repositories that are allowed in Kubernetes resources.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'This constraint template has an `apiVersion` and `kind` that are part of the
    custom resources used only by Gatekeeper. Under the `spec` section, you’ll see
    the name `K8sAllowedRepos`: remember that name, because you’ll use it as the constraint
    kind when creating constraints. You’ll also see a schema that defines an array
    of strings for the cluster administrator to configure. This is done by providing
    a list of allowed container registries. It also contains the raw Rego policy definition
    (under the `target` section). This policy evaluates containers and initContainers
    to ensure that the container repository name starts with the values provided by
    the constraint. The `msg` section defines the message that is sent back to the
    user if the policy is violated.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Creating Constraints
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To instantiate a policy, you must create a constraint that provides the template’s
    required parameters. There may be many constraints that match the kind of a specific
    constraint template. Let’s take a closer look at the constraint we used in [Example 20-2](#example1702),
    which allows only container images that originate from *gcr.io/kuar-demo/*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice that the constraint is of the `kind` “K8sAllowedRepos,” which
    was defined as part of the constraint template. It also defines an `enforcementAction`
    of “deny,” meaning that noncompliant resources will be denied. `enforcementAction`
    also accepts “dryrun” and “warn”: “dryrun” uses the audit feature to test policies
    and verify their impact; “warn” sends a warning back to the user with the associated
    message, but allows them to create or update. The `match` portion defines the
    scope of this constraint, all Pods in the default namespace. Finally, the `parameters`
    section is required to satisfy the constraint template (an array of strings).
    The following demonstrates the user experience when the `enforcementAction` is
    set to “warn”:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Warning
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Constraints are only enforced on resource CREATE and UPDATE events. If you already
    have workloads running on a cluster, Gatekeeper will not reevaluate them until
    a CREATE or UPDATE event takes place.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a real-world example to demonstrate: say you create a policy that only
    allows containers from a specific registry. All workloads that are already running
    on the cluster will continue to do so. If you scale the workload Deployment from
    1 to 2, the ReplicaSet will attempt to create another Pod. If that Pod doesn’t
    have a container from an allowed repository, then it will be denied. It’s important
    to set the `enforcementAction` to “dryrun” and audit to confirm that any policy
    violations are known before setting the `enforcementAction` to “deny.”'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Audit
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being able to enforce policy on new resources is only one piece of the policy
    and governance story. Policies often change over time, and you can also use Gatekeeper
    to confirm that everything currently deployed is still compliant. Additionally,
    you may already have a cluster full of services and wish to install Gatekeeper
    to bring these resources into compliance. Gatekeeper’s audit capabilities allow
    cluster administrators to get a list of current, noncompliant resources on a cluster.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate how auditing works, let’s look at an example. We’re going to
    update the `repo-is-kuar-demo` constraint to have an `enforcementAction` action
    of “dryrun” (as shown in [Example 20-5](#example1707)). This will allow users
    to create noncompliant resources. We will then determine which resources are noncompliant
    using audit.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-5\. allowedrepos-constraint-dryrun.yaml
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Update the constraint by running the following command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a noncompliant Pod using the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To audit the list of noncompliant resources for a given constraint, run a `kubectl
    get constraint` on that constraint and specify that you want the output in YAML
    format as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Under the `status` section, you can see the `auditTimestamp`, which is the last
    time the audit was run. `totalViolations` lists the number of resources that violate
    this constraint. The `violations` section lists the violations. We can see that
    the nginx-noncompliant Pod is in violation and the message with the details why.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using a constraint `enforcementAction` of “dryrun” along with audit is a powerful
    way to confirm that your policy is having the desired impact. It also creates
    a workflow to bring resources into compliance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Mutation
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have covered how you can use constraints to validate if a resource
    is compliant. What about modifying resources to make them compliant? This is handled
    via the mutation feature in Gatekeeper. Earlier in this chapter, we discussed
    two different type of admission webhooks, mutating and validating. By default,
    Gatekeeper is only deployed as a validating admission webhook, but it can be configured
    to operate as a mutating admission webhook.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Mutation features in Gatekeeper are in beta state and may change. We share them
    to demonstrate Gatekeeper’s upcoming capabilities. The installation steps in this
    chapter do not cover enabling mutation. Please refer to the Gatekeeper project
    for more information on [enabling mutation](https://oreil.ly/DQKhl).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through an example to demonstrate the power of mutation. In this
    example, we will set the `imagePullPolicy` to “Always” on all Pods. We will assume
    that Gatekeeper is configured correctly to support mutation. [Example 20-6](#example1708)
    defines a mutation assignment that matches all Pods except those in the “system”
    namespace, and assigns a value of “Always” to `imagePullPolicy`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-6\. imagepullpolicyalways-mutation.yaml
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create the mutation assignment:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now create a Pod. This Pod doesn’t have `imagePullPolicy` explicitly set, so
    by default this field is set to “IfNotPresent.” However, we expect Gatekeeper
    to mutate this field to “Always”:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Validate that the `imagePullPolicy` has been successfully mutated to “Always”
    by running the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Mutating admission happens before validating admission, so create constraints
    that validate the mutations you expect to apply to the specific resource.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the Pod using the following command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Delete the mutation assignment using the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Unlike validation, mutation provides a way to remediate noncompliant resources
    automatically on behalf of the cluster administrator.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Data Replication
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When writing constraints you may want to compare the value of one field to
    the value of a field in another resource. A specific example of when you might
    need to do this is making sure that ingress hostnames are unique across a cluster.
    By default, Gatekeeper can only evaluate fields within the current resource: if
    comparisons across resources are required to fulfill a policy, it must be configured.
    Gatekeeper can be configured to cache specific resources into Open Policy Agent
    to allow comparisons across resources. The resource in [Example 20-7](#example1709)
    configures Gatekeeper to cache Namespace and Pod resources.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-7\. config-sync.yaml
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You should only cache the specific resources needed to perform a policy evaluation.
    Having hundreds or thousands of resources cached in OPA will require more memory
    and may also have security implications.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The constraint template in [Example 20-8](#example1710) demonstrates how to
    compare something in the Rego section (in this case, unique ingress hostnames).
    Specifically, “data.inventory” refers to the cache resources, as opposed to “input,”
    which is the resource sent for evaluation from the Kubernetes API server as part
    of the admission flow. This example is based on the [Gatekeeper policy library](https://oreil.ly/gGrts).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Example 20-8\. uniqueingresshost-constraint-template.yaml
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Data replication is a powerful tool that allows you to make comparisons across
    Kubernetes resources. We recommend only configuring it if you have policies that
    require it to function. If you use it, scope it only to the relevant resources.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gatekeeper emits metrics in Prometheus format to enable continuous resource
    compliance monitoring. You can view simple metrics regarding Gatekeeper’s overall
    health, such as the numbers of constraints, constraint templates, and requests
    being set to Gatekeeper.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, details on policy compliance and governance are also available:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The total number of audit violations
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of constraints by `enforcementAction`
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audit duration
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Completely automating the policy and governance process is the ideal goal, so
    we strongly recommended that you monitor Gatekeeper from an external monitoring
    system and set alerts based on resource compliance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Policy Library
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the core tenets of the Gatekeeper project is to create reusable policy
    libraries that can be shared between organizations. Being able to share policies
    reduces boilerplate policy work and allows cluster administrators to focus on
    applying policy rather than writing it. The Gatekeeper project has a great [policy
    library](https://oreil.ly/uBY2h). It contains a general library with the most
    common policies as well as a *pod-security-policy* library that models the capabilities
    of the PodSecurityPolicy API as Gatekeeper policy. The great thing about this
    library is that it is always expanding and is open source, so feel free to contribute
    any policies that you write.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned about policy and governance and why they are
    important as more and more resources are deployed to Kubernetes. We covered the
    Gatekeeper project, a Kubernetes-native policy controller built on Open Policy
    Agent, and showed you to use it to meet your policy and governance requirements.
    From writing policies to auditing, you are now equipped with the know-how to meet
    your compliance needs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
