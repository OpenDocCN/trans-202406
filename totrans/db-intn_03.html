<html><head></head><body><section data-pdf-bookmark="Chapter 2. B-Tree Basics" data-type="chapter" epub:type="chapter"><div class="chapter" id="update_in_place_storage">&#13;
<h1><span class="label">Chapter 2. </span>B-Tree Basics</h1>&#13;
&#13;
&#13;
<p>In the previous chapter, we separated storage structures in two groups: <em>mutable</em> and <em>immutable</em> ones, and identified immutability as one of the core concepts influencing their design and implementation. Most of the mutable storage structures<a data-primary="in-place updates" data-type="indexterm" id="idm46466889524552"/> use an <em>in-place update</em> mechanism. During insert, delete, or update operations, data records are updated directly in their locations in the target file.</p>&#13;
&#13;
<p>Storage engines often allow multiple versions of the same data record to be present in the database; for example, when using multiversion concurrency control (see <a data-type="xref" href="ch05.html#mvcc">“Multiversion Concurrency Control”</a>) or slotted page organization (see <a data-type="xref" href="ch03.html#slotted_pages">“Slotted Pages”</a>). For the sake of simplicity, for now we assume that each key is associated only with one data record, which has a unique location.</p>&#13;
&#13;
<p>One of the most popular storage structures is a B-Tree. Many open source database systems are B-Tree based, and over the years they’ve proven to cover the majority of use cases.</p>&#13;
&#13;
<p>B-Trees are not a recent invention: they were introduced by Rudolph Bayer and Edward M. McCreight back in 1971 and gained popularity over the years. By 1979, there were already quite a few variants of B-Trees. Douglas Comer collected and systematized some of them <a data-type="xref" href="app01.html#COMER79">[COMER79]</a>.</p>&#13;
&#13;
<p>Before we dive into B-Trees, let’s first talk about why we should consider alternatives to traditional search trees, such as, for example, binary search trees, 2-3-Trees, and AVL Trees <a data-type="xref" href="app01.html#KNUTH98">[KNUTH98]</a>. For that, let’s recall what binary search trees are.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Binary Search Trees" data-type="sect1"><div class="sect1" id="search_trees">&#13;
<h1>Binary Search Trees</h1>&#13;
&#13;
<p>A <em>binary search tree</em> (BST) is<a data-primary="binary search trees (BSTs)" data-secondary="structure of" data-type="indexterm" id="idm46466889513960"/> a sorted in-memory data structure, used for efficient key-value lookups. BSTs consist of multiple nodes. Each tree node is represented by a key, a value associated with this key, and two child pointers (hence the name binary). BSTs start from a single node, called<a data-primary="root nodes" data-type="indexterm" id="idm46466889513112"/> a <em>root node</em>. There can be only one root in the tree. <a data-type="xref" href="#binary_tree">Figure 2-1</a> shows an example of a binary search tree.</p>&#13;
&#13;
<figure class="width-50"><div class="figure" id="binary_tree">&#13;
<img alt="dbin 0201" src="assets/dbin_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>Binary search tree</h6>&#13;
</div></figure>&#13;
&#13;
<p>Each<a data-primary="subtrees" data-type="indexterm" id="idm46466889507960"/> node splits the search space into left and right <em>subtrees</em>, as <a data-type="xref" href="#binary_tree_invariants">Figure 2-2</a> shows: a node key is <em>greater than</em> any key stored in its left subtree and <em>less than</em> any key stored in its right subtree <a data-type="xref" href="app01.html#SEDGEWICK11">[SEDGEWICK11]</a>.</p>&#13;
&#13;
<figure class="width-50"><div class="figure" id="binary_tree_invariants">&#13;
<img alt="dbin 0202" src="assets/dbin_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>Binary tree node invariants</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Following left pointers from the root of the tree down to the leaf level (the level where nodes have no children) locates the node holding the smallest key within the tree and a value associated with it. Similarly, following right pointers locates the node holding the largest key within the tree and a value associated with it. Values are allowed to be stored in all nodes in the tree. Searches start from the root node, and may terminate before reaching the bottom level of the tree if the searched key was found on a higher level.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tree Balancing" data-type="sect2"><div class="sect2" id="idm46466889500808">&#13;
<h2>Tree Balancing</h2>&#13;
&#13;
<p>Insert operations<a data-primary="binary search trees (BSTs)" data-secondary="balancing" data-type="indexterm" id="idm46466889499304"/><a data-primary="tree balancing" data-type="indexterm" id="idm46466889498040"/> do not follow any specific pattern, and element insertion might lead to the situation where the tree is unbalanced (i.e., one of its branches is longer than the other one). The worst-case scenario is shown in <a data-type="xref" href="#binary_tree_balancing">Figure 2-3</a> (b), where we end up<a data-primary="pathological trees" data-type="indexterm" id="idm46466889493352"/> with a <em>pathological</em> tree, which looks more like a linked list, and instead of desired logarithmic complexity, we get linear, as illustrated in <a data-type="xref" href="#binary_tree_balancing">Figure 2-3</a> (a).</p>&#13;
&#13;
<figure><div class="figure" id="binary_tree_balancing">&#13;
<img alt="dbin 0203" src="assets/dbin_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>Balanced (a) and unbalanced or pathological (b) tree examples</h6>&#13;
</div></figure>&#13;
&#13;
<p>This example might slightly exaggerate the problem, but it illustrates why the tree needs to be balanced: even though it’s somewhat unlikely that all the items end up on one side of the tree, at least some of them certainly will, which will significantly slow down searches.</p>&#13;
&#13;
<p>The <em>balanced</em> tree<a data-primary="balanced search trees" data-type="indexterm" id="idm46466889489864"/> is defined as one that has a height of <code>log<sub>2</sub> N</code>, where <code>N</code> is the total number of items in the tree, and the difference in height between the two subtrees is not greater than one<sup><a data-type="noteref" href="ch02.html#idm46466889485960" id="idm46466889485960-marker">1</a></sup> <a data-type="xref" href="app01.html#KNUTH98">[KNUTH98]</a>. Without balancing, we lose performance benefits of the binary search tree structure, and allow insertions and deletions order to determine tree shape.</p>&#13;
&#13;
<p>In the balanced tree, following the left or right node pointer reduces the search space in half on average, so lookup complexity is logarithmic: <code>O(log<sub>2</sub> N)</code>. If the tree is not balanced, worst-case complexity goes up to <code>O(N)</code>, since we might end up in the situation where all elements end up on one side of the tree.</p>&#13;
&#13;
<p>Instead of adding new elements to one of the tree branches and making it longer, while the other one remains empty (as shown in <a data-type="xref" href="#binary_tree_balancing">Figure 2-3</a> (b)), the tree is <em>balanced</em> after each operation. Balancing is done by reorganizing nodes in a way that minimizes tree height and keeps the number of nodes on each side within bounds.</p>&#13;
&#13;
<p>One of the ways to keep the tree balanced is to perform a rotation step after nodes are added or removed. If the insert operation leaves a branch unbalanced (two consecutive nodes in the branch have only one child), we can <em>rotate</em> nodes around the middle one. In the example shown in  <a data-type="xref" href="#binary_tree_rotation">Figure 2-4</a>, during rotation the middle node (3), known as a rotation <em>pivot</em>, is promoted one level higher, and its parent becomes its right child.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="binary_tree_rotation">&#13;
<img alt="dbin 0204" src="assets/dbin_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>Rotation step example</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Trees for Disk-Based Storage" data-type="sect2"><div class="sect2" id="trees_for_disk_based_storage">&#13;
<h2>Trees for Disk-Based Storage</h2>&#13;
&#13;
<p>As<a data-primary="binary search trees (BSTs)" data-secondary="trees for disk-based storage" data-type="indexterm" id="idm46466889474920"/><a data-primary="disk-based DBMS" data-secondary="trees for disk-based storage" data-type="indexterm" id="idm46466889473720"/> previously mentioned, unbalanced trees have a worst-case complexity of <code>O(N)</code>. Balanced trees give us an average <code>O(log<sub>2</sub> N)</code>. At the same time, due to<a data-primary="fanout" data-type="indexterm" id="idm46466889470760"/> low <em>fanout</em> (fanout is the maximum allowed number of children per node), we have to perform balancing, relocate nodes, and update pointers rather frequently. Increased maintenance costs make BSTs impractical as on-disk data structures <a data-type="xref" href="app01.html#NIEVERGELT74">[NIEVERGELT74]</a>.</p>&#13;
&#13;
<p>If we wanted to maintain a BST on disk, we’d face several problems. One problem is locality: since elements are added in random order, there’s no guarantee that a newly created node is written close to its parent, which means that node child pointers may span across several disk pages. We can improve the situation to a certain extent by modifying the tree layout and using paged binary trees (see <a data-type="xref" href="#paged_binary_trees">“Paged Binary Trees”</a>).</p>&#13;
&#13;
<p>Another problem, closely related to the cost of following child pointers, is<a data-primary="tree height" data-type="indexterm" id="idm46466889464152"/> tree height. Since binary trees have a fanout of just two, height is a binary logarithm of the number of the elements in the tree, and we have to perform <code>O(log<sub>2</sub> N)</code> seeks to locate the searched element and, subsequently, perform the same number of disk transfers. 2-3-Trees and other low-fanout trees have a similar limitation: while they are useful as <span class="keep-together">in-memory</span> data structures, small node size makes them impractical for external storage <a data-type="xref" href="app01.html#COMER79">[COMER79]</a>.</p>&#13;
&#13;
<p>A naive on-disk BST implementation would require as many disk seeks as comparisons, since there’s no built-in concept of locality. This sets us on a course to look for a data structure that would exhibit this property.</p>&#13;
&#13;
<p>Considering these factors, a version of the tree that would be better suited for disk implementation has to exhibit the following properties:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><em>High fanout</em> to improve locality of the neighboring keys.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Low height</em> to reduce the number of seeks during traversal.</p>&#13;
</li>&#13;
</ul>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Fanout and height are inversely correlated: the higher the fanout, the lower the height. If fanout is high, each node can hold more children, reducing the number of nodes and, subsequently, reducing height.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Disk-Based Structures" data-type="sect1"><div class="sect1" id="idm46466889457240">&#13;
<h1>Disk-Based Structures</h1>&#13;
&#13;
<p>We’ve<a data-primary="data" data-secondary="disk-based structures for" data-type="indexterm" id="Ddiskbased02"/><a data-primary="disk-based data structures" data-secondary="uses for" data-type="indexterm" id="idm46466889454776"/> talked about memory and disk-based storage (see <a data-type="xref" href="ch01.html#memory_vs_disk_based_stores">“Memory- Versus Disk-Based DBMS”</a>) in general terms. We can draw the same distinction for specific data structures: some are better suited to be used on disk and some work better in memory.</p>&#13;
&#13;
<p>As we have discussed, not every data structure that satisfies space and complexity requirements can be effectively used for on-disk storage. Data structures used in databases have to be adapted to account for persistent medium limitations.</p>&#13;
&#13;
<p>On-disk data structures are often used when the amounts of data are so large that keeping an entire dataset in memory is impossible or not feasible. Only a fraction of the data can<a data-primary="caching" data-type="indexterm" id="idm46466889450296"/> be <em>cached</em> in memory at any time, and the rest has to be stored on disk in a manner that allows efficiently accessing it.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Hard Disk Drives" data-type="sect2"><div class="sect2" id="idm46466889451256">&#13;
<h2>Hard Disk Drives</h2>&#13;
&#13;
<p>Most<a data-primary="disk-based data structures" data-secondary="hard disk drives" data-type="indexterm" id="idm46466889448696"/><a data-primary="hard disk drives (HDDs)" data-type="indexterm" id="idm46466889446648"/> traditional algorithms were developed when<a data-primary="spinning disks" data-type="indexterm" id="idm46466889444920"/> spinning disks were the most widespread persistent storage medium, which significantly influenced their design. Later, new developments in storage media, such as<a data-primary="flash drives" data-type="indexterm" id="idm46466889446040"/> flash drives, inspired new algorithms and modifications to the existing ones, exploiting the capabilities of the new hardware. These days, new types of data structures are emerging, optimized to work with nonvolatile byte-addressable storage (for example, <a data-type="xref" href="app01.html#XIA17">[XIA17]</a> <a data-type="xref" href="app01.html#KANNAN18">[KANNAN18]</a>).</p>&#13;
&#13;
<p>On<a data-primary="seeks" data-type="indexterm" id="idm46466889442648"/><a data-primary="sectors" data-type="indexterm" id="idm46466889441704"/> spinning disks, <em>seeks</em> increase costs of random reads because they require disk rotation and mechanical head movements to position the read/write head to the desired location. However, once the expensive part is done, reading or writing contiguous bytes (i.e., sequential operations) is <em>relatively</em> cheap.</p>&#13;
&#13;
<p>The smallest transfer unit of a spinning drive is a <em>sector</em>, so when some operation is performed, at least an entire sector can be read or written. Sector sizes typically range from 512 bytes to 4 Kb.</p>&#13;
&#13;
<p>Head positioning is the most expensive part of an operation on the HDD. This<a data-primary="sequential I/O" data-type="indexterm" id="idm46466889437448"/> is one of the reasons we often hear about the positive effects of <em>sequential</em> I/O: reading and writing contiguous memory segments from disk.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solid State Drives" data-type="sect2"><div class="sect2" id="idm46466889436456">&#13;
<h2>Solid State Drives</h2>&#13;
&#13;
<p>Solid state drives (SSDs)<a data-primary="solid state drives (SSDs)" data-type="indexterm" id="idm46466889434728"/><a data-primary="disk-based data structures" data-secondary="solid state drives (SSDs)" data-type="indexterm" id="idm46466889433688"/><a data-primary="memory cells" data-type="indexterm" id="idm46466889432840"/><a data-primary="strings" data-type="indexterm" id="idm46466889432136"/><a data-primary="arrays" data-type="indexterm" id="idm46466889431336"/><a data-primary="pages" data-type="indexterm" id="idm46466889430536"/><a data-primary="blocks" data-type="indexterm" id="idm46466889429736"/> do not have moving parts: there’s no disk that spins, or head that has to be positioned for the read. A typical SSD is built of <em>memory cells</em>, connected into <em>strings</em> (typically 32 to 64 cells per string), strings are combined into <em>arrays</em>, arrays are combined into <em>pages</em>, and pages are combined into <em>blocks</em> <a data-type="xref" href="app01.html#LARRIVEE15">[LARRIVEE15]</a>.</p>&#13;
&#13;
<p>Depending on the exact technology used, a cell can hold one or multiple bits of data. Pages vary in size between devices, but typically their sizes range from 2 to 16 Kb. Blocks typically contain 64 to 512 pages. Blocks are organized into planes and, finally, planes are placed on<a data-primary="dies" data-type="indexterm" id="idm46466889425592"/> a <em>die</em>. SSDs can have one or more dies. <a data-type="xref" href="#ssd_architecture">Figure 2-5</a> shows this hierarchy.</p>&#13;
&#13;
<figure><div class="figure" id="ssd_architecture">&#13;
<img alt="dbin 0205" src="assets/dbin_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>SSD organization schematics</h6>&#13;
</div></figure>&#13;
&#13;
<p>The smallest unit that can be written (programmed) or read is a page. However, we can only make changes to the empty memory cells (i.e., to ones that have been erased before the write). The smallest<a data-primary="erase blocks" data-type="indexterm" id="idm46466889421192"/> erase entity is not a page, but a block that holds multiple pages, which is why it is often called an <em>erase block</em>. Pages in an empty block have to be written sequentially.</p>&#13;
&#13;
<p>The<a data-primary="Flash Translation Layer (FTL)" data-type="indexterm" id="idm46466889419864"/><a data-primary="garbage collection" data-type="indexterm" id="idm46466889419016"/> part of a flash memory controller responsible for mapping page IDs to their physical locations, tracking empty, written, and discarded pages, is called the Flash Translation Layer (FTL) (see <a data-type="xref" href="ch07.html#flash_translation_layer">“Flash Translation Layer”</a> for more about FTL). It is also responsible for <em>garbage collection</em>, during which FTL finds blocks it can safely erase. Some blocks might still contain live pages. In this case, it relocates live pages from these blocks to new locations and remaps page IDs to point there. After this, it erases the now-unused blocks, making them available for writes.</p>&#13;
&#13;
<p>Since<a data-primary="block device" data-type="indexterm" id="idm46466889416168"/> in both device types (HDDs and SSDs) we are addressing chunks of memory rather than individual bytes (i.e., accessing data block-wise), most operating systems have a <em>block device</em> abstraction <a data-type="xref" href="app01.html#CESATI05">[CESATI05]</a>. It hides an internal disk structure and buffers I/O operations internally, so when we’re reading a <em>single word</em> from a block device, the <em>whole block</em> containing it is read. This is a constraint we cannot ignore and should always take into account when working with disk-resident data structures.</p>&#13;
&#13;
<p>In SSDs, we don’t have a strong emphasis on<a data-primary="random I/O" data-type="indexterm" id="idm46466889413304"/> random versus sequential I/O, as in HDDs, because the difference in latencies between random and sequential reads is not as large. There is <em>still</em> some difference caused by prefetching, reading contiguous pages, and internal parallelism <a data-type="xref" href="app01.html#GOOSSAERT14">[GOOSSAERT14]</a>.</p>&#13;
&#13;
<p>Even though garbage collection is usually a background operation, its effects may negatively impact write performance, especially in cases of random and unaligned write workloads.</p>&#13;
&#13;
<p>Writing only full blocks, and combining subsequent writes to the same block, can help to reduce the number of required I/O operations. We discuss buffering and immutability as ways to achieve that in later chapters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="On-Disk Structures" data-type="sect2"><div class="sect2" id="on_disk_data_structures">&#13;
<h2>On-Disk Structures</h2>&#13;
&#13;
<p>Besides<a data-primary="on-disk data structures" data-type="indexterm" id="idm46466889406728"/><a data-primary="disk-based data structures" data-secondary="on-disk structures" data-type="indexterm" id="idm46466889406120"/> the cost of disk access itself, the main limitation and design condition for building efficient on-disk structures is the fact that the smallest unit of disk operation is a block. To follow a pointer to the specific location within the block, we have to fetch an entire block. Since we already have to do that, we can change the layout of the data structure to take advantage of it.</p>&#13;
&#13;
<p>We’ve mentioned pointers several times throughout this chapter already, but this word has slightly different semantics for on-disk structures. On disk, most of the time we manage the data layout manually (unless, for example, we’re<a data-primary="memory mapped files" data-type="indexterm" id="idm46466889400344"/> using <a href="https://databass.dev/links/64">memory mapped files</a>). This is still similar to regular pointer <span class="keep-together">operations,</span> but we have to compute the target pointer addresses and follow the pointers explicitly.</p>&#13;
&#13;
<p>Most of the time, on-disk offsets are precomputed (in cases when the pointer is written on disk before the part it points to) or cached in memory until they are flushed on the disk. Creating long dependency chains in on-disk structures greatly increases code and structure complexity, so it is preferred to keep the number of pointers and their spans to a minimum.</p>&#13;
&#13;
<p>In summary, on-disk structures are designed with their target storage specifics in mind and generally optimize for fewer disk accesses. We can do this by improving locality, optimizing the internal representation of the structure, and reducing the number of out-of-page pointers.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#search_trees">“Binary Search Trees”</a>, we<a data-primary="paged binary trees" data-type="indexterm" id="idm46466889402696"/> came to the conclusion<a data-primary="fanout" data-type="indexterm" id="idm46466889401656"/><a data-primary="tree height" data-type="indexterm" id="idm46466889401048"/> that <em>high fanout</em> and <em>low height</em> are desired properties for an optimal on-disk data structure. We’ve also just discussed additional space overhead coming from pointers, and maintenance overhead from remapping these pointers as a result of balancing. B-Trees combine these ideas: increase node fanout, and reduce tree height, the number of node pointers, and the frequency of balancing operations.<a data-primary="" data-startref="Ddiskbased02" data-type="indexterm" id="idm46466889395592"/></p>&#13;
<aside class="pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="paged_binary_trees">&#13;
<h5>Paged Binary Trees</h5>&#13;
<p>Laying out a binary tree by grouping nodes into pages, as <a data-type="xref" href="#paged_binary_trees_1">Figure 2-6</a> shows, improves the situation with locality. To find the next node, it’s only necessary to follow a pointer in an already fetched page. However, there’s still some overhead incurred by the nodes and pointers between them. Laying the structure out on disk and its further maintenance are nontrivial endeavors, especially if keys and values are not presorted and added in random order. Balancing requires page reorganization, which in turn causes pointer updates.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="paged_binary_trees_1">&#13;
<img alt="dbin 0206" src="assets/dbin_0206.png"/>&#13;
<h6><span class="label">Figure 2-6. </span>Paged binary trees</h6>&#13;
</div></figure>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ubiquitous B-Trees" data-type="sect1"><div class="sect1" id="b_trees">&#13;
<h1>Ubiquitous B-Trees</h1>&#13;
<blockquote>&#13;
<p>We are braver than a bee, and a… longer than a tree…</p>&#13;
<p data-type="attribution">Winnie the Pooh</p>&#13;
</blockquote>&#13;
&#13;
<p>B-Trees<a data-primary="B-Trees, basics of" data-secondary="similarities between BSTs and B-Trees" data-type="indexterm" id="idm46466889385304"/><a data-primary="binary search trees (BSTs)" data-secondary="similarities to B-Trees" data-type="indexterm" id="idm46466889384456"/><a data-primary="ubiquitous B-Trees" data-type="indexterm" id="BTBubiq02"/> can be thought of as a vast catalog room in the library: you first have to pick the correct cabinet, then the correct shelf in that cabinet, then the correct drawer on the shelf, and then browse through the cards in the drawer to find the one you’re searching for. Similarly, a B-Tree builds a hierarchy that helps to navigate and locate the searched items quickly.</p>&#13;
&#13;
<p>As we discussed in <a data-type="xref" href="#search_trees">“Binary Search Trees”</a>, B-Trees build upon the foundation of balanced search trees and are different in that they have higher fanout (have more child nodes) and smaller height.</p>&#13;
&#13;
<p>In most of the literature, binary tree nodes are drawn as circles. Since each node is responsible just for one key and splits the range into two parts, this level of detail is sufficient and intuitive. At the same time, B-Tree nodes are often drawn as rectangles, and pointer blocks are also shown explicitly to highlight the relationship between child nodes and separator keys. <a data-type="xref" href="#btree_node_vs_2_3_tree_node_vs_binary_tree_node">Figure 2-7</a> shows binary tree, 2-3-Tree, and B-Tree nodes side by side, which helps to understand the similarities and differences between them.</p>&#13;
&#13;
<figure><div class="figure" id="btree_node_vs_2_3_tree_node_vs_binary_tree_node">&#13;
<img alt="dbin 0207" src="assets/dbin_0207.png"/>&#13;
<h6><span class="label">Figure 2-7. </span>Binary tree, 2-3-Tree, and B-Tree nodes side by side</h6>&#13;
</div></figure>&#13;
&#13;
<p>Nothing prevents us from depicting binary trees in the same way. Both structures have similar pointer-following semantics, and differences start showing in how the balance is maintained. <a data-type="xref" href="#binary_tree_alternative_representation">Figure 2-8</a> shows that and hints at similarities between BSTs and B-Trees: in both cases, keys split the tree into subtrees, and are used for navigating the tree and finding searched keys. You can compare it to <a data-type="xref" href="#binary_tree">Figure 2-1</a>.</p>&#13;
&#13;
<figure class="width-50"><div class="figure" id="binary_tree_alternative_representation">&#13;
<img alt="dbin 0208" src="assets/dbin_0208.png"/>&#13;
<h6><span class="label">Figure 2-8. </span>Alternative representation of a binary tree</h6>&#13;
</div></figure>&#13;
&#13;
<p>B-Trees are <em>sorted</em>: keys inside the B-Tree nodes are stored in order. Because of that, to locate a searched key, we can use an algorithm like binary search. This also implies that lookups in B-Trees have logarithmic complexity. For example, finding a searched key among 4 billion (<code>4 × 10<sup>9</sup></code>) items takes about 32 comparisons (see <a data-type="xref" href="#b_tree_complexity">“B-Tree Lookup Complexity”</a> for more on this subject). If we had to make a disk seek for each one of these comparisons, it would significantly slow us down, but since B-Tree nodes store dozens or even hundreds of items, we only have to make one disk seek per level jump. We’ll discuss a lookup algorithm in more detail later in this chapter.</p>&#13;
&#13;
<p>Using B-Trees, we can efficiently execute<a data-primary="B-Trees, basics of" data-secondary="point and range queries using" data-type="indexterm" id="idm46466889368536"/><a data-primary="point queries" data-type="indexterm" id="idm46466889365432"/><a data-primary="range queries" data-type="indexterm" id="idm46466889364328"/> both <em>point</em> and <em>range</em> queries. Point queries, expressed by the equality (<code>=</code>) predicate in most query languages, locate a single item. On the other hand, range queries, expressed by comparison (<code>&lt;</code>, <code>&gt;</code>, <code>≤</code>, and <code>≥</code>) predicates, are used to query multiple data items in order.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="B-Tree Hierarchy" data-type="sect2"><div class="sect2" id="idm46466889367880">&#13;
<h2>B-Tree Hierarchy</h2>&#13;
&#13;
<p>B-Trees<a data-primary="B-Trees, basics of" data-secondary="B-Tree hierarchy" data-type="indexterm" id="idm46466889359144"/> consist of multiple nodes. Each node holds up to <code>N</code> keys and <code>N + 1</code> pointers to the child nodes. These nodes are logically grouped into three groups:</p>&#13;
<dl>&#13;
<dt>Root node</dt>&#13;
<dd>&#13;
<p>This<a data-primary="root nodes" data-type="indexterm" id="idm46466889355176"/> has no parents and is the top of the tree.</p>&#13;
</dd>&#13;
<dt>Leaf nodes</dt>&#13;
<dd>&#13;
<p>These<a data-primary="leaf nodes" data-type="indexterm" id="idm46466889352984"/> are the bottom layer nodes that have no child nodes.</p>&#13;
</dd>&#13;
<dt>Internal nodes</dt>&#13;
<dd>&#13;
<p>These<a data-primary="internal nodes" data-type="indexterm" id="idm46466889350728"/> are all other nodes, connecting root with leaves. There is usually more than one level of internal nodes.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>This hierarchy is shown in <a data-type="xref" href="#b_tree_hierarchy">Figure 2-9</a>.</p>&#13;
&#13;
<figure><div class="figure" id="b_tree_hierarchy">&#13;
<img alt="dbin 0209" src="assets/dbin_0209.png"/>&#13;
<h6><span class="label">Figure 2-9. </span>B-Tree node hierarchy</h6>&#13;
</div></figure>&#13;
&#13;
<p>Since B-Trees are<a data-primary="pages" data-type="indexterm" id="idm46466889346024"/> a <em>page</em> organization technique (i.e., they are used to organize and navigate fixed-size pages), we often use terms <em>node</em> and <em>page</em> interchangeably.</p>&#13;
&#13;
<p>The<a data-primary="occupancy" data-type="indexterm" id="idm46466889343384"/> relation between the node capacity and the number of keys it actually holds is called <em>occupancy</em>.</p>&#13;
&#13;
<p>B-Trees<a data-primary="fanout" data-type="indexterm" id="idm46466889341912"/> are characterized by their <em>fanout</em>: the number of keys stored in each node. Higher fanout helps to amortize the cost of structural changes required to keep the tree balanced and to reduce the number of seeks by storing keys and pointers to child nodes in a single block or multiple consecutive blocks. Balancing operations<a data-primary="splits" data-type="indexterm" id="idm46466889340120"/><a data-primary="merges" data-type="indexterm" id="idm46466889339480"/><a data-primary="balancing operations" data-type="indexterm" id="idm46466889338776"/> (namely, <em>splits</em> and <em>merges</em>) are triggered when the nodes are full or nearly empty.</p>&#13;
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46466889338072">&#13;
<h5>B<sup>+</sup>-Trees</h5>&#13;
<p>We’re<a data-primary="B+-Trees" data-primary-sortas="Ba-trees" data-type="indexterm" id="idm46466889334440"/> using the term <em>B-Tree</em> as an umbrella for a family of data structures that share all or most of the mentioned properties. A more precise name for the described data structure is B<sup>+</sup>-Tree. <a data-type="xref" href="app01.html#KNUTH98">[KNUTH98]</a> refers to trees with<a data-primary="multiway trees" data-type="indexterm" id="idm46466889331528"/> a high fanout as <em>multiway</em> trees.</p>&#13;
&#13;
<p>B-Trees allow storing values on any level: in root, internal, and leaf nodes. B<sup>+</sup>-Trees store values <em>only</em> in leaf nodes. Internal nodes store only <em>separator keys</em> used to guide the search algorithm to the associated value stored on the leaf level.</p>&#13;
&#13;
<p>Since values in B<sup>+</sup>-Trees are stored only on the leaf level, all operations (inserting, updating, removing, and retrieving data records) affect only leaf nodes and propagate to higher levels only during splits and merges.</p>&#13;
&#13;
<p>B<sup>+</sup>-Trees became widespread, and we refer to them as B-Trees, similar to other literature the subject. For example, in <a data-type="xref" href="app01.html#GRAEFE11">[GRAEFE11]</a> B<sup>+</sup>-Trees are referred to as a default design, and MySQL InnoDB refers to its B<sup>+</sup>-Tree implementation as B-tree.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Separator Keys" data-type="sect2"><div class="sect2" id="separator_keys">&#13;
<h2>Separator Keys</h2>&#13;
&#13;
<p>Keys stored<a data-primary="B-Trees, basics of" data-secondary="separator keys in" data-type="indexterm" id="idm46466889322440"/><a data-primary="separator keys" data-type="indexterm" id="idm46466889321160"/><a data-primary="index entries" data-type="indexterm" id="idm46466889320552"/><a data-primary="divider cells" data-seealso="separator keys" data-type="indexterm" id="idm46466889319944"/> in B-Tree nodes are called <em>index entries</em>, <em>separator keys</em>, or <em>divider cells</em>. They<a data-primary="subtrees" data-type="indexterm" id="idm46466889317112"/><a data-primary="branches" data-type="indexterm" id="idm46466889316616"/> split the tree into <em>subtrees</em> (also called <em>branches</em> or <em>subranges</em>), holding corresponding key ranges. Keys are stored in sorted order to allow binary search. A subtree is found by locating a key and following a corresponding pointer from the higher to the lower level.</p>&#13;
&#13;
<p>The first pointer in the node points to the subtree holding items <em>less than</em> the first key, and the last pointer in the node points to the subtree holding items <em>greater than or equal</em> to the last key. Other pointers are reference subtrees <em>between</em> the two keys: <code>K<sub>i-1</sub> ≤ K<sub>s</sub> &lt; K<sub>i</sub></code>, where <code>K</code> is a set of keys, and <code>K<sub>s</sub></code> is a key that belongs to the subtree. <a data-type="xref" href="#tree_subrange">Figure 2-10</a> shows these invariants.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="tree_subrange">&#13;
<img alt="dbin 0210" src="assets/dbin_0210.png"/>&#13;
<h6><span class="label">Figure 2-10. </span>How separator keys split a tree into subtrees</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Some B-Tree variants also have sibling node pointers, most often on the leaf level, to simplify range scans. These pointers help avoid going back to the parent to find the next sibling. Some implementations have pointers in both directions, forming a double-linked list on the leaf level, which makes the reverse iteration possible.</p>&#13;
&#13;
<p>What sets B-Trees apart is that, rather than being built from top to bottom (as binary search trees), they’re constructed the other way around—from bottom to top. The number of leaf nodes grows, which increases the number of internal nodes and tree height.</p>&#13;
&#13;
<p>Since B-Trees reserve extra space inside nodes for future insertions and updates, tree storage utilization can get as low as 50%, but is usually considerably higher. Higher occupancy does not influence B-Tree performance negatively.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="B-Tree Lookup Complexity" data-type="sect2"><div class="sect2" id="b_tree_complexity">&#13;
<h2>B-Tree Lookup Complexity</h2>&#13;
&#13;
<p>B-Tree<a data-primary="B-Trees, basics of" data-secondary="B-Tree lookup complexity" data-type="indexterm" id="idm46466889303272"/> lookup complexity can be viewed from two standpoints: the number of block transfers and the number of comparisons done during the lookup.</p>&#13;
&#13;
<p>In terms of number of transfers, the logarithm base is <code>N</code> (number of keys per node). There are <code>N</code> times more nodes on each new level, and following a child pointer reduces the search space by the factor of <code>N</code>. During lookup, at most <code>log<sub>N</sub> M</code> (where <code>M</code> is a total number of items in the B-Tree) pages are addressed to find a searched key. The number of child pointers that have to be followed on the root-to-leaf pass is also equal to the number of levels, in other words, the height <code>h</code> of the tree.</p>&#13;
&#13;
<p>From the perspective of number of comparisons, the logarithm base is <code>2</code>, since searching a key inside each node is done using binary search. Every comparison halves the search space, so complexity is <code>log<sub>2</sub> M</code>.</p>&#13;
&#13;
<p>Knowing the distinction between the number of seeks and the number of comparisons helps us gain the intuition about how searches are performed and understand what lookup complexity is, from both perspectives.</p>&#13;
&#13;
<p>In textbooks and articles,<sup><a data-type="noteref" href="ch02.html#idm46466889296840" id="idm46466889296840-marker">2</a></sup> B-Tree lookup complexity is generally referenced as <code>log M</code>. Logarithm base is generally not used in complexity analysis, since changing the base simply adds a <a href="https://databass.dev/links/65">constant factor</a>, and multiplication by a constant factor does not change complexity. For example, given the nonzero constant factor <code>c</code>, <code>O(|c| × n) == O(n)</code> <a data-type="xref" href="app01.html#KNUTH97">[KNUTH97]</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="B-Tree Lookup Algorithm" data-type="sect2"><div class="sect2" id="b_tree_lookup_algorithm">&#13;
<h2>B-Tree Lookup Algorithm</h2>&#13;
&#13;
<p>Now<a data-primary="B-Trees, basics of" data-secondary="B-Tree lookup algorithm" data-type="indexterm" id="idm46466889287176"/> that we have covered the structure and internal organization of B-Trees, we can define algorithms for lookups, insertions, and removals. To find an item in a B-Tree, we have to perform a single traversal from root to leaf. The objective of this search is to find a searched key or its predecessor. Finding an exact match is used for point queries, updates, and deletions; finding its predecessor is useful for range scans and inserts.</p>&#13;
&#13;
<p>The algorithm starts from the root and performs a binary search, comparing the searched key with the keys stored in the root node until it finds the first separator key that is greater than the searched value. This locates a searched subtree. As we’ve <span class="keep-together">discussed</span> previously, index keys split the tree into subtrees with boundaries <em>between</em> two neighboring keys. As soon as we find the subtree, we follow the pointer that corresponds to it and continue the same search process (locate the separator key, follow the pointer) until we reach a target leaf node, where we either find the searched key or conclude it is not present by locating its predecessor.</p>&#13;
&#13;
<p>On each level, we get a more detailed view of the tree: we start on the most coarse-grained level (the root of the tree) and descend to the next level where keys represent more precise, detailed ranges, until we finally reach leaves, where the data records are located.</p>&#13;
&#13;
<p>During the point query, the search is done after finding or failing to find the searched key. During the range scan, iteration starts from the closest found key-value pair and continues by following sibling pointers until the end of the range is reached or the range predicate is exhausted.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Counting Keys" data-type="sect2"><div class="sect2" id="counting_keys">&#13;
<h2>Counting Keys</h2>&#13;
&#13;
<p>Across<a data-primary="B-Trees, basics of" data-secondary="counting keys" data-type="indexterm" id="idm46466889279864"/><a data-primary="child offsets" data-type="indexterm" id="idm46466889285144"/> the literature, you can find different ways to describe key and child offset counts. <a data-type="xref" href="app01.html#BAYER72">[BAYER72]</a> mentions the device-dependent natural number <code>k</code> that represents an optimal page size. Pages, in this case, can hold between <code>k</code> and <code>2k</code> keys, but can be partially filled and hold at least <code>k + 1</code> and at most <code>2k + 1</code> pointers to child nodes. The root page can hold between <code>1</code> and <code>2k</code> keys. Later, a number <code>l</code> is introduced, and it is said that any nonleaf page can have <code>l + 1</code> keys.</p>&#13;
&#13;
<p>Other<a data-primary="separator keys" data-type="indexterm" id="idm46466889273112"/><a data-primary="pointers" data-type="indexterm" id="idm46466889272392"/> sources, for example <a data-type="xref" href="app01.html#GRAEFE11">[GRAEFE11]</a>, describe nodes that can hold up to <code>N</code> <em>separator keys</em> and <code>N + 1</code> <em>pointers</em>, with otherwise similar semantics and invariants.</p>&#13;
&#13;
<p>Both approaches bring us to the same result, and differences are only used to emphasize the contents of each source. In this book, we stick to <code>N</code> as the number of keys (or key-value pairs, in the case of the leaf nodes) for clarity.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="B-Tree Node Splits" data-type="sect2"><div class="sect2" id="b_tree_splits">&#13;
<h2>B-Tree Node Splits</h2>&#13;
&#13;
<p>To<a data-primary="node splits" data-type="indexterm" id="idm46466889265288"/><a data-primary="B-Trees, basics of" data-secondary="B-Tree node splits" data-type="indexterm" id="idm46466889264680"/> insert the value into a B-Tree, we first have to locate the target leaf and find the insertion point. For that, we use the algorithm described in the previous section. After the leaf is located, the key and value are appended to it. Updates in B-Trees work by locating a target leaf node using a lookup algorithm and associating a new value with an existing key.</p>&#13;
&#13;
<p>If<a data-primary="overflow" data-type="indexterm" id="idm46466889263224"/><a data-primary="splits" data-type="indexterm" id="idm46466889262616"/> the target node doesn’t have enough room available, we say that the node has <em>overflowed</em> <a data-type="xref" href="app01.html#NICHOLS66">[NICHOLS66]</a> and has to be split in two to fit the new data. More precisely, the node is split if the following conditions hold:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>For leaf nodes: if the node can hold up to <code>N</code> key-value pairs, and inserting one more key-value pair brings it <em>over</em> its maximum capacity <code>N</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>For nonleaf nodes: if the node can hold up to <code>N + 1</code> pointers, and inserting one more pointer brings it <em>over</em> its maximum capacity <code>N + 1</code>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Splits are done by allocating the new node, transferring half the elements from the splitting node to it, and adding its first key and pointer to the parent node. In this case, we say that the<a data-primary="promotion" data-type="indexterm" id="idm46466889253880"/> key is <em>promoted</em>. The index at which the split is performed is called<a data-primary="split point" data-type="indexterm" id="idm46466889252056"/> the <em>split point</em> (also called the midpoint). All elements after the split point (including split point in the case of leaf node split) are transferred to the newly created sibling node, and the rest of the elements remain in the splitting node.</p>&#13;
&#13;
<p>If the parent node is full and does not have space available for the promoted key and pointer to the newly created node, it has to be split as well. This operation might propagate recursively all the way to the root.</p>&#13;
&#13;
<p>As soon as the tree reaches its capacity (i.e., split propagates all the way up to the root), we have to split the root node. When the root node is split, a new root, holding a split point key, is allocated. The old root (now holding only half the entries) is demoted to the next level along with its newly created sibling, increasing the tree height by one. The tree height changes when the root node is split and the new root is allocated, or when two nodes are merged to form a new root. On the leaf and internal node levels, the tree only grows <em>horizontally</em>.</p>&#13;
&#13;
<p><a data-type="xref" href="#b_tree_split_2">Figure 2-11</a> shows a fully occupied <em>leaf</em> node during insertion of the new element <code>11</code>. We draw the line in the middle of the full node, leave half the elements in the node, and move the rest of elements to the new one. A split point value is placed into the parent node to serve as a separator key.</p>&#13;
&#13;
<figure><div class="figure" id="b_tree_split_2">&#13;
<img alt="dbin 0211" src="assets/dbin_0211.png"/>&#13;
<h6><span class="label">Figure 2-11. </span>Leaf node split during the insertion of <code>11</code>. New element and promoted key are shown in gray.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#b_tree_split_1">Figure 2-12</a> shows the split process of a fully occupied <em>nonleaf</em> (i.e., root or internal) node during insertion of the new element <code>11</code>. To perform a split, we first create a new node and move elements starting from index <code>N/2 + 1</code> to it. The split point key is promoted to the parent.</p>&#13;
&#13;
<figure><div class="figure" id="b_tree_split_1">&#13;
<img alt="dbin 0212" src="assets/dbin_0212.png"/>&#13;
<h6><span class="label">Figure 2-12. </span>Nonleaf node split during the insertion of <code>11</code>. New element and promoted key are shown in gray.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Since nonleaf node splits are always a manifestation of splits propagating from the levels below, we have an additional pointer (to the newly created node on the next level). If the parent does not have enough space, it has to be split as well.</p>&#13;
&#13;
<p>It doesn’t matter whether the leaf or nonleaf node is split (i.e., whether the node holds keys and values or just the keys). In the case of leaf split, keys are moved together with their associated values.</p>&#13;
&#13;
<p>When the split is done, we have two nodes and have to pick the correct one to finish insertion. For that, we can use the separator key invariants. If the inserted key is less than the promoted one, we finish the operation by inserting to the split node. Otherwise, we insert to the newly created one.</p>&#13;
&#13;
<p>To summarize, node splits are done in four steps:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Allocate a new node.</p>&#13;
</li>&#13;
<li>&#13;
<p>Copy half the elements from the splitting node to the new one.</p>&#13;
</li>&#13;
<li>&#13;
<p>Place the new element into the corresponding node.</p>&#13;
</li>&#13;
<li>&#13;
<p>At the parent of the split node, add a separator key and a pointer to the new node.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="B-Tree Node Merges" data-type="sect2"><div class="sect2" id="b_tree_merges">&#13;
<h2>B-Tree Node Merges</h2>&#13;
&#13;
<p>Deletions<a data-primary="B-Trees, basics of" data-secondary="B-Tree node merges" data-type="indexterm" id="idm46466889230216"/><a data-primary="node merges" data-type="indexterm" id="idm46466889229368"/><a data-primary="merges" data-type="indexterm" id="idm46466889228760"/> are done by first locating the target leaf. When the leaf is located, the key and the value associated with it are removed.</p>&#13;
&#13;
<p>If neighboring nodes have too few values (i.e., their occupancy falls under a threshold), the sibling nodes are merged. This<a data-primary="underflow" data-type="indexterm" id="idm46466889225752"/> situation is called <em>underflow</em>. <a data-type="xref" href="app01.html#BAYER72">[BAYER72]</a> describes two underflow scenarios: if two adjacent nodes have a common parent and their contents fit into a single node, their contents should be merged (concatenated); if their contents do not fit into a single node, keys are redistributed between them to restore balance (see <a data-type="xref" href="ch04.html#btree_rebalancing">“Rebalancing”</a>). More precisely, two nodes are merged if the following conditions hold:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>For leaf nodes: if a node can hold up to <code>N</code> key-value pairs, and a combined number of key-value pairs in two neighboring nodes is less than or equal to <code>N</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>For nonleaf nodes: if a node can hold up to <code>N + 1</code> pointers, and a combined number of pointers in two neighboring nodes is less than or equal to <code>N + 1</code>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-type="xref" href="#b_tree_merge_3">Figure 2-13</a> shows the merge during deletion of element <code>16</code>. To do this, we move elements from one of the siblings to the other one. Generally, elements from the <em>right</em> sibling are moved to the <em>left</em> one, but it can be done the other way around as long as the key order is preserved.</p>&#13;
&#13;
<figure><div class="figure" id="b_tree_merge_3">&#13;
<img alt="dbin 0213" src="assets/dbin_0213.png"/>&#13;
<h6><span class="label">Figure 2-13. </span>Leaf node merge</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#b_tree_merge_1">Figure 2-14</a> shows two sibling nonleaf nodes that have to be merged during deletion of element <code>10</code>. If we combine their elements, they fit into one node, so we can have one node instead of two. During the merge of nonleaf nodes, we have to pull the corresponding separator key from the parent (i.e., demote it). The number of pointers is reduced by one because the merge is a result of the propagation of the pointer deletion from the lower level, caused by the page removal. Just as with splits, merges can propagate all the way to the root level.</p>&#13;
&#13;
<figure><div class="figure" id="b_tree_merge_1">&#13;
<img alt="dbin 0214" src="assets/dbin_0214.png"/>&#13;
<h6><span class="label">Figure 2-14. </span>Nonleaf node merge</h6>&#13;
</div></figure>&#13;
&#13;
<p>To summarize, node merges are done in three steps, assuming the element is already removed:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Copy all elements from the <em>right</em> node to the <em>left</em> one.</p>&#13;
</li>&#13;
<li>&#13;
<p>Remove the <em>right</em> node pointer from the parent (or <em>demote</em> it in the case of a nonleaf merge).</p>&#13;
</li>&#13;
<li>&#13;
<p>Remove the right node.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>One of the techniques often implemented in B-Trees to reduce the number of splits and merges is rebalancing, which we discuss in <a data-type="xref" href="ch04.html#btree_rebalancing">“Rebalancing”</a>.<a data-primary="" data-startref="BTBubiq02" data-type="indexterm" id="idm46466889204216"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="chapter_1_summary">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we started with a motivation to create specialized structures for on-disk storage. Binary search trees might have similar complexity characteristics, but still fall short of being suitable for disk because of low fanout and a large number of relocations and pointer updates caused by balancing. B-Trees solve both problems by increasing the number of items stored in each node (high fanout) and less frequent balancing operations.</p>&#13;
&#13;
<p>After that, we discussed internal B-Tree structure and outlines of algorithms for lookup, insert, and delete operations. Split and merge operations help to restructure the tree to keep it balanced while adding and removing elements. We keep the tree depth to a minimum and add items to the existing nodes while there’s still some free space in them.</p>&#13;
&#13;
<p>We can use this knowledge to create in-memory B-Trees. To create a disk-based implementation, we need to go into details of how to lay out B-Tree nodes on disk and compose on-disk layout using data-encoding formats.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46466889200936">&#13;
<h5>Further Reading</h5>&#13;
<p>If you’d like to learn more about the concepts mentioned in this chapter, you can refer to the following sources:</p>&#13;
<dl>&#13;
<dt>Binary search trees</dt>&#13;
<dd>&#13;
<p>Sedgewick, Robert and Kevin Wayne. 2011. <em>Algorithms (4th Ed.)</em>. Boston: <span class="keep-together">Pearson.</span></p>&#13;
&#13;
<p>Knuth, Donald E. 1997. <em>The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms</em>. Boston: Addison-Wesley Longman.</p>&#13;
</dd>&#13;
<dt>Algorithms for splits and merges in B-Trees</dt>&#13;
<dd>&#13;
<p>Elmasri, Ramez and Shamkant Navathe. 2011. <em>Fundamentals of Database Systems (6th Ed.)</em>. Boston: Pearson.</p>&#13;
&#13;
<p>Silberschatz, Abraham, Henry F. Korth, and S. Sudarshan. 2010. <em>Database Systems Concepts (6th Ed.)</em>. New York: McGraw-Hill.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm46466889485960"><sup><a href="ch02.html#idm46466889485960-marker">1</a></sup> This property is imposed by AVL Trees and several other data structures. More generally, binary search trees keep the difference in heights between subtrees within a small constant factor.</p><p data-type="footnote" id="idm46466889296840"><sup><a href="ch02.html#idm46466889296840-marker">2</a></sup> For example, <a data-type="xref" href="app01.html#KNUTH98">[KNUTH98]</a>.</p></div></div></section></body></html>