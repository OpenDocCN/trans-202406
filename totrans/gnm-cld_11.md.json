["```\n$ cat ~/book/code/config/google.conf\n\n# This is an example configuration file that directs Cromwell to execute\n# workflow tasks via the Google Pipelines API backend and allows it to retrieve \n# input files from GCS buckets. It is intended only as a relatively simple example \n# and leaves out many options that are useful or important for production-scale\n# work. See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more\n# complete documentation. \n\nengine {\n  filesystems {\n    gcs {\n      auth = \"application-default\"\n      project = \"<google-billing-project-id>\"\n    }\n  }\n}\n\nbackend {\n  default = PAPIv2\n\n  providers {\n    PAPIv2 {\n      actor-factory = \"cromwell.backend.google.pipelines.v2alpha1.PipelinesApi\n      LifecycleActorFactory\"\n      config {\n        # Google project\n        project = \"<google-project-id>\"\n\n        # Base bucket for workflow executions\n        root = \"gs://<google-bucket-name>/cromwell-execution\"\n\n        # Polling for completion backs-off gradually for slower-running jobs.\n        # This is the maximum polling interval (in seconds):\n        maximum-polling-interval = 600\n\n        # Optional Dockerhub Credentials. Can be used to access private docker images.\n        dockerhub {\n          # account = \"\"\n          # token = \"\"\n        }\n\n        # Number of workers to assign to PAPI requests\n        request-workers = 3\n\n        genomics {\n          # A reference to an auth defined in the `google` stanza at the top. This auth is used\n          # to create\n          # Pipelines and manipulate auth JSONs.\n          auth = \"application-default\"\n\n          # Endpoint for APIs, no reason to change this unless directed by Google.\n          endpoint-url = \"https://genomics.googleapis.com/\"\n\n          # Pipelines v2 only: specify the number of times localization and delocalization \n          # operations should be attempted\n          # There is no logic to determine if the error was transient or not, everything \n          # is retried upon failure\n          # Defaults to 3\n          localization-attempts = 3\n\n        }\n\n        filesystems {\n          gcs {\n            auth = \"application-default\"\n            project = \"<google-billing-project-id>\"\n            }\n          }\n        }\n\n        default-runtime-attributes {\n          cpu: 1\n          failOnStderr: false\n          continueOnReturnCode: 0\n          memory: \"2048 MB\"\n          bootDiskSizeGb: 10\n          # Allowed to be a String, or a list of Strings\n          disks: \"local-disk 10 SSD\"\n          noAddress: false\n          preemptible: 0\n          zones: [\"us-east4-a\", \"us-east4-b\"]\n        }\n      }\n    }\n  }\n}\n```", "```\n$ mkdir ~/sandbox-10\n$ cp ~/book/code/config/google.conf ~/sandbox-10/my-google.conf\n```", "```\n$ export CONF=~/sandbox-10\n$ export BIN=~/book/bin\n$ export WF=~/book/code/workflows\n```", "```\n$ export BUCKET=\"gs://my-bucket\"\n```", "```\n$ nano ~/sandbox-10/my-google.conf\n```", "```\nbackend {\n  default = PAPIv2\n\n  providers {\n    PAPIv2 {\n```", "```\nconfig {\n        // Google project\n        project = \"<google-project-id>\"\n\n        // Base bucket for workflow executions\n        root = \"gs://<google-bucket-name>/cromwell-execution\"\n```", "```\nconfig {\n        // Google project\n        project = \"ferrous-layout-260200\"\n\n        // Base bucket for workflow executions\n        root = \"gs://my-bucket/cromwell-execution\"\n```", "```\ngcs {\n      auth = \"application-default\"\n      project = \"<google-billing-project-id>\"\n    }\n```", "```\ngcs {\n      auth = \"application-default\"\n      project = \"ferrous-layout-260200\"\n    }\n```", "```\n$ gcloud auth application-default login\n```", "```\n$ cat $WF/scatter-hc/scatter-haplotypecaller.gcs.inputs.json\n```", "```\n$ java -Dconfig.file=$CONF/my-google.conf -jar $BIN/cromwell-48.jar \\\n    run $WF/scatter-hc/scatter-haplotypecaller.wdl \\\n    -i $WF/scatter-hc/scatter-haplotypecaller.gcs.inputs.json\n```", "```\n[2019-12-14 18:37:49,48] [info] PAPI request worker batch interval is \n33333 milliseconds\n```", "```\n[2019-12-14 18:37:52,56] [info] MaterializeWorkflowDescriptorActor [68271de1]:\nCall-to-Backend assignments: ScatterHaplotypeCallerGVCF.HaplotypeCallerG\nVCF -> PAPIv2, ScatterHaplotypeCallerGVCF.MergeVCFs -> PAPIv2\n```", "```\n// Polling for completion backs-off gradually for slower-running jobs.\n// This is the maximum polling interval (in seconds): maximum-polling-interval = 600\n\n```", "```\n[INFO] … SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.\n \"outputs\": {\n \"ScatterHaplotypeCallerGVCF.output_gvcf\": \"gs://genomics-book-test-99/cromwell-\nexecution/ScatterHaplotypeCallerGVCF/68271de1-4220-4818-bfaa-5694551cbe81/call-\nMergeVCFs/mother.merged.g.vcf\"\n },\n```", "```\n command {\n    set -e\n    set -o pipefail\n\n    ${samtools_path} view -h -T ${ref_fasta} ${input_cram} |\n    ${samtools_path} view -b -o ${sample_name}.bam -\n    ${samtools_path} index -b ${sample_name}.bam\n    mv ${sample_name}.bam.bai ${sample_name}.bai\n  }\n```", "```\nFloat ref_size = size(ref_fasta, \"GB\") +\nsize(ref_fasta_index, \"GB\") + size(ref_dict, \"GB\")\nInt disk_size = ceil(((size(input_bam, \"GB\") + 30) / hc_scatter) + ref_size) + 20\n```", "```\nparameter_meta {\n    input_bam: {\n      localization_optional: true\n    }\n  }\n```", "```\n runtime {\n    docker: gatk_docker\n    preemptible: preemptible_tries\n    memory: \"6.5 GiB\"\n    cpu: \"2\"\n    disks: \"local-disk \" + disk_size + \" HDD\"\n  }\n```", "```\n$ export WR_CONF=~/book/code/config\n$ export WR_PIPE=~/book/wdl_runner/wdl_runner\n```", "```\n$ gcloud alpha genomics pipelines run \\\n      --pipeline-file $WR_PIPE/wdl_pipeline.yaml \\\n      --regions us-east4 \\\n      --inputs-from-file WDL=$WF/scatter-hc/scatter-haplotypecaller.wdl,\\\n WORKFLOW_INPUTS=$WF/scatter-hc/scatter-haplotypecaller.gcs.inputs.json,\\\n WORKFLOW_OPTIONS=$WR_CONF/empty.options.json \\\n      --env-vars WORKSPACE=$BUCKET/wdl_runner/test/work,\\\n OUTPUTS=$BUCKET/wdl_runner/test/output \\\n      --logging $BUCKET/wdl_runner/test/logging\n```", "```\nERROR: (gcloud.alpha.genomics.pipelines.run) INVALID_ARGUMENT: Error: validating\npipeline: zones and regions cannot be specified together\n```", "```\n$ gcloud config set compute/zone \"\"\n```", "```\nRunning [projects/ferrous-layout-260200/operations/8124090577921753814].\n```", "```\n$ cd ~/book/wdl_runner\n$ bash monitoring_tools/monitor_wdl_pipeline.sh 7973899330424684165\nLogging:\nWorkspace:\nOutputs:\n2019-12-15 09:21:19: operation not complete\nNo operations logs found.\nThere are 1 output files\nSleeping 60 seconds\n```", "```\n2019-12-15 09:42:47: operation complete\n```", "```\nCompleted operation status information\n done: true\n metadata:\n   events:\n   - description: Worker released\n```", "```\nCompleted operation status information\n done: true\n error:\n   code: 9\n   message: ’Execution failed: while running \"WDL_Runner\": unexpected exit status 1\nwas not ignored’\n```"]