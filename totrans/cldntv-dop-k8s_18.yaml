- en: Chapter 16\. Metrics in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible to know so much about a subject that you become totally ignorant.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Frank Herbert, *Chapterhouse: Dune*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll take the concept of metrics that we introduced in [Chapter 15](ch15.html#observability)
    and dive into the details for Kubernetes: what kind of metrics are there, which
    ones are important for cloud native services, how do you choose which metrics
    to focus on, how do you analyze metrics data to get actionable information, and
    how do you turn raw metrics data into useful dashboards and alerts? Finally, we’ll
    outline some of the options for metrics tools and platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: What Are Metrics, Really?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [“Introducing Metrics”](ch15.html#metrics-intro), metrics are numerical
    measures of specific things. A familiar example from the world of traditional
    servers is the memory usage of a particular machine. If only 10% of physical memory
    is currently allocated to user processes, the machine has spare capacity. But
    if 90% of the memory is in use, the machine is probably pretty busy.
  prefs: []
  type: TYPE_NORMAL
- en: So one valuable kind of information that metrics can give us is a snapshot of
    what’s going on at a particular instant. But we can do more. Memory usage goes
    up and down all the time as workloads start and stop, but sometimes what we’re
    interested in is the *change* in memory usage over time.
  prefs: []
  type: TYPE_NORMAL
- en: Time-Series Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you sample memory usage regularly, you can create a *time series* of that
    data. [Figure 16-1](#img-timeseries) shows a graph of the time-series data for
    memory usage on a Google Kubernetes Engine node, over one week. This gives a much
    more intelligible picture of what’s happening than a handful of instantaneous
    values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph showing fluctuating memory usage](assets/cnd2_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-1\. Time-series graph of memory usage for a GKE node
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Most metrics that we’re interested in for cloud native observability purposes
    are expressed as time series. They are also all numeric. Unlike log data, for
    example, metrics are values that you can do math and statistics on.
  prefs: []
  type: TYPE_NORMAL
- en: Counters and Gauges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What kind of numbers are they? While some quantities can be represented by integers
    (the number of physical CPUs in a machine, for example), most require a decimal
    part, and to save having to handle two different types of numbers, metrics are
    almost always represented as floating-point decimal values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that, there are two main types of metric values: *counters* and *gauges*.
    Counters can only go up (or reset to zero); they’re suitable for measuring things
    like number of requests served and number of errors received. Gauges, on the other
    hand, can vary up and down; they’re useful for continuously varying quantities
    like memory usage, or for expressing ratios of other quantities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answers to some questions are just yes or no: whether a particular endpoint
    is responding to HTTP connections, for example. In this case, the appropriate
    metric will be a gauge with a limited range of values: 0 and 1, perhaps.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, an HTTP check of an endpoint might be named something like `http.can_connect`,
    and its value might be 1 when the endpoint is responding, and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: What Can Metrics Tell Us?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What use are metrics? Well, as we’ve seen earlier in this chapter, metrics can
    tell you when things are broken. For example, if your error rate suddenly goes
    up (or requests to your support page suddenly spike), that may indicate a problem.
    You can generate alerts automatically for certain metrics based on a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: But metrics can also tell you how well things are working, for example, how
    many simultaneous users your application is currently supporting. Long-term trends
    in these numbers can be useful for operations decision-making and for business
    intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing Good Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first, you might think “If metrics are good, then lots of metrics must be
    even better!” But it doesn’t work that way. You can’t monitor everything. Google
    Cloud’s Operations suite, for example, captures data for literally hundreds of
    built-in metrics about your cloud resources, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`instance/network/sent_packets_count`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of network packets sent by each compute instance
  prefs: []
  type: TYPE_NORMAL
- en: '`storage/object_count`'
  prefs: []
  type: TYPE_NORMAL
- en: The total number of objects in each storage bucket
  prefs: []
  type: TYPE_NORMAL
- en: '`container/cpu/utilization`'
  prefs: []
  type: TYPE_NORMAL
- en: The percentage of its CPU allocation that a container is currently using
  prefs: []
  type: TYPE_NORMAL
- en: The list [goes on](https://oreil.ly/QBgM0) (and on, and on). Even if you could
    display graphs of all these metrics at once, which would need a monitor screen
    the size of a house, you’d never be able to take in all that information and deduce
    anything useful from it. To do that, we need to *focus* on the subset of metrics
    that we care about.
  prefs: []
  type: TYPE_NORMAL
- en: So what should you focus on when observing your own applications? Only you can
    answer that, but we have a few suggestions that may be helpful. In the rest of
    this section, we’ll outline some common metrics patterns for observability, aimed
    at different audiences and designed to meet different requirements.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth saying that this is a perfect opportunity for some DevOps collaboration,
    and you should start thinking and talking about what metrics you’ll need at the
    beginning of development, not at the end (see [“Learning Together”](ch01.html#learningtogether)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Services: The RED Pattern'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most people using Kubernetes are running some kind of web service: users make
    requests, and the application sends responses. The *users* could be programs or
    other services; in a distributed system based on microservices, each service makes
    requests to other services or to a central API gateway server and uses the results
    to serve information back to yet more services. Either way, it’s a request-driven
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: What’s useful to know about a request-driven system?
  prefs: []
  type: TYPE_NORMAL
- en: One obvious thing is the number of *requests* you’re getting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another is the number of requests that failed in various ways; that is to say,
    the number of *errors*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A third useful metric is the *duration* of each request. This gives you an idea
    how well your service is performing and how unhappy your users might be getting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Requests-Errors-Duration* (RED) pattern is a classic observability tool
    that goes back to the earliest days of online services. [Google’s *Site Reliability
    Engineering* book](https://oreil.ly/zD72v) talks about the Four Golden Signals,
    which are essentially requests, errors, duration, and *saturation* (we’ll talk
    about saturation in a moment).
  prefs: []
  type: TYPE_NORMAL
- en: 'Engineer Tom Wilkie, who coined the *RED* acronym, has outlined the rationale
    behind this pattern in a blog post:'
  prefs: []
  type: TYPE_NORMAL
- en: Why should you measure the same metrics for every service? Surely each service
    is special? The benefits of treating each service the same, from a monitoring
    perspective, is scalability in your operations teams. By making every service
    look, feel and taste the same, this reduces the cognitive load on those responding
    to an incident. As an aside, if you treat all your services the same, many repetitive
    tasks become automatable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tom Wilkie
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So how exactly do we measure these numbers? Since the total number of requests
    only ever goes up, it’s more useful to look at request *rate*: the number of requests
    per second, for example. This gives us a meaningful idea of how much traffic the
    system is handling over a given time interval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because error rate is related to request rate, it’s a good idea to measure
    errors as a percentage of requests. So, for example, a typical service dashboard
    might show:'
  prefs: []
  type: TYPE_NORMAL
- en: Requests received per second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of requests that returned an error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duration of requests (also known as *latency*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resources: The USE Pattern'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve seen that the RED pattern gives you useful information about how your
    services are performing, and how your users are experiencing them. You could think
    of this as a top-down way of looking at observability data.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the [USE pattern](https://oreil.ly/PpB0f), developed by Netflix
    performance engineer Brendan Gregg, is a bottom-up approach that is intended to
    help analyze performance issues and find bottlenecks. USE stands for Utilization,
    Saturation, and Errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than services, with USE we’re interested in *resources*: lower-level
    infrastructure server components such as CPU and disks, or network interfaces
    and links. Any of these could be a bottleneck in system performance, and the USE
    metrics will help us find out which:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilization
  prefs: []
  type: TYPE_NORMAL
- en: The average time that the resource was busy serving requests, or the amount
    of resource capacity that’s currently in use. For example, a disk that is 90%
    full would have a utilization of 90%.
  prefs: []
  type: TYPE_NORMAL
- en: Saturation
  prefs: []
  type: TYPE_NORMAL
- en: The extent to which the resource is overloaded, or the length of the queue of
    requests waiting for this resource to become available. For example, if there
    are 10 processes waiting to run on a CPU, it has a saturation value of 10.
  prefs: []
  type: TYPE_NORMAL
- en: Errors
  prefs: []
  type: TYPE_NORMAL
- en: The number of times an operation on that resource failed. For example, a disk
    with some bad sectors might have an error count of 25 failed reads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring this data for the key resources in your system is a good way to spot
    bottlenecks and potential upcoming problems. Resources with low utilization, no
    saturation, and no errors are probably fine. Anything that deviates from this
    is worth looking into. For example, if one of your network links is saturated,
    or has a high number of errors, it may be contributing to overall performance
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The USE Method is a simple strategy you can use to perform a complete check
    of system health, identifying common bottlenecks and errors. It can be deployed
    early in the investigation and quickly identify problem areas, which then can
    be studied in more detail other methodologies, if need be.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The strength of USE is its speed and visibility: by considering all resources,
    you are unlikely to overlook any issues. It will, however, only find certain types
    of issues–bottlenecks and errors–and should be considered as one tool in a larger
    toolbox.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Brendan Gregg
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Business Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve looked at application and service metrics ([“Services: The RED Pattern”](#redpattern)),
    which are likely to be of most interest to developers, and infrastructure metrics
    ([“Resources: The USE Pattern”](#usepattern)), which are helpful to ops and platform
    engineers. But what about the business? Can observability help managers and executives
    understand how the business is performing and give them useful input for business
    decisions? And what metrics would contribute to this?'
  prefs: []
  type: TYPE_NORMAL
- en: Most businesses already track the key performance indicators (KPIs) that matter
    to them, such as sales revenue, profit margin, and cost of customer acquisition.
    These metrics usually come from the finance department and don’t need support
    from developers and infrastructure staff.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are other useful business metrics that can be generated by applications
    and services. For example, a subscription business, such as a software-as-a-service
    (SaaS) product, needs to know data about its subscribers:'
  prefs: []
  type: TYPE_NORMAL
- en: Funnel analytics (how many people hit the landing page, how many click through
    to the sign-up page, how many complete the transaction, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate of sign-ups and cancellations (churn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revenue per customer (useful for calculating monthly recurring revenue, average
    revenue per customer, and lifetime value of a customer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effectiveness of help and support pages (for example, percentage of people who
    answered yes to the question “Did this page solve your problem?”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic to the *system status* announcement page (which often spikes when there
    are outages or degraded services)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much of this information is often easier to gather by generating real-time metrics
    data from your applications, rather than by trying to analyze after-the-fact by
    processing logs and querying databases. When you’re instrumenting your applications
    to produce metrics, don’t neglect information that is important to the business.
  prefs: []
  type: TYPE_NORMAL
- en: There isn’t necessarily a clear line between the observability information the
    business and customer engagement experts need, and what the technical experts
    need. In fact, there’s a lot of overlap. It’s wise to discuss metrics at an early
    stage with all the stakeholders involved, and agree on what data needs to be collected,
    how often, how it’s aggregated, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, these two (or more) groups have different questions to ask of the
    observability data that you’re gathering, so each will need its own view on that
    data. You can use the common *data lake* to create dashboards (see [“Graphing
    Metrics with Dashboards”](#dashboards)) and reports for each of the different
    groups involved.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve talked about observability and metrics in general terms, and looked at
    different types of data and ways to analyze it. So how does all this apply to
    Kubernetes? What metrics is it worth tracking for Kubernetes clusters, and what
    kinds of decisions can they help us make?
  prefs: []
  type: TYPE_NORMAL
- en: At the lowest level, a tool called `cAdvisor` monitors the resource usage and
    performance statistics for the containers running on each cluster node—for example,
    how much CPU, memory, and disk space each container is using.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes itself consumes this `cAdvisor` data by querying the kubelet, and
    uses the information to make decisions about scheduling, autoscaling, and so on.
    But you can also export this data to a third-party metrics service, where you
    can graph it and alert on it. For example, it would be useful to track how much
    CPU and memory each container is using.
  prefs: []
  type: TYPE_NORMAL
- en: You can also monitor Kubernetes itself, using a tool called [`kube-state-metrics`](https://oreil.ly/ZW25p).
    This listens to the Kubernetes API and reports information about logical objects
    such as nodes, Pods, and Deployments. This data can also be very useful for cluster
    observability. For example, if there are replicas configured for a Deployment
    that can’t currently be scheduled for some reason (perhaps the cluster doesn’t
    have enough capacity), you probably want to know about it.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, the problem is not a shortage of metrics data, but deciding which
    key metrics to focus on, track, and visualize. Here are some suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster health metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To monitor the health and performance of your cluster at the top level, you
    should be looking at least at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node health status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of Pods per node, and overall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource usage/allocation per node, and overall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These overview metrics will help you understand how your cluster is performing,
    whether it has enough capacity, how its usage is changing over time, and whether
    you need to expand or reduce the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using a managed Kubernetes service such as GKE, unhealthy nodes will
    be detected automatically and autorepaired (providing autorepair is enabled for
    your cluster and node pool). It’s still useful to know if you’re getting an unusual
    number of failures, which may indicate an underlying problem.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For all your deployments, it’s worth knowing:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of configured replicas per deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of unavailable replicas per deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s especially useful to be able to track this information over time if you
    have enabled some of the various autoscaling options available in Kubernetes (see
    [“Autoscaling”](ch06.html#autoscaling)). Data on unavailable replicas in particular
    will help alert you about capacity issues.
  prefs: []
  type: TYPE_NORMAL
- en: Container metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the container level, the most useful things to know are:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of containers/Pods per node, and overall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource usage for each container against its requests/limits (see [“Resource
    Requests”](ch05.html#resourcerequests))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liveness/readiness of containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of container/Pod restarts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network in/out traffic and errors for each container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because Kubernetes automatically restarts containers that have failed or exceeded
    their resource limits, you need to know how often this is happening. An excessive
    number of restarts may tell you there’s a problem with a particular container.
    If a container is regularly busting its resource limits, that could be a sign
    of a program bug, or maybe just that you need to increase the limits, such as
    giving it more memory.
  prefs: []
  type: TYPE_NORMAL
- en: Application metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whichever language or software platform your application uses, there’s probably
    a library or tool available to allow you to export custom metrics from it. These
    are primarily useful for developers and operations teams to be able to see what
    the application is doing, how often it’s doing it, and how long it takes. These
    are key indicators of performance problems or availability issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of application metrics to capture and export depends on exactly
    what your application does. But there are some common patterns. For example, if
    your service consumes messages from a queue, processes them, and takes some action
    based on the message, you might want to report the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of messages received
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of successfully processed messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of messages still in the queue waiting to be processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of invalid or erroneous messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time to process and act on each message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of successful actions generated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of failed actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarly, if your application is primarily request-driven, you can use the
    RED pattern (see [“Services: The RED Pattern”](#redpattern)):'
  prefs: []
  type: TYPE_NORMAL
- en: Requests received
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Errors returned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duration (time to handle each request)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be difficult to know what metrics are going to be useful when you’re
    at an early stage of development. If in doubt, record everything. Metrics are
    relatively cheap for most applications to output and for time-series databases
    to store; you may discover an unforeseen production issue a long way down the
    line, thanks to metrics data that didn’t seem important at the time.
  prefs: []
  type: TYPE_NORMAL
- en: If it moves, graph it. Even it doesn’t move, graph it anyway, because it might
    someday.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Laurie Denness](https://oreil.ly/HTMse) (Bloomberg)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are going to have your application generate business metrics (see [“Business
    Metrics”](#businessmetrics)), you can calculate and export these as custom metrics
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that may be useful to the business is to see how your applications
    are performing against any Service Level Objectives (SLO) or Service Level Agreements
    (SLA) that you may have with customers, or how vendor services are performing
    against SLOs. You could create a custom metric to show the target request duration
    (for example, 200 ms), and create a dashboard that overlays this on the actual
    current performance.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the runtime level, most metrics libraries will also report useful data about
    what the program is doing, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of processes/threads/goroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heap and stack usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonheap memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network I/O buffer pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garbage collector runs and pause durations (for garbage-collected languages)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File descriptors/network sockets in use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This kind of information can be very valuable for diagnosing poor performance,
    or even crashes. For example, it’s quite common for long-running applications
    to gradually use more and more memory until they are killed and restarted due
    to exceeding Kubernetes resource limits. Application runtime metrics may help
    you work out exactly where this memory is going, especially in combination with
    custom metrics about what the application is doing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have some idea what metrics data is worth capturing, in the next
    section we’ll look at what to *do* with this data: in other words, how to analyze
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data is not the same thing as understanding. In order to get useful information
    out of the raw data we’ve captured, we need to aggregate, process, and analyze
    it, which means doing *statistics* on it. Statistics can be a slippery business,
    especially in the abstract, so let’s illustrate this discussion with a concrete
    example: *request duration*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [“Services: The RED Pattern”](#redpattern), we mentioned that you should
    track the duration metric for service requests, but we didn’t say exactly how
    to do that. What precisely do we mean by *duration*? Usually, we’re interested
    in the time the user has to wait to get a response to some request.'
  prefs: []
  type: TYPE_NORMAL
- en: With a website, for example, we might define *duration* as the time between
    when the user connects to the server and when the server first starts sending
    data in response. (The user’s total waiting time is actually longer than that
    because making the connection takes some time, and so does reading the response
    data and rendering it in a browser. We usually don’t have access to that data,
    though, so we just capture what we can.)
  prefs: []
  type: TYPE_NORMAL
- en: And every request has a different duration, so how do we aggregate the data
    for hundreds or even thousands of requests into a single number?
  prefs: []
  type: TYPE_NORMAL
- en: What’s Wrong with a Simple Average?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The obvious answer is to take the average. But, on closer inspection, what *average*
    means isn’t necessarily straightforward. An old joke in statistics is that the
    average person has slightly less than two legs. To put it another way, most people
    have more than the average number of legs. How can this be?
  prefs: []
  type: TYPE_NORMAL
- en: Most people have two legs, but some have one or none, bringing down the overall
    average. (Possibly some people have more than two, but many more people have fewer
    than two.) A simple average doesn’t give us much useful information about leg
    distribution in the population, or about most people’s experience of leg ownership.
  prefs: []
  type: TYPE_NORMAL
- en: There is also more than one kind of average. You probably know that the commonplace
    notion of *average* refers to the *mean*. The mean of a set of values is the total
    of all the values, divided by the number of values. For example, the mean age
    of a group of three people is the total of their ages divided by 3.
  prefs: []
  type: TYPE_NORMAL
- en: The *median*, on the other hand, refers to the value that would divide the set
    into two equal halves, one containing values larger than the median, and the other
    containing smaller values. For example, in any group of people, half of them are
    taller than the median height, by definition, and half of them are shorter.
  prefs: []
  type: TYPE_NORMAL
- en: Means, Medians, and Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What’s the problem with taking a straightforward average (mean) of request
    duration? One important problem is that the mean is easily skewed by *outliers*:
    one or two extreme values can distort the average quite a bit.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the median, which is less affected by outliers, is a more helpful
    way of averaging metrics than the mean. If the median latency for a service is
    one second, half your users experience a latency less than one second, and half
    experience more.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-2](#img-anscombe-quartet) shows how averages can be misleading.
    All four sets of data have the same mean value, but look very different when shown
    graphically (statisticians know this example as *Anscombe’s quartet*). Incidentally,
    this is also a good way to demonstrate the importance of graphing data, rather
    than just looking at raw numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatter plots of four datasets with the same mean](assets/cnd2_1602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. These four datasets all have the same average (mean) value [(image](https://oreil.ly/ieutR)
    by Schutz, CC BY-SA 3.0).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Discovering Percentiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we talk about metrics for observing request-driven systems, we’re usually
    interested in knowing what the *worst* latency experience is for users, rather
    than the average. After all, having a median latency of 1 second for all users
    is no comfort to the small group who may be experiencing latencies of 10 seconds
    or more.
  prefs: []
  type: TYPE_NORMAL
- en: The way to get this information is to break down the data into *percentiles*.
    The 90th percentile latency (often referred to as *P90*) is the value that is
    greater than that experienced by 90% of your users. To put it another way, 10%
    of users will experience a latency higher than the P90 value.
  prefs: []
  type: TYPE_NORMAL
- en: Expressed in this language, the median is the 50th percentile, or P50\. Other
    percentiles that are often measured in observability are P95 and P99, the 95th
    and 99th percentile, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Percentiles to Metrics Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Igor Wiedler of Travis CI has produced a nice [demonstration](https://igor.io/latency)
    of what this means in concrete terms, starting from a dataset of 135,000 requests
    to a production service over 10 minutes ([Figure 16-3](#img-latency-raw)). As
    you can see, this data is noisy and spiky, and it’s not easy to draw any useful
    conclusions from it in a raw state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph of raw latency values](assets/cnd2_1603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. Raw latency data for 135,000 requests, in ms
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let’s see what happens if we average that data over 10-second intervals
    ([Figure 16-4](#img-latency-avg)). This looks wonderful: all the data points are
    below 50 ms. So it looks as though most of our users are experiencing latencies
    of less than 50 ms. But is that really true?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph of average latency values (all very low)](assets/cnd2_1604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. Average (mean) latency for the same data, over 10-second intervals
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s graph the P99 latency instead. This is the maximum latency observed, if
    we discard the highest 1% of samples. It looks very different ([Figure 16-5](#img-latency-p99)).
    Now we see a jagged pattern with most of the values clustering between 0 and 500
    ms, with several requests spiking close to 1,000 ms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph of P99 latency (spiking up to 1 second)](assets/cnd2_1605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-5\. P99 (99th percentile) latency for the same data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We Usually Want to Know the Worst
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we disproportionately notice slow web requests, the P99 data is likely
    to give us a more realistic picture of the latency experienced by users. For example,
    consider a high-traffic website with 1 million page views per day. If the P99
    latency is 10 seconds, then 10,000 page views take longer than 10 seconds. That’s
    a lot of unhappy users.
  prefs: []
  type: TYPE_NORMAL
- en: 'But it gets worse: in distributed systems, each page view may require tens
    or even hundreds of internal requests to fulfill. If the P99 latency of each internal
    service is 10s, and 1 page view makes 10 internal requests, then the number of
    slow page views rises to 100,000 per day. Now around 10% of users are unhappy,
    which is a [big problem](https://oreil.ly/zO9HV).'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Percentiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One problem with percentile latencies, as implemented by many metrics services,
    is that requests tend to be sampled locally, and statistics then aggregated centrally.
    Consequently, you end up with your P99 latency being an average of the P99 latencies
    reported by each agent, potentially across hundreds of agents.
  prefs: []
  type: TYPE_NORMAL
- en: Well, a percentile is already an average, and trying to average averages is
    a well-known [statistical trap](https://oreil.ly/ZHCMU).^([1](ch16.html#idm45979374096544))
    The result is not necessarily the same as the real average.
  prefs: []
  type: TYPE_NORMAL
- en: Depending how we choose to aggregate the data, the final P99 latency figure
    can vary by as much as a factor of 10\. That doesn’t bode well for a meaningful
    result. Unless your metrics service ingests every single raw event and produces
    a true average, this figure will be unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Engineer [Yan Cui](https://oreil.ly/XgFDV) suggests that a better approach
    is to monitor what’s *wrong*, not what’s *right*:'
  prefs: []
  type: TYPE_NORMAL
- en: What could we use instead of per­centiles as the pri­ma­ry met­ric to mon­i­tor
    our application’s per­for­mance with and alert us when it starts to dete­ri­o­rate?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you go back to your SLOs or SLAs, you prob­a­bly have some­thing along the
    lines of “99% of requests should com­plete in 1s or less.” In oth­er words, less
    than 1% of requests is allowed to take more than 1s to com­plete.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So what if we mon­i­tor the per­cent­age of requests that are over the thresh­old
    instead? To alert us when our SLAs are vio­lat­ed, we can trig­ger alarms when
    that per­cent­age is greater than 1% over some pre­de­fined time win­dow.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yan Cui
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If each agent submits a metric of total requests and the number of requests
    that were over threshold, we *can* usefully average that data to produce a percentage
    of requests that exceeded SLO—and alert on it.
  prefs: []
  type: TYPE_NORMAL
- en: Graphing Metrics with Dashboards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter we’ve learned about why metrics are useful, what metrics
    we should record, and some useful statistical techniques for analyzing them in
    bulk. All well and good, but what are we actually going to *do* with all these
    metrics?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is simple: we’re going to graph them, group them into dashboards,
    and possibly alert on them. We’ll talk about alerting in the next section, but
    for now, let’s look at some tools and techniques for graphing and dashboarding.'
  prefs: []
  type: TYPE_NORMAL
- en: Use a Standard Layout for All Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have more than a handful of services, it makes sense to always lay
    out your dashboards in the same way for each service. Someone responding to an
    on-call page can glance at the dashboard for the affected service and know how
    to interpret it immediately, without having to be familiar with that specific
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tom Wilkie, in a [Weaveworks blog post](https://oreil.ly/GTzpX), suggests the
    following standard format (see [Figure 16-6](#img-dashboard-2)):'
  prefs: []
  type: TYPE_NORMAL
- en: One row per service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Request and error rate on the left, with errors as a percentage of requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency on the right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Dashboard screenshot](assets/cnd2_1606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-6\. Weaveworks’ suggested dashboard layout for services
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You don’t have to use this exact layout; the important thing is that you always
    use the same layout for every dashboard, and that everyone is familiar with it.
    You should review your key dashboards regularly (at least once a week), looking
    at the previous week’s data, so that everyone knows what *normal* looks like.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *requests, errors, duration* dashboard works well for services (see [“Services:
    The RED Pattern”](#redpattern)). For resources, such as cluster nodes, disks,
    and networks, the most useful things to know are usually *utilization, saturation,
    errors* (see [“Resources: The USE Pattern”](#usepattern)).'
  prefs: []
  type: TYPE_NORMAL
- en: Build an Information Radiator with Primary Dashboards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have a hundred services, you have a hundred dashboards, but you probably
    won’t look at them very often. It’s still important to have that information available
    (to help spot which service is failing, for example), but at this scale you need
    a more general overview.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, make a primary dashboard that shows requests, errors, and duration
    across *all* your services, in aggregate. Don’t do anything fancy like stacked
    area charts; stick to simple line graphs of total requests, total error percentage,
    and total latency. These are easier to interpret, and more accurate visualizations
    than complex charts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, you’ll be using an *information radiator* (also known as a wallboard,
    or Big Visible Chart). This is a large screen showing key observability data that
    is visible to everybody in the relevant team or office. Or, for distributed teams,
    maybe this is the homepage of the monitoring website that everyone sees when they
    first log in. The purpose of an information radiator is:'
  prefs: []
  type: TYPE_NORMAL
- en: To show the current system status at a glance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To send a clear message about which metrics the team considers important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make people familiar with what *normal* looks like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What should you include on this radiator screen? Only vital information. *Vital*,
    in the sense of *really important*, but also in the sense of *vital signs*: information
    that tells you about the life of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The vital signs monitors you’ll see next to a hospital bed are a good example.
    They show the key metrics for human beings: heart rate, blood pressure, oxygen
    saturation, temperature, and breathing rate. There are many other metrics you
    could track for a patient, and they have medically important uses, but at the
    primary dashboard level, these are the key ones. Any serious medical problem will
    show up in one or more of these metrics; everything else is a matter of diagnostics.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, your information radiator should show the vital signs of your business
    or service. If it has numbers, it should probably have no more than four or five
    numbers. If it has graphs, it should have no more than four or five graphs.
  prefs: []
  type: TYPE_NORMAL
- en: It’s tempting to cram too much information into a dashboard so it looks complicated
    and technical. That’s not the goal. The goal is to focus on a few key things and
    make them easily visible from across a room (see [Figure 16-7](#img-radiator)).
  prefs: []
  type: TYPE_NORMAL
- en: Dashboard Things That Break
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from your main information radiator, and dashboards for individual services
    and resources, you may want to create dashboards for specific metrics that tell
    you important things about the system. You might be able to think of some of these
    things already, based on the system architecture. But another useful source of
    information is *things that break*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dashboard showing request and latency data](assets/cnd2_1607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-7\. Example information radiator produced by [Grafana Dash Gen](https://oreil.ly/lyEwc)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Every time you have an incident or outage, look for a metric, or combination
    of metrics, which would have alerted you to this problem in advance. For example,
    if you have a production outage caused by a server running out of disk space,
    it’s possible that a graph of disk space on that server would have warned you
    beforehand that the available space was trending downward and heading into outage
    territory.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not talking here about problems that happen over a period of minutes or
    even hours; those are usually caught by automated alerts (see [“Alerting on Metrics”](#alerting)).
    Rather, we’re interested in the slow-moving icebergs that draw closer over days
    or weeks. Those are dangers that, if you don’t spot them and take avoiding action,
    will sink your system at the worst possible time.
  prefs: []
  type: TYPE_NORMAL
- en: After an incident, always ask, “What would have warned us about this problem
    in advance, if only we’d been aware of it?” If the answer is a piece of data you
    already had but didn’t pay attention to, take action to highlight that data. A
    dashboard is one possible way to do this.
  prefs: []
  type: TYPE_NORMAL
- en: While alerts can tell you that some value has exceeded a preset threshold, you
    may not always know in advance what the danger level is. A graph lets you visualize
    how that value is behaving over long periods of time, and helps you detect problematic
    trends before they actually affect the system.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting on Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be surprised that we’ve spent most of this chapter talking about observability
    and monitoring without mentioning alerts. For some people, alerts are what monitoring
    is all about. We think that philosophy needs to change, for a number of reasons.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Wrong with Alerts?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alerts indicate some unexpected deviation from a stable, working state. Well,
    distributed systems don’t have those states!
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ve mentioned, large-scale distributed systems are never completely *up*;
    they’re almost always in a state of partially degraded service (see [“Cloud native
    applications are never “up””](ch15.html#neverup)). They have so many metrics that
    if you alert every time some metric goes outside normal limits, you’d be sending
    hundreds of pages a day to no good purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: People are over-paging themselves because their observability blows and they
    don’t trust their tools to let them reliably debug and diagnose the problem. So
    they get tens or hundreds of alerts, which they pattern-match for clues about
    what the root cause might be. They’re flying blind. In the chaotic future we’re
    all hurtling toward, you actually have to have the discipline to have radically
    *fewer* paging alerts, not more. Request rate, latency, error rate, saturation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Charity Majors](https://oreil.ly/FiRbV)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For some unfortunate people, on-call alert pages are a way of life. This is
    a bad thing, not just for the obvious human reasons. Alert fatigue is a well-known
    issue in medicine, where clinicians can rapidly become desensitized by constant
    alarms, making them more likely to overlook a serious issue when it does arise.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a monitoring system to be useful, it has to have a very high signal-to-noise
    ratio. False alarms are not only annoying, but dangerous: they reduce trust in
    the system, and condition people that alerts can be safely ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: Excessive, incessant, and irrelevant alarms were a major factor in the [Three
    Mile Island disaster](https://oreil.ly/cXEOk), and even when individual alarms
    are well designed, operators can be overwhelmed by too many of them going off
    simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alert should mean one very simple thing: [*action needs to be taken now,
    by a person*](https://oreil.ly/pMZqD).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If no action is needed, no alert is needed. If action needs to happen *sometime*,
    but not right now, the alert can be downgraded to a lower priority notification,
    like an email or chat message. If the action can be taken by an automated system,
    then automate it: don’t wake up a valuable human being.'
  prefs: []
  type: TYPE_NORMAL
- en: On-Call Should Not Be Hell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the idea of being on-call for your own services is key to the DevOps philosophy,
    it’s equally important that being on-call should be as painless an experience
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Alert pages should be a rare and exceptional occurrence. When they do happen,
    there should be a well-established and effective procedure for handling them,
    which puts as little strain as possible on the responder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nobody should be on-call all the time. If this is the case, add more people
    to the rotation. You don’t need to be a subject-matter expert to be on-call: your
    main task is to triage the problem, decide if it needs action, and escalate it
    to the right people.'
  prefs: []
  type: TYPE_NORMAL
- en: While the burden of on-call should be fairly distributed, people’s personal
    circumstances differ. If you have a family, or other commitments outside work,
    it may not be so easy for you to take on-call shifts. It takes careful and sensitive
    management to arrange on-call in a way that’s fair to everybody.
  prefs: []
  type: TYPE_NORMAL
- en: If the job involves being on-call, that should be made clear to the person when
    they’re hired. Expectations about the frequency and circumstances of on-call shifts
    should be written into their contract. It’s not fair to hire someone for a strictly
    nine–to-five job, and then decide you also want them to be on-call nights and
    weekends.
  prefs: []
  type: TYPE_NORMAL
- en: On-call should be properly compensated with cash, time off in lieu, or some
    other meaningful benefit. This applies whether or not you actually receive any
    alerts; when you’re on-call, to some extent you’re at work.
  prefs: []
  type: TYPE_NORMAL
- en: There should also be a hard limit on the amount of time someone can spend on-call.
    People with more spare time or energy may want to volunteer to help reduce the
    stress on their coworkers, and that’s great, but don’t let anyone take on too
    much.
  prefs: []
  type: TYPE_NORMAL
- en: Recognize that when you put people on-call, you are spending human capital.
    Spend it wisely.
  prefs: []
  type: TYPE_NORMAL
- en: Urgent, Important, and Actionable Alerts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If alerts are so terrible, why are we talking about them at all? Well, you still
    need alerts. Things go wrong, blow up, fall over, and grind to a halt—usually
    at the most inconvenient time.
  prefs: []
  type: TYPE_NORMAL
- en: Observability is wonderful, but you can’t find a problem when you’re not looking
    for one. Dashboards are great, but you don’t pay somebody to sit looking at a
    dashboard all day. For detecting an outage or issue that’s happening right now,
    and drawing a human’s attention to it, you can’t beat automated alerts based on
    thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might want the system to alert you if error rates for a given
    service exceed 10% for some period of time, like five minutes. You might generate
    an alert when P99 latency for a service goes above some fixed value, like 1000
    ms.
  prefs: []
  type: TYPE_NORMAL
- en: In general, if a problem has real or potential business impact, and action needs
    to be taken now, by a person, it’s a possible candidate for an urgent alert notification.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t alert on every metric. Out of hundreds, or possibly thousands, of metrics,
    you should have only a handful of metrics that can generate alerts. Even when
    they do generate alerts, that doesn’t necessarily mean you need to page somebody.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pages should be restricted to only *urgent*, *important*, and *actionable*
    alerts:'
  prefs: []
  type: TYPE_NORMAL
- en: Alerts that are important, but not urgent, can be dealt with during normal working
    hours. Only things that can’t wait till morning should be paged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts that are urgent, but not important, don’t justify waking someone up.
    For example, the failure of a little-used internal service that doesn’t affect
    customers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there’s no immediate action that can be taken to fix it, there’s no point
    paging about it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For everything else, you can send asynchronous notifications: emails, chat
    messages, support tickets, project issues, and so on. They will be seen and dealt
    with in a timely fashion, if your system is working properly. You don’t need to
    send someone’s cortisol levels skyrocketing by waking them up in the middle of
    the night with a blaring alarm.'
  prefs: []
  type: TYPE_NORMAL
- en: Track Your Alerts, Out-of-Hours Pages, and Wake-Ups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your people are just as critical to your infrastructure as your cloud servers
    and Kubernetes clusters, in fact, more so. It makes sense, then, to monitor what’s
    happening to your people in just the same way as you monitor what’s happening
    with your services.
  prefs: []
  type: TYPE_NORMAL
- en: The number of alerts sent in a given week is a good indicator of the overall
    health and stability of your system. The number of urgent pages, especially the
    number of pages sent out of hours, on weekends, and during normal sleep times,
    is a good indicator of your team’s overall health and morale.
  prefs: []
  type: TYPE_NORMAL
- en: You should set a budget for the number of urgent pages, especially out of hours,
    and it should be very low. One or two out-of-hours pages per on-call engineer
    per week should probably be the limit. If you’re regularly exceeding this, you
    need to fix the alerts, fix the system, or hire more engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Review all urgent pages at least weekly, and fix or eliminate any false alarms
    or unnecessary alerts. If you don’t take this seriously, people won’t take your
    alerts seriously. And if you regularly interrupt people’s sleep and private life
    with unnecessary alerts, they will start looking for better jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Tools and Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s get into some specifics. What tools or services should you use to
    collect, analyze, and communicate metrics? In [“Don’t build your own monitoring
    infrastructure”](ch15.html#dontbuildyourown), we made the point that, when faced
    with a commodity problem, you should use a commodity solution. Does that mean
    you should necessarily use a third-party, hosted metrics service like Datadog
    or New Relic?
  prefs: []
  type: TYPE_NORMAL
- en: The answer here isn’t quite so clear-cut. While these services offer lots of
    powerful features, they can be expensive, especially at scale. The decision to
    run your own metrics servers or not will largely depend on your situation, including
    how many applications you manage and how much data you are collecting. If you
    decide to set up your own metrics infrastructure, there is an excellent free and
    open source product available.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The de facto standard metrics solution in the cloud native world is [Prometheus](https://prometheus.io).
    It’s very widely used, especially with Kubernetes, and almost everything can interoperate
    with Prometheus in some way, so it’s the first thing you should consider when
    you’re thinking about metrics-monitoring options.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is an open source systems-monitoring and alerting toolkit, based
    on time-series metrics data. The core of Prometheus is a server that collects
    and stores metrics. It also has various other optional components, such as an
    alerting tool ([Alertmanager](https://oreil.ly/jaKyF)), and client libraries for
    programming languages such as Go, which you can use to instrument your applications.
  prefs: []
  type: TYPE_NORMAL
- en: It all sounds rather complicated, but in practice it’s very simple. You can
    install Prometheus in your Kubernetes cluster with one command, using the [community
    Helm chart](https://oreil.ly/P2Qym). It will then gather metrics automatically
    from the cluster, and also from any applications you tell it to, using a process
    called *scraping*.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus scrapes metrics by making an HTTP connection to your application
    on a prearranged port, and downloading whatever metrics data is available. It
    then stores the data in its database, where it will be available for you to query,
    graph, or alert on.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Prometheus’s approach to collecting metrics is called *pull* monitoring. In
    this scheme, the monitoring server contacts the application and requests metrics
    data. The opposite approach, called *push*, and used by some other monitoring
    tools such as StatsD, works the other way: applications contact the monitoring
    server to send it metrics. Prometheus also supports the push model with their
    [Pushgateway](https://oreil.ly/NAucC) component.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like Kubernetes itself, Prometheus is inspired by Google’s own infrastructure.
    It was developed at SoundCloud, but it takes many of its ideas from a tool called
    Borgmon. Borgmon, as the name suggests, was designed to monitor Google’s Borg
    container orchestration system (see [“From Borg to Kubernetes”](ch01.html#borgtok8s)):'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes directly builds on Google’s decade-long experience with their own
    cluster scheduling system, Borg. Prometheus’s bonds to Google are way looser but
    it draws a lot of inspiration from Borgmon, the internal monitoring system Google
    came up with at about the same time as Borg. In a very sloppy comparison, you
    could say that Kubernetes is Borg for mere mortals, while Prometheus is Borgmon
    for mere mortals. Both are “second systems” trying to iterate on the good parts
    while avoiding the mistakes and dead ends of their ancestors.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Björn Rabenstein](https://www.oreilly.com/ideas/google-infrastructure-for-everyone-else)
    (SoundCloud)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can read more about Prometheus on its [site](https://prometheus.io), including
    instructions on how to install and configure it for your environment.
  prefs: []
  type: TYPE_NORMAL
- en: While Prometheus itself is focused on the job of collecting and storing metrics,
    there are other high-quality open source options for graphing, dashboarding, and
    alerting. [Grafana](https://grafana.com) is a powerful and capable graphing engine
    for time-series data ([Figure 16-8](#img-grafana)).
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus project includes a tool called [Alertmanager](https://oreil.ly/jaKyF),
    which works well with Prometheus but can also operate independently of it. Alertmanager’s
    job is to receive alerts from various sources, including Prometheus servers, and
    process them (see [“Alerting on Metrics”](#alerting)).
  prefs: []
  type: TYPE_NORMAL
- en: The first step in processing alerts is to deduplicate them. Alertmanager can
    then group alerts it detects to be related; for example, a major network outage
    might result in hundreds of individual alerts, but Alertmanager can group all
    of these into a single message so that responders aren’t overwhelmed with pages.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Alertmanager will route the processed alerts to an appropriate notification
    service, such as PagerDuty, Slack, or email.
  prefs: []
  type: TYPE_NORMAL
- en: Conveniently, the Prometheus metrics format is supported by a very wide range
    of tools and services, and this de facto standard is now the basis for [OpenMetrics](https://openmetrics.io),
    a Cloud Native Computing Foundation project to produce a neutral standard format
    for metrics data. Many popular hosted monitoring tools such as Amazon CloudWatch,
    Operations Suite, Datadog, and New Relic can import and understand Prometheus
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Grafana dashboard](assets/cnd2_1608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. Grafana dashboard showing Prometheus data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Google Operations Suite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Operations Suite was previously called Stackdriver, and although now a part
    of Google, it’s not limited to Google Cloud: it also works with AWS. The Cloud
    Monitoring component can collect, graph, and alert on metrics and log data from
    a variety of sources. It will autodiscover and monitor your cloud resources, including
    VMs, databases, and Kubernetes clusters. Operations Suite brings all this data
    into a central web console where you can create custom dashboards and alerts.'
  prefs: []
  type: TYPE_NORMAL
- en: Operations Suite understands how to get operational metrics from such popular
    software tools as PostgreSQL, NGINX, Cassandra, and Elasticsearch. If you want
    to include your own custom metrics from your applications, you can use Operations
    Suite’s client library to export whatever data you want. It also offers the ability
    to run a managed Prometheus instance for you, allowing you to continue using your
    existing Prometheus exporters and Grafana dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re in Google Cloud, Operations Suite is free for all GCP-related metrics;
    for custom metrics, or metrics from other cloud platforms, you pay per megabyte
    of monitoring data per month.
  prefs: []
  type: TYPE_NORMAL
- en: AWS CloudWatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon’s own cloud monitoring product, CloudWatch, has a similar feature set
    to Operations Suite. It integrates with all AWS services, and you can export custom
    metrics using the CloudWatch SDK or command-line tool.
  prefs: []
  type: TYPE_NORMAL
- en: CloudWatch has a free tier that allows you to gather *basic* metrics (such as
    CPU utilization for VMs) at five-minute intervals, a certain number of dashboards
    and alarms, and so on. Over and above those you pay per metric, per dashboard,
    or per alarm, and you can also pay for high-resolution metrics (one-minute intervals)
    on a per-instance basis.
  prefs: []
  type: TYPE_NORMAL
- en: '[CloudWatch](https://aws.amazon.com/cloudwatch) is basic, but effective. If
    your primary cloud infrastructure is AWS, CloudWatch is a good place to start
    working with metrics, and for small deployments it may be all you ever need.'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Monitor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Azure Monitor](https://oreil.ly/FPvPR) is the Microsoft equivalent of GCP’s
    Operations Suite or AWS CloudWatch. It collects logs and metrics data from all
    your Azure resources, including Kubernetes clusters, and allows you to visualize
    and alert on it. It also offers a Prometheus-based metric scraper so that you
    do not need to use a different instrumentation tool in your applications if you
    already have Prometheus configured.'
  prefs: []
  type: TYPE_NORMAL
- en: Datadog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In comparison to the cloud providers’ built-in tools like Operations Suite and
    CloudWatch, [Datadog](https://www.datadoghq.com) is a very sophisticated and powerful
    monitoring and analytics platform. It offers integrations for over 250 platforms
    and services, including all the cloud services from major providers, and popular
    software such as Jenkins, NGINX, Consul, PostgreSQL, and MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Datadog also offers an application performance monitoring (APM) component,
    along with a log aggregation product, designed to help you monitor and analyze
    how your applications are performing. Whether you use Go, Java, Ruby, or any other
    software platform, Datadog can gather metrics, logs, and traces from your software,
    and answer questions for you like:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the user experience like for a specific, individual user of my service?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are the 10 customers who see the slowest responses on a particular endpoint?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which of my various distributed services are contributing to the overall latency
    of requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together with the usual dashboarding (see [Figure 16-9](#img-datadog)) and alerting
    features (automatable via the Datadog API and client libraries, including Terraform),
    Datadog also provides features like anomaly detection, powered by machine learning,
    and they also support collecting Prometheus metrics from your applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Datadog screenshot](assets/cnd2_1609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-9\. Datadog dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: New Relic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New Relic is a very well established and widely used metrics platform focused
    on application performance monitoring (APM). Its chief strength is in diagnosing
    performance problems and bottlenecks inside applications and distributed systems
    (see [Figure 16-10](#img-new-relic)). However, it also offers infrastructure metrics
    and monitoring, alerting, software analytics, and everything else you’d expect.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re in the market for a premium corporate metrics platform, you’ll probably
    be looking either at New Relic (slightly more application focused) or Datadog
    (slightly more infrastructure focused). Both also offer good infrastructure as
    code support; for example, you can create monitoring dashboards and alerts for
    both New Relic and Datadog using official Terraform providers.
  prefs: []
  type: TYPE_NORMAL
- en: '![New Relic interface](assets/cnd2_1610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-10\. New Relic APM dashboard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Measure twice, cut once* is a favorite saying of many engineers. In the cloud
    native world, without proper metrics and observability data it’s very difficult
    to know what’s going on. On the other hand, once you open the metrics floodgates,
    too much information can be just as useless as too little.'
  prefs: []
  type: TYPE_NORMAL
- en: The trick is to gather the right data in the first place, process it in the
    right way, use it to answer the right questions, visualize it in the right way,
    and use it to alert the right people at the right time about the right things.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you forget everything else in this chapter, remember this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Focus on the key metrics for each service: requests, errors, and duration (RED).
    For each resource: utilization, saturation, and errors (USE).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrument your apps to expose custom metrics, both for internal observability
    and for business KPIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful Kubernetes metrics include, at the cluster level, the number of nodes,
    Pods per node, and resource usage of nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the deployment level, track deployments and replicas, especially unavailable
    replicas, which might indicate a capacity problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the container level, track resource usage per container, liveness/readiness
    states, restarts, network traffic, and network errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a dashboard for each service, using a standard layout and a primary information
    radiator that reports the vital signs of the whole system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you alert on metrics, alerts should be urgent, important, and actionable.
    Alert noise creates fatigue and damages morale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track and review the number of urgent pages your team receives, especially wake-ups
    and weekends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The de facto standard metrics solution in the cloud native world is Prometheus,
    and almost everything speaks the Prometheus data format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular third-party managed metrics services include Google Operations Suite,
    Amazon CloudWatch, Datadog, and New Relic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch16.html#idm45979374096544-marker)) [The Wikipedia entry on Simpson’s
    paradox](https://oreil.ly/ZHCMU) provides more information.
  prefs: []
  type: TYPE_NORMAL
