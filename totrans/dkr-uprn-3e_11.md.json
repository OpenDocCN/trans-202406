["```\n$ ssh 172.17.4.1\n…\n\nubuntu@172.17.4.1:$ sudo docker swarm init --advertise-addr 172.17.4.1\n\nSwarm initialized: current node (hypysglii5syybd2zew6ovuwq) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n docker swarm join --token SWMTKN-1-14……a4o55z01zq 172.17.4.1:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager'\nand follow the instructions.\n```", "```\nsudo docker swarm join-token --quiet worker\n```", "```\n$ docker -H 172.17.4.1 system info\n```", "```\n…\nSwarm: active\n  NodeID: l9gfcj7xwii5deveu3raf4782\n  Is Manager: true\n  ClusterID: mvdaf2xsqwjwrb94kgtn2mzsm\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 172.17.4.1\n  Manager Addresses:\n   172.17.4.1:2377\n…\n```", "```\n$ docker -H 172.17.4.1 node ls\n\nID      HOSTNAME      STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nl9…82 * ip-172-17-4-1 Ready  Active       Leader         20.10.7\n```", "```\n$ ssh 172.17.4.2 \\\n    \"sudo docker swarm join --token SWMTKN-1-14……a4o55z01zq 172.17.4.1:2377\"\n\nThis node joined a swarm as a worker.\n\n$ ssh 172.17.4.3 \\\n    \"sudo docker swarm join --token SWMTKN-1-14……a4o55z01zq 172.17.4.1:2377\"\n\nThis node joined a swarm as a worker.\n```", "```\n$ docker -H 172.17.4.1 node ls\n\nID      HOSTNAME      STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nl9…82 * ip-172-17-4-1 Ready  Active       Leader         20.10.7\n3d…7b   ip-172-17-4-2 Ready  Active                      20.10.7\nip…qe   ip-172-17-4-3 Ready  Active                      20.10.7\n```", "```\n$ docker -H 172.17.4.1 network create --driver=overlay default-net\n\nckwh5ph4ksthvx6843ytrl5ik\n\n$ docker -H 172.17.4.1 network ls\n\nNETWORK ID     NAME              DRIVER    SCOPE\n494e1a1bf8f3   bridge            bridge    local\nxqgshg0nurzu   default-net       overlay   swarm\n2e7d2d7aaf0f   docker_gwbridge   bridge    local\ndf0376841891   host              host      local\nn8kjd6oa44fr   ingress           overlay   swarm\nb4720ea133d6   none              null      local\n```", "```\n$ docker -H 172.17.4.1 service create --detach=true --name quantum \\\n    --replicas 2 --publish published=80,target=8080 --network default-net \\\n    spkane/quantum-game:latest\n\ntiwtsbf270mh83032kuhwv07c\n```", "```\n$ docker -H 172.17.4.1 service ps quantum\n\nID    NAME      IMAGE       NODE          DESIRED… CURRENT… ERROR PORTS\nrk…13 quantum.1 spkane/qua… ip-172-17-4-1 Running  Running…\nlz…t3 quantum.2 spkane/qua… ip-172-17-4-2 Running  Running…\n```", "```\nhttp://172.17.4.1/\n```", "```\nTo get a list of all the services, we can use +service ls+:\n```", "```\n$ docker -H 172.17.4.1 service ls\n\nID    NAME    MODE       REPLICAS IMAGE                      PORTS\niu…9f quantum replicated 2/2      spkane/quantum-game:latest *:80->8080/tcp\n```", "```\n$ docker -H 172.17.4.1 service inspect --pretty quantum\n```", "```\nID:    iuoh6oxrec9fk67ybwuikutqa\nName:    quantum\nService Mode:  Replicated\n Replicas:  2\nPlacement:\nUpdateConfig:\n Parallelism:  1\n On failure:  pause\n Monitoring Period: 5s\n Max failure ratio: 0\n Update order:      stop-first\nRollbackConfig:\n Parallelism:  1\n On failure:  pause\n Monitoring Period: 5s\n Max failure ratio: 0\n Rollback order:    stop-first\nContainerSpec:\n Image:    spkane/quantum-game:latest@sha256:1f57…4a8c\n Init:    false\nResources:\nNetworks: default-net\nEndpoint Mode:  vip\nPorts:\n  PublishedPort = 80\n  Protocol = tcp\n  TargetPort = 8080\n  PublishMode = ingress\n```", "```\n$ docker -H 172.17.4.1 service scale --detach=false quantum=4\n\nquantum scaled to 4\noverall progress: 4 out of 4 tasks\n1/4: running   [==================================================>]\n2/4: running   [==================================================>]\n3/4: running   [==================================================>]\n4/4: running   [==================================================>]\nverify: Service converged\n```", "```\n$ docker -H 172.17.4.1 service ps quantum\n\nID    NAME      IMAGE        NODE          DESIRED… CURRENT… ERROR PORTS\nrk…13 quantum.1 spkane/quan… ip-172-17-4-1 Running  Running…\nlz…t3 quantum.2 spkane/quan… ip-172-17-4-2 Running  Running…\nmh…g8 quantum.3 spkane/quan… ip-172-17-4-3 Running  Running…\ncn…xb quantum.4 spkane/quan… ip-172-17-4-1 Running  Running…\n```", "```\n$ docker -H 172.17.4.1 service update --update-delay 10s \\\n    --update-failure-action rollback --update-monitor 5s \\\n    --update-order start-first --update-parallelism 1 \\\n    --detach=false \\\n    --image spkane/quantum-game:latest-plus quantum\n\nquantum\noverall progress: 4 out of 4 tasks\n1/4: running   [==================================================>]\n2/4: running   [==================================================>]\n3/4: running   [==================================================>]\n4/4: running   [==================================================>]\nverify: Service converged\n```", "```\n$ docker -H 172.17.4.1 service rollback quantum\n\nquantum\nrollback: manually requested rollback\noverall progress: rolling back update: 4 out of 4 tasks\n1/4: running   [>                                                  ]\n2/4: running   [>                                                  ]\n3/4: running   [>                                                  ]\n4/4: running   [>                                                  ]\nverify: Service converged\n```", "```\n$ git clone https://github.com/spkane/rocketchat-hubot-demo.git \\\n    --config core.autocrlf=input\n```", "```\n$ cd rocketchat-hubot-demo/stack\n```", "```\n$ docker -H 172.17.4.1 stack deploy --compose-file docker-compose-stack.yaml \\\n    rocketchat\n\nCreating network rocketchat_default\nCreating service rocketchat_hubot\nCreating service rocketchat_mongo\nCreating service rocketchat_rocketchat\nCreating service rocketchat_zmachine\n```", "```\n$ docker -H 172.17.4.1 stack ls\n\nNAME         SERVICES   ORCHESTRATOR\nrocketchat   4          Swarm\n\n$ docker -H 172.17.4.1 service ls\n\nID    NAME         …  …  IMAGE                              PORTS\niu…9f quantum      … 2/2 spkane/quantum-game:latest         *:80->8080/tcp\nnh…jd …_hubot      … 1/1 rocketchat/hubot-rocketchat:latest *:3001->8080/tcp\ngw…qv …_mongo      … 1/1 spkane/mongo:4.4\nm3…vd …_rocketchat … 1/1 rocketchat/rocket.chat:5.0.4       *:3000->3000/tcp\nlb…91 …_zmachine   … 1/1 spkane/zmachine-api:latest\n```", "```\n$ docker -H 172.17.4.1 stack ps -f \"desired-state=running\" rocketchat\n\nID    NAME           IMAGE                    NODE … CURRENT STATE           …\nb5…1h …_hubot.1      rocketchat/hubot-rocket… …-1  … Running 14 seconds ago\neq…88 …_mongo.1      spkane/mongo:4.4         …-2  … Running 11 minutes ago\n5x…8u …_rocketchat.1 rocketchat/rocket.chat:… …-3  … Running 11 minutes ago\nr5…x4 …_zmachine.1   spkane/zmachine-api:lat… …-4  … Running 12 minutes ago\n```", "```\n$ docker -H 172.17.4.1 stack rm rocketchat\n\nRemoving service rocketchat_hubot\nRemoving service rocketchat_mongo\nRemoving service rocketchat_rocketchat\nRemoving service rocketchat_zmachine\nRemoving network rocketchat_default\n```", "```\n docker -H 172.17.4.1 node ls\n\nID      HOSTNAME      STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\nl9…82 * ip-172-17-4-1 Ready  Active       Leader         20.10.7\n3d…7b   ip-172-17-4-2 Ready  Active                      20.10.7\nip…qe   ip-172-17-4-3 Ready  Active                      20.10.7\n```", "```\n$ docker -H 172.17.4.1 service ps -f \"desired-state=running\" quantum\n\nID    NAME        IMAGE       NODE          DESIRED… CURRENT… ERROR   PORTS\nsc…1h quantum.1   spkane/qua… ip-172-17-4-1 Running  Running…\nax…om quantum.2   spkane/qua… ip-172-17-4-2 Running  Running…\np4…8h quantum.3   spkane/qua… ip-172-17-4-3 Running  Running…\ng8…tw quantum.4   spkane/qua… ip-172-17-4-1 Running  Running…\n```", "```\n$ docker -H 172.17.4.1 node update --availability drain ip-172-17-4-3\n\nip-172-17-4-3\n```", "```\n$ docker -H 172.17.4.1 node inspect --pretty ip-172-17-4-3\n```", "```\nID:      ipohyw73hvf70td9licnls9qe\nHostname:                ip-172-17-4-3\nJoined at:               2022-09-04 16:59:52.922451346 +0000 utc\nStatus:\n State:      Ready\n Availability:           Drain\n Address:    172.17.4.3\nPlatform:\n Operating System:  linux\n Architecture:    x86_64\nResources:\n CPUs:      2\n Memory:    7.795GiB\nPlugins:\n Log:    awslogs, fluentd, gcplogs, gelf, journald, json-file, local,\n         logentries, splunk, syslog\n Network:    bridge, host, ipvlan, macvlan, null, overlay\n Volume:    local\nEngine Version:    20.10.7\nTLS Info:\n TrustRoot:\n…\n\n Issuer Subject:  …\n Issuer Public Key:  …\n```", "```\n$ docker -H 172.17.4.1 service ps -f \"desired-state=running\" quantum\n\nID    NAME        IMAGE       NODE          DESIRED… CURRENT… ERROR   PORTS\nsc…1h quantum.1   spkane/qua… ip-172-17-4-1 Running  Running…\nax…om quantum.2   spkane/qua… ip-172-17-4-2 Running  Running…\np4…8h quantum.3   spkane/qua… ip-172-17-4-2 Running  Running…\ng8…tw quantum.4   spkane/qua… ip-172-17-4-1 Running  Running…\n```", "```\n$ docker -H 172.17.4.1 node update --availability active ip-172-17-4-3\n\nip-172-17-4-3\n```", "```\n$ docker -H 172.17.4.1 service rm quantum\n\nquantum\n\n$ docker -H 172.17.4.1 network rm default-net\n\ndefault-net\n```", "```\n$ docker -H 172.17.4.1 service ps quantum\n\nno such service: quantum\n\n$ docker -H 172.17.4.1 network ls\n\nNETWORK ID     NAME              DRIVER    SCOPE\n494e1a1bf8f3   bridge            bridge    local\n2e7d2d7aaf0f   docker_gwbridge   bridge    local\ndf0376841891   host              host      local\nn8kjd6oa44fr   ingress           overlay   swarm\nb4720ea133d6   none              null      local\n```", "```\n$ brew install minikube\n```", "```\n==> Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/…/1.25.0\nAlready downloaded: …/Homebrew/downloads/…kubernetes-cli…manifest.json\n==> Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/blobs/sha256…\nAlready downloaded: …/Homebrew/downloads/…kubernetes-cli--1.25…bottle.tar.gz\n==> Downloading https://ghcr.io/v2/homebrew/core/minikube/manifests/1.26.1\nAlready downloaded: …/Homebrew/downloads/…minikube-1.26.1.…_manifest.json\n==> Downloading https://ghcr.io/v2/homebrew/core/minikube/blobs/sha256:…\nAlready downloaded: …/Homebrew/downloads/…minikube--1.26.1…bottle.tar.gz\n==> Installing dependencies for minikube: kubernetes-cli\n==> Installing minikube dependency: kubernetes-cli\n==> Pouring kubernetes-cli--1.25.0.arm64_monterey.bottle.tar.gz  /opt/homebrew/Cellar/kubernetes-cli/1.25.0: 228 files, 52.8MB\n==> Installing minikube\n==> Pouring minikube--1.26.1.arm64_monterey.bottle.tar.gz\n==> Caveats\nBash completion has been installed to:\n  /opt/homebrew/etc/bash_completion.d\n==> Summary  /opt/homebrew/Cellar/minikube/1.26.1: 9 files, 70.6MB\n==> Running `brew cleanup minikube`…\nDisable this behavior by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==> Caveats\n==> minikube\nBash completion has been installed to:\n  /opt/homebrew/etc/bash_completion.d\n```", "```\n$ which minikube\n/opt/homebrew/bin/minikube\n```", "```\n$ brew install kubernetes-cli\n```", "```\n$ which kubectl\n/opt/homebrew/bin/kubectl\n```", "```\n# Download the file, save as 'minikube'\n$ curl -Lo minikube \\\n  https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n\n# Make it executable\n$ chmod +x minikube\n\n# Move it to /usr/local/bin\n$ sudo mv minikube /usr/local/bin/\n```", "```\n# Get the latest version number\n$ KUBE_VERSION=$(curl -s \\\n    https://storage.googleapis.com/kubernetes-release/release/stable.txt)\n\n# Fetch the executable\n$ curl -LO \\\n    https://storage.googleapis.com/kubernetes-release/\\\nrelease/$(KUBE_VERSION)/bin/linux/amd64/kubectl\n\n# Make it executable\n$ chmod +x kubectl\n\n# Move it to /usr/local/bin\n$ sudo mv kubectl /usr/local/bin/\n```", "```\n$ minikube start\n\n  minikube v1.26.1 on Darwin 12.5.1 (arm64)\n  Automatically selected the docker driver. Other choices: parallels, ssh, …\n  Using Docker Desktop driver with root privileges\n  Starting control plane node minikube in cluster minikube\n  Pulling base image …\n  Downloading Kubernetes v1.24.3 preload …\n    > preloaded-images-k8s-v18-v1…: 342.82 MiB / 342.82 MiB  100.00% 28.22 M\n    > gcr.io/k8s-minikube/kicbase: 348.00 MiB / 348.00 MiB  100.00% 18.13 MiB\n    > gcr.io/k8s-minikube/kicbase: 0 B [________________________] ?% ? p/s 16s\n  Creating docker container (CPUs=2, Memory=4000MB) …\n  Preparing Kubernetes v1.24.3 on Docker 20.10.17 …\n    ▪ Generating certificates and keys …\n    ▪ Booting up control plane …\n    ▪ Configuring RBAC rules …\n  Verifying Kubernetes components…\n    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n  Enabled addons: storage-provisioner, default-storageclass\n  Done! kubectl is now configured to use \"minikube\" cluster and\n     \"default\" namespace by default\n```", "```\n$ minikube ssh\n\ndocker@minikube:~$\n```", "```\ndocker@minikube:~$ docker container ls\n\n…ID   IMAGE       COMMAND               …  NAMES\n48…cf ba…57      \"/storage-provisioner\" … k8s_storage-provisioner_storage-…\n4e…8d ed…e8      \"/coredns -conf /etc…\" … k8s_coredns_coredns-6d4b75cb6d-…\n1d…3d …pause:3.6 \"/pause\"               … k8s_POD_coredns-6d4b75cb6d-…\n82…d3 7a…dc      \"/usr/local/bin/kube…\" … k8s_kube-proxy_kube-proxy-…\n27…10 …pause:3.6 \"/pause\"               … k8s_POD_kube-proxy-zb6w2_kube-…\n15…ce …pause:3.6 \"/pause\"               … k8s_POD_storage-provisioner_kube-…\nff…3d f9…55      \"kube-controller-man…\" … k8s_kube-controller-manager_kube-…\n33…c5 …pause:3.6 \"/pause\"               … k8s_POD_kube-controller-manager-…\n30…97 a9…df      \"etcd --advertise-cl…\" … k8s_etcd_etcd-minikube_kube-…\nf5…41 53…a6      \"kube-apiserver --ad…\" … k8s_kube-apiserver_kube-apiserver-…\n5b…08 8f…73      \"kube-scheduler --au…\" … k8s_kube-scheduler_kube-scheduler-…\n87…cc …pause:3.6 \"/pause\"               … k8s_POD_kube-apiserver-…\n5a…14 …pause:3.6 \"/pause\"               … k8s_POD_etcd-minikube_kube-…\n6f…0c …pause:3.6 \"/pause\"               … k8s_POD_kube-scheduler-…\n```", "```\ndocker@minikube:~$ exit\n```", "```\n$ minikube status\n\nminikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n```", "```\n$ kubectl create deployment hello-minikube \\\n    --image=kennethreitz/httpbin:latest --port=80\n\ndeployment.apps/hello-minikube created\n```", "```\n$ kubectl get all\n\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/hello-minikube-ff49df9b8-svl68   1/1     Running   0          2m39s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   98m\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/hello-minikube   1/1     1            1           2m39s\n\nNAME                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/hello-minikube-ff49df9b8   1         1         1       2m39s\n```", "```\n$ kubectl expose deployment hello-minikube --type=NodePort\nservice/hello-minikube exposed\n```", "```\n$ kubectl get services\n\nNAME           TYPE      CLUSTER-IP     EXTERNAL-IP PORT(S)        AGE\nhello-minikube NodePort  10.105.184.177 <none>      80:32557/TCP   8s\nkubernetes     ClusterIP 10.96.0.1      <none>      443/TCP        107m\n```", "```\n$ minikube service hello-minikube --url\nhttp://192.168.99.100:30616\n```", "```\n Because you are using a Docker driver on darwin,\n   the terminal needs to be open to run it.\n\n```", "```\n$ curl -H foo:bar $(minikube service hello-minikube --url)/get\n```", "```\n{\n  \"args\": {},\n  \"headers\": {\n    \"Accept\": \"*/*\",\n    \"Foo\": \"bar\",\n    \"Host\": \"127.0.0.1:56695\",\n    \"User-Agent\": \"curl/7.85.0\"\n  },\n  \"origin\": \"172.17.0.1\",\n  \"url\": \"http://127.0.0.1:56695/get\"\n}\n```", "```\n$ kubectl delete service hello-minikube\nservice \"hello-minikube\" deleted\n\n$ kubectl delete deployment hello-minikube\ndeployment.apps \"hello-minikube\" deleted\n\n$ kubectl get all\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   138m\n```", "```\n$ git clone \\\n    https://github.com/bluewhalebook/\\\ndocker-up-and-running-3rd-edition.git --config core.autocrlf=input\n\nCloning into 'docker-up-and-running-3rd-edition'…\n…\n\n$ cd docker-up-and-running-3rd-edition/chapter_10/kubernetes\n```", "```\napiVersion: v1\nkind: Service\nmetadata:\n  name: lazyraster\n  labels:\n    app: lazyraster\nspec:\n  type: NodePort\n  ports:\n    - port: 8000\n      targetPort: 8000\n      protocol: TCP\n  selector:\n    app: lazyraster\n```", "```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cache-data-claim\n  labels:\n    app: lazyraster\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\n```", "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lazyraster\n  labels:\n    app: lazyraster\nspec:\n  selector:\n    matchLabels:\n      app: lazyraster\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: lazyraster\n    spec:\n      containers:\n      - image: relistan/lazyraster:demo\n        name: lazyraster\n        env:\n        - name: RASTER_RING_TYPE\n          value: memberlist\n        - name: RASTER_BASE_DIR\n          value: /data\n        ports:\n        - containerPort: 8000\n          name: lazyraster\n        volumeMounts:\n        - name: cache-data\n          mountPath: /data\n      volumes:\n      - name: cache-data\n        persistentVolumeClaim:\n          claimName: cache-data-claim\n```", "```\n$ kubectl get all\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   160m\n```", "```\n$ kubectl apply -f ./lazyraster-service.yaml\n\nservice/lazyraster created\npersistentvolumeclaim/cache-data-claim created\ndeployment.apps/lazyraster created\n```", "```\n$ kubectl get all\n\nNAME                              READY   STATUS    RESTARTS   AGE\npod/lazyraster-644cb5c66c-zsjxd   1/1     Running   0          17s\n\nNAME               TYPE      CLUSTER-IP     EXTERNAL-IP PORT(S)        AGE\nservice/kubernetes ClusterIP 10.96.0.1      <none>      443/TCP        161m\nservice/lazyraster NodePort  10.109.116.225 <none>      8000:32544/TCP 17s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/lazyraster   1/1     1            1           17s\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/lazyraster-644cb5c66c   1         1         1       17s\n```", "```\n$ kubectl get pvc\n\nNAME             STATUS VOLUME    CAPACITY ACCESS MODES STORAGECLASS AGE\ncache-data-claim Bound  pvc-1a…41 100Mi    RWO          standard     65s\n```", "```\n$ minikube service --url lazyraster\nhttp://192.168.99.100:32185\n```", "```\n$ kubectl scale --replicas=2 deploy/lazyraster\ndeployment.apps/lazyraster scaled\n```", "```\n$ kubectl get deployment/lazyraster\n\nNAME         READY   UP-TO-DATE   AVAILABLE   AGE\nlazyraster   2/2     2            2           16m\n```", "```\n$ kubectl logs deployment/lazyraster\n\nFound 2 pods, using pod/lazyraster-644cb5c66c-zsjxd\nTrying to clear existing Lazyraster cached files (if any) in the background…\nLaunching Lazyraster service…\ntime=\"2022-09-10T21:14:16Z\" level=info msg=\"Settings -----------------…\ntime=\"2022-09-10T21:14:16Z\" level=info msg=\"  * BaseDir: /data\"\ntime=\"2022-09-10T21:14:16Z\" level=info msg=\"  * HttpPort: 8000\"\n…\ntime=\"2022-09-10T21:14:16Z\" level=info msg=\"  * LoggingLevel: info\"\ntime=\"2022-09-10T21:14:16Z\" level=info msg=\"--------------------------…\n…\ntime=\"2022-09-10T21:14:16Z\" level=info msg=\"Listening on tcp://:6379\"\n…\n```", "```\n$ kubectl proxy\nStarting to serve on 127.0.0.1:8001\n```", "```\n{\n…\n  \"subsets\": [\n    {\n      \"addresses\": [\n        {\n          \"ip\": \"172.17.0.5\",\n          \"nodeName\": \"minikube\",\n          \"targetRef\": {\n            \"kind\": \"Pod\",\n            \"namespace\": \"default\",\n            \"name\": \"lazyraster-644cb5c66c-zsjxd\",\n            \"uid\": \"9631395d-7e68-47fa-bb9f-9641d724d8f7\"\n          }\n        },\n        {\n          \"ip\": \"172.17.0.6\",\n          \"nodeName\": \"minikube\",\n          \"targetRef\": {\n            \"kind\": \"Pod\",\n            \"namespace\": \"default\",\n            \"name\": \"lazyraster-644cb5c66c-pvcmj\",\n            \"uid\": \"e909d424-7a91-4a74-aed3-69562b74b422\"\n          }\n        }\n      ],\n      \"ports\": [\n        {\n          \"port\": 8000,\n          \"protocol\": \"TCP\"\n        }\n      ]\n    }\n  ]\n}\n```", "```\n$ kubectl delete -f ./lazyraster-service.yaml\n\nservice \"lazyraster\" deleted\npersistentvolumeclaim \"cache-data-claim\" deleted\ndeployment.apps \"lazyraster\" deleted\n```", "```\n$ minikube delete\n\n  Deleting \"minikube\" in docker …\n  Deleting container \"minikube\" …\n  Removing /Users/spkane/.minikube/machines/minikube …\n  Removed all traces of the \"minikube\" cluster.\n```", "```\n$ kubectl config current-context\n\ndocker-desktop\n```", "```\n$ kubectl config use-context docker-desktop --namespace=default\n\nSwitched to context \"docker-desktop\".\n```", "```\n$ kubectl config unset current-context\n\nProperty \"current-context\" unset.\n```", "```\n$ kind create cluster --name test\n\nCreating cluster \"test\" …\n ✓ Ensuring node image (kindest/node:v1.25.3) \n ✓ Preparing nodes \n ✓ Writing configuration \n ✓ Starting control-plane \n ✓ Installing CNI \n ✓ Installing StorageClass \nSet kubectl context to \"kind-test\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-test\n\nThanks for using kind!  \n```", "```\n$ kubectl cluster-info\n\nKubernetes control plane is running at https://127.0.0.1:56499\nCoreDNS is running at\nhttps://127.0.0.1:56499/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n```", "```\n$ kubectl config view --minify\n```", "```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://127.0.0.1:56499\n  name: kind-test\ncontexts:\n- context:\n    cluster: kind-test\n    user: kind-test\n  name: kind-test\ncurrent-context: kind-test\nkind: Config\npreferences: {}\nusers:\n- name: kind-test\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n```", "```\n$ kind delete cluster --name test\n\nDeleting cluster \"test\" …\n```", "```\n$ brew update\n$ brew install awscli\n```", "```\n$ pip install awscli --upgrade --user\n```", "```\n$ easy_install awscli\n```", "```\n$ aws --version\n\naws-cli/1.14.50 Python/3.6.4 Darwin/17.3.0 botocore/1.9.3\n```", "```\n$ aws configure\n\nAWS Access Key ID [None]: EXAMPLEEXAMPLEEXAMPLE\nAWS Secret Access Key [None]: ExaMPleKEy/7EXAMPL3/EXaMPLeEXAMPLEKEY\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n```", "```\n$ aws iam list-users\n```", "```\n{\n    \"Users\": [\n        {\n            \"Path\": \"/\",\n            \"UserName\": \"administrator\",\n            \"UserId\": \"ExmaPL3ExmaPL3ExmaPL3Ex\",\n            \"Arn\": \"arn:aws:iam::936262807352:user/myuser\",\n            \"CreateDate\": \"2021-04-08T17:22:23+00:00\",\n            \"PasswordLastUsed\": \"2022-09-05T15:56:21+00:00\"\n        }\n    ]\n}\n```", "```\n$ aws ecs create-cluster --cluster-name fargate-testing\n```", "```\n{\n    \"cluster\": {\n        \"clusterArn\": \"arn:aws:ecs:us-east-1:1…2:cluster/fargate-testing\",\n\"clusterName\": \"fargate-testing\",\n        \"status\": \"ACTIVE\",\n        \"registeredContainerInstancesCount\": 0,\n        \"runningTasksCount\": 0,\n        \"pendingTasksCount\": 0,\n        \"activeServicesCount\": 0,\n        \"statistics\": [],\n        \"tags\": [],\n        \"settings\": [\n            {\n                \"name\": \"containerInsights\",\n                \"value\": \"disabled\"\n            }\n        ],\n        \"capacityProviders\": [],\n        \"defaultCapacityProviderStrategy\": []\n    }\n}\n```", "```\n{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"web-game\",\n      \"image\": \"spkane/quantum-game\",\n      \"cpu\": 0,\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 8080,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"essential\": true,\n      \"environment\": [],\n      \"mountPoints\": [],\n      \"volumesFrom\": []\n    }\n  ],\n  \"family\": \"fargate-game\",\n  \"networkMode\": \"awsvpc\",\n  \"volumes\": [],\n  \"placementConstraints\": [],\n  \"requiresCompatibilities\": [\n    \"FARGATE\"\n  ],\n  \"cpu\": \"256\",\n  \"memory\": \"512\"\n}\n```", "```\ngit clone \\\n https://github.com/bluewhalebook/\\\ndocker-up-and-running-3rd-edition.git \\\n --config core.autocrlf=input\n```", "```\n$ aws ecs register-task-definition --cli-input-json file://./webgame-task.json\n```", "```\n{\n    \"taskDefinition\": {\n        \"taskDefinitionArn\": \"arn:aws:ecs:…:task-definition/fargate-game:1\",\n        \"containerDefinitions\": [\n            {\n                \"name\": \"web-game\",\n                \"image\": \"spkane/quantum-game\",\n                \"cpu\": 0,\n                \"portMappings\": [\n                    {\n                        \"containerPort\": 8080,\n                        \"hostPort\": 8080,\n                        \"protocol\": \"tcp\"\n                    }\n                ],\n                \"essential\": true,\n                \"environment\": [],\n                \"mountPoints\": [],\n                \"volumesFrom\": []\n            }\n        ],\n        \"family\": \"fargate-game\",\n        \"networkMode\": \"awsvpc\",\n        \"revision\": 1,\n        \"volumes\": [],\n        \"status\": \"ACTIVE\",\n        \"requiresAttributes\": [\n            {\n                \"name\": \"com.amazonaws.ecs.capability.docker-remote-api.1.18\"\n            },\n            {\n                \"name\": \"ecs.capability.task-eni\"\n            }\n        ],\n        \"placementConstraints\": [],\n        \"compatibilities\": [\n            \"EC2\",\n            \"FARGATE\"\n        ],\n        \"requiresCompatibilities\": [\n            \"FARGATE\"\n        ],\n        \"cpu\": \"256\",\n        \"memory\": \"512\",\n        \"registeredAt\": \"2022-09-05T09:10:18.184000-07:00\",\n        \"registeredBy\": \"arn:aws:iam::…:user/me\"\n    }\n}\n```", "```\n$ aws ecs list-task-definitions\n```", "```\n{\n    \"taskDefinitionArns\": [\n        \"arn:aws:ecs:us-east-1:…:task-definition/fargate-game:1\",\n    ]\n}\n```", "```\nawsvpcConfiguration={subnets=[subnet-abcd1234],\n                     securityGroups=[sg-abcd1234],\n                     assignPublicIp=ENABLED}\n```", "```\n$ aws ecs create-service --cluster fargate-testing --service-name \\\n    fargate-game-service --task-definition fargate-game:1 --desired-count 1 \\\n    --launch-type \"FARGATE\" --network-configuration \\\n    \"awsvpcConfiguration={subnets=[subnet-abcd1234],\\\n securityGroups=[sg-abcd1234]}\"\n```", "```\n{\n    \"service\": {\n        \"serviceArn\": \"arn:aws:ecs:…:service/fargate-game-service\",\n        \"serviceName\": \"fargate-game-service\",\n        \"clusterArn\": \"arn:aws:ecs:…:cluster/fargate-testing\",\n        \"loadBalancers\": [],\n        \"serviceRegistries\": [],\n        \"status\": \"ACTIVE\",\n        \"desiredCount\": 1,\n        \"runningCount\": 0,\n        \"pendingCount\": 0,\n        \"launchType\": \"FARGATE\",\n        \"platformVersion\": \"LATEST\",\n        \"platformFamily\": \"Linux\",\n        \"taskDefinition\": \"arn:aws:ecs:…:task-definition/fargate-game:1\",\n        \"deploymentConfiguration\": {\n            \"deploymentCircuitBreaker\": {\n                \"enable\": false,\n                \"rollback\": false\n            },\n            \"maximumPercent\": 200,\n            \"minimumHealthyPercent\": 100\n        },\n        \"deployments\": [\n            {\n                \"id\": \"ecs-svc/…\",\n                \"status\": \"PRIMARY\",\n                \"taskDefinition\": \"arn:aws:ecs:…definition/fargate-game:1\",\n                \"desiredCount\": 1,\n                \"pendingCount\": 0,\n                \"runningCount\": 0,\n                \"failedTasks\": 0,\n                \"createdAt\": \"2022-09-05T09:14:51.653000-07:00\",\n                \"updatedAt\": \"2022-09-05T09:14:51.653000-07:00\",\n                \"launchType\": \"FARGATE\",\n                \"platformVersion\": \"1.4.0\",\n                \"platformFamily\": \"Linux\",\n                \"networkConfiguration\": {\n…\n                },\n                \"rolloutState\": \"IN_PROGRESS\",\n                \"rolloutStateReason\": \"ECS deployment ecs-svc/… in progress.\"\n            }\n        ],\n        \"roleArn\": \"…aws-service-role/ecs.amazonaws.com/AWSServiceRoleForECS\",\n        \"events\": [],\n        \"createdAt\": \"2022-09-05T09:14:51.653000-07:00\",\n        \"placementConstraints\": [],\n        \"placementStrategy\": [],\n        \"networkConfiguration\": {\n…\n        },\n        \"schedulingStrategy\": \"REPLICA\",\n        \"createdBy\": \"arn:aws:iam::…:user/me\",\n        \"enableECSManagedTags\": false,\n        \"propagateTags\": \"NONE\",\n        \"enableExecuteCommand\": false\n    }\n}\n```", "```\n\"role/aws-service-role/ecs.amazonaws.com/\nAWSServiceRoleForECS\"\n```", "```\n$ aws iam create-service-linked-role \\\n    --aws-service-name ecs.amazonaws.com\n```", "```\n$ aws ecs list-services --cluster fargate-testing\n```", "```\n{\n    \"serviceArns\": [\n        \"arn:aws:ecs:us-west-2:…:service/fargate-testing/fargate-game-service\"\n    ]\n}\n```", "```\n$ aws ecs describe-services --cluster fargate-testing \\\n    --services fargate-game-service\n```", "```\n{\n    \"services\": [\n        {\n…\n            \"deployments\": [\n                {\n                    \"id\": \"ecs-svc/…\",\n                    \"status\": \"PRIMARY\",\n                    \"taskDefinition\": \"arn:…:task-definition/fargate-game:1\",\n                    \"desiredCount\": 1,\n                    \"pendingCount\": 1,\n                    \"runningCount\": 0,\n                    \"createdAt\": \"2022-09-05T09:14:51.653000-07:00\",\n                    \"updatedAt\": \"2022-09-05T09:14:51.653000-07:00\",\n                    \"launchType\": \"FARGATE\",\n                    \"platformVersion\": \"1.4.0\",\n                    \"platformFamily\": \"Linux\",\n                    \"networkConfiguration\": {\n…\n                    },\n                    \"rolloutState\": \"IN_PROGRESS\",\n                    \"rolloutStateReason\": \"ECS deployment ecs-svc/…progress.\"\n                }\n            ],\n            \"roleArn\": \"…role/ecs.amazonaws.com/AWSServiceRoleForECS\",\n            \"events\": [\n                {\n                    \"id\": \"83bd5c2eed5d4866bb7ec8c3c938666c\",\n                    \"createdAt\": \"2022-09-05T09:14:54.950000-07:00\",\n                    \"message\": \"(…game-service) has started 1 tasks: (…).\"\n                }\n            ],\n…\n        }\n    ],\n    \"failures\": []\n}\n```", "```\n$ aws ecs list-tasks --cluster fargate-testing\n```", "```\n{\n    \"taskArns\": [\n        \"arn:aws:ecs:…:task/fargate-testing/83bd5c2eed5d4866bb7ec8c3c938666c\"\n    ]\n}\n```", "```\n$ aws ecs describe-tasks --cluster fargate-testing \\\n  --task 83bd5c2eed5d4866bb7ec8c3c938666c\n```", "```\n{\n    \"tasks\": [\n        {\n            \"attachments\": [\n                {\n…\n                    \"details\": [\n…\n                        {\n                            \"name\": \"networkInterfaceId\",\n                            \"value\": \"eni-00a40225208c9411a\"\n                        },\n…\n                        {\n                            \"name\": \"privateIPv4Address\",\n                            \"value\": \"172.31.42.184\"\n                        }\n                    ]\n                }\n            ],\n            \"attributes\": [\n…\n            ],\n            \"availabilityZone\": \"us-west-2b\",\n            \"clusterArn\": \"arn:aws:ecs:us-west-2:…:cluster/fargate-testing\",\n            \"connectivity\": \"CONNECTED\",\n            \"connectivityAt\": \"2022-09-05T09:23:46.929000-07:00\",\n            \"containers\": [\n                {\n                    \"containerArn\": \"arn:…:container/fargate-testing/…\",\n                    \"taskArn\": \"arn:…:task/fargate-testing/…\",\n                    \"name\": \"web-game\",\n                    \"image\": \"spkane/quantum-game\",\n                    \"runtimeId\": \"83bd…998\",\n                    \"lastStatus\": \"RUNNING\",\n                    \"networkInterfaces\": [\n                        {\n                            \"attachmentId\": \"ddab…373a\",\n                            \"privateIpv4Address\": \"172.31.42.184\"\n                        }\n                    ],\n                    \"healthStatus\": \"UNKNOWN\",\n                    \"cpu\": \"0\"\n                }\n            ],\n            \"cpu\": \"256\",\n            \"createdAt\": \"2022-09-05T09:23:42.700000-07:00\",\n            \"desiredStatus\": \"RUNNING\",\n            \"enableExecuteCommand\": false,\n            \"group\": \"service:fargate-game-service\",\n            \"healthStatus\": \"UNKNOWN\",\n            \"lastStatus\": \"RUNNING\",\n            \"launchType\": \"FARGATE\",\n            \"memory\": \"512\",\n            \"overrides\": {\n                \"containerOverrides\": [\n                    {\n                        \"name\": \"web-game\"\n                    }\n                ],\n                \"inferenceAcceleratorOverrides\": []\n            },\n            \"platformVersion\": \"1.4.0\",\n            \"platformFamily\": \"Linux\",\n            \"pullStartedAt\": \"2022-09-05T09:59:36.554000-07:00\",\n            \"pullStoppedAt\": \"2022-09-05T09:59:46.361000-07:00\",\n            \"startedAt\": \"2022-09-05T09:59:48.546000-07:00\",\n            \"startedBy\": \"ecs-svc/…\",\n            \"tags\": [],\n            \"taskArn\": \"arn:aws:…:task/fargate-testing/83bd…666c\",\n            \"taskDefinitionArn\": \"arn:aws:…:task-definition/fargate-game:1\",\n            \"version\": 4,\n            \"ephemeralStorage\": {\n                \"sizeInGiB\": 20\n            }\n        }\n    ],\n    \"failures\": []\n}\n```", "```\nhttp://172.31.42.184:8080/\n```", "```\n$ aws ecs list-tasks --cluster fargate-testing\n```", "```\n{\n    \"taskArns\": [\n        \"arn:aws:ecs:…:task/fargate-testing/83bd5c2eed5d4866bb7ec8c3c938666c\"\n    ]\n}\n```", "```\n$ aws ecs describe-services --cluster fargate-testing \\\n    --services fargate-game-service\n```", "```\n{\n…\n                {\n                    \"id\": \"6b7f…0384\",\n                    \"createdAt\": \"2022-09-05T09:59:23.917000-07:00\",\n                    \"message\": \"…: (task 83bd5c2eed5d4866bb7ec8c3c938666c).\"\n                }\n…\n}\n```", "```\n$ aws ecs stop-task --cluster fargate-testing \\\n    --task 83bd5c2eed5d4866bb7ec8c3c938666c\n```", "```\n{\n        \"desiredStatus\": \"STOPPED\",\n…\n        \"lastStatus\": \"RUNNING\",\n…\n        \"stopCode\": \"UserInitiated\",\n        \"stoppedReason\": \"Task stopped by user\",\n        \"stoppingAt\": \"2022-09-05T10:29:05.110000-07:00\",\n…\n}\n```", "```\n$ aws ecs describe-tasks --cluster fargate-testing \\\n    --task 83bd5c2eed5d4866bb7ec8c3c938666c\n```", "```\n{\n…\n            \"desiredStatus\": \"STOPPED\",\n…\n            \"lastStatus\": \"STOPPED\",\n…\n}\n```", "```\n$ aws ecs list-tasks --cluster fargate-testing\n```", "```\n{\n    \"taskArns\": []\n}\n```", "```\n$ aws ecs delete-service --cluster fargate-testing \\\n  --service fargate-game-service  --force\n…\n\n$ aws ecs delete-cluster --cluster fargate-testing\n…\n```"]