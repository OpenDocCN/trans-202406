- en: Chapter 6\. Automated Placement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Automated Placement* is the core function of the Kubernetes scheduler for
    assigning new Pods to nodes that match container resource requests and honor scheduling
    policies. This pattern describes the principles of the Kubernetes scheduling algorithm
    and how to influence the placement decisions from the outside.'
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A reasonably sized microservices-based system consists of tens or even hundreds
    of isolated processes. Containers and Pods do provide nice abstractions for packaging
    and deployment but do not solve the problem of placing these processes on suitable
    nodes. With a large and ever-growing number of microservices, assigning and placing
    them individually to nodes is not a manageable activity.
  prefs: []
  type: TYPE_NORMAL
- en: Containers have dependencies among themselves, dependencies to nodes, and resource
    demands, and all of that changes over time too. The resources available on a cluster
    also vary over time, through shrinking or extending the cluster or by having it
    consumed by already-placed containers. The way we place containers impacts the
    availability, performance, and capacity of the distributed systems as well. All
    of that makes scheduling containers to nodes a moving target.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, assigning Pods to nodes is done by the scheduler. It is a part
    of Kubernetes that is highly configurable, and it is still evolving and improving.
    In this chapter, we cover the main scheduling control mechanisms, driving forces
    that affect the placement, why to choose one or the other option, and the resulting
    consequences. The Kubernetes scheduler is a potent and time-saving tool. It plays
    a fundamental role in the Kubernetes platform as a whole, but similar to other
    Kubernetes components (API Server, Kubelet), it can be run in isolation or not
    used at all.
  prefs: []
  type: TYPE_NORMAL
- en: At a very high level, the main operation the Kubernetes scheduler performs is
    to retrieve each newly created Pod definition from the API Server and assign it
    to a node. It finds the most suitable node for every Pod (as long as there is
    such a node), whether that is for the initial application placement, scaling up,
    or when moving an application from an unhealthy node to a healthier one. It does
    this by considering runtime dependencies, resource requirements, and guiding policies
    for high availability; by spreading Pods horizontally; and also by colocating
    Pods nearby for performance and low-latency interactions. However, for the scheduler
    to do its job correctly and allow declarative placement, it needs nodes with available
    capacity and containers with declared resource profiles and guiding policies in
    place. Let’s look at each of these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Available Node Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, the Kubernetes cluster needs to have nodes with enough resource
    capacity to run new Pods. Every node has capacity available for running Pods,
    and the scheduler ensures that the sum of the container resources requested for
    a Pod is less than the available allocatable node capacity. Considering a node
    dedicated only to Kubernetes, its capacity is calculated using the following formula
    in [Example 6-1](#ex-node-resources).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Node capacity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t reserve resources for system daemons that power the OS and Kubernetes
    itself, the Pods can be scheduled up to the full capacity of the node, which may
    cause Pods and system daemons to compete for resources, leading to resource starvation
    issues on the node. Even then, memory pressure on the node can affect all Pods
    running on it through OOMKilled errors or cause the node to go temporarily offline.
    OOMKilled is an error message displayed when the Linux kernel’s Out-of-Memory
    (OOM) killer terminates a process because the system is out of memory. Eviction
    thresholds are the last resort for the Kubelet to reserve memory on the node and
    attempt to evict Pods when the available memory drops below the reserved value.
  prefs: []
  type: TYPE_NORMAL
- en: Also keep in mind that if containers are running on a node that is not managed
    by Kubernetes, the resources used by these containers are not reflected in the
    node capacity calculations by Kubernetes. A workaround is to run a placeholder
    Pod that doesn’t do anything but has only resource requests for CPU and memory
    corresponding to the untracked containers’ resource use amount. Such a Pod is
    created only to represent and reserve the resource consumption of the untracked
    containers and helps the scheduler build a better resource model of the node.
  prefs: []
  type: TYPE_NORMAL
- en: Container Resource Demands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important requirement for an efficient Pod placement is to define the
    containers’ runtime dependencies and resource demands. We covered that in more
    detail in [Chapter 2, “Predictable Demands”](ch02.html#PredictableDemands). It
    boils down to having containers that declare their resource profiles (with `request`
    and `limit`) and environment dependencies such as storage or ports. Only then
    are Pods optimally assigned to nodes and can run without affecting one another
    and facing resource starvation during peak usage.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler Configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next piece of the puzzle is having the right filtering or priority configurations
    for your cluster needs. The scheduler has a default set of predicate and priority
    policies configured that is good enough for most use cases. In Kubernetes versions
    before v1.23, a scheduling policy can be used to configure the predicates and
    priorities of a scheduler. Newer versions of Kubernetes moved to scheduling profiles
    to achieve the same effect. This new approach exposes the different steps of the
    scheduling process as an extension point and allows you to configure plugins that
    override the default implementations of the steps. [Example 6-2](#ex-scheduler-plugin)
    demonstrates how to override the `PodTopologySpread` plugin from the `score` step
    with custom plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-2\. A scheduler configuration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_automated_placement_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The plugins in this phase provide a score to each node that has passed the filtering
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_automated_placement_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This plugin implements topology spread constraints that we will see later in
    the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_automated_placement_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The disabled plugin in the previous step is replaced by a new one.
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scheduler plugins and custom schedulers should be defined only by an administrator
    as part of the cluster configuration. As a regular user deploying applications
    on a cluster, you can just refer to predefined schedulers.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the scheduler uses the default-scheduler profile with default plugins.
    It is also possible to run multiple schedulers on the cluster, or multiple profiles
    on the scheduler, and allow Pods to specify which profile to use. Each profile
    must have a unique name. Then when defining a Pod, you can add the field `.spec.schedulerName`
    with the name of your profile to the Pod specification, and the Pod will be processed
    by the desired scheduler profile.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pods get assigned to nodes with certain capacities based on placement policies.
    For completeness, [Figure 6-1](#img-scheduler) visualizes at a high level how
    these elements get together and the main steps a Pod goes through when being scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Pod to node assignment process](assets/kup2_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. A Pod-to-node assignment process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As soon as a Pod is created that is not assigned to a node yet, it gets picked
    by the scheduler together with all the available nodes and the set of filtering
    and priority policies. In the first stage, the scheduler applies the filtering
    policies and removes all nodes that do not qualify. Nodes that meet the Pod’s
    scheduling requirements are called *feasible nodes*. In the second stage, the
    scheduler runs a set of functions to score the remaining feasible nodes and orders
    them by weight. In the last stage, the scheduler notifies the API server about
    the assignment decision, which is the primary outcome of the scheduling process.
    This whole process is also referred to as *scheduling*, *placement*, *node assignment*,
    or *binding*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, it is better to let the scheduler do the Pod-to-node assignment
    and not micromanage the placement logic. However, on some occasions, you may want
    to force the assignment of a Pod to a specific node or group of nodes. This assignment
    can be done using a node selector. The `.spec.nodeSelector` Pod field specifies
    a map of key-value pairs that must be present as labels on the node for the node
    to be eligible to run the Pod. For example, let’s say you want to force a Pod
    to run on a specific node where you have SSD storage or GPU acceleration hardware.
    With the Pod definition in [Example 6-3](#ex-node-selector) that has `nodeSelector`
    matching `disktype: ssd`, only nodes that are labeled with `disktype=ssd` will
    be eligible to run the Pod.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-3\. Node selector based on type of disk available
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_automated_placement_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Set of node labels a node must match to be considered the node of this Pod.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to specifying custom labels to your nodes, you can use some of the
    default labels that are present on every node. Every node has a unique `kubernetes.io/hostname`
    label that can be used to place a Pod on a node by its hostname. Other default
    labels that indicate the OS, architecture, and instance type can be useful for
    placement too.
  prefs: []
  type: TYPE_NORMAL
- en: Node Affinity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes supports many more flexible ways to configure the scheduling processes.
    One such feature is *node affinity*, which is a more expressive way of the node
    selector approach described previously that allows specifying rules as either
    required or preferred. *Required rules* must be met for a Pod to be scheduled
    to a node, whereas preferred rules only imply preference by increasing the weight
    for the matching nodes without making them mandatory. In addition, the node affinity
    feature greatly expands the types of constraints you can express by making the
    language more expressive with operators such as `In`, `NotIn`, `Exists`, `DoesNotExist`,
    `Gt`, or `Lt`. [Example 6-4](#ex-node-affinity) demonstrates how node affinity
    is declared.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-4\. Pod with node affinity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_automated_placement_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Hard requirement that the node must have more than three cores (indicated by
    a node label) to be considered in the scheduling process. The rule is not reevaluated
    during execution if the conditions on the node change.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_automated_placement_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Match on labels. In this example, all nodes are matched that have a label `numberCores`
    with a value greater than 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_automated_placement_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Soft requirements, which is a list of selectors with weights. For every node,
    the sum of all weights for matching selectors is calculated, and the highest-valued
    node is chosen, as long as it matches the hard requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Affinity and Anti-Affinity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Pod affinity* is a more powerful way of scheduling and should be used when
    `nodeSelector` is not enough. This mechanism allows you to constrain which nodes
    a Pod can run based on label or field matching. It doesn’t allow you to express
    dependencies among Pods to dictate where a Pod should be placed relative to other
    Pods. To express how Pods should be spread to achieve high availability, or be
    packed and colocated together to improve latency, you can use Pod affinity and
    anti-affinity.'
  prefs: []
  type: TYPE_NORMAL
- en: Node affinity works at node granularity, but Pod affinity is not limited to
    nodes and can express rules at various topology levels based on the Pods already
    running on a node. Using the `topologyKey` field, and the matching labels, it
    is possible to enforce more fine-grained rules, which combine rules on domains
    like node, rack, cloud provider zone, and region, as demonstrated in [Example 6-5](#ex-pod-affinity).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-5\. Pod with Pod affinity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_automated_placement_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Required rules for the Pod placement concerning other Pods running on the target
    node.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_automated_placement_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Label selector to find the Pods to be colocated with.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_automated_placement_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The nodes on which Pods with labels `confidential=high` are running are supposed
    to carry a `security-zone` label. The Pod defined here is scheduled to a node
    with the same label and value.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_automated_placement_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Anti-affinity rules to find nodes where a Pod would *not* be placed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_automated_placement_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Rule describing that the Pod should not (but could) be placed on any node where
    a Pod with the label `confidential=none` is running.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to node affinity, there are hard and soft requirements for Pod affinity
    and anti-affinity, called `requiredDuringSchedulingIgnoredDuringExecution` and
    `preferredDuringSchedulingIgnoredDuringExecution`, respectively. Again, as with
    node affinity, the `IgnoredDuringExecution` suffix is in the field name, which
    exists for future extensibility reasons. At the moment, if the labels on the node
    change and affinity rules are no longer valid, the Pods continue running,^([1](ch06.html#idm45902104710704))
    but in the future, runtime changes may also be taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: Topology Spread Constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pod affinity rules allow the placement of unlimited Pods to a single topology,
    whereas Pod anti-affinity disallows Pods to colocate in the same topology. Topology
    spread constraints give you more fine-grained control to evenly distribute Pods
    on your cluster and achieve better cluster utilization or high availability of
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example to understand how topology spread constraints can help.
    Let’s suppose we have an application with two replicas and a two-node cluster.
    To avoid downtime and a single point of failure, we can use Pod anti-affinity
    rules to prevent the coexistence of the Pods on the same node and spread them
    into both nodes. While this setup makes sense, it will prevent you from performing
    rolling upgrades because the third replacement Pod cannot be placed on the existing
    nodes because of the Pod anti-affinity constraints. We will have to either add
    another node or change the Deployment strategy from rolling to recreate. Topology
    spread constraints would be a better solution in this situation as they allow
    you to tolerate some degree of uneven Pod distribution when the cluster is running
    out of resources. [Example 6-6](#topology-spread-constraint) allows the placement
    of the third rolling deployment Pod on one of the two nodes because it allows
    imbalances—i.e., a skew of one Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-6\. Pod with topology spread constraints
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_automated_placement_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Topology spread constraints are defined in the `topologySpreadConstraints` field
    of the Pod spec.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_automated_placement_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxSkew` defines the maximum degree to which Pods can be unevenly distributed
    in the topology.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_automated_placement_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: A topology domain is a logical unit of your infrastructure. And a `topologyKey`
    is the key of the Node label where identical values are considered to be in the
    same topology.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_automated_placement_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The `whenUnsatisfiable` field defines what action should be taken when `maxSkew`
    can’t be satisfied. `DoNotSchedule` is a hard constraint preventing the scheduling
    of Pods, whereas `ScheduleAnyway` is a soft constraint that gives scheduling priority
    to nodes that reduce cluster imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_automated_placement_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: '`labelSelector` Pods that match this selector are grouped together and counted
    when spreading them to satisfy the constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: Topology spread constraints is a feature that is still evolving at the time
    of this writing. Built-in cluster-level topology spread constraints allow certain
    imbalances based on default Kubernetes labels and give you the ability to honor
    or ignore node affinity and taint policies.
  prefs: []
  type: TYPE_NORMAL
- en: Taints and Tolerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A more advanced feature that controls where Pods can be scheduled and allowed
    to run is based on taints and tolerations. While node affinity is a property of
    Pods that allows them to choose nodes, taints and tolerations are the opposite.
    They allow the nodes to control which Pods should or should not be scheduled on
    them. A *taint* is a characteristic of the node, and when it is present, it prevents
    Pods from scheduling onto the node unless the Pod has toleration for the taint.
    In that sense, taints and tolerations can be considered an *opt-in* to allow scheduling
    on nodes that by default are not available for scheduling, whereas affinity rules
    are an *opt-out* by explicitly selecting on which nodes to run and thus exclude
    all the nonselected nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A taint is added to a node by using `kubectl`: `kubectl taint nodes control-plane-node
    node-role.kubernetes.io/control-plane="true":NoSchedule`, which has the effect
    shown in [Example 6-7](#ex-taint). A matching toleration is added to a Pod as
    shown in [Example 6-8](#ex-toleration). Notice that the values for `key` and `effect`
    in the `taints` section of [Example 6-7](#ex-taint) and the `tolerations` section
    in [Example 6-8](#ex-toleration) are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-7\. Tainted node
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_automated_placement_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Mark this node as unschedulable except when a Pod tolerates this taint.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-8\. Pod tolerating node taints
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_automated_placement_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Tolerate (i.e., consider for scheduling) nodes, which have a taint with key
    `node-role.kubernetes.io/control-plane`. On production clusters, this taint is
    set on the control plane node to prevent scheduling of Pods on this node. A toleration
    like this allows this Pod to be installed on the control plane node nevertheless.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_automated_placement_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Tolerate only when the taint specifies a `NoSchedule` effect. This field can
    be empty here, in which case the toleration applies to every effect.
  prefs: []
  type: TYPE_NORMAL
- en: There are hard taints that prevent scheduling on a node (`effect=NoSchedule`),
    soft taints that try to avoid scheduling on a node (`effect=PreferNoSchedule`),
    and taints that can evict already-running Pods from a node (`effect=NoExecute`).
  prefs: []
  type: TYPE_NORMAL
- en: Taints and tolerations allow for complex use cases like having dedicated nodes
    for an exclusive set of Pods, or force eviction of Pods from problematic nodes
    by tainting those nodes.
  prefs: []
  type: TYPE_NORMAL
- en: You can influence the placement based on the application’s high availability
    and performance needs, but try not to limit the scheduler too much and back yourself
    into a corner where no more Pods can be scheduled and there are too many stranded
    resources. For example, if your containers’ resource requirements are too coarse-grained,
    or nodes are too small, you may end up with stranded resources in nodes that are
    not utilized.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 6-2](#img-resources), we can see node A has 4 GB of memory that cannot
    be utilized as there is no CPU left to place other containers. Creating containers
    with smaller resource requirements may help improve this situation. Another solution
    is to use the Kubernetes *descheduler*, which helps defragment nodes and improve
    their utilization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Processes scheduled to nodes and stranded resources](assets/kup2_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Processes scheduled to nodes and stranded resources
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once a Pod is assigned to a node, the job of the scheduler is done, and it does
    not change the placement of the Pod unless the Pod is deleted and recreated without
    a node assignment. As you have seen, with time, this can lead to resource fragmentation
    and poor utilization of cluster resources. Another potential issue is that the
    scheduler decisions are based on its cluster view at the point in time when a
    new Pod is scheduled. If a cluster is dynamic and the resource profile of the
    nodes changes or new nodes are added, the scheduler will not rectify its previous
    Pod placements. Apart from changing the node capacity, you may also alter the
    labels on the nodes that affect placement, but past placements are not rectified.
  prefs: []
  type: TYPE_NORMAL
- en: All of these scenarios can be addressed by the descheduler. The Kubernetes descheduler
    is an optional feature that is typically run as a Job whenever a cluster administrator
    decides it is a good time to tidy up and defragment a cluster by rescheduling
    the Pods. The descheduler comes with some predefined policies that can be enabled
    and tuned or disabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the policy used, the descheduler avoids evicting the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Node- or cluster-critical Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods not managed by a ReplicaSet, Deployment, or Job, as these Pods cannot be
    recreated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods managed by a DaemonSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods that have local storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods with PodDisruptionBudget, where eviction would violate its rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods that have a non-nil `DeletionTimestamp` field set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deschedule Pod itself (achieved by marking itself as a critical Pod)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, all evictions respect Pods’ QoS levels by choosing *Best-Efforts*
    Pods first, then *Burstable* Pods, and finally *Guaranteed* Pods as candidates
    for eviction. See [Chapter 2, “Predictable Demands”](ch02.html#PredictableDemands),
    for a detailed explanation of these QoS levels.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Placement is the art of assigning Pods to nodes. You want to have as minimal
    intervention as possible, as the combination of multiple configurations can be
    hard to predict. In simpler scenarios, scheduling Pods based on resource constraints
    should be sufficient. If you follow the guidelines from [Chapter 2, “Predictable
    Demands”](ch02.html#PredictableDemands), and declare all the resource needs of
    a container, the scheduler will do its job and place the Pod on the most feasible
    node possible.
  prefs: []
  type: TYPE_NORMAL
- en: However, in more realistic scenarios, you may want to schedule Pods to specific
    nodes according to other constraints such as data locality, Pod colocality, application
    high availability, and efficient cluster resource utilization. In these cases,
    there are multiple ways to steer the scheduler toward the desired deployment topology.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-3](#img-placement) shows one approach to thinking and making sense
    of the different scheduling techniques in Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pod-to-Node and Pod-to-Pod dependencies](assets/kup2_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Pod-to-Pod and Pod-to-Node and dependencies
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Start by identifying the forces and dependencies between the Pod and the nodes
    (for example, based on dedicated hardware capabilities or efficient resource utilization).
    Use the following node affinity techniques to direct the Pod to the desired nodes,
    or use anti-affinity techniques to steer the Pod away from the undesired nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: nodeName
  prefs: []
  type: TYPE_NORMAL
- en: This field provides the simplest form of hard wiring a Pod to a node. This field
    should ideally be populated by the scheduler, which is driven by policies rather
    than manual node assignment. Assigning a Pod to a node through this approach prevents
    the scheduling of the Pod to any other node. If the named node has no capacity,
    or the node doesn’t exist, the Pod will never run. This throws us back into the
    pre-Kubernetes era, when we explicitly needed to specify the nodes to run our
    applications. Setting this field manually is not a Kubernetes best practice and
    should be used only as an exception.
  prefs: []
  type: TYPE_NORMAL
- en: nodeSelector
  prefs: []
  type: TYPE_NORMAL
- en: A node selector is a label map. For the Pod to be eligible to run on a node,
    the Pod must have the indicated key-value pairs as the label on the node. Having
    put some meaningful labels on the Pod and the node (which you should do anyway),
    a node selector is one of the simplest recommended mechanisms for controlling
    the scheduler choices.
  prefs: []
  type: TYPE_NORMAL
- en: Node affinity
  prefs: []
  type: TYPE_NORMAL
- en: This rule improves the manual node assignment approaches and allows a Pod to
    express dependency toward nodes using logical operators and constraints that provides
    fine-grained control. It also offers soft and hard scheduling requirements that
    control the strictness of node affinity constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Taints and tolerations
  prefs: []
  type: TYPE_NORMAL
- en: Taints and tolerations allow the node to control which Pods should or should
    not be scheduled on them without modifying existing Pods. By default, Pods that
    don’t have tolerations for the node taint will be rejected or evicted from the
    node. Another advantage of taints and tolerations is that if you expand the Kubernetes
    cluster by adding new nodes with new labels, you don’t need to add the new labels
    on all Pods but only on those that should be placed on the new nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the desired correlation between a Pod and the nodes is expressed in Kubernetes
    terms, identify the dependencies between different Pods. Use Pod affinity techniques
    for Pod colocation for tightly coupled applications, and use Pod anti-affinity
    techniques to spread Pods on nodes and avoid a single point of failure:'
  prefs: []
  type: TYPE_NORMAL
- en: Pod affinity and anti-affinity
  prefs: []
  type: TYPE_NORMAL
- en: These rules allow scheduling based on Pods’ dependencies on other Pods rather
    than nodes. Affinity rules help for colocating tightly coupled application stacks
    composed of multiple Pods on the same topology for low-latency and data locality
    requirements. The anti-affinity rule, on the other hand, can spread Pods across
    your cluster among failure domains to avoid a single point of failure, or prevent
    resource-intensive Pods from competing for resources by avoiding placing them
    on the same node.
  prefs: []
  type: TYPE_NORMAL
- en: Topology spread constraints
  prefs: []
  type: TYPE_NORMAL
- en: 'To use these features, platform administrators have to label nodes and provide
    topology information such as regions, zones, or other user-defined domains. Then,
    a workload author creating the Pod configurations must be aware of the underlying
    cluster topology and specify the topology spread constraints. You can also specify
    multiple topology spread constraints, but all of them must be satisfied for a
    Pod to be placed. You must ensure that they do not conflict with one another.
    You can also combine this feature with NodeAffinity and NodeSelector to filter
    nodes where evenness should be applied. In that case, be sure to understand the
    difference: multiple topology spread constraints are about calculating the result
    set independently and producing an AND-joined result, while combining it with
    NodeAffinity and NodeSelector, on the other hand, filters results of node constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In some scenarios, all of these scheduling configurations might not be flexible
    enough to express bespoke scheduling requirements. In that case, you may have
    to customize and tune the scheduler configuration or even provide a custom scheduler
    implementation that can understand your custom needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler tuning
  prefs: []
  type: TYPE_NORMAL
- en: The default scheduler is responsible for the placement of new Pods onto nodes
    within the cluster, and it does it well. However, it is possible to alter one
    or more stages in the filtering and prioritization phases. This mechanism with
    extension points and plugins is specifically designed to allow small alterations
    without the need for a completely new scheduler implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Custom scheduler
  prefs: []
  type: TYPE_NORMAL
- en: If none of the preceding approaches is good enough, or if you have complex scheduling
    requirements, you can also write your own custom scheduler. A custom scheduler
    can run instead of, or alongside, the standard Kubernetes scheduler. A hybrid
    approach is to have a “scheduler extender” process that the standard Kubernetes
    scheduler calls out to as a final pass when making scheduling decisions. This
    way, you don’t have to implement a full scheduler but only provide HTTP APIs to
    filter and prioritize nodes. The advantage of having your scheduler is that you
    can consider factors outside of the Kubernetes cluster like hardware cost, network
    latency, and better utilization while assigning Pods to nodes. You can also use
    multiple custom schedulers alongside the default scheduler and configure which
    scheduler to use for each Pod. Each scheduler could have a different set of policies
    dedicated to a subset of the Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, there are lots of ways to control the Pod placement, and choosing
    the right approach or combining multiple approaches can be overwhelming. The takeaway
    from this chapter is this: size and declare container resource profiles, and label
    Pods and nodes for the best resource-consumption-driven scheduling results. If
    that doesn’t deliver the desired scheduling outcome, start with small and iterative
    changes. Strive for a minimal policy-based influence on the Kubernetes scheduler
    to express node dependencies and then inter-Pod dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: More Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Automated Placement Example](https://oreil.ly/N-iAz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Assigning Pods to Nodes](https://oreil.ly/QlbMB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scheduler Configuration](https://oreil.ly/iPbBT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pod Topology Spread Constraints](https://oreil.ly/qkp60)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Configure Multiple Schedulers](https://oreil.ly/appyT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Descheduler for Kubernetes](https://oreil.ly/4lPFX)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Disruptions](https://oreil.ly/oNGSR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Guaranteed Scheduling for Critical Add-On Pods](https://oreil.ly/w9tKY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Keep Your Kubernetes Cluster Balanced: The Secret to High Availability](https://oreil.ly/_MODM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced Kubernetes Pod to Node Scheduling](https://oreil.ly/6Tog3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch06.html#idm45902104710704-marker)) However, if node labels change and
    allow for unscheduled Pods to match their node affinity selector, these Pods are
    scheduled on this node.
  prefs: []
  type: TYPE_NORMAL
