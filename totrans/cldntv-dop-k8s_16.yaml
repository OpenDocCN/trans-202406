- en: Chapter 14\. Continuous Deployment in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tao does not do, but nothing is not done.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lao Tzu
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at a key DevOps principle—*continuous integration*
    and *continuous deployment* (CI/CD) and see how we can achieve this in a cloud
    native, Kubernetes-based environment. We outline some of the options for setting
    up continuous deployment pipelines to work with Kubernetes, and show you a fully
    worked example using Google’s Cloud Build. We will also cover the concept of GitOps
    and walk through how to automatically deploy to Kubernetes using a GitOps tool
    called Flux.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Continuous Deployment?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Continuous deployment* (CD) is the automatic deployment of successful builds
    to production. Like the test suite, deployment should also be managed centrally
    and automated. Developers should be able to deploy new versions by either pushing
    a button, or merging a merge request, or pushing a Git release tag.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CD is often associated with *continuous integration* (CI): the automatic integration
    and testing of developers’ changes against the mainline branch. The idea is that
    if you’re making changes on a branch that would break the build when merged to
    the mainline, continuous integration will let you know that right away, rather
    than waiting until you finish your branch and do the final merge. The combination
    of continuous integration and deployment is often referred to as *CI/CD*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The machinery of continuous deployment is often referred to as a *pipeline*:
    a series of automated actions that take code from the developer’s workstation
    to production, via a sequence of test and acceptance stages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical pipeline for containerized applications might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A developer pushes their code changes to the repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The build system automatically builds the current version of the code and runs
    tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If all tests pass, the container image will be published into the central container
    registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The newly built container is deployed automatically to a staging environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The staging environment undergoes some automated and/or manual acceptance tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The verified container image is deployed to production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A key point is that the artifact that is tested and deployed through your various
    environments is not the *source code*, but the *container*. There are many ways
    for errors to creep in between source code and a running binary, and testing the
    container instead of the code can help catch a lot of these.
  prefs: []
  type: TYPE_NORMAL
- en: The great benefit of CD is *no surprises in production*; nothing gets deployed
    unless the exact binary image has already been successfully tested in staging.
  prefs: []
  type: TYPE_NORMAL
- en: You can see a detailed example of a CD pipeline like this in [“A CI/CD Pipeline
    with Cloud Build”](#cloudbuild-example).
  prefs: []
  type: TYPE_NORMAL
- en: Which CD Tool Should I Use?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As usual, the problem is not a shortage of available tools, but the sheer range
    of choices. There are several new CI/CD tools designed specifically for cloud
    native applications, and long-established traditional build tools such as Jenkins
    also now have plug-ins to allow them to work with Kubernetes and containers.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, if you are already doing CI/CD, you probably don’t need to switch
    to a whole new system. If you are migrating existing applications to Kubernetes,
    you can likely do this with a few small tweaks to your existing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will briefly cover some of the popular hosted and self-hosted
    options for CI/CD tools. We certainly won’t be able to cover them all, but here
    is a quick list that should get you started in your search.
  prefs: []
  type: TYPE_NORMAL
- en: Hosted CI/CD Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are looking for an out-of-the-box solution for your CI/CD pipelines where
    you do not need to maintain the underlying infrastructure, then you should consider
    using a hosted offering. The main cloud providers all offer CI/CD tools that integrate
    well within their ecosystems, so it would be worth first exploring the tools that
    are already part of your cloud account.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microsoft’s Azure DevOps service (formerly known as Visual Studio Team Services)
    includes a continuous delivery pipeline facility, called [Azure Pipelines](https://oreil.ly/pVbMb),
    similar to Google Cloud Build.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Build
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you run your infrastructure on Google Cloud Platform, then you should look
    at [Cloud Build](https://cloud.google.com/build). It runs containers as the various
    build steps and the configuration YAML for the pipeline lives in your code repository.
  prefs: []
  type: TYPE_NORMAL
- en: You can configure Cloud Build to watch your Git repository. When a preset condition
    is triggered, such as pushing to a certain branch or tag, Cloud Build will run
    your specified pipeline, such as building a new container image, running your
    test suite, publishing the image, and perhaps deploying the new version to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: For a complete working example of a CD pipeline in Cloud Build, see [“A CI/CD
    Pipeline with Cloud Build”](#cloudbuild-example).
  prefs: []
  type: TYPE_NORMAL
- en: Codefresh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Codefresh](https://codefresh.io) is a managed service for testing and deploying
    applications to Kubernetes. One interesting feature is the ability to deploy temporary
    staging environments for every feature branch.'
  prefs: []
  type: TYPE_NORMAL
- en: Using containers, Codefresh can build, test, and deploy on-demand environments,
    and then you can configure how you would like to deploy your containers into various
    environments in your clusters.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GitHub Actions](https://oreil.ly/eeWSL) is integrated into the popular hosted
    Git repository site. Actions are shared using GitHub repos, making it very easy
    to mix and match and share build tools across different applications. Azure has
    published a [popular GitHub Action](https://oreil.ly/GcHam) for deploying to Kubernetes
    clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub also offers the option to run GitHub Action runners locally on your own
    servers to keep your builds inside of your network.
  prefs: []
  type: TYPE_NORMAL
- en: GitLab CI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GitLab is a popular alternative to GitHub for hosting Git repositories. You
    can use their hosted offering, or you can run GitLab yourself on your own infrastructure.
    It comes with a powerful built-in CI/CD tool, [GitLab CI](https://oreil.ly/e5f11),
    that can be used for testing and deploying your code. If you are already using
    GitLab, it makes sense to look at GitLab CI for implementing your continuous deployment
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Hosted CI/CD Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you would rather own more of the underlying infrastructure for your pipeline,
    then there are also several good options for CI/CD tools that you can run wherever
    you like. Some of these tools have been around long before Kubernetes, and some
    have been developed specifically for Kubernetes-based CI/CD pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Jenkins](https://jenkins.io) is a very widely adopted CI/CD tool and has been
    around for years. It has plug-ins for just about everything you could want to
    use in a workflow, including Docker, `kubectl`, and Helm. There is also a newer
    dedicated project for running Jenkins in Kubernetes called [JenkinsX](https://jenkins-x.io).'
  prefs: []
  type: TYPE_NORMAL
- en: Drone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Drone](https://oreil.ly/qtXoR) is a tool built with, and for, containers.
    It is simple and lightweight, with the pipeline defined by a single YAML file.
    Since each build step consists of running a container, it means that anything
    you can run in a container you can run on Drone.'
  prefs: []
  type: TYPE_NORMAL
- en: Tekton
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Tekton](https://tekton.dev) introduces an interesting concept where CI/CD
    components actually consist of Kubernetes CRDs. You can therefore construct your
    build, test, and deployment steps using native Kubernetes resources, and manage
    the pipeline the same way you manage anything else in your Kubernetes clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Concourse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Concourse](https://concourse-ci.org) is an open source CD tool written in
    Go. It also adopts the declarative pipeline approach, much like Drone and Cloud
    Build, using a YAML file to define and execute build steps. Concourse provides
    an [official Helm chart](https://oreil.ly/UCvPz) to deploy it on Kubernetes, making
    it easy to get a containerized pipeline up and running quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Spinnaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spinnaker](https://spinnaker.io) is very powerful and flexible, but can be
    a little daunting at first glance. Developed originally by Netflix, it excels
    at large-scale and complex deployments, such as blue/green deployments (see [“Blue/Green
    Deployments”](ch13.html#bluegreen)). There is a free ebook about Spinnaker, titled
    [*Continuous Delivery with Spinnaker*](https://oreil.ly/hkb64) (O’Reilly), that
    should give you some idea whether Spinnaker fits your needs.'
  prefs: []
  type: TYPE_NORMAL
- en: Argo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Argo CD](https://oreil.ly/8FmGn) is a GitOps tool similar to Flux (see [“GitOps”](#gitops))
    that automates deployments by syncing what is running in Kubernetes with manifests
    stored in a central Git repo. Rather than “pushing” changes via `kubectl` or `helm`,
    Argo continuously “pulls” in changes from the Git repo and applies them from within
    the cluster. Argo also offers a [popular pipeline tool](https://argoproj.github.io)
    for running any sort of pipeline workflows, not necessarily just for CI/CD.'
  prefs: []
  type: TYPE_NORMAL
- en: Keel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Keel](https://keel.sh) is not a full end-to-end CI/CD tool but is solely concerned
    with deploying new container images when they are published into a container registry.
    It can be configured to respond to webhooks, send and receive Slack messages,
    and wait for approvals before deploying to a new environment. If you already have
    a CI process that works well for you but just need a way to automate the CD part,
    then Keel may be worth evaluating.'
  prefs: []
  type: TYPE_NORMAL
- en: A CI/CD Pipeline with Cloud Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know the general principles of CI/CD, and have learned about some
    of the tooling options, let’s look at a complete, end-to-end example of a demo
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is not that you should necessarily use exactly the same tools and configuration
    as we have here; rather, we hope you’ll get a sense of how everything fits together,
    and can adapt some parts of this example to suit your own environment.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’ll be using GitHub, Google Kubernetes Engine (GKE) clusters,
    and Google Cloud Build, but we don’t rely on any specific features of those products.
    You can replicate this kind of pipeline using whatever tools you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to work through this example using your own GCP account, please
    bear in mind that it uses some billable resources. You’ll want to delete and clean
    up any test cloud resources afterward to make sure you don’t get charged unexpectedly.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d rather try out a CI/CD example locally without using any Google Cloud
    resources, skip down to [“GitOps”](#gitops).
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Google Cloud and GKE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are signing up for a new Google Cloud account for the first time, you’ll
    be eligible for some free credits, which should enable you to run a Kubernetes
    cluster, and other cloud resources, without being billed for a few months. However,
    you should definitely monitor your usage when trying out any cloud service to
    make sure that you aren’t accruing any unexpected charges. You can find out more
    about the free-tier offering and create an account at the [Google Cloud Platform
    site](https://cloud.google.com/free).
  prefs: []
  type: TYPE_NORMAL
- en: Once you are signed up and logged into your own Google Cloud project, create
    a GKE cluster following [these instructions](https://oreil.ly/zcLVd). An Autopilot
    cluster will be fine for this example, and choose a region that is close to your
    location. You’ll also need to enable the [Cloud Build](https://oreil.ly/O8ZNV)
    and [Artifact Registry](https://oreil.ly/b86m9) APIs in your new project, as we’ll
    be using those services along with GKE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we’ll walk you through the following steps to prepare for creating the
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Fork the demo repository into your own personal GitHub account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a container repository in Artifact Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Authenticate Cloud Build to use Artifact Registry and GKE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Cloud Build trigger for building and testing on a push to any Git branch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a trigger for deploying to GKE based on Git tags.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forking the Demo Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using your GitHub account, use the GitHub interface to fork the [demo repo](https://oreil.ly/LAI8f).
    If you are unfamiliar with a repo fork, you can learn more about it in the [GitHub
    docs](https://oreil.ly/S37E1).
  prefs: []
  type: TYPE_NORMAL
- en: Create Artifact Registry Container Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GCP offers a private artifact repository tool called Artifact Registry that
    can store Docker containers, Python packages, npm packages, and other types of
    artifacts. We will use this for hosting the demo container image that we will
    build.
  prefs: []
  type: TYPE_NORMAL
- en: Browse to the Artifact Registry page in the [Google Cloud web console](https://oreil.ly/aIZet)
    and create a new Docker repository called `demo` following [these instructions](https://oreil.ly/sBY0O).
    Create it in the same Google Cloud region where you created your GKE cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need to authorize the Cloud Build service account to have permission
    to make changes to your Kubernetes Engine cluster. Under the IAM section in GCP,
    grant the service account for Cloud Build—the *Kubernetes Engine Developer* and
    *Artifact Registry Repository Administrator*—IAM roles in your project following
    [the instructions in the GCP docs](https://oreil.ly/tMtAR).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Cloud Build
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s look at the steps in our build pipeline. In many modern CI/CD platforms,
    each step of a pipeline consists of running a container. The build steps are defined
    using a YAML file that lives in your Git repo. Using containers for each step
    means that you can easily package, version, and share common tools and scripts
    between different pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the demo repository, there is a directory called *hello-cloudbuild-v2*.
    Inside that directory, you will find the *cloudbuild.yaml* file that defines our
    Cloud Build pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice the *-v2* in the directory name for *hello-cloudbuild-v2*. There are
    some changes here for the second edition of the book that do not match the first
    edition. In order to avoid breaking any of the examples used in the first edition,
    we are using a completely different directory for this example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at each of the build steps in this file in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Test Container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Like all Cloud Build steps, this consists of a set of YAML key-value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id` gives a human-friendly label to the build step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dir` specifies the subdirectory of the Git repo to work in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` identifies the container to run for this step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`entrypoint` specifies the command to run in the container, if not the default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args` gives the necessary arguments to the entrypoint command. (We’re using
    a little trick here with `bash -c |` to keep our `args` together on a single line,
    just to make it easier to read.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of this first step is to build a container that we can use to run
    our application’s tests. Since we are using a multistage build (see [“Understanding
    Dockerfiles”](ch02.html#multistagedockerfile)), we want to build only the first
    stage for now. So we are using the `--target build` argument, which tells Docker
    to only build the part in the Dockerfile under `FROM golang:1.17-alpine AS build`
    and stop before moving on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the resulting container will still have Go installed, along
    with any of the packages or files used in the step labeled `...AS build`, which
    means that we can use this image to run the test suite. It is often the case that
    you need packages in your container for running tests that you do not want to
    be in your final production images. Here we are essentially building a throwaway
    container that is only used for running the test suite, and is discarded afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s the next step in our `cloudbuild.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we tagged our throwaway container as `demo:test`, that temporary image
    will still be available for the rest of this build inside Cloud Build. This step
    will run the `go test` command against that container. If any tests fail, this
    step will fail and the build will exit. Otherwise, it will continue on to the
    next step.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Application Container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we run `docker build` again, but without the `--target` flag so that we
    run the entire multistage build, ending up with the final application container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Substitution Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to make Cloud Build pipeline files reusable and flexible we use variables,
    or what Cloud Build calls *substitutions*. Anything that begins with a `$` will
    be substituted when the pipeline runs. For example, `$PROJECT_ID` will interpolate
    as the Google Cloud Project where a particular build is running, and `$COMMIT_SHA`
    is the specific Git commit SHA that triggered this build. User-defined substitutions
    in Cloud Build must begin with an underscore character (`_`) and use only uppercase
    letters and numbers. We will use the `${_REGION}` substitution variable below
    when we create the build trigger.
  prefs: []
  type: TYPE_NORMAL
- en: Git SHA Tags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may ask why we are using `$COMMIT_SHA` for our container image tag. In Git,
    every commit has a unique identifier, called a SHA (named for the Secure Hash
    Algorithm that generates it). A SHA is a long string of hex digits, like `5ba6bfd64a31eb4013ccaba27d95cddd15d50ba3`.
  prefs: []
  type: TYPE_NORMAL
- en: If you use this SHA to tag your image, it provides a link to the exact Git commit
    that generated it—which is also a complete snapshot of the code that is in the
    container. The nice thing about tagging build artifacts with the originating Git
    SHA is that you can build and test lots of feature branches simultaneously, without
    any conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the Kubernetes Manifests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point in the pipeline, we have built a new container that has passed
    tests and is ready to deploy. But before we do, we’d also like to do a quick check
    to make sure that our Kubernetes manifests are valid. In this final step of the
    build, we’ll run `helm template` to generate the rendered version of our Helm
    chart, and then pipe that to the `kubeval` tool (see [“kubeval”](ch12.html#kubeval))
    to check for any issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that we’re using our own Helm container image here (`cloudnatived/helm-cloudbuilder`),
    which contains `helm` and `kubeval`, but you could also create and use your own
    “builder images,” containing any additional build or testing tools that you use.
    Just remember that it’s important to keep your builder images small and lean (see
    [“Minimal Container Images”](ch02.html#minimalcontainers-intro)). When you’re
    running tens or hundreds of builds a day, the increased pull time of large containers
    can really add up.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing the Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming each step in the pipeline completes successfully, Cloud Build can
    then publish the resulting container image to the Artifact Registry repository
    you created earlier. To specify which images from the build that you want to publish,
    list them under `images` in the Cloud Build file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Creating the First Build Trigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’ve seen how the pipeline works, let’s create the build triggers
    in Google Cloud that will actually execute the pipeline, based on our specified
    conditions. A Cloud Build trigger specifies a Git repo to watch, a condition on
    which to activate (such as pushing to a particular branch or tag), and a pipeline
    file to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and create a new Cloud Build trigger now. Log in to your Google Cloud
    project and browse to the [Cloud Build triggers page](https://oreil.ly/LLuWt).
  prefs: []
  type: TYPE_NORMAL
- en: Click the Add Trigger button to make a new build trigger, and select GitHub
    as the source repository. You’ll be asked to grant permission for Google Cloud
    to access your GitHub repo. Select `YOUR_GITHUB_USERNAME/demo` and Google Cloud
    will link to your forked copy of the demo repository. You can name the trigger
    whatever you like.
  prefs: []
  type: TYPE_NORMAL
- en: Under the Branch section, select `.*` so that it will match any branch.
  prefs: []
  type: TYPE_NORMAL
- en: Under the Configuration section, choose the Cloud Build configuration file and
    set the location to *hello-cloudbuild-v2/cloudbuild.yaml*, which is where the
    file lives in the demo repo.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to create some substitution variables so that we can reuse
    this same *cloudbuild.yaml* file for different builds.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, you’ll need to add the following substitution variable to
    your trigger:'
  prefs: []
  type: TYPE_NORMAL
- en: '`_REGION` should be the GCP region where you deployed your Artifact Registry
    and GKE cluster, such as `us-central1`, or `southamerica-east1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click the Create trigger button when you are done. You’re now ready to test
    the trigger and see what happens!
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Trigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Go ahead and make a change to your forked copy of the demo repository. Edit
    both *main.go* and *main_test.go*, replacing `Hello` with `Hola`, or whatever
    you like, and save both files (we’ll use `sed` in the example below). You can
    also run the tests locally, if you have Golang installed, to make sure that the
    test suite still passes. When ready, commit and push the changes to your forked
    copy of the Git repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you look in the [Cloud Build web UI](https://oreil.ly/YAc9a), you will see
    the list of recent builds in your project. You should see one at the top of the
    list for the current change you just pushed. It may be still running, or it may
    have already finished.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully you will see a green check indicating that all steps passed. If not,
    check the log output in the build and see what failed.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming it passed, a container should have been published into your private
    Google Artifact Registry tagged with the Git commit SHA of your change.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying from a CI/CD Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you can trigger a build with a Git push, run tests, and publish the
    final container to the registry, you are ready to deploy that container to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example we will imagine there are two environments, one for `production`,
    and one for `staging`, and we will deploy them into separate namespaces: `staging-demo`
    and `production-demo`. Both will run in the same GKE cluster (although you would
    probably want to use separate clusters for your real applications).'
  prefs: []
  type: TYPE_NORMAL
- en: To keep things simple, we’re going to use the Git tags `production` and `staging`
    for triggering deployments to each environment. You may have your own process
    for managing versions, such as using semantic version ([SemVer](https://semver.org)),
    release tags, or automatically deploying to a staging environment whenever the
    main or trunk branch is updated. Feel free to adapt these examples to your own
    situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will configure Cloud Build to deploy to staging when the `staging` Git tag
    is pushed to the repo, and to production when the `production` tag is pushed.
    This requires a new pipeline that uses a different Cloud Build YAML file, *cloudbuild-deploy.yaml*.
    Let’s take a look at the steps that are in our deploy pipepline:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting credentials for the Kubernetes cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy to Kubernetes with Cloud Build, the build will need a working `KUBECONFIG`,
    which we can get with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Deploying to the cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the build is authenticated, it can run Helm to actually upgrade (or install)
    the application in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We are passing a few additional flags to the `helm upgrade` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`namespace`'
  prefs: []
  type: TYPE_NORMAL
- en: The namespace where the application should be deployed
  prefs: []
  type: TYPE_NORMAL
- en: '`values`'
  prefs: []
  type: TYPE_NORMAL
- en: The Helm values file to use for this environment
  prefs: []
  type: TYPE_NORMAL
- en: '`set container.image`'
  prefs: []
  type: TYPE_NORMAL
- en: Sets the container name to deploy
  prefs: []
  type: TYPE_NORMAL
- en: '`set container.tag`'
  prefs: []
  type: TYPE_NORMAL
- en: Deploys the image with this specific tag (the originating Git SHA)
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Deploy Trigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s create a new Cloud Build trigger for deploying to our imaginary `staging`
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new trigger in the Cloud Build web UI just as you did in [“Creating
    the First Build Trigger”](#create-build-trigger). The repo will be the same, but
    this time configure it to trigger when a *tag* is pushed instead of a branch,
    and set the tag name to match *staging*.
  prefs: []
  type: TYPE_NORMAL
- en: Also, instead of using the *cloudbuild.yaml* file, for this build we will use
    `*hello-cloudbuild-v2/cloudbuild-deploy.yaml*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Substitution variables` section, we’ll set some values that are specific
    to the deploy builds:'
  prefs: []
  type: TYPE_NORMAL
- en: '`_REGION` will be the same as you used in the first trigger. It should match
    the GCP availability region where you created your GKE cluster and Artifact Registry
    repo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_CLOUDSDK_CONTAINER_CLUSTER` is the name of your GKE cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these variables here means that we can use the same YAML file for deploying
    both staging and production, even if those environments were in separate clusters,
    or in separate GCP projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have created the trigger for the `staging` tag, go ahead and try it
    out by pushing a `staging` tag to the repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As before, you can watch the [build progress](https://oreil.ly/UQmGq) in the
    Cloud Build UI. If all goes as planned, Cloud Build should successfully authenticate
    to your GKE cluster and deploy the staging version of your application into the
    `staging-demo` namespace. You can verify this by checking the [GKE dashboard](https://oreil.ly/LbLjU)
    (or use `helm status`).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, follow the same steps to create a separate Cloud Build trigger that
    deploys to production on a push to the `production` tag. If all goes well, you’ll
    have another copy of the app running in a new `production-demo` namespace. Again,
    in this example we deployed both environments to the same GKE cluster but for
    real applications you would likely want to keep these separate.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting the Example Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are done trying out the demo pipeline you will want to delete any of
    the GCP resources you created for testing, including the GKE cluster, the `demo`
    Artifact Registry repository, and your Cloud Build triggers.
  prefs: []
  type: TYPE_NORMAL
- en: We hope this example demonstrates the key concepts of a CI/CD pipeline. If you’re
    using Cloud Build, you can use these examples as a starting point for setting
    up your own pipelines. If you’re using other tools, we hope you can easily adapt
    the patterns we’ve shown here to work in your own environment. Automating the
    build, test, and deployment steps for your applications will greatly improve the
    experience of creating and deploying software for everyone involved.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in [“Infrastructure as Code”](ch01.html#infra-as-code), an integral
    part of the industry’s shift toward DevOps was the need to manage infrastructure
    by means of code and source control. “GitOps” is a newer term that seems to mean
    something slightly different depending on who you ask. But a high-level GitOps
    involves using source control (Git being one of the more popular source control
    tools) to track and manage infrastructure in an automated way. Imagine the Kubernetes
    reconciliation loop but applied in a broader sense, where any and all infrastructure
    is configured and deployed solely by pushing changes to a Git repo. A number of
    existing CI/CD tools have rebranded themselves as being “GitOps” tools and we
    expect this concept to grow and evolve rapidly throughout the software industry
    in the coming years.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use a tool called Flux to automatically deploy the
    demo application to a local Kubernetes cluster by pushing changes to a GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: Flux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weaveworks (the creators of [`eksctl`](https://eksctl.io)) may have been the
    first to coin the [GitOps term](https://oreil.ly/Bjd3D). They have also built
    one of the more popular GitOps tools called [Flux](https://oreil.ly/Eoqs6). It
    can be used to automatically deploy changes to a Kubernetes cluster by polling
    a Git repo, watching for any changes, and automatically applying the changes from
    the Flux Pods running inside of the cluster. Let’s try out an example to see how
    it works.
  prefs: []
  type: TYPE_NORMAL
- en: Set Up Flux
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`flux` is the CLI tool used for interacting with Flux and can also be used
    to install the Flux components in Kubernetes. Follow the [installation instructions](https://oreil.ly/erpNZ)
    for your operating system and point your `kubectl` at your test Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: You will need to create a GitHub personal access token so that Flux can talk
    securely to GitHub. You can generate one in the Settings page of your GitHub profile
    under the Developer Settings section. It will need the `repo` permission and you
    should decide if you want to have it automatically expire on a schedule, or set
    it to never expire. For this example either is fine, but in a real production
    system you should always have a process to rotate any credentials on a regular
    basis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the [GitHub instructions](https://oreil.ly/amjIN) to generate a personal
    access token, export it into your environment along with your username, and check
    to see if Flux is ready to install using the `flux check` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Install Flux
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Assuming the check passes, you are ready to install Flux! As part of the process
    it will use your personal access token to automatically create a new GitHub repo
    in your account and then use that repo for managing your cluster going forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Among other things, you should see that Flux successfully connected to your
    repo. You can browse to your new repo and see that inside of this repo Flux created
    a directory called *clusters/demo-cluster/flux-system* containing all of the Flux
    Kubernetes manifests that are now running in your cluster in the new `flux-system`
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Create a New Deployment Using Flux
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s use Flux to automatically deploy a new namespace and deployment to
    your cluster. In proper GitOps fashion we will do this only by pushing changes
    to the Git repo. You’ll need to clone the new repo that Flux created, which means
    you will need to set up your credentials with GitHub in order to push new commits.
    If you have not done this yet, you can follow [these instructions from GitHub](https://oreil.ly/UVcoA).
    Once you have your repo cloned, make a new directory alongside `flux-system` for
    our new `flux-demo` Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we will generate the YAML needed for a new Namespace and Deployment called
    `flux-demo` using `kubectl` and the `--dry-run` flag. After saving those to new
    manifest files, we will commit and push them to the repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Since Flux is regularly polling the Git repo and watching for any changes,
    it will automatically create and deploy your new `flux-demo` manifests when it
    detects the new files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In addition to plain Kubernetes manifests, Flux can manage Helm releases, and
    manifests that use kustomize. You can also configure Flux to poll your container
    registry and automatically deploy new images. It will then make a new Git commit
    back to the repo, tracking the image version that it deployed, keeping your Git
    repo in sync with what is actually running in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Also, just like the Kubernetes reconciliation loop, Flux continuously monitors
    for any manual changes to the resources that it manages. It will attempt to keep
    the cluster in sync with what is in the Git repo. This way any manual changes
    to anything managed with Flux should automatically be rolled back, allowing you
    to better trust your Git repo as the ultimate source, or truth, for what should
    be running in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is one of the main goals of GitOps: that you should be able to manage
    Kubernetes automatically using code that is tracked in Git. Pushing changes to
    this repo is the only way you should ever make changes to your clusters when using
    Flux. Using Git to manage your infrastructure means that you will have a record
    of all changes in your commit history, and also that all changes can be peer-reviewed
    as part of your team’s merge-request process.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up a continuous deployment pipeline for your applications allows you
    to deploy software consistently, reliably, and quickly. Ideally, developers should
    be able to push code to the source control repository and all of the build, test,
    and deploy phases happen automatically in a centralized pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because there are so many options for CI/CD software and techniques, we can’t
    give you a single recipe that’ll work for everybody. Instead, we’ve aimed to show
    you how and why CD is beneficial, and give you a few important things to think
    about when you come to implement it in your own organization:'
  prefs: []
  type: TYPE_NORMAL
- en: Deciding which CI/CD tools to use is an important process when building a new
    pipeline. All of the tools we mention throughout this book could likely be incorporated
    into almost any existing environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jenkins, GitHub Actions, GitLab, Drone, Cloud Build, and Spinnaker are just
    a few of the popular CI/CD tools that work well with Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the build pipeline steps with code allows you to track and modify these
    steps alongside application code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers enable developers to promote build artifacts up through environments,
    such as testing, staging, and eventually production, ideally without having to
    rebuild a new container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our example pipeline using Cloud Build should be easily adaptable for other
    tools and types of applications. The overall build, test, and deploy steps are
    largely the same in any CI/CD pipeline, regardless of the tools used or type of
    software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitOps is a newer term used when talking about CI/CD pipelines. The main idea
    is that deployments and infrastructure changes should be managed using code that
    is tracked in source control (Git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flux and Argo have popular GitOps tools that can automatically apply changes
    to your clusters whenever you push code changes to a Git repo.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
