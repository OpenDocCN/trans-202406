- en: Chapter 9\. Data Analytics on Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章\. Kubernetes 上的数据分析
- en: Progress in technology is when we have the ability to be more lazy.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 技术的进步在于我们有能力变得更懒。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dr. Laurian Chirica
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 拉里安·基里卡博士
- en: 'In the early 2000s, Google captivated the internet with a declared public goal:
    “to organize the world’s information and make it universally accessible and useful.”
    This was an ambitious goal and accomplishing it would, to paraphrase, take “computer
    sciencing” the bits out of it. Given the increasing rate of data creation, Google
    needed to invent (and reinvent) ways of managing data volumes no one had ever
    considered. An entirely new community, culture, and industry were born around
    analyzing data called *analytics*, tackling what was eventually labeled “big data.”
    Today, analytics is a full-fledged member of almost every application stack and
    not just relegated to a Google problem. Now it’s everyone’s problem; instead of
    an art form restricted to a small club of experts, we all need to know how to
    make analytics work. Organizations need reliable and fast ways to deploy applications
    with analytics so that they can do more with less.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2000 年代初期，谷歌以公开宣布的目标“组织世界信息，使其普遍可访问和有用”迷住了互联网。这是一个雄心勃勃的目标，要实现它，就像用“计算机科学化”这个词来概括，需要从中提取出位。鉴于数据创建的速度不断增加，谷歌需要发明（和重新发明）管理以前从未考虑过的数据量的方法。围绕分析数据的全新社区、文化和行业应运而生，解决了最终被标记为“大数据”的问题。如今，分析已成为几乎每个应用程序堆栈的全面成员，而不仅仅是谷歌的问题。现在，这是每个人的问题；不再是一种仅限于少数专家的艺术形式，我们都需要知道如何使分析工作。组织需要可靠且快速的方法来部署带有分析功能的应用程序，以便他们可以做更多的事情而花费更少的精力。
- en: The laziness Dr. Chirica was talking about in a tongue-in-cheek way in the quote
    that opens this chapter describes an ideal future. Instead of having a hundred-person
    team working night and day to analyze a petabyte of data, what if you could reduce
    that to one person and a few minutes? The cloud native way of running data infrastructure
    is a path we should all work toward to achieve that kind of glorious laziness.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 基里卡博士在本章开头引用的玩笑话中谈到的懒惰描述了一个理想的未来。与其让一百人的团队日夜工作来分析一百万亿字节的数据，不如将其减少到一个人和几分钟？云原生方式运行数据基础设施是我们所有人应该朝着实现这种光荣懒惰的路径努力。
- en: We’ve already looked at several aspects of moving stateful workloads onto Kubernetes,
    including storage, databases, and streaming. In this chapter, it’s time to look
    at analytics to complete the picture. As a bit of a preview, [Figure 9-1](#the_cloud_native_virtual_datacenter)
    shows how data analytics fits as the final part of our roadmap of managing the
    complete data stack using Kubernetes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过将有状态工作负载迁移到 Kubernetes 的几个方面，包括存储、数据库和流处理。在本章中，是时候看看分析以完成整个图景了。作为预览，[图 9-1](#the_cloud_native_virtual_datacenter)
    显示了数据分析如何作为使用 Kubernetes 管理完整数据栈路线图的最后一部分。
- en: '![The cloud native virtual datacenter](assets/mcdk_0901.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![云原生虚拟数据中心](assets/mcdk_0901.png)'
- en: Figure 9-1\. The cloud native virtual datacenter
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. 云原生虚拟数据中心
- en: In this architecture, there are no more external network requirements bridging
    to resources in or out of the Kubernetes cluster, just a single, virtual datacenter
    that serves our bespoke needs for cloud native applications. The large blocks
    represent the macro components of data infrastructure we discussed in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra),
    with the addition of user application code, deployed in microservices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，不再需要外部网络要求来连接 Kubernetes 集群内外的资源，只需一个单一的虚拟数据中心，为我们的云原生应用程序服务。大块表示我们在[第
    1 章](ch01.html#introduction_to_cloud_native_data_infra)中讨论过的数据基础设施的宏组件，再加上用户应用程序代码，部署为微服务。
- en: Introduction to Analytics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析简介
- en: Analytic workloads and the accompanying infrastructure operations are much different
    from other workloads. Analytics isn’t just another containerized system to orchestrate.
    The typical stateful applications like databases we examined in previous chapters
    have many similar characteristics but tend to stay static or predictably slow-growing
    once deployed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工作负载及其伴随的基础设施操作与其他工作负载大不相同。分析不仅仅是另一个容器化的系统来编排。我们在前几章中研究的典型有状态应用程序（如数据库）具有许多相似的特征，但一旦部署，它们往往保持静态或可预见的缓慢增长。
- en: 'However, one aspect of analytic workloads strikes fear in many administrators:
    volume. While persistent data stores like databases can consume gigabytes to terabytes
    of storage, analytic volumes can easily soar into petabytes, creating an entirely
    new class of problems to solve. They don’t call it “big data” for nothing.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分析工作负载中的一个方面让许多管理员感到恐惧：数据量。虽然像数据库这样的持久数据存储可以消耗从千兆字节到百万兆字节的存储空间，但分析数据量可能会轻松地飙升到几个拍字节，从而产生一整套新的问题。它们不是毫无缘由地称为“大数据”。
- en: 'The *Oxford English Dictionary* defines analytics as “the systematic computational
    analysis of data or statistics.” [Wikipedia](https://oreil.ly/Hc1Pp) adds, “It
    is used for the discovery, interpretation, and communication of meaningful patterns
    in data.” Combine those definitions with large volumes of data and what sort of
    outcome should we expect for cloud native applications? Let’s break down the different
    types of analytics workflows and methodologies:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*牛津英语词典*将分析定义为“数据或统计量的系统计算分析”。[维基百科](https://oreil.ly/Hc1Pp)补充道：“它被用于发现、解释和传达数据中的有意义的模式。”结合这些定义与大量的数据，我们应该期待云原生应用的何种结果？让我们详细分析各种类型的分析工作流程和方法：'
- en: Batch analytics
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 批量分析
- en: In computer science, a *batch* is a series of instructions applied to data with
    little or no user interaction. The idea of running batch Jobs is as old as general-purpose
    computing. In distributed systems such as Apache Hadoop or Apache Spark, each
    individual Job consists of a program that can operate on smaller bits of data
    in parallel and in stages or pipelines. The smaller results are combined into
    a single, final result at the end of a Job. An example of this is MapReduce, discussed
    later in this chapter. In most cases, statistical analysis such as count, average,
    and percentile measurement is done. Batch analytics is the focus of this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，*批处理*是一系列应用于数据的指令，几乎没有用户交互。批处理作业的概念与通用计算同样古老。在像Apache Hadoop或Apache
    Spark这样的分布式系统中，每个单独的作业由一个程序组成，可以并行和分阶段地处理较小的数据块。较小的结果最终在作业结束时合并为一个单一的最终结果。MapReduce就是这方面的一个例子，在本章的后续部分讨论。在大多数情况下，会进行统计分析，如计数、平均值和百分位数测量。批量分析是本章的重点。
- en: Stream analytics
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析
- en: As discussed in [Chapter 8](ch08.html#streaming_data_on_kubernetes), stream
    analytics is about what is *happening,* whereas batch analytics is about what
    *happened*. Many of the same APIs and developer methodologies are used in both
    stream analytics and batch analytics. This can be confusing and lead people to
    believe that they are the same thing when, in fact, they have very different use
    cases and implementations. A good example is fraud detection. The time frames
    for detecting and stopping fraud can be measured in milliseconds to seconds, which
    fits the stream analytics use case. Batch analytics would be used to find fraud
    patterns over larger time periods.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第8章](ch08.html#streaming_data_on_kubernetes)中讨论的那样，流分析关注的是*正在发生的事情*，而批量分析关注的是*已经发生的事情*。许多相同的API和开发方法在流分析和批量分析中都被使用。这可能会让人感到困惑，并使人误以为它们是同一件事情，但实际上它们有着非常不同的用例和实现方式。一个很好的例子是欺诈检测。用于检测和阻止欺诈的时间跨度可以以毫秒到秒计算，这符合流分析的用例。批量分析将用于在较长时间段内发现欺诈模式。
- en: Artificial intelligence/machine learning (AI/ML)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能/机器学习（AI/ML）
- en: While AI and ML can be considered a subset of batch analytics, they are such
    specialized fields that they deserve a special callout. AI and ML are often mentioned
    together; however, they have two different output goals. AI attempts to emulate
    human cognition in decision making. ML uses algorithms to derive meaning from
    pools of data, sometimes in ways that aren’t readily obvious. Both approaches
    require the application of computing resources across volumes of data. This topic
    is discussed in greater detail in [Chapter 10](ch10.html#machine_learning_and_other_emerging_use).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人工智能（AI）和机器学习（ML）可以被视为批量分析的子集，但它们是如此专业化的领域，以至于它们值得特别强调。AI和ML经常被一起提到；然而，它们有着两种不同的输出目标。AI试图在决策制定中模拟人类认知。ML使用算法从数据池中推导意义，有时的方式并不容易明显。这两种方法都需要在大量数据上应用计算资源。这个主题在[第10章](ch10.html#machine_learning_and_other_emerging_use)中有更详细的讨论。
- en: Deploying Analytic Workloads in Kubernetes
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中部署分析工作负载
- en: 'The original focus of Kubernetes was on scaling and orchestrating stateless
    applications. As you’re learning in this book, Kubernetes is evolving to support
    stateful applications. The promise of operational efficiency by moving more and
    more workloads into virtual datacenters has been highly motivating. The world
    of analytics can take advantage of the progress made in reducing the operational
    burden for stateless and stateful workloads. However, Kubernetes has some unique
    challenges in managing analytic workloads; many are still a work in progress.
    What features of Kubernetes are required to complete the data picture and put
    analytic workloads on par with other parts of the stack like microservices and
    databases? Here are a few of the key considerations we’ll examine in this chapter:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes最初关注的是无状态应用程序的扩展和编排。正如你在本书中学到的，Kubernetes正在演变以支持有状态应用程序。通过将更多工作负载移入虚拟数据中心，提高运行效率的承诺具有很高的激励作用。分析领域可以利用在减少无状态和有状态工作负载操作负担方面取得的进展。然而，Kubernetes在管理分析工作负载方面面临一些独特的挑战；许多问题仍在解决中。Kubernetes需要哪些功能来完善数据视图，并使分析工作负载达到与微服务和数据库等堆栈其他部分相当的水平？以下是本章将要探讨的一些关键考虑因素：
- en: Orderly execution
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有序执行
- en: An essential aspect of analytic workloads is the order of operations required
    to analyze large volumes of data. This involves far more than just making sure
    Pods are started with the proper storage and networking resources. It also includes
    a mapping of the application with the orderly execution run in each Pod. The Kubernetes
    component primarily responsible for this task is `kube-scheduler` (see [Chapter 5](ch05.html#automating_database_management_on_kuber)),
    but the controllers for Jobs and CronJobs are involved as well. This is a particular
    area of attention for the Kubernetes communities focusing on analytics, which
    we will further cover in the chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工作负载的一个重要方面是分析大量数据所需的操作顺序。这不仅仅是确保Pod以适当的存储和网络资源启动，还包括应用程序与每个Pod中有序执行运行的映射。在这项任务中，Kubernetes组件主要负责的是`kube-scheduler`（参见[第5章](ch05.html#automating_database_management_on_kuber)），但Jobs和CronJobs的控制器也参与其中。这是Kubernetes社区在关注分析时的一个特定领域，我们将在本章进一步讨论。
- en: Storage management
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 存储管理
- en: Analytic workloads use ephemeral and persistent storage in different Jobs that
    process data. The real trouble occurs when it comes to identifying and selecting
    the right storage per Job. Many analytic workloads require ephemeral storage for
    short periods and more-efficient (cheaper) persistent storage for long terms.
    As you learned in [Chapter 2](ch02.html#managing_data_storage_on_kubernetes),
    Kubernetes storage has greatly increased maturity. Analytics projects that run
    on Kubernetes need to take advantage of the work already done with stateful workloads
    and continue to partner with the Kubernetes community for future enhancements
    in areas like StorageClasses and different access patterns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工作负载在处理数据的不同作业中使用临时和持久存储。真正的麻烦出现在识别和选择每个作业所需的正确存储时。许多分析工作负载需要临时存储来进行短期处理，以及更高效（更便宜）的持久存储来进行长期处理。正如你在[第2章](ch02.html#managing_data_storage_on_kubernetes)学到的，Kubernetes存储已经显著成熟。在Kubernetes上运行的分析项目需要利用已经在有状态工作负载上完成的工作，并继续与Kubernetes社区合作，以改进StorageClasses和不同访问模式等领域的未来增强功能。
- en: Efficient use of resources
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 资源的高效利用
- en: There is an old saying that “everything counts in large amounts,” and nothing
    makes that more evident than analytics. A Job may require 1,000 Pods for 10 minutes,
    but what if it needs 10,000? That’s a challenging problem for the Kubernetes control
    plane. Another Job might require terabytes of swap disk space that is needed only
    for the duration of a Job. In a cloud native world, Jobs should be able to quickly
    allocate the resources they need and release the resources when finished. Making
    these operations as efficient as possible saves time and, more importantly, money.
    The fast and bursty nature of analytics has created some challenges for the Kubernetes
    API server and scheduler to keep up with all the Jobs that need to be run. Several
    of those challenges are already being addressed, as discussed later in the chapter,
    and some are still a work in progress.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有句老话说，“大量事物的每一个都重要”，在分析中尤为明显。一项作业可能需要1,000个Pods运行10分钟，但如果需要10,000个呢？这对于Kubernetes控制平面是一个具有挑战性的问题。另一项作业可能需要数TB的交换磁盘空间，而这些空间仅在作业执行期间需要。在云原生世界中，作业应能快速分配所需资源，并在完成后释放这些资源。尽可能提高这些操作的效率不仅节省时间，更重要的是节省资金。分析的快速和爆发性特性给Kubernetes
    API服务器和调度器带来了一些挑战，以保持跟上需要运行的所有作业。如本章后续讨论，其中一些挑战已在处理中，而一些仍在进行中。
- en: Those are the challenges, but none of them are showstoppers that will get in
    the way of our dream of a complete cloud native stack deployed as a single virtual
    datacenter in Kubernetes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是挑战，但没有一项是妨碍我们在Kubernetes中部署作为单一虚拟数据中心的完整云原生堆栈的梦想的绊脚石。
- en: Engineers can be their own worst enemies. Often when we go to solve one problem,
    it creates a few more that need to be solved. We can count this as progress, however,
    when it comes to managing data. Every step up we take, despite the challenges,
    allows for new solutions that were never available before. It’s a staggering thought—today,
    a small number of people can perform analytics tasks that massive teams would
    not have been able to accomplish just a few years ago. See the quote about laziness
    at the beginning of the chapter. There is still work to be done, and next we will
    look at the tools available for analyzing data in Kubernetes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师们有时是自己最大的敌人。通常情况下，解决一个问题可能会带来更多需要解决的问题。然而，这在处理数据时可以看作是进步。尽管面临诸多挑战，但我们每一步的提升都为以前无法实现的新解决方案铺平了道路。今天，少数人可以完成曾几何时庞大团队无法完成的分析任务，这令人震惊。请参阅本章开头关于懒惰的引言。尽管仍需努力，接下来我们将看看在Kubernetes中分析数据的可用工具。
- en: Introduction to Apache Spark
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark简介
- en: 'Google changed the world of data analytics with the MapReduce algorithm simply
    by describing it to the world in an academic paper. Not long after the [MapReduce
    paper](https://oreil.ly/mryO0) got engineers talking, an open source implementation
    was created: the now-famous Apache Hadoop. A massive ecosystem was built up around
    Hadoop with tooling and complementary projects such as Hadoop Distributed File
    System (HDFS).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌通过在学术论文中简要描述，利用MapReduce算法改变了数据分析的世界。不久之后，[MapReduce论文](https://oreil.ly/mryO0)引发了工程师们的讨论，随之而来的是一个开源实现：如今著名的Apache
    Hadoop。围绕Hadoop建立了庞大的生态系统，包括工具和补充项目，如Hadoop分布式文件系统（HDFS）。
- en: Growing pains from this fast-moving project opened the door for the next generation
    of tools that built on the lessons learned with Hadoop. One project that grew
    in popularity as an alternative to Hadoop was [Apache Spark](https://oreil.ly/BlT4i).
    Spark addressed reliability and processing efficiency problems by introducing
    the Resilient Distributed Dataset (RDD) API and Directed Acyclic Graph (DAG).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个快速发展的项目中遇到的成长阵痛为基于Hadoop的经验教训的下一代工具打开了大门。作为Hadoop替代方案，增长迅速的一个项目是[Apache Spark](https://oreil.ly/BlT4i)。Spark通过引入弹性分布式数据集（RDD）API和有向无环图（DAG）解决了可靠性和处理效率问题。
- en: The RDD was a significant improvement over the forced linear processing patterns
    of MapReduce, which involved a lot of reading from disk, processing, and then
    writing back to disk only to be redone over and over. This put the burden on developers
    to reason through how data was processed. RDDs shifted the responsibility away
    from developers as an API that created a unified view of all data while abstracting
    the actual processing details. Those details were created in a workflow to perform
    each task expressed in a DAG. The DAG is nothing more than an optimized path that
    describes data and operations to be completed in an orderly fashion until the
    final result is produced. RDDs were eventually replaced with the Dataset and DataFrame
    APIs, further enhancing developer productivity over large volumes of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 相比于 MapReduce 的强制线性处理模式是一个重大进步，后者涉及大量的磁盘读取、处理，然后再写回磁盘，反复进行。这使开发人员需要通过推理数据如何被处理来思考。RDD
    将责任从开发人员转移开来，作为一个 API 创建了所有数据的统一视图，同时抽象了实际的处理细节。这些细节以工作流的方式创建，执行每个任务的 DAG 来表达。DAG
    只是描述数据和操作以有序方式完成最终结果的优化路径。最终，RDD 被 Dataset 和 DataFrame API 取代，进一步提升了在大数据量上的开发人员生产力。
- en: Spark’s operational complexity is greatly reduced compared to Hadoop, which
    notoriously tipped the scale with the infrastructure required even for basic Jobs.
    Spark is an excellent example of one of the benefits of being a next-generation
    implementation with great hindsight. Much effort was put into simplifying Spark’s
    architecture, leveraging distributed systems concepts. The result is the three
    familiar components you should be familiar with in a Spark cluster, shown in [Figure 9-2](#components_of_a_spark_cluster).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的操作复杂性大大降低，与 Hadoop 相比，后者因其基础作业所需的基础设施而臭名昭著。Spark 是下一代实现的一个极好例子，具有极为深刻的远见。在简化
    Spark 架构方面付出了大量努力，利用分布式系统的概念。结果就是你在 Spark 集群中应该熟悉的三个常见组件，如图 [9-2](#components_of_a_spark_cluster)
    所示。
- en: '![Components of a Spark cluster](assets/mcdk_0902.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 集群的组件](assets/mcdk_0902.png)'
- en: Figure 9-2\. Components of a Spark cluster
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. Spark 集群的组件
- en: 'Let’s review the responsibilities of each of these components:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些组件各自的职责：
- en: Cluster Manager
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器
- en: The Cluster Manager is the central hub for activity in the Spark cluster where
    new Jobs are submitted for processing. The Cluster Manager also acquires the resources
    needed to complete the task submitted. Different versions of the Cluster Manager
    are primarily based on how resources are managed (standalone, YARN, Mesos, and
    Kubernetes). The Cluster Manager is critical for deploying your Spark application
    using Kubernetes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器是 Spark 集群中活动的中心枢纽，新作业在此提交进行处理。集群管理器还获取完成所提交任务所需的资源。集群管理器的不同版本主要基于资源管理方式（独立部署、YARN、Mesos
    和 Kubernetes）。在使用 Kubernetes 部署你的 Spark 应用程序时，集群管理器至关重要。
- en: Worker Node
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点
- en: When Spark Jobs run, they are broken into manageable pieces by the Cluster Manager
    and handed to the Worker Nodes to perform the processing. They serve as the local
    manager for hardware resources as a single point of contact. Worker Nodes invoke
    and manage Spark Executors.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Spark 作业运行时，集群管理器将其分解为可管理的部分，并交给工作节点进行处理。它们作为硬件资源的本地管理器，是单一联系点。工作节点调用和管理 Spark
    执行器。
- en: Spark Executor
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 执行器
- en: Each application sent to a Worker Node will get its own Spark Executor. Each
    Executor is a standalone JVM process that operates independently and communicates
    back with the Worker Node. The tasks for the application are broken into threads
    that consume the compute resources allocated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个发送到工作节点的应用程序将获得自己的 Spark 执行器。每个执行器是一个独立的 JVM 进程，独立运行并与工作节点进行通信。为应用程序的任务分配的计算资源被分解为线程消耗。
- en: These are the traditional components of Spark as designed early in the project.
    We’ll see that the need to deploy a cloud native version of Spark forced some
    architectural evolution. The fundamentals are the same, but the execution framework
    has adapted to take advantage of what Kubernetes provides and eliminate duplication
    in orchestration overhead. In the next section, we’ll look at those changes and
    how to work with Spark in Kubernetes.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 Spark 项目早期设计的传统组件。我们将看到，部署云原生版本的 Spark 的需求迫使一些架构进行了演进。基础原则保持不变，但执行框架已适应利用
    Kubernetes 提供的优势并消除编排开销中的重复。在接下来的部分中，我们将看到这些变化以及如何在 Kubernetes 中使用 Spark。
- en: Deploying Apache Spark in Kubernetes
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中部署Apache Spark
- en: As of Apache Spark version 2.3, Kubernetes is one of the supported modes in
    the Cluster Manager. It would be easy to understate what that has meant for Spark
    as a cloud native analytics tool. Starting with Spark 3.1, Kubernetes mode is
    considered production-ready, continually adding steady improvements. When the
    Spark project looked at what it takes to run a clustered analytics system inside
    a cluster orchestration platform, a lot of overlaps became obvious. Kubernetes
    already had the mechanisms in place for the lifecycle management of containers
    and the dynamic provisioning and deprovisioning of compute elements, so Spark
    lets Kubernetes take care of this work. The redundant parts were removed, and
    Spark is closer to the way Kubernetes works as a result. The `spark-submit` command-line
    tool was extended to interface with Kubernetes clusters using the Kubernetes API,
    maintaining a familiar toolchain for developers and data engineers. These unique
    aspects of a Spark deployment in Kubernetes are shown in [Figure 9-3](#spark_on_kubernetes).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自Apache Spark 2.3版本起，Kubernetes是集群管理器中支持的模式之一。这对于作为云原生分析工具的Spark意味着什么可能轻易地被低估了。从Spark
    3.1开始，Kubernetes模式被认为是可以投入生产的，不断增加稳定的改进。当Spark项目考虑在集群编排平台内运行集群分析系统时，许多重叠之处变得显而易见。Kubernetes已经具备了容器的生命周期管理、计算元素的动态提供和取消提供的机制，因此Spark让Kubernetes来处理这些工作。多余的部分被移除，Spark与Kubernetes的工作方式更为接近。`spark-submit`命令行工具通过使用Kubernetes
    API与Kubernetes集群进行交互，为开发人员和数据工程师提供了一个熟悉的工具链。这些在Kubernetes中部署Spark时的独特方面显示在[图9-3](#spark_on_kubernetes)中。
- en: '![Spark on Kubernetes](assets/mcdk_0903.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Spark on Kubernetes](assets/mcdk_0903.png)'
- en: Figure 9-3\. Spark on Kubernetes
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. Kubernetes中的Spark
- en: 'Let’s highlight a few of the differences:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来突出几个不同之处：
- en: Spark Driver
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Driver
- en: The dedicated Cluster Manager of a standalone Spark cluster is replaced with
    native Kubernetes cluster management and the Spark Driver for Spark-specific management.
    The Spark Driver Pod is created when the Kubernetes API server receives a Job
    from the `spark-submit` tool. It invokes the Spark Executor Pods to satisfy the
    Job requirements. It is also responsible for cleaning up Executor Pods after the
    Job, making it a crucial part of elastic workloads.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 独立的Spark集群的专用集群管理器被原生Kubernetes集群管理替代，Spark特定的管理由Spark Driver负责。当Kubernetes
    API服务器从`spark-submit`工具接收到作业时，将创建Spark Driver Pod。它调用Spark Executor Pods来满足作业的需求。它还负责在作业结束后清理Executor
    Pods，使其成为弹性工作负载的重要组成部分。
- en: Spark Executor
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Executor
- en: Like a standalone Spark cluster, Executors are where the work gets done and
    where the most compute resources are consumed. Invoked from the Spark Driver,
    they take Job instructions passed by `spark-submit` with details such as CPU and
    memory limits, storage information, and security credentials. The containers used
    in Executor Pods are pre-created by the user.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 就像独立的Spark集群一样，Executors是工作的执行地点，也是消耗最多计算资源的地方。它们由Spark Driver调用，接收由`spark-submit`传递的作业指令，包括CPU和内存限制、存储信息以及安全凭证。Executor
    Pods中使用的容器是用户预先创建的。
- en: Custom Executor container
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 定制Executor容器
- en: Before a Job is sent for processing using `spark-submit`, users must build a
    custom container image tailored to meet the application requirements. The Spark
    distribution download contains a Dockerfile that can be customized and used in
    conjunction with the *docker-image-tool.sh* script to build and upload the container
    required when submitting a Spark Job in Kubernetes. The custom container has everything
    it needs to work within a Kubernetes environment, like a Spark Executor based
    on the Spark distribution version required.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`spark-submit`发送作业进行处理之前，用户必须构建一个定制的容器镜像，以满足应用程序的要求。Spark发行版下载包含一个Dockerfile，可以根据需要进行定制，并与*docker-image-tool.sh*脚本一起使用，用于在Kubernetes中提交Spark作业时构建和上传所需的容器。定制容器包含了在Kubernetes环境中运行所需的一切，例如基于所需的Spark发行版版本的Spark
    Executor。
- en: The workflow for preparing and running Spark Jobs when using Kubernetes and
    defaults can be relatively simple, requiring only a couple of steps. This is especially
    true if you are already familiar with and running Spark in production. You will
    need a running Kubernetes cluster and a download of Spark in a local filepath
    along with your Spark application source code.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes和默认设置准备和运行Spark作业的工作流可能相对简单，只需几个步骤即可。如果您已经熟悉并在生产环境中运行Spark，则尤其如此。您需要一个运行中的Kubernetes集群，以及在本地文件路径中下载的Spark和Spark应用程序源代码。
- en: Build Your Custom Container
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建您的自定义容器
- en: 'An executor container encapsulates your application and the runtime needed
    to act as an executor Pod. The build script takes an argument for the source code
    repository and a tag assignment for the output image when pushed to your Docker
    registry:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Executor 容器封装了您的应用程序以及作为执行器 Pod 所需的运行时。构建脚本需要一个参数用于源代码仓库，并在推送到 Docker 注册表时为输出镜像分配标签：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will be a Docker image with a JAR file containing your application
    code. You will then need to push this image to your Docker registry:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是一个包含您的应用程序代码的 JAR 文件的 Docker 镜像。然后，您需要将此镜像推送到您的 Docker 注册表中：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Docker Image Tags
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 镜像标签
- en: Be mindful that your tag name is labeled and versioned correctly. Reusing the
    same tag name in production could have unintended consequences, as some of us
    have learned from experience.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，标签名称必须正确标记和版本化。在生产中重复使用相同的标签名称可能会导致意外后果，正如我们有些人通过经验学到的那样。
- en: Submit and Run Your Application
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提交并运行您的应用程序
- en: 'Once the Docker image is pushed to the repo, use `**spark-submit**` to start
    the process of running the Spark application inside Kubernetes. This is the same
    `spark-submit` used for other modes, so many of the same arguments are used. This
    corresponds to (1) in [Figure 9-3](#spark_on_kubernetes):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Docker 镜像推送到仓库，使用 `**spark-submit**` 启动在 Kubernetes 中运行 Spark 应用程序的过程。这与其他模式中使用的
    `spark-submit` 相同，因此使用许多相同的参数。这对应于[图 9-3](#spark_on_kubernetes)中的 (1)：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Quite a few things are happening here, but the *most important* is in the `--master`
    parameter. To indicate this is for Kubernetes, the URL in the argument must start
    with `k8s://` and point to the API server in the default Kubernetes cluster specified
    in your local *.kubeconfig* file. The `*<spark-image>*` is the Docker image you
    created in (1), and the application path refers to your application stored inside
    the image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了很多事情，但*最重要*的在于 `--master` 参数。为了指示这是为 Kubernetes 准备的，参数中的 URL 必须以 `k8s://`
    开头，并指向本地 *.kubeconfig* 文件中指定的默认 Kubernetes 集群中的 API 服务器。`*<spark-image>*` 是您在
    (1) 中创建的 Docker 镜像，应用程序路径指的是镜像中存储的应用程序。
- en: Next is (2), where `spark-submit` interacts with the Kubernetes cluster to schedule
    the Spark Driver Pod (3) and (4). The Spark Driver parses the Job parameters and
    works with the Kubernetes scheduler to set up Spark Executor Pods (5), (6), and
    (7) to run the application code contained in the customer container image. The
    application will run to completion, and eventually the Pod used will be terminated
    and resources returned to the Kubernetes cluster in a process called *garbage
    collection*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 (2)，其中 `spark-submit` 与 Kubernetes 集群交互以安排 Spark Driver Pod (3) 和 (4)。Spark
    Driver 解析作业参数，并与 Kubernetes 调度程序一起设置 Spark Executor Pods (5)、(6) 和 (7) 来运行包含在客户容器镜像中的应用程序代码。应用程序将运行完成，最终使用的
    Pod 将被终止，并将资源返回到 Kubernetes 集群中，这个过程称为*垃圾收集*。
- en: This is just an overview of how Spark natively works with Kubernetes. Please
    refer to the [official documentation](https://oreil.ly/upJeL) to go into much
    greater detail. There are many ways to customize the arguments and parameters
    to best fit your specific needs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是 Spark 在 Kubernetes 中的本机工作方式的概述。请参阅[官方文档](https://oreil.ly/upJeL)以获取更详细的信息。有许多方法可以自定义参数和参数，以最佳适应您的特定需求。
- en: Security Considerations when Running Spark in Kubernetes
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上运行 Spark 时的安全考虑
- en: Security is not enabled by default when using Spark in Kubernetes. The first
    line of defense is authentication. Production Spark applications should use the
    built-in authentication in Spark to ensure that the users and processes accessing
    your application are the ones you intended.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Kubernetes 中使用 Spark 时，默认情况下未启用安全性。第一道防线是认证。生产中的 Spark 应用程序应使用 Spark 中的内置认证功能，以确保访问应用程序的用户和进程是您预期的。
- en: When creating a container for your application, the Spark documentation highly
    recommends changing the `USER` directive to an unprivileged unique identifier
    (UID) and group identifier (GID) to mitigate against privilege escalation attacks.
    This can also be accomplished with a SecurityContext inside the Pod template file
    provided as a parameter to `spark-submit`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 创建应用程序容器时，Spark 文档强烈建议将 `USER` 指令更改为非特权唯一标识符（UID）和组标识符（GID），以减少特权升级攻击的风险。这也可以通过作为
    `spark-submit` 参数提供的 Pod 模板文件中的 SecurityContext 来完成。
- en: Storage access should also be restricted with the Spark Driver and Spark Executor.
    Specifically, you should limit the paths that can be accessed by the running application
    to eliminate any accidental access in the event of a vulnerability. These can
    be set inside a PodSecurityAdmission, which the Spark documentation [recommends](https://oreil.ly/xQcom).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 也应限制与Spark Driver和Spark Executor的存储访问。具体来说，应限制可以被运行应用程序访问的路径，以消除在漏洞事件中的任意访问。这些可以在PodSecurityAdmission内设置，Spark文档[建议](https://oreil.ly/xQcom)。
- en: For optimal security of your Spark applications, use the security primitives
    Kubernetes provides and customize the defaults for your environment. The best
    security is the one you don’t have to think about. If you are an SRE, this is
    one of the best things you can do for your developers and data engineers. Default
    secure!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于优化您的Spark应用程序的安全性，请使用Kubernetes提供的安全原语，并根据您的环境自定义默认设置。最好的安全性是您无需考虑的安全性。如果您是SRE，这是您可以为开发人员和数据工程师做的最好的事情之一。默认安全！
- en: Kubernetes Operator for Apache Spark
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark的Kubernetes操作员
- en: If Spark can run in Kubernetes via `spark-submit`, why do we need an operator?
    As you learned in previous chapters, Kubernetes operators give you more flexibility
    in managing applications and a more cloud native experience overall. Using `spark-submit`
    to run your Spark applications requires your production systems to be set up with
    a local installation of Spark, including all dependencies. The Spark on Kubernetes
    Operator allows SREs and developers to manage park applications declaratively
    using Kubernetes tools such as Helm and `kubectl`. It also allows better observability
    on running Jobs and exporting metrics to external systems like Prometheus. Finally,
    using the operator provides an experience much closer to running other applications
    in Kubernetes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Spark可以通过`spark-submit`在Kubernetes中运行，为什么我们需要一个操作员呢？正如您在前面章节中学到的，Kubernetes操作员为您管理应用程序提供了更多的灵活性，并且总体上提供了更云原生的体验。使用`spark-submit`来运行您的Spark应用程序需要在生产系统中设置具有本地安装Spark及其所有依赖项。Spark
    on Kubernetes Operator允许SRE和开发人员使用Kubernetes工具（如Helm和`kubectl`）以声明方式管理Spark应用程序。它还允许更好地观察正在运行的作业并将指标导出到外部系统如Prometheus。最后，使用操作员提供了一个体验，更接近于在Kubernetes中运行其他应用程序。
- en: 'The first step is to install the operator into your Kubernetes cluster using
    Helm:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用Helm将操作员安装到您的Kubernetes集群中：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once completed, you will have a SparkApplication controller running and looking
    for SparkApplication objects. This is the first big departure from `spark-submit`.
    Instead of a long list of command-line arguments, you use the SparkApplication
    CRD to define the Spark Job in a YAML file. Let’s look at a config file from the
    [Spark on Kubernetes Operator documentation](https://oreil.ly/quNfG):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您将有一个运行中的SparkApplication控制器，并且会查找SparkApplication对象。这是与`spark-submit`的第一个重大分歧。不再使用长长的命令行参数列表，而是使用SparkApplication
    CRD在YAML文件中定义Spark作业。让我们看一下来自[Spark on Kubernetes Operator文档](https://oreil.ly/quNfG)的配置文件：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `spec:` section is similar to the parameters you passed in `spark-submit`
    with details about your application. The most important is the location of the
    container image. This example uses a default Spark container with the `spark-examples`
    JAR file preinstalled. You will need to use *docker-image-tool.sh* to build the
    image for your application as described in [“Build Your Custom Container”](#build_your_custom_container),
    and modify the `mainClass` and `mainApplicationFile` as appropriate for your application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec:`部分类似于您在`spark-submit`中传递的应用程序参数，包含有关您的应用程序的详细信息。最重要的是容器镜像的位置。本示例使用了预安装了`spark-examples`
    JAR文件的默认Spark容器。您将需要使用*docker-image-tool.sh*根据应用程序的要求构建镜像，如[“构建您的自定义容器”](#build_your_custom_container)中所述，并根据应用程序的需求修改`mainClass`和`mainApplicationFile`。'
- en: Two other notable fields under `spec` are `driver` and `executor`. These provide
    the specifications for the Spark Driver Pods and Spark Executor Pods that the
    Spark Operator will deploy. For `driver`, only one core is required, but CPU and
    memory allocations need to be enough to maintain the number of executors you require.
    The number is set in the `executor` section under `instances`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec`下的另外两个显著字段是`driver`和`executor`。它们为Spark Driver Pods和Spark Executor Pods的规范提供了说明，这些由Spark
    Operator部署。对于`driver`，只需要一个核心，但是CPU和内存分配需要足够以维持所需的执行器数量。此数字在`executor`部分的`instances`下设置。'
- en: Minding Your Resources
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意资源管理
- en: For resource management, the requests you make under `driver` and `spec` need
    to be carefully considered for resource management. The number of instances plus
    their allocated CPU and memory could use up resources quickly. Jobs can hang indefinitely
    while waiting for resources to free up, which may never happen.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于资源管理，在 `driver` 和 `spec` 下进行的请求需要仔细考虑资源管理。实例的数量以及其分配的 CPU 和内存可能会迅速耗尽资源。作业可能会无限期地挂起，等待资源释放，而这可能永远不会发生。
- en: Now that your configuration YAML is ready, it’s time to put it into action.
    For a walk-through, refer to [Figure 9-4](#spark_on_kubernetes_operator).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的配置 YAML 已准备就绪，是时候将其投入实际应用了。有关操作步骤，请参考 [图 9-4](#spark_on_kubernetes_operator)。
- en: '![Spark on Kubernetes Operator](assets/mcdk_0904.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Spark on Kubernetes Operator](assets/mcdk_0904.png)'
- en: Figure 9-4\. Spark on Kubernetes Operator
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. Spark on Kubernetes Operator
- en: First, use `kubectl apply -f *<filename>*` (1) to apply the SparkApplication
    into your running Kubernetes cluster (2). The Spark Operator listens for new applications
    (3), and when a new config object is applied, the submission runner controller
    begins the tasks of building out the required Pods. From here the actions taken
    in the Kubernetes cluster are the same as if you used `spark-submit`, with all
    of the parameters being supplied in this case via the SparkApplication YAML. The
    submission runner starts the Spark Driver Pod (4) which in turn directs the Spark
    Executor Pods (5), which runs the application code to completion. The Pod monitor
    included in the Spark Operator exports Spark metrics to observability tools such
    as Prometheus.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用 `kubectl apply -f *<filename>*`（1）将 SparkApplication 应用到正在运行的 Kubernetes
    集群中（2）。Spark Operator 会监听新应用程序（3），当应用新配置对象时，提交运行控制器开始构建所需的 Pod。从这里开始，在 Kubernetes
    集群中执行的操作与直接使用 `spark-submit` 完全相同，但本例中所有参数都通过 SparkApplication YAML 提供。提交运行器启动
    Spark Driver Pod（4），然后指导 Spark Executor Pods（5），执行应用程序代码直至完成。包含在 Spark Operator
    中的 Pod 监视器将 Spark 指标导出到 Prometheus 等可观察性工具。
- en: The Spark Operator fills in the gaps between the way `spark-submit` works and
    the way SREs and developers typically deploy applications into Kubernetes. This
    was a long answer to the question posed at the beginning of this section. We need
    an operator to make using Spark more cloud native and thus more manageable in
    the long run. The cloud native way of doing things includes taking a declarative
    approach to managing resources and making those resources observable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Operator 填补了 `spark-submit` 的工作方式与 SRE 和开发人员通常在 Kubernetes 中部署应用程序的方式之间的差距。这是对本节开头提出的问题的长篇回答。我们需要一个操作员来使使用
    Spark 在云原生环境中更加便捷，从而在长期运行中更易管理。云原生的做法包括采用声明式方法来管理资源，并使这些资源可观察。
- en: Alternative Schedulers for Kubernetes
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 的替代调度程序
- en: 'As you learned in [Chapter 5](ch05.html#automating_database_management_on_kuber),
    the Kubernetes scheduler has a basic but essential job: take requests for resources
    and assign the compute, network, and storage to satisfy the requirements. Let’s
    look at the default approach for this action, as shown in [Figure 9-5](#typical_kubernetes_scheduling).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 [第 5 章](ch05.html#automating_database_management_on_kuber) 中所学到的，Kubernetes
    调度程序的基本但至关重要的任务是接收资源请求并分配计算、网络和存储以满足要求。让我们看一下执行此操作的默认方法，如 [图 9-5](#typical_kubernetes_scheduling)
    所示。
- en: '![Typical Kubernetes scheduling](assets/mcdk_0905.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![典型的 Kubernetes 调度](assets/mcdk_0905.png)'
- en: Figure 9-5\. Typical Kubernetes scheduling
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 典型的 Kubernetes 调度
- en: A typical scheduling effort begins when you create a *deployment.yaml* file
    describing the resources required (1), including which Pod resources are needed
    and how many. When the YAML file is submitted (2) to the Kubernetes cluster API
    server using `kubectl apply`, the Pod resources are created with the supplied
    parameters and are ready for assignment to a node. Nodes have the needed pool
    of resources, and it’s the job of `kube-scheduler` to be the matchmaker between
    nodes and Pods. The scheduler performs state matching whenever a new Pod resource
    is created (3), and checks whether the Pod has an assigned node. If not, the scheduler
    makes the calculations needed to find an available node. It examines the requirements
    for the Pod, scores the available nodes using an internal set of rules, and selects
    a node to run the Pod (4). This is where the real work of container orchestration
    in Kubernetes gets done.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的调度工作从创建描述所需资源的 *deployment.yaml* 文件开始（1），包括需要哪些 Pod 资源以及数量。当提交 YAML 文件（2）到
    Kubernetes 集群的 API 服务器时，使用 `kubectl apply`，Pod 资源将以提供的参数创建，并准备好分配给节点。节点具有所需的资源池，`kube-scheduler`
    的任务是在节点和 Pod 之间进行匹配。调度器在创建新的 Pod 资源时（3）执行状态匹配，并检查该 Pod 是否已分配了节点。如果没有，调度器将进行必要的计算以找到可用的节点。它检查
    Pod 的需求，使用内部一组规则对可用节点进行评分，并选择一个节点来运行该 Pod（4）。这就是 Kubernetes 中容器编排的真正工作场所。
- en: 'However, we have a problem with analytic workloads: the default Kubernetes
    scheduler was not designed for batch workloads. The design is just too basic to
    work the way that’s needed for analytics. As mentioned in [“Analytics on Kubernetes
    Is the Next Frontier”](#analytics_on_kubernetes_is_the_next_fro), Kubernetes was
    built for the needs of stateless workloads. These are long-running processes that
    may expand or contract over time but tend to remain relatively static. Analytic
    applications such as Spark are different, requiring the scheduling of potentially
    thousands of short-lived jobs.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在分析工作负载方面遇到了问题：默认的 Kubernetes 调度器并未设计用于批处理工作负载。设计过于基础，无法满足分析所需的方式。正如在 [“在
    Kubernetes 上进行分析是下一个前沿”](#analytics_on_kubernetes_is_the_next_fro) 中提到的，Kubernetes
    是为无状态工作负载的需求而构建的。这些是长时间运行的进程，虽然随着时间的推移可能会扩展或收缩，但倾向于保持相对静态。而像 Spark 这样的分析应用程序则不同，需要调度可能数以千计的短暂作业。
- en: Thankfully, the developers of Kubernetes anticipated expanded requirements for
    future scheduling needs and made it possible for users to specify their scheduler
    in a configuration, bypassing the default scheduling approach.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes 的开发者预见到未来调度需求的扩展，并允许用户在配置中指定他们的调度器，绕过默认的调度方法。
- en: 'The strong desire to manage the entire application stack with a common control
    plane has been an innovation driver. As demonstrated in [“Deploying Apache Spark
    in Kubernetes”](#deploying_apache_spark_in_kubernetes), Spark has been moving
    closer to Kubernetes. In this section, we’ll look at how some teams have been
    bringing Kubernetes closer to Spark by building more appropriate schedulers. Two
    open source projects are leading the way in this effort: Volcano and Apache YuniKorn.
    These schedulers share similar guiding principles that make them more appropriate
    for batch workloads by providing the following alternative features:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈的愿望通过共同的控制平面管理整个应用程序栈一直是创新的推动力。正如在 [“在 Kubernetes 上部署 Apache Spark”](#deploying_apache_spark_in_kubernetes)
    中展示的那样，Spark 正在向 Kubernetes 靠拢。在本节中，我们将看看一些团队如何通过构建更合适的调度器将 Kubernetes 与 Spark
    更紧密地结合起来。两个开源项目在这方面处于领先地位：Volcano 和 Apache YuniKorn。这些调度器共享相似的指导原则，通过提供以下替代特性，使它们更适合批处理工作负载：
- en: Multitenant resource management
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户资源管理
- en: The default Kubernetes scheduler allocates Pods as requested until no more available
    resources match Pod requirements. Both YuniKorn and Volcano provide a wide variety
    of resourcing modes to match your application needs better, especially in multitenant
    environments. Fairness in resource management prevents one analytic Job from starving
    out other Jobs for required resources. As these Jobs are scheduled, the entire
    resource pool is considered to balance utilization based on priority and throughput.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 Kubernetes 调度器根据请求分配 Pod，直到没有更多可用的资源与 Pod 的要求匹配。YuniKorn 和 Volcano 都提供了多种资源模式，以更好地满足您的应用需求，特别是在多租户环境中。资源管理的公平性可以防止一个分析作业因所需资源而饿死其他作业。在调度这些作业时，整个资源池都会考虑到基于优先级和吞吐量的平衡利用。
- en: '*Gang scheduling* adds another layer of intelligence. If a submitted Job needs
    a certain amount of resources, it doesn’t make sense to start the Job if every
    Pod can’t be started. The default scheduler will start Pods until the cluster
    runs out of resources, potentially stranding Jobs as they wait for more Pods to
    come online. Gang scheduling implements an all-or-nothing approach, as Jobs will
    start only when all resources needed are available for the complete Job.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*团队调度* 添加了另一层智能。如果提交的作业需要一定数量的资源，如果不能启动每个 Pod，启动作业就没有意义。默认调度器会启动 Pod，直到集群资源耗尽，可能会导致作业等待更多
    Pod 上线。团队调度器实施全有或全无的方式，只有当所有需要的资源对于完整的作业都可用时，作业才会启动。'
- en: Job queue management
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作业队列管理
- en: Smarter queue management can also lead to better resource management. If one
    Job needs few resources and can be run while larger Jobs are being run, the scheduler
    can fit the Job in and therefore increase the Kubernetes cluster’s overall throughput.
    In some cases, users need control over what Jobs have priority and which can preempt
    or pause other running Jobs as they are submitted. Queues can be reordered or
    reprioritized after Jobs are submitted. Observability tooling provides queue insights
    that help determine total cluster health and resource usage.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 更智能的队列管理也能导致更好的资源管理。如果一个作业只需要少量资源，并且可以在运行更大作业的同时运行，调度器可以安排这个作业，从而提高 Kubernetes
    集群的整体吞吐量。在某些情况下，用户需要控制哪些作业优先级高，并且哪些作业可以在提交时抢占或暂停其他正在运行的作业。作业提交后可以重新排序或重新设置优先级。可观察性工具提供队列洞察，帮助确定整个集群的健康状况和资源使用情况。
- en: If you are considering a production deployment of analytic workloads, you should
    avoid using the default scheduler, `kube-scheduler`. It wasn’t designed for your
    needs in this case. Starting with a better scheduler lets you future-proof your
    Kubernetes experience. Let’s examine some highlights of each scheduler.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑将分析工作负载投入生产环境，你应避免使用默认的调度器，`kube-scheduler`。在这种情况下，它并不适合你的需求。选择一个更好的调度器可以让你为
    Kubernetes 的体验未来做好准备。让我们来看看每个调度器的一些亮点。
- en: Apache YuniKorn
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache YuniKorn
- en: The *YuniKorn project* was built by engineers from Cloudera out of the operational
    frustration of working with analytic workloads in Spark. In the spirit of using
    open source to solve problems as a community, YuniKorn was donated to the Apache
    Software Foundation and accepted as an incubating project in 2020\. The name comes
    directly from the two systems it supports, YARN and Kubernetes. (Y unified K.
    YuniKorn. Get it?) It addresses the specific resource management and user control
    needs of analytic workloads from a Spark cluster administration point of view.
    YuniKorn also added support for TensorFlow and Flink jobs with the same level
    of resource control. No doubt, this support was born of the same operation frustrations
    found in Spark.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*YuniKorn 项目* 是由 Cloudera 的工程师基于在 Spark 中处理分析工作负载时的操作困难构建而成的。在使用开源解决问题的社区精神下，YuniKorn
    于 2020 年被捐赠给 Apache 软件基金会，并作为孵化项目被接受。名称直接来自其支持的两个系统，YARN 和 Kubernetes。（Y 统一 K.
    YuniKorn. 明白了吧？）它从 Spark 集群管理的角度解决了分析工作负载的特定资源管理和用户控制需求。YuniKorn 还增加了对 TensorFlow
    和 Flink 作业的支持，并具有相同水平的资源控制。毫无疑问，这种支持源于在 Spark 中找到的相同操作困难。'
- en: YuniKorn is [installed](https://yunikorn.apache.org/docs) in Kubernetes using
    Helm. The goal of YuniKorn is to transform your Kubernetes cluster into a place
    that is friendly to the resource requirements of batch Jobs. A key part of that
    transformation is replacing the default `kube-scheduler`. To demonstrate how,
    let’s use [Figure 9-6](#yunikorn_architecture) to walk through the components.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Helm 在 Kubernetes 中安装 YuniKorn。YuniKorn 的目标是将你的 Kubernetes 集群转变为一个对批处理作业资源需求友好的地方。这种转变的关键部分是替换默认的
    `kube-scheduler`。为了演示这一点，让我们使用 [图 9-6](#yunikorn_architecture) 来逐步了解其组件。
- en: 'YuniKorn is meant to be a drop-in scheduler replacement with minimal changes
    to your existing Spark workflow, so we will start there. When new resource requests
    (1) are sent to the Kubernetes API server via `spark-submit` (2), the default
    `kube-scheduler` (3) is typically used to match Pods and nodes. When YuniKorn
    is deployed in your cluster, an `admissions-controller` Pod is created. The function
    of the `admissions-controller` is to listen for new resource requests (4) and
    make a small change, adding `schedulerName: yunikorn` to the resource request.
    If you need more fine-grained control, you can disable the `admissions-controller`
    and enable YuniKorn on a per Job basis by manually adding the following line to
    the SparkApplication YAML:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'YuniKorn旨在成为现有Spark工作流程最小更改的调度器替代方案的一部分，因此我们将从这里开始。当新的资源请求（1）通过`spark-submit`（2）发送到Kubernetes
    API服务器时，默认使用`kube-scheduler`（3）来匹配Pod和节点。当在您的集群中部署YuniKorn时，将创建一个`admissions-controller`
    Pod。`admissions-controller`的功能是监听新的资源请求（4），并进行小的更改，将`schedulerName: yunikorn`添加到资源请求中。如果需要更精细的控制，您可以禁用`admissions-controller`并通过手动将以下行添加到SparkApplication
    YAML来按作业基础启用YuniKorn：'
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![YuniKorn architecture](assets/mcdk_0906.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![YuniKorn架构](assets/mcdk_0906.png)'
- en: Figure 9-6\. YuniKorn architecture
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. YuniKorn架构
- en: All scheduling needs will now be handled by the YuniKorn Scheduler (5). YuniKorn
    is built to run with multiple orchestration engines and provides an API translation
    layer called *Kubernetes shim* to manage communication between Kubernetes and
    the YuniKorn core (6). yunikorn-core extends the basic filter and score algorithm
    available in the default `kube-scheduler` by adding options appropriate for batch
    workloads such as Spark. These options range from simple resource-based queues
    to more advanced hierarchical queue management that allows for queues and resource
    pools to map to organizational structures. Hierarchical pooling can be helpful
    for those with a massive analytics footprint across many parts of a large enterprise
    and is critical for multitenant environments when running in a single Kubernetes
    cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所有调度需求现在将由YuniKorn Scheduler（5）处理。YuniKorn被设计为与多个编排引擎一起运行，并提供一个称为*Kubernetes
    shim*的API转换层，用于管理Kubernetes与YuniKorn核心之间的通信（6）。yunikorn-core通过添加适用于批处理工作负载（如Spark）的选项，扩展了默认的`kube-scheduler`中可用的基本过滤器和评分算法。这些选项从简单的基于资源的队列到更高级的分层队列管理，允许队列和资源池映射到组织结构。在运行在单个Kubernetes集群中的多租户环境中，分层池对于那些在大型企业的许多部分中具有大量分析足迹的人士非常有帮助，也是至关重要的。
- en: 'YuniKorn core is [configured](https://oreil.ly/2Tp19) using the *queues.yaml*
    file, which contains all the details of how YuniKorn will schedule Pods to nodes,
    including the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*queues.yaml*文件配置YuniKorn核心，该文件包含YuniKorn调度Pod到节点的所有细节，包括以下内容：
- en: Partitions
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 分区
- en: One or more named [configuration sections](https://oreil.ly/kxtHf) for different
    application requirements.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个或多个命名的[配置部分](https://oreil.ly/kxtHf)用于不同的应用需求。
- en: Queues
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 队列
- en: Fine-grained [control](https://oreil.ly/M5W1A) over resources in a hierarchical
    arrangement to provide resource guarantees in a multitenant environment.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在分层排列中对资源进行精细化的[控制](https://oreil.ly/M5W1A)，以在多租户环境中提供资源保证。
- en: Node sort policy
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 节点排序策略
- en: How Nodes are selected by available resources. Choices are [FairnessPolicy](https://oreil.ly/RUI7q)
    and [BinPackingPolicy](https://oreil.ly/sED2J).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 节点如何通过可用资源选择。选择包括[FairnessPolicy](https://oreil.ly/RUI7q)和[BinPackingPolicy](https://oreil.ly/sED2J)。
- en: Placement Rules
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 放置规则
- en: Description and [filters](https://oreil.ly/sHvMP) for Pod placement based on
    user or group membership.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 描述和[过滤器](https://oreil.ly/sHvMP)基于用户或组成员资格进行Pod位置安置。
- en: Limits
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 限制
- en: '[Definitions](https://oreil.ly/ytYU9) for fine-grained resource limits on partitions
    or queues.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在分区或队列上设置精细化资源限制的[定义](https://oreil.ly/ytYU9)。
- en: New Jobs are processed by YuniKorn core by matching details and assigning the
    right queue. At this point, the scheduler can make a decision to assign Pods to
    nodes, which are then brought online (7).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 新作业由YuniKorn核心处理，通过匹配详细信息并分配正确的队列。在这一点上，调度程序可以决定将Pod分配给节点，然后将其上线（7）。
- en: YuniKorn also ships with an observability web-based tool called *scheduler UI*
    that provides insights into Job and queue status. It can be used to monitor scheduler
    health and provide better insights to troubleshoot any Job issues.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: YuniKorn还配备了一个名为*scheduler UI*的可观察性基于Web的工具，提供作业和队列状态的洞察。它可以用于监控调度器的健康状况，并提供更好的洞察以排除任何作业问题。
- en: Volcano
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 火山
- en: '*Volcano* was developed as a general-purpose scheduler for running high performance
    computing (HPC) workloads in Kubernetes. Volcano supports a variety of workloads,
    including Spark, Flink, PyTorch, TensorFlow, and specialized systems such as KubeGene
    for genome sequencing. Engineers built Volcano at Huawei, Tencent, and Baidu,
    to name a few of the long list of contributors. Donated to the CNCF, it was accepted
    as a Sandbox project in 2020.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*Volcano*被开发为在Kubernetes中运行高性能计算（HPC）工作负载的通用调度器。Volcano支持多种工作负载，包括Spark、Flink、PyTorch、TensorFlow以及专用系统，例如用于基因组测序的KubeGene。工程师们在华为、腾讯和百度等公司开发了Volcano，并将其捐赠给CNCF，于2020年被接受为沙盒项目。'
- en: Volcano is installed using Helm and creates CRDs for Jobs and queues, making
    the configuration a core part of your Kubernetes cluster as compared with YuniKorn,
    which is more of a bypass. This is a reflection of the general-purpose nature
    of Volcano. When installed, the Volcano scheduler is available for any process
    needing advanced scheduling and queuing. Let’s use [Figure 9-7](#volcano_architecture)
    to walk through how it works.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Helm安装Volcano并创建作业和队列的CRDs，使得配置成为Kubernetes集群的核心部分，与更多作为旁路的YuniKorn不同。这反映了Volcano的通用性质。安装后，Volcano调度器可用于任何需要高级调度和排队的进程。我们将使用[图 9-7](#volcano_architecture)来演示其工作原理。
- en: 'To use Volcano with your batch Jobs, you will need to explicitly add the scheduler
    configuration to your Job YAML file (1). If you are using Volcano for Spark, it
    is [recommended](https://volcano.sh/en/docs/spark_on_volcano) by the Volcano project
    to use the Spark Operator for Kubernetes and add one field to your SparkApplication
    YAML:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要将Volcano用于批处理作业，您需要在作业的YAML文件中明确添加调度器配置（1）。如果您正在使用Volcano来运行Spark作业，Volcano项目建议使用Kubernetes的Spark
    Operator，并在您的SparkApplication YAML中添加一个字段：
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Volcano architecture](assets/mcdk_0907.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![Volcano architecture](assets/mcdk_0907.png)'
- en: Figure 9-7\. Volcano architecture
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. Volcano架构
- en: You can then use `kubectl apply` as you normally would to submit your Job (2).
    Without specifying the Volcano scheduler, Kubernetes will match Pods and nodes
    with the default `kube-scheduler` (3).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以像平常一样使用`kubectl apply`提交您的作业（2）。如果没有指定Volcano调度器，Kubernetes将使用默认的`kube-scheduler`来匹配Pod和节点（3）。
- en: 'A Helm installation of Volcano will install the CRDs for Job, queue, and PodGroup
    and create a new Pod called Volcano Admission. Volcano Admission (4) attaches
    to the API server and validates Volcano-specific CRD entries and Jobs asking for
    the Volcano scheduler:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Helm安装Volcano将安装作业、队列和PodGroup的CRDs，并创建一个名为Volcano Admission的新Pod。Volcano
    Admission（4）连接到API服务器，并验证Volcano特定的CRD条目和要求使用Volcano调度器的作业：
- en: Job
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 工作
- en: A Volcano-specific Job with extended [configuration](https://oreil.ly/CknE0)
    for HPC.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 面向HPC的特定于Volcano的作业，具有扩展的[配置](https://oreil.ly/CknE0)。
- en: Queue
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 队列
- en: A collection of PodGroups to be managed as a first in, first out (FIFO) resource
    group. [Configuration](https://oreil.ly/MW2xq) dictates the behavior of the queue
    for different situations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 作为先进先出（FIFO）资源组管理的PodGroup集合。[配置](https://oreil.ly/MW2xq)定义了队列在不同情况下的行为。
- en: PodGroup
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: PodGroup
- en: A collection of Pods related to their purpose. Examples would be groups for
    Spark and TensorFlow with different [properties](https://oreil.ly/LzeBj) for each.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与其目的相关的Pod集合。例如，为Spark和TensorFlow创建不同的[属性](https://oreil.ly/LzeBj)的组。
- en: When selected as the scheduler for a Job (5), the Volcano scheduler will take
    the CRDs and start to work (6). Incoming Jobs marked to use Volcano as the scheduler
    are matched with a PodGroup and queue. Based on this assignment, a final node
    placement is made for each Pod (7).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 选定Volcano作为作业的调度器时（5），Volcano调度器将获取CRDs并开始工作（6）。选择使用Volcano作为调度器的传入作业将与PodGroup和队列进行匹配。基于此分配，为每个Pod进行最终的节点放置（7）。
- en: 'The cluster-specific configuration for the Volcano scheduler core is stored
    in a ConfigMap named `volcano-scheduler-configmap`. This config file contains
    two main sections: `actions` and `plugins`. [Actions](https://oreil.ly/frTKk)
    are an ordered list of each step in the node selection for each Job: enqueue,
    allocate, preempt, reclaim, and backfill. Each step is optional and can be reordered
    to match the type of work that needs to be performed.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Volcano调度器核心的集群特定配置存储在名为`volcano-scheduler-configmap`的ConfigMap中。此配置文件包含两个主要部分：`actions`和`plugins`。[Actions](https://oreil.ly/frTKk)是节点选择过程中每个作业步骤的有序列表：enqueue、allocate、preempt、reclaim和backfill。每个步骤都是可选的，并且可以根据需要重新排序。
- en: 'Plug-ins are the algorithms used to match Pods with nodes. Each has a different
    use case and purpose and can be combined as an ensemble:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 插件是用于将 Pod 与节点匹配的算法。每种算法具有不同的用例和目的，并可以作为整体组合使用：
- en: Gang
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 群组
- en: This plug-in looks for higher-priority tasks in the queue and performs preemption
    and eviction if needed to free up resources for them.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此插件在队列中查找优先级较高的任务，并在需要时执行抢占和驱逐以释放资源。
- en: BinPack
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: BinPack
- en: This is a classic algorithm for finding the best fit for using every resource
    available by mixing different-size resource requests in the most efficient manner.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个经典的算法，用于通过以最有效的方式混合不同大小的资源请求来找到使用每个可用资源的最佳适合。
- en: Conformance
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 符合性
- en: This ignores any task in the Namespace `kube-system` for eviction decisions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略 Namespace `kube-system` 中任何任务以进行驱逐决策。
- en: Dominant Resource Fairness (DRF)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 主导资源公平性（DRF）
- en: This is an algorithm to address issues of fairness across multiple resource
    types to ensure that all Jobs have equal throughput.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种算法，用于解决跨多种资源类型的公平性问题，确保所有作业具有相等的吞吐量。
- en: Proportion
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 比例
- en: This is a multitenant algorithm to allocate dedicated portions of cluster allocation
    for running Jobs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种多租户算法，为运行作业分配独立的集群分配部分。
- en: Task-topology
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 任务拓扑
- en: This algorithm uses affinity to put network-intensive Jobs physically closer
    together for more efficient network use.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法使用亲和性将网络密集型作业放置在物理上更接近以提高网络使用效率。
- en: NodeOrder
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: NodeOrder
- en: This plug-in takes multiple user-defined dimensions to score every available
    node before selection.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此插件在选择之前会使用多个用户定义的维度对每个可用节点进行评分。
- en: Predicate
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 谓词
- en: This looks for certain predicates in nodes for selection (but currently supports
    only the GPU sharing predicate).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能在节点中寻找特定的谓词以进行选择（但目前仅支持 GPU 共享谓词）。
- en: Priority
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级
- en: This plug-in chooses task priority based on the user-supplied configuration
    in `priorityClassName`, `createTime`, and `id`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此插件根据用户提供的 `priorityClassName`、`createTime` 和 `id` 配置选择任务优先级。
- en: Service level agreement (SLA)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 服务级别协议（SLA）
- en: This uses the parameter `JobWaitingTime` to allow individual Jobs the control
    over priority based on when they are needed.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能使用参数 `JobWaitingTime`，允许各个作业基于其需求时间控制优先级。
- en: Time-division multiplexing (TDM)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 分时复用（TDM）
- en: When nodes are used for both Kubernetes and YARN, TDM will schedule Pods that
    share resources in this arrangement.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点同时用于 Kubernetes 和 YARN 时，TDM 将安排共享资源的 Pod。
- en: NUMA-aware
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: NUMA 感知
- en: This provides scheduling for Pods with an awareness of CPU resource topology.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 提供带有对 CPU 资源拓扑的意识的 Pod 调度。
- en: Outside of the Kubernetes installation, Volcano also ships with a command-line
    tool called `vcctl`. Managing Volcano can be done solely through the use of `kubectl`.
    However, `vcctl` presents an interface for operators more familiar with Job control
    systems.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Kubernetes 安装外，Volcano 还附带了一个名为 `vcctl` 的命令行工具。管理 Volcano 可完全通过 `kubectl`
    完成。然而，`vcctl` 为熟悉作业控制系统的操作员提供了一个界面。
- en: As you can see from the list of features offered by YuniKorn and Volcano, having
    choices is a beautiful thing. Regardless of which project you choose, you’ll have
    a better experience running analytic workloads in Kubernetes with one of these
    alternate schedulers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从 YuniKorn 和 Volcano 提供的功能列表中可以看出，拥有选择权是一件美好的事情。无论您选择哪个项目，您都将在 Kubernetes 中运行分析工作负载时获得更好的体验。
- en: Analytic Engines for Kubernetes
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 的分析引擎
- en: Spark is a powerful tool that solves many use cases in analytics. However, having
    just a single choice can be restrictive once that tool no longer works the way
    you do. Google developed MapReduce in 2004 to fill a need for data transformation,
    such as taking a pool of data and creating a count of the things in it, and this
    is still a relevant problem today given the volumes of data we create. Even before
    MapReduce, massively parallel processing (MPP) was a popular approach for data
    analysis. These “supercomputers” consisted of rows and rows of individual computers
    presented as a single processing grid for researchers in fields such as physics
    and meteorology to run massive calculations that would take far too long on a
    single computer.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个强大的工具，解决了许多分析用例。然而，一旦该工具不再按您的方式工作，仅有一个选择可能会限制性。Google 在 2004 年开发了 MapReduce
    来满足数据转换的需求，例如对数据池进行计数等操作，这在今天依然是一个相关的问题，考虑到我们所创造的数据量。即使在 MapReduce 之前，大规模并行处理（MPP）也是数据分析的一种流行方法。这些“超级计算机”由一排排的个体计算机组成，被呈现为研究人员在物理学和气象学等领域运行大规模计算的单一处理网格。
- en: 'A similar computing need arises when tackling the ML and AI tasks in analytics:
    many processes need to analyze a large volume of data. Libraries such as TensorFlow
    require analytic tools beyond data transformation. With Kubernetes, data scientists
    and engineers can now create virtual datacenters quickly with commodity compute,
    network, and storage to rival some of the supercomputers of the past. This combination
    of technologies brings a completely new and exciting future for developers: building
    ML-based and AI-based applications based on a self-service usage model without
    having to wait for time on a very expensive supercomputer (yes, this was a thing).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析中处理 ML 和 AI 任务时，会出现类似的计算需求：许多过程需要分析大量数据。诸如 TensorFlow 的库需要超出数据转换的分析工具。借助
    Kubernetes，数据科学家和工程师现在可以快速创建基于廉价计算、网络和存储的虚拟数据中心，以匹敌过去某些超级计算机的能力。这些技术的结合为开发人员带来了全新而令人兴奋的未来：基于自助服务使用模型构建基于
    ML 和 AI 的应用，无需等待昂贵超级计算机上的时间（是的，这曾经是一件事）。
- en: 'Access via the right APIs and ability via the right infrastructure built on
    Kubernetes is a powerful combination that the data science and Python community
    has been working to make a reality. Two new projects are already making a mark:
    Dask and Ray. As pointed out by Dean, Python is the preferred language for data
    science. Both Ray and Dask provide a native Python interface for massively parallel
    processing both inside and outside of Kubernetes.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正确的 API 访问和在基于 Kubernetes 构建的正确基础设施上具备的能力，是数据科学和 Python 社区一直努力实现的强大组合。两个新项目已经开始崭露头角：Dask
    和 Ray。正如 Dean 所指出的，Python 是数据科学的首选语言。Ray 和 Dask 都为内部和外部 Kubernetes 的大规模并行处理提供了本地
    Python 接口。
- en: Dask
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask
- en: '[Dask](https://www.dask.org) is a Python-based clustering tool for large-scale
    processing that abstracts away the complicated setup steps. It can be used for
    anything that you can express in a Python program but has found a real home in
    data science with the countless libraries available. scikit-learn, NumPy, TensorFlow,
    and Pandas are all mature data science libraries that can be used on a laptop
    and then scaled to a massive cluster of computers thanks to Dask.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dask](https://www.dask.org) 是一个基于 Python 的大规模处理工具，用于抽象复杂的设置步骤。它可以用于任何您可以用
    Python 程序表达的事物，但在数据科学领域因其众多可用的库而找到了真正的用武之地。scikit-learn、NumPy、TensorFlow 和 Pandas
    都是成熟的数据科学库，可以在笔记本电脑上使用，然后借助 Dask 扩展到大规模计算机集群。'
- en: Dask integrates nicely with Kubernetes to provide the easy user experience that
    operators and developers have come to expect with Python. The Dask storage primitives
    Array, DataFrame, and Bag map to many cloud native storage choices. For example,
    you could map a DataFrame to a file stored in a PersistentVolume or an object
    bucket such as S3\. Your storage scale is limited only by the underlying resources
    and your budget. As the Python code is working with your data, Dask manages the
    chunking across multiple workers seamlessly.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 与 Kubernetes 集成良好，提供了操作员和开发人员已经习惯的便捷用户体验。Dask 的存储原语 Array、DataFrame 和 Bag
    映射到许多云原生存储选择。例如，您可以将 DataFrame 映射到存储在 PersistentVolume 或对象存储桶（如 S3）中的文件。您的存储规模仅受基础资源和预算的限制。当
    Python 代码处理数据时，Dask 无缝地管理多个工作节点之间的分块。
- en: 'Deployment options include the manual Helm install we are now familiar with
    from [Chapter 4](ch04.html#automating_database_deployment_on_kuber), as you can
    see in this example:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 部署选项包括我们从 [第四章](ch04.html#automating_database_deployment_on_kuber) 熟悉的手动 Helm
    安装示例：
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Or as an alternative, you can install a Dask cluster in Kubernetes with a Jupyter
    Notebook instance for working inside the cluster:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 或者作为另一种选择，您可以在 Kubernetes 中安装一个带有 Jupyter Notebook 实例的 Dask 集群进行内部工作：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once your Dask cluster is running inside Kubernetes, you can connect as a client
    and run your Python code across the compute nodes using the `HelmCluster` object.
    Connect using the name you gave your cluster given at the time of installation:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的 Dask 集群在 Kubernetes 中运行起来，您可以作为客户端连接，并使用 `HelmCluster` 对象在计算节点上运行您的 Python
    代码。连接时，请使用安装时给集群命名的名称：
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If that wasn’t easy enough, you can completely skip the Helm installation and
    just let Dask do that part for you. The `KubeCluster` object takes an argument
    specifying the Pod configuration either using a `make_pod_spec` method or specifying
    a YAML configuration file. It will connect to the default Kubernetes cluster accessible
    via `kubectl` and invoke the cluster creation inside your Kubernetes cluster as
    a part of the running Python program:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够简单，您可以完全跳过Helm安装，让Dask为您完成这部分工作。`KubeCluster`对象接受一个参数，该参数可以使用`make_pod_spec`方法或指定一个YAML配置文件来指定Pod配置。它将连接到通过`kubectl`访问的默认Kubernetes集群，并在运行的Python程序的一部分中在您的Kubernetes集群内调用集群创建：
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Developer access to Kubernetes clusters for parallel computing couldn’t get
    much easier, and this is the appeal new tools like Dask can provide.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者访问Kubernetes集群以进行并行计算变得更加简单，而像Dask这样的新工具正是它们的吸引力所在。
- en: Ray
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray
- en: 'In a significant difference from the arbitrary Python code in Dask, *Ray* takes
    a different approach to Python clustering by operating as a parallel task manager
    that includes a distributed computing framework with a large [ecosystem of integrations](https://oreil.ly/XY5rC).
    For the end user, Ray provides low-level C++ libraries to run distributed code
    purpose-built for compute-intensive workloads typical in data science. The base
    is Ray Core, which does all the work of distributing workloads using the concept
    of a task. When developers write Python code using Ray, each task is expressed
    as a remote function, as shown in this example from the [Ray documentation](https://oreil.ly/gCmBs):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在与Dask中任意的Python代码有显著差异的同时，*Ray*采用了不同的方法来进行Python集群化，通过作为并行任务管理器运行，并包含一个具有大型[集成生态系统](https://oreil.ly/XY5rC)的分布式计算框架。对于最终用户，Ray提供了低级别的C++库，用于运行专门为数据科学中的计算密集工作负载而构建的分布式代码。Ray核心是基础，它通过任务的概念来分发工作负载。当开发者使用Ray编写Python代码时，每个任务都表现为一个远程函数，正如从[Ray文档](https://oreil.ly/gCmBs)中的这个例子所示：
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this basic example, you can see the difference in the approach Ray takes
    for distributing work. Developers have to be explicit in what work is distributed
    with Ray Core handling the compute management with the Cluster Manager.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基本的例子中，您可以看到Ray在分发工作上采取的方法的不同。开发者必须明确指出使用Ray Core处理计算管理与集群管理器的工作。
- en: 'A Ray deployment in Kubernetes is designed to leverage compute and network
    resource management within dynamic workloads. The Ray Operator includes a custom
    controller and CRD to deploy everything needed to attach code to a Ray cluster.
    A Helm chart is provided for easy installation. However, since the chart is unavailable
    in a public repository, you must first download the entire Ray distribution to
    your local filesystem. An extensive configuration YAML file can be modified, but
    to get a simple Ray cluster working, the defaults are fine, as you can see in
    this example from the [documentation](https://oreil.ly/XjXJo):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中部署Ray旨在利用动态工作负载中的计算和网络资源管理。Ray Operator包括自定义控制器和CRD，用于部署连接到Ray集群所需的所有内容。提供了一个Helm图表以便于安装。但是，由于图表不在公共仓库中提供，因此您必须首先将整个Ray分发下载到本地文件系统。可以修改一个广泛的配置YAML文件，但是要使一个简单的Ray集群工作起来，使用默认设置即可，正如从[文档](https://oreil.ly/XjXJo)中所见：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the creation of two types of Pods being installed. The head
    node handles the communication and orchestration of running tasks in the cluster,
    and the Worker Node handles where tasks execute their code. With a Ray cluster
    running inside a Kubernetes cluster, there are two ways to run a Ray Job: interactively
    with the Ray client or as a Job submitted via `kubectl`.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致安装了两种类型的Pod。头节点处理集群中运行任务的通信和编排，而工作节点处理任务执行其代码的地方。在Kubernetes集群内运行Ray集群时，有两种方式可以运行Ray作业：通过Ray客户端进行交互式操作或作为通过`kubectl`提交的作业。
- en: 'The Ray client is embedded into a Python program file and initializes the connection
    to the Ray cluster. This requires the head service IP to be exposed through either
    Ingress or local port forwarding. Along with the remote function code, an initializer
    will establish the connection to the externalized Ray cluster host IP and port:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Ray客户端嵌入到一个Python程序文件中，并初始化与Ray集群的连接。这需要通过Ingress或本地端口转发来公开头部服务IP。除了远程函数代码外，初始化程序还将建立与外部化Ray集群主机IP和端口的连接：
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Another option is to run your code inside the Kubernetes cluster and attach
    it to an internal service and port. You use `kubectl` to submit the Job to run
    and pass a Job description YAML file that outlines the Python program to use and
    Pod resource information. Here is an example Job file from the [Ray documentation](https://oreil.ly/gCmBs):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是在Kubernetes集群内部运行您的代码，并将其附加到内部服务和端口。您可以使用`kubectl`提交作业以运行，并传递作业描述的YAML文件，该文件概述了要使用的Python程序和Pod资源信息。以下是来自[Ray文档](https://oreil.ly/gCmBs)的示例作业文件：
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This file can then be submitted to the cluster using `kubectl`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用`kubectl`将此文件提交到集群：
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Inside the Python file submitted, we can use the DNS name of the Ray service
    head and let Kubernetes ensure that the network path is routed:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在提交的Python文件内部，我们可以使用Ray服务头的DNS名称，并让Kubernetes确保网络路径路由：
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For both external and internal modes of running Ray programs, the head node
    utilizes the Kubernetes scheduler to manage the Worker Node Pod lifecycle to complete
    the submitted Job. Ray provides a simple programming API for developers to utilize
    large-scale cluster computing without learning Kubernetes administration. SREs
    can create and manage Kubernetes clusters that can be easily used by data scientists
    using their preferred Python programming language.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于运行Ray程序的外部和内部模式，头节点利用Kubernetes调度程序来管理Worker Node Pod的生命周期以完成提交的作业。Ray为开发人员提供了一个简单的编程API，可以利用大规模集群计算而不需要学习Kubernetes管理。SRE（站点可靠性工程师）可以创建和管理Kubernetes集群，数据科学家可以使用他们偏爱的Python编程语言轻松使用这些集群。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This wraps up the tour of data components in your cloud native application stack.
    Adding analytics completes the total data picture by enabling you to find insights
    in larger volumes of data that can complement other parts of your application.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就完成了云原生应用程序堆栈中数据组件的导览。通过添加分析功能，可以通过在大数据量中找到洞察力来完善整体数据图片，这些数据可以补充应用程序的其他部分。
- en: Analytics is at the frontier of cloud native data innovation, and for this reason
    big data isn’t something you should assume fits into Kubernetes in the same way
    as other data infrastructure. Two primary differences are the volumes of data
    involved and the bursty nature of the workloads. Further improvements are needed
    to make Apache Spark run more effectively on Kubernetes, especially in the areas
    of Job management and storage APIs However, the knowledge is available to help
    you deploy with confidence today. Projects such as Apache YuniKorn and Volcano
    are already leading the way in open source to give Kubernetes a better foundation
    for analytic workloads. Emerging analytic engines such as Dask and Ray may be
    a better choice for your use case, and they can be used in combination with other
    tools.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 分析处于云原生数据创新的前沿，因此，大数据不是你应该假设与其他数据基础设施同样适合于Kubernetes的内容。两个主要区别是涉及的数据量和工作负载的突发性质。需要进一步改进，使Apache
    Spark在Kubernetes上更有效地运行，特别是在作业管理和存储API方面。然而，现有的知识可以帮助您今天放心部署。像Apache YuniKorn和Volcano这样的项目已经在开源中引领了分析工作负载的更好基础。新兴的分析引擎如Dask和Ray可能是您用例的更好选择，并且可以与其他工具结合使用。
- en: While analytic workloads may not have been in your original plans for deployment
    in Kubernetes, they can’t be skipped if your goal is to build the complete picture
    of a virtual datacenter, purpose-designed to run your application.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分析工作负载可能不在您最初的Kubernetes部署计划中，但如果您的目标是构建虚拟数据中心的完整图片，特别是设计用来运行您的应用程序的目的，那么它们不能被跳过。
