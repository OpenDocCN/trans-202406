- en: Chapter 9\. Data Analytics on Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Progress in technology is when we have the ability to be more lazy.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dr. Laurian Chirica
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the early 2000s, Google captivated the internet with a declared public goal:
    “to organize the world’s information and make it universally accessible and useful.”
    This was an ambitious goal and accomplishing it would, to paraphrase, take “computer
    sciencing” the bits out of it. Given the increasing rate of data creation, Google
    needed to invent (and reinvent) ways of managing data volumes no one had ever
    considered. An entirely new community, culture, and industry were born around
    analyzing data called *analytics*, tackling what was eventually labeled “big data.”
    Today, analytics is a full-fledged member of almost every application stack and
    not just relegated to a Google problem. Now it’s everyone’s problem; instead of
    an art form restricted to a small club of experts, we all need to know how to
    make analytics work. Organizations need reliable and fast ways to deploy applications
    with analytics so that they can do more with less.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The laziness Dr. Chirica was talking about in a tongue-in-cheek way in the quote
    that opens this chapter describes an ideal future. Instead of having a hundred-person
    team working night and day to analyze a petabyte of data, what if you could reduce
    that to one person and a few minutes? The cloud native way of running data infrastructure
    is a path we should all work toward to achieve that kind of glorious laziness.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already looked at several aspects of moving stateful workloads onto Kubernetes,
    including storage, databases, and streaming. In this chapter, it’s time to look
    at analytics to complete the picture. As a bit of a preview, [Figure 9-1](#the_cloud_native_virtual_datacenter)
    shows how data analytics fits as the final part of our roadmap of managing the
    complete data stack using Kubernetes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![The cloud native virtual datacenter](assets/mcdk_0901.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The cloud native virtual datacenter
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this architecture, there are no more external network requirements bridging
    to resources in or out of the Kubernetes cluster, just a single, virtual datacenter
    that serves our bespoke needs for cloud native applications. The large blocks
    represent the macro components of data infrastructure we discussed in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra),
    with the addition of user application code, deployed in microservices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Analytics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analytic workloads and the accompanying infrastructure operations are much different
    from other workloads. Analytics isn’t just another containerized system to orchestrate.
    The typical stateful applications like databases we examined in previous chapters
    have many similar characteristics but tend to stay static or predictably slow-growing
    once deployed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'However, one aspect of analytic workloads strikes fear in many administrators:
    volume. While persistent data stores like databases can consume gigabytes to terabytes
    of storage, analytic volumes can easily soar into petabytes, creating an entirely
    new class of problems to solve. They don’t call it “big data” for nothing.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Oxford English Dictionary* defines analytics as “the systematic computational
    analysis of data or statistics.” [Wikipedia](https://oreil.ly/Hc1Pp) adds, “It
    is used for the discovery, interpretation, and communication of meaningful patterns
    in data.” Combine those definitions with large volumes of data and what sort of
    outcome should we expect for cloud native applications? Let’s break down the different
    types of analytics workflows and methodologies:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Batch analytics
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: In computer science, a *batch* is a series of instructions applied to data with
    little or no user interaction. The idea of running batch Jobs is as old as general-purpose
    computing. In distributed systems such as Apache Hadoop or Apache Spark, each
    individual Job consists of a program that can operate on smaller bits of data
    in parallel and in stages or pipelines. The smaller results are combined into
    a single, final result at the end of a Job. An example of this is MapReduce, discussed
    later in this chapter. In most cases, statistical analysis such as count, average,
    and percentile measurement is done. Batch analytics is the focus of this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Stream analytics
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 8](ch08.html#streaming_data_on_kubernetes), stream
    analytics is about what is *happening,* whereas batch analytics is about what
    *happened*. Many of the same APIs and developer methodologies are used in both
    stream analytics and batch analytics. This can be confusing and lead people to
    believe that they are the same thing when, in fact, they have very different use
    cases and implementations. A good example is fraud detection. The time frames
    for detecting and stopping fraud can be measured in milliseconds to seconds, which
    fits the stream analytics use case. Batch analytics would be used to find fraud
    patterns over larger time periods.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence/machine learning (AI/ML)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: While AI and ML can be considered a subset of batch analytics, they are such
    specialized fields that they deserve a special callout. AI and ML are often mentioned
    together; however, they have two different output goals. AI attempts to emulate
    human cognition in decision making. ML uses algorithms to derive meaning from
    pools of data, sometimes in ways that aren’t readily obvious. Both approaches
    require the application of computing resources across volumes of data. This topic
    is discussed in greater detail in [Chapter 10](ch10.html#machine_learning_and_other_emerging_use).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Analytic Workloads in Kubernetes
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The original focus of Kubernetes was on scaling and orchestrating stateless
    applications. As you’re learning in this book, Kubernetes is evolving to support
    stateful applications. The promise of operational efficiency by moving more and
    more workloads into virtual datacenters has been highly motivating. The world
    of analytics can take advantage of the progress made in reducing the operational
    burden for stateless and stateful workloads. However, Kubernetes has some unique
    challenges in managing analytic workloads; many are still a work in progress.
    What features of Kubernetes are required to complete the data picture and put
    analytic workloads on par with other parts of the stack like microservices and
    databases? Here are a few of the key considerations we’ll examine in this chapter:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes最初关注的是无状态应用程序的扩展和编排。正如你在本书中学到的，Kubernetes正在演变以支持有状态应用程序。通过将更多工作负载移入虚拟数据中心，提高运行效率的承诺具有很高的激励作用。分析领域可以利用在减少无状态和有状态工作负载操作负担方面取得的进展。然而，Kubernetes在管理分析工作负载方面面临一些独特的挑战；许多问题仍在解决中。Kubernetes需要哪些功能来完善数据视图，并使分析工作负载达到与微服务和数据库等堆栈其他部分相当的水平？以下是本章将要探讨的一些关键考虑因素：
- en: Orderly execution
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有序执行
- en: An essential aspect of analytic workloads is the order of operations required
    to analyze large volumes of data. This involves far more than just making sure
    Pods are started with the proper storage and networking resources. It also includes
    a mapping of the application with the orderly execution run in each Pod. The Kubernetes
    component primarily responsible for this task is `kube-scheduler` (see [Chapter 5](ch05.html#automating_database_management_on_kuber)),
    but the controllers for Jobs and CronJobs are involved as well. This is a particular
    area of attention for the Kubernetes communities focusing on analytics, which
    we will further cover in the chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工作负载的一个重要方面是分析大量数据所需的操作顺序。这不仅仅是确保Pod以适当的存储和网络资源启动，还包括应用程序与每个Pod中有序执行运行的映射。在这项任务中，Kubernetes组件主要负责的是`kube-scheduler`（参见[第5章](ch05.html#automating_database_management_on_kuber)），但Jobs和CronJobs的控制器也参与其中。这是Kubernetes社区在关注分析时的一个特定领域，我们将在本章进一步讨论。
- en: Storage management
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 存储管理
- en: Analytic workloads use ephemeral and persistent storage in different Jobs that
    process data. The real trouble occurs when it comes to identifying and selecting
    the right storage per Job. Many analytic workloads require ephemeral storage for
    short periods and more-efficient (cheaper) persistent storage for long terms.
    As you learned in [Chapter 2](ch02.html#managing_data_storage_on_kubernetes),
    Kubernetes storage has greatly increased maturity. Analytics projects that run
    on Kubernetes need to take advantage of the work already done with stateful workloads
    and continue to partner with the Kubernetes community for future enhancements
    in areas like StorageClasses and different access patterns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工作负载在处理数据的不同作业中使用临时和持久存储。真正的麻烦出现在识别和选择每个作业所需的正确存储时。许多分析工作负载需要临时存储来进行短期处理，以及更高效（更便宜）的持久存储来进行长期处理。正如你在[第2章](ch02.html#managing_data_storage_on_kubernetes)学到的，Kubernetes存储已经显著成熟。在Kubernetes上运行的分析项目需要利用已经在有状态工作负载上完成的工作，并继续与Kubernetes社区合作，以改进StorageClasses和不同访问模式等领域的未来增强功能。
- en: Efficient use of resources
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 资源的高效利用
- en: There is an old saying that “everything counts in large amounts,” and nothing
    makes that more evident than analytics. A Job may require 1,000 Pods for 10 minutes,
    but what if it needs 10,000? That’s a challenging problem for the Kubernetes control
    plane. Another Job might require terabytes of swap disk space that is needed only
    for the duration of a Job. In a cloud native world, Jobs should be able to quickly
    allocate the resources they need and release the resources when finished. Making
    these operations as efficient as possible saves time and, more importantly, money.
    The fast and bursty nature of analytics has created some challenges for the Kubernetes
    API server and scheduler to keep up with all the Jobs that need to be run. Several
    of those challenges are already being addressed, as discussed later in the chapter,
    and some are still a work in progress.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Those are the challenges, but none of them are showstoppers that will get in
    the way of our dream of a complete cloud native stack deployed as a single virtual
    datacenter in Kubernetes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Engineers can be their own worst enemies. Often when we go to solve one problem,
    it creates a few more that need to be solved. We can count this as progress, however,
    when it comes to managing data. Every step up we take, despite the challenges,
    allows for new solutions that were never available before. It’s a staggering thought—today,
    a small number of people can perform analytics tasks that massive teams would
    not have been able to accomplish just a few years ago. See the quote about laziness
    at the beginning of the chapter. There is still work to be done, and next we will
    look at the tools available for analyzing data in Kubernetes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Apache Spark
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google changed the world of data analytics with the MapReduce algorithm simply
    by describing it to the world in an academic paper. Not long after the [MapReduce
    paper](https://oreil.ly/mryO0) got engineers talking, an open source implementation
    was created: the now-famous Apache Hadoop. A massive ecosystem was built up around
    Hadoop with tooling and complementary projects such as Hadoop Distributed File
    System (HDFS).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Growing pains from this fast-moving project opened the door for the next generation
    of tools that built on the lessons learned with Hadoop. One project that grew
    in popularity as an alternative to Hadoop was [Apache Spark](https://oreil.ly/BlT4i).
    Spark addressed reliability and processing efficiency problems by introducing
    the Resilient Distributed Dataset (RDD) API and Directed Acyclic Graph (DAG).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: The RDD was a significant improvement over the forced linear processing patterns
    of MapReduce, which involved a lot of reading from disk, processing, and then
    writing back to disk only to be redone over and over. This put the burden on developers
    to reason through how data was processed. RDDs shifted the responsibility away
    from developers as an API that created a unified view of all data while abstracting
    the actual processing details. Those details were created in a workflow to perform
    each task expressed in a DAG. The DAG is nothing more than an optimized path that
    describes data and operations to be completed in an orderly fashion until the
    final result is produced. RDDs were eventually replaced with the Dataset and DataFrame
    APIs, further enhancing developer productivity over large volumes of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Spark’s operational complexity is greatly reduced compared to Hadoop, which
    notoriously tipped the scale with the infrastructure required even for basic Jobs.
    Spark is an excellent example of one of the benefits of being a next-generation
    implementation with great hindsight. Much effort was put into simplifying Spark’s
    architecture, leveraging distributed systems concepts. The result is the three
    familiar components you should be familiar with in a Spark cluster, shown in [Figure 9-2](#components_of_a_spark_cluster).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Components of a Spark cluster](assets/mcdk_0902.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Components of a Spark cluster
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s review the responsibilities of each of these components:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Manager
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The Cluster Manager is the central hub for activity in the Spark cluster where
    new Jobs are submitted for processing. The Cluster Manager also acquires the resources
    needed to complete the task submitted. Different versions of the Cluster Manager
    are primarily based on how resources are managed (standalone, YARN, Mesos, and
    Kubernetes). The Cluster Manager is critical for deploying your Spark application
    using Kubernetes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Worker Node
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: When Spark Jobs run, they are broken into manageable pieces by the Cluster Manager
    and handed to the Worker Nodes to perform the processing. They serve as the local
    manager for hardware resources as a single point of contact. Worker Nodes invoke
    and manage Spark Executors.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Spark Executor
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Each application sent to a Worker Node will get its own Spark Executor. Each
    Executor is a standalone JVM process that operates independently and communicates
    back with the Worker Node. The tasks for the application are broken into threads
    that consume the compute resources allocated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: These are the traditional components of Spark as designed early in the project.
    We’ll see that the need to deploy a cloud native version of Spark forced some
    architectural evolution. The fundamentals are the same, but the execution framework
    has adapted to take advantage of what Kubernetes provides and eliminate duplication
    in orchestration overhead. In the next section, we’ll look at those changes and
    how to work with Spark in Kubernetes.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Apache Spark in Kubernetes
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of Apache Spark version 2.3, Kubernetes is one of the supported modes in
    the Cluster Manager. It would be easy to understate what that has meant for Spark
    as a cloud native analytics tool. Starting with Spark 3.1, Kubernetes mode is
    considered production-ready, continually adding steady improvements. When the
    Spark project looked at what it takes to run a clustered analytics system inside
    a cluster orchestration platform, a lot of overlaps became obvious. Kubernetes
    already had the mechanisms in place for the lifecycle management of containers
    and the dynamic provisioning and deprovisioning of compute elements, so Spark
    lets Kubernetes take care of this work. The redundant parts were removed, and
    Spark is closer to the way Kubernetes works as a result. The `spark-submit` command-line
    tool was extended to interface with Kubernetes clusters using the Kubernetes API,
    maintaining a familiar toolchain for developers and data engineers. These unique
    aspects of a Spark deployment in Kubernetes are shown in [Figure 9-3](#spark_on_kubernetes).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark on Kubernetes](assets/mcdk_0903.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Spark on Kubernetes
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s highlight a few of the differences:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Spark Driver
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The dedicated Cluster Manager of a standalone Spark cluster is replaced with
    native Kubernetes cluster management and the Spark Driver for Spark-specific management.
    The Spark Driver Pod is created when the Kubernetes API server receives a Job
    from the `spark-submit` tool. It invokes the Spark Executor Pods to satisfy the
    Job requirements. It is also responsible for cleaning up Executor Pods after the
    Job, making it a crucial part of elastic workloads.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Spark Executor
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Like a standalone Spark cluster, Executors are where the work gets done and
    where the most compute resources are consumed. Invoked from the Spark Driver,
    they take Job instructions passed by `spark-submit` with details such as CPU and
    memory limits, storage information, and security credentials. The containers used
    in Executor Pods are pre-created by the user.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Custom Executor container
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Before a Job is sent for processing using `spark-submit`, users must build a
    custom container image tailored to meet the application requirements. The Spark
    distribution download contains a Dockerfile that can be customized and used in
    conjunction with the *docker-image-tool.sh* script to build and upload the container
    required when submitting a Spark Job in Kubernetes. The custom container has everything
    it needs to work within a Kubernetes environment, like a Spark Executor based
    on the Spark distribution version required.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The workflow for preparing and running Spark Jobs when using Kubernetes and
    defaults can be relatively simple, requiring only a couple of steps. This is especially
    true if you are already familiar with and running Spark in production. You will
    need a running Kubernetes cluster and a download of Spark in a local filepath
    along with your Spark application source code.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Build Your Custom Container
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An executor container encapsulates your application and the runtime needed
    to act as an executor Pod. The build script takes an argument for the source code
    repository and a tag assignment for the output image when pushed to your Docker
    registry:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will be a Docker image with a JAR file containing your application
    code. You will then need to push this image to your Docker registry:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Docker Image Tags
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Be mindful that your tag name is labeled and versioned correctly. Reusing the
    same tag name in production could have unintended consequences, as some of us
    have learned from experience.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Submit and Run Your Application
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the Docker image is pushed to the repo, use `**spark-submit**` to start
    the process of running the Spark application inside Kubernetes. This is the same
    `spark-submit` used for other modes, so many of the same arguments are used. This
    corresponds to (1) in [Figure 9-3](#spark_on_kubernetes):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Quite a few things are happening here, but the *most important* is in the `--master`
    parameter. To indicate this is for Kubernetes, the URL in the argument must start
    with `k8s://` and point to the API server in the default Kubernetes cluster specified
    in your local *.kubeconfig* file. The `*<spark-image>*` is the Docker image you
    created in (1), and the application path refers to your application stored inside
    the image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Next is (2), where `spark-submit` interacts with the Kubernetes cluster to schedule
    the Spark Driver Pod (3) and (4). The Spark Driver parses the Job parameters and
    works with the Kubernetes scheduler to set up Spark Executor Pods (5), (6), and
    (7) to run the application code contained in the customer container image. The
    application will run to completion, and eventually the Pod used will be terminated
    and resources returned to the Kubernetes cluster in a process called *garbage
    collection*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: This is just an overview of how Spark natively works with Kubernetes. Please
    refer to the [official documentation](https://oreil.ly/upJeL) to go into much
    greater detail. There are many ways to customize the arguments and parameters
    to best fit your specific needs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Security Considerations when Running Spark in Kubernetes
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security is not enabled by default when using Spark in Kubernetes. The first
    line of defense is authentication. Production Spark applications should use the
    built-in authentication in Spark to ensure that the users and processes accessing
    your application are the ones you intended.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: When creating a container for your application, the Spark documentation highly
    recommends changing the `USER` directive to an unprivileged unique identifier
    (UID) and group identifier (GID) to mitigate against privilege escalation attacks.
    This can also be accomplished with a SecurityContext inside the Pod template file
    provided as a parameter to `spark-submit`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Storage access should also be restricted with the Spark Driver and Spark Executor.
    Specifically, you should limit the paths that can be accessed by the running application
    to eliminate any accidental access in the event of a vulnerability. These can
    be set inside a PodSecurityAdmission, which the Spark documentation [recommends](https://oreil.ly/xQcom).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: For optimal security of your Spark applications, use the security primitives
    Kubernetes provides and customize the defaults for your environment. The best
    security is the one you don’t have to think about. If you are an SRE, this is
    one of the best things you can do for your developers and data engineers. Default
    secure!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Operator for Apache Spark
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If Spark can run in Kubernetes via `spark-submit`, why do we need an operator?
    As you learned in previous chapters, Kubernetes operators give you more flexibility
    in managing applications and a more cloud native experience overall. Using `spark-submit`
    to run your Spark applications requires your production systems to be set up with
    a local installation of Spark, including all dependencies. The Spark on Kubernetes
    Operator allows SREs and developers to manage park applications declaratively
    using Kubernetes tools such as Helm and `kubectl`. It also allows better observability
    on running Jobs and exporting metrics to external systems like Prometheus. Finally,
    using the operator provides an experience much closer to running other applications
    in Kubernetes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to install the operator into your Kubernetes cluster using
    Helm:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once completed, you will have a SparkApplication controller running and looking
    for SparkApplication objects. This is the first big departure from `spark-submit`.
    Instead of a long list of command-line arguments, you use the SparkApplication
    CRD to define the Spark Job in a YAML file. Let’s look at a config file from the
    [Spark on Kubernetes Operator documentation](https://oreil.ly/quNfG):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `spec:` section is similar to the parameters you passed in `spark-submit`
    with details about your application. The most important is the location of the
    container image. This example uses a default Spark container with the `spark-examples`
    JAR file preinstalled. You will need to use *docker-image-tool.sh* to build the
    image for your application as described in [“Build Your Custom Container”](#build_your_custom_container),
    and modify the `mainClass` and `mainApplicationFile` as appropriate for your application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Two other notable fields under `spec` are `driver` and `executor`. These provide
    the specifications for the Spark Driver Pods and Spark Executor Pods that the
    Spark Operator will deploy. For `driver`, only one core is required, but CPU and
    memory allocations need to be enough to maintain the number of executors you require.
    The number is set in the `executor` section under `instances`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Minding Your Resources
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For resource management, the requests you make under `driver` and `spec` need
    to be carefully considered for resource management. The number of instances plus
    their allocated CPU and memory could use up resources quickly. Jobs can hang indefinitely
    while waiting for resources to free up, which may never happen.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Now that your configuration YAML is ready, it’s time to put it into action.
    For a walk-through, refer to [Figure 9-4](#spark_on_kubernetes_operator).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark on Kubernetes Operator](assets/mcdk_0904.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Spark on Kubernetes Operator
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: First, use `kubectl apply -f *<filename>*` (1) to apply the SparkApplication
    into your running Kubernetes cluster (2). The Spark Operator listens for new applications
    (3), and when a new config object is applied, the submission runner controller
    begins the tasks of building out the required Pods. From here the actions taken
    in the Kubernetes cluster are the same as if you used `spark-submit`, with all
    of the parameters being supplied in this case via the SparkApplication YAML. The
    submission runner starts the Spark Driver Pod (4) which in turn directs the Spark
    Executor Pods (5), which runs the application code to completion. The Pod monitor
    included in the Spark Operator exports Spark metrics to observability tools such
    as Prometheus.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The Spark Operator fills in the gaps between the way `spark-submit` works and
    the way SREs and developers typically deploy applications into Kubernetes. This
    was a long answer to the question posed at the beginning of this section. We need
    an operator to make using Spark more cloud native and thus more manageable in
    the long run. The cloud native way of doing things includes taking a declarative
    approach to managing resources and making those resources observable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Alternative Schedulers for Kubernetes
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you learned in [Chapter 5](ch05.html#automating_database_management_on_kuber),
    the Kubernetes scheduler has a basic but essential job: take requests for resources
    and assign the compute, network, and storage to satisfy the requirements. Let’s
    look at the default approach for this action, as shown in [Figure 9-5](#typical_kubernetes_scheduling).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![Typical Kubernetes scheduling](assets/mcdk_0905.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Typical Kubernetes scheduling
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A typical scheduling effort begins when you create a *deployment.yaml* file
    describing the resources required (1), including which Pod resources are needed
    and how many. When the YAML file is submitted (2) to the Kubernetes cluster API
    server using `kubectl apply`, the Pod resources are created with the supplied
    parameters and are ready for assignment to a node. Nodes have the needed pool
    of resources, and it’s the job of `kube-scheduler` to be the matchmaker between
    nodes and Pods. The scheduler performs state matching whenever a new Pod resource
    is created (3), and checks whether the Pod has an assigned node. If not, the scheduler
    makes the calculations needed to find an available node. It examines the requirements
    for the Pod, scores the available nodes using an internal set of rules, and selects
    a node to run the Pod (4). This is where the real work of container orchestration
    in Kubernetes gets done.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we have a problem with analytic workloads: the default Kubernetes
    scheduler was not designed for batch workloads. The design is just too basic to
    work the way that’s needed for analytics. As mentioned in [“Analytics on Kubernetes
    Is the Next Frontier”](#analytics_on_kubernetes_is_the_next_fro), Kubernetes was
    built for the needs of stateless workloads. These are long-running processes that
    may expand or contract over time but tend to remain relatively static. Analytic
    applications such as Spark are different, requiring the scheduling of potentially
    thousands of short-lived jobs.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, the developers of Kubernetes anticipated expanded requirements for
    future scheduling needs and made it possible for users to specify their scheduler
    in a configuration, bypassing the default scheduling approach.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'The strong desire to manage the entire application stack with a common control
    plane has been an innovation driver. As demonstrated in [“Deploying Apache Spark
    in Kubernetes”](#deploying_apache_spark_in_kubernetes), Spark has been moving
    closer to Kubernetes. In this section, we’ll look at how some teams have been
    bringing Kubernetes closer to Spark by building more appropriate schedulers. Two
    open source projects are leading the way in this effort: Volcano and Apache YuniKorn.
    These schedulers share similar guiding principles that make them more appropriate
    for batch workloads by providing the following alternative features:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Multitenant resource management
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The default Kubernetes scheduler allocates Pods as requested until no more available
    resources match Pod requirements. Both YuniKorn and Volcano provide a wide variety
    of resourcing modes to match your application needs better, especially in multitenant
    environments. Fairness in resource management prevents one analytic Job from starving
    out other Jobs for required resources. As these Jobs are scheduled, the entire
    resource pool is considered to balance utilization based on priority and throughput.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '*Gang scheduling* adds another layer of intelligence. If a submitted Job needs
    a certain amount of resources, it doesn’t make sense to start the Job if every
    Pod can’t be started. The default scheduler will start Pods until the cluster
    runs out of resources, potentially stranding Jobs as they wait for more Pods to
    come online. Gang scheduling implements an all-or-nothing approach, as Jobs will
    start only when all resources needed are available for the complete Job.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Job queue management
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Smarter queue management can also lead to better resource management. If one
    Job needs few resources and can be run while larger Jobs are being run, the scheduler
    can fit the Job in and therefore increase the Kubernetes cluster’s overall throughput.
    In some cases, users need control over what Jobs have priority and which can preempt
    or pause other running Jobs as they are submitted. Queues can be reordered or
    reprioritized after Jobs are submitted. Observability tooling provides queue insights
    that help determine total cluster health and resource usage.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: If you are considering a production deployment of analytic workloads, you should
    avoid using the default scheduler, `kube-scheduler`. It wasn’t designed for your
    needs in this case. Starting with a better scheduler lets you future-proof your
    Kubernetes experience. Let’s examine some highlights of each scheduler.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Apache YuniKorn
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *YuniKorn project* was built by engineers from Cloudera out of the operational
    frustration of working with analytic workloads in Spark. In the spirit of using
    open source to solve problems as a community, YuniKorn was donated to the Apache
    Software Foundation and accepted as an incubating project in 2020\. The name comes
    directly from the two systems it supports, YARN and Kubernetes. (Y unified K.
    YuniKorn. Get it?) It addresses the specific resource management and user control
    needs of analytic workloads from a Spark cluster administration point of view.
    YuniKorn also added support for TensorFlow and Flink jobs with the same level
    of resource control. No doubt, this support was born of the same operation frustrations
    found in Spark.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: YuniKorn is [installed](https://yunikorn.apache.org/docs) in Kubernetes using
    Helm. The goal of YuniKorn is to transform your Kubernetes cluster into a place
    that is friendly to the resource requirements of batch Jobs. A key part of that
    transformation is replacing the default `kube-scheduler`. To demonstrate how,
    let’s use [Figure 9-6](#yunikorn_architecture) to walk through the components.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'YuniKorn is meant to be a drop-in scheduler replacement with minimal changes
    to your existing Spark workflow, so we will start there. When new resource requests
    (1) are sent to the Kubernetes API server via `spark-submit` (2), the default
    `kube-scheduler` (3) is typically used to match Pods and nodes. When YuniKorn
    is deployed in your cluster, an `admissions-controller` Pod is created. The function
    of the `admissions-controller` is to listen for new resource requests (4) and
    make a small change, adding `schedulerName: yunikorn` to the resource request.
    If you need more fine-grained control, you can disable the `admissions-controller`
    and enable YuniKorn on a per Job basis by manually adding the following line to
    the SparkApplication YAML:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![YuniKorn architecture](assets/mcdk_0906.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. YuniKorn architecture
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All scheduling needs will now be handled by the YuniKorn Scheduler (5). YuniKorn
    is built to run with multiple orchestration engines and provides an API translation
    layer called *Kubernetes shim* to manage communication between Kubernetes and
    the YuniKorn core (6). yunikorn-core extends the basic filter and score algorithm
    available in the default `kube-scheduler` by adding options appropriate for batch
    workloads such as Spark. These options range from simple resource-based queues
    to more advanced hierarchical queue management that allows for queues and resource
    pools to map to organizational structures. Hierarchical pooling can be helpful
    for those with a massive analytics footprint across many parts of a large enterprise
    and is critical for multitenant environments when running in a single Kubernetes
    cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'YuniKorn core is [configured](https://oreil.ly/2Tp19) using the *queues.yaml*
    file, which contains all the details of how YuniKorn will schedule Pods to nodes,
    including the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Partitions
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: One or more named [configuration sections](https://oreil.ly/kxtHf) for different
    application requirements.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained [control](https://oreil.ly/M5W1A) over resources in a hierarchical
    arrangement to provide resource guarantees in a multitenant environment.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Node sort policy
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: How Nodes are selected by available resources. Choices are [FairnessPolicy](https://oreil.ly/RUI7q)
    and [BinPackingPolicy](https://oreil.ly/sED2J).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Placement Rules
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Description and [filters](https://oreil.ly/sHvMP) for Pod placement based on
    user or group membership.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Limits
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[Definitions](https://oreil.ly/ytYU9) for fine-grained resource limits on partitions
    or queues.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: New Jobs are processed by YuniKorn core by matching details and assigning the
    right queue. At this point, the scheduler can make a decision to assign Pods to
    nodes, which are then brought online (7).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: YuniKorn also ships with an observability web-based tool called *scheduler UI*
    that provides insights into Job and queue status. It can be used to monitor scheduler
    health and provide better insights to troubleshoot any Job issues.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Volcano
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Volcano* was developed as a general-purpose scheduler for running high performance
    computing (HPC) workloads in Kubernetes. Volcano supports a variety of workloads,
    including Spark, Flink, PyTorch, TensorFlow, and specialized systems such as KubeGene
    for genome sequencing. Engineers built Volcano at Huawei, Tencent, and Baidu,
    to name a few of the long list of contributors. Donated to the CNCF, it was accepted
    as a Sandbox project in 2020.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Volcano is installed using Helm and creates CRDs for Jobs and queues, making
    the configuration a core part of your Kubernetes cluster as compared with YuniKorn,
    which is more of a bypass. This is a reflection of the general-purpose nature
    of Volcano. When installed, the Volcano scheduler is available for any process
    needing advanced scheduling and queuing. Let’s use [Figure 9-7](#volcano_architecture)
    to walk through how it works.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Volcano with your batch Jobs, you will need to explicitly add the scheduler
    configuration to your Job YAML file (1). If you are using Volcano for Spark, it
    is [recommended](https://volcano.sh/en/docs/spark_on_volcano) by the Volcano project
    to use the Spark Operator for Kubernetes and add one field to your SparkApplication
    YAML:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Volcano architecture](assets/mcdk_0907.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Volcano architecture
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can then use `kubectl apply` as you normally would to submit your Job (2).
    Without specifying the Volcano scheduler, Kubernetes will match Pods and nodes
    with the default `kube-scheduler` (3).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'A Helm installation of Volcano will install the CRDs for Job, queue, and PodGroup
    and create a new Pod called Volcano Admission. Volcano Admission (4) attaches
    to the API server and validates Volcano-specific CRD entries and Jobs asking for
    the Volcano scheduler:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Job
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: A Volcano-specific Job with extended [configuration](https://oreil.ly/CknE0)
    for HPC.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Queue
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: A collection of PodGroups to be managed as a first in, first out (FIFO) resource
    group. [Configuration](https://oreil.ly/MW2xq) dictates the behavior of the queue
    for different situations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: PodGroup
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: A collection of Pods related to their purpose. Examples would be groups for
    Spark and TensorFlow with different [properties](https://oreil.ly/LzeBj) for each.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: When selected as the scheduler for a Job (5), the Volcano scheduler will take
    the CRDs and start to work (6). Incoming Jobs marked to use Volcano as the scheduler
    are matched with a PodGroup and queue. Based on this assignment, a final node
    placement is made for each Pod (7).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'The cluster-specific configuration for the Volcano scheduler core is stored
    in a ConfigMap named `volcano-scheduler-configmap`. This config file contains
    two main sections: `actions` and `plugins`. [Actions](https://oreil.ly/frTKk)
    are an ordered list of each step in the node selection for each Job: enqueue,
    allocate, preempt, reclaim, and backfill. Each step is optional and can be reordered
    to match the type of work that needs to be performed.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Plug-ins are the algorithms used to match Pods with nodes. Each has a different
    use case and purpose and can be combined as an ensemble:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Gang
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: This plug-in looks for higher-priority tasks in the queue and performs preemption
    and eviction if needed to free up resources for them.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: BinPack
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic algorithm for finding the best fit for using every resource
    available by mixing different-size resource requests in the most efficient manner.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Conformance
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: This ignores any task in the Namespace `kube-system` for eviction decisions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Dominant Resource Fairness (DRF)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: This is an algorithm to address issues of fairness across multiple resource
    types to ensure that all Jobs have equal throughput.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Proportion
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: This is a multitenant algorithm to allocate dedicated portions of cluster allocation
    for running Jobs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Task-topology
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm uses affinity to put network-intensive Jobs physically closer
    together for more efficient network use.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: NodeOrder
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: This plug-in takes multiple user-defined dimensions to score every available
    node before selection.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Predicate
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: This looks for certain predicates in nodes for selection (but currently supports
    only the GPU sharing predicate).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Priority
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: This plug-in chooses task priority based on the user-supplied configuration
    in `priorityClassName`, `createTime`, and `id`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Service level agreement (SLA)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: This uses the parameter `JobWaitingTime` to allow individual Jobs the control
    over priority based on when they are needed.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Time-division multiplexing (TDM)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: When nodes are used for both Kubernetes and YARN, TDM will schedule Pods that
    share resources in this arrangement.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: NUMA-aware
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: This provides scheduling for Pods with an awareness of CPU resource topology.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Outside of the Kubernetes installation, Volcano also ships with a command-line
    tool called `vcctl`. Managing Volcano can be done solely through the use of `kubectl`.
    However, `vcctl` presents an interface for operators more familiar with Job control
    systems.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the list of features offered by YuniKorn and Volcano, having
    choices is a beautiful thing. Regardless of which project you choose, you’ll have
    a better experience running analytic workloads in Kubernetes with one of these
    alternate schedulers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Analytic Engines for Kubernetes
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is a powerful tool that solves many use cases in analytics. However, having
    just a single choice can be restrictive once that tool no longer works the way
    you do. Google developed MapReduce in 2004 to fill a need for data transformation,
    such as taking a pool of data and creating a count of the things in it, and this
    is still a relevant problem today given the volumes of data we create. Even before
    MapReduce, massively parallel processing (MPP) was a popular approach for data
    analysis. These “supercomputers” consisted of rows and rows of individual computers
    presented as a single processing grid for researchers in fields such as physics
    and meteorology to run massive calculations that would take far too long on a
    single computer.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar computing need arises when tackling the ML and AI tasks in analytics:
    many processes need to analyze a large volume of data. Libraries such as TensorFlow
    require analytic tools beyond data transformation. With Kubernetes, data scientists
    and engineers can now create virtual datacenters quickly with commodity compute,
    network, and storage to rival some of the supercomputers of the past. This combination
    of technologies brings a completely new and exciting future for developers: building
    ML-based and AI-based applications based on a self-service usage model without
    having to wait for time on a very expensive supercomputer (yes, this was a thing).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Access via the right APIs and ability via the right infrastructure built on
    Kubernetes is a powerful combination that the data science and Python community
    has been working to make a reality. Two new projects are already making a mark:
    Dask and Ray. As pointed out by Dean, Python is the preferred language for data
    science. Both Ray and Dask provide a native Python interface for massively parallel
    processing both inside and outside of Kubernetes.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Dask
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Dask](https://www.dask.org) is a Python-based clustering tool for large-scale
    processing that abstracts away the complicated setup steps. It can be used for
    anything that you can express in a Python program but has found a real home in
    data science with the countless libraries available. scikit-learn, NumPy, TensorFlow,
    and Pandas are all mature data science libraries that can be used on a laptop
    and then scaled to a massive cluster of computers thanks to Dask.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Dask integrates nicely with Kubernetes to provide the easy user experience that
    operators and developers have come to expect with Python. The Dask storage primitives
    Array, DataFrame, and Bag map to many cloud native storage choices. For example,
    you could map a DataFrame to a file stored in a PersistentVolume or an object
    bucket such as S3\. Your storage scale is limited only by the underlying resources
    and your budget. As the Python code is working with your data, Dask manages the
    chunking across multiple workers seamlessly.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment options include the manual Helm install we are now familiar with
    from [Chapter 4](ch04.html#automating_database_deployment_on_kuber), as you can
    see in this example:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Or as an alternative, you can install a Dask cluster in Kubernetes with a Jupyter
    Notebook instance for working inside the cluster:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once your Dask cluster is running inside Kubernetes, you can connect as a client
    and run your Python code across the compute nodes using the `HelmCluster` object.
    Connect using the name you gave your cluster given at the time of installation:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If that wasn’t easy enough, you can completely skip the Helm installation and
    just let Dask do that part for you. The `KubeCluster` object takes an argument
    specifying the Pod configuration either using a `make_pod_spec` method or specifying
    a YAML configuration file. It will connect to the default Kubernetes cluster accessible
    via `kubectl` and invoke the cluster creation inside your Kubernetes cluster as
    a part of the running Python program:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Developer access to Kubernetes clusters for parallel computing couldn’t get
    much easier, and this is the appeal new tools like Dask can provide.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Ray
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a significant difference from the arbitrary Python code in Dask, *Ray* takes
    a different approach to Python clustering by operating as a parallel task manager
    that includes a distributed computing framework with a large [ecosystem of integrations](https://oreil.ly/XY5rC).
    For the end user, Ray provides low-level C++ libraries to run distributed code
    purpose-built for compute-intensive workloads typical in data science. The base
    is Ray Core, which does all the work of distributing workloads using the concept
    of a task. When developers write Python code using Ray, each task is expressed
    as a remote function, as shown in this example from the [Ray documentation](https://oreil.ly/gCmBs):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this basic example, you can see the difference in the approach Ray takes
    for distributing work. Developers have to be explicit in what work is distributed
    with Ray Core handling the compute management with the Cluster Manager.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'A Ray deployment in Kubernetes is designed to leverage compute and network
    resource management within dynamic workloads. The Ray Operator includes a custom
    controller and CRD to deploy everything needed to attach code to a Ray cluster.
    A Helm chart is provided for easy installation. However, since the chart is unavailable
    in a public repository, you must first download the entire Ray distribution to
    your local filesystem. An extensive configuration YAML file can be modified, but
    to get a simple Ray cluster working, the defaults are fine, as you can see in
    this example from the [documentation](https://oreil.ly/XjXJo):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the creation of two types of Pods being installed. The head
    node handles the communication and orchestration of running tasks in the cluster,
    and the Worker Node handles where tasks execute their code. With a Ray cluster
    running inside a Kubernetes cluster, there are two ways to run a Ray Job: interactively
    with the Ray client or as a Job submitted via `kubectl`.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'The Ray client is embedded into a Python program file and initializes the connection
    to the Ray cluster. This requires the head service IP to be exposed through either
    Ingress or local port forwarding. Along with the remote function code, an initializer
    will establish the connection to the externalized Ray cluster host IP and port:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Another option is to run your code inside the Kubernetes cluster and attach
    it to an internal service and port. You use `kubectl` to submit the Job to run
    and pass a Job description YAML file that outlines the Python program to use and
    Pod resource information. Here is an example Job file from the [Ray documentation](https://oreil.ly/gCmBs):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This file can then be submitted to the cluster using `kubectl`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Inside the Python file submitted, we can use the DNS name of the Ray service
    head and let Kubernetes ensure that the network path is routed:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For both external and internal modes of running Ray programs, the head node
    utilizes the Kubernetes scheduler to manage the Worker Node Pod lifecycle to complete
    the submitted Job. Ray provides a simple programming API for developers to utilize
    large-scale cluster computing without learning Kubernetes administration. SREs
    can create and manage Kubernetes clusters that can be easily used by data scientists
    using their preferred Python programming language.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This wraps up the tour of data components in your cloud native application stack.
    Adding analytics completes the total data picture by enabling you to find insights
    in larger volumes of data that can complement other parts of your application.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Analytics is at the frontier of cloud native data innovation, and for this reason
    big data isn’t something you should assume fits into Kubernetes in the same way
    as other data infrastructure. Two primary differences are the volumes of data
    involved and the bursty nature of the workloads. Further improvements are needed
    to make Apache Spark run more effectively on Kubernetes, especially in the areas
    of Job management and storage APIs However, the knowledge is available to help
    you deploy with confidence today. Projects such as Apache YuniKorn and Volcano
    are already leading the way in open source to give Kubernetes a better foundation
    for analytic workloads. Emerging analytic engines such as Dask and Ray may be
    a better choice for your use case, and they can be used in combination with other
    tools.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: While analytic workloads may not have been in your original plans for deployment
    in Kubernetes, they can’t be skipped if your goal is to build the complete picture
    of a virtual datacenter, purpose-designed to run your application.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
