<html><head></head><body><section data-pdf-bookmark="Chapter 5. Managing Resources" data-type="chapter" epub:type="chapter"><div class="chapter" id="resources">&#13;
<h1><span class="label">Chapter 5. </span>Managing Resources</h1>&#13;
&#13;
<blockquote class="epigraph">&#13;
<p>Nothing is enough to the man for whom enough is too little.</p>&#13;
<p data-type="attribution">Epicurus</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="Kubernetes" data-secondary="resource management" data-type="indexterm" id="ix_05-resources-adoc0"/>In this chapter, we’ll look at how to make the most of your cluster: how to manage and optimize resource usage, how to manage the life cycle of containers, and how to partition the cluster using namespaces. We’ll also outline some techniques and best practices for keeping down the cost of your cluster, while getting the most for your money.</p>&#13;
&#13;
<p>You’ll learn how to use resource requests, limits, and defaults, and how to optimize them with the Vertical Pod Autoscaler; how to use readiness probes, liveness probes, and Pod disruption budgets to manage containers; how to optimize cloud storage; and how and when to use preemptible or reserved instances to control costs.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understanding Resources" data-type="sect1"><div class="sect1" id="idm45979388794800">&#13;
<h1>Understanding Resources</h1>&#13;
&#13;
<p>Suppose you have a Kubernetes cluster of a given capacity, with a reasonable number of nodes of the right kind of size. How do you get the most bang for your buck out of it? That is, how do you get the best possible utilization of the available cluster resources for your workload, while still ensuring that you have enough headroom to deal with demand spikes, node failures, and bad deployments?</p>&#13;
&#13;
<p><a data-primary="scheduler" data-secondary="resource evaluation and" data-type="indexterm" id="idm45979388792512"/>To answer this, put yourself in the place of the Kubernetes scheduler and try to see things from its point of view. The scheduler’s job is to decide where to run a given Pod. Are there any nodes with enough free resources to run the Pod?</p>&#13;
&#13;
<p>This question is impossible to answer unless the scheduler knows how many resources the Pod will need to run. A Pod that needs 1 GiB of memory cannot be scheduled on a node with only one hundred MiB of free memory.</p>&#13;
&#13;
<p>Similarly, the scheduler has to be able to take action when a greedy Pod is grabbing too many resources and starving other Pods on the same node. But how much is too much? In order to schedule Pods effectively, the scheduler has to know the minimum and maximum allowable resource requirements for each Pod.</p>&#13;
&#13;
<p>That’s where Kubernetes resource requests and limits come in. Kubernetes understands how to manage two kinds of resources: CPU and memory. There are other important types of resources, too, such as network bandwidth, disk I/O operations (IOPS), and disk space, and these may cause contention in the cluster, but Kubernetes doesn’t yet have a way to describe Pods’ requirements for these.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Units" data-type="sect2"><div class="sect2" id="idm45979388789520">&#13;
<h2>Resource Units</h2>&#13;
&#13;
<p><a data-primary="resources" data-secondary="units" data-type="indexterm" id="idm45979388788320"/>CPU usage for Pods is expressed, as you might expect, in units of CPUs. One Kubernetes CPU unit is equivalent to one AWS virtual CPU (vCPU), one Google Cloud Core, one Azure vCore, or one <em>hyperthread</em> on a bare-metal processor that supports hyperthreading. In other words, <em>1 CPU</em> in Kubernetes terms means what you think it does.</p>&#13;
&#13;
<p>Because most Pods don’t need a whole CPU, requests and limits are usually expressed in <em>millicpus</em> (sometimes called <em>millicores</em>). Memory is measured in bytes, or more handily, <em>mebibytes</em> (MiB).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Requests" data-type="sect2"><div class="sect2" id="resourcerequests">&#13;
<h2>Resource Requests</h2>&#13;
&#13;
<p><a data-primary="resources" data-secondary="requests" data-type="indexterm" id="idm45979388782272"/>A Kubernetes <em>resource request</em> specifies the minimum amount of that resource that the Pod needs to run. For example, a resource request of <code>100m</code> (100 millicpus) and <code>250Mi</code> (250 MiB of memory) means that the Pod cannot be scheduled on a node with less than those resources available. If there isn’t any node with enough capacity available, the Pod will remain in a <code>pending</code> state until there is.</p>&#13;
&#13;
<p>For example, if all your cluster nodes have two CPU cores and 4 GiB of memory, a container that requests 2.5 CPUs will never be scheduled, and neither will one that requests 5 GiB of memory.</p>&#13;
&#13;
<p>Let’s see what resource requests would look like, applied to our demo application:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cloudnatived/demo:hello</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">containerPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w">&#13;
</code><code class="w">    </code><strong><code class="nt">resources</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">requests</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">10Mi</code><code class="s">"</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">100m</code><code class="s">"</code></strong></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Limits" data-type="sect2"><div class="sect2" id="resourcelimits">&#13;
<h2>Resource Limits</h2>&#13;
&#13;
<p><a data-primary="limits, resource" data-type="indexterm" id="idm45979388731040"/><a data-primary="resources" data-secondary="limits" data-type="indexterm" id="idm45979388730336"/>A <em>resource limit</em> specifies the maximum amount of resource that a Pod is allowed to use. A Pod that tries to use more than its allocated CPU limit will be <em>throttled</em>, reducing its performance.</p>&#13;
&#13;
<p>A Pod that tries to use more than the allowed memory limit, though, will be terminated. If the terminated Pod can be rescheduled, it will be. In practice, this may mean that the Pod is simply restarted on the same node.</p>&#13;
&#13;
<p>Some applications, such as network servers, can consume more and more resources over time in response to increasing demand. Specifying resource limits is a good way to prevent such hungry Pods from using more than their fair share of the cluster’s capacity.</p>&#13;
&#13;
<p>Here’s an example of setting resource limits on the demo application:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cloudnatived/demo:hello</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">containerPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w">&#13;
</code><code class="w">    </code><strong><code class="nt">resources</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">limits</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">20Mi</code><code class="s">"</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"</code><code class="s">250m</code><code class="s">"</code></strong></pre>&#13;
&#13;
<p>Knowing what limits to set for a particular application is a matter of observation and judgment (see <a data-type="xref" href="#optimizingpods">“Optimizing Pods”</a>).</p>&#13;
&#13;
<p><a data-primary="overcommitting resources" data-type="indexterm" id="idm45979388633856"/>Kubernetes allows resources to be <em>overcommitted</em>; that is, the sum of all the resource limits of containers on a node can exceed the total resources of that node. This is a kind of gamble: the scheduler is betting that, most of the time, most containers will not need to hit their resource limits.</p>&#13;
&#13;
<p>If this gamble fails, and total resource usage starts approaching the maximum capacity of the node, Kubernetes will start being more aggressive in terminating containers. Under conditions of resource pressure, containers that have exceeded their requests, but not their limits, may still be terminated.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Quality of Service" data-type="sect2"><div class="sect2" id="idm45979388720224">&#13;
<h2>Quality of Service</h2>&#13;
&#13;
<p><a data-primary="Quality of Service (QoS)" data-type="indexterm" id="idm45979388642768"/>Based on the requests and limits of a Pod, Kubernetes will classify it as one of the following <a href="https://oreil.ly/x0m1T">Quality of Service (QoS) classes</a>: <em>Guaranteed</em>, <em>Burstable</em>, or <em>BestEffort</em>.</p>&#13;
&#13;
<p>When requests match limits, a Pod is classified in the Guaranteed class, meaning it is considered by the control plane to be among the most important Pods to schedule, and it will do its best to ensure the Pod is only killed if it exceeds the specified limits. For highly critical production workloads you may want to consider setting your limits and requests to match in order to prioritize scheduling these containers.</p>&#13;
&#13;
<p>Burstable Pods have lower priority than Guaranteed Pods, and Kubernetes will allow them to “burst” above their request up to their limit if capacity on the node is available. If the Pod is using more resources than requested, the Pod <em>may</em> be killed, if the control plane needs to make way for scheduling a Pod of a higher QoS class.</p>&#13;
&#13;
<p>If a Pod does not specify any requests or limits, it is considered to be  <em>Best-Effort</em>, which is the lowest priority. Pods are allowed to use whatever CPU and memory is available on the node, but it would be the first to be killed when the cluster needs to make room for higher QoS Pods.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Always specify resource requests and limits for your containers. This helps Kubernetes schedule and manage your Pods properly.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Managing the Container Life Cycle" data-type="sect1"><div class="sect1" id="idm45979388794176">&#13;
<h1>Managing the Container Life Cycle</h1>&#13;
&#13;
<p><a data-primary="containers" data-secondary="managing container life cycles" data-type="indexterm" id="ix_05-resources-adoc1"/>We’ve seen that Kubernetes can best manage your Pods when it knows what their CPU and memory requirements are. But it also has to know when a container is working: that is, when it’s functioning properly and ready to handle requests.</p>&#13;
&#13;
<p>It’s quite common for containerized applications to get into a stuck state, where the process is still running, but it’s not serving any requests. Kubernetes needs a way to detect this situation so that it can restart the container to fix the problem.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Liveness Probes" data-type="sect2"><div class="sect2" id="liveness">&#13;
<h2>Liveness Probes</h2>&#13;
&#13;
<p><a data-primary="liveness probe" data-type="indexterm" id="idm45979388596192"/><a data-primary="probes" data-secondary="liveness" data-type="indexterm" id="idm45979388595488"/>Kubernetes lets you specify a <em>liveness</em> probe as part of the container spec: a health check that determines whether or not the container is alive (that is, working).</p>&#13;
&#13;
<p>For an HTTP server container, the liveness probe specification usually looks something like this:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">livenessProbe</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">httpGet</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/healthz</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">initialDelaySeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">periodSeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">failureThreshold</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2</code><code class="w"/></pre>&#13;
&#13;
<p><a data-primary="httpGet probe" data-type="indexterm" id="idm45979388527712"/><a data-primary="probes" data-secondary="httpGet" data-type="indexterm" id="idm45979388572288"/>The <code>httpGet</code> probe makes an HTTP request to a URI and port you specify; in this case, <code>/healthz</code> on port 8888.</p>&#13;
&#13;
<p><a data-primary="probes" data-secondary="/healthz" data-secondary-sortas="healthz" data-type="indexterm" id="idm45979388570128"/><a data-primary="/healthz endpoint" data-primary-sortas="healthz" data-type="indexterm" id="idm45979388568656"/>If your application doesn’t have a specific endpoint for a health check, you could use <code>/</code>, or any valid URL for your application. It’s common practice, though, to create a <code>/healthz</code> endpoint just for this purpose. (Why the <code>z</code>? Just to make sure it doesn’t collide with an existing path like <code>health</code>, which could be a page about health information, for example).</p>&#13;
&#13;
<p>If the application responds with an HTTP 2xx or 3xx status code, Kubernetes considers it alive. If it responds with anything else, or doesn’t respond at all, the container is considered dead, and will be restarted.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Probe Delay and Frequency" data-type="sect2"><div class="sect2" id="idm45979388564896">&#13;
<h2>Probe Delay and Frequency</h2>&#13;
&#13;
<p>How soon should Kubernetes start checking your liveness probe? No application can start instantly. If Kubernetes tried the liveness probe immediately after starting the container, it would probably fail, causing the container to be restarted—and this loop would repeat forever!</p>&#13;
&#13;
<p><a data-primary="initialDelaySeconds" data-type="indexterm" id="idm45979388562784"/><a data-primary="probes" data-secondary="initialDelaySeconds" data-type="indexterm" id="idm45979388561856"/>The <code>initialDelaySeconds</code> field lets you tell Kubernetes how long to wait before trying the first liveness probe, avoiding this <em>loop of death</em> situation. As of version 1.20 of Kubernetes, there is also a dedicated <code>startupProbe</code> feature for configuring a probe to determine when an application has finished starting up. See <a data-type="xref" href="#startup">“Startup Probes”</a> for more details.</p>&#13;
&#13;
<p>It wouldn’t be a good idea for Kubernetes to hammer your application with requests for the <code>healthz</code> endpoint thousands of times a second. Your health check endpoints should be fast and not add noticeable load to the app. You wouldn’t want your user experience to suffer because your app is otherwise busy responding to a flood of health checks. <a data-primary="periodSeconds" data-type="indexterm" id="idm45979388557392"/><a data-primary="probes" data-secondary="periodSeconds" data-type="indexterm" id="idm45979388556688"/>The <code>periodSeconds</code> field specifies how often the liveness probe should be checked; in this example, every three seconds.</p>&#13;
&#13;
<p><code>failureThreshold</code> allows you to set how many times the probe can fail before Kubernetes considers the application unhealthy. The default is three, which allows for a few hiccups in your app, but you may need to make that lower or higher depending on how aggressive you want the scheduler to be when making decisions about determining an application’s health.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other Types of Probes" data-type="sect2"><div class="sect2" id="idm45979388554256">&#13;
<h2>Other Types of Probes</h2>&#13;
&#13;
<p><a data-primary="probes" data-secondary="tcpSocket" data-type="indexterm" id="idm45979388552976"/><a data-primary="tcpSocket probe" data-type="indexterm" id="idm45979388552000"/><code>httpGet</code> isn’t the only kind of probe available; for network servers that don’t speak HTTP, you can use <code>tcpSocket</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">livenessProbe</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">tcpSocket</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w"/></pre>&#13;
&#13;
<p>If a TCP connection to the specified port succeeds, the container is alive.</p>&#13;
&#13;
<p><a data-primary="exec probe" data-type="indexterm" id="idm45979388500656"/><a data-primary="probes" data-secondary="exec" data-type="indexterm" id="idm45979388500048"/>You can also run an arbitrary command on the container, using an <code>exec</code> probe:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">livenessProbe</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">exec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">command</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">cat</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">/tmp/healthy</code><code class="w"/></pre>&#13;
&#13;
<p>The <code>exec</code> probe runs the specified command inside the container, and the probe succeeds if the command succeeds (that is, exits with a zero status). <code>exec</code> is usually more useful as a readiness probe, and we’ll see how they’re used in the next section.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Readiness Probes" data-type="sect2"><div class="sect2" id="readiness-probes">&#13;
<h2>Readiness Probes</h2>&#13;
&#13;
<p><a data-primary="probes" data-secondary="readiness" data-type="indexterm" id="idm45979388443248"/><a data-primary="readiness probe" data-type="indexterm" id="idm45979388442272"/>Related to the liveness probe, but with different semantics, is the <em>readiness probe</em>. Sometimes an application needs to signal to Kubernetes that it’s temporarily unable to handle requests; perhaps because it’s performing some lengthy initialization process, or waiting for some subprocess to complete. The readiness probe serves this function.</p>&#13;
&#13;
<p>If your application doesn’t start listening for HTTP until it’s ready to serve, your readiness probe can be the same as your liveness probe:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">readinessProbe</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">httpGet</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/healthz</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">initialDelaySeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">periodSeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/></pre>&#13;
&#13;
<p>A container that fails its readiness probe will be removed from any Services that match the Pod. This is like taking a failing node out of a load balancer pool: no traffic will be sent to the Pod until its readiness probe starts succeeding again. Note that this is different from a <code>livenessProbe</code> because a failing <code>readinessProbe</code> does not kill and restart the Pod.</p>&#13;
&#13;
<p>Normally, when a Pod starts, Kubernetes will start sending it traffic as soon as the container is in a running state. However, if the container has a readiness probe, Kubernetes will wait until the probe succeeds before sending it any requests so that users won’t see errors from unready containers. <a data-primary="deployments" data-secondary="zero-downtime" data-type="indexterm" id="idm45979388393712"/>This is critically important for zero-downtime upgrades (see <a data-type="xref" href="ch13.html#deploymentstrategies">“Deployment Strategies”</a> for more about these).</p>&#13;
&#13;
<p>A container that is not ready will still be shown as <code>Running</code>, but the <code>READY</code> column will show one or more unready containers in the Pod:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods</code></strong><code class="go">&#13;
</code><code class="go">NAME             READY     STATUS    RESTARTS   AGE&#13;
</code><code class="go">readiness-test   0/1       Running   0          56s</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Readiness probes should only return HTTP <code>200 OK</code> status. Although Kubernetes itself considers both 2xx and 3xx status codes as <em>ready</em>, cloud load balancers may not. If you’re using an Ingress resource coupled with a cloud load balancer (see <a data-type="xref" href="ch09.html#ingress">“Ingress”</a>), and your readiness probe returns a 301 redirect, for example, the load balancer may flag all your Pods as unhealthy. Make sure your readiness probes only return a 200 status code.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Startup Probes" data-type="sect2"><div class="sect2" id="startup">&#13;
<h2>Startup Probes</h2>&#13;
&#13;
<p><a data-primary="probes" data-secondary="startup" data-type="indexterm" id="idm45979388382000"/><a data-primary="startup probes" data-type="indexterm" id="idm45979388381024"/>In addition to liveness probes with <code>initialDelaySeconds</code>, Kubernetes offers another way to determine when an application has finished starting up. Some applications require a longer period of time to initialize, or maybe you would like to instrument a special endpoint in your application for checking startup status that is different from your other liveness and readiness checks.</p>&#13;
&#13;
<p>When a <code>startupProbe</code> is configured, the <code>livenessProbe</code> will wait on it to succeed before beginning the liveness checks. If it never succeeds, Kubernetes will kill and restart the Pod.</p>&#13;
&#13;
<p>The syntax for a <code>startupProbe</code> is similar to liveness and readiness probes:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">livenessProbe</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">httpGet</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/healthz</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">failureThreshold</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2</code><code class="w"/>&#13;
&#13;
<code class="nt">startupProbe</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">httpGet</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/healthz</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8888</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">failureThreshold</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10</code><code class="w"/></pre>&#13;
&#13;
<p>In this example, notice how our <code>livenessProbe</code> will consider the Pod as unhealthy after two failures because of the <code>failureThreshold</code>, but we are giving the application more time to start with <code>failureThreshold: 10</code> in the <code>startupProbe</code>. This would hopefully prevent the situation where the Pod may not start quickly enough and the livenessProbe would otherwise give up and restart it before it has a chance to run.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="gRPC Probes" data-type="sect2"><div class="sect2" id="idm45979388313712">&#13;
<h2>gRPC Probes</h2>&#13;
&#13;
<p><a data-primary="gRPC" data-type="indexterm" id="idm45979388301552"/><a data-primary="probes" data-secondary="gRPC" data-type="indexterm" id="idm45979388300960"/>Although many applications and services communicate via HTTP, it’s increasingly popular to use the <a href="https://grpc.io">Google Remote Procedure Call (gRPC)</a>  protocol instead, especially for microservices. gRPC is an efficient, portable, binary network protocol developed by Google and hosted by the Cloud Native Computing <span class="keep-together">Foundation</span>.</p>&#13;
&#13;
<p><code>httpGet</code> probes will not work with gRPC servers, and although you could use a <code>tcpSocket</code> probe instead, that only tells you that you can make a connection to the socket, not that the server itself is working.</p>&#13;
&#13;
<p>gRPC has a standard health-checking protocol, which most gRPC services support, and to interrogate this health check with a Kubernetes liveness probe you can use the <a href="https://oreil.ly/sJp7V"><code>grpc-health-probe</code> tool</a>. If you add the tool to your container, you can check it using an <code>exec</code> probe.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="File-Based Readiness Probes" data-type="sect2"><div class="sect2" id="idm45979388295248">&#13;
<h2>File-Based Readiness Probes</h2>&#13;
&#13;
<p><a data-primary="file probe" data-type="indexterm" id="idm45979388293696"/><a data-primary="probes" data-secondary="file" data-type="indexterm" id="idm45979388292992"/>Alternatively, you could have the application create a file on the container’s filesystem called something like <em>/tmp/healthy</em>, and use an <code>exec</code> readiness probe to check for the presence of that file.</p>&#13;
&#13;
<p>This kind of readiness probe can be useful because if you want to take the container temporarily out of service to debug a problem, you can attach to the container and delete the <em>/tmp/healthy</em> file. The next readiness probe will fail, and Kubernetes will remove the container from any matching Services. (A better way to do this, though, is to adjust the container’s labels so that it no longer matches the service: see <a data-type="xref" href="ch04.html#services">“Service Resources”</a>.)</p>&#13;
&#13;
<p>You can now inspect and troubleshoot the container at your leisure. Once you’re done, you can either terminate the container and deploy a fixed version, or put the probe file back in place so that the container will start receiving traffic again.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use readiness probes and liveness probes to let Kubernetes know when your application is ready to handle requests, or when it has a problem and needs to be restarted. It is also important to think about how your application functions in the context of the broader ecosystem and what should happen when it fails. You can end up in a cascading failure scenario if your probes are interconnected and share dependencies.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="minReadySeconds" data-type="sect2"><div class="sect2" id="minreadyseconds">&#13;
<h2>minReadySeconds</h2>&#13;
&#13;
<p><a data-primary="minReadySeconds" data-type="indexterm" id="idm45979388226192"/>By default, a container or Pod is considered ready the moment its readiness probe succeeds. In some cases, you may want to run the container for a short while to make sure it is stable. During a deployment, Kubernetes waits until each new Pod is ready before starting the next (see <a data-type="xref" href="ch13.html#rollingupdate">“Rolling Updates”</a>). If a faulty container crashes straightaway, this will halt the rollout, but if it takes a few seconds to crash, all its replicas might be rolled out before you discover the problem.</p>&#13;
&#13;
<p>To avoid this, you can set the <code>minReadySeconds</code> field on the container. A container or Pod will not be considered ready until its readiness probe has been up for <span class="keep-together"><code>minReadySeconds</code></span> (default 0).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Disruption Budgets" data-type="sect2"><div class="sect2" id="poddisruptionbudgets">&#13;
<h2>Pod Disruption Budgets</h2>&#13;
&#13;
<p><a data-primary="eviction" data-type="indexterm" id="idm45979388220576"/><a data-primary="PodDisruptionBudget" data-type="indexterm" id="idm45979388219872"/>Sometimes Kubernetes needs to stop your Pods even though they’re alive and ready (a process called <em>eviction</em>). Perhaps the node they’re running on is being drained prior to an upgrade, for example, and the Pods need to be moved to another node.</p>&#13;
&#13;
<p>However, this needn’t result in downtime for your application, provided enough replicas can be kept running. You can use the <code>PodDisruptionBudget</code> resource to specify, for a given application, how many Pods you can afford to lose at any given time.</p>&#13;
&#13;
<p>For example, you might specify that no more than 10% of your application’s Pods can be disrupted at once. Or perhaps you want to specify that Kubernetes can evict any number of Pods, provided that at least three replicas are always running.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="minAvailable" data-type="sect3"><div class="sect3" id="idm45979388216832">&#13;
<h3>minAvailable</h3>&#13;
&#13;
<p><a data-primary="minAvailable" data-type="indexterm" id="idm45979388215632"/>Here’s an example of a PodDisruptionBudget that specifies a minimum number of Pods to be kept running, using the <code>minAvailable</code> field:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">policy/v1beta1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PodDisruptionBudget</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-pdb</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="nt">minAvailable</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code></strong><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code></pre>&#13;
&#13;
<p>In this example, <code>minAvailable: 3</code> specifies that at least three Pods matching the label <code>app: demo</code> should always be running. Kubernetes can evict as many <code>demo</code> Pods as it wants, so long as there are always at least three left.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="maxUnavailable" data-type="sect3"><div class="sect3" id="idm45979388177696">&#13;
<h3>maxUnavailable</h3>&#13;
&#13;
<p><a data-primary="maxUnavailable" data-type="indexterm" id="idm45979388176448"/>Conversely, you can use <code>maxUnavailable</code> to limit the total number or percentage of Pods that Kubernetes is allowed to evict:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">policy/v1beta1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PodDisruptionBudget</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-pdb</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><strong><code class="nt">maxUnavailable</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10%</code></strong><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code></pre>&#13;
&#13;
<p>Here, no more than 10% of <code>demo</code> Pods are allowed to be evicted at any one time. This only applies to so-called <em>voluntary evictions</em>, though; that is to say, evictions initiated by Kubernetes. If a node suffers a hardware failure or gets deleted, for example, the Pods on it will be involuntarily evicted, even if that would violate the disruption budget.</p>&#13;
&#13;
<p>Since Kubernetes will tend to spread Pods evenly across nodes, all other things being equal, this is worth bearing in mind when considering how many nodes your cluster needs. If you have three nodes, the failure of one could result in the loss of a third of all your Pods, and that may not leave enough to maintain an acceptable level of service (see <a data-type="xref" href="ch03.html#highavailability">“High Availability”</a>).</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Set PodDisruptionBudgets for your business-critical applications to make sure there are always enough replicas to maintain the service, even when Pods are evicted.<a data-startref="ix_05-resources-adoc1" data-type="indexterm" id="idm45979388108144"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Namespaces" data-type="sect1"><div class="sect1" id="namespaces">&#13;
<h1>Using Namespaces</h1>&#13;
&#13;
<p><a data-primary="namespaces" data-secondary="using" data-type="indexterm" id="ix_05-resources-adoc2"/>Another very useful way of managing resource usage across your cluster is to use <em>namespaces</em>. A Kubernetes namespace is a way of partitioning your cluster into separate subdivisions, for whatever purpose you like.</p>&#13;
&#13;
<p>For example, you might have different namespaces for testing out different versions of an application, or a separate namespace per team. As the term <em>namespace</em> suggests, names in one namespace are not visible from a different namespace.</p>&#13;
&#13;
<p>This means that you could have a service called <code>demo</code> in the <code>prod</code> namespace, and a different service called <code>demo</code> in the <code>test</code> namespace, and there won’t be any conflict.</p>&#13;
&#13;
<p>To see the namespaces that exist on your cluster, run the following command:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get namespaces</code></strong><code class="go">&#13;
</code><code class="go">NAME           STATUS    AGE&#13;
</code><code class="go">default        Active    1y&#13;
</code><code class="go">kube-public    Active    1y&#13;
</code><code class="go">kube-system    Active    1y</code></pre>&#13;
&#13;
<p>You can think of namespaces as being a bit like folders on your computer’s hard disk. While you <em>could</em> keep all your files in the same folder, it would be inconvenient. Looking for a particular file would be time-consuming, and it wouldn’t be easy to see which files belong with which others. A namespace groups related resources together, and makes it easier to work with them. Unlike folders, however, namespaces can’t be nested.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Working with Namespaces" data-type="sect2"><div class="sect2" id="idm45979388063184">&#13;
<h2>Working with Namespaces</h2>&#13;
&#13;
<p><a data-primary="namespaces" data-secondary="default" data-type="indexterm" id="idm45979388062048"/>So far, when working with Kubernetes we’ve always used the <em>default namespace</em>. If you don’t specify a namespace when running a <code>kubectl</code> command, such as <code>kubectl run</code>, your command will operate on the default namespace. <a data-primary="namespaces" data-secondary="kube-system" data-type="indexterm" id="idm45979388059472"/>If you’re wondering what the <code>kube-system</code> namespace is, that’s where the Kubernetes internal system components run so that they’re segregated from your own applications.</p>&#13;
&#13;
<p>If, instead, you specify a namespace with the <code>--namespace</code> flag (or <code>-n</code> for short), your command will use that namespace. For example, to get a list of Pods in the <code>prod</code> namespace, run:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods --namespace prod</code></strong></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Namespaces Should I Use?" data-type="sect2"><div class="sect2" id="idm45979388033600">&#13;
<h2>What Namespaces Should I Use?</h2>&#13;
&#13;
<p>It’s entirely up to you how to divide your cluster into namespaces. One idea that makes intuitive sense is to have one namespace per application, or per team. For example, you might create a <code>demo</code> namespace to run the demo application in. You can create a namespace using a Kubernetes namespace resource like the following:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Namespace</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><strong><code class="l-Scalar-Plain">demo</code></strong></pre>&#13;
&#13;
<p>To apply this resource manifest, use the <code>kubectl apply -f</code> command (see <a data-type="xref" href="ch04.html#applying">“Resource Manifests in YAML Format”</a> for more about this.) You’ll find the YAML manifests for all the examples in this section in the demo application repo, in the <em>hello-namespace</em> directory:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">cd demo/hello-namespace</code></strong><code class="go">&#13;
</code><strong><code class="go">ls k8s</code></strong><code class="go">&#13;
</code><code class="go">deployment.yaml    limitrange.yaml    namespace.yaml     resourcequota.yaml&#13;
</code><code class="go">service.yaml</code></pre>&#13;
&#13;
<p>You could go further and create namespaces for each environment your app runs in, such as <code>demo-prod</code>, <code>demo-staging</code>, <code>demo-test</code>, and so on. You could use a namespace as a kind of temporary <em>virtual cluster</em>, and delete the namespace when you’re finished with it. But be careful! Deleting a namespace deletes all the resources within it. You really don’t want to run that command against the wrong namespace. (See <a data-type="xref" href="ch11.html#rbac">“Introducing Role-Based Access Control (RBAC)”</a> for how to grant or deny user permissions on individual namespaces.)</p>&#13;
&#13;
<p>In the current version of Kubernetes, there is no way to <em>protect</em> a resource such as a namespace from being deleted (though a <a href="https://oreil.ly/vk69W">proposal</a> for such a feature is under discussion). So don’t delete namespaces unless they really are temporary and you’re sure they don’t contain any production resources.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Create separate namespaces for each of your applications or each logical component of your infrastructure. Don’t use the default namespace: it’s too easy to make mistakes.</p>&#13;
</div>&#13;
&#13;
<p><a data-primary="network policies" data-type="indexterm" id="idm45979387960176"/>If you need to block all network traffic in or out of a particular namespace, you can use <a href="https://oreil.ly/WOiKZ">Kubernetes Network Policies</a> to enforce this.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service Addresses" data-type="sect2"><div class="sect2" id="idm45979387938656">&#13;
<h2>Service Addresses</h2>&#13;
&#13;
<p>Although namespaces are logically isolated from one another, they can still communicate with Services in other namespaces. <a data-primary="Service objects" data-secondary="DNS names and" data-type="indexterm" id="idm45979387936912"/>You may recall from <a data-type="xref" href="ch04.html#services">“Service Resources”</a> that every <a data-primary="DNS names, communicating with Services in other namespaces" data-type="indexterm" id="idm45979387935136"/>Kubernetes Service has an associated DNS name that you can use to talk to it. Connecting to the hostname <code>demo</code> will connect you to the Service whose name is <code>demo</code>. How does that work across different namespaces?</p>&#13;
&#13;
<p>Service DNS names always follow this pattern:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><code class="go">SERVICE.NAMESPACE.svc.cluster.local</code></pre>&#13;
&#13;
<p>The <code>.svc.cluster.local</code> part is optional, and so is the namespace. But if you want to talk to the <code>demo</code> Service in the <code>prod</code> namespace, for example, you can use:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><code class="go">demo.prod</code></pre>&#13;
&#13;
<p>Even if you have a dozen different Services called <code>demo</code>, each in its own namespace, you can add the namespace to the DNS name for the Service to specify exactly which one you mean.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Quotas" data-type="sect2"><div class="sect2" id="idm45979387897840">&#13;
<h2>Resource Quotas</h2>&#13;
&#13;
<p><a data-primary="ResourceQuota" data-type="indexterm" id="idm45979387896528"/>As well as restricting the CPU and memory usage of individual containers, which you learned about in <a data-type="xref" href="#resourcerequests">“Resource Requests”</a>, you can (and should) restrict the resource usage of a given namespace. The way to do this is to create a ResourceQuota in the namespace.</p>&#13;
&#13;
<p>Here’s an example ResourceQuota:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ResourceQuota</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-resourcequota</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">hard</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">pods</code><code class="p">:</code><code class="w"> </code><code class="s">"100"</code><code class="w"/></pre>&#13;
&#13;
<p>Applying this manifest to a particular namespace (for example, <code>demo</code>) sets a hard limit of one hundred Pods running at once in that namespace. (Note that the <code>metadata.name</code> of the ResourceQuota can be anything you like. The namespaces it affects depends on which namespaces you apply the manifest to.)</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">cd demo/hello-namespace</code></strong><code class="go">&#13;
</code><strong><code class="go">kubectl create namespace demo</code></strong><code class="go">&#13;
</code><code class="go">namespace "demo" created&#13;
</code><strong><code class="go">kubectl apply --namespace demo -f k8s/resourcequota.yaml</code></strong><code class="go">&#13;
</code><code class="go">resourcequota "demo-resourcequota" created</code></pre>&#13;
&#13;
<p>Now Kubernetes will block any API operations in the <code>demo</code> namespace that would exceed the quota. The example ResourceQuota limits the namespace to 100 Pods, so if there are 100 Pods already running and you try to start a new one, you will see an error message like this:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><code class="go">Error from server (Forbidden): pods "demo" is forbidden: exceeded quota:</code>&#13;
<code class="go">demo-resourcequota, requested: pods=1, used: pods=100, limited: pods=100</code></pre>&#13;
&#13;
<p>Using ResourceQuotas is a good way to stop applications in one namespace from grabbing too many resources and starving those in other parts of the cluster.</p>&#13;
&#13;
<p>You can also limit the total CPU and memory usage of Pods in a namespace. This can be useful for budgeting in large organizations where many different teams are sharing a Kubernetes cluster. Teams could be required to set the number of CPUs they will use for their namespace, and if they exceed that quota, they will not be able to use more cluster resources until the ResourceQuota is increased.</p>&#13;
&#13;
<p>A Pod limit can be useful to prevent a misconfiguration or typing error from generating a potentially unlimited number of Pods. It’s easy to forget to clean up some object from a regular task, and find one day that you’ve got thousands of them clogging up your cluster.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use ResourceQuotas in each namespace to enforce a limit on the number of Pods that can run in the namespace.</p>&#13;
</div>&#13;
&#13;
<p>To check if a ResourceQuota is active in a particular namespace, use the <code>kubectl describe resourcequotas</code> command:</p>&#13;
&#13;
<pre class="less_space pagebreak-before" data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl describe resourcequota -n demo</code></strong><code class="go">&#13;
</code><code class="go">Name:       demo-resourcequota&#13;
</code><code class="go">Namespace:  demo&#13;
</code><code class="go">Resource    Used  Hard&#13;
</code><code class="go">--------    ----  ----&#13;
</code><code class="go">pods        1     100</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Default Resource Requests and Limits" data-type="sect2"><div class="sect2" id="idm45979387897248">&#13;
<h2>Default Resource Requests and Limits</h2>&#13;
&#13;
<p><a data-primary="resources" data-secondary="limits" data-type="indexterm" id="idm45979387760144"/>It’s not always easy to know what your container’s resource requirements are going to be in advance. <a data-primary="LimitRange" data-type="indexterm" id="idm45979387758912"/>You can set default requests and limits for all containers in a namespace using a LimitRange resource:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">LimitRange</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-limitrange</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">limits</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">default</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"500m"</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="s">"256Mi"</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">defaultRequest</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">cpu</code><code class="p">:</code><code class="w"> </code><code class="s">"200m"</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">memory</code><code class="p">:</code><code class="w"> </code><code class="s">"128Mi"</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Container</code><code class="w"/></pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>As with ResourceQuotas, the <code>metadata.name</code> of the LimitRange can be whatever you want. It doesn’t correspond to a Kubernetes namespace, for example. A LimitRange or ResourceQuota takes effect in a particular namespace only when you apply the manifest to that namespace.</p>&#13;
</div>&#13;
&#13;
<p>Any container in the namespace that doesn’t specify a resource limit or request will inherit the default value from the <code>LimitRange</code>. For example, a container with no <code>cpu</code> request specified will inherit the value of <code>200m</code> from the <code>LimitRange</code>. Similarly, a container with no <code>memory</code> limit specified will inherit the value of <code>256Mi</code> from the LimitRange.</p>&#13;
&#13;
<p>In theory, then, you could set the defaults in a LimitRange and not bother to specify requests or limits for individual containers. However, this isn’t good practice: it should be possible to look at a container spec and see what its requests and limits are, without having to know whether or not a LimitRange is in effect. Use the LimitRange only as a backstop to prevent problems with containers whose owners forgot to specify requests and limits.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use LimitRanges in each namespace to set default resource requests and limits for containers, but don’t rely on them; treat them as a backstop. Always specify explicit requests and limits in the container spec itself.<a data-startref="ix_05-resources-adoc2" data-type="indexterm" id="idm45979387661664"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimizing Cluster Costs" data-type="sect1"><div class="sect1" id="idm45979388106592">&#13;
<h1>Optimizing Cluster Costs</h1>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="optimizing cluster costs" data-type="indexterm" id="ix_05-resources-adoc3"/>In <a data-type="xref" href="ch06.html#sizing">“Cluster Sizing and Scaling”</a>, we outlined some considerations for choosing the initial size of your cluster, and scaling it over time as your workloads evolve. But, assuming that your cluster is correctly sized and has sufficient capacity, how should you run it in the most cost-effective way?</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubecost" data-type="sect2"><div class="sect2" id="idm45979387656704">&#13;
<h2>Kubecost</h2>&#13;
&#13;
<p><a data-primary="Kubecost" data-type="indexterm" id="idm45979387655536"/>It can often be difficult to get an overall sense of the cost involved with running the Kubernetes infrastructure when there are several applications and teams all sharing the same clusters.</p>&#13;
&#13;
<p>Fortunately there is a tool called <a href="https://oreil.ly/j4ppy">Kubecost</a> for tracking costs per namespace, label, or even down to the container level. <a href="https://www.kubecost.com">Kubecost</a> is currently free for a single cluster, and there are paid versions with support for larger environments.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimizing Deployments" data-type="sect2"><div class="sect2" id="idm45979387629520">&#13;
<h2>Optimizing Deployments</h2>&#13;
&#13;
<p><a data-primary="replicas" data-secondary="sizing" data-type="indexterm" id="idm45979387628080"/>Do you really need quite so many replicas? It may seem an obvious point, but every Pod in your cluster uses up some resources that are thus unavailable to some other Pod.</p>&#13;
&#13;
<p>It can be tempting to run a large number of replicas for everything so that quality of service will never be reduced if individual Pods fail, or during rolling upgrades. Also, the more replicas, the more traffic your apps can handle.</p>&#13;
&#13;
<p>But you should use replicas wisely. Your cluster can only run a finite number of Pods. Give them to applications that really need maximum availability and performance.</p>&#13;
&#13;
<p>If it really doesn’t matter that a given Deployment is down for a few seconds during an upgrade, it doesn’t need a lot of replicas. A surprisingly large number of applications and services can get by perfectly well with one or two replicas.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Review the number of replicas configured for each Deployment, and ask:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>What are the business requirements for performance and availability for this service?</p>&#13;
</li>&#13;
<li>&#13;
<p>Can we meet those requirements with fewer replicas?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>If an app is struggling to handle demand, or users get too many errors when you upgrade the Deployment, it needs more replicas. But in many cases you can reduce the size of a Deployment considerably before you get to the point where the degradation starts to be noticeable.</p>&#13;
&#13;
<p>Later on in <a data-type="xref" href="ch06.html#autoscaling">“Autoscaling”</a>, we will cover ways you can leverage autoscaling to save costs at times when you know your usage is low.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use the minimum number of Pods for a given Deployment that will satisfy your performance and availability requirements. Gradually reduce the number of replicas to just above the point where your service level objectives are met.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimizing Pods" data-type="sect2"><div class="sect2" id="optimizingpods">&#13;
<h2>Optimizing Pods</h2>&#13;
&#13;
<p><a data-primary="resources" data-secondary="limits" data-type="indexterm" id="idm45979387617200"/>Earlier in this chapter, in <a data-type="xref" href="#resourcerequests">“Resource Requests”</a>, we emphasized the importance of setting the correct resource requests and limits for your containers. <a data-primary="resources" data-secondary="requests" data-type="indexterm" id="idm45979387615168"/>If the resource requests are too small, you’ll soon know about it: Pods will start failing. If they are too large, however, the first time you find out about it may be when you get your monthly cloud bill.</p>&#13;
&#13;
<p>You should regularly review the resource requests and limits for your various workloads, and compare them against what was actually used.</p>&#13;
&#13;
<p>Most managed Kubernetes services offer some kind of dashboard showing the CPU and memory usage of your containers over time—we’ll see more about this in <a data-type="xref" href="ch11.html#clustermonitoring">“Monitoring Cluster Status”</a>.</p>&#13;
&#13;
<p>You can also build your own dashboards and statistics using Prometheus and Grafana, and we’ll cover this in detail in <a data-type="xref" href="ch15.html#observability">Chapter 15</a>.</p>&#13;
&#13;
<p>Setting the optimal resource requests and limits is something of an art, and the answer will be different for every kind of workload. Some containers may be idle most of the time, occasionally spiking their resource usage to handle a request; others may be constantly busy, and gradually use more and more memory until they hit their limits.</p>&#13;
&#13;
<p>In general, you should set the resource limits for a container to a little above the maximum it uses in normal operation. For example, if a given container’s memory usage over a few days never exceeds 500 MiB of memory, you might set its memory limit to 600 MiB.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Should containers have limits at all? One school of thought says that containers should have <em>no</em> limits in production, or that the limits should be set so high that the containers will never exceed them. With very large and resource-hungry containers that are expensive to restart, this may make some sense, but we think it’s better to set limits anyway. Without them, a container that has a memory leak, or that uses too much CPU, can gobble up all the resources available on a node, starving other containers.</p>&#13;
&#13;
<p>To avoid this <em>resource Pac-Man</em> scenario, set a container’s limits to a little more than 100% of normal usage. This will ensure it’s not killed as long as it’s working properly, but still minimize the blast radius if something goes wrong.</p>&#13;
</div>&#13;
&#13;
<p>Request settings are less critical than limits, but they still should not be set too high (as the Pod will never be scheduled), or too low (as Pods that exceed their requests are first in line for eviction).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vertical Pod Autoscaler" data-type="sect2"><div class="sect2" id="idm45979387618688">&#13;
<h2>Vertical Pod Autoscaler</h2>&#13;
&#13;
<p><a data-primary="autoscaling" data-secondary="Vertical Pod Autoscaler" data-type="indexterm" id="idm45979387605040"/><a data-primary="Vertical Pod Autoscaler" data-type="indexterm" id="idm45979387604064"/>There is a Kubernetes add-on called the <a href="https://oreil.ly/lQtDN">Vertical Pod Autoscaler</a>, which can help you work out the ideal values for resource requests. It will watch a specified Deployment and automatically adjust the resource requests for its Pods based on what they actually use. It has a dry-run mode that will just make suggestions, without actually modifying the running Pods, and this can be helpful.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimizing Nodes" data-type="sect2"><div class="sect2" id="idm45979387602352">&#13;
<h2>Optimizing Nodes</h2>&#13;
&#13;
<p><a data-primary="nodes" data-secondary="sizing" data-type="indexterm" id="idm45979387600480"/>Kubernetes can work with a wide range of node sizes, but some will perform better than others. To get the best cluster capacity for your money, you need to observe how your nodes perform in practice, under real-demand conditions, with your specific workloads. This will help you determine the most cost-effective instance types.</p>&#13;
&#13;
<p>It’s worth remembering that every node has to have an operating system on it, which consumes disk, memory, and CPU resources. So do the Kubernetes system components and the container runtime. The smaller the node, the bigger a proportion of its total resources this overhead represents.</p>&#13;
&#13;
<p>Larger nodes, therefore, can be more cost-effective, because a greater proportion of their resources are available for your workloads. The trade-off is that losing an individual node has a bigger effect on your cluster’s available capacity.</p>&#13;
&#13;
<p><a data-primary="resources" data-secondary="stranded" data-type="indexterm" id="idm45979387597744"/>Small nodes also have a higher percentage of <em>stranded resources</em>: chunks of memory space and CPU time that are unused, but too small for any existing Pod to claim them.</p>&#13;
&#13;
<p>A good <a href="https://oreil.ly/tTwpL">rule of thumb</a> is that nodes should be big enough to run at least five of your typical Pods, keeping the proportion of stranded resources to around 10% or less. If the node can run 10 or more Pods, stranded resources will be below 5%.</p>&#13;
&#13;
<p>The default limit in Kubernetes is 110 Pods per node. Although you can increase this limit by adjusting the <code>--max-pods</code> setting of the <code>kubelet</code>, this may not be possible with some managed services, and it’s a good idea to stick to the Kubernetes defaults unless there is a strong reason to change them.</p>&#13;
&#13;
<p>The Pods-per-node limit means that you may not be able to take advantage of your cloud provider’s largest instance sizes. Instead, consider running a <a href="https://oreil.ly/e3Sim">larger number of smaller nodes</a> to get better utilization. For example, instead of 6 nodes with 8 vCPUs, run 12 nodes with 4 vCPUs.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Look at the percentage resource utilization of each node, using your cloud provider’s dashboard or <code>kubectl top nodes</code>. The bigger the percentage of CPU in use, the better the utilization. If the larger nodes in your cluster have better utilization, you may be well advised to remove some of the smaller nodes and replace them with larger ones.</p>&#13;
</div>&#13;
&#13;
<p>On the other hand, if larger nodes have low utilization, your cluster may be over capacity and you can therefore either remove some nodes or make them smaller, reducing the total bill.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Larger nodes tend to be more cost-effective, because less of their resources are consumed by system overhead. Size your nodes by looking at real-world utilization figures for your cluster, aiming for between 10 and 100 Pods per node.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimizing Storage" data-type="sect2"><div class="sect2" id="idm45979387601728">&#13;
<h2>Optimizing Storage</h2>&#13;
&#13;
<p>One cloud cost that is often overlooked is that of disk storage. Cloud providers offer varying amounts of disk space with each of their instance sizes, and the price of large-scale storage varies too.</p>&#13;
&#13;
<p>While it’s possible to achieve quite high CPU and memory utilization using Kubernetes resource requests and limits, the same is not true of storage, and many cluster nodes are significantly over-provisioned with disk space.</p>&#13;
&#13;
<p><a data-primary="IOPS (I/O operations per second)" data-type="indexterm" id="idm45979387585440"/>Not only do many nodes have more storage space than they need, but the class of storage can also be a factor. Most cloud providers offer different classes of storage depending on the number of I/O operations per second (IOPS), or bandwidth, allocated.</p>&#13;
&#13;
<p>For example, databases that use persistent disk volumes often need a very high IOPS rating, for fast, high-throughput storage access. This is expensive. You can save on cloud costs by provisioning low-IOPS storage for workloads that don’t need so much bandwidth. On the other hand, if your application is performing poorly because it’s spending a lot of time waiting for storage I/O, you may want to provision more IOPS to handle this.</p>&#13;
&#13;
<p>Your cloud or Kubernetes provider console can usually show you how many IOPS are actually being used on your nodes, and you can use these figures to help you decide where to cut costs.</p>&#13;
&#13;
<p>Ideally, you would be able to set resource requests for containers that need high bandwidth or large amounts of storage. However, Kubernetes does not currently support this, though support for IOPS requests may be added in the future.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Don’t use instance types with more storage than you need. Provision the smallest, lowest-IOPS disk volumes you can, based on the throughput and space that you actually use.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cleaning Up Unused Resources" data-type="sect2"><div class="sect2" id="idm45979387581216">&#13;
<h2>Cleaning Up Unused Resources</h2>&#13;
&#13;
<p>As your Kubernetes clusters grow, you will find many unused, or <em>lost</em> resources hanging around in dark corners. Over time, if these lost resources are not cleaned up, they will start to represent a significant fraction of your overall costs.</p>&#13;
&#13;
<p>At the highest level, you may find cloud instances that are not part of any cluster; it’s easy to forget to terminate a machine when it’s no longer in use.</p>&#13;
&#13;
<p>Other types of cloud resources, such as load balancers, public IPs, and disk volumes, also cost you money even though they’re not in use. You should regularly review your usage of each type of resource to find and remove unused instances.</p>&#13;
&#13;
<p>Similarly, there may be Deployments and Pods in your Kubernetes cluster that are not actually referenced by any Service, and so cannot receive traffic.</p>&#13;
&#13;
<p><a data-primary="garbage collection" data-type="indexterm" id="idm45979387577200"/>Even container images that are not running take up disk space on your nodes. Fortunately, Kubernetes will automatically clean up unused images when the node starts running short of disk space.<sup><a data-type="noteref" href="ch05.html#idm45979387576368" id="idm45979387576368-marker">1</a></sup></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using owner metadata" data-type="sect3"><div class="sect3" id="ownermetadata">&#13;
<h3>Using owner metadata</h3>&#13;
&#13;
<p>One helpful way to minimize unused resources is to have an organization-wide policy that each resource must be tagged with information about its owner. You can use Kubernetes annotations to do this (see <a data-type="xref" href="ch09.html#labelsandannotations">“Labels and Annotations”</a>).</p>&#13;
&#13;
<p>For example, you could annotate each Deployment like this:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">my-brilliant-app</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">annotations</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">example.com/owner</code><code class="p">:</code><code class="w"> </code><code class="s">"Customer</code><code class="nv"> </code><code class="s">Apps</code><code class="nv"> </code><code class="s">Team"</code><code class="w"/>&#13;
<code class="nn">...</code><code class="w"/></pre>&#13;
&#13;
<p>The owner metadata should specify the person or team to be contacted about this resource. This is useful anyway, but it’s especially handy for identifying abandoned or unused resources. (Note that it’s a good idea to prefix custom annotations with the domain name of your company, such as <code>example.com</code>, to prevent collisions with other annotations that might have the same name.)</p>&#13;
&#13;
<p><a data-primary="annotations" data-secondary="owner" data-type="indexterm" id="idm45979387565344"/>You can regularly query the cluster for all resources that do not have an owner annotation and make a list of them for potential termination. An especially strict policy might terminate all unowned resources immediately. Don’t be too strict, though, especially at first: developer goodwill is as important a resource as cluster capacity, if not more so.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Set owner annotations on all your resources, giving information about who to contact if there’s a problem with this resource, or if it seems abandoned and liable for termination.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Finding underutilized resources" data-type="sect3"><div class="sect3" id="idm45979387562816">&#13;
<h3>Finding underutilized resources</h3>&#13;
&#13;
<p>Some resources may be receiving very low levels of traffic, or none at all. Perhaps they became disconnected from a Service frontend due to a change in labels, or maybe they were temporary or experimental.</p>&#13;
&#13;
<p>Every Pod should expose the number of requests it receives as a metric (see <a data-type="xref" href="ch16.html#metrics">Chapter 16</a> for more about this). Use these metrics to find Pods that are getting low or zero traffic, and make a list of resources that can potentially be terminated.</p>&#13;
&#13;
<p>You can also check the CPU and memory utilization figures for each Pod in your web console and find the least-utilized Pods in your cluster. Pods that don’t do anything probably aren’t a good use of resources.</p>&#13;
&#13;
<p>If the Pods have owner metadata, contact their owners to find out whether these Pods are actually needed (for example, they might be for an application still in development).</p>&#13;
&#13;
<p>You could use another custom Kubernetes annotation (<code>example.com/lowtraffic</code> perhaps) to identify Pods that receive no requests but are still needed for one reason or another.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Regularly review your cluster to find underutilized or abandoned resources and eliminate them. Owner annotations can help.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cleaning up completed Jobs" data-type="sect3"><div class="sect3" id="job-ttl">&#13;
<h3>Cleaning up completed Jobs</h3>&#13;
&#13;
<p><a data-primary="Jobs" data-secondary="cleaning up" data-type="indexterm" id="idm45979387541392"/>Kubernetes Jobs (see <a data-type="xref" href="ch09.html#jobs">“Jobs”</a>) are Pods that run once to completion and are not restarted. However, the Job objects still exist in the Kubernetes database, and once there are a significant number of completed Jobs, this can affect API performance. You can tell Kubernetes to automatically remove Jobs after they have finished with the <code>ttlSecondsAfterFinished</code> setting:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">batch/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Job</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-job</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">ttlSecondsAfterFinished</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">60</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="nn">...</code><code class="w"/></pre>&#13;
&#13;
<p>In this example, when your Job finishes, it will automatically be deleted after 60 seconds.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Checking Spare Capacity" data-type="sect2"><div class="sect2" id="idm45979387580880">&#13;
<h2>Checking Spare Capacity</h2>&#13;
&#13;
<p>There should always be enough spare capacity in the cluster to handle the failure of a single worker node. <a data-primary="draining nodes" data-type="indexterm" id="idm45979387484384"/>To check this, try draining your biggest node (see <a data-type="xref" href="ch06.html#drain">“Scaling down”</a>). Once all Pods have been evicted from the node, check that all your applications are still in a working state with the configured number of replicas. If this is not the case, you need to add more capacity to the cluster.</p>&#13;
&#13;
<p>If there isn’t room to reschedule its workloads when a node fails, your services could be degraded at best, and unavailable at worst.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Reserved Instances" data-type="sect2"><div class="sect2" id="idm45979387472096">&#13;
<h2>Using Reserved Instances</h2>&#13;
&#13;
<p><a data-primary="instances" data-secondary="reserved" data-type="indexterm" id="idm45979387470688"/>Some cloud providers offer different instance classes depending on the machine’s life cycle. <em>Reserved</em> instances offer a trade-off between price and flexibility.</p>&#13;
&#13;
<p>For example, AWS reserved instances are about half the price of <em>on-demand</em> instances (the default type). You can reserve instances for various periods: a year, three years, and so on. AWS reserved instances have a fixed size, so if it turns out in three months’ time that you need a larger instance, your reservation will be mostly wasted.</p>&#13;
&#13;
<p>The Google Cloud equivalent of reserved instances is <em>Committed Use Discounts</em>, which allow you to prepay for a certain number of vCPUs and an amount of memory. This is more flexible than AWS reservations, as you can use more resources than you have reserved; you just pay the normal on-demand price for anything not covered by your reservation.</p>&#13;
&#13;
<p>Reserved instances and Committed Use Discounts can be a good choice when you know your requirements for the foreseeable future. However, there’s no refund for reservations that you don’t end up using, and you have to pay up front for the whole reservation period. So you should only choose to reserve instances for a period over which your requirements aren’t likely to change significantly.</p>&#13;
&#13;
<p>If you can plan a year or two ahead, however, using reserved instances could deliver a considerable saving.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use reserved instances when your needs aren’t likely to change for a year or two—but choose your reservations wisely, because they can’t be altered or refunded once they’re made.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Preemptible (Spot) Instances" data-type="sect2"><div class="sect2" id="idm45979387464336">&#13;
<h2>Using Preemptible (Spot) Instances</h2>&#13;
&#13;
<p><a data-primary="instances" data-secondary="preemptible" data-type="indexterm" id="idm45979387463088"/><a data-primary="instances" data-secondary="spot" data-type="indexterm" id="idm45979387410432"/><em>Spot</em> instances, as AWS calls them, or <em>preemptible VMs</em> in Google’s terminology, provide no availability guarantees, and are often limited in life span. Thus, they represent a trade-off between price and availability.</p>&#13;
&#13;
<p>A spot instance is cheap, but may be paused or resumed at any time, and may be terminated altogether. Fortunately, Kubernetes is designed to provide high-availability services despite the loss of individual cluster nodes.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Variable price or variable preemption" data-type="sect3"><div class="sect3" id="idm45979387408064">&#13;
<h3>Variable price or variable preemption</h3>&#13;
&#13;
<p>Spot instances can therefore be a cost-effective choice for your cluster. With AWS spot instances, the per-hour pricing varies according to demand. <a data-primary="nodes" data-secondary="instance types" data-type="indexterm" id="idm45979387406864"/>When demand is high for a given instance type in a particular region and availability zone, the price will rise.</p>&#13;
&#13;
<p>Google Cloud’s preemptible VMs, on the other hand, are billed at a fixed rate, but the rate of preemption varies. Google says that on average, <a href="https://oreil.ly/egdN8">about 5–15% of your nodes will be preempted in a given week</a>. However, preemptible VMs can be up to 80% cheaper than on-demand, depending on the instance type.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Preemptible nodes can halve your costs" data-type="sect3"><div class="sect3" id="idm45979387404192">&#13;
<h3>Preemptible nodes can halve your costs</h3>&#13;
&#13;
<p>Using preemptible nodes for your Kubernetes cluster, then, can be a very effective way to reduce costs. While you may need to run a few more nodes to make sure that your workloads can survive preemption, anecdotal evidence suggests that an overall 50% reduction in the cost per node is achievable.</p>&#13;
&#13;
<p>You may also find that using preemptible nodes is a good way to build a little chaos engineering into your cluster (see <a data-type="xref" href="ch06.html#chaos">“Chaos Testing”</a>)—provided that your application is ready for chaos testing in the first place.</p>&#13;
&#13;
<p>Bear in mind, though, that you should always have enough nonpreemptible nodes to handle your cluster’s minimum workload. Never bet more than you can afford to lose. If you have a lot of preemptible nodes, it might be a good idea to use cluster autoscaling to make sure any preempted nodes are replaced as soon as possible (see <a data-type="xref" href="ch06.html#autoscaling">“Autoscaling”</a>).</p>&#13;
&#13;
<p>In theory, <em>all</em> your preemptible nodes could disappear at the same time. Despite the cost savings, therefore, it’s a good idea to limit your preemptible nodes to no more than, say, two-thirds of your cluster.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Keep costs down by using preemptible or spot instances for some of your nodes, but no more than you can afford to lose. Always keep some nonpreemptible nodes in the mix, too.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using node affinities to control scheduling" data-type="sect3"><div class="sect3" id="nodeaffinities-intro">&#13;
<h3>Using node affinities to control scheduling</h3>&#13;
&#13;
<p><a data-primary="node affinities" data-type="indexterm" id="idm45979387394912"/>You can use Kubernetes <em>node affinities</em> to make sure Pods that can’t tolerate failure are <a href="https://oreil.ly/i52Wu">not scheduled on preemptible nodes</a> (see <a data-type="xref" href="ch09.html#nodeaffinities">“Node Affinities”</a>).</p>&#13;
&#13;
<p>For example, Google Kubernetes Engine (GKE) preemptible nodes carry the label <span class="keep-together"><code>cloud.google.com/gke-preemptible</code></span>. To tell Kubernetes to never schedule a Pod on one of these nodes, add the following to the Pod or Deployment spec:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">affinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">nodeAffinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">requiredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">nodeSelectorTerms</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cloud.google.com/gke-preemptible</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">DoesNotExist</code><code class="w"/></pre>&#13;
&#13;
<p>The <code>requiredDuringScheduling...</code> affinity is mandatory: a Pod with this affinity will <em>never</em> be scheduled on a node that does not match the selector expression (known as a <em>hard affinity</em>).</p>&#13;
&#13;
<p>Alternatively, you might want to tell Kubernetes that some of your less critical Pods, which can tolerate occasional failures, should preferentially be scheduled on preemptible nodes. In this case, you can use a <em>soft affinity</em> with the opposite sense:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">affinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">nodeAffinity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">preferredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">preference</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">matchExpressions</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cloud.google.com/gke-preemptible</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">operator</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Exists</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">weight</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">100</code><code class="w"/></pre>&#13;
&#13;
<p>This effectively means “Please schedule this Pod on a preemptible node if you can; if not, it doesn’t matter.”</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>If you’re running preemptible nodes, use Kubernetes node affinities to make sure critical workloads are not preempted.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Keeping Your Workloads Balanced" data-type="sect2"><div class="sect2" id="balanced">&#13;
<h2>Keeping Your Workloads Balanced</h2>&#13;
&#13;
<p><a data-primary="cluster balancing" data-type="indexterm" id="idm45979387299648"/>We’ve talked about the work that the Kubernetes scheduler does ensuring that workloads are distributed fairly across as many nodes as possible, and trying to place replica Pods on different nodes for high availability.</p>&#13;
&#13;
<p>In general, the scheduler does a great job, but there are some edge cases you need to watch out for.</p>&#13;
&#13;
<p>For example, suppose you have two nodes, and two services, A and B, each with two replicas. In a balanced cluster, there will be one replica of service A on each node, and one of service B on each node (<a data-type="xref" href="#img-unbalance1">Figure 5-1</a>). If one node should fail, both A and B will still be available.</p>&#13;
&#13;
<figure><div class="figure" id="img-unbalance1">&#13;
<img alt="Diagram showing two nodes, each with replicas of Service A and B" src="assets/cndk_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>Services A and B are balanced across the available nodes</h6>&#13;
</div></figure>&#13;
&#13;
<p>So far, so good. But suppose Node 2 does fail. The scheduler will notice that both A and B need an extra replica, and there’s only one node for it to create them on, so it does. Now Node 1 is running two replicas of service A, and two of service B.</p>&#13;
&#13;
<p>Now suppose we spin up a new node to replace the failed Node 2. Even once it’s available, there will be no Pods on it. The scheduler never moves running Pods from one node to another.</p>&#13;
&#13;
<p>We now have an <a href="https://oreil.ly/JWbpO">unbalanced cluster</a>, where all the Pods are on Node 1, and none are on Node 2 (<a data-type="xref" href="#img-unbalance2">Figure 5-2</a>).</p>&#13;
&#13;
<figure><div class="figure" id="img-unbalance2">&#13;
<img alt="Diagram showing two nodes, one with replicas of Service A and B, and the other with no services" src="assets/cndk_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>After the failure of Node 2, all replicas have moved to Node 1</h6>&#13;
</div></figure>&#13;
&#13;
<p>But it gets worse. Suppose you deploy a rolling update to service A (let’s call the new version service A*). The scheduler needs to start two new replicas for service A*, wait for them to come up, and then terminate the old ones.&#13;
Where will it start the new replicas? On the new Node 2, because it’s idle, while Node 1 is already running four Pods. So two new service A* replicas are started on Node 2, and the old ones removed from Node 1 (<a data-type="xref" href="#img-unbalance3">Figure 5-3</a>).</p>&#13;
&#13;
<figure><div class="figure" id="img-unbalance3">&#13;
<img alt="Diagram showing two nodes, one with two replicas of service B, and the other with two replicas of service A*" src="assets/cndk_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>After the rollout of service A*, the cluster is still unbalanced</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now you’re in a bad situation, because both replicas of service B are on the same node (Node 1), while both replicas of service A* are also on the same node (Node 2). Although you have two nodes, you have no high availability. The failure of either Node 1 or Node 2 will result in a service outage.</p>&#13;
&#13;
<p>The key to this problem is that the scheduler never moves Pods from one node to another unless they are restarted for some reason. Also, the scheduler’s goal of placing workloads evenly across nodes is sometimes in conflict with maintaining high availability for individual services.</p>&#13;
&#13;
<p><a data-primary="Descheduler" data-type="indexterm" id="idm45979387238432"/>One way around this is to use a tool called <a href="https://oreil.ly/hCSK9">Descheduler</a>. You can run this tool every so often, as a Kubernetes Job, and it will do its best to rebalance the cluster by finding Pods that need to be moved, and killing them.</p>&#13;
&#13;
<p>Descheduler has various strategies and policies that you can configure. For example, one policy looks for underutilized nodes, and kills Pods on other nodes to force them to be rescheduled on the idle nodes.</p>&#13;
&#13;
<p>Another policy looks for duplicate Pods, where two or more replicas of the same Pod are running on the same node, and evicts them. This fixes the problem that arose in our example, where workloads were nominally balanced, but in fact neither service was highly available.<a data-startref="ix_05-resources-adoc3" data-type="indexterm" id="idm45979387235600"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45979387660368">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Kubernetes is pretty good at running workloads for you in a reliable, efficient way with no real need for manual intervention. Providing you give the scheduler accurate estimates of your containers’ resource needs, you can largely leave Kubernetes to get on with it.</p>&#13;
&#13;
<p>The time you would have spent fixing operations issues can thus be put to better use, like developing applications. Thanks, Kubernetes!</p>&#13;
&#13;
<p>Understanding how Kubernetes manages resources is key to building and running your cluster correctly. The most important points to take away:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Kubernetes allocates CPU and memory resources to containers on the basis of <em>requests</em> and <em>limits</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>A container’s requests are the minimum amounts of resources it needs to run. Its limits specify the maximum amount it’s allowed to use.</p>&#13;
</li>&#13;
<li>&#13;
<p>Minimal container images are faster to build, push, deploy, and start. The smaller the container, the fewer the potential security vulnerabilities.</p>&#13;
</li>&#13;
<li>&#13;
<p>Liveness probes tell Kubernetes whether the container is working properly. If a container’s liveness probe fails, it will be killed and restarted.</p>&#13;
</li>&#13;
<li>&#13;
<p>Readiness probes tell Kubernetes that the container is ready and able to serve requests. If the readiness probe fails, the container will be removed from any Services that reference it, disconnecting it from user traffic.</p>&#13;
</li>&#13;
<li>&#13;
<p>Startup probes are like liveness probes, but are only used for determining if an application has finished starting and is ready for the liveness probe to take over for checking the status.</p>&#13;
</li>&#13;
<li>&#13;
<p>PodDisruptionBudgets let you limit the number of Pods that can be stopped at once during <em>evictions</em>, preserving high availability for your application.</p>&#13;
</li>&#13;
<li>&#13;
<p>Namespaces are a way of logically partitioning your cluster. You might create a namespace for each application, or group of related applications.</p>&#13;
</li>&#13;
<li>&#13;
<p>To refer to a Service in another namespace, you can use a DNS address like this: <em><code>SERVICE.NAMESPACE</code></em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>ResourceQuotas let you set overall resource limits for a given namespace.</p>&#13;
</li>&#13;
<li>&#13;
<p>LimitRanges specify default resource requests and limits for containers in a namespace.</p>&#13;
</li>&#13;
<li>&#13;
<p>Set resource limits so that your applications almost, but don’t quite, exceed them in normal usage.</p>&#13;
</li>&#13;
<li>&#13;
<p>Don’t allocate more cloud storage than you need, and don’t provision high-bandwidth storage unless it’s critical for your application’s performance.</p>&#13;
</li>&#13;
<li>&#13;
<p>Set owner annotations on all your resources, and scan the cluster regularly for unowned resources.</p>&#13;
</li>&#13;
<li>&#13;
<p>Find and clean up resources that aren’t being used (but check with their owners).</p>&#13;
</li>&#13;
<li>&#13;
<p>Reserved instances can save you money if you can plan your usage long term.</p>&#13;
</li>&#13;
<li>&#13;
<p>Preemptible instances can save you money right now, but be ready for them to vanish on short notice. Use node affinities to keep failure-sensitive Pods away from preemptible nodes.<a data-startref="ix_05-resources-adoc0" data-type="indexterm" id="idm45979387214064"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45979387576368"><sup><a href="ch05.html#idm45979387576368-marker">1</a></sup> You can customize this behavior by adjusting the <a href="https://oreil.ly/Pjfrj"><code>kubelet</code> garbage collection</a> settings.</p></div></div></section></body></html>