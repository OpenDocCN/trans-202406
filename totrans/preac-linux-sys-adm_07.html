<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 7. Managing Storage" data-type="chapter" epub:type="chapter"><div class="chapter" id="managing_storage">
<h1><span class="label">Chapter 7. </span>Managing Storage</h1>
<p>In this chapter, you learn how to add storage to your systems. <a contenteditable="false" data-primary="storage" data-type="indexterm" id="ix_stor"/>You’ll see how to add a new disk drive to the system and make it available. You will also explore the logical volume manager and how to manipulate logical volumes, as well as learn about disk formatting, partitioning, and mounting.</p>
<p>In the first section, I cover some general concepts related to disks, filesystems, volumes, partitions, directories, and filesystem mounting.</p>
<section data-pdf-bookmark="Administering Linux Storage" data-type="sect1"><div class="sect1" id="administering_linux_storage">
<h1>Administering Linux Storage</h1>
<p>The price of disk space has decreased so much in recent years that space is no longer a high-value commodity. <a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-type="indexterm" id="ix_storadm"/>You can buy multiterabyte disks for a few dollars per gigabyte. So system administrators rarely have to threaten to implement quotas or other arbitrary limits on disk space. Workstations and laptops often have as much space as servers, so space is no longer at a premium, and managing it is far less of a problem than it was just a few years ago. For example, many sysadmins now bypass old backup methods such as tape for faster and cheaper disk-to-disk backups.</p>
<p>But even though disk space is cheap and available, sysadmins still need to monitor users’ disk usage. You don’t want individuals filling up shared disk spaces or home directories with their music, videos, or other large files, because they waste corporate-owned space and prolong backup times. This section discusses disk-related terminology and how Linux system administrators interpret those terms. The specifics of how to work with each of these items are covered later in the chapter.<a contenteditable="false" data-primary="solid state drives" data-see="SSDs" data-type="indexterm" id="idm45657874051680"/></p>
<section class="pagebreak-before" data-pdf-bookmark="Disks" data-type="sect2"><div class="sect2" id="disks">
<h2 class="less_space">Disks</h2>
<p>Disks (disk drives) are the<a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="disks" data-type="indexterm" id="idm45657874043776"/> devices that <a contenteditable="false" data-primary="disks" data-type="indexterm" id="idm45657874036336"/>we refer to as hard drives or hard disk drives (HDDs), but <em>disks</em> can also refer to solid-state drives (SSDs) and USB thumb drives. <a contenteditable="false" data-primary="SSDs (solid-state drives)" data-type="indexterm" id="idm45657874033696"/><a contenteditable="false" data-primary="USB thumb drives" data-type="indexterm" id="idm45657874032496"/>System administrators make entire disks available to Linux systems using internal connections, USB connectivity, or an over-the-network technology such as Ethernet or fiber optic cabling.<a contenteditable="false" data-primary="mounting and mount points" data-secondary="disks" data-type="indexterm" id="idm45657874035360"/> Before accessing a disk on Linux systems, a system administrator must mount the disk on a directory. <a contenteditable="false" data-primary="hard drives or hard disk drives" data-see="disks" data-type="indexterm" id="idm45657874028368"/><a contenteditable="false" data-primary=" /dev/sdd" data-primary-sortas="dev" data-type="indexterm" id="idm45657874027600"/>For example, to mount a new disk identified by the system as <em>/dev/sdd</em>, the sysadmin creates a new directory, such as <em>/software</em>, and mounts the entire disk on that directory or <em>mount point</em>:</p>
<pre data-type="programlisting">
$ sudo mount /dev/sdd1 /software</pre>
<p>The disk device <em>/dev/sdd1</em> is now mounted on the directory <em>/software</em> and is <span class="keep-together">accessible.</span></p>
<div data-type="tip"><h6>Tip</h6>
<p>The entire disk device is <em>/dev/sdd</em> but initializing the disk <a contenteditable="false" data-primary="partitions" data-secondary="partitioning a disk" data-type="indexterm" id="idm45657874021472"/>requires at least one partition, so if disk <em>/dev/sdd</em> has only a single partition, its name will be <em>/dev/sdd1</em>. Details of this process appear later in this chapter.</p>
</div>
<p>Once the administrator prepares the disk by partitioning and establishing a filesystem on it, users may access space made available to them.</p>
</div></section>
<section data-pdf-bookmark="Filesystems" data-type="sect2"><div class="sect2" id="filesystems">
<h2>Filesystems</h2>
<p>A <em>filesystem</em> is an organizational construct that allows for file storage and retrieval for an operating system. <a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="filesystems" data-type="indexterm" id="idm45657874014736"/><a contenteditable="false" data-primary="filesystems" data-type="indexterm" id="idm45657874013680"/>A filesystem is a data structure the operating system uses to keep track of files on a disk or partition. It is how the files are organized on the disk. A filesystem is a partition or entire disk that stores files.</p>
<p>Contemporary Linux systems give administrators a broad choice of filesystems, although many sysadmins stick with ZFS, XFS, or ext4 when creating new partitions. <a contenteditable="false" data-primary="ext4" data-type="indexterm" id="idm45657875764368"/><a contenteditable="false" data-primary="XFS" data-type="indexterm" id="idm45657874077584"/><a contenteditable="false" data-primary="ZFS" data-type="indexterm" id="idm45657874076816"/>There are many other filesystems available for specific needs and applications.</p>
</div></section>
<section data-pdf-bookmark="Mounting and Mount Points" data-type="sect2"><div class="sect2" id="mounting_and_mount_points">
<h2>Mounting and Mount Points</h2>
<p>Only the root user, or a user with <code>sudo</code> privileges, may mount a filesystem. Mounting a filesystem on a directory is roughly analogous to assigning a drive letter to a disk in Windows.<a contenteditable="false" data-primary="filesystems" data-secondary="mounting and mount points" data-type="indexterm" id="idm45657874006400"/><a contenteditable="false" data-primary="mounting and mount points" data-secondary="filesystems" data-type="indexterm" id="idm45657874004672"/> Linux uses directories rather than drive letters, and those directories are referred to as mount points. In the example in <a data-type="xref" data-xrefstyle="select:nopage" href="#disks">“Disks”</a>, a new disk, <em>/dev/sdd,</em> was mounted on the directory <em>/software</em>. <em>/software</em> is the mount point.</p>
<p class="pagebreak-before">Linux systems provide a generic mount point, <em>/mnt</em>, onto which you may temporarily mount disks. You shouldn’t use the <em>/mnt</em> directory for a permanent mount point because another system administrator might mount another filesystem over it, hiding the original contents.</p>
<p>Mount points need not be directories off of the root directory. They can be subdirectories. For example, you could mount the <em>/dev/sdd</em> disk onto <em>/opt/software</em>:</p>
<pre data-type="programlisting">
$ sudo mount /dev/sdd1 /opt/software</pre>
<div data-type="tip"><h6>Tip</h6>
<p>A mount point (directory) must exist before mounting a disk or filesystem onto it.</p>
</div>
<p>There’s nothing special about a mount point directory. It’s the same as any other directory on the filesystem. Create a new directory, set its permissions, and mount the filesystem or disk onto it.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Don’t mount filesystems or disks onto existing system directories such as <em>/tmp</em>, <em>/var</em>, <em>/usr</em>, <em>/etc</em>, etc. Doing so will cause erratic system behavior and possibly catastrophic failure.<a contenteditable="false" data-primary="mounting and mount points" data-secondary="avoiding use of existing system directories" data-type="indexterm" id="idm45657874011376"/></p>
</div>
<p>To cause disks to <a contenteditable="false" data-primary="disks" data-secondary="causing to mount automatically at boot time" data-type="indexterm" id="idm45657873995184"/>mount automatically at boot time, you must create an entry in the <em>/etc/fstab</em> file that includes the new disk or filesystem and the mount point.<a contenteditable="false" data-primary="/etc/fstab file" data-primary-sortas="etc" data-type="indexterm" id="idm45657873991248"/> For example, to automatically mount the <em>/dev/sdd1</em> partition onto the <em>/opt/software</em> directory, the <em>/etc/fstab</em> entry looks like the following:</p>
<pre data-type="programlisting">
UUID=324ddbc2-353b-4221-a80e-49ec356678dc /opt/software    xfs    defaults    0 0</pre>
<p>If you don’t create this entry, the <em>/dev/sdd1</em> won’t mount automatically upon reboot. This isn’t a problem if that’s your intent or if you only have a few systems, but if you manage dozens or hundreds of systems, you need to set up <em>/etc/fstab</em> for each filesystem or disk you wish to mount automatically.</p>
</div></section>
<section data-pdf-bookmark="Physical and Logical Volumes" data-type="sect2"><div class="sect2" id="physical_and_logical_volumes">
<h2>Physical and Logical Volumes</h2>
<p>When sysadmins speak of physical and logical volumes, they refer to logical volume management (LVM). <a data-type="xref" href="#the_logical_volume_manager">Figure 7-1</a> shows a visual reference of the Logical Volume Manager. <a contenteditable="false" data-primary="LVM (logical volume management)" data-type="indexterm" id="idm45657873982048"/><a contenteditable="false" data-primary="logical volume management (LVM)" data-type="indexterm" id="idm45657873981408"/><a contenteditable="false" data-primary="logical volumes" data-type="indexterm" id="idm45657873980592"/><a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="physical and logical volumes" data-type="indexterm" id="idm45657873979488"/>A physical volume is a partition or disk managed by a logical volume. <a contenteditable="false" data-primary="partitions" data-secondary="physical volumes" data-type="indexterm" id="idm45657873977696"/><a contenteditable="false" data-primary="physical volumes" data-type="indexterm" id="idm45657873976320"/>The physical volume looks exactly like a disk partition. For example, the partition <em>/dev/sdd1</em> is also the physical volume <em>/dev/sdd1</em>.</p>
<figure><div class="figure" id="the_logical_volume_manager"><img alt="" height="385" src="assets/plsa_0701.png" width="600"/>
<h6><span class="label">Figure 7-1. </span>The Logical Volume Manager</h6>
</div></figure>
<p>A volume group contains physical volumes.<a contenteditable="false" data-primary="volume group (VG)" data-type="indexterm" id="idm45657873972304"/><a contenteditable="false" data-primary="VG" data-see="volume group" data-type="indexterm" id="idm45657873971136"/> A logical volume is equivalent to a partition on a disk, but you create logical volume partitions from volume groups. Logical volumes contain filesystems that are named, and those names can be descriptive.</p>
<p>Another way of thinking about logical volumes is that volume groups are analogous to disks, and logical volumes are analogous to disk partitions.</p>
<p>The advantages of abstracting physical volumes into logical ones are that you have the flexibility of spanning multiple disks to create large volume groups and can dynamically resize (shrink and grow) logical volumes without taking the system offline to do so.</p>
</div></section>
<section data-pdf-bookmark="Checking Space" data-type="sect2"><div class="sect2" id="checking_space">
<h2>Checking Space</h2>
<p>Sysadmins should keep a close eye on disk space usage.<a contenteditable="false" data-primary="disks" data-secondary="checking space on" data-type="indexterm" id="idm45657874292224"/><a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="checking disk space usage" data-type="indexterm" id="idm45657873964736"/> Log files can grow to fill filesystems if something goes wrong on a system, such as a buffer overflow. Users often fill filesystems and shared spaces with nonwork files. Developers also often download gigabytes of code and other files without discussing their needs with the sysadmins or anyone else.<a contenteditable="false" data-primary="df (diskfree) command" data-type="indexterm" id="idm45657873963152"/> You can quickly check disk space using the <em>disk free</em> (<code>df</code>) command:</p>
<pre data-type="programlisting">
$ df -h
Filesystem           Size  Used Avail Use% Mounted on
/dev/mapper/cl-root  6.2G  3.3G  3.0G  53% /
/dev/sdb1            1.5G   43M  1.4G   3% /opt/software
/dev/sda1            976M  250M  660M  28% /boot</pre>
<p>The <code>df</code> command lets you know at a glance how much available space there is on your mounted filesystems.<a contenteditable="false" data-primary="df (diskfree) command" data-secondary="-h option" data-type="indexterm" id="idm45657873957184"/> The <code>-h</code> switch means human-readable, which means <code>M</code> for megabytes, <code>G</code> for gigabytes, etc. Monitoring systems or your own scripts can alert you to filesystems that fill beyond a given threshold. For example, setting a 90% threshold would trigger an alert that a particular filesystem is at or over 90% capacity.</p>
<p>The disk usage (<code>du</code>) command is<a contenteditable="false" data-primary="du (disk usage) command" data-type="indexterm" id="idm45657873955648"/> very handy for checking individual directories and provides a breakdown of what’s consuming space. <a contenteditable="false" data-primary="directories" data-secondary="checking disk usage by" data-type="indexterm" id="idm45657873952096"/>For example, the following command checks the space consumed by the <em>/var/log</em> directory:</p>
<pre data-type="programlisting">
$ sudo du -h /var/log
0    /var/log/private
0    /var/log/samba/old
0    /var/log/samba
36K    /var/log/sssd
28K    /var/log/tuned
2.3M    /var/log/audit
0    /var/log/chrony
3.3M    /var/log/anaconda
10M    /var/log</pre>
<p>You can see at a glance which directories consume the most disk space. This is important if you’re auditing your system’s disk space before removing unneeded files.</p>
</div></section>
<section data-pdf-bookmark="Swap Space" data-type="sect2"><div class="sect2" id="swap_space">
<h2>Swap Space</h2>
<p>Swap space is a special type of Linux disk partition that extends a system’s memory beyond the limits of physical random access memory (RAM). <a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="swap space" data-type="indexterm" id="idm45657873948624"/><a contenteditable="false" data-primary="swap space" data-type="indexterm" id="idm45657873946688"/><a contenteditable="false" data-primary="memory" data-secondary="swap space and RAM" data-type="indexterm" id="idm45657873945616"/>Your system’s kernel uses swap space to write inactive programs from memory to disk, freeing up memory for active programs. <a contenteditable="false" data-primary="random access memory (RAM)" data-secondary="extending system memory beyond using swap space" data-type="indexterm" id="idm45657873944240"/><a contenteditable="false" data-primary="RAM" data-see="random access memory" data-type="indexterm" id="idm45657873942544"/>If a user or process activates those swapped programs, the system writes them from disk back into memory.</p>
<p>Expanding swap space is not a remedy for solving memory problems. If your system has memory constraints, one accepted solution is to add more physical RAM rather than increasing swap space or adding another swap partition. A system’s overuse of swap space creates a condition known as <em>thrashing</em>. Thrashing occurs when too many programs are running, the swap partition is too small, or the system has insufficient physical RAM to support its processes.</p>
<p>I discuss how to create and manage swap space later in this chapter.</p>
</div></section>
<section data-pdf-bookmark="RAM-Based Temporary Space (ramfs and tmpfs)" data-type="sect2"><div class="sect2" id="ram_based_temporary_space_left_parenthe">
<h2>RAM-Based Temporary Space (ramfs and tmpfs)</h2>
<p><em>Ramfs</em> and <em>tmpfs</em> are filesystems whose <a contenteditable="false" data-primary="random access memory (RAM)" data-secondary="RAM-based temporary filesystems" data-type="indexterm" id="idm45657873935888"/>files exist in memory <a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="RAM-based temporary space" data-type="indexterm" id="idm45657873934448"/>and are not written to disk. <a contenteditable="false" data-primary="filesystems" data-secondary="RAM-based temporary filesystems" data-type="indexterm" id="idm45657873932464"/><a contenteditable="false" data-primary="tmpfs" data-type="indexterm" id="idm45657873931056"/><a contenteditable="false" data-primary="ramfs" data-type="indexterm" id="idm45657873929952"/>The <em>tmpfs</em> system is the newer and preferred RAM-based temporary filesystem, and <em>tmpfs</em> is the default for all contemporary Linux distributions. One reason for the transition away from <em>ramfs</em> to <em>tmpfs</em> is that <em>ramfs</em> allowed itself to fill to capacity. <em>Tmpfs</em> has limit checking to prevent reaching its maximum capacity. <em>Tmpfs</em> adds the feature of writing files to available swap space to save resources.</p>
<p>The following <a contenteditable="false" data-primary="Ubuntu" data-secondary="tmpfs mount information" data-type="indexterm" id="idm45657873925248"/>listings <a contenteditable="false" data-primary="CentOS" data-secondary="tmpfs mount information" data-type="indexterm" id="idm45657873922960"/>show the <em>tmpfs</em> mount information for both CentOS and Ubuntu systems, respectively:</p>
<pre data-type="programlisting">
$ mount | grep tmpfs
devtmpfs on /dev type devtmpfs (rw,nosuid,seclabel,size=397220k,nr_inodes=9930...
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev,seclabel)
tmpfs on /run type tmpfs (rw,nosuid,nodev,seclabel,mode=755)
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755)
tmpfs on /run/user/1000 type tmpfs (rw,nosuid,nodev,relatime,seclabel,size=828...

$ mount | grep tmpfs
udev on /dev type devtmpfs (rw,nosuid,noexec,relatime,size=456144k,nr_inodes=1...
tmpfs on /run type tmpfs (rw,nosuid,nodev,noexec,relatime,size=100480k,mode=755)
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)
tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
tmpfs on /run/snapd/ns type tmpfs (rw,nosuid,nodev,noexec,relatime,size=100480...
tmpfs on /run/user/1000 type tmpfs (rw,nosuid,nodev,relatime,size=100480k,mode...</pre>
<p>The purpose of <em>tmpfs</em> is to write temporary files and caches to memory rather than to disk because of the speed differences between RAM and disk. <a contenteditable="false" data-primary="disks" data-secondary="speed of RAM versus SSDs" data-type="indexterm" id="idm45657873919840"/>RAM is many times faster than the fastest SSD. <a contenteditable="false" data-primary="SSDs (solid-state drives)" data-secondary="speed of RAM versus SSDs" data-type="indexterm" id="idm45657873917696"/>The downside of <em>tmpfs</em> is that if you reboot or unmount <em>tmpfs</em>, the data you have stored there is lost. Programs, processes, and users may all write to this temporary space. Similar to disk-based filesystems, a warning of “no space left on device” occurs when files, from whatever sources, have filled the allotted space.<a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-startref="ix_storadm" data-type="indexterm" id="idm45657875860944"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Adding a New Disk to a System" data-type="sect1"><div class="sect1" id="adding_a_new_disk_to_a_system">
<h1>Adding a New Disk to a System</h1>
<p>In this section, I show<a contenteditable="false" data-primary="disks" data-secondary="adding new disk to a system" data-type="indexterm" id="ix_diskadd"/> you the steps required to add a new disk to a system and prepare the disk for use. <a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="adding new disk to a system" data-type="indexterm" id="ix_storadmadddsk"/>First, I describe how to add a disk to a physical system. Then I demonstrate the same procedure but using a virtual disk. You’ll also learn how to create a filesystem on the disk and mount it as a single usable directory. I also demonstrate how to set up a logical volume on a disk.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You must have root privileges to perform these system-level <span class="keep-together">functions.</span></p>
</div>
<section class="pagebreak-before" data-pdf-bookmark="Installing the Disk" data-type="sect2"><div class="sect2" id="installing_the_disk">
<h2 class="less_space">Installing the Disk</h2>
<p>You can simply attach a hard drive without powering it down for physical systems with hot-swappable disk interfaces.<a contenteditable="false" data-primary="disks" data-secondary="adding new disk to a system" data-tertiary="installing the disk" data-type="indexterm" id="idm45657873906512"/><a contenteditable="false" data-primary="installation" data-secondary="installing new disk on a system" data-type="indexterm" id="idm45657873907664"/> If your system doesn’t have hot-swappable interfaces, shut it down before adding a new disk. Once you’ve physically added the disk, power on the system (if required) and log in to set it up.<a contenteditable="false" data-primary="hot-swappable disk interfaces" data-type="indexterm" id="idm45657873907536"/></p>
<p>For those who use virtual machines (VMs), shut down your VM, add a new disk, and restart it. <a contenteditable="false" data-primary="virtual machines (VMs)" data-secondary="adding new disk" data-type="indexterm" id="idm45657873900544"/>From then on, the process is the same for physical and virtual systems.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>I can’t demonstrate adding a new virtual disk because there are too many virtual platforms available from which to choose. It generally involves selecting storage, attaching a virtual disk to a virtual controller, adjusting the size of the disk, and saving the configuration.</p>
</div>
</div></section>
<section data-pdf-bookmark="Prepping the Disk for Use" data-type="sect2"><div class="sect2" id="prepping_the_disk_for_use">
<h2>Prepping the Disk for Use</h2>
<p>You must first determine the new disk’s device name. The system assigns the device name automatically. <a contenteditable="false" data-primary="disks" data-secondary="adding new disk to a system" data-tertiary="prepping the disk for use" data-type="indexterm" id="ix_diskaddprep"/>Use the <code>fdisk</code> command to <a contenteditable="false" data-primary="partitions" data-secondary="displaying for all attached disks" data-type="indexterm" id="idm45657874748880"/>display all <a contenteditable="false" data-primary="fsdisk command" data-secondary="displaying all attached disks and partitions" data-type="indexterm" id="idm45657873897456"/>attached disks and <span class="keep-together">partitions:</span></p>
<pre data-type="programlisting">
$ sudo fdisk -l

Disk /dev/sda: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x38e117ab

Device     Boot   Start      End  Sectors Size Id Type
/dev/sda1  *       2048  2099199  2097152   1G 83 Linux
/dev/sda2       2099200 16777215 14678016   7G 8e Linux LVM


Disk /dev/sdb: 1.5 GiB, 1550843904 bytes, 3028992 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xd268486b</pre>
<p class="pagebreak-after">In the preceding listing, the system identified the new disk device as <em>/dev/sdb</em>. Now that you’ve identified the disk’s device name, you <a contenteditable="false" data-primary="fsdisk command" data-secondary="initializing new disk" data-type="indexterm" id="idm45657873891440"/>can begin initializing it using the <code>fdisk</code> command:</p>
<pre class="less_space pagebreak-after" data-type="programlisting">
$ sudo fdisk /dev/sdb

Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): n

Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): &lt;ENTER&gt;
Partition number (1-4, default 1): &lt;ENTER&gt;
First sector (2048-3028991, default 2048): &lt;ENTER&gt;
Last sector, +sectors or +size{K,M,G,T,P} (2048-3028991, default 3028991): &lt;EN...

Created a new partition 1 of type 'Linux' and of size 1.5 GiB.

Command (m for help): w

The partition table has been altered.
Failed to add partition 1 to system: Device or resource busy

The kernel still uses the old partitions. The new table will be used at the ne...
Syncing disks.

$ sudo fdisk -l
Disk /dev/sda: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x38e117ab

Device     Boot   Start      End  Sectors Size Id Type
/dev/sda1  *       2048  2099199  2097152   1G 83 Linux
/dev/sda2       2099200 16777215 14678016   7G 8e Linux LVM


Disk /dev/sdb: 1.5 GiB, 1550843904 bytes, 3028992 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xd268486b

Device     Boot Start     End Sectors  Size Id Type
/dev/sdb1        2048 3028991 3026944  1.5G 83 Linux</pre>
<p>You can ignore the message <code>Failed to add partition 1 to system: Device or resource busy</code>. You can see that the partition <em>/dev/sdb1</em> does exist on the system. You have initialized the disk and created partition <em>/dev/sdb1</em>. Now you must create the filesystem.<a contenteditable="false" data-primary="filesystems" data-secondary="creating XFS for new disk partition" data-type="indexterm" id="idm45657873886512"/><a contenteditable="false" data-primary="XFS" data-secondary="creating for new disk partition" data-type="indexterm" id="idm45657873884336"/> The following command formats the <em>/dev/sdb1</em> partition using XFS:</p>
<pre data-type="programlisting">
$ sudo mkfs.xfs /dev/sdb1
meta-data=/dev/sdb1              isize=512    agcount=4, agsize=94592 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=378368, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</pre>
<p>The following <a contenteditable="false" data-primary="block devices" data-secondary="listing all" data-type="indexterm" id="idm45657873880880"/>command lists all block devices <a contenteditable="false" data-primary="filesystems" data-secondary="listing all with lsblk -f" data-type="indexterm" id="idm45657873879056"/>and filesystems:</p>
<pre data-type="programlisting">
$ sudo lsblk -f
NAME        FSTYPE            LABEL UUID      MOUNTPOINT
loop0       squashfs                          /var/lib/snapd/snap/asciinema/16
loop1       squashfs                          /var/lib/snapd/snap/core/10577
loop2       squashfs                          /var/lib/snapd/snap/core/10583
sda                                                                  
├─sda1      ext4              1825fee2...     /boot
└─sda2      LVM2_member       PqHcgi-X...
  ├─cl-root xfs               41b93d34...     /
  └─cl-swap swap              6b31789a...     [SWAP]
sdb                                                                 
└─sdb1      xfs               ca2701e0... 
sr0</pre>
<p>The <code>lsblk</code> command also <a contenteditable="false" data-primary="lsblk command" data-type="indexterm" id="idm45657873876096"/>displays the devices’ universally unique identifiers (UUIDs), which you need for the last step of the disk preparation process: mounting the filesystem.<a contenteditable="false" data-primary="UUIDs (universally unique identifiers)" data-secondary="displaying for block devices using lsblk" data-type="indexterm" id="idm45657874136336"/><a contenteditable="false" data-primary="universally unique identifiers" data-see="UUIDs" data-type="indexterm" id="idm45657873873072"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>You can <a contenteditable="false" data-primary="blkid command" data-type="indexterm" id="idm45657873870944"/>also display the UUID by issuing the <code>blkid</code> command with the device name as the argument:</p>
<pre data-type="programlisting">$ sudo blkid /dev/sdb1</pre>
</div>
<p>Next, create a directory on which you want to mount<a contenteditable="false" data-primary="partitions" data-secondary="creating new directory to mount new disk partition" data-type="indexterm" id="idm45657873867072"/> the new partition.<a contenteditable="false" data-primary="directories" data-secondary="creating new directory for mounting new disk partition" data-type="indexterm" id="idm45657873869008"/> I use <em>/opt/software</em> as the mount point in this example:</p>
<pre data-type="programlisting">
$ sudo mkdir /opt/software</pre>
<p class="pagebreak-before">Mount the <em>/dev/sdb1</em> partition onto the <em>/opt/software</em> directory:</p>
<pre data-type="programlisting">
$ sudo mount /dev/sdb1 /opt/software</pre>
<p>Check to see if the preceding <code>mount</code> command<a contenteditable="false" data-primary="mount command" data-type="indexterm" id="idm45657873860240"/> worked correctly by using the <code>mount</code> command without any switches:</p>
<pre data-type="programlisting">
$ mount | grep sdb1
/dev/sdb1 on /opt/software type xfs (rw,relatime,seclabel,attr2,inode64,logbuf...</pre>
<p>The partition <em>/dev/sdb1</em> successfully mounted on <em>/opt/software</em>. Now you need to make this mount “permanent,” which means it will survive a system reboot.<a contenteditable="false" data-primary="partitions" data-secondary="making new disk partition mount permanent" data-type="indexterm" id="idm45657873854272"/><a contenteditable="false" data-primary="mounting and mount points" data-secondary="making new disk partition mount permanent" data-type="indexterm" id="idm45657873853376"/> Edit the <em>/etc/fstab</em> file and enter the following information:</p>
<pre data-type="programlisting">
UUID=ca2701e0-3e75-4930-b14e-d83e19a5cffb /opt/software    xfs    defaults    0 0</pre>
<p>Save the file. The <em>/dev/sdb1</em> partition will mount automatically once you reboot the system. <a contenteditable="false" data-primary="/etc/fstab file" data-primary-sortas="etc" data-type="indexterm" id="idm45657873849200"/>Debian-derived systems handle this <em>/etc/fstab</em> syntax a bit differently. On the Ubuntu system, this <em>/etc/fstab</em> entry looks like the following:</p>
<pre data-type="programlisting">
/dev/disk/by-uuid/ca2701e0-3e75-4930-b14e-d83e1... /opt/software xfs defaults 0 0</pre>
<p>To allow users to access the drive, alter the permissions of <em>/opt/software</em> or create subdirectories with appropriate permissions.<a contenteditable="false" data-primary="disks" data-secondary="adding new disk to a system" data-startref="ix_diskaddprep" data-tertiary="prepping the disk for use" data-type="indexterm" id="idm45657873845824"/></p>
</div></section>
<section data-pdf-bookmark="Implementing Logical Volumes" data-type="sect2"><div class="sect2" id="implementing_logical_volumes">
<h2>Implementing Logical Volumes</h2>
<p>Logical volumes give you much flexibility in allocating disk space. <a contenteditable="false" data-primary="disks" data-secondary="adding new disk to a system" data-tertiary="implementing logical volumes" data-type="indexterm" id="ix_diskaddlogvol"/><a contenteditable="false" data-primary="logical volumes" data-secondary="implementing" data-type="indexterm" id="ix_logicvimpl"/>The capability of resizing live partitions is a huge feature because adding or removing disk space from a partition doesn’t require any system downtime. You should still schedule a maintenance window when resizing a logical volume because there is still potential for something to go wrong during the process. If you make a mistake, you could lose data or possibly have to rebuild the entire logical volume. Disasters are somewhat unlikely, but they can happen.</p>
<p>Resizing isn’t the only notable feature of logical volumes. A logical volume can span disks, meaning you can create a very large logical volume from multiple disks. A few years ago, administrators were reluctant to create volumes that span multiple disks because spinning, mechanical hard drives are prone to failure. But using SSDs deprecates the “don’t span” rule. SSDs fail too, but their lifespans and reliability make it much more reasonable to span when necessary.</p>
<p>This section demonstrates how to set up a logical volume from a raw disk. (You can convert a currently used disk to a logical volume, but you will lose all the data in the process.)</p>
<section data-pdf-bookmark="Identifying available disks" data-type="sect3"><div class="sect3" id="identifying_available_disks">
<h3>Identifying available disks</h3>
<p>To check your system for available (unused) disks or disks <a contenteditable="false" data-primary="logical volumes" data-secondary="implementing" data-tertiary="identifying available disks" data-type="indexterm" id="idm45657874049424"/>that you want to convert to logical <a contenteditable="false" data-primary="lsblk command" data-type="indexterm" id="idm45657875862944"/>volumes, use the <code>lsblk</code> command:</p>
<pre data-type="programlisting">
$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0         7:0    0  8.5M  1 loop /var/lib/snapd/snap/asciinema/16
loop1         7:1    0 97.9M  1 loop /var/lib/snapd/snap/core/10577
loop2         7:2    0 97.9M  1 loop /var/lib/snapd/snap/core/10583
sda           8:0    0    8G  0 disk
├─sda1        8:1    0    1G  0 part /boot
└─sda2        8:2    0    7G  0 part
  ├─cl-root 253:0    0  6.2G  0 lvm  /
  └─cl-swap 253:1    0  820M  0 lvm  [SWAP]
sdb           8:16   0  1.5G  0 disk
sr0          11:0    1 1024M  0 rom</pre>
<p>From the listing, in the line containing the disk, <code>sdb</code> shows that it is online and available for use:</p>
<pre data-type="programlisting">
sdb           8:16   0  1.5G  0 disk</pre>
<p>First, you must create the physical volume (PV), which is the basic block device onto which you’ll build logical volumes.<a contenteditable="false" data-primary="physical volumes" data-secondary="creating to build logical volumes onto" data-type="indexterm" id="idm45657873826464"/> Use the <code>pvcreate</code> command and<a contenteditable="false" data-primary="pvcreate command" data-type="indexterm" id="idm45657873824160"/> the device name to initialize the disk as a physical volume:</p>
<pre data-type="programlisting">
$ sudo pvcreate /dev/sdb
WARNING: dos signature detected on /dev/sdb at offset 510. Wipe it? [y/n]: y
  Wiping dos signature on /dev/sdb.
  Physical volume "/dev/sdb" successfully created.</pre>
<p>List all physical volumes and confirm that <em>/dev/sdb</em> is among<a contenteditable="false" data-primary="pvs (physical volume show) command" data-type="indexterm" id="idm45657873820976"/> them using the <code>pvs</code> (PV Show) command:</p>
<pre data-type="programlisting">
$ sudo pvs
  PV         VG Fmt  Attr PSize  PFree
  /dev/sda2  cl lvm2 a--  &lt;7.00g    0
  /dev/sdb      lvm2 ---   1.44g 1.44g</pre>
<p>Use the <code>pvdisplay</code> command to see <a contenteditable="false" data-primary="pvdisplay command" data-type="indexterm" id="idm45657873817904"/>details about your physical volume:</p>
<pre data-type="programlisting">
$ sudo pvdisplay /dev/sdb
  "/dev/sdb" is a new physical volume of "1.44 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb
  VG Name              
  PV Size               1.44 GiB
  Allocatable           NO
  PE Size               0  
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               yl6k0u-cymt-p4jd-VXro-Lit1-EkLe-Jqjc94</pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If your disk or filesystem is already mounted when you attempt to use <code>pvcreate</code>, you’ll receive the <a contenteditable="false" data-primary="pvcreate command" data-secondary="error when disk or filesystem already mounted" data-type="indexterm" id="idm45657873815104"/>following error:</p>
<pre data-type="programlisting">
$ sudo pvcreate /dev/sdb
  Can't open /dev/sdb exclusively.  Mounted filesystem?</pre>
<p>You’ll need to unmount <em>/dev/sdb</em> (<code>umount /dev/sdb</code>) before <span class="keep-together">proceeding.</span></p>
</div>
<p>The second step is to create a volume group (VG) from the physical volume <em>/dev/sdb</em> using the <code>vgcreate</code> command. <a contenteditable="false" data-primary="vgcreate command" data-type="indexterm" id="idm45657873808160"/><a contenteditable="false" data-primary="volume group (VG)" data-secondary="creating" data-type="indexterm" id="idm45657873807184"/>Give the VG a name, as I’ve done here with <em>vgsw</em>:</p>
<pre data-type="programlisting">
$ sudo vgcreate vgsw /dev/sdb
  Volume group "vgsw" successfully created

$ sudo vgs
  VG   #PV #LV #SN Attr   VSize  VFree
  cl     1   2   0 wz--n- &lt;7.00g    0
  vgsw   1   0   0 wz--n-  1.44g 1.44g

$ sudo vgdisplay vgsw
  --- Volume group ---
  VG Name               vgsw
  System ID            
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               1.44 GiB
  PE Size               4.00 MiB
  Total PE              369
  Alloc PE / Size       0 / 0  
  Free  PE / Size       369 / 1.44 GiB
  VG UUID               AsQkWT-UpSu-3dYk-5EWT-DbQR-SovE-2oAmlR</pre>
<p>As depicted in <a data-type="xref" href="#the_logical_volume_manager">Figure 7-1</a>, you create volume groups from physical volumes. In this demonstration, I only used one physical volume. Now that you have a volume group, you must create logical volumes. Creating the logical volume is the third step in this process.</p>
<p>For this demonstration, I begin by deciding how much space to allocate to the logical volume. I decide to use 1 GB of the 1.5 GB disk. It’s now time to create the logical volume using the <code>lvcreate</code> command.<a contenteditable="false" data-primary="lvcreate command" data-type="indexterm" id="idm45657873915344"/></p>
<p class="pagebreak-before">The general syntax of the <code>lvcreate</code> command is as follows:</p>
<pre data-type="programlisting">
$ sudo lvcreate -L <em>size</em> -n <em>lvname</em> vg</pre>
<p>You must provide the size parameter in gigabytes or megabytes, by adding a trailing <code>G</code> or <code>M</code>, respectively. The <code>lvname</code> is the name you want to use for this logical volume (<code>software-lv</code>) and <a contenteditable="false" data-primary="vgsw (volume group name)" data-type="indexterm" id="idm45657873805712"/>you must supply the volume group name (<code>vgsw</code>) from which you want to create the logical volume (<code>software-lv</code>):</p>
<pre data-type="programlisting">
$ sudo lvcreate -L 1G -n software-lv vgsw
WARNING: xfs signature detected on /dev/vgsw/software-lv at offset 0. 
Wipe it? [y/n]: y
  Wiping xfs signature on /dev/vgsw/software-lv.
  Logical volume "software-lv" created.</pre>
<p>From the preceding message, you see that the disk or partition had previously held an <code>xfs signature</code>, which means it’s not a new disk<a contenteditable="false" data-primary="recycled disks" data-type="indexterm" id="idm45657873792400"/> or partition but a recycled one.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There’s no problem with using a previously used disk but realize that all the information on it will be overwritten and unrecoverable in this process.</p>
</div>
<p>Use the <code>lvs</code> and <code>lvdisplay</code> commands <a contenteditable="false" data-primary="lvdisplay command" data-type="indexterm" id="idm45657873787888"/><a contenteditable="false" data-primary="lvs commnd" data-type="indexterm" id="idm45657873786784"/>to list details about your logical volumes:</p>
<pre data-type="programlisting">
$ sudo lvs
  LV          VG   Attr         LSize   Pool Origin Data%  Meta%  Move Log ...
  root        cl   -wi-ao----  &lt;6.20g
  swap        cl   -wi-ao---- 820.00m
  software-lv vgsw -wi-a-----   1.00g

$ sudo lvdisplay /dev/vgsw/software-lv
  --- Logical volume ---
  LV Path                /dev/vgsw/software-lv
  LV Name                software-lv
  VG Name                vgsw
  LV UUID                ebB3ST-3E7k-BShG-8oPi-sj0c-yXXr-C7EgAw
  LV Write Access        read/write
  LV Creation host, time server1, 2021-12-10 07:34:56 -0600
  LV Status              available
  # open                 0
  LV Size                1.00 GiB
  Current LE             256
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:2</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You must use the full path to the <a contenteditable="false" data-primary="paths (file)" data-secondary="required full path to logical volume device" data-type="indexterm" id="idm45657873782864"/>logical volume device (<code>/dev/vgsw/software-lv</code>) when you filter by the logical volume name:</p>
<pre data-type="programlisting">
$ sudo lvs /dev/vgsw/software-lv
  LV     VG   Attr            LSize Pool Origin ...
  software-lv vgsw -wi-a----- 1.00g</pre>
</div>
<p>The <a contenteditable="false" data-primary="filesystems" data-secondary="creating on logical volume" data-type="indexterm" id="idm45657873779648"/>fourth step in this process is to create a filesystem on your logical volume. You perform this task by using the same command that you would use for any partition:</p>
<pre data-type="programlisting">
$ sudo mkfs.xfs /dev/vgsw/software-lv
meta-data=/dev/vgsw/software-lv  isize=512    agcount=4, agsize=65536 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=262144, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</pre>
<p>Create a mount <a contenteditable="false" data-primary="mounting and mount points" data-secondary="creating mount point for logical volume filesystem" data-type="indexterm" id="idm45657873788608"/>point for your filesystem:</p>
<pre data-type="programlisting">
$ sudo mkdir /sw</pre>
<p>Mount the filesystem onto the mount point:</p>
<pre data-type="programlisting">
$ sudo mount /dev/vgsw/software-lv /sw

$ mount | grep software
/dev/mapper/vgsw-software--lv on /sw type xfs  \
(rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota)</pre>
<p>Check available <a contenteditable="false" data-primary="df (diskfree) command" data-type="indexterm" id="idm45657873772736"/>space on the device:</p>
<pre data-type="programlisting">
$ df -h /sw
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/vgsw-software--lv 1014M   40M  975M   4% /sw</pre>
<p>The <a contenteditable="false" data-primary="filesystems" data-secondary="adding logical volume filesystem and its mount point to /etc/fstab" data-type="indexterm" id="idm45657873770496"/>final step is to add the filesystem and <a contenteditable="false" data-primary="/etc/fstab file" data-primary-sortas="etc" data-secondary="adding logical volume filesystem and its mount point to" data-type="indexterm" id="idm45657873768336"/>its mount point to <em>/etc/fstab</em> to mount the logical volume automatically at boot time. My entry looks like the following:</p>
<pre data-type="programlisting">
/dev/vgsw/software-lv /sw       xfs     defaults        0 0</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Don’t use<a contenteditable="false" data-primary="UUIDs (universally unique identifiers)" data-secondary="using device name instead of for defining logical volumes in /etc/fstab" data-type="indexterm" id="idm45657873764240"/> the UUID for defining logical volumes in <em>/etc/fstab</em>. Instead, use the device name (<code>/dev/vgsw/software-lv</code>).</p>
</div>
<p>The logical volume will mount automatically on reboot. The next section describes how to increase the size of, or extend, a logical volume.</p>
</div></section>
<section data-pdf-bookmark="Extending a logical volume" data-type="sect3"><div class="sect3" id="extending_a_logical_volume">
<h3>Extending a logical volume</h3>
<p>For the logical volume that I configured in this scenario, I used 1 GB of the 1.5 GB total disk size for <em>/dev/vgsw/software-lv</em>. <a contenteditable="false" data-primary="lvextend command" data-type="indexterm" id="idm45657873757840"/><a contenteditable="false" data-primary="logical volumes" data-secondary="implementing" data-tertiary="extending a logical volume" data-type="indexterm" id="idm45657873758720"/>To extend this volume, you can use one of the following general commands:</p>
<pre data-type="programlisting">
$ sudo lvextend -L +size(M or G) <em>lvname</em>
$ sudo lvextend -l +100%FREE <em>lvname</em></pre>
<p>In this example, I’ll use the <code>-l</code> (extents) option rather than a specific size to consume the rest of the free space on the device:</p>
<pre data-type="programlisting">
$ df -h /sw
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/vgsw-software--lv 1014M   40M  975M   4% /sw

$ sudo lvextend -l +100%FREE /dev/vgsw/software-lv

$ sudo lvextend -l +100%FREE /dev/vgsw/software-lv
Size of logical volume vgsw/software-lv changed from 1.00 GiB (256 extents) 
to 1.44 GiB (369 extents).
Logical volume vgsw/software-lv successfully resized.</pre>
<p>The <code>lvextend</code> command extends the logical volume to its maximum capacity but a <code>df</code> shows the same amount of available <a contenteditable="false" data-primary="df (diskfree) command" data-type="indexterm" id="idm45657873750976"/>space:</p>
<pre data-type="programlisting">
$ df -h /sw
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/vgsw-software--lv 1014M   40M  975M   4% /sw</pre>
<p>You have extended the logical volume but not the filesystem. <a contenteditable="false" data-primary="filesystems" data-secondary="extending for new logical volume" data-type="indexterm" id="idm45657873748560"/>Now resize the filesystem using the <code>xfs_growfs</code> command with no size parameter.<a contenteditable="false" data-primary="xfs_growfs command" data-type="indexterm" id="idm45657873747312"/> By not specifying a size parameter with <code>-D size</code>, <code>xfs_growfs</code> will extend the filesystem to its maximum value:</p>
<pre data-type="programlisting">
$ sudo xfs_growfs /dev/vgsw/software-lv
meta-data=/dev/mapper/vgsw-software--lv isize=512    agcount=4, agsize=65536 blks
         =                              sectsz=512   attr=2, projid32bit=1
         =                              crc=1        finobt=1, sparse=1, rmapbt=0
         =                              reflink=1
data     =                              bsize=4096   blocks=262144, imaxpct=25
         =                              sunit=0      swidth=0 blks
naming   =version 2                     bsize=4096   ascii-ci=0, ftype=1
log      =internal log                  bsize=4096   blocks=2560, version=2
         =                              sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                          extsz=4096   blocks=0, rtextents=0
data blocks changed from 262144 to 377856

$ df -h /sw
Filesystem                     Size  Used Avail Use% Mounted on
/dev/mapper/vgsw-software--lv  1.5G   43M  1.4G   3% /sw</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You cannot shrink an XFS volume.</p>
</div>
<p>You have successfully extended the filesystem, and your logical volume is ready to use. The actual available size is 1.44 GB (shown as <code>1.4G</code> in the preceding example) because of filesystem overhead.<a contenteditable="false" data-primary="disks" data-secondary="adding new disk to a system" data-startref="ix_diskaddlogvol" data-tertiary="implementing logical volumes" data-type="indexterm" id="idm45657873739472"/><a contenteditable="false" data-primary="logical volumes" data-secondary="implementing" data-startref="ix_logicvimpl" data-type="indexterm" id="idm45657873737744"/><a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-startref="ix_storadmadddsk" data-tertiary="adding new disk to a system" data-type="indexterm" id="idm45657873736096"/><a contenteditable="false" data-primary="disks" data-secondary="adding new disk to a system" data-startref="ix_diskadd" data-type="indexterm" id="idm45657873734240"/></p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Decommissioning and Disk Disposal" data-type="sect1"><div class="sect1" id="decommissioning_and_disk_disposal">
<h1>Decommissioning and Disk Disposal</h1>
<p>Decommissioning includes the wiping or destruction of disks before disposal. <a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-tertiary="disk decommissioning and disposal" data-type="indexterm" id="ix_storadmdskdecom"/><a contenteditable="false" data-primary="disks" data-secondary="decommissioning and disposal" data-type="indexterm" id="ix_diskdecom"/>The decommissioning process differs from company to company but generally follows these steps:</p>
<ul>
<li>
<p>Notification</p>
</li>
<li>
<p>“Scream” test</p>
</li>
<li>
<p>Power down</p>
</li>
<li>
<p>Disk wiping</p>
</li>
<li>
<p>Unracking</p>
</li>
<li>
<p>Palletizing</p>
</li>
<li>
<p>Disposal</p>
</li>
</ul>
<p>The timeline for each of these steps varies. The following sections provide details for each step.</p>
<section data-pdf-bookmark="Notification" data-type="sect2"><div class="sect2" id="notification">
<h2>Notification</h2>
<p>Stakeholders, system administrators, network administrators, storage administrators, and management all receive multiple decommissioning notifications for a list of systems. Large companies generally send the list out weekly for a period of three to four weeks (the actual time varies from company to company and is a matter of policy). These notifications give stakeholders and others a chance to take possession of a system or to prevent its decommissioning via email and then a discussion during a governance meeting. If no one speaks up on behalf of any listed system, the process proceeds to the “scream” test phase.</p>
</div></section>
<section class="pagebreak-before" data-pdf-bookmark="Scream Test" data-type="sect2"><div class="sect2" id="scream_test">
<h2 class="less_space">Scream Test</h2>
<p>The so-called “scream” test is a period of two or more weeks where a system administrator or data center staff member unplugs a given system from the network but doesn’t power down listed systems. The plan is that during this time if someone screams about a system being down, it would be plugged back into the network, and its operations would continue as before. The next governance meeting would remove the system from the decommissioning list.</p>
</div></section>
<section data-pdf-bookmark="Power Down" data-type="sect2"><div class="sect2" id="power_down">
<h2>Power Down</h2>
<p>The next milestone in the decommissioning process is the power-down phase, which lasts two or more weeks. System administrators power down all listed systems. This period is a second chance for interested parties to claim a system or to notify the governance committee that the system needs to remain in operation.</p>
</div></section>
<section data-pdf-bookmark="Disk Wiping" data-type="sect2"><div class="sect2" id="disk_wiping">
<h2>Disk Wiping</h2>
<p>After several weeks of notifications and waiting, the governance committee finalizes the list and submits it to system administrators for disk wiping. The system administrator powers on each system and uses a disk-wiping utility to overwrite every local disk. Leveraged or shared storage such as storage area network (SAN), network-attached storage (NAS), or nonlocal disk storage is not included in this process. This process ensures no data is left on a system’s local storage.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>I have relied on <a href="https://oreil.ly/PnMgA">Darik’s Boot and Nuke (DBAN) utility</a> for years to wipe disks for decommissioning and disposal. It is a free, open source utility for hard disk drives (HDDs). This product is <em>not</em> for solid-state drives (SSDs).</p>
</div>
</div></section>
<section class="pagebreak-after" data-pdf-bookmark="Unracking and Palletizing" data-type="sect2"><div class="sect2" id="unracking_and_palletizing">
<h2>Unracking and Palletizing</h2>
<p>Once a system’s disks have been wiped, the list goes to data center personnel for unracking. Technicians remove the systems from data center racks and place them onto shipping pallets. Once a pallet is full, the technician labels it, and it goes into a disposal queue.</p>
</div></section>
<section data-pdf-bookmark="Disposal" data-type="sect2"><div class="sect2" id="disposal">
<h2 class="less_space">Disposal</h2>
<p>The disposal process may consist of bidding, where companies bulk purchase palletized systems for sale or redeployment. Sometimes the systems go to a recycling facility where technicians remove disks, CPUs, memory, and other salvageable components for individual or bulk resale. System disposal can also mean that those whole systems are disposed of by crushing or shredding and sold as recyclable material.<a contenteditable="false" data-primary="storage" data-secondary="administering Linux storage" data-startref="ix_storadmdskdecom" data-tertiary="disk decommissioning and disposal" data-type="indexterm" id="idm45657873685216"/><a contenteditable="false" data-primary="disks" data-secondary="decommissioning and disposal" data-startref="ix_diskdecom" data-type="indexterm" id="idm45657875452736"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00007">
<h1>Summary</h1>
<p>This chapter covered multiple aspects of disk management, including general disk-related information, adding a new disk to a system, logical volume management, and decommissioning and disposal.<a contenteditable="false" data-primary="storage" data-startref="ix_stor" data-type="indexterm" id="idm45657873680592"/></p>
<p>In the next chapter, I cover two important tasks for system administrators: health and housekeeping.</p>
</div></section>
</div></section></div></body></html>