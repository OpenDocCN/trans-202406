<html><head></head><body><section data-pdf-bookmark="Chapter 2. Managing Data Storage on Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="managing_data_storage_on_kubernetes">&#13;
<h1><span class="label">Chapter 2. </span>Managing Data Storage on Kubernetes</h1>&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>There is no such thing as a stateless architecture. All applications store state somewhere.</p>&#13;
<p data-type="attribution">Alex Chircop, CEO, StorageOS</p>&#13;
</blockquote>&#13;
<p><a contenteditable="false" data-primary="data storage" data-secondary="managing on Kubernetes" data-type="indexterm" id="ds_ch"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="managing data storage on" data-type="indexterm" id="kub_ds"/><a contenteditable="false" data-primary="Chircop, Alex" data-type="indexterm" id="idm46183198843728"/><a contenteditable="false" data-primary="data storage" data-secondary="about" data-type="indexterm" id="idm46183198842752"/>In the previous chapter, we painted a picture of a possible near future with powerful, stateful, data-intensive applications running on Kubernetes. To get there, we’re going to need data infrastructure for persistence, streaming, and analytics. To build out this infrastructure, we’ll need to leverage the primitives that Kubernetes provides to help manage the three commodities of cloud computing: compute, network, and storage. In the next several chapters, we’ll begin to look at these primitives, starting with storage, in order to see how they can be combined to create the data infrastructure we need.</p>&#13;
<p>To echo the point raised by Alex Chircop, all applications must store their state somewhere, which is why we’ll focus in this chapter on the basic abstractions Kubernetes provides for interacting with storage. We’ll also look at the emerging innovations being offered by storage vendors and open source projects creating storage infrastructure for Kubernetes that itself embodies cloud native principles.</p>&#13;
<p>Let’s start our exploration with a look at managing persistence in containerized applications in general and use that as a jumping-off point for our investigation into data storage on Kubernetes.</p>&#13;
<section data-pdf-bookmark="Docker, Containers, and State" data-type="sect1"><div class="sect1" id="dockercomma_containerscomma_and_state">&#13;
<h1>Docker, Containers, and State</h1>&#13;
<p><a contenteditable="false" data-primary="data storage" data-secondary="containers" data-type="indexterm" id="ds_c"/><a contenteditable="false" data-primary="data storage" data-secondary="Docker" data-type="indexterm" id="ds_d"/><a contenteditable="false" data-primary="data storage" data-secondary="state" data-type="indexterm" id="ds_s"/>The problem of managing state in distributed, cloud native applications is not unique to Kubernetes. A quick search will show that stateful workloads have been an area of concern on other container orchestration platforms such as Mesos and Docker Swarm. Part of this has to do with the nature of container orchestration, and part is driven by the nature of containers themselves.</p>&#13;
<p><a contenteditable="false" data-primary="containers" data-secondary="about" data-type="indexterm" id="idm46183199319520"/>First, let’s consider containers. One of the key value propositions of containers is their ephemeral nature. Containers are designed to be disposable and replaceable, so they need to start quickly and use as few resources for overhead processing <span class="keep-together">as possible.</span> For this reason, most container images are built from base images containing streamlined, Linux-based, open source operating systems such as Ubuntu, that boot quickly and incorporate only essential libraries for the contained application or microservice. As the name implies, containers are designed to be self-contained, incorporating all their dependencies in immutable images, while their configuration and data are externalized. These properties make containers portable so that we <span class="keep-together">can run</span> them anywhere a compatible container runtime is available.</p>&#13;
<p><a contenteditable="false" data-primary="hypervisor layer" data-type="indexterm" id="idm46183199181824"/>As shown in <a data-type="xref" href="#comparing_containerization_to_virtualiz">Figure 2-1</a>, containers require less overhead than traditional VMs, which run a guest operating system per VM, with a <a href="https://oreil.ly/5gE1u">hypervisor layer</a> to implement system calls onto the underlying host operating system.</p>&#13;
<figure><div class="figure" id="comparing_containerization_to_virtualiz">&#13;
<img alt="Comparing containerization to virtualization" src="assets/mcdk_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>Comparing containerization to virtualization</h6>&#13;
</div></figure>&#13;
<p>Although containers have made applications more portable, it’s proven a bigger <span class="keep-together">challenge</span> to make their data portable. Since a container itself is ephemeral, any data that is to survive beyond the life of the container must by definition reside externally. The key feature for a container technology is to provide mechanisms to link to persistent storage, and the key feature for a container orchestration technology is the ability to schedule containers in such a way that they can access persistent storage efficiently.</p>&#13;
<section data-pdf-bookmark="Managing State in Docker" data-type="sect2"><div class="sect2" id="managing_state_in_docker">&#13;
<h2>Managing State in Docker</h2>&#13;
<p><a contenteditable="false" data-primary="Docker" data-secondary="managing state in" data-type="indexterm" id="idm46183198853232"/><a contenteditable="false" data-primary="state, managing in Docker" data-type="indexterm" id="idm46183198851792"/>Let’s take a look at the most popular container technology, Docker, to see how containers can store data. The key storage concept in Docker is the volume. From the perspective of a Docker container, a <em>volume</em> is a directory that can support read-only or read/write access. Docker supports the mounting of multiple data stores as volumes. We’ll introduce several options so we can later note their equivalents in Kubernetes.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Bind Mounts" data-type="sect2"><div class="sect2" id="bind_mounts">&#13;
<h2>Bind Mounts</h2>&#13;
<p><a contenteditable="false" data-primary="bind mounts" data-type="indexterm" id="idm46183198864160"/>The simplest approach for creating a volume is to bind a directory in the container to a directory on the host system. This is called a <em>bind mount</em>, as shown in <a data-type="xref" href="#using_docker_bind_mounts_to_access_the">Figure 2-2</a>.</p>&#13;
<figure><div class="figure" id="using_docker_bind_mounts_to_access_the">&#13;
<img alt="Using Docker bind mounts to access the host filesystem" src="assets/mcdk_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>Using Docker bind mounts to access the host filesystem</h6>&#13;
</div></figure>&#13;
<p>When starting a container within Docker, you specify a bind mount with the <span class="keep-together"><code>--volume</code></span> or <code>-v</code> option and the local filesystem path and container path to use. For example, you could start an instance of the Nginx web server and map a local project folder from your development machine into the container. This is a command you can test out in your own environment if you have Docker installed:</p>&#13;
<pre data-type="programlisting"><strong>docker run -it --rm -d --name web -p 8080:80 \&#13;
  -v ~/site-content:/usr/share/nginx/html nginx</strong>&#13;
</pre>&#13;
<p>This exposes the web server on port 8080 on your local host. If the local path directory does not already exist, the Docker runtime will create it. Docker allows you to create bind mounts with read-only or read/write permissions. Because the volume is represented as a directory, the application running in the container can put anything that can be represented as a file into the volume—even a database.</p>&#13;
<p class="pagebreak-before">Bind mounts are quite useful for development work. However, using bind mounts is not suitable for a production environment since this leads to a container being dependent on a file being present in a specific host. This might be fine for a single-machine deployment, but production deployments tend to be spread across multiple hosts. Another concern is the potential security hole that is presented by opening up access from the container to the host filesystem. For these reasons, we need another approach for production deployments.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Volumes" data-type="sect2"><div class="sect2" id="volumes">&#13;
<h2>Volumes</h2>&#13;
<p><a contenteditable="false" data-primary="Docker" data-secondary="volumes" data-type="indexterm" id="idm46183199022592"/><a contenteditable="false" data-primary="volumes" data-secondary="about" data-type="indexterm" id="idm46183199016160"/>The preferred option within Docker is to use volumes. Docker volumes are created and managed by Docker under a specific directory on the host filesystem. The Docker <code>volume create</code> command is used to create a volume. For example, you might create a volume called <code>site-content</code> to store files for a website:</p>&#13;
<pre data-type="programlisting"><strong>docker volume create site-content</strong></pre>&#13;
<p>If no name is specified, Docker assigns a random name. After creation, the resulting volume is available to mount in a container using the form <code>-v VOLUME-NAME:CONTAINER-PATH</code>. For example, you might use a volume like the one just created to allow an Nginx container to read the content, while allowing another container to edit the content, using the <code>ro</code> option:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>docker run -it --rm -d --name web \&#13;
  -v site-content:/usr/share/nginx/html:ro nginx</strong>&#13;
</pre>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Docker Volume Mount Syntax</h1>&#13;
<p>Docker also supports a <code>--mount</code> syntax that allows you to specify the source and target folders more explicitly. This notation is considered more modern, but it is also more verbose. The syntax shown in the preceding example is still valid and is the more commonly used syntax.</p>&#13;
</div>&#13;
<p>As we’ve implied, a Docker volume can be mounted in more than one container at once, as shown in <a data-type="xref" href="#creating_docker_volumes_to_share_data_b">Figure 2-3</a>.</p>&#13;
&#13;
<p>The advantage of using Docker volumes is that Docker manages the filesystem access for containers, which makes it much simpler to enforce capacity and security restrictions on containers.</p>&#13;
&#13;
<figure><div class="figure" id="creating_docker_volumes_to_share_data_b">&#13;
<img alt="Creating Docker volumes to share data between containers on the host" src="assets/mcdk_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>Creating Docker volumes to share data between containers on the host</h6>&#13;
</div></figure>&#13;
&#13;
</div></section>&#13;
<section data-pdf-bookmark="Tmpfs Mounts" data-type="sect2"><div class="sect2" id="tmpfs_mounts">&#13;
<h2>Tmpfs Mounts</h2>&#13;
<p><a contenteditable="false" data-primary="Docker" data-secondary="named pipes" data-type="indexterm" id="idm46183198797536"/><a contenteditable="false" data-primary="named pipes" data-type="indexterm" id="idm46183198797888"/><a contenteditable="false" data-primary="Docker" data-secondary="Tmpfs mounts" data-type="indexterm" id="idm46183198795472"/><a contenteditable="false" data-primary="Tmpfs mounts" data-type="indexterm" id="idm46183198794256"/>Docker supports two types of mounts that are specific to the operating system used by the host system: <em>tmpfs</em> (or <em>temporary filesystem</em>) and <em>named pipes</em>. Named pipes are available on Docker for Windows, but since they are typically not used in Kubernetes, we won’t give much consideration to them here.</p>&#13;
<p>Tmpfs mounts are available when running Docker on Linux. A tmpfs mount exists only in memory for the lifespan of the container, so the contents are never present on disk, as shown in <a data-type="xref" href="#creating_a_temporary_volume_using_docke">Figure 2-4</a>. Tmpfs mounts are useful for applications that are written to persist a relatively small amount of data, especially sensitive data that you don’t want written to the host filesystem. Because the data is stored in memory, faster access is a side benefit.</p>&#13;
<figure><div class="figure" id="creating_a_temporary_volume_using_docke">&#13;
<img alt="Creating a temporary volume using Docker tmpfs" src="assets/mcdk_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>Creating a temporary volume using Docker tmpfs</h6>&#13;
</div></figure>&#13;
<p class="pagebreak-before">To create a tmpfs mount, use the <code>docker run --tmpfs</code> option. For example, you could use a command like this to specify a tmpfs volume to store Nginx logs for a web server processing sensitive data:</p>&#13;
<pre data-type="programlisting"><strong>docker run -it --rm -d --name web —-tmpfs /var/log/nginx nginx</strong></pre>&#13;
<p>The <code>--mount</code> option may also be used for more control over configurable options.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Volume Drivers" data-type="sect2"><div class="sect2" id="volume_drivers">&#13;
<h2>Volume Drivers</h2>&#13;
<p><a contenteditable="false" data-primary="Docker Engine" data-type="indexterm" id="idm46183198787440"/><a contenteditable="false" data-primary="third-party storage plug-ins" data-type="indexterm" id="idm46183198786688"/><a contenteditable="false" data-primary="Docker" data-secondary="volume drivers" data-type="indexterm" id="idm46183198785712"/><a contenteditable="false" data-primary="volume drivers" data-type="indexterm" id="idm46183198784496"/>Docker Engine has an extensible architecture that allows you to add customized behavior via plug-ins for capabilities including networking, storage, and authorization. Third-party <a href="https://oreil.ly/b9P9X">storage plug-ins</a> are available for multiple open source and commercial providers, including the public clouds and various networked filesystems. Taking advantage of these involves installing the plug-in with Docker Engine and then specifying the associated volume driver when starting Docker containers using that storage, as shown in <a data-type="xref" href="#using_docker_volume_drivers_to_access_n">Figure 2-5</a>.</p>&#13;
<figure><div class="figure" id="using_docker_volume_drivers_to_access_n">&#13;
<img alt="Using Docker volume drivers to access networked storage" src="assets/mcdk_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>Using Docker volume drivers to access networked storage</h6>&#13;
</div></figure>&#13;
<p><a contenteditable="false" data-primary="docker run command" data-type="indexterm" id="idm46183198778656"/>For more information on working with the various types of volumes supported in Docker, see the <a href="https://oreil.ly/vVPb4">Docker storage documentation</a>, as well as the documentation for the <a href="https://oreil.ly/Tj3NT"><code>docker run</code> command</a>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="filecomma_blockcomma_and_object_storage">&#13;
<h5>File, Block, and Object Storage</h5>&#13;
<p><a contenteditable="false" data-primary="data storage" data-secondary="formats for" data-type="indexterm" id="idm46183198774112"/>In our modern era of cloud architectures, the three main formats in which storage is traditionally provided to applications are files, blocks, and objects. Each stores and provides access to data in different ways:</p>&#13;
<dl>&#13;
<dt>File storage</dt>&#13;
<dd><a contenteditable="false" data-primary="file storage" data-type="indexterm" id="idm46183206404080"/><a contenteditable="false" data-primary="Google Cloud Filestore" data-type="indexterm" id="idm46183198770256"/><a contenteditable="false" data-primary="Amazon Elastic Filestore" data-type="indexterm" id="idm46183198769280"/><a contenteditable="false" data-primary="Gluster" data-type="indexterm" id="idm46183198768304"/><a contenteditable="false" data-primary="NFS (Network File System)" data-type="indexterm" id="idm46183198767328"/>Represents data as a hierarchy of folders, each of which can contain files. The file is the basic unit of access for both storage and retrieval. The root directory that is to be accessed by a container is mounted into the container filesystem such that it looks like any other directory. Each of the public clouds provides its own file storage (for example, Google Cloud Filestore or Amazon Elastic Filestore).&#13;
<a href="https://www.gluster.org">Gluster</a> is an open source distributed filesystem. Many of these systems are compatible with the <a href="https://oreil.ly/yrSWs">Network File System</a> (NFS), a distributed filesystem protocol invented at Sun Microsystems in 1984 that is still in common use.</dd>&#13;
<dt>Block storage</dt>&#13;
<dd><a contenteditable="false" data-primary="block storage" data-type="indexterm" id="idm46183198761776"/>Organizes data in chunks and allocates those chunks across a set of managed volumes. When you provide data to a block storage system, it divides the data into chunks of varying sizes and distributes those chunks in order to use the underlying volumes most efficiently. When you query a block storage system, it retrieves the chunks from their various locations and provides the data back to you. This flexibility makes block storage a great solution when you have a heterogeneous set of storage devices available. Block storage doesn’t provide a lot of metadata handling, which can place more burden on the application.</dd>&#13;
<dt>Object storage</dt>&#13;
<dd><p><a contenteditable="false" data-primary="object storage" data-type="indexterm" id="idm46183198760320"/><a contenteditable="false" data-primary="S3 (Simple Storage Service)" data-type="indexterm" id="idm46183198759456"/><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="about" data-type="indexterm" id="idm46183198758384"/>Organizes data in units known as <em>objects</em>. Each object is referenced by a unique identifier, or <em>key</em>, and can support rich metadata tagging that enables searching. Objects are organized in buckets. This flat, nonhierarchical organization makes object storage easy to scale. S3 is the canonical example of object storage, and most object storage products will claim compatibility with the S3 API.</p>&#13;
<p>If you’re tasked with building or selecting data infrastructure, you need to understand the strengths and weaknesses of these patterns. Throughout the rest of the book, you’ll learn how each storage type is used by various data infrastructure projects. There are trade-offs to consider when choosing a storage format and whether to use a centralized or distributed storage architecture. For example, in <a data-type="xref" href="ch07.html#the_kubernetes_native_database">Chapter 7</a>, we’ll look at a refactored version of Cassandra that uses object storage for long-term persistence instead of file storage on local disks<a contenteditable="false" data-primary="" data-startref="ds_c" data-type="indexterm" id="idm46183198872800"/><a contenteditable="false" data-primary="" data-startref="ds_d" data-type="indexterm" id="idm46183198815072"/><a contenteditable="false" data-primary="" data-startref="ds_s" data-type="indexterm" id="idm46183198813808"/>.</p></dd>&#13;
</dl>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Kubernetes Resources for Data Storage" data-type="sect1"><div class="sect1" id="kubernetes_resources_for_data_storage">&#13;
<h1 class="less_space">Kubernetes Resources for Data Storage</h1>&#13;
<p><a contenteditable="false" data-primary="data storage" data-secondary="Kubernetes resources for" data-type="indexterm" id="ds_kr"/>Now that you understand basic concepts of container and cloud storage, let’s see what Kubernetes brings to the table. In this section, we’ll introduce some of the key Kubernetes concepts, or <em>resources</em> in the API for attaching storage to containerized applications. Even if you are already somewhat familiar with these resources, you’ll want to stay tuned, as we’ll focus particularly on how each one relates to stateful data.</p>&#13;
<section data-pdf-bookmark="Pods and Volumes" data-type="sect2"><div class="sect2" id="pods_and_volumes">&#13;
<h2>Pods and Volumes</h2>&#13;
<p><a contenteditable="false" data-primary="data storage" data-secondary="Pods" data-type="indexterm" id="ds_p"/><a contenteditable="false" data-primary="data storage" data-secondary="volumes" data-type="indexterm" id="ds_v"/><a contenteditable="false" data-primary="Pods" data-secondary="about" data-type="indexterm" id="po_ab"/><a contenteditable="false" data-primary="volumes" data-secondary="about" data-type="indexterm" id="vo_ab"/><a contenteditable="false" data-primary="control plane" data-secondary="about" data-type="indexterm" id="idm46183198733696"/><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183198732480"/><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183198731504"/>One of the first Kubernetes resources new users encounter is the <em>Pod</em>. This is the basic unit of deployment of a Kubernetes workload. A Pod provides an environment for running containers, and the Kubernetes control plane is responsible for deploying Pods to Kubernetes Worker Nodes.</p> &#13;
&#13;
<p>The <em>Kubelet</em> is a component of the <a href="https://oreil.ly/1ITFv">Kubernetes control plane</a> that runs on each Worker Node. It is responsible for running Pods on a node, as well as monitoring the health of these Pods and the containers inside them. These elements are summarized in <a data-type="xref" href="#using_volumes_in_kubernetes_pods">Figure 2-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="using_volumes_in_kubernetes_pods">&#13;
<img alt="Using volumes in Kubernetes Pods" src="assets/mcdk_0206.png"/>&#13;
<h6><span class="label">Figure 2-6. </span>Using volumes in Kubernetes Pods</h6>&#13;
</div></figure>&#13;
&#13;
<p><a contenteditable="false" data-primary="init containers" data-type="indexterm" id="idm46183198724176"/><a contenteditable="false" data-primary="containers" data-secondary="init" data-type="indexterm" id="idm46183198724528"/><a contenteditable="false" data-primary="sidecar containers" data-type="indexterm" id="idm46183198722048"/><a contenteditable="false" data-primary="containers" data-secondary="sidecar" data-type="indexterm" id="idm46183198720944"/>While a Pod can contain multiple containers, the best practice is for a Pod to contain a single application container, along with optional additional helper containers, as shown in <a data-type="xref" href="#using_volumes_in_kubernetes_pods">Figure 2-6</a>. These helper containers might include <em>init containers</em> that run prior to the main application container in order to perform configuration tasks, or <em>sidecar containers</em> that run alongside the main application container to provide helper services such as observability or management. In later chapters, you’ll see how data infrastructure deployments can take advantage of these architectural patterns.</p>&#13;
&#13;
<p>Now let’s see how persistence is supported within this Pod architecture. As with Docker, the “on disk” data in a container is lost when a container crashes. The Kubelet is responsible for restarting the container, but this new container is a <em>replacement</em> for the original container—it will have a distinct identity and start with a completely new state.</p>&#13;
<p>In Kubernetes, the term <em>volume</em> is used to represent access to storage within a Pod. By using a volume, the container has the ability to persist data that will outlive the container (and potentially the Pod as well, as we’ll see shortly). A volume may be accessed by multiple containers in a Pod. Each container has its own <em>volumeMount</em> within the Pod that specifies the directory to which it should be mounted, allowing the mount point to differ among containers.</p>&#13;
<p>In multiple cases, you might want to share data between containers in a Pod:</p>&#13;
<ul>&#13;
<li><p>An init container creates a custom configuration file for the particular environment that the application container mounts to obtain configuration values.</p></li>&#13;
<li><p>The application Pod writes logs, and a sidecar Pod reads those logs to identify alert conditions that are reported to an external monitoring tool.</p></li>&#13;
</ul>&#13;
<p>However, you’ll likely want to avoid situations in which multiple containers are writing to the same volume, because you’ll have to ensure that the multiple writers don’t conflict—Kubernetes does not do that for you.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Preparing to Run Sample Code</h1>&#13;
<p>The examples in this book assume you have access to a running Kubernetes cluster. For the examples in this chapter, a development cluster on your local machine such as kind, K3s, or Docker Desktop should be sufficient. The source code used in this section is located at <a href="https://oreil.ly/VjIq1">the book’s repository</a>.</p>&#13;
</div>&#13;
<p>Using a volume in a Pod requires two steps: defining the volume and mounting the volume in each container that needs access. Let’s look at a sample YAML configuration that defines a Pod with a single application container, the Nginx web server, and a single volume. The <a href="https://oreil.ly/nlBJA">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: my-pod&#13;
spec:&#13;
  containers:&#13;
  - name: my-app&#13;
    image: nginx&#13;
    volumeMounts:&#13;
    - name: web-data&#13;
      mountPath: /app/config&#13;
  volumes:&#13;
  - name: web-data</pre>&#13;
<p>Notice the two parts of the configuration: the volume is defined under <code>spec.volumes</code>, and the usage of the volumes is defined under <code>spec.containers.volumeMounts</code>. First, the <code>name</code> of the volume is referenced under <code>volumeMounts</code>, and the directory where it is to be mounted is specified by <code>mountPath</code>. When declaring a Pod specification, volumes and volume mounts go together. For your configuration to be valid, a volume must be declared before being referenced, and a volume must be used by at least one container in the Pod.</p>&#13;
<p><a contenteditable="false" data-primary="kubectl command" data-type="indexterm" id="idm46183198747760"/>You may have also noticed that the volume has only a <code>name</code>. You haven’t specified any additional information. What do you think this will do? You could try this out for yourself by using the example source code file <em>nginx-pod.yaml</em> or cutting and pasting the preceding configuration to a file with that name, and executing the <code>kubectl</code> command against a configured Kubernetes cluster:</p>&#13;
<pre data-type="programlisting"><strong>kubectl apply -f nginx-pod.yaml</strong></pre>&#13;
<p><a contenteditable="false" data-primary="kubectl get pod command" data-type="indexterm" id="idm46183198901184"/>You can get more information about the Pod that was created by using the <code>kubectl get pod</code> command, for example:</p>&#13;
<pre data-type="programlisting"><strong>kubectl get pod my-pod -o yaml | grep -A 5 "  volumes:"</strong></pre>&#13;
<p>And the results might look something like this:</p>&#13;
<pre data-type="programlisting">  volumes:&#13;
  - emptyDir: {}&#13;
    name: web-data&#13;
  - name: default-token-2fp89&#13;
    secret:&#13;
      defaultMode: 420</pre>&#13;
<p>As you can see, Kubernetes supplied additional information when creating the requested volume, defaulting it to a type of <code>emptyDir</code>. Other default attributes may differ depending on what Kubernetes engine you are using, but we won’t discuss them further here.</p>&#13;
<p>Several types of volumes can be mounted in a container; let’s have a look.</p>&#13;
<section data-pdf-bookmark="Ephemeral volumes" data-type="sect3"><div class="sect3" id="ephemeral_volumes">&#13;
<h3>Ephemeral volumes</h3>&#13;
<p><a contenteditable="false" data-primary="ephemeral volumes" data-type="indexterm" id="idm46183199069232"/><a contenteditable="false" data-primary="volumes" data-secondary="ephemeral" data-type="indexterm" id="idm46183198711648"/><a contenteditable="false" data-primary="PV (PersistentVolumes)" data-type="indexterm" id="idm46183198710432"/>You’ll remember tmpfs volumes from our previous discussion of Docker volumes, which provide temporary storage for the lifespan of a single container. Kubernetes provides the concept of an <a href="https://oreil.ly/zaiKG"><em>ephemeral volume</em></a>, which is similar, but at the scope of a Pod. The <code>emptyDir</code> introduced in the preceding example is a type of ephemeral volume.</p>&#13;
<p>Ephemeral volumes can be useful for data infrastructure or other applications that want to create a cache for fast access. Although they do not persist beyond the lifespan of a Pod, they can still exhibit some of the typical properties of other volumes for longer-term persistence, such as the ability to snapshot. Ephemeral volumes are slightly easier to set up than PersistentVolumes because they are declared entirely inline in the Pod definition without reference to other Kubernetes resources. As you will see next, creating and using PersistentVolumes is a bit more involved.</p>&#13;
<div data-type="tip">&#13;
<h1>Other Ephemeral Storage Providers</h1>&#13;
<p>Some of the in-tree and CSI storage drivers we’ll discuss next that provide PersistentVolumes also provide an ephemeral volume option. You’ll want to check the documentation of the specific provider to see what options are available.</p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Configuration volumes" data-type="sect3"><div class="sect3" id="configuration_volumes">&#13;
<h3>Configuration volumes</h3>&#13;
<p><a contenteditable="false" data-primary="configuration volumes" data-type="indexterm" id="idm46183206943056"/><a contenteditable="false" data-primary="volumes" data-secondary="configuration" data-type="indexterm" id="idm46183198894688"/>Kubernetes provides several constructs for injecting configuration data into a Pod as a volume. These volume types are also considered ephemeral in the sense that they do not provide a mechanism for allowing applications to persist their own data.</p>&#13;
<p>The following volume types are relevant to our exploration in this book since they provide a useful means of configuring applications and data infrastructure running on Kubernetes. We’ll describe each of them briefly:</p>&#13;
<dl>&#13;
<dt>ConfigMap volumes</dt>&#13;
<dd><a contenteditable="false" data-primary="ConfigMaps" data-type="indexterm" id="idm46183198748512"/><a contenteditable="false" data-primary="volumes" data-secondary="ConfigMap" data-type="indexterm" id="idm46183198701040"/>A ConfigMap is a Kubernetes resource that is used to store configuration values external to an application as a set of name-value pairs. For example, an application might require connection details for an underlying database such as an IP address and port number. Defining these in a ConfigMap is a good way to externalize this information from the application. The resulting configuration data can be mounted into the application as a volume, where it will appear as a directory. Each configuration value is represented as a file wherein the filename is the key and the contents of the file contain the value. See the Kubernetes documentation for more information on <a href="https://oreil.ly/zaiKG">mounting ConfigMaps as volumes</a>.</dd>&#13;
<dt>Secret volumes</dt>&#13;
<dd><a contenteditable="false" data-primary="Secret volumes" data-type="indexterm" id="idm46183198698176"/><a contenteditable="false" data-primary="volumes" data-secondary="Secret" data-type="indexterm" id="idm46183198697008"/>A Secret is similar to a ConfigMap, only it is intended for securing access to sensitive data that requires protection. For example, you might want to create a Secret containing database access credentials such as a username and password. Configuring and accessing Secrets is similar to using ConfigMap, with the additional benefit that Kubernetes helps decrypt the Secret upon access within the Pod. See the Kubernetes documentation for more information on <a href="https://oreil.ly/mPkMB">mounting Secrets as volumes</a>.</dd>&#13;
<dt>Downward API volumes</dt>&#13;
<dd><p><a contenteditable="false" data-primary="downward API volumes" data-type="indexterm" id="idm46183199441776"/><a contenteditable="false" data-primary="volumes" data-secondary="downward API" data-type="indexterm" id="idm46183199011264"/>The Kubernetes downward API exposes metadata about Pods and containers either as environment variables or as volumes. This is the same metadata that is used by <code>kubectl</code> and other clients.</p>&#13;
<p>The available Pod metadata includes the Pod’s name, ID, Namespace, labels, and annotations. The containerized application might aim to use the Pod information for logging and metrics reporting, or to determine database or table names.</p>&#13;
<p>The available container metadata includes the requested and maximum amounts of resources such as CPU, memory, and ephemeral storage. The containerized application might seek to use this information in order to throttle its own resource usage. See the Kubernetes documentation for an example of <a href="https://oreil.ly/LrOn2">injecting Pod information as a volume</a>.</p></dd>&#13;
</dl>&#13;
</div></section>&#13;
<section data-pdf-bookmark="hostPath volumes" data-type="sect3"><div class="sect3" id="hostpath_volumes">&#13;
<h3>hostPath volumes</h3>&#13;
<p><a contenteditable="false" data-primary="hostPath volumes" data-type="indexterm" id="idm46183198816288"/><a contenteditable="false" data-primary="volumes" data-secondary="hostPath" data-type="indexterm" id="idm46183198816640"/><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183198678336"/><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183198677360"/>A <a href="https://oreil.ly/kjr8P"><code>hostPath</code></a> volume mounts a file or directory into a Pod from the Kubernetes Worker Node where it is running. This is analogous to the bind mount concept in Docker, discussed in <a data-type="xref" href="#bind_mounts">“Bind Mounts”</a>. Using a <code>hostPath</code> volume has one advantage over an <code>emptyDir</code> volume: the data will survive the restart of a Pod.</p>&#13;
<p>However, using <code>hostPath</code> volumes has some disadvantages. First, in order for a replacement Pod to access the data of the original Pod, it will need to be restarted on the same Worker Node. While Kubernetes does give you the ability to control which node a Pod is placed on using affinity, this tends to constrain the Kubernetes scheduler from optimal placement of Pods, and if the node goes down for some reason, the data in the <code>hostPath</code> volume is lost. Second, as with Docker bind mounts, there is a security concern with <code>hostPath</code> volumes in terms of allowing access to the local filesystem. For these reasons, <code>hostPath</code> volumes are recommended only for development deployments.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Cloud volumes" data-type="sect3"><div class="sect3" id="cloud_volumes">&#13;
<h3>Cloud volumes</h3>&#13;
<p><a contenteditable="false" data-primary="cloud volumes" data-type="indexterm" id="idm46183198742208"/><a contenteditable="false" data-primary="volumes" data-secondary="cloud" data-type="indexterm" id="idm46183198666864"/>It is possible to create Kubernetes volumes that reference storage locations beyond just the Worker Node where a Pod is running, as shown in <a data-type="xref" href="#kubernetes_pods_directly_mounting_cloud">Figure 2-7</a>. These can be grouped into volume types that are provided by named cloud providers, and those that attempt to provide a more generic interface.</p>&#13;
&#13;
<p>These include the following:</p>&#13;
<ul>&#13;
<li><p><a contenteditable="false" data-primary="awsElasticBlockStore volume" data-type="indexterm" id="idm46183198664576"/><a contenteditable="false" data-primary="EBS (Elastic Block Storage)" data-type="indexterm" id="idm46183198662304"/><a contenteditable="false" data-primary="AWS (Amazon Web Services)" data-type="indexterm" id="idm46183198661232"/>The <a href="https://oreil.ly/CmTCt"><code>awsElasticBlockStore</code></a> volume type is used to mount volumes on Amazon Web Services (AWS) Elastic Block Store (EBS). Many databases use block storage as their underlying storage layer.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="PDs (persistent disks)" data-type="indexterm" id="idm46183198675152"/><a contenteditable="false" data-primary="gcePersistentDisk volume" data-type="indexterm" id="idm46183198657392"/><a contenteditable="false" data-primary="GCE (Google Compute Engine)" data-type="indexterm" id="idm46183198656416"/>The <a href="https://oreil.ly/01JEm"><code>gcePersistentDisk</code></a> volume type is used to mount Google Compute Engine (GCE) persistent disks (PD), another example of block storage.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="azureDisk" data-type="indexterm" id="idm46183198653120"/><a contenteditable="false" data-primary="azureFile" data-type="indexterm" id="idm46183198653760"/><a contenteditable="false" data-primary="Microsoft Azure" data-type="indexterm" id="idm46183198654400"/>Two types of volumes are supported for Microsoft Azure: <a href="https://oreil.ly/pIann"><code>azureDisk</code></a> for Azure Data Disk volumes, and <a href="https://oreil.ly/kInGC"><code>azureFile</code></a> for Azure File volumes.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="OpenStack" data-type="indexterm" id="idm46183198648336"/><a contenteditable="false" data-primary="cinder volume" data-type="indexterm" id="idm46183198647136"/>The <a href="https://oreil.ly/VVLrx"><code>cinder</code></a> volume type can be used to access OpenStack Cinder volumes for OpenStack deployments.</p></li>&#13;
</ul>&#13;
<figure><div class="figure" id="kubernetes_pods_directly_mounting_cloud">&#13;
<img alt="Kubernetes pods directly mounting cloud provider storage" src="assets/mcdk_0207.png"/>&#13;
<h6><span class="label">Figure 2-7. </span>Kubernetes Pods directly mounting cloud provider storage</h6>&#13;
</div></figure>&#13;
<p>Usage of these types typically requires configuration on the cloud provider, and access from Kubernetes clusters is typically confined to storage in the same cloud region and account. Check your cloud provider’s documentation for more details.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Additional volume providers" data-type="sect3"><div class="sect3" id="additional_volume_providers">&#13;
<h3>Additional volume providers</h3>&#13;
<p><a contenteditable="false" data-primary="fibreChannel volume" data-type="indexterm" id="idm46183198682080"/><a contenteditable="false" data-primary="volumes" data-secondary="additional providers" data-type="indexterm" id="idm46183198639168"/>Numerous additional volume providers vary in the types of storage provided. Here are a few examples:</p>&#13;
<ul>&#13;
<li><p>The <code>fibreChannel</code> volume type can be used for SAN solutions implementing the Fibre Channel protocol.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="gluster volume" data-type="indexterm" id="idm46183198633776"/>The <code>gluster</code> volume type is used to access file storage using the <a href="https://www.gluster.org">Gluster</a> distributed filesystem referenced previously.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="iscsi volume" data-type="indexterm" id="idm46183198631952"/>An <code>iscsi</code> volume mounts an existing Internet Small Computer Systems Interface (iSCSI) volume into your Pod.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="nfs volume" data-type="indexterm" id="idm46183198628720"/>An <code>nfs</code> volume allows an existing NFS share to be mounted into a Pod.</p></li>&#13;
</ul>&#13;
<p><a contenteditable="false" data-primary="Container Attached Storage pattern" data-type="indexterm" id="idm46183198626944"/>We’ll examine more volume providers that implement the Container Attached Storage pattern in <a data-type="xref" href="#container_attached_storage">“Container Attached Storage”</a>. <a data-type="xref" href="#comparing_docker_and_kubernetes_storage">Table 2-1</a> compares Docker and Kubernetes storage concepts we’ve covered so far.</p>&#13;
<table class="border" id="comparing_docker_and_kubernetes_storage">&#13;
<caption><span class="label">Table 2-1. </span>Comparing Docker and Kubernetes storage options</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Type of storage</th>&#13;
<th>Docker</th>&#13;
<th>Kubernetes</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>Access to persistent storage from various providers</td>&#13;
<td>Volume (accessed via volume drivers)</td>&#13;
<td>Volume (accessed via in-tree or CSI drivers)</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Access to host filesystem (not recommended for production)</td>&#13;
<td>Bind mount</td>&#13;
<td><code>hostPath</code> volume</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Temporary storage available while container (or Pod) <span class="keep-together">is running</span></td>&#13;
<td>Tmpfs</td>&#13;
<td><code>emptyDir</code> and other ephemeral volumes</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Configuration and environment data (read-only)</td>&#13;
<td>(No direct equivalent)</td>&#13;
<td>ConfigMap, Secret, downward API</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="how_do_you_choose_a_kubernetes_storage">&#13;
<h5>How Do You Choose a Kubernetes Storage Solution?</h5>&#13;
<p><a contenteditable="false" data-primary="data storage" data-secondary="choosing solutions for" data-type="indexterm" id="idm46183198610336"/>Given the number of storage options available, trying to determine the kind of storage you should use for your application can certainly be intimidating. Along with determining whether you need file, block, or object storage, you’ll want to consider your latency and throughput requirements, as well as your expected storage volume. For example, if your read latency requirements are aggressive, you’ll most likely need a storage solution that keeps data in the same datacenter where it is accessed.</p>&#13;
<p>Next, you’ll want to consider any existing commitments or resources you have. Perhaps your organization has a mandate or bias toward using services from a preferred cloud provider. The cloud providers will frequently provide cost incentives for using their services, but you’ll want to weigh this against the risk of lock-in to a specific service. Alternatively, you might have an investment in a storage solution in an on-premises datacenter that you need to leverage.</p>&#13;
<p>Overall, cost tends to be the overriding factor in choosing storage solutions, especially over the long term. Make sure your modeling includes not only the cost of the physical storage and any managed services, but also the operational cost involved in managing your chosen solution.</p>&#13;
</div></aside>&#13;
<p>In this section, we’ve discussed how to use volumes to provide storage that can be shared by multiple containers within the same Pod. While using volumes is sufficient for some use cases, it doesn’t address all needs. A volume doesn’t provide the ability to share storage resources among Pods. The definition of a particular storage location is tied to the definition of the Pod. Managing storage for individual Pods doesn’t scale well as the number of Pods deployed in your Kubernetes cluster <span class="keep-together">increases.</span></p>&#13;
<p>Thankfully, Kubernetes provides additional primitives that help simplify the process of provisioning and mounting storage volumes for both individual Pods and groups of related Pods. We’ll investigate these concepts in the next several sections<a contenteditable="false" data-primary="" data-startref="ds_p" data-type="indexterm" id="idm46183198601824"/><a contenteditable="false" data-primary="" data-startref="ds_v" data-type="indexterm" id="idm46183198829712"/><a contenteditable="false" data-primary="" data-startref="po_ab" data-type="indexterm" id="idm46183198641104"/><a contenteditable="false" data-primary="" data-startref="vo_ab" data-type="indexterm" id="idm46183198608832"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="PersistentVolumes" data-type="sect2"><div class="sect2" id="persistentvolumes">&#13;
<h2>PersistentVolumes</h2>&#13;
<p><a contenteditable="false" data-primary="PV (PersistentVolumes)" data-type="indexterm" id="idm46183198578160"/>The key innovation the Kubernetes developers introduced for managing storage is the <a href="https://oreil.ly/ec8BB">PersistentVolume subsystem</a>. This subsystem consists of three additional Kubernetes resources that work together: PersistentVolumes, PersistentVolumeClaims, and StorageClasses. These allow you to separate the definition and lifecycle of storage from the way it is used by Pods, as shown in <a data-type="xref" href="#persistentvolumescomma_persistentvolume">Figure 2-8</a>:</p>&#13;
<ul>&#13;
<li><p>Cluster administrators define PersistentVolumes, either explicitly or by creating a StorageClass that can dynamically provision new PersistentVolumes.</p></li>&#13;
<li><p>Application developers create PersistentVolumeClaims that describe the storage resource needs of their applications, and these PersistentVolumeClaims can be referenced as part of volume definitions in Pods.</p></li>&#13;
<li><p>The Kubernetes control plane manages the binding of PersistentVolumeClaims to PersistentVolumes.</p></li>&#13;
</ul>&#13;
<figure><div class="figure" id="persistentvolumescomma_persistentvolume">&#13;
<img alt="PersistentVolumes, PersistentVolumeClaims, and StorageClasses" src="assets/mcdk_0208.png"/>&#13;
<h6><span class="label">Figure 2-8. </span>PersistentVolumes, PersistentVolumeClaims, and StorageClasses</h6>&#13;
</div></figure>&#13;
<p>Let’s look first at the <em>PersistentVolume</em> resource (often abbreviated <em>PV</em>), which defines access to storage at a specific location. PersistentVolumes are typically defined by cluster administrators for use by application developers. Each PV can represent storage of the same types discussed in the previous section, such as storage offered by cloud providers, networked storage, or storage directly on the Worker Node, <span class="keep-together">as shown in</span> <a data-type="xref" href="#types_of_kubernetes_persistentvolumes">Figure 2-9</a>. Since they are tied to specific storage locations, <span class="keep-together">PersistentVolumes</span> are not portable between Kubernetes clusters.</p>&#13;
<figure><div class="figure" id="types_of_kubernetes_persistentvolumes">&#13;
<img alt="Types of Kubernetes PersistentVolumes" src="assets/mcdk_0209.png"/>&#13;
<h6><span class="label">Figure 2-9. </span>Types of Kubernetes PersistentVolumes</h6>&#13;
</div></figure>&#13;
&#13;
<section data-pdf-bookmark="Local PersistentVolumes" data-type="sect3"><div class="sect3" id="local_persistentvolumes">&#13;
<h3>Local PersistentVolumes</h3>&#13;
<p><a contenteditable="false" data-primary="local PersistentVolumes" data-type="indexterm" id="lpv_ab"/><a data-type="xref" href="#types_of_kubernetes_persistentvolumes">Figure 2-9</a> also introduces a PersistentVolume type called <code>local</code>, which represents storage mounted directly on a Kubernetes Worker Node such as a disk or partition. Like <code>hostPath</code> volumes, a <code>local</code> volume may also represent a directory. A key difference between <code>local</code> and <code>hostPath</code> volumes is that when a Pod using a <code>local</code> volume is restarted, the Kubernetes scheduler ensures that the Pod is rescheduled on the same node so it can be attached to the same persistent state. For this reason, <code>local</code> volumes are frequently used as the backing store for data infrastructure that manages its own replication, as we’ll see in <a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a>.</p>&#13;
<p>The syntax for defining a PersistentVolume will look familiar, as it is similar to defining a volume within a Pod. For example, here is a YAML configuration file that defines a local PersistentVolume. The <a href="https://oreil.ly/b1zHe">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: PersistentVolume&#13;
metadata:&#13;
  name: my-volume&#13;
spec:&#13;
  capacity:&#13;
    storage: 3Gi&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  local:&#13;
    path: /app/data&#13;
  nodeAffinity:&#13;
    required:&#13;
      nodeSelectorTerms:&#13;
      - matchExpressions:&#13;
        - key: kubernetes.io/hostname&#13;
          operator: In&#13;
          values:&#13;
          - node1</pre>&#13;
<p>As you can see, this code defines a <code>local</code> volume named <code>my-volume</code> on the Worker Node <code>node1</code>, 3 GB in size, with an access mode of <code>ReadWriteOnce</code>. The following <a href="https://oreil.ly/mm5HT">access modes</a> are supported for PersistentVolumes:</p>&#13;
<dl>&#13;
<dt><code>ReadWriteOnce</code></dt>&#13;
<dd><a contenteditable="false" data-primary="ReadWriteOnce access" data-type="indexterm" id="idm46183198548112"/>The volume can be mounted for both reading and writing by a single node at a time, although multiple Pods running on that node may access the volume.</dd>&#13;
<dt><code>ReadOnlyMany</code></dt>&#13;
<dd><a contenteditable="false" data-primary="ReadOnlyMany access" data-type="indexterm" id="idm46183198572880"/>The volume can be mounted by multiple nodes simultaneously, for reading only.</dd>&#13;
<dt><code>ReadWriteMany</code></dt>&#13;
<dd><a contenteditable="false" data-primary="ReadWriteMany access" data-type="indexterm" id="idm46183198542672"/>The volume can be mounted for both reading and writing by many nodes at the same time.</dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Choosing a Volume Access Mode</h1>&#13;
<p><a contenteditable="false" data-primary="access modes, choosing" data-type="indexterm" id="idm46183198540496"/><a contenteditable="false" data-primary="volumes" data-secondary="choosing access modes for" data-type="indexterm" id="idm46183198539424"/>The right access mode for a given volume will be driven by the type of workload. For example, many distributed databases will be configured with dedicated storage per Pod, making <code>ReadWriteOnce</code> a good choice.</p>&#13;
</div>&#13;
<p>Besides <a href="https://oreil.ly/TSKOD">capacity</a> and access mode, other attributes for PersistentVolumes include the following:</p>&#13;
<ul>&#13;
<li><p><a contenteditable="false" data-primary="volumeMode" data-type="indexterm" id="idm46183198534368"/>The <code>volumeMode</code>, which defaults to <code>Filesystem</code> but may be overridden to <code>Block</code>.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="reclaimPolicy" data-type="indexterm" id="idm46183198532192"/>The <code>reclaimPolicy</code> defines what happens when a Pod releases its claim on this PersistentVolume. The legal values are <code>Retain</code>, <code>Recycle</code>, and <code>Delete</code>.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="nodeAffinity" data-type="indexterm" id="idm46183198527952"/>A PersistentVolume can have a <code>nodeAffinity</code> that designates which Worker Node or nodes can access this volume. This is optional for most types but required for the <code>local</code> volume type.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="class attribute" data-type="indexterm" id="idm46183198718416"/>The <code>class</code> attribute binds this PV to a particular StorageClass, which is a concept we’ll introduce later in this chapter.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="mountOptions" data-type="indexterm" id="idm46183198522800"/>Some PersistentVolume types expose <code>mountOptions</code> that are specific to that type.</p></li>&#13;
</ul>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Differences in Volume Options</h1>&#13;
<p><a contenteditable="false" data-primary="volumes" data-secondary="differences in options for" data-type="indexterm" id="idm46183198520656"/>Options differ among volume types. For example, not every access mode or reclaim policy is accessible for every PersistentVolume type, so consult the documentation on your chosen type for more details.</p>&#13;
</div>&#13;
<p><a contenteditable="false" data-primary="kubectly describe persistentvolume command" data-type="indexterm" id="idm46183198553616"/>You use the <code>kubectl describe persistentvolume</code> command (or <code>kubectl describe pv</code> for short) to see the status of the PersistentVolume:</p>&#13;
<pre data-type="programlisting"><strong>kubectl describe pv my-volume</strong>&#13;
Name:              my-volume&#13;
Labels:            &lt;none&gt;&#13;
Annotations:       &lt;none&gt;&#13;
Finalizers:        [kubernetes.io/pv-protection]&#13;
StorageClass:&#13;
Status:            Available&#13;
Claim:&#13;
Reclaim Policy:    Retain&#13;
Access Modes:      RWO&#13;
VolumeMode:        Filesystem&#13;
Capacity:          3Gi&#13;
Node Affinity:&#13;
  Required Terms:&#13;
    Term 0:        kubernetes.io/hostname in [node1]&#13;
Message:&#13;
Source:&#13;
    Type:  LocalVolume (a persistent volume backed by local storage on a node)&#13;
    Path:  /app/data&#13;
Events:    &lt;none&gt;</pre>&#13;
<p>The PersistentVolume has a status of <code>Available</code> when first created. A PersistentVolume can have multiple status values:</p>&#13;
<dl>&#13;
<dt><code>Available</code></dt>&#13;
<dd>The PersistentVolume is free and not yet bound to a claim.</dd>&#13;
<dt><code>Bound</code></dt>&#13;
<dd><a contenteditable="false" data-primary="Bound status value" data-type="indexterm" id="idm46183198510752"/>The PersistentVolume is bound to a PersistentVolumeClaim, which is listed elsewhere in the <code>describe</code> output.</dd>&#13;
<dt><code>Released</code></dt>&#13;
<dd><a contenteditable="false" data-primary="Released status value" data-type="indexterm" id="idm46183198513920"/>An existing claim on the PersistentVolume has been deleted, but the resource has not yet been reclaimed, so the resource is not yet <code>Available</code>.</dd>&#13;
<dt><code>Failed</code></dt>&#13;
<dd><a contenteditable="false" data-primary="Failed status value" data-type="indexterm" id="idm46183198506080"/>The volume has failed its automatic reclamation.</dd>&#13;
</dl>&#13;
<p>Now that you’ve learned how storage resources are defined in Kubernetes, the next step is to learn how to use that storage in your applications<a contenteditable="false" data-primary="" data-startref="lpv_ab" data-type="indexterm" id="idm46183198504672"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="PersistentVolumeClaims" data-type="sect2"><div class="sect2" id="persistentvolumeclaims">&#13;
<h2>PersistentVolumeClaims</h2>&#13;
<p><a contenteditable="false" data-primary="PVCs (PersistentVolumeClaims)" data-type="indexterm" id="idm46183198501440"/>As we’ve discussed, Kubernetes separates the definition of storage from its usage. Often these tasks are performed by different roles: cluster administrators define the storage, while application developers use the storage. PersistentVolumes are typically defined by the administrators and reference storage locations that are specific to that cluster. Developers can then specify the storage needs of their applications using <em>PersistentVolumeClaims (PVCs)</em>, which Kubernetes uses to associate Pods with a PersistentVolume meeting the specified criteria. As shown in <a data-type="xref" href="#accessing_persistentvolumes_using_persi">Figure 2-10</a>, a PersistentVolumeClaim is used to reference the various volume types we introduced previously, including local PersistentVolumes, or external storage provided by cloud or networked storage vendors.</p>&#13;
<figure><div class="figure" id="accessing_persistentvolumes_using_persi">&#13;
<img alt="Accessing PersistentVolumes using PersistentVolumeClaims" src="assets/mcdk_0210.png"/>&#13;
<h6><span class="label">Figure 2-10. </span>Accessing PersistentVolumes using PersistentVolumeClaims</h6>&#13;
</div></figure>&#13;
<p>Here’s what the process looks like from an application developer perspective. First, you’ll create a PVC representing your desired storage criteria. For example, here’s a claim that requests 1 GB of storage with exclusive read/write access. The <a href="https://oreil.ly/njKPH">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  name: my-claim&#13;
spec:&#13;
  storageClassName: ""&#13;
  accessModes:&#13;
  - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      storage: 1Gi</pre>&#13;
<p>One interesting thing you may have noticed about this claim is that the <code>storageClassName</code> is set to an empty string. We’ll explain the significance of this when we discuss StorageClasses in the next section. You can reference the claim in the definition of a Pod like this. The <a href="https://oreil.ly/VnJN4">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: my-pod&#13;
spec:&#13;
  containers:&#13;
  - name: nginx&#13;
    image: nginx&#13;
    volumeMounts:&#13;
    - mountPath: "/app/data"&#13;
      name: my-volume&#13;
  volumes:&#13;
  - name: my-volume&#13;
    persistentVolumeClaim:&#13;
      claimName: my-claim</pre>&#13;
<p>As you can see, the PersistentVolume is represented within the Pod as a volume. The volume is given a name and a reference to the claim. This is considered to be a volume of the <code>persistentVolumeClaim</code> type. As with other volumes, the volume is mounted into a container at a specific mount point—in this case, into the main application Nginx container at the path <em>/app/data</em>.</p>&#13;
<p>A PVC also has a state, which you can see if you retrieve the status:</p>&#13;
<pre data-type="programlisting"><strong>kubectl describe pvc my-claim</strong>&#13;
Name:          my-claim&#13;
Namespace:     default&#13;
StorageClass:&#13;
Status:        Bound&#13;
Volume:        my-volume&#13;
Labels:        &lt;none&gt;&#13;
Annotations:   pv.kubernetes.io/bind-completed: yes&#13;
               pv.kubernetes.io/bound-by-controller: yes&#13;
Finalizers:    [kubernetes.io/pvc-protection]&#13;
Capacity:      3Gi&#13;
Access Modes:  RWO&#13;
VolumeMode:    Filesystem&#13;
Mounted By:    &lt;none&gt;&#13;
Events:        &lt;none&gt;</pre>&#13;
<p>A PVC has one of two status values: <code>Bound</code>, meaning it is bound to a volume (as in this example), or <code>Pending</code>, meaning that it has not yet been bound to a volume. Typically, a status of <code>Pending</code> means that no PV matching the claim exists.</p>&#13;
<p>Here’s what’s happening behind the scenes. Kubernetes uses the PVCs referenced as volumes in a Pod and takes those into account when scheduling the Pod. Kubernetes identifies PersistentVolumes that match properties associated with the claim and binds the smallest available module to the claim. The properties might include a label, or node affinity, as we saw previously for <code>local</code> volumes.</p>&#13;
<p>When starting up a Pod, the Kubernetes control plane makes sure the PersistentVolumes are mounted to the Worker Node. Then, each requested storage volume is mounted into the Pod at the specified mount point.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="StorageClasses" data-type="sect2"><div class="sect2" id="storageclasses">&#13;
<h2>StorageClasses</h2>&#13;
<p><a contenteditable="false" data-primary="SC (StorageClasses)" data-type="indexterm" id="sc_ab"/><a contenteditable="false" data-primary="dynamic provisioning" data-type="indexterm" id="idm46183198822512"/><a contenteditable="false" data-primary="static provisioning" data-type="indexterm" id="idm46183198473184"/>The previous example demonstrates how Kubernetes can bind PVCs to PersistentVolumes that already exist. This model in which PersistentVolumes are explicitly created in the Kubernetes cluster is known as <em>static provisioning</em>. The Kubernetes PersistentVolume subsystem also supports <em>dynamic provisioning</em> of volumes using <em>StorageClasses</em> (often abbreviated <em>SC</em>). The StorageClass is responsible for provisioning (and deprovisioning) PersistentVolumes according to the needs of applications running in the cluster, as shown in <a data-type="xref" href="#storageclasses_support_dynamic_provisio">Figure 2-11</a>.</p>&#13;
<figure><div class="figure" id="storageclasses_support_dynamic_provisio">&#13;
<img alt="StorageClasses support dynamic provisioning of volumes" src="assets/mcdk_0211.png"/>&#13;
<h6><span class="label">Figure 2-11. </span>StorageClasses support dynamic provisioning of volumes</h6>&#13;
</div></figure>&#13;
<p><a contenteditable="false" data-primary="kubectl get sc command" data-type="indexterm" id="idm46183198466448"/>Depending on the Kubernetes cluster you are using, at least one StorageClass is likely already available. You can verify this using the command <code>kubectl get sc</code>. If you’re running a simple Kubernetes distribution on your local machine and don’t see any StorageClasses, you can install an open source local storage provider from Rancher with the following command:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>set GH_LINK=https://raw.githubusercontent.com&#13;
kubectl apply -f \&#13;
  $GH_LINK/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml</strong></pre>&#13;
&#13;
<p>This storage provider comes preinstalled in K3s, a desktop distribution also provided by Rancher. If you take a look at the YAML configuration referenced in that statement, you’ll see the following definition of a StorageClass. The <a href="https://oreil.ly/nTocI">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: storage.k8s.io/v1&#13;
kind: StorageClass&#13;
metadata:&#13;
  name: local-path&#13;
provisioner: rancher.io/local-path&#13;
volumeBindingMode: WaitForFirstConsumer&#13;
reclaimPolicy: Delete</pre>&#13;
<p>As you can see from the definition, a StorageClass is defined by a few key attributes:</p>&#13;
<ul>&#13;
<li><p><a contenteditable="false" data-primary="in-tree storage plug-ins" data-type="indexterm" id="idm46183198482032"/><a contenteditable="false" data-primary="provisioner" data-type="indexterm" id="idm46183198462576"/><a contenteditable="false" data-primary="CSI (Container Storage Interface)" data-secondary="about" data-type="indexterm" id="idm46183198461600"/>The <code>provisioner</code> interfaces with an underlying storage provider such as a public cloud or storage system in order to allocate the actual storage. The provisioner can be either one of the Kubernetes built-in provisioners (referred to as <em>in-tree</em> because they are part of the Kubernetes source code), or a provisioner that conforms to the Container Storage Interface (CSI), which we’ll examine later in this chapter.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="reclaimPolicy" data-type="indexterm" id="idm46183198897040"/>The <code>reclaimPolicy</code> describes whether storage is reclaimed when the PersistentVolume is deleted. The default, <code>Delete</code>, can be overridden to <code>Retain</code>, in which case the storage administrator would be responsible for managing the future state of that storage with the storage provider.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="volumeBindingMode" data-type="indexterm" id="idm46183198507360"/>The <a href="https://oreil.ly/iFvrm"><code>volumeBindingMode</code></a> controls when the storage is provisioned and bound. If the value is <code>Immediate</code>, a PersistentVolume is immediately provisioned as soon as a PersistentVolumeClaim referencing the StorageClass is created, and the claim is bound to the PersistentVolume, regardless of whether the claim is referenced in a Pod. Many storage plug-ins also support a second mode known as <code>WaitForFirstConsumer</code>, in which case no PersistentVolume is provisioned until a Pod is created that references the claim. This behavior is considered preferable since it gives the Kubernetes scheduler more flexibility.</p></li>&#13;
<li><p>Although  not shown in this example, there is also an optional <code>allowVolumeExpansion</code> flag. This indicates whether the StorageClass supports the ability for volumes to be expanded. If <code>true</code>, the volume can be expanded by increasing the size of the <code>storage.request</code> field of the PersistentVolumeClaim. This value defaults to <code>false</code>.</p></li>&#13;
<li><p>Some StorageClasses also define <code>parameters</code>, specific configuration options for the storage provider that are passed to the provisioner. Common options include filesystem type, encryption settings, and throughput in terms of I/O operations per second (IOPS). Check the documentation for the storage provider for more details.</p></li>&#13;
</ul>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Limits on Dynamic Provisioning</h1>&#13;
<p>Local <a contenteditable="false" data-primary="dynamic provisioning" data-type="indexterm" id="idm46183198451072"/>PVs cannot be dynamically provisioned by a StorageClass, so you must create them manually yourself.</p>&#13;
</div>&#13;
<p>Application developers can reference a specific StorageClass when creating a PVC by adding a <code>storageClass</code> property to the definition. For example, here is a YAML configuration for a PVC referencing the <code>local-path</code> StorageClass. The <a href="https://oreil.ly/Ixwv7">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  name: my-local-path-claim&#13;
spec:&#13;
  storageClassName: local-path&#13;
  accessModes:&#13;
  - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      storage: 1Gi</pre>&#13;
<p>If no <code>storageClass</code> is specified in the claim, the default StorageClass is used. The default StorageClass can be set by the cluster administrator. As we showed in  <a data-type="xref" href="#persistentvolumes">“PersistentVolumes”</a>, you can opt out of using StorageClasses by using the empty string, which indicates that you are using statically provisioned storage.</p>&#13;
<p>StorageClasses provide a useful abstraction that cluster administrators and application developers can use as a contract: administrators define the StorageClasses, and developers reference the StorageClasses by name. The details of the underlying <span class="keep-together">StorageClass</span> implementation can differ across Kubernetes platform providers, promoting portability of applications.</p>&#13;
<p>This flexibility allows administrators to create StorageClasses representing a variety of storage options—for example, to distinguish between different quality-of-service guarantees in terms of throughput or latency. This concept is known as <em>profiles</em> in other storage systems. See <a data-type="xref" href="#how_developers_are_driving_the_future_o">“How Developers Are Driving the Future of Kubernetes Storage”</a> for more ideas on how StorageClasses can be leveraged in innovative ways<a contenteditable="false" data-primary="" data-startref="sc_ab" data-type="indexterm" id="idm46183198437040"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Kubernetes Storage Architecture" data-type="sect1"><div class="sect1" id="kubernetes_storage_architecture">&#13;
<h1>Kubernetes Storage Architecture</h1>&#13;
<p><a contenteditable="false" data-primary="architecture" data-secondary="of data storage" data-type="indexterm" id="arc_ds"/><a contenteditable="false" data-primary="data storage" data-secondary="architecture of" data-type="indexterm" id="ds_arc"/>In the preceding sections, we discussed the various storage resources that Kubernetes supports via its <a href="https://oreil.ly/k1Ttm">API</a>. In the remainder of the chapter, we’ll look at how these solutions are constructed, as they can give us valuable insights into constructing cloud native data solutions.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Defining Cloud Native Storage</h1>&#13;
<p><a contenteditable="false" data-primary="CNCF (Cloud Native Computing Foundation)" data-type="indexterm" id="idm46183198896496"/>Most of the storage technologies we discuss in this chapter are captured as part of the “cloud native storage” solutions listed in the <a href="https://oreil.ly/vY3wF">CNCF landscape</a>. The <a href="https://oreil.ly/bKRi9">CNCF Storage Whitepaper</a> is a helpful resource that defines key terms and concepts for cloud native storage. Both of these resources are updated regularly.</p>&#13;
</div>&#13;
<section data-pdf-bookmark="Flexvolume" data-type="sect2"><div class="sect2" id="flexvolume">&#13;
<h2>Flexvolume</h2>&#13;
<p><a contenteditable="false" data-primary="Flexvolume" data-type="indexterm" id="idm46183198423504"/><a contenteditable="false" data-primary="in-tree storage plug-ins" data-type="indexterm" id="idm46183198422528"/>Originally, the Kubernetes codebase contained multiple in-tree storage plug-ins (that is, included in the same GitHub repo as the rest of the Kubernetes code). This helped standardize the code for connecting to different storage platforms, but there were a couple of disadvantages. First, many Kubernetes developers had limited expertise across the broad set of included storage providers. More significantly, the ability to upgrade storage plug-ins was tied to the Kubernetes release cycle, meaning that if you needed a fix or enhancement for a storage plug-in, you’d have to wait until it was accepted into a Kubernetes release. This slowed the maturation of storage technology for Kubernetes and as a result, adoption slowed as well.</p>&#13;
<p>The Kubernetes community created the <em>Flexvolume specification</em> to allow development of plug-ins independently—that is, out of the Kubernetes source code tree and thus not tied to the Kubernetes release cycle. Around the same time, storage plug-in standards were emerging for other container orchestration systems, and developers from these communities began to question the wisdom of developing multiple standards to solve the same basic problem.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Future Flexvolume Support</h1>&#13;
<p>The Flexvolume feature has been deprecated in Kubernetes 1.23 in favor of the Container Storage <span class="keep-together">Interface.</span></p>&#13;
</div>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Container Storage Interface" data-type="sect2"><div class="sect2" id="container_storage_interface_left_parent">&#13;
<h2 class="less_space">Container Storage Interface</h2>&#13;
<p><a contenteditable="false" data-primary="“The State of State in Kubernetes”" data-primary-sortas="state" data-type="indexterm" id="idm46183198398576"/><a contenteditable="false" data-primary="CSI (Container Storage Interface)" data-secondary="about" data-type="indexterm" id="idm46183198398928"/><a contenteditable="false" data-primary="Ali, Saad" data-type="indexterm" id="idm46183198396720"/>The <em>Container Storage Interface (CSI)</em> initiative was established as an industry standard for storage for containerized applications. CSI is an open standard used to define plug-ins that will work across container orchestration systems including Kubernetes, Mesos, and Cloud Foundry. As Saad Ali, Google engineer and chair of the Kubernetes <a href="https://oreil.ly/JDsVv">Storage Special Interest Group (SIG)</a>, noted in <a href="https://oreil.ly/sUzfM">“The State of State in Kubernetes”</a> in <em>The New Stack</em>, “The Container Storage Interface allows Kubernetes to interact directly with an arbitrary storage system.”</p>&#13;
<p>The CSI specification is available on <a href="https://oreil.ly/kCOhg">GitHub</a>. Support for the CSI in Kubernetes began with the 1.<em>x</em> release, and it <a href="https://oreil.ly/AbUpe">went general availability (GA)</a> in the 1.13 release. Kubernetes continues to track updates to the CSI specification.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Additional CSI Resources</h1>&#13;
<p><a contenteditable="false" data-primary="CSI (Container Storage Interface)" data-secondary="documentation" data-type="indexterm" id="idm46183198388368"/>The <a href="https://oreil.ly/KFIXI">CSI documentation site</a> provides guidance for developers and storage providers who are interested in developing CSI-compliant drivers. The site also provides a very useful <a href="https://oreil.ly/wHkva">list of CSI-compliant drivers</a>. This list is generally more up-to-date than one provided on the Kubernetes documentation site.</p>&#13;
</div>&#13;
<p>Once a CSI implementation is deployed on a Kubernetes cluster, its capabilities are accessed through the standard Kubernetes storage resources such as PVCs, PVs, and SCs. On the backend, each CSI implementation must provide two plug-ins: a node plug-in and a controller plug-in, as depicted in <a data-type="xref" href="#csi_mapped_to_kubernetes">Figure 2-12</a>.</p> &#13;
&#13;
&#13;
&#13;
<p>The CSI specification defines required interfaces for these plug-ins using gRPC but does not specify exactly how the plug-ins are to be deployed. Let’s briefly look at the role of each of these services:</p>&#13;
<dl>&#13;
<dt>The controller plug-in</dt>&#13;
<dd><a contenteditable="false" data-primary="controller plug-in" data-type="indexterm" id="idm46183199052992"/>This plug-in supports operations on volumes such as create, delete, listing, publishing/unpublishing, tracking, and expanding volume capacity. It also tracks volume status including what nodes each volume is attached to. The controller plug-in is also responsible for taking and managing snapshots, and using snapshots to clone a volume. The controller plug-in can run on any node—it is a standard Kubernetes controller.</dd>&#13;
<dt>The node plug-in</dt>&#13;
<dd><a contenteditable="false" data-primary="node plug-in" data-type="indexterm" id="idm46183198379792"/><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183198764800"/><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183198763696"/>This plug-in runs on each Kubernetes Worker Node where provisioned volumes will be attached. The node plug-in is responsible for local storage, as well as mounting and unmounting volumes onto the node. The Kubernetes control plane directs the plug-in to mount a volume prior to any Pods being scheduled on the node that require the volume.</dd>&#13;
</dl>&#13;
<figure><div class="figure" id="csi_mapped_to_kubernetes">&#13;
<img alt="CSI mapped to Kubernetes" src="assets/mcdk_0212.png"/>&#13;
<h6><span class="label">Figure 2-12. </span>CSI mapped to Kubernetes</h6>&#13;
</div></figure>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="csi_migration">&#13;
<h5>CSI Migration</h5>&#13;
<p><a contenteditable="false" data-primary="GA (general availability)" data-type="indexterm" id="idm46183198371504"/><a contenteditable="false" data-primary="CSI (Container Storage Interface)" data-secondary="migration" data-type="indexterm" id="idm46183198370608"/>The Kubernetes community has been very conscious of preserving forward and backward compatibility among versions, and the transition from in-tree storage plug-ins to the CSI is no exception. Features in Kubernetes are typically introduced as alpha features, and progress to beta, before being released as GA. Introducing a new API such as the CSI presents a more complex challenge because it also involves the deprecation of older APIs.</p>&#13;
<p>The <a href="https://oreil.ly/qduG8">CSI migration approach</a> was introduced to promote a coherent experience for users of storage plug-ins. The implementation of each corresponding in-tree plug-in is changed to a facade when an equivalent CSI-compliant driver becomes available. Calls on the in-tree plug-in are delegated to the underlying CSI-compliant driver. The migration capability is itself a feature that can be enabled on a Kubernetes cluster.</p>&#13;
<p>This allows a staged adoption process that can be used as existing clusters are updated to newer Kubernetes versions. Each application can be updated independently to use CSI-compliant drivers instead of in-tree drivers. This approach to maturing and replacing APIs is a helpful pattern for promoting stability of the overall platform and providing administrators control over their migration to the new API.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Container Attached Storage" data-type="sect2"><div class="sect2" id="container_attached_storage">&#13;
<h2>Container Attached Storage</h2>&#13;
<p><a contenteditable="false" data-primary="Container Attached Storage pattern" data-type="indexterm" id="idm46183198408864"/>While the CSI is an important step forward in standardizing storage management across container orchestrators, it does not provide implementation guidance on how or where the storage software runs. Some CSI implementations are basically thin wrappers around legacy storage management software running outside of the Kubernetes cluster. While this reuse of existing storage assets certainly has its benefits, many developers have expressed a desire for storage management solutions that run entirely in Kubernetes alongside their applications.</p>&#13;
<p><em>Container Attached Storage</em> is a design pattern that provides a more cloud native approach to managing storage. The logic to manage storage operations such as attaching volumes to applications is itself composed of microservices running in containers. This allows the storage layer to have the same properties as other applications deployed on Kubernetes and reduces the number of different management interfaces administrators have to keep track of. The storage layer becomes just another Kubernetes application.</p>&#13;
<p><a contenteditable="false" data-primary="Powell, Evan, “Container Attached Storage: A Primer”" data-type="indexterm" id="idm46183198969840"/><a contenteditable="false" data-primary="“Container Attached Storage: A Primer” (Powell)" data-primary-sortas="container" data-type="indexterm" id="idm46183198591472"/>As Evan Powell noted in <a href="https://oreil.ly/zplhD">“Container Attached Storage: A Primer”</a> on the <em>CNCF Blog</em>:</p>&#13;
&#13;
<blockquote>&#13;
<p>Container Attached Storage reflects a broader trend of solutions that reinvent particular categories or create new ones—by being built on Kubernetes and microservices and that deliver capabilities to Kubernetes-based microservice environments. For example, new projects for security, DNS, networking, network policy management, messaging, tracing, logging and more have emerged in the cloud-native ecosystem.</p>&#13;
</blockquote>&#13;
<p>Several examples of projects and products embody the CAS approach to storage. Let’s examine a few of the open source options.</p>&#13;
&#13;
<section data-pdf-bookmark="OpenEBS" data-type="sect3"><div class="sect3" id="openebs">&#13;
<h3>OpenEBS</h3>&#13;
<p><a contenteditable="false" data-primary="EBS (Elastic Block Storage)" data-type="indexterm" id="idm46183198360736"/><a contenteditable="false" data-primary="OpenEBS" data-type="indexterm" id="idm46183198424640"/><em>OpenEBS</em> is a project created by MayaData and donated to the CNCF, where it became a Sandbox project in 2019. The name is a play on Amazon’s Elastic Block Store, and OpenEBS is an attempt to provide an open source equivalent to this popular managed service. OpenEBS provides storage engines for managing both local and NVMe PersistentVolumes.</p>&#13;
&#13;
<p>OpenEBS provides a great example of a CSI-compliant implementation deployed onto Kubernetes, as shown in <a data-type="xref" href="#openebs_architecture">Figure 2-13</a>. The control plane includes the OpenEBS provisioner, which implements the CSI controller interface, and the OpenEBS API server, which provides a configuration interface for clients and interacts with the rest of the Kubernetes control plane.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="NDM (Node Disk Manager)" data-type="indexterm" id="idm46183198372752"/>The OpenEBS data plane consists of the Node Disk Manager (NDM) as well as dedicated pods for each PersistentVolume. The NDM runs on each Kubernetes worker where storage will be accessed. It implements the CSI node interface and provides the helpful functionality of automatically detecting block storage devices attached to a Worker Node.</p>&#13;
&#13;
<figure><div class="figure" id="openebs_architecture">&#13;
<img alt="OpenEBS architecture" src="assets/mcdk_0213.png"/>&#13;
<h6><span class="label">Figure 2-13. </span>OpenEBS architecture</h6>&#13;
</div></figure>&#13;
&#13;
<p>OpenEBS creates multiple Pods for each volume. A controller Pod is created as the primary replica, and additional replica Pods are created on other Kubernetes Worker Nodes for high availability. Each Pod includes sidecars that expose interfaces for metrics collection and management, which allows the control plane to monitor and manage the data plane.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Longhorn" data-type="sect3"><div class="sect3" id="longhorn">&#13;
<h3>Longhorn</h3>&#13;
<p><a contenteditable="false" data-primary="Longhorn" data-type="indexterm" id="idm46183198566416"/><a contenteditable="false" data-primary="Rancher" data-type="indexterm" id="idm46183198565152"/><a contenteditable="false" data-primary="NFS (Network File System)" data-type="indexterm" id="idm46183198339616"/><a href="https://longhorn.io">Longhorn</a> is an open source, distributed block storage system for Kubernetes. It was originally developed by Rancher and became a CNCF Sandbox project in 2019. Longhorn focuses on providing an alternative to cloud-vendor storage and expensive external storage arrays. Longhorn supports providing incremental backups to NFS or S3-compatible storage, and live replication to a separate Kubernetes cluster for disaster recovery.</p>&#13;
&#13;
<p>Longhorn uses a similar architecture to that shown for OpenEBS; according to the <a href="https://oreil.ly/TXTjG">documentation</a>, “Longhorn creates a dedicated storage controller for each block device volume and synchronously replicates the volume across multiple replicas stored on multiple nodes. The storage controller and replicas are themselves orchestrated using Kubernetes.” Longhorn also provides an integrated user interface to simplify operations.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Rook and Ceph" data-type="sect3"><div class="sect3" id="rookandceph">&#13;
<h3>Rook and Ceph</h3>&#13;
<p><a contenteditable="false" data-primary="Ceph" data-type="indexterm" id="idm46183198352048"/><a contenteditable="false" data-primary="Rook" data-type="indexterm" id="idm46183198344592"/><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="about" data-type="indexterm" id="idm46183198333632"/>According to its website, “Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments.” Rook was originally created as a containerized version of Ceph that could be deployed in Kubernetes. <a href="https://ceph.io/en">Ceph</a> is an open source distributed storage framework that provides block, file, and object storage. Rook was the first storage project accepted by the CNCF and is now considered a <a href="https://oreil.ly/xmc1i">CNCF graduated project</a>.</p>&#13;
&#13;
<p>Rook is a truly Kubernetes native implementation in the sense that it makes use of Kubernetes custom resources (CRDs) and custom controllers called operators. Rook provides operators for Ceph, Cassandra, and NFS. We’ll learn more about custom resources and operators in <a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a>.</p>&#13;
&#13;
<p>Some commercial solutions for Kubernetes also embody the CAS pattern. These include <a href="https://mayadata.io">MayaData</a> (creators of OpenEBS), <a href="https://portworx.com">Portworx</a> by <a href="https://oreil.ly/3rJuQ">Pure Storage</a>, <a href="https://robin.io">Robin.io</a>, and <a href="https://storageos.com">StorageOS</a>. These companies provide both raw storage in block and file formats, as well as integrations for simplified deployments of additional data infrastructure such as databases and streaming solutions.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Container Object Storage Interface" data-type="sect2"><div class="sect2" id="container_object_storage_interface_left">&#13;
<h2>Container Object Storage Interface</h2>&#13;
<p><a contenteditable="false" data-primary="COSI (Container Object Storage Interface)" data-type="indexterm" id="cosi_ab"/>The CSI provides support for file and block storage, but object storage APIs require different semantics and don’t quite fit the CSI paradigm of mounting volumes. In Fall 2020, a group of companies led by <a href="https://min.io">MinIO</a> began work on a new API for object storage in container orchestration platforms: the <em>Container Object Storage Interface (COSI)</em>. COSI provides a Kubernetes <a href="https://oreil.ly/BwcKA">API</a> more suited to provisioning and accessing object storage, defining a <code>bucket</code> custom resource, and including operations to create buckets and manage access to buckets. The design of the COSI control plane and data plane is modeled after the CSI. COSI is an emerging standard with a great start and potential for wide adoption in the Kubernetes community and potentially beyond.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="how_developers_are_driving_the_future_o">&#13;
<h5>How Developers Are Driving the Future of Kubernetes Storage</h5>&#13;
<p><em>With Kiran Mova, Cofounder and CTO of MayaData and member of the Kubernetes</em> <a href="https://oreil.ly/GRRuD"><em>Storage SIG</em></a><em/></p>&#13;
<p><a contenteditable="false" data-primary="Mova, Kiran" data-type="indexterm" id="idm46183198322336"/><a contenteditable="false" data-primary="microservices" data-type="indexterm" id="idm46183198318464"/>Many organizations are just starting their containerization journey. Kubernetes is the shiny object, and everybody wants to run everything in Kubernetes. But not all teams are ready for Kubernetes, much less managing stateful workloads on Kubernetes.</p>&#13;
<p>Application developers are the ones driving the push for stateful workloads on Kubernetes. These developers get started with cloud resources that are available to them, even a single-node Kubernetes cluster, and assume they’re ready to run that in production. Developers are “Kuberneticizing” their in-house applications, and the demands on storage are quite different from what the platform teams that support them are used to.</p>&#13;
<p>Microservices and Kubernetes have changed the way storage volumes are provisioned. Platform teams are used to thinking about data in terms of provisioning volumes with the required throughput or capacity. In the old way, the platform team would meet with the application team, estimate the size of the data, do a month of planning, provision a 2–3 TB volume, and mount it into the VMs or bare-metal servers, and that would provide enough storage capacity for the next year.</p>&#13;
<p>With Kubernetes, provisioning has become ad hoc and much easier. You can run things in a highly cost-effective and agile way by adopting Kubernetes. But many platform teams are still working to catch up. Some teams are simply focused on provisioning storage correctly, while others are beginning to focus on “day two” operations, such as automated provisioning, expanding volumes, or disconnecting and destroying volumes.</p>&#13;
<p>Platform teams don’t yet have a foolproof way to run stateful workloads in Kubernetes, so they often offload persistence to public cloud providers. The public clouds make a strong case for their managed services, claiming they have everything that you’ll need to run a storage system, but once you start using managed services for state, you can become dependent on those cloud providers and get stuck.</p>&#13;
<p>Meanwhile, innovations in storage technology are happening in parallel:</p>&#13;
<ul>&#13;
<li><p>The landscape is shifting back and forth between hyperconverged and disaggregated. This rearchitecture is happening at all the layers of the stack, and it’s not just the software: it includes processes and the people who consume the data.</p></li>&#13;
<li><p>Hardware trends are driving toward low-latency solutions including NVMe and DPDK/SPDK, and changes to the Linux kernel like <span class="keep-together">io_uring</span> to take advantage of faster hardware.</p></li>&#13;
<li><p>Container Attached Storage will help us manage storage more effectively—for example, being able to reclaim storage space when workloads shrink. This can be a difficult problem with data distributed across multiple nodes. We’ll need better logic for relocating data onto existing nodes.</p></li>&#13;
<li><p>Technologies that bring more automation for compliance and operations are coming into the picture as well.</p></li>&#13;
</ul>&#13;
<p>With all these innovations, it can be a bit overwhelming to understand the big picture and determine how to leverage this technology for maximum benefit. Platform SREs need to learn about Kubernetes, declarative deployments, GitOps principles, new volume types, and even database concepts like eventual consistency.</p>&#13;
<p><a contenteditable="false" data-primary="fast storage" data-type="indexterm" id="idm46183198381936"/><a contenteditable="false" data-primary="slow storage" data-type="indexterm" id="idm46183198669232"/><a contenteditable="false" data-primary="metadata" data-type="indexterm" id="idm46183198780608"/>We envision a future in which application developers will specify their Kubernetes storage needs in terms of the required quality of service, such as IOPS and throughput. Developers should be able to specify different storage needs for their workloads in more human-relatable terms. For example, platform teams could define StorageClasses for “fast storage” versus “slow storage,” or perhaps “metadata storage” versus “data storage.” These StorageClasses will make different cost/performance trade-offs and provide specific service level agreements (SLAs). We may even see some standard definitions start to emerge for these new StorageClasses.</p>&#13;
<p>Ideally, application teams should not be choosing storage solutions. The only thing an application developer should be concerned with is specifying PersistentVolumeClaims for their application, with the StorageClasses they need.  The other details of managing storage should be hidden, although of course the storage subsystem will report errors including status and logs via the standard Kubernetes mechanisms. This capability will make things a lot simpler for application developers, whether they’re deploying a database or some other stateful workload.</p>&#13;
<p>These innovations will guide us to an optimal place with storage on Kubernetes. Today, deploying infrastructure is easy. Let’s work together to get to a place where deploying the <em>right</em> infrastructure is easy.</p>&#13;
</div></aside>&#13;
<p>As you can see, storage on Kubernetes is an area comprising a lot of innovation, including multiple open source projects and commercial vendors competing to provide the most usable, cost-effective, and performant solutions. The <a href="https://oreil.ly/cm4Ms">cloud native storage section</a> of the CNCF landscape provides a helpful listing of storage providers and related tools, including the technologies referenced in this chapter and many more.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000001">&#13;
<h1>Summary</h1>&#13;
<p><a contenteditable="false" data-primary="" data-startref="ds_kr" data-type="indexterm" id="idm46183198460080"/><a contenteditable="false" data-primary="" data-startref="arc_ds" data-type="indexterm" id="idm46183198458640"/><a contenteditable="false" data-primary="" data-startref="ds_arc" data-type="indexterm" id="idm46183198708320"/><a contenteditable="false" data-primary="" data-startref="cosi_ab" data-type="indexterm" id="idm46183198706944"/><a contenteditable="false" data-primary="" data-startref="ds_ch" data-type="indexterm" id="idm46183198705568"/><a contenteditable="false" data-primary="" data-startref="kub_ds" data-type="indexterm" id="idm46183198282576"/>In this chapter, we’ve explored how persistence is managed in container systems like Docker, and container orchestration systems like Kubernetes. You’ve learned about the various Kubernetes resources that can be used to manage stateful workloads, including Volumes, PersistentVolumes, PersistentVolumeClaims, and StorageClasses. We’ve seen how the Container Storage Interface and Container Attached Storage pattern point the way toward more cloud native approaches to managing storage. Now you’re ready to learn how to use these building blocks and design principles to manage stateful workloads including databases, streaming data, and more.</p>&#13;
</div></section>&#13;
</div></section></body></html>