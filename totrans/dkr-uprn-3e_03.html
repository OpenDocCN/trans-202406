<html><head></head><body><section data-pdf-bookmark="Chapter 2. The Docker Landscape" data-type="chapter" epub:type="chapter"><div class="chapter" id="docker_glance">&#13;
<h1><span class="label">Chapter 2. </span>The Docker Landscape</h1>&#13;
&#13;
&#13;
<p>Before you dive into configuring and installing Docker, a broad survey is in order to explain what Docker is and what it brings to the table. It is a powerful technology but not a tremendously complicated one at its core. In this chapter, we’ll cover the generalities of how Docker and Linux containers work, what makes them powerful, and some of the reasons you might use them. If you’re reading this, you probably have your reasons to use containers, but it never hurts to augment your understanding before you jump in.</p>&#13;
&#13;
<p>Don’t worry—this chapter should not hold you up for too long. In the next chapter, we’ll dive right into getting Docker installed and running on your system.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Process Simplification" data-type="sect1"><div class="sect1" id="idm46803170927664">&#13;
<h1>Process Simplification</h1>&#13;
&#13;
<p>Because Docker is a piece of software,<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="process simplification" data-type="indexterm" id="ch02-procsimp"/><a data-primary="workflow with Docker" data-secondary="process simplification" data-type="indexterm" id="ch02-procsimp2"/><a data-primary="deployment" data-secondary="simplified with Linux containers" data-type="indexterm" id="ch02-procsimp4"/><a data-primary="Linux containers" data-secondary="workflow with Docker" data-see="workflow with Docker" data-type="indexterm" id="idm46803170921632"/> it may not be obvious that it can also have a big positive impact on company and team processes if it is adopted and implemented well. So, let’s dig in and see how Docker and Linux containers can simplify both workflows and communication. This usually starts with the deployment story. Traditionally, the cycle of getting an application to production often looks something like the following (illustrated in <a data-type="xref" href="#figure2-1">Figure 2-1</a>):</p>&#13;
<ol>&#13;
<li>&#13;
<p>Application developers request resources from operations engineers.</p>&#13;
</li>&#13;
<li>&#13;
<p>Resources are provisioned and handed over to developers.</p>&#13;
</li>&#13;
<li>&#13;
<p>Developers script and tool their deployment.</p>&#13;
</li>&#13;
<li>&#13;
<p>Operations engineers and developers tweak the deployment repeatedly.</p>&#13;
</li>&#13;
<li>&#13;
<p>Additional application dependencies are discovered by developers.</p>&#13;
</li>&#13;
<li>&#13;
<p>Operations engineers work to install the additional requirements.</p>&#13;
</li>&#13;
<li>&#13;
<p>Loop over steps 4 through 6 <em>n</em> more times.</p>&#13;
</li>&#13;
<li>&#13;
<p>The application is deployed.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<figure><div class="figure" id="figure2-1">&#13;
<img alt="A Non-Docker deployment workflow" src="assets/dur3_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>A traditional deployment workflow (without Docker)</h6>&#13;
</div></figure>&#13;
&#13;
<p>Our experience has shown that when you are following traditional processes, deploying a brand-new application into production can take the better part of a week for a complex new system. That’s not very productive, and even though DevOps practices work to alleviate many of the barriers, it often still requires a lot of effort and communication between teams of people. This process can be both technically challenging and expensive, but even worse, it can limit the kinds of innovation that development teams will undertake in the future. If deploying new software is hard, time-consuming, and dependent on resources from another team, then developers may just build everything into the existing application in order to avoid suffering the new deployment penalty, or even worse, they may simply avoid solving problems that require new development efforts.</p>&#13;
&#13;
<p>Push-to-deploy systems<a data-primary="Heroku deployments" data-type="indexterm" id="idm46803167867200"/><a data-primary="deployment" data-secondary="push-to-deploy systems" data-type="indexterm" id="idm46803167866464"/> like <a href="https://www.heroku.com">Heroku</a> have shown developers what the world can look like if you are in control of your application and a majority of your dependencies. Talking with developers about deployment will often turn up discussions of how easy things are on Heroku or similar systems. If you’re an operations engineer, you’ve probably heard complaints about how much slower your internal systems are compared with “push-button” solutions like Heroku, which are built on top of Linux container technology.</p>&#13;
&#13;
<p>Heroku is a whole environment, not just a container engine. <a data-primary="dependencies" data-secondary="about Docker" data-type="indexterm" id="ch02-dep"/>While Docker doesn’t try to be everything that is included in Heroku, it provides a clean separation of responsibilities and encapsulation of dependencies, which results in a similar boost in productivity. Docker also allows even more fine-grained control than Heroku by putting developers in control of everything, down to the exact files and package versions that ship alongside their application. Some of the tooling and orchestrators that have been built on top of Docker (e.g., Kubernetes, Docker Swarm mode, and Mesos) aim to replicate the simplicity of systems like Heroku. But even though these platforms wrap more around Docker to provide a more capable and complex environment, a simple platform that uses only Docker still provides all of the core process benefits without the added complexity of a larger system.</p>&#13;
&#13;
<p>As a company, Docker adopts an approach of “batteries included but removable.” This means that its tools come with everything most people need to get the job done while still being built from interchangeable parts that can easily be swapped in and out to support custom solutions. By using an image repository as the hand-off point, Docker allows the responsibility of building the application image to be separated from the deployment and operation of the container. What this means in practice is that development teams can build their application with all of its dependencies, run it in development and test environments, and then just ship the exact same bundle of application and dependencies to production. Because those bundles all look the same from the outside, operations engineers can then build or install standard tooling to deploy and run the applications. The cycle described in <a data-type="xref" href="#figure2-1">Figure 2-1</a> then looks somewhat like this (illustrated in <a data-type="xref" href="#figure2-2">Figure 2-2</a>):</p>&#13;
<ol>&#13;
<li>&#13;
<p>Developers build the Docker image and ship it to the registry.</p>&#13;
</li>&#13;
<li>&#13;
<p>Operations engineers provide configuration details to the container and provision resources.</p>&#13;
</li>&#13;
<li>&#13;
<p>Developers trigger deployment.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<figure><div class="figure" id="figure2-2">&#13;
<img alt="A Docker deployment workflow" src="assets/dur3_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>A Docker deployment workflow</h6>&#13;
</div></figure>&#13;
&#13;
<p>This is possible because Docker allows all of the dependency issues to be discovered during the development and test cycles. By the time the application is ready for its first deployment, that work has already been done. And it usually doesn’t require as many handoffs between the development and operations teams. In a well-refined pipeline, this can completely alleviate the need for anyone other than the development team to be involved in the creation and deployment of a new service. That’s a lot simpler and saves a lot of time. Better yet, it leads to more robust software through testing of the deployment environment before release.<a data-startref="ch02-procsimp" data-type="indexterm" id="idm46803167855136"/><a data-startref="ch02-procsimp2" data-type="indexterm" id="idm46803167854432"/><a data-startref="ch02-procsimp4" data-type="indexterm" id="idm46803167853760"/><a data-startref="ch02-dep" data-type="indexterm" id="idm46803167853088"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Broad Support and Adoption" data-type="sect1"><div class="sect1" id="idm46803170927040">&#13;
<h1>Broad Support and Adoption</h1>&#13;
&#13;
<p>Docker is well supported, with<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="about" data-tertiary="broad support for" data-type="indexterm" id="ch02-supp"/><a data-primary="Linux containers" data-secondary="about" data-tertiary="broad support for" data-type="indexterm" id="ch02-supp2"/><a data-primary="Amazon ECS (Elastic Container Service)" data-secondary="Linux container support" data-type="indexterm" id="idm46803167847200"/><a data-primary="Elastic Container Service" data-see="Amazon ECS (Elastic Container Service)" data-type="indexterm" id="idm46803167846192"/><a data-primary="ECS" data-see="Amazon ECS (Elastic Container Service)" data-type="indexterm" id="idm46803171446096"/><a data-primary="Amazon EKS (Elastic Kubernetes Service)" data-secondary="Linux container use" data-type="indexterm" id="idm46803171445136"/><a data-primary="EKS" data-see="Amazon EKS (Elastic Kubernetes Service)" data-type="indexterm" id="idm46803171444176"/><a data-primary="Elastic Kubernetes Service" data-see="Amazon EKS (Elastic Kubernetes Service)" data-type="indexterm" id="idm46803171443216"/> the majority of the large public clouds offering some direct support for it. For example, Docker and Linux containers have been used in Amazon Web Services (AWS) via multiple products like Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon Fargate, and Amazon Elastic Beanstalk. Linux containers can also be used on Google App Engine (GAE), Google Kubernetes Engine, Red Hat OpenShift, IBM Cloud, Microsoft Azure, and many more. <a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="history of development" data-type="indexterm" id="idm46803171441616"/><a data-primary="Brewer, Eric" data-type="indexterm" id="idm46803171440400"/><a data-primary="Google supporting Docker" data-type="indexterm" id="idm46803171439728"/>At DockerCon 2014, Google’s Eric Brewer announced that Google would be supporting Docker as its primary internal container format. Rather than just being good PR for these companies, what this meant for the Docker community was that a lot of money began to back the stability and success of the Docker platform.</p>&#13;
&#13;
<p>Further building its influence, Docker’s image format<a data-primary="OCI images" data-secondary="about" data-tertiary="standard format for cloud providers" data-type="indexterm" id="idm46803171438656"/> for Linux containers has become the lingua franca among cloud providers, offering the potential for “write once, run anywhere” cloud applications. When Docker released its <code>libswarm</code> development library, an engineer from Orchard demonstrated deploying a Linux container to a heterogeneous mix of cloud providers at the same time. This kind of orchestration had not been easy before because every cloud provider offered a different API or toolset for managing instances, which were usually the smallest item you could manage with an API. What was only a promise from Docker in 2014 has since become fully mainstream as the largest companies continue to invest in the platform, support, and tooling. With most providers offering some form of Docker and Linux container orchestration as well as the container runtime itself, Docker is well supported for nearly any kind of workload in common production environments. If all of your tooling is built around Docker and Linux containers, then your applications can be deployed in a cloud-agnostic manner, allowing for new flexibility that was not previously possible.</p>&#13;
&#13;
<p>In 2017, <a href="https://thenewstack.io/docker-donate-container-runtime-containerd-cloud-native-computing-foundation">Docker donated its <code>containerd</code> runtime</a> to the <a href="https://www.cncf.io">Cloud Native Computing Foundation (CNCF)</a>, <a data-primary="containerd runtime" data-secondary="Cloud Native Computing Foundation" data-type="indexterm" id="idm46803171434288"/><a data-primary="Cloud Native Computing Foundation (CNCF)" data-secondary="containerd runtime" data-type="indexterm" id="idm46803171433296"/>and in 2019, it was elevated to the graduated project status.</p>&#13;
&#13;
<p>Today, the use of Linux containers in development, delivery, and production is bigger than ever. <a data-primary="Kubernetes" data-secondary="containerd runtime supported" data-type="indexterm" id="idm46803171431632"/>In 2022, we saw that Docker started to lose a share of the server market to the newest versions of Kubernetes that no longer require the Docker daemon, but even these releases of Kubernetes rely very heavily on the <code>containerd</code> runtime, which was initially developed by Docker. Docker also continues to have a very strong presence in many developer and CI/CD workflows.</p>&#13;
&#13;
<p>So, what about OS vendor support and adoption? <a data-primary="Docker server" data-primary-sortas="docker-a" data-type="indexterm" id="idm46803171429552"/><a data-primary="Docker client" data-primary-sortas="docker-a" data-type="indexterm" id="idm46803171428576"/>The Docker client runs directly on most major operating systems, and the server can run on Linux or Windows Server. The vast majority of the ecosystem is built around Linux servers, but other platforms are increasingly being supported. The beaten path is and will likely continue to revolve around Linux servers running Linux containers.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It is possible to run Windows containers<a data-primary="Windows containers" data-secondary="running natively" data-type="indexterm" id="idm46803171426464"/> natively (without a VM) on 64-bit versions of Windows Server 2016+. However, 64-bit versions of Windows 10+ Professional still require Hyper-V to provide the Windows Server kernel that is used for Windows containers. We will dive into a little more detail about this in <a data-type="xref" href="ch05.html#windows_containers">“Windows Containers”</a>.</p>&#13;
&#13;
<p>It is also worth noting here that<a data-primary="Linux containers" data-secondary="about" data-tertiary="Windows running" data-type="indexterm" id="idm46803171424144"/><a data-primary="Windows Subsystem for Linux (WSL 2)" data-type="indexterm" id="idm46803171422896"/><a data-primary="Windows running Docker" data-secondary="WSL 2" data-see="Windows Subsystem for Linux" data-type="indexterm" id="idm46803171422160"/> Windows can run Linux containers outside a virtual machine by leveraging WSL 2 (Windows Subsystem for Linux, version 2).</p>&#13;
</div>&#13;
&#13;
<p>To support the growing demand for Docker tooling in development environments, <a data-primary="Windows running Docker" data-secondary="about" data-type="indexterm" id="idm46803170964640"/><a data-primary="macOS running Docker" data-secondary="about" data-type="indexterm" id="idm46803170963696"/><a data-primary="Linux" data-secondary="Docker requiring" data-type="indexterm" id="idm46803170962752"/><a data-primary="Linux" data-secondary="kernel" data-tertiary="virtual machine providing" data-type="indexterm" id="idm46803170961808"/>Docker has released easy-to-use implementations for macOS and Windows. These appear to run natively but are still utilizing a small Linux virtual machine to provide the Docker server and Linux kernel. Docker has traditionally been developed on the Ubuntu Linux distribution, but most Linux distributions and other major operating systems are now supported where possible. Red Hat, for example, has gone all in on containers, and all of its platforms have first-class support for Docker. <a data-primary="Fedora CoreOS (Red Hat)" data-secondary="Linux container workload support" data-type="indexterm" id="idm46803170959952"/><a data-primary="Red Hat" data-secondary="Fedora CoreOS" data-see="Fedora CoreOS" data-type="indexterm" id="idm46803170958992"/>With the near-ubiquity of containers in the Linux realm, we now have distributions like Red Hat’s Fedora CoreOS, which is built entirely for Linux container workloads.</p>&#13;
&#13;
<p id="run_times">In the first years after Docker’s release,<a data-primary="OCI images" data-secondary="about" data-tertiary="history of development" data-type="indexterm" id="idm46803170956736"/> a set of competitors and service providers voiced concerns about Docker’s proprietary image format. Containers on Linux did not have a standard image format, so Docker, Inc., created its own according to the needs of its business.</p>&#13;
&#13;
<p>Service providers and commercial vendors were particularly reluctant to build platforms that might be subject to the whims of a company with overlapping interests to their own. Docker as a company faced some public challenges in that period as a result. <a data-primary="Open Container Initiative (OCI)" data-secondary="about OCI images" data-type="indexterm" id="idm46803170954976"/><a data-primary="OCI images" data-secondary="about" data-type="indexterm" id="idm46803170953984"/>To gain some goodwill and support wider adoption in the marketplace, Docker, Inc., decided to help sponsor <a href="https://www.opencontainers.org">the Open Container Initiative (OCI)</a> in June of 2015. The first full specification from that effort was released in July 2017 and was based in large part on version 2 of the Docker image format. It is now possible to apply for OCI certification for both container images and container runtimes.</p>&#13;
&#13;
<p>This is the primary high-level OCI-certified runtime:<a data-primary="containerd runtime" data-secondary="about" data-type="indexterm" id="idm46803170951664"/><a data-primary="runtimes" data-secondary="containerd and its runtimes" data-type="indexterm" id="idm46803170950688"/></p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://containerd.io"><code>containerd</code></a>, which is the default high-level runtime in modern versions of Docker and Kubernetes.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>These lower-level OCI-certified runtimes can be used by <code>containerd</code> to manage and create containers:<a data-primary="containerd runtime" data-secondary="lower-level OCI-certified runtimes used by" data-type="indexterm" id="idm46803170946384"/><a data-primary="runc as containerd default runtime" data-type="indexterm" id="idm46803170945312"/><a data-primary="crun runtime" data-type="indexterm" id="idm46803170944624"/><a data-primary="Kata Containers runtime" data-type="indexterm" id="idm46803170943952"/><a data-primary="gVisor runtime" data-type="indexterm" id="idm46803170943280"/><a data-primary="Nabla Containers runtime" data-type="indexterm" id="idm46803170942608"/><a data-primary="runtimes" data-secondary="runc as default for containerd" data-type="indexterm" id="idm46803170941920"/><a data-primary="runtimes" data-secondary="crun runtime" data-type="indexterm" id="idm46803170940960"/><a data-primary="runtimes" data-secondary="Kata Containers runtime" data-type="indexterm" id="idm46803170940016"/><a data-primary="runtimes" data-secondary="gVisor runtime" data-type="indexterm" id="idm46803170939072"/><a data-primary="runtimes" data-secondary="Nabla Containers runtime" data-type="indexterm" id="idm46803170938128"/><a data-primary="runtimes" data-secondary="containerd and its runtimes" data-tertiary="runc as default" data-type="indexterm" id="idm46803170937168"/></p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://github.com/opencontainers/runc"><code>runc</code></a> is often used as the default lower-level runtime by <code>containerd</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://github.com/containers/crun"><code>crun</code></a> is written in C and designed to be fast and have a small memory footprint.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://katacontainers.io">Kata Containers</a> from Intel, Hyper, and the OpenStack Foundation is a virtualized runtime that can run a mix of containers and virtual machines.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://github.com/google/gvisor">gVisor</a> from Google is a sandboxed runtime, implemented entirely in user space.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://nabla-containers.github.io">Nabla Containers</a> provide another sandboxed runtime designed to significantly reduce the attack surface of Linux containers.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The space around deploying containers and orchestrating entire systems of containers continues to expand, too. Many of these are open source and available both on premises and as cloud or software as a service (SaaS) offerings from various providers, either in their clouds or yours. Given the amount of investment continuing to pour into the Linux container space, it’s likely that Docker will continue to have an important role in the modern internet.<a data-startref="ch02-supp" data-type="indexterm" id="idm46803169486304"/><a data-startref="ch02-supp2" data-type="indexterm" id="idm46803169485600"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Architecture" data-type="sect1"><div class="sect1" id="idm46803169484672">&#13;
<h1>Architecture</h1>&#13;
&#13;
<p>Docker is a powerful technology, <a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="about" data-type="indexterm" id="idm46803169482912"/><a data-primary="architecture of Docker" data-secondary="about" data-type="indexterm" id="idm46803169481392"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="about" data-type="indexterm" id="idm46803169480448"/>and that often indicates both tools and processes that come with a high level of complexity. And, under the hood, Docker is fairly complex; however, its fundamental user-facing structure is indeed a simple client/server model. Several pieces are sitting behind the Docker API, including <code>containerd</code> and <code>runc</code>, but the basic system interaction is a client talking over an API to a server. Underneath this simple exterior, Docker heavily leverages kernel mechanisms such as iptables, virtual bridging, Linux control groups (cgroups), Linux namespaces, Linux capabilities, secure computing mode, various filesystem drivers, and more. We’ll talk about some of these in <a data-type="xref" href="ch11.html#advanced_topics">Chapter 11</a>. For now, we’ll go over how the client and server work and give a brief introduction to the network layer that sits underneath a Linux container in Docker.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Client/Server Model" data-type="sect2"><div class="sect2" id="idm46803169477312">&#13;
<h2>Client/Server Model</h2>&#13;
&#13;
<p>It’s easiest to think of Docker as consisting<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="client/server model" data-type="indexterm" id="idm46803169475792"/><a data-primary="client/server model of Docker" data-type="indexterm" id="idm46803169474272"/><a data-primary="architecture of Docker" data-secondary="client/server model" data-type="indexterm" id="idm46803169473632"/><a data-primary="Docker client" data-primary-sortas="docker-a" data-secondary="client/server model" data-type="indexterm" id="idm46803169472688"/><a data-primary="image registries" data-secondary="client/server model" data-type="indexterm" id="idm46803169471472"/><a data-primary="Docker daemon" data-primary-sortas="docker-a" data-secondary="client/server model" data-seealso="Docker server" data-type="indexterm" id="idm46803169470528"/> of two parts: the client and the server/daemon (see <a data-type="xref" href="#figure2-3">Figure 2-3</a>). Optionally there is a third component called the <em>registry</em>, which stores Docker images and their metadata. The server does the ongoing work of building, running, and managing your containers, and you use the client to tell the server what to do. <a data-primary="daemon" data-see="Docker daemon" data-type="indexterm" id="idm46803168494352"/><a data-primary="docker commands" data-primary-sortas="docker-z" data-secondary="about" data-tertiary="client/server model" data-type="indexterm" id="idm46803168493376"/><a data-primary="dockerd" data-primary-sortas="docker-z" data-secondary="about Docker server" data-tertiary="client/server model" data-type="indexterm" id="idm46803168491888"/>The Docker <a href="https://en.wikipedia.org/wiki/Daemon_(computing)">daemon</a> can run on any number of servers in the infrastructure, and a single client can address any number of servers. Clients drive all of the communication, but Docker servers can talk directly to image registries when told to do so by the client. Clients are responsible for telling servers what to do, and servers focus on hosting and managing containerized applications.</p>&#13;
&#13;
<figure><div class="figure" id="figure2-3">&#13;
<img alt="Docker Client/Server Model" src="assets/dur3_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>Docker client/server model</h6>&#13;
</div></figure>&#13;
&#13;
<p>Docker is a little different in structure from some other client/server software. It has a <code>docker</code> client and a <code>dockerd</code> server, but rather than being entirely monolithic, the server then orchestrates a few other components behind the scenes on behalf of the client, including <code>containerd-shim-runc-v2</code>, which is used to interact with <code>runc</code> and <code>containerd</code>. Docker cleanly hides any complexity behind the simple server API, though, so you can just think of it as a straightforward client and server for most purposes. Each Docker host will normally have one Docker server running that can manage any number of containers. You can then use the <code>docker</code> command-line tool to talk to the server, either from the server itself or, if properly secured, from a remote client. We’ll talk more about that shortly.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Network Ports and Unix Sockets" data-type="sect2"><div class="sect2" id="idm46803168484112">&#13;
<h2>Network Ports and Unix Sockets</h2>&#13;
&#13;
<p>The <code>docker</code> command-line tool and <code>dockerd</code> daemon can<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="network ports and Unix sockets" data-type="indexterm" id="idm46803168481536"/><a data-primary="architecture of Docker" data-secondary="network ports and Unix sockets" data-type="indexterm" id="idm46803168479968"/><a data-primary="networking" data-secondary="ports of Docker" data-type="indexterm" id="idm46803168479008"/><a data-primary="Unix sockets of Docker" data-type="indexterm" id="idm46803168478064"/><a data-primary="docker commands" data-primary-sortas="docker-z" data-secondary="about" data-tertiary="network ports and Unix sockets" data-type="indexterm" id="idm46803168477392"/><a data-primary="dockerd" data-primary-sortas="docker-z" data-secondary="about Docker server" data-tertiary="network ports and Unix sockets" data-type="indexterm" id="idm46803168475888"/><a data-primary="Internet Assigned Numbers Authority (IANA)" data-type="indexterm" id="idm46803168474384"/><a data-primary="TCP port for unencrypted traffic" data-type="indexterm" id="idm46803168473680"/><a data-primary="encrypted SSL connection port" data-type="indexterm" id="idm46803168472992"/><a data-primary="SSL encrypted connection port" data-type="indexterm" id="idm46803168472304"/><a data-primary="ports for Docker networking" data-type="indexterm" id="idm46803168471616"/> talk to each other over Unix sockets and network ports. Docker, Inc., has registered three ports with the <a href="https://www.iana.org">Internet Assigned Numbers Authority (IANA)</a> for use by the Docker daemon and client: TCP port 2375 for unencrypted traffic, port 2376 for encrypted SSL connections, and port 2377 for Docker Swarm mode. Using a different port is easily configurable for scenarios where you need to use different settings. The default setting for the Docker installer is to only use a Unix socket for communication with the local Docker daemon. This ensures that the system defaults to the most secure installation possible. <a data-primary="Docker daemon" data-primary-sortas="docker-a" data-secondary="user authentication and access controls lacking" data-type="indexterm" id="idm46803168470016"/>This is also easily configurable, but it is highly recommended that network ports are not used with Docker, due to the lack of user authentication and role-based access controls within the Docker daemon. The Unix socket can be located in different paths on different operating systems, but in most cases, it can be found here: <em>/var/run/docker.sock</em>. If you have strong preferences for a different location, you can usually specify this at install time or simply change the server configuration afterward and restart the daemon. If you don’t, then the defaults will probably work for you. As with most software, following the defaults will save you a lot of trouble if you don’t need to change them.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Recent versions of Docker Desktop may create the <em>docker.sock</em> file in the user’s home directory inside <em>.docker/run/</em> and then simply link <em>_/var/run/docker.sock</em> to this location.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Robust Tooling" data-type="sect2"><div class="sect2" id="idm46803168465456">&#13;
<h2>Robust Tooling</h2>&#13;
&#13;
<p>Among the many things that have led<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="tooling shipped with" data-type="indexterm" id="idm46803170660032"/><a data-primary="architecture of Docker" data-secondary="tooling shipped with" data-type="indexterm" id="idm46803170658512"/> to Docker’s strong adoption is its simple and powerful tooling. Since its initial release, its capabilities have been expanding ever wider, thanks to efforts from the Docker community at large. The tooling that Docker ships with supports building Docker images, basic deployment to individual Docker daemons, a distributed mode called Swarm mode, and all the functionality needed to manage a remote Docker server. Beyond the included Swarm mode, community efforts have focused on managing whole fleets (or clusters) of Docker servers and scheduling and orchestrating container deployments.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>When we talk about <a href="https://docs.docker.com/engine/swarm">Docker Swarm or Swarm mode</a> in this book,<a data-primary="Docker Swarm (classic)" data-primary-sortas="docker-a" data-type="indexterm" id="idm46803170655680"/><a data-primary="Docker Swarm mode" data-primary-sortas="docker-a" data-secondary="about" data-type="indexterm" id="idm46803170654704"/> we are referring to the built-in Swarm functionality in the Docker client and server, which leverages another underlying library called SwarmKit. When searching for articles on the internet, you may find references to an older standalone version of Docker Swarm, which is often referred to as <a href="https://github.com/docker-archive/classicswarm">Docker Swarm “Classic”</a> nowadays.</p>&#13;
</div>&#13;
&#13;
<p>Docker has also launched its own<a data-primary="architecture of Docker" data-secondary="tooling shipped with" data-tertiary="orchestration tools" data-type="indexterm" id="idm46803170651792"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="orchestration tools" data-type="indexterm" id="idm46803170650544"/><a data-primary="orchestration tools" data-secondary="Docker" data-type="indexterm" id="idm46803170649056"/><a data-primary="deployment" data-secondary="orchestration tools" data-tertiary="Docker" data-type="indexterm" id="idm46803170648112"/> orchestration toolset, including <a href="https://github.com/docker/compose">Compose</a>, <a href="https://www.docker.com/products/docker-desktop">Docker Desktop</a>, and <a href="https://docs.docker.com/engine/swarm">Swarm mode</a>, which creates a cohesive deployment story for developers. <a data-primary="production" data-secondary="orchestration tools of Docker" data-type="indexterm" id="idm46803170644512"/><a data-primary="Kubernetes" data-secondary="production orchestration" data-type="indexterm" id="idm46803170643440"/><a data-primary="production" data-secondary="Kubernetes for orchestration" data-type="indexterm" id="idm46803170642480"/>Docker’s offerings in the &#13;
<span class="keep-together">production</span> orchestration space have been largely overshadowed by Google’s Kubernetes, although it should be noted that <a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq">Kubernetes relied heavily on Docker until v1.24 was released in early 2022</a>. But Docker’s orchestration tools remain useful, with Compose being particularly handy for local development.</p>&#13;
&#13;
<p>Because Docker provides both<a data-primary="REST API" data-type="indexterm" id="idm46803170639504"/><a data-primary="docker commands" data-primary-sortas="docker-z" data-secondary="command-line tool" data-type="indexterm" id="idm46803170638800"/> a command-line tool and a remote REST API, it is easy to add further tooling in any language. The command-line tool lends itself well to shell scripting, and anything the client can do can also be done programmatically via the REST API. The Docker CLI is so well-known that many other Linux container CLI tools, like <a href="https://podman.io"><code>podman</code></a> and <a href="https://github.com/containerd/nerdctl"><code>nerdctl</code></a>, mimic its arguments for compatibility and easy adoption.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Docker Command-Line Tool" data-type="sect2"><div class="sect2" id="idm46803170635600">&#13;
<h2>Docker Command-Line Tool</h2>&#13;
&#13;
<p>The command-line tool <code>docker</code> is the main<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="command-line tool" data-type="indexterm" id="idm46803170633584"/><a data-primary="architecture of Docker" data-secondary="command-line tool" data-type="indexterm" id="idm46803170632032"/><a data-primary="Go language" data-secondary="docker client" data-secondary-sortas="docker-z" data-type="indexterm" id="idm46803170631088"/><a data-primary="docker commands" data-primary-sortas="docker-z" data-secondary="about" data-tertiary="Go language used for" data-type="indexterm" id="idm46803170629872"/><a data-primary="Docker client" data-primary-sortas="docker-a" data-secondary="Go language used for" data-type="indexterm" id="idm46803171129344"/> interface that most people will have with Docker. The Docker client is a <a href="https://golang.org">Go program</a> that compiles and runs on all common architectures and operating systems. The command-line tool is available as part of the main Docker distribution on various platforms and also compiles directly from the Go source. Some of the things you can typically do with the Docker command-line tool include, but are not limited to, the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Building a container image</p>&#13;
</li>&#13;
<li>&#13;
<p>Pulling images from a registry to a Docker daemon or pushing them up to a registry from the Docker daemon</p>&#13;
</li>&#13;
<li>&#13;
<p>Starting a container on a Docker server either in the foreground or background</p>&#13;
</li>&#13;
<li>&#13;
<p>Retrieving the Docker logs from a remote server</p>&#13;
</li>&#13;
<li>&#13;
<p>Interactively running a command inside a running container on a remote server</p>&#13;
</li>&#13;
<li>&#13;
<p>Monitoring statistics about your container</p>&#13;
</li>&#13;
<li>&#13;
<p>Getting a process listing from your container</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>You can probably see how these can be composed into a workflow for building, deploying, and observing applications. But the Docker command-line tool is not the only way to interact with Docker, and it’s not necessarily the most powerful.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Docker Engine API" data-type="sect2"><div class="sect2" id="idm46803171119952">&#13;
<h2>Docker Engine API</h2>&#13;
&#13;
<p>Like many other pieces of modern software,<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="daemon API" data-type="indexterm" id="idm46803171118560"/><a data-primary="architecture of Docker" data-secondary="daemon API" data-type="indexterm" id="idm46803171117040"/><a data-primary="Docker daemon" data-primary-sortas="docker-a" data-secondary="API" data-type="indexterm" id="idm46803171116096"/><a data-primary="APIs" data-secondary="Docker daemon" data-type="indexterm" id="idm46803171114880"/> the Docker daemon has an API. This is in fact what the Docker command-line tool uses to communicate with the daemon. But because the API is documented and public, it’s quite common for external tooling to use the API directly. This provides a convenient mechanism that allows any tool to create, inspect, and manage all of the images and containers that are under the Docker daemon’s management. While it’s unlikely that beginners will initially want to talk directly to the Docker API, it’s a great tool to have available. As your organization embraces Docker over time, you will increasingly find the API to be a good integration point for &#13;
<span class="keep-together">this tooling.</span></p>&#13;
&#13;
<p>Extensive documentation for<a data-primary="APIs" data-secondary="Docker daemon" data-tertiary="documentation" data-type="indexterm" id="idm46803171112640"/><a data-primary="Docker daemon" data-primary-sortas="docker-a" data-secondary="API" data-tertiary="documentation" data-type="indexterm" id="idm46803171111392"/><a data-primary="architecture of Docker" data-secondary="daemon API" data-tertiary="documentation" data-type="indexterm" id="idm46803171109904"/><a data-primary="resources online" data-secondary="Docker documentation" data-tertiary="API" data-type="indexterm" id="idm46803171108688"/> the <a href="https://dockr.ly/2wxCHnx">API</a> is on the Docker site. As the ecosystem has matured, robust implementations of Docker API libraries have emerged for all popular languages. Docker maintains <a href="https://dockr.ly/2wxCHnx">SDKs for Python and Go</a>, and there are additional libraries maintained by third parties that are worth considering. For example, over the years we have used these <a href="https://github.com/fsouza/go-dockerclient">Go</a> and <a href="https://github.com/upserve/docker-api">Ruby</a> libraries and have found them to be both robust and rapidly updated as new versions of Docker are released.</p>&#13;
&#13;
<p>Most of the things you can do with the Docker command-line tooling are supported relatively easily via the API. Two notable exceptions are the endpoints that require streaming or terminal access: running remote shells or executing the container in interactive mode. In these cases, it’s often easier to use one of these solid client libraries or the command-line tool.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Container Networking" data-type="sect2"><div class="sect2" id="idm46803171103504">&#13;
<h2>Container Networking</h2>&#13;
&#13;
<p>Even though Linux containers are largely<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="container networking" data-type="indexterm" id="idm46803171101712"/><a data-primary="architecture of Docker" data-secondary="container networking" data-type="indexterm" id="idm46803171100192"/><a data-primary="container networking" data-seealso="networking" data-type="indexterm" id="idm46803171099248"/><a data-primary="networking" data-secondary="about container networking" data-type="indexterm" id="idm46803171098304"/><a data-primary="Linux containers" data-secondary="container networking" data-seealso="networking" data-type="indexterm" id="idm46803171097392"/><a data-primary="processes" data-secondary="Linux containers" data-type="indexterm" id="idm46803171096176"/> made up of processes running on the host system itself, they usually behave quite differently from other processes at the network layer. Docker initially supported a single networking model but now supports a robust assortment of configurations that handle most application requirements. <a data-primary="bridge mode" data-type="indexterm" id="idm46803171095104"/><a data-primary="Linux containers" data-secondary="bridge mode" data-type="indexterm" id="idm46803171094432"/>Most people run their containers in the default configuration, called <em>bridge mode</em>. So let’s take a look at how it works.</p>&#13;
&#13;
<p>To understand bridge mode, it’s easiest to think of each of your Linux containers as behaving like a host on a private network. The Docker server acts as a virtual bridge, and the containers are clients behind it. A bridge is just a network device that repeats traffic from one side to another. So you can think of it like a mini virtual network, with each container acting like a host attached to that network. The actual implementation (see <a data-type="xref" href="#figure02-1">Figure 2-4</a>) is that<a data-primary="IP addresses" data-secondary="container networking" data-type="indexterm" id="idm46803171648576"/><a data-primary="vpnkit library" data-type="indexterm" id="idm46803171647600"/> each container has a virtual Ethernet interface connected to the Docker bridge and an IP address allocated to the virtual interface. Docker lets you bind and expose individual or groups of ports on the host to the container so that the outside world can reach your container on those ports. The traffic is largely managed by the <a href="https://github.com/moby/vpnkit">vpnkit</a> library.</p>&#13;
&#13;
<p>Docker allocates the private subnet<a data-primary="RFC 1918 private subnet block" data-type="indexterm" id="idm46803171645504"/><a data-primary="subnet from unused private subnet block" data-type="indexterm" id="idm46803171644784"/> from an unused <a href="https://www.rfc-editor.org/rfc/rfc1918">RFC 1918</a> private subnet block. It detects which network blocks are unused on the host and allocates one of those to the virtual network. <a data-primary="Docker server" data-primary-sortas="docker-a" data-secondary="docker0 interface" data-type="indexterm" id="idm46803171643120"/><a data-primary="docker0 server interface" data-primary-sortas="docker-z" data-type="indexterm" id="idm46803171641872"/><a data-primary="dockerd" data-primary-sortas="docker-z" data-secondary="about Docker server" data-tertiary="docker0 server interface" data-type="indexterm" id="idm46803171640912"/>That is bridged to the host’s local network through an interface on the server called <code>docker0</code>. This means that, by default, all of the containers are on a network together and can talk to one another directly. But to get to the host or the outside world, they go over the <code>docker0</code> virtual bridge interface.</p>&#13;
&#13;
<figure><div class="figure" id="figure02-1">&#13;
<img alt="The network on a typical Docker server" src="assets/dur3_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>The network on a typical Docker server</h6>&#13;
</div></figure>&#13;
&#13;
<p>There is a dizzying array of ways in which you can configure Docker’s network layer, from allocating your own network blocks to configuring your own custom bridge interface. People often run with the default mechanisms, but there are times when something more complex or specific to your application is required. <a data-primary="documentation" data-secondary="Docker" data-tertiary="networking" data-type="indexterm" id="idm46803171635904"/><a data-primary="resources online" data-secondary="Docker documentation" data-tertiary="networking" data-type="indexterm" id="idm46803171634656"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="documentation" data-tertiary="networking" data-type="indexterm" id="idm46803171633440"/>You can find much more detail about Docker networking in the <a href="https://dockr.ly/2otp461">documentation</a>, and we will cover more details in <a data-type="xref" href="ch11.html#advanced_topics">Chapter 11</a>.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>When developing your Docker workflow,<a data-primary="workflow with Docker" data-secondary="starting with default networking" data-type="indexterm" id="idm46803171628960"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="starting with default networking" data-type="indexterm" id="idm46803171627920"/><a data-primary="networking" data-secondary="default approach recommended" data-type="indexterm" id="idm46803171626400"/><a data-primary="docker container" data-primary-sortas="docker-z" data-secondary="run" data-tertiary="--net=host switch" data-tertiary-sortas="net=host" data-type="indexterm" id="idm46803171625440"/><a data-primary="docker container" data-primary-sortas="docker-z" data-secondary="run" data-tertiary="default network approach when starting" data-type="indexterm" id="idm46803171623680"/> you should get started with the default networking approach. You might later find that you don’t want or need this default virtual network. Networking is configurable per container, and you can switch off the whole virtual network layer entirely for a container using the <code>--net=host</code> switch to <code>docker container run</code>. When running in that mode, Linux containers use the host’s own network devices and addresses, and no virtual interfaces or bridges are provisioned. Note that host networking has security implications you might need to consider. Other network topologies are possible and discussed in <a data-type="xref" href="ch11.html#advanced_topics">Chapter 11</a>.</p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Getting the Most from Docker" data-type="sect1"><div class="sect1" id="idm46803171619952">&#13;
<h1>Getting the Most from Docker</h1>&#13;
&#13;
<p>Like most tools, Docker has a number<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="characteristics" data-tertiary="about" data-type="indexterm" id="idm46803171617984"/> of great use cases, and others that aren’t so good. You can, for example, open a glass jar with a hammer. But that has its downsides. Understanding how to best use the tool, or even simply determining if it’s the right tool, can get you on the correct path much more quickly.</p>&#13;
&#13;
<p>To begin with, Docker’s architecture is aimed<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="stateless or state externalized" data-type="indexterm" id="idm46803171615648"/><a data-primary="architecture of Docker" data-secondary="stateless or state externalized" data-type="indexterm" id="idm46803171614160"/> squarely at applications that are either stateless or where the state is externalized into data stores like databases or caches. Those are the easiest to containerize. Docker enforces some good development &#13;
<span class="keep-together">principles</span> for this class of application, and we’ll talk later about how that’s powerful. <a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="about" data-tertiary="capabilities not handled by" data-type="indexterm" id="idm46803171612416"/>But this means that doing things like putting a database engine inside Docker is a bit like swimming against the current. It’s not that you can’t do it, or even that you shouldn’t do it; it’s just that this is not the most obvious use case for Docker, so if it’s the one you start with, you may find yourself disappointed early on. Databases that run well in Docker are now often deployed this way, but this is not the simple path. Some good applications for beginning with Docker include web frontends, backend APIs, and short-running tasks like maintenance scripts that might normally be handled by <code>cron</code>.</p>&#13;
&#13;
<p>If you focus first on building an understanding of running stateless or externalized-state applications inside containers, you will have a foundation on which to start considering other use cases. We strongly recommend starting with stateless applications and learning from that experience before tackling other use cases. The community is continuously working on how to better support stateful applications in Docker, and there are likely to be many developments in this area.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Containers Are Not Virtual Machines" data-type="sect2"><div class="sect2" id="idm46803171609296">&#13;
<h2>Containers Are Not Virtual Machines</h2>&#13;
&#13;
<p>A good way to start shaping your understanding<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="characteristics" data-tertiary="containers as ephemeral wrappers" data-type="indexterm" id="idm46803170769008"/><a data-primary="Linux containers" data-secondary="characteristics" data-tertiary="ephemeral wrappers" data-type="indexterm" id="idm46803170767424"/><a data-primary="virtual machines (VM)" data-secondary="containers versus" data-type="indexterm" id="ch02-vmvscon"/> of how to leverage Docker is to think of Linux containers not as virtual machines (VMs) but as very lightweight wrappers around a single Unix process. During actual implementation, that process might spawn other processes, but on the other hand, one statically compiled binary could be all that’s inside your container (see <a data-type="xref" href="ch09.html#dependencies">“Outside Dependencies”</a> for more information). Containers are also ephemeral: they may come and go much more readily than a traditional virtual machine.</p>&#13;
&#13;
<p>Virtual machines are by design a stand-in for real hardware that you might throw in a rack and leave there for a few years. Because a real server is what they’re abstracting, virtual machines are often long-lived in nature. Even in the cloud where companies often spin virtual machines up and down on demand, they usually have a running life span of days or more. On the other hand, a particular container might exist for months, or it may be created, run a task for a minute, and then be destroyed. All of that is OK, but it’s a fundamentally different approach than the one virtual machines are typically used for.</p>&#13;
&#13;
<p>To help drive this differentiation home, if you run Docker on a mac or Windows system, you are leveraging a Linux virtual machine to run <code>dockerd</code>, the Docker server. <a data-primary="dockerd" data-primary-sortas="docker-z" data-secondary="about Docker server" data-tertiary="native on Linux" data-type="indexterm" id="idm46803170762544"/>However, on Linux, <code>dockerd</code> can be run natively, and therefore there is no need for a virtual machine to be run anywhere on the system (see <a data-type="xref" href="#figure2-4">Figure 2-5</a>).</p>&#13;
&#13;
<figure><div class="figure" id="figure2-4">&#13;
<img alt="Typical Docker Installations" src="assets/dur3_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>Typical Docker installations</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Limited Isolation" data-type="sect2"><div class="sect2" id="idm46803170757376">&#13;
<h2>Limited Isolation</h2>&#13;
&#13;
<p>Containers are isolated from one another,<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="characteristics" data-tertiary="limited isolation of containers" data-type="indexterm" id="idm46803170755808"/><a data-primary="Linux containers" data-secondary="characteristics" data-tertiary="limited isolation" data-type="indexterm" id="idm46803170754272"/><a data-primary="processes" data-secondary="Linux containers" data-tertiary="limited isolation" data-type="indexterm" id="idm46803170753056"/> but that isolation is probably more limited than you might expect. While you can put limits on their resources, the default container configuration just has them all sharing CPU and memory on the host system, much as you would expect from colocated Unix processes. This means that unless you constrain them, containers can compete for resources on your production machines. That might be fine for your use case, but it impacts your design decisions. Limits on CPU and memory use are encouraged through Docker, but in most cases, they are not the default like they would be with a virtual machine.</p>&#13;
&#13;
<p>It’s often the case that many containers share one or more common filesystem layers. That’s one of the more powerful design decisions in Docker, but it also means<a data-primary="OCI images" data-secondary="about" data-tertiary="updating shared images" data-type="indexterm" id="idm46803170751328"/> that if you update a shared image, you may also need to rebuild and redeploy containers that are still utilizing the older image.</p>&#13;
&#13;
<p>Containerized processes are just processes<a data-primary="Linux" data-secondary="kernel" data-tertiary="container as just another process" data-type="indexterm" id="idm46803170749360"/> on the Docker server itself. They are running on the same instance of the Linux kernel as the host operating system. All container processes show up in the normal <code>ps</code> output on the Docker server. That is utterly different from a hypervisor, where the depth of process isolation usually includes running an entirely separate instance of the operating system kernel for each virtual machine.</p>&#13;
&#13;
<p>This light containment can lead to the tempting option of exposing more resources from the host, such as shared filesystems to allow the storage of state. But you should think hard before further exposing resources from the host into the container unless they are used exclusively by the container. We’ll talk about the security of containers later, <a data-primary="Security-Enhanced Linux (SELinux)" data-type="indexterm" id="idm46803170747168"/><a data-primary="AppArmor" data-type="indexterm" id="idm46803170746448"/><a data-primary="Linux" data-secondary="AppArmor" data-type="indexterm" id="idm46803170745776"/>but generally, you might consider helping to enforce isolation further by applying <a href="https://www.redhat.com/en/topics/linux/what-is-selinux">Security-Enhanced Linux (SELinux)</a> or <a href="https://apparmor.net">AppArmor</a> policies rather than compromising the existing barriers.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>By default, many containers use UID 0 to<a data-primary="processes" data-secondary="Linux containers" data-tertiary="UID 0 to launch" data-type="indexterm" id="idm46803170741824"/> launch processes. Because the container is <em>contained</em>, this seems safe, but in reality, it isn’t very safe. <a data-primary="Linux" data-secondary="kernel" data-tertiary="container root user unauthorized access" data-type="indexterm" id="idm46803170740000"/>Because everything is running on the same kernel, many types of security vulnerabilities or simple misconfiguration can give the container’s <code>root</code> user unauthorized access to the host’s system resources, files, and processes. Refer to <a data-type="xref" href="ch11.html#security">“Security”</a> for a discussion of how to mitigate this.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Containers Are Lightweight" data-type="sect2"><div class="sect2" id="idm46803170736672">&#13;
<h2>Containers Are Lightweight</h2>&#13;
&#13;
<p>We’ll get more into the details of how<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="characteristics" data-tertiary="containers using minimal disk space" data-type="indexterm" id="idm46803170735344"/><a data-primary="Linux containers" data-secondary="characteristics" data-tertiary="minimal disk space" data-type="indexterm" id="idm46803170733808"/> this works later, but creating a new container can take up very little disk space. A quick test reveals that a newly created container from an existing image takes a whopping 12 kilobytes of disk space. That’s pretty lightweight. On the other hand, a new virtual machine created from a golden image might require hundreds or thousands of megabytes, since at a minimum it requires a full operating install to exist on that disk. The new container, on the other hand, is so small because it is just a reference to a layered filesystem image and some metadata about the configuration. By default, no copy of the data is allocated to the container. Containers are just processes on the existing system that may only need to read information from the disk, so there may not be a need to copy any data for the exclusive use of the container, until a time when it needs to write data that is unique to that container instance.</p>&#13;
&#13;
<p>The lightness of containers means that you can use them for situations where creating another virtual machine would be too heavyweight or where you need something to be truly ephemeral. You probably wouldn’t, for instance, spin up an entire virtual machine to run a <code>curl</code> command to a website from a remote location, but you might spin up a new container for this purpose.<a data-startref="ch02-vmvscon" data-type="indexterm" id="idm46803170731456"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Toward an Immutable Infrastructure" data-type="sect2"><div class="sect2" id="idm46803170730496">&#13;
<h2>Toward an Immutable Infrastructure</h2>&#13;
&#13;
<p>By deploying most of your applications<a data-primary="immutable infrastructure" data-secondary="container deployment for" data-type="indexterm" id="idm46803170728944"/><a data-primary="deployment" data-secondary="immutable infrastructure via containers" data-type="indexterm" id="idm46803170727888"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="characteristics" data-tertiary="immutable infrastructure" data-type="indexterm" id="idm46803171395968"/><a data-primary="Linux containers" data-secondary="characteristics" data-tertiary="immutable infrastructure" data-type="indexterm" id="idm46803171394464"/> within containers, you can start simplifying your configuration management story by moving toward an immutable infrastructure, where components are replaced entirely rather than changed in place. The idea of an immutable infrastructure has gained popularity in response to how difficult it is, in reality, to maintain a truly idempotent configuration management codebase. As your configuration management codebase grows, it can become as unwieldy and unmaintainable as large, monolithic legacy applications.</p>&#13;
&#13;
<p>With Docker, it is possible to deploy a very<a data-primary="Docker server" data-primary-sortas="docker-a" data-secondary="configuration management minimal" data-type="indexterm" id="idm46803171392720"/><a data-primary="dockerd" data-primary-sortas="docker-z" data-secondary="about Docker server" data-tertiary="server configuration management minimal" data-type="indexterm" id="idm46803171391456"/> lightweight Docker server that needs almost no configuration management, or in many cases, none at all. You handle all of your application management simply by deploying and redeploying containers to the server. When the server needs an important update to something like the Docker daemon or the Linux kernel, you can simply bring up a new server with the changes, deploy your containers there, and then decommission or reinstall the old server.</p>&#13;
&#13;
<p>Container-based Linux distributions like <a href="https://getfedora.org/en/coreos">Red Hat’s Fedora CoreOS</a> are designed<a data-primary="Fedora CoreOS (Red Hat)" data-secondary="OS configuration minimal" data-type="indexterm" id="idm46803171388672"/> around this principle. But rather than requiring you to decommission the instance, Fedora CoreOS can entirely update itself and switch to the updated OS. Your configuration and workload largely remain in your containers, and you don’t have to configure the OS very much at all.</p>&#13;
&#13;
<p>Because of this clean separation between deployment and configuration of your servers, many container-based production systems are using tools<a data-primary="Packer (HashiCorp)" data-type="indexterm" id="idm46803171386656"/><a data-primary="HashiCorp" data-secondary="Packer" data-type="indexterm" id="idm46803171385952"/> such as <a href="https://www.packer.io/intro/index.html">HashiCorp’s Packer</a> to build cloud virtual server images, and then leveraging Docker to nearly or entirely avoid configuration &#13;
<span class="keep-together">management</span> systems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Stateless Applications" data-type="sect2"><div class="sect2" id="idm46803171383232">&#13;
<h2>Stateless Applications</h2>&#13;
&#13;
<p>A good example of the kind of application<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="characteristics" data-tertiary="stateless applications" data-type="indexterm" id="idm46803171381808"/><a data-primary="stateless applications" data-type="indexterm" id="ch02-state"/> that containerizes well is a web application that keeps its state in a database. Stateless applications are normally designed to immediately answer a single self-contained request and have no need to track information between requests from one or more clients. You might also run something like ephemeral <a href="https://memcached.org">Memcached</a> instances in containers. If you think about your web application, though, it probably has some local state that you rely on, like configuration files. That might not seem like a lot of state, but if you bake that configuration into your images, it means that you’ve limited the reusability of your image and made it more challenging to deploy into different environments, without maintaining multiple images for different deployment targets.</p>&#13;
&#13;
<p>In many cases, the process of containerizing your application means that you move configuration state into environment variables that can be passed to your application at runtime. Rather than baking the configuration into the container, you apply the configuration to the container when it is deployed. This allows you to easily do things like use the same container to run in either production or staging environments. In most companies, those environments would require many different configuration settings like the connection URLs for various external services that the application utilizes.</p>&#13;
&#13;
<p>With containers, you might also find that you are always decreasing the size of your containerized application as you optimize it down to the bare essentials required to run. We have found that thinking of anything that you need to run in a distributed way as a container can lead to some interesting design decisions. If, for example, you have a service that collects some data, processes it, and returns the result, you might configure containers on many servers to run the job and then aggregate the response on another container.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Externalizing State" data-type="sect2"><div class="sect2" id="idm46803171377472">&#13;
<h2>Externalizing State</h2>&#13;
&#13;
<p>If Docker works best for stateless applications,<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="characteristics" data-tertiary="externalizing state" data-type="indexterm" id="idm46803171375920"/><a data-primary="configuration" data-secondary="environment variables" data-tertiary="stateless applications" data-type="indexterm" id="idm46803171374432"/><a data-primary="environment variables" data-secondary="configuration via" data-type="indexterm" id="idm46803171373216"/><a data-primary="environment variables" data-secondary="metadata of Linux containers" data-type="indexterm" id="idm46803171372272"/><a data-primary="Linux containers" data-secondary="metadata" data-tertiary="state stored in environment variables" data-type="indexterm" id="idm46803171371360"/><a data-primary="metadata of Linux containers" data-secondary="state stored in environment variables" data-type="indexterm" id="idm46803171370176"/> how do you best store state when you need to? Configuration is typically passed by environment variables, for example. Docker supports environment variables natively, and they are stored in the metadata that makes up a container configuration. This means that restarting the container will ensure that the same configuration is passed to your application each time. It also makes the configuration of the container easily observable while it’s running, which can make debugging a lot easier, although there are some security concerns around exposing secrets in environment variables. <a data-primary="datastores for configuration information" data-type="indexterm" id="idm46803171369072"/>It is also possible to store and retrieve your application configuration inside an external datastore, like <a href="https://www.consul.io">Consul</a> or <a href="https://www.postgresql.org">PostgreSQL</a>.</p>&#13;
&#13;
<p>Databases are often where scaled applications store state, and nothing in Docker interferes with doing that for containerized applications. <a data-primary="filesystem state storage" data-type="indexterm" id="idm46803171366176"/>Applications that need to store files, however, face some challenges. Storing things to the container’s filesystem is not performant, will be limited by space, and will not preserve state when a container is re-created. If you redeploy a stateful service without utilizing storage external to the container, you will lose all of that state. Applications that need to store filesystem state should be carefully considered before you put them into Docker. If you decide that you can benefit from Linux containers in these cases, it’s best to design a solution where the state can be stored in a centralized location that could be accessed regardless of which host a container runs on. In certain cases, this might mean using a service like Amazon Simple Storage Service (Amazon S3), OpenStack Swift, or a local block store, or even mounting EBS volumes or iSCSI disks inside the container. <a href="https://docs.docker.com/engine/extend/plugins_volume">Docker volume plug-ins</a> provide some additional options and are briefly discussed in <a data-type="xref" href="ch11.html#advanced_topics">Chapter 11</a>.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Although it is possible to externalize state<a data-primary="dependencies" data-secondary="filesystem state storage" data-type="indexterm" id="idm46803171362768"/> on the host’s local filesystem, it is not generally encouraged by the community and should be considered an advanced use case. It is strongly recommended that you start with applications that don’t need persistent state. There are multiple reasons why this is typically discouraged, but in almost all cases it is because it introduces dependencies between the container and the host that interfere with using Docker as a truly dynamic, horizontally scalable application delivery service. If your container maintains state on the local host filesystem, then it can only be deployed to the system that houses that local filesystem. Remote volumes that can be dynamically attached are a good solution but also an advanced use case.<a data-startref="ch02-state" data-type="indexterm" id="idm46803171361648"/></p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="The Docker Workflow" data-type="sect1"><div class="sect1" id="idm46803171360464">&#13;
<h1>The Docker Workflow</h1>&#13;
&#13;
<p>Like many tools, Docker strongly encourages a particular workflow. It’s a very enabling workflow that maps well to how many companies are organized, but it’s probably a little different than what you or your team are doing now. Having adapted our own organizations’ workflows to the Docker approach, we can confidently say that this is a change that can have a wide-reaching positive impact on many teams in your organization. If the workflow is implemented well, it can help you realize the promise of reduced communication overhead between teams.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Revision Control" data-type="sect2"><div class="sect2" id="idm46803171357936">&#13;
<h2>Revision Control</h2>&#13;
&#13;
<p>The first thing that Docker gives you<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="revision control" data-type="indexterm" id="idm46803171356608"/><a data-primary="workflow with Docker" data-secondary="revision control" data-tertiary="about" data-type="indexterm" id="idm46803171355024"/><a data-primary="revision control" data-secondary="about" data-type="indexterm" id="idm46803171353808"/> out of the box is two forms of revision control. One of them is used to track the filesystem layers that each Docker image is comprised of, and the other is a tagging system for those images.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Filesystem layers" data-type="sect3"><div class="sect3" id="idm46803171352608">&#13;
<h3>Filesystem layers</h3>&#13;
&#13;
<p>Linux containers are made up of stacked filesystem layers,<a data-primary="revision control" data-secondary="filesystem layers" data-type="indexterm" id="idm46803171351088"/><a data-primary="workflow with Docker" data-secondary="revision control" data-tertiary="filesystem layers" data-type="indexterm" id="idm46803169354384"/><a data-primary="Linux containers" data-secondary="filesystem layers" data-type="indexterm" id="idm46803169353168"/><a data-primary="filesystem layers of Linux containers" data-type="indexterm" id="idm46803169352224"/><a data-primary="OCI images" data-secondary="filesystem layers" data-type="indexterm" id="idm46803169351584"/><a data-primary="hashes" data-secondary="container filesystem layer identification" data-type="indexterm" id="idm46803169350640"/><a data-primary="revision control" data-secondary="Git" data-see="Git" data-type="indexterm" id="idm46803169349728"/><a data-primary="revision control" data-secondary="GitHub" data-see="GitHub" data-type="indexterm" id="idm46803169348512"/> each identified by a unique hash, where each new set of changes made during the build process is laid on top of the previous changes. That’s great because it means that when you do a new build, you only have to rebuild the layers that follow the change you’re deploying. This saves time and bandwidth because containers are shipped around as layers, and you don’t have to ship layers that a server already has stored. If you’ve done deployments with many classic deployment tools, you know that you can end up shipping hundreds of megabytes of the same data to a server over and over with each deployment. That’s incredibly inefficient, and worse, you can’t be sure exactly what changed between deployments. <a data-primary="dependencies" data-secondary="filesystem layers of Linux containers" data-type="indexterm" id="idm46803169347168"/>Because of the layering effect, and because Linux containers include all of the application dependencies, with Docker you can be more confident about the changes that you are shipping to production.</p>&#13;
&#13;
<p>To simplify this a bit, remember that a Docker image contains everything required to run your application. If you change one line of code, you certainly don’t want to waste time rebuilding every dependency that your code requires into a new image. Instead, by leveraging the build cache, Docker can ensure that only the layers affected by the code change are rebuilt.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Image tags" data-type="sect3"><div class="sect3" id="idm46803169345568">&#13;
<h3>Image tags</h3>&#13;
&#13;
<p>The second kind of revision control offered<a data-primary="workflow with Docker" data-secondary="revision control" data-tertiary="image tags" data-type="indexterm" id="idm46803169344240"/><a data-primary="revision control" data-secondary="image tags" data-type="indexterm" id="idm46803169342992"/><a data-primary="image tags" data-secondary="revision control" data-type="indexterm" id="idm46803169342048"/> by Docker makes it easy to answer an important question: what was the previous version of the application that was deployed? That’s not always easy to answer. There are a lot of solutions for non-containerized applications, from Git tags for each release, to deployment logs, to tagged builds for deployment, and many more. If you’re coordinating your deployment with <a href="https://capistranorb.com">Capistrano</a>, for example, it will handle this for you by keeping a set number of previous releases on the server and then using symlinks to make one of them the current release.</p>&#13;
&#13;
<p>But what you find in any scaled production environment is that each application has a unique way of handling deployment revisions. Many of them do the same thing, but some may be different. Worse, in heterogeneous language environments, the deployment tools are often entirely different between applications, and very little is shared. So the question “What was the previous version?” can have many answers depending on whom you ask and which application you’re referring to. Docker has a built-in mechanism for handling this: image tagging a standard build step. <a data-primary="rollbacks" data-secondary="image tags enabling" data-type="indexterm" id="idm46803169339840"/>You can easily leave multiple revisions of your application on the server so that performing a rollback is trivial. This is not rocket science, and it’s not functionality that is hard to find in other deployment tooling, but with container images, it can easily be made standard across all of your applications, and everyone can have the same expectations about how things will be tagged for all applications. This makes communication easier between teams, and it makes tooling much simpler because there is one source of truth for application releases.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>In many examples online and in this book,<a data-primary="image tags" data-secondary="latest tag" data-type="indexterm" id="idm46803169336768"/><a data-primary="latest tag" data-type="indexterm" id="idm46803169335792"/><a data-primary="revision control" data-secondary="image tags" data-tertiary="latest tag" data-type="indexterm" id="idm46803169335120"/> you will see people use the <code>latest</code> tag for a container image. This is useful when you’re getting started and when you’re writing examples, as it will always grab the most recent build of an image. But since this is a floating tag, it is a really bad idea to use <code>latest</code> in most production workflows, as your dependencies can get updated out from under you, and it is impossible to roll back to <code>latest</code> because the old version is no longer the one tagged <code>latest</code>. It also makes it hard to verify if the same image is running on different servers. The rule of thumb is: don’t use the <code>latest</code> tag in production. It’s not even a good idea to use the <code>latest</code> tag from upstream images, for the same reasons.</p>&#13;
&#13;
<p>It is highly recommended that you tag your CI/CD builds with something that uniquely identifies the exact source code commit that was used to build them. In a <code>git</code> workflow, this could be the git hash related to the commit. <a data-primary="image tags" data-secondary="semantic versioning" data-type="indexterm" id="idm46803169330016"/><a data-primary="semantic versioning for image tags" data-type="indexterm" id="idm46803169328960"/><a data-primary="revision control" data-secondary="image tags" data-tertiary="semantic versioning" data-type="indexterm" id="idm46803169328272"/>Once you are ready to release an image, the recommendation is that you use <a href="https://semver.org">semantic versioning</a> and provide your image with tags, like 1.4.3, 2.0.0, etc.</p>&#13;
&#13;
<p>Pinning versions requires a bit more work to keep them current, but it will also prevent many unfortunate and poorly timed surprises during builds and deployments.</p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Building" data-type="sect2"><div class="sect2" id="idm46803169325168">&#13;
<h2>Building</h2>&#13;
&#13;
<p>Building applications is a black art in many<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="building applications" data-type="indexterm" id="idm46803169323136"/><a data-primary="workflow with Docker" data-secondary="building applications" data-type="indexterm" id="idm46803169321600"/><a data-primary="building a Docker image" data-secondary="Docker workflow" data-type="indexterm" id="idm46803169320656"/><a data-primary="deployment" data-secondary="Docker workflow" data-type="indexterm" id="idm46803169319712"/><a data-primary="OCI images" data-secondary="building an image" data-tertiary="Docker workflow" data-type="indexterm" id="idm46803169318768"/><a data-primary="docker image" data-primary-sortas="docker-z" data-secondary="build" data-tertiary="Docker workflow" data-type="indexterm" id="idm46803169317552"/> organizations, where a few people know all the levers to pull and knobs to turn to spit out a well-formed, shippable artifact. Part of the heavy cost of getting a new application deployed is getting the build just right. Docker doesn’t solve all of these problems, but it does provide a standardized tool configuration and toolset for builds. That makes it a lot easier for people to learn how to build your applications and to get new builds up and running.</p>&#13;
&#13;
<p>The Docker command-line tool contains<a data-primary="build flag" data-type="indexterm" id="idm46803169315200"/><a data-primary="Dockerfiles" data-primary-sortas="docker-a" data-secondary="build flag" data-type="indexterm" id="idm46803169314496"/> a <code>build</code> flag that will consume a <em>Dockerfile</em> and produce a Docker image. Each command in a <em>Dockerfile</em> generates a new layer in the image, so it’s easy to reason about what the build is going to do by looking at the <em>Dockerfile</em> itself. The great part of all of this standardization is that any engineer who has worked with a <em>Dockerfile</em> can dive right in and modify the build of any other application. Because the Docker image is a standardized artifact, all of the tooling behind the build will be the same regardless of the development language or base image that is being used or the number of layers needed. The <em>Dockerfile</em> is usually checked into a revision control system, which also means that tracking changes to the build is simplified. Modern multistage Docker builds also allow you to define the build environment separately from the final artifact image. This provides huge “configure ability” for your build environment just like you’d have for a production container.</p>&#13;
&#13;
<p>Many Docker builds are a single invocation of the <code>docker image build</code> command and generate a single artifact, the container image. Because it’s usually the case that most of the logic about the build is wholly contained in the <em>Dockerfile</em>, it’s <a data-primary="Jenkins build system" data-type="indexterm" id="idm46803169308208"/>easy to create standard build jobs for any team to use in build systems like <a href="https://jenkins-ci.org">Jenkins</a>. As a further standardization of the build process, many companies—eBay, for example—have standardized Linux containers to do the image builds from a <em>Dockerfile</em>. SaaS build offerings like <a href="https://travis-ci.com">Travis CI</a> and <a href="https://codeship.com">CodeShip</a> also have first-class support for Docker builds.</p>&#13;
&#13;
<p>It is also possible to automate<a data-primary="building a Docker image" data-secondary="Docker workflow" data-tertiary="BuildKit support in Docker" data-type="indexterm" id="idm46803171189792"/> the creation of multiple images that support different underlying compute architectures, like x86 and ARM, by utilizing the newer <a href="https://github.com/moby/buildkit">BuildKit</a> support in Docker.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Testing" data-type="sect2"><div class="sect2" id="idm46803171187328">&#13;
<h2>Testing</h2>&#13;
&#13;
<p>While Docker itself does not include<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="testing" data-type="indexterm" id="idm46803171185728"/><a data-primary="workflow with Docker" data-secondary="testing" data-type="indexterm" id="idm46803171184144"/><a data-primary="testing" data-secondary="Docker workflow" data-type="indexterm" id="idm46803171183200"/> a built-in framework for testing, the way containers are built lends some advantages to testing with Linux containers.</p>&#13;
&#13;
<p>Testing a production application can take many forms, from unit testing to full integration testing in a semi-live environment. Docker facilitates better testing by guaranteeing that the artifact that passed testing will be the one that ships to production. This can be guaranteed because we can either use the Docker SHA for the container, or a custom tag to make sure we’re consistently shipping the same version of the application.</p>&#13;
&#13;
<p>Since, by design, containers include<a data-primary="dependencies" data-secondary="reliability of tests on containers" data-type="indexterm" id="idm46803171181200"/> all of their dependencies, tests run on containers are very reliable. If a unit test framework says tests were successful against a container image, you can be sure that you will not experience a problem with the versioning of an underlying library at deployment time, for example. That’s not easy with most other technologies, and even Java WAR (Java Web application ARchive) files, for example, don’t include testing of the application server itself. That same Java application deployed in a Linux container will generally also include an application server like Tomcat, and the whole stack can be smoke tested before shipping to production.</p>&#13;
&#13;
<p>A secondary benefit of shipping applications<a data-primary="APIs" data-secondary="multiple applications communicating via" data-type="indexterm" id="idm46803171179648"/> in Linux containers is that in places where there are multiple applications that talk to one another remotely via something like an API, developers of one application can easily develop against a version of the other service that is currently tagged for the environment they require, like production or staging. Developers on each team don’t have to be experts in how the other service works or is deployed just to do development on their own application. If you expand this to a service-oriented architecture with innumerable microservices, Linux containers can be a real lifeline to developers or QA engineers who need to wade into the swamp of inter-microservice API calls.</p>&#13;
&#13;
<p>A common practice in organizations that run Linux containers in production is for automated integration tests to pull down a versioned set of Linux containers for different services, matching the current deployed versions. The new service can then be integration-tested against the very same versions it will be deployed alongside. Doing this in a heterogeneous language environment would previously have required a lot of custom tooling, but it becomes reasonably simple to implement because of the standardization provided by Linux containers.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Packaging" data-type="sect2"><div class="sect2" id="idm46803171178016">&#13;
<h2>Packaging</h2>&#13;
&#13;
<p>Docker builds produce an image<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="packaging" data-type="indexterm" id="idm46803171176688"/><a data-primary="workflow with Docker" data-secondary="packaging" data-type="indexterm" id="idm46803171175152"/><a data-primary="packaging" data-secondary="Docker workflow" data-type="indexterm" id="idm46803171174208"/><a data-primary="building a Docker image" data-secondary="Docker workflow" data-tertiary="packaging" data-type="indexterm" id="idm46803171173264"/><a data-primary="OCI images" data-secondary="building an image" data-tertiary="packaging" data-type="indexterm" id="idm46803171172048"/> that can be treated as a single build artifact, although technically they may consist of multiple filesystem layers. No matter which language your application is written in or which distribution of Linux you run it on, you get a layered Docker image as the result of your build. And it is all built and handled by the Docker tooling. That build image is the shipping container metaphor that Docker is named for: a single, transportable unit that universal tooling can handle, regardless of what it contains. Like oceanic cargo ships that package everything into steel containers, your Docker tooling will only ever have to deal with one kind of package: the Docker image. That’s powerful, because it’s a huge facilitator of tool reuse between applications, and it means that someone else’s off-the-shelf container tools will work with your build images.</p>&#13;
&#13;
<p>Applications that traditionally took a lot of custom configuration to deploy onto a new host or development system become very portable with Docker. Once a container is built, it can easily be deployed on any system with a running Docker server on the same architecture.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploying" data-type="sect2"><div class="sect2" id="idm46803171170064">&#13;
<h2>Deploying</h2>&#13;
&#13;
<p>Deployments are handled by so<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="deploying" data-type="indexterm" id="idm46803171168512"/><a data-primary="workflow with Docker" data-secondary="deploying" data-type="indexterm" id="idm46803171166976"/><a data-primary="deployment" data-secondary="Docker workflow" data-type="indexterm" id="idm46803171166032"/> many kinds of tools in different shops that it would be impossible to list them here. Some of these tools include shell scripting, <a href="https://capistranorb.com">Capistrano</a>, <a href="https://www.fabfile.org">Fabric</a>, <a href="https://www.ansible.com">Ansible</a>, and in-house custom tooling. In our experience with multiteam organizations, there are usually one or two people on each team who know the magical incantation to get deployments to work. When something goes wrong, the team is dependent on them to get it running again. As you probably expect by now, Docker makes most of that a nonissue. The built-in tooling supports a simple, one-line deployment strategy to get a build onto a host and up and running. The standard Docker client handles deploying only to a single host at a time, but there is a large array of tools available that make it easy to deploy into a cluster of Docker or other compatible Linux container hosts. Because of the standardization Docker provides, your build can be deployed into any of these systems, with low complexity on the part of the development teams.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Docker Ecosystem" data-type="sect2"><div class="sect2" id="idm46803171162576">&#13;
<h2>The Docker Ecosystem</h2>&#13;
&#13;
<p>Over the years, a wide community<a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="workflow with Linux containers" data-tertiary="tools for additional capabilities" data-type="indexterm" id="ch02-addtool"/><a data-primary="workflow with Docker" data-secondary="tools for additional capabilities" data-type="indexterm" id="ch02-addtool2"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="about" data-tertiary="broad support for" data-type="indexterm" id="idm46803171157952"/><a data-primary="Linux containers" data-secondary="about" data-tertiary="broad support for" data-type="indexterm" id="idm46803171156464"/> has formed around Docker, driven by both developers and system administrators. Like the DevOps movement, this has facilitated better tools by applying code to operations problems. Where there are gaps in the tooling provided by Docker, other companies and individuals have stepped up to the plate. Many of these tools are also open source. That means they are expandable and can be modified by any other company to fit its needs.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Docker is a commercial company<a data-primary="Docker, Inc." data-primary-sortas="docker-a" data-type="indexterm" id="idm46803171154144"/> that has contributed much of the core Docker source code to the open source community. Companies are strongly encouraged to join the community and contribute back to the open source efforts. <a data-primary="website for Docker" data-type="indexterm" id="idm46803171152832"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="website" data-type="indexterm" id="idm46803171152160"/><a data-primary="resources online" data-secondary="Docker website" data-type="indexterm" id="idm46803171150944"/>If you are looking for supported versions of the core Docker tools, you can find out more about its offerings at the <a href="https://www.docker.com/support">Docker website</a>.</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Orchestration" data-type="sect3"><div class="sect3" id="idm46803171148768">&#13;
<h3>Orchestration</h3>&#13;
&#13;
<p>The first important category of tools<a data-primary="workflow with Docker" data-secondary="tools for additional capabilities" data-tertiary="orchestration" data-type="indexterm" id="idm46803171147184"/><a data-primary="deployment" data-secondary="orchestration tools" data-tertiary="third party" data-type="indexterm" id="idm46803171145920"/><a data-primary="orchestration tools" data-secondary="third party" data-type="indexterm" id="idm46803171144704"/><a data-primary="Centurion (New Relic)" data-type="indexterm" id="idm46803171143760"/><a data-primary="New Relic Centurion" data-type="indexterm" id="idm46803171143088"/><a data-primary="Helios (Spotify)" data-type="indexterm" id="idm46803171142416"/><a data-primary="Spotify Helios" data-type="indexterm" id="idm46803171141744"/><a data-primary="Ansible Docker tooling" data-type="indexterm" id="idm46803171141072"/> that add functionality to the core Docker distribution and Linux container experience contains orchestration and mass deployment tools. Early mass deployment tools like <a href="https://github.com/newrelic/centurion">New Relic’s Centurion</a>, <a href="https://github.com/spotify/helios">Spotify’s Helios</a>, and the <a href="https://oreil.ly/V8X_f">Ansible Docker tooling</a><sup><a data-type="noteref" href="ch02.html#idm46803169305904" id="idm46803169305904-marker">1</a></sup> still work largely like traditional deployment tools but leverage the container as the distribution artifact. They take a fairly simple, easy-to-implement approach. You get a lot of the benefits of Docker without much complexity, but many of these tools have been replaced by more robust and flexible tools, like Kubernetes.</p>&#13;
&#13;
<p>Fully automatic schedulers<a data-primary="Kubernetes" data-secondary="automatic scheduler" data-type="indexterm" id="idm46803169302720"/><a data-primary="Mesos Marathon scheduler" data-type="indexterm" id="idm46803169301808"/><a data-primary="Marathon scheduler" data-type="indexterm" id="idm46803169301072"/><a data-primary="Apache Mesos Marathon scheduler" data-type="indexterm" id="idm46803169300400"/><a data-primary="HashiCorp" data-secondary="Nomad" data-type="indexterm" id="idm46803169299712"/><a data-primary="Nomad (HashiCorp)" data-type="indexterm" id="idm46803169298768"/><a data-primary="Mesosphere DC/OS (Datacenter Operating System)" data-type="indexterm" id="idm46803169298096"/><a data-primary="DC/OS (Datacenter Operating System; Mesosphere)" data-type="indexterm" id="idm46803169297392"/>  like <a href="https://kubernetes.io">Kubernetes</a> or <a href="https://mesos.apache.org">Apache Mesos</a> with the <a href="https://mesosphere.github.io/marathon">Marathon scheduler</a> are more powerful options that take nearly complete control of a pool of hosts on your behalf. Other commercial entries are widely available, such as <a href="https://www.nomadproject.io">HashiCorp’s Nomad</a>, <a href="https://dcos.io">Mesosphere’s DC/OS (Datacenter Operating System)</a>, and <a href="https://rancher.com">Rancher</a>.<sup><a data-type="noteref" href="ch02.html#idm46803169292128" id="idm46803169292128-marker">2</a></sup> The ecosystems of both free and commercial options continue to grow rapidly.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Immutable atomic hosts" data-type="sect3"><div class="sect3" id="idm46803169291152">&#13;
<h3>Immutable atomic hosts</h3>&#13;
&#13;
<p>One additional idea that you can leverage<a data-primary="workflow with Docker" data-secondary="tools for additional capabilities" data-tertiary="immutable atomic hosts" data-type="indexterm" id="idm46803169289440"/><a data-primary="immutable infrastructure" data-secondary="immutable atomic hosts" data-type="indexterm" id="idm46803169288176"/><a data-primary="atomic hosts" data-secondary="immutable atomic hosts" data-type="indexterm" id="idm46803169287216"/><a data-primary="atomic hosts" data-secondary="Fedora CoreOS as" data-seealso="Fedora CoreOS (Red Hat)" data-type="indexterm" id="idm46803169286272"/><a data-primary="Fedora CoreOS (Red Hat)" data-secondary="immutable atomic hosts" data-type="indexterm" id="idm46803169285056"/> to enhance your Docker experience is immutable atomic hosts. Traditionally, servers and virtual machines are systems that an organization will carefully assemble, configure, and maintain to provide a wide variety of functionality that supports a broad range of usage patterns. Updates must often be applied via nonatomic operations, and there are many ways in which host configurations can diverge and introduce unexpected behavior into the system. Most running systems are patched and updated in place in today’s world. Conversely, in the world of software deployments, most people deploy an entire copy of their application, rather than trying to apply patches to a running system. Part of the appeal of containers is that they help make applications even more atomic than traditional deployment models.</p>&#13;
&#13;
<p>What if you could extend that core container pattern down into the operating system? Instead of relying on configuration management to try to update, patch, and coalesce changes to your OS components, what if you could simply pull down a new, thin OS image and reboot the server? And then if something breaks, easily roll back to the exact image you were previously using?</p>&#13;
&#13;
<p>This is one of the core ideas<a data-primary="Bottlerocket OS" data-type="indexterm" id="idm46803169282800"/> behind Linux-based atomic host distributions, like <a href="https://getfedora.org/en/coreos">Red Hat’s Fedora CoreOS</a>, <a href="https://github.com/bottlerocket-os/bottlerocket">Bottlerocket OS</a>, and others. Not only should you be able to easily tear down and redeploy your applications, but the same philosophy should apply for the whole software stack. This pattern helps provide very high levels of consistency and resilience to the whole stack.</p>&#13;
&#13;
<p>Some of the typical characteristics of an immutable or <a href="https://gist.github.com/jzb/0f336c6f23a0ba145b0a">atomic host</a> are a minimal footprint, a design focused on supporting Linux containers and Docker, and atomic OS updates and rollbacks that can easily be controlled via multihost orchestration tools on both bare-metal and common virtualization platforms.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#installing_docker">Chapter 3</a>, we will discuss how you can easily use these immutable hosts in your development process. If you are also using these hosts as deployment targets, this process creates a previously unheard-of amount of software stack symmetry between your development and production environments.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Additional tools" data-type="sect3"><div class="sect3" id="idm46803169277232">&#13;
<h3>Additional tools</h3>&#13;
&#13;
<p>Docker is not just a standalone solution. It has a massive feature set, but there is always a case where someone needs more than it can deliver on its own. There is a wide ecosystem of tools to either improve or augment Docker’s functionality. <a data-primary="monitoring" data-secondary="Prometheus for" data-type="indexterm" id="idm46803169275456"/><a data-primary="Prometheus for monitoring" data-type="indexterm" id="idm46803169274480"/><a data-primary="Ansible Docker tooling" data-type="indexterm" id="idm46803169273840"/>Some good production tools leverage the Docker API, like <a href="https://prometheus.io">Prometheus</a> for monitoring and <a href="https://www.ansible.com">Ansible</a> for simple orchestration. <a data-primary="architecture of Docker" data-secondary="plug-in architecture" data-type="indexterm" id="idm46803169271664"/><a data-primary="Docker" data-primary-sortas="docker-a" data-secondary="architecture" data-tertiary="plug-in architecture" data-type="indexterm" id="idm46803169270688"/><a data-primary="plug-ins for Docker" data-secondary="about" data-type="indexterm" id="idm46803169269200"/><a data-primary="APIs" data-secondary="Docker daemon" data-tertiary="plug-ins leveraging" data-type="indexterm" id="idm46803169268256"/>Others leverage Docker’s plug-in architecture. Plug-ins are executable programs that conform to a specification for receiving and returning data to Docker.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Many of the Docker plug-ins are considered legacy and are being replaced with better approaches. Make sure that you perform adequate research before deciding on a plug-in that you are going to utilize, to ensure that it is the best option and is not going to be unsupported or quickly replaced.</p>&#13;
</div>&#13;
&#13;
<p>There are many more good tools that either talk to the API or run as plug-ins. Many of these have sprung up to make life with Docker easier on the various cloud providers. These help with seamless integration between Docker and the cloud. As the community continues to innovate, the ecosystem continues to grow. There are new solutions and tools available in this space on an ongoing basis. If you find you are struggling with something in your environment, look to the ecosystem!<a data-startref="ch02-addtool" data-type="indexterm" id="idm46803169264592"/><a data-startref="ch02-addtool2" data-type="indexterm" id="idm46803169263888"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Wrap-Up" data-type="sect1"><div class="sect1" id="idm46803169262960">&#13;
<h1>Wrap-Up</h1>&#13;
&#13;
<p>There you have it: a quick tour through Docker. We’ll return to this discussion later on with a slightly deeper dive into the architecture of Docker, more examples of how to use the community tooling, and an exploration of some of the thinking behind designing robust container platforms. But you’re probably itching to try it all out, so in the next chapter, we’ll get Docker installed and running.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm46803169305904"><sup><a href="ch02.html#idm46803169305904-marker">1</a></sup> Full URL: <a class="bare" href="https://docs.ansible.com/ansible/latest/collections/community/docker/docsite/scenario_guide.html#ansible-collections-community-docker-docsite-scenario-guide"><em class="hyperlink">https://docs.ansible.com/ansible/latest/collections/community/docker/docsite/scenario_guide.html#ansible-collections-community-docker-docsite-scenario-guide</em></a></p><p data-type="footnote" id="idm46803169292128"><sup><a href="ch02.html#idm46803169292128-marker">2</a></sup> Some of these commercial offerings have free editions of their platforms.</p></div></div></section></body></html>