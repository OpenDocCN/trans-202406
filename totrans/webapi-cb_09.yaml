- en: Chapter 9\. The Web Speech API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the age of smart devices and assistants, your voice has become another commonly
    used input method. Whether you’re dictating a text message or asking for tomorrow’s
    weather forecast, speech recognition and synthesis are becoming useful tools in
    app development. With the Web Speech API, you can make your app speak or listen
    for a user’s voice input.
  prefs: []
  type: TYPE_NORMAL
- en: Speech Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Web Speech API brings speech *recognition* to the browser. Once the user
    gives you permission to use the microphone, it listens for speech. When it recognizes
    a series of words, it triggers an event with the recognized content.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Speech recognition may not be supported by all browsers yet. See [CanIUse](https://oreil.ly/SGLlc)
    for the latest compatibility data.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need the user’s permission before you can start listening for speech.
    Due to privacy settings, the first time you attempt to listen, the user is prompted
    to grant your app permission to use the microphone (see [Figure 9-1](#microphonePermission)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A microphone permission request in Chrome](assets/wacb_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A microphone permission request in Chrome
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some browsers, such as Chrome, use an external server for analyzing the captured
    audio to recognize speech. This means speech recognition won’t work when you’re
    offline, and it might also raise privacy concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Speech Synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Web Speech API also provides speech *synthesis*. Given some text, it can
    create a synthesized voice that speaks the text. The browser has a set of built-in
    voices that it can use to speak your content. Once you have selected a voice appropriate
    for the target language, you can customize the voice’s pitch and speaking rate.
  prefs: []
  type: TYPE_NORMAL
- en: You can combine speech recognition and synthesis to create conversational voice
    user interfaces. They can listen for a question or command and speak the output
    or feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Browser Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing, support for the Web Speech API is somewhat limited.
  prefs: []
  type: TYPE_NORMAL
- en: The specification for this API also adds a few other pieces that enhance speech
    recognition and synthesis once they are supported in browsers.
  prefs: []
  type: TYPE_NORMAL
- en: The first of these is custom grammar, which lets you fine-tune speech recognition
    by specifying words and phrases that you want to recognize. For example, if you
    were designing a calculator with voice commands, your custom grammar would include
    digits (“one,” “two,” etc.) and calculator operations (“plus,” “minus,” etc.).
    Using a custom grammar helps guide the speech recognition engine to capture the
    words your application is looking for.
  prefs: []
  type: TYPE_NORMAL
- en: The SpeechSynthesis API supports Speech Synthesis Markup Language (SSML). SSML
    is an XML language that customizes speech synthesis. You can switch between male
    and female voices or specify that the browser should read something letter by
    letter. At the time of writing, SSML markup is parsed and understood—the engine
    won’t speak the markup tags—but browsers currently ignore most instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Dictation to a Text Field
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to recognize spoken text and add it to a text field’s content, allowing
    the user to dictate the text field’s contents.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the `SpeechRecognition` interface to listen for speech. When speech is recognized,
    extract the recognized text and append it to the text field (see [Example 9-1](#example9-1)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. Adding basic dictation to a text field
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing, in the WebKit browsers that support it, the `SpeechRecognition`
    constructor is prefixed as `webkitSpeechRecognition`. In unsupported browsers,
    neither `SpeechRecognition` nor `webkitSpeechRecognition` are defined, so it’s
    important to check the browser support before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: To future-proof the code, the example checks for either the prefixed version
    (`webkitSpeechRecognition`) as well as the standard `SpeechRecognition` version.
    This way, you won’t have to change the code to accommodate browsers that implement
    the API in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the `startDictation` function creates a `SpeechRecognition` object and
    sets its `continuous` flag to `true`. By default, no further recognition is performed
    once a result is recognized. Setting the `continuous` flag tells the speech recognition
    engine to continue listening and to deliver additional results.
  prefs: []
  type: TYPE_NORMAL
- en: When the recognition engine recognizes some speech, it triggers a `result` event.
    This event has a `results` property that is an array-like object (actually a `SpeechRecognitionResultList`
    object) containing the results.
  prefs: []
  type: TYPE_NORMAL
- en: When operating in continuous mode, as this example does, the `results` list
    contains all results that the recognition engine recognized. The first time the
    user speaks and some speech is recognized, this has a single result. When the
    user speaks again and the browser recognizes some more words, there are *two*
    results—the original result and the new one that was just recognized. If you set
    `continuous` to `false` (the default), the engine only recognizes one phrase,
    then no further `result` events are triggered.
  prefs: []
  type: TYPE_NORMAL
- en: Helpfully, the event also has a `resultIndex` property that points to the index
    within the list of the new result that triggered this event.
  prefs: []
  type: TYPE_NORMAL
- en: The result object is another array-like object (a `SpeechRecognitionAlternative`
    object). When you create a `SpeechRecognition` object, you can give it the property
    `maxAlternatives`. The browser presents a list of possible matches for the recognized
    speech, each with a confidence value. However, the default `maxAlternatives` value
    is 1, so this dictation code only ever has one `SpeechRecognitionAlternative`
    object in the list.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this object has a `transcript` property that is the actual phrase that
    the engine recognized. You can take this value and append it to the text field’s
    current value.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `start` on the recognition object begins listening for speech, emitting
    events when it hears something. The `startDictation` function then returns the
    recognition object, so that you can stop recognition once the user is finished
    dictating.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like with any API, it’s also important to handle any errors that occur. With
    speech recognition, some common errors you might face are:'
  prefs: []
  type: TYPE_NORMAL
- en: Permission error
  prefs: []
  type: TYPE_NORMAL
- en: If the user did not grant permission to use the microphone. The event has an
    `error` property of `not-allowed`.
  prefs: []
  type: TYPE_NORMAL
- en: Network error
  prefs: []
  type: TYPE_NORMAL
- en: If the browser couldn’t reach the speech recognition service. This has an `error`
    of `network`.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware error
  prefs: []
  type: TYPE_NORMAL
- en: If the browser was unable to access the microphone. This has an error code of
    `audio-capture`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Promise Helper for Speech Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to encapsulate speech recognition into a single function call.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wrap the speech recognition call in a new `Promise` inside a helper function.
    Within the helper function, create a new `SpeechRecognition` object and listen
    for speech. You can resolve the `Promise` when the browser recognizes some speech
    (see [Example 9-2](#code_promiseHelper)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. A `Promise` helper for speech recognition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `captureSpeech` helper does not use `continuous` mode. This means you can
    only use it to listen for a single speech recognition event. If you want to capture
    additional speech after the returned `Promise` resolves, you need to call `captureSpeech`
    again and wait on the new `Promise`.
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that [Example 9-2](#code_promiseHelper) doesn’t return the
    `Promise` directly. Instead, it calls `finally` on that `Promise` to stop the
    speech recognition engine regardless of the outcome. The `captureSpeech` function
    lets you quickly recognize speech by just waiting on a `Promise` (see [Example 9-3](#example9-3)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. Using the `captureSpeech` helper
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Getting the Available Voices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to determine what speech synthesis voices are available in the current
    browser.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Query the voice list by calling `speechSynthesis.getVoices`, and then listen
    for the `voiceschanged` event if necessary, as shown in [Example 9-4](#code_getVoices).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-4\. Getting the list of available speech synthesis voices
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some browsers, such as Chrome, load the list of voices asynchronously. If you
    call `getVoices` before the list is ready, you’ll get an empty array back. The
    `speech` `Synthesis` object triggers a `voiceschanged` event when the list is
    ready.
  prefs: []
  type: TYPE_NORMAL
- en: Other browsers, including Firefox, have the voice list available right away.
    In these browsers, the `voiceschanged` event never fires. The code in [Example 9-4](#code_getVoices)
    handles both cases.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each voice has a `lang` property that specifies the voice’s language. When speaking
    text, the voice uses the pronunciation rules for its language. Make sure you use
    a voice with the correct language for the text you’re synthesizing. Otherwise,
    the pronunciation won’t sound right.
  prefs: []
  type: TYPE_NORMAL
- en: Synthesizing Speech
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want your app to speak some text to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a `SpeechSynthesisUtterance` and pass it to the `speechSynthesis.speak`
    method (see [Example 9-5](#example9-5)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-5\. Speaking some text with the Web Speech API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An utterance is the set of words you want the browser to speak. It’s created
    with a `SpeechSynthesisUtterance` object.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Browsers will only permit speech synthesis once the user has interacted with
    the page in some way. This is to stop a page from speaking immediately upon loading.
    As such, the `speakText` helper function will not speak anything until there is
    some user activity on the page.
  prefs: []
  type: TYPE_NORMAL
- en: This speaks the text with the default voice. If you want to use a different
    supported system voice, you can use the technique from [“Getting the Available
    Voices”](#recipe_getVoices) to get the array of available voices. You can set
    the utterance’s `voice` property to one of the voice objects from this array,
    as shown in [Example 9-6](#example9-6).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-6\. Using another voice
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Customizing Speech Synthesis Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to speed up, slow down, or adjust the pitch of the spoken text.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating a `SpeechSynthesisUtterance`, use the `rate` and `pitch` properties
    to customize the speaking voice (see [Example 9-7](#example9-7)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-7\. Customizing speech output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pitch` option is a float number that can have a value between 0 and 2\.
    Lower values result in a lower pitch, and higher values result in a higher pitch.
    Lowering the pitch does not affect the speaking rate. Depending on the browser
    or voice being used, the range of supported pitch values may be limited.
  prefs: []
  type: TYPE_NORMAL
- en: To speed up or slow down the speaking rate, you can adjust the `rate` property.
    Each voice has a default speaking rate, which is represented by a `rate` of 1\.
    The value of `rate` has a multiplier effect. If you set `rate` to 0.5, it is half
    of the default speaking rate. Similarly, if you set `rate` to 1.5, it is 50% faster
    than the default rate. The specification defines the valid range as 0.1 to 10,
    but browsers and voices typically limit this to a smaller range.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically Pausing Speech
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When your app is speaking, you want to pause speech when you switch to another
    tab so that it doesn’t interfere with the usage of the other tab. You also want
    to stop speaking when leaving the page.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Listen for the `visibilitychange` event and check the `document.visibilityState`
    property. When the page becomes hidden, pause speech synthesis. When it becomes
    visible again, resume speaking (see [Example 9-8](#code_pauseSpeech)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-8\. Pausing speech when the page becomes hidden
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, if you switch to another tab while the Web Speech API is speaking
    some text, it continues speaking. This might be what you expect—after all, the
    same thing happens if you are streaming audio or video then change to another
    tab; you’ll continue to hear the audio from the other tab.
  prefs: []
  type: TYPE_NORMAL
- en: When you switch tabs, the `visibilitychange` event fires. The event itself doesn’t
    have any information about the visibility state, but you can get that by checking
    the `document.visibilityState` property. [Example 9-8](#code_pauseSpeech) pauses
    the speech when you switch to another tab. When you switch back, it continues
    where it left off.
  prefs: []
  type: TYPE_NORMAL
- en: Some browsers keep playing the speech even when you navigate away from the page
    or perform a full page refresh. Leaving or refreshing the page also triggers the
    `visibilitychange` event, so the code in [Example 9-8](#code_pauseSpeech) correctly
    stops the speech in these cases as well.
  prefs: []
  type: TYPE_NORMAL
