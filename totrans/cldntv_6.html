<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Best Practices"><div class="chapter" id="best_practices">
<h1><span class="label">Chapter 6. </span>Best Practices</h1>


<p>Throughout this book, you have learned about the fundamentals of cloud native applications—how to design, develop, and operate them as well as how to deal with data.<a data-type="indexterm" data-primary="best practices" id="ix_bstpr"/> To conclude, this chapter aims to provide a laundry list covering tips, proven techniques, and proven best practices to build and manage reactive cloud native applications.</p>






<section data-type="sect1" data-pdf-bookmark="Moving to Cloud Native"><div class="sect1" id="moving_to_cloud_native">
<h1>Moving to Cloud Native</h1>

<p>In <a data-type="xref" href="ch02.xhtml#fundamentals">Chapter 2</a>, you learned about the process that many customers follow when moving traditional applications to the cloud.<a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" id="ix_bstprmv"/><a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" id="ix_clnamv"/> There are many best practices and lessons learned that you should consider when moving an existing application into the cloud.</p>








<section data-type="sect2" data-pdf-bookmark="Breaking Up the Monolith for the Right Reasons"><div class="sect2" id="breaking_up_the_monolith_for_the_right">
<h2>Breaking Up the Monolith for the Right Reasons</h2>

<p>“Never change a running system” is a widely used statement in software development, and it is also applicable when you consider moving your application to the cloud.<a data-type="indexterm" data-primary="monolithic applications" data-secondary="breaking up for right reasons" id="idm45987078104984"/><a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="breaking up monolithic applications" id="idm45987078102040"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="breaking up monolithic applications for right reasons" id="idm45987078103688"/> If your sole requirement is to move your application to the cloud, you can always consider moving it on Infrastructure as a Service (IaaS)—in fact, that should be your very first step.<a data-type="indexterm" data-primary="Infrastructure as a Service (IaaS)" data-secondary="moving applications on" id="idm45987078099144"/> That said, there are benefits of redesigning your application to be cloud native, but you need to weigh the pros and cons.<a data-type="indexterm" data-primary="cloud" data-secondary="moving applications into" id="idm45987078098520"/> Following are some guidelines indicating that a redesign makes sense:</p>

<ul>
<li>
<p>Your codebase has grown to a point that it takes very long to release an updated version and thus you cannot react to new market or customer requirements quickly.<a data-type="indexterm" data-primary="releases" data-secondary="difficulties posed by large codebase" id="idm45987078099864"/></p>
</li>
<li>
<p>Components of your applications have different scale requirements.<a data-type="indexterm" data-primary="scaling" data-secondary="application components having different scale requirements" id="idm45987078093416"/> A good example is a traditional three-tier application consisting of a frontend, business, and data tier. Only the frontend tier might experience heavy load in user requests, whereas the business and data tier are still comfortably handling the load. As mentioned in <a data-type="xref" href="ch02.xhtml#fundamentals">Chapter 2</a> and <a data-type="xref" href="ch03.xhtml#designing_cloud-native_applications">Chapter 3</a>, cloud native applications allow you to scale services independently.</p>
</li>
<li>
<p>Better technology choices have emerged. There is constant innovation in the technology sector, and some new technologies might be better suited for parts of your application.</p>
</li>
</ul>

<p>After you have decided that you want to redesign your application, you need to consider many things. In the following sections, we provide a comprehensive look at these considerations.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Decouple Simple Services First"><div class="sect2" id="decouple_simple_services_first">
<h2>Decouple Simple Services First</h2>

<p>Start by breaking off components that provide simpler functionality because they usually do not have a lot of dependencies and, thus, are not deeply integrated within the monolith.<a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="decoupling simple services first" id="idm45987078086440"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="decoupling simple services first" id="idm45987078085112"/><a data-type="indexterm" data-primary="services" data-secondary="decoupling simple services from monolithic code base" id="idm45987078089640"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Learn to Operate on a Small Scale"><div class="sect2" id="learn_to_operate_on_a_small_scale">
<h2>Learn to Operate on a Small Scale</h2>

<p>Use the first service as a learning path for how to operate in a cloud native world. <a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="learning to operate on small scale" id="idm45987078081032"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="learning to operate on small scale" id="idm45987078079944"/>Starting with a simple service, you can focus on setting up automation to provision the infrastructure and the CI/CD pipeline so that you become familiar with the process of developing, deploying, and operating a cloud native service. Having a simple service and minimal infrastructure will allow you to learn, exercise, and improve your new process ahead of time, without substantial impact on the monolith and your end users.<a data-type="indexterm" data-primary="CI/CD (continuous integration/continuous delivery)" id="idm45987078081832"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use an Anticorruption Layer Pattern"><div class="sect2" id="use_an_anticorruption_layer_pattern">
<h2>Use an Anticorruption Layer Pattern</h2>

<p>Nothing is perfect, especially in the software development world, so you will eventually end up with a new service that makes calls back to the monolith.<a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="using anticorruption layer pattern" id="idm45987078078808"/><a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="using anticorruption layer pattern" id="idm45987078073288"/><a data-type="indexterm" data-primary="anticorruption layer pattern" id="idm45987078076248"/> In this case, you might want to use the <em>Anticorruption Layer</em> pattern. This pattern is used to implement a facade or adapter between components that don’t share the same semantics. The purpose of the anticorruption layer is to translate the request from one component to another; for example, implementing protocol or schema translations.<a data-type="indexterm" data-primary="schemas, implementing translations of" id="idm45987078075528"/><a data-type="indexterm" data-primary="protocols, translations of" id="idm45987078074632"/></p>

<p>To implement this, you design and create a new API in the monolith that makes calls through the anticorruption layer in the new service, as shown in <a data-type="xref" href="#anticorruption_layer_pattern">Figure 6-1</a>.</p>

<figure><div id="anticorruption_layer_pattern" class="figure">
<img src="Images/clna_0601.png" alt="clna 0601" width="1411" height="702"/>
<h6><span class="label">Figure 6-1. </span><em>Anticorruption Layer</em> pattern</h6>
</div></figure>

<p>There are a couple of considerations when you are using this approach. As <a data-type="xref" href="#anticorruption_layer_pattern">Figure 6-1</a> illustrates, the anticorruption layer is a service on its own, so you need to think about how to scale and operate the layer. Also, you need to think about whether you want to retire the anticorruption layer after the monolithic application has been fully moved into a cloud native application.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use a Strangler Pattern"><div class="sect2" id="use_a_strangler_pattern">
<h2>Use a Strangler Pattern</h2>

<p>When you are decomposing your monolith to move to microservices and functions, you can use a gateway and a pattern such as a <em>Strangler</em> pattern.<a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="using strangler pattern" id="idm45987078060664"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="using strangler pattern" id="idm45987078062760"/><a data-type="indexterm" data-primary="strangler pattern" id="idm45987078061672"/> The idea behind the Strangler pattern is to use the gateway as a facade while you gradually move the backend monolith to a new architecture—either services, functions, or a combination of both. As you’re making progress breaking up the monolith and implementing those pieces of functionality as services or functions, you update the gateway to redirect requests to the new functionality, instead as shown in <a data-type="xref" href="#migrating_from_monolith_using_the_strang">Figure 6-2</a>.</p>

<figure><div id="migrating_from_monolith_using_the_strang" class="figure">
<img src="Images/clna_0602.png" alt="clna 0602" width="1374" height="607"/>
<h6><span class="label">Figure 6-2. </span>Migrating from monolith using the <em>Strangler</em> pattern</h6>
</div></figure>

<p>Note that the Strangler pattern might not be suitable for the instance in which you can’t intercept the requests going to the backing monolith. The pattern also might not make sense if you have a smaller system, for which it’s easier and faster to replace the entire system, instead of gradually moving it.</p>

<p>The Anticorruption Layer and Strangler patterns have been proven many times as good approaches to move a monolithic legacy application to a cloud native application because both promote a gradual approach.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Come Up with a Data Migration Strategy"><div class="sect2" id="come_up_with_a_data_migration_strategy">
<h2>Come Up with a Data Migration Strategy</h2>

<p>In a monolith, you are usually working with a centrally shared datastore where data is read from and written to by multiple places and services. <a data-type="indexterm" data-primary="data, working with" data-secondary="migration of data to cloud native" id="idm45987078051832"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="developing data migration strategy" id="idm45987078050792"/><a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="developing data migration strategy" id="idm45987078057400"/>To truly move to the cloud native architecture, you need to decouple data as well. Your data migration strategy might consist of multiple phases, especially if you can’t migrate everything at the same time. However, in most cases, you will need to do an incremental migration while keeping the entire system running. A gradual migration will probably involve writing data twice (to the new and old datastore) for a while. After you have data in both places and synchronized, you will need to modify where the data is being read from and then read everything from the new store. Finally, you should be able to stop writing data to the old store completely.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Rewrite Any Boilerplate Code"><div class="sect2" id="rewrite_any_boilerplate_code">
<h2>Rewrite Any Boilerplate Code</h2>

<p>Monoliths will usually have large amounts of code that deals with the configuration, data caching, datastore access, and so on and is probably using older libraries and frameworks.<a data-type="indexterm" data-primary="boilerplate code, rewriting for cloud native" id="idm45987078047112"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="rewriting boilerplate code" id="idm45987078046728"/><a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="rewriting boilerplate code" id="idm45987078045576"/> When moving capabilities to a new service, you should rewrite this code. The best option is to throw away the old code and rewrite it from scratch instead of modifying the existing code and molding it so it fits the new service.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Reconsider Frameworks, Languages, Data Structures, and Datastores"><div class="sect2" id="reconsider_frameworks_comma_languages_co">
<h2>Reconsider Frameworks, Languages, Data Structures, and Datastores</h2>

<p>Moving to microservices gives you an option to rethink the existing implementation.<a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="reconsidering frameworks, languages, data structures, and datastores" id="idm45987078041848"/><a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="reconsidering frameworks, languages, data structures, and datastores" id="idm45987078040728"/> Are there new frameworks or languages that you could use to rewrite the current code that provide better features and functionalities for your scenarios? If it makes sense to rewrite the code, do it! Also, reconsider any data structures in the current code. Would they still make sense when moved to a service? You should also evaluate whether you want to use different datastores. <a data-type="xref" href="ch04.xhtml#working_with_data">Chapter 4</a> outlines what datastores are best suited for certain data structures and query patterns.<a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-startref="ix_clnamv" id="idm45987078037144"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-startref="ix_bstprmv" id="idm45987078034136"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Retire Code"><div class="sect2" id="retire_code">
<h2>Retire Code</h2>

<p>After you’ve created a new service and all the traffic is redirected to that service, you need to retire and remove the old code that resides in the monolith.<a data-type="indexterm" data-primary="code" data-secondary="retiring for monolithic applications" id="idm45987078031096"/><a data-type="indexterm" data-primary="cloud native applications" data-secondary="moving to" data-tertiary="retiring old code" id="idm45987078032280"/><a data-type="indexterm" data-primary="best practices" data-secondary="moving to cloud native" data-tertiary="retiring old code" id="idm45987078029672"/> Using this approach, you are shrinking the monolith and expanding your services.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Ensuring Resiliency"><div class="sect1" id="ensuring_resiliency">
<h1>Ensuring Resiliency</h1>

<p>Resiliency is the ability of a system to recover from failures and continue to function and serve requests.<a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" id="ix_bstprres"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" id="ix_resi"/> Resiliency is not about avoiding failures; instead, it is all about responding to failures in such a manner that avoids significant downtime or data loss.</p>








<section data-type="sect2" data-pdf-bookmark="Handle Transient Failures with Retries"><div class="sect2" id="handle_transient_failures_with_retries">
<h2>Handle Transient Failures with Retries</h2>

<p>Requests can fail due to multiple reasons such as network latency, dropped connections, or timeouts if downstream services are busy.<a data-type="indexterm" data-primary="retries" data-secondary="handling transient failures with" id="idm45987078022120"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="handling transient failures with retries" id="idm45987078020536"/><a data-type="indexterm" data-primary="failures" data-secondary="transient, handling with retries" id="idm45987078019432"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="handling transient failures with retries" id="idm45987078018232"/> You can avoid most of these failures if you retry the request. Retrying can also improve the stability of your application. However, before blindly retrying all requests, you need to implement a bit of logic that determines whether the request should be retried. If the failure is not transient or there is a likelihood that a retry won’t be successful, it is better for the component to cancel the request and respond with an appropriate error message. For example, retrying a failed login because of an incorrect password is futile and retries won’t help. If failure is due to a rare network issue, you can retry the request right away given that the same issue probably won’t persist. Finally, if the failure happens because the downstream service is busy or you are being rate limited, for example, you should retry after a delay. Here are some common strategies for delaying between retry operations:</p>
<dl>
<dt>Constant</dt>
<dd>
<p>Wait for the same time between each attempt.</p>
</dd>
<dt>Linear</dt>
<dd>
<p>Incrementally increase the time between each retry. For example, you can start with one second, then three seconds, five seconds, and so on.</p>
</dd>
<dt>Exponential back-off</dt>
<dd>
<p>Exponentially increase time between each retry. For example, start with 3 seconds, 12 seconds, 30 seconds, and so on.</p>
</dd>
</dl>

<p>Depending on what type of failure you are dealing with, you can also immediately retry the operation once and then use one of the delay strategies mentioned in the preceding list. You can handle retries in the component’s source code by using the retry and transient failure logic provided by many of the service SDKs, or at the infrastructure layer if you are using a service mesh, such as Istio.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use a Finite Number of Retries"><div class="sect2" id="use_finite_number_of_retries">
<h2>Use a Finite Number of Retries</h2>

<p>Regardless of which retry strategy you’re using, always make sure to use a finite number of retries.<a data-type="indexterm" data-primary="retries" data-secondary="using finite number of" id="idm45987078005176"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="using finite number of retries" id="idm45987078013112"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="using finite number of retries" id="idm45987078012472"/> Having an infinite number of retries will cause an unnecessary strain on the <span class="keep-together">system.</span></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Circuit Breakers for Nontransient Failures"><div class="sect2" id="use_circuit_breakers_for_nontransient_fa">
<h2>Use Circuit Breakers for Nontransient Failures</h2>

<p>The purpose of a circuit breaker is to prevent components from doing operations that will likely fail and are not transient. <a data-type="indexterm" data-primary="failures" data-secondary="nontransient, handling with circuit breakers" id="idm45987078000136"/><a data-type="indexterm" data-primary="circuit breakers" data-secondary="using for nontransient failures" id="idm45987078008808"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="using circuit breakers for nontransient failures" id="idm45987078007896"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="using circuit breakers for nontransient failures" id="idm45987077999000"/>Circuit breakers monitor the number of faults, and based on that information decide whether the request should continue or an error should be returned without even invoking the downstream service. If a circuit breaker trips, the number of failures has exceeded a predefined value, and the circuit breaker will automatically return errors for a preset time. After the preset time elapses, it will reset the failure count and allow requests to go through to the downstream service again. A well-known library that implements the circuit breaker pattern is Hystrix from Netflix. If you are using a service mesh like Istio or Envoy proxies, you can take advantage of the circuit breaker implementation in those solutions.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Graceful Degradation"><div class="sect2" id="graceful_degradation">
<h2>Graceful Degradation</h2>

<p>Services should degrade gracefully, so even if they fail, they still provide an acceptable user experience if it makes sense.<a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="graceful degradation" id="idm45987077997656"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="graceful degradation" id="idm45987077995048"/><a data-type="indexterm" data-primary="degradation, graceful" id="idm45987077992840"/><a data-type="indexterm" data-primary="graceful degradation" id="idm45987077992136"/> For example, if you can’t retrieve the data, you could display a cached version of the data, and as soon as the data source recovers, you show the latest data.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use a Bulkhead Pattern"><div class="sect2" id="use_a_bulkhead_pattern">
<h2>Use a Bulkhead Pattern</h2>

<p>The <em>Bulkhead</em> pattern refers to isolating different parts of your system into groups in such a way that if one fails, the others will continue running unaffected.<a data-type="indexterm" data-primary="bulkhead pattern" id="idm45987077988968"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="using bulkhead pattern" id="idm45987077988472"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="using bulkhead pattern" id="idm45987077985048"/> Grouping your services this way allows you isolate failures and continue serving requests even when there’s a failure.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Implement Health Checks and Readiness Checks"><div class="sect2" id="implement_health_checks_and_readiness_ch">
<h2>Implement Health Checks and Readiness Checks</h2>

<p>Implement a health check and a readiness check for every service you deploy. The platform can use these to determine whether the service is healthy and performing correctly as well as when the service is ready to start accepting requests.<a data-type="indexterm" data-primary="health checks" data-secondary="implementing" id="idm45987077983368"/><a data-type="indexterm" data-primary="readiness checks" data-secondary="implementing" id="idm45987077981592"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="implementing health checks and readiness checks" id="idm45987077980680"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="implementing health checks and readiness checks" id="idm45987077979464"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="probes" id="idm45987077977992"/> In Kubernetes, health checks are called <em>probes</em>. The liveness probe is used to determine when a container should be restarted, whereas the readiness probe determines whether a pod should start receiving traffic.</p>

<p>The initial delay defines the number of seconds after the container has started before liveness or readiness probes are active, whereas the period defines how often the probe is performed. There are also additional settings such as success/failure threshold and timeouts that you can use to fine-tune the probes.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Define CPU and Memory Limits for Your Containers"><div class="sect2" id="define_cpu_and_memory_limits_for_your_co">
<h2>Define CPU and Memory Limits for Your Containers</h2>

<p>You should define CPU and memory limits to isolate resources and prevent certain services instances from consuming too many resources.<a data-type="indexterm" data-primary="containers" data-secondary="defining CPU and memory limits for" id="idm45987077974808"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="defining CPU and memory limits for containers" id="idm45987077971192"/><a data-type="indexterm" data-primary="resources" data-secondary="limiting consumption of CPU and memory" id="idm45987077970072"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="defining CPU and memory limits for containers" id="idm45987077972392"/> In Kubernetes, you can achieve this by defining the memory and CPU limits within the pod definition.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Implement Rate Limiting and Throttling"><div class="sect2" id="implement_rate_limiting_and_throttling">
<h2>Implement Rate Limiting and Throttling</h2>

<p>You use rate limiting and throttling to limit the number of incoming or outgoing requests for a service.<a data-type="indexterm" data-primary="rate limiting" id="idm45987077967384"/><a data-type="indexterm" data-primary="throttling" id="idm45987077969944"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-tertiary="implementing rate limiting and throttling" id="idm45987077966248"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-tertiary="implementing rate limiting and throttling" id="idm45987077965608"/><a data-type="indexterm" data-primary="request/response" data-secondary="rate limiting and throttling for requests" id="idm45987077963176"/> Implementing those can help you to keep your service responsive even in the case of a sudden spike in requests. Throttling, on the other hand, is often used for outgoing requests. Think about using it when you want to control the number of requests sent to an external service to minimize the costs or to make sure that your service does not look like the origin of a Denial-of-Service attack.<a data-type="indexterm" data-primary="best practices" data-secondary="ensuring resiliency" data-startref="ix_bstprres" id="idm45987077956664"/><a data-type="indexterm" data-primary="resiliency" data-secondary="ensuring" data-startref="ix_resi" id="idm45987077955736"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Ensuring Security"><div class="sect1" id="ensuring_security">
<h1>Ensuring Security</h1>

<p>Security in the cloud native world is based on the shared responsibility model. The cloud <a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" id="ix_bstprsec"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" id="ix_secens"/>providers are not solely responsible for the security of their customers’ solutions; instead, they share that responsibility with the customers. From an application perspective you should consider adopting the defense-in-depth concept, which is discussed in <a data-type="xref" href="ch03.xhtml#designing_cloud-native_applications">Chapter 3</a>. The best practices listed in this section will help you to ensure security.</p>








<section data-type="sect2" data-pdf-bookmark="Treat Security Requirements the Same as Any Other Requirements"><div class="sect2" id="treat_security_requirements_the_same_as">
<h2>Treat Security Requirements the Same as Any Other Requirements</h2>

<p>Having fully automated processes<a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="treating security requirements like any other requirement" id="idm45987077950776"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="treating security requirements like any other requirement" id="idm45987077949416"/> is in spirit of the cloud native development. To achieve this, all security requirements must be treated as any other requirement and be pushed through your development pipeline.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Incorporate Security in Your Designs"><div class="sect2" id="incorporate_security_in_your_designs">
<h2>Incorporate Security in Your Designs</h2>

<p>As you’re planning and designing your cloud native solutions, you need to think about security and incorporate the security features in your design.<a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="incorporating security into designs" id="idm45987077947064"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="incorporating security into designs" id="idm45987077945240"/> As part of your design, you also should call out any additional security concerns that need to be addressed during component development.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Grant Least-Privileged Access"><div class="sect2" id="grant_least-privileged_access">
<h2>Grant Least-Privileged Access</h2>

<p>If your services or functions need access to any resources, they should be granted specific<a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="granting least-privileged access" id="idm45987077946232"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="granting least-privileged access" id="idm45987077939464"/><a data-type="indexterm" data-primary="access control" data-secondary="granting least-privileged access" id="idm45987077938824"/> permissions that have the least amount of access set to them. For example, if your service is reading only from the database, it does not need to use an account that has write permissions.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Separate Accounts/Subscriptions/Tenants"><div class="sect2" id="use_separate_accounts_solidus_subscripti">
<h2>Use Separate Accounts/Subscriptions/Tenants</h2>

<p>Depending on the terminology of your cloud provider, your cloud native system should use separate accounts, subscriptions, and/or tenants.<a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="using separate accounts, subscriptions, and tenants" id="idm45987077940664"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="using separate accounts, subscriptions, and tenants" id="idm45987077934072"/><a data-type="indexterm" data-primary="tenants, separate" id="idm45987077933432"/><a data-type="indexterm" data-primary="accounts, separate" id="idm45987077932904"/><a data-type="indexterm" data-primary="publish/subscribe (pub/sub)" data-secondary="using separate subscriptions" id="idm45987077932104"/> At the very least, you will need a separate account for every environment you will be using; that way, you can ensure proper isolation between environments.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Securely Store All Secrets"><div class="sect2" id="securely_store_all_secrets">
<h2>Securely Store All Secrets</h2>

<p>Any secrets within your system, used either by your components or Continuous Integration/Continuous Development (CI/CD) pipeline, need to be encrypted and securely stored.<a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="securely storing secrets" id="idm45987077932776"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="securely storing secrets" id="idm45987077928056"/><a data-type="indexterm" data-primary="secrets" data-secondary="storing securely" id="idm45987077926408"/> It might sound like a no-brainer, but never store any secrets in plain text: always encrypt them. It’s always best to use existing and proven secret management systems that take care of these things for you.<a data-type="indexterm" data-primary="Kubernetes" data-secondary="Secrets" id="idm45987077922648"/> The simplest option is to use Kubernetes Secrets to store the secrets used by services within the cluster. Secrets are stored in etcd, a distributed key/value store. However, managed and centralized solutions have multiple advantages over Kubernetes secrets: everything is stored in a centralized location, you can define access control policies, secrets are encrypted, auditing support is provided, and more. Some examples of managed solutions are Microsoft Azure Key Vault, Amazon Secrets Manager, and HashiCorp Vault.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Obfuscate Data"><div class="sect2" id="obfuscate_data">
<h2>Obfuscate Data</h2>

<p>Any data your component uses needs to be properly obfuscated. For example, you never want to log any data classified as Personally Identifiable Information (PII) in plain text; if you need to log or store it, ensure that it’s either obfuscated (if logging it) or encrypted (if storing it).<a data-type="indexterm" data-primary="data" data-secondary="obfuscating" id="idm45987077923352"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="obfuscating data" id="idm45987077917336"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="obfuscating data" id="idm45987077915704"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Encrypt Data in Transit"><div class="sect2" id="encrypt_data_in_transit">
<h2>Encrypt Data in Transit</h2>

<p>Encrypting data in transit protects your data if communications are intercepted while the data moves between components.<a data-type="indexterm" data-primary="encrypting data in transit" id="idm45987077917832"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="encrypting data in transit" id="idm45987077918296"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="encrypting data in transit" id="idm45987077918840"/><a data-type="indexterm" data-primary="data" data-secondary="encrypting in transit" id="idm45987077912328"/> To achieve this protection, you need to encrypt the data before transmitting it, authenticate the endpoints, and finally decrypt and verify the data after it reaches the endpoint. Transport Layer Security (TLS) is used to encrypt data in transit for transport security. If you are using a service mesh, TLS might already be implemented between the proxies in the mesh.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Federated Identity Management"><div class="sect2" id="use_federated_identity_management">
<h2>Use Federated Identity Management</h2>

<p>Using an existing federated identity management service (Auth0, for example) to handle how users sign up, sign in, and sign out allows you to redirect users to a third-party page for authentication.<a data-type="indexterm" data-primary="federated identity management" id="idm45987077909608"/><a data-type="indexterm" data-primary="identity" data-secondary="using federated identity management" id="idm45987077905480"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="using federated identity management" id="idm45987077907000"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="using federated identity management" id="idm45987077908952"/><a data-type="indexterm" data-primary="authentication" id="idm45987077907720"/><a data-type="indexterm" data-primary="authorization" id="idm45987077904088"/> Your component should delegate authentication and authorization whenever possible.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Role-Based Access Control"><div class="sect2" id="use_role-based_access_control">
<h2>Use Role-Based Access Control</h2>

<p>Role-Based Access Control (RBAC) has been around for a long time.<a data-type="indexterm" data-primary="access control" data-secondary="role-based" id="idm45987077899816"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="using role-based access control" id="idm45987077898968"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="using role-based access control" id="idm45987077897880"/><a data-type="indexterm" data-primary="role-based access control (RBAC)" id="idm45987077896648"/> RBAC is a control access mechanism around roles and privileges, and as you have learned, it can be a great asset to your defense-in-depth strategy because it allows you to provide fine-grained access to users to only the resources they need.<a data-type="indexterm" data-primary="Kubernetes" data-secondary="role-based access control" id="idm45987077901272"/> Kubernetes RBAC, for example, controls permissions to the Kubernetes API. Using RBAC, you can allow or deny specific users from creating deployments or listing pods, and more. It’s a good practice to scope Kubernetes RBAC permissions by namespaces rather than cluster roles.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Isolate Kubernetes Pods"><div class="sect2" id="isolate_kubernetes_pods">
<h2>Isolate Kubernetes Pods</h2>

<p>Any pods running in a Kubernetes cluster are not isolated and can accept requests from any source.<a data-type="indexterm" data-primary="Kubernetes" data-secondary="isolating pods" id="idm45987077894024"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-tertiary="isolating Kubernetes pods" id="idm45987077893512"/><a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-tertiary="isolating Kubernetes pods" id="idm45987077892776"/><a data-type="indexterm" data-primary="pods (Kubernetes)" data-secondary="isolating" id="idm45987077889144"/> Defining a network policy on pods allows you to isolate pods and make them reject any connections that are not allowed by the policy. For example, if a component in your system is compromised, a network policy will prevent the malicious actor from communicating with services with which you don’t want them to communicate. Using a NetworkPolicy resource in Kubernetes, you can define a pod selector and detailed ingress and egress policies.<a data-type="indexterm" data-primary="best practices" data-secondary="ensuring security" data-startref="ix_bstprsec" id="idm45987077882152"/><a data-type="indexterm" data-primary="security" data-secondary="ensuring" data-startref="ix_secens" id="idm45987077880936"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Working with Data"><div class="sect1" id="working_with_data-id1">
<h1>Working with Data</h1>

<p>Most modern applications have some need to store and work with data. A growing number of data storage and analytics services are available as cloud provider–managed services. <a data-type="indexterm" data-primary="best practices" data-secondary="working with data" id="ix_bstprwkda"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" id="ix_dawkbp"/>Cloud native applications are designed to take full advantage of cloud provider–managed data systems and are designed to evolve to take advantage of a growing number of features. When working with data in the cloud, many of the standard data best practices still apply: have a disaster recovery plan, keep business logic out of the database, avoid overfetching or excessively chatty I/O, use data access implementations that prevent SQL injections attacks, and so on.</p>








<section data-type="sect2" data-pdf-bookmark="Use Managed Databases and Analytics Services"><div class="sect2" id="use_managed_databases_and_analytics_serv">
<h2>Use Managed Databases and Analytics Services</h2>

<p>Whenever possible use a managed database.<a data-type="indexterm" data-primary="databases" data-secondary="using managed databases" id="idm45987077877976"/><a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="using managed databases and analytics services" id="idm45987077877128"/><a data-type="indexterm" data-primary="data analytics" data-secondary="using analytics services" id="idm45987077876008"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="using managed databases and analytics services" id="idm45987077875064"/> Provisioning a database on virtual machines (VMs) or in a Kubernetes cluster can often be a quick and easy task. Production databases that require backups and replicas can quickly increase the time and burden of operating data storage systems. By offloading the operational burden of deploying and managing a database, teams are able to focus more on development.</p>

<p>In some cases, a data storage technology might not be available as a managed service or it might be necessary to have access to some configurations that are not available in a managed version of the system.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use a Datastore That Best Fits Data Requirements"><div class="sect2" id="use_a_datastore_that_best_fits_data_requ">
<h2>Use a Datastore That Best Fits Data Requirements</h2>

<p>When designing on-premises applications, architects would often try to avoid using multiple databases. <a data-type="indexterm" data-primary="databases" data-secondary="using datastore best fitting requirements" id="idm45987077871880"/><a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="using datastore best fitting requirements" id="idm45987077871368"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="using datastore best fitting requirements" id="idm45987077870664"/>Each database technology used would require database administrators with the skillset to deploy and manage the database, significantly increasing the operational costs of the application. The reduced operational costs of cloud-managed databases make it possible to use multiple different types of datastores to put data in a system best suited for the data type, read, and write requirements. Cloud native applications take full advantage of this, using multiple data storage technologies.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Keep Data in Multiple Regions or Zones"><div class="sect2" id="keep_data_in_multiple_regions_or_zones">
<h2>Keep Data in Multiple Regions or Zones</h2>

<p>Store production data for applications across multiple regions or zones. How the data is stored across the zones or regions will depend on the application’s availability requirements; for example, the data might be backups or a replicated database.<a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="keeping data in multiple regions or zones" id="idm45987077864376"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="keeping data in multiple regions or zones" id="idm45987077863288"/> If a cloud provider experiences a failure of a zone or region, the data can be available to be used for recovery or failover.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Data Partitioning and Replication for Scale"><div class="sect2" id="use_data_partitioning_and_replication_fo">
<h2>Use Data Partitioning and Replication for Scale</h2>

<p>Cloud native applications are designed to scale out as opposed to scale up. Scaling a database up is achieved by increasing the resources available to a database instance; for example, adding more cores or memory.<a data-type="indexterm" data-primary="partitioning" data-secondary="using data partitioning and replication for scale" id="idm45987077861816"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="using data partitioning and replication for scale" id="idm45987077859400"/><a data-type="indexterm" data-primary="scaling" data-secondary="using data partitioning and replication for scale" id="idm45987077857544"/><a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="using data partitioning and replication for scale" id="idm45987077856536"/> This ultimately encounters a hard limit and can be costly. Scaling databases out is achieved through distributing the data across multiple instances of a database. The database is partitioned, or broken up, and stored in multiple databases.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Avoid Overfetching and Chatty I/O"><div class="sect2" id="avoid_overfetching_and_chatty_i_solidus">
<h2>Avoid Overfetching and Chatty I/O</h2>

<p>Overfetching is when an application requests data from a database but needs only a fraction of the data for the operation.<a data-type="indexterm" data-primary="overfetching, avoiding" id="idm45987077853512"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="avoiding overfetching and chatty I/O" id="idm45987077852904"/><a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="avoiding overfetching and chatty I/O" id="idm45987077851752"/><a data-type="indexterm" data-primary="chatty I/O, avoiding" id="idm45987077849880"/><a data-type="indexterm" data-primary="I/O, chatty, avoiding" id="idm45987077849176"/> For example, an application might display a list of orders with a simple summary but request the entire order and order details without needing it. A chatty application, on the other hand, makes a lot of small calls to complete an operation when a single request can be made to the database.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Don’t Put Business Logic in the Database"><div class="sect2" id="donat_put_business_logic_in_the_database">
<h2>Don’t Put Business Logic in the Database</h2>

<p>Too many application scaling issues are the result of putting too much logic in the database.<a data-type="indexterm" data-primary="databases" data-secondary="leaving business logic out of" id="idm45987077844488"/><a data-type="indexterm" data-primary="business logic, not putting in databases" id="idm45987077843976"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="leaving business logic out of databases" id="idm45987077843528"/><a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="leaving business logic out of databases" id="idm45987077842424"/> Databases made it easy to put business logic inside the database by supporting standard development languages, and it became convenient to perform these tasks in the database. This often introduces scaling issues because a database is commonly an expensive shared resource.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Test with Production-like Data"><div class="sect2" id="test_with_production-like_data">
<h2>Test with Production-like Data</h2>

<p>Create automation to anonymize production data that can be updated with new rules as the data changes.<a data-type="indexterm" data-primary="production" data-secondary="testing with production-like data" id="idm45987077837848"/><a data-type="indexterm" data-primary="testing" data-secondary="using production-like data" id="idm45987077837336"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="testing with production-like data" id="idm45987077836280"/><a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="testing with production-like data" id="idm45987077835640"/> Applications should be tested with production-like data. Data is sometimes pulled from production systems, scrubbed, and loaded into test systems to provide production-like data. You should automate this process so that it is easy to update as the data changes.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Handle Transient Failures"><div class="sect2" id="handle_transient_failures">
<h2>Handle Transient Failures</h2>

<p>As mentioned in the resiliency section of this chapter, failures will happen.<a data-type="indexterm" data-primary="failures" data-secondary="handling transient failures" id="idm45987077831912"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-tertiary="handling transient failures" id="idm45987077831064"/><a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-tertiary="handling transient failures" id="idm45987077829464"/> Expect failures when making calls to a database and be prepared to handle them. Many of the database client libraries support transient fault handling already. It’s important to understand whether they do and how it’s supported.<a data-type="indexterm" data-primary="best practices" data-secondary="working with data" data-startref="ix_bstprwkda" id="idm45987077838312"/><a data-type="indexterm" data-primary="data, working with" data-secondary="best practices" data-startref="ix_dawkbp" id="idm45987077828200"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Performance and Scalability"><div class="sect1" id="performance_and_scalability">
<h1>Performance and Scalability</h1>

<p>Performance indicates how well a system can execute an operation within a certain time frame, whereas scalability refers to how a system can handle load increase without impact on the performance.<a data-type="indexterm" data-primary="best practices" data-secondary="performance and scalability" id="idm45987077826312"/><a data-type="indexterm" data-primary="performance" data-secondary="and scalability" data-tertiary="best practices for" data-secondary-sortas="scalability" id="idm45987077823512"/><a data-type="indexterm" data-primary="scalability" data-secondary="performance and" id="idm45987077822120"/> Predicting periods of increased activity to a system can be tough, so the components need to be able to scale out as needed to meet the increased demand and then scale down, after the demand decreases. The subsections that follow present some best practices to help you achieve optimal performance and scalability.</p>








<section data-type="sect2" data-pdf-bookmark="Design Stateless Services That Scale Out"><div class="sect2" id="design_stateless_services_that_scale_out">
<h2>Design Stateless Services That Scale Out</h2>

<p>Services should be designed to scale out.<a data-type="indexterm" data-primary="scalability" data-secondary="performance and" data-tertiary="designing stateless services that scale out" id="idm45987077818120"/><a data-type="indexterm" data-primary="best practices" data-secondary="performance and scalability" data-tertiary="designing stateless services that scale out" id="idm45987077817000"/><a data-type="indexterm" data-primary="state" data-secondary="designing stateless services that scale out" id="idm45987077815816"/> Scaling out is an approach to increasing the scale of a service by adding more instances of a service. Scaling up is an approach to scaling a service by adding more resources like memory or cores, but this method generally has a hard limit. By designing a service to scale out and back in, you can scale the service to handle variations in the load without impacting the availability of the service.</p>

<p>Stateful applications are inherently difficult to scale and should be avoided. If stateful services are necessary, it’s generally best to separate the functionality from the application and use a partitioning strategy and managed services if they are available.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Platform Autoscaling Features"><div class="sect2" id="use_platform_autoscaling_features">
<h2>Use Platform Autoscaling Features</h2>

<p>When possible, use any autoscaling features that are built into the platform before implementing your own.<a data-type="indexterm" data-primary="autoscaling" id="idm45987077808488"/><a data-type="indexterm" data-primary="platforms, autoscaling features" id="idm45987077808104"/><a data-type="indexterm" data-primary="scalability" data-secondary="performance and" data-tertiary="using platform autoscaling" id="idm45987077813624"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Horizontal Pod Autoscaler (HPA)" id="idm45987077812984"/> Kubernetes offers Horizontal Pod Autoscaler (HPA). HPA scales the pods based on the CPU, memory, or custom metrics. You specify the metric (e.g., 85% of CPU or 16 GB of memory) and the minimum and maximum number of pod replicas. After the target metric is reached, Kubernetes automatically scales the pods. Similarly, cluster autoscaling scales the number of cluster nodes if pods can’t be scheduled. Cluster autoscaling uses the requested resources in the pod specification to determine whether nodes should be added.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Caching"><div class="sect2" id="use_caching">
<h2>Use Caching</h2>

<p>Caching is a technique that can help improve the performance of your component by temporarily storing frequently used data in storage that’s close to the component.<a data-type="indexterm" data-primary="performance" data-secondary="and scalability" data-tertiary="using caching to improve performance" data-secondary-sortas="scalability" id="idm45987077803208"/><a data-type="indexterm" data-primary="caching" data-secondary="using to improve performance" id="idm45987077802072"/><a data-type="indexterm" data-primary="best practices" data-secondary="performance and scalability" data-tertiary="using caching" id="idm45987077805496"/><a data-type="indexterm" data-primary="scalability" data-secondary="performance and" data-tertiary="using caching" id="idm45987077804232"/> This improves the response time because the component does not need to go to the original source. The most basic type of cache is an in-memory store that is being used by a single process. If you have multiple instances of your component, each instance will have its own independent copy of the in-memory cache.<a data-type="indexterm" data-primary="consistency" data-secondary="problems with in-memory caches" id="idm45987077800840"/> This can cause consistency problems if data is not static because the different instances will have different versions of cached data. To solve this problem, you can use shared caching, which ensures that different component instances use the same cached data. In this case, cache is stored separately, usually in front of the database.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Partitioning to Scale Beyond Service Limits"><div class="sect2" id="use_partitioning_to_scale_beyond_service">
<h2>Use Partitioning to Scale Beyond Service Limits</h2>

<p>Cloud services will often have some defined scale limits. It’s important to understand the scalability limits of each of the services used and how much they can be scaled up. <a data-type="indexterm" data-primary="partitioning" data-secondary="using to scale beyond service limits" id="idm45987077793960"/><a data-type="indexterm" data-primary="best practices" data-secondary="performance and scalability" data-tertiary="using partitioning to scale beyond service limits" id="idm45987077793720"/><a data-type="indexterm" data-primary="scalability" data-secondary="performance and" data-tertiary="using partitioning to scale beyond service limits" id="idm45987077796792"/>If a single service is unable to scale to meet the application’s requirements, create multiple service instances and partition work across the instances.<a data-type="indexterm" data-primary="gateways" id="idm45987077791096"/> For example, if a managed gateway was capable of handling 80% of the application’s intended load, create another gateway and split the services across the gateway.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Functions"><div class="sect1" id="functions-id1">
<h1>Functions</h1>

<p>Much of the software development life cycle (SDLC) and general server architecture best practices are the same for serverless architectures.<a data-type="indexterm" data-primary="best practices" data-secondary="for functions" id="ix_bstprfnc"/><a data-type="indexterm" data-primary="functions" data-secondary="best practices" id="ix_fnctbp"/> Given serverless is a different operating model, there are, however, some best practices specific to functions.</p>








<section data-type="sect2" data-pdf-bookmark="Write Single-Purpose Functions"><div class="sect2" id="write_single-purpose_functions">
<h2>Write Single-Purpose Functions</h2>

<p>Follow the single-responsibility principle and only write functions that have a single responsibility.<a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-tertiary="writing single-purpose functions" id="idm45987077783192"/><a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-tertiary="writing single-purpose functions" id="idm45987077782552"/><a data-type="indexterm" data-primary="single-responsibility principle" id="idm45987077781912"/> This will make your functions easier to reason about, test, and, when the time comes, debug.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Don’t Chain Functions"><div class="sect2" id="donat_chain_functions">
<h2>Don’t Chain Functions</h2>

<p>In general, functions should push messages/data to a queue or a datastore to trigger any other functions if needed.<a data-type="indexterm" data-primary="chaining functions, avoiding" id="idm45987077779704"/><a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-tertiary="not chaining functions" id="idm45987077783560"/><a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-tertiary="not chaining functions" id="idm45987077778808"/> Having one or more functions call other functions is often considered an antipattern that additionally increases your cost and makes the debugging more difficult. If your application requires the daisy-chaining of functions, you should consider using function offerings such as Azure Durable Functions or AWS Step Functions.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Keep Functions Light and Simple"><div class="sect2" id="keep_functions_light_and_simple">
<h2>Keep Functions Light and Simple</h2>

<p>Each function should do just one thing and rely on only a minimal number of external libraries.<a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-tertiary="keeping them light and simple" id="idm45987077772664"/><a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-tertiary="keeping them light and simple" id="idm45987077771992"/><a data-type="indexterm" data-primary="code" data-secondary="extra and unnecessary, in functions" id="idm45987077771176"/> Any extra and unnecessary code in the function makes the function bigger in size, and that affects the start time.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Make Functions Stateless"><div class="sect2" id="make_functions_stateless">
<h2>Make Functions Stateless</h2>

<p>Don’t save any data in your functions because new function instances usually run in their own isolated environment and don’t share anything with other functions or invocations of the same function.<a data-type="indexterm" data-primary="state" data-secondary="making functions stateless" id="idm45987077769384"/><a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-tertiary="making functions stateless" id="idm45987077764984"/><a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-tertiary="making functions stateless" id="idm45987077767512"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Separate Function Entry Point from the Function Logic"><div class="sect2" id="separate_function_entry-point_from_the">
<h2>Separate Function Entry Point from the Function Logic</h2>

<p>Functions will have an entry point invoked by the function framework. Framework-specific context is generally passed to the function entry point, along with invocation context.<a data-type="indexterm" data-primary="entry-point for functions" id="idm45987077763464"/><a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-tertiary="separating entry-point from function logic" id="idm45987077762120"/><a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-tertiary="separating entry-point from function logic" id="idm45987077761032"/> For example, if the function is invoked through an HTTP request like an API gateway, the context will contain HTTP-specific details. The entry-point method should separate these entry-point details from the rest of the code. This will improve manageability, testability, and portability of the functions.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Avoid Long-Running Functions"><div class="sect2" id="avoid_long-running_functions">
<h2>Avoid Long-Running Functions</h2>

<p>Most Function as a Service (FaaS) offerings have an upper limit for execution time per function.<a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-tertiary="avoiding long-running functions" id="idm45987077755544"/><a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-tertiary="avoiding long-running functions" id="idm45987077754872"/> As a result, long-running functions can cause issues such as increased load times and timeouts. Whenever possible, refactor large functions into smaller ones that work together.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Queues for Cross-Function Communication"><div class="sect2" id="use_queues_for_cross-function_communicat">
<h2>Use Queues for Cross-Function Communication</h2>

<p>Instead of passing information among one another, functions should use a queue to which to post the messages.<a data-type="indexterm" data-primary="queues" data-secondary="using for cross-function communication" id="idm45987077753320"/><a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-tertiary="using queues for cross-function communication" id="idm45987077751896"/><a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-tertiary="using queues for cross-function communication" id="idm45987077750952"/> Other functions can be triggered and executed based off the events that happen on that queue (item added, removed, updated, etc.).<a data-type="indexterm" data-primary="best practices" data-secondary="for functions" data-startref="ix_bstprfnc" id="idm45987077750312"/><a data-type="indexterm" data-primary="functions" data-secondary="best practices" data-startref="ix_fnctbp" id="idm45987077747832"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Operations"><div class="sect1" id="operations">
<h1>Operations</h1>

<p>A DevOps practice provides the foundation necessary for organizations to make the best use of cloud technologies.<a data-type="indexterm" data-primary="best practices" data-secondary="operations" id="ix_bstprops"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" id="ix_opsbp"/><a data-type="indexterm" data-primary="DevOps" data-secondary="operations best practices" id="ix_DOops"/> Cloud native applications utilize DevOps principles and best practices that are detailed in <a data-type="xref" href="ch05.xhtml#devops">Chapter 5</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Deployments and Releases Are Separate Activities"><div class="sect2" id="deployments_and_releases_are_separate_ac">
<h2>Deployments and Releases Are Separate Activities</h2>

<p>It is important to make a distinction between deployment and release.<a data-type="indexterm" data-primary="deployments" data-secondary="separation from releases" id="idm45987077736136"/><a data-type="indexterm" data-primary="releases" data-secondary="separation from deployments" id="idm45987077735288"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="deployments and releases are separate" id="idm45987077734440"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="deployments and releases are separate" id="idm45987077733288"/> Deployment is the act of taking the built component and placing it within an environment—the component is fully configured and ready to go; however, there is no traffic being sent to it. As part of the component release, we begin to allow traffic to the deployed component. This separation allows you to do gradual releases, A/B testing, and canary deployments in a controlled manner.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Keep Deployments Small"><div class="sect2" id="keep_deployments_small">
<h2>Keep Deployments Small</h2>

<p>Each component deployment should be a small event that can be performed by a single team in a short time.<a data-type="indexterm" data-primary="deployments" data-secondary="keeping small" id="idm45987077731704"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="keeping deployments small" id="idm45987077731320"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="keeping deployments small" id="idm45987077730600"/> There is no general rule about how small a deployment should be and how much time it should take to deploy a component, because this is highly dependent on the component, your process, and the change to the component. A good approach is to be able to roll out a critical fix within a day.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="CI/CD Definition Lives with the Component"><div class="sect2" id="ci_solidus_cd_definition_lives_with_the">
<h2>CI/CD Definition Lives with the Component</h2>

<p>You need to store and version any CI/CD configuration and dependencies alongside the component.<a data-type="indexterm" data-primary="components" data-secondary="CI/CD configuration and dependencies storing with" id="idm45987077722600"/><a data-type="indexterm" data-primary="dependencies" data-secondary="storing with components" id="idm45987077722088"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="CI/CD definition, storing with the component" id="idm45987077721512"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="CI/CD definition, storing with the component" id="idm45987077720440"/><a data-type="indexterm" data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="definition, storing with components" id="idm45987077719800"/> Each push to the component’s branch triggers the pipeline and executes jobs defined in the CI/CD configuration. To control component deployments to different environments (development, staging, production), you can use the Git branch names and configure your pipeline to deploy the master branch only to a production environment, for example.<a data-type="indexterm" data-primary="environments" data-secondary="controlling component deployments to" id="idm45987077714152"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Consistent Application Deployment"><div class="sect2" id="consistent_application_deployment">
<h2>Consistent Application Deployment</h2>

<p>With a consistently reliable and repeatable deployment process, you can minimize errors.<a data-type="indexterm" data-primary="deployments" data-secondary="consistent application deployments" id="idm45987077715896"/><a data-type="indexterm" data-primary="applications" data-secondary="consistent deployment of" id="idm45987077715384"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="consistent application deployment" id="idm45987077714760"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="consistent application deployment" id="idm45987077711544"/> Automate as many processes as possible and ensure that you have a rollback plan defined in case deployment fails.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Zero-Downtime Releases"><div class="sect2" id="use_zero-downtime_releases">
<h2>Use Zero-Downtime Releases</h2>

<p>To maximize the availability of your system during releases, consider using zero-downtime releases such as blue/green or canary.<a data-type="indexterm" data-primary="releases" data-secondary="using zero-downtime releases" id="idm45987077710600"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="zero-downtime releases" id="idm45987077707704"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="zero-downtime releases" id="idm45987077706552"/> Using one of these approaches also allows you to quickly roll back the update in case of failures.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Don’t Modify Deployed Infrastructure"><div class="sect2" id="donat_modify_deployed_infrastructure">
<h2>Don’t Modify Deployed Infrastructure</h2>

<p>Infrastructure should be immutable.<a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="not modifying deployed infrastructure" id="idm45987077702280"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="not modifying deployed infrastructure" id="idm45987077701192"/> Modifying deployed infrastructure can quickly get out of hand, and keeping track of what changed can be complicated. If you need to update the infrastructure, redeploy it instead.<a data-type="indexterm" data-primary="infrastructure" data-secondary="deployed, not modifying" id="idm45987077700104"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Containerized Build"><div class="sect2" id="use_containerized_build">
<h2>Use Containerized Build</h2>

<p>To avoid configuring build environments, package your build process into Docker containers.<a data-type="indexterm" data-primary="containers" data-secondary="using containerized build" id="idm45987077721384"/><a data-type="indexterm" data-primary="builds" data-secondary="using containerized build" id="idm45987077695048"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="using containerized build" id="idm45987077694536"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="using containerized build" id="idm45987077693784"/> Consider using multiple images and containers for builds instead of creating a single, monolithic build image.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Describe Infrastructure Using Code"><div class="sect2" id="describe_infrastructure_using_code">
<h2>Describe Infrastructure Using Code</h2>

<p>Infrastructure should be described using either cloud provider’s declarative templates or a programming language or scripts that provision the infrastructure.<a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="describing infrastructure with code" id="idm45987077692840"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="describing infrastructure with code" id="idm45987077688120"/><a data-type="indexterm" data-primary="infrastructure" data-secondary="describing using code" id="idm45987077689544"/><a data-type="indexterm" data-primary="code" data-secondary="describing infrastructure with" id="idm45987077687112"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Namespaces to Organize Services in Kubernetes"><div class="sect2" id="use_namespaces_to_organize_services_in">
<h2>Use Namespaces to Organize Services in Kubernetes</h2>

<p>Every resource in a Kubernetes cluster belongs to a namespace.<a data-type="indexterm" data-primary="namespaces" data-secondary="using to organize services in Kubernetes" id="idm45987077683048"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="using namespaces to organize services" id="idm45987077682200"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="organizing services in Kubernetes with namespaces" id="idm45987077681320"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="using namespaces to organize services in Kubernetes" id="idm45987077680040"/> By default, newly created resources go into a namespace called <em>default</em>. For better organization of services, it is a good practice to use descriptive names and group services into bounded <span class="keep-together">contexts.</span></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Isolate the Environments"><div class="sect2" id="isolate_the_environments">
<h2>Isolate the Environments</h2>

<p>Use a dedicated production cluster and physically separate the production cluster for your development, staging, or testing environments.<a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="isolating enironments" id="idm45987077677000"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="isolating enironments" id="idm45987077675256"/><a data-type="indexterm" data-primary="environments" data-secondary="development, staging, and testing, isolating" id="idm45987077674072"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Separate Function Source Code"><div class="sect2" id="separate_function_source_code">
<h2>Separate Function Source Code</h2>

<p>Each function must be independently versioned and have its own dependencies.<a data-type="indexterm" data-primary="functions" data-secondary="separate function source code" id="idm45987077670040"/><a data-type="indexterm" data-primary="source code" data-secondary="separate, for functions" id="idm45987077669304"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="separate function source code" id="idm45987077668424"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="separate function source code" id="idm45987077667160"/> If that’s not the case, you will end up with a monolith and a tightly coupled codebase.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Correlate Deployments with Commits"><div class="sect2" id="correlate_deployments_with_commits">
<h2>Correlate Deployments with Commits</h2>

<p>Pick a branching strategy that allows you to correlate the deployments to specific commits in your branch and that also allows you to identify which version of the source code is deployed.<a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-tertiary="correlating deployments with commits" id="idm45987077665288"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-tertiary="correlating deployments with commits" id="idm45987077663224"/><a data-type="indexterm" data-primary="deployments" data-secondary="correlating with commits" id="idm45987077662024"/><a data-type="indexterm" data-primary="source code" data-secondary="commits, correlating with deployments" id="idm45987077661032"/><a data-type="indexterm" data-primary="best practices" data-secondary="operations" data-startref="ix_bstprops" id="idm45987077659928"/><a data-type="indexterm" data-primary="DevOps" data-secondary="operations best practices" data-startref="ix_DOops" id="idm45987077658840"/><a data-type="indexterm" data-primary="operations" data-secondary="best practices" data-startref="ix_opsbp" id="idm45987077657672"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Logging, Monitoring, and Alerting"><div class="sect1" id="logging_comma_monitoring_comma_and_alert">
<h1>Logging, Monitoring, and Alerting</h1>

<p>Application and infrastructure logging can provide much more value than just root-cause analysis.<a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" id="ix_bstprlma"/> A proper logging solution will provide valuable insights into applications and systems, and it’s often necessary for monitoring the health of an application and alerting operations of important events. As cloud applications become more distributed, logging and instrumentation become increasingly challenging and important.</p>








<section data-type="sect2" data-pdf-bookmark="Use a Unified Logging System"><div class="sect2" id="use_a_unified_logging_system">
<h2>Use a Unified Logging System</h2>

<p>Use a unified logging system capable of capturing log messages across all services and levels of a system and store them in a centralized store.<a data-type="indexterm" data-primary="logging" data-secondary="using a unified logging system" id="idm45987077649752"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="using a unified logging system" id="idm45987077649384"/> Whether you move all logs to a centralized store for analysis and search, or you leave them on the machine with the necessary tools in place to run a distributed query, it’s important that an engineer can find and analyze logs without having to go from one system to the next.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Correlation IDs"><div class="sect2" id="use_correlation_ids">
<h2>Use Correlation IDs</h2>

<p>Include a unique correlation ID (CID) that is passed through all services.<a data-type="indexterm" data-primary="identity" data-secondary="using correlation IDs" id="idm45987077644984"/><a data-type="indexterm" data-primary="correlation IDs (CIDs)" id="idm45987077644104"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="using correlation IDs" id="idm45987077643400"/><a data-type="indexterm" data-primary="tracing" data-secondary="using correlation IDs for" id="idm45987077641928"/> If one of the services fails, the correlation ID is used to trace the request through the system and pinpoint where the failure occurred.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Include Context with Log Entries"><div class="sect2" id="include_context_with_log_entries">
<h2>Include Context with Log Entries</h2>

<p>Each log entry should contain additional context that can help when you are investigating issues.<a data-type="indexterm" data-primary="logging" data-secondary="including context with log entries" id="idm45987077637880"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="including context with log entries" id="idm45987077637368"/> For example, include all exception handling, retry attempts, service name or ID, image version, binary version, and so on.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Common and Structured Logging Format"><div class="sect2" id="common_and_structured_logging_format">
<h2>Common and Structured Logging Format</h2>

<p>Decide on a common and structured logging format that all components will use. This will allow you to quickly search and parse the logs later on.<a data-type="indexterm" data-primary="logging" data-secondary="using common and structured logging format" id="idm45987077636360"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="common and structured logging format" id="idm45987077634536"/> Also, make sure you are using the same time zone information in all your components. In general, it is best to adhere to a common time format such as Coordinated Universal Time (UTC).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Tag Your Metrics Appropriately"><div class="sect2" id="tag_your_metrics_appropriately">
<h2>Tag Your Metrics Appropriately</h2>

<p>In addition to using clear and unique metric names, make sure that you are storing any additional information, such as component name, environment, function name, region, and so forth, in the metric tags.<a data-type="indexterm" data-primary="measurements" data-secondary="tagging metrics appropriately" id="idm45987077630968"/><a data-type="indexterm" data-primary="monitoring" data-secondary="tagging metrics appropriately" id="idm45987077630232"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="tagging metrics appropriately" id="idm45987077628968"/><a data-type="indexterm" data-primary="tagging" data-secondary="of monitoring metrics" id="idm45987077627688"/> With tags in place, you can create queries, dashboards, and reports using multiple dimensions (e.g., average latency across a specific region or across regions for a specific function).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Avoid Alert Fatigue"><div class="sect2" id="avoid_alert_fatigue">
<h2>Avoid Alert Fatigue</h2>

<p>The sheer number of metrics makes it difficult to determine how to set up the alerting and what to alert on.<a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="avoiding alert fatigue" id="idm45987077624360"/><a data-type="indexterm" data-primary="alerting" data-secondary="avoiding alert fatigue" id="idm45987077623976"/> If you are firing off too many alerts, eventually people will stop paying attention to them and no longer take them seriously. Also, investigating a bunch of alerts can become overwhelming and it could be the only thing your team is doing. It is important to classify alerts by severity: low, moderate, and high. The purpose of low-severity alerts is to potentially use them later, when doing root-cause analysis of a high-severity alert. You can use them to uncover certain patterns, but they do not require any immediate action when fired. A moderate-severity alert should either create a notification or open a ticket. These are the alerts you want to look at, but are not high priority and don’t need immediate action. They could represent a temporary condition (increase demand, for example) that eventually goes away. They also give you an early warning of a possible high-severity alert. Finally, high-severity alerts are the ones that will wake people up in the middle of the night and require immediate action. Recently, machine learning–based approaches to automatically triage issues and raise alerts are gaining in popularity, and the term AIOps has even been introduced.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Define and Alert on Key Performance Indicators"><div class="sect2" id="define_and_alert_on_key_performance_indi">
<h2>Define and Alert on Key Performance Indicators</h2>

<p>Cloud native systems will have a plethora of signals that are being emitted and monitored.<a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="defining alerts on key performance indicators" id="idm45987077617624"/><a data-type="indexterm" data-primary="performance" data-secondary="defining alerts on key performance indicators" id="idm45987077616984"/><a data-type="indexterm" data-primary="alerting" data-secondary="defining alerts on key performance indicators" id="idm45987077616376"/> You need to filter down those signals and determine which ones are the most important and valuable.<a data-type="indexterm" data-primary="key performance indicators (KPIs)" id="idm45987077614152"/> These Key Performance Indicators (KPIs) give you insight into the health of your system. For example, one KPI is latency, which measures the time it takes to service a request. If you begin seeing latency increase or deviate from an acceptable range, it is probably time to issue an alert and have someone take a look at it. In addition to KPIs, you can use other signals and metrics to determine why something is failing.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Continuous Testing in Production"><div class="sect2" id="continuous_testing_in_production">
<h2>Continuous Testing in Production</h2>

<p>Using continuous testing you can generate requests that are sent throughout the system and simulate real users. <a data-type="indexterm" data-primary="testing" data-secondary="continuous, in production" id="idm45987077612952"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="continuous testing in production" id="idm45987077612568"/><a data-type="indexterm" data-primary="alerting" data-secondary="testing in production" id="idm45987077611880"/><a data-type="indexterm" data-primary="monitoring" data-secondary="testing in production" id="idm45987077611368"/><a data-type="indexterm" data-primary="production" data-secondary="continuous testing in" id="idm45987077608968"/>You can utilize this traffic to get test coverage for the components, discover potential issues, and test your monitoring and alerting. <a data-type="indexterm" data-primary="blue/green deployments" id="idm45987077607992"/><a data-type="indexterm" data-primary="A/B tests" id="idm45987077605768"/><a data-type="indexterm" data-primary="canary testing" id="idm45987077607128"/>Following are some common continuous testing practices:</p>

<ul>
<li>
<p>Blue/green deployments</p>
</li>
<li>
<p>Canary testing</p>
</li>
<li>
<p>A/B testing</p>
</li>
</ul>

<p>These practices are discussed in <a data-type="xref" href="ch05.xhtml#devops">Chapter 5</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Start with Basic Metrics"><div class="sect2" id="start_with_basic_metrics">
<h2>Start with Basic Metrics</h2>

<p>Ensure that you are always collecting traffic (how much demand is placed on the component), latency (the time it takes to service a request), and errors (rate of requests that fail) for each component in your system.<a data-type="indexterm" data-primary="latency" id="idm45987077597320"/><a data-type="indexterm" data-primary="error rate" id="idm45987077596712"/><a data-type="indexterm" data-primary="measurements" data-secondary="starting with basic metrics" id="idm45987077596104"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-tertiary="starting with basic metrics" id="idm45987077595256"/><a data-type="indexterm" data-primary="best practices" data-secondary="logging, monitoring, and alerting" data-startref="ix_bstprlma" id="idm45987077594120"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Service Communication"><div class="sect1" id="service_communications">
<h1>Service Communication</h1>

<p>Service communication is an important part of cloud native applications.<a data-type="indexterm" data-primary="best practices" data-secondary="service communications" id="ix_bstprsercm"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" id="ix_commserbp"/> Whether it’s a client communicating with a backend, a service communicating with a database, or the individual services in a distributed architecture communicating with one another, these interactions are an important part of cloud native applications. Many different forms of communication are used depending on the requirements. The following subsections offer some best practices for service communication.</p>








<section data-type="sect2" data-pdf-bookmark="Design for Backward and Forward Compatibility"><div class="sect2" id="design_for_backward_and_forward_compatib">
<h2>Design for Backward and Forward Compatibility</h2>

<p>With backward compatibility, you ensure that new functionality added to a service or component does not break any existing service.<a data-type="indexterm" data-primary="backward and forward compatibility" data-secondary="designing services for" id="idm45987077581448"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-tertiary="designing for backward and forward compatibility" id="idm45987077586712"/><a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-tertiary="designing for backward and forward compatibility" id="idm45987077585512"/> For example, in <a data-type="xref" href="#backward_compatibility-id1">Figure 6-3</a>, Service A v1.0 works with Service B v1.0. Backward compatibility means that the release of Service B v1.1 will not break the functionality of Service A.</p>

<figure><div id="backward_compatibility-id1" class="figure">
<img src="Images/clna_0603.png" alt="clna 0603" width="916" height="466"/>
<h6><span class="label">Figure 6-3. </span>Backward compatibility</h6>
</div></figure>

<p>To ensure backward compatibility, any new fields added to the API should be optional or have sensible defaults. Any existing fields should never be renamed, because that will break the backward compatibility.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><em>Parallel change</em>, also known as the <em>Expand and Contract</em> pattern, can be used to safely introduce backward-incompatible changes.<a data-type="indexterm" data-primary="parallel change" id="idm45987077575336"/><a data-type="indexterm" data-primary="expand and contract pattern" id="idm45987077574488"/> As an example, say a service owner would like to change a property or resource on an interface. The service owner will expand the interface with a new property or resource, and then after all consumers have had a chance to move the service interface, the previous property is removed.</p>
</div>

<p>If your system or components need to ensure rollback functionality,<a data-type="indexterm" data-primary="rollbacks" id="idm45987077573032"/> you will need to think about the forward compatibility as you’re making changes to your service. Forward compatibility means that your components are compatible with future versions. Your service should be able to accept “future” data and messaging formats and handle them appropriately. A good example of forward compatibility is HTML: when it encounters an unknown tag or attribute, it’s not going to fail; it will just skip it.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Define Service Contracts That Do Not Leak Internal Details"><div class="sect2" id="define_service_contracts_that_do_not_lea">
<h2>Define Service Contracts That Do Not Leak Internal Details</h2>

<p>A service that exposes an API should define contracts and test against the contracts when releasing updates.<a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-tertiary="defining service contracts that don't leak internal details" id="idm45987077572040"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-tertiary="service contracts that don't leak internal details" id="idm45987077571400"/><a data-type="indexterm" data-primary="REST APIs" data-secondary="service contracts, defining" id="idm45987077570504"/> For example, a REST-based service would generally define a contract in the OpenAPI format or as documentation, and consumers of the service would build to this contract.<a data-type="indexterm" data-primary="OpenAPI" id="idm45987077564760"/> Updates to the service can be pushed, and as long as it doesn’t introduce any breaking changes to the API contract, these releases would not affect the consumer. Leaking internal implementations of a service can make it difficult to make changes and introduces coupling. Don’t assume a consumer is not using some piece of data exposed through the API.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Services that publish messages to a queue or a stream should also define a contract in the same way.<a data-type="indexterm" data-primary="queues" data-secondary="services publishing messages to" id="idm45987077565672"/><a data-type="indexterm" data-primary="streams" data-secondary="services publishing messages to" id="idm45987077563464"/> The service publishing the events will generally own the contract.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Prefer Asynchronous Communication"><div class="sect2" id="prefer_asynchronous_communications">
<h2>Prefer Asynchronous Communication</h2>

<p>Use asynchronous communication whenever possible.<a data-type="indexterm" data-primary="asynchronous communication" id="idm45987077560904"/><a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-tertiary="preferring asynchronous communication" id="idm45987077560296"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-tertiary="preferring asynchronous communication" id="idm45987077557800"/><a data-type="indexterm" data-primary="streams" id="idm45987077556584"/><a data-type="indexterm" data-primary="messaging" data-secondary="message bus" id="idm45987077555784"/> It works well with distributed systems and decouples the execution of two or more services. A message bus or a stream is often used when implementing this approach, but you could use direct calls through something like gRPC as well. Both use a message bus as a channel.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Efficient Serialization Techniques"><div class="sect2" id="use_efficient_serialization_techniques">
<h2>Use Efficient Serialization Techniques</h2>

<p>Distributed applications like those built using a microservices architecture rely more heavily on communications and messaging between services.<a data-type="indexterm" data-primary="serialization" data-secondary="efficient techniques, using in service communications" id="idm45987077550632"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-tertiary="using efficient serialization techniques" id="idm45987077552840"/><a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-tertiary="using efficient serialization techniques" id="idm45987077551688"/> The data serialization and deserialization can add a lot of overhead in service communication.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In one case, serialization and deserialization were found to account for nearly 40% of the CPU utilization across all the services.<a data-type="indexterm" data-primary="JSON" data-secondary="serialization library" id="idm45987077549432"/> Replacing the standard JSON serialization library with a custom one reduced this overhead to roughly 15% of overall CPU utilization.</p>
</div>

<p>Use efficient serialization formats like protocol buffers, commonly used in gRPC.<a data-type="indexterm" data-primary="protocol buffers (protobufs)" id="idm45987077546312"/> Understand the trade-offs with the different serialization formats, because tooling and consumer requirements might not make this a feasible option. You can also use other techniques to reduce the need for serialization in some services by placing some of the data into headers. For example, if a service receives a request and operates on only a handful of fields in a large message payload before passing it to a downstream service, by putting these fields into headers the service does not need to deserialize or reserialize the payload. The service reads and writes headers and then simply passes the entire payload through to the downstream services.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Queues or Streams to Handle Heavy Loads and Traffic Spikes"><div class="sect2" id="use_queues_or_streams_to_handle_heavy">
<h2>Use Queues or Streams to Handle Heavy Loads and Traffic Spikes</h2>

<p>A queue or a stream between components acts as a buffer and stores the message until it is retrieved.<a data-type="indexterm" data-primary="queues" data-secondary="using to handle heavy loads and traffic spikes" id="idm45987077541560"/><a data-type="indexterm" data-primary="streams" data-secondary="using to handle heavy loads and traffic spikes" id="idm45987077541048"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-tertiary="using queues or streams to handle heavy loads or traffic spikes" id="idm45987077540472"/><a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-tertiary="using queues or streams to handle heavy loads or traffic spikes" id="idm45987077539736"/> Using a queue allows the components to process the messages at their own pace, regardless of the incoming volume or load. Consequently, this helps maximize the availability and scalability of your services.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Batch Requests for Efficiency"><div class="sect2" id="batch_requests_for_efficiency">
<h2>Batch Requests for Efficiency</h2>

<p>Queues can be used for batching multiple requests and performing an action only once.<a data-type="indexterm" data-primary="queues" data-secondary="using to batch requests" id="idm45987077536648"/><a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-tertiary="batching requests for effiiciency" id="idm45987077535800"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-tertiary="batching requests for efficiency" id="idm45987077532904"/> For example, it is more efficient to write 1,000 batched entries into the database instead of one entry at a time 1,000 times.<a data-type="indexterm" data-primary="batch processing" data-secondary="batching requests for efficiency" id="idm45987077530568"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Split Up Large Messages"><div class="sect2" id="split_up_large_messages">
<h2>Split Up Large Messages</h2>

<p>Sending, receiving, and manipulating large messages requires more resources and can slow down your entire system.<a data-type="indexterm" data-primary="messaging" data-secondary="splitting up large messages" id="idm45987077529400"/><a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-tertiary="splitting up large messages" id="idm45987077528888"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-tertiary="splitting up large messages" id="idm45987077528248"/><a data-type="indexterm" data-primary="claim-check pattern" id="idm45987077527608"/> The <em>Claim-Check</em> pattern talks about splitting a large message into two parts. You store the entire message in an external service (database, for example) and send only the reference to the message. Any interested message receivers can use the reference to obtain the full message from the database.<a data-type="indexterm" data-primary="best practices" data-secondary="service communications" data-startref="ix_bstprsercm" id="idm45987077525432"/><a data-type="indexterm" data-primary="communication (services)" data-secondary="best practices" data-startref="ix_commserbp" id="idm45987077523496"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Containers"><div class="sect1" id="containers-id1">
<h1>Containers</h1>

<p>It’s possible to run most applications in a Docker container without very much effort.<a data-type="indexterm" data-primary="best practices" data-secondary="containers" id="ix_bstprcntr"/><a data-type="indexterm" data-primary="containers" data-secondary="best practices" id="ix_cntrbp"/> However, there are some potential pitfalls when running containers in production and streamlining the build, deployment, and monitoring. A number of best practices have been identified to help avoid the pitfalls and improve the results.</p>








<section data-type="sect2" data-pdf-bookmark="Store Images in a Trusted Registry"><div class="sect2" id="store_images_in_trusted_registry">
<h2>Store Images in a Trusted Registry</h2>

<p>Any images running on the platform should come from the trusted container image registry. Kubernetes exposes a webhook (validating admission) that can be used to ensure pods can use images only from a trusted registry.<a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="storing images in trusted registry" id="idm45987077513112"/><a data-type="indexterm" data-primary="registries (container)" id="idm45987077511928"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="storing images in trusted registry" id="idm45987077515496"/><a data-type="indexterm" data-primary="security" data-secondary="trusted container images" id="idm45987077514232"/> If you’re using Google Cloud, you can take advantage of the binary authorization security measure that ensures only trusted images are deployed on your cluster.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Utilize the Docker Build Cache"><div class="sect2" id="utilize_docker_build_cache">
<h2>Utilize the Docker Build Cache</h2>

<p>Using the build cache when building Docker images can speed up the build process. All images are built up from layers, and each line in the Dockerfile contributes a layer to the final image.<a data-type="indexterm" data-primary="Docker" data-secondary="build cache" id="idm45987077509784"/><a data-type="indexterm" data-primary="caching" data-secondary="Docker build cache" id="idm45987077508888"/><a data-type="indexterm" data-primary="builds" data-secondary="Docker build cache for container images" id="idm45987077506888"/><a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="using Docker build cache" id="idm45987077505944"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="using Docker build cache" id="idm45987077504360"/> During the build, Docker will try to reuse a layer from a previous build instead of building it again. However, it can reuse only the cached layers if all previous build steps used it as well. To get the most out of the Docker build cache, put the commands that change more often (e.g., adding the source code to the image, building the source code) at the end of the Dockerfile. That way, any preceding steps will be reused.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Don’t Run Containers in Privileged Mode"><div class="sect2" id="donat_run_containers_in_privileged_mode">
<h2>Don’t Run Containers in Privileged Mode</h2>

<p>Running containers in privileged mode allows access to everything on the host. Use the security policy on the pod to prevent containers from running in privileged mode. <a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="not running containers in privileged mode" id="idm45987077498888"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="not running containers in privileged mode" id="idm45987077503176"/><a data-type="indexterm" data-primary="privileged mode (containers)" id="idm45987077499768"/>If a container does for some reason require privileged mode to make changes to the host environment, consider separating that functionality from the container and into the infrastructure provisioning.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Explicit Container Image Tags"><div class="sect2" id="use_explicit_container_image_tags">
<h2>Use Explicit Container Image Tags</h2>

<p>Always tag your container images with specific tags that tightly link the container image to the code that is packaged in the image.<a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="using explicit container image tags" id="idm45987077495000"/><a data-type="indexterm" data-primary="tagging" data-secondary="using explicit container image tags" id="idm45987077492712"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="using explicit container image tags" id="idm45987077496264"/> To tag the images properly, you can either use a Git commit hash that uniquely identifies the version of the code (e.g., <code>1f7a7a472</code>) or use a semantic version (e.g., <code>1.0.1</code>). The tag <code>latest</code> is used as a default value if no tag is provided; however, because it’s not tightly linked to the specific version of the code, you should avoid using it. The latest tag should never be used in a production environment because it can cause inconsistent behavior that can be difficult to troubleshoot.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Keep Container Images Small"><div class="sect2" id="keep_container_images_small">
<h2>Keep Container Images Small</h2>

<p>In addition to taking up less space in a container registry or the host system using the image to run a container, smaller images improve image push and pull performance.<a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="keeping container images small" id="idm45987077486264"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="keeping container images small" id="idm45987077483880"/> This in turn improves the performance when you start containers as part of deploying or scaling a service. The application and its dependencies will have some impact on the size of the image, but you can reduce most of the image size by using lean base images and ensuring that unnecessary files are not included in the image. For example, the alpine 3.9.4 image is only 3 MB, with the Debian stretch image at 45 MB, and the CentOS 7.6.1810 at 75 MB. The distributions generally offer a slim version that removes more from the base image that might not be needed by the application. Generally, there are two things to keep in mind for keeping images lean:</p>

<ul>
<li>
<p>Start with a lean base image</p>
</li>
<li>
<p>Include only the files needed for the operation of the application</p>
</li>
</ul>

<p>You can use the Container Builder pattern <a data-type="indexterm" data-primary="container builder pattern" id="idm45987077480952"/>to create lean images by separating the images used to build the artifacts from the base image used to run the application. Docker’s multistage build is often used to implement this.<a data-type="indexterm" data-primary="Docker" data-secondary="multistage builds" id="idm45987077478360"/><a data-type="indexterm" data-primary="builds" data-secondary="Docker multistage build" id="idm45987077482024"/> You can create Docker build files that can start from different images used for executing the commands to build and test artifacts, and then define another base image as part of creating the image to run the application.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Using a <em>.dockerignore</em> file can improve build speed by excluding files that are not needed in the Docker build.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Run One Application per Container"><div class="sect2" id="one_application_per_container">
<h2>Run One Application per Container</h2>

<p>Always run a single application within a container. Containers were designed to run a single application, with the container having the same life cycle as the application running in the container.<a data-type="indexterm" data-primary="applications" data-secondary="one per container" id="idm45987077479224"/><a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="one application per container" id="idm45987077474792"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="one application per container" id="idm45987077472040"/> Running multiple applications within the same container makes it difficult to manage, and you might end up with a container in which one of the processes has crashed or is unresponsive.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Verified Images from Trusted Repositories"><div class="sect2" id="use_verified_images_from_trusted_reposit">
<h2>Use Verified Images from Trusted Repositories</h2>

<p>There’s a large and growing number of publicly available images that are helpful when working with containers.<a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="using verified images from trusted repositories" id="idm45987077468808"/><a data-type="indexterm" data-primary="tagging" data-secondary="Docker repository tags" id="idm45987077467928"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="using verified images from trusted repositories" id="idm45987077467416"/><a data-type="indexterm" data-primary="repositories" data-secondary="verified images from trusted repositories" id="idm45987077466520"/> Docker repository tags are mutable, so it’s important to understand that the images can change. When using images in an external repository it’s best to copy or re-create them from the external repository into one managed by the organization. The organization’s repository is usually closer to the CI services, and this approach removes another service dependency that could impact build.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Vulnerability Scanning Tools on Images"><div class="sect2" id="use_vulnerability_scanning_tools_on_imag">
<h2>Use Vulnerability Scanning Tools on Images</h2>

<p>You need to be aware of any vulnerabilities that affect your images because this can compromise the security of your system.<a data-type="indexterm" data-primary="vulnerability scanning tools, using on container images" id="idm45987077463400"/><a data-type="indexterm" data-primary="security" data-secondary="using vulnerability scanning tools on container images" id="idm45987077462456"/><a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="using vulnerability scanning tools on images" id="idm45987077461544"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="using vulnerability scanning tools on images" id="idm45987077460040"/> If a vulnerability is discovered, you need to rebuild the image with the patches and fixes included and then redeploy it. Some cloud providers offer vulnerability scanning with their image registry solutions, so make sure you are taking advantage of those features.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Scan an image as often as possible because new cybersecurity vulnerabilities and exposures (CVE) are released daily.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Don’t Store Data in Containers"><div class="sect2" id="donat_store_data_in_containers">
<h2>Don’t Store Data in Containers</h2>

<p>Containers are ephemeral—they can be stopped, destroyed, or replaced without any loss of data.<a data-type="indexterm" data-primary="data" data-secondary="not storing in containers" id="idm45987077452360"/><a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="not storing data in containers" id="idm45987077451208"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="not storing data in containers" id="idm45987077450568"/> If the service running in a container needs to store data, use a volume mount to save the data. The contents in a volume exist outside the life cycle of a container and a volume does not increase the size of a container. If the container requires temporary nonpersistent writes, use a tmpfs mount, which will improve performance by avoiding writes to a container’s writable layer.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Never Store Secrets or Configuration Inside an Image"><div class="sect2" id="never_store_secrets_or_configuration_ins">
<h2>Never Store Secrets or Configuration Inside an Image</h2>

<p>Hardcoding any type of secrets within an image is something you want to avoid.<a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-tertiary="never storing secrets or configuration in an image" id="idm45987077448024"/><a data-type="indexterm" data-primary="secrets" data-secondary="never storing in container images" id="idm45987077447048"/><a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-tertiary="not storing secrets or configuration in images" id="idm45987077446168"/><a data-type="indexterm" data-primary="configuration" data-secondary="not storing in container images" id="idm45987077444952"/> If your container requires any secrets, define them within environment variables or as files, mounted to the container through a volume.<a data-type="indexterm" data-primary="best practices" data-secondary="containers" data-startref="ix_bstprcntr" id="idm45987077440936"/><a data-type="indexterm" data-primary="containers" data-secondary="best practices" data-startref="ix_cntrbp" id="idm45987077448568"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="summary-id5">
<h1>Summary</h1>

<p>We could easily fill an entire book covering best practices for cloud native applications given the number of technologies involved. However, there are certain areas that have been coming up repeatedly in customer conversations, and this chapter has covered a collection of best practices, tips, and proven patterns for cloud native applications for those areas. You should have a better understanding of the factors you may want to consider.<a data-type="indexterm" data-primary="best practices" data-startref="ix_bstpr" id="idm45987077438552"/></p>
</div></section>







</div></section></div>



  </body></html>