<html><head></head><body><section data-pdf-bookmark="Chapter 12. Maintenance and Troubleshooting" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch_day2ops">&#13;
<h1><span class="label">Chapter 12. </span>Maintenance and Troubleshooting</h1>&#13;
&#13;
&#13;
<p>In this chapter, you will find recipes that deal with both app-level and cluster-level maintenance.<a data-primary="maintenance" data-type="indexterm" id="id1121"/> We cover various aspects of troubleshooting, from debugging pods and containers to testing service connectivity, interpreting a resource’s status, and maintaining nodes. Last but not least, we look at how to deal with etcd, the Kubernetes control-plane storage component. <a data-primary="etcd" data-type="indexterm" id="id1122"/>This chapter is relevant for both cluster admins and app developers.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.1 Enabling Autocomplete for kubectl" data-type="sect1"><div class="sect1" id="kubectl_autocomplete">&#13;
<h1>12.1 Enabling Autocomplete for kubectl</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id112">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>It is cumbersome to type full commands and arguments for the <code>kubectl</code> CLI, so you want an autocomplete function for it.<a data-primary="autocompletion for kubectl" data-type="indexterm" id="id1123"/><a data-primary="kubectl" data-secondary="enabling autocompletion for" data-type="indexterm" id="id1124"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id113">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Enable autocompletion for <code>kubectl</code>.</p>&#13;
&#13;
<p>For the <a data-primary="maintenance" data-secondary="enabling autocompletion for kubectl" data-type="indexterm" id="id1125"/><a data-primary="shells, enabling kubectl autocompletion" data-type="indexterm" id="id1126"/><a data-primary="bash shell, enabling kubectl autocompletion" data-type="indexterm" id="id1127"/>bash shell, you can enable <code>kubectl</code> autocompletion in your current shell with the following command:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>source &lt;(kubectl completion bash)</strong>&#13;
</pre>&#13;
&#13;
<p>Add this to your <em>~/.bashrc</em> file so that autocomplete loads in all of your shell sessions:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>echo 'source &lt;(kubectl completion bash)' &gt;&gt;~/.bashrc</strong>&#13;
</pre>&#13;
&#13;
<p>Note that autocompletion for bash depends on <a href="https://oreil.ly/AdlLN">bash-completion</a> being installed.<a data-primary="bash-completion" data-type="indexterm" id="id1128"/></p>&#13;
&#13;
<p class="pagebreak-before">For the zsh shell, you can enable <code>kubectl</code> autocompletion <a data-primary="zsh shell, enabling kubectl autocompletion" data-type="indexterm" id="id1129"/>with the following  &#13;
<span class="keep-together">command:</span></p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>source &lt;(kubectl completion zsh)</strong>&#13;
</pre>&#13;
&#13;
<p>And you can add this same command to your <em>~/.zshrc</em> file for autocomplete to load in all your shell sessions.</p>&#13;
&#13;
<p>For autocompletion to work in zsh, you may need to have these commands at the start of your <em>~/.zshrc</em> file:</p>&#13;
<pre data-type="programlisting">&#13;
autoload -Uz compinit&#13;
compinit&#13;
</pre>&#13;
&#13;
<p>For other operating systems and shells, please check the <a href="https://oreil.ly/G3das">documentation</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id244">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>Another popular improvement to the <code>kubectl</code> developer experience is to define an alias to shorten <code>kubectl</code> to just the letter <code>k</code>. <a data-primary="kubectl" data-secondary="alias shortening kubectl to k" data-type="indexterm" id="id1130"/>This can be achieved by executing the following commands or adding them to your shell start-up script:</p>&#13;
<pre data-type="programlisting">&#13;
alias k=kubectl&#13;
complete -o default -F __start_kubectl k&#13;
</pre>&#13;
&#13;
<p>Then, you can simply type commands like <code>k apply -f myobject.yaml</code>. This combined with autocompletion makes life a lot easier.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1131">&#13;
<h2>See Also</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/mu6PZ">Overview of <code>kubectl</code></a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/Yrk3C"><code>kubectl</code> Cheat Sheet</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.2 Removing a Pod from a Service" data-type="sect1"><div class="sect1" id="relabel">&#13;
<h1>12.2 Removing a Pod from a Service</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id245">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You have a well-defined service (see <a data-type="xref" href="ch05.html#simple_service">Recipe 5.1</a>) backed by several pods. <a data-primary="maintenance" data-secondary="removing a pod from a service" data-type="indexterm" id="ix_maintrmvpod"/><a data-primary="pods" data-secondary="removing a pod from a service" data-type="indexterm" id="ix_podrmv"/>But one of the pods is causing problems (e.g., crashing or not responding), and you want to take it out of the list of endpoints to examine it at a later time.<a data-primary="services" data-secondary="removing a pod from" data-type="indexterm" id="ix_srvrmvpod"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id114">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Relabel the pod using the <code>--overwrite</code> option—​this will allow you to change the value of the <code>run</code> label on the pod. <a data-primary="labels" data-secondary="relabeling a pod to take it out of service" data-type="indexterm" id="id1132"/>By overwriting this label, you can ensure that it will not be selected by the service selector (<a data-type="xref" href="ch05.html#simple_service">Recipe 5.1</a>) and will be removed from the list of endpoints. At the same time, the replica set watching over your pods will see that a pod has disappeared and will start a new replica.<a data-primary="deployments" data-secondary="using kubectl run" data-type="indexterm" id="id1133"/><a data-primary="run label" data-type="indexterm" id="id1134"/></p>&#13;
&#13;
<p>To see this in action, start with a straightforward deployment generated with <code>kubectl run</code> (see <a data-type="xref" href="ch04.html#deployments">Recipe 4.5</a>):</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl create deployment nginx --image nginx:1.25.2 --replicas 4</strong>&#13;
</pre>&#13;
&#13;
<p>When you list the pods and show the label with key <code>app</code>, you’ll see four pods with the value <code>nginx</code> (<code>app=nginx</code> is the label that is automatically generated by the <code>kubectl create deployment</code> command):</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get pods -Lapp</strong>&#13;
NAME                      READY   STATUS    RESTARTS   AGE     APP&#13;
nginx-748c667d99-85zxr    1/1     Running   0          14m     nginx&#13;
nginx-748c667d99-jrhpc    1/1     Running   0          14m     nginx&#13;
nginx-748c667d99-rddww    1/1     Running   0          14m     nginx&#13;
nginx-748c667d99-x6h6h    1/1     Running   0          14m     nginx&#13;
</pre>&#13;
&#13;
<p>You can then expose this deployment with a service and check the endpoints, which correspond to the IP addresses <a data-primary="endpoints for service, getting" data-type="indexterm" id="id1135"/>of each pod:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl expose deployments nginx --port 80</strong>&#13;
&#13;
$ <strong>kubectl get endpoints</strong>&#13;
NAME            ENDPOINTS                                                  AGE&#13;
kubernetes      192.168.49.2:8443                                          3h36m&#13;
nginx           10.244.0.10:80,10.244.0.11:80,10.244.0.13:80 + 1 more...   13m&#13;
</pre>&#13;
&#13;
<p>Let’s imagine that the first pod in the list is causing problems, even though its status is <em>Running</em>.</p>&#13;
&#13;
<p>Moving the first pod out of the<a data-primary="kubectl" data-secondary="label command" data-tertiary="--overwrite option" data-type="indexterm" id="id1136"/> service pool via relabeling is done with a single command:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl label pod nginx-748c667d99-85zxr app=notworking --overwrite</strong>&#13;
</pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>To find the IP address of a pod, you can use a Go template to format the pod information and <a data-primary="IP addresses" data-secondary="finding IP address for a pod" data-type="indexterm" id="id1137"/>show only its IP address:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get pod nginx-748c667d99-jrhpc \&#13;
    --template '{{.status.podIP}}' </strong>&#13;
10.244.0.11&#13;
</pre>&#13;
</div>&#13;
&#13;
<p class="pagebreak-before">You will see a new pod appear with the label <code>app=nginx</code>, and you will see that your nonworking pod still exists but no longer appears in the list of service endpoints:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get pods -Lapp</strong>&#13;
NAME                      READY   STATUS    RESTARTS   AGE     APP&#13;
nginx-748c667d99-85zxr    1/1     Running   0          14m     notworking&#13;
nginx-748c667d99-jrhpc    1/1     Running   0          14m     nginx&#13;
nginx-748c667d99-rddww    1/1     Running   0          14m     nginx&#13;
nginx-748c667d99-x6h6h    1/1     Running   0          14m     nginx&#13;
nginx-748c667d99-xfgqp    1/1     Running   0          2m17s   nginx&#13;
&#13;
$ <strong>kubectl describe endpoints nginx</strong>&#13;
Name:         nginx&#13;
Namespace:    default&#13;
Labels:       app=nginx&#13;
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2023-04-13T13...&#13;
Subsets:&#13;
  Addresses:          10.244.0.10,10.244.0.11,10.244.0.13,10.244.0.9&#13;
  NotReadyAddresses:  &lt;none&gt;&#13;
  Ports:&#13;
    Name     Port  Protocol&#13;
    ----     ----  --------&#13;
    &lt;unset&gt;  80    TCP&#13;
&#13;
Events:  &lt;none&gt;&#13;
</pre>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.3 Accessing a ClusterIP Service Outside the Cluster" data-type="sect1"><div class="sect1" id="quick-proxy">&#13;
<h1>12.3 Accessing a ClusterIP Service Outside the Cluster</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id246">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You have an internal service that is causing you trouble, and you want to test that it is working well locally without exposing the service externally.<a data-primary="maintenance" data-secondary="removing a pod from a service" data-startref="ix_maintrmvpod" data-type="indexterm" id="id1138"/><a data-primary="pods" data-secondary="removing a pod from a service" data-startref="ix_podrmv" data-type="indexterm" id="id1139"/><a data-primary="services" data-secondary="removing a pod from" data-startref="ix_srvrmvpod" data-type="indexterm" id="id1140"/><a data-primary="ClusterIP type" data-secondary="accessing ClusterIP service outside the cluster" data-type="indexterm" id="id1141"/><a data-primary="services" data-secondary="accessing ClusterIP service outside the cluster" data-type="indexterm" id="id1142"/><a data-primary="troubleshooting" data-secondary="accessing ClusterIP service outside the cluster" data-type="indexterm" id="id1143"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id247">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use a local proxy to the<a data-primary="proxy command (kubectl)" data-secondary="using local proxy to Kubernetes API server with" data-type="indexterm" id="id1144"/> Kubernetes API server with <code>kubectl proxy</code>.</p>&#13;
&#13;
<p>Let’s assume that you have created a deployment and a service as described in <a data-type="xref" href="#relabel">Recipe 12.2</a>. You should see an <code>nginx</code> service when you list the services:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get svc</strong>&#13;
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE&#13;
nginx           ClusterIP   10.108.44.174   &lt;none&gt;        80/TCP     37m&#13;
</pre>&#13;
&#13;
<p>This service is not reachable outside the Kubernetes cluster. However, you can run a proxy in a separate terminal and then reach it on <em>localhost</em>.</p>&#13;
&#13;
<p class="pagebreak-before">Start by running the proxy in a<a data-primary="kubectl" data-secondary="proxy command" data-type="indexterm" id="id1145"/> separate terminal:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl proxy</strong>&#13;
Starting to serve on 127.0.0.1:8001&#13;
</pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>You can specify the port that you want the proxy to run on with the <code>--port</code> option.</p>&#13;
</div>&#13;
&#13;
<p>In your original terminal, you can then use your browser or <code>curl</code> to access the application exposed by your service:<a data-primary="curl utility" data-secondary="accessing application exposed by service" data-type="indexterm" id="id1146"/></p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>curl http://localhost:8001/api/v1/namespaces/default/services/nginx/proxy/</strong>&#13;
&lt;!DOCTYPE html&gt;&#13;
&lt;html&gt;&#13;
&lt;head&gt;&#13;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;&#13;
...&#13;
</pre>&#13;
&#13;
<p>Note the specific path to the service; it contains a <code class="keep-together">/proxy</code> part. Without this, you get the JSON object representing the service.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Note that you can now also access the entire Kubernetes API over <em>localhost</em> using <code>curl</code>.<a data-primary="curl utility" data-secondary="ability to access entire Kubernetes API over localhost" data-type="indexterm" id="id1147"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1148">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>This recipe demonstrates an approach that is suitable for debugging and should not be used for regular access to services in production. Rather, use the secure <a data-type="xref" href="ch05.html#ingress">Recipe 5.5</a> for production scenarios.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.4 Understanding and Parsing Resource Statuses" data-type="sect1"><div class="sect1" id="res-status">&#13;
<h1>12.4 Understanding and Parsing Resource Statuses</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id115">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You want to watch an object, such as a pod, and react to changes in the object’s status.<a data-primary="resources" data-secondary="understanding and parsing statuses" data-type="indexterm" id="ix_resrcstat"/><a data-primary="status of resources, understanding and parsing" data-type="indexterm" id="ix_status"/><a data-primary="troubleshooting" data-secondary="understanding and parsing resource statuses" data-type="indexterm" id="ix_trblresstat"/> Sometimes these state changes trigger events in CI/CD pipelines.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id116">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use <code>kubectl get $KIND/$NAME -o json</code> and parse the JSON output using one of the two methods described here.<a data-primary="jq utility" data-type="indexterm" id="id1149"/><a data-primary="JSON" data-secondary="parsing using jq query utility" data-type="indexterm" id="id1150"/><a data-primary="Quality of Service (QoS), checking for a pod" data-type="indexterm" id="id1151"/></p>&#13;
&#13;
<p>If you have the JSON query utility <code>jq</code> <a href="https://oreil.ly/qopuJ">installed</a>, you can use it to parse the resource status.<a data-primary="pods" data-secondary="querying status of" data-type="indexterm" id="id1152"/> Let’s assume you have a pod called <code>jump</code>. You can do this to find out what <a href="https://oreil.ly/3CcxH">Quality of Service (QoS) class</a> the pod is in:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl run jump --image=nginx</strong>&#13;
pod/jump created&#13;
&#13;
$ <strong>kubectl get po/jump -o json | jq --raw-output .status.qosClass</strong>&#13;
BestEffort&#13;
</pre>&#13;
&#13;
<p>Note that the <code>--raw-output</code> argument for <code>jq</code> will show the raw value and that <code class="keep-together">.status.qosClass</code> is the expression that matches the respective subfield.</p>&#13;
&#13;
<p>Another status query could be around events or state transitions. For example:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get po/jump -o json | jq .status.conditions</strong>&#13;
[&#13;
  {&#13;
    "lastProbeTime": null,&#13;
    "lastTransitionTime": "2023-04-13T14:00:13Z",&#13;
    "status": "True",&#13;
    "type": "Initialized"&#13;
  },&#13;
  {&#13;
    "lastProbeTime": null,&#13;
    "lastTransitionTime": "2023-04-13T14:00:18Z",&#13;
    "status": "True",&#13;
    "type": "Ready"&#13;
  },&#13;
  {&#13;
    "lastProbeTime": null,&#13;
    "lastTransitionTime": "2023-04-13T14:00:18Z",&#13;
    "status": "True",&#13;
    "type": "ContainersReady"&#13;
  },&#13;
  {&#13;
    "lastProbeTime": null,&#13;
    "lastTransitionTime": "2023-04-13T14:00:13Z",&#13;
    "status": "True",&#13;
    "type": "PodScheduled"&#13;
  }&#13;
]&#13;
</pre>&#13;
&#13;
<p class="pagebreak-before">Of course, these queries are not limited to pods—​you can apply this technique to any resource. <a data-primary="deployments" data-secondary="querying revisions of" data-type="indexterm" id="id1153"/>For example, you can query the revisions of a deployment:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl create deployment wordpress --image wordpress:6.3.1</strong>&#13;
deployment.apps/wordpress created&#13;
&#13;
$ <strong>kubectl get deploy/wordpress -o json | jq .metadata.annotations</strong>&#13;
{&#13;
  "deployment.kubernetes.io/revision": "1"&#13;
}&#13;
</pre>&#13;
&#13;
<p>Or you can list all<a data-primary="endpoints for service, getting" data-type="indexterm" id="id1154"/> the endpoints that make up a service:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get ep/nginx -o json | jq '.subsets'</strong>&#13;
[&#13;
  {&#13;
    "addresses": [&#13;
      {&#13;
        "ip": "10.244.0.10",&#13;
        "nodeName": "minikube",&#13;
        "targetRef": {&#13;
          "kind": "Pod",&#13;
          "name": "nginx-748c667d99-x6h6h",&#13;
          "namespace": "default",&#13;
          "uid": "a0f3118f-32f5-4a65-8094-8e43979f7cec"&#13;
        }&#13;
      },&#13;
    ...&#13;
    ],&#13;
    "ports": [&#13;
      {&#13;
        "port": 80,&#13;
        "protocol": "TCP"&#13;
      }&#13;
    ]&#13;
  }&#13;
]&#13;
&#13;
</pre>&#13;
&#13;
<p>Now that you’ve seen <code>jq</code> in action, let’s move on to a method that doesn’t require external tooling—​that is, the built-in feature of using Go templates.<a data-primary="Go language" data-secondary="templates for text or data transformation" data-type="indexterm" id="id1155"/></p>&#13;
&#13;
<p>The Go programming language defines templates in a package called <code>text/template</code> that can be used for any kind of text or data transformation, and <code>kubectl</code> has built-in support for it. <a data-primary="kubectl" data-secondary="support for Go templates to transform text or data" data-type="indexterm" id="id1156"/><a data-primary="container images" data-secondary="listing all in current namespace" data-type="indexterm" id="id1157"/><a data-primary="namespaces" data-secondary="listing all container images in" data-type="indexterm" id="id1158"/>For example, to list all the container images used in the current namespace, do this:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl get pods -o go-template \&#13;
    --template="{{range .items}}{{range .spec.containers}}{{.image}} \&#13;
          {{end}}{{end}}"</strong>&#13;
fluent/fluentd:v1.16-1   nginx&#13;
</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id248">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>You may also want to take a look at JSONPath as an alternative way to parse the JSON produced by <code>kubectl</code>. <a data-primary="JSON" data-secondary="parsing with JSONPath" data-type="indexterm" id="id1159"/>It provides a syntax that can be considered more readable and easier to reason about. Examples can be found in the <a href="https://oreil.ly/muOnq">Kubernetes documentation</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1160">&#13;
<h2>See Also</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/Z7rul">The <code>jq</code> manual</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://jqplay.org">jqplay</a> to try out queries without installing <code>jq</code></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/qfQAO">The Go <code>template</code> package</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.5 Debugging Pods" data-type="sect1"><div class="sect1" id="debug_pods">&#13;
<h1>12.5 Debugging Pods</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id117">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You have a situation where a pod is either not getting to or remaining in the running state as expected or fails altogether after some time.<a data-primary="troubleshooting" data-secondary="understanding and parsing resource statuses" data-startref="ix_trblresstat" data-type="indexterm" id="id1161"/><a data-primary="status of resources, understanding and parsing" data-startref="ix_status" data-type="indexterm" id="id1162"/><a data-primary="resources" data-secondary="understanding and parsing statuses" data-startref="ix_resrcstat" data-type="indexterm" id="id1163"/><a data-primary="troubleshooting" data-secondary="debugging pods" data-type="indexterm" id="ix_trbldbgpod"/><a data-primary="pods" data-secondary="debugging" data-type="indexterm" id="ix_poddbg"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id118">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>To systematically discover and fix the cause of the <a data-primary="OODA loops" data-type="indexterm" id="id1164"/>problem, enter an <a href="https://oreil.ly/alw1o">OODA loop</a>:</p>&#13;
<ol>&#13;
<li>&#13;
<p><em>Observe</em>. What do you see in the container logs? What events have occurred? How is the network connectivity?</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Orient</em>. Formulate a set of plausible hypotheses—​stay as open-minded as possible and don’t jump to conclusions.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Decide</em>. Pick one of the hypotheses.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Act</em>. Test the hypothesis. If it’s confirmed, you’re done; otherwise, go back to step 1 and continue.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Let’s take a look at a concrete example where a pod fails. Create a manifest called <em>unhappy-pod.yaml</em> with this content:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">unhappy</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nevermind</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nevermind</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">shell</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">busybox:1.36</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">command</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"sh"</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"-c"</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"echo</code><code class="nv"> </code><code class="s">I</code><code class="nv"> </code><code class="s">will</code><code class="nv"> </code><code class="s">just</code><code class="nv"> </code><code class="s">print</code><code class="nv"> </code><code class="s">something</code><code class="nv"> </code><code class="s">here</code><code class="nv"> </code><code class="s">and</code><code class="nv"> </code><code class="s">then</code><code class="nv"> </code><code class="s">exit"</code><code class="w"/></pre>&#13;
&#13;
<p>Now when you launch that deployment and look at the pod it creates, you’ll see it’s unhappy:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl apply -f unhappy-pod.yaml</strong>&#13;
deployment.apps/unhappy created&#13;
&#13;
$ <strong>kubectl get pod -l app=nevermind</strong>&#13;
NAME                         READY   STATUS             RESTARTS      AGE&#13;
unhappy-576954b454-xtb2g     0/1     CrashLoopBackOff   2 (21s ago)   42s&#13;
&#13;
$ <strong>kubectl describe pod -l app=nevermind</strong>&#13;
Name:             unhappy-576954b454-xtb2g&#13;
Namespace:        default&#13;
Priority:         0&#13;
Service Account:  default&#13;
Node:             minikube/192.168.49.2&#13;
Start Time:       Thu, 13 Apr 2023 22:31:28 +0200&#13;
Labels:           app=nevermind&#13;
                  pod-template-hash=576954b454&#13;
Annotations:      &lt;none&gt;&#13;
Status:           Running&#13;
IP:               10.244.0.16&#13;
IPs:&#13;
  IP:           10.244.0.16&#13;
Controlled By:  ReplicaSet/unhappy-576954b454&#13;
...&#13;
Conditions:&#13;
  Type              Status&#13;
  Initialized       True&#13;
  Ready             False&#13;
  ContainersReady   False&#13;
  PodScheduled      True&#13;
Volumes:&#13;
  kube-api-access-bff5c:&#13;
    Type:                    Projected (a volume that contains injected data...)&#13;
    TokenExpirationSeconds:  3607&#13;
    ConfigMapName:           kube-root-ca.crt&#13;
    ConfigMapOptional:       &lt;nil&gt;&#13;
    DownwardAPI:             true&#13;
QoS Class:                   BestEffort&#13;
Node-Selectors:              &lt;none&gt;&#13;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exist...&#13;
                             node.kubernetes.io/unreachable:NoExecute op=Exist...&#13;
Events:&#13;
  Type     Reason     ...   Message&#13;
  ----     ------     ---   -------&#13;
  Normal   Scheduled  ...   Successfully assigned default/unhappy-576954b454-x...&#13;
  Normal   Pulled     ...   Successfully pulled image "busybox" in 2.945704376...&#13;
  Normal   Pulled     ...   Successfully pulled image "busybox" in 1.075044917...&#13;
  Normal   Pulled     ...   Successfully pulled image "busybox" in 1.119703875...&#13;
  Normal   Pulling    ...   Pulling image "busybox"&#13;
  Normal   Created    ...   Created container shell&#13;
  Normal   Started    ...   Started container shell&#13;
  Normal   Pulled     ...   Successfully pulled image "busybox" in 1.055005126...&#13;
  Warning  BackOff    ...   Back-off restarting failed container shell in pod...&#13;
</pre>&#13;
&#13;
<p>As you can see at the bottom of the description, in the <code>Events</code> section, Kubernetes considers this pod as not ready to serve traffic because “Back-off restarting failed…​.”</p>&#13;
&#13;
<p>Another way to observe this is by using the Kubernetes dashboard to view the deployment (<a data-type="xref" href="#unhappy-deployment-screenshot">Figure 12-1</a>), as well as the supervised replica set and the pod (<a data-type="xref" href="#unhappy-pod-events-screenshot">Figure 12-2</a>). With Minikube you can easily open the dashboard by <a data-primary="Minikube" data-secondary="dashboard command" data-type="indexterm" id="id1165"/>running the &#13;
<span class="keep-together">command</span> <code>minikube dashboard</code>.</p>&#13;
&#13;
<figure><div class="figure" id="unhappy-deployment-screenshot">&#13;
<img alt="Screen Shot Of Deployment In Error State" src="assets/kcb2_1201.png"/>&#13;
<h6><span class="label">Figure 12-1. </span>Deployment in error state</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="unhappy-pod-events-screenshot">&#13;
<img alt="Screen Shot Of Pod In Error State" src="assets/kcb2_1202.png"/>&#13;
<h6><span class="label">Figure 12-2. </span>Pod in error state</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id249">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>An issue, be it a pod failing or a node behaving strangely, can have many different causes. <a data-primary="nodes" data-secondary="behaving strangely, causes of" data-type="indexterm" id="id1166"/>Here are some things you’ll want to check before suspecting software bugs:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Is the manifest correct? Check with a tool such as <a href="https://oreil.ly/q_e39">Kubeconform</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Are you able to run the container locally outside of Kubernetes?</p>&#13;
</li>&#13;
<li>&#13;
<p>Can Kubernetes reach the container registry and actually pull the container image?</p>&#13;
</li>&#13;
<li>&#13;
<p>Can the nodes talk to each other?</p>&#13;
</li>&#13;
<li>&#13;
<p>Can the nodes reach the control plane?</p>&#13;
</li>&#13;
<li>&#13;
<p>Is DNS available in the cluster?</p>&#13;
</li>&#13;
<li>&#13;
<p>Are there sufficient resources available on the nodes, such as CPU, memory, and disk space?</p>&#13;
</li>&#13;
<li>&#13;
<p>Did you restrict the container or the namespace’s resource usage?</p>&#13;
</li>&#13;
<li>&#13;
<p>What are the events in the object description saying?</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1167">&#13;
<h2>See Also</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/nuThZ">“Debug Pods”</a> in the Kubernetes documentation</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/61xce">“Debug Running Pods”</a> in the Kubernetes documentation</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/XrF29">“Debug Services”</a> in the Kubernetes documentation</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/LD9oN">“Troubleshooting Clusters”</a> in the Kubernetes documentation</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.6 Influencing a Pod’s Start-up Behavior" data-type="sect1"><div class="sect1" id="init-containers">&#13;
<h1>12.6 Influencing a Pod’s Start-up Behavior</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id250">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>For your pod to function properly, it depends on some other service being available.<a data-primary="troubleshooting" data-secondary="debugging pods" data-startref="ix_trbldbgpod" data-type="indexterm" id="id1168"/><a data-primary="pods" data-secondary="debugging" data-startref="ix_poddbg" data-type="indexterm" id="id1169"/> You want to influence the pod’s start-up behavior so that it starts only once the pods it depends on are available.<a data-primary="troubleshooting" data-secondary="influencing start-up behavior of pods" data-type="indexterm" id="ix_trblstrtpod"/><a data-primary="pods" data-secondary="influencing start-up behavior" data-type="indexterm" id="ix_podstrt"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id119">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use <a href="https://oreil.ly/NWpRM">init containers</a> to influence the start-up behavior of a pod.<a data-primary="init containers" data-type="indexterm" id="id1170"/></p>&#13;
&#13;
<p>Imagine you want to launch an NGINX web server that depends on a backend service to serve content.<a data-primary="NGINX" data-secondary="influencing start-up behavior of nginx pod" data-type="indexterm" id="id1171"/> You therefore want to make sure that the NGINX pod starts up only once the backend service is up and running.</p>&#13;
&#13;
<p>First, create the backend service the web server depends on:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl create deployment backend --image=gcr.io/google-samples/hello-app:2.0</strong>&#13;
deployment.apps/backend created&#13;
$ <strong>kubectl expose deployment backend --port=80 --target-port=8080</strong>&#13;
</pre>&#13;
&#13;
<p>Then you can use the following manifest, <em>nginx-init-container.yaml</em>, to launch the NGINX instance and make sure it starts up only when the <code>backend</code> deployment is ready to accept connections:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w"/>&#13;
<code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">1</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">webserver</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:1.25.2</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">ports</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">containerPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">initContainers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">checkbackend</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">busybox:1.36</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">command</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">'sh'</code><code class="p-Indicator">,</code><code class="w"> </code><code class="s">'-c'</code><code class="p-Indicator">,</code><code class="w"> </code><code class="s">'until</code><code class="nv"> </code><code class="s">nc</code><code class="nv"> </code><code class="s">-w</code><code class="nv"> </code><code class="s">5</code><code class="nv"> </code><code class="s">backend.default.svc.cluster.local</code><code class="w"/>&#13;
<code class="w">                 </code><code class="s">80;</code><code class="nv"> </code><code class="s">do</code><code class="nv"> </code><code class="s">echo</code><code class="w"/>&#13;
<code class="w">                 </code><code class="s">"Waiting</code><code class="nv"> </code><code class="s">for</code><code class="nv"> </code><code class="s">backend</code><code class="nv"> </code><code class="s">to</code><code class="nv"> </code><code class="s">accept</code><code class="nv"> </code><code class="s">connections";</code><code class="nv"> </code><code class="s">sleep</code><code class="nv"> </code><code class="s">3;</code><code class="nv"> </code><code class="s">done;</code><code class="nv"> </code><code class="s">echo</code><code class="w"/>&#13;
<code class="w">                 </code><code class="s">"Backend</code><code class="nv"> </code><code class="s">is</code><code class="nv"> </code><code class="s">up,</code><code class="nv"> </code><code class="s">ready</code><code class="nv"> </code><code class="s">to</code><code class="nv"> </code><code class="s">launch</code><code class="nv"> </code><code class="s">web</code><code class="nv"> </code><code class="s">server"'</code><code class="p-Indicator">]</code><code class="w"/></pre>&#13;
&#13;
<p>Now you can launch the <code>nginx</code> deployment and verify whether the init container has done its job by looking at the logs of the pod it is supervising:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl apply -f nginx-init-container.yaml</strong>&#13;
deployment.apps/nginx created&#13;
&#13;
$ <strong>kubectl get po</strong>&#13;
NAME                       READY   STATUS    RESTARTS   AGE&#13;
backend-8485c64ccb-99jdh   1/1     Running   0          4m33s&#13;
nginx-779d9fcdf6-2ntpn     1/1     Running   0          32s&#13;
&#13;
$ <strong>kubectl logs nginx-779d9fcdf6-2ntpn -c checkbackend</strong>&#13;
Server:   10.96.0.10&#13;
Address:  10.96.0.10:53&#13;
&#13;
&#13;
Name: backend.default.svc.cluster.local&#13;
Address: 10.101.119.67&#13;
&#13;
Backend is up, ready to launch web server&#13;
</pre>&#13;
&#13;
<p>As you can see, the command in the init container indeed worked as planned.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id251">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>Init containers are useful to prevent your application from crash looping while it is waiting for a service to become available. For example, if you are deploying an application that needs to connect to a database server, you can configure an init container that checks and waits for the database server to become ready before your application attempts a connection with it.</p>&#13;
&#13;
<p>However, it is important to keep in mind that Kubernetes can technically kill a pod at any time, even after it was started successfully. Therefore, it is also important that you build resiliency into your application such that it can survive failures in other dependent services.<a data-primary="troubleshooting" data-secondary="influencing start-up behavior of pods" data-type="indexterm" id="id1172"/><a data-primary="pods" data-secondary="influencing start-up behavior" data-startref="ix_podstrt" data-type="indexterm" id="id1173"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.7 Getting a Detailed Snapshot of the Cluster State" data-type="sect1"><div class="sect1" id="cluster_dump">&#13;
<h1>12.7 Getting a Detailed Snapshot of the Cluster State</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id252">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You want to get a detailed snapshot of the overall cluster state for orientation, auditing, or troubleshooting purposes.<a data-primary="maintenance" data-secondary="getting detailed snapshot of cluster state" data-type="indexterm" id="ix_maintclstrst"/><a data-primary="clusters" data-secondary="getting detailed snapshot of cluster state" data-type="indexterm" id="ix_clstrstate"/><a data-primary="troubleshooting" data-secondary="getting detailed snapshot of cluster state" data-type="indexterm" id="ix_trblclsstate"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id120">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use the <code>kubectl cluster-info dump</code> command. <a data-primary="kubectl" data-secondary="cluster-info dump command" data-type="indexterm" id="id1174"/><a data-primary="cluster-info dump command (kubectl)" data-type="indexterm" id="id1175"/>For example, to create a dump of the cluster state in a subdirectory <em>cluster-state-2023-04-13</em>, do this:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>mkdir cluster-state-2023-04-13</strong>&#13;
&#13;
$ <strong>kubectl cluster-info dump --all-namespaces \&#13;
    --output-directory=cluster-state-2023-04-13</strong>&#13;
Cluster info dumped to cluster-state-2023-04-13&#13;
&#13;
$ <strong>tree ./cluster-state-2023-04-13</strong>&#13;
./cluster-state-2023-04-13&#13;
├── default&#13;
│   ├── daemonsets.json&#13;
│   ├── deployments.json&#13;
│   ├── es-598664765b-tpw59&#13;
│   │   └── logs.txt&#13;
│   ├── events.json&#13;
│   ├── fluentd-vw7d9&#13;
│   │   └── logs.txt&#13;
│   ├── jump&#13;
│   │   └── logs.txt&#13;
│   ├── kibana-5847789b45-bm6tn&#13;
│   │   └── logs.txt&#13;
    ...&#13;
├── ingress-nginx&#13;
│   ├── daemonsets.json&#13;
│   ├── deployments.json&#13;
│   ├── events.json&#13;
│   ├── ingress-nginx-admission-create-7qdjp&#13;
│   │   └── logs.txt&#13;
│   ├── ingress-nginx-admission-patch-cv6c6&#13;
│   │   └── logs.txt&#13;
│   ├── ingress-nginx-controller-77669ff58-rqdlq&#13;
│   │   └── logs.txt&#13;
│   ├── pods.json&#13;
│   ├── replicasets.json&#13;
│   ├── replication-controllers.json&#13;
│   └── services.json&#13;
├── kube-node-lease&#13;
│   ├── daemonsets.json&#13;
│   ├── deployments.json&#13;
│   ├── events.json&#13;
│   ├── pods.json&#13;
│   ├── replicasets.json&#13;
│   ├── replication-controllers.json&#13;
│   └── services.json&#13;
├── kube-public&#13;
│   ├── daemonsets.json&#13;
│   ├── deployments.json&#13;
│   ├── events.json&#13;
│   ├── pods.json&#13;
│   ├── replicasets.json&#13;
│   ├── replication-controllers.json&#13;
│   └── services.json&#13;
├── kube-system&#13;
│   ├── coredns-787d4945fb-9k8pn&#13;
│   │   └── logs.txt&#13;
│   ├── daemonsets.json&#13;
│   ├── deployments.json&#13;
│   ├── etcd-minikube&#13;
│   │   └── logs.txt&#13;
│   ├── events.json&#13;
│   ├── kube-apiserver-minikube&#13;
│   │   └── logs.txt&#13;
│   ├── kube-controller-manager-minikube&#13;
│   │   └── logs.txt&#13;
│   ├── kube-proxy-x6zdw&#13;
│   │   └── logs.txt&#13;
│   ├── kube-scheduler-minikube&#13;
│   │   └── logs.txt&#13;
│   ├── pods.json&#13;
│   ├── replicasets.json&#13;
│   ├── replication-controllers.json&#13;
│   ├── services.json&#13;
│   └── storage-provisioner&#13;
│       └── logs.txt&#13;
├── kubernetes-dashboard&#13;
│   ├── daemonsets.json&#13;
│   ├── dashboard-metrics-scraper-5c6664855-sztn5&#13;
│   │   └── logs.txt&#13;
│   ├── deployments.json&#13;
│   ├── events.json&#13;
│   ├── kubernetes-dashboard-55c4cbbc7c-ntjwk&#13;
│   │   └── logs.txt&#13;
│   ├── pods.json&#13;
│   ├── replicasets.json&#13;
│   ├── replication-controllers.json&#13;
│   └── services.json&#13;
└── nodes.json&#13;
&#13;
30 directories, 66 files&#13;
</pre>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.8 Adding Kubernetes Worker Nodes" data-type="sect1"><div class="sect1" id="adding_nodes">&#13;
<h1>12.8 Adding Kubernetes Worker Nodes</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id253">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You need to add a worker node to your Kubernetes cluster, for instance because you want to increase the capacity of your cluster.<a data-primary="maintenance" data-secondary="getting detailed snapshot of cluster state" data-startref="ix_maintclstrst" data-type="indexterm" id="id1176"/><a data-primary="clusters" data-secondary="getting detailed snapshot of cluster state" data-startref="ix_clstrstate" data-type="indexterm" id="id1177"/><a data-primary="troubleshooting" data-secondary="getting detailed snapshot of cluster state" data-startref="ix_trblclsstate" data-type="indexterm" id="id1178"/><a data-primary="worker nodes" data-secondary="adding to a cluster" data-type="indexterm" id="id1179"/><a data-primary="clusters" data-secondary="adding worker nodes to a cluster" data-type="indexterm" id="id1180"/><a data-primary="maintenance" data-secondary="adding worker nodes to a cluster" data-type="indexterm" id="id1181"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id121">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Provision a new machine in whatever way your environment requires (for example, in a bare-metal environment you might need to physically install a new server in a rack, in a public cloud setting you need to create a new VM, etc.), and then install, as daemons, the three components that make up a Kubernetes worker node:</p>&#13;
<dl>&#13;
<dt><code>kubelet</code></dt>&#13;
<dd>&#13;
<p>This is the node manager <a data-primary="kubelet" data-secondary="component of worker nodes" data-type="indexterm" id="id1182"/>and supervisor for all pods, no matter if they’re controlled by the API server or running locally, such as static pods. Note that the <code>kubelet</code> is the final arbiter of what pods can or cannot run on a given node, and it takes care of the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Reporting node and pod statuses to the API server</p>&#13;
</li>&#13;
<li>&#13;
<p>Periodically executing liveness probes</p>&#13;
</li>&#13;
<li>&#13;
<p>Mounting the pod volumes and downloading secrets</p>&#13;
</li>&#13;
<li>&#13;
<p>Controlling the container runtime (see the following)</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
<dt>Container runtime</dt>&#13;
<dd>&#13;
<p>This is responsible for downloading container images and running containers. <a data-primary="container runtimes" data-type="indexterm" id="id1183"/><a data-primary="Container Runtime Interface (CRI)" data-type="indexterm" id="id1184"/><a data-primary="CRI (Container Runtime Interface)" data-type="indexterm" id="id1185"/>Kubernetes requires the use of a runtime that conforms to the <a href="https://oreil.ly/6hmkR">Container Runtime Interface (CRI)</a>, like <a href="http://cri-o.io">cri-o</a>, <a href="https://docs.docker.com/engine">Docker Engine</a>, or <a href="https://containerd.io">containerd</a>.</p>&#13;
</dd>&#13;
<dt><code>kube-proxy</code></dt>&#13;
<dd>&#13;
<p>This process dynamically configures iptables rules on the node to enable the Kubernetes service abstraction (redirecting the VIP to the endpoints, one or more pods representing the service).<a data-primary="kube-proxy" data-type="indexterm" id="id1186"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p class="pagebreak-before">The actual installation of the components depends heavily on your environment and the installation method used (cloud, <code>kubeadm</code>, etc.). For a list of available options, see the <a href="https://oreil.ly/8XBRS"><code>kubelet</code> reference</a> and <a href="https://oreil.ly/mED8e"><code>kube-proxy</code> reference</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id122">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>Worker nodes, unlike other Kubernetes resources such as deployments or services, are not directly created by the Kubernetes control plane but only managed by it. <a data-primary="control plane" data-secondary="management of worker nodes" data-type="indexterm" id="id1187"/>That means when Kubernetes creates a node, it actually only creates an object that <em>represents</em> the worker node. It validates the node by health checks based on the node’s <code class="keep-together">metadata.name</code> field, and if the node is valid—​that is, all necessary components are running—​it is considered part of the cluster; otherwise, it will be ignored for any cluster activity until it becomes valid.<a data-primary="health checks" data-secondary="node, using metadata.name field" data-type="indexterm" id="id1188"/><a data-primary="metadata.name field (nodes)" data-type="indexterm" id="id1189"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1190">&#13;
<h2>See Also</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/MQ4ZV">“Nodes”</a> in the Kubernetes cluster architecture concepts</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/ePukq">“Communication Between Nodes and the Control Plane”</a> in the Kubernetes documentation</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/_OKBq">“Create Static Pods”</a> in the Kubernetes documentation</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="12.9 Draining Kubernetes Nodes for Maintenance" data-type="sect1"><div class="sect1" id="drain_nodes">&#13;
<h1>12.9 Draining Kubernetes Nodes for Maintenance</h1>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id254">&#13;
<h2>Problem</h2>&#13;
&#13;
<p>You need to carry out maintenance on a node—​for example, to apply a security patch or upgrade the operating system.<a data-primary="maintenance" data-secondary="draining Kubernetes nodes for" data-type="indexterm" id="ix_maintdrnnd"/><a data-primary="nodes" data-secondary="draining Kubernetes nodes for maintenance" data-type="indexterm" id="ix_nodedrain"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id123">&#13;
<h2>Solution</h2>&#13;
&#13;
<p>Use the <code>kubectl drain</code> command. <a data-primary="drain command (kubectl)" data-type="indexterm" id="id1191"/><a data-primary="kubectl" data-secondary="drain command" data-type="indexterm" id="id1192"/>For example, list nodes with <code>kubectl get nodes</code>, and then to do maintenance on node <code>123-worker</code>, do this:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>kubectl drain 123-worker</strong>&#13;
</pre>&#13;
&#13;
<p>When you are ready to put the node back into service, use <code>kubectl uncordon 123-worker</code>, which will make the node schedulable again.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id124">&#13;
<h2>Discussion</h2>&#13;
&#13;
<p>The <code>kubectl drain</code> command first marks the specified node as unschedulable to prevent new pods from arriving (essentially a <code>kubectl cordon</code>).<a data-primary="eviction" data-type="indexterm" id="id1193"/> Then it evicts the pods if the API server supports <a href="https://oreil.ly/xXLII">eviction</a>. Otherwise, it will use <code>kubectl delete</code> to delete the pods.<a data-primary="delete command (kubectl)" data-type="indexterm" id="id1194"/><a data-primary="kubectl" data-secondary="delete command" data-type="indexterm" id="id1195"/> The Kubernetes docs have a concise sequence diagram of the steps, reproduced in <a data-type="xref" href="#drain-node">Figure 12-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="drain-node">&#13;
<img alt="Node drain sequence diagram" src="assets/kcb2_1203.png"/>&#13;
<h6><span class="label">Figure 12-3. </span>Node drain sequence diagram</h6>&#13;
</div></figure>&#13;
&#13;
<p>The <code>kubectl drain</code> command evicts or deletes all pods except mirror pods (which cannot be deleted through the API server). <a data-primary="drain command (kubectl)" data-secondary="--ignore-daemonsets option" data-type="indexterm" id="id1196"/><a data-primary="DaemonSet object" data-secondary="pods supervised by, drain command and" data-type="indexterm" id="id1197"/>For pods supervised by a <code>DaemonSet</code>, <code>drain</code> will not proceed without using <code>--ignore-daemonsets</code>, and regardless it will not delete any <code>DaemonSet</code>-managed pods—​those pods would be immediately replaced by the <code>DaemonSet</code> controller, which ignores unschedulable markings.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p><code>drain</code> waits for graceful termination, so you should not operate on this node until the <code>kubectl drain</code> command has completed.<a data-primary="drain command (kubectl)" data-secondary="drain $NODE --force" data-type="indexterm" id="id1198"/> Note that <code>kubectl drain $NODE --force</code> will also evict pods not managed by a <code>ReplicationController</code>, <code>ReplicaSet</code>, <code>Job</code>, <code>DaemonSet</code>, or <code>StatefulSet</code>.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id255">&#13;
<h2>See Also</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/upbMl">“Safely Drain a Node”</a> in the Kubernetes documentation</p>&#13;
</li>&#13;
<li>&#13;
<p><code>kubectl drain</code> <a href="https://oreil.ly/YP6zg">reference docs</a><a data-primary="maintenance" data-secondary="draining Kubernetes nodes for" data-startref="ix_maintdrnnd" data-type="indexterm" id="id1199"/><a data-primary="nodes" data-secondary="draining Kubernetes nodes for maintenance" data-startref="ix_nodedrain" data-type="indexterm" id="id1200"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section></body></html>