<html><head></head><body><section data-pdf-bookmark="Chapter 11. Security, Backups, and Cluster Health" data-type="chapter" epub:type="chapter"><div class="chapter" id="security">&#13;
<h1><span class="label">Chapter 11. </span>Security, Backups, and Cluster Health</h1>&#13;
&#13;
<blockquote class="epigraph">&#13;
<p><a data-primary="Kubernetes" data-secondary="security" data-type="indexterm" id="ix_11-security-adoc0"/><a data-primary="security" data-type="indexterm" id="ix_11-security-adoc1"/>If you think technology can solve your security problems, then you don’t understand the problems and you don’t understand the technology.</p>&#13;
<p data-type="attribution">Bruce Schneier, <cite><em>Applied Cryptography</em></cite></p>&#13;
</blockquote>&#13;
&#13;
<p>In this chapter, we’ll explore the security and access control machinery in Kubernetes, including Role-Based Access Control (RBAC), outline some vulnerability scanning tools and services, and explain how to back up your Kubernetes data and state (and even more importantly, how to restore it). We’ll also look at some useful ways to get information about what’s happening in your cluster.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Access Control and Permissions" data-type="sect1"><div class="sect1" id="idm45979379538192">&#13;
<h1>Access Control and Permissions</h1>&#13;
&#13;
<p><a data-primary="access control" data-type="indexterm" id="ix_11-security-adoc2"/><a data-primary="permissions" data-type="indexterm" id="ix_11-security-adoc3"/>Small tech companies tend to start out with just a few employees, and everyone has administrator access on every system.</p>&#13;
&#13;
<p>As the organization grows, though, eventually it becomes clear that it is no longer a good idea for everyone to have administrator rights: it’s too easy for someone to make a mistake and change something they shouldn’t. The same applies to Kubernetes.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Managing Access by Cluster" data-type="sect2"><div class="sect2" id="idm45979379533712">&#13;
<h2>Managing Access by Cluster</h2>&#13;
&#13;
<p>One of the easiest and most effective things you can do to secure your Kubernetes cluster is limit who has access to it. There are generally two groups of people who need to access Kubernetes clusters: <em>cluster operators</em> and <em>application developers</em>, and they often need different permissions and privileges as part of their job function.</p>&#13;
&#13;
<p>Also, you may well have multiple deployment environments, such as production and staging. These separate environments will need different policies, depending on your organization. Production may be restricted to only some individuals, whereas staging may be open to a broader group of engineers.</p>&#13;
&#13;
<p>As we saw in <a data-type="xref" href="ch06.html#multiclusters">“Do I need multiple clusters?”</a>, it’s often a good idea to have separate clusters for production and staging or testing. If someone accidentally deploys something in staging that brings down the cluster nodes, it will not impact production.</p>&#13;
&#13;
<p>If one team should not have access to another team’s software and deployment process, each team could have their own dedicated cluster and not even have credentials on the other team’s clusters.</p>&#13;
&#13;
<p>This is certainly the most secure approach, but additional clusters come with tradeoffs. Each needs to be patched and monitored, and many small clusters tend to run less efficiently than larger clusters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Introducing Role-Based Access Control (RBAC)" data-type="sect2"><div class="sect2" id="rbac">&#13;
<h2>Introducing Role-Based Access Control (RBAC)</h2>&#13;
&#13;
<p><a data-primary="RBAC (Role-Based Access Control)" data-type="indexterm" id="ix_11-security-adoc4"/><a data-primary="Role-Based Access Control (RBAC)" data-type="indexterm" id="ix_11-security-adoc5"/>Another way you can manage access is by controlling who can perform certain operations inside the cluster, using Kubernetes’ Role-Based Access Control (RBAC) system.</p>&#13;
&#13;
<p>RBAC is designed to grant specific permissions to specific users (or service accounts, which are user accounts associated with automated systems). For example, you can grant the ability to list all Pods in the cluster to a particular user if they need it.</p>&#13;
&#13;
<p>The first and most important thing to know about RBAC is that it should be turned on. RBAC was introduced in Kubernetes 1.6 as an option when setting up clusters. However, whether this option is actually enabled in your cluster depends on your cloud provider or Kubernetes installer.</p>&#13;
&#13;
<p>If you’re running a self-hosted cluster, try this command to see whether or not RBAC is enabled on your cluster:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl describe pod -n kube-system -l component=kube-apiserver</code></strong><code class="go">&#13;
</code><code class="go">Name:         kube-apiserver-docker-for-desktop&#13;
</code><code class="go">Namespace:    kube-system&#13;
</code><code class="go">...&#13;
</code><code class="go">Containers:&#13;
</code><code class="go">  kube-apiserver:&#13;
</code><code class="go">      ...&#13;
</code><code class="go">    Command:&#13;
</code><code class="go">      kube-apiserver&#13;
</code><code class="go">      ...&#13;
</code><code class="go">      --authorization-mode=Node,RBAC</code></pre>&#13;
&#13;
<p>If <code>--authorization-mode</code> doesn’t contain <code>RBAC</code>, then RBAC is not enabled for your cluster. Check the documentation for the installer to see how to rebuild the cluster with RBAC enabled.</p>&#13;
&#13;
<p>Without RBAC, anyone with access to the cluster has the power to do anything, including running arbitrary code or deleting workloads. This probably isn’t what you want.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understanding Roles" data-type="sect2"><div class="sect2" id="idm45979379510176">&#13;
<h2>Understanding Roles</h2>&#13;
&#13;
<p>So, assuming you have RBAC enabled, how does it work? The most important concepts to understand are <em>users</em>, <em>roles</em>, and <em>role bindings</em>.</p>&#13;
&#13;
<p>Every time you connect to a Kubernetes cluster, you do so as a specific user. Exactly how you authenticate to the cluster depends on your provider; for example, in GKE, you use the <code>gcloud</code> tool to get an access token to a particular cluster. On EKS, you use your AWS IAM credentials. There are also service accounts in the cluster; for example, there is a default service account for each namespace. Users and service accounts can all have different sets of permissions.</p>&#13;
&#13;
<p>These are governed by Kubernetes <em>roles</em>. A role describes a specific set of permissions. Kubernetes includes some predefined roles to get you started. <a data-primary="cluster-admin role" data-type="indexterm" id="idm45979379505296"/>For example, the <code>cluster-admin</code> role, intended for superusers, is permitted access to read and change any resource in the cluster. <a data-primary="view role" data-type="indexterm" id="idm45979379481936"/>By contrast, the <code>view</code> role can list and examine most objects in a given namespace, but not modify them.</p>&#13;
&#13;
<p>You can define roles on the namespace level (using the <code>Role</code> object) or across the whole cluster (using the ClusterRole object). Here’s an example of a ClusterRole manifest that grants read access to Secrets in any namespace:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ClusterRole</code><code class="w"/>&#13;
<code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">secret-reader</code><code class="w"/>&#13;
<code class="nt">rules</code><code class="p">:</code><code class="w"/>&#13;
<code class="p-Indicator">-</code><code class="w"> </code><code class="nt">apiGroups</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">""</code><code class="p-Indicator">]</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">resources</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"secrets"</code><code class="p-Indicator">]</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">verbs</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"get"</code><code class="p-Indicator">,</code><code class="w"> </code><code class="s">"watch"</code><code class="p-Indicator">,</code><code class="w"> </code><code class="s">"list"</code><code class="p-Indicator">]</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Binding Roles to Users" data-type="sect2"><div class="sect2" id="idm45979379470992">&#13;
<h2>Binding Roles to Users</h2>&#13;
&#13;
<p><a data-primary="ClusterRoleBinding" data-type="indexterm" id="idm45979379401632"/><a data-primary="RoleBinding" data-type="indexterm" id="idm45979379400928"/>How do you associate a user with a role? You can do that using a role binding. Just like with roles, you can create a <code>RoleBinding</code> object that applies to a specific namespace, or a ClusterRoleBinding that applies at the cluster level.</p>&#13;
&#13;
<p><a data-primary="edit role" data-type="indexterm" id="idm45979379455344"/>Here’s the RoleBinding manifest that gives the <code>daisy</code> user the <code>edit</code> role in the <code>demo</code> namespace only:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">RoleBinding</code><code class="w"/>&#13;
<code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io/v1</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">daisy-edit</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo</code><code class="w"/>&#13;
<code class="nt">subjects</code><code class="p">:</code><code class="w"/>&#13;
<code class="p-Indicator">-</code><code class="w"> </code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">User</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">daisy</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">apiGroup</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io</code><code class="w"/>&#13;
<code class="nt">roleRef</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">ClusterRole</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">edit</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">apiGroup</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rbac.authorization.k8s.io</code><code class="w"/></pre>&#13;
&#13;
<p>In Kubernetes, permissions are <em>additive</em>; users start with no permissions, and you can add them using Roles and RoleBindings. You can’t subtract permissions from someone who already has them.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>You can read more about the details of RBAC, and the available roles and permissions, in the Kubernetes <a href="https://oreil.ly/SOxYN">documentation</a>.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Roles Do I Need?" data-type="sect2"><div class="sect2" id="idm45979379345808">&#13;
<h2>What Roles Do I Need?</h2>&#13;
&#13;
<p>So what roles and bindings should you set up in your cluster? The predefined roles <code>cluster-admin</code>, <code>edit</code>, and <code>view</code> will probably cover most requirements. To see what permissions a given role has, use the <code>kubectl describe</code> command:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><strong><code class="l-Scalar-Plain">kubectl</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">describe</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">clusterrole/edit</code></strong><code class="w">&#13;
</code><code class="l-Scalar-Plain">Name</code><code class="p-Indicator">:</code><code class="w">         </code><code class="l-Scalar-Plain">edit</code><code class="w">&#13;
</code><code class="nt">Labels</code><code class="p">:</code><code class="w">       </code><code class="l-Scalar-Plain">kubernetes.io/bootstrapping=rbac-defaults</code><code class="w">&#13;
</code><code class="nt">Annotations</code><code class="p">:</code><code class="w">  </code><code class="l-Scalar-Plain">rbac.authorization.kubernetes.io/autoupdate=true</code><code class="w">&#13;
</code><code class="nt">PolicyRule</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="l-Scalar-Plain">Resources</code><code class="l-Scalar-Plain">   </code><code class="l-Scalar-Plain">...</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">Verbs</code><code class="w">&#13;
</code><code class="w">  </code><code class="l-Scalar-Plain">---------</code><code class="l-Scalar-Plain">   </code><code class="l-Scalar-Plain">...</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">-----</code><code class="w">&#13;
</code><code class="w">  </code><code class="l-Scalar-Plain">bindings</code><code class="l-Scalar-Plain">    </code><code class="l-Scalar-Plain">...</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">[get</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">list</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">watch]</code><code class="w">&#13;
</code><code class="w">  </code><code class="l-Scalar-Plain">configmaps</code><code class="l-Scalar-Plain">  </code><code class="l-Scalar-Plain">...</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">[create</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">delete</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">deletecollection</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">get</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">list</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">patch</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">update</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">watch]</code><code class="w">&#13;
</code><code class="w">  </code><code class="l-Scalar-Plain">endpoints</code><code class="l-Scalar-Plain">   </code><code class="l-Scalar-Plain">...</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">[create</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">delete</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">deletecollection</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">get</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">list</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">patch</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">update</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">watch]</code><code class="w">&#13;
</code><code class="w">  </code><code class="l-Scalar-Plain">...</code></pre>&#13;
&#13;
<p>You could create roles for specific people or jobs within your organization (for example, a developer role), or individual teams (for example, QA or security).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Guard Access to cluster-admin" data-type="sect2"><div class="sect2" id="idm45979379187136">&#13;
<h2>Guard Access to cluster-admin</h2>&#13;
&#13;
<p>Be very careful about who has access to the <code>cluster-admin</code> role. This is the cluster superuser, equivalent to the root user on Unix systems. They can do anything to anything. Never give this role to users who are not cluster operators, and especially not to service accounts for apps that might be exposed to the internet, such as the Kubernetes Dashboard (see <a data-type="xref" href="#dashboard">“Kubernetes Dashboard”</a>).</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Don’t fix problems by granting <em>cluster-admin</em> unnecessarily. You’ll find some bad advice about this on sites like Stack Overflow. When faced with a Kubernetes permissions error, a common response is to grant the <code>cluster-admin</code> role to the application. <em>Don’t do this</em>. Yes, it makes the errors go away, but at the expense of bypassing all security checks and potentially opening up your cluster to an attacker. Instead, grant the application a role with the fewest privileges it needs to do its job.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Applications and Deployment" data-type="sect2"><div class="sect2" id="rbac-apps">&#13;
<h2>Applications and Deployment</h2>&#13;
&#13;
<p><a data-primary="service accounts, application default" data-type="indexterm" id="idm45979379201952"/>Apps running in Kubernetes usually don’t need any special RBAC permissions. Unless you specify otherwise, all Pods will run as the <code>default</code> service account in their namespace, which has no roles associated with it.</p>&#13;
&#13;
<p>If your app needs access to the Kubernetes API for some reason (for example, a monitoring tool that needs to list Pods), create a dedicated service account for the app, use a <code>RoleBinding</code> to associate it with the necessary role (for example, <code>view</code>), and limit it to specific namespaces.</p>&#13;
&#13;
<p>What about the permissions required to deploy applications to the cluster? The most secure way is to allow only a continuous deployment tool to deploy apps (see <a data-type="xref" href="ch14.html#continuous">Chapter 14</a>). It can use a dedicated service account, with permission to create and delete Pods in a particular namespace.</p>&#13;
&#13;
<p>The <code>edit</code> role is ideal for this. Users with the <code>edit</code> role can create and destroy resources in the namespace, but can’t create new roles or grant permissions to other users.</p>&#13;
&#13;
<p>If you don’t have an automated deployment tool, and developers have to deploy directly to the cluster, they will need edit rights to the appropriate namespaces too. Grant these on an application-by-application basis; don’t give anyone edit rights across the whole cluster. People who don’t need to deploy apps should have only the <code>view</code> role by default.</p>&#13;
&#13;
<p>Ideally, you should set up a centralized CI/CD deployment process so that developers do not need to directly deploy to Kubernetes. We will cover this more in <a data-type="xref" href="ch14.html#continuous">Chapter 14</a> and in <a data-type="xref" href="ch14.html#gitops">“GitOps”</a>.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Make sure RBAC is enabled in all your clusters. Give <code>cluster-admin</code> rights only to users who actually need the power to destroy everything in the cluster. If your app needs access to cluster resources, create a service account for it and bind it to a role with only the permissions it needs, in only the namespaces where it needs them.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="RBAC Troubleshooting" data-type="sect2"><div class="sect2" id="idm45979379147040">&#13;
<h2>RBAC Troubleshooting</h2>&#13;
&#13;
<p>If you’re running an older third-party application that isn’t RBAC-aware, or if you’re still working out the required permissions for your own application, you may run into RBAC permission errors. What do these look like?</p>&#13;
&#13;
<p>If an application makes an API request for something it doesn’t have permission to do (for example, list nodes) it will see a <em>Forbidden</em> error response (HTTP status 403) from the API server:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><code class="go">Error from server (Forbidden): nodes.metrics.k8s.io is forbidden: User</code>&#13;
<code class="go">"demo" cannot list nodes.metrics.k8s.io at the cluster scope.</code></pre>&#13;
&#13;
<p>If the application doesn’t log this information, or you’re not sure which application is failing, you can check the API server’s log (see <a data-type="xref" href="ch07.html#containerlogs">“Viewing a Container’s Logs”</a> for more about this). It will record messages like this, containing the string <code>RBAC DENY</code> with a description of the error:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl logs -n kube-system -l component=kube-apiserver | grep "RBAC DENY"</code></strong><code class="go">&#13;
</code><code class="go">RBAC DENY: user "demo" cannot "list" resource "nodes" cluster-wide</code></pre>&#13;
&#13;
<p>(You won’t be able to do this on a GKE cluster, or any other managed Kubernetes service that doesn’t give you access to the control plane: see the documentation for your Kubernetes provider to find out how to access API server logs.)</p>&#13;
&#13;
<p><code>kubectl</code> also includes a useful command for testing out permissions called <code>auth can-i</code>. This allows you to try out a Kubernetes operation using your current role, or you can test the permisisons of someone else to see if their role allows a particular command or not:</p>&#13;
&#13;
<pre data-type="programlisting">[source, console, subs="quotes"]&#13;
*kubectl auth can-i list secrets*&#13;
yes&#13;
*kubectl auth can-i create deployments --as test-user*&#13;
no</pre>&#13;
&#13;
<p>RBAC has a reputation for being complicated, but it’s really not. Just grant users the minimum privileges they need, keep <code>cluster-admin</code> safe, and you’ll be fine<a data-startref="ix_11-security-adoc5" data-type="indexterm" id="idm45979379106864"/><a data-startref="ix_11-security-adoc4" data-type="indexterm" id="idm45979379106272"/>.<a data-startref="ix_11-security-adoc3" data-type="indexterm" id="idm45979379105472"/><a data-startref="ix_11-security-adoc2" data-type="indexterm" id="idm45979379104768"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Security Scanning" data-type="sect1"><div class="sect1" id="cluster-security-scanning">&#13;
<h1>Cluster Security Scanning</h1>&#13;
&#13;
<p><a data-primary="cluster security" data-type="indexterm" id="ix_11-security-adoc6"/><a data-primary="clusters" data-secondary="security scanning" data-type="indexterm" id="ix_11-security-adoc7"/>In order to check for potential known security issues, there are tools for scanning your clusters that will let you know of any issues detected.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Gatekeeper/OPA" data-type="sect2"><div class="sect2" id="idm45979379087760">&#13;
<h2>Gatekeeper/OPA</h2>&#13;
&#13;
<p><a data-primary="cluster security" data-secondary="Gatekeeper/OPA" data-type="indexterm" id="idm45979379086512"/><a data-primary="Gatekeeper" data-type="indexterm" id="idm45979379085664"/><a data-primary="Open Policy Agent (OPA)" data-type="indexterm" id="idm45979379085056"/>In February of 2021, the CNCF graduated the <a href="https://www.openpolicyagent.org">Open Policy Agent (OPA)</a> project, meaning that it meets the standards and maturity required to be included alongside the other officially adopted CNCF projects. OPA is a <em>policy engine</em> tool that allows you to define security policies for any of your cloud native tools, including Kubernetes.</p>&#13;
&#13;
<p><a href="https://oreil.ly/vosvu">Gatekeeper</a> is a related project that takes the OPA engine and makes it run inside of Kubernetes as native resources.</p>&#13;
&#13;
<p>For example, using Gatekeeper you could add the following constraint to prevent containers in a given namespace from running if they are using the <code>latest</code> tag (see <a data-type="xref" href="ch08.html#latesttag">“The latest Tag”</a> for more info):</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">constraints.gatekeeper.sh/v1beta1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">K8sDisallowedTags</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">container-image-must-not-have-latest-tag</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">match</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">kinds</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">apiGroups</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">""</code><code class="p-Indicator">]</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">kinds</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"Pod"</code><code class="p-Indicator">]</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">namespaces</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"my-namespace"</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">parameters</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">tags</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="s">"latest"</code><code class="p-Indicator">]</code><code class="w"/></pre>&#13;
&#13;
<p>There are many other Kubernetes-specific policies maintained in the <a href="https://oreil.ly/PPDYg"><code>gatekeeper-library</code> repo</a>. Other examples include ensuring that all Pods have a known set of labels, or forcing containers to come from a list of trusted registry sources. You can also consider using OPA for auditing and securing your non-Kubernetes cloud resources.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="kube-bench" data-type="sect2"><div class="sect2" id="idm45979379053168">&#13;
<h2>kube-bench</h2>&#13;
&#13;
<p><a data-primary="cluster security" data-secondary="kube-bench" data-type="indexterm" id="idm45979379004688"/><a data-primary="kube-bench" data-type="indexterm" id="idm45979379003712"/><a href="https://oreil.ly/IFV8p">kube-bench</a> is a tool for auditing your Kubernetes cluster against a set of benchmarks produced by the Center for Internet Security (CIS). In effect, it verifies that your cluster is set up according to security best practices. Although you probably won’t need to, you can configure the tests that kube-bench runs, and even add your own, specified as YAML documents.</p>&#13;
&#13;
<p>Similar to <a data-type="xref" href="ch06.html#sonobuoy">“Conformance Testing with Sonobuoy”</a>, kube-bench runs when you deploy a Job to Kubernetes and inspect the results. Download the appropriate Job YAML file for your cluster, based on the <a href="https://oreil.ly/8zI5c">repository docs</a>, and install it to your cluster:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl apply -f job.yaml</code></strong><code class="go">&#13;
</code><code class="go">job.batch/kube-bench created&#13;
</code><code class="go">&#13;
</code><strong><code class="go">kubectl logs job/kube-bench</code></strong><code class="go">&#13;
</code><code class="go">...&#13;
</code><code class="go">== Summary total ==&#13;
</code><code class="go">63 checks PASS&#13;
</code><code class="go">13 checks FAIL&#13;
</code><code class="go">46 checks WARN&#13;
</code><code class="go">0 checks INFO&#13;
</code><code class="go">...</code></pre>&#13;
&#13;
<p>You can then read through the logs to get the specifics about any warnings or failures that kube-bench has found.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubescape" data-type="sect2"><div class="sect2" id="idm45979378957856">&#13;
<h2>Kubescape</h2>&#13;
&#13;
<p><a data-primary="cluster security" data-secondary="Kubescape" data-type="indexterm" id="idm45979378956960"/><a data-primary="Kubescape" data-type="indexterm" id="idm45979378956080"/>Another tool in this space is called <a href="https://oreil.ly/Bt9be">Kubescape</a>, and it checks to see if your clusters are secure according to recent standards defined in the <a href="https://oreil.ly/vT8Kl">CIS Benchmark</a>. It will let you know about containers that are running as root, any potentially insecure exposed ports, check that your authentication and audit log settings are configured securely, and other similar items defined in the CIS checklist.<a data-startref="ix_11-security-adoc7" data-type="indexterm" id="idm45979378953568"/><a data-startref="ix_11-security-adoc6" data-type="indexterm" id="idm45979378952864"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Container Security Scanning" data-type="sect1"><div class="sect1" id="idm45979378951936">&#13;
<h1>Container Security Scanning</h1>&#13;
&#13;
<p><a data-primary="containers" data-secondary="security scanning" data-type="indexterm" id="ix_11-security-adoc8"/>If you’re running third-party software in your cluster, it’s wise to check it for security problems and malware. But even your own containers may have software in them that you’re not aware of, and that needs to be checked too.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Clair" data-type="sect2"><div class="sect2" id="idm45979378926560">&#13;
<h2>Clair</h2>&#13;
&#13;
<p><a data-primary="Clair" data-type="indexterm" id="idm45979378925312"/><a href="https://oreil.ly/RDSHS">Clair</a> is an open source container scanner produced by the CoreOS project. It statically analyzes container images, before they are actually run, to see if they contain any software or versions that are known to be insecure.</p>&#13;
&#13;
<p>You can run Clair manually to check specific images for problems, or integrate it into your CD pipeline to test all images before they are deployed (see <a data-type="xref" href="ch14.html#continuous">Chapter 14</a>).</p>&#13;
&#13;
<p>Alternatively, Clair can hook into your container registry to scan any images that are pushed to it and report problems.</p>&#13;
&#13;
<p>It’s worth mentioning that you shouldn’t automatically trust base images, such as <code>alpine</code>. Clair is preloaded with security checks for many popular base images, and will tell you immediately if you’re using one that has a known vulnerability.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Aqua" data-type="sect2"><div class="sect2" id="idm45979378921360">&#13;
<h2>Aqua</h2>&#13;
&#13;
<p><a data-primary="Aqua" data-type="indexterm" id="idm45979378919808"/><a href="https://oreil.ly/rLSPJ">Aqua’s Container Security Platform</a> is a full-service commercial container security offering, allowing organizations to scan containers for vulnerabilities, malware, and suspicious activity, as well as providing policy enforcement and regulatory <span class="keep-together">compliance.</span></p>&#13;
&#13;
<p>As you’d expect, Aqua’s platform integrates with your container registry, CI/CD pipeline, and multiple orchestration systems, including Kubernetes.</p>&#13;
&#13;
<p>Aqua also offers <a href="https://oreil.ly/T4bSS">Trivy</a>, a free-to-use tool that you can add to your container images to scan installed packages for known vulnerabilities from the same database that the Aqua Security Platform uses.</p>&#13;
&#13;
<p>If you want Trivy to scan a particular Docker image, install the CLI tool and run:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">trivy image [YOUR_CONTAINER_IMAGE_NAME]</code></strong></pre>&#13;
&#13;
<p>You can also use it to scan for security issues and misconfigurations in your Dockerfiles, Terraform files, and even your Kubernetes manifests:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">trivy config [YOUR_CODE_DIR]</code></strong></pre>&#13;
&#13;
<p><a data-primary="kube-hunter" data-type="indexterm" id="idm45979378887776"/>Another handy open source tool from Aqua is <a href="https://kube-hunter.aquasec.com">kube-hunter</a>, designed to find security issues in your Kubernetes cluster itself. If you run it as a container on a machine outside your cluster, as an attacker might, it will check for various kinds of problems: exposed email addresses in certificates, unsecured dashboards, open ports and endpoints, and so on.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Anchore Engine" data-type="sect2"><div class="sect2" id="idm45979378886032">&#13;
<h2>Anchore Engine</h2>&#13;
&#13;
<p><a data-primary="Anchore Engine" data-type="indexterm" id="idm45979378884832"/>The <a href="https://oreil.ly/O0ik7">Anchore Engine</a> is an open source tool for scanning container images, not only for known vulnerabilities, but to identify the <em>bill of materials</em> of everything present in the container, including libraries, configuration files, and file permissions. You can use this to verify containers against user-defined policies: for example, you can block any images that contain security credentials, or application source code.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Synk" data-type="sect2"><div class="sect2" id="idm45979378882512">&#13;
<h2>Synk</h2>&#13;
&#13;
<p><a data-primary="Synk" data-type="indexterm" id="idm45979378881072"/>Docker partnered with <a href="https://snyk.io">Synk</a> to add vulnerability scanning directly into the <code>docker</code> CLI tool. You can scan any image using the <code>docker scan</code> command:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">docker scan golang:1.17-alpine</code></strong><code class="go">&#13;
</code><code class="go">&#13;
</code><code class="go">Testing golang:1.17-alpine...&#13;
</code><code class="go">&#13;
</code><code class="go">Package manager:   apk&#13;
</code><code class="go">Project name:      docker-image|golang&#13;
</code><code class="go">Docker image:      golang:1.17-alpine&#13;
</code><code class="go">Platform:          linux/amd64&#13;
</code><code class="go">Base image:        golang:1.17.1-alpine3.14&#13;
</code><code class="go">&#13;
</code><code class="go">✓ Tested 15 dependencies for known vulnerabilities, no vulnerable paths found.</code></pre>&#13;
&#13;
<p>Synk will tell you about known security issues and deprecations found in the image:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">docker scan golang:1.14-alpine</code></strong><code class="go">&#13;
</code><code class="go">...&#13;
</code><code class="go">Tested 15 dependencies for known vulnerabilities, found 10 vulnerabilities.&#13;
</code><code class="go">&#13;
</code><code class="go">Base Image          Vulnerabilities  Severity&#13;
</code><code class="go">golang:1.14-alpine  10               2 critical, 5 high, 2 medium, 1 low&#13;
</code><code class="go">&#13;
</code><code class="go">Recommendations for base image upgrade:&#13;
</code><code class="go">&#13;
</code><code class="go">Minor upgrades&#13;
</code><code class="go">Base Image            Vulnerabilities  Severity&#13;
</code><code class="go">golang:1.17.0-alpine  0                0 critical, 0 high, 0 medium, 0 low</code></pre>&#13;
&#13;
<p>This can be a quick and easy way to add some basic security scanning into your Docker workflow as well as your CI/CD pipelines.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Don’t run containers from untrusted sources, or when you’re not sure what’s in them. Run a scanning tool like Clair or Synk over all containers, especially those you build yourself, to make sure there are no known vulnerabilities in any of the base images or dependencies<a data-startref="ix_11-security-adoc8" data-type="indexterm" id="idm45979378821136"/>.<a data-startref="ix_11-security-adoc1" data-type="indexterm" id="idm45979378820336"/><a data-startref="ix_11-security-adoc0" data-type="indexterm" id="idm45979378819632"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Backups" data-type="sect1"><div class="sect1" id="idm45979378818704">&#13;
<h1>Backups</h1>&#13;
&#13;
<p><a data-primary="backups" data-type="indexterm" id="ix_11-security-adoc9"/><a data-primary="Kubernetes" data-secondary="backups and restoration" data-type="indexterm" id="ix_11-security-adoc10"/>You might be wondering whether you still need backups in cloud native architectures. After all, Kubernetes is inherently reliable and can handle the loss of several nodes at once, without losing state or even degrading application performance too much.</p>&#13;
&#13;
<p>Also, Kubernetes is a declarative IaC system. All Kubernetes resources are described by data stored in a reliable database (<code>etcd</code>). In the event of some Pods being accidentally deleted, their supervising Deployment will re-create them from the spec held in the database.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Do I Need to Back Up Kubernetes?" data-type="sect2"><div class="sect2" id="idm45979378790544">&#13;
<h2>Do I Need to Back Up Kubernetes?</h2>&#13;
&#13;
<p>So do you still need backups? Well, yes. The data stored on persistent volumes, for example, is vulnerable to failure (see <a data-type="xref" href="ch08.html#persistent">“Persistent Volumes”</a>). While your cloud vendor may provide nominally high-availability volumes (replicating the data across two different availability zones, for example), that’s not the same as backup.</p>&#13;
&#13;
<p>Let’s repeat that point, because it’s not obvious:</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p><a data-primary="replication" data-type="indexterm" id="idm45979378786992"/><em>Replication is not backup</em>. While replication may protect you from the failure of the underlying storage volume, it won’t protect you from accidentally deleting the volume by mis-clicking in a web console, for example.</p>&#13;
</div>&#13;
&#13;
<p>Nor will replication prevent a misconfigured application from overwriting its data, or an operator from running a command with the wrong environment variables and accidentally dropping the production database instead of the development one. (<a href="https://oreil.ly/0bxEk">This has happened</a>, probably more often than anyone’s willing to admit.<sup><a data-type="noteref" href="ch11.html#idm45979378784352" id="idm45979378784352-marker">1</a></sup>)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Backing Up etcd" data-type="sect2"><div class="sect2" id="idm45979378782736">&#13;
<h2>Backing Up etcd</h2>&#13;
&#13;
<p><a data-primary="etcd" data-type="indexterm" id="idm45979378781360"/>As we saw in <a data-type="xref" href="ch03.html#highavailability">“High Availability”</a>, Kubernetes stores all its state in the <code>etcd</code> database, so any failure or data loss here could be catastrophic. That’s one very good reason why we recommend that you use managed services that guarantee the availability of <code>etcd</code> and the control plane generally (see <a data-type="xref" href="ch03.html#usemanaged">“Use Managed Kubernetes if You Can”</a>).</p>&#13;
&#13;
<p>If you run your own control plane nodes, you are responsible for managing <code>etcd</code> clustering and replication. Even with regular data snapshots, it still takes a certain amount of time to retrieve and verify the snapshot, rebuild the cluster, and restore the data. During this time, your cluster will likely be unavailable or seriously degraded.</p>&#13;
&#13;
<p>This is why it is critical that you keep your Kubernetes manifests and Helm charts in source control, and implement efficient deployment processes so that you can get your clusters back up and running quickly if you do have an issue with <code>etcd</code>. We will cover this more in <a data-type="xref" href="ch14.html#continuous">Chapter 14</a>.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use a managed or turnkey service provider to run your control plane nodes with <code>etcd</code> clustering and backups. If you run them yourself, be very sure you know what you’re doing. Resilient <code>etcd</code> management is a specialist job, and the consequences of getting it wrong can be serious.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Backing Up Resource State" data-type="sect2"><div class="sect2" id="idm45979378772224">&#13;
<h2>Backing Up Resource State</h2>&#13;
&#13;
<p>Apart from <code>etcd</code> failures, there is also the question of saving the state of your individual resources. If you delete the wrong Deployment, for example, how would you re-create it?</p>&#13;
&#13;
<p>Throughout this book we emphasize the value of the <em>infrastructure as code</em> paradigm, and recommend that you always manage your Kubernetes resources declaratively, by applying YAML manifests or Helm charts stored in version control.</p>&#13;
&#13;
<p>In theory, then, to re-create the total state of your cluster workloads, you should be able to check out the relevant version control repos and apply all the resources in them. <em>In theory.</em></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Backing Up Cluster State" data-type="sect2"><div class="sect2" id="idm45979378768688">&#13;
<h2>Backing Up Cluster State</h2>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="backing up cluster state" data-type="indexterm" id="ix_11-security-adoc11"/>In practice, not everything you have in version control is running in your cluster right now. Some apps may have been taken out of service, or replaced by newer versions. Some may not be ready to deploy.</p>&#13;
&#13;
<p>We’ve recommended throughout this book that you should avoid making direct changes to resources, and instead apply changes from the updated manifest files (see <a data-type="xref" href="ch07.html#dontuseimperative">“When Not to Use Imperative Commands”</a>). However, people don’t always follow good advice.</p>&#13;
&#13;
<p>In any case, it’s likely that during initial deployment and testing of apps, engineers may be adjusting settings like replica count and node affinities on the fly, and only storing them in version control once they’ve arrived at the right values.</p>&#13;
&#13;
<p>Supposing your cluster were to be shut down completely, or have all its resources deleted (hopefully an unlikely scenario, but a useful thought experiment). How quickly could you re-create it?</p>&#13;
&#13;
<p>Even if you have an admirably well-designed and up-to-date cluster automation system that can redeploy everything to a fresh cluster, how do you <em>know</em> that the state of this cluster matches the one that was lost?</p>&#13;
&#13;
<p>One way to help ensure this is to make a snapshot of the running cluster, which you can refer to later in case of problems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Large and Small Disasters" data-type="sect2"><div class="sect2" id="idm45979378761312">&#13;
<h2>Large and Small Disasters</h2>&#13;
&#13;
<p>It’s not very likely that you’d lose the whole cluster: thousands of Kubernetes contributors have worked hard to make sure that doesn’t happen.</p>&#13;
&#13;
<p>What’s more likely is that you (or your newest team member) might delete a namespace by accident, shut down a Deployment without meaning to, or specify the wrong set of labels to a <code>kubectl delete</code> command, removing more than you intended.</p>&#13;
&#13;
<p>Whatever the cause, disasters do happen, so let’s look at a backup tool that can help you avoid them.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Velero" data-type="sect2"><div class="sect2" id="velero">&#13;
<h2>Velero</h2>&#13;
&#13;
<p><a data-primary="Velero" data-type="indexterm" id="ix_11-security-adoc12"/><a href="https://velero.io">Velero</a> (formerly known as Ark) is a free and open source tool that can<a data-primary="Velero" data-secondary="function of" data-type="indexterm" id="idm45979378754736"/> back up and restore your cluster state and persistent data.</p>&#13;
&#13;
<p>Velero runs in your cluster and connects to a cloud storage service of your choice (for example, Amazon S3 or Azure Storage).</p>&#13;
&#13;
<p>Go to the <a href="https://velero.io/docs/main">velero.io website</a> for the instructions for setting up Velero on your <span class="keep-together">platform.</span></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Configuring Velero" data-type="sect3"><div class="sect3" id="idm45979378751152">&#13;
<h3>Configuring Velero</h3>&#13;
&#13;
<p><a data-primary="Velero" data-secondary="configuring" data-type="indexterm" id="idm45979378749600"/>Before you use Velero, you need to create a <code>BackupStorageLocation</code> object in your Kubernetes cluster, telling it where to store backups (for example, an AWS S3 cloud storage bucket). Here’s an example that configures Velero to back up to the <code>demo-backup</code> bucket:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">velero.io/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">BackupStorageLocation</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">default</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">velero</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">provider</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">aws</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">objectStorage</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">bucket</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">demo-backup</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">config</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">region</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">us-east-1</code><code class="w"/></pre>&#13;
&#13;
<p>You must have at least a storage location called <code>default</code>, though you can add others with any names you like.</p>&#13;
&#13;
<p>Velero can also back up the contents of your persistent volumes. To tell it where to store them, you need to create a <code>VolumeSnapshotLocation</code> object:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">velero.io/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">VolumeSnapshotLocation</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">aws-default</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">namespace</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">velero</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">provider</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">aws</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">config</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">region</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">us-east-1</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating a Velero backup" data-type="sect3"><div class="sect3" id="idm45979378721808">&#13;
<h3>Creating a Velero backup</h3>&#13;
&#13;
<p>When you create a backup using the <code>velero backup</code> command, the Velero server queries the Kubernetes API to retrieve the resources matching the selector you provided (by default, it backs up all resources). <a data-primary="Velero" data-secondary="commands" data-tertiary="backup create" data-type="indexterm" id="idm45979378620128"/>You can back up a set of namespaces, or the whole cluster:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">velero backup create demo-backup --include-namespaces demo</code></strong></pre>&#13;
&#13;
<p>It will then export all these resources to a named file in your cloud storage bucket, according to your configured BackupStorageLocation. The metadata and contents of your persistent volumes will also be backed up to your configured VolumeSnapshotLocation.</p>&#13;
&#13;
<p>Alternatively, you can back up everything in your cluster <em>except</em> specified namespaces (for example, <code>kube-system</code>). You can also schedule automatic backups: for example, you can have Velero back up your cluster nightly, or even hourly.</p>&#13;
&#13;
<p>Each Velero backup is complete in itself, not an incremental backup. So, to restore a backup, you only need the most recent backup file.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Restoring data" data-type="sect3"><div class="sect3" id="idm45979378614496">&#13;
<h3>Restoring data</h3>&#13;
&#13;
<p><a data-primary="Velero" data-secondary="commands" data-tertiary="backup download" data-type="indexterm" id="idm45979378613216"/><a data-primary="Velero" data-secondary="commands" data-tertiary="backup get" data-type="indexterm" id="idm45979378611968"/>You can list your available backups using the <code>velero backup get</code> command. And to see what’s in a particular backup, use <code>velero backup download</code>:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">velero backup download demo-backup</code></strong><code class="go">&#13;
</code><code class="go">Backup demo-backup has been successfully downloaded to&#13;
</code><code class="gp">$</code><code>PWD/demo-backup-data.tar.gz</code></pre>&#13;
&#13;
<p>The downloaded file is a <em>tar.gz</em> archive that you can unpack and inspect using standard tools. If you only want the manifest for a specific resource, for example, you can extract it from the backup file and restore it individually with <code>kubectl apply -f</code>.</p>&#13;
&#13;
<p><a data-primary="Velero" data-secondary="commands" data-tertiary="restore" data-type="indexterm" id="idm45979378554416"/>To restore the whole backup, the <code>velero restore</code> command will start the process, and Velero will re-create all the resources and volumes described in the specified snapshot, skipping anything that already exists.</p>&#13;
&#13;
<p>If the resource <em>does</em> exist, but is different from the one in the backup, Velero will warn you, but not overwrite the existing resource. So, for example, if you want to reset the state of a running Deployment to the way it was in the most recent snapshot, delete the running Deployment first, then restore it with Velero.</p>&#13;
&#13;
<p>Alternatively, if you’re restoring a backup of a namespace, you can delete the namespace first, and then restore the backup.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Restore procedures and tests" data-type="sect3"><div class="sect3" id="idm45979378550960">&#13;
<h3>Restore procedures and tests</h3>&#13;
&#13;
<p>You should write a detailed, step-by-step procedure describing how to restore data from backups, and make sure all staff know where to find this document. When a disaster happens, it’s usually at an inconvenient time, the key people aren’t available, everyone’s in a panic, and your procedure should be so clear and precise that it can be carried out by someone who isn’t familiar with Velero or even Kubernetes.</p>&#13;
&#13;
<p>Each month, run a restore test by having a different team member execute the restore procedure against a temporary cluster. This verifies that your backups are good and that the restore procedure is correct, and makes sure everyone is familiar with how to do it.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scheduling Velero backups" data-type="sect3"><div class="sect3" id="idm45979378548240">&#13;
<h3>Scheduling Velero backups</h3>&#13;
&#13;
<p>All backups should be automated, and Velero is no exception. <a data-primary="Velero" data-secondary="commands" data-tertiary="schedule create" data-type="indexterm" id="idm45979378527264"/>You can schedule a regular backup using the <code>velero schedule create</code> command:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">velero schedule create demo-schedule --schedule="0 1 * * *" --include-namespaces&#13;
</code><code class="go">demo</code></strong><code class="go">&#13;
</code><code class="go">Schedule "demo-schedule" created successfully.</code></pre>&#13;
&#13;
<p>The <code>schedule</code> argument specifies when to run the backup, in Unix <code>cron</code> format (see <a data-type="xref" href="ch09.html#cronjobs">“CronJobs”</a>). In the example, <code>0 1 * * *</code> runs the backup at 01:00 every day.</p>&#13;
&#13;
<p><a data-primary="Velero" data-secondary="commands" data-tertiary="schedule get" data-type="indexterm" id="idm45979378518736"/>To see what backups you have scheduled, use <code>velero schedule get</code>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other uses for Velero" data-type="sect3"><div class="sect3" id="idm45979378516784">&#13;
<h3>Other uses for Velero</h3>&#13;
&#13;
<p>While Velero is extremely useful for disaster recovery, you can also use it to migrate resources and data from one cluster.</p>&#13;
&#13;
<p>Making regular Velero backups can also help you understand how your Kubernetes usage is changing over time, comparing the current state to the state a month ago, six months ago, and a year ago, for example.</p>&#13;
&#13;
<p>The snapshots can also be a useful source of audit information: for example, finding out what was running in your cluster at a given date or time, and how and when the cluster state changed.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use Velero to back up your cluster state and persistent data regularly: at least nightly. Run a restore test at least <a data-startref="ix_11-security-adoc12" data-type="indexterm" id="idm45979378493808"/>monthly<a data-startref="ix_11-security-adoc11" data-type="indexterm" id="idm45979378493072"/>.<a data-startref="ix_11-security-adoc10" data-type="indexterm" id="idm45979378492240"/><a data-startref="ix_11-security-adoc9" data-type="indexterm" id="idm45979378491536"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Monitoring Cluster Status" data-type="sect1"><div class="sect1" id="clustermonitoring">&#13;
<h1>Monitoring Cluster Status</h1>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="monitoring cluster status" data-type="indexterm" id="ix_11-security-adoc13"/><a data-primary="Kubernetes" data-secondary="cluster monitoring" data-type="indexterm" id="ix_11-security-adoc14"/>Monitoring cloud native applications is a big topic, which, as we’ll see in <a data-type="xref" href="ch15.html#observability">Chapter 15</a>, includes things like observability, metrics, logging, tracing, and traditional closed-box monitoring.</p>&#13;
&#13;
<p>However, in this chapter, we’ll be concerned only with monitoring the Kubernetes cluster itself: the health of the cluster, the status of individual nodes, and the utilization of the cluster and the progress of its workloads.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="kubectl" data-type="sect2"><div class="sect2" id="idm45979378484800">&#13;
<h2>kubectl</h2>&#13;
&#13;
<p>We introduced the invaluable <code>kubectl</code> command in <a data-type="xref" href="ch02.html#firststeps">Chapter 2</a>, but we haven’t yet exhausted its possibilities. As well as being a general administration tool for Kubernetes resources, <code>kubectl</code> can also report useful information about the state of the cluster components.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Control plane status" data-type="sect3"><div class="sect3" id="idm45979378481360">&#13;
<h3>Control plane status</h3>&#13;
&#13;
<p><a data-primary="control plane" data-secondary="status information" data-type="indexterm" id="idm45979378479984"/><a data-primary="kubectl" data-secondary="commands" data-tertiary="get componentstatuses" data-type="indexterm" id="idm45979378479008"/>The <code>kubectl get componentstatuses</code> command (or <code>kubectl get cs</code> for short) gives health information for the control plane components—the scheduler, the controller manager, and <code>etcd</code>:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get componentstatuses</code></strong><code class="go">&#13;
</code><code class="go">NAME                 STATUS    MESSAGE              ERROR&#13;
</code><code class="go">controller-manager   Healthy   ok&#13;
</code><code class="go">scheduler            Healthy   ok&#13;
</code><code class="go">etcd-0               Healthy   {"health": "true"}</code></pre>&#13;
&#13;
<p>If there were a serious problem with any of the control plane components, it would soon become apparent anyway, but it’s still handy to be able to check and report on them, as a sort of top-level health indicator for the cluster.</p>&#13;
&#13;
<p>If any of your control plane components is not in a <code>Healthy</code> state, it will need to be fixed. This should never be the case with a managed Kubernetes service, but for self-hosted clusters, you will have to take care of this yourself.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Node status" data-type="sect3"><div class="sect3" id="idm45979378468304">&#13;
<h3>Node status</h3>&#13;
&#13;
<p><a data-primary="kubectl" data-secondary="commands" data-tertiary="get nodes" data-type="indexterm" id="idm45979378466896"/><a data-primary="nodes" data-secondary="status" data-type="indexterm" id="idm45979378465648"/>Another useful command is <code>kubectl get nodes</code>, which will list all the nodes in your cluster, and report their status and Kubernetes version:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get nodes</code></strong><code class="go">&#13;
</code><code class="go">NAME             STATUS   ROLES                  AGE   VERSION&#13;
</code><code class="go">docker-desktop   Ready    control-plane,master   24d   v1.21.4</code></pre>&#13;
&#13;
<p>Since Docker Desktop clusters only have one node, this output isn’t particularly informative; <a data-primary="Google Kubernetes Engine (GKE)" data-secondary="console" data-type="indexterm" id="idm45979378439328"/>let’s look at the output from a small GKE cluster for something more realistic:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get nodes</code></strong><code class="go">&#13;
</code><code class="go">NAME                                         STATUS   ROLES   AGE  VERSION&#13;
</code><code class="go">gke-k8s-cluster-1-n1-standard-2-pool--8l6n   Ready    &lt;none&gt;  9d   v1.20.2-gke.1&#13;
</code><code class="go">gke-k8s-cluster-1-n1-standard-2-pool--dwtv   Ready    &lt;none&gt;  19d  v1.20.2-gke.1&#13;
</code><code class="go">gke-k8s-cluster-1-n1-standard-2-pool--67ch   Ready    &lt;none&gt;  20d  v1.20.2-gke.1&#13;
</code><code class="go">...</code></pre>&#13;
&#13;
<p>Note that in the Docker Desktop <code>get nodes</code> output, the node <em>role</em> was shown as <code>control-plane</code>. Naturally enough, since there’s only one node, that must be the control plane and the lone worker node, too.</p>&#13;
&#13;
<p>In managed Kubernetes services, you typically don’t have direct access to the control plane nodes. Accordingly, <code>kubectl get nodes</code> lists the worker nodes only.</p>&#13;
&#13;
<p>If any of the nodes shows a status of <code>NotReady</code>, there is a problem. A reboot of the node may fix it, but if not, it may need further debugging—or you could just delete it and create a new node instead.</p>&#13;
&#13;
<p>For detailed troubleshooting of bad nodes, you can use the <code>kubectl describe node</code> command to get more information:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl describe nodes/gke-k8s-cluster-1-n1-standard-2-pool--8l6n</code></strong></pre>&#13;
&#13;
<p>This will show you, for example, the memory and CPU capacity of the node, and the resources currently in use by Pods.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Workloads" data-type="sect3"><div class="sect3" id="workloads">&#13;
<h3>Workloads</h3>&#13;
&#13;
<p>You may recall from <a data-type="xref" href="ch04.html#querykubectl">“Querying the Cluster with kubectl”</a> that you can also use <code>kubectl</code> to list all Pods (or any resources) in your cluster. <a data-primary="kubectl" data-secondary="flags" data-tertiary="--all-namespaces" data-type="indexterm" id="idm45979378379872"/>In that example, you listed only the Pods in the default namespace, but the <code>--all-namespaces</code> flag (or just <code>-A</code> for short) will allow you to see all Pods in the entire cluster:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods --all-namespaces</code></strong><code class="go">&#13;
</code><code class="go">NAMESPACE     NAME                          READY  STATUS           RESTARTS  AGE&#13;
</code><code class="go">cert-manager  cert-manager-cert-manager-55  1/1    Running          1         10d&#13;
</code><code class="go">pa-test       permissions-auditor-15281892  0/1    CrashLoopBackOff 1720      6d&#13;
</code><code class="go">metrics-api   metrics-api-779758f445        3/3    Running          5         20d&#13;
</code><code class="go">...</code></pre>&#13;
&#13;
<p>This can give you a helpful overview of what’s running in your cluster, and any Pod-level problems. If any Pods are not in <code>Running</code> status, like the <code>permissions-auditor</code> Pod in the example, it may need further investigation.</p>&#13;
&#13;
<p><a data-primary="containers" data-secondary="ready status" data-type="indexterm" id="idm45979378353296"/>The <code>READY</code> column shows how many containers in the Pod are actually running, compared to the number configured. For example, the <code>metrics-api</code> Pod shows <code>3/3</code>: 3 out of 3 containers are running, so all is well.</p>&#13;
&#13;
<p><a data-primary="CrashLoopBackOff" data-type="indexterm" id="idm45979378350672"/><a data-primary="kubectl" data-secondary="commands" data-tertiary="top" data-type="indexterm" id="idm45979378349744"/>On the other hand, <code>permissions-auditor</code> shows <code>0/1</code> containers ready: 0 containers running, but 1 required. The reason is shown in the <code>STATUS</code> column: <code>CrashLoopBackOff</code>. The container is failing to start properly.</p>&#13;
&#13;
<p>When a container crashes, Kubernetes will keep trying to restart it at increasing intervals, starting at 10 seconds and doubling each time, up to 5 minutes. This strategy is called <em>exponential backoff</em>, hence the <code>CrashLoopBackOff</code> status message.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CPU and Memory Utilization" data-type="sect2"><div class="sect2" id="idm45979378324416">&#13;
<h2>CPU and Memory Utilization</h2>&#13;
&#13;
<p><a data-primary="CPU utilization" data-type="indexterm" id="idm45979378323072"/><a data-primary="memory utilization" data-type="indexterm" id="idm45979378322368"/>Another useful view on your cluster is provided by the <code>kubectl top</code> command. For nodes, it will show you the CPU and memory capacity of each node, and how much of each is currently in use:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl top nodes</code></strong><code class="go">&#13;
</code><code class="go">NAME                           CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%&#13;
</code><code class="go">gke-k8s-cluster-1-n1-...8l6n   151m         7%        2783Mi          49%&#13;
</code><code class="go">gke-k8s-cluster-1-n1-...dwtv   155m         8%        3449Mi          61%&#13;
</code><code class="go">gke-k8s-cluster-1-n1-...67ch   580m         30%       3172Mi          56%&#13;
</code><code class="go">...</code></pre>&#13;
&#13;
<p>For Pods, it will show how much CPU and memory each specified Pod is using:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl top pods -n kube-system</code></strong><code class="go">&#13;
</code><code class="go">NAME                                    CPU(cores)   MEMORY(bytes)&#13;
</code><code class="go">event-exporter-v0.1.9-85bb4fd64d-2zjng  0m           27Mi&#13;
</code><code class="go">fluentd-gcp-scaler-7c5db745fc-h7ntr     10m          27Mi&#13;
</code><code class="go">fluentd-gcp-v3.0.0-5m627                11m          171Mi&#13;
</code><code class="go">...</code></pre>&#13;
&#13;
<p>Another useful tool for getting a wide look at the health of the cluster and Pods is to use <a data-type="xref" href="ch07.html#lens">“Lens”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cloud Provider Console" data-type="sect2"><div class="sect2" id="idm45979378284048">&#13;
<h2>Cloud Provider Console</h2>&#13;
&#13;
<p>If you’re using a managed Kubernetes service that is offered by your cloud provider, then you will have access to a web-based console that can show you useful information about your cluster, its nodes, and workloads.</p>&#13;
&#13;
<p>You can also list nodes, services, and configuration details for the cluster. This is much the same information as you can get from using the <code>kubectl</code> tool, but the cloud consoles also allow you to perform administration tasks: create clusters, upgrade nodes, and everything you’ll need to manage your cluster on a day-to-day basis.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Dashboard" data-type="sect2"><div class="sect2" id="dashboard">&#13;
<h2>Kubernetes Dashboard</h2>&#13;
&#13;
<p><a data-primary="Kubernetes Dashboard" data-type="indexterm" id="idm45979378279552"/>The <a href="https://oreil.ly/3ty9U">Kubernetes Dashboard</a> is a web-based user interface for Kubernetes clusters (<a data-type="xref" href="#img-dashboard-1">Figure 11-1</a>). If you’re running your own Kubernetes cluster, rather than using a managed service, you can run the Kubernetes Dashboard to get more or less the same information as a managed service console would provide.</p>&#13;
&#13;
<figure><div class="figure" id="img-dashboard-1">&#13;
<img alt="Screenshot of the Kubernetes Dashboard" src="assets/cnd2_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>The Kubernetes Dashboard displays useful information about your cluster</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you’d expect, the Dashboard lets you see the status of your clusters, nodes, and workloads, in much the same way as the <code>kubectl</code> tool, but with a graphical interface. You can also create and destroy resources using the Dashboard.</p>&#13;
&#13;
<p>Because the Dashboard exposes a great deal of information about your cluster and workloads, it’s very important to secure it properly, and never expose it to the public internet. The Dashboard lets you view the contents of ConfigMaps and Secrets, which could contain credentials and crypto keys, so you need to control access to the Dashboard as tightly as you would to those secrets themselves.</p>&#13;
&#13;
<p>In 2018, security firm RedLock found <a href="https://oreil.ly/3me4L">hundreds of Kubernetes Dashboard consoles</a> accessible over the internet without any password protection, including one owned by Tesla, Inc. From these they were able to extract cloud security credentials and use them to access further sensitive information.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>If you don’t have to run the Kubernetes Dashboard (for example, if you already have a Kubernetes console provided by a managed service such as GKE), don’t run it. If you do run it, make sure it has <a href="https://oreil.ly/aCZ1e">minimum privileges</a>, and never expose it to the internet. Instead, access it via <code>kubectl proxy</code>.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Weave Scope" data-type="sect2"><div class="sect2" id="idm45979378250240">&#13;
<h2>Weave Scope</h2>&#13;
&#13;
<p><a data-primary="Weave Scope" data-type="indexterm" id="idm45979378249040"/><a href="https://oreil.ly/01cLN">Weave Scope</a> is a great visualization and monitoring tool for your cluster, showing you a real-time map of your nodes, containers, and processes. You can also see metrics and metadata, and even start or stop containers using Scope.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="kube-ops-view" data-type="sect2"><div class="sect2" id="idm45979378247184">&#13;
<h2>kube-ops-view</h2>&#13;
&#13;
<p><a data-primary="kube-ops-view" data-type="indexterm" id="idm45979378245952"/><a href="https://oreil.ly/pmTGh">kube-ops-view</a> gives you a visualization of what’s happening in your cluster: what nodes there are, the CPU and memory utilization on each, how many Pods each one is running, and the status of those Pods. This is a great way to get a general overview of your cluster and what it’s doing.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="node-problem-detector" data-type="sect2"><div class="sect2" id="idm45979378244048">&#13;
<h2>node-problem-detector</h2>&#13;
&#13;
<p><a data-primary="node-problem-detector" data-type="indexterm" id="idm45979378242848"/><a href="https://oreil.ly/rE7Sl">node-problem-detector</a> is a Kubernetes add-on that can detect and report several kinds of node-level issues: hardware problems, such as CPU or memory errors, filesystem corruption, and wedged container runtimes.</p>&#13;
&#13;
<p>Currently, node-problem-detector reports problems by sending events to the Kubernetes API, and comes with a Go client library that you can use to integrate with your own tools.</p>&#13;
&#13;
<p>Although Kubernetes currently does not take any action in response to events from node-problem-detector, there may be further integration in the future that will allow the scheduler to avoid running Pods on problem nodes, for example.<a data-startref="ix_11-security-adoc14" data-type="indexterm" id="idm45979378240656"/><a data-startref="ix_11-security-adoc13" data-type="indexterm" id="idm45979378239952"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Further Reading" data-type="sect1"><div class="sect1" id="idm45979378490144">&#13;
<h1>Further Reading</h1>&#13;
&#13;
<p>Kubernetes security and cluster management is a complex and specialized topic, and we’ve only scratched the surface of it here. It really deserves a book of its own… and there is one. <a data-primary="Hausenblas, Michael" data-type="indexterm" id="idm45979378237888"/><a data-primary="Rice, Liz" data-type="indexterm" id="idm45979378237184"/>Security experts Liz Rice and Michael Hausenblas have written the excellent <a class="orm:hideurl" href="https://www.oreilly.com/library/view/kubernetes-security/9781492039075/"><em>Kubernetes Security</em></a> (O’Reilly), covering secure cluster setup, container security, secrets management, and more.</p>&#13;
&#13;
<p>Another great resource is <a class="orm:hideurl" href="https://www.oreilly.com/library/view/production-kubernetes/9781492092292//"><em>Production Kubernetes</em></a> (O’Reilly) by Josh Rosso, Rich Lander, Alex Brand, and John Harris. We recommend both books highly.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45979378233280">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Security is not a product or an end goal, but an ongoing process that requires knowledge, thought, and attention. Container security is no different, and the machinery to help ensure it is there for you to use. If you’ve read and understood the information in this chapter, you know everything you need to know to configure your containers securely in Kubernetes—but we’re sure you get the point that this should be the start, not the end, of your security process.</p>&#13;
&#13;
<p>The main things to keep in mind:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>RBAC gives you fine-grained management of permissions in Kubernetes. Make sure it’s enabled, and use RBAC roles to grant specific users and apps only the minimum privileges they need to do their jobs.</p>&#13;
</li>&#13;
<li>&#13;
<p>Containers aren’t magically exempt from security and malware problems. Use a scanning tool to check any containers that you run in production.</p>&#13;
</li>&#13;
<li>&#13;
<p>Using Kubernetes does not mean that you don’t need backups. Use Velero to back up your data and the state of the cluster. It’s handy for moving things between clusters, too.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>kubectl</code> is a powerful tool for inspecting and reporting on all aspects of your cluster and its workloads. Get friendly with <code>kubectl</code>. You’ll be spending a lot of time together.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use your Kubernetes provider’s web console and <code>kube-ops-view</code> for a graphical overview of what’s going on. If you use the Kubernetes Dashboard, secure it as tightly as you would your cloud credentials and crypto keys.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45979378784352"><sup><a href="ch11.html#idm45979378784352-marker">1</a></sup> See “The Junior Dev Who Deleted the Production Database,” 10 Jun 2017, by David Cassel, on <a href="https://thenewstack.io">thenewstack.io blog</a>.</p></div></div></section></body></html>