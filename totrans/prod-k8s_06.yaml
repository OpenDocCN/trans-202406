- en: Chapter 5\. Pod Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 Pod 网络
- en: Since the early days of networking, we have concerned ourselves with how to
    facilitate host-to-host communication. These concerns include uniquely addressing
    hosts, routing of packets across networks, and propagation of known routes. For
    more than a decade, software-defined networks (SDNs) have seen rapid growth by
    solving these concerns in our increasingly dynamic environments. Whether it is
    in your datacenter with VMware NSX or in the cloud with Amazon VPCs, you are likely
    a consumer of an SDN.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自网络的早期以来，我们一直关注如何促进主机之间的通信。这些问题包括唯一地标识主机、在网络之间路由数据包以及已知路由的传播。在过去的十多年中，软件定义网络（SDN）通过解决这些在日益动态的环境中的问题而迅速增长。无论是在您的数据中心使用
    VMware NSX 还是在云中使用 Amazon VPC，您很可能是 SDN 的消费者之一。
- en: In Kubernetes, these principles and desires hold. Although our unit moves from
    hosts to Pods, we need to ensure we have addressability and routability of our
    workloads. Additionally, given Pods are running as software on our hosts, we will
    most commonly establish networks that are entirely software-defined.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，这些原则和愿望仍然适用。虽然我们的单元从主机转移到 Pod，但我们需要确保我们的工作负载具有可寻址性和路由性。此外，考虑到
    Pod 作为软件在我们的主机上运行，我们通常会建立完全基于软件定义的网络。
- en: This chapter will explore the concept of Pod networks. We will start off by
    addressing some key networking concepts that must be understood and considered
    before implementing Pod networks. Then we will cover the [Container Networking
    Interface (CNI)](https://github.com/containernetworking/cni), which enables your
    choice of network implementation based on your networking requirements. Lastly,
    we will examine common plug-ins, such as Calico and Cilium, in the ecosystem to
    make the trade-offs more concrete. In the end, you’ll be more equipped to make
    decisions around the right networking solution and configuration for your application
    platform.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨 Pod 网络的概念。我们将首先解释一些必须理解和考虑的关键网络概念，然后我们将介绍[容器网络接口（CNI）](https://github.com/containernetworking/cni)，根据您的网络需求选择网络实现。最后，我们将研究生态系统中的常见插件，如
    Calico 和 Cilium，以使权衡更加具体化。最终，您将更加能够就应用平台的正确网络解决方案和配置做出决策。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Networking is a vast subject in itself. Our intentions are to give you just
    enough to make informed decisions on your Pod network. If your background is not
    networking, we highly recommend you go over these concepts with your networking
    team. Kubernetes does not negate the need to have networking expertise in your
    organization!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 网络本身是一个广泛的主题。我们的意图是为您提供足够的信息，以便在 Pod 网络上做出知情的决策。如果您的背景不是网络，强烈建议您与您的网络团队一起学习这些概念。Kubernetes
    并不消除您组织中需要具备网络专业知识的必要性！
- en: Networking Considerations
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络考虑事项
- en: 'Before diving into implementation details around Pod networks, we should start
    with a few key areas of consideration. These areas include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论 Pod 网络的实施细节之前，我们应该首先考虑几个关键领域。这些领域包括：
- en: IP Address Management (IPAM)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP 地址管理（IPAM）
- en: Routing protocols
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由协议
- en: Encapsulation and tunneling
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 封装和隧道
- en: Workload routability
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作负载的路由性
- en: IPv4 and IPv6
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPv4 和 IPv6
- en: Encrypted workload traffic
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加密的工作负载流量
- en: Network policy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略
- en: With an understanding of these areas, you can begin to make determinations around
    the correct networking solution for your platform.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了这些领域后，您可以开始确定适合您平台的正确网络解决方案。
- en: IP Address Management
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IP 地址管理
- en: In order to communicate to and from Pods, we must ensure they are uniquely addressable.
    In Kubernetes, each Pod receives an IP. These IPs may be internal to the cluster
    or externally routable. Each Pod having its own address simplifies the networking
    model, considering we do not have to be concerned with colliding ports on shared
    IPs. However, this IP-per-Pod model does come with its own challenges.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与 Pod 进行通信，我们必须确保它们具有唯一的可寻址性。在 Kubernetes 中，每个 Pod 都会分配一个 IP 地址。这些 IP 地址可能是集群内部的，也可能是外部可路由的。每个
    Pod 拥有自己的地址简化了网络模型，因为我们不必担心共享 IP 上的端口冲突。然而，这种每个 Pod 拥有独立 IP 的模型也带来了它自己的挑战。
- en: Pods are best thought of as ephemeral. Specifically, they are prone to being
    restarted or rescheduled based on the needs of the cluster or system failure.
    This requires IP allocation to execute quickly and the management of the cluster’s
    IP pool to be efficient. This management is often referred to as [IPAM](https://oreil.ly/eWJki)
    and is not unique to Kubernetes. As we dive deeper into container networking approaches,
    we will explore a variety of ways IPAM is implemented.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 可以被视为短暂的实体。具体来说，它们容易因集群的需求或系统故障而重新启动或重新调度。这要求 IP 分配能够快速执行，并且集群 IP 池的管理必须高效。这种管理通常称为[IPAM](https://oreil.ly/eWJki)，并不限于
    Kubernetes。在我们深入研究容器网络方法时，我们将探讨多种实现 IPAM 的方式。
- en: Warning
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This ephemeral expectation of a workload’s IP causes issues in some legacy workloads,
    for example, workloads that pin themselves to a specific IP and expect it to remain
    static. Depending on your implementation of container networking (covered later
    in this chapter), you may be able to explicitly reserve IPs for specific workloads.
    However, we recommend against this model unless necessary. There are many capable
    service discovery or DNS mechanisms that workloads can take advantage of to properly
    remedy this issue. Review [Chapter 6](ch06.html#chapter6) for examples.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作负载 IP 的短暂期望在某些传统工作负载中会引发问题，例如将自身固定到特定 IP 并期望其保持静态的工作负载。根据您的容器网络实现（本章后面将介绍），您可以为特定工作负载明确保留
    IP。但是，除非必要，我们建议不要采用这种模型。有许多功能强大的服务发现或 DNS 机制可以帮助工作负载有效解决此问题。请查看[第 6 章](ch06.html#chapter6)以获取示例。
- en: 'IPAM is implemented based on your choice of CNI plug-in. There are a few commonalities
    in these plug-ins that pertain to Pod IPAM. First, when clusters are created,
    a Pod network’s [Classless Inter-Domain Routing](https://oreil.ly/honRv) (CIDR)
    can be specified. How it is set varies based on how you bootstrap Kubernetes.
    In the case of `kubeadm`, a flag can be passed as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: IPAM 是基于您选择的 CNI 插件实现的。这些插件中有一些关于 Pod IPAM 的共同点。首先，创建集群时可以指定 Pod 网络的[无类域间路由](https://oreil.ly/honRv)（CIDR）。如何设置取决于您如何引导
    Kubernetes。在使用 `kubeadm` 的情况下，可以传递一个标志，如下所示：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In effect, this sets the `--cluster-cidr` flag on the kube-controller-manager.
    Kubernetes will then allocate a chunk of this cluster-cidr to every node. By default,
    each node is allocated `/24`. However, this can be controlled by setting the `--node-cidr-mask-size-ipv4`
    and/or `--node-cidr-mask-size-ipv6` flags on the kube-controller-manager. A Node
    object featuring this allocation is as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上设置了 kube-controller-manager 上的 `--cluster-cidr` 标志。Kubernetes 随后将为每个节点分配此集群
    CIDR 的一部分。默认情况下，每个节点分配 `/24`。但是，可以通过在 kube-controller-manager 上设置 `--node-cidr-mask-size-ipv4`
    和/或 `--node-cidr-mask-size-ipv6` 标志来控制这一点。具有此分配的节点对象如下：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_pod_networking_CO1-1)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_pod_networking_CO1-1)'
- en: This field exists for compatibility. `podCIDRs` was later introduced as an array
    to support dual stack (IPv4 and IPv6 CIDRs) on a single node.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字段存在是为了兼容性。`podCIDRs`后来被引入为一个数组，支持在单个节点上支持双栈（IPv4 和 IPv6 CIDR）。
- en: '[![2](assets/2.png)](#co_pod_networking_CO1-2)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_pod_networking_CO1-2)'
- en: The IP range assigned to this node is 10.30.0.0 - 10.30.0.255\. This is 254
    addresses for Pods, out of the 65,534 available in the 10.30.0.0/16 cluster CIDR.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给此节点的 IP 范围是 10.30.0.0 - 10.30.0.255。这是用于 Pods 的 254 个地址，来自于 10.30.0.0/16
    集群 CIDR 中可用的 65,534 个地址之一。
- en: Whether these values are used in IPAM is up to the CNI plug-in. For example,
    Calico detects and respects this setting, while Cilium offers an option to either
    manage IP pools independent of Kubernetes (default) or respect these allocations.
    In most CNI implementations, it is important that your CIDR choice *does not*
    overlap with the cluster’s host/node network. However, assuming your Pod network
    will remain internal to the cluster, the CIDR chosen can overlap with network
    space outside the cluster. [Figure 5-1](#the_ip_spaces_and_ip_allocations_of_the_most_network)
    demonstrates the relationship of these various IP spaces and examples of allocations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值是否在 IPAM 中使用取决于 CNI 插件。例如，Calico 检测并遵守此设置，而 Cilium 则提供一个选项，可以独立于 Kubernetes
    管理 IP 池（默认情况下），或者遵守这些分配。在大多数 CNI 实现中，重要的是您的 CIDR 选择*不要*与集群的主机/节点网络重叠。但是，假设您的 Pod
    网络将保持集群内部，所选的 CIDR 可以与集群外部的网络空间重叠。[图 5-1](#the_ip_spaces_and_ip_allocations_of_the_most_network)展示了这些不同
    IP 空间之间的关系及分配示例。
- en: Note
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: How large you should set your cluster’s Pod CIDR is often a product of your
    networking model. In most deployments, a Pod network is entirely internal to the
    cluster. As such, the Pod CIDR can be very large to accommodate for future scale.
    When the Pod CIDR is routable to the larger network, thus consuming address space,
    you may have to do more careful consideration. Multiplying the number of Pods
    per node by your eventual node count can give you a rough estimate. The number
    of Pods per node is configurable on the kubelet, but by default is 110.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 设置集群的 Pod CIDR 多大通常取决于您的网络模型。在大多数部署中，Pod 网络完全是集群内部的。因此，Pod CIDR 可以非常大以适应未来的规模。当
    Pod CIDR 能够路由到更大的网络时，因此消耗地址空间，您可能需要更谨慎地考虑。将每个节点的 Pod 数量乘以最终节点数量可以给出一个粗略的估计。默认情况下，kubelet
    可以在每个节点上配置的 Pod 数量为 110。
- en: '![prku 0501](assets/prku_0501.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0501](assets/prku_0501.png)'
- en: Figure 5-1\. The IP spaces and IP allocations of the host network, Pod network,
    and each [host] local CIDR.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 主机网络、Pod 网络和每个[主机]本地 CIDR 的 IP 空间和 IP 分配。
- en: Routing Protocols
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路由协议
- en: Once Pods are addressed, we need to ensure that routes to and from them are
    understood. This is where routing protocols come in to play. Routing protocols
    can be thought of as different ways to propagate routes to and from places. Introducing
    a routing protocol often enables dynamic routing, relative to configuring [static
    routes](https://oreil.ly/97En2). In Kubernetes, understanding a multitude of routes
    becomes important when not leveraging encapsulation (covered in the next section),
    since the network will often be unaware of how to route workload IPs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦地址为 Pods，我们需要确保能够理解到它们的路由。这就是路由协议发挥作用的地方。可以将路由协议视为传播到各个地方和从各个地方路由的不同方式。引入路由协议通常会启用动态路由，相对于配置[静态路由](https://oreil.ly/97En2)。在
    Kubernetes 中，当不利用封装（在下一节中介绍）时，理解多种路由变得重要，因为网络通常不知道如何路由工作负载 IP。
- en: Border Gateway Protocol (BGP) is one of the most commonly used protocols to
    distribute workload routes. It is used in projects such as [Calico](https://www.projectcalico.org)
    and [Kube-Router](https://www.kube-router.io). Not only does BGP enable communication
    of workload routes in the cluster but its internal routers can also be peered
    with external routers. Doing so can make external network fabrics aware of how
    to route to Pod IPs. In implementations such as Calico, a BGP daemon is run as
    part of the Calico Pod. This Pod runs on every host. As routes to workloads become
    known, the Calico Pod modifies the kernel routing table to include routes to each
    potential workload. This provides native routing via the workload IP, which can
    work especially well when running in the same L2 segment. [Figure 5-2](#the_calico_pod_sharing_routes_via_its_bgp_peer)
    demonstrates this behavior.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 边界网关协议（BGP）是分发工作负载路由的最常用的协议之一。它在诸如[Calico](https://www.projectcalico.org)和[Kube-Router](https://www.kube-router.io)的项目中使用。BGP
    不仅能够在集群中通信工作负载路由，而且其内部路由器还可以与外部路由器进行对等连接。这样可以使外部网络结构意识到如何路由到 Pod IP。在诸如 Calico
    的实现中，BGP 守护进程作为 Calico Pod 的一部分运行。这个 Pod 在每个主机上运行。随着对工作负载的路由变得已知，Calico Pod 修改内核路由表以包括每个潜在工作负载的路由。这提供了通过工作负载
    IP 进行原生路由的功能，特别是当在相同的 L2 段中运行时。[图 5-2](#the_calico_pod_sharing_routes_via_its_bgp_peer)展示了这种行为。
- en: '![prku 0502](assets/prku_0502.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0502](assets/prku_0502.png)'
- en: Figure 5-2\. The `calico-pod` sharing routes via its BGP peer. The kernel routing
    table is then programmed accordingly.
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 使用其 BGP 对等体共享路由的`calico-pod`。然后相应地编程内核路由表。
- en: Warning
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Making Pod IPs routable to larger networks may seem appealing at first glance
    but should be carefully considered. See [“Workload Routability”](#workload_routability)
    for more details.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来将 Pod IP 路由到更大的网络可能是有吸引力的，但应仔细考虑。有关更多详情，请参阅[“工作负载可路由性”](#workload_routability)。
- en: 'In many environments, native routing to workload IPs is not possible. Additionally,
    routing protocols such as BGP may not be able to integrate with an underlying
    network; such is the case running in a cloud-provider’s network. For example,
    let’s consider a CNI deployment where we wish to support native routing and share
    routes via BGP. In an AWS environment, this can fail for two reasons:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多环境中，原生路由到工作负载 IP 是不可能的。此外，诸如 BGP 的路由协议可能无法与底层网络集成；这种情况在运行在云提供商网络时很常见。例如，让我们考虑一个
    CNI 部署，在这个部署中，我们希望支持原生路由并通过 BGP 共享路由。在 AWS 环境中，这可能由于两个原因而失败：
- en: Source/Destination checks are enabled
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 源/目标检查已启用
- en: This ensures that packets hitting the host have the destination (and source
    IP) of the target host. If it does not match, the packet is dropped. This setting
    can be disabled.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了命中主机的数据包具有目标主机的目的地（和源IP）。如果不匹配，数据包将被丢弃。此设置可以禁用。
- en: Packet needs to traverse subnets
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包需要穿越子网
- en: If the packet needs to leave the subnet, the destination IP is evaluated by
    the underlying AWS routers. When the Pod IP is present, it will not be able to
    route.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据包需要离开子网，底层AWS路由器将评估目标IP。当存在Pod IP时，将无法进行路由。
- en: In these scenarios, we look to tunneling protocols.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些场景中，我们看到了隧道协议的应用。
- en: Encapsulation and Tunneling
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 封装和隧道
- en: Tunneling protocols give you the ability to run your Pod network in a way that
    is mostly unknown to the underlying network. This is achieved using encapsulation.
    As the name implies, encapsulation involves putting a packet (the inner packet)
    inside another packet (the outer packet). The inner packet’s src IP and dst IP
    fields will reference the workload (Pod) IPs, whereas the outer packet’s src IP
    and dst IP fields will reference the host/node IPs. When the packet leaves a node,
    it will appear to the underlying network as any other packet since the workload-specific
    data is in the payload. There are a variety of tunneling protocols such as VXLAN,
    Geneve, and GRE. In Kuberntes, VXLAN has become one of the most commonly used
    methods by networking plug-ins. [Figure 5-3](#vxlan_encapsulation_used_to_move_an_inner_packet)
    demonstrates an encapsulated packet crossing the wire via VXLAN.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 隧道协议使您能够以大部分未知于底层网络的方式运行您的Pod网络。这是通过封装来实现的。顾名思义，封装涉及将一个数据包（内部数据包）放入另一个数据包（外部数据包）中。内部数据包的源IP和目标IP字段将引用工作负载（Pod）的IP地址，而外部数据包的源IP和目标IP字段将引用主机/节点的IP地址。当数据包离开节点时，对于底层网络来说，它将像任何其他数据包一样，因为工作负载特定数据位于有效负载中。有许多隧道协议，如VXLAN、Geneve和GRE。在Kubernetes中，VXLAN已成为网络插件中最常用的方法之一。[图5-3](#vxlan_encapsulation_used_to_move_an_inner_packet)展示了通过VXLAN传输封装数据包。
- en: As you can see, VXLAN puts an entire Ethernet frame inside a UDP packet. This
    essentially gives you a fully virtualized layer-2 network, often referred to as
    an overlay network. The network beneath the overlay, referred to as the underlay
    network, does not concern itself with the overlay. This is one of the primary
    benefits to tunneling protocols.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，VXLAN将整个以太网帧封装在UDP数据包中。这本质上提供了一个完全虚拟化的二层网络，通常称为覆盖网络。覆盖网络下方的网络，称为基础网络，不关心覆盖网络。这是隧道协议的主要优势之一。
- en: '![prku 0503](assets/prku_0503.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0503](assets/prku_0503.png)'
- en: Figure 5-3\. VXLAN encapsulation used to move an inner packet, for workloads,
    across hosts. The network cares only about the outer packet, so it needs to have
    zero awareness of workload IPs and their routes.
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3. VXLAN封装用于跨主机移动内部数据包，用于工作负载。网络只关心外部数据包，因此不需要了解工作负载IP及其路由。
- en: 'Often, you choose whether to use a tunneling protocol based on the requirements/capabilities
    of your environment. Encapsulation has the benefit of working in many scenarios
    since the overlay is abstracted from the underlay network. However, this approach
    comes with a few key downsides:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您根据环境的要求/能力选择是否使用隧道协议。封装的优点是在许多场景中都能工作，因为覆盖网络与基础网络被抽象化。然而，这种方法也有一些关键的缺点：
- en: Traffic can be harder to understand and troubleshoot
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 流量可能更难理解和排除故障
- en: Packets within packets can create extra complexity when troubleshooting network
    issues.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 包内嵌包可能在网络故障排除时增加额外复杂性。
- en: Encapsulation/decapsulation will incur processing cost
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 封装/解封装会产生处理成本
- en: When a packet goes to leave a host it must be encapsulated, and when it enters
    a host it must be decapsulated. While likely small, this will add overhead relative
    to native routing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据包离开主机时，它必须进行封装，当进入主机时，它必须进行解封装。虽然可能很小，但这将增加相对于本地路由的开销。
- en: Packets will be larger
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包将会变得更大
- en: Due to the embedding of packets, they will be larger when transmitted over the
    wire. This may require adjustments to the [maximum transmission unit (MTU)](https://oreil.ly/dzYBz)
    to ensure they fit on the network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据包的嵌套，它们在通过网络传输时会变得更大。这可能需要调整[最大传输单元（MTU）](https://oreil.ly/dzYBz)以确保其适合网络。
- en: Workload Routability
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作负载路由能力
- en: In most clusters, Pod networks are internal to the cluster. This means Pods
    can directly communicate with each other, but external clients cannot reach Pod
    IPs directly. Considering Pod IPs are ephemeral, communicating directly with a
    Pod’s IP is often bad practice. Relying on service discovery or load balancing
    mechanics that abstract the underlying IP is preferable. A huge benefit to the
    internal Pod network is that it *does not* occupy precious address space within
    your organization. Many organizations manage address space to ensure addresses
    stay unique within the company. Thus, you would certainly get a dirty look when
    you ask for a `/16` space (65,536 IPs) for each Kubernetes cluster you bootstrap!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数集群中，Pod 网络是集群内部的。这意味着 Pod 可以直接彼此通信，但外部客户端不能直接到达 Pod IP。考虑到 Pod IP 是临时的，直接与
    Pod 的 IP 进行通信通常是不良实践。依赖服务发现或负载均衡机制来抽象底层 IP 更为可取。内部 Pod 网络的一个巨大优势是它*不会*占用组织内宝贵的地址空间。许多组织管理地址空间以确保地址在公司内保持唯一。因此，当你要求每个
    Kubernetes 集群引导时使用 `/16` 空间（65,536 个 IP）时，你肯定会受到质疑！
- en: When Pods are not directly routable, we have several patterns to facilitate
    external traffic to Pod IPs. Commonly we will expose an Ingress controller on
    the host network of a subset of dedicated nodes. Then, once the packet enters
    the Ingress controller proxy, it can route directly to Pod IPs since it takes
    part in the Pod network. Some cloud providers even include (external) load balancer
    integration that wires this all together automatically. We explore a variety of
    these ingress models, and their trade-offs, in [Chapter 6](ch06.html#chapter6).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pod 不直接可路由时，我们有几种模式来促进外部流量到 Pod IP。通常我们会在一些专用节点的主机网络上暴露一个入口控制器。然后，一旦数据包进入入口控制器代理，它可以直接路由到
    Pod IP，因为它参与了 Pod 网络。一些云服务提供商甚至包括（外部）负载均衡器集成，自动将这一切连接在一起。我们在[第六章](ch06.html#chapter6)中探讨了多种这样的入口模型及其权衡。
- en: At times, requirements necessitate that Pods are routable to the larger network.
    There are two primary means to accomplish this. The first is to use a networking
    plug-in that integrates with the underlying network directly. For example, [AWS’s
    VPC CNI](https://github.com/aws/amazon-vpc-cni-k8s) attaches multiple secondary
    IPs to each node and allocates them to Pods. This makes each Pod routable just
    as an EC2 host would be. The primary downside to this model is it will consume
    IPs in your subnet/VPC. The second option is to propagate routes to Pods via a
    routing protocol such as BGP, as described in [“Routing Protocols”](#routing_protocols).
    Some plug-ins using BGP will even enable you to make a subset of your Pod network
    routable, rather than having to expose the entire IP space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，需求要求 Pod 能够路由到更大的网络。有两种主要方法可以实现这一点。第一种是使用与底层网络直接集成的网络插件。例如，[AWS 的 VPC CNI](https://github.com/aws/amazon-vpc-cni-k8s)
    将多个次要 IP 地址附加到每个节点并分配给 Pod。这样，每个 Pod 就可以像 EC2 主机一样路由。这种模型的主要缺点是它会消耗你子网/VPC 中的
    IP。第二个选项是通过诸如 BGP 的路由协议传播路由到 Pod。某些使用 BGP 的插件甚至可以使你可以使 Pod 网络的子集可路由，而不是必须暴露整个
    IP 空间。
- en: Warning
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Avoid making your Pod network externally routable unless absolutely necessary.
    We often see legacy applications driving the desire for routable Pods. For example,
    consider a TCP-based workload where a client must be pinned to the same backend.
    Typically, we recommend updating the application(s) to fit within the container
    networking paradigm using service discovery and possibly re-architecting the backend
    to not require client-server affinity (when possible). While exposing the Pod
    networking may seem like a simple solution, doing so comes at the cost of eating
    up IP space and potentially making IPAM and route propagation configurations more
    complex.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 避免使你的 Pod 网络在绝对必要之外变得可外部路由。我们经常看到传统应用程序驱动对可路由 Pod 的渴望。例如，考虑一个基于 TCP 的工作负载，其中客户端必须固定在同一个后端。通常，我们建议更新应用程序以适应容器网络范式，使用服务发现并可能重新设计后端，以避免需要客户端-服务器亲和（如果可能的话）。尽管暴露
    Pod 网络看起来像是一个简单的解决方案，但这样做会增加 IP 空间的消耗，并可能使 IPAM 和路由传播配置变得更加复杂。
- en: IPv4 and IPv6
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IPv4 和 IPv6
- en: The overwhelming majority of clusters today run IPv4 exclusively. However, we
    are seeing the desire to run IPv6-networked clusters in certain clients such as
    telcos where addressability of many workloads is critical. Kubernetes does support
    IPv6 via [dual-stack](https://oreil.ly/sj_jN) as of `1.16`. At the time of this
    writing, dual-stack is an alpha feature. Dual-stack enables you to configure IPv4
    and IPv6 address spaces in your clusters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当前绝大多数集群仅运行 IPv4。然而，我们注意到在某些客户端（如电信运营商），确保多个工作负载的可寻址性是至关重要的。Kubernetes 从 `1.16`
    版本开始支持通过[dual-stack](https://oreil.ly/sj_jN)。在撰写本文时，双栈功能仍处于 alpha 版本。双栈功能使您能够在集群中配置
    IPv4 和 IPv6 地址空间。
- en: 'If your use case requires IPv6, it can easily be enabled but requires a few
    components to line up:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的使用场景需要 IPv6，可以轻松启用，但需要几个组件保持一致：
- en: While still in alpha, a feature-gate must be enabled on the kube-apiserver and
    kubelet.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管仍处于 alpha 版本，但 kube-apiserver 和 kubelet 必须启用一个功能开关。
- en: The kube-apiserver, kube-controller-manager, and kube-proxy all require an additional
    configuration to specify the IPv4 and IPv6 space.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kube-apiserver、kube-controller-manager 和 kube-proxy 需要额外配置以指定 IPv4 和 IPv6 空间。
- en: You must use a CNI plug-in that supports IPv6, such as [Calico](https://projectcalico.org)
    or [Cilium](https://cilium.io).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须使用支持 IPv6 的 CNI 插件，例如[Calico](https://projectcalico.org)或[Cilium](https://cilium.io)。
- en: 'With the preceding in place, you will see two CIDR allocations on each Node
    object:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述配置完成后，您将在每个节点对象上看到两个 CIDR 分配：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The CNI plug-in’s IPAM is responsible for determining whether an IPv4, IPv6,
    or both is assigned to each Pod.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 插件的 IPAM 负责确定是否为每个 Pod 分配 IPv4、IPv6 或两者皆有。
- en: Encrypted Workload Traffic
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加密的工作负载流量
- en: Pod-to-Pod traffic is rarely (if ever) encrypted by default. This means that
    packets sent over the wire without encryption, such as TLS, can be sniffed as
    plain text. Many network plug-ins support encrypting traffic over the wire. For
    example, Antrea supports encryption with [IPsec](https://oreil.ly/jqzCQ) when
    using a GRE tunnel. Calico is able to encrypt traffic by tapping into a node’s
    [WireGuard](https://www.wireguard.com) installation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 与 Pod 之间的流量通常不会默认进行加密。这意味着未加密的数据包（如 TLS）在传输过程中可能会被截获为明文。许多网络插件支持在传输过程中加密流量。例如，Antrea
    在使用 GRE 隧道时支持使用[IPsec](https://oreil.ly/jqzCQ)进行加密。Calico 可以通过利用节点的[WireGuard](https://www.wireguard.com)安装来加密流量。
- en: Enabling encryption may seem like a no-brainer. However, there are trade-offs
    to be considered. We recommend talking with your networking team to understand
    how host-to-host traffic is handled today. Is data encrypted when it goes between
    hosts in your datacenter? Additionally, what other encryption mechanisms may be
    at play? For example, does every service talk over TLS? Do you plan to leverage
    a service mesh where workload proxies leverage mTLS? If so, is encrypting at the
    service proxy and CNI layer required? While encryption will increase the depth
    of defense, it will also add complexity to network management and troubleshooting.
    Most importantly, needing to encrypt and decrypt packets will impact performance,
    thus lowering your potential throughput.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 启用加密可能看起来是一个明显的选择。然而，需要考虑权衡。我们建议与您的网络团队交流，了解今天在主机到主机的流量处理中是否已加密数据。例如，当服务之间是否都通过
    TLS 进行通信？您是否计划利用服务网格，其中工作负载代理使用 mTLS？如果是这样，是否需要在服务代理和 CNI 层面进行加密？虽然加密将增加防御深度，但也会增加网络管理和故障排除的复杂性。最重要的是，需要加密和解密数据包将影响性能，从而降低潜在的吞吐量。
- en: Network Policy
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络策略
- en: Once the Pod network is wired up, a logical next step is to consider how to
    set up network policy. Network policy is similar to firewall rules or security
    groups, where we can define what ingress and egress traffic is allowed. Kubernetes
    offers a [NetworkPolicy API](https://oreil.ly/1UV_3), as part of the core networking
    APIs. Any cluster can have policies added to it. However, it is incumbent on the
    CNI provider to *implement* the policy. This means that a cluster running a CNI
    provider that does not support NetworkPolicy, such as [flannel](https://github.com/coreos/flannel),
    will accept NetworkPolicy objects but not act on them. Today, most CNIs have some
    level of support for NetworkPolicy. Those that do not can often be used alongside
    plug-ins such as Calico, where the plug-in runs in a mode where it provides only
    policy enforcement.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Pod 网络连接完毕，逻辑上的下一步是考虑如何设置网络策略。网络策略类似于防火墙规则或安全组，我们可以定义允许的入口和出口流量。Kubernetes
    提供了一个 [NetworkPolicy API](https://oreil.ly/1UV_3)，作为核心网络 API 的一部分。任何集群都可以向其添加策略。但是，实施策略是
    CNI 提供者的责任。这意味着运行不支持 NetworkPolicy 的 CNI 提供者的集群，例如 [flannel](https://github.com/coreos/flannel)，将接受
    NetworkPolicy 对象但不会执行它们。如今，大多数 CNI 都对 NetworkPolicy 有一定程度的支持。那些不支持的可以通常与插件（例如
    Calico）一起使用，插件运行在仅提供策略执行的模式下。
- en: NetworkPolicy being available inside of Kubernetes adds yet another layer where
    firewall-style rules can be managed. For example, many networks provide subnet
    or host-level rules available via a distributed firewall or security group mechanism.
    While good, often these existing solutions do not have visibility into the Pod
    network. This prevents the level of granularity that may be desired in setting
    up rules for Pod-based workload communication. Another compelling aspect of Kubernetes
    NetworkPolicy is that, like most objects we deal with in Kubernetes, it is defined
    declaratively and, we think, far easier to manage relative to most firewall management
    solutions! For these reasons, we generally recommend considering implementing
    network policy at the Kubernetes level rather than trying to make existing firewall
    solutions fit this new paradigm. This does not mean you should throw out your
    existing host-to-host firewall solution(s). More so, let Kubernetes handle the
    intra-workload policy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 内部提供的 NetworkPolicy 又增加了一层类似防火墙样式规则管理的机制。例如，许多网络通过分布式防火墙或安全组机制提供子网或主机级别的规则。虽然这些现有解决方案通常不具有对
    Pod 网络的可见性，这限制了在设置基于 Pod 工作负载通信规则时可能期望的粒度。Kubernetes NetworkPolicy 的另一个引人注目的方面是，与
    Kubernetes 中的大多数对象一样，它是以声明方式定义的，并且我们认为相对于大多数防火墙管理解决方案来说，更易于管理！因此，我们通常建议考虑在 Kubernetes
    层面实施网络策略，而不是尝试使现有防火墙解决方案适应这种新的范式。这并不意味着您应该放弃您现有的主机到主机防火墙解决方案。更多的是，让 Kubernetes
    处理工作负载内部策略。
- en: 'Should you choose to utilize NetworkPolicy, it is important to note these policies
    are *Namespace-scoped*. By default, when NetworkPolicy objects are not present,
    Kubernetes allows all communication to and from workloads. When setting a policy,
    you can select what workloads the policy applies to. When present, the default
    behavior inverts and any egress and ingress traffic not allowed by the policy
    will be blocked. This means that the Kubernetes NetworkPolicy API specifies only
    what traffic is allowed. Additionally, the policy in a Namespace is additive.
    Consider the following NetworkPolicy object that configures ingress and egress
    rules:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择使用 NetworkPolicy，重要的是要注意这些策略是 *命名空间范围* 的。默认情况下，当不存在 NetworkPolicy 对象时，Kubernetes
    允许与工作负载之间的所有通信。设置策略时，您可以选择策略适用于哪些工作负载。当存在时，默认行为将反转，任何未被策略允许的出口和入口流量将被阻止。这意味着 Kubernetes
    NetworkPolicy API 仅指定了允许的流量。此外，命名空间中的策略是累加的。考虑以下配置了入口和出口规则的 NetworkPolicy 对象：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_pod_networking_CO2-1)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_pod_networking_CO2-1)'
- en: The empty `podSelector` implies this policy applies to all Pods in this Namespace.
    Alternatively, you can match against a label.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 空的 `podSelector` 意味着此策略适用于该命名空间中的所有 Pod。或者，您可以根据标签进行匹配。
- en: '[![2](assets/2.png)](#co_pod_networking_CO2-2)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_pod_networking_CO2-2)'
- en: This ingress rule allows traffic from sources with an IP in the range of 10.40.0.0/24,
    when the protocol is TCP and the port is 80.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此入口规则允许来自 IP 范围为 10.40.0.0/24 的源的流量，协议为 TCP，端口为 80。
- en: '[![3](assets/3.png)](#co_pod_networking_CO2-3)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_pod_networking_CO2-3)'
- en: This egress rule allows DNS traffic from workloads.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此出口规则允许来自工作负载的 DNS 流量。
- en: '[![4](assets/4.png)](#co_pod_networking_CO2-4)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_pod_networking_CO2-4)'
- en: This egress rule limits sending traffic to packets destined for workloads in
    the `org-2` Namespace with the label `team-b`. Additionally, the protocol must
    be TCP and the destination port is 80.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此出口规则限制向 `org-2` 命名空间中标有 `team-b` 标签的工作负载发送 TCP 协议且目标端口为 80 的流量。
- en: 'Over time, we have seen the NetworkPolicy API be limiting to certain use cases.
    Some common desires include:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们发现 NetworkPolicy API 对某些用例有所限制。一些常见的需求包括：
- en: Complex condition evaluation
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的条件评估
- en: Resolution of IPs based on DNS records
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 DNS 记录解析 IP 地址
- en: L7 rules (host, path, etc.)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L7 规则（主机、路径等）
- en: Cluster-wide policy, enabling global rules to be put in place, rather than having
    to replicate them in every Namespace.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群范围的策略，使全局规则能够生效，而不是需要在每个命名空间中复制这些规则。
- en: To satisfy these desires, some CNI plug-ins offer their own, more capable, policy
    APIs. The primary trade-off to using provider-specific APIs is that your rules
    are no longer portable across plug-ins. We will explore examples of these when
    we cover Calico and Cilium later in the chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，一些 CNI 插件提供了更强大的策略 API。使用供应商特定的 API 的主要折衷是，您的规则不再跨插件可移植。在本章稍后的部分，我们将探讨
    Calico 和 Cilium 的示例。
- en: 'Summary: Networking Considerations'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要：网络考虑因素
- en: 'In the previous sections we have covered key networking considerations that
    will enable you to make an informed decision about your Pod networking strategy.
    Before diving into CNI and plug-ins, let’s recap some of the key areas of consideration:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经涵盖了关键的网络考虑因素，这些将帮助您就 Pod 网络策略做出明智的决策。在深入讨论 CNI 和插件之前，让我们回顾一些关键的考虑领域：
- en: How large should your Pod CIDR be per cluster?
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个集群的 Pod CIDR 应该有多大？
- en: What networking constraints does your underlay network put on your future Pod
    network?
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的底层网络对未来 Pod 网络有什么网络约束？
- en: If using a Kubernetes managed service or vendor offering, what networking plug-in(s)
    are supported?
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用 Kubernetes 管理的服务或供应商提供的服务，支持哪些网络插件？
- en: Are routing protocols such as BGP supported in your infrastructure?
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的基础设施是否支持诸如 BGP 的路由协议？
- en: Could unencapsulated (native) packets be routed through the network?
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否可以通过网络路由未封装（原生）的数据包？
- en: Is using a tunnel protocol (encapsulation) desirable or required?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用隧道协议（封装）是可取的还是必需的？
- en: Do you need to support (externally) routable Pods?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您是否需要支持（外部可路由的）Pods？
- en: Is running IPv6 a requirement for your workloads?
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 IPv6 是否是您工作负载的要求？
- en: On what level(s) will you expect to enforce network policy or firewall rules?
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望在哪些级别执行网络策略或防火墙规则？
- en: Does your Pod network need to encrypt traffic on the wire?
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的 Pod 网络是否需要在传输时加密流量？
- en: With the answers to these questions fleshed out, you are in a good place to
    start learning about what enables you to plug in the correct technology to solve
    these issues, the Container Networking Interface (CNI).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些问题的答案，您可以开始学习如何插入正确的技术来解决这些问题，即容器网络接口 (CNI)。
- en: The Container Networking Interface (CNI)
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器网络接口 (CNI)
- en: All the considerations discussed thus far make it clear that different use cases
    warrant different container networking solutions. In the early days of Kubernetes,
    most clusters were running a networking plug-in called [flannel](https://github.com/coreos/flannel).
    Over time, solutions such as [Calico](https://www.projectcalico.org) and others
    gained popularity. These new plug-ins brought different approaches to creating
    and running networks. This drove the creation of a standard for how systems such
    as Kubernetes could request networking resources for its workloads. This standard
    is known as the [Container Networking Interface (CNI)](https://github.com/containernetworking/cni).
    Today, all networking options compatible with Kubernetes conform to this interface.
    Similar to the Container Storage Interface (CSI) and Container Runtime Interface
    (CRI), this gives us flexibility in the networking stack of our application platform.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的所有考虑都清楚地表明，不同的用例需要不同的容器网络解决方案。在 Kubernetes 的早期阶段，大多数集群都在运行名为 [flannel](https://github.com/coreos/flannel)
    的网络插件。随着时间的推移，诸如 [Calico](https://www.projectcalico.org) 等解决方案变得更加流行。这些新的插件带来了创建和运行网络的不同方法。这促使了像
    Kubernetes 这样的系统如何请求其工作负载的网络资源的标准化。这个标准被称为 [Container Networking Interface (CNI)](https://github.com/containernetworking/cni)。今天，与
    Kubernetes 兼容的所有网络选项都符合这一接口。与容器存储接口 (CSI) 和容器运行时接口 (CRI) 类似，这为我们的应用程序平台的网络堆栈提供了灵活性。
- en: 'The CNI specification defines a few key operations:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: CNI规范定义了几个关键操作：
- en: '`ADD`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`ADD`'
- en: Adds a container to the network and responds with the associated interface(s),
    IP(s), and more.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个容器到网络并返回相关接口、IP及其他信息。
- en: '`DELETE`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`DELETE`'
- en: Removes a container from the network and releases all associated resources.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络中删除一个容器并释放所有相关资源。
- en: '`CHECK`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`CHECK`'
- en: Verifies a container’s network is set up properly and responds with an error
    if there are issues.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 验证容器的网络设置是否正确，并在出现问题时返回错误。
- en: '`VERSION`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`VERSION`'
- en: Returns the CNI version(s) supported by the plug-in.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 返回插件支持的CNI版本。
- en: 'This functionality is implemented in a binary that is installed on the host.
    The kubelet will communicate with the appropriate CNI binary based on the configuration
    it expects on the host. An example of this configuration file is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能是通过安装在主机上的二进制文件实现的。kubelet将根据其在主机上预期的配置与适当的CNI二进制文件通信。这个配置文件的示例如下：
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_pod_networking_CO3-1)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_pod_networking_CO3-1)'
- en: The CNI (specification) version this plug-in expects to talk over.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此插件期望使用的CNI（规范）版本进行通信。
- en: '[![2](assets/2.png)](#co_pod_networking_CO3-2)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_pod_networking_CO3-2)'
- en: The CNI driver (binary) to send networking setup requests to.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 发送网络设置请求的CNI驱动（二进制文件）。
- en: '[![3](assets/3.png)](#co_pod_networking_CO3-3)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_pod_networking_CO3-3)'
- en: The IPAM driver to use, specified when the CNI plug-in does not handle IPAM.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNI插件不处理IPAM时指定要使用的IPAM驱动程序。
- en: Note
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Multiple CNI configurations may exist in the CNI *conf* directory. They are
    evaluated lexicographically, and the first configuration will be used.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: CNI *conf*目录中可能存在多个CNI配置。它们按字典顺序评估，并使用第一个配置。
- en: Along with the CNI configuration and CNI binary, most plug-ins run a Pod on
    each host that handles concerns beyond interface attachment and IPAM. This includes
    responsibilities such as route propagation and network policy programming.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了CNI配置和CNI二进制文件外，大多数插件还在每个处理接口附加和IPAM之外的主机上运行一个Pod。这包括路由传播和网络策略编程等职责。
- en: CNI Installation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI安装
- en: 'CNI drivers must be installed on every node taking part in the Pod network.
    Additionally, the CNI configuration must be established. The installation is typically
    handled when you deploy a CNI plug-in. For example, when deploying Cilium, a DaemonSet
    is created, which puts a `cilium` Pod on every node. This Pod features a PostStart
    command that runs the baked-in script *install-cni.sh*. This script will start
    by installing two drivers. First it will install the loopback driver to support
    the `lo` interface. Then it will install the `cilium` driver. The script executes
    conceptually as follows (the example has been greatly simplified for brevity):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参与Pod网络的节点必须安装CNI驱动程序。此外，必须建立CNI配置。通常在部署CNI插件时进行安装。例如，当部署Cilium时，将创建一个DaemonSet，它在每个节点上放置一个`cilium`
    Pod。此Pod具有一个PostStart命令，运行内置脚本*install-cni.sh*。此脚本首先安装回环驱动程序以支持`lo`接口。然后安装`cilium`驱动程序。该脚本的执行概念如下（为简洁起见，此示例已大大简化）：
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After installation, the kubelet still needs to know which driver to use. It
    will look within */etc/cni/net.d/* (configurable via flag) to find a CNI configuration.
    The same *install-cni.sh* script adds this as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，kubelet仍然需要知道要使用哪个驱动程序。它将在*/etc/cni/net.d/*（可通过标志配置）中查找CNI配置。同样的*install-cni.sh*脚本将其添加如下：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To demonstrate this order of operations, let’s take a look a newly bootstrapped,
    single-node cluster. This cluster was bootstrapped using `kubeadm`. Examining
    all Pods reveals that the `core-dns` Pods are not running:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这些操作的顺序，让我们看看一个新引导的单节点集群。此集群使用`kubeadm`引导。检查所有Pod后发现`core-dns` Pods未运行：
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After examining the kubelet logs on the host scheduled to run `core-dns`, it
    becomes clear that the lack of CNI configuration is causing the container runtime
    to not start the Pod:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析预定运行`core-dns`的主机上的kubelet日志后，清楚地表明缺少CNI配置导致容器运行时无法启动Pod：
- en: Note
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This case of DNS not starting is one of the most common indicators of CNI issues
    after cluster bootstrapping. Another symptom is nodes reporting `NotReady` status.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群引导后，DNS未启动的情况是CNI问题最常见的指标之一。另一个症状是节点报告`NotReady`状态。
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The reason Pods such as `kube-apiserver` and `kube-controller-manager` started
    successfully is due to their use of the host network. Since they leverage the
    host network and do not rely on the Pod network, they are not susceptible to the
    same behavior seen by `core-dns`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 `kube-apiserver` 和 `kube-controller-manager` 这样的 Pod 之所以成功启动，是因为它们使用了主机网络。由于它们利用主机网络而不依赖于
    Pod 网络，因此不会受到 `core-dns` 所见行为的影响。
- en: 'Cilium can be deployed to the cluster by simply applying a YAML file from the
    Cilium documentation. In doing so, the aforementioned `cilium` Pod is deployed
    on every node, and the *cni-install.sh* script is run. Examining the CNI bin and
    configuration directories, we can see the installed components:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 可以通过从 Cilium 文档中的 YAML 文件应用到集群来部署。这样做会在每个节点上部署上述的 `cilium` Pod，并运行 *cni-install.sh*
    脚本。检查 CNI bin 和配置目录，我们可以看到安装的组件：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this in place, the kubelet and container runtime are functioning as expected.
    Most importantly, the `core-dns` Pod is up and running! [Figure 5-4](#docker_is_used_to_run_containers_the_kubelet_interacts_with_the_cni_to_attach_network)
    demonstrates the relationship we’ve covered thus far in this section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个配置，kubelet 和容器运行时按预期运行。最重要的是，`core-dns` Pod 正在运行！[图 5-4](#docker_is_used_to_run_containers_the_kubelet_interacts_with_the_cni_to_attach_network)
    展示了我们在本节中所涵盖的关系。
- en: '![prku 0504](assets/prku_0504.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0504](assets/prku_0504.png)'
- en: Figure 5-4\. Docker is used to run containers. The kubelet interacts with the
    CNI to attach network interfaces and configure the Pod’s network.
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. Docker 用于运行容器。kubelet 与 CNI 交互，附加网络接口并配置 Pod 的网络。
- en: While this example explored installation via Cilium, most plug-ins follow a
    similar deployment model. The key justification for plug-in choice is based on
    the discussion in [“Networking Considerations”](#networking_considerations). With
    this in mind, we’ll transition to exploring some CNI plug-ins to better understand
    different approaches.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然此示例通过 Cilium 进行安装探索，但大多数插件遵循类似的部署模型。选择插件的关键理由基于 [“网络考虑”](#networking_considerations)
    中的讨论。考虑到这一点，我们将转向探索一些 CNI 插件，以更好地理解不同的方法。
- en: CNI Plug-ins
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNI 插件
- en: Now we are going to explore a few implementations of CNI. CNI has one of the
    largest array of options relative to other interfaces such as CRI. As such, we
    won’t be exhaustive in the plug-ins we cover and encourage you to explore more
    than what we will. We chose the following plug-ins as a factor of being the most
    common we see at clients and unique enough to demonstrate the variety of approaches.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨几种 CNI 的实现方式。与其他接口（如 CRI）相比，CNI 拥有最多的选项之一。因此，我们在涵盖的插件方面不会穷尽一切，并鼓励您探索更多内容。我们选择以下插件作为客户中最常见且足够独特以展示各种方法的因素。
- en: Note
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A Pod network is foundational to any Kubernetes cluster. As such, your CNI plug-in
    will be in the critical path. As time goes on, you may wish to change your CNI
    plug-in. If this occurs, we recommend rebuilding clusters as opposed to doing
    in-place migrations. In this approach, you spin up a new cluster featuring the
    new CNI. Then, depending on your architecture and operational model, migrate workloads
    to the new cluster. It is possible to do an in-place CNI migration, but it takes
    on nontrivial risk and should be carefully weighed against our recommendation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 网络是任何 Kubernetes 集群的基础。因此，您的 CNI 插件将处于关键路径上。随着时间推移，您可能希望更改您的 CNI 插件。如果发生这种情况，我们建议重建集群，而不是进行原地迁移。在这种方法中，您会启动一个新的集群，使用新的
    CNI。然后，根据您的架构和操作模型，将工作负载迁移到新集群。虽然可以进行原地 CNI 迁移，但这带来了非常重要的风险，应仔细权衡我们的建议。
- en: Calico
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Calico
- en: Calico is a well-established CNI plug-in in the cloud native ecosystem. [Project
    Calico](https://www.projectcalico.org) is the open source project that supports
    this CNI plug-in, and [Tigera](https://www.tigera.io) is the commercial company
    offering enterprise features and support. Calico makes heavy use of BGP to propagate
    workload routes between nodes and to offer integration with larger datacenter
    fabrics. Along with installing a CNI binary, Calico runs a `calico-node` agent
    on each host. This agent features a BIRD daemon for facilitating BGP peering between
    nodes and a Felix agent, which takes the known routes and programs them into the
    kernel route tables. This relationship is demonstrated in [Figure 5-5](#calico_component_relationship_showing_the_bgp_peering_to_communicate).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Calico是云原生生态系统中一个成熟的CNI插件。[Project Calico](https://www.projectcalico.org)是支持此CNI插件的开源项目，[Tigera](https://www.tigera.io)是提供企业功能和支持的商业公司。Calico大量使用BGP在节点之间传播工作负载路由，并与更大的数据中心结构集成。除了安装CNI二进制文件外，Calico还在每个主机上运行一个`calico-node`代理。该代理使用BIRD守护程序促进节点之间的BGP对等体系，并使用Felix代理将已知路由编程到内核路由表中。这种关系在[图5-5](#calico_component_relationship_showing_the_bgp_peering_to_communicate)中有所展示。
- en: '![prku 0505](assets/prku_0505.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0505](assets/prku_0505.png)'
- en: Figure 5-5\. Calico component relationship showing the BGP peering to communicate
    routes and the programming of iptables and kernel routing tables accordingly.
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5. Calico组件关系展示BGP对等体系以及相应地通信路由和iptables与内核路由表的编程。
- en: 'For IPAM, Calico initially respects the `cluster-cidr` setting described in
    [“IP Address Management”](#IPAM). However, its capabilities are far greater than
    relying on a CIDR allocation per node. Calico creates CRDs called [IPPools](https://oreil.ly/-Nd-Q).
    This provides a lot of flexibility in IPAM, specifically enabling features such
    as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IPAM，Calico最初遵循描述在[“IP地址管理”](#IPAM)中的`cluster-cidr`设置。然而，它的功能远不止依赖于每个节点的CIDR分配。Calico创建称为[IPPools](https://oreil.ly/-Nd-Q)的CRD，这在IPAM中提供了很大的灵活性，特别是支持以下功能：
- en: Configuring block size per node
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置每个节点的块大小
- en: Specifying what node(s) an IPPool applies to
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定IP池适用于哪些节点
- en: Allocating IPPools to Namespaces, rather than nodes
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将IPPools分配给命名空间，而不是节点
- en: Configuring routing behavior
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置路由行为
- en: 'Paired with the ability to have multiple pools per cluster, you have a lot
    of flexibility in IPAM and network architecture. By default, clusters run a single
    IPPool, as shown here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 配合每个集群可以有多个池的能力，您在IPAM和网络架构上有很大的灵活性。默认情况下，集群运行单个IP池，如下所示：
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_pod_networking_CO4-1)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_pod_networking_CO4-1)'
- en: The cluster’s Pod network CIDR.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的Pod网络CIDR。
- en: '[![2](assets/2.png)](#co_pod_networking_CO4-2)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_pod_networking_CO4-2)'
- en: The size of each node-level CIDR allocation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点级别CIDR分配的大小。
- en: '[![3](assets/3.png)](#co_pod_networking_CO4-3)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_pod_networking_CO4-3)'
- en: The encapsulation mode.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 封装模式。
- en: 'Calico offers a variety of ways to route packets inside of the cluster. This
    includes:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Calico提供多种路由集群内部数据包的方式。这包括：
- en: Native
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本地
- en: No encapsulation of packets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不封装数据包。
- en: IP-in-IP
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: IP-in-IP
- en: Simple encapsulation. IP packet is placed in the payload of another.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 简单封装。IP数据包放置在另一个数据包的负载中。
- en: VXLAN
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN
- en: Advanced encapsulation. An entire L2 frame is encapsulated within a UDP packet.
    Establishes a virtual L2 overlay.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 高级封装。整个L2帧被封装在一个UDP数据包中。建立虚拟L2覆盖。
- en: Your choice is often a function of what your network can support. As described
    in [“Routing Protocols”](#routing_protocols), native routing will likely provide
    the best performance, smallest packet size, and simplest troubleshooting experience.
    However, in many environments, especially those involving multiple subnets, this
    mode is not possible. The encapsulation approaches work in most environments,
    especially VXLAN. Additionally, the VXLAN mode does not require usage of BGP,
    which can be a solution to environments where BGP peering is blocked. One unique
    feature of Calico’s encapsulation approach is that it can be enabled exclusively
    for traffic that crosses a subnet boundary. This enables near native performance
    when routing within the subnet while not breaking routing outside the subnet.
    This can be enabled by setting the IPPool’s `ipipMode` to `CrossSubnet`. [Figure 5-6](#traffic_behavior_when_cross_subnet)
    demonstrates this behavior.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你的选择往往取决于你的网络支持能力。正如在[“路由协议”](#routing_protocols)中所描述的，本地路由很可能提供最佳性能、最小的数据包大小和最简单的故障排除体验。然而，在许多环境中，特别是涉及多个子网的情况下，这种模式是不可能的。封装方法在大多数环境中都有效，特别是VXLAN。此外，VXLAN模式不需要使用BGP，这可以解决BGP对等连接被阻止的环境。Calico封装方法的一个独特特性是，它可以专门用于跨越子网边界的流量。这使得在子网内进行路由时接近本地性能，同时不会破坏子网外的路由。通过将IP池的`ipipMode`设置为`CrossSubnet`即可启用此功能。[图 5-6](#traffic_behavior_when_cross_subnet)展示了这种行为。
- en: '![prku 0506](assets/prku_0506.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0506](assets/prku_0506.png)'
- en: Figure 5-6\. Traffic behavior when CrossSubnet IP-in-IP mode is enabled.
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 启用CrossSubnet IP-in-IP模式时的流量行为。
- en: 'For deployments of Calico that keep BGP enabled, by default, no additional
    work is needed thanks to the built-in BGP daemon in the `calico-node` Pod. In
    more complex architectures, organizations use this BGP functionality as a way
    to introduce [route reflectors](https://tools.ietf.org/html/rfc4456), sometimes
    required at large scales when the (default) full-mesh approach becomes limited.
    Along with route reflectors, peering can be configured to talk to network routers,
    which in turn can make the overall network aware of routes to Pod IPs. This is
    all configured using Calico’s BGPPeer CRD, seen here:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于保持启用BGP的Calico部署，默认情况下，由于`calico-node` Pod中内置了BGP守护程序，不需要额外工作。在更复杂的架构中，组织使用此BGP功能作为引入[路由反射器](https://tools.ietf.org/html/rfc4456)的一种方式，特别是在大规模使用默认的全网格方法受限时。除了路由反射器，还可以配置对网络路由器的对等连接，从而使整个网络了解到Pod
    IP的路由。这一切都可以使用Calico的BGPPeer CRD进行配置，如下所示：
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_pod_networking_CO5-1)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_pod_networking_CO5-1)'
- en: The IP of the device to (bgp) peer with.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与之对等连接的设备的IP。
- en: '[![2](assets/2.png)](#co_pod_networking_CO5-2)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_pod_networking_CO5-2)'
- en: The [autonomous system](https://oreil.ly/HiXLN) ID of the cluster.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '集群的自治系统[标识](https://oreil.ly/HiXLN)。 '
- en: '[![3](assets/3.png)](#co_pod_networking_CO5-3)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_pod_networking_CO5-3)'
- en: Which cluster nodes should peer with this device. This field is optional. When
    omitted, the BGPPeer configuration is considered *global*. Not peering global
    is advisable only when a certain set of nodes should offer a unique routing capability,
    such as offering routable IPs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 应与此设备进行对等连接的集群节点。此字段是可选的。当省略时，BGPPeer配置被视为*全局*。仅在某些节点集应提供唯一路由功能（例如提供可路由的IP时）时，不对等全局是明智的选择。
- en: In terms of network policy, Calico fully implements the Kubernetes NetworkPolicy
    API. Calico offers two additional CRDs for increased functionality. These include
    [(projectcalico.org/v3).NetworkPolicy](https://oreil.ly/oMsCm) and [GlobalNetworkPolicy](https://oreil.ly/3pUOs).
    These Calico-specific APIs look similar to Kubernetes NetworkPolicy but feature
    more capable rules and richer expressions for evaluation. Also, policy ordering
    and application layer policy (requires integration with Istio) are supported.
    GlobalNetworkPolicy is particularly useful because it applies policy at a cluster-wide
    level. This makes it easier to achieve models such as micro-segmentation, where
    all traffic is denied by default and egress/ingress is opened up based on the
    workload needs. You can apply a GlobalNetworkPolicy that denies all traffic except
    for critical services such as DNS. Then, at a Namespace level, you can open up
    access to ingress and egress accordingly. Without GlobalNetworkPolicy, we’d need
    to add and manage deny-all rules in *every* Namespace.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络策略方面，Calico完全实现了Kubernetes NetworkPolicy API。Calico提供了两个额外的CRD以增强功能。这些包括[(projectcalico.org/v3).NetworkPolicy](https://oreil.ly/oMsCm)和[GlobalNetworkPolicy](https://oreil.ly/3pUOs)。这些Calico特定的API看起来类似于Kubernetes
    NetworkPolicy，但具有更强大的规则和更丰富的表达式用于评估。此外，支持策略排序和应用层策略（需要与Istio集成）。GlobalNetworkPolicy特别有用，因为它在集群范围内应用策略。这使得实现微分段化等模型变得更加容易，其中所有流量默认被拒绝，根据工作负载需求打开出口/入口。您可以应用一个GlobalNetworkPolicy，拒绝除DNS等关键服务之外的所有流量。然后，在Namespace级别，您可以相应地开放对入口和出口的访问。如果没有GlobalNetworkPolicy，我们将需要在*每个*Namespace中添加和管理拒绝所有规则。
- en: Note
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Historically, Calico has made use of iptables to implement packet routing decisions.
    For Services, Calico relies on the programming done by kube-proxy to resolve an
    endpoint for a Service. For network policy, Calico programs iptables to determine
    whether a packet should be allowed to enter or leave the host. At the time of
    this writing, Calico has introduced an eBPF dataplane option. We expect, over
    time, more functionality used by Calico to be moved into this model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在历史上，Calico使用iptables来实现数据包路由决策。对于服务，Calico依赖kube-proxy进行编程，以解析服务的端点。对于网络策略，Calico编程iptables确定数据包是否允许进入或离开主机。在撰写本文时，Calico引入了eBPF数据平面选项。我们预计随着时间的推移，Calico使用的更多功能将转移到这种模型中。
- en: Cilium
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cilium
- en: Cilium is a newer CNI plug-in relative to Calico. It’s the first CNI plug-in
    to utilize the [extended Berkeley Packet Filter (eBPF)](https://ebpf.io). This
    means that rather than processing packets in userspace it is able to do so without
    leaving kernel space. Paired with [eXpress Data Path (XDP)](https://oreil.ly/M3m6t),
    hooks may be established in the NIC driver to make routing decisions. This enables
    routing decisions to occur immediately when the packet is received.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于Calico，Cilium是一种较新的CNI插件。它是第一个利用[扩展伯克利数据包过滤器（eBPF）](https://ebpf.io)的CNI插件。这意味着它可以在不离开内核空间的情况下处理数据包。与[eXpress
    Data Path（XDP）](https://oreil.ly/M3m6t)配对，可以在NIC驱动程序中建立钩子以进行路由决策。这使得数据包接收时能够立即进行路由决策。
- en: As a technology, eBPF has demonstrated performance and scale at organizations
    such as [Facebook](https://oreil.ly/agUXl) and [Netflix](https://oreil.ly/Ubt1Q).
    With the usage of eBPF, Cilium is able to claim increased features around scalability,
    observability, and security. This deep integration with BPF means that common
    CNI concerns such as NetworkPolicy enforcement are no longer handled via iptables
    in userspace. Instead, extensive use of [eBPF maps](https://oreil.ly/4Rdvf) enable
    decisions to occur quickly in a way that scales as more and more rules are added.
    [Figure 5-7](#cilium_interacts_with_ebpf_maps_and_programs_at_the_kernel_level)
    shows a high-level overview of the stack with Cilium installed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种技术，eBPF已经在Facebook和Netflix等组织中展示了性能和规模。使用eBPF，Cilium能够宣称在可伸缩性、可观察性和安全性方面具有增强功能。与BPF的深度集成意味着，像NetworkPolicy执行这样的常见CNI问题不再通过用户空间中的iptables处理。相反，广泛使用的[eBPF
    maps](https://oreil.ly/4Rdvf)使决策能够快速发生，并且随着添加更多规则而扩展。[Figure 5-7](#cilium_interacts_with_ebpf_maps_and_programs_at_the_kernel_level)显示了安装了Cilium后的堆栈的高级概述。
- en: '![prku 0507](assets/prku_0507.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0507](assets/prku_0507.png)'
- en: Figure 5-7\. Cilium interacts with eBPF maps and programs at the kernel level.
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. Cilium与内核级别的eBPF映射和程序交互。
- en: 'For IPAM, Cilium follows the model of either delegating IPAM to a cloud provider
    integration or managing it itself. In the most common scenario of Cilium managing
    IPAM, it will allocate Pod CIDRs to each node. By default, Cilium will manage
    these CIDRs independent of Kubernetes Node allocations. The node-level addressing
    will be exposed in the `CiliumNode` CRD. This will provide greater flexibility
    in management of IPAM and is preferable. If you wish to stick to the default CIDR
    allocations done in Kubernetes based on its Pod CIDR, Cilium offer a `kubernetes`
    IPAM mode. This will rely on the Pod CIDR allocated to each node, which is exposed
    in the Node object. Following is an example of a `CiliumNode` object. You can
    expect one of these to exist for each node in the cluster:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IPAM，Cilium遵循将IPAM委托给云提供商集成或自行管理的模型。在Cilium管理IPAM的最常见情况下，它将为每个节点分配Pod CIDR。默认情况下，Cilium将独立管理这些CIDR，而不依赖于Kubernetes节点分配。节点级别的地址将在`CiliumNode`
    CRD中公开。这将提供更大的IPAM管理灵活性，并且是首选的方法。如果希望保持基于Kubernetes的Pod CIDR默认分配的CIDR分配方式，Cilium提供了`kubernetes`
    IPAM模式。这将依赖于每个节点分配的Pod CIDR，该CIDR在节点对象中公开。以下是`CiliumNode`对象的示例。您可以期望在集群中的每个节点上存在这样一个对象：
- en: '[PRE12]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_pod_networking_CO6-1)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_pod_networking_CO6-1)'
- en: IP address of this workload node.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此工作负载节点的IP地址。
- en: '[![2](assets/2.png)](#co_pod_networking_CO6-2)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_pod_networking_CO6-2)'
- en: 'CIDR allocated to this node. The size of this allocation can be controlled
    in Cilium’s config using `cluster-pool-ipv4-mask-size: "24"`.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '分配给此节点的CIDR。可以使用Cilium的配置控制此分配的大小，例如`cluster-pool-ipv4-mask-size: "24"`。'
- en: 'Similar to Calico, Cilium offers encapsulated and native routing modes. The
    default mode is encapsulated. Cilium supports using tunneling protocols VXLAN
    or Geneve. This mode should work with most networks as long as host-to-host routability
    pre-exists. To run in native mode, Pod routes must be understood at some level.
    For example, Cilium supports using AWS’s ENI for IPAM. In this model, the Pod
    IPs are known to the VPC and are inherently routable. To run a native-mode with
    Cilium-managed IPAM, assuming the cluster runs in the same L2 segment, `auto-direct-node-routes:
    true` can be added to Cilium’s configuration. Cilium will then program the host’s
    route tables accordingly. If you span L2 networks, you may need to introduce additional
    routing protocols such as BGP to distribute routes.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于Calico，Cilium提供了封装和本地路由模式。默认模式是封装。Cilium支持使用隧道协议VXLAN或Geneve。只要主机到主机的路由可达，此模式应该适用于大多数网络。要以本地模式运行，必须在某个级别上理解Pod路由。例如，Cilium支持使用AWS的ENI进行IPAM。在这种模型中，Pod的IP已知于VPC并且天然可路由。要在Cilium管理的IPAM下运行本地模式，假设集群运行在相同的L2段内，可以在Cilium的配置中添加`auto-direct-node-routes:
    true`。然后Cilium将相应地编程主机的路由表。如果跨越L2网络，则可能需要引入额外的路由协议，如BGP来分发路由。'
- en: In terms of network policy, Cilium can enforce the Kubernetes [NetworkPolicy
    API](https://oreil.ly/_WUKS). As an alternative to this policy, Cilium offers
    its own [CiliumNetworkPolicy](https://oreil.ly/EpkhJ) and [CiliumClusterwideNetworkPolicy](https://oreil.ly/RtYH5).
    The key difference between these two is the scope of the policy. CiliumNetworkPolicy
    is Namespace scoped, while CiliumClusterwideNetworkPolicy is cluster-wide. Both
    of these have increased functionality beyond the capabilities of Kubernetes NetworkPolicy.
    Along with supporting label-based layer 3 policy, they support policy based on
    DNS resolution and application-level (layer 7) requests.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络策略方面，Cilium可以强制执行Kubernetes的[NetworkPolicy API](https://oreil.ly/_WUKS)。作为此策略的替代，Cilium提供了自己的[CiliumNetworkPolicy](https://oreil.ly/EpkhJ)和[CiliumClusterwideNetworkPolicy](https://oreil.ly/RtYH5)。这两者之间的关键区别在于策略的范围。CiliumNetworkPolicy是命名空间范围的，而CiliumClusterwideNetworkPolicy是整个集群范围的。这两者都具有超出Kubernetes
    NetworkPolicy功能的增强功能。除了支持基于标签的第三层策略外，它们还支持基于DNS解析和应用级（第7层）请求的策略。
- en: 'While most CNI plug-ins don’t involve themselves with Services, Cilium offers
    a fully featured kube-proxy replacement. This functionality is built into the
    `cilium-agent` deployed to each node. To deploy in the mode, you’ll want to ensure
    kube-proxy is absent from your cluster and that the `KubeProxyReplacement` setting
    is set to `strict` in Cilium. When using this mode, Cilium will configure routes
    for Services within eBPF maps, making resolution as fast as O(1). This is in contrast
    to kube-proxy, which implements Services in iptables chains and can cause issues
    at scale and/or when there is high churn of Services. Additionally, the CLI provided
    by Cilium offers a good experience when troubleshooting constructs such as Services
    or network policy. Rather than trying to interpret iptables chains, you can query
    the system as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数 CNI 插件不涉及服务，Cilium 提供了一个完整的 kube-proxy 替代方案。此功能内置于部署在每个节点的 `cilium-agent`
    中。在部署此模式时，您需要确保集群中不存在 kube-proxy，并且在 Cilium 中将 `KubeProxyReplacement` 设置为 `strict`。在使用此模式时，Cilium
    将通过 eBPF 映射为服务配置路由，使解析速度达到 O(1)。这与 kube-proxy 实现服务在 iptables 链中有所不同，在规模或服务高变动时可能会出现问题。此外，Cilium
    提供的 CLI 在诸如服务或网络策略的故障排除时提供了良好的体验。您可以直接查询系统，而无需解释 iptables 链：
- en: '[PRE13]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Cilium’s use of eBPF programs and maps makes it an extremely compelling and
    interesting CNI option. By continuing to leverage eBPF programs, more functionality
    is being introduced that integrates with Cilium—for example, the ability to extract
    flow data, policy violations, and more. To extract and present this valuable data,
    [hubble](https://github.com/cilium/hubble) was introduced. It makes use of Cilium’s
    eBPF programs to provide a UI and CLI for operators.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 的使用 eBPF 程序和映射使其成为一种非常引人注目和有趣的 CNI 选项。通过继续利用 eBPF 程序，引入了更多与 Cilium 集成的功能，例如提取流数据、策略违规等。为了提取并展示这些宝贵的数据，引入了
    [hubble](https://github.com/cilium/hubble)。它利用 Cilium 的 eBPF 程序为运维人员提供了 UI 和 CLI。
- en: Lastly, we should mention that the eBPF functionality made available by Cilium
    can be run alongside many existing CNI providers. This is achieved by running
    Cilium in its CNI chaining mode. In this mode, an existing plug-in such as AWS’s
    VPC CNI will handle routing and IPAM. Cilium’s responsibility will exclusively
    be the functionality offered by its various eBPF programs including network observability,
    load balancing, and network policy enforcement. This approach can be preferable
    when you either cannot fully run Cilium in your environment or wish to test out
    its functionality alongside your current CNI choice.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该提到，Cilium 提供的 eBPF 功能可以与许多现有的 CNI 提供程序并行运行。这是通过在其 CNI 链接模式下运行 Cilium
    实现的。在此模式下，如 AWS 的 VPC CNI 这样的现有插件将处理路由和 IPAM。Cilium 的责任将专门是其各种 eBPF 程序提供的功能，包括网络可观察性、负载均衡和网络策略强制执行。在您无法完全在环境中运行
    Cilium 或希望在当前 CNI 选择旁边测试其功能时，这种方法可能更可取。
- en: AWS VPC CNI
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS VPC CNI
- en: AWS’s VPC CNI demonstrates a very different approach to what we have covered
    thus far. Rather than running a Pod network independent of the node network, it
    fully integrates Pods into the same network. Since a second network is not being
    introduced, the concerns around distributing routes or tunneling protocols are
    no longer needed. When a Pod is provided an IP, it becomes part of the network
    in the same way an EC2 host would. It is subject to the same [route tables](https://oreil.ly/HYHHp)
    as any other host in the subnet. Amazon refers to this as native VPC networking.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 的 VPC CNI 展示了与迄今为止讨论的内容非常不同的方法。它不是将 Pod 网络独立于节点网络运行，而是完全将 Pod 集成到相同的网络中。由于不引入第二个网络，因此不再需要关注路由分发或隧道协议的问题。当为
    Pod 提供 IP 后，它就像 EC2 主机一样成为网络的一部分。它受到与子网中任何其他主机相同的 [路由表](https://oreil.ly/HYHHp)
    影响。亚马逊称之为本地 VPC 网络。
- en: For IPAM, a daemon will attach a second [elastic network interface (ENI)](https://oreil.ly/NBjs3)
    to the Kubernetes node. It will then maintain a pool of [secondary IPs](https://oreil.ly/vUGdI)
    that will eventually get attached to Pods. The amount of IPs available to a node
    depends on the EC2 instance size. These IPs are typically “private” IPs from within
    the VPC. As mentioned earlier in this chapter, this will consume IP space from
    your VPC and make the IPAM system more complex than a completely independent Pod
    network. However, the routing of traffic and troubleshooting has been significantly
    simplified given we are not introducing a new network! [Figure 5-8](#the_ipam_daemon_is_responsible_for_maintaining_the_eni_and_pool_of)
    demonstrates the IPAM setup with AWS VPC CNI.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 IPAM，守护进程将在 Kubernetes 节点上附加第二个[弹性网络接口（ENI）](https://oreil.ly/NBjs3)。然后，它将维护一个[次要IP地址池](https://oreil.ly/vUGdI)，这些IP地址最终会附加到
    Pods 上。每个节点可用的 IP 数量取决于 EC2 实例的大小。这些 IP 通常是 VPC 内的“私有”IP地址。正如本章前面提到的，这将消耗 VPC
    中的 IP 空间，并使 IPAM 系统比完全独立的 Pod 网络更加复杂。但是，由于我们没有引入新的网络，流量路由和故障排除显著简化了！[图 5-8](#the_ipam_daemon_is_responsible_for_maintaining_the_eni_and_pool_of)
    展示了使用 AWS VPC CNI 的 IPAM 设置。
- en: '![prku 0508](assets/prku_0508.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0508](assets/prku_0508.png)'
- en: Figure 5-8\. The IPAM daemon is responsible for maintaining the ENI and pool
    of secondary IPs.
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. IPAM 守护程序负责维护 ENI 和次要 IP 地址池。
- en: Note
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The use of ENIs will impact the number of Pods you can run per node. AWS [maintains
    a list on its GitHub page](https://oreil.ly/jk_XL) that correlates instance type
    to max Pods.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ENI 将影响您可以在每个节点上运行的 Pod 数量。AWS 在其 GitHub 页面上[维护了一个列表](https://oreil.ly/jk_XL)，将实例类型与最大
    Pod 数量进行了对应。
- en: Multus
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Multus
- en: So far, we have covered specific CNI plug-ins that attach an interface to a
    Pod, thus making it available on a network. But what if a Pod needs to be attached
    to more than one network? This is where the Multus CNI plug-in comes in. While
    not extremely common, there are use cases in the telecommunications industry that
    require their network function virtualizations (NFVs) to route traffic to a specific,
    dedicated, network.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了特定的 CNI 插件，这些插件将一个接口附加到一个 Pod 上，从而使其在网络上可用。但是，如果一个 Pod 需要连接到多个网络怎么办？这就是
    Multus CNI 插件发挥作用的地方。虽然不是非常常见，但在电信行业中存在一些使用案例，这些案例要求它们的网络功能虚拟化（NFV）将流量路由到特定的专用网络。
- en: 'Multus can be thought of as a CNI that enables using multiple other CNIs. In
    this model, Multus becomes the CNI plug-in interacted with by Kubernetes. Multus
    is configured with a default network that is commonly the network expected to
    facilitate Pod-to-Pod communication. This could even be one of the plug-ins we’ve
    talked about in this chapter! Then, Multus supports configuring secondary networks
    by specifying additional plug-ins that can be used to attach another interface
    to a Pod. Pods can then be annotated with something like `k8s.v1.cni.cncf.io/networks:
    sriov-conf` to attach an additional network. [Figure 5-9](#shows_the_traffic_flow_of_a_multi_network_multus_configuration)
    shows the result of this configuration.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 'Multus 可以被视为启用多个其他 CNI 的 CNI。在这种模型中，Multus 成为 Kubernetes 交互的 CNI 插件。Multus
    配置了一个默认网络，通常是预期用于促进 Pod 与 Pod 通信的网络。这甚至可能是本章中我们讨论过的插件之一！然后，Multus 支持通过指定其他可以用于将另一个接口附加到
    Pod 的插件来配置次要网络。Pods 可以像这样被注释：`k8s.v1.cni.cncf.io/networks: sriov-conf` 来附加额外的网络。[图
    5-9](#shows_the_traffic_flow_of_a_multi_network_multus_configuration) 展示了这种配置的结果。'
- en: '![prku 0509](assets/prku_0509.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![prku 0509](assets/prku_0509.png)'
- en: Figure 5-9\. The traffic flow of a multinetwork Multus configuration.
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 多网络 Multus 配置的流量流向。
- en: Additional Plug-ins
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外的插件
- en: 'The landscape of plug-ins is vast, and we’ve covered only a very small subset.
    However, the ones covered in this chapter do identify some of the key variances
    you’ll find in plug-ins. The majority of alternatives take differing approaches
    to the engine used to facilitate the networking, yet many core principles stay
    the same. The following list identifies some additional plug-ins and gives a small
    glimpse into their networking approach:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 插件的选择很广泛，我们只覆盖了非常小的一部分。然而，本章介绍的插件确实识别了一些插件中的关键差异。大多数替代方案采用不同的方法来实现网络功能，但许多核心原则保持不变。以下列表识别了一些额外的插件，并对其网络方法给予了一小部分示例：
- en: '[Antrea](https://antrea.io/docs)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[Antrea](https://antrea.io/docs)'
- en: Data plane is facilitated via [Open vSwitch](https://www.openvswitch.org). Offers
    high-performance routing along with the ability to introspect flow data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面通过 [Open vSwitch](https://www.openvswitch.org) 实现。提供高性能路由以及检查流数据的能力。
- en: '[Weave](https://www.weave.works/oss/net)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[Weave](https://www.weave.works/oss/net)'
- en: Overlay network that provides many mechanisms to route traffic—for example,
    the fast datapath options using OVS modules to keep packet processing in the kernel.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 提供许多机制来路由流量的覆盖网络，例如使用 OVS 模块的快速数据路径选项来保持内核中的数据包处理。
- en: '[flannel](https://github.com/coreos/flannel)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[flannel](https://github.com/coreos/flannel)'
- en: Simple layer-3 network for Pods and one of the early CNIs. It supports multiple
    backends yet is most commonly configured to use VXLAN.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的第三层网络用于 Pods 和早期的一个 CNI。它支持多种后端配置，但通常配置为使用 VXLAN。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The Kubernetes/container networking ecosystem is filled with options. This is
    good! As we’ve covered throughout this chapter, networking requirements can vary
    significantly from organization to organization. Choosing a CNI plug-in is likely
    to be one of the most foundational considerations for your eventual application
    platform. While exploring the many options may feel overwhelming, we highly recommend
    you work to better understand the networking requirements of your environment
    and applications. With a deep understanding of this, the right networking plug-in
    choice should fall into place!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes/容器网络生态系统充满了选择。这是件好事！正如我们在本章中所介绍的，网络需求在不同组织之间可能有显著差异。选择一个 CNI 插件可能是您最终应用平台中最基础的考虑之一。虽然探索如此多的选择可能会让人感到不知所措，但我们强烈建议您努力更好地理解您环境和应用程序的网络需求。通过深入了解，正确的网络插件选择应该就会水到渠成！
