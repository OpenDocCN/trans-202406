- en: Chapter 5\. Managing Resources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nothing is enough to the man for whom enough is too little.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Epicurus
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll look at how to make the most of your cluster: how to
    manage and optimize resource usage, how to manage the life cycle of containers,
    and how to partition the cluster using namespaces. We’ll also outline some techniques
    and best practices for keeping down the cost of your cluster, while getting the
    most for your money.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: You’ll learn how to use resource requests, limits, and defaults, and how to
    optimize them with the Vertical Pod Autoscaler; how to use readiness probes, liveness
    probes, and Pod disruption budgets to manage containers; how to optimize cloud
    storage; and how and when to use preemptible or reserved instances to control
    costs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Resources
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you have a Kubernetes cluster of a given capacity, with a reasonable
    number of nodes of the right kind of size. How do you get the most bang for your
    buck out of it? That is, how do you get the best possible utilization of the available
    cluster resources for your workload, while still ensuring that you have enough
    headroom to deal with demand spikes, node failures, and bad deployments?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: To answer this, put yourself in the place of the Kubernetes scheduler and try
    to see things from its point of view. The scheduler’s job is to decide where to
    run a given Pod. Are there any nodes with enough free resources to run the Pod?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This question is impossible to answer unless the scheduler knows how many resources
    the Pod will need to run. A Pod that needs 1 GiB of memory cannot be scheduled
    on a node with only one hundred MiB of free memory.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the scheduler has to be able to take action when a greedy Pod is
    grabbing too many resources and starving other Pods on the same node. But how
    much is too much? In order to schedule Pods effectively, the scheduler has to
    know the minimum and maximum allowable resource requirements for each Pod.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s where Kubernetes resource requests and limits come in. Kubernetes understands
    how to manage two kinds of resources: CPU and memory. There are other important
    types of resources, too, such as network bandwidth, disk I/O operations (IOPS),
    and disk space, and these may cause contention in the cluster, but Kubernetes
    doesn’t yet have a way to describe Pods’ requirements for these.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Resource Units
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CPU usage for Pods is expressed, as you might expect, in units of CPUs. One
    Kubernetes CPU unit is equivalent to one AWS virtual CPU (vCPU), one Google Cloud
    Core, one Azure vCore, or one *hyperthread* on a bare-metal processor that supports
    hyperthreading. In other words, *1 CPU* in Kubernetes terms means what you think
    it does.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Because most Pods don’t need a whole CPU, requests and limits are usually expressed
    in *millicpus* (sometimes called *millicores*). Memory is measured in bytes, or
    more handily, *mebibytes* (MiB).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Resource Requests
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Kubernetes *resource request* specifies the minimum amount of that resource
    that the Pod needs to run. For example, a resource request of `100m` (100 millicpus)
    and `250Mi` (250 MiB of memory) means that the Pod cannot be scheduled on a node
    with less than those resources available. If there isn’t any node with enough
    capacity available, the Pod will remain in a `pending` state until there is.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: For example, if all your cluster nodes have two CPU cores and 4 GiB of memory,
    a container that requests 2.5 CPUs will never be scheduled, and neither will one
    that requests 5 GiB of memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what resource requests would look like, applied to our demo application:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resource Limits
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *resource limit* specifies the maximum amount of resource that a Pod is allowed
    to use. A Pod that tries to use more than its allocated CPU limit will be *throttled*,
    reducing its performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: A Pod that tries to use more than the allowed memory limit, though, will be
    terminated. If the terminated Pod can be rescheduled, it will be. In practice,
    this may mean that the Pod is simply restarted on the same node.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Some applications, such as network servers, can consume more and more resources
    over time in response to increasing demand. Specifying resource limits is a good
    way to prevent such hungry Pods from using more than their fair share of the cluster’s
    capacity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of setting resource limits on the demo application:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Knowing what limits to set for a particular application is a matter of observation
    and judgment (see [“Optimizing Pods”](#optimizingpods)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes allows resources to be *overcommitted*; that is, the sum of all
    the resource limits of containers on a node can exceed the total resources of
    that node. This is a kind of gamble: the scheduler is betting that, most of the
    time, most containers will not need to hit their resource limits.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: If this gamble fails, and total resource usage starts approaching the maximum
    capacity of the node, Kubernetes will start being more aggressive in terminating
    containers. Under conditions of resource pressure, containers that have exceeded
    their requests, but not their limits, may still be terminated.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Quality of Service
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the requests and limits of a Pod, Kubernetes will classify it as one
    of the following [Quality of Service (QoS) classes](https://oreil.ly/x0m1T): *Guaranteed*,
    *Burstable*, or *BestEffort*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: When requests match limits, a Pod is classified in the Guaranteed class, meaning
    it is considered by the control plane to be among the most important Pods to schedule,
    and it will do its best to ensure the Pod is only killed if it exceeds the specified
    limits. For highly critical production workloads you may want to consider setting
    your limits and requests to match in order to prioritize scheduling these containers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Burstable Pods have lower priority than Guaranteed Pods, and Kubernetes will
    allow them to “burst” above their request up to their limit if capacity on the
    node is available. If the Pod is using more resources than requested, the Pod
    *may* be killed, if the control plane needs to make way for scheduling a Pod of
    a higher QoS class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: If a Pod does not specify any requests or limits, it is considered to be *Best-Effort*,
    which is the lowest priority. Pods are allowed to use whatever CPU and memory
    is available on the node, but it would be the first to be killed when the cluster
    needs to make room for higher QoS Pods.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Always specify resource requests and limits for your containers. This helps
    Kubernetes schedule and manage your Pods properly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Managing the Container Life Cycle
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve seen that Kubernetes can best manage your Pods when it knows what their
    CPU and memory requirements are. But it also has to know when a container is working:
    that is, when it’s functioning properly and ready to handle requests.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: It’s quite common for containerized applications to get into a stuck state,
    where the process is still running, but it’s not serving any requests. Kubernetes
    needs a way to detect this situation so that it can restart the container to fix
    the problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Liveness Probes
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes lets you specify a *liveness* probe as part of the container spec:
    a health check that determines whether or not the container is alive (that is,
    working).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'For an HTTP server container, the liveness probe specification usually looks
    something like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `httpGet` probe makes an HTTP request to a URI and port you specify; in
    this case, `/healthz` on port 8888.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: If your application doesn’t have a specific endpoint for a health check, you
    could use `/`, or any valid URL for your application. It’s common practice, though,
    to create a `/healthz` endpoint just for this purpose. (Why the `z`? Just to make
    sure it doesn’t collide with an existing path like `health`, which could be a
    page about health information, for example).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: If the application responds with an HTTP 2xx or 3xx status code, Kubernetes
    considers it alive. If it responds with anything else, or doesn’t respond at all,
    the container is considered dead, and will be restarted.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Probe Delay and Frequency
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How soon should Kubernetes start checking your liveness probe? No application
    can start instantly. If Kubernetes tried the liveness probe immediately after
    starting the container, it would probably fail, causing the container to be restarted—and
    this loop would repeat forever!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The `initialDelaySeconds` field lets you tell Kubernetes how long to wait before
    trying the first liveness probe, avoiding this *loop of death* situation. As of
    version 1.20 of Kubernetes, there is also a dedicated `startupProbe` feature for
    configuring a probe to determine when an application has finished starting up.
    See [“Startup Probes”](#startup) for more details.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: It wouldn’t be a good idea for Kubernetes to hammer your application with requests
    for the `healthz` endpoint thousands of times a second. Your health check endpoints
    should be fast and not add noticeable load to the app. You wouldn’t want your
    user experience to suffer because your app is otherwise busy responding to a flood
    of health checks. The `periodSeconds` field specifies how often the liveness probe
    should be checked; in this example, every three seconds.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`failureThreshold` allows you to set how many times the probe can fail before
    Kubernetes considers the application unhealthy. The default is three, which allows
    for a few hiccups in your app, but you may need to make that lower or higher depending
    on how aggressive you want the scheduler to be when making decisions about determining
    an application’s health.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Other Types of Probes
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`httpGet` isn’t the only kind of probe available; for network servers that
    don’t speak HTTP, you can use `tcpSocket`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If a TCP connection to the specified port succeeds, the container is alive.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also run an arbitrary command on the container, using an `exec` probe:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `exec` probe runs the specified command inside the container, and the probe
    succeeds if the command succeeds (that is, exits with a zero status). `exec` is
    usually more useful as a readiness probe, and we’ll see how they’re used in the
    next section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Readiness Probes
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Related to the liveness probe, but with different semantics, is the *readiness
    probe*. Sometimes an application needs to signal to Kubernetes that it’s temporarily
    unable to handle requests; perhaps because it’s performing some lengthy initialization
    process, or waiting for some subprocess to complete. The readiness probe serves
    this function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'If your application doesn’t start listening for HTTP until it’s ready to serve,
    your readiness probe can be the same as your liveness probe:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A container that fails its readiness probe will be removed from any Services
    that match the Pod. This is like taking a failing node out of a load balancer
    pool: no traffic will be sent to the Pod until its readiness probe starts succeeding
    again. Note that this is different from a `livenessProbe` because a failing `readinessProbe`
    does not kill and restart the Pod.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Normally, when a Pod starts, Kubernetes will start sending it traffic as soon
    as the container is in a running state. However, if the container has a readiness
    probe, Kubernetes will wait until the probe succeeds before sending it any requests
    so that users won’t see errors from unready containers. This is critically important
    for zero-downtime upgrades (see [“Deployment Strategies”](ch13.html#deploymentstrategies)
    for more about these).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'A container that is not ready will still be shown as `Running`, but the `READY`
    column will show one or more unready containers in the Pod:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Readiness probes should only return HTTP `200 OK` status. Although Kubernetes
    itself considers both 2xx and 3xx status codes as *ready*, cloud load balancers
    may not. If you’re using an Ingress resource coupled with a cloud load balancer
    (see [“Ingress”](ch09.html#ingress)), and your readiness probe returns a 301 redirect,
    for example, the load balancer may flag all your Pods as unhealthy. Make sure
    your readiness probes only return a 200 status code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Startup Probes
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to liveness probes with `initialDelaySeconds`, Kubernetes offers
    another way to determine when an application has finished starting up. Some applications
    require a longer period of time to initialize, or maybe you would like to instrument
    a special endpoint in your application for checking startup status that is different
    from your other liveness and readiness checks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: When a `startupProbe` is configured, the `livenessProbe` will wait on it to
    succeed before beginning the liveness checks. If it never succeeds, Kubernetes
    will kill and restart the Pod.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax for a `startupProbe` is similar to liveness and readiness probes:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this example, notice how our `livenessProbe` will consider the Pod as unhealthy
    after two failures because of the `failureThreshold`, but we are giving the application
    more time to start with `failureThreshold: 10` in the `startupProbe`. This would
    hopefully prevent the situation where the Pod may not start quickly enough and
    the livenessProbe would otherwise give up and restart it before it has a chance
    to run.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: gRPC Probes
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although many applications and services communicate via HTTP, it’s increasingly
    popular to use the [Google Remote Procedure Call (gRPC)](https://grpc.io) protocol
    instead, especially for microservices. gRPC is an efficient, portable, binary
    network protocol developed by Google and hosted by the Cloud Native Computing
    Foundation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '`httpGet` probes will not work with gRPC servers, and although you could use
    a `tcpSocket` probe instead, that only tells you that you can make a connection
    to the socket, not that the server itself is working.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: gRPC has a standard health-checking protocol, which most gRPC services support,
    and to interrogate this health check with a Kubernetes liveness probe you can
    use the [`grpc-health-probe` tool](https://oreil.ly/sJp7V). If you add the tool
    to your container, you can check it using an `exec` probe.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: File-Based Readiness Probes
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alternatively, you could have the application create a file on the container’s
    filesystem called something like */tmp/healthy*, and use an `exec` readiness probe
    to check for the presence of that file.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of readiness probe can be useful because if you want to take the
    container temporarily out of service to debug a problem, you can attach to the
    container and delete the */tmp/healthy* file. The next readiness probe will fail,
    and Kubernetes will remove the container from any matching Services. (A better
    way to do this, though, is to adjust the container’s labels so that it no longer
    matches the service: see [“Service Resources”](ch04.html#services).)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: You can now inspect and troubleshoot the container at your leisure. Once you’re
    done, you can either terminate the container and deploy a fixed version, or put
    the probe file back in place so that the container will start receiving traffic
    again.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use readiness probes and liveness probes to let Kubernetes know when your application
    is ready to handle requests, or when it has a problem and needs to be restarted.
    It is also important to think about how your application functions in the context
    of the broader ecosystem and what should happen when it fails. You can end up
    in a cascading failure scenario if your probes are interconnected and share dependencies.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: minReadySeconds
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, a container or Pod is considered ready the moment its readiness
    probe succeeds. In some cases, you may want to run the container for a short while
    to make sure it is stable. During a deployment, Kubernetes waits until each new
    Pod is ready before starting the next (see [“Rolling Updates”](ch13.html#rollingupdate)).
    If a faulty container crashes straightaway, this will halt the rollout, but if
    it takes a few seconds to crash, all its replicas might be rolled out before you
    discover the problem.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, you can set the `minReadySeconds` field on the container. A container
    or Pod will not be considered ready until its readiness probe has been up for
    `minReadySeconds` (default 0).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Pod Disruption Budgets
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes Kubernetes needs to stop your Pods even though they’re alive and ready
    (a process called *eviction*). Perhaps the node they’re running on is being drained
    prior to an upgrade, for example, and the Pods need to be moved to another node.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: However, this needn’t result in downtime for your application, provided enough
    replicas can be kept running. You can use the `PodDisruptionBudget` resource to
    specify, for a given application, how many Pods you can afford to lose at any
    given time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might specify that no more than 10% of your application’s Pods
    can be disrupted at once. Or perhaps you want to specify that Kubernetes can evict
    any number of Pods, provided that at least three replicas are always running.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: minAvailable
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s an example of a PodDisruptionBudget that specifies a minimum number
    of Pods to be kept running, using the `minAvailable` field:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this example, `minAvailable: 3` specifies that at least three Pods matching
    the label `app: demo` should always be running. Kubernetes can evict as many `demo`
    Pods as it wants, so long as there are always at least three left.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: maxUnavailable
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Conversely, you can use `maxUnavailable` to limit the total number or percentage
    of Pods that Kubernetes is allowed to evict:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, no more than 10% of `demo` Pods are allowed to be evicted at any one time.
    This only applies to so-called *voluntary evictions*, though; that is to say,
    evictions initiated by Kubernetes. If a node suffers a hardware failure or gets
    deleted, for example, the Pods on it will be involuntarily evicted, even if that
    would violate the disruption budget.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Since Kubernetes will tend to spread Pods evenly across nodes, all other things
    being equal, this is worth bearing in mind when considering how many nodes your
    cluster needs. If you have three nodes, the failure of one could result in the
    loss of a third of all your Pods, and that may not leave enough to maintain an
    acceptable level of service (see [“High Availability”](ch03.html#highavailability)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set PodDisruptionBudgets for your business-critical applications to make sure
    there are always enough replicas to maintain the service, even when Pods are evicted.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Using Namespaces
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another very useful way of managing resource usage across your cluster is to
    use *namespaces*. A Kubernetes namespace is a way of partitioning your cluster
    into separate subdivisions, for whatever purpose you like.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might have different namespaces for testing out different versions
    of an application, or a separate namespace per team. As the term *namespace* suggests,
    names in one namespace are not visible from a different namespace.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: This means that you could have a service called `demo` in the `prod` namespace,
    and a different service called `demo` in the `test` namespace, and there won’t
    be any conflict.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the namespaces that exist on your cluster, run the following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can think of namespaces as being a bit like folders on your computer’s hard
    disk. While you *could* keep all your files in the same folder, it would be inconvenient.
    Looking for a particular file would be time-consuming, and it wouldn’t be easy
    to see which files belong with which others. A namespace groups related resources
    together, and makes it easier to work with them. Unlike folders, however, namespaces
    can’t be nested.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Working with Namespaces
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, when working with Kubernetes we’ve always used the *default namespace*.
    If you don’t specify a namespace when running a `kubectl` command, such as `kubectl
    run`, your command will operate on the default namespace. If you’re wondering
    what the `kube-system` namespace is, that’s where the Kubernetes internal system
    components run so that they’re segregated from your own applications.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'If, instead, you specify a namespace with the `--namespace` flag (or `-n` for
    short), your command will use that namespace. For example, to get a list of Pods
    in the `prod` namespace, run:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: What Namespaces Should I Use?
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s entirely up to you how to divide your cluster into namespaces. One idea
    that makes intuitive sense is to have one namespace per application, or per team.
    For example, you might create a `demo` namespace to run the demo application in.
    You can create a namespace using a Kubernetes namespace resource like the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To apply this resource manifest, use the `kubectl apply -f` command (see [“Resource
    Manifests in YAML Format”](ch04.html#applying) for more about this.) You’ll find
    the YAML manifests for all the examples in this section in the demo application
    repo, in the *hello-namespace* directory:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You could go further and create namespaces for each environment your app runs
    in, such as `demo-prod`, `demo-staging`, `demo-test`, and so on. You could use
    a namespace as a kind of temporary *virtual cluster*, and delete the namespace
    when you’re finished with it. But be careful! Deleting a namespace deletes all
    the resources within it. You really don’t want to run that command against the
    wrong namespace. (See [“Introducing Role-Based Access Control (RBAC)”](ch11.html#rbac)
    for how to grant or deny user permissions on individual namespaces.)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In the current version of Kubernetes, there is no way to *protect* a resource
    such as a namespace from being deleted (though a [proposal](https://oreil.ly/vk69W)
    for such a feature is under discussion). So don’t delete namespaces unless they
    really are temporary and you’re sure they don’t contain any production resources.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create separate namespaces for each of your applications or each logical component
    of your infrastructure. Don’t use the default namespace: it’s too easy to make
    mistakes.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: If you need to block all network traffic in or out of a particular namespace,
    you can use [Kubernetes Network Policies](https://oreil.ly/WOiKZ) to enforce this.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Service Addresses
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although namespaces are logically isolated from one another, they can still
    communicate with Services in other namespaces. You may recall from [“Service Resources”](ch04.html#services)
    that every Kubernetes Service has an associated DNS name that you can use to talk
    to it. Connecting to the hostname `demo` will connect you to the Service whose
    name is `demo`. How does that work across different namespaces?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Service DNS names always follow this pattern:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `.svc.cluster.local` part is optional, and so is the namespace. But if
    you want to talk to the `demo` Service in the `prod` namespace, for example, you
    can use:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Even if you have a dozen different Services called `demo`, each in its own namespace,
    you can add the namespace to the DNS name for the Service to specify exactly which
    one you mean.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Resource Quotas
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As well as restricting the CPU and memory usage of individual containers, which
    you learned about in [“Resource Requests”](#resourcerequests), you can (and should)
    restrict the resource usage of a given namespace. The way to do this is to create
    a ResourceQuota in the namespace.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example ResourceQuota:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Applying this manifest to a particular namespace (for example, `demo`) sets
    a hard limit of one hundred Pods running at once in that namespace. (Note that
    the `metadata.name` of the ResourceQuota can be anything you like. The namespaces
    it affects depends on which namespaces you apply the manifest to.)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now Kubernetes will block any API operations in the `demo` namespace that would
    exceed the quota. The example ResourceQuota limits the namespace to 100 Pods,
    so if there are 100 Pods already running and you try to start a new one, you will
    see an error message like this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using ResourceQuotas is a good way to stop applications in one namespace from
    grabbing too many resources and starving those in other parts of the cluster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: You can also limit the total CPU and memory usage of Pods in a namespace. This
    can be useful for budgeting in large organizations where many different teams
    are sharing a Kubernetes cluster. Teams could be required to set the number of
    CPUs they will use for their namespace, and if they exceed that quota, they will
    not be able to use more cluster resources until the ResourceQuota is increased.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: A Pod limit can be useful to prevent a misconfiguration or typing error from
    generating a potentially unlimited number of Pods. It’s easy to forget to clean
    up some object from a regular task, and find one day that you’ve got thousands
    of them clogging up your cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use ResourceQuotas in each namespace to enforce a limit on the number of Pods
    that can run in the namespace.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'To check if a ResourceQuota is active in a particular namespace, use the `kubectl
    describe resourcequotas` command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Default Resource Requests and Limits
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s not always easy to know what your container’s resource requirements are
    going to be in advance. You can set default requests and limits for all containers
    in a namespace using a LimitRange resource:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tip
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with ResourceQuotas, the `metadata.name` of the LimitRange can be whatever
    you want. It doesn’t correspond to a Kubernetes namespace, for example. A LimitRange
    or ResourceQuota takes effect in a particular namespace only when you apply the
    manifest to that namespace.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Any container in the namespace that doesn’t specify a resource limit or request
    will inherit the default value from the `LimitRange`. For example, a container
    with no `cpu` request specified will inherit the value of `200m` from the `LimitRange`.
    Similarly, a container with no `memory` limit specified will inherit the value
    of `256Mi` from the LimitRange.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, then, you could set the defaults in a LimitRange and not bother
    to specify requests or limits for individual containers. However, this isn’t good
    practice: it should be possible to look at a container spec and see what its requests
    and limits are, without having to know whether or not a LimitRange is in effect.
    Use the LimitRange only as a backstop to prevent problems with containers whose
    owners forgot to specify requests and limits.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use LimitRanges in each namespace to set default resource requests and limits
    for containers, but don’t rely on them; treat them as a backstop. Always specify
    explicit requests and limits in the container spec itself.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Cluster Costs
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Cluster Sizing and Scaling”](ch06.html#sizing), we outlined some considerations
    for choosing the initial size of your cluster, and scaling it over time as your
    workloads evolve. But, assuming that your cluster is correctly sized and has sufficient
    capacity, how should you run it in the most cost-effective way?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Kubecost
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It can often be difficult to get an overall sense of the cost involved with
    running the Kubernetes infrastructure when there are several applications and
    teams all sharing the same clusters.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately there is a tool called [Kubecost](https://oreil.ly/j4ppy) for tracking
    costs per namespace, label, or even down to the container level. [Kubecost](https://www.kubecost.com)
    is currently free for a single cluster, and there are paid versions with support
    for larger environments.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Deployments
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Do you really need quite so many replicas? It may seem an obvious point, but
    every Pod in your cluster uses up some resources that are thus unavailable to
    some other Pod.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: It can be tempting to run a large number of replicas for everything so that
    quality of service will never be reduced if individual Pods fail, or during rolling
    upgrades. Also, the more replicas, the more traffic your apps can handle.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: But you should use replicas wisely. Your cluster can only run a finite number
    of Pods. Give them to applications that really need maximum availability and performance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: If it really doesn’t matter that a given Deployment is down for a few seconds
    during an upgrade, it doesn’t need a lot of replicas. A surprisingly large number
    of applications and services can get by perfectly well with one or two replicas.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Review the number of replicas configured for each Deployment, and ask:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: What are the business requirements for performance and availability for this
    service?
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we meet those requirements with fewer replicas?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an app is struggling to handle demand, or users get too many errors when
    you upgrade the Deployment, it needs more replicas. But in many cases you can
    reduce the size of a Deployment considerably before you get to the point where
    the degradation starts to be noticeable.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Later on in [“Autoscaling”](ch06.html#autoscaling), we will cover ways you can
    leverage autoscaling to save costs at times when you know your usage is low.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use the minimum number of Pods for a given Deployment that will satisfy your
    performance and availability requirements. Gradually reduce the number of replicas
    to just above the point where your service level objectives are met.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Pods
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, in [“Resource Requests”](#resourcerequests), we emphasized
    the importance of setting the correct resource requests and limits for your containers.
    If the resource requests are too small, you’ll soon know about it: Pods will start
    failing. If they are too large, however, the first time you find out about it
    may be when you get your monthly cloud bill.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: You should regularly review the resource requests and limits for your various
    workloads, and compare them against what was actually used.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Most managed Kubernetes services offer some kind of dashboard showing the CPU
    and memory usage of your containers over time—we’ll see more about this in [“Monitoring
    Cluster Status”](ch11.html#clustermonitoring).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: You can also build your own dashboards and statistics using Prometheus and Grafana,
    and we’ll cover this in detail in [Chapter 15](ch15.html#observability).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Setting the optimal resource requests and limits is something of an art, and
    the answer will be different for every kind of workload. Some containers may be
    idle most of the time, occasionally spiking their resource usage to handle a request;
    others may be constantly busy, and gradually use more and more memory until they
    hit their limits.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In general, you should set the resource limits for a container to a little above
    the maximum it uses in normal operation. For example, if a given container’s memory
    usage over a few days never exceeds 500 MiB of memory, you might set its memory
    limit to 600 MiB.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Should containers have limits at all? One school of thought says that containers
    should have *no* limits in production, or that the limits should be set so high
    that the containers will never exceed them. With very large and resource-hungry
    containers that are expensive to restart, this may make some sense, but we think
    it’s better to set limits anyway. Without them, a container that has a memory
    leak, or that uses too much CPU, can gobble up all the resources available on
    a node, starving other containers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this *resource Pac-Man* scenario, set a container’s limits to a little
    more than 100% of normal usage. This will ensure it’s not killed as long as it’s
    working properly, but still minimize the blast radius if something goes wrong.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Request settings are less critical than limits, but they still should not be
    set too high (as the Pod will never be scheduled), or too low (as Pods that exceed
    their requests are first in line for eviction).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Vertical Pod Autoscaler
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a Kubernetes add-on called the [Vertical Pod Autoscaler](https://oreil.ly/lQtDN),
    which can help you work out the ideal values for resource requests. It will watch
    a specified Deployment and automatically adjust the resource requests for its
    Pods based on what they actually use. It has a dry-run mode that will just make
    suggestions, without actually modifying the running Pods, and this can be helpful.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Nodes
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes can work with a wide range of node sizes, but some will perform better
    than others. To get the best cluster capacity for your money, you need to observe
    how your nodes perform in practice, under real-demand conditions, with your specific
    workloads. This will help you determine the most cost-effective instance types.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth remembering that every node has to have an operating system on it,
    which consumes disk, memory, and CPU resources. So do the Kubernetes system components
    and the container runtime. The smaller the node, the bigger a proportion of its
    total resources this overhead represents.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Larger nodes, therefore, can be more cost-effective, because a greater proportion
    of their resources are available for your workloads. The trade-off is that losing
    an individual node has a bigger effect on your cluster’s available capacity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Small nodes also have a higher percentage of *stranded resources*: chunks of
    memory space and CPU time that are unused, but too small for any existing Pod
    to claim them.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: A good [rule of thumb](https://oreil.ly/tTwpL) is that nodes should be big enough
    to run at least five of your typical Pods, keeping the proportion of stranded
    resources to around 10% or less. If the node can run 10 or more Pods, stranded
    resources will be below 5%.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The default limit in Kubernetes is 110 Pods per node. Although you can increase
    this limit by adjusting the `--max-pods` setting of the `kubelet`, this may not
    be possible with some managed services, and it’s a good idea to stick to the Kubernetes
    defaults unless there is a strong reason to change them.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The Pods-per-node limit means that you may not be able to take advantage of
    your cloud provider’s largest instance sizes. Instead, consider running a [larger
    number of smaller nodes](https://oreil.ly/e3Sim) to get better utilization. For
    example, instead of 6 nodes with 8 vCPUs, run 12 nodes with 4 vCPUs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Look at the percentage resource utilization of each node, using your cloud provider’s
    dashboard or `kubectl top nodes`. The bigger the percentage of CPU in use, the
    better the utilization. If the larger nodes in your cluster have better utilization,
    you may be well advised to remove some of the smaller nodes and replace them with
    larger ones.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if larger nodes have low utilization, your cluster may be
    over capacity and you can therefore either remove some nodes or make them smaller,
    reducing the total bill.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Larger nodes tend to be more cost-effective, because less of their resources
    are consumed by system overhead. Size your nodes by looking at real-world utilization
    figures for your cluster, aiming for between 10 and 100 Pods per node.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Storage
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One cloud cost that is often overlooked is that of disk storage. Cloud providers
    offer varying amounts of disk space with each of their instance sizes, and the
    price of large-scale storage varies too.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: While it’s possible to achieve quite high CPU and memory utilization using Kubernetes
    resource requests and limits, the same is not true of storage, and many cluster
    nodes are significantly over-provisioned with disk space.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Not only do many nodes have more storage space than they need, but the class
    of storage can also be a factor. Most cloud providers offer different classes
    of storage depending on the number of I/O operations per second (IOPS), or bandwidth,
    allocated.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: For example, databases that use persistent disk volumes often need a very high
    IOPS rating, for fast, high-throughput storage access. This is expensive. You
    can save on cloud costs by provisioning low-IOPS storage for workloads that don’t
    need so much bandwidth. On the other hand, if your application is performing poorly
    because it’s spending a lot of time waiting for storage I/O, you may want to provision
    more IOPS to handle this.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Your cloud or Kubernetes provider console can usually show you how many IOPS
    are actually being used on your nodes, and you can use these figures to help you
    decide where to cut costs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, you would be able to set resource requests for containers that need
    high bandwidth or large amounts of storage. However, Kubernetes does not currently
    support this, though support for IOPS requests may be added in the future.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t use instance types with more storage than you need. Provision the smallest,
    lowest-IOPS disk volumes you can, based on the throughput and space that you actually
    use.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning Up Unused Resources
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As your Kubernetes clusters grow, you will find many unused, or *lost* resources
    hanging around in dark corners. Over time, if these lost resources are not cleaned
    up, they will start to represent a significant fraction of your overall costs.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: At the highest level, you may find cloud instances that are not part of any
    cluster; it’s easy to forget to terminate a machine when it’s no longer in use.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Other types of cloud resources, such as load balancers, public IPs, and disk
    volumes, also cost you money even though they’re not in use. You should regularly
    review your usage of each type of resource to find and remove unused instances.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there may be Deployments and Pods in your Kubernetes cluster that
    are not actually referenced by any Service, and so cannot receive traffic.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Even container images that are not running take up disk space on your nodes.
    Fortunately, Kubernetes will automatically clean up unused images when the node
    starts running short of disk space.^([1](ch05.html#idm45979387576368))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Using owner metadata
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One helpful way to minimize unused resources is to have an organization-wide
    policy that each resource must be tagged with information about its owner. You
    can use Kubernetes annotations to do this (see [“Labels and Annotations”](ch09.html#labelsandannotations)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you could annotate each Deployment like this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The owner metadata should specify the person or team to be contacted about this
    resource. This is useful anyway, but it’s especially handy for identifying abandoned
    or unused resources. (Note that it’s a good idea to prefix custom annotations
    with the domain name of your company, such as `example.com`, to prevent collisions
    with other annotations that might have the same name.)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'You can regularly query the cluster for all resources that do not have an owner
    annotation and make a list of them for potential termination. An especially strict
    policy might terminate all unowned resources immediately. Don’t be too strict,
    though, especially at first: developer goodwill is as important a resource as
    cluster capacity, if not more so.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set owner annotations on all your resources, giving information about who to
    contact if there’s a problem with this resource, or if it seems abandoned and
    liable for termination.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Finding underutilized resources
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some resources may be receiving very low levels of traffic, or none at all.
    Perhaps they became disconnected from a Service frontend due to a change in labels,
    or maybe they were temporary or experimental.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Every Pod should expose the number of requests it receives as a metric (see
    [Chapter 16](ch16.html#metrics) for more about this). Use these metrics to find
    Pods that are getting low or zero traffic, and make a list of resources that can
    potentially be terminated.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: You can also check the CPU and memory utilization figures for each Pod in your
    web console and find the least-utilized Pods in your cluster. Pods that don’t
    do anything probably aren’t a good use of resources.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: If the Pods have owner metadata, contact their owners to find out whether these
    Pods are actually needed (for example, they might be for an application still
    in development).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: You could use another custom Kubernetes annotation (`example.com/lowtraffic`
    perhaps) to identify Pods that receive no requests but are still needed for one
    reason or another.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularly review your cluster to find underutilized or abandoned resources and
    eliminate them. Owner annotations can help.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up completed Jobs
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes Jobs (see [“Jobs”](ch09.html#jobs)) are Pods that run once to completion
    and are not restarted. However, the Job objects still exist in the Kubernetes
    database, and once there are a significant number of completed Jobs, this can
    affect API performance. You can tell Kubernetes to automatically remove Jobs after
    they have finished with the `ttlSecondsAfterFinished` setting:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this example, when your Job finishes, it will automatically be deleted after
    60 seconds.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Checking Spare Capacity
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There should always be enough spare capacity in the cluster to handle the failure
    of a single worker node. To check this, try draining your biggest node (see [“Scaling
    down”](ch06.html#drain)). Once all Pods have been evicted from the node, check
    that all your applications are still in a working state with the configured number
    of replicas. If this is not the case, you need to add more capacity to the cluster.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: If there isn’t room to reschedule its workloads when a node fails, your services
    could be degraded at best, and unavailable at worst.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Using Reserved Instances
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some cloud providers offer different instance classes depending on the machine’s
    life cycle. *Reserved* instances offer a trade-off between price and flexibility.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, AWS reserved instances are about half the price of *on-demand*
    instances (the default type). You can reserve instances for various periods: a
    year, three years, and so on. AWS reserved instances have a fixed size, so if
    it turns out in three months’ time that you need a larger instance, your reservation
    will be mostly wasted.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The Google Cloud equivalent of reserved instances is *Committed Use Discounts*,
    which allow you to prepay for a certain number of vCPUs and an amount of memory.
    This is more flexible than AWS reservations, as you can use more resources than
    you have reserved; you just pay the normal on-demand price for anything not covered
    by your reservation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Reserved instances and Committed Use Discounts can be a good choice when you
    know your requirements for the foreseeable future. However, there’s no refund
    for reservations that you don’t end up using, and you have to pay up front for
    the whole reservation period. So you should only choose to reserve instances for
    a period over which your requirements aren’t likely to change significantly.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: If you can plan a year or two ahead, however, using reserved instances could
    deliver a considerable saving.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use reserved instances when your needs aren’t likely to change for a year or
    two—but choose your reservations wisely, because they can’t be altered or refunded
    once they’re made.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Using Preemptible (Spot) Instances
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Spot* instances, as AWS calls them, or *preemptible VMs* in Google’s terminology,
    provide no availability guarantees, and are often limited in life span. Thus,
    they represent a trade-off between price and availability.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: A spot instance is cheap, but may be paused or resumed at any time, and may
    be terminated altogether. Fortunately, Kubernetes is designed to provide high-availability
    services despite the loss of individual cluster nodes.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Variable price or variable preemption
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spot instances can therefore be a cost-effective choice for your cluster. With
    AWS spot instances, the per-hour pricing varies according to demand. When demand
    is high for a given instance type in a particular region and availability zone,
    the price will rise.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud’s preemptible VMs, on the other hand, are billed at a fixed rate,
    but the rate of preemption varies. Google says that on average, [about 5–15% of
    your nodes will be preempted in a given week](https://oreil.ly/egdN8). However,
    preemptible VMs can be up to 80% cheaper than on-demand, depending on the instance
    type.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Preemptible nodes can halve your costs
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using preemptible nodes for your Kubernetes cluster, then, can be a very effective
    way to reduce costs. While you may need to run a few more nodes to make sure that
    your workloads can survive preemption, anecdotal evidence suggests that an overall
    50% reduction in the cost per node is achievable.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: You may also find that using preemptible nodes is a good way to build a little
    chaos engineering into your cluster (see [“Chaos Testing”](ch06.html#chaos))—provided
    that your application is ready for chaos testing in the first place.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind, though, that you should always have enough nonpreemptible nodes
    to handle your cluster’s minimum workload. Never bet more than you can afford
    to lose. If you have a lot of preemptible nodes, it might be a good idea to use
    cluster autoscaling to make sure any preempted nodes are replaced as soon as possible
    (see [“Autoscaling”](ch06.html#autoscaling)).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: In theory, *all* your preemptible nodes could disappear at the same time. Despite
    the cost savings, therefore, it’s a good idea to limit your preemptible nodes
    to no more than, say, two-thirds of your cluster.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keep costs down by using preemptible or spot instances for some of your nodes,
    but no more than you can afford to lose. Always keep some nonpreemptible nodes
    in the mix, too.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Using node affinities to control scheduling
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use Kubernetes *node affinities* to make sure Pods that can’t tolerate
    failure are [not scheduled on preemptible nodes](https://oreil.ly/i52Wu) (see
    [“Node Affinities”](ch09.html#nodeaffinities)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, Google Kubernetes Engine (GKE) preemptible nodes carry the label
    `cloud.google.com/gke-preemptible`. To tell Kubernetes to never schedule a Pod
    on one of these nodes, add the following to the Pod or Deployment spec:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `requiredDuringScheduling...` affinity is mandatory: a Pod with this affinity
    will *never* be scheduled on a node that does not match the selector expression
    (known as a *hard affinity*).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you might want to tell Kubernetes that some of your less critical
    Pods, which can tolerate occasional failures, should preferentially be scheduled
    on preemptible nodes. In this case, you can use a *soft affinity* with the opposite
    sense:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This effectively means “Please schedule this Pod on a preemptible node if you
    can; if not, it doesn’t matter.”
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re running preemptible nodes, use Kubernetes node affinities to make
    sure critical workloads are not preempted.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Keeping Your Workloads Balanced
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve talked about the work that the Kubernetes scheduler does ensuring that
    workloads are distributed fairly across as many nodes as possible, and trying
    to place replica Pods on different nodes for high availability.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: In general, the scheduler does a great job, but there are some edge cases you
    need to watch out for.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you have two nodes, and two services, A and B, each with
    two replicas. In a balanced cluster, there will be one replica of service A on
    each node, and one of service B on each node ([Figure 5-1](#img-unbalance1)).
    If one node should fail, both A and B will still be available.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing two nodes, each with replicas of Service A and B](assets/cndk_0501.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Services A and B are balanced across the available nodes
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So far, so good. But suppose Node 2 does fail. The scheduler will notice that
    both A and B need an extra replica, and there’s only one node for it to create
    them on, so it does. Now Node 1 is running two replicas of service A, and two
    of service B.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose we spin up a new node to replace the failed Node 2\. Even once it’s
    available, there will be no Pods on it. The scheduler never moves running Pods
    from one node to another.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: We now have an [unbalanced cluster](https://oreil.ly/JWbpO), where all the Pods
    are on Node 1, and none are on Node 2 ([Figure 5-2](#img-unbalance2)).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing two nodes, one with replicas of Service A and B, and the
    other with no services](assets/cndk_0502.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. After the failure of Node 2, all replicas have moved to Node 1
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But it gets worse. Suppose you deploy a rolling update to service A (let’s call
    the new version service A*). The scheduler needs to start two new replicas for
    service A*, wait for them to come up, and then terminate the old ones. Where will
    it start the new replicas? On the new Node 2, because it’s idle, while Node 1
    is already running four Pods. So two new service A* replicas are started on Node
    2, and the old ones removed from Node 1 ([Figure 5-3](#img-unbalance3)).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing two nodes, one with two replicas of service B, and the other
    with two replicas of service A*](assets/cndk_0503.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. After the rollout of service A*, the cluster is still unbalanced
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now you’re in a bad situation, because both replicas of service B are on the
    same node (Node 1), while both replicas of service A* are also on the same node
    (Node 2). Although you have two nodes, you have no high availability. The failure
    of either Node 1 or Node 2 will result in a service outage.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The key to this problem is that the scheduler never moves Pods from one node
    to another unless they are restarted for some reason. Also, the scheduler’s goal
    of placing workloads evenly across nodes is sometimes in conflict with maintaining
    high availability for individual services.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: One way around this is to use a tool called [Descheduler](https://oreil.ly/hCSK9).
    You can run this tool every so often, as a Kubernetes Job, and it will do its
    best to rebalance the cluster by finding Pods that need to be moved, and killing
    them.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Descheduler has various strategies and policies that you can configure. For
    example, one policy looks for underutilized nodes, and kills Pods on other nodes
    to force them to be rescheduled on the idle nodes.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Another policy looks for duplicate Pods, where two or more replicas of the same
    Pod are running on the same node, and evicts them. This fixes the problem that
    arose in our example, where workloads were nominally balanced, but in fact neither
    service was highly available.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is pretty good at running workloads for you in a reliable, efficient
    way with no real need for manual intervention. Providing you give the scheduler
    accurate estimates of your containers’ resource needs, you can largely leave Kubernetes
    to get on with it.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: The time you would have spent fixing operations issues can thus be put to better
    use, like developing applications. Thanks, Kubernetes!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding how Kubernetes manages resources is key to building and running
    your cluster correctly. The most important points to take away:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes allocates CPU and memory resources to containers on the basis of
    *requests* and *limits*.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A container’s requests are the minimum amounts of resources it needs to run.
    Its limits specify the maximum amount it’s allowed to use.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimal container images are faster to build, push, deploy, and start. The smaller
    the container, the fewer the potential security vulnerabilities.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liveness probes tell Kubernetes whether the container is working properly. If
    a container’s liveness probe fails, it will be killed and restarted.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Readiness probes tell Kubernetes that the container is ready and able to serve
    requests. If the readiness probe fails, the container will be removed from any
    Services that reference it, disconnecting it from user traffic.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Startup probes are like liveness probes, but are only used for determining if
    an application has finished starting and is ready for the liveness probe to take
    over for checking the status.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PodDisruptionBudgets let you limit the number of Pods that can be stopped at
    once during *evictions*, preserving high availability for your application.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespaces are a way of logically partitioning your cluster. You might create
    a namespace for each application, or group of related applications.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To refer to a Service in another namespace, you can use a DNS address like
    this: *`SERVICE.NAMESPACE`*.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResourceQuotas let you set overall resource limits for a given namespace.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LimitRanges specify default resource requests and limits for containers in a
    namespace.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set resource limits so that your applications almost, but don’t quite, exceed
    them in normal usage.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t allocate more cloud storage than you need, and don’t provision high-bandwidth
    storage unless it’s critical for your application’s performance.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要分配比您所需更多的云存储，并且除非对您的应用程序性能至关重要，否则不要配置高带宽存储。
- en: Set owner annotations on all your resources, and scan the cluster regularly
    for unowned resources.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有资源上设置所有者注释，并定期扫描集群以查找无主资源。
- en: Find and clean up resources that aren’t being used (but check with their owners).
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找并清理不再使用的资源（但请与它们的所有者确认）。
- en: Reserved instances can save you money if you can plan your usage long term.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果能够长期规划您的使用情况，预留实例可以为您节省费用。
- en: Preemptible instances can save you money right now, but be ready for them to
    vanish on short notice. Use node affinities to keep failure-sensitive Pods away
    from preemptible nodes.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预留实例可以立即为您节省费用，但要做好它们突然消失的准备。使用节点亲和性，将对故障敏感的 Pod 保持远离预留节点。
- en: ^([1](ch05.html#idm45979387576368-marker)) You can customize this behavior by
    adjusting the [`kubelet` garbage collection](https://oreil.ly/Pjfrj) settings.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#idm45979387576368-marker)) 您可以通过调整[`kubelet`垃圾回收](https://oreil.ly/Pjfrj)设置来自定义此行为。
