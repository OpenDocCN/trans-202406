<html><head></head><body><section data-pdf-bookmark="Chapter 12. Stateful Service" data-type="chapter" epub:type="chapter"><div class="chapter" id="StatefulService">&#13;
<h1><span class="label">Chapter 12. </span>Stateful Service</h1>&#13;
&#13;
&#13;
<p>Distributed<a data-primary="Stateful Service" data-type="indexterm" id="staser12"/> stateful applications require features such as persistent identity, networking, storage, and ordinality. The <em>Stateful Service</em> pattern describes the StatefulSet primitive that provides these building blocks with strong guarantees ideal for the management of stateful applications.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Problem" data-type="sect1"><div class="sect1" id="idm45902101939472">&#13;
<h1>Problem</h1>&#13;
&#13;
<p>We<a data-primary="problems" data-secondary="applications, influencing stateful" data-type="indexterm" id="idm45902101937472"/> have seen many Kubernetes primitives for creating distributed applications: containers with health checks and resource limits, Pods with multiple containers, dynamic cluster-wide placements, batch jobs, scheduled jobs, singletons, and more. The common characteristic of these primitives is that they treat the managed application as a stateless application composed of identical, swappable, and replaceable containers and comply with the<a data-primary="twelve-factor app" data-type="indexterm" id="idm45902101936320"/> <a href="https://12factor.net">twelve-factor app principles</a>.</p>&#13;
&#13;
<p>It is a significant boost to have a platform taking care of the placement, resiliency, and scaling of stateless applications, but there is still a large part of the workload to consider: stateful applications in which every instance is unique and has long-lived characteristics.</p>&#13;
&#13;
<p>In the real world, behind every highly scalable stateless service is a stateful service, typically in the shape of a data store. In the early days of Kubernetes, when it lacked support for stateful workloads, the solution was placing stateless applications on Kubernetes to get the benefits of the cloud native model and keeping stateful components outside the cluster, either on a public cloud or on-premises hardware, managed with the traditional noncloud native mechanisms. Considering that every enterprise has a multitude of stateful workloads (legacy and modern), the lack of support for stateful workloads was a significant limitation in Kubernetes, which was known as a universal cloud native platform.</p>&#13;
&#13;
<p>But what are the typical requirements of a stateful application? We could deploy a stateful application such as<a data-primary="Zookeeper" data-type="indexterm" id="idm45902101933456"/> Apache ZooKeeper, MongoDB, Redis, or MySQL by using a Deployment, which could create a ReplicaSet with <code>replicas=1</code> to make it reliable, use a Service to discover its endpoint, and use PersistentVolumeClaim (PVC) and PersistentVolume (PV) as permanent storage for its state.</p>&#13;
&#13;
<p>While that is mostly true for a single-instance stateful application, it is not entirely true, as a ReplicaSet does not guarantee At-Most-One semantics, and the number of replicas can vary temporarily. Such a situation can be disastrous and lead to data loss for distributed stateful applications. Also, the main challenges arise when it is a distributed stateful service that is composed of multiple instances. A stateful application composed of multiple clustered services requires multifaceted guarantees from the underlying infrastructure. Let’s see some of the most common long-lived persistent prerequisites for distributed stateful applications.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Storage" data-type="sect2"><div class="sect2" id="idm45902101931568">&#13;
<h2>Storage</h2>&#13;
&#13;
<p>We could easily increase the number of <code>replicas</code> in a ReplicaSet and end up with a distributed stateful application. However, how do we define the storage requirements in such a case? Typically, a distributed stateful application such as those mentioned previously would require dedicated, persistent storage for every instance. A ReplicaSet with <code>replicas=3</code> and a PVC definition would result in all three Pods attached to the same PV. While the ReplicaSet and the PVC ensure the instances are up and the storage is attached to whichever node the instances are scheduled on, the storage is not dedicated but shared among all Pod instances.</p>&#13;
&#13;
<p>A workaround is for the application instances to share storage and have an in-app mechanism to split the storage into subfolders and use it without conflicts. While doable, this approach creates a single point of failure with the single storage. Also, it is error-prone as the number of Pods changes during scaling, and it may cause severe challenges around preventing data corruption or loss during scaling.</p>&#13;
&#13;
<p>Another workaround is to have a separate ReplicaSet (with <code>replicas=1</code>) for every instance of the distributed stateful application. In this scenario, every ReplicaSet would get its PVC and dedicated storage. The downside of this approach is that it is intensive in manual labor: scaling up requires creating a new set of ReplicaSet, PVC, or Service definitions. This approach lacks a single abstraction for managing all instances of the stateful application as one.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Networking" data-type="sect2"><div class="sect2" id="idm45902101927552">&#13;
<h2>Networking</h2>&#13;
&#13;
<p>Similar to the storage requirements, a distributed stateful application requires a stable network identity. In addition to storing application-specific data into the storage space, stateful applications also store configuration details such as hostname and connection details of their peers. That means every instance should be reachable in a predictable address that should not change dynamically, as is the case with Pod IP addresses in a ReplicaSet. Here we could address this requirement again through a workaround: create a Service per ReplicaSet and have <code>replicas=1</code>. However, managing such a setup is manual work, and the application itself cannot rely on a stable hostname because it changes after every restart and is also not aware of the Service name it is accessed from.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Identity" data-type="sect2"><div class="sect2" id="idm45902101925552">&#13;
<h2>Identity</h2>&#13;
&#13;
<p>As you can see from the preceding requirements, clustered stateful applications depend heavily on every instance having a hold of its long-lived storage and network identity. That is because in a stateful application, every instance is unique and knows its own identity, and the main ingredients of that identity are the long-lived storage and the networking coordinates. To this list, we could also add the identity/name of the instance (some stateful applications require unique persistent names), which in Kubernetes would be the Pod name. A Pod created with ReplicaSet would have a random name and would not preserve that identity across a restart.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ordinality" data-type="sect2"><div class="sect2" id="idm45902101923968">&#13;
<h2>Ordinality</h2>&#13;
&#13;
<p>In addition to a unique and long-lived identity, the instances of clustered stateful applications have a fixed position in the collection of instances. This ordering typically impacts the sequence in which the instances are scaled up and down. However, it can also be used for data distribution or access and in-cluster behavior positioning such as locks, singletons, or leaders.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other Requirements" data-type="sect2"><div class="sect2" id="idm45902101921984">&#13;
<h2>Other Requirements</h2>&#13;
&#13;
<p>Stable and long-lived storage, networking, identity, and ordinality are among the collective needs of clustered stateful applications. Managing stateful applications also carries many other specific requirements that vary case by case. For example, some applications have the notion of a quorum and require a minimum number of instances to always be available; some are sensitive to ordinality, and some are fine with parallel Deployments; and some tolerate duplicate instances, and some don’t. Planning for all these one-off cases and providing generic mechanisms is an impossible task, and that’s why Kubernetes also allows you to create CustomResourceDefinitions (CRDs) and <em>Operators</em> for managing applications with bespoke requirements. The <em>Operator</em> pattern is explained in <a data-type="xref" href="ch28.html#Operator">Chapter 28</a>.</p>&#13;
&#13;
<p>We have seen some common challenges of managing distributed stateful applications and a few less-than-ideal workarounds. Next, let’s check out the Kubernetes native mechanism for addressing these requirements through the StatefulSet <span class="keep-togehter">primitive</span>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Solution" data-type="sect1"><div class="sect1" id="idm45902101917456">&#13;
<h1>Solution</h1>&#13;
&#13;
<p>To explain what StatefulSet provides for managing stateful applications, we occasionally compare its behavior to the already-familiar ReplicaSet primitive that Kubernetes uses for running stateless workloads. In many ways, StatefulSet is for managing pets, and ReplicaSet is for managing cattle. Pets versus cattle is a famous (but also a controversial) analogy in the DevOps world: identical and replaceable servers are referred to as cattle, and nonfungible unique servers that require individual care are referred to as pets. Similarly, StatefulSet (initially inspired by the analogy and named PetSet) is designed for managing nonfungible Pods, as opposed to ReplicaSet, which is for managing identical replaceable Pods.</p>&#13;
&#13;
<p>Let’s explore how StatefulSets work and how they address the needs of stateful applications. <a data-type="xref" href="#ex-statefulset-example-statefulset">Example 12-1</a> is our random-generator service as a StatefulSet.<sup><a data-type="noteref" href="ch12.html#idm45902101914272" id="idm45902101914272-marker">1</a></sup></p>&#13;
<div data-type="example" id="ex-statefulset-example-statefulset">&#13;
<h5><span class="label">Example 12-1. </span>StatefulSet definition for a stateful application</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">StatefulSet</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">rg</code><code class="w">                         </code><a class="co" href="#callout_stateful_service_CO1-1" id="co_stateful_service_CO1-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">    </code><a class="co" href="#callout_stateful_service_CO1-2" id="co_stateful_service_CO1-2"><img alt="2" src="assets/2.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2</code><code class="w">                      </code><a class="co" href="#callout_stateful_service_CO1-3" id="co_stateful_service_CO1-3"><img alt="3" src="assets/3.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">k8spatterns/random-generator:1.0</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">containerPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">http</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">volumeMounts</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">logs</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">mountPath</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/logs</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">volumeClaimTemplates</code><code class="p">:</code><code class="w">            </code><a class="co" href="#callout_stateful_service_CO1-4" id="co_stateful_service_CO1-4"><img alt="4" src="assets/4.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">logs</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">accessModes</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="w"> </code><code class="s">"</code><code class="s">ReadWriteOnce</code><code class="s">"</code><code class="w"> </code><code class="p-Indicator">]</code><code class="w">&#13;
</code><code class="w">      </code><code class="nt">resources</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">        </code><code class="nt">requests</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">          </code><code class="nt">storage</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10Mi</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_stateful_service_CO1-1" id="callout_stateful_service_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Name of the StatefulSet is used as prefix for the generated node names.</p></dd>&#13;
<dt><a class="co" href="#co_stateful_service_CO1-2" id="callout_stateful_service_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>References the mandatory Service defined in <a data-type="xref" href="#ex-statefulset-example-service">Example 12-2</a>.</p></dd>&#13;
<dt><a class="co" href="#co_stateful_service_CO1-3" id="callout_stateful_service_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Two Pod members in the StatefulSet named <em>rg-0</em> and <em>rg-1</em>.</p></dd>&#13;
<dt><a class="co" href="#co_stateful_service_CO1-4" id="callout_stateful_service_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Template for creating a PVC for each Pod (similar to the Pod’s template).</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Rather than going through the definition in <a data-type="xref" href="#ex-statefulset-example-statefulset">Example 12-1</a> line by line, we explore the overall behavior and the guarantees provided by this StatefulSet definition.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Storage" data-type="sect2"><div class="sect2" id="idm45902102978512">&#13;
<h2>Storage</h2>&#13;
&#13;
<p>While<a data-primary="ReplicaSet" data-secondary="Stateful Service" data-type="indexterm" id="RSstateful12"/> it is not always necessary, the majority of stateful applications store state and thus require per-instance-based dedicated persistent storage. The way to request and associate persistent storage with a Pod in Kubernetes is through PVs and PVCs. To create PVCs the same way it creates Pods, StatefulSet uses a <code>volumeClaimTemplates</code> element. This extra property is one of the main differences between a StatefulSet and a ReplicaSet, which has a <code>persistentVolumeClaim</code> element.</p>&#13;
&#13;
<p>Rather than referring to a predefined PVC, StatefulSets create PVCs by using <code>volumeClaimTemplates</code> on the fly during Pod creation. This mechanism allows every Pod to get its own dedicated PVC during initial creation as well as during scaling up by changing the <code>replicas</code> count of the StatefulSets.</p>&#13;
&#13;
<p>As you probably realize, we said PVCs are created and associated with the Pods, but we didn’t say anything about PVs. That is because StatefulSets do not manage PVs in any way. The storage for the Pods must be provisioned in advance by an admin or provisioned on demand by a PV provisioner based on the requested storage class and ready for consumption by the stateful Pods.</p>&#13;
&#13;
<p>Note the asymmetric behavior here: scaling up a StatefulSet (increasing the <code>replicas</code> count)  creates new Pods and associated PVCs. Scaling down deletes the Pods, but it does not delete any PVCs (or PVs), which means the PVs cannot be recycled or deleted, and Kubernetes cannot free the storage. This behavior is by design and driven by the presumption that the storage of stateful applications is critical and that an accidental scale-down should not cause data loss. If you are sure the stateful application has been scaled down on purpose and has replicated/drained the data to other instances, you can delete the PVC manually, which allows subsequent PV recycling.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Networking" data-type="sect2"><div class="sect2" id="idm45902102971456">&#13;
<h2>Networking</h2>&#13;
&#13;
<p>Each Pod created by a StatefulSet has a stable identity generated by the StatefulSet’s name and an ordinal index (starting from 0). Based on the preceding example, the two Pods are named <code>rg-0</code> and <code>rg-1</code>. The Pod names are generated in a predictable format that differs from the ReplicaSet’s Pod-name-generation mechanism, which contains a random suffix.</p>&#13;
&#13;
<p>Dedicated scalable persistent storage is an essential aspect of stateful applications and so is networking.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#ex-statefulset-example-service">Example 12-2</a>, we define a <em>headless</em> Service. In a headless Service, <code>clusterIP</code> is set to <code>None</code>, which means we don’t want a kube-proxy to handle the Service, and we don’t want a cluster IP allocation or load balancing. Then why do we need a Service?</p>&#13;
<div data-type="example" id="ex-statefulset-example-service">&#13;
<h5><span class="label">Example 12-2. </span>Service for accessing StatefulSet</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w">&#13;
</code><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w">&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="nt">spec</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">clusterIP</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">None</code><code class="w">         </code><a class="co" href="#callout_stateful_service_CO2-1" id="co_stateful_service_CO2-1"><img alt="1" src="assets/1.png"/></a><code class="w">&#13;
</code><code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">random-generator</code><code class="w">&#13;
</code><code class="w">  </code><code class="nt">ports</code><code class="p">:</code><code class="w">&#13;
</code><code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">http</code><code class="w">&#13;
</code><code class="w">    </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_stateful_service_CO2-1" id="callout_stateful_service_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Declares this Service as headless.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>Stateless Pods created through a ReplicaSet are assumed to be identical, and it doesn’t matter on which one a request lands (hence the load balancing with a regular Service). But stateful Pods differ from one another, and we may need to reach a specific Pod by its coordinates.</p>&#13;
&#13;
<p>A headless Service with selectors (notice <code>.selector.app == random-generator</code>) enables exactly this. Such a Service creates endpoint records in the API Server and creates DNS entries to return A records (addresses) that point directly to the Pods backing the Service. Long story short, each Pod gets a DNS entry where clients can directly reach out to it in a predictable way. For example, if our <code>random-generator</code> Service belongs to the <code>default</code> namespace, we can reach our <code>rg-0</code> Pod through its fully qualified domain name: <code>rg-0.random-generator.default.svc.cluster.local</code>, where the Pod’s name is prepended to the Service name. This mapping allows other members of the clustered application or other clients to reach specific Pods if they wish to.</p>&#13;
&#13;
<p>We can also perform DNS lookup for Service (SRV) records (e.g., through <code>dig SRV random-generator.default.svc.cluster.local</code>) and discover all running Pods registered with the StatefulSet’s governing Service. This mechanism allows dynamic cluster member discovery if any client application needs to do so. The association between the headless Service and the StatefulSet is not only based on the selectors, but the StatefulSet should also link back to the Service by its name as <code>serviceName: "random-generator"</code>.</p>&#13;
&#13;
<p>Having dedicated storage defined through <code>volumeClaimTemplates</code> is not mandatory, but linking to a Service through <code>serviceName</code> field is. The governing Service must exist before the StatefulSet is created and is responsible for the network identity of the set. You can always create other types of Services that also load balance across your stateful Pods if that is what you want.</p>&#13;
&#13;
<p>As <a data-type="xref" href="#img-stateful-service-application">Figure 12-1</a> shows, StatefulSets offer a set of building blocks and guaranteed behavior needed for managing stateful applications in a distributed environment. Your job is to choose and use them in a meaningful way for your stateful use case.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="img-stateful-service-application">&#13;
<img alt="A Distributed Stateful Application on Kubernetes" src="assets/kup2_1201.png"/>&#13;
<h6><span class="label">Figure 12-1. </span>A distributed stateful application on Kubernetes</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Identity" data-type="sect2"><div class="sect2" id="idm45902102868976">&#13;
<h2>Identity</h2>&#13;
&#13;
<p><em>Identity</em> is the meta building block all other StatefulSet guarantees are built upon. A predictable Pod name and identity is generated based on StatefulSet’s name. We then use that identity to name PVCs, reach out to specific Pods through headless Services, and more. You can predict the identity of every Pod before creating it and use that knowledge in the application itself if needed.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ordinality" data-type="sect2"><div class="sect2" id="idm45902102866704">&#13;
<h2>Ordinality</h2>&#13;
&#13;
<p>By definition, a distributed stateful application consists of multiple instances that are unique and nonswappable. In addition to their uniqueness, instances may also be related to one another based on their instantiation order/position, and this is where the <em>ordinality</em> requirement comes in.</p>&#13;
&#13;
<p>From a StatefulSet point of view, the only place where ordinality comes into play is during scaling. Pods have names that have an ordinal suffix (starting from 0), and that Pod creation order also defines the order in which Pods are scaled up and down (in reverse order, from <em>n</em> – 1 to 0).</p>&#13;
&#13;
<p>If we create a ReplicaSet with multiple replicas, Pods are scheduled and started together without waiting for the first one to start successfully (running and ready status, as described in<a data-primary="Health Probe" data-type="indexterm" id="idm45902102863200"/><a data-primary="Health Probe" data-secondary="Stateful Service" data-type="indexterm" id="idm45902102862496"/> <a data-type="xref" data-xrefstyle="chap-num-title" href="ch04.html#HealthProbe">Chapter 4, “Health Probe”</a>). The order in which Pods are starting and are ready is not guaranteed. It is the same when we scale down a ReplicaSet (either by changing the <code>replicas</code> count or deleting it). All Pods belonging to a ReplicaSet start shutting down simultaneously without any ordering and dependency among them. This behavior may be faster to complete but is not preferred for stateful applications, especially if data partitioning and distribution are involved among the instances.</p>&#13;
&#13;
<p>To allow proper data synchronization during scale-up and -down, StatefulSet by default performs sequential startup and shutdown. That means Pods start from the first one (with index 0), and only when that Pod has successfully started is the next one scheduled (with index 1), and the sequence continues. During scaling down, the order reverses—first shutting down the Pod with the highest index, and only when it has shut down successfully is the Pod with the next lower index stopped. This sequence continues until the Pod with index 0 is terminated.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other Features" data-type="sect2"><div class="sect2" id="idm45902102859232">&#13;
<h2>Other Features</h2>&#13;
&#13;
<p>StatefulSets have other aspects that are customizable to suit the needs of stateful applications. Each stateful application is unique and requires careful consideration while trying to fit it into the StatefulSet model. Let’s see a few more Kubernetes features that may turn out to be useful while taming stateful applications:</p>&#13;
<dl>&#13;
<dt>Partitioned updates</dt>&#13;
<dd>&#13;
<p>We<a data-primary="partitioned updates" data-type="indexterm" id="idm45902102856288"/><a data-primary="updates" data-secondary="partitioned updates" data-type="indexterm" id="idm45902102855552"/> described earlier the sequential ordering guarantees when scaling a StatefulSet. As for updating an already-running stateful application (e.g., by altering the <code>.spec.template</code> element), StatefulSets allow phased rollout (such as a canary release), which guarantees a certain number of instances to remain intact while applying updates to the rest of the instances.</p>&#13;
&#13;
<p>By using the default rolling update strategy, you can partition instances by specifying a <code>.spec.updateStrategy.rollingUpdate.partition</code> number. The parameter (with a default value of 0) indicates the ordinal at which the StatefulSet should be partitioned for updates. If the parameter is specified, all Pods with an ordinal index greater than or equal to the <code>partition</code> are updated, while all Pods with an ordinal less than that are not updated. That is true even if the Pods are deleted; Kubernetes recreates them at the previous version. This feature can enable partial updates to clustered stateful applications (ensuring the quorum is preserved, for example) and then roll out the changes to the rest of the cluster by setting the <code>partition</code> back to 0.</p>&#13;
</dd>&#13;
<dt>Parallel deployments</dt>&#13;
<dd>&#13;
<p>When<a data-primary="parallel deployments" data-type="indexterm" id="idm45902102850736"/><a data-primary="deployment" data-secondary="parallel" data-type="indexterm" id="idm45902102850000"/> we set <code>.spec.podManagementPolicy</code> to <code>Parallel</code>, the StatefulSet launches or terminates all Pods in parallel and does not wait for Pods to run and become ready or completely terminated before moving to the next one. If sequential processing is not a requirement for your stateful application, this option can speed up operational procedures.</p>&#13;
</dd>&#13;
<dt>At-Most-One Guarantee</dt>&#13;
<dd>&#13;
<p>Uniqueness<a data-primary="At-Most-One" data-type="indexterm" id="idm45902102846480"/> is among the fundamental attributes of stateful application instances, and Kubernetes guarantees that uniqueness by making sure no two Pods of a StatefulSet have the same identity or are bound to the same PV. In contrast, ReplicaSet offers the <em>At-Least-X-Guarantee</em> for its instances. For example, a ReplicaSet with two replicas tries to keep at least two instances up and running at all times. Even if there is occasionally a chance for that number to go higher, the controller’s priority is not to let the number of Pods go below the specified number. It is possible to have more than the specified number of replicas running when a Pod is being replaced by a new one and the old Pod is still not fully terminated. Or, it can go higher if a Kubernetes node is unreachable with <code>NotReady</code> state but still has running Pods. In this scenario, the ReplicaSet’s controller would start new Pods on healthy nodes, which could lead to more running Pods than desired. That is all acceptable within the semantics of At-Least-X.</p>&#13;
&#13;
<p>A StatefulSet controller, on the other hand, makes every possible check to ensure there are no duplicate Pods—hence the <em>At-Most-One Guarantee</em>. It does not start a Pod again unless the old instance is confirmed to be shut down completely. When a node fails, it does not schedule new Pods on a different node unless Kubernetes can confirm that the Pods (and maybe the whole node) are shut down. The At-Most-One semantics of StatefulSets dictates these rules.</p>&#13;
&#13;
<p>It is still possible to break these guarantees and end up with duplicate Pods in a StatefulSet, but this requires active human intervention. For example, deleting an unreachable node resource object from the API Server while the physical node is still running would break this guarantee. Such an action should be performed only when the node is confirmed to be dead or powered down and no Pod processes are running on it. Or, for example, when you are forcefully deleting a Pod with<a data-primary="kubectl" data-secondary="deleting Pods" data-type="indexterm" id="idm45902102843568"/><a data-primary="Pods" data-secondary="deleting with kubectl" data-type="indexterm" id="idm45902102842592"/> <code>kubectl delete pods &lt;pod&gt; --grace-period=0 --force</code>, which does not wait for a confirmation from the Kubelet that the Pod is terminated. This action immediately clears the Pod from the API Server and causes the StatefulSet controller to start a replacement Pod that could lead to duplicates.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>We<a data-primary="Singleton Service" data-type="indexterm" id="idm45902102840400"/><a data-primary="Singleton Service" data-secondary="Stateful Service" data-type="indexterm" id="idm45902102839664"/> discuss other approaches to achieving singletons in more depth in <a data-type="xref" data-xrefstyle="chap-num-title" href="ch10.html#SingletonService">Chapter 10, “Singleton Service”</a>.<a data-primary="" data-startref="RSstateful12" data-type="indexterm" id="idm45902102837392"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discussion" data-type="sect1"><div class="sect1" id="idm45902102836000">&#13;
<h1>Discussion</h1>&#13;
&#13;
<p>In this chapter, we saw some of the standard requirements and challenges in managing distributed stateful applications on a cloud native platform. We discovered that handling a single-instance stateful application is relatively easy, but handling distributed state is a multidimensional challenge. While we typically associate the notion of “state” with “storage,” here we have seen multiple facets of state and how it requires different guarantees from different stateful applications. In this space, StatefulSets is an excellent primitive for implementing distributed stateful applications generically. It addresses the need for persistent storage, networking (through Services), identity, ordinality, and a few other aspects. It provides a good set of building blocks for managing stateful applications in an automated fashion, making them first-class citizens in the cloud native world.</p>&#13;
&#13;
<p>StatefulSets are a good start and a step forward, but the world of stateful applications is unique and complex. In addition to the stateful applications designed for a cloud native world that can fit into a StatefulSet, a ton of legacy stateful applications exist that have not been designed for cloud native platforms and have even more needs. Luckily Kubernetes has an answer for that too. The Kubernetes community has realized that rather than modeling different workloads through Kubernetes resources and implementing their behavior through generic<a data-primary="Controller" data-type="indexterm" id="idm45902102833712"/><a data-primary="Controller" data-secondary="Stateful Service" data-type="indexterm" id="idm45902102833008"/> controllers, it should allow users to implement their custom controllers and even go one step further and allow modeling application resources through custom resource definitions and behavior through<a data-primary="Operator" data-secondary="Stateful Service" data-type="indexterm" id="idm45902102831936"/><a data-primary="Operator" data-type="indexterm" id="idm45902102830992"/> operators.</p>&#13;
&#13;
<p>In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch27.html#Controller">27</a> and&#13;
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch28.html#Operator">28</a>, you will learn about the related <em>Controller</em> and <em>Operator</em> patterns, which are better suited for managing complex stateful applications in cloud native environments.<a data-primary="" data-startref="staser12" data-type="indexterm" id="idm45902102826256"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="More Information" data-type="sect1"><div class="sect1" id="statefulset-more-information">&#13;
<h1>More Information</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/FXeca">Stateful Service Example</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/NdHnS">StatefulSet Basics</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/WyxHN">StatefulSets</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/YECff">Example: Deploying Cassandra with a Stateful Set</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/WzQXP">Running ZooKeeper, a Distributed System Coordinator</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/7GPda">Headless Services</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/ZRTlO">Force Delete StatefulSet Pods</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/7Zw-5">Graceful Scaledown of Stateful Apps in Kubernetes</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45902101914272"><sup><a href="ch12.html#idm45902101914272-marker">1</a></sup> Let’s assume we have invented a highly sophisticated way of generating random numbers in a distributed Random Number Generator (RNG) cluster with several instances of our service as nodes. Of course, that’s not true, but for this example’s sake, it’s a good enough story.</p></div></div></section></body></html>