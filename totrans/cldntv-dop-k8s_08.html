<html><head></head><body><section data-pdf-bookmark="Chapter 6. Operating Clusters" data-type="chapter" epub:type="chapter"><div class="chapter" id="operating">&#13;
<h1><span class="label">Chapter 6. </span>Operating Clusters</h1>&#13;
&#13;
<blockquote class="epigraph">&#13;
<p>If Tetris has taught me anything, it’s that errors pile up and accomplishments disappear.</p>&#13;
<p data-type="attribution">Andrew Clay Shafer</p>&#13;
</blockquote>&#13;
&#13;
<p><a data-primary="Kubernetes" data-secondary="cluster operation" data-type="indexterm" id="ix_06-operating-adoc0"/>Once you have a Kubernetes cluster, how do you know it’s in good shape and running properly? How do you scale to cope with demand, but keep cloud costs to a minimum? In this chapter, we’ll look at the issues involved in operating Kubernetes clusters for production workloads, and some of the tools that can help you.</p>&#13;
&#13;
<p>As we’ve seen in <a data-type="xref" href="ch03.html#gettingk8s">Chapter 3</a>, there are many important things to consider about your Kubernetes cluster: availability, authentication, upgrades, and so on. If you’re using a good managed Kubernetes service, as we recommend, most of these issues should be taken care of for you.</p>&#13;
&#13;
<p>However, what you actually do with the cluster is up to you. In this chapter, you’ll learn how to size and scale the cluster, check it for <em>conformance</em>, and test the resilience of your infrastructure with <em>Chaos Monkeys</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Sizing and Scaling" data-type="sect1"><div class="sect1" id="sizing">&#13;
<h1>Cluster Sizing and Scaling</h1>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="sizing" data-type="indexterm" id="ix_06-operating-adoc1"/>How big does your cluster need to be? With self-hosted Kubernetes clusters, and almost all managed services, the ongoing cost of your cluster depends directly on the number and size of its nodes. If the capacity of the cluster is too small, your workloads won’t run properly, or will fail under heavy traffic. If the capacity is too large, you’re wasting money.</p>&#13;
&#13;
<p>Sizing and scaling your cluster appropriately is very important, so let’s look at some of the decisions involved.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Capacity Planning" data-type="sect2"><div class="sect2" id="idm45979387201488">&#13;
<h2>Capacity Planning</h2>&#13;
&#13;
<p><a data-primary="capacity planning" data-type="indexterm" id="ix_06-operating-adoc2"/>One way to make an initial estimate of the capacity you need is to think about how many traditional servers you would need to run the same applications. For example, if your current architecture runs on 10 separate cloud virtual machine instances, you probably won’t need more than 10 nodes of similar sizes in your Kubernetes cluster to run the same workload, plus another 1 or 2 for redundancy. In fact, you might not even need that many because Kubernetes will balance containers evenly across all machines, and can therefore achieve higher utilization than with traditional servers. But it may take some time and practical experience to tune your cluster for optimal capacity.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The smallest cluster" data-type="sect3"><div class="sect3" id="smallest">&#13;
<h3>The smallest cluster</h3>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="minimum size" data-type="indexterm" id="idm45979387197264"/>When you’re first setting up a cluster, you will probably be using it to play around and experiment, and figure out how to run your application. So you probably don’t need to burn money on a large cluster until you have some idea what capacity you’re going to need.</p>&#13;
&#13;
<p>The smallest possible Kubernetes cluster is a single node. This will allow you to try out Kubernetes and run small workloads for development, as we saw in <a data-type="xref" href="ch02.html#firststeps">Chapter 2</a>. However, a single-node cluster has no resilience against the failure of the node hardware, or of the Kubernetes API server or the kubelet (the agent daemon that is responsible for running workloads on each node).</p>&#13;
&#13;
<p>If you’re using a managed Kubernetes service like GKE, EKS, or AKS (see <a data-type="xref" href="ch03.html#managedclusters">“Managed Kubernetes Services”</a>), then you don’t need to worry about provisioning control plane nodes: this is done for you. If, on the other hand, you’re building your own cluster, you’ll need to decide how to lay out the control plane.</p>&#13;
&#13;
<p>The minimum number of control plane nodes for a resilient Kubernetes cluster is three. One wouldn’t be resilient, and two could disagree about which was the leader, so at least three nodes are needed.</p>&#13;
&#13;
<p>While you can do useful work on a Kubernetes cluster this small, it’s not recommended. A better idea is to add some worker nodes so that your own workloads aren’t competing for resources with the Kubernetes control plane.</p>&#13;
&#13;
<p>Provided your cluster control plane is highly available, you <em>can</em> get by with a single worker node, but two nodes is the sensible minimum to protect against node failure and to allow Kubernetes to run at least two replicas of every Pod. The more nodes there are the better, especially as the Kubernetes scheduler cannot always ensure that workloads are fully balanced across available nodes (see <a data-type="xref" href="ch05.html#balanced">“Keeping Your Workloads Balanced”</a>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="K3S" data-type="sect3"><div class="sect3" id="idm45979387189952">&#13;
<h3>K3S</h3>&#13;
&#13;
<p><a data-primary="K3s" data-type="indexterm" id="idm45979387188848"/>When it comes to lightweight clusters, a tool like <a href="https://k3s.io">K3s</a>&#13;
is worth checking out. It packages all Kubernetes components into a single binary, making it great for environments that are isolated and/or resource constrained.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Kubernetes clusters need at least three nodes running the control plane components in order to be highly available, and you may need more to handle the work of larger clusters. Two worker nodes is the minimum required to make your workloads fault tolerant to the failure of a single node, and three worker nodes is even better.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The biggest cluster" data-type="sect3"><div class="sect3" id="idm45979387185584">&#13;
<h3>The biggest cluster</h3>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="maximum size" data-type="indexterm" id="idm45979387184032"/>Is there a limit to how large Kubernetes clusters can be? The short answer is yes, but you almost certainly won’t have to worry about it; Kubernetes version 1.22 officially supports clusters of up to 5,000 nodes.</p>&#13;
&#13;
<p>Because clustering requires communication between nodes, the number of possible communication paths, and the cumulative load on the underlying database, grows exponentially with the size of the cluster. While Kubernetes <em>may</em> still function with more than 5,000 nodes, it’s not <em>guaranteed</em> to work, or at least to be responsive enough to deal with production workloads.</p>&#13;
&#13;
<p>The Kubernetes documentation advises that <a href="https://oreil.ly/NapFi">supported cluster configurations</a> must have no more than 5,000 nodes, no more than 150,000 total Pods, no more than 300,000 total containers, and no more than 100 Pods per node. It’s worth bearing in mind that the larger the cluster, the bigger the load on the control plane nodes; if you’re responsible for running your own control plane, they’ll need to be pretty powerful machines to cope with a cluster of thousands of nodes.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>For maximum reliability, keep your Kubernetes clusters smaller than 5,000 nodes and 150,000 Pods (this isn’t an issue for most users). If you need more resources, run multiple clusters.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Federated clusters" data-type="sect3"><div class="sect3" id="federation">&#13;
<h3>Federated clusters</h3>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="federation" data-type="indexterm" id="idm45979387177088"/><a data-primary="federation" data-type="indexterm" id="idm45979387176112"/>If you have extremely demanding workloads or need to operate at huge scale, these limits may become a practical problem for you. In this case, you can run multiple Kubernetes clusters, and, if necessary, <em>federate</em> them so that workloads can be <span class="keep-together">replicated</span> across clusters.</p>&#13;
&#13;
<p>Federation provides the ability to keep two or more clusters synchronized, running identical workloads. This can be useful if you need Kubernetes clusters in different cloud providers, for resilience, or in different geographical locations, to reduce latency for your users. A group of federated clusters can keep running even if an individual cluster fails.</p>&#13;
&#13;
<p>You can read more about <em>cluster federation</em> in the Kubernetes <a href="https://oreil.ly/QdAB2">documentation</a>.</p>&#13;
&#13;
<p>For most Kubernetes users, federation isn’t something they need to be concerned with, and, in practice, most users at very large scale are able to handle their workloads with multiple unfederated clusters of a few hundred to a few thousand nodes each. Often, provisioning smaller separated clusters across team or application boundaries ends up being easier to manage compared to large, centralized federated clusters.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>If you need to replicate workloads across multiple clusters, perhaps for geographical redundancy or latency reasons, use federation. Most users don’t need to federate their clusters, though.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Do I need multiple clusters?" data-type="sect3"><div class="sect3" id="multiclusters">&#13;
<h3>Do I need multiple clusters?</h3>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="multiple" data-type="indexterm" id="idm45979387168400"/>Unless you’re operating at very large scale, as we mentioned in the previous section and in <a data-type="xref" href="ch03.html#multicloud">“Multicloud Kubernetes Clusters”</a>, you probably don’t need more than one or two clusters: maybe one for production, and one for staging and testing.</p>&#13;
&#13;
<p>For convenience and ease of resource management, you can divide your cluster into logical partitions using namespaces, which we covered in more detail in <a data-type="xref" href="ch05.html#namespaces">“Using Namespaces”</a>. With a few exceptions, it’s not usually worth the administration overhead of managing multiple clusters.</p>&#13;
&#13;
<p>There are some specific situations, such as security and regulatory compliance, where you might want to ensure that services in one cluster are absolutely isolated from those in another (for example, when dealing with protected health information, or when data can’t be transmitted from one geographical location to another for legal reasons). In those cases, you need to create separate clusters. For most Kubernetes users, this won’t be an issue.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use a single production and a single staging cluster, unless you really need complete isolation of one set of workloads or teams from another. If you just want to partition your cluster for ease of management, use namespaces instead.<a data-startref="ix_06-operating-adoc2" data-type="indexterm" id="idm45979387162976"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Nodes and Instances" data-type="sect2"><div class="sect2" id="idm45979387161888">&#13;
<h2>Nodes and Instances</h2>&#13;
&#13;
<p><a data-primary="nodes" data-secondary="instance types" data-type="indexterm" id="ix_06-operating-adoc3"/>The more capacity a given node has, the more work it can do, where capacity is expressed in terms of the number of CPU cores (virtual or otherwise), available memory, and to a lesser extent, disk space. But is it better to run 10 very large nodes, for example, rather than 100 much smaller ones?</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Picking the right node size" data-type="sect3"><div class="sect3" id="nodesizing">&#13;
<h3>Picking the right node size</h3>&#13;
&#13;
<p><a data-primary="nodes" data-secondary="sizing" data-type="indexterm" id="idm45979387157184"/>There’s no universally correct node size for Kubernetes clusters. The answer depends on your cloud or hardware provider, and on your specific workloads.</p>&#13;
&#13;
<p>The cost per capacity of different instance sizes can have an effect on the way you decide to size your nodes. For example, some cloud providers may offer a slight discount on larger instance sizes so that if your workloads are very compute intensive, it may be cheaper to run them on a few very large nodes instead of many smaller ones.</p>&#13;
&#13;
<p>The number of nodes required in the cluster also affects the choice of node size. To get the advantages that Kubernetes offers, such as Pod replication and high availability, you need to spread work across several nodes. But if nodes have too much spare capacity, that’s a waste of money.</p>&#13;
&#13;
<p>If you need, say, at least 10 nodes for high availability, but each node only needs to run a couple of Pods, the node instances can be very small. On the other hand, if you only need two nodes, you can make them quite large and potentially save money with more favorable instance pricing.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Use the most cost-effective node type that your provider offers. Often, larger nodes work out cheaper, but if you only have a handful of nodes, you might want to add some smaller ones, to help with redundancy.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cloud instance types" data-type="sect3"><div class="sect3" id="idm45979387152000">&#13;
<h3>Cloud instance types</h3>&#13;
&#13;
<p>Because the Kubernetes components themselves, such as the kubelet, use a given amount of resources, and you will need some spare capacity to do useful work, the smallest instance sizes offered by your cloud provider will probably not be suitable for Kubernetes.</p>&#13;
&#13;
<p>A control plane for small clusters (up to around five total nodes) should have at least two CPUs and two GiB of memory, with larger clusters requiring more memory and CPUs for each control plane node.</p>&#13;
&#13;
<p>For larger clusters, with perhaps a few tens of nodes, it may make sense for you to provision a mix of two or three different instance sizes. This means that Pods with compute-intensive workloads requiring a lot of memory can be scheduled by Kubernetes on large nodes, leaving smaller nodes free to handle smaller Pods (see <a data-type="xref" href="ch09.html#nodeaffinities">“Node Affinities”</a>). This gives the Kubernetes scheduler the maximum freedom of choice when deciding where to run a given Pod.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Heterogeneous nodes" data-type="sect3"><div class="sect3" id="idm45979387148128">&#13;
<h3>Heterogeneous nodes</h3>&#13;
&#13;
<p><a data-primary="GPU (graphics processing unit)" data-type="indexterm" id="idm45979387146784"/><a data-primary="heterogeneous nodes" data-type="indexterm" id="idm45979387146112"/><a data-primary="nodes" data-secondary="GPU" data-type="indexterm" id="idm45979387145440"/><a data-primary="nodes" data-secondary="heterogeneous" data-type="indexterm" id="idm45979387144496"/>Not all nodes are created equal. You may need some nodes with special properties, such as a graphics processing unit (GPU). GPUs are high-performance parallel processors that are widely used for compute-intensive problems that have nothing to do with graphics, such as machine learning or data analysis.</p>&#13;
&#13;
<p>You can use the <em>resource limits</em> functionality in Kubernetes (see <a data-type="xref" href="ch05.html#resourcelimits">“Resource Limits”</a>) to specify that a given Pod needs at least one GPU, for example. This will ensure that those Pods will run only on GPU-enabled nodes, and get priority over Pods that can run on any node.</p>&#13;
&#13;
<p>Most Kubernetes nodes probably run Linux of one kind or another, which is suitable for almost all applications. Recall that containers are <em>not</em> virtual machines, so the process inside a container runs directly on the kernel of the operating system on the underlying node. A Windows binary will not run on a Linux Kubernetes node, for example, so if you need to run Windows containers, you will have to provision Windows nodes for them.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Most containers are built for Linux, so you’ll probably want to run mostly Linux-based nodes. You may need to add one or two special types of nodes for specific requirements, such as GPUs or Windows.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Bare-metal servers" data-type="sect3"><div class="sect3" id="idm45979387138400">&#13;
<h3>Bare-metal servers</h3>&#13;
&#13;
<p><a data-primary="bare-metal servers, Kubernetes on" data-type="indexterm" id="idm45979387137200"/>One of the most useful properties of Kubernetes is its ability to connect all sorts of machines of different sizes, architectures, and capabilities to provide a single, unified, logical machine on which workloads can run. While Kubernetes is usually associated with cloud servers, many organizations have large numbers of physical, bare-metal machines in datacenters that can potentially be harnessed into Kubernetes clusters.</p>&#13;
&#13;
<p>We saw in <a data-type="xref" href="ch01.html#revolution">Chapter 1</a> that cloud technology transforms <em>capex</em> infrastructure (purchasing machines as a capital expense) to <em>opex</em> infrastructure (leasing compute capacity as an operating expense), and this makes financial sense. However, if your business already owns a large number of bare-metal servers, you don’t need to write them off just yet: instead, consider joining them into a Kubernetes cluster (see <a data-type="xref" href="ch03.html#baremetal">“Bare-Metal and On-Prem”</a>).</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>If you have hardware servers with spare capacity, or you’re not ready to migrate completely to the cloud yet, use Kubernetes to run container workloads on your existing machines.<a data-startref="ix_06-operating-adoc3" data-type="indexterm" id="idm45979387131632"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scaling the Cluster" data-type="sect2"><div class="sect2" id="idm45979387130544">&#13;
<h2>Scaling the Cluster</h2>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="scaling" data-type="indexterm" id="idm45979387129232"/>Having chosen a sensible starting size for your cluster, and picked the right mix of instance sizes for your worker nodes, is that the end of the story? Almost certainly not: over time, you may need to grow or shrink the cluster to match changes in demand, or in business requirements.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Instance groups" data-type="sect3"><div class="sect3" id="idm45979387127696">&#13;
<h3>Instance groups</h3>&#13;
&#13;
<p><a data-primary="instance group" data-type="indexterm" id="idm45979387126528"/>It’s easy to add nodes to a Kubernetes cluster. If you’re running a self-hosted cluster, a cluster management tool such as kops (see <a data-type="xref" href="ch03.html#kops">“kops”</a>) can do it for you. kops has the concept of an <em>instance group</em>, which is a set of nodes of a given instance type (for example, <code>m5.large</code>). Managed services, such as GKE, have the same facility, called <em>node pools</em>. The Elastic Kubernetes Service (EKS) tool <code>eksctl</code> refers to this concept as a <a href="https://oreil.ly/mRr0j"><em>nodegroup</em></a>.</p>&#13;
&#13;
<p>You can scale instance groups or node pools either by changing the minimum and maximum size for the group, or by changing the specified instance type, or both.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scaling down" data-type="sect3"><div class="sect3" id="drain">&#13;
<h3>Scaling down</h3>&#13;
&#13;
<p>In principle, there’s no problem with scaling down a Kubernetes cluster either. You can tell Kubernetes to <em>drain</em> the nodes you want to remove, which will gradually shut down or move any running Pods on those nodes elsewhere.</p>&#13;
&#13;
<p><a data-primary="draining nodes" data-type="indexterm" id="idm45979387118288"/><a data-primary="kubectl" data-secondary="commands" data-tertiary="drain" data-type="indexterm" id="idm45979387117584"/><a data-primary="nodes" data-secondary="draining" data-type="indexterm" id="idm45979387116368"/>Most cluster management tools will do the node draining for you automatically, or you can use the <code>kubectl drain</code> command to do it yourself. Provided there is enough spare capacity in the rest of the cluster to reschedule the doomed Pods, once the nodes have been successfully drained, you can terminate them.</p>&#13;
&#13;
<p><a data-primary="PodDisruptionBudget" data-type="indexterm" id="idm45979387114512"/>To avoid overly reducing the number of Pod replicas for a given service, you can use PodDisruptionBudgets to specify a minimum number of available Pods, or the maximum number of Pods that can be <em>unavailable</em> at any time (see <a data-type="xref" href="ch05.html#poddisruptionbudgets">“Pod Disruption Budgets”</a>).</p>&#13;
&#13;
<p>If draining a node would cause Kubernetes to exceed these limits, the drain operation will block until you change the limits or free up some more resources in the cluster.</p>&#13;
&#13;
<p>Draining allows Pods to shut down gracefully, cleaning up after themselves and saving any necessary state. For most applications, this is preferable to simply shutting down the node, which will terminate the Pods immediately.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Don’t just shut down nodes when you don’t need them anymore. Drain them first to ensure their workloads are migrated to other nodes, and to make sure you have enough spare capacity remaining in the cluster.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Autoscaling" data-type="sect3"><div class="sect3" id="autoscaling">&#13;
<h3>Autoscaling</h3>&#13;
&#13;
<p><a data-primary="autoscaling" data-secondary="benefits of" data-type="indexterm" id="idm45979387107744"/><a data-primary="clusters" data-secondary="autoscaling" data-type="indexterm" id="idm45979387106768"/>Most cloud providers support autoscaling: automatically increasing or reducing the number of instances in a group according to some metric or schedule. For example, AWS autoscaling groups (ASGs) can maintain a minimum and maximum number of instances so that if one instance fails, another will be started to take its place, or if too many instances are running, some will be shut down.</p>&#13;
&#13;
<p>Alternatively, if your demand fluctuates according to the time of day, you can schedule the group to grow and shrink at specified times. You can also configure the scaling group to grow or shrink dynamically on demand: if the average CPU utilization exceeds 90% over a 15-minute period, for example, instances can be added automatically until the CPU usage falls below the threshold. When demand falls again, the group can be scaled down to save money.</p>&#13;
&#13;
<p>Kubernetes has a Cluster Autoscaler add-on that cluster management tools like kops can take advantage of to enable cloud autoscaling, and managed clusters such as AKS also offer autoscaling.</p>&#13;
&#13;
<p>However, it can take some time and experimentation to get your autoscaling settings right, and for many users it may not be necessary at all. Most Kubernetes clusters start small and grow gradually and monotonically by adding a node here and there as resource usage grows.</p>&#13;
&#13;
<p>For large-scale users, though, or applications where demand is highly variable, cluster autoscaling is a very useful feature.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Don’t enable cluster autoscaling just because it’s there unless you already know you need it. You probably won’t need it unless your demands or workloads are extremely variable. Start by scaling your cluster manually and getting comfortable monitoring usage as you get a sense of how your scale requirements are changing over time.<a data-startref="ix_06-operating-adoc1" data-type="indexterm" id="idm45979387101776"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conformance Checking" data-type="sect1"><div class="sect1" id="conformance">&#13;
<h1>Conformance Checking</h1>&#13;
&#13;
<p><a data-primary="clusters" data-secondary="conformance checking" data-type="indexterm" id="ix_06-operating-adoc4"/><a data-primary="conformance testing" data-secondary="benefits of" data-type="indexterm" id="ix_06-operating-adoc5"/>When is Kubernetes not Kubernetes? The flexibility of Kubernetes means there are lots of different ways to set up Kubernetes clusters, and this presents a potential <span class="keep-together">problem</span>. If Kubernetes is to be a universal platform, you should be able to take a workload and run it on any Kubernetes cluster and have it work the way you expect. That means the same API calls and Kubernetes objects have to be available, they have to have the same behavior, they have to work as advertised, and so on.</p>&#13;
&#13;
<p>Fortunately, Kubernetes itself includes a test suite that verifies that a given Kubernetes cluster is <em>conformant</em>; that is, it satisfies a core set of requirements for a given Kubernetes version. These conformance tests are very useful for Kubernetes administrators.</p>&#13;
&#13;
<p>If your cluster doesn’t pass them, then there is a problem with your setup that needs to be addressed. If it does pass, knowing that it’s conformant gives you confidence that applications designed for Kubernetes will work with your cluster, and that things you build on your cluster will work elsewhere too.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CNCF Certification" data-type="sect2"><div class="sect2" id="idm45979387094208">&#13;
<h2>CNCF Certification</h2>&#13;
&#13;
<p><a data-primary="Cloud Native Computing Foundation (CNCF)" data-type="indexterm" id="ix_06-operating-adoc6"/><a data-primary="CNCF (Cloud Native Computing Foundation)" data-type="indexterm" id="ix_06-operating-adoc7"/>The Cloud Native Computing Foundation (CNCF) is the official owner of the Kubernetes project and trademark (see <a data-type="xref" href="ch01.html#cloudnative">“Cloud Native”</a>), and it provides various kinds of certifications for Kubernetes-related products, engineers, and vendors.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Certified Kubernetes" data-type="sect3"><div class="sect3" id="idm45979387089296">&#13;
<h3>Certified Kubernetes</h3>&#13;
&#13;
<p><a data-primary="Certified Kubernetes" data-type="indexterm" id="idm45979387087776"/>If you use a managed, or partially managed, Kubernetes service, check whether it carries the Certified Kubernetes mark and logo (see <a data-type="xref" href="#img-certifiedk8s">Figure 6-1</a>). This indicates that the vendor and service meet the <a href="https://oreil.ly/NKgpp">Certified Kubernetes standard</a>, as specified by the CNCF.</p>&#13;
&#13;
<figure><div class="figure" id="img-certifiedk8s">&#13;
<img alt="The Certified Kubernetes logo" src="assets/cnd2_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>The Certified Kubernetes mark means that the product or service is approved by the CNCF</h6>&#13;
</div></figure>&#13;
&#13;
<p>If the product has <em>Kubernetes</em> in the name, it must be <a href="https://oreil.ly/A5Mku">certified by the CNCF</a>. This means customers know exactly what they’re getting and can be satisfied that it will be interoperable with other conformant Kubernetes services. Vendors can self-certify their products by running the Sonobuoy conformance checking tool (see <a data-type="xref" href="#sonobuoy">“Conformance Testing with Sonobuoy”</a>).</p>&#13;
&#13;
<p>Certified Kubernetes products also have to track the latest version of Kubernetes, providing updates at least annually. It’s not just managed services that can carry the Certified Kubernetes mark; distributions and installer tools can, too.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Certified Kubernetes Administrator (CKA)" data-type="sect3"><div class="sect3" id="idm45979387079984">&#13;
<h3>Certified Kubernetes Administrator (CKA)</h3>&#13;
&#13;
<p><a data-primary="Certified Kubernetes Administrator (CKA)" data-type="indexterm" id="idm45979387078752"/><a data-primary="CKA (Certified Kubernetes Administrator)" data-type="indexterm" id="idm45979387077952"/>To become a Certified Kubernetes Administrator (CKA), you need to demonstrate that you have the key skills to manage Kubernetes clusters in production, including installation and configuration, networking, maintenance, knowledge of the API, security, and troubleshooting. Anyone can take the CKA exam, which is administered online and includes a series of challenging practical tests. The <a href="https://oreil.ly/810Ot">CNCF website</a> has more info about how to train and how to register to take the exam.</p>&#13;
&#13;
<p>The CKA exam has a reputation as a tough, comprehensive exam that really tests your skills and knowledge. You can be confident that any engineer who is CKA certified really knows Kubernetes. If you run your business on Kubernetes, consider putting some of your staff through the CKA program, especially those directly responsible for managing clusters.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Certified Service Provider (KCSP)" data-type="sect3"><div class="sect3" id="idm45979387075216">&#13;
<h3>Kubernetes Certified Service Provider (KCSP)</h3>&#13;
&#13;
<p><a data-primary="KCSP (Kubernetes Certified Service Provider)" data-type="indexterm" id="idm45979387074048"/><a data-primary="Kubernetes Certified Service Provider (KCSP)" data-type="indexterm" id="idm45979387073248"/>Vendors themselves can apply for the Kubernetes Certified Service Provider (KCSP) program. To be eligible, the vendor has to be a CNCF member, provide enterprise support (for example by supplying field engineers to a customer site), contribute actively to the Kubernetes community, and employ three or more CKA-certified engineers.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Look for the Certified Kubernetes mark to make sure that a product meets CNCF standards. Look for vendors to be KCSP-certified, and if you’re hiring Kubernetes administrators, look for a CKA qualification.<a data-startref="ix_06-operating-adoc7" data-type="indexterm" id="idm45979387070576"/><a data-startref="ix_06-operating-adoc6" data-type="indexterm" id="idm45979387069872"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conformance Testing with Sonobuoy" data-type="sect2"><div class="sect2" id="sonobuoy">&#13;
<h2>Conformance Testing with Sonobuoy</h2>&#13;
&#13;
<p><a data-primary="conformance testing" data-secondary="using Sonobuoy" data-type="indexterm" id="idm45979387066768"/><a data-primary="Sonobuoy" data-type="indexterm" id="idm45979387065792"/>If you’re managing your own cluster, or even if you’re using a managed service but want to double-check that it’s configured properly and up-to-date, you can run the Kubernetes conformance tests to prove it. The standard tool for running these tests is <a href="https://oreil.ly/dS40G"><em>Sonobuoy</em></a>.</p>&#13;
&#13;
<p>Sonobuoy uses a CLI tool and your <code>kubectl</code> authentication to run tests inside of your cluster. Once installed, you can run the test suite using:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">sonobuoy run</code></strong><code class="go">&#13;
</code><code class="go"> ...&#13;
</code><code class="go">INFO[0000] created object&#13;
</code><code class="go">name=sonobuoy namespace= resource=namespaces&#13;
</code><code class="go"> ...</code></pre>&#13;
&#13;
<p>A new namespace is created, the Sonobuoy pods are launched, and they start running the tests. You can see this with <code>kubectl</code>:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">kubectl get pods -n sonobuoy</code></strong><code class="go">&#13;
</code><code class="go">NAME       READY   STATUS    RESTARTS   AGE&#13;
</code><code class="go">sonobuoy   1/1     Running   0          14s&#13;
</code><code class="go">...</code></pre>&#13;
&#13;
<p>The full test suite can take an hour or more to complete! You can add the <code>--mode quick</code> flag to the <code>sonobuoy run</code> command to run a single test to verify connectivity.</p>&#13;
&#13;
<p>Once the conformance tests are complete, you can view the results using the <code>retrieve</code> command, which saves the output locally to a file. You can then inspect that output using the <code>results</code> command:</p>&#13;
&#13;
<pre data-code-language="console" data-type="programlisting"><strong><code class="go">results=$(sonobuoy retrieve)</code></strong><code class="go">&#13;
</code><strong><code class="go">sonobuoy results $results</code></strong><code class="go">&#13;
</code><code class="go">Plugin: e2e&#13;
</code><code class="go">Status: passed&#13;
</code><code class="go">Total: 5771&#13;
</code><code class="go">Passed: 1&#13;
</code><code class="go">Failed: 0&#13;
</code><code class="go">...</code></pre>&#13;
&#13;
<p>Later, in <a data-type="xref" href="ch11.html#cluster-security-scanning">“Cluster Security Scanning”</a>, we will cover similar tools that are focused on scanning your clusters for potential security issues. When used alongside Sonobuoy, these tools can give you a better picture of where your clusters may be out of line with current industry best practices for Kubernetes compliance.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>Run Sonobuoy once your cluster is set up for the first time, to verify that it’s standards compliant and that everything works. Run it again every so often to make sure there are no conformance problems.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Audit Logging" data-type="sect2"><div class="sect2" id="idm45979386995872">&#13;
<h2>Kubernetes Audit Logging</h2>&#13;
&#13;
<p><a data-primary="audit logging" data-type="indexterm" id="idm45979386994704"/><a data-primary="logs" data-secondary="audit logging" data-type="indexterm" id="idm45979386994000"/>Suppose you find a problem on your cluster, such as a Pod you don’t recognize, and you want to know where it came from. How do you find out who did what on the cluster when? The <a href="https://oreil.ly/3NS5l">Kubernetes audit log</a> will tell you.</p>&#13;
&#13;
<p>With audit logging enabled, all requests to the cluster API will be recorded, with a timestamp, saying who made the request (which service account), the details of the request (such as the resources it queried), and what the response was.</p>&#13;
&#13;
<p>The audit events can be sent to your central logging system, where you can filter and alert on them as you would for other log data (see <a data-type="xref" href="ch15.html#observability">Chapter 15</a>). A good managed service such as GKE will include audit logging by default, but otherwise you may need to configure the cluster yourself to enable it.<a data-startref="ix_06-operating-adoc5" data-type="indexterm" id="idm45979386968736"/><a data-startref="ix_06-operating-adoc4" data-type="indexterm" id="idm45979386968128"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Chaos Testing" data-type="sect1"><div class="sect1" id="chaos">&#13;
<h1>Chaos Testing</h1>&#13;
&#13;
<p><a data-primary="chaos testing" data-type="indexterm" id="ix_06-operating-adoc8"/>We pointed out in <a data-type="xref" href="ch03.html#trustbutverify">“Trust, but verify”</a> that the only real way to verify high availability is to kill one or more of your cluster nodes and see what happens. The same applies to the high availability of your Kubernetes Pods and applications. You could pick a Pod at random, for example, terminate it, and check that Kubernetes restarts it, and that your error rate is unaffected.</p>&#13;
&#13;
<p>Doing this manually is time-consuming, and, without realizing it, you may be unconsciously sparing resources that you know are application-critical. To make it a fair test, the process must be automated.</p>&#13;
&#13;
<p>This kind of automated, random interference with production services is sometimes known as <em>Chaos Monkey</em> testing, after the tool of the same name developed by Netflix to test its infrastructure:</p>&#13;
<blockquote>&#13;
<p>Imagine a monkey entering a data center, these farms of servers that host all the critical functions of our online activities. The monkey randomly rips cables, destroys <span class="keep-together">devices...</span></p>&#13;
&#13;
<p>The challenge for IT managers is to design the information system they are responsible for so that it can work despite these monkeys, which no one ever knows when they arrive and what they will destroy.</p>&#13;
<p data-type="attribution">Antonio Garcia Martinez, <cite><em>Chaos Monkeys</em></cite></p>&#13;
</blockquote>&#13;
&#13;
<p>Apart from Chaos Monkey itself, which terminates random cloud servers, the <span class="keep-together">Netflix</span> <em>Simian Army</em> also includes other <em>chaos engineering</em> tools such as Latency Monkey, which introduces communication delays to simulate network issues, Security <span class="keep-together">Monkey</span>, which looks for known vulnerabilities, and Chaos Gorilla, which drops a whole AWS availability zone.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Only Production Is Production" data-type="sect2"><div class="sect2" id="idm45979386954960">&#13;
<h2>Only Production Is Production</h2>&#13;
&#13;
<p>You can apply the Chaos Monkey idea to Kubernetes applications, too. While you can run chaos engineering tools on a staging cluster to avoid disrupting production, that can only tell you so much. To learn about your production environment, you need to test production:</p>&#13;
<blockquote>&#13;
  <p>Many systems are too big, complex, and cost-prohibitive to clone. Imagine trying to spin up a copy of Facebook for testing (with its multiple, globally distributed data centers).</p>&#13;
<p>The unpredictability of user traffic makes it impossible to mock; even if you could perfectly reproduce yesterday’s traffic, you still can’t predict tomorrow’s. Only production is production.</p>&#13;
<p data-type="attribution"><a href="https://oreil.ly/88wsu">Charity Majors </a></p>&#13;
</blockquote>&#13;
&#13;
<p>It’s also important to note that your chaos experiments, to be most useful, need to be automated and continuous. It’s no good doing it once and deciding that your system is reliable for evermore:</p>&#13;
<blockquote>&#13;
  <p>The whole point of automating your chaos experiments is so that you can run them again and again to build trust and confidence in your system. Not just surfacing new weaknesses, but also ensuring that you’ve overcome a weakness in the first place.</p>&#13;
  <p data-type="attribution"><a href="https://oreil.ly/dq0Ij">Russ Miles (ChaosIQ)</a></p>&#13;
</blockquote>&#13;
&#13;
<p>There are several tools you can use for automatically chaos engineering your cluster. Here are a few options.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="chaoskube" data-type="sect2"><div class="sect2" id="idm45979386946992">&#13;
<h2>chaoskube</h2>&#13;
&#13;
<p><a data-primary="chaoskube" data-type="indexterm" id="idm45979386945616"/><a href="https://oreil.ly/ffcdM"><em>chaoskube</em></a> randomly kills Pods in your cluster. By default it operates in dry-run mode, which shows you what it would have done, but doesn’t actually terminate anything.</p>&#13;
&#13;
<p>You can configure chaoskube to include or exclude Pods based on labels (see <a data-type="xref" href="ch09.html#labels">“Labels”</a>), annotations, and namespaces, and to avoid certain time periods or dates (for example, don’t kill anything on Christmas Eve). By default, though, it will potentially kill any Pod in any namespace, including Kubernetes system Pods, and even chaoskube itself.</p>&#13;
&#13;
<p>Once you’re happy with your chaoskube filter configuration, you can disable dry-run mode and let it do its work.</p>&#13;
&#13;
<p>chaoskube is simple to install and set up, and it’s an ideal tool for getting started with chaos engineering.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="kube-monkey" data-type="sect2"><div class="sect2" id="idm45979386941296">&#13;
<h2>kube-monkey</h2>&#13;
&#13;
<p><a data-primary="kube-monkey" data-type="indexterm" id="idm45979386939920"/><a href="https://oreil.ly/HGmrb"><em>kube-monkey</em></a> runs at a preset time (by default, 8 a.m. on weekdays), and builds a schedule of Deployments that will be targeted during the rest of the day (by default, 10 a.m. to 4 p.m.). Unlike some other tools, kube-monkey works on an opt-in basis: only those Pods that specifically enable kube-monkey using annotations will be targeted.</p>&#13;
&#13;
<p>This means that you can add kube-monkey testing to specific apps or services during their development, and set different levels of frequency and aggression depending on the service. For example, the following annotation on a Pod will set a mean time between failures (MTBF) of two days:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kube-monkey/mtbf</code><code class="p">:</code><code class="w"> </code><em><code class="l-Scalar-Plain">2</code></em></pre>&#13;
&#13;
<p>The <code>kill-mode</code> annotation lets you specify how many of a Deployment’s Pods will be killed, or a maximum percentage. The following annotations will kill up to 50% of the Pods in the targeted Deployment:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kube-monkey/kill-mode</code><code class="p">:</code><code class="w"> </code><code class="s">"random-max-percent"</code><code class="w"/>&#13;
<code class="nt">kube-monkey/kill-value</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">50</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PowerfulSeal" data-type="sect2"><div class="sect2" id="idm45979386940960">&#13;
<h2>PowerfulSeal</h2>&#13;
&#13;
<p><a data-primary="PowerfulSeal" data-type="indexterm" id="idm45979386927792"/><a href="https://oreil.ly/Wwe31"><em>PowerfulSeal</em></a> is an open source Kubernetes chaos engineering tool that works in two modes: interactive and autonomous. Interactive mode lets you explore your cluster and manually break things to see what happens. It can terminate nodes, namespaces, Deployments, and individual Pods.</p>&#13;
&#13;
<p>Autonomous mode uses a set of policies specified by you: which resources to operate on, which to avoid, when to run (you can configure it to only operate during working hours Monday–Friday, for example), and how aggressive to be (kill a given percentage of all matching Deployments, for example). PowerfulSeal’s policy files are very flexible and let you set up almost any imaginable chaos engineering scenario.</p>&#13;
<div data-type="tip"><h1>Best Practice</h1>&#13;
<p>If your applications require high availability, run a chaos testing tool such as chaoskube regularly to make sure that unexpected node or Pod failures don’t cause problems. Make sure you clear this first with the people responsible for operating the cluster and the applications under test.<a data-startref="ix_06-operating-adoc8" data-type="indexterm" id="idm45979386891152"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45979386890048">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>It can be really difficult to know how to size and configure your first Kubernetes clusters. There are a lot of choices you can make, and you don’t really know what you’ll need until you’ve actually gained some production experience.</p>&#13;
&#13;
<p>We can’t make those decisions for you, but we hope we’ve at least given you some helpful things to think about when making them:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Before provisioning your production Kubernetes cluster, think about how many nodes you’ll need, and of what size.</p>&#13;
</li>&#13;
<li>&#13;
<p>You need at least three control plane nodes (unless you’re using a managed service) and at least two (ideally three) worker nodes. This can make Kubernetes clusters seem a little expensive at first when you’re only running a few small workloads, but don’t forget the advantages of built-in resilience and scaling.</p>&#13;
</li>&#13;
<li>&#13;
<p>Kubernetes clusters can scale to many thousands of nodes and hundreds of thousands of containers.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you need to scale beyond that, use multiple clusters (sometimes you need to do this for security or compliance reasons too). You can join clusters together using federation if you need to replicate workloads across clusters.</p>&#13;
</li>&#13;
<li>&#13;
<p>Kubernetes isn’t just for the cloud; it runs on bare-metal servers too. If you’ve got metal sitting around, why not use it?</p>&#13;
</li>&#13;
<li>&#13;
<p>You can scale your cluster up and down manually without too much trouble, and you probably won’t have to do it very often. Autoscaling is nice to have when your workloads grow and shrink.</p>&#13;
</li>&#13;
<li>&#13;
<p>There’s a well-defined standard for Kubernetes vendors and products: the CNCF Certified Kubernetes mark. If you don’t see this, ask why not.</p>&#13;
</li>&#13;
<li>&#13;
<p>Chaos testing is a process of knocking out Pods at random and seeing if your application still works. It’s useful, but the cloud also has a way of doing its own chaos testing anyway, without you asking for it.<a data-startref="ix_06-operating-adoc0" data-type="indexterm" id="idm45979386879888"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>