<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 22. Monitoring MongoDB"><div class="chapter" id="chapter-mms"><h1><span class="label">Chapter 22. </span>Monitoring MongoDB</h1><p>Before<a data-type="indexterm" data-primary="administration, of servers" data-secondary="monitoring" id="ASmonitor22"/><a data-type="indexterm" data-primary="server administration" data-secondary="monitoring" id="SAmonitor22"/> you deploy, it is important to set up some type of
  monitoring. Monitoring should allow you to track what your server is doing
  and alert you if something goes wrong. This chapter will cover:</p><ul><li><p>How to track MongoDB’s memory usage</p></li><li><p>How to track application performance metrics</p></li><li><p>How to diagnose replication issues</p></li></ul><p>We’ll<a data-type="indexterm" data-primary="monitoring" data-secondary="services available" id="idm45882334151736"/> use example graphs from MongoDB Ops Manager to demonstrate
  what to look for when monitoring (see <a href="https://oreil.ly/D4751">installation
  instructions for Ops Manager</a>). The monitoring capabilities of MongoDB Atlas (MongoDB’s cloud database service) are very similar. MongoDB also offers a
  free monitoring service that monitors standalones and replica sets. It keeps
  the monitoring data for 24 hours after it has been uploaded and provides
  coarse-grained statistics on operation execution times, memory usage, CPU
  usage, and operation counts.</p><p>If you do not want to use Ops Manager, Atlas, or MongoDB’s free
  monitoring service, please use some type of monitoring. It will help you
  detect potential issues before they cause problems and diagnose issues when
  they occur.</p><section data-type="sect1" data-pdf-bookmark="Monitoring Memory Usage"><div class="sect1" id="idm45882334148824"><h1>Monitoring Memory Usage</h1><p>Accessing<a data-type="indexterm" data-primary="monitoring" data-secondary="memory usage" id="Mmusage22"/> data in memory is fast, and accessing data on disk is slow.
    Unfortunately, memory is expensive (and disk is cheap), and typically
    MongoDB uses up memory before any other resource. This section covers how
    to monitor MongoDB’s interactions with the CPU, disk, and memory, and what
    to watch for.</p><section data-type="sect2" data-pdf-bookmark="Introduction to Computer Memory"><div class="sect2" id="idm45882334146024"><h2>Introduction to Computer Memory</h2><p>Computers<a data-type="indexterm" data-primary="memory" data-secondary="computer memory basics" id="idm45882334145080"/> tend to have a small amount of fast-to-access memory and
      a large amount of slow-to-access disk. When you request a page of data
      that is stored on disk (and not yet in memory), your system
      <span class="firstterm">page faults</span> and copies the page from disk into
      memory. It can then access the page in memory extremely quickly. If your
      program stops regularly using the page and your memory fills up with
      other pages, the old page will be <span class="firstterm">evicted</span> from
      memory and only live on disk again.</p><p>Copying a page from disk into memory takes a lot longer than
      reading a page from memory. Thus, the less MongoDB has to copy data from
      disk, the better. If MongoDB can operate almost entirely in memory, it
      will be able to access data much faster. Thus, MongoDB’s memory usage is
      one of the most important stats to track.</p></div></section><section data-type="sect2" data-pdf-bookmark="Tracking Memory Usage"><div class="sect2" id="idm45882334141288"><h2>Tracking Memory Usage</h2><p>MongoDB<a data-type="indexterm" data-primary="memory" data-secondary="tracking memory usage" id="idm45882334140168"/> reports on three “types” of memory in Ops Manager:
      <span class="firstterm">resident memory<a data-type="indexterm" data-primary="resident memory" id="idm45882334138344"/></span>, <span class="firstterm">virtual memory<a data-type="indexterm" data-primary="virtual memory" id="idm45882334081768"/></span>, and <span class="firstterm">mapped memory<a data-type="indexterm" data-primary="mapped memory" id="idm45882334080216"/></span>. Resident memory is the memory that MongoDB
      explicitly owns in RAM. For example, if you query for a document and it
      is paged into memory, that page is added to MongoDB’s resident
      memory.</p><p>MongoDB is given an address for that page. This address isn’t the
      literal address of the page in RAM; it’s a <span class="firstterm">virtual
      address</span>. MongoDB can pass it to the kernel and the kernel
      will look up where the page really lives. This way, if the kernel needs
      to evict the page from memory, MongoDB can still use the address to
      access it. MongoDB will request the memory from the kernel, the kernel
      will look at its page cache, see that the page is not there, page fault
      to copy the page into memory, and return it to MongoDB.</p><p>If your data fits entirely in memory, the resident memory should
      be approximately the size of your data. When we talk about data being
      “in memory,” we’re always talking about the data being in RAM.</p><p>MongoDB’s mapped memory includes all of the data MongoDB has ever
      accessed (all the pages of data it has addresses for). It will usually
      be about the size of your dataset.</p><p>Virtual memory is an abstraction provided by the operating system
      that hides the physical storage details from the software process. Each
      process sees a contiguous address space of memory that it can use. In
      Ops Manager, the virtual memory use of MongoDB is typically twice the
      size of the mapped memory.</p><p><a data-type="xref" href="#monitor-mem">Figure 22-1</a> shows the Ops Manager graph for
      memory information, which describes how much virtual, resident, and
      mapped memory MongoDB is using. Mapped memory is relevant only for older
      (pre-4.0) deployments using the MMAP storage engine. Now that MongoDB
      uses the WiredTiger storage engine, you should see zero usage for mapped
      memory. On a machine dedicated to MongoDB, resident memory should be a
      little less than the total memory size (assuming your working set is as
      large or larger than memory). Resident memory is the statistic that
      actually tracks how much data is in physical RAM, but by itself this
      does not tell you much about how <span class="keep-together">MongoDB</span> is using memory.</p><figure style="float: 0"><div id="monitor-mem" class="figure"><img src="Images/mdb3_2201.png" width="398" height="234"/><h6><span class="label">Figure 22-1. </span>From the top line to the bottom: virtual, resident, and mapped
        memory</h6></div></figure><p>If your data fits entirely in memory, resident should be
      approximately the size of your data. When we talk about data being “in
      memory,” we’re always talking about the data being in RAM.</p><p>As you can see from <a data-type="xref" href="#monitor-mem">Figure 22-1</a>, memory metrics
      tend to be fairly steady, but as your dataset grows virtual memory (top
      line) will grow with it. Resident memory (middle line) will grow to the
      size of your available RAM and then hold steady.</p></div></section><section data-type="sect2" data-pdf-bookmark="Tracking Page Faults"><div class="sect2" id="idm45882334141160"><h2>Tracking Page Faults</h2><p>You<a data-type="indexterm" data-primary="page faults, tracking" id="idm45882334069128"/><a data-type="indexterm" data-primary="memory" data-secondary="tracking page faults" id="idm45882334068296"/> can use other statistics to find out how MongoDB is using
      memory, not just how much of each type it has. One useful stat is the
      number of page faults, which tells you how often the data MongoDB is
      looking for is not in RAM. Figures <a data-type="xref" data-xrefstyle="select: labelnumber" href="#monitor-fault-1">22-2</a> and <a data-type="xref" data-xrefstyle="select: labelnumber" href="#monitor-fault-2">22-3</a> are graphs that show page faults over time.
      <a data-type="xref" href="#monitor-fault-2">Figure 22-3</a> is page faulting less than <a data-type="xref" href="#monitor-fault-1">Figure 22-2</a>, but by itself this information is not very
      useful. If the disk in <a data-type="xref" href="#monitor-fault-1">Figure 22-2</a> can handle
      that many faults and the application can handle the delay of the disk
      seeks, there is no particular problem with having so many faults (or
      more). On the other hand, if your application cannot handle the
      increased latency<a data-type="indexterm" data-primary="latency" data-secondary="increased by reading from disk" id="idm45882334061048"/> of reading data from disk, you have no choice but to
      store all of your data in memory (or use SSDs).</p><figure style="float: 0"><div id="monitor-fault-1" class="figure"><img src="Images/mdb3_2202.png" width="403" height="238"/><h6><span class="label">Figure 22-2. </span>A system that is page faulting hundreds of times a
        minute</h6></div></figure><figure style="float: 0"><div id="monitor-fault-2" class="figure"><img src="Images/mdb3_2203.png" width="398" height="236"/><h6><span class="label">Figure 22-3. </span>A system that is page faulting a few times a minute</h6></div></figure><p>Regardless of how forgiving the application is, page faults become
      a problem when the disk is overloaded. The amount of load a disk can
      handle isn’t linear: once a disk begins getting overloaded, each
      operation must queue for a longer and longer period of time, creating a
      chain reaction. There is usually a tipping point where disk performance
      begins degrading quickly. Thus, it is a good idea to stay away from the
      maximum load that your disk can handle.</p><div data-type="note" epub:type="note"><h6>Note</h6><p>Track your page fault numbers over time. If your application is
        behaving well with a certain number of page faults, you have a
        baseline for how many page faults the system can handle. If page
        faults begin to creep up and performance deteriorates, you have a
        threshold to alert on.</p></div><p>You can see page fault stats per database by looking at the
      <code>"page_faults"</code> field of <span class="keep-together"><code>serverStatus</code></span>’s output:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">db</code><code class="p">.</code><code class="nx">adminCommand</code><code class="p">({</code><code class="s2">"serverStatus"</code><code class="o">:</code> <code class="mi">1</code><code class="p">})[</code><code class="s2">"extra_info"</code><code class="p">]</code>
<code class="p">{</code> <code class="s2">"note"</code> <code class="o">:</code> <code class="s2">"fields vary by platform"</code><code class="p">,</code> <code class="s2">"page_faults"</code> <code class="o">:</code> <code class="mi">50</code> <code class="p">}</code></pre><p><code>"page_faults"</code> gives you a
      count of how many times MongoDB has had to go to disk (since
      startup).</p></div></section><section data-type="sect2" data-pdf-bookmark="I/O Wait"><div class="sect2" id="idm45882334038792"><h2>I/O Wait</h2><p>Page<a data-type="indexterm" data-primary="I/O wait" id="idm45882334037912"/><a data-type="indexterm" data-primary="memory" data-secondary="I/O wait" id="idm45882334037112"/> faults in general are closely tied to how long the CPU is
      idling waiting for the disk, called <span class="firstterm"><code>I/O wait</code></span>. Some I/O wait is normal;
      MongoDB has to go to disk sometimes, and although it tries not to block
      anything when it does, it cannot completely avoid it. The important
      thing is that I/O wait is not increasing or near 100%, as shown in <a data-type="xref" href="#figure21-5">Figure 22-4</a>. This indicates that the disk is
      getting<a data-type="indexterm" data-startref="Mmusage22" id="idm45882334032056"/>
      overloaded.</p><figure style="float: 0"><div id="figure21-5" class="figure"><img src="Images/mdb3_2204.png" width="404" height="236"/><h6><span class="label">Figure 22-4. </span>I/O wait hovering around 100%</h6></div></figure></div></section></div></section><section data-type="sect1" data-pdf-bookmark="Calculating the Working Set"><div class="sect1" id="working-set"><h1>Calculating the Working Set</h1><p>In<a data-type="indexterm" data-primary="working set" data-secondary="calculating size of" id="idm45882334028584"/><a data-type="indexterm" data-primary="monitoring" data-secondary="calculating the working set" id="idm45882334027448"/> general, the more data you have in memory, the faster
    MongoDB will perform. Thus, in order from fastest to slowest, an
    application could have:</p><ol><li><p>The entire dataset in memory. This is nice to have but is often
        too expensive or infeasible. It may be necessary for applications that
        depend on fast response times.</p></li><li><p>The <span class="firstterm">working set</span> in memory. This is the
        most common choice.</p><p>Your working set is the data and indexes that your application
        uses. This may be everything, but generally there’s a core dataset
        (e.g., the <em>users</em> collection and the last month of
        activity) that covers 90% of requests. If this working set fits in
        RAM, MongoDB will generally be fast: it only has to go to disk for a
        few “unusual” requests.</p></li><li><p>The indexes in memory.</p></li><li><p>The working set of indexes in memory.</p></li><li><p>No useful subset of data in memory. If possible, avoid this. It
        will be slow.</p></li></ol><p>You must know what your working set is (and how large it is) to know
    if you can keep it in memory. The best way to calculate the size of the
    working set is to track common operations to find out how much your
    application is reading and writing. For example, suppose your application
    creates 2 GB of new data per week and 800 MB of that data is regularly
    accessed. Users tend to access data up to a month old, and data that’s
    older than that is mostly unused. Your working set size is probably about
    3.2 GB (800 MB/week × 4 weeks), plus a fudge factor for indexes, so call
    it 5 GB.</p><p>One way to think about this is to track data accessed over time, as
    shown in <a data-type="xref" href="#monitor-calc">Figure 22-5</a>. If you choose a cutoff where 90%
    of your requests fall, like in <a data-type="xref" href="#monitor-calc2">Figure 22-6</a>, then the
    data (and indexes) generated in that period of time form your working set.
    You can measure for that amount of time to figure out how much your
    dataset grows. Note that this example uses time, but it’s possible that
    there’s another access pattern that makes more sense for your application
    (time being the most common one).</p><figure style="float: 0"><div id="monitor-calc" class="figure"><img src="Images/mdb3_2205.png" width="778" height="522"/><h6><span class="label">Figure 22-5. </span>A plot of data accesses by age of data</h6></div></figure><figure style="float: 0"><div id="monitor-calc2" class="figure"><img src="Images/mdb3_2206.png" width="778" height="522"/><h6><span class="label">Figure 22-6. </span>The working set is data used in the requests before the cutoff of
      “frequent requests” (indicated by the vertical line in the
      graph)</h6></div></figure><section data-type="sect2" data-pdf-bookmark="Some Working Set Examples"><div class="sect2" id="idm45882334015672"><h2>Some Working Set Examples</h2><p>Suppose<a data-type="indexterm" data-primary="working set" data-secondary="examples" id="idm45882334014712"/> that you have a 40 GB working set. A total of 90% of
      requests hit the working set, and 10% hit other data. If you have 500 GB
      of data and 50 GB of RAM, your working set fits entirely in RAM. Once
      your application has accessed the data it usually accesses (a process
      called <span class="firstterm"><em>preheating</em></span>), it
      should never have to go to disk again for the working set. It then has
      10 GB of space available for the 460 GB of less-frequently-accessed
      data. Obviously, MongoDB will almost always have to go to disk for the
      nonworking set data.</p><p>On the other hand, suppose your working set does not fit in
      RAM—say, if you have only 35 GB of RAM. Then the working set will
      generally take up most of the RAM. The working set has a higher
      probability of staying in RAM because it’s accessed more frequently, but
      at some point the less-frequently-accessed data will have to be paged
      in, evicting the working set (or other less-frequently-accessed data).
      Thus, there is a constant churn back and forth from disk: accessing the
      working set does not have predictable performance <span class="keep-together">anymore</span>.</p></div></section></div></section><section data-type="sect1" data-pdf-bookmark="Tracking Performance"><div class="sect1" id="idm45882333980888"><h1>Tracking Performance</h1><p>Performance<a data-type="indexterm" data-primary="performance, tracking" data-seealso="monitoring" id="idm45882333980008"/><a data-type="indexterm" data-primary="monitoring" data-secondary="tracking performance" id="idm45882333978968"/> of queries is often important to track and keep consistent.
    There are several ways to track if MongoDB is having trouble with the
    current request load.</p><p>CPU<a data-type="indexterm" data-primary="I/O wait" id="idm45882333977304"/> can be I/O bound with MongoDB (indicated by a high I/O
    wait). The WiredTiger storage engine is multithreaded and can take
    advantage of additional CPU cores. This can be seen in a higher level of
    usage across CPU metrics when compared with the older MMAP storage engine.
    However, if user or system time is approaching 100% (or 100% multiplied by
    the number of CPUs you have), the most common cause is that you’re missing
    an index on a frequently used query. It is a good idea to track CPU usage
    (particularly after deploying a new version of your application) to ensure
    that all your queries are behaving as they should.</p><p>Note that the graph shown in <a data-type="xref" href="#monitor-cpu">Figure 22-7</a> is fine:
    if there is a low number<a data-type="indexterm" data-primary="page faults, tracking" id="idm45882333974376"/> of page faults, I/O wait may be dwarfed by other CPU
    activities. It is only when the other activities creep up that bad indexes
    may be a culprit.</p><figure style="float: 0"><div id="monitor-cpu" class="figure"><img src="Images/mdb3_2207.png" width="398" height="236"/><h6><span class="label">Figure 22-7. </span>A CPU with minimal I/O wait: the top line is user and the lower
      line is system; the other stats are very close to 0%</h6></div></figure><p>A<a data-type="indexterm" data-primary="queueing" id="idm45882333971480"/> similar metric is queuing: how many requests are waiting to
    be processed by MongoDB. A request is considered queued when it is waiting
    for the lock it needs to do a read or a write. <a data-type="xref" href="#monitor-queue">Figure 22-8</a> shows a graph of read and write queues over
    time. No queues are preferred (basically an empty graph), but this graph
    is nothing to be alarmed about. In a busy system, it isn’t unusual for an
    operation to have to wait a bit for the correct lock to be
    available.</p><figure style="float: 0"><div id="monitor-queue" class="figure"><img src="Images/mdb3_2208.png" width="400" height="233"/><h6><span class="label">Figure 22-8. </span>Read and write queues over time</h6></div></figure><p>The WiredTiger storage engine provides document-level concurrency,
    which allows for multiple simultaneous writes to the same collection. This
    has drastically improved the performance of concurrent operations. The
    ticketing system used controls the number of threads in use to avoid
    starvation: it issues tickets for read and write operations (128 of each,
    by default), after which point new read or write operations <span class="keep-together">will queue</span>.
    The <code>wiredTiger.concurrentTransactions.read.available</code>
    and <code>wire⁠d​Tiger.concurrentTransactions.write.available</code>
    fields of <code>serverStatus</code> can be used to
    track when the number of available tickets reaches zero, indicating the
    respective operations are now queuing up.</p><p>You can see if requests are piling up by looking at the number of
    requests enqueued. Generally, the queue size should be low. A large and
    ever-present queue is an indication that <em class="filename">mongod</em> cannot keep up with its load. You should
    decrease the load on that server as fast as possible.</p></div></section><section data-type="sect1" data-pdf-bookmark="Tracking Free Space"><div class="sect1" id="idm45882333963464"><h1>Tracking Free Space</h1><p>One<a data-type="indexterm" data-primary="disk usage, tracking" id="idm45882333962520"/><a data-type="indexterm" data-primary="space, tracking disk usage" id="idm45882333961656"/><a data-type="indexterm" data-primary="monitoring" data-secondary="tracking free space" id="idm45882333960760"/><a data-type="indexterm" data-primary="free space, tracking" id="idm45882333959656"/> other metric that is basic but important to monitor is disk
    usage. Sometimes users wait until their disk runs out of space before they
    think about how they want to handle it. By monitoring your disk usage and
    tracking free disk space, you can predict how long your current drive will
    be sufficient and plan in advance what to do when it is not.</p><p>As you run out of space, there are several options:</p><ul><li><p>If you are using sharding, add another shard.</p></li><li><p>If you have unused indexes, remove them. These can be identified
        using the<a data-type="indexterm" data-primary="$indexStats operator" id="idm45882333956968"/> aggregation <code>$indexStats</code> for a specific
        collection.</p></li><li><p>If you have not run a compaction operation, then do so on a
        secondary to see if it assists. This is normally only useful in cases
        where a large amount of data or indexes have been removed from a
        collection and will not be replaced.</p></li><li><p>Shut down each member of the replica set (one at a time) and
        copy its data to a larger disk, which can then be mounted. Restart the
        member and proceed to the next.</p></li><li><p>Replace members of your replica set with members with a larger
        drive: remove an old member and add a new member, and allow that one
        to catch up with the rest of the set. Repeat for each member of the
        set.</p></li><li><p>If<a data-type="indexterm" data-primary="--directoryperdb" data-primary-sortas="directoryperdb" id="idm45882333953320"/> you are using the <code class="option">directoryperdb</code>
        option and you have a particularly fast-growing database, move it to
        its own drive. Then mount the volume as a directory in your data
        directory. This way the rest of your data doesn’t have to be
        moved.</p></li></ul><p>Regardless of the technique you choose, plan ahead to minimize the
    impact on your application. You need time to take backups, modify each
    member of your set in turn, and copy your data from place to place.</p></div></section><section data-type="sect1" data-pdf-bookmark="Monitoring Replication"><div class="sect1" id="idm45882333950600"><h1>Monitoring Replication</h1><p>Replication<a data-type="indexterm" data-primary="replication" data-secondary="monitoring" data-tertiary="lag and oplog length" id="idm45882333949480"/><a data-type="indexterm" data-primary="monitoring" data-secondary="replication" id="idm45882333948136"/> lag and oplog length are important metrics to track.
    <span class="firstterm">Lag</span> is when the secondaries cannot keep up with the
    primary. It’s calculated by subtracting the time of the last op applied on
    a secondary from the time of the last op on the primary. For example, if a
    secondary just applied an op with the timestamp 3:26:00 p.m. and the
    primary just applied an op with the timestamp 3:29:45 p.m., the secondary
    is lagging by 3 minutes and 45 seconds. You want lag to be as close to 0
    as possible, and it is generally on the order of milliseconds. If a
    secondary is keeping up with the primary, the replication lag should look
    something like the graph shown in <a data-type="xref" href="#monitor-no-lag">Figure 22-9</a>:
    basically 0 all the time.</p><figure style="float: 0"><div id="monitor-no-lag" class="figure"><img src="Images/mdb3_2209.png" width="405" height="238"/><h6><span class="label">Figure 22-9. </span>A replica set with no lag; this is what you want to see</h6></div></figure><p>If a secondary cannot replicate writes as fast as the primary can
    write, you’ll start seeing a nonzero lag. The most extreme case of this is
    when replication is <span class="firstterm">stuck</span>: the secondary cannot
    apply any more operations for some reason. At this point, lag will grow by
    one second per second, creating the steep slope shown in <a data-type="xref" href="#monitor-repl-stuck">Figure 22-10</a>. This could be caused by network issues or
    a missing <code>"_id"</code> index, which is
    required on every collection for replication to function properly.</p><div data-type="tip"><h6>Tip</h6><p>If a collection is missing an <code>"_id"</code> index, take the server out of the
      replica set, start it as a standalone server, and build the <code>"_id"</code> index. Make sure you create the <code>"_id"</code> index as a <em>unique</em>
      index. Once created, the <code>"_id"</code> index
      cannot be dropped or changed (other than by dropping the whole
      collection).</p></div><p>If a system is overloaded, a secondary may gradually fall behind.
    Some replication will still be happening, so you generally won’t see the
    characteristic “one second per second” slope in the graph. Still, it’s
    important to be aware if the secondaries cannot keep up with peak traffic
    or are gradually falling further behind.</p><figure style="float: 0"><div id="monitor-repl-stuck" class="figure"><img src="Images/mdb3_2210.png" width="416" height="243"/><h6><span class="label">Figure 22-10. </span>Replication getting stuck and, just before February 10, beginning
      to recover; the vertical lines are server restarts</h6></div></figure><p>Primaries do not throttle writes to “help” secondaries catch up, so
    it’s common for secondaries to fall behind on overloaded systems
    (particularly as MongoDB tends to prioritize writes over reads, which
    means replication can be starved on the primary). You can force throttling
    of the primary to some extent by using <code>"w"</code> with your write concern. You also might want
    to try removing load from the secondary by routing any requests it was
    handling to another member.</p><p>If you are on an extremely <em>underloaded</em> system,
    you may see another interesting pattern: sudden spikes in replication lag,
    as shown in <a data-type="xref" href="#monitor-low-write">Figure 22-11</a>. The spikes shown are not
    actually lag—they are caused by variations in sampling. The <em class="filename">mongod</em> is processing one write every couple of
    minutes. Because lag is measured as the difference between timestamps on
    the primary and secondary, measuring the timestamp of the secondary right
    before a write on the primary makes it look minutes behind. If you
    increase the write rate, these spikes should disappear.</p><figure style="float: 0"><div id="monitor-low-write" class="figure"><img src="Images/mdb3_2211.png" width="412" height="244"/><h6><span class="label">Figure 22-11. </span>A low-write system can cause “phantom” lag</h6></div></figure><p>The other important replication metric to track is the length of
    each member’s oplog. Every member that might become primary should have an
    oplog longer than a day. If a member may be a sync source for another
    member, it should have an oplog longer than the time an initial sync takes
    to complete. <a data-type="xref" href="#monitor-repl-1">Figure 22-12</a> shows what a standard
    oplog-length graph looks like. This oplog has an excellent length: 1,111
    hours is over a month of data! In general, oplogs should be as long as you
    can afford the disk space to make them. Given the way they’re used, they
    take up basically no memory, and a long oplog can mean the difference
    between a painful ops experience and an easy one.</p><figure style="float: 0"><div id="monitor-repl-1" class="figure"><img src="Images/mdb3_2212.png" width="405" height="245"/><h6><span class="label">Figure 22-12. </span>A typical oplog-length graph</h6></div></figure><p><a data-type="xref" href="#monitor-repl-2">Figure 22-13</a> shows a slightly unusual variation
    caused by a fairly short oplog and variable traffic. This is still
    healthy, but the oplog on this machine is probably too short (between 6
    and 11 hours of maintenance). The administrator may want to make the oplog
    longer when they get a<a data-type="indexterm" data-startref="SAmonitor22" id="idm45882333924984"/><a data-type="indexterm" data-startref="ASmonitor22" id="idm45882333924152"/> chance.</p><figure style="float: 0"><div id="monitor-repl-2" class="figure"><img src="Images/mdb3_2213.png" width="433" height="257"/><h6><span class="label">Figure 22-13. </span>Oplog-length graph of an application with daily traffic
      peaks</h6></div></figure></div></section></div></section></div>



  </body></html>