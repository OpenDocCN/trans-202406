<html><head></head><body><section data-pdf-bookmark="Chapter 7. The Kubernetes Native Database" data-type="chapter" epub:type="chapter"><div class="chapter" id="the_kubernetes_native_database">&#13;
<h1><span class="label">Chapter 7. </span>The Kubernetes Native Database</h1>&#13;
<p><a contenteditable="false" data-primary="Kubernetes native database" data-secondary="about" data-type="indexterm" id="idm46183196409536"/>The software industry is flush with terms that define major trends in a single word or short phrase. You can see one of them in the title of this book: <em>cloud native</em>. Another example is <em>microservice</em>, a major architectural paradigm that touches much of the technology we’re discussing here. More recently, terms like <em>Kubernetes native</em> and <em>serverless</em> have emerged.</p>&#13;
<p>While succinct and catchy, distilling a complex topic or trend down to a single sound bite leaves room for ambiguity, or at least for reasonable questions such as “What does this <em>actually</em> mean?” To further muddy the waters, terms such as these are frequently used in the context of marketing products as a way to gain leverage or <span class="keep-together">differentiate</span> against other competitive offerings. Whether the content you’re consuming makes an overt statement or it’s just the subtext, you may have wondered whether a given technology must be better to run on Kubernetes than other offerings because it’s labeled <em>Kubernetes native</em>.</p>&#13;
<p>Of course, for these terms to be useful to us in evaluating and picking the right technologies for our applications, the real task is to unpack what they really mean, as we did with the term <em>cloud native data</em> in <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>. In this chapter, we’ll look at what it means for data technology to be Kubernetes native and see if we can arrive at a definition that we can agree on. To do this, we’ll examine a couple of projects that claim these terms and derive the common principles: TiDB and Astra DB. Are you ready? Let’s dive in!</p>&#13;
<section data-pdf-bookmark="Why a Kubernetes Native Approach Is Needed" data-type="sect1"><div class="sect1" id="why_a_kubernetes_native_approach_is_nee">&#13;
<h1>Why a Kubernetes Native Approach Is Needed</h1>&#13;
<p><a contenteditable="false" data-primary="Kubernetes native database" data-secondary="approaches for" data-type="indexterm" id="idm46183196398976"/>First, let’s discuss why the idea of a Kubernetes native database came up in the first place. Up to this point in the book, we’ve focused on deployment of existing databases on Kubernetes including MySQL and Cassandra. These are mature <span class="keep-together">databases</span> that were around before Kubernetes existed and have proven themselves over time. They have large install bases and user communities, and because of this <span class="keep-together">investment,</span> you can see why there’s a large incentive to run these databases in Kubernetes <span class="keep-together">environments,</span> and why there has been such interest in creating operators to automate them.</p>&#13;
<p>At the same time, you’ve probably noticed some of the awkwardness in adapting these databases to run on Kubernetes. While it is pretty straightforward to point a database to Kubernetes-based storage just by changing a mount path, tighter integration with Kubernetes to manage databases that consist of multiple nodes can be a bit more involved. This can range from relatively simple tasks like deploying a legacy management UI in a Pod and exposing access to the HTTP port, to the more complex deployment of sidecars that we saw in <a data-type="xref" href="ch06.html#integrating_data_infrastructure_in_a_ku">Chapter 6</a> to provide APIs for management and metrics collection.</p>&#13;
<p>The recognition of this complexity has led some innovators to develop new databases that are designed to be Kubernetes native from day one. It’s a well-known axiom <span class="keep-together">in the database industry</span> that it takes 5–10 years for a new database engine to reach a point of maturity. Because of this, these Kubernetes native databases tend not to be completely new implementations, but rather refactoring of existing databases into microservices that can be scaled independently, while maintaining compatibility with existing APIs that developers are accustomed to. Thus, the trend of decomposing the monolith has arrived at the data tier. The emerging generation of databases will be based on new architectures to truly leverage the benefits of Kubernetes.</p>&#13;
<p><a contenteditable="false" data-primary="data" data-secondary="principles of" data-type="indexterm" id="idm46183196749264"/>To help us assess what might qualify these new databases as Kubernetes native, let’s use the cloud native data principles introduced in <a data-type="xref" href="ch01.html#principles_of_cloud_native_data_infrast">“Principles of Cloud Native Data Infrastructure”</a> as a guide to formulate some questions to ask how a database interacts with Kubernetes:</p>&#13;
<dl>&#13;
<dt>Principle 1: Leverage compute, network, and storage as commodity APIs</dt>&#13;
<dd>How does the database use Kubernetes compute resources (Pods, Deployments, StatefulSets), network resources (Services and Ingress), and storage resources (PersistentVolumes, PersistentVolumeClaims, StorageClasses)?</dd>&#13;
<dt>Principle 2: Separate the control and data planes</dt>&#13;
<dd>Is the database deployed and managed by an operator? What custom resources does it define? Are other workloads in the control plane besides the operator?</dd>&#13;
<dt>Principle 3: Make observability easy</dt>&#13;
<dd>How do the various services in the architecture expose metrics and logs to support collection by the Kubernetes control plane and third-party extensions?</dd>&#13;
<dt>Principle 4: Make the default configuration secure</dt>&#13;
<dd>Do the database and associated operator use  Kubernetes Secrets to share credentials, and use Roles and RoleBindings to manage access by Role? Do services minimize the number of exposed points and require secure access to them?</dd>&#13;
<dt>Principle 5: Prefer declarative configuration</dt>&#13;
<dd>Extending Principle 2, can the database be managed entirely by creating, updating, or deleting Helm charts and Kubernetes resources (whether built-in or custom resources), or are other tools required?</dd>&#13;
</dl>&#13;
<p>In the sections that follow, we’ll explore the answers to these questions for two databases and see what else we can learn about what it means to be Kubernetes native. That will help us to build a checklist at the end of this chapter that will help solidify our definition. (See <a data-type="xref" href="#what_to_look_for_in_a_kubernetes_native">“What to Look for in a Kubernetes Native Database”</a> for what we come up with.)</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Hybrid Data Access at Scale with TiDB" data-type="sect1"><div class="sect1" id="hybrid_data_access_at_scale_with_tidb">&#13;
<h1>Hybrid Data Access at Scale with TiDB</h1>&#13;
<p><a contenteditable="false" data-primary="TiDB" data-secondary="about" data-type="indexterm" id="idm46183196546384"/><a contenteditable="false" data-primary="hybrid data access, at scale with TiDB" data-type="indexterm" id="hds_tidb"/><a contenteditable="false" data-primary="Kubernetes native database" data-secondary="hybrid data access at scale with TiDB" data-type="indexterm" id="knd_hds"/><a contenteditable="false" data-primary="TiDB" data-secondary="hybrid data access at scale with" data-type="indexterm" id="tidb_hds"/><a contenteditable="false" data-primary="SQL (Standard Query Language)" data-type="indexterm" id="idm46183196342128"/><a contenteditable="false" data-primary="Codd, Edgar" data-type="indexterm" id="idm46183196341152"/>The databases that have received most of our focus in this book so far represent two major trends in database architecture that trace their lineage back for decades or more. MySQL is a relational database that provides its own flavor of the Standard Query Language (SQL), based on rules developed by Edgar Codd in the 1970s.</p>&#13;
<p>In the early 2000s, companies building web-scale applications began to push the limits of what could be accomplished with the relational databases of the day. As database sizes began growing beyond what could feasibly be managed on a single instance, techniques like sharding were used to scale across multiple instances. These were frequently expensive, difficult to operate, and not always reliable.</p>&#13;
<p><a contenteditable="false" data-primary="Brewer, Eric" data-type="indexterm" id="idm46183196358016"/><a contenteditable="false" data-primary="CAP theorem" data-type="indexterm" id="idm46183196364720"/>In response to this need, Cassandra and other so-called <em>NoSQL</em> databases emerged <span class="keep-together">in a period</span> of intense innovation and experimentation. These databases provide linear scalability through adding additional nodes. They offer different data models, or ways of representing data: for example, key-value stores such as Redis, document databases such as MongoDB, graph databases such as Neo4j, and others. NoSQL databases tended to provide weaker consistency guarantees and omit support for more complex behaviors like transactions and joins to achieve high performance and availability at scale, a trade-off documented by Eric Brewer in his <a href="https://oreil.ly/aJq6M">CAP theorem</a>.</p>&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="Calvin paper" data-type="indexterm" id="idm46183197075184"/><a contenteditable="false" data-primary="Google Spanner" data-type="indexterm" id="idm46183196362848"/><a contenteditable="false" data-primary="Spanner paper" data-type="indexterm" id="idm46183197073008"/>Because of the continued developer demand for traditional relational semantics such as strong consistency, transactions, and joins, multiple teams began to revive the idea of supporting these capabilities in distributed databases starting around 2012. These so-called <em>NewSQL</em> databases were based on more efficient and performant consensus algorithms. Two key papers helped drive the emergence of the NewSQL movement. First, the <a href="https://oreil.ly/HLw2M">Calvin paper</a> introduced a global consensus <span class="keep-together">protocol</span> which represented a more reliable and performant approach for guaranteeing strong consistency, later adopted by FaunaDB and other databases. Second, Google’s <a href="https://oreil.ly/zDl5z">Spanner paper</a> introduced a design for a distributed relational database using sharding and a new consensus algorithm that leveraged the improved ability of cloud infrastructure to provide time synchronization across datacenters. Besides Google Spanner, this approach was implemented by databases including CockroachDB and YugabyteDB.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>More on Consistency and Consensus</h1>&#13;
<p><a contenteditable="false" data-primary="Designing Data-Intensive Applications (Kleppmann)" data-type="indexterm" id="idm46183196339216"/><a contenteditable="false" data-primary="Kleppmann, Martin, Designing Data-Intensive Applications" data-type="indexterm" id="idm46183196338080"/><a contenteditable="false" data-primary="consensus" data-type="indexterm" id="idm46183196322240"/><a contenteditable="false" data-primary="consistency, of data" data-type="indexterm" id="idm46183196321264"/><a contenteditable="false" data-primary="data" data-secondary="consistency of" data-type="indexterm" id="idm46183196320288"/>While we don’t have space in this book to dive deeply into the trade-offs between various consensus algorithms and how they are used to provide various data consistency guarantees, an <span class="keep-together">understanding</span> of these concepts is helpful in choosing the right data infrastructure for your cloud applications. If you’re interested in learning more in this area, Martin Kleppmann’s <a class="orm:hideurl" href="https://oreil.ly/6ndic"><em>Designing Data-Intensive Applications</em></a> (O’Reilly) is a great source, especially Chapter 9, “Consistency and Consensus”.</p>&#13;
</div>&#13;
<p><a contenteditable="false" data-primary="PingCAP" data-type="indexterm" id="idm46183196513696"/><a href="https://oreil.ly/jZNAI">TiDB</a> (where <em>Ti</em> stands for <em>Titanium</em>) represents a continuation of the NewSQL trend in the cloud native space. TiDB is an open source, <span class="keep-together">MySQL-compatible</span> database that supports both transactional and analytic workloads. It was initially developed and is primarily supported by PingCAP. While TiDB is a database designed to embody cloud native principles of scalability and elasticity, what makes it especially interesting for our discussion is that it has been explicitly designed to run on Kubernetes and to rely on capabilities provided by the Kubernetes control plane. In this way, one could argue that TiDB is not merely a Kubernetes native database, but also a Kubernetes <em>only</em> database. Let’s dig into the details.</p>&#13;
<section data-pdf-bookmark="TiDB Architecture" data-type="sect2"><div class="sect2" id="tidb_architectur">&#13;
<h2>TiDB Architecture</h2>&#13;
<p><a contenteditable="false" data-primary="architecture" data-secondary="TiDB" data-type="indexterm" id="arc_tidb"/><a contenteditable="false" data-primary="TiDB" data-secondary="architecture" data-type="indexterm" id="tidb_arc"/><a contenteditable="false" data-primary="HTAP (hybrid transactional/analytical processing)" data-type="indexterm" id="idm46183196313920"/><a contenteditable="false" data-primary="ETL (extract, transform and load)" data-type="indexterm" id="idm46183196312944"/><a contenteditable="false" data-primary="F1 project (Google)" data-type="indexterm" id="idm46183196311968"/><a contenteditable="false" data-primary="Google's F1 project" data-type="indexterm" id="idm46183196310992"/>A key characteristic of TiDB which distinguishes it from other databases we’ve <span class="keep-together">examined</span> so far in this book is its ability to support transactional and analytic workloads. This approach, known as <em>hybrid transactional/analytical processing (HTAP)</em>, supports both types of queries without a separate extract, transform, and load (ETL) process. As shown in <a data-type="xref" href="#tidb_architecture">Figure 7-1</a>, TiDB does this by providing two database engines under the hood: TiKV and TiFlash. This approach was inspired by Google’s <a href="https://oreil.ly/lakAf">F1 project</a>, a layer built on top of Spanner.</p>&#13;
<figure><div class="figure" id="tidb_architecture">&#13;
<img alt="TiDB architecture" src="assets/mcdk_0701.png"/>&#13;
<h6><span class="label">Figure 7-1. </span>TiDB architecture</h6>&#13;
</div></figure>&#13;
<p>One key aspect that gives TiDB a cloud native architecture is the packaging of compute and storage operations into separate components, each of which is composed of independently scalable services organized in clusters. Let’s examine the roles of each of these components:</p>&#13;
<dl>&#13;
<dt>TiDB</dt>&#13;
<dd>Each TiDB instance is a stateless service that exposes a MySQL endpoint to client applications. TiDB parses incoming SQL requests and uses metadata from the Placement Driver (PD) to create an execution plan containing queries to make on specific TiKV and TiFlash nodes in the storage cluster. TiDB executes these queries, assembles the results, and returns to the client application. The TiDB cluster is typically deployed with a proxy in front of it to provide load balancing.</dd>&#13;
<dt>TiKV</dt>&#13;
<dd><a contenteditable="false" data-primary="TiKV" data-type="indexterm" id="idm46183198109104"/><a contenteditable="false" data-primary="RocksDB" data-type="indexterm" id="idm46183198107184"/>The storage cluster consists of a mixture of TiKV and TiFlash nodes. First, let’s examine <a href="https://tikv.org"><em>TiKV</em></a>, an open source, distributed key-value database that uses <a href="http://rocksdb.org">RocksDB</a> as its backing storage engine. TiKV exposes a custom distributed SQL API that the TiDB nodes use to execute queries to store and retrieve data and manage distributed transactions. TiKV stores multiple replicas of your data, typically at least three, to support high availability and automatic failover. TiKV is a <a href="https://oreil.ly/ypLlC">CNCF graduated project</a> which can be used independently from TiDB, as we’ll discuss later.</dd>&#13;
<dt>TiFlash</dt>&#13;
<dd><a contenteditable="false" data-primary="TiFlash" data-type="indexterm" id="idm46183196324320"/><a contenteditable="false" data-primary="ClickHouse" data-type="indexterm" id="idm46183196323488"/>The storage cluster also includes TiFlash nodes, to which data is replicated from TiKV nodes as it is written. TiFlash is a columnar database based on the open source <a href="https://oreil.ly/PCVlg">ClickHouse analytic database</a>, which means that it organizes data storage in columns rather than rows. Columnar databases can provide a significant performance advantage for analytic queries requiring the extraction of the same column across multiple rows.</dd>&#13;
<dt>TiSpark</dt>&#13;
<dd><a contenteditable="false" data-primary="TiSpark" data-type="indexterm" id="idm46183196345280"/><a contenteditable="false" data-primary="OLAP (online analytical processing)" data-type="indexterm" id="idm46183196344224"/>This library is built for Apache Spark to support complex OLAP queries. TiSpark integrates with the Spark Driver and Spark Executors, providing the capability to ingest data from TiFlash instances using the distributed SQL API. We’ll examine the Spark architecture and the details of deploying Spark on Kubernetes in <a data-type="xref" href="ch09.html#data_analytics_on_kubernetes">Chapter 9</a>.</dd>&#13;
<dt>Placement Driver (PD)</dt>&#13;
<dd><a contenteditable="false" data-primary="PD (Placement Driver)" data-type="indexterm" id="idm46183197784912"/>The PD manages the metadata for a TiDB installation. PD instances are deployed in a cluster of at least three nodes. TiDB uses a range-based sharding mechanism where the keys in each table are divided into ranges called <em>regions</em>. The PD is responsible for determining the ranges of data assigned to each region, and the TiKV nodes that will store the data for each region. It monitors the amount of data in each region and splits regions that become too large, to facilitate scaling up, and merging smaller regions to scale down.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="TiPrometheus" data-type="indexterm" id="idm46183196299712"/>Because the TiDB architecture consists of well-defined interfaces between the components, it is an extensible architecture in which different pieces can be plugged in. For example, TiKV provides a distributed key-value storage solution that can be reused in other applications. The <a href="https://oreil.ly/PkmqK">TiPrometheus project</a> is an example, providing a Prometheus-compliant compute layer on top of TiKV. For another example, you could provide an alternate implementation of TiKV that implements the distributed SQL API on top of a different storage engine.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Pluggable Storage Engines</h1>&#13;
<p><a contenteditable="false" data-primary="pluggable storage engines" data-type="indexterm" id="idm46183196301120"/><a contenteditable="false" data-primary="database engines" data-type="indexterm" id="idm46183196291344"/><a contenteditable="false" data-primary="LSM tree (log-structured merge tree)" data-type="indexterm" id="idm46183196290368"/>In this chapter so far, we’ve made several mentions of “storage engines” or “database engines.” This term refers to the part of the database that manages the storage and retrieval of data on persistent media. In distributed databases, a distinction is often made between the storage engine and the proxy layer which sits on top of it to manage data replication between nodes. Chapter 3, “Storage and Retrieval,” from <a class="orm:hideurl" href="https://oreil.ly/6ndic"><em>Designing Data-Intensive Applications</em></a> includes discussion of storage engine types such as the B-trees used in most relational databases and the log-structured merge tree (LSM tree) used in Apache Cassandra and other NoSQL databases.</p>&#13;
</div>&#13;
<p>One interesting aspect of TiDB is the way in which it reuses existing technology. We’ve seen examples of this in the usage of components including RocksDB and Spark. TiDB also uses algorithms developed by other organizations. Here are a couple of examples:</p>&#13;
<dl>&#13;
<dt>Raft consensus protocol</dt>&#13;
<dd><a contenteditable="false" data-primary="PingCAP" data-type="indexterm" id="idm46183196450512"/><a contenteditable="false" data-primary="Raft consensus protocol" data-type="indexterm" id="idm46183196456752"/>At the TiDB layer, the <a href="https://oreil.ly/Oi6Dk">Raft consensus protocol</a> is used to manage consistency between replicas. Raft is similar to the Paxos algorithm used by Cassandra in terms of its behavior, but it’s designed to be much simpler to learn and use. TiDB uses a separate Raft group for each region, where a group typically consists of a leader and two or more replicas. If a leader node is lost, an election is run to select a new leader, and a new replica can be added to ensure the desired number of replicas. In addition, the TiFlash nodes are configured as a special type of replica called <em>learner replicas</em>. Data is replicated to learner replicas from the TiDB nodes, but they cannot be selected as a leader. You can read more about how TiDB uses Raft for <a href="https://oreil.ly/BddzV">high availability</a> and other related topics on the <a href="https://oreil.ly/Y2YuS">PingCAP blog</a>.</dd>&#13;
<dt>Percolator transaction management</dt>&#13;
<dd><a contenteditable="false" data-primary="Percolator transaction management" data-type="indexterm" id="idm46183196317376"/>At the TiDB layer, distributed transactions are supported using an implementation of the <a href="https://oreil.ly/heMho">Percolator algorithm</a> with optimizations specific to the TiDB project. Percolator was originally developed at Google for supporting incremental updates to search indexes.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="" data-startref="arc_tidb" data-type="indexterm" id="idm46183196318272"/><a contenteditable="false" data-primary="" data-startref="tidb_arc" data-type="indexterm" id="idm46183196277920"/>One of the arguments we’re making in this chapter is that part of what it means for data infrastructure to be cloud native is to compose existing APIs, services, and algorithms wherever possible, and TiDB is a great example of this.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deploying TiDB in Kubernetes" data-type="sect2"><div class="sect2" id="deploying_tidb_in_kubernetes">&#13;
<h2>Deploying TiDB in Kubernetes</h2>&#13;
<p><a contenteditable="false" data-primary="Kubernetes" data-secondary="deploying TiDB in" data-type="indexterm" id="kub_dep"/><a contenteditable="false" data-primary="TiDB" data-secondary="deploying in Kubernetes" data-type="indexterm" id="tidb_dep"/><a contenteditable="false" data-primary="TiDB Operator" data-type="indexterm" id="idm46183196271568"/>While TiDB can be deployed in a variety of ways including bare metal and VMs, the TiDB team has invested a large effort in tooling and documentation to make TiDB a truly Kubernetes native database. The <a href="https://oreil.ly/xZtGq">TiDB Operator</a> manages TiDB clusters in Kubernetes, including deployment, upgrade, scaling, backup and restore, and more.</p>&#13;
<p><a contenteditable="false" data-primary="GKE (Google Kubernetes Engine)" data-type="indexterm" id="idm46183196268224"/>The operator <a href="https://oreil.ly/iIZc0">documentation</a> provides <a href="https://oreil.ly/5heDA">quick start guides</a> for desktop Kubernetes distributions such as kind, minikube, and Google Kubernetes Engine (GKE). These instructions guide you through steps including installing CRDs and the TiDB operator using Helm, and a simple TiDB cluster including monitoring services. We’ll use the quick start instructions as a vehicle to talk about what makes TiDB a Kubernetes native database.</p>&#13;
<section data-pdf-bookmark="Installing the TiDB CRDs" data-type="sect3"><div class="sect3" id="installing_the_tidb_crds">&#13;
<h3>Installing the TiDB CRDs</h3>&#13;
<p><a contenteditable="false" data-primary="TiDB" data-secondary="installing CRDs" data-type="indexterm" id="idm46183196404784"/><a contenteditable="false" data-primary="CRD (Custom Resource Definition)" data-type="indexterm" id="idm46183196403344"/>After making sure you have a Kubernetes cluster that meets the defined prerequisites such as having a <a href="https://oreil.ly/7myfI">default StorageClass</a>, the first step in deploying TiDB using the operator is installing the CRDs used by the operator. This is done using an instruction such as the following (note the actual operator version number <code>v1.3.2</code> may vary):</p>&#13;
<pre data-type="programlisting">&#13;
<strong>set GH_LINK=https://raw.githubusercontent.com&#13;
kubectl create -f \&#13;
  $GH_LINK/pingcap/tidb-operator/v1.3.2/manifests/crd.yaml</strong></pre>&#13;
<p><a contenteditable="false" data-primary="kubectl get crd command" data-type="indexterm" id="idm46183196257504"/>This results in the creation of several CRDs, which you can observe by running the command <code>kubectl get crd</code> as we have done in previous chapters. We’ll quickly discuss the purpose of each resource since several of them hint at additional features of interest:</p>&#13;
<ul>&#13;
<li><p>The TidbCluster is the primary resource that describes the desired configuration of a TiDB cluster. We’ll look at an example later.</p></li>&#13;
<li><p>The TidbMonitor resource is used to deploy a Prometheus-based monitoring stack to observe one or more TidbClusters. As we have seen with other projects, Prometheus (or at least its API) has become a de facto standard for metrics collection for databases and other infrastructure deployed on Kubernetes.</p></li>&#13;
<li><p>The Backup and Restore resources represent the actions of performing a backup or restoring from a backup. This is similar to other operators we’ve examined previously from the Vitess (see <a data-type="xref" href="ch05.html#planetscale_vitess_operator">“PlanetScale Vitess Operator”</a>) and K8ssandra (<a data-type="xref" href="ch06.html#integrating_data_infrastructure_in_a_ku">Chapter 6</a>) projects. The TiDB Operator also provides a BackupSchedule resource that can be used to configure regular backups.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="initialization tasks, TiDB and" data-type="indexterm" id="idm46183196249760"/>The TidbInitializer is an optional resource that you can create to perform <a href="https://oreil.ly/qFsmu">initialization tasks</a> on a TidbCluster, including setting administrator credentials and executing SQL statements for schema creation or initial data loading.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="auto-scaling, TiDB and" data-type="indexterm" id="idm46183196248992"/>The TidbClusterAutoScaler is another optional resource which can be used to configure <a href="https://oreil.ly/wVbf2">auto-scaling</a> behavior of a TidbCluster. The number of TiKV or TiDB nodes in a TidbCluster can be configured to scale up or down between minimum and maximum limits based on CPU utilization. The addition of scaling rules based on other metrics is on the project roadmap. As we discussed in <a data-type="xref" href="ch05.html#choosing_operators">“Choosing Operators”</a>, auto-scaling is considered a feature of an operator at Level 5 or Autopilot, the highest maturity level.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="continuous profiling, TiDB and" data-type="indexterm" id="idm46183196243888"/>The TidbNGMonitoring is an optional resource that configures a TidbCluster to enable <a href="https://oreil.ly/2n8k5">continuous profiling</a> down to the system call level. The resulting profiling data and flame graph visualizations can be observed using the <a href="https://oreil.ly/23pLs">TiDB Dashboard</a>, which is deployed separately. This is typically used by project engineers looking to optimize the database, but application and platform developers may find this useful as well.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="DM (Data Migration) platform" data-type="indexterm" id="idm46183196449040"/>The DMCluster resource is used to deploy an instance of the <a href="https://oreil.ly/C5NG0">TiDB Data Migration</a> (DM) platform that supports migration of MySQL and MariaDB database instances into a TidbCluster. It can also be configured to migrate from an existing TiDB installation outside of Kubernetes to a TidbCluster. The ability to deploy data migration services alongside a destination TidbCluster in Kubernetes managed by the same operator is a great example of what it means to develop data ecosystems in Kubernetes, a pattern that we hope to see more of in the future.</p></li>&#13;
</ul>&#13;
<p>For the remainder of this section, we’ll focus on the TidbCluster and TidbMonitoring resources.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Installing the TiDB Operator" data-type="sect3"><div class="sect3" id="installing_the_tidb_operator">&#13;
<h3>Installing the TiDB Operator</h3>&#13;
<p><a contenteditable="false" data-primary="installing" data-secondary="TiDB Operator" data-type="indexterm" id="idm46183196239232"/><a contenteditable="false" data-primary="TiDB" data-secondary="installing Operator" data-type="indexterm" id="idm46183196372512"/><a contenteditable="false" data-primary="TiDB Operator" data-type="indexterm" id="idm46183196234080"/>After installing the CRDs, the next step is to install the TiDB Operator using Helm. You’ll need to add the Helm repository first before installing the TiDB Operator in its own Namespace:</p>&#13;
<pre data-type="programlisting"><strong>helm repo add pingcap https://charts.pingcap.org&#13;
helm install –create-namespace --namespace tidb-admin tidb-operator \&#13;
  pingcap/tidb-operator --version v1.3.2</strong></pre>&#13;
<p>You can watch the resulting Pods come online using <code>kubectl get pods</code> and referencing the <code>tidb-admin</code> Namespace. <a data-type="xref" href="#installing_the_tidb_operator_and_crds">Figure 7-2</a> provides a summary of the elements that you’ve installed up to this point. This includes Deployments to manage the TiDB Operator (labeled as <code>tidb-controller-manager</code>) and the TiDB Scheduler.</p>&#13;
<p>The TiDB Scheduler is an optional extension to the Kubernetes built-in scheduler. While it is deployed by default as part of the TiDB Operator, it can be disabled. Assuming the TiDB Scheduler is not disabled, using it for a specific TidbCluster still requires opting in by setting the <code>schedulerName</code> property to <code>tidb-scheduler</code>. If this property is set, the TiDB Operator will assign the TiDB Scheduler as the scheduler that Kubernetes will use when creating TiKV, and PD Pods.</p>&#13;
&#13;
<p>The TiDB Scheduler extends the Kubernetes built-in scheduler to add custom scheduling rules for Pods that are part of a TidbCluster, helping to achieve high availability of the database while spreading the load evenly across the available Worker Nodes in the Kubernetes cluster. While for many types of infrastructure, the existing mechanisms Kubernetes offers for influencing the default scheduler such as affinity rules, taints, and tolerations are sufficient, TiDB provides a useful example of when and how to implement custom scheduling logic. We’ll look at Kubernetes scheduler extensions in more detail in <a data-type="xref" href="ch09.html#data_analytics_on_kubernetes">Chapter 9</a>.</p>&#13;
&#13;
<figure><div class="figure" id="installing_the_tidb_operator_and_crds">&#13;
<img alt="Installing the TiDB Operator and CRDs" src="assets/mcdk_0702.png"/>&#13;
<h6><span class="label">Figure 7-2. </span>Installing the TiDB Operator and CRDs</h6>&#13;
</div></figure>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>TiDB Operator Helm Chart Options</h1>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="TiDB Operator options" data-type="indexterm" id="idm46183196527312"/><a contenteditable="false" data-primary="TiDB Scheduler" data-type="indexterm" id="idm46183196525872"/>This installation omits usage of a <em>values.yaml</em> file, but you can see the available options by running following command:</p>&#13;
<pre data-type="programlisting"><strong>helm show values pingcap/tidb-operator</strong></pre>&#13;
<p>This includes the option to disable the TiDB Scheduler.</p>&#13;
</div>&#13;
&#13;
</div></section>&#13;
<section data-pdf-bookmark="Creating a TidbCluster" data-type="sect3"><div class="sect3" id="creating_a_tidbcluster">&#13;
<h3>Creating a TidbCluster</h3>&#13;
<p><a contenteditable="false" data-primary="TiDB" data-secondary="creating TidbCluster" data-type="indexterm" id="tidb_clu"/><a contenteditable="false" data-primary="TidbCluster, creating" data-type="indexterm" id="clu_cr"/>Once the TiDB Operator has been installed, you’re ready to create a TidbCluster resource. While many <a href="https://oreil.ly/66uf7">example configurations</a> are available in the TiDB Operator GitHub repository, let’s use the one referenced in the quick start guide:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>set GH_LINK=https://raw.githubusercontent.com&#13;
kubectl create namespace tidb-cluster&#13;
kubectl -n tidb-cluster apply -f \&#13;
  $GH_LINK/pingcap/tidb-operator/master/examples/basic/tidb-cluster.yaml</strong></pre>&#13;
<p>While the TidbCluster is being created, you can reference the contents of this file, which look something like this (with comments and some details removed):</p>&#13;
<pre data-type="programlisting">apiVersion: pingcap.com/v1alpha1&#13;
kind: TidbCluster&#13;
metadata:&#13;
  name: basic&#13;
spec:&#13;
  version: v5.4.0&#13;
  ...&#13;
  pd:&#13;
    baseImage: pingcap/pd&#13;
    maxFailoverCount: 0&#13;
    replicas: 1&#13;
    requests:&#13;
      storage: "1Gi"&#13;
    config: {}&#13;
  tikv:&#13;
    baseImage: pingcap/tikv&#13;
    maxFailoverCount: 0&#13;
    evictLeaderTimeout: 1m&#13;
    replicas: 1&#13;
    requests:&#13;
      storage: "1Gi"&#13;
    config:&#13;
      ...&#13;
  tidb:&#13;
    baseImage: pingcap/tidb&#13;
    maxFailoverCount: 0&#13;
    replicas: 1&#13;
    service:&#13;
      type: ClusterIP&#13;
    config: {}</pre>&#13;
<p>Notice that this results in the creation of a TidbCluster named <code>basic</code> in the <code>tidb-cluster</code> Namespace, with one replica each of TiDB, TiKV, and PD, using the standard PingCAP images for each. Additional options are used to specify the minimum amount of compute and storage resources required to achieve a functioning cluster. No TiFlash nodes are included in this simple configuration.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>TidbCluster API</h1>&#13;
<p>The full list of options for a TidbCluster can be found as part of the <a href="https://oreil.ly/XoC02">API</a> available in the GitHub repository. This same page includes options for the other CRDs used by the TiDB Operator. As you explore the options for these CRDs, you’ll see evidence of the common practice of allowing many of the options that will be used to specify underlying resources to be overridden (for example, the Pod specification that will be set on a Deployment).</p>&#13;
</div>&#13;
<p>We encourage you to take the opportunity to use <code>kubectl</code> or your favorite visualization tool to explore the resources created as part of the TidbCluster, a summary of which is provided in <a data-type="xref" href="#a_basic_tidbcluster">Figure 7-3</a>.</p>&#13;
<figure><div class="figure" id="a_basic_tidbcluster">&#13;
<img alt="A basic TidbCluster" src="assets/mcdk_0703.png"/>&#13;
<h6><span class="label">Figure 7-3. </span>A basic TidbCluster</h6>&#13;
</div></figure>&#13;
<p>As you can see, the TiDB Operator creates StatefulSets to manage the TiDB, TiKV, and Placement Driver instances, allocating a PVC for each instance. As an I/O-intensive application, the default configuration is to use local PersistentVolumes as the backing store.</p>&#13;
<p>In addition, a Deployment is created to run a Discovery Service which the various components use to learn of each other’s location. The Discovery Service performs a similar role to that of etcd in other data technologies we’ve examined in the book. The TiDB Operator also configures services for each StatefulSet and Deployment that facilitate communication within the TiDB cluster as well as exposing capabilities to external clients.</p>&#13;
<p>The TiDB Operator supports the deployment of a Prometheus monitoring stack that can manage one or more TiDB clusters. You can add monitoring to the cluster created previously using the following command:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>set GH_LINK=https://raw.githubusercontent.com&#13;
kubectl -n tidb-cluster apply -f \&#13;
  $GH_LINK/pingcap/tidb-operator/master/examples/basic/tidb-monitor.yaml</strong></pre>&#13;
<p>While this is deploying, let’s examine the contents of the <em>tidb-monitor.yaml</em> configuration file:</p>&#13;
<pre data-type="programlisting">apiVersion: pingcap.com/v1alpha1&#13;
kind: TidbMonitor&#13;
metadata:&#13;
  name: basic&#13;
spec:&#13;
  replicas: 1&#13;
  clusters:&#13;
  - name: basic&#13;
  prometheus:&#13;
    baseImage: prom/prometheus&#13;
    version: v2.27.1&#13;
  grafana:&#13;
    baseImage: grafana/grafana&#13;
    version: 7.5.11&#13;
  initializer:&#13;
    baseImage: pingcap/tidb-monitor-initializer&#13;
    version: v5.4.0&#13;
  reloader:&#13;
    baseImage: pingcap/tidb-monitor-reloader&#13;
    version: v1.0.1&#13;
  prometheusReloader:&#13;
    baseImage: quay.io/prometheus-operator/prometheus-config-reloader&#13;
    version: v0.49.0&#13;
  imagePullPolicy: IfNotPresent</pre>&#13;
<p>As you can see, the TidbMonitor resource can point to one or more TidbClusters. This TidbMonitor is configured to manage the <code>basic</code> cluster you created previously. The TidbMonitor resource also allows you to specify the versions of Prometheus, Grafana, and additional tools that are used to initialize and update the monitoring stack. If you examine the contents of the <code>tidb-cluster</code> Namespace, you’ll see additional workloads that have been created to manage these elements.</p>&#13;
<p>TiDB uses the Prometheus stack in a similar way to the K8ssandra project, as we discussed in <a data-type="xref" href="ch06.html#unified_monitoring_infrastructure_with">“Unified Monitoring Infrastructure with <span class="keep-together">Prometheus and Grafana</span>”</a>. In both of these projects, the Prometheus stack is supported as an optional extension to provide a monitoring capability you can use with very little <span class="keep-together">customization.</span> The configurations and provided visualizations focus on the key metrics that drive awareness of database health. Even if you are already managing your own monitoring infrastructure or using a third-party software-as-a-service (SaaS) solution, the configurations and charts can give you a head start on incorporating database monitoring into the rest of your observability approach.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="a_roadmap_for_cloud_native_databases_on">&#13;
<h5>A Roadmap for Cloud Native Databases on Kubernetes</h5>&#13;
<p><em>With Dongxu (Ed) Huang, Cofounder and CTO, PingCAP</em></p>&#13;
<p><a contenteditable="false" data-primary="PingCAP" data-type="indexterm" id="idm46183196395488"/><a contenteditable="false" data-primary="Huang, Dongzu (Ed)" data-type="indexterm" id="idm46183196394656"/><a contenteditable="false" data-primary="Kubernetes native database" data-secondary="roadmap for" data-type="indexterm" id="idm46183196177392"/><a contenteditable="false" data-primary="Google Spanner" data-type="indexterm" id="idm46183196176176"/><a contenteditable="false" data-primary="Spanner paper" data-type="indexterm" id="idm46183196175136"/><a contenteditable="false" data-primary="F1 project (Google)" data-type="indexterm" id="idm46183196174032"/><a contenteditable="false" data-primary="Google's F1 project" data-type="indexterm" id="idm46183196172928"/>TiDB was created out of the experience of maintaining a storage system for a large internet company who ran an Android app store. The distributed MySQL sharding cluster we were using was innovative at the time but also too hard to maintain. With manual sharding, you cannot do cross-shard joins or transactions. It’s painful for the application developer. The Google Spanner and F1 papers provided the inspiration for future databases like TiDB with scalability and high availability, consistency, full-featured SQL, and global transaction support. From the application developers’ perspective, it should feel like going back to the old days of single-node development, but now with horizontal scalability.</p>&#13;
<p><a contenteditable="false" data-primary="OLTP (online transaction processing)" data-type="indexterm" id="idm46183196167024"/>The problem statement was straightforward. We wanted to provide scalable online transaction processing (OLTP) queries with reduced migration cost and an easy-to-use MySQL interface. At that time, there was no open source implementation of Spanner, so we started to build TiKV and donated it to the CNCF. As more and more users started running OLAP queries on top of their real-time data in TiDB, we expanded our OLAP capability to create HTAP—a hybrid approach. The TiFlash engine that supports OLAP queries has recently been made open source as well.</p>&#13;
<p>The TiDB architecture does have some cloud native aspects from its original design, especially since it has a shared-nothing architecture. However, being called a cloud native database requires a higher standard. A cloud native database should make maximum use of the infrastructure your cloud vendor provides (for example, a storage engine that leverages S3 or uses a cloud’s serverless features). By this standard, the most cloud native database is Snowflake. The approach that customers need is this: pay for only what you use. If you have to buy it by the node, it’s not serverless.</p>&#13;
<p>We like to refer to TiDB as a Kubernetes native database. When we saw the first etcd operator released in 2016, we were inspired to create our own operator. At that time, Kubernetes was not as mature as it is today. We didn’t have CRDs, just third-party resources. We had to build our own scheduler to make sure we could handle failover correctly. The hardest part was handling local storage. Kubernetes was not designed from the database engineer’s point of view. At Google, the team didn’t focus on providing access to local disk for databases, since most of the systems were built on top of columnar stores. The team didn’t care about local state, but as a database engineer, you have to be very careful with your use of local disk. Since there was no local storage API in Kubernetes when we started, we wrote our own controller to manage local disks. We put a lot of resources into this effort. It was very complicated and might have been the wrong decision.</p>&#13;
<p>Today things are a lot better. Kubernetes networking, StatefulSets, and CRDs are mature and frequently utilized by application developers and database engineers. At PingCAP, we use Kubernetes to run our managed service on public clouds. We have a lot of users, and yet it’s very stable. We can work with it.</p>&#13;
<p>In the future of cloud native architectures, storage and compute will be separated more and more clearly over time. In the past, you would never have built a database on top of remote storage. But now it might be time to give up doing persistence on local disks. We’re working on a new storage engine for TiDB built on top of shared storage.</p>&#13;
<p>One area where Kubernetes needs to improve is support for multitenancy. Today, building a multitenant application in Kubernetes is hard. Namespaces are not enough of an abstraction to support multitenancy. Similar to control groups (cgroups) for Linux, Kubernetes needs a virtualized cluster or some other multitenancy mechanism within the cluster. A <a href="https://oreil.ly/RY0bJ">Kubernetes SIG</a> is looking into multitenancy, and the work on virtual clusters, or <a href="https://www.vcluster.com"><em>vclusters</em></a>, is promising.</p>&#13;
<p><a contenteditable="false" data-primary="Cloud Hypervisor" data-type="indexterm" id="idm46183196427600"/>A second area where Kubernetes could improve is better support for hypervisors. When you have large clusters, you don’t want to have virtualization on top of hardware and then run Kubernetes on top of that. The Kubernetes community could be more ambitious and put more resources toward embracing hypervisors such as <a href="https://oreil.ly/18jj8">Cloud Hypervisor</a>.</p>&#13;
<p><a contenteditable="false" data-primary="DevOps" data-type="indexterm" id="idm46183196426224"/>For its part, the database world needs to be more focused on Kubernetes. DevOps and application engineers are the mainstream Kubernetes community, but the database folks are outside of that. Most database operators are not written by experienced DBAs. Once you get beyond deploying the database, tuning a database is a hard job; you have a lot of maintenance to do. Once you put a database in Kubernetes Pods, tuning it requires going inside the Pods. For the DBA or DevOps engineer, that’s the trickiest part. As a user, you should always prefer an operator provided by the database vendor. If you’re a database vendor, you need to help the user by making it easier to tune in the Kubernetes environment, not just deployment or upgrades. The real world is not like running a demo.</p>&#13;
</div></aside>&#13;
<p><a contenteditable="false" data-primary="" data-startref="hds_tidb" data-type="indexterm" id="idm46183197782544"/><a contenteditable="false" data-primary="" data-startref="knd_hds" data-type="indexterm" id="idm46183196137840"/><a contenteditable="false" data-primary="" data-startref="tidb_hds" data-type="indexterm" id="idm46183196131088"/><a contenteditable="false" data-primary="" data-startref="kub_dep" data-type="indexterm" id="idm46183196135904"/><a contenteditable="false" data-primary="" data-startref="tidb_dep" data-type="indexterm" id="idm46183196134528"/><a contenteditable="false" data-primary="" data-startref="tidb_clu" data-type="indexterm" id="idm46183196143248"/><a contenteditable="false" data-primary="" data-startref="clu_cr" data-type="indexterm" id="idm46183196124304"/>As you can see, TiDB is a database with a flexible, extensible architecture that has been designed with cloud native principles in mind. It also has a strong bias toward being able to deploy and manage a database effectively in Kubernetes and has provided us with some valuable insights on what it means to be Kubernetes native. Consult the TiDB documentation for more information on features such as <a href="https://oreil.ly/NPHxy">deploying to multiple Kubernetes clusters</a>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Serverless Cassandra with DataStax Astra DB" data-type="sect1"><div class="sect1" id="serverless_cassandra_with_datastax_astr">&#13;
<h1>Serverless Cassandra with DataStax Astra DB</h1>&#13;
<p><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="serverless, with DataStax Astra DB" data-type="indexterm" id="ac_astra"/><a contenteditable="false" data-primary="DataStax" data-type="indexterm" id="datas_ab"/><a contenteditable="false" data-primary="Kubernetes native database" data-secondary="serverless Cassandra with DataStax Astra DB" data-type="indexterm" id="knd_datas"/><a contenteditable="false" data-primary="IaaS (infrastructure as a service)" data-type="indexterm" id="idm46183196115520"/><a contenteditable="false" data-primary="PaaS (platform as a service)" data-type="indexterm" id="idm46183196114544"/><a contenteditable="false" data-primary="SaaS (software as a service)" data-type="indexterm" id="idm46183196113568"/><a contenteditable="false" data-primary="FaaS (functions as a service)" data-type="indexterm" id="idm46183196112592"/>Since the advent of cloud computing in the early 2000s, public cloud providers and infrastructure vendors have made continual advances in commoditizing various layers of our architectural stacks as service offerings. This trend began with offering compute, network, and storage as <em>infrastructure as a service</em> (IaaS) and proceeded into other trends including <em>platform as a service</em> (PaaS), <em>software as a service</em> (SaaS), and <em>functions as a service</em> (FaaS), sometimes conflated with the term <em>serverless</em>.</p>&#13;
<p><a contenteditable="false" data-primary="DBaaS (database as a service)" data-type="indexterm" id="idm46183196108096"/>Most pertinent to our investigation here is the emergence of managed data infrastructure offerings known as <em>database as a service</em> (DBaaS). This category includes the following:<a contenteditable="false" data-primary="databases" data-secondary="traditional" data-type="indexterm" id="idm46183196105824"/><a contenteditable="false" data-primary="traditional databases" data-type="indexterm" id="idm46183196106416"/><a contenteditable="false" data-primary="databases" data-secondary="cloud" data-type="indexterm" id="idm46183196104208"/><a contenteditable="false" data-primary="cloud databases" data-type="indexterm" id="idm46183196102832"/><a contenteditable="false" data-primary="databases" data-secondary="managed NoSQL" data-type="indexterm" id="idm46183196101728"/><a contenteditable="false" data-primary="managed NoSQL databases" data-type="indexterm" id="idm46183196100352"/><a contenteditable="false" data-primary="databases" data-secondary="NewSQL" data-type="indexterm" id="idm46183196099248"/><a contenteditable="false" data-primary="NewSQL databases" data-type="indexterm" id="idm46183196097872"/></p>&#13;
<ul>&#13;
<li><p>Traditional databases offered as a managed cloud service, such as Amazon Relational Database Service (RDS) and PlanetScale</p></li>&#13;
<li><p>Cloud databases like Google BigTable, Amazon Dynamo, and Snowflake that are available only as cloud offerings</p></li>&#13;
<li><p>Managed NoSQL or NewSQL databases that can also be run on premises under an open source or source available license—for example, MongoDB Atlas, DataStax Astra DB, TiDB, and Cockroach DB</p></li>&#13;
</ul>&#13;
<p>Over the past several years, many of the vendors behind these DBaaS services have begun migrating onto Kubernetes to automate operations, manage compute resources more efficiently, and make their solutions portable across clouds. DataStax was one of several vendors that began offering Cassandra as a service. These vendors typically used an architecture based on running traditional Cassandra clusters in a cloud environment, with various “glue code” to integrate aspects like networking, monitoring, and management that didn’t quite fit target deployment environments like Kubernetes and public cloud IaaS. These include techniques like using sidecars to collect metrics and logs, or deploying Cassandra nodes using StatefulSets to manage scaling up and down in an orderly fashion.</p>&#13;
<p>Even with these workarounds for running in Kubernetes, Cassandra’s monolithic architecture doesn’t readily promote the separation of compute and storage, which can lead to some awkwardness when scaling. You scale up a Cassandra cluster by adding additional nodes, where each node has the following capabilities:</p>&#13;
<dl>&#13;
<dt>Coordination</dt>&#13;
<dd><a contenteditable="false" data-primary="coordination, as a node capability" data-type="indexterm" id="idm46183198914608"/>Receiving read and write requests and forwarding them to other nodes as needed to achieve the requested number of replicas (also known as <em>consistency level</em>)</dd>&#13;
<dt>Writing and reading</dt>&#13;
<dd><a contenteditable="false" data-primary="writing, as a node capability" data-type="indexterm" id="idm46183196292976"/><a contenteditable="false" data-primary="reading, as a node capability" data-type="indexterm" id="idm46183196126720"/>Writing data to in-memory cache (memtables) and persistent storage (SSTables), and reading it back as needed</dd>&#13;
<dt>Compaction and repair</dt>&#13;
<dd><a contenteditable="false" data-primary="compaction, as a node capability" data-type="indexterm" id="idm46183196086848"/><a contenteditable="false" data-primary="repairs" data-type="indexterm" id="idm46183196087760"/>Since Cassandra is an LSM-tree database, it does not update datafiles once they are written to persistent storage. Compaction and repair are tasks that run in the background as separate threads. Compaction helps Cassandra stay performant by consolidating SSTables written at different times, ignoring obsolete and deleted values. Repair is the process of comparing stored values across nodes to ensure consistency.</dd>&#13;
</dl>&#13;
<p>Each node in a Cassandra cluster implements all of these capabilities and consumes equivalent compute and storage resources. This makes it difficult to scale compute and storage independently and can lead to situations where a cluster is overprovisioned in compute or storage resources.</p>&#13;
<p><a contenteditable="false" data-primary="“DataStax Astra DB: Designing a Serverless Cloud-Native Database-as-a-Service”" data-primary-sortas="datastax" data-type="indexterm" id="idm46183196450848"/>In 2021, DataStax published a paper entitled <a href="https://oreil.ly/yHSxz">“DataStax Astra DB: Designing a Serverless Cloud-Native Database-as-a-Service”</a> that describes a different approach. <em>Astra DB</em> is a version of Cassandra that has been refactored into microservices to allow more fine-grained scalability and to take advantage of the benefits of Kubernetes. In fact, Astra DB is not only Kubernetes native; it is essentially a Kubernetes-only database. <a data-type="xref" href="#astra_db_architecture">Figure 7-4</a> shows the Astra DB architecture at a high level, broken into a control plane, data plane, and supporting infrastructure.</p>&#13;
<figure><div class="figure" id="astra_db_architecture">&#13;
<img alt="Astra DB architecture" src="assets/mcdk_0704.png"/>&#13;
<h6><span class="label">Figure 7-4. </span>Astra DB architecture</h6>&#13;
</div></figure>&#13;
<p>Let’s do a quick overview of the layers in this architecture:</p>&#13;
<dl>&#13;
<dt>Astra DB control plane</dt>&#13;
<dd><a contenteditable="false" data-primary="control plane" data-secondary="in Astra DB" data-type="indexterm" id="idm46183196076176"/>The control plane is responsible for provisioning Kubernetes clusters in various cloud provider regions. It also provisions Astra DB clusters within those Kubernetes clusters and provides the APIs that allow clients to create and manage databases, either through the Astra DB web application, or programmatically through the DevOps API. Jim Dickinson’s blog post <a href="https://oreil.ly/jhU2Q">“How We Built the DataStax Astra DB Control Plane”</a> describes the architecture of the control plane and how it was migrated to be Kubernetes native.</dd>&#13;
<dt>Astra DB data plane</dt>&#13;
<dd><a contenteditable="false" data-primary="data plane" data-type="indexterm" id="idm46183196072752"/>The data plane is where the actual Astra DB databases run. The data plane consists of multiple microservices which together provide the capabilities that would have been a part of a single monolithic Cassandra node. Each database is deployed in a Kubernetes cluster in a dedicated Namespace and may be shared across multiple tenants, as described in more detail later on.</dd>&#13;
<dt>Astra DB infrastructure</dt>&#13;
<dd><a contenteditable="false" data-primary="infrastructure" data-secondary="in Astra DB" data-type="indexterm" id="idm46183196199744"/>Each Kubernetes cluster also contains a set of infrastructure components that are shared across the Astra DB databases in that cluster, including etcd, Prometheus, and Grafana. etcd is used to store metadata, including the assignment of tenants to databases and database schema for each tenant. It also stores information about the cluster topology, replacing the role of gossip in the traditional Cassandra architecture. Prometheus and Grafana are deployed in a similar way as described in other architectures in this book.</dd>&#13;
</dl>&#13;
<p>Now let’s dig more into a few of the microservices in the data plane:</p>&#13;
<dl>&#13;
<dt>Astra DB Operator</dt>&#13;
<dd><a contenteditable="false" data-primary="operators" data-secondary="in Astra DB" data-type="indexterm" id="idm46183197018624"/>The Astra DB Operator manages the Kubernetes resources required for each database instance as described by a DBInstallation custom resource, as shown in <a data-type="xref" href="#astra_db_cluster_in_kubernetes">Figure 7-5</a>. Similar to the Cass Operator project we discussed in <a data-type="xref" href="ch06.html#managing_cassandra_in_kubernetes_with_c">“Managing Cassandra in Kubernetes with Cass Operator”</a>, the Astra DB Operator automates many of the operational tasks associated with managing a Cassandra cluster that would typically be performed by human operators using <em>nodetool</em>.</dd>&#13;
<dt>Coordination Service</dt>&#13;
<dd><a contenteditable="false" data-primary="Coordination Service, in Astra DB" data-type="indexterm" id="idm46183196242528"/>The Coordination Service is responsible for handling application queries including reads, writes, and schema management. Each Coordination Service is an instance of Stargate (as discussed in <a data-type="xref" href="ch06.html#enabling_developer_productivity_with_st">“Enabling Developer Productivity with Stargate APIs”</a> that exposes endpoints for CQL and other APIs, with an Astra <span class="keep-together">DB–specific</span> plug-in that enables it to route requests intelligently to Data Service instances to actually store and retrieve data. Factoring this compute-intensive routing functionality into its own microservice enables it to be scaled up or down based on query traffic, independent of the volume of data being managed.</dd>&#13;
<dt>Data Service</dt>&#13;
<dd><a contenteditable="false" data-primary="Data Service, in Astra DB" data-type="indexterm" id="idm46183196444384"/>Each Data Service instance is responsible for managing a subset of the data for each assigned tenant based on its position in the Cassandra token ring. The Data Service takes a tiered approach to data storage, maintaining in-memory data structures such as memtables, using local disk for caching, commit logs and indexes, and object storage for longer-term persistence of SSTables. The usage of object storage is one of the key differentiators of Astra DB from other databases we’ve examined so far, and we’ll examine other benefits of this approach throughout this section.</dd>&#13;
<dt>Compaction Service</dt>&#13;
<dd><a contenteditable="false" data-primary="Compaction Service, in Astra DB" data-type="indexterm" id="idm46183196053152"/>The Compaction Service is responsible for performing maintenance tasks including compaction and repair on SSTables in object storage. Compaction and repair are compute-intensive tasks that experienced Cassandra operators have historically scheduled for off-peak hours to limit their impact on cluster performance. In Astra DB, these tasks can be performed at any time the need arises without impacting query performance. The work is handled by a pool of Compaction Service instances which can scale up or down independently to generate repaired, compacted SSTables which are immediately accessible to Data Services.</dd>&#13;
<dt>IAM Service</dt>&#13;
<dd><a contenteditable="false" data-primary="IAM (Identity and Access Management) Service, in Astra DB" data-type="indexterm" id="idm46183196359440"/>All incoming application requests are routed through the Identity and Access Management (IAM) Service, which uses a standard set of roles and permissions defined in the control plane. While Cassandra has long had a pluggable architecture for authentication and authorization, factoring this out into its own microservice allows for more flexibility and support for additional providers such as Okta.</dd>&#13;
</dl>&#13;
<p>The data plane includes additional services which have been omitted from <a data-type="xref" href="#astra_db_architecture">Figure 7-4</a> for simplicity, including a Commitlog Replayer Service for recovery of failed Data Service instances, and an Autoscaling Service which uses analytics and machine learning to recommend to the operator when to scale the number of instances of each service up or down.</p>&#13;
<p><a data-type="xref" href="#astra_db_cluster_in_kubernetes">Figure 7-5</a> shows what a typical DBInstallation looks like in terms of Kubernetes resources. Let’s walk through a few typical interactions focusing on individual instances of key services to demonstrate how each resource plays its part.</p>&#13;
&#13;
&#13;
&#13;
<p>A Kubernetes Ingress is configured for each cluster to manage incoming requests from client applications (1) and route requests to Coordinator Services by the tenant using a Kubernetes Service (2).</p>&#13;
&#13;
<figure><div class="figure" id="astra_db_cluster_in_kubernetes">&#13;
<img alt="Astra DB cluster in Kubernetes" src="assets/mcdk_0705.png"/>&#13;
<h6><span class="label">Figure 7-5. </span>Astra DB cluster in Kubernetes</h6>&#13;
</div></figure>&#13;
&#13;
<p>The Coordinator Service is a stateless service managed by a Deployment (3) which delegates authentication and authorization checks on each call to the IAM Service (4).</p>&#13;
<p>Authorized requests are then routed to one or more Data Services based on the tenant, again using a Kubernetes Service (5).</p>&#13;
<p>Data Services are managed using StatefulSets (6), which are used to assign each instance to a local PersistentVolume used for managing intermediate datafiles such as the commit log, which is populated immediately on writes. When possible, reads are served directly from in-memory data structures.</p>&#13;
<p>As is typical for Cassandra and other LSM tree storage engines, the Data Service occasionally writes SSTable files out to a persistent store (7). For Astra DB, that persistent store is an external object store managed by the cloud provider for high availability. A separate object storage bucket is used per tenant to ensure data privacy.</p>&#13;
<p>The Compaction Service can perform compaction and repair on SSTables in the object store asynchronously (8), with no impact to write and read queries.</p>&#13;
&#13;
&#13;
<p>Astra DB also supports multiregion database clusters, which by definition span multiple Kubernetes clusters. Coordinator and Data Services are deployed across Datacenters (cloud regions) and racks (availability zones) using an approach similar to that described for K8ssandra in <a data-type="xref" href="ch06.html#deploying_multicluster_applications_in">“Deploying Multicluster Applications in Kubernetes”</a>.</p>&#13;
<p><a contenteditable="false" data-primary="shuffle sharding" data-type="indexterm" id="idm46183196122128"/>Astra DB’s microservice architecture allows it to make more optimal use of compute and storage resources and isolate compute-intensive operations, leading to overall cost savings to operate Cassandra clusters in the cloud. These cost savings are extended by the addition of multitenant features that allow each cluster to be shared across multiple tenants. The <a href="https://oreil.ly/Zq0yc">Astra DB whitepaper</a> describes a technique called <em>shuffle sharding</em> which is used to match each tenant to a subset of the available Coordinator and Data Services, effectively creating a separate Cassandra token ring per tenant. As the population of tenants in an Astra DB instance changes, this topology can be easily updated to rebalance load without downtime, and larger tenants can be configured to use their own dedicated databases (DBInstallations). This approach minimizes cost while meeting SLAs for performance and availability.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="building_a_serverless_cassandra">&#13;
<h5>Building a Serverless Cassandra</h5>&#13;
<p><em>With Jake Luciani, Engineering Leader, DataStax</em></p>&#13;
<p><a contenteditable="false" data-primary="Luciani, Jake" data-type="indexterm" id="idm46183196237392"/><a contenteditable="false" data-primary="Kubernetes native database" data-secondary="building serverless Cassandra" data-type="indexterm" id="idm46183196236224"/>Cassandra has always been considered a cloud native database, but it’s not Kubernetes native. The K8ssandra project represents a first step in the direction of making Cassandra more Kubernetes native. It’s a systematic way to run Cassandra on Kubernetes in a more traditional cloud native way, but it represents more of a “lift and shift” approach. The Astra DB approach is more like throwing all of our bags on the Kubernetes bus. It’s a version of Cassandra that you can’t run without Kubernetes.</p>&#13;
<p>We realized early on in the process of building Astra DB that we had to make some modifications to Cassandra’s architecture to make it work in an even more cloud native way. The cost of running stateful systems in the cloud can get very expensive if you do it the wrong way. If you focus on optimizing for cost, you’ll actually end up with the most cloud native solution, because the services that are cheap in the clouds are the ones that have become the most commoditized. They’re also the most hardened parts of the system. By standing on the shoulders of proven cloud technologies like object storage and etcd, you’ll end up with a more reliable solution.</p>&#13;
<p>We often refer to Astra DB as a <em>serverless database</em>, which came from the original inspiration for the project: “How do we make Cassandra more serverless?” <em>Serverless</em> is a term for techniques engineers use in the cloud to make applications more scalable and stateless. The first breakthrough was separating compute from storage. Storing SSTables as immutable data on the object store allows us to scale our IOPS the same way we scale our processing engine. You can remove any component, and it doesn’t matter to the system. Just as you can scale up lambda serverless functions, you can scale up your database.</p>&#13;
<p>The topology is completely ephemeral, just like Cassandra; it can change on the fly. Cassandra has traditionally used a gossip-type protocol to coordinate topology and replicate state across nodes in an eventually consistent way. But since Cassandra was first built, systems like etcd have come along that do a great job of maintaining metadata about schema and topology in a transactional way. etcd is a stateful service in its own right, but we use it only as a way to transition from one state to the next. The object store is ultimately the source of truth for the entire system. You can lose an entire Kubernetes cluster with all of the databases running on it, rebuild the cluster, wipe the disks, and bring the whole system back. This is a great feeling when you go to sleep at night. Currently, we have to run our own etcd inside Kubernetes, even though Kubernetes runs its own etcd cluster. It would be great if we could utilize that infrastructure that’s already running. Instead, we’ve had to build up our own etcd expertise to make sure we know how to run it.</p>&#13;
<p>We use StatefulSets to manage the Cassandra nodes in each availability zone. StatefulSets provide the exact behavior we need in terms of scaling up and down in a fixed order. Although we usually use only local ephemeral disks and the local path provisioner, we’re not precluded from using a PVC with persistent storage if we needed to; it would just be more expensive. To perform upgrades, we create an entire StatefulSet with new Cassandra nodes. Once all of the nodes have joined, we can delete the old StatefulSet. We treat the StatefulSets as immutable infrastructure, throwing them away and starting over.</p>&#13;
<p>One big problem with data on Kubernetes is the rough edges in working with attached disks. Many databases need to stripe disks before using them. On Kubernetes, this means mounting  volumes as raw disks and then striping them during Pod startup. To scale the available IOPS, you have to attach more raw disks and stripe them as well. We avoided this problem in Astra DB by going all in on object storage and local ephemeral disks. The ephemeral disks are just a cache of what’s in the object storage, but they give us the IOPS we need. Cassandra uses ann LSM-tree style of storage engine, similar to RocksDB. This provides a great opportunity for a cloud native separation of disk and storage, because the datafiles are immutable. We never need to perform in-place updates of data on disk, which works out well because object storage doesn’t allow that anyway. Compaction can run as a separate process and scale on its own right, which keeps the reads fast.</p>&#13;
<p>Another challenge with Kubernetes is choosing the right VM types and figuring out how to map Pods to them efficiently. Unfortunately, the Kubernetes APIs are decoupled from the underlying cloud provider capabilities. You have to do a lot of math in your head to set up quotas and node groups, and we haven’t even gotten to disks. There’s a massive market opportunity out there for someone who can solve this problem.</p>&#13;
<p>When you’re running a SaaS, the way you lower prices and keep margins is by being as efficient as possible. For us, this means multitenancy and the ability to shift resources between tenants based on usage. We use a giant, shared pool of Pods and resources, which enables us to move users and their data to different parts of the fleet. This allows us to provide a usage-based pricing model for developers who just want to use it and go, and it empowers them to build cool applications.</p>&#13;
<p>Zooming back out, Kubernetes did a great job with stateless services from the beginning, but stateful workloads are harder. People in Kubernetes want to solve this with changes to Kubernetes, and people who build infrastructure want to solve this in the infrastructure. We’ll get there eventually through a combination of the two. In the meantime, we’re circumventing the issue by using the immutable systems that work well on Kubernetes and moving the state out into object storage. This is the way open source technology works. People try things and make progress. Adopting a new architecture can be a big risk, but once you do, the payoff can be huge.</p>&#13;
</div></aside>&#13;
<p><a contenteditable="false" data-primary="" data-startref="ac_astra" data-type="indexterm" id="idm46183196068704"/><a contenteditable="false" data-primary="" data-startref="datas_ab" data-type="indexterm" id="idm46183196046224"/><a contenteditable="false" data-primary="" data-startref="knd_datas" data-type="indexterm" id="idm46183196021600"/>In this section, we’ve focused on the architecture Astra DB uses to provide a multitenant, serverless Cassandra that embodies both cloud native and Kubernetes native principles using a completely different style of deployment. This continues the tradition of the Amazon Dynamo and Google BigTable papers in generating public discussion around novel database architectures. In addition, several open source projects mentioned in this book including Cass Operator, K8ssandra, and Stargate trace their origins to Astra DB. A lot of innovation is going on in areas such as the core database, control plane, change data capture, streaming integration, data migration, and more, so look for more open source contributions and architecture proposals from this team in the future.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="What to Look for in a Kubernetes Native Database" data-type="sect1"><div class="sect1" id="what_to_look_for_in_a_kubernetes_native">&#13;
<h1>What to Look for in a Kubernetes Native Database</h1>&#13;
<p><a contenteditable="false" data-primary="Kubernetes native database" data-secondary="what to look for in" data-type="indexterm" id="knd_what"/>After everything you’ve learned in the past few chapters about what it takes to deploy and manage various databases on Kubernetes, we are in a great position to define what you should look for in a Kubernetes native database.</p>&#13;
<section data-pdf-bookmark="Basic Requirements" data-type="sect2"><div class="sect2" id="basic_requirements">&#13;
<h2>Basic Requirements</h2>&#13;
<p>Following our cloud native data principles, the following are a few areas that should be considered basic requirements:</p>&#13;
<dl>&#13;
<dt>Maximum leverage of Kubernetes APIs</dt>&#13;
<dd><p>The database should be as tightly integrated with Kubernetes APIs as possible (for example, using PersistentVolumes for both local and remote storage, using Services for routing rather than maintaining lists of IPs of other nodes, and so on). Kubernetes extension points described in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a> should be used to supplement built-in Kubernetes functionality.</p>&#13;
<p>In some areas, the existing Kubernetes APIs may not provide the exact behavior required for a given database or other application, as demonstrated by the creation of alternate StatefulSet implementations by the Vitess and TiDB projects. In these cases, every effort should be made to donate improvements back to the Kubernetes project.</p></dd>&#13;
<dt>Automated, declarative management via operators</dt>&#13;
<dd><p>Databases should be deployed and managed on Kubernetes using operators and custom resources. Operators should serve as the primary control plane elements for managing databases. While it’s arguably helpful to have command-line tools or <code>kubectl</code> extensions that allow DBAs to intervene manually to optimize database performance and fix issues, these are ultimately functions that should be performed by an operator as it achieves the higher levels of maturity discussed in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>.</p>&#13;
<p>The goal should be that all required changes to a database can be accomplished by updating the desired state in a custom resource and letting the operator handle the rest. We’ll be in a great place when we can configure a database in terms of service-level objectives such as latency, throughput, availability, and cost per unit. Operators can determine how many database nodes are needed, what compute and storage tiers to use, when to perform backups, and so on.</p></dd>&#13;
<dt>Observable through standard APIs</dt>&#13;
<dd><p>We’re beginning to see common expectations for observability for data infrastructure on Kubernetes in terms of the familiar triad of metrics, logs, and tracing. The Prometheus-Grafana stack is somewhat of a de facto standard for metrics collection and visualization, with exposure of metrics from database services using the Prometheus format as a minimum criteria. Projects providing Prometheus integration should be flexible enough to provide their own dedicated stack, or push metrics to an existing installation shared with other applications.</p>&#13;
<p>Logs from all database application containers should be pushed to standard output (stdout) using sidecars if necessary—so they can be collected by log aggregation services. While it may take longer to see adoption for tracing, the ability to follow individual client requests through application calls down into the database tier through APIs such as OpenTracing will be an extremely powerful debugging tool for future cloud native applications.</p></dd>&#13;
<dt>Secure by default</dt>&#13;
<dd><p>The Kubernetes project itself provides a great example of what it means to be secure by default—for example, by exposing access to ports on Pods and containers only when specifically enabled, and by providing primitives like Secrets that we can use to protect access to login credentials or sensitive configuration data.</p>&#13;
<p>Databases and other infrastructure need to make use of these tools and adopt industry standards and best practices for zero trust (including changing default administrator credentials), limiting exposure of application and management APIs. Exposed APIs should prefer encrypted protocols such as HTTPS. Data stored in PersistentVolumes should be encrypted, whether this encryption is performed by the application, the database, or the StorageClass provider. Audit logs should be provided as part of application logging, especially with respect to actions that configure user access.</p></dd>&#13;
</dl>&#13;
<p>To summarize, a Kubernetes native database is sympathetic to the way that Kubernetes works. It maximizes reuse of Kubernetes built-in capabilities instead of bringing along its own set of duplicative supporting infrastructure. The experience of using a Kubernetes native database is therefore very much like using Kubernetes itself.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Future of Kubernetes Native" data-type="sect2"><div class="sect2" id="the_future_of_kubernetes_native">&#13;
<h2>The Future of Kubernetes Native</h2>&#13;
<p>As these basic requirements and more advanced expectations for what it means to be Kubernetes native solidify, what comes next? We’re starting to see common patterns within projects deploying databases on Kubernetes that could point to where things are headed in the future. These are admittedly a bit fuzzier, but let’s try to bring a couple of them into focus.</p>&#13;
<section data-pdf-bookmark="Scalability through multidimensional architectures" data-type="sect3"><div class="sect3" id="scalability_through_multidimensional_ar">&#13;
<h3>Scalability through multidimensional architectures</h3>&#13;
<p><a contenteditable="false" data-primary="multidimensional architectures, scalability through" data-type="indexterm" id="idm46183196141984"/><a contenteditable="false" data-primary="scalability" data-secondary="through multidimensional architectures" data-type="indexterm" id="idm46183196142160"/>You may have noticed the repetition of several terms throughout the past few chapters such as <em>multicluster</em>, <em>multitenancy</em>, <em>microservices</em>, and <em>serverless</em>. A common thread uniting these terms is that they represent architectural approaches to scalability, as shown in <a data-type="xref" href="#architectural_approaches_for_scaling_in">Figure 7-6</a>.</p>&#13;
<figure class="width-60"><div class="figure" id="architectural_approaches_for_scaling_in">&#13;
<img alt="Architectural approaches for scaling in multiple dimensions" src="assets/mcdk_0706.png"/>&#13;
<h6><span class="label">Figure 7-6. </span>Architectural approaches for scaling in multiple dimensions</h6>&#13;
</div></figure>&#13;
<p>Consider how each of these approaches provides an independent axis for scalability. The visualization in <a data-type="xref" href="#architectural_approaches_for_scaling_in">Figure 7-6</a> depicts the impact of your application as a three-dimensional surface that grows as you scale along each axis:</p>&#13;
<dl>&#13;
<dt>Microservice architectures</dt>&#13;
<dd><a contenteditable="false" data-primary="architecture" data-secondary="microservice" data-type="indexterm" id="idm46183195961952"/><a contenteditable="false" data-primary="microservices" data-type="indexterm" id="idm46183195961472"/>Microservice architectures break the various functions of a database into independently scalable services. The serverless approach builds on this, encouraging the isolation of persistent state to a limited number of stateful services or even external services as much as possible. Kubernetes storage APIs in the PersistentVolume subsystem make it possible to leverage both local and networked storage options. These trends allow a true separation of compute and storage, and scale these resources independently.</dd>&#13;
<dt>Multicluster</dt>&#13;
<dd><a contenteditable="false" data-primary="multiclusters" data-secondary="about" data-type="indexterm" id="idm46183196016160"/><em>Multicluster</em> refers to the ability to scale an application across multiple Kubernetes clusters. Along with related terms like <em>multiregion</em>, <em>multi-datacenter</em>, and <em>multicloud</em>, this implies expanding the geographic footprint of the capabilities provided across potentially heterogeneous environments. This distribution of capability has positive implications for meeting users where they are with minimum latency, cloud provider cost optimization, and disaster recovery. As we discussed in <a data-type="xref" href="ch06.html#integrating_data_infrastructure_in_a_ku">Chapter 6</a>, Kubernetes has historically not been as strong in its support for cross-cluster networking and service discovery. It will be interesting to track how databases and other applications take advantage of expected advances in Kubernetes federation in the coming years.</dd>&#13;
<dt>Multitenancy</dt>&#13;
<dd><a contenteditable="false" data-primary="multitenancy" data-type="indexterm" id="idm46183196013920"/>This is the ability to share infrastructure between multiple users to achieve the most efficient use of resources. As the public cloud providers have demonstrated in their IaaS offerings, a multitenant approach can be very effective at providing users a low-cost, low-risk access to infrastructure for innovative new projects, and then providing additional resources as these applications grow. Adopting a multitenant approach for data infrastructure has great potentiial value as well, so long as security guarantees are properly met and there is a seamless transition path to dedicated infrastructure for high-volume users before they become “noisy neighbors.” At this point in time, Kubernetes does not provide explicit support for multitenancy, although Namespaces can be a useful tool for providing dedicated resources for specific users.</dd>&#13;
</dl>&#13;
<p>While you may not have immediate need for all three of these axes of scalability for applications or data infrastructure you’re building, consider how growing in each of them can enhance the overall value you’re offering the world.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Community-focused innovation through open source and cloud services" data-type="sect3"><div class="sect3" id="community_focused_innovation_through_op">&#13;
<h3>Community-focused innovation through open source and cloud services</h3>&#13;
<p><a contenteditable="false" data-primary="cloud services, community-focused innovation through" data-type="indexterm" id="idm46183196162144"/><a contenteditable="false" data-primary="open source services, community-focused innovation through" data-type="indexterm" id="idm46183196558464"/>Another pattern you may have noticed in our narrative is the continual innovation loop between open source database projects and DBaaS offerings. PingCAP took the open source MySQL and ClickHouse databases, created a database service leveraging Kubernetes to help it manage the databases at scale, and then released open source projects including TiDB and TiFlash. DataStax took open source Cassandra, factored it into microservices, added an API layer, and deployed it on Kubernetes for its Astra DB, and has created multiple open source projects including Cass Operator, K8ssandra, and Stargate. In the spirit of Dynamo, BigTable, Calvin and other papers, these companies have open source architectures as well.</p>&#13;
<p>This innovation loop mirrors that of the larger Kubernetes community, in which the major cloud providers and storage vendors have helped drive the maturation of the core Kubernetes control plane and PersistentVolume subsystem, respectively. It’s interesting to observe that the highest  momentum and fastest cycle time occurs within innovation loops that center around cloud services, rather than around the classic open core model focused on enterprise versions of open source projects.</p>&#13;
<p>As a software vendor, providing a cloud service allows you to iterate and evaluate new architectures and features more quickly. Flowing these innovations back to open source allows you to grow adoption by supporting a flexible consumption model. Both “run it yourself” and “rent it from us” become legitimate deployment options for your customers, with the ability to flex between approaches for different use cases. Customers gain confidence in the overall maturity and security of your technology, knowing that the open source version they can inspect and contribute to is largely the same as what you are running in your DBaaS.</p>&#13;
<p>A final side effect of these innovation trends is an implicit pull toward proven architectures and components. Consider these examples:</p>&#13;
<ul>&#13;
<li><p>etcd is used as a metadata store across multiple projects we’ve examined in this book, including Vitess and Astra DB.</p></li>&#13;
<li><p>TiDB leverages the architecture of F1, implemented the Raft consensus protocol, and extended the ClickHouse columnar store.</p></li>&#13;
<li><p>Astra DB leverages both the PersistentVolume subsystem and S3-compliant object storage.</p></li>&#13;
</ul>&#13;
<p><a contenteditable="false" data-primary="" data-startref="knd_what" data-type="indexterm" id="idm46183196047344"/>Instead of inventing new technologies to solve problems like metadata management and distributed transactions, these projects are investing their innovation in new features, developer experience, and the scalability axes we’ve examined in this chapter.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000006">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, we’ve taken a deep look at TiDB and Astra DB to search out what makes them Kubernetes native. What was the point of this exercise? Our hope is that this analysis provides a deeper understanding to help consumers ask more insightful questions about the data infrastructure they are consuming, and to help those building data infrastructure and ecosystems to create technology that meets those expectations. We believe that data infrastructure that is not only cloud native but also Kubernetes native will lead to the best outcomes for everyone in terms of performance, availability, and cost.</p>&#13;
</div></section>&#13;
</div></section></body></html>