<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 16. Choosing a Shard Key"><div class="chapter" id="chapter-shardkey"><h1><span class="label">Chapter 16. </span>Choosing a Shard Key</h1><p>The most important task when using sharding is choosing how your data
  will be distributed. To make intelligent choices about this, you have to
  understand how <span class="keep-together">MongoDB</span> distributes data. This chapter helps you make a good
  choice of shard key by covering:</p><ul><li><p>How to decide among multiple possible shard keys</p></li><li><p>Shard keys for several use cases</p></li><li><p>What you can’t use as a shard key</p></li><li><p>Some alternative strategies if you want to customize how data is
      distributed</p></li><li><p>How to manually shard your data</p></li></ul><p>It assumes that you understand the basic components of sharding as
  covered in the previous two chapters.</p><section data-type="sect1" data-pdf-bookmark="Taking Stock of Your Usage"><div class="sect1" id="sect1_zzz_0003"><h1>Taking Stock of Your Usage</h1><p>When<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="initial steps" id="idm45882345452712"/> you shard a collection you choose a field or two to use to
    split up the data. This key (or keys) is called a <em>shard
    key</em>. Once you shard a collection you cannot change your shard
    key, so it is important to choose correctly.</p><p>To choose a good shard key, you need to understand your workload and
    how your shard key is going to distribute your application’s requests.
    This can be difficult to picture, so try to work out some examples—or,
    even better, try it out on a backup dataset with sample traffic. This
    section has lots of diagrams and explanations, but there is no substitute
    for trying it on your own data.</p><p>For each collection that you’re planning to shard, start by
    answering the following questions:</p><ul><li><p>How many shards are you planning to grow to? A three-shard
        cluster has a great deal more flexibility than a thousand-shard
        cluster. As a cluster gets larger, you should not plan to fire off
        queries that can hit all shards, so almost all queries must include
        the shard key.</p></li><li><p>Are you sharding to decrease read or write latency<a data-type="indexterm" data-primary="latency" data-secondary="defined" id="idm45882345448280"/>? (<span class="firstterm">Latency</span> refers to how long
        something takes; e.g., a write takes 20 ms, but you need it to take 10
        ms.) Decreasing write latency usually involves sending requests to
        geographically closer or more powerful machines.</p></li><li><p>Are you sharding to increase read or write throughput?
        (<span class="firstterm">Throughput</span> refers to how many requests the
        cluster can handle at the same time; e.g., the cluster can do 1,000
        writes in 20 ms, but you need it to do 5,000 writes in 20 ms.)
        Increasing throughput usually involves adding more parallelization and
        making sure that requests are distributed evenly across the
        cluster.</p></li><li><p>Are you sharding to increase system resources (e.g., give
        MongoDB more RAM per GB of data)? If so, you want to keep the working
        set size as small as possible.</p></li></ul><p>Use these answers to evaluate the following shard key descriptions
    and decide whether the shard key you’re considering would work well in
    your situation. Does it give you the targeted queries that you need? Does
    it change the throughput or latency of your system in the ways you need?
    If you need a compact working set, does it provide that?</p></div></section><section data-type="sect1" data-pdf-bookmark="Picturing Distributions"><div class="sect1" id="idm45882345443368"><h1>Picturing Distributions</h1><p>The<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="distribution types" data-tertiary="possibly types" id="idm45882345442488"/> most common ways people choose to split their data are via
    ascending, random, and location-based keys. There are other types of keys
    that could be used, but most use cases fall into one of these categories.
    The different types of distributions are discussed in the following
    sections.</p><section data-type="sect2" data-pdf-bookmark="Ascending Shard Keys"><div class="sect2" id="sect2-hotspot"><h2>Ascending Shard Keys</h2><p>Ascending<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="distribution types" data-tertiary="ascending shard keys" id="idm45882345439208"/> shard keys are generally something like a <code>"date"</code> field or <code>ObjectId</code>—anything that steadily increases over
      time. An autoincrementing primary key is another example of an ascending
      field, albeit one that doesn’t show up in MongoDB much (unless you’re
      importing from another database).</p><p>Suppose that we shard on an ascending field, like <code>"_id"</code> on a collection using <span class="keep-together"><code>ObjectId</code>s</span>. If we shard on <code>"_id"</code>, then the data will be split into chunks
      of <code>"_id"</code> ranges, as in <a data-type="xref" href="#ascending-shard-key">Figure 16-1</a>. These chunks will be distributed
      across our sharded cluster of, let’s say, three shards, as shown in
      <a data-type="xref" href="#ascending-shard-dist">Figure 16-2</a>.</p><figure style="float: 0"><div id="ascending-shard-key" class="figure"><img src="Images/mdb3_1601.png" width="755" height="1354"/><h6><span class="label">Figure 16-1. </span>The collection is split into ranges of ObjectIds; each range is
        a chunk</h6></div></figure><p>Suppose we create a new document. Which chunk will it be in? The
      answer is the chunk with the range <code>ObjectId("5112fae0b4a4b396ff9d0ee5")</code> through
      <code>$maxKey</code><a data-type="indexterm" data-primary="$maxKey" id="idm45882345428968"/>. This is called the <em>max
      chunk</em>,<a data-type="indexterm" data-primary="chunks" data-secondary="max chunk" id="idm45882345427560"/><a data-type="indexterm" data-primary="max chunk" id="idm45882345426424"/> as it is the chunk containing <code>$maxKey</code>.</p><p>If we insert another document, it will also be in the max chunk.
      In fact, every subsequent insert will be into the max chunk! Every
      insert’s <code>"_id"</code> field will be closer
      to infinity than the previous one (because
      <code class="classname">ObjectId</code>s are always ascending), so they will all
      go into the max chunk.</p><figure style="float: 0"><div id="ascending-shard-dist" class="figure"><img src="Images/mdb3_1602.png" width="760" height="1596"/><h6><span class="label">Figure 16-2. </span>Chunks are distributed across shards in a random order</h6></div></figure><p>This has a couple of interesting (and often undesirable)
      properties. First, all of your writes will be routed to one shard
      (<em>shard0002</em>, in this case). This chunk will be the
      only one growing and splitting, as it is the only one that receives
      inserts. As you insert data, new chunks will “fall off” of this chunk,
      as shown in <a data-type="xref" href="#ascending-butt">Figure 16-3</a>.</p><figure style="float: 0"><div id="ascending-butt" class="figure"><img src="Images/mdb3_1603.png" width="1444" height="621"/><h6><span class="label">Figure 16-3. </span>The max chunk continues growing and being split into multiple
        chunks</h6></div></figure><p>This pattern often makes it more difficult for MongoDB to keep
      chunks evenly balanced because all the chunks are being created by one
      shard. Therefore, MongoDB must constantly move chunks to other shards
      instead of correcting the small imbalances that might occur in more
      evenly distributed systems.</p><div data-type="note" epub:type="note"><h6>Note</h6><p>In MongoDB 4.2, the move of the autosplit functionality to the
        shard primary <em>mongod</em> added top chunk optimization
        to address the ascending shard key pattern. The balancer<a data-type="indexterm" data-primary="balancer" data-secondary="purpose of" id="idm45882345416744"/> will decide in which other shard to place the top
        chunk. This helps avoid a situation in which all new chunks are
        created on just one shard.</p></div></div></section><section data-type="sect2" data-pdf-bookmark="Randomly Distributed Shard Keys"><div class="sect2" id="idm45882345440328"><h2>Randomly Distributed Shard Keys</h2><p>At<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="distribution types" data-tertiary="randomly distributed shard keys" id="idm45882345414584"/> the other end of the spectrum are randomly distributed
      shard keys. Randomly distributed keys could be usernames, email
      addresses, UUIDs, MD5 hashes, or any other key that has no identifiable
      pattern in your dataset.</p><p>Suppose the shard key is a random number between 0 and 1. We’ll
      end up with a random distribution of chunks on the various shards, as
      shown in <a data-type="xref" href="#random-dist">Figure 16-4</a>.</p><figure style="float: 0"><div id="random-dist" class="figure"><img src="Images/mdb3_1604.png" width="722" height="1595"/><h6><span class="label">Figure 16-4. </span>As in the previous section, chunks are distributed randomly
        around the cluster</h6></div></figure><p>As more data is inserted, the data’s random nature means that
      inserts should hit every chunk fairly evenly. You can prove this to
      yourself by inserting 10,000 documents and seeing where they end
      up:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="kd">var</code> <code class="nx">servers</code> <code class="o">=</code> <code class="p">{}</code>
<code class="o">&gt;</code> <code class="kd">var</code> <code class="nx">findShard</code> <code class="o">=</code> <code class="kd">function</code> <code class="p">(</code><code class="nx">id</code><code class="p">)</code> <code class="p">{</code>
<code class="p">...</code>     <code class="kd">var</code> <code class="nx">explain</code> <code class="o">=</code> <code class="nx">db</code><code class="p">.</code><code class="nx">random</code><code class="p">.</code><code class="nx">find</code><code class="p">({</code><code class="nx">_id</code><code class="o">:</code><code class="nx">id</code><code class="p">}).</code><code class="nx">explain</code><code class="p">();</code>
<code class="p">...</code>     <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="k">in</code> <code class="nx">explain</code><code class="p">.</code><code class="nx">shards</code><code class="p">)</code> <code class="p">{</code>
<code class="p">...</code>         <code class="kd">var</code> <code class="nx">server</code> <code class="o">=</code> <code class="nx">explain</code><code class="p">.</code><code class="nx">shards</code><code class="p">[</code><code class="nx">i</code><code class="p">][</code><code class="mi">0</code><code class="p">];</code>
<code class="p">...</code>         <code class="k">if</code> <code class="p">(</code><code class="nx">server</code><code class="p">.</code><code class="nx">n</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)</code> <code class="p">{</code>
<code class="p">...</code>             <code class="k">if</code> <code class="p">(</code><code class="nx">server</code><code class="p">.</code><code class="nx">server</code> <code class="k">in</code> <code class="nx">servers</code><code class="p">)</code> <code class="p">{</code>
<code class="p">...</code>                 <code class="nx">servers</code><code class="p">[</code><code class="nx">server</code><code class="p">.</code><code class="nx">server</code><code class="p">]</code><code class="o">++</code><code class="p">;</code>
<code class="p">...</code>             <code class="p">}</code> <code class="k">else</code> <code class="p">{</code>
<code class="p">...</code>                 <code class="nx">servers</code><code class="p">[</code><code class="nx">server</code><code class="p">.</code><code class="nx">server</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code><code class="p">;</code>
<code class="p">...</code>             <code class="p">}</code>
<code class="p">...</code>         <code class="p">}</code>
<code class="p">...</code>     <code class="p">}</code>
<code class="p">...</code> <code class="p">}</code>
<code class="o">&gt;</code> <code class="k">for</code> <code class="p">(</code><code class="kd">var</code> <code class="nx">i</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="nx">i</code> <code class="o">&lt;</code> <code class="mi">10000</code><code class="p">;</code> <code class="nx">i</code><code class="o">++</code><code class="p">)</code> <code class="p">{</code>
<code class="p">...</code>     <code class="kd">var</code> <code class="nx">id</code> <code class="o">=</code> <code class="nx">ObjectId</code><code class="p">();</code>
<code class="p">...</code>     <code class="nx">db</code><code class="p">.</code><code class="nx">random</code><code class="p">.</code><code class="nx">insert</code><code class="p">({</code><code class="s2">"_id"</code> <code class="o">:</code> <code class="nx">id</code><code class="p">,</code> <code class="s2">"x"</code> <code class="o">:</code> <code class="nb">Math</code><code class="p">.</code><code class="nx">random</code><code class="p">()});</code>
<code class="p">...</code>     <code class="nx">findShard</code><code class="p">(</code><code class="nx">id</code><code class="p">);</code>
<code class="p">...</code> <code class="p">}</code>
<code class="o">&gt;</code> <code class="nx">servers</code>
<code class="p">{</code>
    <code class="s2">"spock:30001"</code> <code class="o">:</code> <code class="mi">2942</code><code class="p">,</code>
    <code class="s2">"spock:30002"</code> <code class="o">:</code> <code class="mi">4332</code><code class="p">,</code>
    <code class="s2">"spock:30000"</code> <code class="o">:</code> <code class="mi">2726</code>
<code class="p">}</code></pre><p>As writes are randomly distributed, the shards should grow at
      roughly the same rate, limiting the number of migrates that need to
      occur.</p><p>The only downside to randomly distributed shard keys is that
      MongoDB isn’t efficient at randomly accessing data beyond the size of
      RAM. However, if you have the capacity or don’t mind the performance
      hit, random keys nicely distribute load across your cluster.</p></div></section><section data-type="sect2" data-pdf-bookmark="Location-Based Shard Keys"><div class="sect2" id="sect2-shard-tags"><h2>Location-Based Shard Keys</h2><p>Location-based shard keys<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="distribution types" data-tertiary="location-based shard keys" id="idm45882345405048"/> may be things like a user’s IP, latitude and longitude,
      or address. They’re not necessarily related to a physical location
      field: the “location” might be a more abstract way that data should be
      grouped together. In any case, a location-based key is a key where
      documents with some similarity fall into a range based on this field.
      This can be handy for both putting data close to its users and keeping
      related data together on disk. It may also be a legal requirement to
      remain compliant with GDPR or other similar data privacy legislation.
      MongoDB uses Zoned Sharding to manage this.</p><div data-type="note" epub:type="note"><h6>Note</h6><p>In MongoDB 4.0.3+, you can define the zones and the zone ranges
        prior to sharding a collection, which populates chunks for both the
        zone ranges and for the shard key values as well as performing an
        initial chunk distribution of these. This greatly reduces the
        complexity for sharded zone setup.</p></div><p>For example, suppose we have a collection of documents that are
      sharded on IP address. Documents will be organized into chunks based on
      their IPs and randomly spread across the cluster, as shown in <a data-type="xref" href="#shard-2-3-0">Figure 16-5</a>.</p><figure style="float: 0"><div id="shard-2-3-0" class="figure"><img src="Images/mdb3_1605.png" width="1147" height="423"/><h6><span class="label">Figure 16-5. </span>A sample distribution of chunks in the IP address
        collection</h6></div></figure><p>If we wanted certain chunk ranges to be attached to certain
      shards, we could <span class="firstterm">zone</span> these shards and then
      assign chunk ranges to each zone. In this example, suppose that we
      wanted to keep certain IP blocks on certain shards: say, 56.*.*.* (the
      United States Postal Service’s IP block) on
      <em>shard0000</em> and 17.*.*.* (Apple’s IP block) on either
      <em>shard0000</em> or <em>shard0002</em>. We do
      not care where the other IPs live. We could request that the
      balancer<a data-type="indexterm" data-primary="balancer" data-secondary="requesting assignments from" id="idm45882345235496"/> do this by setting up zones:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0000"</code><code class="p">,</code> <code class="s2">"USPS"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0000"</code><code class="p">,</code> <code class="s2">"Apple"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0002"</code><code class="p">,</code> <code class="s2">"Apple"</code><code class="p">)</code></pre><p>Next, we create the rules:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">updateZoneKeyRange</code><code class="p">(</code><code class="s2">"test.ips"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"ip"</code> <code class="o">:</code> <code class="s2">"056.000.000.000"</code><code class="p">},</code> 
<code class="p">...</code> <code class="p">{</code><code class="s2">"ip"</code> <code class="o">:</code> <code class="s2">"057.000.000.000"</code><code class="p">},</code> <code class="s2">"USPS"</code><code class="p">)</code></pre><p>This attaches all IPs greater than or equal to 56.0.0.0 and less
      than 57.0.0.0 to the shard zoned as <code>"USPS"</code>. Next, we add a rule for Apple:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">updateZoneKeyRange</code><code class="p">(</code><code class="s2">"test.ips"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"ip"</code> <code class="o">:</code> <code class="s2">"017.000.000.000"</code><code class="p">},</code> 
<code class="p">...</code> <code class="p">{</code><code class="s2">"ip"</code> <code class="o">:</code> <code class="s2">"018.000.000.000"</code><code class="p">},</code> <code class="s2">"Apple"</code><code class="p">)</code></pre><p>When the balancer moves chunks, it will attempt to move chunks
      with those ranges to those shards. Note that this process is not
      immediate. Chunks that were not covered by a zone key range will be
      moved around normally. The balancer will continue attempting to
      distribute chunks evenly among shards.</p></div></section></div></section><section class="pagebreak-before" data-type="sect1" data-pdf-bookmark="Shard Key Strategies"><div class="sect1" id="idm45882345406104"><h1>Shard Key Strategies</h1><p>This section presents a number of shard key options for various
    types of applications.</p><section data-type="sect2" data-pdf-bookmark="Hashed Shard Key"><div class="sect2" id="idm45882345192600"><h2>Hashed Shard Key</h2><p>For<a data-type="indexterm" data-primary="hashed shard keys" id="hashshardkey16"/><a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="shard key strategies" data-tertiary="hashed shard keys" id="idm45882345149000"/> loading data as fast as possible, hashed shard keys are
      the best option. A hashed shard key can make any field randomly
      distributed, so it is a good choice if you’re going to be using an
      ascending key in a lot of queries but want writes to be randomly
      distributed.</p><p>The trade-off is that you can never do a targeted range query with
      a hashed shard key. If you will not be doing range queries, though,
      hashed shard keys are a good option.</p><p>To<a data-type="indexterm" data-primary="indexes" data-secondary="hashed" id="idm45882345146488"/> create a hashed shard key, first create a hashed
      index:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">db</code><code class="p">.</code><code class="nx">users</code><code class="p">.</code><code class="nx">createIndex</code><code class="p">({</code><code class="s2">"username"</code> <code class="o">:</code> <code class="s2">"hashed"</code><code class="p">})</code></pre><p>Next, shard the collection with:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">shardCollection</code><code class="p">(</code><code class="s2">"app.users"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"username"</code> <code class="o">:</code> <code class="s2">"hashed"</code><code class="p">})</code>
<code class="p">{</code> <code class="s2">"collectionsharded"</code> <code class="o">:</code> <code class="s2">"app.users"</code><code class="p">,</code> <code class="s2">"ok"</code> <code class="o">:</code> <code class="mi">1</code> <code class="p">}</code></pre><p>If you create a hashed shard key on a nonexistent collection,
      <code>shardCollection<a data-type="indexterm" data-primary="shardCollection command" id="idm45882344981448"/></code> behaves interestingly: it assumes that you want
      evenly distributed chunks, so it immediately creates a bunch of empty
      chunks and distributes them around your cluster. For example, suppose
      our cluster looked like this before creating the hashed shard
      key:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">status</code><code class="p">()</code>
<code class="o">---</code> <code class="nx">Sharding</code> <code class="nx">Status</code> <code class="o">---</code> 
  <code class="nx">sharding</code> <code class="nx">version</code><code class="o">:</code> <code class="p">{</code> <code class="s2">"_id"</code> <code class="o">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"version"</code> <code class="o">:</code> <code class="mi">3</code> <code class="p">}</code>
  <code class="nx">shards</code><code class="o">:</code>
        <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"shard0000"</code><code class="p">,</code>  <code class="s2">"host"</code> <code class="o">:</code> <code class="s2">"localhost:30000"</code> <code class="p">}</code>
        <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"shard0001"</code><code class="p">,</code>  <code class="s2">"host"</code> <code class="o">:</code> <code class="s2">"localhost:30001"</code> <code class="p">}</code>
        <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"shard0002"</code><code class="p">,</code>  <code class="s2">"host"</code> <code class="o">:</code> <code class="s2">"localhost:30002"</code> <code class="p">}</code>
  <code class="nx">databases</code><code class="o">:</code>
        <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"admin"</code><code class="p">,</code>  <code class="s2">"partitioned"</code> <code class="o">:</code> <code class="kc">false</code><code class="p">,</code>  <code class="s2">"primary"</code> <code class="o">:</code> <code class="s2">"config"</code> <code class="p">}</code>
        <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"test"</code><code class="p">,</code>  <code class="s2">"partitioned"</code> <code class="o">:</code> <code class="kc">true</code><code class="p">,</code>  <code class="s2">"primary"</code> <code class="o">:</code> <code class="s2">"shard0001"</code> <code class="p">}</code></pre><p>Immediately after <code>shardCollection</code> returns there are two chunks
      on each shard, evenly distributing the key space across the
      cluster:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">status</code><code class="p">()</code>
<code class="o">---</code> <code class="nx">Sharding</code> <code class="nx">Status</code> <code class="o">---</code> 
  <code class="nx">sharding</code> <code class="nx">version</code><code class="o">:</code> <code class="p">{</code> <code class="s2">"_id"</code> <code class="o">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"version"</code> <code class="o">:</code> <code class="mi">3</code> <code class="p">}</code>
  <code class="nx">shards</code><code class="o">:</code>
    <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"shard0000"</code><code class="p">,</code>  <code class="s2">"host"</code> <code class="o">:</code> <code class="s2">"localhost:30000"</code> <code class="p">}</code>
    <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"shard0001"</code><code class="p">,</code>  <code class="s2">"host"</code> <code class="o">:</code> <code class="s2">"localhost:30001"</code> <code class="p">}</code>
    <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"shard0002"</code><code class="p">,</code>  <code class="s2">"host"</code> <code class="o">:</code> <code class="s2">"localhost:30002"</code> <code class="p">}</code>
  <code class="nx">databases</code><code class="o">:</code>
    <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"admin"</code><code class="p">,</code>  <code class="s2">"partitioned"</code> <code class="o">:</code> <code class="kc">false</code><code class="p">,</code>  <code class="s2">"primary"</code> <code class="o">:</code> <code class="s2">"config"</code> <code class="p">}</code>
    <code class="p">{</code>  <code class="s2">"_id"</code> <code class="o">:</code> <code class="s2">"test"</code><code class="p">,</code>  <code class="s2">"partitioned"</code> <code class="o">:</code> <code class="kc">true</code><code class="p">,</code>  <code class="s2">"primary"</code> <code class="o">:</code> <code class="s2">"shard0001"</code> <code class="p">}</code>
        <code class="nx">test</code><code class="p">.</code><code class="nx">foo</code>
            <code class="nx">shard</code> <code class="nx">key</code><code class="o">:</code> <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="s2">"hashed"</code> <code class="p">}</code>
            <code class="nx">chunks</code><code class="o">:</code>
                <code class="nx">shard0000</code>       <code class="mi">2</code>
                <code class="nx">shard0001</code>       <code class="mi">2</code>
                <code class="nx">shard0002</code>       <code class="mi">2</code>
            <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="p">{</code> <code class="s2">"$MinKey"</code> <code class="o">:</code> <code class="kc">true</code> <code class="p">}</code> <code class="p">}</code> 
                <code class="o">--&gt;&gt;</code> <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"-6148914691236517204"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="nx">on</code> <code class="o">:</code> <code class="nx">shard0000</code> <code class="p">{</code> <code class="s2">"t"</code> <code class="o">:</code> <code class="mi">3000</code><code class="p">,</code> <code class="s2">"i"</code> <code class="o">:</code> <code class="mi">2</code> <code class="p">}</code> 
            <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"-6148914691236517204"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="o">--&gt;&gt;</code> <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"-3074457345618258602"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="nx">on</code> <code class="o">:</code> <code class="nx">shard0000</code> <code class="p">{</code> <code class="s2">"t"</code> <code class="o">:</code> <code class="mi">3000</code><code class="p">,</code> <code class="s2">"i"</code> <code class="o">:</code> <code class="mi">3</code> <code class="p">}</code> 
            <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"-3074457345618258602"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="o">--&gt;&gt;</code> <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code> <code class="p">}</code> 
                <code class="nx">on</code> <code class="o">:</code> <code class="nx">shard0001</code> <code class="p">{</code> <code class="s2">"t"</code> <code class="o">:</code> <code class="mi">3000</code><code class="p">,</code> <code class="s2">"i"</code> <code class="o">:</code> <code class="mi">4</code> <code class="p">}</code> 
            <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code> <code class="p">}</code> 
                <code class="o">--&gt;&gt;</code> <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"3074457345618258602"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="nx">on</code> <code class="o">:</code> <code class="nx">shard0001</code> <code class="p">{</code> <code class="s2">"t"</code> <code class="o">:</code> <code class="mi">3000</code><code class="p">,</code> <code class="s2">"i"</code> <code class="o">:</code> <code class="mi">5</code> <code class="p">}</code> 
            <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"3074457345618258602"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="o">--&gt;&gt;</code> <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"6148914691236517204"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="nx">on</code> <code class="o">:</code> <code class="nx">shard0002</code> <code class="p">{</code> <code class="s2">"t"</code> <code class="o">:</code> <code class="mi">3000</code><code class="p">,</code> <code class="s2">"i"</code> <code class="o">:</code> <code class="mi">6</code> <code class="p">}</code> 
            <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"6148914691236517204"</code><code class="p">)</code> <code class="p">}</code> 
                <code class="o">--&gt;&gt;</code> <code class="p">{</code> <code class="s2">"username"</code> <code class="o">:</code> <code class="p">{</code> <code class="s2">"$MaxKey"</code> <code class="o">:</code> <code class="kc">true</code> <code class="p">}</code> <code class="p">}</code> 
                <code class="nx">on</code> <code class="o">:</code> <code class="nx">shard0002</code> <code class="p">{</code> <code class="s2">"t"</code> <code class="o">:</code> <code class="mi">3000</code><code class="p">,</code> <code class="s2">"i"</code> <code class="o">:</code> <code class="mi">7</code> <code class="p">}</code></pre><p>Note that there are no documents in the collection yet, but when
      you start inserting them, writes should be evenly distributed across the
      shards from the get-go. Ordinarily, you would have to wait for chunks to
      grow, split, and move to start writing to other shards. With this
      automatic priming, you’ll immediately have chunk ranges on all
      shards.</p><div data-type="note" epub:type="note"><h6>Note</h6><p>There<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="limitations on shard keys" id="idm45882344808568"/> are some limitations on what your shard key can be if
        you’re using a hashed shard key. First, you cannot use the
        <code class="option">unique</code> option. As with other shard keys, you cannot
        use array fields. Finally, be aware that floating-point values will be
        rounded to whole numbers before hashing, so 1 and 1.999999 will both
        be hashed to the same value.</p></div></div></section><section data-type="sect2" data-pdf-bookmark="Hashed Shard Keys for GridFS"><div class="sect2" id="idm45882345150792"><h2>Hashed Shard Keys for GridFS</h2><p>Before<a data-type="indexterm" data-primary="GridFS" data-secondary="hashed shard keys for" id="idm45882344805624"/><a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="shard key strategies" data-tertiary="hashed shard keys for GridFS" id="idm45882344436072"/> attempting to shard GridFS collections, make sure that
      you understand how GridFS stores data (see <a data-type="xref" href="ch06.xhtml#chapter-idx-types">Chapter 6</a> for an explanation).</p><p>In<a data-type="indexterm" data-primary="chunks" data-secondary="sharding chunks versus GridFS chunks" id="idm45882344433176"/> the following explanation, the term “chunks” is
      overloaded since GridFS splits files into chunks and sharding splits
      collections into chunks. Thus, the two types of chunks are referred to
      as “GridFS chunks” and “sharding chunks.”</p><p>GridFS collections are generally excellent candidates for
      sharding, as they contain massive amounts of file data. However, neither
      of the indexes that are automatically created on <em class="filename">fs.chunks<a data-type="indexterm" data-primary="fs.chunks collection" id="idm45882344430376"/></em> are particularly good shard keys: <code>{"_id" : 1}</code> is an ascending key and <code>{"files_id" : 1, "n" : 1}</code> picks up <em class="filename">fs.files</em>’s <code>"_id"</code> field, so it is also an ascending
      key.</p><p>However, if you create a hashed index on the <code>"files_id"</code> field, each file will be randomly
      distributed across the cluster, and a file will always be contained in a
      single chunk. This is the best of both worlds: writes will go to all
      shards evenly and reading a file’s data will only ever have to hit a
      single shard.</p><p>To set this up, you must create a new index on <code>{"files_id" : "hashed"}</code> (as of this writing,
      <em class="filename">mongos</em> cannot use a subset of the
      compound index as a shard key). Then shard the collection on this
      field:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">db</code><code class="p">.</code><code class="nx">fs</code><code class="p">.</code><code class="nx">chunks</code><code class="p">.</code><code class="nx">ensureIndex</code><code class="p">({</code><code class="s2">"files_id"</code> <code class="o">:</code> <code class="s2">"hashed"</code><code class="p">})</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">shardCollection</code><code class="p">(</code><code class="s2">"test.fs.chunks"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"files_id"</code> <code class="o">:</code> <code class="s2">"hashed"</code><code class="p">})</code>
<code class="p">{</code> <code class="s2">"collectionsharded"</code> <code class="o">:</code> <code class="s2">"test.fs.chunks"</code><code class="p">,</code> <code class="s2">"ok"</code> <code class="o">:</code> <code class="mi">1</code> <code class="p">}</code></pre><p>As a side note, the <em class="filename">fs.files</em>
      collection<a data-type="indexterm" data-primary="fs.files collection" id="idm45882344386520"/> may or may not need to be sharded, as it will be much
      smaller than <em class="filename">fs.chunks</em>. You can
      shard it if you would like, but it is not likely to be<a data-type="indexterm" data-startref="hashshardkey16" id="idm45882344384968"/>
      necessary.</p></div></section><section data-type="sect2" data-pdf-bookmark="The Firehose Strategy"><div class="sect2" id="idm45882344384008"><h2>The Firehose Strategy</h2><p>If<a data-type="indexterm" data-primary="firehose strategy" id="idm45882344382792"/><a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="shard key strategies" data-tertiary="firehose strategy" id="idm45882344381928"/> you have some servers that are more powerful than others,
      you might want to let them handle proportionally more load than your
      less-powerful servers. For example, suppose you have one shard that can
      handle 10 times the load of your other machines. Luckily, you have 10
      other shards. You could force all inserts to go to the more powerful
      shard, and then allow the balancer<a data-type="indexterm" data-primary="balancer" data-secondary="firehose strategy and" id="idm45882344380008"/> to move older chunks to the other shards. This<a data-type="indexterm" data-primary="latency" data-secondary="firehose strategy and" id="idm45882344378776"/> would give lower-latency writes.</p><p>To use this strategy, we have to pin the highest chunk to the more
      powerful shard. First, we zone this shard:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"&lt;shard-name&gt;"</code><code class="p">,</code> <code class="s2">"10x"</code><code class="p">)</code></pre><p>Then we pin the current value of the ascending key through
      infinity to that shard, so all new writes go to it:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">updateZoneKeyRange</code><code class="p">(</code><code class="s2">"&lt;dbName.collName&gt;"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"_id"</code> <code class="o">:</code> <code class="nx">ObjectId</code><code class="p">()},</code> 
<code class="p">...</code> <code class="p">{</code><code class="s2">"_id"</code> <code class="o">:</code> <code class="nx">MaxKey</code><code class="p">},</code> <code class="s2">"10x"</code><code class="p">)</code></pre><p>Now all inserts will be routed to this last chunk, which will
      always live on the shard zoned <code class="varname">"10x"</code>.</p><p>However, ranges from now through infinity will be trapped on this
      shard unless we modify the zone key range. To get around this, we could
      set up a cron job to update the key range once a day, like this:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">use</code> <code class="nx">config</code>
<code class="o">&gt;</code> <code class="kd">var</code> <code class="nx">zone</code> <code class="o">=</code> <code class="nx">db</code><code class="p">.</code><code class="nx">tags</code><code class="p">.</code><code class="nx">findOne</code><code class="p">({</code><code class="s2">"ns"</code> <code class="o">:</code> <code class="s2">"&lt;dbName.collName&gt;"</code><code class="p">,</code> 
<code class="p">...</code> <code class="s2">"max"</code> <code class="o">:</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MaxKey</code><code class="p">}})</code>
<code class="o">&gt;</code> <code class="nx">zone</code><code class="p">.</code><code class="nx">min</code><code class="p">.</code><code class="o">&lt;</code><code class="nx">shardKey</code><code class="o">&gt;</code> <code class="o">=</code> <code class="nx">ObjectId</code><code class="p">()</code>
<code class="o">&gt;</code> <code class="nx">db</code><code class="p">.</code><code class="nx">tags</code><code class="p">.</code><code class="nx">save</code><code class="p">(</code><code class="nx">zone</code><code class="p">)</code></pre><p>Then all of the previous day’s chunks would be able to move to
      other shards.</p><p>Another downside of this strategy is that it requires some changes
      to scale. If your most powerful server can no longer handle the number
      of writes coming in, there is no trivial way to split the load between
      this server and another.</p><p>If you do not have a high-performance server to firehose into or
      you are not using zone sharding, do not use an ascending key as the
      shard key. If you do, all writes will go to a single shard.</p></div></section><section data-type="sect2" data-pdf-bookmark="Multi-Hotspot"><div class="sect2" id="idm45882344383704"><h2>Multi-Hotspot</h2><p>Standalone<a data-type="indexterm" data-primary="multi-hotspot strategy" id="idm45882344119848"/><a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="shard key strategies" data-tertiary="multi-hotspot strategy" id="idm45882344119112"/> <em class="filename">mongod</em> servers are
      most efficient when doing ascending writes. This conflicts with
      sharding, in that sharding is most efficient when writes are spread over
      the cluster. The technique described here basically creates multiple
      hotspots—optimally several on each shard—so that writes are evenly
      balanced across the cluster but, within a shard, ascending.</p><p>To accomplish this, we use a<a data-type="indexterm" data-primary="sharding, basics of" data-secondary="compound shard keys" id="idm45882344243128"/> compound shard key. The first value in the compound key
      is a rough, random value with low-ish cardinality. You can picture each
      value in the first part of the shard key as a chunk, as shown in <a data-type="xref" href="#rough-key">Figure 16-6</a>. This will eventually work itself out as you
      insert more data, although it will probably never be divided up this
      neatly (right on the <code>$minKey</code> lines).
      However, if you insert enough data, you should eventually have
      approximately one chunk per random value. As you continue to insert
      data, you’ll end up with multiple chunks with the same random value,
      which brings us to the second part of the shard key.</p><figure style="float: 0"><div id="rough-key" class="figure"><img src="Images/mdb3_1606.png" width="484" height="826"/><h6><span class="label">Figure 16-6. </span>A subset of the chunks: each chunk contains a single state and
        a range of “_id” values</h6></div></figure><p>The second part of the shard key is an ascending key. This means
      that within a chunk, values are always increasing, as shown in the
      sample documents in <a data-type="xref" href="#figure15-7">Figure 16-7</a>. Thus, if you had one
      chunk per shard, you’d have the perfect setup: ascending writes on every
      shard, as shown in <a data-type="xref" href="#cafeteria-dist">Figure 16-8</a>. Of course, having
      <em>n</em> chunks with <em>n</em> hotspots
      spread across <em>n</em> shards isn’t very extensible: add a
      new shard and it won’t get any writes because there’s no hotspot chunk
      to put on it. Thus, you want a few hotspot chunks per shard (to give you
      room to grow), but not too many. Having a few hotspot chunks will keep
      the effectiveness of ascending writes, but having, say, a thousand
      hotspots on a shard will end up being equivalent to random
      writes.</p><figure style="float: 0"><div id="figure15-7" class="figure"><img src="Images/mdb3_1607.png" width="865" height="1108"/><h6><span class="label">Figure 16-7. </span>A sample list of inserted documents (note that all “_id” values
        are <span class="keep-together">increasing</span>)</h6></div></figure><figure style="float: 0"><div id="cafeteria-dist" class="figure"><img src="Images/mdb3_1608.png" width="869" height="1644"/><h6><span class="label">Figure 16-8. </span>The inserted documents, split into chunks (note that, within
        each chunk, the “_id” values are increasing)</h6></div></figure><p>You can picture this setup as each chunk being a stack of
      ascending documents. There are multiple stacks on each shard, each
      ascending until the chunk is split. Once a chunk is split, only one of
      the new chunks will be a hotspot chunk: the other chunk will essentially
      be “dead” and never grow again. If the stacks are evenly distributed
      across the shards, writes will be evenly distributed.</p></div></section></div></section><section data-type="sect1" data-pdf-bookmark="Shard Key Rules and Guidelines"><div class="sect1" id="idm45882344229832"><h1>Shard Key Rules and Guidelines</h1><p>There<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="rules and guidelines" id="idm45882344228952"/> are several practical restrictions to be aware of before
    choosing a shard key.</p><p>Determining which key to shard on and creating shard keys should be
    reminiscent of indexing because the two concepts are similar. In fact,
    often your shard key may just be the index you use most often (or some
    variation on it).</p><section data-type="sect2" data-pdf-bookmark="Shard Key Limitations"><div class="sect2" id="idm45882344227208"><h2>Shard Key Limitations</h2><p>Shard keys<a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="limitations on shard keys" id="idm45882344226232"/> cannot be arrays. <code class="function">sh.shardCollection()</code> will fail if any key has
      an array value, and inserting an array into that field is not
      allowed.</p><p>Once inserted, a document’s shard key value may be modified unless
      the shard key field is an immutable <code>_id</code> field. In older versions of MongoDB prior
      to 4.2, it was not possible to modify a document’s shard key
      value.</p><p>Most special types of indexes cannot be used for shard keys. In
      particular, you cannot shard on a geospatial index. Using a hashed index
      for a shard key is allowed, as covered previously.</p></div></section><section data-type="sect2" data-pdf-bookmark="Shard Key Cardinality"><div class="sect2" id="idm45882344222744"><h2>Shard Key Cardinality</h2><p>Whether<a data-type="indexterm" data-primary="cardinality" data-secondary="in shard keys" id="idm45882344221576"/><a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="cardinality" id="idm45882344220440"/> your shard key jumps around or increases steadily, it is
      important to choose a key with values that will vary. As with indexes,
      sharding performs better on high-cardinality fields. If, for example,
      you had a <code>"logLevel"</code> key that had
      only values <code>"DEBUG"</code>, <code>"WARN"</code>, or <code>"ERROR"</code>, MongoDB wouldn’t be able to break up
      your data into more than three chunks (because there would be only three
      different values for the shard key). If you have a key with little
      variation and want to use it as a shard key anyway, you can do so by
      creating a compound shard key on that key and a key that varies more,
      like <code class="keep-together">"logLevel"</code>
      and <code>"timestamp"</code>. It is important that
      the combination of keys has high cardinality.</p></div></section></div></section><section data-type="sect1" data-pdf-bookmark="Controlling Data Distribution"><div class="sect1" id="idm45882344221800"><h1>Controlling Data Distribution</h1><p>Sometimes, automatic<a data-type="indexterm" data-primary="data" data-secondary="controlling distribution of" id="Dcontrol16"/><a data-type="indexterm" data-primary="shard keys, choosing" data-secondary="controlling data distribution" id="SKCdatadist16"/> data distribution will not fit your requirements. This
    section gives you some options beyond choosing a shard key and allowing
    MongoDB to do everything automatically.</p><p>As your cluster gets larger or busier, these solutions become less
    practical. However, for small clusters, you may want more control.</p><section data-type="sect2" data-pdf-bookmark="Using a Cluster for Multiple Databases and Collections"><div class="sect2" id="shard-tags"><h2>Using a Cluster for Multiple Databases and Collections</h2><p>MongoDB<a data-type="indexterm" data-primary="collections" data-secondary="using clusters for multiple" id="idm45882344209768"/><a data-type="indexterm" data-primary="databases" data-secondary="using clusters for multiple" id="idm45882344208616"/><a data-type="indexterm" data-primary="clusters" data-secondary="using for multiple databases/collections" id="idm45882344207496"/> evenly distributes collections across every shard in your
      cluster, which works well if you’re storing homogeneous data. However,
      if you have a log collection that is “lower value” than your other data,
      you might not want it taking up space on your more expensive servers.
      Or, if you have one powerful shard, you might want to use it for only a
      real-time collection and not allow other collections to use it. You can
      create separate clusters, but you can also give MongoDB specific
      directions about where you want it to put certain data.</p><p>To set this up, use the <code class="function">sh.addShardToZone()<a data-type="indexterm" data-primary="sh.addShardToZone()" id="idm45882344204808"/></code> helper in the shell:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0000"</code><code class="p">,</code> <code class="s2">"high"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="c1">// shard0001 - no zone</code>
<code class="o">&gt;</code> <code class="c1">// shard0002 - no zone</code>
<code class="o">&gt;</code> <code class="c1">// shard0003 - no zone</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0004"</code><code class="p">,</code> <code class="s2">"low"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0005"</code><code class="p">,</code> <code class="s2">"low"</code><code class="p">)</code></pre><p>Then you can assign different collections to different shards. For
      instance, for your super-important real-time collection:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">updateZoneKeyRange</code><code class="p">(</code><code class="s2">"super.important"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MinKey</code><code class="p">},</code> 
<code class="p">...</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MaxKey</code><code class="p">},</code> <code class="s2">"high"</code><code class="p">)</code></pre><p>This says, “for negative infinity to infinity for this collection,
      store it on shards tagged <code>"high"</code>.”
      This means that no data from the <em>super.important</em>
      collection will be stored on any other server. Note that this does not
      affect how other collections are distributed: they will still be evenly
      distributed between this shard and the others.</p><p>You can perform a similar operation to keep the log collection on
      a low-quality server:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">updateZoneKeyRange</code><code class="p">(</code><code class="s2">"some.logs"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MinKey</code><code class="p">},</code> 
<code class="p">...</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MaxKey</code><code class="p">},</code> <code class="s2">"low"</code><code class="p">)</code></pre><p>The log collection will now be split evenly between
      <em>shard0004</em> and
      <em>shard0005</em>.</p><p>Assigning a zone key range to a collection does not affect it
      instantly. It is an instruction to the balancer stating that, when it
      runs, these are the viable targets to move the collection to. Thus, if
      the entire log collection is on <em>shard0002</em> or evenly
      distributed among the shards, it will take a little while for all of the
      chunks to be migrated to <em>shard0004</em> and
      <em>shard0005</em>.</p><p>As another example, perhaps you have a collection that you don’t
      want on the shard zoned <code>"high"</code>, but
      you don’t care which other shard it goes on. You can zone all of the
      non-high-performance shards to create a new grouping. Shards can have as
      many zones as you need:</p><pre class="pagebreak-before" data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0001"</code><code class="p">,</code> <code class="s2">"whatever"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0002"</code><code class="p">,</code> <code class="s2">"whatever"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0003"</code><code class="p">,</code> <code class="s2">"whatever"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0004"</code><code class="p">,</code> <code class="s2">"whatever"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">addShardToZone</code><code class="p">(</code><code class="s2">"shard0005"</code><code class="p">,</code> <code class="s2">"whatever"</code><code class="p">)</code></pre><p>Now you can specify that you want this collection (call it
      <em class="filename">normal.coll</em>) distributed across
      these five shards:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">updateZoneKeyRange</code><code class="p">(</code><code class="s2">"normal.coll"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MinKey</code><code class="p">},</code> 
<code class="p">...</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MaxKey</code><code class="p">},</code> <code class="s2">"whatever"</code><code class="p">)</code></pre><div data-type="tip"><h6>Tip</h6><p>You cannot assign collections dynamically—i.e., you can’t say,
        “when a collection is created, randomly home it to a shard.” However,
        you could have a cron job that went through and did this for
        you.</p></div><p>If you make a mistake or change your mind, you can remove a shard
      from a zone with <code>sh.removeShardFromZone()</code>:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">removeShardFromZone</code><code class="p">(</code><code class="s2">"shard0005"</code><code class="p">,</code> <code class="s2">"whatever"</code><code class="p">)</code></pre><p>If you remove all shards from zones described by a zone key range
      (e.g., if you remove <em>shard0000</em> from the zone
      <code>"high"</code>), the balancer won’t
      distribute the data anywhere because there aren’t any valid locations
      listed. All the data will still be readable and writable; it just won’t
      be able to migrate until you modify your tags or tag ranges.</p><p>To remove a key range from a zone, use <code>sh.removeRangeFromZone()</code>. The following is an
      example. The range specified must be an exact match to a range
      previously defined for the namespace <em>some.logs</em> and
      a given zone:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">removeRangeFromZone</code><code class="p">(</code><code class="s2">"some.logs"</code><code class="p">,</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MinKey</code><code class="p">},</code> 
<code class="p">...</code> <code class="p">{</code><code class="s2">"&lt;shardKey&gt;"</code> <code class="o">:</code> <code class="nx">MaxKey</code><code class="p">})</code></pre></div></section><section data-type="sect2" data-pdf-bookmark="Manual Sharding"><div class="sect2" id="idm45882343864504"><h2>Manual Sharding</h2><p>Sometimes, for<a data-type="indexterm" data-primary="manual sharding" id="idm45882343800408"/><a data-type="indexterm" data-primary="sharding, basics of" data-secondary="manual sharding" id="idm45882343799672"/> complex requirements or special situations, you may
      prefer to have complete control over which data is distributed where.
      You can turn off the balancer if you don’t want data to be automatically
      distributed and use the <code>moveChunk</code>
      command to manually distribute data.</p><p>To turn off the<a data-type="indexterm" data-primary="balancer" data-secondary="turning off" id="idm45882343797224"/> balancer, connect to a <em class="filename">mongos</em> (any <em class="filename">mongos</em> is fine) using the <em class="filename">mongo</em> shell and disable the balancer using
      the shell helper <code>sh.stopBalancer()<a data-type="indexterm" data-primary="sh.stopBalancer()" id="idm45882343772568"/></code>:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">stopBalancer</code><code class="p">()</code></pre><p>If there is currently a migrate in progress, this setting will not
      take effect until the migrate has completed. However, once any in-flight
      migrations have finished, the <span class="keep-together">balancer</span> will stop moving data around. To
      verify no migrations are in progress after disabling, issue the
      following in the <em class="filename">mongo</em>
      shell:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">use</code> <code class="nx">config</code>
<code class="o">&gt;</code> <code class="k">while</code><code class="p">(</code><code class="nx">sh</code><code class="p">.</code><code class="nx">isBalancerRunning</code><code class="p">())</code> <code class="p">{</code>
<code class="p">...</code>  <code class="nx">print</code><code class="p">(</code><code class="s2">"waiting..."</code><code class="p">);</code>
<code class="p">...</code>  <code class="nx">sleep</code><code class="p">(</code><code class="mi">1000</code><code class="p">);</code>
<code class="p">...</code> <code class="p">}</code></pre><p>Once<a data-type="indexterm" data-primary="db.chunks.find()" id="idm45882343701528"/> the balancer is off, you can move data around manually
      (if necessary). First, find out which chunks are where by looking at
      <em class="filename">config.chunks</em>:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">db</code><code class="p">.</code><code class="nx">chunks</code><code class="p">.</code><code class="nx">find</code><code class="p">()</code></pre><p>Now, use<a data-type="indexterm" data-primary="sh.moveChunk()" id="idm45882343691496"/> the <code class="function">moveChunk</code>
      command<a data-type="indexterm" data-primary="moveChunk command" id="idm45882343688376"/> to migrate chunks to other shards. Specify the lower
      bound of the chunk to be migrated and give the name of the shard that
      you want to move the chunk to:</p><pre data-type="programlisting" data-code-language="javascript"><code class="o">&gt;</code> <code class="nx">sh</code><code class="p">.</code><code class="nx">moveChunk</code><code class="p">(</code>
<code class="p">...</code> <code class="s2">"test.manual.stuff"</code><code class="p">,</code> 
<code class="p">...</code> <code class="p">{</code><code class="nx">user_id</code><code class="o">:</code> <code class="nx">NumberLong</code><code class="p">(</code><code class="s2">"-1844674407370955160"</code><code class="p">)},</code> 
<code class="p">...</code> <code class="s2">"test-rs1"</code><code class="p">)</code></pre><p>However, unless<a data-type="indexterm" data-primary="autosharding" id="idm45882343639288"/><a data-type="indexterm" data-primary="sharding, basics of" data-secondary="autosharding" id="idm45882343644088"/> you are in an exceptional situation, you should use
      MongoDB’s automatic sharding instead of doing it manually. If you end up
      with a hotspot on a shard that you weren’t expecting, you might end up
      with most of your data on that shard.</p><p>In particular, do not combine setting up unusual distributions
      manually with running the balancer. If the balancer detects an uneven
      number of chunks it will simply reshuffle all of your work to get the
      collection evenly balanced again. If you want uneven distribution of
      chunks, use the zone sharding technique<a data-type="indexterm" data-startref="Dcontrol16" id="idm45882343642008"/><a data-type="indexterm" data-startref="SKCdatadist16" id="idm45882343641176"/> discussed in <a data-type="xref" href="#shard-tags">“Using a Cluster for Multiple Databases and Collections”</a>.</p></div></section></div></section></div></section></div>



  </body></html>