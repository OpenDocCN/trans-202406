<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Organizations"><div class="chapter" id="ch-organizations">
<h1><span class="label">Chapter 10. </span>Organizations</h1>


<p>A Kubernetes cluster doesn’t run in a vacuum but in an environment, which could be an on-premises setup such as your own datacenter, or you could be using the offering of a cloud provider.</p>

<p>In contrast to previous chapters, in this chapter we look in detail at the “wetware,” a somewhat tongue-in-cheek term referring to humans rather than the software or hardware used. We will make the point that, despite all the technology available at your hands, running Kubernetes clusters and workloads securely requires you to focus on a nontech piece of the puzzle: humans and
their natural (work) habitat, that is, organizations.</p>

<p>Turns out that humans are, oftentimes, the weakest link in the chain. For example, an attacker may target email, PKI, and administrative interfaces to get behind the walls and steal secrets from the inside out, exfiltrate data, or leak sensitive information to the public.</p>

<p>In general, things external to a cluster can bypass its security if they are 
<span class="keep-together">compromised—for</span> example, the account with a cloud provider, admin credentials, encryption keys, or really anything along the software supply chain.</p>

<p>The shared responsibility model used by cloud providers puts certain security processes on the shoulders of users, while the cloud provides services that are themselves secure. This means that you need to consider the build-out of
the infrastructure topology, data flows, and security configuration as well as its usage.</p>

<p>So, let’s see what our swarthy ol’ Captain Hashjack is facing in this context. The Captain has exhausted all angles of technical attack, and is looking for another way in. It may be easier to attack an organization directly to get it to its secrets.</p>






<section data-type="sect1" data-pdf-bookmark="The Weakest Link"><div class="sect1" id="weakest-link">
<h1>The Weakest Link</h1>

<p>In <a data-type="xref" href="#fig-org-pyramid">Figure 10-1</a> you see the organizational pyramid. This<a data-type="indexterm" data-primary="organizational pyramid" id="orgprym"/> is a conceptual representation of the dependencies<a data-type="indexterm" data-primary="dependencies" data-secondary="organizational" id="idm45302807421104"/> a service or application you’re offering to a customer has. The basic idea is that there are a number of layers that exist in organization that are dependent on each other, and only if the foundations are stable and well developed can we trust that higher layers can
work properly:</p>
<dl>
<dt>Individual people</dt>
<dd>
<p>Platform and ops folks, developers, managers, the summer intern, the CEO’s 
<span class="keep-together">aunt—all</span> of them represent “the base” that everything
else builds atop. Here you should not forget about your vendors, such as folks working at your cloud provider of choice, from account managers to
support.</p>
</dd>
<dt>Teams and hierarchy</dt>
<dd>
<p>The next layer captures how you organize your
team from a functional perspective. The smaller the organization, the less hierarchy, but no matter how small, usually teams with more or less
defined scope are present.</p>
</dd>
<dt>Docs, knowledge bases, and training</dt>
<dd>
<p>An important layer, sometimes overlooked.
Lumping up all kind of written material or otherwise (activities such as a course) in the “documentation, knowledge bases (KB), and training” layer. The majority of people read or depend on them; only few produce and maintain
them.</p>
</dd>
<dt>Policies</dt>
<dd>
<p>Next up are policies, as discussed in <a data-type="xref" href="ch08.xhtml#ch-policy">Chapter 8</a>. The policies themselves are abstract and can be internal or external, regulatory or business types, but in any case they need to be (formally) defined and enforced, which happens
in the next layer.</p>
</dd>
<dt>Tools</dt>
<dd>
<p>The collection of tools—software from Linux to CI pipelines to Kubernetes. This is the “tangible” and mostly visible part that most of us deal with to get our job done.</p>
</dd>
</dl>

<p>The tip of the iceberg, if you like, at the summit of the pyramid (<a data-type="xref" href="#fig-org-pyramid">Figure 10-1</a>) is your application or service. That is, this is the
thing you own and want to release and which should be the focus of your attention, be it in terms of adding new features and/or fixing bugs, from a developer’s point of view.</p>

<p>If you are in a more platform-centric or ops role you typically care more about the “tools” layer in the pyramid and have, toward the developer, more of a coaching function. Given that Kubernetes established an operational vocabulary that both developers and infrastructure operators can use, the communication should, at least in theory, be a better one compared to earlier times.</p>

<figure><div id="fig-org-pyramid" class="figure">
<img src="Images/haku_1001.png" alt="Organizational pyramid" width="855" height="619"/>
<h6><span class="label">Figure 10-1. </span>Organizational pyramid</h6>
</div></figure>

<p>Up until <a data-type="indexterm" data-primary="organizational pyramid" data-startref="orgprym" id="idm45302807405280"/>now we’ve mainly been talking about the top two layers; that is, about tools and policies. We now focus on topics relevant to the other
layers of the pyramid.</p>

<p>To be more precise, in the remainder of this chapter we will discuss organizational challenges structured along the environments in which you use Kubernetes:</p>

<ul>
<li>
<p>First, we look at cloud computing and its providers (<a data-type="xref" href="#cloud-providers">“Cloud Providers”</a>), discussing constraints and good practices in such environments.</p>
</li>
<li>
<p>We then move on to on-premises environments/datacenters (<a data-type="xref" href="#on-premises-envs">“On-Premises Environments”</a>), highlighting challenges unique to those.</p>
</li>
<li>
<p>Finally, in <a data-type="xref" href="#common-env-considerations">“Common Considerations”</a>, we review common considerations; that is, organizational things relevant for both cloud or on-prem 
<span class="keep-together">environments.</span></p>
</li>
</ul>

<p>So let’s jump into the deep end, with cloud providers.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Cloud Providers"><div class="sect1" id="cloud-providers">
<h1>Cloud Providers</h1>

<p>As per the<a data-type="indexterm" data-primary="cloud" data-secondary="computing characteristics" id="clcomp_char"/><a data-type="indexterm" data-primary="NIST (National Institute of Standards and Technology)" id="NIST_cloudcomp"/><a data-type="indexterm" data-primary="National Institute of Standards and Technology (NIST)" id="NIST"/> National Institute of Standards and Technology (NIST)
<a href="https://oreil.ly/XdW3R">Special Publication 800-145</a>, cloud computing, from a conceptual point of view, has to exhibit some essential characteristics:</p>
<dl>
<dt>On-demand self-service</dt>
<dd>
<p>You can unilaterally provision computing, storage, or networking; no human interaction with service provider required.</p>
</dd>
<dt>Broad network access</dt>
<dd>
<p>Capabilities are available over the (public) network and accessed through standard mechanisms.</p>
</dd>
<dt>Resource pooling</dt>
<dd>
<p>Cloud provider’s computing resources are pooled to serve multiple consumers using a multitenant model.</p>
</dd>
<dt>Rapid elasticity</dt>
<dd>
<p>The capabilities can be elastically acquired and released, virtually appearing to be unlimited.</p>
</dd>
<dt>Metered consumption</dt>
<dd>
<p>Automatic control and optimization of resource use by leveraging a metering capability.</p>
</dd>
</dl>

<p>We will scope our <a data-type="indexterm" data-primary="cloud" data-secondary="computing characteristics" data-startref="clcomp_char" id="idm45302807382320"/><a data-type="indexterm" data-primary="NIST (National Institute of Standards and Technology)" data-startref="NIST_cloudcomp" id="idm45302807381056"/><a data-type="indexterm" data-primary="National Institute of Standards and Technology (NIST)" data-startref="NIST" id="idm45302807379984"/>discussion here based on the preceding NIST definition of cloud computing.</p>

<p>In a nutshell, cluster security <a data-type="indexterm" data-primary="clusters" data-secondary="security" id="idm45302807378496"/>depends on organizational practices. To strengthen the organizational muscle, consider holding internal events or participate in external ones, something akin to what was offered, for example,
at <a data-type="indexterm" data-primary="KubeCon" id="idm45302807377184"/>KubeCon:</p>

<ul>
<li>
<p>Attacking and Defending Kubernetes Clusters, presented at <a href="https://oreil.ly/Vwqcp">KubeCon NA 2019 CTF</a>.</p>
</li>
<li>
<p>The <a href="https://oreil.ly/4ItS2">Cloud Native Security Tutorial</a> at KubeCon EU 2020.</p>
</li>
<li>
<p>The TAG Security CTF wrap up videos at
<a href="https://oreil.ly/E940b">KubeCon NA 2020</a> and
<a href="https://oreil.ly/proxS">KubeCon EU 2021</a>.</p>
</li>
</ul>

<p>Let’s now have a closer look at good practices, on the individual and team level, for using Kubernetes in a cloud computing environment.</p>








<section data-type="sect2" data-pdf-bookmark="Shared Responsibility"><div class="sect2" id="shared-responsibility">
<h2>Shared Responsibility</h2>

<p>Typically, cloud providers <a data-type="indexterm" data-primary="cloud providers" data-secondary="distribution of responsibilities" id="cloudprov_respon"/>describe the way security and compliance is handled by defining what part of a service is their responsibility and which part is yours. Often you find this in the service-level agreements (SLAs)<a data-type="indexterm" data-primary="Service Level Agreements (SLAs)" id="idm45302807365792"/><a data-type="indexterm" data-primary="SLAs (Service Level Agreements)" id="idm45302807365104"/> that also define contractual penalties or compensation if a certain service isn’t available. For example, AWS has the
<a href="https://oreil.ly/7FR6v">Shared Responsibility Model</a>,
likewise Azure’s
<a href="https://oreil.ly/jKkOH">variant</a>, and the more specific Google documents on the
<a href="https://oreil.ly/KmpDY">shared responsibility model in GKE</a> and
<a href="https://oreil.ly/rcZD3">Anthos shared responsibility</a>. In addition, the Center for Internet Security (CIS) provides foundational security benchmarks for the three major cloud 
<span class="keep-together">providers.</span></p>

<p class="pagebreak-before">But what, exactly, are your responsibilities in the context of a managed Kubernetes cluster? There a number of aspects to consider:</p>
<ol>
<li>
<p>When standardizing on Kubernetes to, for example, create more interoperable systems or to avoid vendor lock-in, the API focus shifts from a cloud provider–centric one to a Kubernetes-centric one. This means Kubernetes abstracts away many, if not all, resources. While this is great from an interop POV it also means that more responsibility is now on your plate.</p>
</li>
<li>
<p>Serverless offerings such as Fargate and Cloud Run usually mean that more of the responsibility is with the cloud provider. For example, no need to patch nodes in case a new CVE is released.</p>
</li>

</ol>

<p>For example, let’s look at<a data-type="indexterm" data-primary="EKS (Elastic Kubernetes Service)" id="idm45302807356544"/><a data-type="indexterm" data-primary="Elastic Kubernetes Service (EKS)" id="idm45302807355824"/> EKS. The <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Shared Responsibility Model" id="idm45302807355008"/><a data-type="indexterm" data-primary="Shared Responsibility Model (AWS)" id="idm45302807354000"/>AWS Shared Responsibility Model states that Amazon is responsible for the security <em>of the cloud</em> and customers are responsible for security <em>in the cloud</em>, meaning operations and patching of any
and all applications they run using the service, including the control plane, such as custom controllers or CNI plug-ins. Say there’s a security issue with an Amazon-owned Helm chart (for an <a href="https://oreil.ly/wPhqE">ACK controller</a>, for example): Amazon
is responsible for patching the controller and providing updated images and charts, whereas you as the customer are responsible for updating the controller  in your cluster, using the latest, patched version provided by Amazon.</p>

<p>Where you are responsible, be cognizant of the configurations of the software you’re running as well as the version. You can use common standards
and benchmarks to validate your <a data-type="indexterm" data-primary="cloud providers" data-secondary="distribution of responsibilities" data-startref="cloudprov_respon" id="idm45302807350752"/>configurations:</p>

<ul>
<li>
<p><a href="https://oreil.ly/tpqr6">DevSec Hardening for Docker</a></p>
</li>
<li>
<p><a href="https://oreil.ly/uvm1B">DevSec Hardening for Kubernetes</a></p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Account Hygiene"><div class="sect2" id="account-hygiene">
<h2>Account Hygiene</h2>

<p>Access to resources, be <a data-type="indexterm" data-primary="cloud" data-secondary="account hygiene principles" id="cloud_accthyg_princ"/><a data-type="indexterm" data-primary="account hygiene principles" id="accthyg_princ"/>it in-cluster as discussed in <a data-type="xref" href="ch08.xhtml#ch-policy">Chapter 8</a> or to the environment (cloud provider) is a basic requirement to get any work done, for developers and ops folks alike. Oftentimes, the challenge is that one either has a secure or a usable setup. Using a combination of (forcibly rotating) passwords, YubiKeys, one-time passcodes from OTP devices, and other things to establish your identity can be tiresome. So people come up with shortcuts, trying to outsmart the security controls. There are promising approaches available, such as Google’s <a href="https://oreil.ly/Kz3qR">BeyondCorp</a>, however, many companies are not ready for it, yet.</p>

<p class="pagebreak-before">Until the time the identity and access challenge is addressed otherwise,
allow us to propose a structured approach to account hygiene in the context of cloud.
This three-phased process for account hygiene looks as follows:</p>
<dl>
<dt>Onboarding</dt>
<dd>
<p>Most companies get that right. In the <a data-type="indexterm" data-primary="onboarding, account hygiene" id="idm45302807337152"/>process of onboarding a person into the
organization and further into a team, LDAP or POSIX group memberships are
established. In addition you want organization-wide policies that define
what role has what kind of access (read-only or create or whatever) to what
resources. For example, AWS IAM offers
<a href="https://oreil.ly/ARKkw">service control policies</a>
(SCP), allowing you to define central control over the maximum available
permissions for all accounts in an organization.</p>
</dd>
<dt>Auditing and monitoring</dt>
<dd>
<p>During<a data-type="indexterm" data-primary="auditing" data-secondary="account hygiene and" id="idm45302807333856"/> the entire time of a person being part of an org, you want to monitor
access (inside and from the outside) as well as being able to audit
post-facto.</p>
</dd>
<dt>Offboarding</dt>
<dd>
<p>This is where many <a data-type="indexterm" data-primary="offboarding, account hygiene" id="idm45302807331280"/>organizations fall over. Any access, be it to the cluster
itself, the CI pipeline, the container registry, the Git repo, or the Slack
instance, must be revocable. That is, when a person leaves the organization,
access to all resources must be enforced automatically and in an auditable
manner.</p>
</dd>
</dl>

<p>Do you have all three phases covered? Mostly automated? Try to address as many
of the preceding points as feasible in the context of your organization.</p>

<p>A topic that deserves our special attention is that of long-lasting credentials
versus temporary credentials. Long-lasting <a data-type="indexterm" data-primary="credentials" data-secondary="account hygiene and" id="idm45302807328848"/>credentials such as passwords or
SSH keys need to be rotated and/or revoked. Especially in the offboarding
phase, long-lasting credentials can be a dramatic attack vector. In other words:
in the best case the person leaving the org has no hard feelings and simply
ignores the fact that they still can access, say, a Git repo or an S3 bucket with
sensitive information in it. In the not so good case, a disgruntled or
malicious ex-employee might wreak havoc after they have left the organization.</p>

<p>Some tools or protocols, such as SSH, are not (out-of-the-box) very friendly concerning the rotation or revocation activity. Though increasingly solutions become available—for an example see <a href="https://oreil.ly/gaSPi">DevLog: SSH authentication via OAuth2</a>—the question really should be: why not use temporary credentials in the first place? Take, for example, AWS <a data-type="indexterm" data-primary="Security Token Service (STS)" id="idm45302807325568"/><a data-type="indexterm" data-primary="STS (Security Token Service)" id="idm45302807324928"/>Security Token Service (STS). With STS, you can provide users and apps with short-term (minutes to hours) credentials on the fly and with that can escape the rotation and revocation hell of long-lasting credentials. For example, you could use
<a href="https://oreil.ly/bDyav">Okta as an identity provider</a> and single-sign-on solution for EKS, based on short-lived STS tokens. As you would expect, all cloud providers offer some sort of temporary creds; for <a data-type="indexterm" data-primary="cloud" data-secondary="account hygiene" data-tertiary="principles" data-startref="cloud_accthyg_princ" id="idm45302807323024"/><a data-type="indexterm" data-primary="account hygiene principles" data-startref="accthyg_princ" id="idm45302807321536"/>example, Azure’s Shared Access Signature (SAS) or Google’s <a href="https://oreil.ly/ta1bD">STS</a> API.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Grouping People and Resources"><div class="sect2" id="grouping-stuff">
<h2>Grouping People and Resources</h2>

<p>You almost always <em>don’t</em> want to deal <a data-type="indexterm" data-primary="policies" data-secondary="grouping people and" id="pol_grpppl"/>with individual people or resources. What
you want is, based on some characteristic, being able to address a group of people
(role-based, for example) or a group of resources such as “all pods and
services, running in the test stage, for application X.” In case of an incident,
being able to address an incident and foster efficient escalation
requires you to identify who owns a certain resource such as a cluster or a
namespace.</p>

<p>Let’s first talk about people. To group <a data-type="indexterm" data-primary="cloud providers" data-secondary="people policies" id="idm45302807315168"/>people and define team-wide policies, different cloud providers offer different options:</p>
<dl>
<dt>Amazon Web Services</dt>
<dd>
<p>With <a href="https://oreil.ly/SCNaP">AWS Organizations</a> you can <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Organizations" id="idm45302807311536"/><a data-type="indexterm" data-primary="Organizations (AWS)" id="idm45302807310560"/>programmatically create AWS accounts, apply policies to groups, and it also serves as the basis for other security-related services such as AWS Single Sign-On (SSO), all usually orchestrated using a higher-level service called <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Control Tower" id="idm45302807309520"/><a data-type="indexterm" data-primary="Control Tower (AWS)" id="idm45302807308560"/>AWS Control Tower.</p>
</dd>
<dt>Microsoft Azure</dt>
<dd>
<p><a href="https://oreil.ly/HvRzj">Azure Policy</a>
helps enforce organizational standards <a data-type="indexterm" data-primary="Azure" data-secondary="Policy" id="idm45302807305824"/>and to assess compliance across your environments based on business rules. In addition, to control user actions, you would use Azure RBAC.</p>
</dd>
<dt>Google Cloud Platform</dt>
<dd>
<p>The <a href="https://oreil.ly/Zktbx">Organization Policy Service</a> from <a data-type="indexterm" data-primary="Organization Policy Service" id="idm45302807302560"/>Google enables you to define and enforce centralized and programmatic control over your organization’s cloud resources. Whereas Google’s IAM allows you to state who can perform an actions (permissions), the Organization Policy focuses<a data-type="indexterm" data-primary="policies" data-secondary="grouping people and" data-startref="pol_grpppl" id="idm45302807301408"/> on the What (configuration scope).</p>
</dd>
</dl>

<p>To statically or dynamically<a data-type="indexterm" data-primary="policies" data-secondary="grouping resources and" id="pol_grpresc"/> group resources (from worker nodes to entire clusters to cluster-external resources such as load balancers or networked storage like S3 or NFS), again, different cloud vendors offer you a range
of controls:</p>
<dl>
<dt>Amazon Web Services</dt>
<dd>
<p>Amazon Tags and Resource Groups (RG). While the former<a data-type="indexterm" data-primary="Amazon Tags" id="idm45302807295968"/><a data-type="indexterm" data-primary="Resource Groups (RG)" data-see="RG (Resource Groups)" id="idm45302807295264"/><a data-type="indexterm" data-primary="RG (Resource Groups)" id="idm45302807294320"/> was already around for a while and you can lean on <a href="https://oreil.ly/kRMVs">best practices for tagging</a>, the RGs are a relatively new concept. That is, up to the introduction of RGs (which are still optional), you’d essentially have to deal with a flat resource namespace on a per-account basis. Good practice
these days is to establish or follow
<a href="https://oreil.ly/YHaDg">conventions</a> and apply them, as <a href="https://oreil.ly/CuxuK">supported by a service</a>.</p>
</dd>
<dt>Microsoft Azure</dt>
<dd>
<p>While both Azure<a data-type="indexterm" data-primary="Azure" id="idm45302807289952"/> and AWS have “resource groups” allowing you to organize resources, these are not directly comparable. In case of Azure, a resource
is always associated with exactly one resource group (which can be changed, but multiple, tag-based memberships are not supported).</p>
</dd>
<dt>Google Cloud Platform</dt>
<dd>
<p>GCP really<a data-type="indexterm" data-primary="GCP (Google Cloud Platform)" id="idm45302807287520"/><a data-type="indexterm" data-primary="Google Cloud" data-secondary="GCP" id="idm45302807286816"/> pioneered this space: from the very beginning it supported <a href="https://oreil.ly/JZfyg">projects</a>, allowing (and forcing) you to enable and use services, billing, managing collaborators, and permissions for resources. In addition, GCP supports <a href="https://oreil.ly/WaIXA">labeling</a> of resources, semantically pretty much exactly what you know from Kubernetes.</p>
</dd>
</dl>

<p>Another question in this context is that of <a data-type="indexterm" data-primary="cross-account access" id="idm45302807283584"/>cross-account access. Using multiple accounts (as enforced by the preceding structure) helps naturally to overcome per-account limits or quotas but it also strengthens your security posture. As a case study,
see the AWS <a href="https://oreil.ly/CTEGn">multiple account strategy</a> whitepaper. Depending on the cloud provider, you might also have policies available that are directly applied to a resource (rather than the human or team-centric policies) and that can help with cross-account access. For a<a data-type="indexterm" data-primary="policies" data-secondary="grouping resources and" data-startref="pol_grpresc" id="idm45302807281584"/> concrete examplary walkthrough, see  Satya Vajrapu and Jason Smith’s blog post <a href="https://oreil.ly/t1Zfw">“Enabling Cross-Account Access to Amazon EKS Cluster Resources”</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Other Considerations"><div class="sect2" id="idm45302807279456">
<h2>Other Considerations</h2>

<p>In addition to proactive measures such as org-wide policies for
grouping resources and account hygiene, there are a number of things
you can do, in the context of the cloud provider environment, to make your
Kubernetes setup more resilient and secure. Let’s have a look at it.</p>










<section data-type="sect3" data-pdf-bookmark="Dealing with root certificate authorities"><div class="sect3" id="certificate-authorities-pki">
<h3>Dealing with root certificate authorities</h3>

<p>Authentication over TLS <a href="https://oreil.ly/y4bfP">requires PKI certificates</a>, and as such the question arises where to best store private
keys and certs for the root <a data-type="indexterm" data-primary="Certificate Authorities (CAs)" id="idm45302807274992"/><a data-type="indexterm" data-primary="CAs (Certificate Authorities)" id="idm45302807274256"/>Certificate Authorities (CA). You want to keep your
root CA in a Key Management Service (KMS); that is, <a data-type="indexterm" data-primary="KMS (Key Management Services)" id="idm45302807273312"/><a data-type="indexterm" data-primary="Key Management Services (KMS)" id="idm45302807272624"/>in a managed service that
allows you to manage cryptographic keys and define their usage declaratively, org-wide.
These KMS are typically based on hardware security modules (FIPS 140-2 compliant)
and usually also support auditing, for example, in case of AWS via CloudTrail.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Avoid leaking credentials"><div class="sect3" id="leaking-credentials">
<h3>Avoid leaking credentials</h3>

<p>The beginning of the<a data-type="indexterm" data-primary="credentials" data-secondary="leaking" id="idm45302807269552"/> supply chain (<a data-type="xref" href="ch04.xhtml#ch-apps-supply-chain">Chapter 4</a>) is where software
is written; that is, the developer or ops person creating some artifact like source code, a configuration in YAML format, or documentation in, say, Markdown format. These source artifacts usually are kept in a version control system (VCS) such as Git, a popular distributed <a data-type="indexterm" data-primary="VCSs (Version Control Systems)" id="idm45302807267280"/><a data-type="indexterm" data-primary="Version Control Systems (VCSs)" id="idm45302807266544"/>VCS  enabling collaboration and making it possible to revert to previous (good) states.</p>

<p>There is a dark side to VCS as well: whenever we put stuff into them (commit
and/or push in the context of a distributed VCS) we potentially risk leaking
sensitive 
<span class="keep-together">information.</span></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The following is a cautionary tale on leaking sensitive information that happened to one of the authors who shall remain anonymous (hi Michael!). Just before lunch time, said author wanted to check in the documentation of a project. The <em>docs.md</em> file looked innocent enough, describing the steps necessary to connect. There was only one problem, and that was that by copying and pasting the verbatim output from the command line, the actual access key ID and secret access key was included. Within minutes, bad actors monitoring the global GitHub feed noticed the credentials and used them to spin up EC2 instances. This story had a good ending as the AWS Fraud Detection<a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Fraud Detection" id="idm45302807262384"/><a data-type="indexterm" data-primary="Fraud Detection (AWS)" id="idm45302807261392"/> kicked in immediately (the change in usage pattern was clear—said author almost exclusively uses one region) and
did the needful: freezing the account and reaching out to the account owner to fix
the issue. The lesson learned is a clear one: never ever commit any passwords
or keys in plain text in a repo.</p>
</div>

<p>There are two things you can do about it:</p>
<ol>
<li>
<p>Enforce compulsory code scanning prior to check-in. For example, you can use
<a href="https://oreil.ly/29g6M">git-secrets</a>, which helps prevent you
from committing sensitive information to a Git repository as a pre-commit
hook.</p>
</li>
<li>
<p>If you need to keep sensitive information in a (public) repo, at least
do not store it in plain text, encrypt it. For example, Bitnami offers
<a href="https://oreil.ly/MP7L7">sealed-secrets</a>, allowing
for automating the process all the way to the consuming target cluster.</p>
</li>

</ol>

<p>With these considerations out of the way, we now switch gears and look at
an entirely different environment for running Kubernetes: on-prem setups.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="On-Premises Environments"><div class="sect1" id="on-premises-envs">
<h1>On-Premises Environments</h1>

<p>In the following we’re using <a data-type="indexterm" data-primary="datacenters" id="idm45302807253056"/><a data-type="indexterm" data-primary="on-premises environments" id="idm45302807252352"/>the terms <em>on-premises environments</em> and <em>datacenters</em>
somewhat interchangeably. Separate from this are bare-metal deployments, which
may or may not happen in the cloud (AWS, Equinix) or on-prem.</p>

<p>While in the case of cloud providers the perimeter and its implications are
clearly defined, in the case of on-prem environments you have to define it yourself.
For example, in AWS, the account is the basic unit of identity (one
customer is one account) and at scale it’s an AWS Organization that represents
a customer, owning dozens if not hundreds of accounts. The same is true for
resources that belong to exactly one account and can be made accessible for
other accounts or finer-grained constructs such as a VPC.</p>

<p>In the case of on-prem, you need to come up with the perimeter definition
yourself and you should consider the unit of compute (is it a Kubernetes
cluster? is it a Kubernetes namespace?) being part of it, since, depending
on the business requirements, it may imply multitenancy requirements as
discussed in <a data-type="xref" href="ch07.xhtml#ch-hard-multi-tenancy">Chapter 7</a>.</p>

<p>There is also the question of whether you should or want to run
<a href="https://oreil.ly/5lCpA">Kubernetes clusters</a> yourself. This boils down to competitive advantage (are you, for
example, rolling your own Linux distro?) and your role (are you a vendor or an
enduser). Having said this, as per Gartner the overall <a href="https://oreil.ly/vfK9d">cloud adoption is still in the 20% range</a>
and around 50% of Kubernetes users still have a DIY approach. That is, rather
than using a Kubernetes distribution such as <a href="https://oreil.ly/mIkJe">Red Hat OpenShift</a>,
<a href="https://oreil.ly/7LoAV">Rancher</a>, or <a href="https://oreil.ly/SsMTN">Canonical Distribution of Kubernetes</a>,
they build and maintain their own distribution.</p>

<p>You will need to consider options for the following infrastructure pieces (as well as think through their security implications):</p>

<ul>
<li>
<p>Hardware, including buildings, racks, power supply, generators, blades, etc. to run your workloads.</p>
</li>
<li>
<p>Typically some IaaS layer to virtualize the hardware, for example VMware or OpenStack.</p>
</li>
<li>
<p>Some worker node-level provisioning including host operating system, such as
Ansible or TerraForm. These provide the basic requirements for Kubernetes to
be installable.</p>
</li>
<li>
<p>For East-West (in-cluster) network traffic, fulfill the <a href="https://oreil.ly/FRIPi">Kubernetes requirements</a> via SDN or the like.</p>
</li>
<li>
<p>For North-South network traffic, use a load balancer like metalLB or an F5 
<span class="keep-together">offering.</span></p>
</li>
<li>
<p>Use networked storage like an NFS filer and/or SANs and/or object
store (MinIO, OpenIO or the like), Gluster, Ceph, etc.</p>
</li>
<li>
<p>An on-prem certificate management solution.</p>
</li>
<li>
<p>A user directory (typically LDAP-based).</p>
</li>
<li>
<p>Use either the Kubernetes built-in DNS or an existing internal DNS server.</p>
</li>
</ul>

<p>These are the minimal requirements necessary and you also should take into
account that, in contrast to the cloud, almost every on-prem setup is different.
This means that recipes—for example, Ansible scripts—to automate tasks usually
require a lot of customization and testing to ensure they provide useful,
production-ready output.</p>

<p>Some useful resources for further reading in this context are:</p>

<ul>
<li>
<p><a href="https://oreil.ly/Q2R8i">“On-Premise Kubernetes Clusters”</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Nrgfa">“Build Kubernetes Bare-metal Cluster with External Access”</a></p>
</li>
<li>
<p><a href="https://oreil.ly/mc5X1">“Bare-Metal Kubernetes”</a></p>
</li>
</ul>

<p>Further, there are a number of things you should be aware of and need to
address. Most of them usually already in place if you are not facing a
greenfield setup:</p>

<ul>
<li>
<p>One of the most challenging issues is making sure that physical access to your infrastructure is clearly defined and enforced.</p>
</li>
<li>
<p>The worst-case scenarios like an earthquake or a fire are another thing you need to be able to deal with (also: DR plan).</p>
</li>
<li>
<p>DoS attacks are yet another thing that cloud providers take care of for you automatically and
usually free of cost. In the on-prem case you need to come up with a strategy here as well.</p>
</li>
<li>
<p>Lastly, capacity planning and intrusion detection (see <a data-type="xref" href="ch09.xhtml#ch-intrusion-detection">Chapter 9</a>),
are, in the on-prem case, on you.</p>
</li>
</ul>

<p>With this short discussion on the <a href="https://oreil.ly/AfrNT">challenges of running Kubernetes on-prem</a>,
we now move on to considerations relevant to any environment.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Common Considerations"><div class="sect1" id="common-env-considerations">
<h1>Common Considerations</h1>

<p>No matter if you are using managed Kubernetes from a cloud provider or you
are running your own on-prem environment, there are a number of common
considerations targeting humans (social engineering) as well as regulatory
concerns we will discuss in the following sections.</p>








<section data-type="sect2" data-pdf-bookmark="Threat Model Explosion"><div class="sect2" id="tm-explosion">
<h2>Threat Model Explosion</h2>

<p>Kubernetes is a powerful <a data-type="indexterm" data-primary="threat models" data-secondary="Kubernetes attack surface considerations" id="thrtmod_Kubascons"/><a data-type="indexterm" data-primary="attack surface" data-secondary="Kubernetes considerations" id="ataksurf_Kubcons"/>system, however this also means you have to deal
with increased complexity when it comes to understanding how the multiple
abstraction mechanisms and control loop can be exploited by a malicious player.</p>

<p>Have a look at <a data-type="xref" href="#fig-tm-explosion">Figure 10-2</a>. Consider the case of a monolith (depicted
as the <code>app</code>), let’s say, a single binary running on a VM. There are few moving
parts and responsibilities are straightforward. For example, infrastructure folks
look after VM (patching, upgrading) and developers look after the <code>app</code> (scanning on
dependency-level, avoiding SQL injections, etc.).</p>

<figure><div id="fig-tm-explosion" class="figure">
<img src="Images/haku_1002.png" alt="Kubernetes vs monolith attack surface" width="961" height="888"/>
<h6><span class="label">Figure 10-2. </span>Kubernetes versus monolith attack surface</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a data-type="xref" href="#fig-tm-explosion">Figure 10-2</a> is intentionally a little lopsided. Of course,
in any real-world situation, the monolith would also require a load balancer,
for example, and there are many more moving parts. The point of the exercise
is more of the following nature: Kubernetes provides a large number of resources
and APIs, many more than most people can keep in their heads. It also comes with
a number of required and optional (but almost always present) components both
in the control and the data plane. Now, when you are using Kubernetes you can’t
simply opt out and ignore all of this, even if you only need or are interested
in a tiny subset. They still are present and offer our Captain a potential
way to attack. So, take that figure with a grain of salt and simply be aware
of the many moving parts, trying to consider as many as you can in your
threat modeling.</p>
</div>

<p>Now look at the righthand side, the case of the <code>app</code> running in the context of
a containerized microservices setup in Kubernetes. This <code>app</code> is now, together
with, let’s assume, 15 other services, along the request path servicing the user.
The <code>app</code> runs in a pod (a Kubernetes abstraction), which is a collection of
containers on the same node (with containers being a Linux/OCI abstraction), and
only then comes the process (group) that runs the <code>app</code>.</p>

<p>There’s also the <code>kubelet</code> running on the worker node that represents
the local Kubernetes supervisor using the <code>CRI</code> to communicate to the container
runtime what to do (start, stop, etc.) as we discussed in <a data-type="xref" href="ch02.xhtml#ch-pod-level-resources">Chapter 2</a>.
But it doesn’t stop there: there’s also the connection with the Kubernetes
control plane, with the container registry (not shown in the figure, to keep it simple),
the CNI plug-ins, etc.</p>

<p>The complexity introduced with using Kubernetes yields a <a data-type="indexterm" data-primary="threat model explosion (TME)" id="idm45302807200208"/><a data-type="indexterm" data-primary="TME (threat model explosion)" id="idm45302807199408"/>threat model explosion (TME). There are multiple issues:</p>
<ol>
<li>
<p>Whose responsibility is the end-to-end security? Likely, dev and ops will have
to work closely together to keep everything secure.</p>
</li>
<li>
<p>The abstractions make it hard to realize what’s going on (in case of an incident),
and to make sure everything is tested and monitored.</p>
</li>
<li>
<p>Simply put, the many levels of abstractions offer a larger attack surface compared
to the monolith.</p>
</li>

</ol>

<p>Number 1 above is partially the driver for the DevSecOps movement and serves as
a motivation. Numbers 2 and 3 are what we are concerned with and want to
provide some suggestions for.</p>

<p>So what are the concrete things you can do to tackle the TME? Consider adopting
the following good practices:</p>

<ul>
<li>
<p>Provide 360-visibility; that is, measure, and don’t just assume. Every
component in Kubernetes should be monitored and sensible alerts configured.
In <a data-type="xref" href="#slo-pressure">“How SLOs Can Put Additional Pressure on You”</a> we will drill deeper into this topic.</p>
</li>
<li>
<p>Use downtimes or periods of low traffic to do hands-on training and do fire drills.
This helps build up the muscle memory and makes folks more comfortable to
escalate and do the right steps (contain, observe, capture) when under attack.</p>
</li>
<li>
<p>Automate, automate, automate. No matter whether we’re talking observability or
escalation path: all essential processes should be automated with the option
for humans to interfere manually where necessary.</p>
</li>
</ul>

<p>With this, we move on to another operational topic: how much downtime is
allowed and what are the security <a data-type="indexterm" data-primary="threat models" data-secondary="Kubernetes attack surface considerations" data-startref="thrtmod_Kubascons" id="idm45302807188800"/><a data-type="indexterm" data-primary="attack surface" data-secondary="Kubernetes considerations" data-startref="ataksurf_Kubcons" id="idm45302807187456"/>implications?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="How SLOs Can Put Additional Pressure on You"><div class="sect2" id="slo-pressure">
<h2>How SLOs Can Put Additional Pressure on You</h2>

<p>From a site reliability engineering (SRE)<a data-type="indexterm" data-primary="site reliability engineering (SRE), SLOs and" id="idm45302807184304"/><a data-type="indexterm" data-primary="SRE (site reliability engineering), SLOs and" id="idm45302807183504"/><a data-type="indexterm" data-primary="SLOs (Service Level Objectives), SRE and" id="idm45302807182800"/><a data-type="indexterm" data-primary="Service Level Objectives (SLOs), SRE and" id="idm45302807182096"/> perspective, service-level objectives (SLOs) are a great tool to quantify the tolerated user
expectations concerning downtimes. Establishing
SLOs can be tough (see <a href="https://oreil.ly/I12YN">“Implementing SLOs”</a> for reference), but in our security context let’s look at a different
question: how can an attacker misuse SLOs to get inside or exfiltrate
information?</p>

<p>Things you want to consider in the context of SLOs are as follows:</p>

<ul>
<li>
<p>To measure if and to what extent the SLOs have been fulfilled you usually
define a number of service-level indicators (SLIs). These measurements can
be attacked and used to fake an SLO violation, tricking you into believing
something that is, in fact, not true. Whenever we are faced with a stressful
situation we potentially make mistakes; think of it as the equivalent of a
human-centered DoS.</p>
</li>
<li>
<p>Usually, escalation takes place via notification channels such as pages,
text messages, and emails. Popular services such as PagerDuty can, if
not properly configured or used, leak sensitive information.</p>
</li>
</ul>

<p>While the uptake of SLOs/SLIs is not yet that huge, the preceding point is something you may
want to consider when introducing SRE or DevOps in your organization. In early
2021, an initiative called <a href="https://openslo.com">OpenSLO</a> started to standardize
the formal representation of SLOs, so there’s a fair to good chance that by the time
you read this, you can benefit from the fruits of this effort.</p>

<p>We’ll now move on to attacks that do not focus on technical roles: social engineering.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Social Engineering"><div class="sect2" id="social-eng">
<h2>Social Engineering</h2>

<p>The security <a data-type="indexterm" data-primary="social engineering principles" id="soceng_princ"/>economics—for reference, the MIT OpenCourseWare course with the same name available via <a href="https://oreil.ly/Hde78">6.858 Computer Systems Security</a>—oftentimes speak for targeting humans rather than software or hardware. As we already hinted at in <a data-type="xref" href="#weakest-link">“The Weakest Link”</a>, humans are usually the weakest link in the defense chain. Additionally, attackers can benefit from the fact that they only need to find one weakness in the overall
setup (for example, one unmotivated 
<span class="keep-together">or careless</span> human) compared to the Blue Team that has to defend against all attacks and has to motivate and nudge all members of an organization to comply and play their part.</p>

<p>In the context of social engineering, the reconnaissance phase of the attack—present
in all major attack models; see <a data-type="xref" href="#fig-cyber-attack-phases-models">Figure 10-3</a>—is particularly
interesting. There is as of time of writing not enough research and collective
experience in the practitioners community to provide authoritative recommendations
for Kubernetes environments. However, Wojciech Mazurczyk and Luca Caviglione’s March 2021 Communications of the
ACM (CACM) article
<a href="https://oreil.ly/RUHQb">“Cyber Reconnaissance Techniques”</a> provides an excellent starting point for your own
work to secure the human aspect of the perimeter.</p>

<figure><div id="fig-cyber-attack-phases-models" class="figure">
<img src="Images/haku_1003.png" alt="Cyber attack phases models (source: CACM Cyber Reconnaissance Techniques article)" width="1329" height="1533"/>
<h6><span class="label">Figure 10-3. </span>Cyber attack phases models (source: <a href="https://oreil.ly/RUHQb">“Cyber Reconnaissance Techniques”</a>)</h6>
</div></figure>

<p>Social engineering works particularly well in large and hierachical organizations.
As disempowered individuals, potentially feeling like a cog in the machine
and on top of that maybe a lack of ownership (“this is not my money at risk”),
you are easy prey.</p>

<p class="pagebreak-before">So what are common patterns and techniques to watch out for? How can you
combat them?</p>
<dl>
<dt>Appeal to authority</dt>
<dd>
<p>You get an email from your skip-level manager saying that, due to a recently discovered CVE, your application needs to be patched. Click the following link to kick off a Helm upgrade in the CD pipeline. Boom!</p>
</dd>
</dl>

<p>Mitigation: You want to flag external senders, for example, with an <code>EXTERNAL</code> in the subject line, ask your team members to regularly do trainings and fire drills, and audit everything.</p>
<dl>
<dt>Urgency</dt>
<dd>
<p>We repeatedly pointed out that under pressure, when the clock is ticking, the risk for making mistakes increases dramatically. A popular social engineering attack uses some urgency (launch, customer complaints, etc.) to trick you into sharing credentials or applying changes to the cluster (“please merge this PR quickly to address X”).</p>
</dd>
</dl>

<p>Mitigation: When under pressure, slow down. Pair up and speak out loud, double check and verify before applying any change.</p>
<dl>
<dt>Indirect</dt>
<dd>
<p>As discussed in detail in <a data-type="xref" href="ch04.xhtml#ch-apps-supply-chain">Chapter 4</a>, supply chains are prone to being poisoned and malware can be ingested in a less-secure environment and activated in the target environment.</p>
</dd>
</dl>

<p>Mitigation: Make sure to have a proactive approach re: supply chain management and at the very least scan all the items entering your perimeter. For example, static container image scanning is tablestakes now.</p>

<p>These shouldn’t scare you, just make you aware. Let’s move on to some regulatory
challenges<a data-type="indexterm" data-primary="social engineering principles" data-startref="soceng_princ" id="idm45302807152560"/> now.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Privacy and Regulatory Concerns"><div class="sect2" id="priv-regulations">
<h2>Privacy and Regulatory Concerns</h2>

<p>Many enterprises, <a data-type="indexterm" data-primary="privacy concerns" id="idm45302807149888"/><a data-type="indexterm" data-primary="regulatory concerns" id="idm45302807149152"/>in order to do business in certain legislatures, need to be
compliant with certain regulations. This can be general-purpose regulations that
are valid and enforceable in a certain geo, such as the General Data Protection Regulation
(GDPR) in the EU, or pertaining to vertical-specific concerns
such as PCI-DSS or Sarbanes–Oxley. No matter if the intention of the regulation
is to enforce end-user privacy, protect investments, or increase
accountability, you want to make sure you are familiar with the
requirements and find ways to implement these policies and controls as required
(see <a data-type="xref" href="ch08.xhtml#ch-policy">Chapter 8</a> for some technology-related recommendations). In any
case, make sure to involve legal personnel; we ain’t no lawyers and that’s why we
don’t provide any directions beyond the technical ones, in this context.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45302807146464">
<h1>Conclusion</h1>

<p>In this chapter we tried to highlight the fact that technology often is the
“easy” part and the weakest link, as so often in a security context, is the human.
Different environments such as cloud or on-prem offer different trade-offs. There
is no right or wrong, just an awareness of where your priorities are and what you
want to own yourself and which parts you want to offload.</p>

<p>We also have reached the end of the book, but unfortunately not of the Captain
and their crew! We nevertheless hope this was useful food for thought for you
and wish you the best of luck and success in implementing a secure and sustainable
way to use and benefit from Kubernetes.</p>
</div></section>







</div></section></div></body></html>