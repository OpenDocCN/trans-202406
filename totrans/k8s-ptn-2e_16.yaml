- en: Chapter 13\. Service Discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Service Discovery* pattern provides a stable endpoint through which consumers
    of a service can access the instances providing the service. For this purpose,
    Kubernetes provides multiple mechanisms, depending on whether the service consumers
    and producers are located on or off the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Applications deployed on Kubernetes rarely exist on their own, and usually
    they have to interact with other services within the cluster or systems outside
    the cluster. The interaction can be initiated internally within the service or
    through external stimulus. Internally initiated interactions are usually performed
    through a polling consumer: either after startup or later, an application connects
    to another system and starts sending and receiving data. Typical examples are
    an application running within a Pod that reaches a file server and starts consuming
    files, or a message that connects to a message broker and starts receiving or
    sending messages, or an application that uses a relational database or a key-value
    store and starts reading or writing data.'
  prefs: []
  type: TYPE_NORMAL
- en: The critical distinction here is that the application running within the Pod
    decides at some point to open an outgoing connection to another Pod or external
    system and starts exchanging data in either direction. In this scenario, we don’t
    have an external stimulus for the application, and we don’t need any additional
    setup in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the patterns described in [Chapter 7, “Batch Job”](ch07.html#BatchJob),
    or [Chapter 8, “Periodic Job”](ch08.html#PeriodicJob), we often use this technique.
    In addition, long-running Pods in DaemonSets or ReplicaSets sometimes actively
    connect to other systems over the network. The more common use case for Kubernetes
    workloads occurs when we have long-running services expecting external stimulus,
    most commonly in the form of incoming HTTP connections from other Pods within
    the cluster or external systems. In these cases, service consumers need a mechanism
    for discovering Pods that are dynamically placed by the scheduler and sometimes
    elastically scaled up and down.
  prefs: []
  type: TYPE_NORMAL
- en: It would be a significant challenge if we had to track, register, and discover
    endpoints of dynamic Kubernetes Pods ourselves. That is why Kubernetes implements
    the *Service Discovery* pattern through different mechanisms, which we explore
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we look at the “Before Kubernetes Era,” the most common mechanism of service
    discovery was through client-side discovery. In this architecture, when a service
    consumer had to call another service that might be scaled to multiple instances,
    the service consumer would have a discovery agent capable of looking at a registry
    for service instances and then choosing one to call. Classically, that would be
    done, for example, either with an embedded agent within the consumer service (such
    as a ZooKeeper client, Consul client, or Ribbon) or with another colocated process
    looking up the service in a registry, as shown in [Figure 13-1](#img-service-discovery-client).
  prefs: []
  type: TYPE_NORMAL
- en: '![Client-side service discovery](assets/kup2_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. Client-side service discovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the “Post Kubernetes Era,” many of the nonfunctional responsibilities of
    distributed systems such as placement, health checks, healing, and resource isolation
    are moving into the platform, and so is service discovery and load balancing.
    If we use the definitions from service-oriented architecture (SOA), a service
    provider instance still has to register itself with a service registry while providing
    the service capabilities, and a service consumer has to access the information
    in the registry to reach the service.
  prefs: []
  type: TYPE_NORMAL
- en: In the Kubernetes world, all that happens behind the scenes so that a service
    consumer calls a fixed virtual Service endpoint that can dynamically discover
    service instances implemented as Pods. [Figure 13-2](#img-service-discovery-server)
    shows how registration and lookup are embraced by Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Server-side service discovery on Kubernetes](assets/kup2_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. Server-side service discovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At first glance, *Service Discovery* may seem like a simple pattern. However,
    multiple mechanisms can be used to implement this pattern, which depends on whether
    a service consumer is within or outside the cluster and whether the service provider
    is within or outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Internal Service Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume we have a web application and want to run it on Kubernetes. As
    soon as we create a Deployment with a few replicas, the scheduler places the Pods
    on the suitable nodes, and each Pod gets a cluster-internal IP address assigned
    before starting up. If another client service within a different Pod wishes to
    consume the web application endpoints, there isn’t an easy way to know the IP
    addresses of the service provider Pods in advance.
  prefs: []
  type: TYPE_NORMAL
- en: This challenge is what the Kubernetes Service resource addresses. It provides
    a constant and stable entry point for a collection of Pods offering the same functionality.
    The easiest way to create a Service is through `kubectl expose`, which creates
    a Service for a Pod or multiple Pods of a Deployment or ReplicaSet. The command
    creates a virtual IP address referred to as the `clusterIP`, and it pulls both
    Pod selectors and port numbers from the resources to create the Service definition.
    However, to have full control over the definition, we create the Service manually,
    as shown in [Example 13-1](#ex-service-discovery-service).
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-1\. A simple Service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_discovery_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Selector matching Pod labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_discovery_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Port over which this Service can be contacted.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_service_discovery_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Port on which the Pods are listening.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition in this example will create a Service named `random-generator`
    (the name is important for discovery later) and `type: ClusterIP` (which is the
    default) that accepts TCP connections on port 80 and routes them to port 8080
    on all the matching Pods with the selector `app: random-generator`. It doesn’t
    matter when or how the Pods are created—any matching Pod becomes a routing target,
    as illustrated in [Figure 13-3](#img-service-discovery-internal).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Internal service discovery](assets/kup2_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. Internal service discovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The essential points to remember here are that once a Service is created, it
    gets a `clusterIP` assigned that is accessible only from within the Kubernetes
    cluster (hence the name), and that IP remains unchanged as long as the Service
    definition exists. However, how can other applications within the cluster figure
    out what this dynamically allocated `clusterIP` is? There are two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovery through environment variables
  prefs: []
  type: TYPE_NORMAL
- en: When Kubernetes starts a Pod, its environment variables get populated with the
    details of all Services that exist up to that moment. For example, our `random-generator`
    Service listening on port 80 gets injected into any newly starting Pod, as the
    environment variables shown in [Example 13-2](#ex-service-discovery-env) demonstrate.
    The application running that Pod would know the name of the Service it needs to
    consume and can be coded to read these environment variables. This lookup is a
    simple mechanism that can be used from applications written in any language and
    is also easy to emulate outside the Kubernetes cluster for development and testing
    purposes. The main issue with this mechanism is the temporal dependency on Service
    creation. Since environment variables cannot be injected into already-running
    Pods, the Service coordinates are available only for Pods started after the Service
    is created in Kubernetes. That requires the Service to be defined before starting
    the Pods that depend on the Service—or if this is not the case, the Pods need
    to be restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-2\. Service-related environment variables set automatically in Pod
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discovery through DNS lookup
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes runs a DNS server that all the Pods are automatically configured
    to use. Moreover, when a new Service is created, it automatically gets a new DNS
    entry that all Pods can start using. Assuming a client knows the name of the Service
    it wants to access, it can reach the Service by a fully qualified domain name
    (FQDN) such as `random-generator.default.svc.cluster.local`. Here, `random-generator`
    is the name of the Service, `default` is the name of the namespace, `svc` indicates
    it is a Service resource, and `cluster.local` is the cluster-specific suffix.
    We can omit the cluster suffix if desired, and the namespace as well when accessing
    the Service from the same namespace.
  prefs: []
  type: TYPE_NORMAL
- en: The DNS discovery mechanism doesn’t suffer from the drawbacks of the environment-variable-based
    mechanism, as the DNS server allows lookup of all Services to all Pods as soon
    as a Service is defined. However, you may still need to use the environment variables
    to look up the port number to use if it is a nonstandard one or unknown by the
    service consumer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some other high-level characteristics of the Service with `type: ClusterIP`
    that other types build upon:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple ports
  prefs: []
  type: TYPE_NORMAL
- en: A single Service definition can support multiple source and target ports. For
    example, if your Pod supports both HTTP on port 8080 and HTTPS on port 8443, there
    is no need to define two Services. A single Service can expose both ports on 80
    and 443, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Session affinity
  prefs: []
  type: TYPE_NORMAL
- en: 'When there is a new request, the Service randomly picks a Pod to connect to
    by default. That can be changed with `sessionAffinity: ClientIP`, which makes
    all requests originating from the same client IP stick to the same Pod. Remember
    that Kubernetes Services performs L4 transport layer load balancing, and it cannot
    look into the network packets and perform application-level load balancing such
    as HTTP cookie-based session affinity.'
  prefs: []
  type: TYPE_NORMAL
- en: Readiness probes
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4, “Health Probe”](ch04.html#HealthProbe), you learned how to define
    a `readinessProbe` for a container. If a Pod has defined readiness checks, and
    they are failing, the Pod is removed from the list of Service endpoints to call
    even if the label selector matches the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual IP
  prefs: []
  type: TYPE_NORMAL
- en: 'When we create a Service with `type: ClusterIP`, it gets a stable virtual IP
    address. However, this IP address does not correspond to any network interface
    and doesn’t exist in reality. It is the kube-proxy that runs on every node that
    picks this new Service and updates the iptables of the node with rules to catch
    the network packets destined for this virtual IP address and replaces it with
    a selected Pod IP address. The rules in the iptables do not add ICMP rules, but
    only the protocol specified in the Service definition, such as TCP or UDP. As
    a consequence, it is not possible to `ping` the IP address of the Service as that
    operation uses the ICMP.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing ClusterIP
  prefs: []
  type: TYPE_NORMAL
- en: During Service creation, we can specify an IP to use with the field `.spec.clusterIP`.
    It must be a valid IP address and within a predefined range. While not recommended,
    this option can turn out to be handy when dealing with legacy applications configured
    to use a specific IP address, or if there is an existing DNS entry we wish to
    reuse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes Services with `type: ClusterIP` are accessible only from within
    the cluster; they are used for discovery of Pods by matching selectors and are
    the most commonly used type. Next, we will look at other types of Services that
    allow discovery of endpoints that are manually specified.'
  prefs: []
  type: TYPE_NORMAL
- en: Manual Service Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we create a Service with `selector`, Kubernetes tracks the list of matching
    and ready-to-serve Pods in the list of endpoint resources. For [Example 13-1](#ex-service-discovery-service),
    you can check all endpoints created on behalf of the Service with `kubectl get
    endpoints random-generator`. Instead of redirecting connections to Pods within
    the cluster, we could also redirect connections to external IP addresses and ports.
    We can do that by omitting the `selector` definition of a Service and manually
    creating endpoint resources, as shown in [Example 13-3](#ex-service-discovery-service-plain).
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-3\. Service without selector
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, in [Example 13-4](#ex-service-discovery-endpoints), we define an endpoint
    resource with the same name as the Service and containing the target IPs and ports.
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-4\. Endpoints for an external service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_discovery_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Name must match the Service that accesses these endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: This Service is also accessible only within the cluster and can be consumed
    in the same way as the previous ones, through environment variables or DNS lookup.
    The difference is that the list of endpoints is manually maintained and those
    values usually point to IP addresses outside the cluster, as demonstrated in [Figure 13-4](#img-service-discovery-manual-discovery).
  prefs: []
  type: TYPE_NORMAL
- en: While connecting to an external resource is this mechanism’s most common use,
    it is not the only one. Endpoints can hold IP addresses of Pods but not virtual
    IP addresses of other Services. One good thing about the Service is that it allows
    you to add and remove selectors and point to external or internal providers without
    deleting the resource definition that would lead to a Service IP address change.
    So service consumers can continue using the same Service IP address they first
    pointed to while the actual service provider implementation is migrated from on-premises
    to Kubernetes without affecting the client.
  prefs: []
  type: TYPE_NORMAL
- en: '![Manual service discovery](assets/kup2_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. Manual service discovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this category of manual destination configuration, there is one more type
    of Service, as shown in [Example 13-5](#ex-service-discovery-external-name).
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-5\. Service with an external destination
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This Service definition does not have a `selector` either, but its type is `ExternalName`.
    That is an important difference from an implementation point of view. This Service
    definition maps to the content pointed by `externalName` using DNS only, or more
    specifically, `database-service.<namespace>.svc.cluster.local` will now point
    to `my.database.example.com`. It is a way of creating an alias for an external
    endpoint using DNS CNAME rather than going through the proxy with an IP address.
    But fundamentally, it is another way of providing a Kubernetes abstraction for
    endpoints located outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Service Discovery from Outside the Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The service discovery mechanisms discussed so far in this chapter all use a
    virtual IP address that points to Pods or external endpoints, and the virtual
    IP address itself is accessible only from within the Kubernetes cluster. However,
    a Kubernetes cluster doesn’t run disconnected from the rest of the world, and
    in addition to connecting to external resources from Pods, very often the opposite
    is also required—external applications wanting to reach to endpoints provided
    by the Pods. Let’s see how to make Pods accessible for clients living outside
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first method to create a Service and expose it outside of the cluster is
    through `type: NodePort`. The definition in [Example 13-6](#ex-service-discovery-node-port)
    creates a Service as earlier, serving Pods that match the selector `app: random-generator`,
    accepting connections on port 80 on the virtual IP address and routing each to
    port 8080 of the selected Pod. However, in addition to all of that, this definition
    also reserves port 30036 on all the nodes and forwards incoming connections to
    the Service. This reservation makes the Service accessible internally through
    the virtual IP address, as well as externally through a dedicated port on every
    node.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-6\. Service with type NodePort
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_discovery_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Open port on all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_discovery_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Specify a fixed port (which needs to be available) or leave this out to get
    a randomly selected port assigned.
  prefs: []
  type: TYPE_NORMAL
- en: While this method of exposing services (illustrated in [Figure 13-5](#img-service-discovery-node-port))
    may seem like a good approach, it has drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Node port Service Discovery](assets/kup2_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. Node port service discovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s see some of its distinguishing characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Port number
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of picking a specific port with `nodePort: 30036`, you can let Kubernetes
    pick a free port within its range.'
  prefs: []
  type: TYPE_NORMAL
- en: Firewall rules
  prefs: []
  type: TYPE_NORMAL
- en: Since this method opens a port on all the nodes, you may have to configure additional
    firewall rules to let external clients access the node ports.
  prefs: []
  type: TYPE_NORMAL
- en: Node selection
  prefs: []
  type: TYPE_NORMAL
- en: An external client can open connection to any node in the cluster. However,
    if the node is not available, it is the responsibility of the client application
    to connect to another healthy node. For this purpose, it may be a good idea to
    put a load balancer in front of the nodes that picks healthy nodes and performs
    failover.
  prefs: []
  type: TYPE_NORMAL
- en: Pods selection
  prefs: []
  type: TYPE_NORMAL
- en: 'When a client opens a connection through the node port, it is routed to a randomly
    chosen Pod that may be on the same node where the connection was open or a different
    node. It is possible to avoid this extra hop and always force Kubernetes to pick
    a Pod on the node where the connection was opened by adding `externalTrafficPolicy:
    Local` to the Service definition. When this option is set, Kubernetes does not
    allow you to connect to Pods located on other nodes, which can be an issue. To
    resolve that, you have to either make sure there are Pods placed on every node
    (e.g., by using daemon services) or make sure the client knows which nodes have
    healthy Pods placed on them.'
  prefs: []
  type: TYPE_NORMAL
- en: Source addresses
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some peculiarities around the source addresses of packets sent to
    different types of Services. Specifically, when we use type `NodePort`, client
    addresses are source NAT’d, which means the source IP addresses of the network
    packets containing the client IP address are replaced with the node’s internal
    addresses. For example, when a client application sends a packet to node 1, it
    replaces the source address with its node address, replaces the destination address
    with the Pod’s address, and forwards the packet to node 2, where the Pod is located.
    When the Pod receives the network packet, the source address is not equal to the
    original client’s address but is the same as node 1’s address. To prevent this
    from happening, we can set `externalTrafficPolicy: Local` as described earlier
    and forward traffic only to Pods located on node 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to perform Service Discovery for external clients is through a
    load balancer. You have seen how a `type: NodePort` Service builds on top of a
    regular Service with `type: ClusterIP` by also opening a port on every node. The
    limitation of this approach is that we still need a load balancer for client applications
    to pick a healthy node. The Service type `LoadBalancer` addresses this limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to creating a regular Service, and opening a port on every node,
    as with `type: NodePort`, it also exposes the service externally using a cloud
    provider’s load balancer. [Figure 13-6](#img-service-discovery-load-balancer)
    shows this setup: a proprietary load balancer serves as a gateway to the Kubernetes
    cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Load balancer Service Discovery](assets/kup2_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Load balancer service discovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So this type of Service works only when the cloud provider has Kubernetes support
    and provisions a load balancer. We can create a Service with a load balancer by
    specifying the type `LoadBalancer`. Kubernetes then will add IP addresses to the
    `.spec` and `.status` fields, as shown in [Example 13-7](#ex-service-discovery-load-balancer).
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-7\. Service of type LoadBalancer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_discovery_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes assigns `clusterIP` and `loadBalancerIP` when they are available.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_discovery_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `status` field is managed by Kubernetes and adds the Ingress IP.
  prefs: []
  type: TYPE_NORMAL
- en: With this definition in place, an external client application can open a connection
    to the load balancer, which picks a node and locates the Pod. The exact way that
    load-balancer provisioning and service discovery are performed varies among cloud
    providers. Some cloud providers will allow you to define the load-balancer address
    and some will not. Some offer mechanisms for preserving the source address, and
    some replace that with the load-balancer address. You should check the specific
    implementation provided by your cloud provider of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Yet another type of Service is available: *headless* services, for which you
    don’t request a dedicated IP address. You create a headless service by specifying
    `clusterIP None` within the Service’s `spec` section. For headless services, the
    backing Pods are added to the internal DNS server and are most useful for implementing
    Services to StatefulSets, as described in detail in [Chapter 12, “Stateful Service”](ch12.html#StatefulService).'
  prefs: []
  type: TYPE_NORMAL
- en: Application Layer Service Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the mechanisms discussed so far, Ingress is not a service type but a
    separate Kubernetes resource that sits in front of Services and acts as a smart
    router and entry point to the cluster. Ingress typically provides HTTP-based access
    to Services through externally reachable URLs, load balancing, TLS termination,
    and name-based virtual hosting, but there are also other specialized Ingress implementations.
    For Ingress to work, the cluster must have one or more Ingress controllers running.
    A simple Ingress that exposes a single Service is shown in [Example 13-8](#ex-service-discovery-ingress).
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-8\. An Ingress definition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on the infrastructure Kubernetes is running on, and the Ingress controller
    implementation, this definition allocates an externally accessible IP address
    and exposes the `random-generator` Service on port 80\. But this is not very different
    from a Service with `type: LoadBalancer`, which requires an external IP address
    per Service definition. The real power of Ingress comes from reusing a single
    external load balancer and IP to service multiple Services and reduce the infrastructure
    costs. A simple fan-out configuration for routing a single IP address to multiple
    Services based on HTTP URI paths looks like [Example 13-9](#ex-service-discovery-ingress-with-mapping).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 13-9\. A definition for Nginx Ingress controller
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_service_discovery_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated rules for the Ingress controller for dispatching requests based on
    the request path.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_service_discovery_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Redirect every request to Service `random-generator`…​
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_service_discovery_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: …​ except `/cluster-status`, which goes to another Service.
  prefs: []
  type: TYPE_NORMAL
- en: Since every Ingress controller implementation is different, apart from the usual
    Ingress definition, a controller may require additional configuration, which is
    passed through annotations. Assuming the Ingress is configured correctly, the
    preceding definition would provision a load balancer and get an external IP address
    that services two Services under two different paths, as shown in [Figure 13-7](#img-service-discovery-ingress).
  prefs: []
  type: TYPE_NORMAL
- en: Ingress is the most powerful and at the same time most complex service discovery
    mechanism on Kubernetes. It is most useful for exposing multiple services under
    the same IP address and when all services use the same L7 (typically HTTP) protocol.
  prefs: []
  type: TYPE_NORMAL
- en: '![Application layer service discovery](assets/kup2_1307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-7\. Application layer service discovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the favorite service discovery mechanisms on Kubernetes.
    Discovery of dynamic Pods from within the cluster is always achieved through the
    Service resource, though different options can lead to different implementations.
    The Service abstraction is a high-level cloud native way of configuring low-level
    details such as virtual IP addresses, iptables, DNS records, or environment variables.
    Service discovery from outside the cluster builds on top of the Service abstraction
    and focuses on exposing the Services to the outside world. While a `NodePort`
    provides the basics of exposing Services, a highly available setup requires integration
    with the platform infrastructure provider.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 13-1](#table-service-discovery-types) summarizes the various ways service
    discovery is implemented in Kubernetes. This table aims to organize the various
    service discovery mechanisms in this chapter from more straightforward to more
    complex. We hope it can help you build a mental model and understand them better.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-1\. Service Discovery mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Configuration | Client type | Summary |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ClusterIP |'
  prefs: []
  type: TYPE_TB
- en: '`type: ClusterIP`'
  prefs: []
  type: TYPE_NORMAL
- en: '`.spec.selector`'
  prefs: []
  type: TYPE_NORMAL
- en: '| Internal | The most common internal discovery mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| Manual IP |'
  prefs: []
  type: TYPE_TB
- en: '`type: ClusterIP`'
  prefs: []
  type: TYPE_NORMAL
- en: '`kind: Endpoints`'
  prefs: []
  type: TYPE_NORMAL
- en: '| Internal | External IP discovery |'
  prefs: []
  type: TYPE_TB
- en: '| Manual FQDN |'
  prefs: []
  type: TYPE_TB
- en: '`type: ExternalName`'
  prefs: []
  type: TYPE_NORMAL
- en: '`.spec.externalName`'
  prefs: []
  type: TYPE_NORMAL
- en: '| Internal | External FQDN discovery |'
  prefs: []
  type: TYPE_TB
- en: '| Headless Service |'
  prefs: []
  type: TYPE_TB
- en: '`type: ClusterIP`'
  prefs: []
  type: TYPE_NORMAL
- en: '`.spec.clusterIP: None`'
  prefs: []
  type: TYPE_NORMAL
- en: '| Internal | DNS-based discovery without a virtual IP |'
  prefs: []
  type: TYPE_TB
- en: '| NodePort | `type: NodePort` | External | Preferred for non-HTTP traffic |'
  prefs: []
  type: TYPE_TB
- en: '| LoadBalancer | `type: LoadBalancer` | External | Requires supporting cloud
    infrastructure |'
  prefs: []
  type: TYPE_TB
- en: '| Ingress | `kind: Ingress` | External | L7/HTTP-based smart routing mechanism
    |'
  prefs: []
  type: TYPE_TB
- en: This chapter gave a comprehensive overview of all the core concepts in Kubernetes
    for accessing and discovering services. However, the journey does not stop here.
    With the *Knative* project, new primitives on top of Kubernetes have been introduced,
    which help application developers with advanced serving and eventing.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the *Service Discovery* pattern, the *Knative Serving* subproject
    is of particular interest as it introduces a new Service resource with the same
    kind as the Services introduced here (but with a different API group). Knative
    Serving provides support for application revision but also for a very flexible
    scaling of services behind a load balancer. We give a short shout-out to Knative
    Serving in [“Knative”](ch29.html#elastic-scale-knative), but a full discussion
    of Knative is beyond the scope of this book. In [“More Information”](ch29.html#elastic-scale-more-information),
    you will find links that point to detailed information about Knative.
  prefs: []
  type: TYPE_NORMAL
- en: More Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Service Discovery Example](https://oreil.ly/nagmD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kubernetes Service](https://oreil.ly/AEDi5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DNS for Services and Pods](https://oreil.ly/WRT5H)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Debug Services](https://oreil.ly/voVbw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Source IP](https://oreil.ly/mGjzg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Create an External Load Balancer](https://oreil.ly/pzOiM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ingress](https://oreil.ly/Idv2c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kubernetes NodePort Versus LoadBalancer Versus Ingress? When Should I Use
    What?](https://oreil.ly/W4i8U)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kubernetes Ingress Versus OpenShift Route](https://oreil.ly/fXicP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
