- en: Chapter 3\. Advanced Resource Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing available resources is a critical aspect of effectively running Kubernetes
    workloads in production. Without the proper sizing of workloads and management
    of CPU, memory, disk, graphics processing units (GPUs), pods, containers, and
    other resources, it is impossible for engineers and operations teams to control
    the service-level agreement (SLA) of applications and services between a client
    and provider. This SLA is the defining contract that determines the level of availability
    and performance that the client can expect from the system and is often backed
    by financial penalties.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll discuss a variety of tools and techniques available to the Kubernetes
    engineer for controlling the allocation of these resources. In this chapter, we
    begin with a discussion of proper scheduling of pod resources, where we cover
    topics like priority scheduling, quality of service, and the impact resource limits
    can have on scheduling pods. Next, we provide an overview of capacity planning
    and management approaches for ensuring the scalability of your Kubernetes platform.
    We then conclude this chapter with a discussion of admission controllers and how
    they can be used to enforce additional constraints on resources.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Resources and Scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proper scheduling of workload is critical to maintaining the availability and
    performance of your applications. Without effective scheduling, you can end up
    with an overloaded worker node with insufficient memory and CPU resources. The
    most desired outcome in these situations is a graceful shutdown of workload instances
    that have additional replicas running in the system, resulting in little or no
    service disruption. In contrast, the worst-case scenario is that the Linux Out
    of Memory (OOM) Killer^([1](ch03.html#ch01fn21)) comes through and starts randomly
    destroying processes. In extreme cases, an improperly configured `kubelet` component
    on a Kubernetes node could actually destroy the worker node itself.
  prefs: []
  type: TYPE_NORMAL
- en: Driving Scheduler Decisions via Resource Requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the key mechanisms that Kubernetes uses for making scheduling decisions
    is the resource request construct. A *resource request* for a container is the
    mechanism that the developer uses to inform Kubernetes how much CPU, memory, and
    disk resources will be needed to run the associated container. The [Kubernetes
    official documentation on pod resources](https://oreil.ly/Sxnzu) provides an excellent
    overview of resource requests. Let’s take a look at a basic Pod that uses resource
    requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: All of this information provided in the pod and container definition are the
    hints used by the Kubernetes scheduler for placement in the cluster. When we look
    more closely at scheduling, we’ll gain a better understanding of how the scheduler
    processes this information.
  prefs: []
  type: TYPE_NORMAL
- en: Node Available Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the scheduling process, Kubernetes is looking for nodes that fit the
    requested resources of the pod to be scheduled. To determine this, the scheduler
    is looking at allocatable resources minus allocated resources to find available
    resources. *Allocatable resources* are the resources that can be consumed by user-controlled
    pods on a worker node. In Kubernetes, the allocatable resource quantity for a
    worker node is defined by the total resource available in the node minus the capacity
    that is reserved for system daemons and Kubernetes runtime components.^([2](ch03.html#ch01fn22))
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `--kube-reserved` and `--system-reserved` `kubelet` flags are critical to
    maintaining the stability of your worker nodes. Without proper configuration,
    the user pods can easily overwhelm the resources available on the node and start
    to compete with system processes and `kubelet`, kube-proxy, and other container
    runtime components for resources. We recommend a healthy reservation for kube
    and system components to ensure the health of the node when faced with hungry
    user container components.
  prefs: []
  type: TYPE_NORMAL
- en: For administrators managing Kubernetes in production, it’s critical to properly
    configure these resource reservations for both the Kubernetes platform and the
    system. What’s the best way to determine how to set these flags? Experience and
    real-world testing are invaluable. Trial and error aren’t fun, but given how varied
    every Kubernetes worker may be, it’s worth spending some time testing out various
    settings. A good starting place may lie with your friendly cloud provider. IBM
    Cloud has done extensive scale testing and evaluation of production systems to
    come up with [a set of safe resource reservations](https://oreil.ly/7g71U). As
    mentioned in the upstream documentation, these reservations are largely based
    on pod limits per node, kube resource reservations, and kernel memory for system
    resource reservations. If your configuration has any significant system-level
    runtime components, you may need to adjust.
  prefs: []
  type: TYPE_NORMAL
- en: These reservations play two key roles. Here we are looking at how they impact
    allocatable resources for each worker. Later in this chapter, we’ll talk about
    the post-scheduling life cycle where these reservations play a role in the life
    cycle of a pod with pod quality of service.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re now armed with critical knowledge for the scheduling process with resource
    reservations and node-allocatable resources. Finding a node that fits our resource
    requirements is not the only metric at play in scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes uses several different factors to determine the placement of pods
    on worker nodes. Let’s cover some of the basic concepts of scheduling and how
    you can leverage this knowledge to build applications and services with higher
    availability and performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [kube-scheduler](https://oreil.ly/jca9A) uses a two-phase process that
    first filters for nodes that can run the pod and then scores the filtered nodes
    to determine which is a best fit. A number of [predicates](https://oreil.ly/oVThs)
    are used to determine if a node is fit for running a given pod. The [priorities](https://oreil.ly/NA3Z9)
    then rank the remaining nodes to determine final placement. There are numerous
    predicates and policies. We’ll cover a few that have the greatest impact on the
    day-to-day operation of a Kubernetes or OpenShift cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PodFitsResources`'
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly considered predicate, `PodFitsResources` evaluates the resource
    requests of the pods and filters out any nodes that do not have sufficient available
    resources. There may be instances where there are sufficient total available resources
    in the cluster but not one node has enough resources available to fit the pod.
    We’ll discuss pod priority and preemption, which can assist with this, in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: '`PodMatchNodeSelector`'
  prefs: []
  type: TYPE_NORMAL
- en: While seemingly not that complex, this predicate is responsible for handling
    all of the pod and node affinity and anti-affinity rules that can be specified.
    These rules are critical for handling blast radius control and the availability
    of applications across zones.
  prefs: []
  type: TYPE_NORMAL
- en: '`PodToleratesNodeTaints`'
  prefs: []
  type: TYPE_NORMAL
- en: Taints are often used to isolate groups of nodes that may be dedicated to specific
    workloads in a cluster. In some cases, administrators may use this to reserve
    a set of nodes for a specific namespace or tenant.^([3](ch03.html#ch01fn23))
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you think about availability, you should consider the “blast radius” of
    any specific failure. If a node fails, all pods on that node will become unavailable.
    If an availability zone fails, all nodes supporting the control plane and the
    application pods will become unavailable. When you think about the “blast radius,”
    it really is just a way of reasoning about the impact (and the cascading effects)
    of that failure.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Priority and Preemption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are occasions where a pod with resource needs that cannot be met needs
    to be scheduled. If [pod priority and preemption](https://oreil.ly/nrgbg) are
    used, the scheduler can evict lower-priority pods to make room for the higher-priority
    pod. Priority classes will also factor into which pods will be scheduled first.
    If a high-priority pod and a low-priority pod are both pending, the low-priority
    pod will not be scheduled until the high-priority pod is running.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting use case for priority classes is to have pods with larger resource
    requests be assigned to the higher priority class. In some situations, this can
    help by moving smaller pods onto other nodes and making room for a larger pod
    to fit. While this can result in smaller, lower-priority pods being unscheduled
    and stuck in Pending, it does help to better use the capacity of your cluster
    by compressing smaller pods into the gaps available on nodes. Let’s consider an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initial state:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pod P has resource requests of CPU: 1,000, priority 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pod Q has resource requests of CPU: 100, priority 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node N has 900 available CPU out of 1,000 total
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node O has 300 available CPU out of 300 total
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod P is Pending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod Q is Running on Node N
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, Pod P will not fit on either Node N or Node O. However, because
    Pod P has higher priority than Pod Q, Q will be evicted from Node N; thus, Pod
    P can fit onto Node N and be scheduled. Pod Q will then enter the scheduler again
    and can fit on Node O. The result is that both pods are now scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: 'End state:'
  prefs: []
  type: TYPE_NORMAL
- en: Pod P is running on Node N
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod Q is running on Node O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node N has 0 available CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node O has 200 available CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may not be a common use case for users, but it is an interesting process
    that can be used to maximize the utilization of the available node resources.
  prefs: []
  type: TYPE_NORMAL
- en: Post-Scheduling Pod Life Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our pod has been scheduled to a node, we’re done, right? Not so. Once
    the pod is running on a node, a number of factors will determine the ongoing life
    cycle of the pod. Kubernetes controls the resource consumption of pods, may evict
    pods to protect the health of the node, and may even preempt a running pod to
    make way for a higher-priority pod, as discussed in [“Scheduling”](#scheduling).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already reviewed resource requests, which are used for making scheduling
    decisions in Kubernetes. Once pods are in the Running state, resource limits are
    the attributes most critical to the pod’s life cycle. It’s worth noting that resource
    requests continue to serve a valuable purpose as they help to determine the quality
    of service of the pod and are factored into eviction decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Pod Quality of Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The intersection of requests and limits is [*quality of service* (QoS)](https://oreil.ly/QukmD).
    QoS does not have any impact on scheduling; only requests are factored here. However,
    QoS does determine the pod eviction selection process and what we would have called
    overcommit in the world of virtualized infrastructure. There are yet other factors
    in eviction that we’ll discuss in detail in [“Node Eviction”](#node_eviction).
  prefs: []
  type: TYPE_NORMAL
- en: Before containers, there were VMs. A VM was a huge leap forward in efficiency
    for developers and operations teams alike. The VM allowed users to take a single
    physical computer system and subdivide it into multiple logical operating system
    instances. If you ever worked with a VM infrastructure management platform, especially
    OpenStack, then you have likely run across the concept of overcommit. Overcommit
    is a number or multiplier that determines how much more CPU, memory, and disk
    would be allocated than is actually available for a given node. Kubernetes does
    not have the notion of overcommit, but rather QoS. However, you will notice some
    similarities between the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the world of VMs, the VM creator picks a CPU and memory size for a given
    VM, and that is how much memory is carved out of the physical host system for
    that VM: no more, no less. In Kubernetes, the creator of a pod or container can
    choose how much CPU and memory they would like to have for their pod or container
    (resource request) and chooses limits separately. This allows for more efficient
    utilization of resources in the Kubernetes cluster where there are pods that are
    less sensitive to their available CPU and memory. The delta between the resource
    requests and resource limits is how Kubernetes provides the ability to overcommit
    node resources.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that some resources are compressible and some are incompressible.
    What does this mean? Well, unless it’s 1990 and you are running RAM Doubler and
    Disk Doubler, then these resources are finite and cannot be shared. These nonshareable
    resources (RAM and disk) are known as *incompressible*. When there is competition
    for disk or memory, then processes will lose out and be evicted to make room for
    other processes. However, CPU can be split, or compressed, to allow multiple processes
    to compete for CPU cycles. Let’s say that there are two processes that each wants
    to do one thousand pieces of work per time unit. If the CPU can do one thousand
    cycles per unit of time, then both processes continue to run, but they will take
    double the time to complete their task.
  prefs: []
  type: TYPE_NORMAL
- en: Pod QoS Levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have completed our detailed investigation of resource types, we
    can discuss QoS a bit further. There are three QoS levels. *Guaranteed* is the
    highest level of QoS. These are pods that have their resource requests and limits
    set to the same value for all resources. *Burstable* pods have requests set, but
    their limit values are higher, which permits them to consume more resources if
    needed. *BestEffort* pods have no requests or limits set. Following are some examples
    of resource settings for container specs within a pod:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a Guaranteed example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a Burstable example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is a BestEffort example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When `requests` and `limits` are equal, you do not need to explicitly set both.
    If only the limit is set, then the requests are automatically assumed to be the
    same as the limit by the Kubernetes scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, we mentioned that if there is competition for memory or disk, then
    one or more processes will be killed. How does Kubernetes decide which processes
    to kill? It uses QoS to make this decision. If a Kubernetes worker node comes
    under resource pressure, then it will first kill off BestEffort pods, then Burstable,
    and then Guaranteed. For a Guaranteed pod to be evicted from a node, it would
    require some system-level resource pressures.
  prefs: []
  type: TYPE_NORMAL
- en: Because this QoS is defined on a container-by-container basis and is under the
    container creator’s control, we have the opportunity for much higher resource
    utilization without putting our critical containers at risk of being starved for
    resources or potentially having their performance diminished. We get the best
    of both worlds in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: What about pod priority? It sounds a lot like QoS. However, as we saw in [“Scheduling”](#scheduling),
    pod priority affects preemption during scheduling but does not affect the eviction
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway from all this QoS discussion is that your developers’ configuration
    of resource requests and limits will have a significant impact on how your cluster
    behaves and handles pods. It’s also worth noting that using anything other than
    Guaranteed QoS can make debugging your workloads and managing capacity very difficult.
    Your developers keep asking why their pods are constantly dying, only to find
    out that they are being evicted to make room for higher QoS pods from other teams.
    As an admin, you are trying to figure out how to manage the capacity of your cluster,
    but you can’t tell how much room is really available because half of your pods
    are Burstable. Yes, there are monitoring tools that will calculate the cluster’s
    true available capacity based on allocatable resources versus requested resources
    on all your nodes, but your users are in for a rude awakening when their Burstable
    capacity starts getting reclaimed to make way for Guaranteed pods. Sure, you may
    end up leaving a bit of CPU or memory on the table, but your cluster admin and
    your development teams will have a much more predictable result in production.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Avoid the use of BestEffort QoS at all costs in production. It can result in
    very unpredictable scheduling. The result is an environment that may be very unstable
    as pods compete for resources.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Resource Limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Limits control how many resources a container is given access to after it is
    running. Various container runtimes may have different methods for implementing
    this. For our purposes, we’ll focus on traditional Linux container runtimes. The
    following rules apply for the [CRI-O](https://cri-o.io) and [containerd](https://containerd.io)
    container runtimes. The basic implementation of limits for CPU and memory are
    implemented using Linux control groups (cgroups). Specifically, in these examples,
    we are using an OpenShift 4.7 cluster and the CRI-O 1.17.4-19 container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: CPU limits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s see what this means for actual running pods. We will start by looking
    at compressible CPU resources in a Burstable configuration, as it is the most
    interesting. We have created a test deployment we can use to scale and view the
    impact on our CPU resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This sample will let us scale our workload up and down on a four-vCPU worker
    node and see what the impact is on CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'At three replicas, all pods are hitting their CPU limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'But when we scale up to higher numbers of pods, we can see there is competition
    for resources and the cgroups start slicing the CPU thinner. And if we max out
    the schedulable pods based on our CPU request of 200m, we still end up with even
    distribution of CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s take a look at what happens when we add BestEffort load. Let’s start
    with `cpu-noise`, which has no requests or limits (BestEffort) and has enough
    load to consume five vCPU if available. We start with the following load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we add a `cpu-use` pod to the mix with requests and limits, this new pod
    is given not only its requested CPU, but also its limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we scale up `cpu-use` and get to see the real difference between Burstable
    and BestEffort QoS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In these results, we see that the Burstable pods are well past their requested
    CPU resources, but the `cpu-noise` BestEffort pod is just getting scraps.
  prefs: []
  type: TYPE_NORMAL
- en: This is the part where you take note and remember that CPU requests are your
    friend. You’ll be ensuring that your pod won’t be at the bottom of the CPU barrel.
  prefs: []
  type: TYPE_NORMAL
- en: Memory limits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve taken a closer look at the rather interesting control of compressible
    CPU resources. It’s worth looking at memory, but it is not quite as interesting.
    Let’s get started with our `memory-use` workload first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is that we can fit right around six pods per 16 GB host after accounting
    for node reservations and other critical pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If we start to apply memory pressure with our `memory-noise` deployment (same
    as above, just no resource requests or limits), then we’ll start to see eviction
    and OOM Killer take over; our Guaranteed pods are spared in the mayhem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: “OOMKilled” is a status condition indicating that the process running within
    the pod was killed because it reached an OOM error. The supporting node could
    have had all memory exhausted because of unrestricted pods running on the same
    compute host, or the container process could have reached its specified memory
    limit (and was thus terminated). The oom_killer is a process in the Linux kernel
    that takes over whenever a process is identified for termination due to memory
    constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, these examples help to clarify what you can expect from the `kubelet`
    and Linux memory allocation and management techniques. Ensure that your developers
    have a clear understanding as well. Once developers realize they will be first
    in line for OOM Killer’s wrath when they don’t provide strong requests and limits
    for their containers, they typically give much better container definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Node Eviction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Eventually, we’ll exhaust the noncompressible resources of our worker node,
    and it is at that point that eviction starts to take over. We won’t go into the
    details of things like eviction thresholds here; the [official Kubernetes documentation](https://oreil.ly/imjls)
    provides plenty of information about these settings.
  prefs: []
  type: TYPE_NORMAL
- en: It is, however, worth reviewing [the process for evicting end-user pods](https://oreil.ly/G9chi).
    In short, when the `kubelet` is unable to free sufficient resources on the node
    to alleviate any resource pressure, it will first evict BestEffort or Burstable
    pods that are exceeding their resource requests. The last pods it will evict are
    those Guaranteed pods that are underutilizing their resource requests or limits.
    If only Guaranteed pods are used on a node, then resource pressure may be coming
    from system-reserved or kube-reserved processes on the node. It will start evicting
    user Guaranteed pods in self-preservation. It’s worth investigating the `kubelet`
    configuration for reserved resources to ensure a more stable and predictable node
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: The last resort in the situation where a system out of memory situation is encountered
    before pods can be gracefully evicted is the OOM Killer process itself. More details
    can be found in the [Kubernetes documentation](https://oreil.ly/hO0h2). Typically,
    this comes from a process that is consuming resources at a very rapid pace, such
    that the `kubelet` cannot address the memory-pressure situation as fast as the
    process is consuming memory. Debugging can be quite a chore. If your pods all
    have limits in place, then this becomes less of an issue since the cgroup limits
    placed on individual pods will OOM Kill the process before the node reaches memory
    pressure. Containers and pods with no memory limits are your most likely culprits
    and should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity Planning and Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with many modern platforms and service frameworks, our focus is more about
    capacity management than it is about predicting the future. You can plan for how
    you will scale your Kubernetes platform for your enterprise. Part of this planning
    should include how and when to scale master and/or worker nodes, as well as when
    it is the right time to add additional clusters to your fleet.
  prefs: []
  type: TYPE_NORMAL
- en: Single cluster or multiple clusters? It is unrealistic to expect to run a single
    cluster for all your workloads. There are some factors to consider. Geographic
    distribution, tenancy, single-cluster scalability, and blast radius are other
    factors to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Worker Node Capacity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The starting point for effective and consistent worker node capacity management
    comes from using Guaranteed QoS for your pods. While it’s possible to manage capacity
    with Burstable or BestEffort pods, it can be extremely challenging to decide when
    a cluster needs to be scaled.
  prefs: []
  type: TYPE_NORMAL
- en: What is it like to manage capacity without Guaranteed QoS? Administrators are
    left trying to formulate a guess on when scale is needed based on a combination
    of monitoring tools for the worker node resources and application metrics coming
    from the services and applications running on the cluster. Do your monitoring
    tools show that you have 25% unused memory and CPU and your application metrics
    are all hitting their *service-level objectives* (SLOs)? Fantastic, you’ve gotten
    lucky. It’s only a matter of time before those resources start getting pinched
    and your SLOs start to suffer. It’s at this point where you can just start throwing
    more and/or bigger worker nodes at the cluster in hopes that your SLOs come back
    in line. However, it may be that there are ill-behaving pods elsewhere in the
    cluster that are forcing you into this position and you are just throwing money
    away. With Guaranteed QoS, your CPU will never be compressed, and your memory
    utilization will never result in randomly evicted pods.
  prefs: []
  type: TYPE_NORMAL
- en: A number of tools are at our disposal to help monitor and manage the capacity
    of our cluster. Let’s consider some of the options available to us.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Monitoring is critical to having a strong understanding of how your capacity
    is being used. Even with all Guaranteed QoS pods, you’ll want to have monitoring
    data to see if you have significantly underutilized resources or rogue system
    or kube processes that are consuming resources. Monitoring systems gather metrics
    from the entire system and aggregate this data into a centralized tool. These
    metrics can include CPU consumption, available memory, disk performance, network
    throughput, and application-specific data. With this centralized tool, we typically
    have the ability to view historical data, as well as to define alarms and thresholds
    to notify engineers of impending issues. Together, this historical data and alerting
    provide powerful insight into the performance and scale of the system. Two first-rate
    monitoring options to consider are [Prometheus](https://prometheus.io) and [Sysdig
    Monitor](https://oreil.ly/3p9YR).
  prefs: []
  type: TYPE_NORMAL
- en: Limit ranges and quotas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve now talked extensively about resource requests and limits. Fortunately,
    Kubernetes has even more tools at your disposal to assist with controlling resource
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: '[Limit ranges](https://oreil.ly/BvBNH) enable an administrator to enforce the
    use of requests and limits. This includes setting defaults for all containers
    and pods to inject them at runtime, as well as setting minimum and maximum values
    for a namespace.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Limit ranges are really a safeguard against poorly configured pods. Administrators
    should strongly encourage application owners to set their own proper requests
    and limits based on performance testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Quotas allow an administrator to set maximum requests and limits per namespace.
    While potentially aggravating for the developer who expects unlimited access to
    resources, this gives the cluster administrator ultimate control over preventing
    resource contention. Administrators should consider setting alerts in combination
    with total cluster capacity as well as on individual namespace quotas to help
    plan for future capacity needs. Quotas provide the control needed to ensure that
    additional compute resources can be onboarded before user demand for resources
    exceeds the cluster supply.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes autoscaling comes in two flavors. The first of these is the cluster
    autoscaler, which modifies the number of worker nodes in the cluster. The other
    is workload autoscaling, which takes many forms, such as horizontal pod autoscaler,
    vertical pod autoscaler, addon-resizer, and cluster proportional autoscaler. Typically,
    a cluster administrator is more concerned with the cluster autoscaler, and the
    workload autoscaling options are more the domain of those responsible for the
    applications and services running on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster autoscaler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The [cluster autoscaler](https://oreil.ly/ccMvo) works by evaluating pods that
    remain in Pending state due to insufficient resources and responds by adding additional
    worker nodes or removing underutilized worker nodes. In clusters where most pods
    are using Guaranteed QoS, the cluster autoscaler can be a very efficient solution
    to the management of worker node capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pod autoscaler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most common form of autoscaling used is [horizontal pod autoscaling (HPA)](https://oreil.ly/Y6vOg).
    Autoscaling factors in the actual CPU utilization of a pod based on metrics provided
    via the metrics API `metrics.k8s.io` (or directly from heapster, pre-Kubernetes
    1.11 only). With this approach, the resource requests and limits just need to
    be reasonable for the given workload, and the autoscaler will look at real-world
    CPU utilization to determine when to scale. Let’s look at an example via our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a simple web app to begin exploring autoscaling. Let’s see what
    we can do from a scaling perspective. Step one—create an autoscaling policy for
    this deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Excellent! We are ready to scale! Let’s throw some load, using [Locust](http://locust.io)
    or similar, at our fancy new web application and see what happens next. Now when
    we check to see the CPU utilization of our pod, we can see it is using 43m cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is more than double the resource request we specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the HPA has increased the number of replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Utilization is still above our policy limit, and thus in time the HPA will
    continue to scale up and reduce the load below the threshold of the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to note that the metrics collection and HPA are not real-time
    systems. The [Kubernetes documentation](https://oreil.ly/9R9cE) provides a bit
    more detail about the controller-manager settings and other intricacies of the
    HPA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we reduce the load on the deployment by killing the load-generating
    pod, and it is automatically scaled down again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How does this help us in hybrid scenarios? It guarantees that regardless of
    the inherent performance of any one cluster or worker node, the autoscaler will
    ensure that we have the appropriate resources allocated to support our workload.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: HPA can be configured to use all kinds of metrics, including request response
    times or request rates, in order to meet application service-level requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical pod autoscaler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The [vertical pod autoscaler (VPA)](https://oreil.ly/ostge) is an excellent
    solution for a situation in which you have a deployment that needs to scale up
    rather than out. Whereas the HPA adds more replicas as memory and CPU utilization
    increase, the VPA increases the memory and CPU requests of your deployment. For
    this example, let’s reuse our hello example. We begin by installing the VPA according
    to the steps provided. If you recall, we started with requests of 20m. First,
    let’s apply our VPA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s apply load using a load generation tool such as [Locust](http://locust.io)
    to the application and observe the appropriate response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then view the resource requests for our `hello` deployment and see that
    they have been automatically adjusted to match real-world utilization of our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The VPA is a powerful tool that can enable us to scale our applications and
    services in a manner that is best suited to their performance characteristics.
    Scaling out an application doesn’t always provide the most efficient use of available
    resources. Leveraging the VPA in these situations can lead to improved performance
    and lower costs if applied appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster proportional autoscaler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A couple of other common autoscaler implementations are used in addition to
    the HPA. The first of these is the [cluster proportional autoscaler](https://oreil.ly/pvIDT),
    which looks at the size of the cluster in terms of workers and resource capacity
    to decide how many replicas of a given service are needed. Famously, this is used
    by CoreDNS; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The number of cores and nodes are used to determine how many replicas of CoreDNS
    are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Addon-resizer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another great example is the [addon-resizer](https://oreil.ly/3sF7E) (aka `pod_nanny`),
    which performs vertical scaling of resource requests based on cluster size. It
    scales the resource’s requests of a singleton based on the number of workers in
    the cluster. This autoscaler has been used by the Kubernetes [metrics-server](https://oreil.ly/GIyey),
    which is a core component responsible for providing a simple API frontend to basic
    worker node and pod metrics. Here we can see the use of the `pod_nanny` to scale
    the metrics-server as the cluster grows in size, with `extra-cpu` and `extra-memory`
    defining how much additional CPU and memory resources should be allocated to the
    metrics-server for each additional worker node in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This autoscaling technique is very helpful as it can resize applications based
    on known scaling measurements as cluster size grows. This can help to avoid issues
    with OOM or CPU starvation for an application before a traditional horizontal
    or vertical autoscaler would even be able to collect enough metrics to make a
    judgment on the need to scale up.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Master Capacity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kubernetes [scalability special interest group (sig-scalability)](https://oreil.ly/IjmVi)
    is a group of Kubernetes community members focused on measuring and improving
    the scalability of Kubernetes itself. The sig-scalability team has done fantastic
    work to quantify and improve the limits of a Kubernetes cluster, yet it still
    has its boundaries.^([4](ch03.html#ch01fn24)) The limits specified by the sig-scalability
    team are based primarily on the impact of adding nodes and pods. The limiting
    factor in kube scalability is the performance of etcd/kube-apiserver to handle
    the load of the controllers and objects they are managing. In the standard tests,
    there are a limited number of objects and controllers at work. In these tests,
    the `kubelet`, controller-manager, and scheduler are the primary consumers of
    etcd/apiserver resources.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-1](#recommended_resources_needed_for_kuberne) lists the recommended
    master node systems based on the number of worker nodes based on [the data from
    sig-scalability](https://oreil.ly/lXkTu).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Recommended resources needed for Kubernetes control plane based
    on cluster worker node counts
  prefs: []
  type: TYPE_NORMAL
- en: '| Worker nodes | Master vCPU | Master memory |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1–10 | 2 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 11–100 | 4 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| 101–250 | 8 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| 251–500 | 16 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| 500+ | 32 | 128 |'
  prefs: []
  type: TYPE_TB
- en: 'The [maximum cluster size documented officially](https://oreil.ly/DxmGf) is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: No more than 5,000 nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more than 150,000 total pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more than 300,000 total containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more than 100 pods per node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, as we’ll discuss in this chapter, there are far more factors to consider.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to pods and worker nodes, it’s important to consider the number
    of Kubernetes services, secrets, `ConfigMaps`, persistent volumes, and other standard
    Kubernetes objects and controllers. There have been considerable improvements
    in kube-proxy to mitigate the performance impact of a large number of services,
    but it’s still a factor. Remember that kube-proxy needs to track changes to the
    endpoints of all services in order to keep iptables or IP Virtual Server (IPVS)
    configurations up to date. There is no hard limit on the number of services that
    we know of, but more endpoints and more ports will have a huge impact on the scalability
    of each kube-proxy and increase the load on etcd/apiserver. Consider also the
    load of secrets and `ConfigMaps` on etcd/apiserver. Every mounted secret means
    another watch on that secret for the `kubelet`. In aggregate, these secret watches
    can have a significant impact on etcd/apiserver performance and scale.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One option to consider to help Kubernetes scale more efficiently is to skip
    mounting the service account secret into your pods. This means the `kubelet` will
    not need to watch this service account secret for changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how to skip mounting the service account token into the
    pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This will help with scalability of the kube-apiserver as well as the node itself.
    Workload for kube-apiserver is reduced because the `kubelet` will not need to
    call the kube-apiserver to get the token. The node performance is improved because
    there are fewer volume mounts for tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Not convinced that many factors contribute to the scalability of the cluster?
    Consider that the modern Kubernetes workload often includes a significant number
    of other objects and controllers. Kubernetes 1.7 introduced the ability for users
    to define their own objects via *custom resource definitions* (CRDs). These CRDs,
    along with the Kubernetes controller model for reconciling declared states, enabled
    users to extend the capabilities of Kubernetes to manage almost any resource imaginable.
    Although they are very powerful tools, CRDs and controllers can sometimes lead
    to an explosion of objects in the Kubernetes data model. Along with all those
    new objects may come many active actors writing and reading to and from that model.
    Left unchecked, these controllers and new objects can have a massive impact on
    scale and performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'With so many moving parts contributing to the scale limitations of your kube
    cluster, the one thing you can count on is this: in production, you will need
    to shard. In Chapters [5](ch05.html#continuous_delivery_across_clusters), [6](ch06.html#multicluster_fleets_provision_and_upgrad),
    and [8](ch08.html#working_example_of_multicluster_applicat), we go into more detail
    on how to manage multiple clusters and distribute workload across them. We provide
    a formula for sharding that allows for further expansion of our scaling capabilities.
    For the purposes of this discussion, we simply need to establish what the scalability
    limits are of a single Kubernetes cluster running *your* unique workload. There
    is simply no substitute for automated scale testing that you can repeat regularly.
    One of your teams wants to introduce a new set of CRDs and a controller for them?
    Time to run scale testing! Before they can ship that new code to production, you
    need to understand the impacts of those changes. It may be that you find that
    it is a significant enough impact that you can break the scale capabilities of
    your clusters in production. In such cases, you may need to roll out a new set
    of clusters for these services. What if this service is already in production?
    You may need to migrate this service off of an existing set of multitenant clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Admission Controller Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Admission controllers are powerful tools available to the administrator of Kubernetes
    clusters to enforce additional constraints and behaviors on objects in the cluster.
    Most commonly, these controllers are used to enforce security, performance, and
    other best practices. These add-ons to the cluster can either modify or constrain
    objects that are being created, updated, or deleted in the kube-apiserver.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s important to note that as with many features of Kubernetes, enabling admission
    controllers can have an impact on the performance and scale of your cluster. This
    is especially true when using webhook-based admission controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Admission Controllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Upstream Kubernetes includes a number of admission controllers as part of all
    control planes. As of Kubernetes 1.18, [the set of admission controllers enabled
    by default](https://oreil.ly/DzrON) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamespaceLifecycle`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LimitRanger`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ServiceAccount`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TaintNodesByCondition`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Priority`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DefaultTolerationSeconds`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DefaultStorageClass`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StorageObjectInUseProtection`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PersistentVolumeClaimResize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RuntimeClass`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CertificateApproval`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CertificateSigning`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CertificateSubjectRestriction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DefaultIngressClass`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MutatingAdmissionWebhook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ValidatingAdmissionWebhook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResourceQuota`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Not all of these are particularly interesting, and the [official documentation](https://oreil.ly/4EHzv)
    has plenty of information about what they all do. There are two on this list that
    we’ll spend more time on: `MutatingAdmissionWebhook` and `ValidatingAdmissionWebhook`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to those dynamic admission controllers, we’ll cover a few
    of the standard controllers in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LimitRanger`'
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we talked about the importance of resource limits to
    ensure that pod resource consumption is kept in check. `LimitRanger` ensures that
    the `LimitRange` settings configured on a namespace are adhered to. This provides
    one more tool for cluster administrators to keep workloads in check.^([5](ch03.html#ch01fn25))
  prefs: []
  type: TYPE_NORMAL
- en: '`Priority`'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed earlier, `PriorityClasses` play a key role in the scheduling
    and post-scheduling behavior of pods. The `Priority` admission controller helps
    to map useful names to an integer that represents the relative value of one class
    versus another.
  prefs: []
  type: TYPE_NORMAL
- en: '`ResourceQuota`'
  prefs: []
  type: TYPE_NORMAL
- en: Critical to the function of a production-ready cluster, this controller is responsible
    for enforcing the quotas we discussed in [“Limit ranges and quotas”](#limit_ranges_and_quotas).
  prefs: []
  type: TYPE_NORMAL
- en: '`PodSecurityPolicy`'
  prefs: []
  type: TYPE_NORMAL
- en: This controller is not enabled by default but should be in any production-ready
    Kubernetes or OpenShift cluster. [Pod security policies](https://oreil.ly/fLhIC)
    are essential for enforcing security and compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Collectively, these additional admission controllers provide a great deal of
    the core behaviors we have come to take as standard for Kubernetes. These admission
    controllers help to build the rules that govern how administrators and users deploy
    their workloads within set boundaries of performance, scale, and security.
  prefs: []
  type: TYPE_NORMAL
- en: Admission Webhooks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s helpful to talk a bit about what admission webhooks are and how they work.
    The [upstream documentation](https://oreil.ly/pSA4X) also refers to them as *dynamic
    admission controllers* and provides excellent information about these tools. An
    *admission* *webhook* is a Kubernetes configuration that references a web service
    that can run either on the cluster or external to the cluster and provides modification
    (mutating) or validation of objects as they are modified in the kube-apiserver.
    A simple example of a webhook configuration is provided for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notable features of this config are the rules that determine which objects the
    kube-apiserver should call the webhook for and the `clientConfig`, which determines
    the target endpoint and authentication mechanism to be used for the calls.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In our example, we have a `clientConfig` that references an in-cluster service
    to contact for webhook calls. Alternatively, the `service:` stanza can be replaced
    with a `url: “https://myexamplewebhook:9443/path”` to refer via a specific address
    rather than a service.'
  prefs: []
  type: TYPE_NORMAL
- en: ValidatingAdmissionWebhook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Validating webhooks are powerful tools that can be used to enforce security
    constraints, resource constraints, or best practices for a cluster. A great example
    is [Portieris](https://oreil.ly/xpu1F), which is used to validate that only images
    from approved registries are used for containers. The kube-apiserver calls the
    validating webhook and expects a Boolean response to determine if the request
    should be admitted.
  prefs: []
  type: TYPE_NORMAL
- en: An important operational note is that a dependency chain could inadvertently
    be built that prevents the validating application from starting in the cluster.
    As noted, it is not uncommon for admission controllers to be run in the cluster;
    if we were to run Portieris in our cluster in the `portieris` namespace and then
    created a webhook rule that required validation before starting any pods in the
    `portieris` namespace, we would have to remove the webhook configuration before
    we could even restart the pods that were serving the webhook. Be sure to avoid
    such circular dependencies by excluding namespaces from the webhook configuration
    where appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: MutatingAdmissionWebhook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mutating webhooks are commonly used to inject sidecars into pods, modify resource
    constraints, apply default settings, and other useful enforcement. There may often
    be a validating and a mutating option for similar purposes. There may be a validating
    webhook that ensures that pods are not started without having a specific service
    account in use, and a mutating alternative might modify the service account of
    the pod dynamically. Both have similar results: one requires the user to make
    the modifications required on their own before running their applications, while
    the other modifies on the fly. There are pros and cons to both approaches. Mutating
    is typically best reserved for adding function to a pod, such as a sidecar. Validating
    is better suited for enforcing best practices and forcing users to make the modifications
    on their own, thus raising awareness of those practices for the team.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the tools that govern how user applications consume
    resources in the cluster and how administrators can put rules in place with admission
    controllers to guide their users to success. We also covered how the Kubernetes
    scheduler uses these user-provided details to efficiently schedule workload in
    the cluster. Finally, we covered some of the tools of the trade that can be used
    to help ensure strong performance of our applications with monitoring and autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: When combined, these resource definitions, autoscaling techniques, and admission
    controls provide the groundwork for more advanced management of resources in any
    Kubernetes or OpenShift cluster. Proper application of all of these tools can
    lead to a higher level of application performance, more stability for the system,
    and lower costs through efficient resource utilization. Now that we have a foundation
    for understanding how resources are managed in Kubernetes and OpenShift, we will
    move on to how to use these tools to increase the availability of applications
    and services.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#ch01fn21-marker)) The Out of Memory Killer is a Linux process
    with the job of weighing all running processes on the system and selecting one
    or more for termination when the system is critically low on available memory.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#ch01fn22-marker)) See the Kubernetes documentation, [“Reserve
    Compute Resources for System Daemons”](https://oreil.ly/YQjrt).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#ch01fn23-marker)) See the Kubernetes documentation on [Taints
    and Tolerations Example Use Cases](https://oreil.ly/gSnJ9).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#ch01fn24-marker)) See the Kubernetes documentation, [“Considerations
    for Large Clusters”](https://oreil.ly/qaWKY).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.html#ch01fn25-marker)) See the Kubernetes documentation on [Limit
    Ranges](https://oreil.ly/gPLXG).
  prefs: []
  type: TYPE_NORMAL
