<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 8. HTTP Load Balancing with Ingress" data-type="chapter" epub:type="chapter"><div class="chapter" id="ingress">
<h1><span class="label">Chapter 8. </span>HTTP Load Balancing with Ingress</h1>
<p>A critical part of any application is getting network traffic to and from that application.<a data-primary="load balancing" data-secondary="HTTP load balancing with Ingress" data-type="indexterm" id="ix_ldbalIng"/> As described in <a data-type="xref" href="ch07.xhtml#service_discovery">Chapter 7</a>, Kubernetes has a set of capabilities to enable services to be exposed outside of the cluster.<a data-primary="Ingress" data-type="indexterm" id="ix_Ingr"/> For many users and simple use cases, these capabilities are sufficient.<a data-primary="HTTP load balancing" data-see="load balancing" data-type="indexterm" id="idm45664078057472"/></p>
<p>But the Service object operates at Layer 4 (according to the OSI model).<sup><a data-type="noteref" href="ch08.xhtml#idm45664078055792" id="idm45664078055792-marker">1</a></sup> This means that it only forwards TCP and UDP connections and doesn’t look inside of those connections. Because of this, hosting many applications on a cluster uses many different exposed services. <a data-primary="services" data-secondary="of type NodePort" data-secondary-sortas="type" data-seealso="NodePorts" data-type="indexterm" id="idm45664078052256"/>In the case where these services are <code>type: NodePort</code>, you’ll have to have clients connect to a unique port per service.<a data-primary="NodePorts" data-secondary="services of type NodePort" data-type="indexterm" id="idm45664078050208"/> In the case where these services are <code>type: LoadBalancer</code>, you’ll be allocating (often expensive or scarce) cloud resources for each service. But for HTTP (Layer 7)-based services, we can do better.<a data-primary="services" data-secondary="of type LoadBalancer" data-secondary-sortas="type" data-seealso="Ingress; LoadBalancer type" data-type="indexterm" id="idm45664078048416"/><a data-primary="LoadBalancer type" data-type="indexterm" id="idm45664078046832"/></p>
<p>When solving a similar problem in non-Kubernetes situations, users often turn to the idea of “virtual hosting.” This is a mechanism to host many HTTP sites on a single IP address.<a data-primary="virtual hosting" data-type="indexterm" id="idm45664078045712"/> Typically, the user uses a load balancer or reverse proxy to accept incoming connections on HTTP (80) and HTTPS (443) ports. That program then parses the HTTP connection and, based on the <code>Host</code> header and the URL path that is requested, proxies the HTTP call to some other program. In this way, that load balancer or reverse proxy directs traffic for decoding and directing incoming connections to the right “upstream” server.</p>
<p>Kubernetes calls its HTTP-based load-balancing system <em>Ingress</em>. Ingress is a Kubernetes-native way to implement the “virtual hosting” pattern we just discussed. One of the more complex aspects of the pattern is that the user has to manage the load balancer configuration file. In a dynamic environment and as the set of virtual hosts expands, this can be very complex. The Kubernetes Ingress system works to simplify this by (a) standardizing that configuration, (b) moving it to a standard Kubernetes object, and (c) merging multiple Ingress objects into a single config for the load balancer.<a data-primary="Ingress" data-secondary="typical software Ingress controller configuration" data-type="indexterm" id="idm45664078043312"/></p>
<p>The typical software base implementation looks something like what is depicted in <a data-type="xref" href="#figingressflow">Figure 8-1</a>. The Ingress controller is a software system made up of two parts. The first is the Ingress proxy, which is exposed outside the cluster using a service of <code>type: LoadBalancer</code>. This proxy sends requests to “upstream” servers. The other component is the Ingress reconciler, or operator. The Ingress operator is responsible for reading and monitoring Ingress objects in the Kubernetes API and reconfiguring the Ingress proxy to route traffic as specified in the Ingress resource. There are many different Ingress implementations. In some, these two components are combined in a single container; in others, they are distinct components that are deployed separately in the Kubernetes cluster. In <a data-type="xref" href="#figingressflow">Figure 8-1</a>, we introduce one example of an Ingress controller.</p>
<figure><div class="figure" id="figingressflow">
<img alt="" height="470" src="assets/kur3_0801.png" width="1061"/>
<h6><span class="label">Figure 8-1. </span>The typical software Ingress controller configuration</h6>
</div></figure>
<section data-pdf-bookmark="Ingress Spec Versus Ingress Controllers" data-type="sect1"><div class="sect1" id="idm45664078037584">
<h1>Ingress Spec Versus Ingress Controllers</h1>
<p>While conceptually simple, at an implementation level, Ingress is very different from pretty much every other regular resource object in Kubernetes.<a data-primary="Ingress" data-secondary="resource specification versus controller implementation" data-type="indexterm" id="idm45664078036128"/> Specifically, it is split into a common resource specification and a controller implementation. There is no “standard” Ingress controller that is built into Kubernetes, so the user must install one of many optional implementations.</p>
<p>Users can create and modify Ingress objects just like every other object. But, by default, there is no code running to actually act on those objects. It is up to the users (or the distribution they are using) to install and manage an outside controller. In this way, the controller is pluggable.</p>
<p>There are a couple of reasons that Ingress ended up like this. First of all, there is no one single HTTP load balancer that can be used universally.<a data-primary="load balancers" data-secondary="no single HTTP load balancer for universal use" data-type="indexterm" id="idm45664078033712"/> In addition to many software load balancers (both open source and proprietary), there are also load-balancing capabilities provided by cloud providers (e.g., ELB on AWS), and hardware-based load balancers. The second reason is that the Ingress object was added to Kubernetes before any of the common extensibility capabilities were added (see <a data-type="xref" href="ch17.xhtml#extending_kubernetes">Chapter 17</a>). As Ingress progresses, it is likely that it will evolve to use these mechanisms.</p>
</div></section>
<section data-pdf-bookmark="Installing Contour" data-type="sect1"><div class="sect1" id="idm45664078031152">
<h1>Installing Contour</h1>
<p>While there are many available Ingress controllers, for the examples here, we use an Ingress controller called Contour.<a data-primary="Contour Ingress controller, installing" data-type="indexterm" id="ix_Cntr"/><a data-primary="Ingress" data-secondary="installing Contour controller" data-type="indexterm" id="ix_IngrinstCon"/> This is a controller built to configure the open source (and CNCF project) load balancer called Envoy. Envoy is built to be dynamically configured via an API. The Contour Ingress controller takes care of translating the Ingress objects into something that Envoy can understand.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <a href="https://oreil.ly/5IHmq">Contour project</a> was created by Heptio in collaboration with real-world customers and is used in production settings but is now an independent open source project.</p>
</div>
<p>You can install Contour with<a data-primary="kubectl tool" data-secondary="commands" data-tertiary="apply -f" data-type="indexterm" id="idm45664078024160"/> a simple one-line invocation:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f https://projectcontour.io/quickstart/contour.yaml</strong></pre>
<p>Note that this requires execution by a user who has <code>cluster-admin</code>
permissions.<a data-primary="cluster-admin permissions" data-type="indexterm" id="idm45664078020880"/></p>
<p>This one line works for most configurations. It creates a namespace called <code>projectcontour</code>. <a data-primary="projectcontour namespace" data-type="indexterm" id="idm45664078018928"/><a data-primary="namespaces" data-secondary="projectcontour" data-type="indexterm" id="idm45664078018176"/>Inside of that namespace it creates a deployment (with two replicas) and an external-facing service of <code>type: LoadBalancer</code>. In addition, it sets up the correct permissions via a service account and installs a CustomResourceDefinition (see <a data-type="xref" href="ch17.xhtml#extending_kubernetes">Chapter 17</a>) for some extended capabilities discussed in <a data-type="xref" href="#future_of_ingress">“The Future of Ingress”</a>.</p>
<p>Because it is a global install, you need to ensure that you have wide admin permissions on the cluster you are installing into. After you install it, you can fetch the external address of Contour via:</p>
<pre data-type="programlisting">$ <strong> kubectl get -n projectcontour service envoy -o wide</strong>
NAME      CLUSTER-IP     EXTERNAL-IP          PORT(S)      ...
contour   10.106.53.14   a477...amazonaws.com 80:30274/TCP ...</pre>
<p>Look at the <code>EXTERNAL-IP</code> column. This can be either an IP address (for GCP and Azure) or a hostname (for AWS). Other clouds and environments may differ. If your Kubernetes cluster doesn’t support services of <code>type: LoadBalancer</code>, you’ll have to change the YAML for installing Contour to use <code>type: NodePort</code> and route traffic to machines on the cluster via a mechanism that works in your configuration.<a data-primary="NodePorts" data-secondary="installing Contour to use services of type NodePort" data-type="indexterm" id="idm45664078011072"/><a data-primary="minikube" data-secondary="minikube tunnel command" data-type="indexterm" id="idm45664078010000"/></p>
<p>If you are using <code>minikube</code>, you probably won’t have anything listed for <code>EXTERNAL-IP</code>. To fix this, you need to open a separate terminal window and run <code>minikube tunnel</code>. This configures networking routes such that you have unique IP addresses assigned to every service of <code>type: LoadBalancer</code>.</p>
<section data-pdf-bookmark="Configuring DNS" data-type="sect2"><div class="sect2" id="idm45664078006640">
<h2>Configuring DNS</h2>
<p>To make Ingress work well, you need to configure DNS entries to the external address for your load balancer.<a data-primary="DNS" data-secondary="configuring entries to external address for load balancer" data-type="indexterm" id="idm45664078005152"/><a data-primary="Contour Ingress controller, installing" data-secondary="configuring DNS" data-type="indexterm" id="idm45664078004128"/> You can map multiple hostnames to a single external endpoint and the Ingress controller will direct incoming requests to the appropriate upstream service based on that hostname.</p>
<p>For this chapter, we assume that you have a domain called <code>example.com</code>. You need to configure two DNS entries: <code>alpaca.example.com</code> and <code>bandicoot.example.com</code>. If you have an IP address for your external load balancer, you’ll want to create A records. If you have a hostname, you’ll want to configure CNAME records.</p>
<p>The <a href="https://oreil.ly/ILdEj">ExternalDNS project</a> is
a cluster add-on that you can use to manage DNS records for you. <a data-primary="ExternalDNS" data-type="indexterm" id="idm45664077999856"/>ExternalDNS monitors your Kubernetes cluster and synchronizes IP addresses for Kubernetes
Service resources with an external DNS provider. ExternalDNS supports a wide
variety of DNS providers including traditional domain registrars as well
as public cloud providers.</p>
</div></section>
<section data-pdf-bookmark="Configuring a Local hosts File" data-type="sect2"><div class="sect2" id="idm45664077998400">
<h2>Configuring a Local hosts File</h2>
<p>If you don’t have a domain or if you are using a local solution such as <code>minikube</code>, you can set up a local configuration by editing your <em>/etc/hosts</em> file to add an IP address. <a data-primary="hosts file (local), configuring for Contour" data-type="indexterm" id="idm45664077995664"/><a data-primary="Contour Ingress controller, installing" data-secondary="configuring local hosts file" data-type="indexterm" id="idm45664077994864"/>You need admin/root privileges on your workstation. The location of the file may differ on your platform, and making it take effect may require extra steps. For example, on Windows the file is usually at <em>C:\Windows\System32\drivers\etc\hosts</em>, and for recent versions of macOS, you need to run <code>sudo killall -HUP mDNSResponder</code> after changing the file.</p>
<p>Edit the file to add a line like the following:</p>
<pre>&lt;<em>ip-address</em>&gt; alpaca.example.com bandicoot.example.com</pre>
<p>For <code>&lt;<em>ip-address</em>&gt;</code>, fill in the external IP address for Contour. If all you have is a hostname (like from AWS), you can get an IP address (that may change in the future) by executing <code>host -t a <em>&lt;address&gt;</em></code>.</p>
<p>Don’t forget to undo these changes when you are done!<a data-primary="Contour Ingress controller, installing" data-startref="ix_Cntr" data-type="indexterm" id="idm45664077988752"/><a data-primary="Ingress" data-secondary="installing Contour controller" data-startref="ix_IngrinstCon" data-type="indexterm" id="idm45664077987760"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Using Ingress" data-type="sect1"><div class="sect1" id="idm45664078030560">
<h1>Using Ingress</h1>
<p>Now that we have an Ingress controller configured, let’s put it through its paces. <a data-primary="Ingress" data-secondary="using" data-type="indexterm" id="ix_Ingruse"/><a data-primary="services" data-secondary="backend, creating" data-type="indexterm" id="idm45664077983744"/>First, we’ll create a few upstream (also sometimes referred to as “backend”) services to play with by executing the following commands:</p>
<pre data-type="programlisting">$ <strong>kubectl create deployment be-default \
  --image=gcr.io/kuar-demo/kuard-amd64:blue \
  --replicas=3 \
  --port=8080</strong>
$ <strong>kubectl expose deployment be-default</strong>
$ <strong>kubectl create deployment alpaca \
  --image=gcr.io/kuar-demo/kuard-amd64:green \
  --replicas=3 \
  --port=8080</strong>
$ <strong>kubectl expose deployment alpaca</strong>
$ <strong>kubectl create deployment bandicoot \
  --image=gcr.io/kuar-demo/kuard-amd64:purple \
  --replicas=3 \
  --port=8080</strong>
$ <strong>kubectl expose deployment bandicoot</strong>
$ <strong>kubectl get services -o wide</strong>

NAME             CLUSTER-IP    ... PORT(S)  ... SELECTOR
alpaca           10.115.245.13 ... 8080/TCP ... run=alpaca
bandicoot        10.115.242.3  ... 8080/TCP ... run=bandicoot
be-default       10.115.246.6  ... 8080/TCP ... run=be-default
kubernetes       10.115.240.1  ... 443/TCP  ... &lt;none&gt;</pre>
<section data-pdf-bookmark="Simplest Usage" data-type="sect2"><div class="sect2" id="idm45664077978224">
<h2>Simplest Usage</h2>
<p>The simplest way to use Ingress is to have it just blindly pass everything that it sees through to an upstream service.<a data-primary="Ingress" data-secondary="using" data-tertiary="simplest usage" data-type="indexterm" id="idm45664077976656"/> There is limited support for imperative commands to work with Ingress in <code>kubectl</code>, so we’ll start with a YAML file (see <a data-type="xref" href="#simple-ingress-ex">Example 8-1</a>).</p>
<div data-type="example" id="simple-ingress-ex">
<h5><span class="label">Example 8-1. </span>simple-ingress.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">simple-ingress</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">defaultBackend</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">service</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">alpaca</code><code class="w"/>
<code class="w">      </code><code class="nt">port</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="nt">number</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w"/></pre></div>
<p class="pagebreak-before less_space">Create this <a data-primary="kubectl tool" data-secondary="commands" data-tertiary="apply -f" data-type="indexterm" id="idm45664077957744"/>Ingress with <code>kubectl apply</code>:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f simple-ingress.yaml</strong>
ingress.extensions/simple-ingress created</pre>
<p>You can verify that it <a data-primary="kubectl tool" data-secondary="commands" data-tertiary="describe" data-type="indexterm" id="idm45664077949456"/><a data-primary="kubectl tool" data-secondary="commands" data-tertiary="get ingress" data-type="indexterm" id="idm45664077948176"/>was set up correctly using <code>kubectl get</code> and <code>kubectl describe</code>:</p>
<pre data-type="programlisting">$ <strong>kubectl get ingress</strong>
NAME             HOSTS   ADDRESS   PORTS   AGE
simple-ingress   *                 80      13m

$ <strong>kubectl describe ingress simple-ingress</strong>
Name:             simple-ingress
Namespace:        default
Address:
Default backend:  alpaca:8080
(172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080)
Rules:
  Host  Path  Backends
  ----  ----  --------
  *     *     alpaca:8080 (172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080)
Annotations:
  ...

Events:  &lt;none&gt;</pre>
<p>This sets things up so that <em>any</em> HTTP request that hits the Ingress controller is forwarded on to the <code>alpaca</code> service. You can now access the <code>alpaca</code> instance of <code>kuard</code> on any of the raw IPs/CNAMEs of the service; in this case, either <code>alpaca.example.com</code> or <code>bandicoot.example.com</code>. This doesn’t, at this point, add much value over a simple service of <code>type: LoadBalancer</code>. The following sections experiment with more complex configurations.</p>
</div></section>
<section data-pdf-bookmark="Using Hostnames" data-type="sect2"><div class="sect2" id="idm45664077940720">
<h2>Using Hostnames</h2>
<p>Things start to get interesting when we direct traffic based on properties of the request. <a data-primary="Ingress" data-secondary="using" data-tertiary="hostnames, routing traffic via" data-type="indexterm" id="idm45664077882032"/><a data-primary="hostnames" data-secondary="using with Ingress to direct traffic" data-type="indexterm" id="idm45664077880944"/>The most common example of this is to have the Ingress system look at the HTTP host header (which is set to the DNS domain in the original URL) and direct traffic based on that header. Let’s add <em>another</em> Ingress object for directing traffic to the <code>alpaca</code> service for any traffic directed to <code>alpaca.example.com</code> (see <a data-type="xref" href="#host-ingress">Example 8-2</a>).</p>
<div data-type="example" id="host-ingress">
<h5><span class="label">Example 8-2. </span>host-ingress.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">host-ingress</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">defaultBackend</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="nt">service</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">be-default</code><code class="w"/>
<code class="w">      </code><code class="nt">port</code><code class="p">:</code><code class="w"/>
<code class="w">        </code><code class="nt">number</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w"/>
<code class="w">  </code><code class="nt">rules</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">host</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">alpaca.example.com</code><code class="w"/>
<code class="w">    </code><code class="nt">http</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">paths</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">pathType</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Prefix</code><code class="w"/>
<code class="w">        </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/</code><code class="w"/>
<code class="w">        </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>
<code class="w">          </code><code class="nt">service</code><code class="p">:</code><code class="w"/>
<code class="w">            </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">alpaca</code><code class="w"/>
<code class="w">            </code><code class="nt">port</code><code class="p">:</code><code class="w"/>
<code class="w">              </code><code class="nt">number</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w"/></pre></div>
<p>Create this Ingress with <code>kubectl apply</code>:</p>
<pre data-type="programlisting">$ <strong>kubectl apply -f host-ingress.yaml</strong>
ingress.extensions/host-ingress created</pre>
<p>We can verify that things are set up correctly as follows:</p>
<pre data-type="programlisting">$ <strong>kubectl get ingress</strong>
NAME             HOSTS               ADDRESS   PORTS   AGE
host-ingress     alpaca.example.com            80      54s
simple-ingress   *                             80      13m

$ <strong>kubectl describe ingress host-ingress</strong>
Name:             host-ingress
Namespace:        default
Address:
Default backend:  be-default:8080 (&lt;none&gt;)
Rules:
  Host                Path  Backends
  ----                ----  --------
  alpaca.example.com
                      /   alpaca:8080 (&lt;none&gt;)
Annotations:
  ...

Events:  &lt;none&gt;</pre>
<p>There are a couple of confusing things here. First, there is a reference to the <code>default-http-backend</code>. This is a convention that only some Ingress controllers use to handle requests that aren’t handled in any other way. These controllers send those requests to a service called <code>default-http-backend</code> in the <code>kube-system</code> namespace. This convention is surfaced client-side in <code>kubectl</code>. Next, there are no endpoints listed for the <code>alpaca</code> backend service. This is a bug in <code>kubectl</code> that is fixed in Kubernetes v1.14.</p>
<p>Regardless, you should now be able to address the <code>alpaca</code> service via <span><em>http://alpaca.example.com</em></span>. If instead you reach the service endpoint via other methods, you should get the default service.</p>
</div></section>
<section data-pdf-bookmark="Using Paths" data-type="sect2"><div class="sect2" id="idm45664077820416">
<h2>Using Paths</h2>
<p>The next interesting scenario is to direct traffic based on not just the hostname, but also the path in the HTTP request.<a data-primary="Ingress" data-secondary="using" data-tertiary="paths of HTTP requests" data-type="indexterm" id="idm45664077818688"/><a data-primary="paths" data-secondary="of HTTP requests, using with Ingress" data-secondary-sortas="HTTP" data-type="indexterm" id="idm45664077817440"/> We can do this easily by specifying a path in the <code>paths</code> entry (see <a data-type="xref" href="#path-ingress">Example 8-3</a>). In this example, we direct everything coming into <span><em>http://bandicoot.example.com</em></span> to the <code>bandicoot</code> service, but we also send <span><em>http://bandicoot.example.com/a</em></span> to the <code>alpaca</code> service. This type of scenario can be used to host multiple services on different paths of a single domain.</p>
<div data-type="example" id="path-ingress">
<h5><span class="label">Example 8-3. </span>path-ingress.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">path-ingress</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">rules</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">host</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bandicoot.example.com</code><code class="w"/>
<code class="w">    </code><code class="nt">http</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">paths</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">pathType</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Prefix</code><code class="w"/>
<code class="w">        </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="s">"/"</code><code class="w"/>
<code class="w">        </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>
<code class="w">          </code><code class="nt">service</code><code class="p">:</code><code class="w"/>
<code class="w">            </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">bandicoot</code><code class="w"/>
<code class="w">            </code><code class="nt">port</code><code class="p">:</code><code class="w"/>
<code class="w">              </code><code class="nt">number</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w"/>
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">pathType</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Prefix</code><code class="w"/>
<code class="w">        </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="s">"/a/"</code><code class="w"/>
<code class="w">        </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>
<code class="w">          </code><code class="nt">service</code><code class="p">:</code><code class="w"/>
<code class="w">            </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">alpaca</code><code class="w"/>
<code class="w">            </code><code class="nt">port</code><code class="p">:</code><code class="w"/>
<code class="w">              </code><code class="nt">number</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w"/></pre></div>
<p>When there are multiple paths on the same host listed in the Ingress system, the longest prefix matches. So, in this example, traffic starting with <code>/a/</code> is forwarded to the <code>alpaca</code> service, while all other traffic (starting with <code>/</code>) is directed to the <code>bandicoot</code> service.</p>
<p>As requests get proxied to the upstream service, the path remains unmodified. That means a request to <code>bandicoot.example.com/a/</code> shows up to the upstream server that is configured for that request hostname and path. The upstream service needs to be ready to serve traffic on that subpath. In this case, <code>kuard</code> has special code for testing, where it responds on the root path (<code>/</code>) along with a predefined set of subpaths (<code>/a/</code>, <code>/b/</code>, and <code>/c/</code>).</p>
</div></section>
<section data-pdf-bookmark="Cleanup" data-type="sect2"><div class="sect2" id="idm45664077575232">
<h2>Cleanup</h2>
<p>To clean up, execute <a data-primary="kubectl tool" data-secondary="commands" data-tertiary="delete" data-type="indexterm" id="idm45664077573872"/><a data-primary="Ingress" data-secondary="using" data-tertiary="cleaning up" data-type="indexterm" id="idm45664077572592"/>the following:</p>
<pre data-type="programlisting">$ <strong>kubectl delete ingress host-ingress path-ingress simple-ingress</strong>
$ <strong>kubectl delete service alpaca bandicoot be-default</strong>
$ <strong>kubectl delete deployment alpaca bandicoot be-default</strong></pre>
</div></section>
</div></section>
<section data-pdf-bookmark="Advanced Ingress Topics and Gotchas" data-type="sect1"><div class="sect1" id="idm45664077569152">
<h1>Advanced Ingress Topics and Gotchas</h1>
<p>Ingress supports some other fancy features.<a data-primary="Ingress" data-secondary="using" data-startref="ix_Ingruse" data-type="indexterm" id="idm45664077567648"/> The level of support for these features differs based on the Ingress controller implementation, and two controllers may implement a feature in slightly different ways.<a data-primary="Ingress" data-secondary="advanced topics and gotchas" data-type="indexterm" id="ix_Ingradv"/></p>
<p>Many of the extended features are exposed via annotations on the Ingress object. <a data-primary="annotations" data-secondary="on Ingress object" data-secondary-sortas="Ingress" data-type="indexterm" id="idm45664077541264"/>Be careful; these annotations can be hard to validate and are easy to get wrong. Many of these annotations apply to the entire Ingress object and so can be more general than you might like. To scope the annotations down, you can always split a single Ingress object into multiple Ingress objects. The Ingress controller should read them and merge them together.</p>
<section data-pdf-bookmark="Running Multiple Ingress Controllers" data-type="sect2"><div class="sect2" id="idm45664077539920">
<h2>Running Multiple Ingress Controllers</h2>
<p>There are multiple Ingress controller implementations, and you may want to run multiple Ingress controllers on a single cluster.<a data-primary="controllers" data-secondary="Ingress, running multiple" data-type="indexterm" id="idm45664077538784"/><a data-primary="Ingress" data-secondary="advanced topics and gotchas" data-tertiary="running multiple Ingress controllers" data-type="indexterm" id="idm45664077537872"/> To solve this
case, the IngressClass resource exists so that an Ingress resource can
request a particular implementation. <a data-primary="resources" data-secondary="Ingress, specifying" data-type="indexterm" id="idm45664077536592"/><a data-primary="IngressClass" data-type="indexterm" id="idm45664077535648"/>When you create an Ingress resource,
you use the <code>spec.ingressClassName</code> field to specify the specific Ingress
resource.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In Kubernetes prior to version 1.18, the <code>IngressClassName</code> field did not exist and the <code>kubernetes.io/ingress.class</code> annotation was used instead. While this is still supported by many controllers, it is
recommended that people move away from the annotation as it will likely be
deprecated by controllers in the future.</p>
</div>
<p>If the <code>spec.ingressClassName</code> annotation is missing, a default Ingress controller is used. It is specified by adding the <code>ingressclass.kubernetes.io/is-default-class</code> annotation to the correct IngressClass resource.</p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Multiple Ingress Objects" data-type="sect2"><div class="sect2" id="idm45664077530336">
<h2>Multiple Ingress Objects</h2>
<p>If you specify multiple Ingress objects, the Ingress controllers should read them all and try to merge them into a coherent configuration.<a data-primary="Ingress" data-secondary="advanced topics and gotchas" data-tertiary="multiple Ingress objects" data-type="indexterm" id="idm45664077528576"/> However, if you specify duplicate and conflicting configurations, the behavior is undefined. It is likely that different Ingress controllers will behave differently. Even a single implementation may do different things depending on nonobvious factors.</p>
</div></section>
<section data-pdf-bookmark="Ingress and Namespaces" data-type="sect2"><div class="sect2" id="idm45664077526592">
<h2>Ingress and Namespaces</h2>
<p>Ingress interacts with namespaces in some nonobvious ways.<a data-primary="namespaces" data-secondary="Ingress interactions with" data-type="indexterm" id="idm45664077525120"/><a data-primary="Ingress" data-secondary="advanced topics and gotchas" data-tertiary="interactions with namespaces" data-type="indexterm" id="idm45664077524128"/> First, due to an abundance of security caution, an Ingress object can refer to only an upstream service in the same namespace. This means that you can’t use an Ingress object to point a subpath to a service in another namespace.</p>
<p>However, multiple Ingress objects in different namespaces can specify subpaths for the same host. These Ingress objects are then merged to come up with the final config for the Ingress controller.</p>
<p>This cross-namespace behavior means that coordinating Ingress globally across the cluster is necessary. If not coordinated carefully, an Ingress object in one namespace could cause problems (and undefined behavior) in other namespaces.</p>
<p>Typically there are no restrictions built into the Ingress controller around which namespaces are allowed to specify which hostnames and paths. Advanced users may try to enforce a policy for this using a custom admission controller. There are also <span class="keep-together">evolutions</span> of Ingress described in <a data-type="xref" href="#future_of_ingress">“The Future of Ingress”</a> that address this problem.</p>
</div></section>
<section data-pdf-bookmark="Path Rewriting" data-type="sect2"><div class="sect2" id="idm45664077519072">
<h2>Path Rewriting</h2>
<p>Some Ingress controller implementations support, optionally, doing path rewriting. <a data-primary="paths" data-secondary="path rewriting by Ingress controllers" data-type="indexterm" id="idm45664077517808"/><a data-primary="Ingress" data-secondary="advanced topics and gotchas" data-tertiary="path rewriting" data-type="indexterm" id="idm45664077516768"/>This can be used to modify the path in the HTTP request as it gets proxied. This is usually specified by an annotation on the Ingress object and applies to all requests that are specified by that object. For example, if we were using the NGINX Ingress controller, we could specify an annotation of <code>nginx.ingress​.kuber⁠netes.io/rewrite-target: /</code>. This can sometimes make upstream services work on a subpath even if they weren’t built to do so.</p>
<p>There are multiple implementations that not only implement path rewriting, but also support regular expressions when specifying the path.<a data-primary="NGINX" data-secondary="Ingress controller" data-type="indexterm" id="idm45664077514304"/> For example, the NGINX controller allows regular expressions to capture parts of the path and then use that captured content when doing rewriting. How this is done (and what variant of regular expressions is used) is implementation-specific.</p>
<p>Path rewriting isn’t a silver bullet, though, and can often lead to bugs. Many web applications assume that they can link within themselves using absolute paths. In that case, the app in question may be hosted on <code>/subpath</code> but have requests show up to it on <code>/</code>. It may then send a user to <code>/app-path</code>. There is then the question of whether that is an “internal” link for the app (in which case it should instead be <code>/subpath/app-path</code>) or a link to some other app. For this reason, it is probably best to avoid subpaths for any complicated applications if you can help it.</p>
</div></section>
<section data-pdf-bookmark="Serving TLS" data-type="sect2"><div class="sect2" id="idm45664077510080">
<h2>Serving TLS</h2>
<p>When serving websites, it is becoming increasingly necessary to do so securely using TLS and HTTPS.<a data-primary="Ingress" data-secondary="advanced topics and gotchas" data-tertiary="serving TLS" data-type="indexterm" id="idm45664077508464"/><a data-primary="TLS (Transport Layer Security)" data-secondary="serving in Ingress system" data-type="indexterm" id="idm45664077507152"/> Ingress supports this (as do most Ingress controllers).<a data-primary="secrets" data-secondary="specifying with TLS certificate and keys" data-type="indexterm" id="idm45664077506048"/><a data-primary="certificates" data-secondary="TLS" data-tertiary="creating secret to store" data-type="indexterm" id="idm45664077505008"/></p>
<p>First, users need to specify a Secret with their TLS certificate and keys⁠—something like what is outlined in <a data-type="xref" href="#tls-secret">Example 8-4</a>. You can also create a Secret imperatively with <code>kubectl create secret tls &lt;<em>secret-name</em>&gt; --cert &lt;<em>certificate-pem-file</em>&gt; --key &lt;<em>private-key-pem-file</em>&gt;</code>.</p>
<div data-type="example" id="tls-secret">
<h5><span class="label">Example 8-4. </span>tls-secret.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Secret</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">creationTimestamp</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">null</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">tls-secret-name</code><code class="w"/>
<code class="nt">type</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">kubernetes.io/tls</code><code class="w"/>
<code class="nt">data</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">tls.crt</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">&lt;base64 encoded certificate&gt;</code><code class="w"/>
<code class="w">  </code><code class="nt">tls.key</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">&lt;base64 encoded private key&gt;</code><code class="w"/></pre></div>
<p>Once you have the certificate uploaded, you can reference it in an Ingress object. This specifies a list of certificates along with the hostnames that those certificates should be used for (see <a data-type="xref" href="#tls-ingress">Example 8-5</a>). Again, if multiple Ingress objects specify certificates for the same hostname, the behavior is undefined.</p>
<div data-type="example" id="tls-ingress">
<h5><span class="label">Example 8-5. </span>tls-ingress.yaml</h5>
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code class="w"/>
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Ingress</code><code class="w"/>
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">tls-ingress</code><code class="w"/>
<code class="nt">spec</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="nt">tls</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">hosts</code><code class="p">:</code><code class="w"/>
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">alpaca.example.com</code><code class="w"/>
<code class="w">    </code><code class="nt">secretName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">tls-secret-name</code><code class="w"/>
<code class="w">  </code><code class="nt">rules</code><code class="p">:</code><code class="w"/>
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">host</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">alpaca.example.com</code><code class="w"/>
<code class="w">    </code><code class="nt">http</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="nt">paths</code><code class="p">:</code><code class="w"/>
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">backend</code><code class="p">:</code><code class="w"/>
<code class="w">          </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">alpaca</code><code class="w"/>
<code class="w">          </code><code class="nt">servicePort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">8080</code><code class="w"/></pre></div>
<p>Uploading and managing TLS secrets can be difficult. In addition, certificates can often come at a significant cost.<a data-primary="“Let's Encrypt”, running free Certificate Authority" data-primary-sortas="Lets" data-type="indexterm" id="idm45664077473168"/> To help solve this problem, there is a nonprofit called <a href="https://letsencrypt.org">“Let’s Encrypt”</a> running a free Certificate Authority that is API-driven. Since it is API-driven, it is possible to set up a Kubernetes cluster that automatically fetches and installs TLS certificates for you. It can be tricky to set up, but when working, it’s very simple to use. The missing piece is an open source project called  <a href="https://cert-manager.io">cert-manager</a> created by Jetstack, a UK startup,
onboarded to the CNCF. The <em>cert-manager.io</em> website or <a href="https://oreil.ly/S0PU4">GitHub repository</a> has details on how to install cert-manager and get started.<a data-primary="Ingress" data-secondary="advanced topics and gotchas" data-startref="ix_Ingradv" data-type="indexterm" id="idm45664077372336"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Alternate Ingress Implementations" data-type="sect1"><div class="sect1" id="idm45664077371120">
<h1>Alternate Ingress Implementations</h1>
<p>There are many different implementations of Ingress controllers, each building on the base Ingress object with unique features.<a data-primary="Ingress" data-secondary="alternate implementations" data-type="indexterm" id="idm45664077369616"/> It is a vibrant ecosystem.</p>
<p>First, each cloud provider has an Ingress implementation that exposes the specific cloud-based L7 load balancer for that cloud.<a data-primary="cloud" data-secondary="Ingress implementations by cloud providers" data-type="indexterm" id="idm45664077368016"/> Instead of configuring a software load balancer running in a Pod, these controllers take Ingress objects and use them to configure, via an API, the cloud-based load balancers. This reduces the load on the cluster and the management burden for the operators, but can often come at a cost.</p>
<p>The most popular generic Ingress controller is probably the open source <a href="https://oreil.ly/EstHX">NGINX Ingress controller</a>. Be aware that there is also a commercial controller based on the proprietary NGINX Plus. <a data-primary="NGINX" data-secondary="Ingress controller" data-type="indexterm" id="idm45664077365760"/>The open source controller essentially reads Ingress objects and merges them into an NGINX configuration file. It then signals to the NGINX process to restart with the new configuration (while responsibly serving existing in-flight connections). The open source NGINX controller has an enormous number of features and options exposed via <a href="https://oreil.ly/V8nM7">annotations</a>.</p>
<p><a href="https://oreil.ly/5HDun">Emissary</a> and <a href="https://oreil.ly/rZDlX">Gloo</a> are two other Envoy-based Ingress controllers that are focused on being API gateways.<a data-primary="Envoy-based Ingress controllers" data-type="indexterm" id="idm45664077362304"/></p>
<p><a href="https://traefik.io">Traefik</a> is a reverse proxy implemented in Go that also can function as an Ingress controller. It has a set of features and dashboards that are very developer-friendly.</p>
<p>This just scratches the surface. The Ingress ecosystem is very active, and there are many new projects and commercial offerings that build on the humble Ingress object in unique ways.</p>
</div></section>
<section data-pdf-bookmark="The Future of Ingress" data-type="sect1"><div class="sect1" id="future_of_ingress">
<h1>The Future of Ingress</h1>
<p>As you have seen, the Ingress object provides a very useful abstraction for configuring L7 load balancers⁠—but it hasn’t scaled to all the features that users want and various implementations are looking to offer.<a data-primary="Ingress" data-secondary="future of" data-type="indexterm" id="idm45664077288096"/> Many of the features in Ingress are underdefined. Implementations can surface these features in different ways, reducing the portability of configurations between implementations.</p>
<p>Another problem is that it is easy to misconfigure Ingress.<a data-primary="configurations" data-secondary="misconfiguring Ingress" data-type="indexterm" id="idm45664077286544"/> The way that multiple objects compbine opens the door for conflicts that are resolved differently by different implementations. In addition, the way that these are merged across namespaces breaks the idea of namespace isolation.</p>
<p>Ingress was also created before the idea of a service mesh (exemplified by projects such as Istio and Linkerd) was well known. The intersection of Ingress and service meshes is still being defined. Service meshes are covered in greater
detail in 
<span class="keep-together"><a data-type="xref" href="ch15.xhtml#service_mesh">Chapter 15</a></span>.</p>
<p>The future of HTTP load balancing for Kubernetes looks to be the <em>Gateway</em>
API, which is in the midst of development by the Kubernetes special interest
group (SIG) dedicated to networking.<a data-primary="Gateway API" data-type="indexterm" id="idm45664077282400"/> The Gateway API project is intended
to develop a more modern API for routing in Kubernetes. Though it is more
focused on HTTP balancing, Gateway also includes resources for controlling
Layer 4 (TCP) balancing. The Gateway APIs are still very much under
development, so it is strongly recommended that people stick to the existing
Ingress and Service resources that are currently present in Kubernetes.
The current state of the <a href="https://oreil.ly/zhlil">Gateway API can be found online</a>.</p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45664077280784">
<h1>Summary</h1>
<p>Ingress is a unique system in Kubernetes. It is simply a schema, and the implementations of a controller for that schema must be installed and managed separately. But it is also a critical system for exposing services to users in a practical and cost-efficient way. As Kubernetes continues to mature, expect to see Ingress become more and more relevant.<a data-primary="load balancing" data-secondary="HTTP load balancing with Ingress" data-startref="ix_ldbalIng" data-type="indexterm" id="idm45664077279088"/><a data-primary="Ingress" data-startref="ix_Ingr" data-type="indexterm" id="idm45664077277872"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45664078055792"><sup><a href="ch08.xhtml#idm45664078055792-marker">1</a></sup> The <a href="https://oreil.ly/czfCd">Open Systems Interconnection (OSI) model</a> is a standard way to describe how different networking layers build on each other.<a data-primary="networks" data-secondary="layers in OSI model" data-type="indexterm" id="idm45664078054448"/><a data-primary="OSI (Open Systems Interconnection) model" data-type="indexterm" id="idm45664078053504"/> TCP and UDP are considered to be Layer 4, while HTTP is Layer 7.</p></div></div></section></div></body></html>