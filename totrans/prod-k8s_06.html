<html><head></head><body><section data-pdf-bookmark="Chapter 5. Pod Networking" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter5">&#13;
<h1><span class="label">Chapter 5. </span>Pod Networking</h1>&#13;
&#13;
&#13;
<p>Since the early days of networking, we have concerned ourselves with how to facilitate host-to-host communication.<a data-primary="Pods" data-secondary="networking" data-type="indexterm" id="ix_Podnt"/> These concerns include uniquely addressing hosts, routing of packets across networks, and propagation of known routes. For more than a decade, software-defined networks (SDNs) have seen rapid growth by solving these concerns in our increasingly dynamic environments.<a data-primary="software-defined networks (SDNs)" data-type="indexterm" id="idm45611998906904"/> Whether it is in your datacenter with VMware NSX or in the cloud with Amazon VPCs, you are likely a consumer of an SDN.</p>&#13;
&#13;
<p>In Kubernetes, these principles and desires hold. Although our unit moves from hosts to Pods, we need to ensure we have addressability and routability of our workloads. Additionally, given Pods are running as software on our hosts, we will most commonly establish networks that are entirely software-defined.</p>&#13;
&#13;
<p>This chapter will explore the concept of Pod networks. We will start off by addressing some key networking concepts that must be understood and considered before implementing Pod networks. Then we will cover the <a href="https://github.com/containernetworking/cni">Container Networking Interface (CNI)</a>, which enables your choice of network implementation based on your networking requirements. Lastly, we will examine common plug-ins, such as Calico and Cilium, in the ecosystem to make the trade-offs more concrete. In the end, you’ll be more equipped to make decisions around the right networking solution and configuration for your application platform.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Networking is a vast subject in itself. Our intentions are to give you just enough to make informed decisions on your Pod network. If your background is not networking, we highly recommend you go over these concepts with your networking team. Kubernetes does not negate the need to have networking expertise in your &#13;
<span class="keep-together">organization</span>!</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Networking Considerations" data-type="sect1"><div class="sect1" id="networking_considerations">&#13;
<h1>Networking Considerations</h1>&#13;
&#13;
<p>Before diving into implementation details around Pod networks, we should start with a few key areas of consideration.<a data-primary="networking" data-secondary="considerations" data-type="indexterm" id="ix_netcon"/> These areas include:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>IP Address Management (IPAM)</p>&#13;
</li>&#13;
<li>&#13;
<p>Routing protocols</p>&#13;
</li>&#13;
<li>&#13;
<p>Encapsulation and tunneling</p>&#13;
</li>&#13;
<li>&#13;
<p>Workload routability</p>&#13;
</li>&#13;
<li>&#13;
<p>IPv4 and IPv6</p>&#13;
</li>&#13;
<li>&#13;
<p>Encrypted workload traffic</p>&#13;
</li>&#13;
<li>&#13;
<p>Network policy</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With an understanding of these areas, you can begin to make determinations around the correct networking solution for your platform.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="IP Address Management" data-type="sect2"><div class="sect2" id="IPAM">&#13;
<h2>IP Address Management</h2>&#13;
&#13;
<p>In order to communicate to and from Pods, we must ensure they are uniquely addressable.<a data-primary="Pods" data-secondary="networking" data-tertiary="IP address management" data-type="indexterm" id="ix_PodntIPAM"/><a data-primary="networking" data-secondary="considerations" data-tertiary="IP address management" data-type="indexterm" id="ix_netconIPAM"/><a data-primary="IP address management (IPAM)" data-type="indexterm" id="ix_IPAM"/> In Kubernetes, each Pod receives an IP. These IPs may be internal to the cluster or externally routable. Each Pod having its own address simplifies the networking model, considering we do not have to be concerned with colliding ports on shared IPs. However, this IP-per-Pod model does come with its own challenges.</p>&#13;
&#13;
<p>Pods are best thought of as ephemeral. Specifically, they are prone to being restarted or rescheduled based on the needs of the cluster or system failure. This requires IP allocation to execute quickly and the management of the cluster’s IP pool to be efficient. This management is often referred to as <a href="https://oreil.ly/eWJki">IPAM</a> and is not unique to Kubernetes. As we dive deeper into container networking approaches, we will explore a variety of ways IPAM is implemented.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>This ephemeral expectation of a workload’s IP causes issues in some legacy workloads, for example, workloads that pin themselves to a specific IP and expect it to remain static. Depending on your implementation of container networking (covered later in this chapter), you may be able to explicitly reserve IPs for specific workloads. However, we recommend against this model unless necessary. There are many capable service discovery or DNS mechanisms that workloads can take advantage of to properly remedy this issue. Review <a data-type="xref" href="ch06.html#chapter6">Chapter 6</a> for examples.</p>&#13;
</div>&#13;
&#13;
<p>IPAM is implemented based on your choice of CNI plug-in.<a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins for Pod IPAM" data-type="indexterm" id="idm45611998798520"/><a data-primary="CIDR" data-see="Classless Inter-Domain Routing" data-type="indexterm" id="idm45611998797528"/> There are a few commonalities in these plug-ins that pertain to Pod IPAM. First, when clusters are created, a Pod network’s <a href="https://oreil.ly/honRv">Classless Inter-Domain Routing</a> (CIDR) can be specified.<a data-primary="Classless Inter-Domain Routing (CIDR)" data-secondary="setting up Pod CIDR" data-type="indexterm" id="idm45611998795592"/> How it is set varies based on how you bootstrap Kubernetes. In the case of <code>kubeadm</code>, a flag can be passed as follows:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubeadm init --pod-network-cidr 10.30.0.0/16</pre>&#13;
&#13;
<p>In effect, this sets the <code>--cluster-cidr</code> flag <a data-primary="kube-controller-manager" data-secondary="--cluster-cidr flag" data-type="indexterm" id="idm45611998791992"/>on the kube-controller-manager.  Kubernetes will then allocate a chunk of this cluster-cidr to every node. By default, each node is allocated <code>/24</code>. However, this can be controlled by setting the <code>--node-cidr-mask-size-ipv4</code> and/or <code>--node-cidr-mask-size-ipv6</code> flags on the kube-controller-manager.<a data-primary="kube-controller-manager" data-secondary="--node-cidr-mask-size-ipv4 flag" data-type="indexterm" id="idm45611998789608"/><a data-primary="kube-controller-manager" data-secondary="--node-cidr-mask-size-ipv6 flag" data-type="indexterm" id="idm45611998787576"/> A Node object featuring this allocation is as follows:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Node</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">labels</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">kubernetes.io/arch</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">amd64</code><code>&#13;
</code><code>    </code><code class="nt">kubernetes.io/hostname</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">test</code><code>&#13;
</code><code>    </code><code class="nt">kubernetes.io/os</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">linux</code><code>&#13;
</code><code>    </code><code class="nt">manager</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">kubeadm</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">master-0</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">podCIDR</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">10.30.0.0/24</code><code> </code><a class="co" href="#callout_pod_networking_CO1-1" id="co_pod_networking_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">podCIDRs</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">10.30.0.0/24</code><code> </code><a class="co" href="#callout_pod_networking_CO1-2" id="co_pod_networking_CO1-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_pod_networking_CO1-1" id="callout_pod_networking_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This field exists for compatibility. <code>podCIDRs</code> was later introduced as an array to support dual stack (IPv4 and IPv6 CIDRs) on a single node.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO1-2" id="callout_pod_networking_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The IP range assigned to this node is 10.30.0.0 - 10.30.0.255. This is 254 addresses for Pods, out of the 65,534 available in the 10.30.0.0/16 cluster CIDR.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Whether these values are used in IPAM is up to the CNI plug-in. For example, Calico detects and respects this setting, while Cilium offers an option to either manage IP pools independent of Kubernetes (default) or respect these allocations.<a data-primary="Calico" data-secondary="Pod CIDR, IP range values" data-type="indexterm" id="idm45611998774600"/><a data-primary="Cilium" data-secondary="Pod CIDR, IP range values" data-type="indexterm" id="idm45611998773608"/> In most CNI implementations, it is important that your CIDR choice <em>does not</em> overlap with the cluster’s host/node network.<a data-primary="clusters" data-secondary="host/node network, Pod CIDR and" data-type="indexterm" id="idm45611998771992"/> However, assuming your Pod network will remain internal to the cluster, the CIDR chosen can overlap with network space outside the cluster. <a data-type="xref" href="#the_ip_spaces_and_ip_allocations_of_the_most_network">Figure 5-1</a> demonstrates the relationship of these various IP spaces and examples of allocations.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>How large you should set your cluster’s Pod CIDR is often a product of your networking model. In most deployments, a Pod network is entirely internal to the cluster. As such, the Pod CIDR can be very large to accommodate for future scale. When the Pod CIDR is routable to the larger network, thus consuming address space, you may have to do more careful consideration. Multiplying the number of Pods per node by your eventual node count can give you a rough estimate. The number of Pods per node is configurable on the kubelet, but by default is 110.</p>&#13;
</div>&#13;
&#13;
<figure><div class="figure" id="the_ip_spaces_and_ip_allocations_of_the_most_network">&#13;
<img alt="prku 0501" src="assets/prku_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>The IP spaces and IP allocations of the host network, Pod network, and each [host] local CIDR.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Routing Protocols" data-type="sect2"><div class="sect2" id="routing_protocols">&#13;
<h2>Routing Protocols</h2>&#13;
&#13;
<p>Once Pods are addressed, we need to ensure that routes to and from them are understood.<a data-primary="Pods" data-secondary="networking" data-startref="ix_PodntIPAM" data-tertiary="IP address management" data-type="indexterm" id="idm45611998689576"/><a data-primary="networking" data-secondary="considerations" data-startref="ix_netconIPAM" data-tertiary="IP address management" data-type="indexterm" id="idm45611998688056"/><a data-primary="IP address management (IPAM)" data-startref="ix_IPAM" data-type="indexterm" id="idm45611998686568"/><a data-primary="Pods" data-secondary="networking" data-tertiary="routing protocols" data-type="indexterm" id="ix_Podntrt"/><a data-primary="routing protocols" data-type="indexterm" id="ix_rtpro"/> This is where routing protocols come in to play. Routing protocols can be thought of as different ways to propagate routes to and from places. Introducing a routing protocol often enables dynamic routing, relative to configuring <a href="https://oreil.ly/97En2">static routes</a>.<a data-primary="static routes" data-type="indexterm" id="idm45611998665736"/><a data-primary="dynamic routing" data-type="indexterm" id="idm45611998665000"/> In Kubernetes, understanding a multitude of routes becomes important when not leveraging encapsulation (covered in the next section), since the network will often be unaware of how to route workload IPs.</p>&#13;
&#13;
<p>Border Gateway Protocol (BGP) is one of the most commonly used protocols to distribute workload routes.<a data-primary="BGP (Border Gateway Protocol)" data-type="indexterm" id="idm45611998663384"/><a data-primary="Calico" data-secondary="BGP routing protocol" data-type="indexterm" id="idm45611998662712"/> It is used in projects such as <a href="https://www.projectcalico.org">Calico</a> and <a href="https://www.kube-router.io">Kube-Router</a>.<a data-primary="Kube-Router" data-type="indexterm" id="idm45611998660232"/> Not only does BGP enable communication of workload routes in the cluster but its internal routers can also be peered with external routers. Doing so can make external network fabrics aware of how to route to Pod IPs. In implementations such as Calico, a BGP daemon is run as part of the Calico Pod. This Pod runs on every host.<a data-primary="Border Gateway Protocol" data-see="BGP" data-type="indexterm" id="idm45611998659016"/> As routes to workloads become known, the Calico Pod modifies the kernel routing table to include routes to each potential workload.<a data-primary="native routing" data-type="indexterm" id="idm45611998657800"/> This provides native routing via the workload IP, which can work especially well when running in the same L2 segment. <a data-type="xref" href="#the_calico_pod_sharing_routes_via_its_bgp_peer">Figure 5-2</a> demonstrates this behavior.</p>&#13;
&#13;
<figure><div class="figure" id="the_calico_pod_sharing_routes_via_its_bgp_peer">&#13;
<img alt="prku 0502" src="assets/prku_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>The <code>calico-pod</code> sharing routes via its BGP peer. The kernel routing table is then programmed accordingly.</h6>&#13;
</div></figure>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Making Pod IPs routable to larger networks may seem appealing at first glance but should be carefully considered. See <a data-type="xref" href="#workload_routability">“Workload Routability”</a> for more details.<a data-primary="networks" data-secondary="making Pod IPs routable to larger networks" data-type="indexterm" id="idm45611998651800"/></p>&#13;
</div>&#13;
&#13;
<p class="pagebreak-before">In many<a data-primary="cloud computing" data-secondary="difficulties with native routing to workload IPs" data-type="indexterm" id="idm45611998650072"/> environments, native routing to workload IPs is not possible. Additionally, routing protocols such as BGP may not be able to integrate with an underlying network; such is the case running in a cloud-provider’s network. For example, let’s consider a CNI deployment where we wish to support native routing and share routes via BGP. In an AWS environment, this can fail for two reasons:</p>&#13;
<dl>&#13;
<dt>Source/Destination checks are enabled</dt>&#13;
<dd>&#13;
<p>This ensures that packets hitting the host have the destination (and source IP) of the target host. If it does not match, the packet is dropped. This setting can be disabled.</p>&#13;
</dd>&#13;
<dt>Packet needs to traverse subnets</dt>&#13;
<dd>&#13;
<p>If the packet needs to leave the subnet, the destination IP is evaluated by the underlying AWS routers. When the Pod IP is present, it will not be able to route.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In these scenarios, we look to tunneling protocols.<a data-primary="routing protocols" data-startref="ix_rtpro" data-type="indexterm" id="idm45611998644760"/><a data-primary="Pods" data-secondary="networking" data-startref="ix_Podntrt" data-tertiary="routing protocols" data-type="indexterm" id="idm45611998643784"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Encapsulation and Tunneling" data-type="sect2"><div class="sect2" id="idm45611998642168">&#13;
<h2>Encapsulation and Tunneling</h2>&#13;
&#13;
<p>Tunneling protocols give you the ability to run your Pod network in a way that is mostly unknown to the underlying network.<a data-primary="tunneling protocols" data-type="indexterm" id="idm45611998640296"/><a data-primary="Pods" data-secondary="networking" data-tertiary="encapsulation and tunneling" data-type="indexterm" id="idm45611998639592"/><a data-primary="networking" data-secondary="considerations" data-tertiary="encapsulation and tunneling" data-type="indexterm" id="idm45611998638360"/><a data-primary="encapsulation and tunneling" data-type="indexterm" id="ix_encptun"/> This is achieved using encapsulation. As the name implies, encapsulation involves putting a packet (the inner packet) inside another packet (the outer packet). The inner packet’s src IP and dst IP fields will reference the workload (Pod) IPs, whereas the outer packet’s src IP and dst IP fields will reference the host/node IPs. When the packet leaves a node, it will appear to the underlying network as any other packet since the workload-specific data is in the payload. There are a variety of tunneling protocols such as VXLAN, Geneve, and GRE.<a data-primary="Geneve tunneling protocol" data-type="indexterm" id="idm45611998635464"/><a data-primary="VXLAN tunneling protocol" data-type="indexterm" id="idm45611998634776"/> In Kuberntes, VXLAN has become one of the most commonly used methods by networking plug-ins. <a data-type="xref" href="#vxlan_encapsulation_used_to_move_an_inner_packet">Figure 5-3</a> demonstrates an encapsulated packet crossing the wire via VXLAN.</p>&#13;
&#13;
<p>As you can see, VXLAN puts an entire Ethernet frame inside a UDP packet. This essentially gives you a fully virtualized layer-2 network, often referred to as an overlay network. The network beneath the overlay, referred to as the underlay network, does not concern itself with the overlay. This is one of the primary benefits to tunneling protocols.</p>&#13;
&#13;
<figure><div class="figure" id="vxlan_encapsulation_used_to_move_an_inner_packet">&#13;
<img alt="prku 0503" src="assets/prku_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>VXLAN encapsulation used to move an inner packet, for workloads, across hosts. The network cares only about the outer packet, so it needs to have zero awareness of workload IPs and their routes.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Often, you choose whether to use a tunneling protocol based on the requirements/capabilities of your environment. Encapsulation has the benefit of working in many scenarios since the overlay is abstracted from the underlay network. However, this approach comes with a few key downsides:</p>&#13;
<dl>&#13;
<dt>Traffic can be harder to understand and troubleshoot</dt>&#13;
<dd>&#13;
<p>Packets within packets can create extra complexity when troubleshooting network issues.</p>&#13;
</dd>&#13;
<dt>Encapsulation/decapsulation will incur processing cost</dt>&#13;
<dd>&#13;
<p>When a packet goes to leave a host it must be encapsulated, and when it enters a host it must be decapsulated. While likely small, this will add overhead relative to native routing.<a data-primary="native routing" data-type="indexterm" id="idm45611998626424"/></p>&#13;
</dd>&#13;
<dt>Packets will be larger</dt>&#13;
<dd>&#13;
<p>Due to the embedding of packets, they will be larger when transmitted over the wire.<a data-primary="maximum transmission unit (MTU)" data-type="indexterm" id="idm45611998624440"/> This may require adjustments to the <a href="https://oreil.ly/dzYBz">maximum transmission unit (MTU)</a> to ensure they fit on the network.<a data-primary="encapsulation and tunneling" data-startref="ix_encptun" data-type="indexterm" id="idm45611998622936"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Workload Routability" data-type="sect2"><div class="sect2" id="workload_routability">&#13;
<h2>Workload Routability</h2>&#13;
&#13;
<p>In most clusters, Pod networks are internal to the cluster.<a data-primary="routing" data-secondary="workload routability" data-type="indexterm" id="idm45611998619864"/><a data-primary="workloads" data-secondary="routability" data-type="indexterm" id="idm45611998618888"/><a data-primary="networking" data-secondary="considerations" data-tertiary="workload routability" data-type="indexterm" id="idm45611998617944"/><a data-primary="Pods" data-secondary="networking" data-tertiary="workload routability" data-type="indexterm" id="idm45611998616728"/> This means Pods can directly communicate with each other, but external clients cannot reach Pod IPs directly. Considering Pod IPs are ephemeral, communicating directly with a Pod’s IP is often bad practice. Relying on service discovery or load balancing mechanics that abstract the underlying IP is preferable. A huge benefit to the internal Pod network is that it <em>does not</em> occupy precious address space within your organization. Many organizations manage address space to ensure addresses stay unique within the company. Thus, you would certainly get a dirty look when you ask for a <code>/16</code> space (65,536 IPs) for each Kubernetes cluster you bootstrap!</p>&#13;
&#13;
<p>When Pods are not directly routable, we have several patterns to facilitate external traffic to Pod IPs. Commonly we will expose an Ingress controller on the host network of a subset of dedicated nodes. Then, once the packet enters the Ingress controller proxy, it can route directly to Pod IPs since it takes part in the Pod network. Some cloud providers even include (external) load balancer integration that wires this all together automatically. We explore a variety of these ingress models, and their trade-offs, in <a data-type="xref" href="ch06.html#chapter6">Chapter 6</a>.</p>&#13;
&#13;
<p>At times, requirements necessitate that Pods are routable to the larger network. There are two primary means to accomplish this. The first is to use a networking plug-in that integrates with the underlying network directly. For example, <a href="https://github.com/aws/amazon-vpc-cni-k8s">AWS’s VPC CNI</a> attaches multiple secondary IPs to each node and allocates them to Pods.<a data-primary="AWS (Amazon Web Services)" data-secondary="VPC CNI" data-type="indexterm" id="idm45611998610792"/> This makes each Pod routable just as an EC2 host would be. The primary downside to this model is it will consume IPs in your subnet/VPC.<a data-primary="BGP (Border Gateway Protocol)" data-type="indexterm" id="idm45611998609560"/> The second option is to propagate routes to Pods via a routing protocol such as BGP, as described in <a data-type="xref" href="#routing_protocols">“Routing Protocols”</a>. Some plug-ins using BGP will even enable you to make a subset of your Pod network routable, rather than having to expose the entire IP space.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Avoid making your Pod network externally routable unless absolutely necessary. We often see legacy applications driving the desire for routable Pods. For example, consider a TCP-based workload where a client must be pinned to the same backend. Typically, we recommend updating the application(s) to fit within the container networking paradigm using service discovery and possibly re-architecting the backend to not require client-server affinity (when possible). While exposing the Pod networking may seem like a simple solution, doing so comes at the cost of eating up IP space and potentially making IPAM and route propagation configurations more complex.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="IPv4 and IPv6" data-type="sect2"><div class="sect2" id="idm45611998605816">&#13;
<h2>IPv4 and IPv6</h2>&#13;
&#13;
<p>The overwhelming majority of clusters today run IPv4 exclusively.<a data-primary="IPv4 and IPv6" data-type="indexterm" id="idm45611998604024"/><a data-primary="networking" data-secondary="considerations" data-tertiary="IPv4 and IPv6" data-type="indexterm" id="idm45611998603320"/><a data-primary="Pods" data-secondary="networking" data-tertiary="IPv4 and IPv6" data-type="indexterm" id="idm45611998602104"/> However, we are seeing the desire to run IPv6-networked clusters in certain clients such as telcos where addressability of many workloads is critical.<a data-primary="dual-stack" data-type="indexterm" id="idm45611998600600"/> Kubernetes does support IPv6 via <a href="https://oreil.ly/sj_jN">dual-stack</a> as of <code>1.16</code>. At the time of this writing, dual-stack is an alpha feature. Dual-stack enables you to configure IPv4 and IPv6 address spaces in your clusters.</p>&#13;
&#13;
<p>If your use case requires IPv6, it can easily be enabled but requires a few components to line up:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>While still in alpha, a feature-gate must be enabled on the kube-apiserver and kubelet.</p>&#13;
</li>&#13;
<li>&#13;
<p>The kube-apiserver, kube-controller-manager, and kube-proxy all require an additional configuration to specify the IPv4 and IPv6 space.</p>&#13;
</li>&#13;
<li>&#13;
<p>You must use a CNI plug-in that supports IPv6, such as <a href="https://projectcalico.org">Calico</a> or <a href="https://cilium.io">Cilium</a>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With the preceding<a data-primary="IP address management (IPAM)" data-secondary="CNI plug-in IPAM, IPv4 and IPv6" data-type="indexterm" id="idm45611998593080"/> in place, you will see two CIDR allocations on each Node object:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">podCIDR</code><code class="p">:</code> <code class="l-Scalar-Plain">10.30.0.0/24</code>&#13;
  <code class="nt">podCIDRs</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">10.30.0.0/24</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">2002:1:1::/96</code></pre>&#13;
&#13;
<p>The CNI plug-in’s IPAM is responsible for determining whether an IPv4, IPv6, or both is assigned to each Pod.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Encrypted Workload Traffic" data-type="sect2"><div class="sect2" id="idm45611998582856">&#13;
<h2>Encrypted Workload Traffic</h2>&#13;
&#13;
<p>Pod-to-Pod traffic is rarely (if ever) encrypted by default.<a data-primary="encryption" data-secondary="workload traffic" data-type="indexterm" id="idm45611998581720"/><a data-primary="networking" data-secondary="considerations" data-tertiary="encrypted workload traffic" data-type="indexterm" id="idm45611998580744"/><a data-primary="Pods" data-secondary="networking" data-tertiary="encrypted workload traffic" data-type="indexterm" id="idm45611998574856"/> This means that packets sent over the wire without encryption, such as TLS, can be sniffed as plain text. Many network plug-ins support encrypting traffic over the wire. For example, Antrea supports encryption with <a href="https://oreil.ly/jqzCQ">IPsec</a> when using a GRE tunnel. Calico is able to encrypt traffic by tapping into a node’s <a href="https://www.wireguard.com">WireGuard</a> installation.</p>&#13;
&#13;
<p>Enabling encryption may seem like a no-brainer. However, there are trade-offs to be considered. We recommend talking with your networking team to understand how host-to-host traffic is handled today. Is data encrypted when it goes between hosts in your datacenter? Additionally, what other encryption mechanisms may be at play? For example, does every service talk over TLS? Do you plan to leverage a service mesh where workload proxies leverage mTLS? If so, is encrypting at the service proxy and CNI layer required? While encryption will increase the depth of defense, it will also add complexity to network management and troubleshooting. Most importantly, needing to encrypt and decrypt packets will impact performance, thus lowering your potential throughput.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Network Policy" data-type="sect2"><div class="sect2" id="idm45611998570504">&#13;
<h2>Network Policy</h2>&#13;
&#13;
<p>Once the Pod network is wired up, a logical next step is to consider how to set up network policy.<a data-primary="networking" data-secondary="considerations" data-tertiary="network policy" data-type="indexterm" id="idm45611998568600"/><a data-primary="Pods" data-secondary="networking" data-tertiary="network policy" data-type="indexterm" id="idm45611998567352"/> Network policy is similar to firewall rules or security groups, where we can define what ingress and egress traffic is allowed. Kubernetes offers a <a href="https://oreil.ly/1UV_3">NetworkPolicy API</a>, as part of the core networking APIs.<a data-primary="NetworkPolicy objects" data-type="indexterm" id="idm45611998565192"/><a data-primary="Container Networking Interface (CNI)" data-secondary="CNI providers implementing network policy" data-type="indexterm" id="idm45611998564488"/> Any cluster can have policies added to it. However, it is incumbent on the CNI provider to <em>implement</em> the policy. This means that a cluster running a CNI provider that does not support NetworkPolicy, such as <a href="https://github.com/coreos/flannel">flannel</a>, will accept NetworkPolicy objects but not act on them. Today, most CNIs have some level of support for NetworkPolicy. Those that do not can often be used alongside plug-ins such as Calico, where the plug-in runs in a mode where it provides only policy &#13;
<span class="keep-together">enforcement</span>.<a data-primary="Calico" data-secondary="network policy enforcement" data-type="indexterm" id="idm45611998561192"/></p>&#13;
&#13;
<p>NetworkPolicy being available inside of Kubernetes adds yet another layer where firewall-style rules can be managed. For example, many networks provide subnet or host-level rules available via a distributed firewall or security group mechanism. While good, often these existing solutions do not have visibility into the Pod network. This prevents the level of granularity that may be desired in setting up rules for Pod-based workload communication. Another compelling aspect of Kubernetes NetworkPolicy is that, like most objects we deal with in Kubernetes, it is defined declaratively and, we think, far easier to manage relative to most firewall management solutions! For these reasons, we generally recommend considering implementing network policy at the Kubernetes level rather than trying to make existing firewall solutions fit this new paradigm. This does not mean you should throw out your existing host-to-host firewall solution(s). More so, let Kubernetes handle the intra-workload policy.</p>&#13;
&#13;
<p>Should you choose to utilize NetworkPolicy, it is important to note these policies are <em>Namespace-scoped</em>.<a data-primary="Namespace-scoped NetworkPolicy" data-type="indexterm" id="idm45611998558056"/> By default, when NetworkPolicy objects are not present, Kubernetes allows all communication to and from workloads. When setting a policy, you can select what workloads the policy applies to. When present, the default behavior inverts and any egress and ingress traffic not allowed by the policy will be blocked. This means that the Kubernetes NetworkPolicy API specifies only what traffic is allowed. Additionally, the policy in a Namespace is additive. Consider the following NetworkPolicy object<a data-primary="NetworkPolicy objects" data-secondary="configuring Ingress/Egress rules" data-type="indexterm" id="idm45611998556712"/> that configures ingress and egress rules:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">NetworkPolicy</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">team-netpol</code><code>&#13;
</code><code>  </code><code class="nt">namespace</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">org-1</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">podSelector</code><code class="p">:</code><code> </code><code class="p-Indicator">{</code><code class="p-Indicator">}</code><code> </code><a class="co" href="#callout_pod_networking_CO2-1" id="co_pod_networking_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">policyTypes</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">Ingress</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">Egress</code><code>&#13;
</code><code>  </code><code class="nt">ingress</code><code class="p">:</code><code> </code><a class="co" href="#callout_pod_networking_CO2-2" id="co_pod_networking_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">from</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">ipBlock</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">cidr</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">10.40.0.0/24</code><code>&#13;
</code><code>    </code><code class="nt">ports</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">protocol</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TCP</code><code>&#13;
</code><code>      </code><code class="nt">port</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code><code>&#13;
</code><code>  </code><code class="nt">egress</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">to</code><code class="p">:</code><code> </code><a class="co" href="#callout_pod_networking_CO2-3" id="co_pod_networking_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>    </code><code class="nt">ports</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">protocol</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">UDP</code><code>&#13;
</code><code>      </code><code class="nt">port</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">53</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">to</code><code class="p">:</code><code> </code><a class="co" href="#callout_pod_networking_CO2-4" id="co_pod_networking_CO2-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">namespaceSelector</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">matchLabels</code><code class="p">:</code><code>&#13;
</code><code>          </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">org-2</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">podSelector</code><code class="p">:</code><code>&#13;
</code><code>        </code><code class="nt">matchLabels</code><code class="p">:</code><code>&#13;
</code><code>          </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">team-b</code><code>&#13;
</code><code>    </code><code class="nt">ports</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">protocol</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TCP</code><code>&#13;
</code><code>      </code><code class="nt">port</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_pod_networking_CO2-1" id="callout_pod_networking_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The empty <code>podSelector</code> implies this policy applies to all Pods in this Namespace. Alternatively, you can match against a label.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO2-2" id="callout_pod_networking_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This ingress rule allows traffic from sources with an IP in the range of 10.40.0.0/24, when the protocol is TCP and the port is 80.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO2-3" id="callout_pod_networking_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>This egress rule allows DNS traffic from workloads.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO2-4" id="callout_pod_networking_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>This egress rule limits sending traffic to packets destined for workloads in the <code>org-2</code> Namespace with the label <code>team-b</code>. Additionally, the protocol must be TCP and the destination port is 80.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Over time, we have seen the NetworkPolicy API be limiting to certain use cases. Some common <a data-primary="NetworkPolicy objects" data-secondary="use cases" data-type="indexterm" id="idm45611998373976"/>desires include:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Complex condition evaluation</p>&#13;
</li>&#13;
<li>&#13;
<p>Resolution of IPs based on DNS records</p>&#13;
</li>&#13;
<li>&#13;
<p>L7 rules (host, path, etc.)</p>&#13;
</li>&#13;
<li>&#13;
<p>Cluster-wide policy, enabling global rules to be put in place, rather than having to replicate them in every Namespace.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>To satisfy these desires, some CNI plug-ins offer their own, more capable, policy APIs.<a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins offering network policy APIs" data-type="indexterm" id="idm45611998338888"/> The primary trade-off to using provider-specific APIs is that your rules are no longer portable across plug-ins. We will explore examples of these when we cover Calico and Cilium later in the chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary: Networking Considerations" data-type="sect2"><div class="sect2" id="idm45611998337496">&#13;
<h2>Summary: Networking Considerations</h2>&#13;
&#13;
<p>In the previous sections we have covered key networking considerations that will enable you to make an informed decision about your Pod networking strategy.<a data-primary="networking" data-secondary="considerations" data-tertiary="summary of key concerns" data-type="indexterm" id="idm45611998336056"/> Before diving into CNI and plug-ins, let’s recap some of the key areas of consideration:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>How large should your Pod CIDR be per cluster?</p>&#13;
</li>&#13;
<li>&#13;
<p>What networking constraints does your underlay network put on your future Pod network?</p>&#13;
</li>&#13;
<li>&#13;
<p>If using a Kubernetes managed service or vendor offering, what networking plug-in(s) are supported?</p>&#13;
</li>&#13;
<li>&#13;
<p>Are routing protocols such as BGP supported in your infrastructure?</p>&#13;
</li>&#13;
<li>&#13;
<p>Could unencapsulated (native) packets be routed through the network?</p>&#13;
</li>&#13;
<li>&#13;
<p>Is using a tunnel protocol (encapsulation) desirable or required?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do you need to support (externally) routable Pods?</p>&#13;
</li>&#13;
<li>&#13;
<p>Is running IPv6 a requirement for your workloads?</p>&#13;
</li>&#13;
<li>&#13;
<p>On what level(s) will you expect to enforce network policy or firewall rules?</p>&#13;
</li>&#13;
<li>&#13;
<p>Does your Pod network need to encrypt traffic on the wire?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With the answers to these questions fleshed out, you are in a good place to start learning about what enables you to plug in the correct technology to solve these issues, the Container Networking Interface (CNI).<a data-primary="networking" data-secondary="considerations" data-startref="ix_netcon" data-type="indexterm" id="idm45611998324664"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Container Networking Interface (CNI)" data-type="sect1"><div class="sect1" id="idm45611998916776">&#13;
<h1>The Container Networking Interface (CNI)</h1>&#13;
&#13;
<p>All the considerations discussed thus far make it clear that different use cases warrant different container networking solutions.<a data-primary="Pods" data-secondary="networking" data-tertiary="Container Networking Interface" data-type="indexterm" id="idm45611998321256"/><a data-primary="Container Networking Interface (CNI)" data-type="indexterm" id="idm45611998320040"/> In the early days of Kubernetes, most &#13;
<span class="keep-together">clusters</span> were running a networking plug-in called <a href="https://github.com/coreos/flannel">flannel</a>.<a data-primary="flannel" data-type="indexterm" id="idm45611998317960"/> Over time, solutions such as <a href="https://www.projectcalico.org">Calico</a> and others gained popularity.<a data-primary="Calico" data-type="indexterm" id="idm45611998316472"/> These new plug-ins brought different approaches to creating and running networks. This drove the creation of a standard for how systems such as Kubernetes could request networking resources for its workloads. This standard is known as the <a href="https://github.com/containernetworking/cni">Container Networking Interface (CNI)</a>. Today, all networking options compatible with Kubernetes conform to this interface. Similar to the Container Storage Interface (CSI) and Container Runtime Interface (CRI), this gives us flexibility in the networking stack of our application platform.</p>&#13;
&#13;
<p>The CNI specification defines a few key operations:</p>&#13;
<dl>&#13;
<dt><code>ADD</code></dt>&#13;
<dd>&#13;
<p>Adds a container to the network and responds with the associated interface(s), IP(s), and more.</p>&#13;
</dd>&#13;
<dt><code>DELETE</code></dt>&#13;
<dd>&#13;
<p>Removes a container from the network and releases all associated resources.</p>&#13;
</dd>&#13;
<dt><code>CHECK</code></dt>&#13;
<dd>&#13;
<p>Verifies a container’s network is set up properly and responds with an error if there are issues.</p>&#13;
</dd>&#13;
<dt><code>VERSION</code></dt>&#13;
<dd>&#13;
<p>Returns the CNI version(s) supported by the plug-in.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>This functionality is implemented in a binary<a data-primary="Container Networking Interface (CNI)" data-secondary="binary and configuration" data-type="indexterm" id="idm45611998307288"/> that is installed on the host. The kubelet will communicate with the appropriate CNI binary based on the  configuration it expects on the host. An example of this configuration file is as follows:</p>&#13;
&#13;
<pre data-code-language="json" data-type="programlisting"><code class="p">{</code><code>&#13;
  </code><code class="nt">"cniVersion"</code><code class="p">:</code><code> </code><code class="s2">"0.4.0"</code><code class="p">,</code><code> </code><a class="co" href="#callout_pod_networking_CO3-1" id="co_pod_networking_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
  </code><code class="nt">"name"</code><code class="p">:</code><code> </code><code class="s2">"dbnet"</code><code class="p">,</code><code> </code><a class="co" href="#callout_pod_networking_CO3-2" id="co_pod_networking_CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
  </code><code class="nt">"type"</code><code class="p">:</code><code> </code><code class="s2">"bridge"</code><code class="p">,</code><code>&#13;
  </code><code class="nt">"bridge"</code><code class="p">:</code><code> </code><code class="s2">"cni0"</code><code class="p">,</code><code>&#13;
  </code><code class="nt">"args"</code><code class="p">:</code><code> </code><code class="p">{</code><code>&#13;
    </code><code class="nt">"labels"</code><code> </code><code class="p">:</code><code> </code><code class="p">{</code><code>&#13;
        </code><code class="nt">"appVersion"</code><code> </code><code class="p">:</code><code> </code><code class="s2">"1.0"</code><code>&#13;
    </code><code class="p">}</code><code>&#13;
  </code><code class="p">}</code><code class="p">,</code><code>&#13;
  </code><code class="nt">"ipam"</code><code class="p">:</code><code> </code><code class="p">{</code><code> </code><a class="co" href="#callout_pod_networking_CO3-3" id="co_pod_networking_CO3-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
    </code><code class="nt">"type"</code><code class="p">:</code><code> </code><code class="s2">"host-local"</code><code class="p">,</code><code>&#13;
    </code><code class="nt">"subnet"</code><code class="p">:</code><code> </code><code class="s2">"10.1.0.0/16"</code><code class="p">,</code><code>&#13;
    </code><code class="nt">"gateway"</code><code class="p">:</code><code> </code><code class="s2">"10.1.0.1"</code><code>&#13;
  </code><code class="p">}</code><code>&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_pod_networking_CO3-1" id="callout_pod_networking_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The CNI (specification) version this plug-in expects to talk over.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO3-2" id="callout_pod_networking_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The CNI driver (binary) to send networking setup requests to.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO3-3" id="callout_pod_networking_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The IPAM driver to use, specified when the CNI plug-in does not handle IPAM.</p></dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Multiple CNI configurations may exist in the CNI <em>conf</em> directory. They are evaluated lexicographically, and the first configuration will be used.</p>&#13;
</div>&#13;
&#13;
<p>Along with the CNI configuration and CNI binary, most plug-ins run a Pod on each host that handles concerns beyond interface attachment and IPAM. This includes responsibilities such as route propagation and network policy programming.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CNI Installation" data-type="sect2"><div class="sect2" id="idm45611998186392">&#13;
<h2>CNI Installation</h2>&#13;
&#13;
<p>CNI drivers must be installed on every node taking part in the Pod network. Additionally, the CNI configuration must be established.<a data-primary="Container Networking Interface (CNI)" data-secondary="installation" data-type="indexterm" id="idm45611998184296"/><a data-primary="Pods" data-secondary="networking" data-tertiary="CNI installation" data-type="indexterm" id="idm45611998183352"/> The installation is typically handled when you deploy a CNI plug-in. For example, when deploying Cilium, a DaemonSet is created, which puts a <code>cilium</code> Pod on every node. This Pod features a PostStart command that runs the baked-in script <em>install-cni.sh</em>. This script will start by installing two drivers. First it will install the loopback driver to support the <code>lo</code> interface. Then it will install the <code>cilium</code> driver. The script executes conceptually as follows (the example has been greatly simplified for brevity):</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="c"># Install CNI drivers to host</code>&#13;
&#13;
<code class="c"># Install the CNI loopback driver; allow failure</code>&#13;
cp /cni/loopback /opt/cin/bin/ <code class="o">||</code> <code class="nb">true</code>&#13;
&#13;
<code class="c"># install the cilium driver</code>&#13;
cp /opt/cni/bin/cilium-cni /opt/cni/bin/</pre>&#13;
&#13;
<p>After installation, the kubelet still needs to know which driver to use. It will look within <em>/etc/cni/net.d/</em> (configurable via flag) to find a CNI configuration. The same <em>install-cni.sh</em> script adds this as follows:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">cat &gt; /etc/cni/net.d/05-cilium.conf <code class="s">&lt;&lt;EOF</code>&#13;
<code class="s">{</code>&#13;
<code class="s">  "cniVersion": "0.3.1",</code>&#13;
<code class="s">  "name": "cilium",</code>&#13;
<code class="s">  "type": "cilium-cni",</code>&#13;
<code class="s">  "enable-debug": ${ENABLE_DEBUG}</code>&#13;
<code class="s">}</code>&#13;
<code class="s">EOF</code></pre>&#13;
&#13;
<p>To demonstrate this order of operations, let’s take a look a newly bootstrapped, single-node cluster. This cluster was bootstrapped using <code>kubeadm</code>. Examining all Pods reveals that the <code>core-dns</code> Pods are not running:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE&#13;
kube-system   coredns-f9fd979d6-26lfr        0/1     Pending   <code class="m">0</code>          3m14s&#13;
kube-system   coredns-f9fd979d6-zqzft        0/1     Pending   <code class="m">0</code>          3m14s&#13;
kube-system   etcd-test                      1/1     Running   <code class="m">0</code>          3m26s&#13;
kube-system   kube-apiserver-test            1/1     Running   <code class="m">0</code>          3m26s&#13;
kube-system   kube-controller-manager-test   1/1     Running   <code class="m">0</code>          3m26s&#13;
kube-system   kube-proxy-xhh2p               1/1     Running   <code class="m">0</code>          3m14s&#13;
kube-system   kube-scheduler-test            1/1     Running   <code class="m">0</code>          3m26s</pre>&#13;
&#13;
<p>After examining the kubelet logs on the host scheduled to run <code>core-dns</code>, it becomes<a data-primary="DNS (Domain Name System)" data-secondary="core-dns Pod not starting due to CNI issues" data-type="indexterm" id="idm45611998121288"/> clear that the lack of CNI configuration is causing the container runtime to not start the Pod:</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>This case of DNS not starting is one of the most common indicators of CNI issues after cluster bootstrapping. Another symptom is nodes reporting <code>NotReady</code> status.</p>&#13;
</div>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="c"># journalctl -f -u kubelet</code>&#13;
&#13;
-- Logs begin at Sun 2020-09-27 15:40:13 UTC. --&#13;
Sep <code class="m">27</code> 17:11:18 <code class="nb">test </code>kubelet<code class="o">[</code>2972<code class="o">]</code>: E0927 17:11:18.817089 <code class="m">2972</code> kubelet.go:2103<code class="o">]</code>&#13;
Container runtime network not ready: <code class="nv">NetworkReady</code><code class="o">=</code><code class="nb">false</code>&#13;
reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni&#13;
config uninitialized&#13;
Sep <code class="m">27</code> 17:11:19 <code class="nb">test </code>kubelet<code class="o">[</code>2972<code class="o">]</code>: W0927 17:11:19.198643 <code class="m">2972</code> cni.go:239<code class="o">]</code>&#13;
Unable to update cni config: no networks found in /etc/cni/net.d</pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The reason Pods such as <code>kube-apiserver</code> and <code>kube-controller-manager</code> started successfully is due to their use of the host network. Since they leverage the host network and do not rely on the Pod network, they are not susceptible to the same behavior seen by <code>core-dns</code>.</p>&#13;
</div>&#13;
&#13;
<p>Cilium can be deployed to the cluster by simply applying a YAML file from the Cilium documentation.<a data-primary="Cilium" data-secondary="deploying to cluster" data-type="indexterm" id="idm45611990384568"/> In doing so, the aforementioned <code>cilium</code> Pod is deployed on every node, and the <em>cni-install.sh</em> script is run. Examining the CNI bin and configuration directories, we can see the installed components:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="c"># ls /opt/cni/bin/ | grep -i cilium</code>&#13;
cilium-cni&#13;
&#13;
<code class="c"># ls /etc/cni/net.d/ | grep -i cilium</code>&#13;
05-cilium.conf</pre>&#13;
&#13;
<p>With this in place, the kubelet and container runtime are functioning as expected. Most importantly, the <code>core-dns</code> Pod is up and running! <a data-type="xref" href="#docker_is_used_to_run_containers_the_kubelet_interacts_with_the_cni_to_attach_network">Figure 5-4</a> demonstrates the relationship we’ve covered thus far in this section.<a data-primary="kubelet" data-secondary="interacting with CNI to attach network" data-type="indexterm" id="idm45611990358408"/></p>&#13;
&#13;
<figure><div class="figure" id="docker_is_used_to_run_containers_the_kubelet_interacts_with_the_cni_to_attach_network">&#13;
<img alt="prku 0504" src="assets/prku_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>Docker is used to run containers. The kubelet interacts with the CNI to attach network interfaces and configure the Pod’s network.</h6>&#13;
</div></figure>&#13;
&#13;
<p>While this example explored installation via Cilium, most plug-ins follow a similar deployment model. The key justification for plug-in choice is based on the discussion in <a data-type="xref" href="#networking_considerations">“Networking Considerations”</a>. With this in mind, we’ll transition to exploring some CNI plug-ins to better understand different approaches.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CNI Plug-ins" data-type="sect1"><div class="sect1" id="idm45611998322824">&#13;
<h1>CNI Plug-ins</h1>&#13;
&#13;
<p>Now we are going to explore a few implementations of CNI.<a data-primary="Pods" data-secondary="networking" data-tertiary="CNI plug-ins" data-type="indexterm" id="ix_PodntCNIpl"/><a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-type="indexterm" id="ix_CNIplug"/> CNI has one of the largest array of options relative to other interfaces such as CRI. As such, we won’t be exhaustive in the plug-ins we cover and encourage you to explore more than what we will. We chose the following plug-ins as a factor of being the most common we see at clients and unique enough to demonstrate the variety of approaches.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>A Pod network is foundational to any Kubernetes cluster. As such, your CNI plug-in will be in the critical path. As time goes on, you may wish to change your CNI plug-in. If this occurs, we recommend rebuilding clusters as opposed to doing in-place migrations. In this approach, you spin up a new cluster featuring the new CNI. Then, depending on your architecture and operational model, migrate workloads to the new cluster. It is possible to do an in-place CNI migration, but it takes on nontrivial risk and should be carefully weighed against our recommendation.</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Calico" data-type="sect2"><div class="sect2" id="idm45611990349864">&#13;
<h2>Calico</h2>&#13;
&#13;
<p>Calico is<a data-primary="Calico" data-secondary="overview" data-type="indexterm" id="idm45611990345832"/><a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-tertiary="Calico" data-type="indexterm" id="idm45611990344936"/> a well-established CNI plug-in in the cloud native ecosystem. <a href="https://www.projectcalico.org">Project Calico</a> is the open source project that supports this CNI plug-in, and <a href="https://www.tigera.io">Tigera</a> is the commercial company offering enterprise features and support.<a data-primary="routing" data-secondary="in Calico CNI plug-in" data-type="indexterm" id="idm45611990342264"/><a data-primary="BGP (Border Gateway Protocol)" data-secondary="use by Calico" data-type="indexterm" id="idm45611990341288"/> Calico makes heavy use of BGP to propagate workload routes between nodes and to offer integration with larger datacenter fabrics. Along with installing a CNI binary, Calico runs a <code>calico-node</code> agent on each host. This agent features a BIRD daemon for facilitating BGP peering between nodes and a Felix agent, which takes the known routes and programs them into the kernel route tables. This relationship is demonstrated in <a data-type="xref" href="#calico_component_relationship_showing_the_bgp_peering_to_communicate">Figure 5-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="calico_component_relationship_showing_the_bgp_peering_to_communicate">&#13;
<img alt="prku 0505" src="assets/prku_0505.png"/>&#13;
<h6><span class="label">Figure 5-5. </span>Calico component relationship showing the BGP peering to communicate routes and the programming of iptables and kernel routing tables accordingly.</h6>&#13;
</div></figure>&#13;
&#13;
<p>For IPAM, Calico <a data-primary="IP address management (IPAM)" data-secondary="Calico CNI plug-in" data-type="indexterm" id="idm45611990297640"/><a data-primary="Classless Inter-Domain Routing (CIDR)" data-secondary="in CNI Calico plug-in" data-type="indexterm" id="idm45611990296792"/>initially respects the <code>cluster-cidr</code> setting described in <a data-type="xref" href="#IPAM">“IP Address Management”</a>. However, its capabilities are far greater than relying on a CIDR allocation per node. Calico creates CRDs called <a href="https://oreil.ly/-Nd-Q">IPPools</a>. <a data-primary="IPPools" data-type="indexterm" id="idm45611990293944"/><a data-primary="custom resource definitions (CRDs)" data-secondary="IPPools in Calico CNI plug-in" data-type="indexterm" id="idm45611990293336"/>This provides a lot of flexibility in IPAM, specifically enabling features such as:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Configuring block size per node</p>&#13;
</li>&#13;
<li>&#13;
<p>Specifying what node(s) an IPPool applies to</p>&#13;
</li>&#13;
<li>&#13;
<p>Allocating IPPools to Namespaces, rather than nodes</p>&#13;
</li>&#13;
<li>&#13;
<p>Configuring routing behavior</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">Paired with the ability to have multiple pools per cluster, you have a lot of flexibility in IPAM and network architecture. By default, clusters run a single IPPool, as shown here:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">projectcalico.org/v3</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">IPPool</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">default-ipv4-ippool</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">cidr</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">10.30.0.0/16</code><code> </code><a class="co" href="#callout_pod_networking_CO4-1" id="co_pod_networking_CO4-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">blockSize</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">29</code><code> </code><a class="co" href="#callout_pod_networking_CO4-2" id="co_pod_networking_CO4-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="nt">ipipMode</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Always</code><code> </code><a class="co" href="#callout_pod_networking_CO4-3" id="co_pod_networking_CO4-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="nt">natOutgoing</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">true</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_pod_networking_CO4-1" id="callout_pod_networking_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The cluster’s Pod network CIDR.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO4-2" id="callout_pod_networking_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The size of each node-level CIDR allocation.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO4-3" id="callout_pod_networking_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The encapsulation mode.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Calico <a data-primary="Calico" data-secondary="routing packets inside the cluster" data-type="indexterm" id="idm45611990245848"/>offers a variety of ways to route packets inside of the cluster. This includes:</p>&#13;
<dl>&#13;
<dt>Native</dt>&#13;
<dd>&#13;
<p>No encapsulation of packets.</p>&#13;
</dd>&#13;
<dt>IP-in-IP</dt>&#13;
<dd>&#13;
<p>Simple encapsulation.<a data-primary="encapsulation and tunneling" data-secondary="in Calico CNI plug-in" data-type="indexterm" id="idm45611990209848"/> IP packet is placed in the payload of another.</p>&#13;
</dd>&#13;
<dt>VXLAN</dt>&#13;
<dd>&#13;
<p>Advanced encapsulation. An entire L2 frame is encapsulated within a UDP packet. Establishes a virtual L2 overlay.<a data-primary="VXLAN tunneling protocol" data-secondary="use by Calico" data-type="indexterm" id="idm45611990207592"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Your choice is often a function of what your network can support. As described in <a data-type="xref" href="#routing_protocols">“Routing Protocols”</a>, native routing will<a data-primary="native routing" data-type="indexterm" id="idm45611990205272"/> likely provide the best performance, smallest packet size, and simplest troubleshooting experience. However, in many environments, especially those involving multiple subnets, this mode is not possible. The encapsulation approaches work in most environments, especially VXLAN. Additionally, the VXLAN mode does not require usage of BGP, which can be a solution to environments where BGP peering is blocked.<a data-primary="BGP (Border Gateway Protocol)" data-secondary="VXLAN and" data-type="indexterm" id="idm45611990204120"/> One unique feature of Calico’s encapsulation approach is that it can be enabled exclusively for traffic that crosses a subnet boundary. This enables near native performance when routing within the subnet while not breaking routing outside the subnet. This can be enabled by<a data-primary="CrossSubnet IP-in-IP mode" data-type="indexterm" id="idm45611990202856"/><a data-primary="IPPools" data-secondary="ipipMode set to CrossSubnet" data-type="indexterm" id="idm45611990202248"/> setting the IPPool’s <code>ipipMode</code> to <code>CrossSubnet</code>. <a data-type="xref" href="#traffic_behavior_when_cross_subnet">Figure 5-6</a> demonstrates this &#13;
<span class="keep-together">behavior</span>.</p>&#13;
&#13;
<figure><div class="figure" id="traffic_behavior_when_cross_subnet">&#13;
<img alt="prku 0506" src="assets/prku_0506.png"/>&#13;
<h6><span class="label">Figure 5-6. </span>Traffic behavior when CrossSubnet IP-in-IP mode is enabled.</h6>&#13;
</div></figure>&#13;
&#13;
<p>For deployments of Calico that keep BGP enabled, by default, no additional work is needed thanks to the built-in BGP daemon in the <code>calico-node</code> Pod. <a data-primary="route reflectors" data-type="indexterm" id="idm45611990196616"/>In more complex architectures, organizations use this BGP functionality as a way to introduce <a href="https://tools.ietf.org/html/rfc4456">route reflectors</a>, sometimes required at large scales when the (default) full-mesh approach becomes limited. Along with route reflectors, peering can be configured to talk to network routers, which in turn can make the overall network aware of routes to Pod IPs.<a data-primary="BGPPeer CRD" data-type="indexterm" id="idm45611990194888"/><a data-primary="custom resource definitions (CRDs)" data-secondary="Calico's BGPPeer" data-type="indexterm" id="idm45611990194280"/> This is all configured using Calico’s BGPPeer CRD, seen here:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">projectcalico.org/v3</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">BGPPeer</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">external-router</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">peerIP</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">192.23.11.100</code><code> </code><a class="co" href="#callout_pod_networking_CO5-1" id="co_pod_networking_CO5-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">asNumber</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">64567</code><code> </code><a class="co" href="#callout_pod_networking_CO5-2" id="co_pod_networking_CO5-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="nt">nodeSelector</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">routing-option</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">==</code><code class="l-Scalar-Plain"> </code><code class="l-Scalar-Plain">'external'</code><code> </code><a class="co" href="#callout_pod_networking_CO5-3" id="co_pod_networking_CO5-3"><img alt="3" src="assets/3.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_pod_networking_CO5-1" id="callout_pod_networking_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The IP of the device to (bgp) peer with.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO5-2" id="callout_pod_networking_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <a href="https://oreil.ly/HiXLN">autonomous system</a> ID of the cluster.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO5-3" id="callout_pod_networking_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Which cluster nodes should peer with this device. This field is optional. When omitted, the BGPPeer configuration is considered <em>global</em>. Not peering global is advisable only when a certain set of nodes should offer a unique routing capability, such as offering routable IPs.</p></dd>&#13;
</dl>&#13;
&#13;
<p>In terms of network policy, Calico fully implements the Kubernetes NetworkPolicy API.<a data-primary="NetworkPolicy API" data-secondary="Calico implementation of" data-type="indexterm" id="idm45611990149832"/> Calico offers two additional CRDs for increased functionality. These include <a href="https://oreil.ly/oMsCm">(projectcalico.org/v3).NetworkPolicy</a> and <a href="https://oreil.ly/3pUOs">GlobalNetworkPolicy</a>. These Calico-specific APIs look similar to Kubernetes NetworkPolicy but feature more capable rules and richer expressions for evaluation. Also, policy ordering and application layer policy (requires integration with Istio) are supported.<a data-primary="GlobalNetworkPolicy" data-type="indexterm" id="idm45611990115736"/> GlobalNetworkPolicy &#13;
<span class="keep-together">is particularly</span> useful because it applies policy at a cluster-wide level. This makes it easier to achieve models such as micro-segmentation, where all traffic is denied by default and egress/ingress is opened up based on the workload needs. You can apply a &#13;
<span class="keep-together">GlobalNetworkPolicy</span> that denies all traffic except for critical services such as DNS. Then, at a Namespace level, you can open up access to ingress and egress accordingly. Without GlobalNetworkPolicy, we’d need to add and manage deny-all rules in <em>every</em> Namespace.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Historically, Calico has made use of iptables to implement packet routing decisions.<a data-primary="iptables" data-secondary="use by Calico" data-type="indexterm" id="idm45611990112120"/> For Services, Calico relies on the programming done by kube-proxy to resolve an endpoint for a Service. For network policy, Calico programs iptables to determine whether a packet should be allowed to enter or leave the host. At the time of this writing, Calico has introduced an eBPF dataplane option.<a data-primary="eBPF (extended Berkeley Packet Filter)" data-secondary="eBPF data plane option in Calico" data-type="indexterm" id="idm45611990110824"/> We expect, over time, more functionality used by Calico to be moved into this model.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cilium" data-type="sect2"><div class="sect2" id="idm45611990347032">&#13;
<h2>Cilium</h2>&#13;
&#13;
<p>Cilium is a newer CNI plug-in relative to Calico.<a data-primary="Cilium" data-secondary="overview" data-type="indexterm" id="idm45611990107928"/><a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-tertiary="Cilium" data-type="indexterm" id="idm45611990107080"/> It’s the first CNI plug-in to utilize the <a href="https://ebpf.io">extended Berkeley Packet Filter (eBPF)</a>. This means that rather<a data-primary="eBPF (extended Berkeley Packet Filter)" data-secondary="use by Cilium" data-type="indexterm" id="idm45611990105240"/> than processing packets in userspace it is able to do so without leaving kernel space. Paired with <a href="https://oreil.ly/M3m6t">eXpress Data Path (XDP)</a>, hooks may be established in the NIC driver to make routing decisions.<a data-primary="routing" data-secondary="in Cilium CNI plug-in" data-type="indexterm" id="idm45611990103528"/> This enables<a data-primary="eXpress Data Path (XDP)" data-type="indexterm" id="idm45611990102552"/><a data-primary="XDP (eXpress Data Path)" data-type="indexterm" id="idm45611990101944"/> routing decisions to occur immediately when the packet is received.</p>&#13;
&#13;
<p>As a technology, eBPF has demonstrated performance and scale at organizations such as <a href="https://oreil.ly/agUXl">Facebook</a> and <a href="https://oreil.ly/Ubt1Q">Netflix</a>. With the usage of eBPF, Cilium is able to claim increased features around scalability, observability, and security. This deep integration with BPF means that common CNI concerns such as NetworkPolicy enforcement are no longer handled via iptables in userspace. Instead, extensive use of <a href="https://oreil.ly/4Rdvf">eBPF maps</a> enable decisions <a data-primary="eBPF (extended Berkeley Packet Filter)" data-secondary="maps" data-type="indexterm" id="idm45611990098552"/>to occur quickly in a way that scales as more and more rules are added. <a data-type="xref" href="#cilium_interacts_with_ebpf_maps_and_programs_at_the_kernel_level">Figure 5-7</a> shows a high-level overview of the stack with Cilium installed.</p>&#13;
&#13;
<figure><div class="figure" id="cilium_interacts_with_ebpf_maps_and_programs_at_the_kernel_level">&#13;
<img alt="prku 0507" src="assets/prku_0507.png"/>&#13;
<h6><span class="label">Figure 5-7. </span>Cilium interacts with eBPF maps and programs at the kernel level.</h6>&#13;
</div></figure>&#13;
&#13;
<p>For IPAM, Cilium follows the model of either delegating IPAM to a cloud provider integration or managing it itself.<a data-primary="IP address management (IPAM)" data-secondary="in Cilium CNI plug-in" data-type="indexterm" id="idm45611990094488"/><a data-primary="Classless Inter-Domain Routing (CIDR)" data-secondary="in Cilium CNI plug-in" data-type="indexterm" id="idm45611990093640"/> In the most common scenario of Cilium managing IPAM, it will allocate Pod CIDRs to each node. By default, Cilium will manage these CIDRs independent of Kubernetes Node allocations. The node-level addressing will be exposed in the <code>CiliumNode</code> CRD.<a data-primary="CiliumNode CRD" data-type="indexterm" id="idm45611990092040"/> This will provide greater flexibility in management of IPAM and is preferable. If you wish to stick to the default CIDR allocations done in Kubernetes based on its Pod CIDR, Cilium offer a <code>kubernetes</code> IPAM mode. This will rely on the Pod CIDR allocated to each node, which is exposed in the Node object. Following is an example of a <code>CiliumNode</code> object.<a data-primary="nodes" data-secondary="CiliumNode object" data-type="indexterm" id="idm45611990090184"/> You can expect one of these to exist for each node in the cluster:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">cilium.io/v2</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">CiliumNode</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">node-a</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">addresses</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">ip</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">192.168.122.126</code><code> </code><a class="co" href="#callout_pod_networking_CO6-1" id="co_pod_networking_CO6-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>    </code><code class="nt">type</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">InternalIP</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">ip</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">10.0.0.245</code><code>&#13;
</code><code>    </code><code class="nt">type</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">CiliumInternalIP</code><code>&#13;
</code><code>  </code><code class="nt">health</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">ipv4</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">10.0.0.78</code><code>&#13;
</code><code>  </code><code class="nt">ipam</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">podCIDRs</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">10.0.0.0/24</code><code> </code><a class="co" href="#callout_pod_networking_CO6-2" id="co_pod_networking_CO6-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_pod_networking_CO6-1" id="callout_pod_networking_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>IP address of this workload node.</p></dd>&#13;
<dt><a class="co" href="#co_pod_networking_CO6-2" id="callout_pod_networking_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>CIDR allocated to this node. The size of this allocation can be controlled in Cilium’s config using <code>cluster-pool-ipv4-mask-size: "24"</code>.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Similar to Calico, Cilium offers encapsulated and native routing modes. The default mode is encapsulated.<a data-primary="encapsulation and tunneling" data-secondary="in Cilium CNI plug-in" data-type="indexterm" id="idm45611990022168"/> Cilium supports using tunneling protocols VXLAN or Geneve.<a data-primary="VXLAN tunneling protocol" data-secondary="support by Cilium" data-type="indexterm" id="idm45611990021192"/><a data-primary="Geneve tunneling protocol" data-secondary="support by Cilium" data-type="indexterm" id="idm45611990020344"/> This mode should work with most networks as long as host-to-host routability pre-exists.<a data-primary="native routing" data-type="indexterm" id="idm45611990019256"/> To run in native mode, Pod routes must be understood at some level. For example, Cilium supports using AWS’s ENI for IPAM. In this model, the Pod IPs are known to the VPC and are inherently routable. To run a native-mode with Cilium-managed IPAM, assuming the cluster runs in the same L2 segment, <code>auto-direct-node-routes: true</code> can be added to Cilium’s configuration. Cilium will then program the host’s route tables accordingly. If you span L2 networks, you may need to introduce additional routing protocols such as BGP to distribute routes.</p>&#13;
&#13;
<p>In terms of network policy, Cilium can enforce the Kubernetes <a href="https://oreil.ly/_WUKS">NetworkPolicy API</a>. As<a data-primary="NetworkPolicy API" data-secondary="Cilium enforcement of" data-type="indexterm" id="idm45611989989192"/> an alternative to this policy, Cilium offers its own <a href="https://oreil.ly/EpkhJ">CiliumNetworkPolicy</a> and <a href="https://oreil.ly/RtYH5">CiliumClusterwideNetworkPolicy</a>. The key difference between these two is the scope of the policy.<a data-primary="CiliumClusterwideNetworkPolicy" data-type="indexterm" id="idm45611989986968"/><a data-primary="CiliumNetworkPolicy" data-type="indexterm" id="idm45611989986360"/> CiliumNetworkPolicy is Namespace scoped, while CiliumClusterwideNetworkPolicy is cluster-wide. Both of these have increased functionality beyond the capabilities of Kubernetes NetworkPolicy. Along with supporting label-based layer 3 policy, they support policy based on DNS resolution and application-level (layer 7) requests.</p>&#13;
&#13;
<p>While most CNI plug-ins don’t involve themselves with Services, Cilium offers a fully featured kube-proxy replacement.<a data-primary="kube-proxy" data-secondary="Cilium replacement of" data-type="indexterm" id="idm45611989984664"/><a data-primary="Services" data-secondary="Cilium capabilities with" data-type="indexterm" id="idm45611989983816"/> This functionality is built into the <code>cilium-agent</code> deployed to each node. To deploy in the mode, you’ll want to ensure kube-proxy is absent from your cluster and that the <code>KubeProxyReplacement</code> setting is set to <code>strict</code> in Cilium.<a data-primary="KubeProxyReplacement setting" data-type="indexterm" id="idm45611989981544"/> When using this mode, Cilium will configure routes for Services within eBPF maps, making resolution as fast as O(1). This is in contrast to kube-proxy, which implements Services in iptables chains and can cause issues at scale and/or when there is high churn of Services.<a data-primary="CLIs (command-line interfaces)" data-secondary="Cilium CLI" data-type="indexterm" id="idm45611989980520"/> Additionally, the CLI provided by Cilium offers a good experience when troubleshooting constructs such as Services or network policy. Rather than trying to interpret iptables chains, you can query the system as &#13;
<span class="keep-together">follows</span>:</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="shell" data-type="programlisting">kubectl <code class="nb">exec</code> -it -n kube-system cilium-fmh8d -- cilium service list&#13;
&#13;
ID   Frontend               Service Type   Backend&#13;
<code class="o">[</code>...<code class="o">]</code>&#13;
<code class="m">7</code>    192.40.23.111:80       ClusterIP      <code class="nv">1</code> <code class="o">=</code>&gt; 10.30.0.28:80&#13;
                                           <code class="nv">2</code> <code class="o">=</code>&gt; 10.30.0.21:80</pre>&#13;
&#13;
<p>Cilium’s use of eBPF programs and maps makes it an extremely compelling and interesting CNI option. By continuing to leverage eBPF programs, more functionality is being introduced that integrates with Cilium—for example, the ability to extract flow data, policy violations, and more. To extract and present this valuable data, <a href="https://github.com/cilium/hubble">hubble</a> was introduced.<a data-primary="hubble" data-type="indexterm" id="idm45611989970248"/> It makes use of Cilium’s eBPF programs to provide a UI and CLI for operators.</p>&#13;
&#13;
<p>Lastly, we should mention that the eBPF functionality made available by Cilium can be run alongside many existing CNI providers. This is achieved by running Cilium in its CNI chaining mode. In this mode, an existing plug-in such as AWS’s VPC CNI will handle routing and IPAM. Cilium’s responsibility will exclusively be the functionality offered by its various eBPF programs including network observability, load balancing, and network policy enforcement. This approach can be preferable when you either cannot fully run Cilium in your environment or wish to test out its functionality alongside your current CNI choice.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="AWS VPC CNI" data-type="sect2"><div class="sect2" id="idm45611990109128">&#13;
<h2>AWS VPC CNI</h2>&#13;
&#13;
<p>AWS’s VPC CNI demonstrates a very different approach to what we have covered thus far.<a data-primary="AWS (Amazon Web Services)" data-secondary="VPC CNI" data-type="indexterm" id="idm45611989960920"/><a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-tertiary="AWS VPC CNI" data-type="indexterm" id="idm45611989960008"/><a data-primary="Virtual Private Clouds (VPCs)" data-secondary="AWS VPC CNI" data-type="indexterm" id="idm45611989958824"/> Rather than running a Pod network independent of the node network, it fully integrates Pods into the same network. <a data-primary="routing" data-secondary="in AWS VPC CNI" data-type="indexterm" id="idm45611989957656"/>Since a second network is not being introduced, the concerns around distributing routes or tunneling protocols are no longer needed. When a Pod is provided an IP, it becomes part of the network in the same way an EC2 host would. It is subject to the same <a href="https://oreil.ly/HYHHp">route tables</a> as any other host in the subnet. Amazon refers to this as native VPC &#13;
<span class="keep-together">networking</span>.</p>&#13;
&#13;
<p>For IPAM, a daemon will attach a second <a href="https://oreil.ly/NBjs3">elastic network interface (ENI)</a> to the Kubernetes node.<a data-primary="elastic network interface (ENI)" data-type="indexterm" id="idm45611989953912"/><a data-primary="IP address management (IPAM)" data-secondary="in AWS VPC CNI" data-type="indexterm" id="idm45611989953176"/> It will then maintain a pool of <a href="https://oreil.ly/vUGdI">secondary IPs</a> that will eventually get attached to Pods. The amount of IPs available to a node depends on the EC2 instance size. These IPs are typically “private” IPs from within the VPC. As mentioned earlier in this chapter, this will consume IP space from your VPC and make the IPAM system more complex than a completely independent Pod network. However, the routing of traffic and troubleshooting has been significantly simplified given we are not introducing a new network! <a data-type="xref" href="#the_ipam_daemon_is_responsible_for_maintaining_the_eni_and_pool_of">Figure 5-8</a> demonstrates the IPAM setup with AWS VPC CNI.</p>&#13;
&#13;
<figure><div class="figure" id="the_ipam_daemon_is_responsible_for_maintaining_the_eni_and_pool_of">&#13;
<img alt="prku 0508" src="assets/prku_0508.png"/>&#13;
<h6><span class="label">Figure 5-8. </span>The IPAM daemon is responsible for maintaining the ENI and pool of secondary IPs.</h6>&#13;
</div></figure>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The use of ENIs will impact the number of Pods you can run per node. AWS <a href="https://oreil.ly/jk_XL">maintains a list on its GitHub page</a> that correlates instance type to max Pods.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multus" data-type="sect2"><div class="sect2" id="idm45611989946248">&#13;
<h2>Multus</h2>&#13;
&#13;
<p>So far, we have covered specific CNI plug-ins that attach an interface to a Pod, thus making it available on a network.<a data-primary="Multus CNI plug-in" data-type="indexterm" id="idm45611989944824"/><a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-tertiary="Multus" data-type="indexterm" id="idm45611989944120"/> But what if a Pod needs to be attached to more than one network? This is where the Multus CNI plug-in comes in.<a data-primary="network function virtualizations (NFVs)" data-type="indexterm" id="idm45611989942632"/> While not extremely common, there are use cases in the telecommunications industry that require their network function virtualizations (NFVs) to route traffic to a specific, dedicated, &#13;
<span class="keep-together">network</span>.</p>&#13;
&#13;
<p>Multus can be thought of as a CNI that enables using multiple other CNIs. In this model, Multus becomes the CNI plug-in interacted with by Kubernetes. Multus is configured with a default network that is commonly the network expected to facilitate Pod-to-Pod communication. This could even be one of the plug-ins we’ve talked about in this chapter! Then, Multus supports configuring secondary networks by specifying additional plug-ins that can be used to attach another interface to a Pod. Pods can then be annotated with something like <code>k8s.v1.cni.cncf.io/networks: sriov-conf</code> to attach an additional network. <a data-type="xref" href="#shows_the_traffic_flow_of_a_multi_network_multus_configuration">Figure 5-9</a> shows the result of this &#13;
<span class="keep-together">configuration</span>.</p>&#13;
&#13;
<figure><div class="figure" id="shows_the_traffic_flow_of_a_multi_network_multus_configuration">&#13;
<img alt="prku 0509" src="assets/prku_0509.png"/>&#13;
<h6><span class="label">Figure 5-9. </span>The traffic flow of a multinetwork Multus configuration.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Additional Plug-ins" data-type="sect2"><div class="sect2" id="idm45611989919464">&#13;
<h2>Additional Plug-ins</h2>&#13;
&#13;
<p>The landscape of plug-ins is vast, and we’ve covered only a very small subset. However, the ones covered<a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-tertiary="other" data-type="indexterm" id="idm45611989918200"/> in this chapter do identify some of the key variances you’ll find in plug-ins. The majority of alternatives take differing approaches to the engine used to facilitate the networking, yet many core principles stay the same. The following list identifies some additional plug-ins and gives a small glimpse into their networking approach:</p>&#13;
<dl>&#13;
<dt><a href="https://antrea.io/docs">Antrea</a></dt>&#13;
<dd>&#13;
<p>Data plane is facilitated via <a href="https://www.openvswitch.org">Open vSwitch</a>. Offers <a data-primary="Open vSwitch" data-type="indexterm" id="idm45611989914136"/><a data-primary="Antrea" data-type="indexterm" id="idm45611989913528"/>high-performance routing along with the ability to introspect flow data.</p>&#13;
</dd>&#13;
<dt><a href="https://www.weave.works/oss/net">Weave</a></dt>&#13;
<dd>&#13;
<p>Overlay network that provides many mechanisms to route traffic—for example, the fast datapath options using OVS modules to keep packet processing in the kernel.<a data-primary="flannel" data-type="indexterm" id="idm45611989911096"/><a data-primary="Weave" data-type="indexterm" id="idm45611989910488"/></p>&#13;
</dd>&#13;
<dt><a href="https://github.com/coreos/flannel">flannel</a></dt>&#13;
<dd>&#13;
<p>Simple layer-3 network for Pods and one of the early CNIs. It supports multiple backends yet is most commonly configured to use VXLAN.<a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-startref="ix_CNIplug" data-type="indexterm" id="idm45611989908216"/><a data-primary="Pods" data-secondary="networking" data-startref="ix_PodntCNIpl" data-tertiary="CNI plug-ins" data-type="indexterm" id="idm45611989907128"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611989905320">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>The Kubernetes/container networking ecosystem is filled with options. This is good! As we’ve covered throughout this chapter, networking requirements can vary significantly from organization to organization. Choosing a CNI plug-in is likely to be one of the most foundational considerations for your eventual application platform. While exploring the many options may feel overwhelming, we highly recommend you work to better understand the networking requirements of your environment and applications. With a deep understanding of this, the right networking plug-in choice should fall into place!<a data-primary="Pods" data-secondary="networking" data-startref="ix_Podnt" data-type="indexterm" id="idm45611989903368"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>