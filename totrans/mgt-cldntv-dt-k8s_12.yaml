- en: Chapter 11\. Migrating Data Workloads to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first chapter, we presented a vision for combining all of the infrastructure
    needed for your cloud native applications into one place: Kubernetes. Our argument
    was simple: if you’re excluding data and its supporting infrastructure from your
    Kubernetes deployments, you haven’t fully embraced cloud native principles. We’ve
    covered a lot of ground since then, examining how various types of data infrastructure
    work on Kubernetes and demonstrating the art of the possible.'
  prefs: []
  type: TYPE_NORMAL
- en: So, where do you go from here? What are the steps to fully realize this vision?
    At this point, you may already have some parts of your applications in Kubernetes.
    More than likely, you also have several previous generations of infrastructure
    such as containers, VMs, or bare-metal servers, whether running in your own datacenters
    or in the cloud. In this final chapter, we’ll leverage everything you’ve learned
    so far to help you create a plan to fully manage your cloud native data in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vision: Application-Aware Platforms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the book, we’ve heard a diverse range of voices in the community
    present their wisdom about data in Kubernetes and practical advice for this monumental
    undertaking. No matter where you are in the process, whether you’re a Kubernetes
    beginner or a seasoned multiyear operator, we all have things to learn from their
    expertise. Now it’s time to zoom out and consider how the move to Kubernetes intersects
    with other trends in the software industry. Craig McLuckie was part of the team
    that created Kubernetes at Google and eventually shepherded it into open source.
    He’s been very active in the cloud native infrastructure community and shares
    some possibilities and challenges as we move toward data on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Craig offers an inspiring vision for a future where infrastructure conforms
    to the application instead of the infrastructure-coupled applications we have
    today. As you’ve seen in the technologies we’ve explored in this book, the idea
    of declarative infrastructure that reconciles via the Kubernetes control plane
    is everywhere. Now we can begin to flip the script by building applications from
    the top down instead of from the bottom up. This is an opportunity to change the
    way your organization leverages data technology. Are you ready to start? It’s
    time to map out your journey.
  prefs: []
  type: TYPE_NORMAL
- en: Charting Your Path to Success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In preparing to migrate your stateful workloads to Kubernetes, you’ll probably
    have a few questions in mind, like “What technologies should we use?” and “How
    will we roll out the changes?” and “How do we make sure our team is ready?” Most
    of these questions will map nicely to the classic IT framework of people, process,
    and technology (PPT). Since every organization’s journey will be different, we’ll
    provide recommendations in each category instead of a detailed roadmap. An important
    part of your exercise is choosing what migrates into Kubernetes and what doesn’t.
    Every migration should have a strong case.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will likely have some of these recommendations in place already, so the
    actual work needed is to ensure that your efforts in all three areas work together
    toward your desired outcome. One word of warning: this is not the time to “run
    fast and break things.” You’ll have plenty of time to do that after you have the
    core elements in place. With a strong foundation, you will achieve levels of agility
    and speed you haven’t seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: People
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core of any IT organization is its people. Migrating any workload to Kubernetes
    represents a massive shift in mindset for your organization and requires proper
    training and preparation. You will need people who understand the technology already
    or are willing to learn. This requirement is even more true in preparing to migrate
    to stateful workloads. Beyond the apparent tasks of training up on Kubernetes
    and reading books like this one, we’d like to draw your attention to two areas:
    specific job roles that successful organizations execute well and leveraging open
    source communities as a force multiplier for your teams.'
  prefs: []
  type: TYPE_NORMAL
- en: Critical people roles for cloud native data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We could list many roles that are key to a successful migration, but we’ll
    highlight three that are central to managing cloud native data and discuss how
    they relate:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud architects
  prefs: []
  type: TYPE_NORMAL
- en: Architects provide technical direction to the development of cloud applications,
    influencing everything from the clouds and regions where you’ll deploy your applications,
    to the data infrastructure you’ll use. This includes when to rely on self-managed
    open source projects versus managed services. An effective cloud architect carefully
    selects technology to meet current business needs while leaving room for future
    extensibility.
  prefs: []
  type: TYPE_NORMAL
- en: Site reliability engineers
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra), we talked
    about adopting an SRE mindset. While this mindset is something that every engineer
    in your organization should be working toward, DBAs have an incredibly strategic
    opportunity to make the transition into an SRE role. Instead of just deploying
    a database and walking away, a data-focused SRE takes a holistic view of the data
    infrastructure and how it supports the system’s overall goals, with an eye toward
    the best performance for the cost.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers
  prefs: []
  type: TYPE_NORMAL
- en: Whereas data scientists are concerned about extracting the value from data,
    data engineers are responsible for operationalizing data. They build data processes,
    assemble systems, and think about the end-user consumption of data products. Data
    engineers should be versed not only in Kubernetes-based technology but also in
    what cloud services can be used in concert for an optimized outcome. Data engineers
    will play a significant role in selecting and deploying technology that supports
    the AI/ML workloads we discussed in [Chapter 10](ch10.html#machine_learning_and_other_emerging_use),
    composing multiple components to create flows that deliver real-time insights
    into your applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'To think about how these roles work together in an organization, consider the
    analogy of a farming operation:'
  prefs: []
  type: TYPE_NORMAL
- en: The architect is like the planner who determines what crops to grow, and in
    what quantities in each season.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SRE is like the farmer who plants and cultivates the crops to ensure they
    are healthy and productive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data engineer is like a distributor who harvests the crops and ensures they
    reach their proper destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don’t already have these roles defined within your organization, don’t
    worry. In many cases, it is possible to retrain engineers in your organization
    who are currently in a different role.
  prefs: []
  type: TYPE_NORMAL
- en: Communities to fast-track your innovation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To paraphrase the sword-wielding old man in *The Legend of Zelda*, it’s dangerous
    to go alone. Bring friends. Communities are a core part of working in technology,
    and we work together, learn together, and share successes and failures. When embarking
    on a new technology journey, look for the communities that form around that technology.
    The following are a few notable communities in cloud native data. You can seek
    them out for information, join the conversation, and hopefully contribute:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Native Computing Foundation
  prefs: []
  type: TYPE_NORMAL
- en: Also known as the [CNCF](https://www.cncf.io), this organization is a part of
    the more extensive [Linux Foundation](https://www.linuxfoundation.org), a nonprofit
    organization devoted to open source advocacy. The CNCF is the home for Kubernetes
    and many projects that run in Kubernetes, including several featured in this book.
    You can see the amount of energy put into Kubernetes native projects from the
    graduated and incubating projects list. Members of CNCF pay a fee that goes to
    support the advocacy and administration of the foundation and its projects.
  prefs: []
  type: TYPE_NORMAL
- en: The Technical Oversight Committee (TOC) approves and maintains the technical
    vision for CNCF projects. With so many projects to maintain, [Technical Advisory
    Groups (TAGs)](https://oreil.ly/KSxmL) have been formed to handle cross-project
    concerns. Each TAG maintains its autonomy within an initial charter to create
    a place for similarly grouped projects to maintain interoperability standards.
    Each maintains its own Slack workspace and mailing lists for community discussions.
  prefs: []
  type: TYPE_NORMAL
- en: All development activity for a project is centered around its GitHub repository.
    To get involved in contributing code, search for the [“good first issue”](https://oreil.ly/xt2QR)
    tag in GitHub Issues for each project. If you have broader interests, you might
    consider joining the conversation happening in TAGs to help shape future direction.
    Twice a year, the [KubeCon + CloudNativeCon user conferences](https://oreil.ly/Ijlki)
    are held by the CNCF in North America, China, and Europe, with an enormous session
    list. Some of the best sessions are the user stories about deploying specific
    cloud native technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Software Foundation
  prefs: []
  type: TYPE_NORMAL
- en: The [ASF](https://www.apache.org) is a nonprofit organization for software conservancy.
    ASF members provide governance, services, and support for accepted projects. After
    going through an incubation process, projects graduate to become top-level projects
    where they earn the Apache name (e.g., Apache Cassandra, Apache Spark, and Apache
    Pulsar). Each project is run independently by a project management committee (PMC),
    and users with the right to make project changes are known as *committers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to note the distinction between the project and user communities
    around Apache projects. The project community is concerned with building the project,
    and the user community is downstream and primarily focuses on using the project
    in their applications. This separation of concerns is evident in the two mailing
    lists available for most projects: *dev@<project name>.apache.org* and *user@<project
    name>.apache.org*.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in contributing code, jumping right in is the best way
    to start. Apache projects use [Jira](https://oreil.ly/Odauf) to track changes
    and bugs. Look for “low hanging fruit” or “good first project” tags on the Jira
    issues. In the user community, participating in the mailing list or Stack Overflow
    is a great way to start contributing by helping others. Giving presentations about
    Apache projects is the lifeblood of awareness for each project and one of the
    best contributions.
  prefs: []
  type: TYPE_NORMAL
- en: Data on Kubernetes Community (DoKC)
  prefs: []
  type: TYPE_NORMAL
- en: A different kind of organization than the CNCF and ASF, DoKC is a knowledge
    community composed of industry vendors and end users. DoKC isn’t a place for hosting
    software projects but a central gathering place for people in a growing field
    within infrastructure. Technology vendors sponsor the community, but the charter
    is to remain vendor neutral in all activities. Those activities include in-person
    and online meetups, blogs on the [*dok.community* website](https://dok.community),
    and a companion event to KubeCon, DoK Day.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to gathering the community, DoKC also produces useful resources
    to guide users as they make decisions about data technology on Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the number of data technologies available, the [DoK Landscape](https://oreil.ly/HgYlL)
    has been created to help compare and evaluate the various options. You can search
    by attributes such as open source versus commercial licensing, or whether an operator
    or Helm chart is available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An annual [DoK survey](https://oreil.ly/ZmaQu) is also conducted to gauge industry
    opinions and provide guidance on common problems. The report is free and can be
    used in your presentations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a knowledge community, the best way to participate in the DoKC is sharing
    knowledge. When the community was being formed, the amount of information about
    end users running stateful workloads in Kubernetes was scarce. Creating a space
    to focus on data topics has led to a growing set of common interests and concepts.
    Most of the interviews in this book came from people we met in the DoKC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the book, we’ve seen the benefits of contributions from each of
    these communities toward making data technologies run effectively on Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: The PersistentVolume subsystem we discussed in [Chapter 2](ch02.html#managing_data_storage_on_kubernetes)
    has provided a solid foundation for a wide variety of open source and commercial
    storage solutions on Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operator frameworks we discussed in [Chapter 5](ch05.html#automating_database_management_on_kuber),
    including Operator SDK, Kubebuilder, and KUDO have proven to be a great enabler
    toward developing operators for a variety of data infrastructure from the ASF
    and other open source projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes StatefulSets (first introduced in [Chapter 3](ch03.html#databases_on_kubernetes_the_hard_way))
    are an interesting case. While they have proven quite valuable for managing distributed
    databases, the community has also identified some opportunities for improvement
    that we look forward to seeing addressed in the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, Spark and other projects in the analytics community have identified
    challenges with the Kubernetes default scheduler, as you learned in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
    Thankfully, Kubernetes provides APIs for extending the scheduler that projects
    like Apache YuniKorn and Volcano can leverage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, plenty of work remains to be done in this ecosystem of interconnected
    communities, and it will take contributions from all corners of the cloud native
    world to get us to the next stage of maturity as an industry. Remember, community
    participation isn’t limited to providing code to a project. One of the most important
    contributions to any community is sharing your story. Think about your experiences
    of learning new technologies, and you’ll likely recall good documentation, great
    examples, and the most valuable of all: “how we built this” stories. Please consider
    sharing your story any way you can. Your community needs you!'
  prefs: []
  type: TYPE_NORMAL
- en: Technology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For many of you, this is the most exciting part. Cool toys! As you consider
    your journey to cloud native data, you’ll have important decisions in terms of
    the technologies you choose to use and the way you integrate them into your applications.
    You’ll recall from [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra)
    the critical guiding principles for deploying cloud native data in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 1: Leverage compute, network, and storage as commodity APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principle 2: Separate the control and data planes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principle 3: Make observability easy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principle 4: Make the default configuration secure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principle 5: Prefer declarative configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As it turns out, these principles are useful for technology selection and integration,
    which you’ll see next.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting cloud native data projects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The years of building massive scale infrastructure, especially in data, have
    yielded an enormous supply of tooling to pick from, provided by various vendors
    and open source communities. For our examination here, we’ve made a deliberate
    choice to reason in terms of selecting projects instead of selecting technologies.
    Projects encapsulate the needed technology while integrating with the processes
    we need, created by the people who will drive the success. You’re here because
    you believe Kubernetes is one of these enabling projects, but how do you make
    your next set of choices? Here are some principles we recommend:'
  prefs: []
  type: TYPE_NORMAL
- en: Ready for Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.html#the_kubernetes_native_database), outlined requirements
    for a Kubernetes native database, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum leverage of Kubernetes APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated, declarative management via operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observable through standard APIs (such as Prometheus)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure by default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While not every project you use has to be Kubernetes native, the criterion for
    being Kubernetes-ready is a bit broader. At a minimum, projects you use should
    have an operator or Helm chart. The next level is a step toward the Kubernetes
    native idea of built-in awareness of Kubernetes for deeper integration. An example
    is Apache Spark, with the Kubernetes cluster deployment option that uses specialized
    containers. The highest level of maturity is populated by fully realized cloud
    native projects that can run only in Kubernetes because they depend on components
    in a Kubernetes cluster. An example of this type of project is KServe, which has
    no way of running outside of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Open source
  prefs: []
  type: TYPE_NORMAL
- en: Using an open source project in the age of cloud native is about choice. You
    can deploy what you need, where you need it. If you choose to use a managed service
    based on an open source project, it should be completely compatible with the open
    source version, with no restrictions in moving back to a self-managed solution.
    Choosing the right license gives you the confidence to use a project and maintain
    your choice. We recommend projects with the Apache License 2.0 (APLv2). All ASF
    and CNCF projects use this license, so projects from either source guarantee you
    a permissive license. [Many other licenses](https://oreil.ly/61pjy) offer differing
    levels of permissiveness and restrictions, and you should carefully consider how
    they will affect your deployment and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, project selections aren’t something you can do in isolation. Upstream
    decisions influence each subsequent decision, and in turn can constrain what choices
    are available. This is why, in many cases, it makes sense to look at combinations
    of projects that work well together, either by deliberate design or by standard
    interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: New architectures for cloud native data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The future of cloud native data should focus less on new projects and more on
    new architectures. This means using the projects we have today in combinations
    that make the best use of each. As we’ve discussed previously, the software industry
    has a history of leveraging ideas from prior generations lasting a decade or more
    to innovate from a new point of view. In the cloud native world, the past decade
    has been spent building scale infrastructure, and the next 10 years will likely
    be about how we can combine these projects for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: The infrastructure community has historically demonstrated a fondness for integrated
    infrastructure stacks that solve a common set of problems. One example is the
    LAMP stack popularized for web applications in the early 2000s, consisting of
    the Linux operating system, the Apache HTTP Server, MySQL, and either PHP, Perl,
    or Python, depending on who you asked. The 2010s brought us the SMACK stack for
    big data applications, with the Spark engine, Mesos as the resource manager, Akka,
    Cassandra, and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: While it’s tempting to describe such a stack for cloud native data, the reality
    is that the variety of use cases and available projects are simply too large to
    come up with a one-size-fits-all stack. Instead, let’s consider a candidate solution
    architecture for a simple weather application case, as shown in [Figure 11-1](#sample_architecture_for_a_weather_appli).
    This architecture demonstrates the principles and recommendations discussed throughout
    the book, leveraging our data infrastructure categories of persistence, streaming,
    and analytics. This is a conceptual vision that we can discuss, critique, and
    improve as a community. Each choice we’ve made here has alternatives and should
    be considered a starting point for the sake of discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample architecture for a weather application](assets/mcdk_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Sample architecture for a weather application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s walk through the flow of data to understand how this architecture satisfies
    the needs of a weather application with multiple data requirements. We’ll assume
    that the entire server-side infrastructure stack is contained in a single Kubernetes
    cluster. More advanced forms of this architecture could include multicluster deployments
    or inclusion of networking capabilities such as load balancing or Ingress. For
    now, this will serve to illustrate the data architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Weather data is collected from weather stations and posted to a waiting API
    with an Ingress port into your running Kubernetes cluster. The business logic
    and server-side application code are containerized and run as microservices in
    the `application` Namespace. Client-side web and mobile applications also use
    the microservices via API calls, so all external data communications pass through
    the microservices layer.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data is sent to Cassandra for immediate use in the `persistence` Namespace.
    Once the data is committed at the desired consistency level, change data capture
    (CDC) emits the fully committed data to a Pulsar topic in the `streaming` Namespace.
    A Pulsar sink exports the raw data into a Parquet file put in object storage.
    At the same time, a Flink consumer subscribed to the topic analyzes new data for
    user-defined limits such as high or low temperatures. If a boundary condition
    is triggered, the temperature and station data is sent back to the microservices,
    which will send push alerts to the user application.
  prefs: []
  type: TYPE_NORMAL
- en: In the `analytics` Namespace, two separate processes will use the Parquet data
    in object storage. Spark Jobs are used to group temperature averages across geographic
    data. This application code needs a wide view of the data stored for multiple
    locations and times. Ray applies analysis code written in Python to accomplish
    the predictive analysis of weather forecasting. The following five-day forecast
    is built daily by looking at recent data and applying against models built over
    historical trends. Both the Spark and Ray jobs populate new tables of fast transactional
    data in Cassandra.
  prefs: []
  type: TYPE_NORMAL
- en: 'This candidate architecture also demonstrates some recommendations that aren’t
    specific to a weather application that you should consider for all your deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Namespaces to separate domains within applications
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying hundreds of Pods into a Kubernetes cluster can create organizational
    issues you won’t encounter with a small cluster on your laptop. Our recommendation
    here is simple: use Namespaces liberally to create order in your complex deployments.
    In the weather application example, we used simple Namespaces for each functional
    area of infrastructure: `application`, `persistence`, `streaming`, `analytics`,
    `security`, and `observability`. This approach will provide clear boundaries and
    naming when addressing services or managing Pods.'
  prefs: []
  type: TYPE_NORMAL
- en: Automate certificate management
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.html#streaming_data_on_kubernetes), we asserted that the
    best security solutions are the ones you don’t have to think about. Automating
    your certificate management with cert-manager is an excellent example of a solution
    that makes that a possibility. Use TLS for all inter-service communication. For
    Ingress routes, ensure all traffic is HTTPS. Both cases use ACME plug-ins to rotate
    and assign certificates and never suffer another outage due to an expired certificate.
    When a security audit comes around, you can check the box that says you enforce
    all policies and guidelines and that all network communication is adequately encrypted.
    Just do it.
  prefs: []
  type: TYPE_NORMAL
- en: Prefer object storage
  prefs: []
  type: TYPE_NORMAL
- en: When choosing storage for the stateful services in your Kubernetes cluster,
    you should prefer object storage where possible. As discussed in [Chapter 7](ch07.html#the_kubernetes_native_database),
    several reasons behind this recommendation will put you in a better place for
    deploying cloud native data. The primary one is the impact of immutability on
    separating storage from running processes. Block storage is generally tightly
    aligned with compute infrastructure and has a higher level of complexity. The
    tight coupling between compute and storage must be broken to build truly serverless
    data infrastructure. Object storage has proven to be a key enabler. You can choose
    to implement your own object storage inside Kubernetes or via a cloud service.
  prefs: []
  type: TYPE_NORMAL
- en: Standardize on Prometheus APIs for metrics
  prefs: []
  type: TYPE_NORMAL
- en: Observability is mandatory for the complex infrastructure being built and run
    in Kubernetes, and the Prometheus API is the most widely adopted for metrics.
    Ensure that all services expose metrics in Prometheus format and that you collect
    them in a single place. The Prometheus API is implemented on various backends
    such as VictoriaMetrics and InfluxDB, giving you options for managing your own
    Prometheus deployment or connecting to a cloud service. Finally, collecting metrics
    is only one part of the challenge, and using those metrics to build dashboards
    and alerting completes the package.
  prefs: []
  type: TYPE_NORMAL
- en: 'As this architecture demonstrates, you can now deploy all of the infrastructure
    needed to support a complex application in a single deployment in Kubernetes.
    It’s a flexible architecture in which new components can be tried and rejected
    or replaced as your requirements change: are you using the database that best
    fits your application needs? Should data be analyzed in the stream or after it
    is at rest? Architecture represents a series of choices based on capabilities,
    limits, knowledge, and philosophy.'
  prefs: []
  type: TYPE_NORMAL
- en: We look forward to future conversations and conference talks sharing the patterns
    that work and the antipatterns to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy services, not servers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One pattern we recommend is to start delivering capabilities at a higher level
    of abstraction: as services instead of servers. To help frame this discussion,
    think about the architectural design of a building. An architect must understand
    a structure’s requirements and then apply knowledge of materials and style to
    create a plan for builders to implement. When an architect considers where to
    place a door, it must be in a useful location, but one that will not weaken the
    overall structure. At no point do they specify minute details such as whether
    the door has to have brass hinges.'
  prefs: []
  type: TYPE_NORMAL
- en: In the software industry, we’ve historically required a lot of minute details
    about individual compute, network, and storage resources well before we get to
    the deployment stage. For example, in the days of bare-metal infrastructure, the
    idea of installing a server was a significant event. Each server represented a
    physical device with a network connection that needed a whole bill of software,
    including operating system and applications to fill its role in the system. Procuring,
    configuring, and deploying a web server or database server was a process that
    could take months.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the migration to cloud computing came the aphorism that we should
    treat servers as “cattle, not pets.” Despite this helpful emphasis, the care and
    feeding of individual servers persist in plenty of cases. Where a network server
    accepts requests and then responds with data, it still requires people installing
    these systems to get much further into the details needed for today’s cloud native
    applications. These details create friction.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has encouraged a lot of progress in this area, emphasizing managing
    fleets of both stateless and stateful services with Deployments and StatefulSets,
    instead of focusing on individual Pods. It’s time to take this kind of thinking
    to the next level, and Kubernetes gives us the tools to make it happen.
  prefs: []
  type: TYPE_NORMAL
- en: Consider how the architecture in the previous section can be described as a
    vertically integrated service—a weather service—consisting of an assembly of microservices
    and data infrastructure built from Kubernetes primitives for compute, network,
    and storage. Recalling the “virtual datacenter” concept from [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra),
    [Figure 11-2](#vertically_integrated_service) depicts the contents of a vertically
    integrated service that exposes a simple API. While such a service could encompass
    a wide range of business logic and infrastructure, that complexity is hidden behind
    a simple API.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vertically integrated Service](assets/mcdk_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Vertically integrated Service
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Returning to the example of the preceding weather Service, let’s examine how
    this represents a lot of power behind a deceptively simple API. When you zoom
    out, the collection of deployed infrastructure looks like a function machine.
    Rather than a simple microservice that merely gets and puts data records, this
    function machine takes multiple inputs and produces multiple outputs, as shown
    in [Figure 11-3](#weather_service_as_a_function_machine).
  prefs: []
  type: TYPE_NORMAL
- en: '![Weather Service as a function machine](assets/mcdk_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. Weather Service as a function machine
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a function machine, the weather Service takes a stream of temperature measurements
    and produces multiple outputs. Beyond the ability to retrieve the individual records
    originally inserted, it produces value-added information like statistics, alerts,
    and forecasts that help users make sense of the data and how it relates to them
    personally.
  prefs: []
  type: TYPE_NORMAL
- en: A single traditional server won’t service the variety required, which is why
    modern data infrastructure and architectures exist. It takes architectural work
    to assemble the right parts, connect them, and create new data from the single
    input value.
  prefs: []
  type: TYPE_NORMAL
- en: Users and other applications expect service endpoints that respond to the data
    they need. What happens inside the function machine is left to the implementation
    meeting the API contract. When thinking shifts to outcomes, it’s clear how deploying
    services replaces the focus on deploying individual servers. Data services that
    can operate at various scales, built with resilience, using automation to keep
    us from worrying about minute details of the tools deployed. Using Kubernetes,
    you can specify what the function machine will do by using compute, network, and
    storage the same way you use any other consumable resource.
  prefs: []
  type: TYPE_NORMAL
- en: Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have discussed the people and technology aspects of moving stateful
    workloads to Kubernetes, let’s look at the practical process steps required to
    successfully execute this transition. To be clear, *process* doesn’t mean more
    meetings or people involved in decision making. The dictionary defines a process
    as “a series of actions or steps to achieve a particular end.” For a cloud native
    deployment process, let’s append the word “automated,” and that’s the right spirit.
    The goal is to define and codify an automated process that enables you to deploy
    constantly with confidence. You’ll know you’ve succeeded when you have not only
    a complete cloud native application stack managed in Kubernetes, but also a repeatable
    set of steps to reproduce that stack.
  prefs: []
  type: TYPE_NORMAL
- en: Where are you in your cloud native journey? You can be at the starting line
    or somewhere further along. For either starting point, we recommend the stages
    shown in [Figure 11-4](#stages_of_moving_data_workloads_to_kube) for adopting
    cloud native data in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Stages of moving data workloads to Kubernetes](assets/mcdk_1104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. Stages of moving data workloads to Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each stage contains core competencies developed by organizations that successfully
    made this transition. You’ll want to adopt and stabilize these competencies before
    moving on. Take your time and use the many resources available to become proficient
    in each stage. We’ll explore each stage in greater detail next.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before you even begin the adoption of Kubernetes, you should completely embrace
    two areas of managing cloud native infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration/continuous delivery (CI/CD)
  prefs: []
  type: TYPE_NORMAL
- en: DevOps teams have already widely used CI/CD for years. Correctly implemented,
    the outcome is a system that gives you the agility to make changes multiple times
    in a day with high confidence. For cloud native infrastructure, this has also
    been described as [GitOps](https://oreil.ly/p20Gt). Using source control as the
    starting point for infrastructure changes that a system like [Argo CD](https://oreil.ly/fdj2s)
    will use to automate deployments. Made a mistake? Roll it back.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs: []
  type: TYPE_NORMAL
- en: You may have heard the phrase “Trust but verify,” and nowhere is that more important
    than a highly complex cloud native deployment. You need to see what’s happening
    and make adjustments, especially when using CI/CD. In the process of building
    services that perform to meet SLAs, every step must be observed. This builds confidence
    that the changes you make are working; if not, you can roll back and try again.
    Every step is being watched and recorded.
  prefs: []
  type: TYPE_NORMAL
- en: While specific implementation details of your CI/CD and observability practices
    will inevitably change as you begin to adopt Kubernetes, having a firm foundation
    in these areas will set you up nicely for success.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Kubernetes maturity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are just starting with Kubernetes, this is a vital stage. Setting up
    a basic Kubernetes deployment on your laptop or cloud is an excellent way to learn,
    but your first production Kubernetes projects will stress the capability of your
    operations. It’s realistic to take several months in this phase to fully understand
    all of the potential issues and solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and managing clusters
  prefs: []
  type: TYPE_NORMAL
- en: This is the most fundamentally important experience you can have. While there
    is great learning in building your own Kubernetes clusters and installing your
    own databases, as we noted in [Chapter 3](ch03.html#databases_on_kubernetes_the_hard_way),
    you can make progress toward production capability more quickly by using a managed
    Kubernetes service or tools like Terraform that can help automate Kubernetes cluster
    deployments. You’ll get the most value from learning how to deploy and connect
    services both inside and outside the cluster, tasks that many new users find surprisingly
    tricky. You’ll also want to understand the metrics collected for various elements
    of a cluster and what they can tell you about performance and capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Moving stateless workloads
  prefs: []
  type: TYPE_NORMAL
- en: Once you are proficient at working with Kubernetes and understand some of the
    complexities, you can begin moving stateless workloads. The resource requirements
    for these workloads tend to be more straightforward to understand, and the body
    of prior art of deploying stateless workloads is deep. You’ll likely need to manage
    external networking to stateful workloads and data infrastructure that you aren’t
    yet moving during this stage. After a few successful migrations, you should feel
    comfortable with managing production workloads in Kubernetes and begin to see
    improvements in your operational tempo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few competencies we recommend building as you start to move stateless
    workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage continuous delivery
  prefs: []
  type: TYPE_NORMAL
- en: Using `kubectl` on the command line is great for learning, but terrible for
    daily operations. Get used to managing groups of resources as services instead
    of individual Pods and let the Kubernetes Operators do the work of maintaining
    your systems.
  prefs: []
  type: TYPE_NORMAL
- en: Network routing and Ingress
  prefs: []
  type: TYPE_NORMAL
- en: Bad things happen when you fight the way Kubernetes works, and one place that
    people new to Kubernetes fail is with network communications. You should prefer
    service names over IP addresses and understand how the LoadBalancer and Ingress
    APIs work.
  prefs: []
  type: TYPE_NORMAL
- en: Default security and observability
  prefs: []
  type: TYPE_NORMAL
- en: Deployed services should default to a secure state and expose observability
    interfaces such as metrics endpoints with no manual configuration required. Ensure
    that every new service is deployed with network-level encryption. To manage systems
    effectively, SREs must have the metrics available to diagnose problems without
    gaps in coverage.
  prefs: []
  type: TYPE_NORMAL
- en: These competencies will serve you well as you move into the following stages.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy stateful workloads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next stage is to migrate stateful workloads to Kubernetes, including their
    supporting data infrastructure. In this case, we recommend a phased approach in
    roughly the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Persistence
  prefs: []
  type: TYPE_NORMAL
- en: We recommend migrating databases as your first stateful workloads. Databases
    have been running in Kubernetes for far longer than other stateful workloads,
    with a higher level of maturity and documentation. Chapters [4](ch04.html#automating_database_deployment_on_kuber)
    and [5](ch05.html#automating_database_management_on_kuber) provide guidance on
    deploying with Helm and operators, respectively. Start with your development environment
    and parallel the same production traffic loads outside Kubernetes. Get proficient
    at backups and restore operations. Make sure your test cases include the loss
    of database compute and storage resources and move into staging and production
    when you feel your recovery response is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kubernetes readiness for streaming workloads is becoming much more mature,
    but we still recommend you migrate these workloads after persistence workloads.
    As we discussed in [Chapter 8](ch08.html#streaming_data_on_kubernetes), streaming
    workloads have some unique properties that can make them easier for migrations:
    most use cases don’t need long-term message storage, so switching from one streaming
    service to the other typically doesn’t require data migration. Since streaming
    is network intensive, proficiency with Kubernetes networking is a must.'
  prefs: []
  type: TYPE_NORMAL
- en: Analytics
  prefs: []
  type: TYPE_NORMAL
- en: The complex nature of analytic workloads makes them the next logical choice
    for migration into Kubernetes after persistence and streaming are in place. A
    good starting approach is to deploy analytic workloads into a dedicated Kubernetes
    cluster so that you can learn the Kubernetes deployment modes and special considerations
    for job management and data access. Ultimately you should consider using a different
    scheduler to support batch workloads such as YuniKorn or Volcano, as we discussed
    in [Chapter 9](ch09.html#data_analytics_on_kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: AI/ML workloads
  prefs: []
  type: TYPE_NORMAL
- en: You may consider your AI/ML workloads for extra migration bonus points. As we
    discussed in [Chapter 10](ch10.html#machine_learning_and_other_emerging_use),
    this is one of the least mature areas organizations have concerning data infrastructure.
    Projects like KServe and Feast are well suited for Kubernetes, so this isn’t the
    concern. The real question is whether your organization is proficient in MLOps
    and data engineering. You may be ready, but as a general recommendation for most
    organizations, this is an area you should address after other analytic workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The details of your specific adoption plan will vary according to your Kubernetes
    readiness, the maturity of each workload, and the underlying data infrastructure
    on which it is built. The Kubernetes native definitions in [Chapter 7](ch07.html#the_kubernetes_native_database)
    provide a valuable way of assessing the readiness of your infrastructure and where
    you may encounter additional work to properly deploy and manage it on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Continually optimize your deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the early days of the internet explosion, known as the “dot-com” years,
    startups were vying for venture capital and presenting plans. Almost every pitch
    deck would include a slide showing the planned datacenter build-out. It was there
    for a good reason: datacenters were a significant capital cost, and when asking
    for money, that had to be in the budget.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Things are different today. Startups now rent what they need from a cloud provider,
    and larger enterprises that still manage datacenters are reducing their footprints
    quickly. In this cloud native world, we have a lot more flexibility over the infrastructure
    we use, which gives greater opportunity for managing elements like cost, quality,
    and the trade-offs between them:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing cost
  prefs: []
  type: TYPE_NORMAL
- en: In any business, you have things that add to the ledger and subtract. People
    in finance call that the *cost of goods sold* (*COGS*). If you are building cars,
    COGS may account for costs like steel, the factory, and labor. Selling cars covers
    COGS and brings profit to the company. Controlling costs and making them predictable
    is a way to create a sustainable business.
  prefs: []
  type: TYPE_NORMAL
- en: 'In application software technology today, there are four main components of
    COGS: human labor, compute, network, and storage. These metrics have been tracked
    for a long time, and a lot of progress has been made to reduce costs and make
    things more predictable. DevOps has reduced the amount of human interaction needed,
    and cloud has normalized infrastructure costs. As mentioned in [Chapter 1](ch01.html#introduction_to_cloud_native_data_infra),
    Kubernetes wasn’t a revolution. It was an evolution and a place to converge to
    help solve the problem of COGS with application software technology, a solution
    that doesn’t compromise on quality and creates predictability.'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticity is one aspect of cloud native data that can lead to significant cost
    savings. If services are initially deployed with fixed capacity, optimize your
    deployments with the ability to not only scale up but also scale down when needed.
    When possible, use automation such as [HorizontalPodAutoscaler](https://oreil.ly/AoiRm)
    for hands-off scaling with the added benefit of scaling under load to maintain
    performance. Choosing projects that can support elastic workload management is
    the most crucial way to be sure you are getting the best performance for the cost.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing quality (availability and performance)
  prefs: []
  type: TYPE_NORMAL
- en: Reducing human toil reduces the number of people needed to run your Kubernetes
    deployments. Automated deployments and sane defaults go a long way to reducing
    labor, but self-healing infrastructure will reduce the number of people that need
    to be on hand for when things go bad. Optimize your self-healing deployments by
    testing the recovery of services by injecting failures into your cluster. Kill
    a Pod or a StatefulSet. What happens to the surrounding services? If that scenario
    makes you nervous, you need to optimize your Deployment until you are comfortable
    with failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducing costs should never be optimized by sacrificing quality. Continuously
    optimize for price and performance. As you constantly look to optimize your Kubernetes
    deployments, you should ask yourself these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Are you maintaining SLAs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the need for human interaction reduced?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you scale to zero with no traffic?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the current trends in operations, *AIOps* is a term that will soon enter
    your vocabulary, if it hasn’t already. AIOps doesn’t mean operations for AI/ML
    workloads; it refers to the use of AI/ML to manage infrastructure intelligently.
    With a strong baseline of observability, the metrics and other information you’re
    collecting can be analyzed and used to generate recommended adjustments to your
    infrastructure. Automated scaling up and scaling down of Deployments and StatefulSets
    is just the beginning. We hope to soon see advanced AIOps capabilities, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: A system that detects increased usage of a vertically integrated service in
    a given region and responds by deploying microservices and supporting infrastructure
    into that region, and proactively replicating data to optimize latency for client
    applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A multitenant system that detects when a particular tenant is demonstrating
    increased usage and migrates traffic for that tenant to dedicated infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a couple of examples of what we might be able to achieve. We
    already have the foundations in the controller-reconciler pattern implemented
    by the Kubernetes control plane. Today’s operators are heavily procedural, but
    what kind of decision flexibility could we build into future operators in order
    to achieve a desired quality of service? Stay tuned, because the cloud native
    world is constantly evolving.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of Cloud Native Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of a career in information technology, you’re likely to see
    several generational shifts. A subtle evolution occurs over five- or ten-year
    periods as changes slowly build on a previous generation of technology, until
    the day you realize that the way you work is fundamentally new. Perhaps you’ve
    spent part of your career installing operating systems on physical servers. In
    a more recent generation, we’ve started using scripts to provision cloud instances
    with operating system images ready for software to be installed. Kubernetes represents
    the latest generation, where engineers define everything they need in a text file,
    and the control plane converges the state while performing all of the tasks every
    previous generation of engineers had to do manually.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of progress will we continue to see from generation to generation?
    The following is a fictional story about a very possible near future. This story
    provides an example of where we could go as a community of data infrastructure
    engineers. The changes will be subtle but profound.
  prefs: []
  type: TYPE_NORMAL
- en: This story aims to help you look beyond the drudgery of configuration files
    and the shiny distraction of hot new projects and focus on how embracing cloud
    native data opens the door for a more fantastic tomorrow. When the toil of infrastructure
    is reduced or even removed, think of the new abilities we have and how this could
    translate into tangible daily outcomes. This isn’t science fiction, and you don’t
    have to wait for the next generational breakthrough. All of this is feasible today
    with the correct application of existing technology.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You made it! We have taken quite a journey together and covered a lot of ground,
    not only in this chapter but in the entire book. At the outset, we presented an
    ambitious goal of putting stateful workloads on Kubernetes. As we learned from
    Craig McLuckie, this is very much in line with the original goals of the Kubernetes
    project. Ultimately, we will reverse the trend of infrastructure-aware applications
    and have application-aware platforms and building applications with speed, efficiency,
    and confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, we’ve convinced you that this is achievable technically and extremely
    compelling from a cost and quality standpoint. In this chapter, we’ve focused
    on helping you chart the course to make this transition by focusing on the people,
    process, and technology changes you’ll need to make to be successful:'
  prefs: []
  type: TYPE_NORMAL
- en: Help people in your organization skill up on Kubernetes and data technologies,
    including those we’ve covered here. If you are in leadership, help place people
    in roles that give them direct responsibility and accountability for infrastructure
    choices. Empower them to interact and contribute in open source communities and
    be the catalyst for change in your organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select data infrastructure technologies that embody cloud native and Kubernetes
    native principles. Use Kubernetes custom resources and operators to raise the
    level of abstraction in your architecture to begin thinking about managing services
    that implement well-defined APIs instead of managing individual servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update your processes to automate “all the things”—from integration and delivery
    (CI/CD) to observability and management (AIOps). Leverage these mature processes
    as you strategically migrate stateful workloads to Kubernetes. Carefully balance
    the trade-offs between cost and quality to sustainably deliver the best experiences
    for your end users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the narrative of this journey shifts to you and where you choose to take
    us next. While this book has provided a broad overview of the world of data infrastructure
    on Kubernetes, each chapter could easily fill a book on its own. We encourage
    you to continue learning where your specific interests take you and share what
    you learn to continue to fill the gaps in our collective knowledge. As you successfully
    manage your cloud native data on Kubernetes, we hope to hear your story.
  prefs: []
  type: TYPE_NORMAL
