<html><head></head><body>
<div id="sbo-rt-content"><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. Introduction" data-type="chapter" epub:type="chapter"><div class="chapter" id="introduction">
<h1><span class="label">Chapter 1. </span>Introduction</h1>
<p>Kubernetes is an open source orchestrator for deploying containerized
applications.<a data-primary="Kubernetes, introduction to" data-type="indexterm" id="idm45664088727888"/> It was originally developed by Google, inspired by a
decade of experience deploying scalable, reliable systems in containers via
application-oriented APIs.<sup><a data-type="noteref" href="ch01.xhtml#idm45664088726992" id="idm45664088726992-marker">1</a></sup></p>
<p>Since its introduction in 2014, Kubernetes has grown to be one of the
largest and most popular open source projects in the world. It has
become the standard API for building cloud native applications, present in
nearly every public cloud. Kubernetes is a proven infrastructure for distributed
systems that is suitable for cloud native developers of all scales, from a
cluster of Raspberry Pi computers to a datacenter full of the latest machines.
It provides the software necessary to successfully build and deploy
reliable, scalable distributed systems.<a data-primary="Raspberry Pi" data-type="indexterm" id="idm45664088724144"/></p>
<p>You may be wondering what we mean when we say “reliable, scalable distributed
systems.” More and more services are delivered over the network via APIs.<a data-primary="distributed systems, reliable and scalable" data-type="indexterm" id="idm45664093223520"/> These
APIs are often delivered by a <em>distributed system</em>,
the various pieces that implement the API running on different machines,
connected via the network and coordinating their actions via network
communication. Because we increasingly rely on these APIs for all aspects of our
daily lives (e.g., finding directions to the nearest hospital), these systems
must be highly <em>reliable</em>. They cannot fail, even if a part of the system
crashes or otherwise stops working.<a data-primary="scalability" data-type="indexterm" id="idm45664093221744"/><a data-primary="reliability" data-type="indexterm" id="idm45664093221040"/><a data-primary="availability" data-type="indexterm" id="idm45664093220368"/> Likewise, they must maintain <em>availability</em> even
during software rollouts or other maintenance events. Finally, because more and
more of the world is coming online and using such services, they must be highly
<em>scalable</em> so that they can grow their capacity to keep up with ever-increasing
usage without radical redesign of the distributed system that implements the
services. In many cases this also means growing (and shrinking) the capacity automatically so that your application can be maximally efficient.</p>
<p>Depending on when and why you have come to hold this book in your hands, you may
have varying degrees of experience with containers, distributed systems, and
Kubernetes. You may be planning on building your application on top of public
cloud infrastructure, in private data centers, or in some hybrid environment.
Regardless of your experience, this book should enable you to
make the most of Kubernetes.<a data-primary="containers" data-secondary="and container APIs, benefits" data-secondary-sortas="container" data-type="indexterm" id="idm45664093218304"/></p>
<p>There are many reasons people come to use containers and container APIs like Kubernetes, but
we believe they can all be traced back to one of these
benefits:</p>
<ul>
<li>
<p>Development velocity</p>
</li>
<li>
<p>Scaling (of both software and teams)</p>
</li>
<li>
<p>Abstracting your infrastructure</p>
</li>
<li>
<p>Efficiency</p>
</li>
<li>
<p>Cloud native ecosystem</p>
</li>
</ul>
<p>In the following sections, we describe how Kubernetes can help provide
each of these features.</p>
<section data-pdf-bookmark="Velocity" data-type="sect1"><div class="sect1" id="idm45664093313536">
<h1>Velocity</h1>
<p>Velocity is the key component in nearly all
software development today.<a data-primary="development, velocity in" data-type="indexterm" id="idm45664093311536"/><a data-primary="velocity in software development" data-type="indexterm" id="idm45664093310688"/> The software industry has evolved from shipping
products as boxed CDs or DVDs to software that is delivered over the network via
web-based services that are updated hourly. This changing landscape means that
the difference between you and your competitors is often the speed with which
you can develop and deploy new components and features, or the speed with which
you can respond to innovations developed by others.</p>
<p>It is important to note, however, that velocity is not defined in terms of
simply raw speed. While your users are always looking for iterative improvements,
they are more interested in a highly reliable service. Once upon a time, it was
OK for a service to be down for maintenance at midnight every night. But today,
all users expect constant uptime, even if the software they are running is
changing constantly.</p>
<p>Consequently, velocity is measured not in terms of the raw number
of features you can ship per hour or day, but rather in terms
of the number of things you can ship while maintaining a highly
available service.<a data-primary="availability" data-secondary="benefits of containers and Kubernetes for" data-type="indexterm" id="idm45664093309072"/></p>
<p>In this way, containers and Kubernetes can provide the tools that
you need to move quickly, while staying available. The
core concepts that enable this are:</p>
<ul>
<li>
<p>Immutability</p>
</li>
<li>
<p>Declarative configuration</p>
</li>
<li>
<p>Online self-healing systems</p>
</li>
<li>
<p>Shared reusable libraries and tools</p>
</li>
</ul>
<p>These ideas all interrelate to radically improve the speed with which you can
reliably deploy new software.</p>
<section data-pdf-bookmark="The Value of Immutability" data-type="sect2"><div class="sect2" id="idm45664091517920">
<h2>The Value of Immutability</h2>
<p>Containers and Kubernetes encourage developers to build distributed systems
that adhere to the principles of immutable infrastructure.<a data-primary="immutability" data-secondary="value of" data-type="indexterm" id="idm45664091515904"/> With <em>immutable</em>
infrastructure, once an artifact is created in the system, it does not change via
user modifications.</p>
<p>Traditionally, computers and software systems have been treated as <em>mutable</em> infrastructure.<a data-primary="mutability" data-type="indexterm" id="idm45664091513360"/> With mutable infrastructure,
changes are applied as incremental updates to an existing system. These updates
can occur all at once, or spread out across a long period of time. A system
upgrade via the <code>apt-get update</code> tool is a good example of an update to a
mutable system. Running <code>apt</code> sequentially downloads any updated binaries,
copies them on top of older binaries, and makes incremental updates to
configuration files. With a mutable system, the current state of the
infrastructure is not represented as a single artifact, but rather as an
accumulation of incremental updates and changes over time. On many systems, these
incremental updates come not just from system upgrades, but operator
modifications as well. Furthermore, in any system run by a large team, it is
highly likely that these changes will have been performed by many different
people and, in many cases, will not have been recorded anywhere.</p>
<p>In contrast, in an immutable system, rather than a series of incremental updates
and changes, an entirely new, complete image is built, where the update simply
replaces the entire image with the newer image in a single operation. There are
no incremental changes. As you can imagine, this is a significant shift from the
more traditional world of configuration management.</p>
<p>To make this more concrete in the world of containers, consider two different
ways to upgrade your software:</p>
<ul>
<li>
<p>You can log in to a container, run a command to download your new software,
kill the old server, and start the new one.</p>
</li>
<li>
<p>You can build a new container image, push it to a container registry, kill the
existing container, and start a new one.</p>
</li>
</ul>
<p>At first blush, these two approaches might seem largely indistinguishable. So
what is it about the act of building a new
container that improves reliability?</p>
<p>The key differentiation is the artifact that you create, and the record of how
you created it. These records make it easy to understand exactly the differences
in some new version and, if something goes wrong, to determine what has changed and
how to fix it.</p>
<p>Additionally, building a new image rather than modifying an existing one means
the old image is still around, and can quickly be used for a rollback if an error occurs. In
contrast, once you copy your new binary over an existing binary, such a rollback
is nearly impossible.</p>
<p>Immutable container images are at the core of everything that you will build in
Kubernetes.<a data-primary="container images" data-secondary="immutable" data-type="indexterm" id="idm45664088611264"/> It is possible to imperatively change running containers, but this
is an antipattern to be used only in extreme cases where there are no other
options (e.g., if it is the only way to temporarily repair a mission-critical
production system). And even then, the changes must also be recorded through a
declarative configuration update at some later time, after the fire is out.</p>
</div></section>
<section data-pdf-bookmark="Declarative Configuration" data-type="sect2"><div class="sect2" id="idm45664088609904">
<h2>Declarative Configuration</h2>
<p>Immutability extends beyond containers running in
your cluster to the way you describe your application to Kubernetes.<a data-primary="declarative configuration objects" data-type="indexterm" id="idm45664088608112"/> Everything
in Kubernetes is a <em>declarative configuration object</em> that represents the
desired state of the system. It is the job of Kubernetes to ensure that the
actual state of the world matches this desired state.<a data-primary="declarative configuration" data-secondary="versus imperative configuration" data-secondary-sortas="imperative" data-type="indexterm" id="idm45664088606784"/><a data-primary="imperative versus declarative configuration" data-type="indexterm" id="idm45664088605488"/><a data-primary="configurations" data-secondary="declarative versus imperative" data-type="indexterm" id="idm45664088604784"/></p>
<p>Much like mutable versus immutable infrastructure, declarative configuration is
an alternative to <em>imperative</em> configuration,
where the state of the world is defined by the execution of a series of
instructions rather than a declaration of the desired state of the world. While
imperative commands define actions, declarative configurations define state.</p>
<p>To understand these two approaches, consider the task of producing three
replicas of a piece of software. With an imperative approach, the configuration
would say “run A, run B, and run C.” The corresponding declarative
configuration would be “replicas equals three.”</p>
<p>Because it describes the state of the world, declarative configuration does not
have to be executed to be understood. Its impact is concretely declared. Since
the effects of declarative configuration can be understood before they are
executed, declarative configuration is far less error-prone. Further, the
traditional tools of software development, such as source control, code review,
and unit testing, can be used in declarative configuration in ways that are
impossible for imperative instructions. The idea of storing declarative
configuration in source control is often referred to as “infrastructure as
code.”</p>
<p>Lately the idea of GitOps has begun to formalize the practice of
infrastructure as code with source control as the source of truth. When you
adopt GitOps, changes to production are made entirely via pushes to a Git
repository, which are then reflected into your cluster via automation. Indeed, your production Kubernetes cluster is viewed as effectively
a read-only environment. Additionally, GitOps is being increasingly integrated
into cloud-provided Kubernetes services as the easiest way to declaratively
manage your cloud native infrastructure.</p>
<p>The combination of declarative state stored in a version control system and
the ability of Kubernetes to make reality match this declarative state makes
rollback of a
change trivially easy. It is simply restating the previous declarative state of
the system. This is usually impossible with
imperative systems, because although the
imperative instructions describe how to get you from point A to point B,
they rarely include the reverse instructions that can get you back.</p>
</div></section>
<section data-pdf-bookmark="Self-Healing Systems" data-type="sect2"><div class="sect2" id="idm45664093456016">
<h2>Self-Healing Systems</h2>
<p>Kubernetes is an online, self-healing system. <a data-primary="self-healing systems" data-type="indexterm" id="idm45664093454448"/>When it receives a
desired state configuration, it does not simply take a set of actions to make the current
state match the desired state a single time.<a data-primary="state" data-secondary="maintaining desired state in self-healing" data-type="indexterm" id="idm45664093453616"/> It <em>continuously</em> takes actions to
ensure that the current state matches the desired state. This means that not
only will Kubernetes initialize your system, but it will guard it against any
failures or perturbations that might destabilize the system and affect
reliability.</p>
<p>A more traditional operator repair involves a manual series of mitigation steps,
or human intervention, performed in response to some sort of alert. Imperative
repair like this is more expensive (since it generally requires an on-call
operator to be available to enact the repair). It is also generally slower,
since a human must often wake up and log in to respond. Furthermore, it is less
reliable because the imperative series of repair operations suffers from all of the
problems of imperative management described in the previous section.
Self-healing systems like Kubernetes both reduce the burden on operators and
improve the overall reliability of the system by performing reliable repairs
more quickly.</p>
<p>As a concrete example of this self-healing behavior, if you assert a desired
state of three replicas to Kubernetes, it does not just create three
replicas—it continuously ensures that there are exactly three replicas. If you
manually create a fourth replica, Kubernetes will destroy one to bring the number
back to three. If you manually destroy a replica, Kubernetes will create one to
again return you to the desired state.</p>
<p>Online self-healing systems improve developer velocity because the time and
energy you might otherwise have spent on operations and maintenance can instead
be spent on developing and testing new features.</p>
<p>In a more advanced form of self-healing, there has been significant recent work
in the <em>operator</em> paradigm for Kubernetes.<a data-primary="operator paradigm for Kubernetes" data-type="indexterm" id="idm45664093450080"/> With operators, more advanced logic
needed to maintain, scale, and heal a specific piece of software (MySQL, for
example) is encoded into an operator application that runs as a container in
the cluster. The code in the operator is responsible for more targeted and
advanced health detection and healing than can be achieved via Kubernetes’s
generic self-healing. Often this is packaged up as “operators,” which are
discussed in <a data-type="xref" href="ch17.xhtml#extending_kubernetes">Chapter 17</a>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Scaling Your Service and Your Teams" data-type="sect1"><div class="sect1" id="idm45664093312912">
<h1>Scaling Your Service and Your Teams</h1>
<p>As your product grows, it’s inevitable that you will
need to scale both your software and the teams that develop it.<a data-primary="scalability" data-secondary="scaling your service and your teams" data-type="indexterm" id="ix_sclsertm"/> Fortunately,
Kubernetes can help with both of these goals. <a data-primary="decoupled architectures" data-type="indexterm" id="idm45664093061408"/>Kubernetes achieves scalability by
favoring <em>decoupled</em> architectures.</p>
<section data-pdf-bookmark="Decoupling" data-type="sect2"><div class="sect2" id="idm45664093060016">
<h2>Decoupling</h2>
<p>In a decoupled architecture, each component is separated from other components by
defined APIs and service load balancers.<a data-primary="APIs" data-secondary="for each component in decoupled architectures" data-secondary-sortas="each" data-type="indexterm" id="idm45664093058656"/><a data-primary="scalability" data-secondary="scaling your service and your teams" data-tertiary="decoupling of each component" data-type="indexterm" id="idm45664093057376"/><a data-primary="decoupled architectures" data-secondary="decoupling of components in" data-type="indexterm" id="idm45664093056128"/>
APIs and load balancers isolate each piece of the system from the others. APIs
provide a buffer between implementer and consumer, and load balancers provide a
buffer between running instances of each <span class="keep-together">service</span>.</p>
<p>Decoupling components via load balancers makes<a data-primary="load balancers" data-secondary="decoupling components via" data-type="indexterm" id="idm45664093053936"/> it easy to scale the programs that make up your service,
because increasing the size (and therefore the capacity) of the program can be
done without adjusting or reconfiguring any of the other layers of your service.</p>
<p>Decoupling servers via APIs makes
it easier to scale the development teams because each
team can focus on a single, smaller <em>microservice</em> with a comprehensible surface
area.<a data-primary="microservices" data-type="indexterm" id="idm45664093052016"/><a data-primary="servers, decoupling via APIs" data-type="indexterm" id="idm45664093051312"/> Crisp  APIs between microservices limit the amount of cross-team
communication overhead required to build and deploy software. This communication
overhead is often the major restricting factor when scaling teams.</p>
</div></section>
<section data-pdf-bookmark="Easy Scaling for Applications and Clusters" data-type="sect2"><div class="sect2" id="idm45664093050192">
<h2>Easy Scaling for Applications and Clusters</h2>
<p>Concretely, when you
need to scale your
service, the immutable, declarative nature of Kubernetes makes this scaling
trivial to implement.<a data-primary="scalability" data-secondary="scaling your service and your teams" data-tertiary="easy scaling for applications and clusters" data-type="indexterm" id="idm45664093048688"/><a data-primary="clusters" data-secondary="easy scaling for" data-type="indexterm" id="idm45664093046736"/> Because your containers are immutable, and the number of
replicas is merely a number in a declarative config, scaling your service upward
is simply a matter of changing a number in a configuration file, asserting this
new declarative state to Kubernetes, and letting it take care of the rest.
Alternatively, you can set up autoscaling and let Kubernetes do it for you.</p>
<p>Of course, that sort of scaling assumes that there are resources available in
your cluster to consume. Sometimes you actually need to scale up the cluster
itself. Again, Kubernetes makes this task easier. Because many machines in a
cluster are entirely identical to other machines in that set and the applications
themselves are decoupled from the details of the machine by containers, adding
additional resources to the cluster is simply a matter of imaging a new machine of the same class
and joining it into the cluster. This can be accomplished via a few simple
commands or via a prebaked machine image.</p>
<p>One of the challenges of scaling machine resources is predicting their use.<a data-primary="machine resources" data-secondary="predicting use for scaling purposes" data-type="indexterm" id="idm45664093044864"/><a data-primary="forecasting future compute costs" data-type="indexterm" id="idm45664093043872"/><a data-primary="costs" data-secondary="forecasting future compute costs" data-type="indexterm" id="idm45664093043184"/> If
you are running on physical infrastructure, the time to obtain a new machine is
measured in days or weeks. On both physical and cloud infrastructures, predicting
future costs is difficult because it is hard to predict the growth and scaling
needs of specific <span class="keep-together">applications</span>.</p>
<p>Kubernetes can simplify forecasting future compute costs. To understand why this
is true, consider scaling up three teams: A, B, and C. Historically you have
seen that each team’s growth is highly variable and thus hard to predict.<a data-primary="provisioning" data-type="indexterm" id="idm45664093040992"/> If you
are provisioning individual machines for each service, you have no choice but to
forecast based on the maximum expected growth for each service, since machines
dedicated to one team cannot be used for another team. If, instead, you use
Kubernetes to decouple the teams from the specific machines they are using, you
can forecast growth based on the aggregate growth of all three services.
Combining three variable growth rates into a single growth rate reduces
statistical noise and produces a more reliable forecast of expected growth.
Furthermore, decoupling the teams from specific machines means that teams can
share fractional parts of one another’s machines, reducing even further the
overheads associated with forecasting growth of computing resources.</p>
<p>Finally, Kubernetes makes it possible to achieve automatic scaling (both up and down) of resources.<a data-primary="automatic scaling of resources" data-type="indexterm" id="idm45664093039664"/> Especially in a cloud environment where new machines
can be created via APIs, combining Kubernetes with autoscaling for both
the applications and the clusters themselves means that you can always
rightsize your costs for the current load.</p>
</div></section>
<section data-pdf-bookmark="Scaling Development Teams with Microservices" data-type="sect2"><div class="sect2" id="idm45664093038688">
<h2>Scaling Development Teams with Microservices</h2>
<p>As noted in a variety of research, the ideal team size is the “two-pizza
team,” or roughly six to eight people.<a data-primary="microservices" data-secondary="scaling development teams with" data-type="indexterm" id="idm45664093037056"/><a data-primary="scalability" data-secondary="scaling your service and your teams" data-tertiary="scaling development teams with microservices" data-type="indexterm" id="idm45664093036112"/><a data-primary="development teams, scaling with microservices" data-type="indexterm" id="idm45664093034848"/> This group size often results in
good knowledge sharing, fast decision making, and a common sense of purpose.
Larger teams tend to suffer from issues of hierarchy, poor visibility, and infighting,
which hinder agility and <span class="keep-together">success</span>.</p>
<p>However, many projects require significantly more resources to be successful and
achieve their goals. Consequently, there is a tension between the ideal team
size for agility and the necessary team size for the product’s end goals.</p>
<p>The common solution to this tension has been the development of decoupled,
service-oriented teams that each build a single microservice. Each small team is
responsible for the design and delivery of a service that is consumed by other
small teams. The aggregation of all of these services ultimately provides the
implementation of the overall product’s surface area.<a data-primary="decoupled architectures" data-secondary="microservice, abstractions and APIs provided by Kubernetes for" data-type="indexterm" id="idm45664093343808"/></p>
<p>Kubernetes provides numerous
abstractions and APIs that make it easier to build these decoupled microservice
architectures:</p>
<ul>
<li>
<p><em>Pods</em>, or groups of containers, can group together container images developed
by different teams into a single deployable unit.<a data-primary="Pods" data-type="indexterm" id="idm45664093340864"/></p>
</li>
<li>
<p>Kubernetes <em>services</em> provide load balancing, naming, and discovery to isolate
one microservice from another.<a data-primary="services" data-type="indexterm" id="idm45664093338768"/></p>
</li>
<li>
<p><em>Namespaces</em> provide isolation and access control, so that each microservice can
control the degree to which other services interact with it.<a data-primary="namespaces" data-type="indexterm" id="idm45664093336800"/></p>
</li>
<li>
<p><em>Ingress</em> objects provide an easy-to-use frontend that can combine multiple
microservices into a single externalized API surface area.<a data-primary="Ingress objects" data-type="indexterm" id="idm45664093334832"/></p>
</li>
</ul>
<p>Finally, decoupling the application container image and machine means that
different microservices can colocate on the
same machine without interfering with one another, reducing the overhead and cost
of microservice architectures.<a data-primary="machine resources" data-secondary="decoupling application container image from machine" data-type="indexterm" id="idm45664093333488"/><a data-primary="containers" data-secondary="decoupling application container image from machines" data-type="indexterm" id="idm45664093332480"/><a data-primary="health checks" data-type="indexterm" id="idm45664093331504"/><a data-primary="rollouts" data-type="indexterm" id="idm45664093330832"/> The health-checking and rollout features of
Kubernetes guarantee a consistent approach to application rollout and
reliability, which ensures that a proliferation of microservice teams does not
also result in a proliferation of different approaches to service production
life cycle and operations.</p>
</div></section>
<section data-pdf-bookmark="Separation of Concerns for Consistency and Scaling" data-type="sect2"><div class="sect2" id="idm45664093306944">
<h2>Separation of Concerns for Consistency and Scaling</h2>
<p>In addition to the
consistency that Kubernetes brings to operations,
the decoupling and separation of concerns produced by the Kubernetes stack lead
to significantly greater consistency for the lower levels of your infrastructure.<a data-primary="decoupled architectures" data-secondary="separation of concerns" data-type="indexterm" id="idm45664093305600"/><a data-primary="scalability" data-secondary="scaling your service and your teams" data-tertiary="separation concerns for consistency and scaling" data-type="indexterm" id="idm45664093304624"/><a data-primary="operations teams" data-type="indexterm" id="idm45664093303344"/> This enables you to scale infrastructure operations to manage many machines with a single small,
focused team. We have talked at length about the decoupling of application
container and machine/operating system (OS), but an important aspect of this
decoupling is that the container orchestration API becomes a crisp contract that
separates the responsibilities of the application operator from the cluster
orchestration operator. We call this the “not my monkey, not my circus” line.
The application developer relies on the service-level agreement (SLA) delivered
by the container orchestration API, without worrying about the details of how
this SLA is achieved. Likewise, the container orchestration API reliability
engineer focuses on delivering the orchestration API’s SLA without worrying
about the applications that are running on top of it.</p>
<p>Decoupling concerns means that a small team running a Kubernetes cluster
can be responsible for supporting hundreds or even thousands of teams running
applications within that cluster (<a data-type="xref" href="#fig0101">Figure 1-1</a>). Likewise, a small team can be
responsible for dozens (or more) of clusters running around the world. It’s
important to note that the same decoupling of containers and OS enables the OS
reliability engineers to focus on the SLA of the individual machine’s OS. This
becomes another line of separate responsibility, with the Kubernetes operators
relying on the OS SLA, and the OS operators worrying solely about delivering
that SLA. Again, this enables you to scale a small team of OS experts to a fleet
of thousands of machines.</p>
<figure><div class="figure" id="fig0101">
<img alt="kur3 0101" height="743" src="assets/kur3_0101.png" width="581"/>
<h6><span class="label">Figure 1-1. </span>An illustration of how different operations teams are decoupled using APIs</h6>
</div></figure>
<p>Of course, devoting even a small team to managing an OS is beyond the scope of
many organizations. <a data-primary="managed Kubernetes-as-a-Service (KaaS)" data-type="indexterm" id="idm45664093298864"/><a data-primary="KaaS (Kubernetes-as-a-Service)" data-type="indexterm" id="idm45664093298144"/><a data-primary="Kubernetes-as-a-Service (KaaS)" data-type="indexterm" id="idm45664093297456"/>In these environments, a managed Kubernetes-as-a-Service (KaaS) provided by a public cloud provider is a great
option. As Kubernetes has become increasingly ubiquitous, KaaS has become
increasingly available as well, to the point where it is now offered on nearly
every public cloud. Of course, using KaaS has some limitations, since the
operator makes decisions for you about how the Kubernetes clusters are built and
configured. For example, many KaaS platforms disable alpha features because they
can destabilize the managed cluster.</p>
<p>In addition to a fully managed Kubernetes service, there is a thriving ecosystem
of companies and projects that help to install and manage Kubernetes. There is a
full spectrum of solutions between doing it “the hard way” and a fully managed
service.<a data-primary="APIs" data-secondary="decoupling of operations teams via" data-type="indexterm" id="idm45664093296256"/></p>
<p>Consequently, whether to use KaaS or manage it yourself (or
something in between) is a decision each user needs to make based on the skills and
demands of their situation. Often for small organizations, KaaS provides an
easy-to-use solution that enables them to focus their time and energy on
building the software to support their work rather than managing a cluster. For larger organizations that can afford a dedicated team for managing its
Kubernetes cluster, managing it that way may make sense since it enables
greater flexibility in terms of cluster
capabilities and <span class="keep-together">operations</span>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Abstracting Your Infrastructure" data-type="sect1"><div class="sect1" id="idm45664093294064">
<h1>Abstracting Your Infrastructure</h1>
<p>The goal of the public cloud is to provide<a data-primary="scalability" data-secondary="scaling your service and your teams" data-startref="ix_sclsertm" data-type="indexterm" id="idm45664093292496"/>
easy-to-use, self-service infrastructure for developers to
consume.<a data-primary="infrastructure" data-secondary="abstracting" data-type="indexterm" id="idm45664092895728"/><a data-primary="abstracting your infrastructure" data-type="indexterm" id="idm45664092894704"/> However, too often cloud APIs are oriented around mirroring the
infrastructure that IT expects (e.g., “virtual machines”), not the concepts (e.g., “applications”) that developers want to consume. Additionally, in
many cases the cloud comes with particular details in implementation or services
that are specific to the cloud provider. Consuming these APIs directly makes it
difficult to run your application in multiple environments, or spread between
cloud and physical environments.</p>
<p>The move to application-oriented container APIs like Kubernetes has two concrete
benefits. <a data-primary="APIs" data-secondary="benefits of move to application-oriented container APIs" data-type="indexterm" id="idm45664092893392"/><a data-primary="machine resources" data-secondary="separation of developers from specific machines" data-type="indexterm" id="idm45664092892448"/>First, as we described previously, it separates developers from
specific machines. This makes the machine-oriented IT role easier,
since machines can simply be added in aggregate to scale the cluster, and in the
context of the cloud it also enables a high degree of portability since
developers are consuming a higher-level API that is implemented in terms of the
specific cloud infrastructure APIs.<a data-primary="portability, benefits of move to application-oriented container APIs" data-type="indexterm" id="idm45664092891344"/></p>
<p>When your developers build their applications in terms of container images and
deploy them in terms of portable Kubernetes APIs, transferring your application
between environments, or even running in hybrid environments, is simply a matter
of sending the declarative config to a new cluster. Kubernetes has a number of
plug-ins that can abstract you from a particular cloud.<a data-primary="load balancers" data-secondary="creation on different types of infrastructure" data-type="indexterm" id="idm45664092890240"/> For example, Kubernetes
services know how to create load balancers on all major public clouds as well as
several different private and physical infrastructures. Likewise, Kubernetes
PersistentVolumes and PersistentVolumeClaims can be used to
abstract your applications away from specific storage implementations. Of
course, to achieve this portability, you need to avoid cloud-managed services
(e.g., Amazon’s DynamoDB, Azure’s Cosmos DB, or Google’s Cloud Spanner), which means that you will
be forced to deploy and manage open source storage solutions like Cassandra,
MySQL, or MongoDB.</p>
<p>Putting it all together, building on top of Kubernetes application-oriented
abstractions ensures that the effort you put into building, deploying, and
managing your application is truly portable across a wide variety of
environments.</p>
</div></section>
<section data-pdf-bookmark="Efficiency" data-type="sect1"><div class="sect1" id="idm45664092888592">
<h1>Efficiency</h1>
<p>In addition to the developer and IT management benefits that
containers and Kubernetes provide, there is also a concrete economic benefit to
the abstraction.<a data-primary="efficiency provided by Kubernetes" data-type="indexterm" id="idm45664092887024"/> Because developers no longer think in terms of machines, their
applications can be colocated on the same machines without impacting the
applications themselves. This means that tasks from multiple users can be packed
tightly onto fewer machines.</p>
<p>Efficiency can be measured by the ratio of the useful work performed by a
machine or process to the total amount of energy spent doing so. When it comes
to deploying and managing applications, many of the available tools and
processes (e.g., bash scripts, <code>apt</code> updates, or imperative configuration
management) are somewhat inefficient. When discussing efficiency, it’s often
helpful to think of both the monetary cost of running a server and the human cost
required to manage it.</p>
<p>Running a server incurs a cost based on power usage, cooling requirements, data-center space, and raw compute power. Once a server is racked and powered on (or
clicked and spun up), the meter literally starts running. Any idle CPU time is
money wasted. Thus, it becomes part of the system administrator’s
responsibilities to keep utilization at acceptable levels, which requires
ongoing management. This is where containers and the Kubernetes workflow come
in. Kubernetes provides tools that automate the distribution of applications
across a cluster of machines, ensuring higher levels of utilization than are
possible with traditional tooling.</p>
<p>A further increase in efficiency comes from the fact that a developer’s test
environment can be quickly and cheaply created as a set of containers running in a
personal view of a shared Kubernetes cluster (using a feature
called <em>namespaces</em>). <a data-primary="test environment, efficient creation with Kubernetes" data-type="indexterm" id="idm45664092883920"/><a data-primary="namespaces" data-type="indexterm" id="idm45664092883216"/>In the past, turning up a test cluster for a developer
might have meant turning up three machines. With Kubernetes, it is simple to have
all developers share a single test cluster, aggregating their usage onto a much
smaller set of machines. Reducing the overall number of machines used in turn
drives up the efficiency of each system: since more of the resources (CPU, RAM,
etc.) on each individual machine are used, the overall cost of each container
becomes much lower.<a data-primary="costs" data-secondary="reduction through Kubernetes’ efficiency" data-type="indexterm" id="idm45664092881920"/></p>
<p>Reducing the cost of development instances in your stack enables development
practices that might previously have been cost-prohibitive. For example, with
your application deployed via Kubernetes, it becomes conceivable to deploy and
test every single commit contributed by every developer throughout your entire
stack.</p>
<p>When the cost of each deployment is measured in terms of a small number of
containers, rather than multiple complete virtual machines (VMs), the cost you
incur for such testing is dramatically lower. Returning to the original value of
Kubernetes, this increased testing also increases velocity, since you have strong signals as to the reliability of your code as well as the granularity of
detail required to quickly identify where a problem may have been introduced.</p>
<p>Finally, as mentioned in previous sections, the use of automatic scaling to
add resources when needed, but remove them when they are not, can also be used
to drive the overall efficiency of your applications while maintaining their
required performance characteristics.</p>
</div></section>
<section data-pdf-bookmark="Cloud Native Ecosystem" data-type="sect1"><div class="sect1" id="idm45664089996016">
<h1>Cloud Native Ecosystem</h1>
<p>Kubernetes was designed from the ground up to be an extensible environment
and a broad and welcoming community.<a data-primary="cloud" data-secondary="cloud native ecosystem, navigating with Kubernetes" data-type="indexterm" id="idm45664089994560"/> These design goals and its ubiquity in
so many compute environments have led to a vibrant and large ecosystem of
tools and services that have grown up around Kubernetes. Following the lead
of Kubernetes (and Docker and Linux before it), most of these projects are also
open source. This means that a developer beginning to build does not have
to start from scratch. In the years since it was released, tools for nearly
every task, from machine learning to continuous development and serverless
programming models have been built for Kubernetes. Indeed, in many cases the
challenge isn’t finding a potential solution, but rather deciding which of
the many solutions is best suited to the task. The wealth of tools in the
cloud native ecosystem has itself become a strong reason for many people to
adopt Kubernetes. When you leverage the cloud native ecosystem, you can use
community-built and supported projects for nearly every part of your system,
allowing you to focus on the development of the core business logic and
services that are uniquely yours.</p>
<p>As with any open source ecosystem, the primary challenge is the variety of
possible solutions and the fact that there is often a lack of end-to-end
integration. One possible way to navigate this complexity is the technical guidance of the  Cloud Native Computing Foundation (CNCF).<a data-primary="Cloud Native Computing Foundation (CNCF)" data-type="indexterm" id="idm45664089991968"/> The CNCF acts as
a industry-neutral home for cloud native projects’ code and intellectual
property. It has three levels of project maturity to
help guide your adoption of cloud native projects. The majority of projects
in the CNCF are in the <em>sandbox</em> stage. Sandbox indicates that a project
is still in early development, and adoption is not recommended unless you are
an early adopter and/or interested in contributing to the development of
the project. The next stage in maturity is <em>incubating</em>. Incubating projects
are ones that have proven their utility and stability via adoption and
production usage; however, they are still developing and growing their
communities. While there are hundreds of sandbox projects, there are barely
more than 20 incubating projects. The final stage of CNCF projects
is <em>graduated</em>. These projects are fully mature and widely adopted. There
are only a few graduated projects, including Kubernetes itself.</p>
<p>Another way to navigate the cloud native ecosystem is via integration with
Kubernetes-as-a-Service. At this point, most of the <a data-primary="KaaS (Kubernetes-as-a-Service)" data-type="indexterm" id="idm45664089988672"/><a data-primary="Kubernetes-as-a-Service (KaaS)" data-type="indexterm" id="idm45664089987952"/>KaaS
offerings also have additional services via open source projects from
the cloud native ecosystem. Because these services are integrated into
cloud-supported products, you can be assured that the projects are mature
and production ready.</p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45664089986624">
<h1>Summary</h1>
<p>Kubernetes was built to radically change the way that applications are built and
deployed in the cloud. Fundamentally, it was designed to give developers more
velocity, efficiency, and agility. At this point, many of the internet services and applications that you use every day are running on top of Kubernetes. You are probably already a Kubernetes user, you just didn’t know it! We hope this chapter has given you
an idea of why you should deploy your applications using Kubernetes. Now that
you are convinced of that, the following chapters will teach you <em>how</em> to deploy
your applications.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45664088726992"><sup><a href="ch01.xhtml#idm45664088726992-marker">1</a></sup> Brendan Burns et al., “Borg, Omega, and Kubernetes: Lessons Learned from Three Container-Management Systems over a Decade,” <em>ACM Queue</em> 14 (2016): 70–93, available at <a href="https://oreil.ly/ltE1B"><em>https://oreil.ly/ltE1B</em></a>.</p></div></div></section></div></body></html>