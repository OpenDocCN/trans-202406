- en: Chapter 5\. Observability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 观察性
- en: In this chapter we will discuss the difference between monitoring and observability
    in the context of Kubernetes deployments. We will explain best practices and tools
    for implementing observability in your Kubernetes cluster. In the next chapter
    we will cover how you can use observability to secure your cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在 Kubernetes 部署背景下监控和可观察性之间的区别。我们将解释在您的 Kubernetes 集群中实施可观察性的最佳实践和工具。在下一章中，我们将介绍如何利用可观察性来保护您的集群。
- en: Observability has been a topic of discussion recently in the Kubernetes community
    and has garnered a lot of interest. We begin by understanding the difference between
    monitoring and observability. We then look at why observability is critical to
    security in a distributed application like Kubernetes, and review tools and reference
    implementations for observability. While observability is a broad topic and applies
    to several areas, we will keep the discussion focused on Kubernetes in this chapter.
    Let’s start by looking at monitoring and observability and how they are different.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性最近在 Kubernetes 社区中成为讨论的话题，并引起了很多关注。我们首先要理解监控和可观察性之间的区别。然后，我们将看看为何在像 Kubernetes
    这样的分布式应用中，可观察性对安全至关重要，并审查实现可观察性的工具和参考实现。虽然可观察性是一个广泛的话题，并适用于多个领域，但在本章中，我们将讨论重点放在
    Kubernetes 上。让我们从监控和可观察性开始，并了解它们的区别。
- en: Monitoring
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控
- en: 'Monitoring is a known set of measurements in a system that are used to alert
    for deviations from a normal range. The following are examples of types of data
    you can monitor in Kubernetes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 监控是系统中用于警示从正常范围偏离的已知一组测量。以下是在 Kubernetes 中您可以监视的数据类型示例：
- en: Pod logs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 日志
- en: Network flow logs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络流日志
- en: Application flow logs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序流日志
- en: Audit logs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审计日志
- en: 'Examples of metrics you can monitor include the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以监控的指标示例包括以下内容：
- en: Connections per second
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒连接数
- en: Packets per second, bytes per second
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒数据包数，每秒字节数
- en: Application (API) requests per second
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序（API）每秒请求数
- en: CPU and memory utilization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU 和内存利用率
- en: These logs and metrics can help you identify known failures and provide more
    information about the symptom to help you remediate the issue.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些日志和指标可以帮助您识别已知故障，并提供有关症状的更多信息，以帮助您解决问题。
- en: 'In order to monitor your Kubernetes cluster, you use techniques like polling
    and uptime checks depending on the SLAs you need to maintain for their cluster.
    The following are examples of metrics you could monitor for SLAs:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监视您的 Kubernetes 集群，您使用诸如轮询和正常运行时间检查的技术，具体取决于您需要为其集群维护的 SLA。以下是您可以监视 SLA 的示例指标：
- en: Polling of application/API endpoints
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮询应用程序/API 端点
- en: Application response codes (e.g., HTTP or database error codes)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用响应代码（例如 HTTP 或数据库错误代码）
- en: Application response time (e.g., HTTP duration, database transaction time)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序响应时间（例如 HTTP 持续时间，数据库事务时间）
- en: Node availability for scale-out use cases
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于扩展用例的节点可用性
- en: Memory/CPU/disk/IO resources on a node
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点上的内存/CPU/磁盘/IO 资源
- en: The other important part of monitoring is alerting. You need an alerting system
    as part of your monitoring solution that generates alerts for any metric that
    violates the specified threshold. Tools like Grafana, Prometheus, OpenMetrics,
    OpenTelemetry, and Fluentd are used as monitoring tools to collect logs and metrics,
    and generate reports, dashboards, and alerts for Kubernetes clusters. Kubernetes
    offers several integrations to tools like Opsgenie, PagerDuty, Slack, and JIRA
    for alert forwarding and management.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的另一个重要部分是警报。您需要一个警报系统作为监控解决方案的一部分，为任何违反指定阈值的度量生成警报。像 Grafana、Prometheus、OpenMetrics、OpenTelemetry
    和 Fluentd 这样的工具被用作监控工具，用于收集日志和指标，并为 Kubernetes 集群生成报告、仪表板和警报。Kubernetes 提供了与类似
    Opsgenie、PagerDuty、Slack 和 JIRA 这样的工具集成的选项，用于警报转发和管理。
- en: 'Monitoring your production Kubernetes cluster has the following issues:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 监控您的生产 Kubernetes 集群存在以下问题：
- en: Amount of log data
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 日志数据量
- en: In a system like Kubernetes, a node has several pods that run on the host, and
    each pod comes with its own logs, its own network identity, and its own resources.
    This means you have logs from the application operation, network flow logs, Kubernetes
    activity (audit) logs, and application flow logs for each pod. In a non-Kubernetes
    environment, you typically had an application running on a node and so it would
    be just one set of logs as opposed to one set of logs per pod running on the node.
    This multiplies the amount of log data that needs to be collected/inspected. In
    addition to the per-pod logs, you also need to collect cluster logs from Kubernetes.
    Typically these are also known as audit logs that provide visibility into Kubernetes
    cluster activity. The number of logs in the system will make monitoring very resource-intensive
    and expensive to maintain. Your log collection cluster should not be more expensive
    to operate than the cluster running your applications!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似 Kubernetes 的系统中，一个节点上有多个运行在主机上的 pod，每个 pod 都有其自己的日志、网络标识和资源。这意味着您需要收集来自应用程序操作、网络流量日志、Kubernetes
    活动（审计）日志以及每个 pod 的应用程序流量日志。在非 Kubernetes 环境中，通常一个节点上运行一个应用程序，因此日志集合只是单一的一组日志，而不是每个运行在节点上的
    pod 的一组日志。这增加了需要收集/检查的日志数据量。除了每个 pod 的日志外，您还需要收集 Kubernetes 的集群日志。通常这些也称为审计日志，提供了对
    Kubernetes 集群活动的可见性。系统中的日志数量将使监视变得非常耗费资源且昂贵。您的日志收集集群不应该比运行应用程序的集群更昂贵！
- en: Monitoring distributed applications
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 监视分布式应用程序
- en: In a Kubernetes cluster, applications are distributed across the Kubernetes
    cluster network. An application that needs more than one pod (e.g., a deployment
    set or a service) will have logs for each pod that need to be examined in addition
    to the context of the set of pods (e.g., scale out, error handling, etc.). We
    have multiple pods that need to be considered as a group before we generate an
    alert for the application. Please note the goal is to monitor the application
    and generate alerts for the application, and generating alerts for pods that are
    a part of the application independently does not provide an accurate representation
    of the state of the application. There is also the case of the microservices application,
    where a single application is deployed as a set of services known as *microservices*,
    and each microservice is responsible for a part of the functionality of the application.
    In this case, you need to monitor each microservice as an entity (note a microservice
    is a set of one or more pods) and then understand which microservices impact any
    given application transaction. Only then can you report an alert for the application.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群中，应用程序分布在 Kubernetes 集群网络中。一个需要多个 pod 的应用程序（例如，一个部署集或一个服务）将会有每个
    pod 的日志需要检查，除了需要考虑 pod 集合的上下文（例如，扩展，错误处理等）。在生成应用程序警报之前，我们需要将多个 pod 视为一个组。请注意，目标是监视应用程序并为其生成警报，独立为应用程序的一部分生成警报并不会准确反映应用程序的状态。还有微服务应用程序的情况，其中单个应用程序部署为一组称为*微服务*的服务，每个微服务负责应用程序功能的一部分。在这种情况下，您需要将每个微服务作为一个实体进行监视（请注意，一个微服务是一个或多个
    pod 的集合），然后理解哪些微服务影响了任何给定的应用程序事务。只有在这之后才能为应用程序报告警报。
- en: Declarative nature of Kubernetes
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的声明性质
- en: As we have covered, Kubernetes is declarative and allows you to specify exactly
    how you want pods to be created and run in the cluster. Kubernetes allows you
    to specify resource limits for memory, CPU, storage, etc., and you can also create
    custom resources and specify limits for these resources. The scheduler will find
    a node that has the required resources and schedule a pod on the node. Kubernetes
    also monitors usage for pods and will terminate pods that consume more resources
    than those allocated to them. In addition, Kubernetes provides detailed metrics
    that can be used to monitor pods and cluster state. For example, you can use a
    tool like [Prometheus](https://oreil.ly/zzjVG) that can monitor pods and cluster
    state and use the metrics, and you can automatically scale pods or other cluster
    resources with a mechanism known as the [Horizontal Pod Autoscaler](https://oreil.ly/luM5u).
    What this means is that Kubernetes as a part of its operation is monitoring and
    making changes to the cluster to maintain operations as per the configured specification.
    In this scenario, an alert from monitoring a single metric can be a result of
    Kubernetes making changes to adapt to the load in the cluster, or it could be
    a real issue. You need to be able to distinguish between the two scenarios to
    be able to accurately monitor your application.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所涵盖的，Kubernetes 是声明式的，允许您精确指定如何在集群中创建和运行 pod。Kubernetes 允许您为内存、CPU、存储等指定资源限制，还可以创建自定义资源并为这些资源指定限制。调度器将查找具有所需资源的节点，并在节点上调度
    pod。Kubernetes 还监视 pod 的使用情况，并将消耗超过分配资源的 pod 终止。此外，Kubernetes 提供详细的指标，可用于监控 pod
    和集群状态。例如，您可以使用类似 [Prometheus](https://oreil.ly/zzjVG) 的工具监控 pod 和集群状态，并使用这些指标，还可以使用所谓的
    [水平 Pod 自动伸缩器](https://oreil.ly/luM5u) 自动扩展 pod 或其他集群资源。这意味着 Kubernetes 作为其操作的一部分正在监视并对集群进行更改，以维持根据配置的规范运行。在这种情况下，监控单个指标的警报可能是
    Kubernetes 进行更改以适应集群负载的结果，也可能是一个真正的问题。您需要能够区分这两种情况，以准确监控您的应用程序。
- en: Now that we understand monitoring and how it can be implemented and the challenges
    with using monitoring for a Kubernetes cluster, let’s look at observability and
    how it can help overcome these challenges.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了监控及其如何实施以及在使用监控时 Kubernetes 集群面临的挑战后，让我们来看看可观察性以及它如何帮助克服这些挑战。
- en: Observability
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可观察性
- en: Observability is defined as the ability to understand the internal state of
    a system by only looking at external outputs of the system. [*Observability Engineering*
    by Charity Majors et al. (O’Reilly)](https://oreil.ly/3hPEr) is an excellent resource
    to learn more about observability. The book’s second chapter discusses monitoring
    and observability and is very relevant to this discussion.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性是指仅通过观察系统的外部输出就能理解系统内部状态的能力。[*可观察性工程*，查理蒂·梅杰斯等人（O’Reilly）](https://oreil.ly/3hPEr)
    是了解可观察性的绝佳资源。该书的第二章讨论了监控和可观察性，对本次讨论非常相关。
- en: Observability builds on monitoring and enables you to gain insights about the
    internal state of your application. For example, in a Kubernetes cluster an unexpected
    pod restart event may have limited to no impact on services as other instances
    of the pod may be adequate to handle the load at the time of the restart. A monitoring
    system will generate an alert that an unexpected pod restart occurred, and an
    observability system will generate a medium-priority event with the context that
    an unexpected pod restart occurred but had no impact on the system if there is
    no other event like application errors at the time of the pod restart. Another
    example is when an event is generated at the application layer (e.g., duration
    for HTTP request is larger than the norm). In this scenario, the observability
    system will provide context for the reason of degradation in application response
    time (e.g., network layer issue, retransmits, application pod restarts due to
    resource or other application issues, a Kubernetes infrastructure issue like DNS
    latency or API server load). As explained previously, an observability system
    can look at multiple events that impact application state and report application
    status after considering all of them. Now let’s look at how you can implement
    observability in a Kubernetes system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性基于监控，并使您能够获取有关应用程序内部状态的见解。例如，在 Kubernetes 集群中，意外的 pod 重新启动事件可能对服务没有或只有很少的影响，因为在重新启动时，可能已经有足够的
    pod 实例来处理负载。监控系统将生成警报，说明发生了意外的 pod 重新启动事件；可观察性系统将生成一个中等优先级事件，并说明发生了意外的 pod 重新启动事件，但如果在
    pod 重新启动时没有应用程序错误等其他事件，则对系统没有影响。另一个例子是在应用程序层生成事件时（例如，HTTP 请求的持续时间大于正常值）。在这种情况下，可观察性系统将为应用程序响应时间下降的原因提供背景信息（例如，网络层问题、重传、由于资源或其他应用程序问题导致的应用程序
    pod 重新启动、Kubernetes 基础设施问题，如 DNS 延迟或 API 服务器负载）。正如前面所解释的，可观察性系统可以查看影响应用程序状态的多个事件，并在考虑所有这些事件后报告应用程序状态。现在让我们看看如何在
    Kubernetes 系统中实现可观察性。
- en: How Observability Works for Kubernetes
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可观察性在 Kubernetes 中的工作原理
- en: The declarative nature of Kubernetes helps a lot in implementing an observability
    system. We recommend that you build a system that is native to Kubernetes and
    is able to understand operations in a cluster. For example, a system that understands
    Kubernetes will monitor a pod (e.g., restarts, out of memory, network activity,
    etc.) but also understand if a pod is a standalone instance or part of a deployment,
    replica set, or service. It will also know how critical the pod is to the service
    or deployment (e.g., how the service is configured for scalability and high availability).
    So when it reports any event related to the pod, it will provide all this context
    and help you easily make a decision about how you need to respond to the event.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的声明性特性在实施可观察性系统中非常有帮助。我们建议您构建一个与 Kubernetes 本地化且能够理解集群操作的系统。例如，一个了解
    Kubernetes 的系统将监视一个 pod（例如，重新启动、内存不足、网络活动等），但还将了解一个 pod 是否是独立实例或部署、副本集或服务的一部分。它还将知道
    pod 对服务或部署的关键性有多大（例如，服务配置为可扩展性和高可用性的方式）。因此，当它报告与 pod 相关的任何事件时，它将提供所有这些上下文，并帮助您轻松地做出关于如何响应事件的决定。
- en: 'Another thing to remember is that in Kubernetes you can deploy applications
    as pods that are a part of higher-level constructs like a deployment or a service.
    In order to appreciate the complexity in implementing observability for these
    constructs, we will use an example to explain them. When you configure a service,
    Kubernetes manages all pods associated with the service and ensures that traffic
    is delivered to available pods that are a part of the service. Let’s take a look
    at an example of [service definition from the Kubernetes documentation](https://oreil.ly/ijVz5):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要记住的是，在 Kubernetes 中，您可以将应用程序部署为 pod，这些 pod 是高级结构的一部分，例如部署或服务。为了理解为这些结构实施可观察性所带来的复杂性，我们将使用一个示例来解释它们。当您配置一个服务时，Kubernetes
    管理与该服务关联的所有 pod，并确保将流量传递到服务的可用 pod。让我们来看一个来自 [Kubernetes 文档的服务定义示例](https://oreil.ly/ijVz5)：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example all pods that have the label MyApp and listen on TCP port 9376
    become part of the service, and all traffic destined to the service is redirected
    to these pods. We cover this concept in detail in [Chapter 8](ch08.xhtml#managing_trust_across_teams).
    So in this scenario, the observability solution should also work to provide insights
    at the service level. Monitoring a pod in this case is not sufficient. What is
    needed is that the observability aggregates metrics across all pods in a service
    and uses the aggregated information for more analytics and alerts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，所有标签为 MyApp 并监听 TCP 端口 9376 的 pod 都成为服务的一部分，所有发送到服务的流量都会被重定向到这些 pod 上。我们在[第8章](ch08.xhtml#managing_trust_across_teams)中详细讨论了这个概念。因此，在这种场景下，观测解决方案也应该能够提供服务级别的洞察力。仅仅监视一个
    pod 是不够的。所需的是观测能够在服务中汇总所有 pod 的指标，并使用聚合信息进行更多的分析和警报。
- en: 'Now let’s look at an example of [deployments](https://oreil.ly/23Eam) in Kubernetes.
    Deployments allow you to manage pods and replica sets (replicas of a pod, typically
    used for scaling and high availability). The following is an example configuration
    for a deployment in Kubernetes:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一个在 Kubernetes 中的[部署](https://oreil.ly/23Eam)的例子。部署允许你管理 pod 和副本集（pod
    的副本，通常用于扩展和高可用性）。以下是 Kubernetes 中部署的一个示例配置：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This configuration will create a deployment for nginx with three replica pods
    with the configured metadata and specification. Kubernetes has a deployment controller
    to ensure that all pods and replicas that are a part of the deployment are available.
    There are several other benefits, like rolling updates, autoscaling, etc., that
    can be achieved by using the deployment resource in Kubernetes. In such a scenario
    for observability, the tool you use should look at the activity of all pods (replicas)
    in a deployment as an aggregate (e.g., all traffic to/from pods in a deployment,
    pod restarts and their effect on a deployment, etc.). Monitoring and alerting
    for each pod will not be sufficient to understand how the deployment is operating.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置将创建一个 nginx 的部署，其中有三个副本 pod，具有配置的元数据和规范。Kubernetes 拥有一个部署控制器来确保部署中所有的 pod
    和副本都是可用的。使用部署资源在 Kubernetes 中还可以实现滚动更新、自动扩展等多种其他好处。在这样的情况下，观测工具应该查看部署中所有 pod（副本）的活动作为一个整体（例如，部署中所有
    pod 的流量、pod 的重新启动及其对部署的影响等）。仅监视和警报每个 pod 并不足以理解部署的运行情况。
- en: In both these examples it is clear that the collection of metrics needs to be
    in the context of Kubernetes. Instead of collecting all data and metrics at a
    pod-level granularity, the collection engine should collect data at a deployment-
    or service-level granularity when applicable to deliver an accurate representation
    of the state of the deployment or service. Remember, Kubernetes abstracts pod-level
    details, and so we need to focus on measuring and alerting at a higher level than
    pods. Aggregation of data at a deployment and service level will reduce the number
    of logs you need to collect all the time and address the concern of the costs
    associated with a large number of logs. Please note the tool needs to have the
    ability to drill down and capture pod-level details when the operator needs to
    analyze an issue. We will cover this later in this chapter when we discuss data
    collection.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个例子中都清楚地表明，指标的收集需要在 Kubernetes 的上下文中进行。我们不应该在 pod 级别粒度上收集所有数据和指标，而是在适用时应该在部署或服务级别粒度上收集数据，以提供部署或服务状态的准确表现。请记住，Kubernetes
    抽象了 pod 级别的细节，因此我们需要在更高的级别上进行测量和警报。在部署和服务级别进行数据聚合将减少你需要一直收集的日志数量，并解决与大量日志相关的成本问题。请注意，该工具需要在操作员需要分析问题时有能力深入了解并捕获
    pod 级别的细节。我们将在本章后面讨论数据收集时详细介绍这一点。
- en: Now that we understand how we can leverage the declarative nature of Kubernetes
    to help with observability and reduce the amount of log data we need to collect
    and generate relevant alerts, let’s explore the distributed nature of Kubernetes
    and its impact on observability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何利用 Kubernetes 的声明性特性来帮助观测，并减少我们需要收集和生成相关警报的日志数据量，让我们来探讨 Kubernetes
    的分布式特性及其对观测的影响。
- en: In a microservices-based application deployment, a single application comprises
    several microservices that are deployed in a Kubernetes cluster. This means that
    in order to service a single transaction from the user, one or more services need
    to interact with each other, resulting in one or more subtransactions. A great
    example of a sample microservices application is the [Google online boutique demo
    microservices application](https://oreil.ly/wx7bj). [Figure 5-1](#architecture_of_the_google_microservice)
    shows the architecture for this application.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于微服务的应用部署中，单个应用由多个部署在 Kubernetes 集群中的微服务组成。这意味着为了为用户服务一个单一事务，一个或多个服务需要相互交互，从而产生一个或多个子事务。一个典型的微服务应用示例是
    [Google 在线精品演示微服务应用](https://oreil.ly/wx7bj)。[图 5-1](#architecture_of_the_google_microservice)
    展示了该应用的架构。
- en: '[Figure 5-1](#architecture_of_the_google_microservice) shows how an online
    boutique application can be deployed as microservices in Kubernetes. There are
    11 microservices, each responsible for some aspect of the application. We encourage
    you to review this application as we will use it to demonstrate how you can implement
    observability later in the chapter. If you look at the checkout transaction, a
    user makes a request to the frontend service, which then makes a request to the
    checkout service. The checkout service needs to interact with several services
    (e.g., PaymentService, Shipping Service, CurrencyService, EmailService, ProductCatalog
    Service, CartService) to complete the transaction. So in this scenario if we see
    our HTTP application log indicate a larger-than-expected duration for the checkout
    process API response time, we will need to review each of the subtransactions
    and see if there is an issue with each one and what the issue is (an application
    issue, network issue, etc.). Another thing that makes this complicated is the
    fact that each subtransaction is asynchronous and each microservice is serving
    several independent transactions simultaneously. In such a scenario you need to
    use a technique known as *distributed tracing* to trace the flow of a single transaction
    across a set of microservices. Distributed tracing can happen by instrumenting
    the application or instrumenting the kernel. We will cover distributed tracing
    later in the chapter.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](#architecture_of_the_google_microservice) 展示了在线精品应用如何作为 Kubernetes
    中的微服务部署。这里有 11 个微服务，每个负责应用的某个方面。我们鼓励您查看这个应用，因为我们将在本章后面使用它来演示如何实现可观测性。如果您查看结帐交易，用户会向前端服务发出请求，然后前端服务会向结帐服务发出请求。结帐服务需要与多个服务进行交互（例如
    PaymentService、Shipping Service、CurrencyService、EmailService、ProductCatalog Service、CartService）来完成交易。因此，在这种情况下，如果我们的
    HTTP 应用日志显示结帐流程 API 响应时间大于预期，我们需要审查每个子事务，并查看是否存在问题以及问题是什么（应用问题、网络问题等）。另一个使情况复杂化的因素是每个子事务都是异步的，而每个微服务同时提供几个独立的事务。在这种情况下，您需要使用一种称为
    *分布式跟踪* 的技术来跟踪单个事务在一组微服务中的流动。分布式跟踪可以通过对应用程序或内核进行仪器化来实现。我们将在本章后面介绍分布式跟踪。'
- en: '![](Images/ksao_0501.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ksao_0501.png)'
- en: Figure 5-1\. Architecture of the Google microservices demo application
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 谷歌微服务演示应用架构
- en: Now that we understand observability and how you should think about it for a
    Kubernetes cluster, let’s look at the components for an observability tool for
    Kubernetes. [Figure 5-2](#components_of_an_observability_tool_for) shows a block
    diagram of the various components of an observability tool for Kuebrnetes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了可观测性以及在 Kubernetes 集群中如何考虑它之后，让我们来看看 Kubernetes 可观测性工具的组件。[图 5-2](#components_of_an_observability_tool_for)
    展示了 Kubernetes 可观测性工具的各个组件的块图。
- en: '![](Images/ksao_0502.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ksao_0502.png)'
- en: Figure 5-2\. Components of an observability tool for Kubernetes
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. Kubernetes 可观测性工具的组件
- en: '[Figure 5-2](#components_of_an_observability_tool_for) shows that you need
    the following components for your observability implementation:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-2](#components_of_an_observability_tool_for) 显示了实现可观测性所需的以下组件：'
- en: Telemetry collection
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 遥测数据收集
- en: As mentioned, your observability solution needs to collect telemetry data from
    various sensors in your cluster. It needs to be distributed and Kubernetes-native.
    It must support sensors across all layers, from L3 to L7\. It also needs to collect
    information about Kubernetes infrastructure (e.g., DNS and API server logs) and
    Kubernetes activity (these are known as audit logs). As described, this information
    must be collected in the context of deployments and services.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您的可观测性解决方案需要从集群中的各种传感器收集遥测数据。它需要是分布式的且本地化于 Kubernetes。它必须支持从 L3 到 L7 的所有层的传感器。它还需要收集有关
    Kubernetes 基础设施（例如 DNS 和 API 服务器日志）以及 Kubernetes 活动（这些称为审计日志）的信息。正如描述的那样，这些信息必须在部署和服务的上下文中收集。
- en: Analytics and visibility
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 分析和可见性
- en: In this layer, the system must provide visualizations that are specific to Kubernetes
    operations (e.g., service graph, Kubernetes platform view, application views).
    We will cover some common visualizations that are native to Kubernetes. We recommend
    you pick a solution that leverages machine learning techniques for baselining
    and reporting anomalies. Finally, the system needs to support the ability for
    operators to enable pod-to-pod packet capture (note that this is not the same
    as enabling packet capture on the host interface, as the pod-level visibility
    is lost). We will cover this in the next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一层面上，系统必须提供特定于 Kubernetes 操作的可视化（例如服务图、Kubernetes 平台视图、应用程序视图）。我们将涵盖一些本地化于
    Kubernetes 的常见可视化。我们建议您选择一个利用机器学习技术进行基线化和报告异常的解决方案。最后，系统需要支持运营人员启用 pod 到 pod 的数据包捕获能力（请注意，这与在主机接口上启用数据包捕获不同，因为会丢失
    pod 级别的可见性）。我们将在下一节中详细介绍这一点。
- en: Security and troubleshooting applications
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和故障排除应用程序
- en: The observability system you implement must support distributed tracing as described
    in the previous section to help troubleshoot applications. We also recommend the
    use of advanced machine learning techniques to understand Kubernetes cluster behavior
    and predict performance or security concerns. Please note that this is a new area
    and there is ongoing innovation in it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您实施的可观测性系统必须支持分布式跟踪，如前一节所述，以帮助排除应用程序问题。我们还建议使用先进的机器学习技术来理解 Kubernetes 集群的行为并预测性能或安全问题。请注意，这是一个新领域，正在不断创新。
- en: Now that we have covered what is needed to implement observability in a Kubernetes
    cluster, let’s review each of the components in detail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了在 Kubernetes 集群中实施可观测性所需的内容，让我们详细审查每个组件。
- en: Implementing Observability for Kubernetes
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施 Kubernetes 的可观测性
- en: In this section we will review each component needed to build an effective observability
    system in Kubernetes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将审查构建 Kubernetes 中有效观测系统所需的每个组件。
- en: 'You should think of log collection as a set of sensors that are distributed
    in your cluster. You need to ensure that the sensors are efficient and do not
    interfere with system operation (e.g., adding latency). We will cover methods
    of collection later in this section that will show how you can efficiently collect
    metrics. You should consider deploying sensors (or collecting information) across
    all the layers of the stack, as shown in [Figure 5-2](#components_of_an_observability_tool_for).
    Kubernetes audit logs are an excellent source of information to understand the
    complete life cycle of various Kubernetes resources. In addition to audit logs,
    Kubernetes provides a variety of options for [monitoring](https://oreil.ly/FSwUs).
    Next you need to focus on traffic flow logs (Layer 3/Layer 4) to understand the
    operation of the Kubernetes cluster network. Given the declarative nature of Kubernetes,
    it is important to collect logs related to application flows (e.g., HTTP or MySQL),
    where the logs provide visibility into the application behavior (e.g., response
    time, availability, etc.) as seen by the user. In order to help with troubleshooting,
    you should also collect logs related to Kubernetes cluster infrastructure (e.g.,
    API server, DNS). Some advanced troubleshooting systems also collect information
    from the Linux kernel that is a result of activity by a pod (e.g., process information,
    socket stats for a flow initiated by a pod) and provide a way to enable packet
    capture (raw packets) for pod-to-pod traffic. The following describes what you
    should collect for each:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该将日志收集视为分布在你的集群中的一组传感器。您需要确保这些传感器是高效的，并且不会干扰系统操作（例如增加延迟）。我们将在本节后面涵盖收集方法，展示如何有效地收集指标。您应该考虑在整个堆栈的所有层部署传感器（或收集信息），如
    [图 5-2](#components_of_an_observability_tool_for) 所示。Kubernetes 审计日志是了解各种 Kubernetes
    资源完整生命周期的重要信息源。除了审计日志外，Kubernetes 还提供了多种选项用于 [监控](https://oreil.ly/FSwUs)。接下来，您需要关注流量流日志（Layer
    3/Layer 4），以理解 Kubernetes 集群网络操作。考虑到 Kubernetes 的声明性特性，重要的是收集与应用程序流（例如 HTTP 或
    MySQL）相关的日志，这些日志可以展示用户看到的应用程序行为可见性（例如响应时间、可用性等）。为了帮助故障排除，您还应该收集与 Kubernetes 集群基础设施相关的日志（例如
    API 服务器、DNS）。一些高级故障排除系统还会收集由 pod 活动引起的 Linux 内核信息（例如，由 pod 发起的流的进程信息、套接字统计信息），并提供一种启用
    pod 之间流量的数据包捕获（原始数据包）的方式。以下描述了您应该每个流收集的内容：
- en: Kubernetes audit logs
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 审计日志
- en: Kubernetes provides the ability to collect and monitor activity. Here is an
    excellent [guide](https://oreil.ly/aKmCE) that explains how you can control what
    to collect and also mechanisms for logging and alerting. We suggest you review
    what you need to collect and set the audit policy carefully—we recommend against
    just collecting everything. For example, you should log API requests, usernames,
    RBAC, decisions, request verbs, the client (user-agent) that made the request,
    and response codes for API requests. We will show a sample Kubernetes activity
    dashboard in the visualization section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了收集和监控活动的能力。这是一个优秀的 [指南](https://oreil.ly/aKmCE)，解释了如何控制收集什么以及日志记录和警报机制。我们建议您仔细审查您需要收集和设置审计策略
    —— 我们建议不要仅仅收集所有内容。例如，您应该记录 API 请求、用户名、RBAC、决策、请求动词、发出请求的客户端（用户代理）以及 API 请求的响应代码。我们将在可视化部分展示一个样本
    Kubernetes 活动仪表板。
- en: Network flow logs
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 网络流量日志
- en: Network flow logs (Layer 3/Layer 4) are key to understanding the Kubernetes
    cluster network operation. Typically these include the five-tuple (source and
    destination IP addresses/ports and port). It is also important to collect Kubernetes
    metadata associated with pods (source and destination namespaces, pod names, labels
    associated with pods, host on which the pods were running) and aggregate bytes/packets
    for each flow. Note this can result in a large amount of flow data, as there could
    be a large number of pods on a node. We will address this in the following section
    about aggregation at collection time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 网络流量日志（Layer 3/Layer 4）是理解 Kubernetes 集群网络操作的关键。通常包括五元组（源和目标 IP 地址/端口和端口）。收集与
    pods 相关的 Kubernetes 元数据（源和目标命名空间、pod 名称、与 pods 相关的标签、运行 pods 的主机）以及每个流的聚合字节/数据包是非常重要的。请注意，这可能导致大量的流量数据，因为一个节点上可能有大量的
    pods。在下一节关于集合时的聚合部分，我们将讨论如何解决这个问题。
- en: DNS flow logs
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 流量日志
- en: Along with the API server, the DNS server is a critical part of the Kubernetes
    cluster and is used by applications to resolve domain names in order to connect
    other services/pods as a part of normal operation. An issue with the DNS server
    can impact several applications in the cluster. It is important to collect information
    from the client’s perspective. You should log DNS requests by pods that capture
    request count, latency, which DNS server was used to resolve the request, the
    DNS response code, and the response. This should be collected with Kubernetes
    metadata (e.g., namespace, pod name, labels, etc.), as this will help associate
    the DNS issue with a service and facilitate further troubleshooting.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 API 服务器，DNS 服务器是 Kubernetes 集群的关键部分，被应用程序用于解析域名以连接其他服务/pod，作为正常运行的一部分。DNS
    服务器的问题可能会影响集群中的多个应用程序。从客户端的角度收集信息非常重要。应记录由 pod 发出的 DNS 请求，包括请求计数、延迟、用于解析请求的 DNS
    服务器、DNS 响应代码和响应。这些信息应与 Kubernetes 元数据（例如，命名空间、pod 名称、标签等）一起收集，因为这将有助于将 DNS 问题与服务关联并进一步进行故障排除。
- en: Application logs
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序日志
- en: As explained, the collection of application logs (HTTP, MySQL) is very important
    in a declarative system like Kubernetes. These logs provide a view into the user
    experience (e.g., response time or availability). The logs will be application-specific
    information but must include response codes (status), response time, and other
    application-specific context. For example, for HTTP requests, you should log domains
    (part of the URL), user agent, number of requests, HTTP response codes, and in
    some cases complete URL paths. Again, logs should include Kubernetes metadata
    (e.g., namespace, service, labels, pod names, etc.).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所解释的那样，在声明式系统如 Kubernetes 中，收集应用程序日志（HTTP、MySQL）非常重要。这些日志提供了用户体验的视图（例如，响应时间或可用性）。日志将是特定于应用程序的信息，但必须包括响应码（状态）、响应时间和其他特定于应用程序的上下文。例如，对于
    HTTP 请求，应记录域（URL 的一部分）、用户代理、请求次数、HTTP 响应代码，并在某些情况下完整的 URL 路径。再次强调，日志应包括 Kubernetes
    元数据（例如，命名空间、服务、标签、pod 名称等）。
- en: Process information and socket stats
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 处理信息和套接字统计
- en: As mentioned, these stats are not part of typical observability implementations,
    but we recommend that you consider collecting these stats as they provide a more
    comprehensive view of the Kubernetes cluster operation. For example, if you can
    get information about processes (that run in a pod), this can be an excellent
    way to correlate with application performance data (e.g., co-relating memory usage,
    or garbage collection events in a Java-based application to response time and
    network activity initiated by the process). Socket stats are details of a TCP
    flow between two endpoints (e.g., network round-trip time, TCP congestion windows,
    TCP retransmits, etc.). These stats when associated with pods can provide a view
    into the impact of the underlying network on pod-to-pod communication.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些统计信息不属于典型的可观测性实现的一部分，但我们建议您考虑收集这些统计信息，因为它们提供了 Kubernetes 集群操作的更全面视图。例如，如果您可以获取进程信息（运行在
    pod 中的进程），这可以很好地与应用程序性能数据相关联（例如，将基于 Java 的应用程序中的内存使用情况或垃圾收集事件与响应时间和由进程发起的网络活动进行关联）。套接字统计是两个端点之间
    TCP 流的详细信息（例如，网络往返时间、TCP 拥塞窗口、TCP 重传等）。当这些统计信息与 pod 关联时，可以查看底层网络对 pod 间通信的影响。
- en: Now that we have covered what you need to collect for a complete observability
    solution, let’s look at the tools and techniques available to implement collection.
    [Figure 5-3](#reference_implementation_for_collection) is an example reference
    implementation to show how you can implement collection on a node in your Kubernetes
    cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了完整的可观测性解决方案所需收集的内容，让我们来看看可用于实施收集的工具和技术。[图 5-3](#reference_implementation_for_collection)
    是一个示例参考实现，展示了如何在你的 Kubernetes 集群中的节点上实现收集。
- en: '[Figure 5-3](#reference_implementation_for_collection) shows a node in your
    Kubernetes cluster that has applications deployed as services, deployments, and
    pods in namespaces as you would see in a typical Kubernetes cluster. In order
    to facilitate collection, a few components are added as shown in the observability
    components section, and it shows a few additions to the Linux kernel to facilitate
    collection. Let’s explore the functions of each of these components.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-3](#reference_implementation_for_collection) 展示了你的 Kubernetes 集群中的一个节点，该节点部署了作为服务、部署和命名空间中的
    pod，这与典型的 Kubernetes 集群中看到的情况相同。为了方便收集，如可观测性组件部分所示，添加了一些组件，并展示了一些用于方便收集的 Linux
    内核补丁。让我们探讨每个组件的功能。'
- en: '![](Images/ksao_0503.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ksao_0503.png)'
- en: Figure 5-3\. Reference implementation for collection on a node
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 节点上的参考实现收集
- en: Linux Kernel Tools
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linux 内核工具
- en: 'The Linux kernel offers several options that you can use to help with data
    collection. It is very important that the tool you use leverages these tools instead
    of focusing on processing raw logs that are generated by other tools:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 内核提供了几个选项，可用于帮助数据收集。非常重要的是，您使用的工具利用这些工具，而不是专注于处理其他工具生成的原始日志：
- en: eBPF programs and kprobes
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF 程序和 kprobes
- en: eBPF stands for extended Berkley Packet Filter. It is an exciting technology
    that can be used for collection and observability. It was originally designed
    for packet filtering, but was then extended to allow adding programs to various
    hooks in the kernel to be used as trace points. In case you are using an eBPF-based
    dataplane, the eBPF programs that are managing the packet path will also provide
    packet and flow information. We recommend reading [Brendan Gregg’s blog post “Linux
    Extended BPF (eBPF) Tracing Tools”](https://oreil.ly/s54kG) to understand how
    to use eBPF for performance and tracing. In the context of this discussion, you
    can attach an eBPF program to a kernel probe (kprobe), which is essentially a
    trace point that is triggered and executes the program whenever the code executes
    the function for which the kprobe is registered. The kernel documentation for
    [kprobes](https://oreil.ly/hLziH) provides more details. This is a great way to
    get information from the Linux kernel for observability.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF 是扩展的伯克利数据包过滤器的缩写。它是一项令人兴奋的技术，可用于收集和可观察性。它最初设计用于数据包过滤，但后来扩展为允许将程序添加到内核中各种挂钩点，用作跟踪点。如果您正在使用基于
    eBPF 的数据平面，管理数据包路径的 eBPF 程序还将提供数据包和流量信息。我们建议阅读 [Brendan Gregg 的博客文章“Linux Extended
    BPF (eBPF) Tracing Tools”](https://oreil.ly/s54kG) 以了解如何使用 eBPF 进行性能和跟踪。在本讨论的背景下，您可以将
    eBPF 程序附加到内核探针（kprobe），这实质上是一个跟踪点，当注册了 kprobe 的函数执行时触发并执行程序。有关 [kprobes 的内核文档](https://oreil.ly/hLziH)
    提供了更多细节。这是从 Linux 内核获取可观察性信息的绝佳方式。
- en: NFLOG and conntrack
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: NFLOG 和 conntrack
- en: If you are using the standard Linux networking dataplane (iptables-based), there
    are tools available to track packets and network flows. We recommend using NFLOG,
    which is a mechanism to be used in conjunction with iptables to log packets. You
    can review the details in the iptables documentation; at a high level NFLOG can
    be set as a target for an iptables rule, and it will log packets via a netlink
    socket on a multicast address that a user space process can subscribe to and collect
    packets from. Conntrack is another module used in conjunction with iptables to
    query the connection state of a packet or a flow, and it can be used to update
    statistics for a flow.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用标准的 Linux 网络数据平面（基于 iptables），有可用工具来跟踪数据包和网络流量。我们建议使用 NFLOG，这是一个与 iptables
    结合使用的机制，用于记录数据包。您可以在 iptables 文档中查看详细信息；在高层次上，NFLOG 可以被设置为 iptables 规则的目标，并通过
    netlink 套接字在多播地址上记录数据包，用户空间进程可以订阅并收集数据包。Conntrack 是另一个模块，与 iptables 结合使用，用于查询数据包或流的连接状态，并可用于更新流的统计信息。
- en: We recommend you review options (e.g., Net Filter) that the Linux kernel provides
    and leverage them in sensors that are used to collect information. This is very
    important as it will be an efficient way to collect data, since these options
    provided by the Linux kernel are highly optimized.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您审查 Linux 内核提供的选项（例如 Net Filter），并在用于收集信息的传感器中利用它们。这非常重要，因为这将是一种高效的数据收集方式，由于
    Linux 内核提供的这些选项都经过了高度优化。
- en: Observability Components
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可观察性组件
- en: 'Now that we understand how to collect data from the Linux kernel, let’s look
    at how this data needs to be processed in user space to ensure we have an effective
    observability solution:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何从 Linux 内核收集数据，让我们看看如何在用户空间处理这些数据，以确保我们有一个有效的可观察性解决方案：
- en: Log collector
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 日志收集器
- en: This is a very important component in the system. The goal of this component
    is to add context from the Kubernetes cluster to the data collected from other
    sensors—for example, to add pod metadata (name, namespace, label, etc.) to source
    and destination IP addresses, respectively, from a network flow. This is how you
    can add Kubernetes context to raw network flow logs. Likewise, any data you collect
    from kernel probes can also be enriched by adding relevant Kubernetes metadata.
    This way you can have log data that associates activity in the kernel to objects
    in your Kubernetes cluster (e.g., pods, services, deployments). It is critical
    for you to be able derive insights about your Kubernetes cluster operation. Please
    note that this component is something you need to implement, or you must ensure
    that the tool you choose for observability has this functionality. It is a critical
    part of your observability implementation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是系统中非常重要的一个组件。该组件的目标是从Kubernetes集群中为从其他传感器收集的数据添加上下文，例如，将Pod元数据（名称、命名空间、标签等）添加到网络流的源和目标IP地址中。这是您可以将Kubernetes上下文添加到原始网络流日志的方式。同样，您收集的任何来自内核探测器的数据也可以通过添加相关的Kubernetes元数据来丰富。通过这种方式，您可以获得将内核中的活动与Kubernetes集群中的对象（例如Pod、服务、部署）关联起来的日志数据。您能够从中推断出有关Kubernetes集群操作的见解。请注意，这个组件是您需要实现的，或者必须确保您选择的可观察性工具具有此功能。这是您可观察性实施中的关键部分。
- en: Envoy (proxy)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy（代理）
- en: We discussed the importance of having a collection of application-specific data,
    and for this we recommend that you use [Envoy](https://oreil.ly/0niF8), a well-known
    proxy that is used to analyze application protocols and log application transaction
    flows (e.g., HTTP transactions on a single HTTP connection). Please note that
    Envoy can be used as a sidecar pattern where it attaches to every pod as a sidecar
    and tracks packets to/from the pod. It can also be deployed as a daemonset (a
    transparent proxy) where you can use the dataplane to redirect traffic to pass
    through an envoy instance running on the host. We strongly recommend using Envoy
    with this latter configuration, as using the sidecar pattern has security concerns
    and can be disruptive to applications. In the context of this discussion, the
    Envoy daemonset will be the source of application flow logs to the log collector.
    The log collector can now use the pod metadata (name, namespace, labels, deployments,
    services, IP addresses) to correlate this data with the data received from the
    kernel and further enrich it with application data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了拥有一个应用特定数据集的重要性，并为此推荐您使用[Envoy](https://oreil.ly/0niF8)，这是一个广为人知的代理工具，用于分析应用协议并记录应用事务流（例如，在单个HTTP连接上的HTTP事务）。请注意，Envoy可以作为侧车模式使用，附加到每个Pod，并跟踪与Pod的数据包进出。它还可以部署为守护进程集（透明代理），您可以使用数据平面将流量重定向，通过运行在主机上的Envoy实例。我们强烈建议您使用后一种配置的Envoy，因为使用侧车模式存在安全问题，并且可能会对应用程序造成干扰。在这次讨论的背景下，Envoy守护进程集将是应用流日志的来源，提供给日志收集器。现在，日志收集器可以使用Pod元数据（名称、命名空间、标签、部署、服务、IP地址）将这些数据与从内核接收的数据进行关联，并进一步丰富应用数据。
- en: Fluentd
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd
- en: Note that the data collection discussed so far is processed by the log collector
    on every node in the cluster. You need to implement a mechanism to send the data
    from all nodes to a datastore or security information and event management (SIEM),
    where it can be picked up by analytics and visualization tools. [Fluentd](https://oreil.ly/11s22)
    is an excellent option to send collected data to the datastore of your choice.
    It offers excellent integrations and is a tool that is Kubernetes native. There
    are other options available, but we recommend you use Fluentd for shipping collected
    log data to a data store.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，到目前为止讨论的数据收集是由集群中每个节点上的日志收集器处理的。您需要实现一种机制，将所有节点的数据发送到数据存储或安全信息和事件管理（SIEM）中，在那里可以被分析和可视化工具获取。[Fluentd](https://oreil.ly/11s22)是将收集的数据发送到您选择的数据存储的绝佳选择。它提供了出色的集成，是一个本地支持Kubernetes的工具。还有其他选择可用，但我们建议您使用Fluentd来将收集的日志数据发送到数据存储。
- en: Prometheus
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus
- en: We’ve discussed how you collect flow logs; now we need a component for the collection
    of metrics and alerting. Prometheus, a Kubernetes-native tool, is a great choice
    for metrics collection and alerting. It’s deployed as endpoints that scrape metrics
    and send them to a time-series database that’s a part of the Prometheus server
    for analysis and query. You can also define alerts for data sent to the Prometheus
    server. It’s a widely used option and has integrations to dashboards and alerting
    tools. We recommend you consider it as an option for your cluster.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何收集流日志；现在我们需要一个组件来收集指标和警报。Prometheus，一个原生于Kubernetes的工具，是收集指标和警报的一个很好的选择。它部署为端点，这些端点抓取指标并将其发送到Prometheus服务器中的时间序列数据库进行分析和查询。您还可以为发送到Prometheus服务器的数据定义警报。它是一个广泛使用的选项，并且具有与仪表板和警报工具的集成。我们建议您考虑它作为集群的一个选项。
- en: We hope that this discussion has given you an idea of how you can implement
    data collection for your Kubernetes cluster. Now let’s look at aggregation and
    correlation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这次讨论使您了解如何为您的Kubernetes集群实现数据收集。现在让我们来看一下聚合和相关性。
- en: Aggregation and Correlation
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合和相关性
- en: In the previous section we covered data collection and discussed how you can
    collect data from various sources in your cluster (API server, network flows,
    kernel probes, application flows). This is very useful, but we still need to address
    the concern of data volume if we keep the collection at pod-level granularity.
    Another thing to note is that the data volume concern multiples if we keep data
    from various sources separate and then associate it at query time. You can say
    that it’s better to keep as much raw data as possible, and there are efficient
    tools to query and aggregate data after collection (offline), so why not use that
    approach? Yes, that is a valid point, but there are a couple of things to think
    about. The large volume of data would mean aggregation and query time joins of
    data will be resource-intensive (it can very well be more expensive to operate
    your data collection system than your Kubernetes cluster!). Also, given the ephemeral
    nature of Kubernetes (pod life cycles can be very short), the latency in analyzing
    data offline prevents any kind of reasonable response to data collected to mitigate
    the issue reported by the data. In some cases, if correlation is not done at collection
    time, it will not be possible to associate two different collections. For example,
    you cannot collect a list of policies and a list of flows and then associate the
    policy with a flow offline without rerunning the policy evaluation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们涵盖了数据收集并讨论了如何从集群中的各种来源（API服务器、网络流量、内核探测器、应用程序流量）收集数据。这非常有用，但如果我们将收集保持在Pod级别的粒度，我们仍然需要解决数据量的问题。另一个需要注意的是，如果我们保持来自不同来源的数据分开，然后在查询时关联数据，数据量的问题会成倍增加。可以说，尽可能保留尽可能多的原始数据是更好的选择，而且在收集后有高效的工具来查询和聚合数据（离线），为什么不采用这种方法呢？是的，这是一个有效的观点，但有几件事需要考虑。大量数据意味着数据的聚合和查询时间的连接将消耗大量资源（操作您的数据收集系统可能比操作您的Kubernetes集群更昂贵！）。此外，鉴于Kubernetes的临时性质（Pod生命周期可能非常短暂），离线分析数据的延迟阻止了对收集到的数据报告问题的任何合理响应。在某些情况下，如果在收集时未进行相关性，则无法关联两个不同的收集。例如，您无法收集策略列表和流量列表，然后在离线模式下重新运行策略评估而不将策略与流量关联。
- en: We also discussed the declarative nature of Kubernetes and how a deployment
    and a service are higher-level constructs than a pod. In such a scenario, we recommend
    that you consider the aggregation of data at a deployment or a service level.
    This means data from all pods for a service is aggregated; you would collect data
    between deployments and data to services as a default option. This will give you
    the right level of granularity. You can provide an option to reduce the aggregation
    to collect pod-level data as a drill-down action or in response to an event. This
    way you can address the concern about large amounts of log data collected and
    the associated processing cost. Also, the data collection is more Kubernetes-native
    as Kubernetes monitors deployments/services as a unit and makes adjustments to
    ensure the deployment/service is operating as per the specification.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了Kubernetes的声明性质，以及部署（deployment）和服务（service）是比Pod更高级的构建。在这种情况下，我们建议您考虑在部署或服务级别聚合数据。这意味着服务的所有Pod的数据被聚合；您将默认收集部署之间和服务之间的数据。这将为您提供正确的粒度级别。您可以提供一个选项来减少聚合以收集Pod级别的数据，作为深入操作或响应事件的一部分。这样，您可以解决关于收集大量日志数据及相关处理成本的问题。此外，数据收集更符合Kubernetes本身，因为Kubernetes监控部署/服务作为一个单元，并进行调整，以确保部署/服务按照规范运行。
- en: In the data collection section we discussed the log collector component that
    receives data from various sources. It could be used as a source to correlate
    data at collection time, so you don’t have to do any additional correlation after
    data collection, and you also benefit from not having to collect redundant data
    for each source. For example, if the kprobe in the kernel collects socket data
    for five-tuple (IP addresses, ports, protocol), and the NFLOG provides other information
    like bytes and packets for the same five-tuple, the log collector can create a
    single log with the five-tuple, the Kubernetes metadata, the network flow data,
    and the socket statistics. This will provide logs with very high context and low
    occupancy for collection and processing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集部分，我们讨论了日志收集器组件，它从各种来源接收数据。它可以用作收集时数据关联的源，因此您无需在数据收集后进行任何额外的关联，并且您还将受益于不必为每个来源收集冗余数据。例如，如果内核中的kprobe收集五元组（IP地址、端口、协议）的套接字数据，并且NFLOG为相同的五元组提供其他信息，如字节和数据包，日志收集器可以创建一个包含五元组、Kubernetes元数据、网络流数据和套接字统计数据的单个日志。这将为收集和处理提供具有非常高上下文和低占用率的日志。
- en: 'Now let’s go back to the Google online boutique example and see a sample of
    what a log will look like with aggregation and correlation of kernel and network
    flow data. The sample log is generated using the collection and aggregation concepts
    described previously for a transaction between the frontend service and the currencyservice
    of the application. It is a gRPC-based transaction:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到Google在线精品示例，并看一看带有内核和网络流数据聚合和关联的日志样本。样本日志是使用前面描述的收集和聚合概念为应用程序的前端服务和货币服务之间的交易生成的，这是基于gRPC的交易：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is an example of a flow log from Calico Enterprise. There are a few things
    to note about the log: It aggregates data from all pods backing the frontend service
    (`frontend-6f794fbff7-*`) and all pods belonging to the currencyservice (`currencyservice-7fd6c64-*`).
    The data from the kprobe and socket statistics are aggregated as mean, min, and
    max for each metric for the data between services. The process ID and the process
    name received from the kernel are correlated with the other data, and we also
    see the network policy action and the network policies impacting the flow correlated
    with other data. This is an example of what you want to achieve for data collection
    in your Kubernetes cluster!'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Calico Enterprise的流日志示例。有几点需要注意：它聚合了支持前端服务的所有Pod（`frontend-6f794fbff7-*`）和属于currencyservice的所有Pod（`currencyservice-7fd6c64-*`）的数据。来自kprobe和套接字统计的数据被聚合为每个指标的平均值、最小值和最大值，用于服务之间的数据。来自内核的进程ID和进程名称与其他数据相关联，我们还看到网络策略操作和影响流的网络策略与其他数据相关联。这是您在Kubernetes集群中进行数据收集的目标示例！
- en: Now that we have covered how to collect, aggregate, and correlate data in a
    Kubernetes-native manner, let’s explore visualization of data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何以Kubernetes本地方式收集、聚合和关联数据，让我们来探索数据的可视化。
- en: Visualization
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: There are some great tools that support the visualization of the data collected.
    For example, Prometheus offers an integration with Grafana that provides very
    good dashboards to visualize data. There are also some commercial tools like Datadog,
    New Relic, and Calico Enterprise that support the collection and visualization
    of data. We will cover a few common visualizations that are useful for Kubernetes
    clusters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些很棒的工具支持收集数据的可视化。例如，Prometheus 与 Grafana 的集成提供了非常好的仪表板来可视化数据。还有一些商业工具如 Datadog、New
    Relic 和 Calico Enterprise，支持数据的收集和可视化。我们将介绍一些对 Kubernetes 集群有用的常见可视化方式。
- en: Service Graph
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Service Graph
- en: This is a representation of your Kubernetes cluster as a graph showing services
    in a Kubernetes cluster and interactions between them. If we go back to the Google
    microservices online boutique example, [Figure 5-4](#service_graph_representation_of_the_onl)
    shows the online boutique application implemented and represented as a service
    graph.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将您的 Kubernetes 集群表示为一个图表，显示 Kubernetes 集群中的服务及其之间的交互。如果我们回到 Google 微服务在线精品店的示例，[图 5-4](#service_graph_representation_of_the_onl)
    显示了实施并表示为服务图的在线精品店应用程序。
- en: '[Figure 5-4](#service_graph_representation_of_the_onl) is a visualization of
    the online boutique namespace as a service graph, with the nodes representing
    services and pods backing a service or a group of pods either standalone or as
    a part of a deployment. The edges show network activity and policy action. The
    graph is interactive and allows you to pick a service (e.g., frontend service)
    and allows the viewing of detailed logs collected for the service. [Figure 5-5](#detailed_view_of_the_frontend_microserv)
    shows a summarized view of all collected data for the service selected (frontend).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](#service_graph_representation_of_the_onl) 是将在线精品店命名空间表示为服务图的可视化，其中节点表示服务和支持服务或部署的一组
    Pod，无论是独立的还是作为部署的一部分。边缘显示了网络活动和策略操作。该图是交互式的，允许您选择一个服务（例如前端服务），并允许查看为该服务收集的详细日志。[图 5-5](#detailed_view_of_the_frontend_microserv)
    显示了所选服务（前端）的所有收集数据的汇总视图。'
- en: '[Figure 5-5](#detailed_view_of_the_frontend_microserv) shows a detailed view
    of the frontend service as a drill-down—it shows information from all sources
    in one view, so it’s very easy to analyze the operation of the service.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-5](#detailed_view_of_the_frontend_microserv) 显示了前端服务的详细视图，作为一种钻取方式——它展示了来自所有来源的信息，因此非常容易分析服务的操作。'
- en: Service graph is a very common pattern to represent Kubernetes cluster topology.
    There are several tools that provide this view, such as Kiali, Datadog, and Calico
    Enterprise.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 服务图是表示 Kubernetes 集群拓扑结构的一种非常常见的模式。有几种工具提供了这种视图，例如 Kiali、Datadog 和 Calico Enterprise。
- en: '![](Images/ksao_0504.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ksao_0504.png)'
- en: Figure 5-4\. Service graph representation of the online boutique application
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 在线精品店应用程序的服务图表示
- en: '![](Images/ksao_0505.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ksao_0505.png)'
- en: Figure 5-5\. Detailed view of the frontend microservice
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 前端微服务的详细视图
- en: Visualization of Network Flows
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络流量的可视化
- en: '[Figure 5-6](#network_flow_visualization) shows a common pattern used to visualize
    flows. This is ring-based visualization, where each ring represents an aggregation
    level. In the example shown in [Figure 5-5](#detailed_view_of_the_frontend_microserv),
    the outermost ring represents a namespace and all flows within the namespaces.
    Selecting a ring in the middle shows all flows for a service, and selecting the
    innermost ring shows all flows for pods backing the service. The panel on the
    right is a selector to enable more-granular views using filtering and details
    like flows and policy action for the selection. This is an excellent way to visualize
    network flows in your cluster.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-6](#network_flow_visualization) 显示了一种用于可视化流量的常见模式。这是基于环的可视化，其中每个环代表一个聚合级别。在
    [图 5-5](#detailed_view_of_the_frontend_microserv) 中显示的示例中，最外层的环代表一个命名空间及其内的所有流量。选择中间的环显示了一个服务的所有流量，选择最内层的环显示了支持该服务的
    Pod 的所有流量。右侧面板是一个选择器，可以通过过滤和详细信息如选择的流量和策略操作来启用更细粒度的视图。这是在集群中可视化网络流量的一个绝佳方式。'
- en: '![](Images/ksao_0506.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ksao_0506.png)'
- en: Figure 5-6\. Network flow visualization
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 网络流量可视化
- en: In this section we have covered some common visualization patterns and tried
    to show how they can be applied to Kubernetes. Please note that there are several
    visualizations that can be applied to Kubernetes; these are examples to show how
    you can represent data collected in a Kubernetes cluster.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些常见的可视化模式，并尝试展示它们如何应用于Kubernetes。请注意，有几种可应用于Kubernetes的可视化方法；这些只是示例，展示了如何表示在Kubernetes集群中收集的数据。
- en: Now that we have covered data collection, aggregation, correlation, and visualization,
    let’s explore some advanced topics to utilize the data collected to derive insights
    into the operation of the Kubernetes cluster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讲解了数据收集、聚合、关联和可视化，让我们探讨一些高级主题，利用收集的数据来洞察Kubernetes集群的运行情况。
- en: Analytics and Troubleshooting
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析和故障排除
- en: In this section we will explore analytics applications that leverage the collection,
    aggregation, and correlation components to help provide additional insights. Note
    that there are many applications that can be built to leverage the context-rich
    data in a Kubernetes cluster. We cover some applications as examples.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将探讨分析应用程序，这些应用程序利用收集、聚合和关联组件来提供额外的洞察力。请注意，有许多应用程序可以构建以利用Kubernetes集群中的上下文丰富数据。我们提供一些应用程序作为示例。
- en: Distributed Tracing
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式跟踪
- en: We explained distributed tracing before and discussed its importance in a microservices-based
    architecture, where it is critical to trace a single user request across multiple
    transactions that need to happen between various microservices. There are two
    well-known approaches to implementing distributed tracing,
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前解释过分布式跟踪，并讨论了它在基于微服务的架构中的重要性，在这种架构中，跟踪单个用户请求穿越需要在各种微服务之间发生的多个事务是至关重要的。有两种众所周知的实现分布式跟踪的方法，
- en: Instrument transaction request headers
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 仪器化事务请求头
- en: In this method the HTTP headers are instrumented with a request ID, and the
    request ID is preserved in headers across calls to various other services. [Envoy](https://oreil.ly/jprHR)
    is a very popular tool used to implement distributed tracing. It supports integrations
    with other well-known application tracers like Lightstep and AWS X-Ray. We recommend
    that you use Envoy if you are fine with instrumenting applications to add and
    preserve the request ID across calls between microservices.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，HTTP头部被仪器化为一个请求ID，并且请求ID在调用其他服务时通过头部保留。[Envoy](https://oreil.ly/jprHR)是一个非常流行的工具，用于实现分布式跟踪。它支持与其他知名应用程序跟踪器如Lightstep和AWS
    X-Ray的集成。如果您可以接受仪器化应用程序以在微服务之间的调用中添加和保留请求ID，我们建议您使用Envoy。
- en: eBPF and kprobes
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF和kprobes
- en: In the method described for using Envoy, there is a change required to the application
    traffic. It is possible to implement distributed tracing for service-to-service
    calls using eBPF and Linux kernel probes. You can attach eBPF programs to kprobes/uprobes
    and other trace points in the kernel and build a distributed tracing application.
    Note the detailed implementation of such an application is beyond the scope of
    this book, but we wanted to mention this as an option for distributed tracing
    in case you are wary of altering application traffic.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Envoy描述的方法中，需要对应用程序流量进行更改。可以使用eBPF和Linux内核探针为服务间调用实现分布式跟踪。您可以将eBPF程序附加到内核中的kprobes/uprobes和其他跟踪点，并构建一个分布式跟踪应用程序。请注意，此类应用程序的详细实现超出了本书的范围，但我们想提到这是实现分布式跟踪的一个选项，以防您担心修改应用程序流量。
- en: Now that we have covered distributed tracing, let’s look at how you can implement
    packet capture in your Kubernetes cluster.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讲解了分布式跟踪，让我们看看如何在您的Kubernetes集群中实现数据包捕获。
- en: Packet Capture
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据包捕获
- en: In your Kubernetes cluster we recommend that you implement or pick a tool that
    supports raw packet captures between pods. The tool should support a selector-based
    packet capture (e.g., pod labels) and role-based access control to enable and
    view packet captures. This is a simple yet very effective feature that can be
    used as a response action to an event (e.g., increased application latency) to
    analyze raw packet flows to understand the issue and find the root cause. In order
    to implement raw packet captures, we recommend using [libpcap](https://oreil.ly/c2UFJ),
    which supports the ability to capture packets on an interface on Linux systems.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的 Kubernetes 集群中，我们建议您实施或选择一个支持 pod 之间原始数据包捕获的工具。该工具应支持基于选择器的数据包捕获（例如，pod
    标签）和基于角色的访问控制，以便启用和查看数据包捕获。这是一个简单而非常有效的功能，可以作为响应事件（例如，应用程序延迟增加）的一种操作来分析原始数据包流，以理解问题并找到根本原因。为了实施原始数据包捕获，我们建议使用
    [libpcap](https://oreil.ly/c2UFJ)，它支持在 Linux 系统上的接口上捕获数据包的能力。
- en: Conclusion
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter we covered what observability is and how to implement it for
    your Kubernetes cluster. The following are the highlights of this chapter:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了什么是可观察性，以及如何为您的 Kubernetes 集群实施它。以下是本章的要点：
- en: Monitoring needs to be a part of your observability strategy; monitoring alone
    is not sufficient.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控需要成为您可观察性战略的一部分；仅仅进行监控是不够的。
- en: It is important to leverage the declarative nature of Kubernetes when you implement
    an observability solution.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实施可观察性解决方案时，利用 Kubernetes 的声明性特性非常重要。
- en: The key components for implementing observability for your Kubernetes cluster
    are log collection, log aggregation and correlation, visualization, distributed
    tracing, and analytics.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实施 Kubernetes 集群的可观察性，关键组件包括日志收集、日志聚合与关联、可视化、分布式追踪和分析。
- en: You must implement your observability using a tool that is native to Kubernetes.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须使用适用于 Kubernetes 的本地工具来实现您的可观察性。
- en: You should use tools available in the Linux kernel to drive efficient collection
    and aggregation of data (e.g., NFLOG, eBPF-based probes).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该使用 Linux 内核中提供的工具来驱动数据的高效收集和聚合（例如，NFLOG、基于 eBPF 的探针）。
