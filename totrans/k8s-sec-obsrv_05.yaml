- en: Chapter 5\. Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will discuss the difference between monitoring and observability
    in the context of Kubernetes deployments. We will explain best practices and tools
    for implementing observability in your Kubernetes cluster. In the next chapter
    we will cover how you can use observability to secure your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Observability has been a topic of discussion recently in the Kubernetes community
    and has garnered a lot of interest. We begin by understanding the difference between
    monitoring and observability. We then look at why observability is critical to
    security in a distributed application like Kubernetes, and review tools and reference
    implementations for observability. While observability is a broad topic and applies
    to several areas, we will keep the discussion focused on Kubernetes in this chapter.
    Let’s start by looking at monitoring and observability and how they are different.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monitoring is a known set of measurements in a system that are used to alert
    for deviations from a normal range. The following are examples of types of data
    you can monitor in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Pod logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network flow logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application flow logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audit logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of metrics you can monitor include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Connections per second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packets per second, bytes per second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application (API) requests per second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU and memory utilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These logs and metrics can help you identify known failures and provide more
    information about the symptom to help you remediate the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to monitor your Kubernetes cluster, you use techniques like polling
    and uptime checks depending on the SLAs you need to maintain for their cluster.
    The following are examples of metrics you could monitor for SLAs:'
  prefs: []
  type: TYPE_NORMAL
- en: Polling of application/API endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application response codes (e.g., HTTP or database error codes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application response time (e.g., HTTP duration, database transaction time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node availability for scale-out use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory/CPU/disk/IO resources on a node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other important part of monitoring is alerting. You need an alerting system
    as part of your monitoring solution that generates alerts for any metric that
    violates the specified threshold. Tools like Grafana, Prometheus, OpenMetrics,
    OpenTelemetry, and Fluentd are used as monitoring tools to collect logs and metrics,
    and generate reports, dashboards, and alerts for Kubernetes clusters. Kubernetes
    offers several integrations to tools like Opsgenie, PagerDuty, Slack, and JIRA
    for alert forwarding and management.
  prefs: []
  type: TYPE_NORMAL
- en: 'Monitoring your production Kubernetes cluster has the following issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Amount of log data
  prefs: []
  type: TYPE_NORMAL
- en: In a system like Kubernetes, a node has several pods that run on the host, and
    each pod comes with its own logs, its own network identity, and its own resources.
    This means you have logs from the application operation, network flow logs, Kubernetes
    activity (audit) logs, and application flow logs for each pod. In a non-Kubernetes
    environment, you typically had an application running on a node and so it would
    be just one set of logs as opposed to one set of logs per pod running on the node.
    This multiplies the amount of log data that needs to be collected/inspected. In
    addition to the per-pod logs, you also need to collect cluster logs from Kubernetes.
    Typically these are also known as audit logs that provide visibility into Kubernetes
    cluster activity. The number of logs in the system will make monitoring very resource-intensive
    and expensive to maintain. Your log collection cluster should not be more expensive
    to operate than the cluster running your applications!
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring distributed applications
  prefs: []
  type: TYPE_NORMAL
- en: In a Kubernetes cluster, applications are distributed across the Kubernetes
    cluster network. An application that needs more than one pod (e.g., a deployment
    set or a service) will have logs for each pod that need to be examined in addition
    to the context of the set of pods (e.g., scale out, error handling, etc.). We
    have multiple pods that need to be considered as a group before we generate an
    alert for the application. Please note the goal is to monitor the application
    and generate alerts for the application, and generating alerts for pods that are
    a part of the application independently does not provide an accurate representation
    of the state of the application. There is also the case of the microservices application,
    where a single application is deployed as a set of services known as *microservices*,
    and each microservice is responsible for a part of the functionality of the application.
    In this case, you need to monitor each microservice as an entity (note a microservice
    is a set of one or more pods) and then understand which microservices impact any
    given application transaction. Only then can you report an alert for the application.
  prefs: []
  type: TYPE_NORMAL
- en: Declarative nature of Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: As we have covered, Kubernetes is declarative and allows you to specify exactly
    how you want pods to be created and run in the cluster. Kubernetes allows you
    to specify resource limits for memory, CPU, storage, etc., and you can also create
    custom resources and specify limits for these resources. The scheduler will find
    a node that has the required resources and schedule a pod on the node. Kubernetes
    also monitors usage for pods and will terminate pods that consume more resources
    than those allocated to them. In addition, Kubernetes provides detailed metrics
    that can be used to monitor pods and cluster state. For example, you can use a
    tool like [Prometheus](https://oreil.ly/zzjVG) that can monitor pods and cluster
    state and use the metrics, and you can automatically scale pods or other cluster
    resources with a mechanism known as the [Horizontal Pod Autoscaler](https://oreil.ly/luM5u).
    What this means is that Kubernetes as a part of its operation is monitoring and
    making changes to the cluster to maintain operations as per the configured specification.
    In this scenario, an alert from monitoring a single metric can be a result of
    Kubernetes making changes to adapt to the load in the cluster, or it could be
    a real issue. You need to be able to distinguish between the two scenarios to
    be able to accurately monitor your application.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand monitoring and how it can be implemented and the challenges
    with using monitoring for a Kubernetes cluster, let’s look at observability and
    how it can help overcome these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability is defined as the ability to understand the internal state of
    a system by only looking at external outputs of the system. [*Observability Engineering*
    by Charity Majors et al. (O’Reilly)](https://oreil.ly/3hPEr) is an excellent resource
    to learn more about observability. The book’s second chapter discusses monitoring
    and observability and is very relevant to this discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Observability builds on monitoring and enables you to gain insights about the
    internal state of your application. For example, in a Kubernetes cluster an unexpected
    pod restart event may have limited to no impact on services as other instances
    of the pod may be adequate to handle the load at the time of the restart. A monitoring
    system will generate an alert that an unexpected pod restart occurred, and an
    observability system will generate a medium-priority event with the context that
    an unexpected pod restart occurred but had no impact on the system if there is
    no other event like application errors at the time of the pod restart. Another
    example is when an event is generated at the application layer (e.g., duration
    for HTTP request is larger than the norm). In this scenario, the observability
    system will provide context for the reason of degradation in application response
    time (e.g., network layer issue, retransmits, application pod restarts due to
    resource or other application issues, a Kubernetes infrastructure issue like DNS
    latency or API server load). As explained previously, an observability system
    can look at multiple events that impact application state and report application
    status after considering all of them. Now let’s look at how you can implement
    observability in a Kubernetes system.
  prefs: []
  type: TYPE_NORMAL
- en: How Observability Works for Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The declarative nature of Kubernetes helps a lot in implementing an observability
    system. We recommend that you build a system that is native to Kubernetes and
    is able to understand operations in a cluster. For example, a system that understands
    Kubernetes will monitor a pod (e.g., restarts, out of memory, network activity,
    etc.) but also understand if a pod is a standalone instance or part of a deployment,
    replica set, or service. It will also know how critical the pod is to the service
    or deployment (e.g., how the service is configured for scalability and high availability).
    So when it reports any event related to the pod, it will provide all this context
    and help you easily make a decision about how you need to respond to the event.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to remember is that in Kubernetes you can deploy applications
    as pods that are a part of higher-level constructs like a deployment or a service.
    In order to appreciate the complexity in implementing observability for these
    constructs, we will use an example to explain them. When you configure a service,
    Kubernetes manages all pods associated with the service and ensures that traffic
    is delivered to available pods that are a part of the service. Let’s take a look
    at an example of [service definition from the Kubernetes documentation](https://oreil.ly/ijVz5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example all pods that have the label MyApp and listen on TCP port 9376
    become part of the service, and all traffic destined to the service is redirected
    to these pods. We cover this concept in detail in [Chapter 8](ch08.xhtml#managing_trust_across_teams).
    So in this scenario, the observability solution should also work to provide insights
    at the service level. Monitoring a pod in this case is not sufficient. What is
    needed is that the observability aggregates metrics across all pods in a service
    and uses the aggregated information for more analytics and alerts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at an example of [deployments](https://oreil.ly/23Eam) in Kubernetes.
    Deployments allow you to manage pods and replica sets (replicas of a pod, typically
    used for scaling and high availability). The following is an example configuration
    for a deployment in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This configuration will create a deployment for nginx with three replica pods
    with the configured metadata and specification. Kubernetes has a deployment controller
    to ensure that all pods and replicas that are a part of the deployment are available.
    There are several other benefits, like rolling updates, autoscaling, etc., that
    can be achieved by using the deployment resource in Kubernetes. In such a scenario
    for observability, the tool you use should look at the activity of all pods (replicas)
    in a deployment as an aggregate (e.g., all traffic to/from pods in a deployment,
    pod restarts and their effect on a deployment, etc.). Monitoring and alerting
    for each pod will not be sufficient to understand how the deployment is operating.
  prefs: []
  type: TYPE_NORMAL
- en: In both these examples it is clear that the collection of metrics needs to be
    in the context of Kubernetes. Instead of collecting all data and metrics at a
    pod-level granularity, the collection engine should collect data at a deployment-
    or service-level granularity when applicable to deliver an accurate representation
    of the state of the deployment or service. Remember, Kubernetes abstracts pod-level
    details, and so we need to focus on measuring and alerting at a higher level than
    pods. Aggregation of data at a deployment and service level will reduce the number
    of logs you need to collect all the time and address the concern of the costs
    associated with a large number of logs. Please note the tool needs to have the
    ability to drill down and capture pod-level details when the operator needs to
    analyze an issue. We will cover this later in this chapter when we discuss data
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how we can leverage the declarative nature of Kubernetes
    to help with observability and reduce the amount of log data we need to collect
    and generate relevant alerts, let’s explore the distributed nature of Kubernetes
    and its impact on observability.
  prefs: []
  type: TYPE_NORMAL
- en: In a microservices-based application deployment, a single application comprises
    several microservices that are deployed in a Kubernetes cluster. This means that
    in order to service a single transaction from the user, one or more services need
    to interact with each other, resulting in one or more subtransactions. A great
    example of a sample microservices application is the [Google online boutique demo
    microservices application](https://oreil.ly/wx7bj). [Figure 5-1](#architecture_of_the_google_microservice)
    shows the architecture for this application.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-1](#architecture_of_the_google_microservice) shows how an online
    boutique application can be deployed as microservices in Kubernetes. There are
    11 microservices, each responsible for some aspect of the application. We encourage
    you to review this application as we will use it to demonstrate how you can implement
    observability later in the chapter. If you look at the checkout transaction, a
    user makes a request to the frontend service, which then makes a request to the
    checkout service. The checkout service needs to interact with several services
    (e.g., PaymentService, Shipping Service, CurrencyService, EmailService, ProductCatalog
    Service, CartService) to complete the transaction. So in this scenario if we see
    our HTTP application log indicate a larger-than-expected duration for the checkout
    process API response time, we will need to review each of the subtransactions
    and see if there is an issue with each one and what the issue is (an application
    issue, network issue, etc.). Another thing that makes this complicated is the
    fact that each subtransaction is asynchronous and each microservice is serving
    several independent transactions simultaneously. In such a scenario you need to
    use a technique known as *distributed tracing* to trace the flow of a single transaction
    across a set of microservices. Distributed tracing can happen by instrumenting
    the application or instrumenting the kernel. We will cover distributed tracing
    later in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Architecture of the Google microservices demo application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we understand observability and how you should think about it for a
    Kubernetes cluster, let’s look at the components for an observability tool for
    Kubernetes. [Figure 5-2](#components_of_an_observability_tool_for) shows a block
    diagram of the various components of an observability tool for Kuebrnetes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Components of an observability tool for Kubernetes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 5-2](#components_of_an_observability_tool_for) shows that you need
    the following components for your observability implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry collection
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, your observability solution needs to collect telemetry data from
    various sensors in your cluster. It needs to be distributed and Kubernetes-native.
    It must support sensors across all layers, from L3 to L7\. It also needs to collect
    information about Kubernetes infrastructure (e.g., DNS and API server logs) and
    Kubernetes activity (these are known as audit logs). As described, this information
    must be collected in the context of deployments and services.
  prefs: []
  type: TYPE_NORMAL
- en: Analytics and visibility
  prefs: []
  type: TYPE_NORMAL
- en: In this layer, the system must provide visualizations that are specific to Kubernetes
    operations (e.g., service graph, Kubernetes platform view, application views).
    We will cover some common visualizations that are native to Kubernetes. We recommend
    you pick a solution that leverages machine learning techniques for baselining
    and reporting anomalies. Finally, the system needs to support the ability for
    operators to enable pod-to-pod packet capture (note that this is not the same
    as enabling packet capture on the host interface, as the pod-level visibility
    is lost). We will cover this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Security and troubleshooting applications
  prefs: []
  type: TYPE_NORMAL
- en: The observability system you implement must support distributed tracing as described
    in the previous section to help troubleshoot applications. We also recommend the
    use of advanced machine learning techniques to understand Kubernetes cluster behavior
    and predict performance or security concerns. Please note that this is a new area
    and there is ongoing innovation in it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered what is needed to implement observability in a Kubernetes
    cluster, let’s review each of the components in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Observability for Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we will review each component needed to build an effective observability
    system in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should think of log collection as a set of sensors that are distributed
    in your cluster. You need to ensure that the sensors are efficient and do not
    interfere with system operation (e.g., adding latency). We will cover methods
    of collection later in this section that will show how you can efficiently collect
    metrics. You should consider deploying sensors (or collecting information) across
    all the layers of the stack, as shown in [Figure 5-2](#components_of_an_observability_tool_for).
    Kubernetes audit logs are an excellent source of information to understand the
    complete life cycle of various Kubernetes resources. In addition to audit logs,
    Kubernetes provides a variety of options for [monitoring](https://oreil.ly/FSwUs).
    Next you need to focus on traffic flow logs (Layer 3/Layer 4) to understand the
    operation of the Kubernetes cluster network. Given the declarative nature of Kubernetes,
    it is important to collect logs related to application flows (e.g., HTTP or MySQL),
    where the logs provide visibility into the application behavior (e.g., response
    time, availability, etc.) as seen by the user. In order to help with troubleshooting,
    you should also collect logs related to Kubernetes cluster infrastructure (e.g.,
    API server, DNS). Some advanced troubleshooting systems also collect information
    from the Linux kernel that is a result of activity by a pod (e.g., process information,
    socket stats for a flow initiated by a pod) and provide a way to enable packet
    capture (raw packets) for pod-to-pod traffic. The following describes what you
    should collect for each:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes audit logs
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides the ability to collect and monitor activity. Here is an
    excellent [guide](https://oreil.ly/aKmCE) that explains how you can control what
    to collect and also mechanisms for logging and alerting. We suggest you review
    what you need to collect and set the audit policy carefully—we recommend against
    just collecting everything. For example, you should log API requests, usernames,
    RBAC, decisions, request verbs, the client (user-agent) that made the request,
    and response codes for API requests. We will show a sample Kubernetes activity
    dashboard in the visualization section.
  prefs: []
  type: TYPE_NORMAL
- en: Network flow logs
  prefs: []
  type: TYPE_NORMAL
- en: Network flow logs (Layer 3/Layer 4) are key to understanding the Kubernetes
    cluster network operation. Typically these include the five-tuple (source and
    destination IP addresses/ports and port). It is also important to collect Kubernetes
    metadata associated with pods (source and destination namespaces, pod names, labels
    associated with pods, host on which the pods were running) and aggregate bytes/packets
    for each flow. Note this can result in a large amount of flow data, as there could
    be a large number of pods on a node. We will address this in the following section
    about aggregation at collection time.
  prefs: []
  type: TYPE_NORMAL
- en: DNS flow logs
  prefs: []
  type: TYPE_NORMAL
- en: Along with the API server, the DNS server is a critical part of the Kubernetes
    cluster and is used by applications to resolve domain names in order to connect
    other services/pods as a part of normal operation. An issue with the DNS server
    can impact several applications in the cluster. It is important to collect information
    from the client’s perspective. You should log DNS requests by pods that capture
    request count, latency, which DNS server was used to resolve the request, the
    DNS response code, and the response. This should be collected with Kubernetes
    metadata (e.g., namespace, pod name, labels, etc.), as this will help associate
    the DNS issue with a service and facilitate further troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Application logs
  prefs: []
  type: TYPE_NORMAL
- en: As explained, the collection of application logs (HTTP, MySQL) is very important
    in a declarative system like Kubernetes. These logs provide a view into the user
    experience (e.g., response time or availability). The logs will be application-specific
    information but must include response codes (status), response time, and other
    application-specific context. For example, for HTTP requests, you should log domains
    (part of the URL), user agent, number of requests, HTTP response codes, and in
    some cases complete URL paths. Again, logs should include Kubernetes metadata
    (e.g., namespace, service, labels, pod names, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Process information and socket stats
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, these stats are not part of typical observability implementations,
    but we recommend that you consider collecting these stats as they provide a more
    comprehensive view of the Kubernetes cluster operation. For example, if you can
    get information about processes (that run in a pod), this can be an excellent
    way to correlate with application performance data (e.g., co-relating memory usage,
    or garbage collection events in a Java-based application to response time and
    network activity initiated by the process). Socket stats are details of a TCP
    flow between two endpoints (e.g., network round-trip time, TCP congestion windows,
    TCP retransmits, etc.). These stats when associated with pods can provide a view
    into the impact of the underlying network on pod-to-pod communication.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered what you need to collect for a complete observability
    solution, let’s look at the tools and techniques available to implement collection.
    [Figure 5-3](#reference_implementation_for_collection) is an example reference
    implementation to show how you can implement collection on a node in your Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-3](#reference_implementation_for_collection) shows a node in your
    Kubernetes cluster that has applications deployed as services, deployments, and
    pods in namespaces as you would see in a typical Kubernetes cluster. In order
    to facilitate collection, a few components are added as shown in the observability
    components section, and it shows a few additions to the Linux kernel to facilitate
    collection. Let’s explore the functions of each of these components.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Reference implementation for collection on a node
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linux Kernel Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Linux kernel offers several options that you can use to help with data
    collection. It is very important that the tool you use leverages these tools instead
    of focusing on processing raw logs that are generated by other tools:'
  prefs: []
  type: TYPE_NORMAL
- en: eBPF programs and kprobes
  prefs: []
  type: TYPE_NORMAL
- en: eBPF stands for extended Berkley Packet Filter. It is an exciting technology
    that can be used for collection and observability. It was originally designed
    for packet filtering, but was then extended to allow adding programs to various
    hooks in the kernel to be used as trace points. In case you are using an eBPF-based
    dataplane, the eBPF programs that are managing the packet path will also provide
    packet and flow information. We recommend reading [Brendan Gregg’s blog post “Linux
    Extended BPF (eBPF) Tracing Tools”](https://oreil.ly/s54kG) to understand how
    to use eBPF for performance and tracing. In the context of this discussion, you
    can attach an eBPF program to a kernel probe (kprobe), which is essentially a
    trace point that is triggered and executes the program whenever the code executes
    the function for which the kprobe is registered. The kernel documentation for
    [kprobes](https://oreil.ly/hLziH) provides more details. This is a great way to
    get information from the Linux kernel for observability.
  prefs: []
  type: TYPE_NORMAL
- en: NFLOG and conntrack
  prefs: []
  type: TYPE_NORMAL
- en: If you are using the standard Linux networking dataplane (iptables-based), there
    are tools available to track packets and network flows. We recommend using NFLOG,
    which is a mechanism to be used in conjunction with iptables to log packets. You
    can review the details in the iptables documentation; at a high level NFLOG can
    be set as a target for an iptables rule, and it will log packets via a netlink
    socket on a multicast address that a user space process can subscribe to and collect
    packets from. Conntrack is another module used in conjunction with iptables to
    query the connection state of a packet or a flow, and it can be used to update
    statistics for a flow.
  prefs: []
  type: TYPE_NORMAL
- en: We recommend you review options (e.g., Net Filter) that the Linux kernel provides
    and leverage them in sensors that are used to collect information. This is very
    important as it will be an efficient way to collect data, since these options
    provided by the Linux kernel are highly optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Observability Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understand how to collect data from the Linux kernel, let’s look
    at how this data needs to be processed in user space to ensure we have an effective
    observability solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Log collector
  prefs: []
  type: TYPE_NORMAL
- en: This is a very important component in the system. The goal of this component
    is to add context from the Kubernetes cluster to the data collected from other
    sensors—for example, to add pod metadata (name, namespace, label, etc.) to source
    and destination IP addresses, respectively, from a network flow. This is how you
    can add Kubernetes context to raw network flow logs. Likewise, any data you collect
    from kernel probes can also be enriched by adding relevant Kubernetes metadata.
    This way you can have log data that associates activity in the kernel to objects
    in your Kubernetes cluster (e.g., pods, services, deployments). It is critical
    for you to be able derive insights about your Kubernetes cluster operation. Please
    note that this component is something you need to implement, or you must ensure
    that the tool you choose for observability has this functionality. It is a critical
    part of your observability implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy (proxy)
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the importance of having a collection of application-specific data,
    and for this we recommend that you use [Envoy](https://oreil.ly/0niF8), a well-known
    proxy that is used to analyze application protocols and log application transaction
    flows (e.g., HTTP transactions on a single HTTP connection). Please note that
    Envoy can be used as a sidecar pattern where it attaches to every pod as a sidecar
    and tracks packets to/from the pod. It can also be deployed as a daemonset (a
    transparent proxy) where you can use the dataplane to redirect traffic to pass
    through an envoy instance running on the host. We strongly recommend using Envoy
    with this latter configuration, as using the sidecar pattern has security concerns
    and can be disruptive to applications. In the context of this discussion, the
    Envoy daemonset will be the source of application flow logs to the log collector.
    The log collector can now use the pod metadata (name, namespace, labels, deployments,
    services, IP addresses) to correlate this data with the data received from the
    kernel and further enrich it with application data.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd
  prefs: []
  type: TYPE_NORMAL
- en: Note that the data collection discussed so far is processed by the log collector
    on every node in the cluster. You need to implement a mechanism to send the data
    from all nodes to a datastore or security information and event management (SIEM),
    where it can be picked up by analytics and visualization tools. [Fluentd](https://oreil.ly/11s22)
    is an excellent option to send collected data to the datastore of your choice.
    It offers excellent integrations and is a tool that is Kubernetes native. There
    are other options available, but we recommend you use Fluentd for shipping collected
    log data to a data store.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed how you collect flow logs; now we need a component for the collection
    of metrics and alerting. Prometheus, a Kubernetes-native tool, is a great choice
    for metrics collection and alerting. It’s deployed as endpoints that scrape metrics
    and send them to a time-series database that’s a part of the Prometheus server
    for analysis and query. You can also define alerts for data sent to the Prometheus
    server. It’s a widely used option and has integrations to dashboards and alerting
    tools. We recommend you consider it as an option for your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that this discussion has given you an idea of how you can implement
    data collection for your Kubernetes cluster. Now let’s look at aggregation and
    correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation and Correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section we covered data collection and discussed how you can
    collect data from various sources in your cluster (API server, network flows,
    kernel probes, application flows). This is very useful, but we still need to address
    the concern of data volume if we keep the collection at pod-level granularity.
    Another thing to note is that the data volume concern multiples if we keep data
    from various sources separate and then associate it at query time. You can say
    that it’s better to keep as much raw data as possible, and there are efficient
    tools to query and aggregate data after collection (offline), so why not use that
    approach? Yes, that is a valid point, but there are a couple of things to think
    about. The large volume of data would mean aggregation and query time joins of
    data will be resource-intensive (it can very well be more expensive to operate
    your data collection system than your Kubernetes cluster!). Also, given the ephemeral
    nature of Kubernetes (pod life cycles can be very short), the latency in analyzing
    data offline prevents any kind of reasonable response to data collected to mitigate
    the issue reported by the data. In some cases, if correlation is not done at collection
    time, it will not be possible to associate two different collections. For example,
    you cannot collect a list of policies and a list of flows and then associate the
    policy with a flow offline without rerunning the policy evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed the declarative nature of Kubernetes and how a deployment
    and a service are higher-level constructs than a pod. In such a scenario, we recommend
    that you consider the aggregation of data at a deployment or a service level.
    This means data from all pods for a service is aggregated; you would collect data
    between deployments and data to services as a default option. This will give you
    the right level of granularity. You can provide an option to reduce the aggregation
    to collect pod-level data as a drill-down action or in response to an event. This
    way you can address the concern about large amounts of log data collected and
    the associated processing cost. Also, the data collection is more Kubernetes-native
    as Kubernetes monitors deployments/services as a unit and makes adjustments to
    ensure the deployment/service is operating as per the specification.
  prefs: []
  type: TYPE_NORMAL
- en: In the data collection section we discussed the log collector component that
    receives data from various sources. It could be used as a source to correlate
    data at collection time, so you don’t have to do any additional correlation after
    data collection, and you also benefit from not having to collect redundant data
    for each source. For example, if the kprobe in the kernel collects socket data
    for five-tuple (IP addresses, ports, protocol), and the NFLOG provides other information
    like bytes and packets for the same five-tuple, the log collector can create a
    single log with the five-tuple, the Kubernetes metadata, the network flow data,
    and the socket statistics. This will provide logs with very high context and low
    occupancy for collection and processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s go back to the Google online boutique example and see a sample of
    what a log will look like with aggregation and correlation of kernel and network
    flow data. The sample log is generated using the collection and aggregation concepts
    described previously for a transaction between the frontend service and the currencyservice
    of the application. It is a gRPC-based transaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an example of a flow log from Calico Enterprise. There are a few things
    to note about the log: It aggregates data from all pods backing the frontend service
    (`frontend-6f794fbff7-*`) and all pods belonging to the currencyservice (`currencyservice-7fd6c64-*`).
    The data from the kprobe and socket statistics are aggregated as mean, min, and
    max for each metric for the data between services. The process ID and the process
    name received from the kernel are correlated with the other data, and we also
    see the network policy action and the network policies impacting the flow correlated
    with other data. This is an example of what you want to achieve for data collection
    in your Kubernetes cluster!'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered how to collect, aggregate, and correlate data in a
    Kubernetes-native manner, let’s explore visualization of data.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some great tools that support the visualization of the data collected.
    For example, Prometheus offers an integration with Grafana that provides very
    good dashboards to visualize data. There are also some commercial tools like Datadog,
    New Relic, and Calico Enterprise that support the collection and visualization
    of data. We will cover a few common visualizations that are useful for Kubernetes
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Service Graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a representation of your Kubernetes cluster as a graph showing services
    in a Kubernetes cluster and interactions between them. If we go back to the Google
    microservices online boutique example, [Figure 5-4](#service_graph_representation_of_the_onl)
    shows the online boutique application implemented and represented as a service
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-4](#service_graph_representation_of_the_onl) is a visualization of
    the online boutique namespace as a service graph, with the nodes representing
    services and pods backing a service or a group of pods either standalone or as
    a part of a deployment. The edges show network activity and policy action. The
    graph is interactive and allows you to pick a service (e.g., frontend service)
    and allows the viewing of detailed logs collected for the service. [Figure 5-5](#detailed_view_of_the_frontend_microserv)
    shows a summarized view of all collected data for the service selected (frontend).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-5](#detailed_view_of_the_frontend_microserv) shows a detailed view
    of the frontend service as a drill-down—it shows information from all sources
    in one view, so it’s very easy to analyze the operation of the service.'
  prefs: []
  type: TYPE_NORMAL
- en: Service graph is a very common pattern to represent Kubernetes cluster topology.
    There are several tools that provide this view, such as Kiali, Datadog, and Calico
    Enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Service graph representation of the online boutique application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/ksao_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Detailed view of the frontend microservice
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visualization of Network Flows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 5-6](#network_flow_visualization) shows a common pattern used to visualize
    flows. This is ring-based visualization, where each ring represents an aggregation
    level. In the example shown in [Figure 5-5](#detailed_view_of_the_frontend_microserv),
    the outermost ring represents a namespace and all flows within the namespaces.
    Selecting a ring in the middle shows all flows for a service, and selecting the
    innermost ring shows all flows for pods backing the service. The panel on the
    right is a selector to enable more-granular views using filtering and details
    like flows and policy action for the selection. This is an excellent way to visualize
    network flows in your cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ksao_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Network flow visualization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section we have covered some common visualization patterns and tried
    to show how they can be applied to Kubernetes. Please note that there are several
    visualizations that can be applied to Kubernetes; these are examples to show how
    you can represent data collected in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered data collection, aggregation, correlation, and visualization,
    let’s explore some advanced topics to utilize the data collected to derive insights
    into the operation of the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Analytics and Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we will explore analytics applications that leverage the collection,
    aggregation, and correlation components to help provide additional insights. Note
    that there are many applications that can be built to leverage the context-rich
    data in a Kubernetes cluster. We cover some applications as examples.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explained distributed tracing before and discussed its importance in a microservices-based
    architecture, where it is critical to trace a single user request across multiple
    transactions that need to happen between various microservices. There are two
    well-known approaches to implementing distributed tracing,
  prefs: []
  type: TYPE_NORMAL
- en: Instrument transaction request headers
  prefs: []
  type: TYPE_NORMAL
- en: In this method the HTTP headers are instrumented with a request ID, and the
    request ID is preserved in headers across calls to various other services. [Envoy](https://oreil.ly/jprHR)
    is a very popular tool used to implement distributed tracing. It supports integrations
    with other well-known application tracers like Lightstep and AWS X-Ray. We recommend
    that you use Envoy if you are fine with instrumenting applications to add and
    preserve the request ID across calls between microservices.
  prefs: []
  type: TYPE_NORMAL
- en: eBPF and kprobes
  prefs: []
  type: TYPE_NORMAL
- en: In the method described for using Envoy, there is a change required to the application
    traffic. It is possible to implement distributed tracing for service-to-service
    calls using eBPF and Linux kernel probes. You can attach eBPF programs to kprobes/uprobes
    and other trace points in the kernel and build a distributed tracing application.
    Note the detailed implementation of such an application is beyond the scope of
    this book, but we wanted to mention this as an option for distributed tracing
    in case you are wary of altering application traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered distributed tracing, let’s look at how you can implement
    packet capture in your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Packet Capture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your Kubernetes cluster we recommend that you implement or pick a tool that
    supports raw packet captures between pods. The tool should support a selector-based
    packet capture (e.g., pod labels) and role-based access control to enable and
    view packet captures. This is a simple yet very effective feature that can be
    used as a response action to an event (e.g., increased application latency) to
    analyze raw packet flows to understand the issue and find the root cause. In order
    to implement raw packet captures, we recommend using [libpcap](https://oreil.ly/c2UFJ),
    which supports the ability to capture packets on an interface on Linux systems.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we covered what observability is and how to implement it for
    your Kubernetes cluster. The following are the highlights of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring needs to be a part of your observability strategy; monitoring alone
    is not sufficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to leverage the declarative nature of Kubernetes when you implement
    an observability solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key components for implementing observability for your Kubernetes cluster
    are log collection, log aggregation and correlation, visualization, distributed
    tracing, and analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You must implement your observability using a tool that is native to Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should use tools available in the Linux kernel to drive efficient collection
    and aggregation of data (e.g., NFLOG, eBPF-based probes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
