<html><head></head><body><section data-pdf-bookmark="Chapter 6. Service Routing" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter6">&#13;
<h1><span class="label">Chapter 6. </span>Service Routing</h1>&#13;
&#13;
&#13;
<p>Service routing is a crucial capability of a Kubernetes-based platform.<a data-primary="service routing" data-type="indexterm" id="ix_serrt"/> While the container networking layer takes care of the low-level primitives that connect Pods, developers need higher-level mechanisms to interconnect services (i.e., east-west service routing) and to expose applications to their clients (i.e., north-south service routing). Service routing encompasses three concerns that provide such mechanisms: Services, Ingress, and service mesh.<a data-primary="routing" data-secondary="service" data-see="service routing" data-type="indexterm" id="idm45611989898568"/></p>&#13;
&#13;
<p>Services provide a way to treat a set of Pods as a single unit or network service.<a data-primary="Services" data-type="indexterm" id="idm45611989896936"/> They provide load balancing and routing features that enable horizontal scaling of applications across the cluster. Furthermore, Services offer service discovery mechanisms that applications can use to discover and interact with their dependencies. Finally, Services also provide layer 3/4 mechanisms to expose workloads to network clients outside of the cluster.</p>&#13;
&#13;
<p>Ingress handles north-south routing in the cluster.<a data-primary="Ingress" data-type="indexterm" id="idm45611989895336"/> It serves as an entry point into workloads running in the cluster, mainly HTTP and HTTPS services. Ingress provides layer 7 load balancing capabilities that enable more granular traffic routing than Services. The load balancing of traffic is handled by an Ingress controller, which must be installed in the cluster. Ingress controllers leverage proxy technologies such as Envoy, NGINX, or HAProxy.<a data-primary="proxy technologies" data-type="indexterm" id="idm45611989894088"/> The controller gets the Ingress configuration from the Kubernetes API and configures the proxy accordingly.</p>&#13;
&#13;
<p>A service mesh is a service routing layer that provides advanced routing, security, and observability features.<a data-primary="service meshes" data-type="indexterm" id="idm45611989892648"/> It is mainly concerned with east-west service routing, but some implementations can also handle north-south routing.<a data-primary="proxies" data-secondary="service mesh communication via" data-type="indexterm" id="idm45611989891688"/> Services in the mesh communicate with each other through proxies that augment the connection. The use of proxies makes meshes compelling, as they enhance workloads without changes to source code.</p>&#13;
&#13;
<p>This chapter digs into these service routing capabilities, which are critical in production Kubernetes platforms. First, we will discuss Services, the different Service types, and how they are implemented. Next, we will explore Ingress, Ingress controllers, and the different considerations to take into account when running Ingress in production. Finally, we will cover service meshes, how they work on Kubernetes, and considerations to make when adopting a service mesh in a production platform.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Services" data-type="sect1"><div class="sect1" id="kubernetes_services">&#13;
<h1>Kubernetes Services</h1>&#13;
&#13;
<p>The Kubernetes Service is foundational to service routing.<a data-primary="Services" data-type="indexterm" id="ix_SerK"/><a data-primary="service routing" data-secondary="Kubernetes Services" data-type="indexterm" id="ix_serrtKS"/> The Service is a network abstraction that provides basic load balancing across several Pods. In most cases, workloads running in the cluster use Services to communicate with each other. Using Services instead of Pod IPs is preferred because of the fungible nature of Pods.</p>&#13;
&#13;
<p>In this section, we will review Kubernetes Services and the different Service types. We will also look at Endpoints, another Kubernetes resource that is intimately related to Services. We will then dive into the Service implementation details and discuss kube-proxy. Finally, we will discuss Service Discovery and considerations to make for the in-cluster DNS server.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Service Abstraction" data-type="sect2"><div class="sect2" id="idm45611989884056">&#13;
<h2>The Service Abstraction</h2>&#13;
&#13;
<p>The Service is a core API resource in Kubernetes that load balances traffic across multiple Pods.<a data-primary="Services" data-secondary="Service abstraction" data-type="indexterm" id="idm45611989882440"/> The Service does load balancing at the L3/L4 layers in the OSI model.<a data-primary="load balancing" data-secondary="by Services" data-type="indexterm" id="idm45611989881336"/> It takes a packet with a destination IP and port and forwards it to a backend Pod.</p>&#13;
&#13;
<p>Load balancers typically have a frontend and a backend pool. Services do as well. The frontend of a Service is the ClusterIP.<a data-primary="ClusterIP Service" data-type="indexterm" id="idm45611989879656"/> The ClusterIP is a virtual IP address (VIP) that is accessible from within the cluster.<a data-primary="virtual IP address (VIP)" data-type="indexterm" id="idm45611989878712"/><a data-primary="IP addresses" data-secondary="virtual IP address (VIP)" data-type="indexterm" id="idm45611989878072"/> Workloads use this VIP to communicate with the Service.<a data-primary="Pods" data-secondary="Service backend pool" data-type="indexterm" id="idm45611989876984"/> The backend pool is a collection of Pods that satisfy the Service’s Pod selector. These Pods receive the traffic destined for the Cluster IP. <a data-type="xref" href="#the_service_has_a_frontend_and_a_backend_pool">Figure 6-1</a> depicts the frontend of a Service and its backend pool.</p>&#13;
&#13;
<figure><div class="figure" id="the_service_has_a_frontend_and_a_backend_pool">&#13;
<img alt="prku 0601" src="assets/prku_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>The Service has a frontend and a backend pool. The frontend is the ClusterIP, while the backend is a set of Pods.</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service IP Address Management" data-type="sect3"><div class="sect3" id="idm45611989872600">&#13;
<h3>Service IP Address Management</h3>&#13;
&#13;
<p>As we discussed in the previous chapter, you configure two ranges of IP addresses when deploying Kubernetes. <a data-primary="Services" data-secondary="IP address management" data-type="indexterm" id="idm45611989871176"/><a data-primary="IP address management (IPAM)" data-secondary="Service" data-type="indexterm" id="idm45611989870200"/>On the one hand, the Pod IP range or CIDR block provides IP addresses to each Pod in the cluster.<a data-primary="Classless Inter-Domain Routing (CIDR)" data-secondary="Service CIDR block" data-type="indexterm" id="idm45611989868952"/> On the other hand, the Service CIDR block provides the IP addresses for Services in the cluster. This CIDR is the range that Kubernetes uses to assign ClusterIPs to Services.</p>&#13;
&#13;
<p>The API server handles the IP Address Management (IPAM) for Kubernetes Services.<a data-primary="API server" data-secondary="IP address management for Services" data-type="indexterm" id="idm45611989867288"/> When you create a Service, the API Server (with the help of etcd) allocates an IP address from the Service CIDR block and writes it to the Service’s ClusterIP field.</p>&#13;
&#13;
<p>When creating Services, you can also specify the ClusterIP in the Service specification. In this case, the API Server makes sure that the requested IP address is available and within the Services CIDR block. With that said, explicitly setting ClusterIPs is an antipattern.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Service resource" data-type="sect3"><div class="sect3" id="idm45611989865048">&#13;
<h3>The Service resource</h3>&#13;
&#13;
<p>The Service resource<a data-primary="Services" data-secondary="Service resource" data-type="indexterm" id="idm45611989863720"/> contains the configuration of a given Service, including the name, type, ports, etc. <a data-type="xref" href="#service_definitions_that_exposes_nginz_on_a_clusterip">Example 6-1</a> is an example Service definition<a data-primary="NGNIX" data-secondary="Service definition exposing NGINX on ClusterIP" data-type="indexterm" id="idm45611989861752"/><a data-primary="ClusterIP Service" data-secondary="Service definition exposing NGINX on" data-type="indexterm" id="idm45611989860776"/> in its YAML representation named <code>nginx</code>.</p>&#13;
<div data-type="example" id="service_definitions_that_exposes_nginz_on_a_clusterip">&#13;
<h5><span class="label">Example 6-1. </span>Service definition that exposes NGINX on a ClusterIP</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Service</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">selector</code><code class="p">:</code><code> </code><a class="co" href="#callout_service_routing_CO1-1" id="co_service_routing_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>    </code><code class="nt">app</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>&#13;
</code><code>  </code><code class="nt">ports</code><code class="p">:</code><code> </code><a class="co" href="#callout_service_routing_CO1-2" id="co_service_routing_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="nt">protocol</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TCP</code><code> </code><a class="co" href="#callout_service_routing_CO1-3" id="co_service_routing_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>      </code><code class="nt">port</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code><code> </code><a class="co" href="#callout_service_routing_CO1-4" id="co_service_routing_CO1-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>      </code><code class="nt">targetPort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">8080</code><code> </code><a class="co" href="#callout_service_routing_CO1-5" id="co_service_routing_CO1-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>  </code><code class="nt">clusterIP</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">172.21.219.227</code><code> </code><a class="co" href="#callout_service_routing_CO1-6" id="co_service_routing_CO1-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>  </code><code class="nt">type</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ClusterIP</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO1-1" id="callout_service_routing_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The Pod selector. Kubernetes uses this selector to find the Pods that belong to this Service.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO1-2" id="callout_service_routing_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Ports that are accessible through the Service.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO1-3" id="callout_service_routing_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Kubernetes supports TCP, UDP, and SCTP protocols in Services.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO1-4" id="callout_service_routing_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Port where the Service can be reached.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO1-5" id="callout_service_routing_CO1-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Port where the backend Pod is listening, which can be different than the port exposed by the Service (the <code>port</code> field above).</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO1-6" id="callout_service_routing_CO1-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>Cluster IP that Kubernetes allocated for this Service.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>The <a data-primary="Pods" data-secondary="Service Pod selector" data-type="indexterm" id="idm45611989743528"/>Service’s Pod selector determines the Pods that belong to the Service. The Pod selector is a collection of key/value pairs that Kubernetes evaluates against Pods in the same Namespace as the Service. If a Pod has the same key/value pairs in their labels, Kubernetes adds the Pod’s IP address to the backend pool of the Service. The management of the backend pool is handled by the Endpoints controller through Endpoints resources. We will discuss Endpoints in more detail later in this chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service types" data-type="sect3"><div class="sect3" id="idm45611989741784">&#13;
<h3>Service types</h3>&#13;
&#13;
<p>Up to this point, we have mainly talked about the <a data-primary="Services" data-secondary="Service types" data-type="indexterm" id="ix_Sertyp"/>ClusterIP Service, which is the default Service type. Kubernetes offers multiple Service types that offer additional features besides the Cluster IP. In this section, we will discuss each Service type and how they are useful.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ClusterIP" data-type="sect4"><div class="sect4" id="idm45611989738824">&#13;
<h4>ClusterIP</h4>&#13;
&#13;
<p>We have already discussed this Service type in the previous sections.<a data-primary="Services" data-secondary="Service types" data-tertiary="ClusterIP" data-type="indexterm" id="idm45611989737688"/><a data-primary="ClusterIP Service" data-type="indexterm" id="idm45611989736600"/> To recap, the ClusterIP Service creates a virtual IP address (VIP) that is backed by one or more Pods.<a data-primary="virtual IP address (VIP)" data-type="indexterm" id="idm45611989735752"/><a data-primary="IP addresses" data-secondary="virtual IP address (VIP)" data-type="indexterm" id="idm45611989735144"/> Usually, the VIP is available only to workloads running inside the cluster. <a data-type="xref" href="#the_clusterip_service_is_a_vip_that_is_accessible_to_workloads_running_within_the_cluster">Figure 6-2</a> shows a ClusterIP Service.</p>&#13;
&#13;
<figure><div class="figure" id="the_clusterip_service_is_a_vip_that_is_accessible_to_workloads_running_within_the_cluster">&#13;
<img alt="prku 0602" src="assets/prku_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>The ClusterIP Service is a VIP that is accessible to workloads running within the cluster.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="NodePort" data-type="sect4"><div class="sect4" id="idm45611989731096">&#13;
<h4>NodePort</h4>&#13;
&#13;
<p>The NodePort Service is useful when you need to expose a Service to network clients outside of the cluster, such as existing applications running in VMs or users of a web application.<a data-primary="NodePort Service" data-type="indexterm" id="idm45611989729496"/><a data-primary="Services" data-secondary="Service types" data-tertiary="NodePort" data-type="indexterm" id="idm45611989728792"/></p>&#13;
&#13;
<p>As the name suggests, the NodePort Service exposes the Service on a port across all cluster nodes. The port is assigned randomly from a configurable port range. Once assigned, all nodes in the cluster listen for connections on the given port. <a data-type="xref" href="#the_nodeport_service_opens_a_random_port_on_all_cluster_nodes">Figure 6-3</a> shows a NodePort Service.</p>&#13;
&#13;
<figure><div class="figure" id="the_nodeport_service_opens_a_random_port_on_all_cluster_nodes">&#13;
<img alt="prku 0603" src="assets/prku_0603.png"/>&#13;
<h6><span class="label">Figure 6-3. </span>The NodePort Service opens a random port on all cluster nodes. Clients outside of the cluster can reach the Service through this port.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The primary challenge with NodePort Services is that clients need to know the Service’s node port number and the IP address of at least one cluster node to reach the Service. This is problematic because nodes can fail or be removed from the cluster.</p>&#13;
&#13;
<p>A common way to solve this challenge is to use an external load balancer in front of the NodePort Service.<a data-primary="load balancers" data-secondary="external, using in front of NodePort Service" data-type="indexterm" id="idm45611989722856"/> With this approach, clients don’t need to know the IP addresses of cluster nodes or the Service’s port number. Instead, the load balancer functions as the single entry point to the Service.</p>&#13;
&#13;
<p>The downside to this solution is that you need to manage external load balancers and update their configuration constantly. Did a developer create a new NodePort Service? Create a new load balancer. Did you add a new node to the cluster? Add the new node to the backend pool of all load balancers.</p>&#13;
&#13;
<p>In most cases, there are better alternatives to using a NodePort Service. The LoadBalancer Service, which we’ll discuss next, is one of those options. Ingress controllers are another option, which we’ll explore later in this chapter in <a data-type="xref" href="#ingress">“Ingress”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="LoadBalancer" data-type="sect4"><div class="sect4" id="idm45611989719112">&#13;
<h4>LoadBalancer</h4>&#13;
&#13;
<p>The LoadBalancer Service builds upon the NodePort Service to address some of its downsides.<a data-primary="LoadBalancer Service" data-type="indexterm" id="idm45611989717496"/><a data-primary="Services" data-secondary="Service types" data-tertiary="LoadBalancer" data-type="indexterm" id="idm45611989716792"/> At its core, the LoadBalancer Service is a NodePort Service under the hood. However, the LoadBalancer Service has additional functionality that is satisfied by a controller.<a data-primary="controllers" data-secondary="LoadBalancer Service" data-type="indexterm" id="idm45611989715256"/></p>&#13;
&#13;
<p>The controller, also known<a data-primary="cloud provider integration" data-type="indexterm" id="idm45611989713928"/> as a cloud provider integration, is responsible for automatically gluing the NodePort Service with an external load balancer. In other words, the controller takes care of creating, managing, and configuring external load balancers in response to the configuration of LoadBalancer Services in the cluster. The controller does this by interacting with APIs that provision or configure load balancers. This interaction is depicted in <a data-type="xref" href="#the_loadbalancer_service_leverages_a_cloud_provider_integration">Figure 6-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="the_loadbalancer_service_leverages_a_cloud_provider_integration">&#13;
<img alt="prku 0604" src="assets/prku_0604.png"/>&#13;
<h6><span class="label">Figure 6-4. </span>The LoadBalancer Service leverages a cloud provider integration to create an external load balancer, which forwards traffic to the node ports. At the node level, the Service is the same as a NodePort.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Kubernetes has built-in controllers for several cloud providers, including Amazon Web Services (AWS), Google Cloud, and Microsoft Azure.<a data-primary="Azure, Kubernetes built-in controller for" data-type="indexterm" id="idm45611989708920"/><a data-primary="AWS (Amazon Web Services)" data-secondary="Kubernetes built-in controller for" data-type="indexterm" id="idm45611989708184"/><a data-primary="Google Cloud, Kubernetes built-in controller for" data-type="indexterm" id="idm45611989707208"/><a data-primary="cloud computing" data-secondary="Kubernetes cloud provider controllers" data-type="indexterm" id="idm45611989706504"/> These integrated controllers are usually called in-tree cloud providers, as they are implemented inside the Kubernetes source code tree.<a data-primary="Microsoft Azure" data-see="Azure" data-type="indexterm" id="idm45611989705256"/></p>&#13;
&#13;
<p>As the Kubernetes project evolved, out-of-tree cloud providers emerged as an alternative to in-tree providers. Out-of-tree providers enabled load balancer vendors to provide their implementations of the LoadBalancer Service control loop. At this time, Kubernetes supports both in-tree and out-of-tree providers. However, the community is quickly adopting out-of-tree providers, given that the in-tree providers are deprecated.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611989703352">&#13;
<h5>LoadBalancer Services Without a Cloud Provider</h5>&#13;
<p>If you run Kubernetes without a cloud provider integration, you will notice that LoadBalancer Services remain in the “Pending” state. A great example of this problem is bare metal deployments. If you are running your platform on bare-metal, you might be able to leverage MetalLB to support LoadBalancer Services.<a data-primary="MetalLB" data-type="indexterm" id="idm45611989701544"/></p>&#13;
&#13;
<p><a href="https://metallb.universe.tf">MetalLB</a> is an open source project that provides support for LoadBalancer Services on bare metal. MetalLB runs in the cluster, and it can operate in one of two modes. In layer 2 mode, one of the cluster nodes becomes the leader and starts responding to ARP requests for the external IPs of LoadBalancer Services. Once traffic reaches the leader, kube-proxy handles the rest. If the leader fails, another node in the cluster takes over and begins handling requests. A big downside of this mode is that it does not offer true load balancing capabilities, given that a single node is the one responding to the ARP requests.</p>&#13;
&#13;
<p>The second mode of operation<a data-primary="BGP (Border Gateway Protocol)" data-secondary="MetalLB using BGP peer with network routers" data-type="indexterm" id="idm45611989698840"/> uses BGP to peer with your network routers. Through the peering relationship, MetalLB advertises the external IPs of LoadBalancer Services. Similar to the layer 2 mode, kube-proxy takes care of routing the traffic from one of the cluster nodes to the backend Pod. The BGP mode addresses the limitations of the layer 2 mode, given that traffic is load balanced across multiple nodes instead of a single, leader node.</p>&#13;
&#13;
<p>If you need to support LoadBalancer Services, MetalLB might provide a viable path forward. In most cases, however, you can get away without supporting LoadBalancer Services. For example, if a large proportion of your applications are HTTP services, you can leverage an Ingress controller to load balance and bring traffic into these applications.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ExternalName" data-type="sect4"><div class="sect4" id="idm45611989696264">&#13;
<h4>ExternalName</h4>&#13;
&#13;
<p>The ExternalName Service type does not perform any kind of load balancing or proxying.<a data-primary="Services" data-secondary="Service types" data-tertiary="ExternalName" data-type="indexterm" id="idm45611989694936"/><a data-primary="ExternalName Service" data-type="indexterm" id="idm45611989693688"/> Instead, an ExternalName Service is primarily a service discovery construct implemented in the cluster’s DNS. An ExternalName Service maps a cluster Service to a DNS name. Because there is no load balancing involved, Services of this type lack ClusterIPs.</p>&#13;
&#13;
<p>ExternalName Services are useful in different ways. Piecemeal application migration efforts, for example, can benefit from ExternalName Services. If you migrate components of an application to Kubernetes while leaving some of its dependencies outside, you can use an ExternalName Service as a bridge while you complete the migration. Once you migrate the entire application, you can change the Service type to a ClusterIP without having to change the application deployment.</p>&#13;
&#13;
<p>Even though useful in creative ways, the ExternalName Service is probably the least common Service type in use.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Headless Service" data-type="sect4"><div class="sect4" id="idm45611989690936">&#13;
<h4>Headless Service</h4>&#13;
&#13;
<p>Like the ExternalName Service, the Headless Service type does not allocate a ClusterIP or provide any load balancing. <a data-primary="Services" data-secondary="Service types" data-tertiary="Headless" data-type="indexterm" id="idm45611989689480"/><a data-primary="Headless Service" data-type="indexterm" id="idm45611989688232"/>The Headless Service merely functions as a way to register a Service and its Endpoints in the Kubernetes API and the cluster’s DNS server.</p>&#13;
&#13;
<p>Headless Services are useful when applications need to connect to specific replicas or Pods of a service. Such applications can use service discovery to find all the Pod IPs behind the Service and then establish connections to specific Pods.<a data-primary="Services" data-secondary="Service types" data-startref="ix_Sertyp" data-type="indexterm" id="idm45611989686632"/></p>&#13;
</div></section>&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Supported communication protocols" data-type="sect3"><div class="sect3" id="idm45611989685128">&#13;
<h3>Supported communication protocols</h3>&#13;
&#13;
<p>Kubernetes Services support a specific set of protocols: TCP, UDP, and SCTP. Each port listed in a Service resource specifies the port number and the protocol.<a data-primary="Services" data-secondary="communication protocols supported by" data-type="indexterm" id="idm45611989683320"/><a data-primary="UDP" data-secondary="support by Kubernetes Services" data-type="indexterm" id="idm45611989682376"/><a data-primary="SCTP, support by Kubernetes services" data-type="indexterm" id="idm45611989681464"/><a data-primary="TCP" data-secondary="support by Kubernetes Services" data-type="indexterm" id="idm45611989680776"/><a data-primary="communication protocols supported by Services" data-type="indexterm" id="idm45611989679816"/> Services can expose multiple ports with different protocols.<a data-primary="Services" data-secondary="multiple ports and protocols" data-type="indexterm" id="idm45611989678984"/> For example, the following snippet shows the YAML definition of the <code>kube-dns</code> Service.<a data-primary="kube-dns" data-type="indexterm" id="idm45611989677352"/> Notice how the list of ports includes TCP port 53 and UDP port 53:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">k8s-app</code><code class="p">:</code> <code class="l-Scalar-Plain">kube-dns</code>&#13;
    <code class="nt">kubernetes.io/cluster-service</code><code class="p">:</code> <code class="s">"true"</code>&#13;
    <code class="nt">kubernetes.io/name</code><code class="p">:</code> <code class="l-Scalar-Plain">KubeDNS</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">kube-dns</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">kube-system</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">clusterIP</code><code class="p">:</code> <code class="l-Scalar-Plain">10.96.0.10</code>&#13;
  <code class="nt">ports</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">dns</code>&#13;
    <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">53</code>&#13;
    <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">UDP</code>&#13;
    <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">53</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">dns-tcp</code>&#13;
    <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">53</code>&#13;
    <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
    <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">53</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">metrics</code>&#13;
    <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">9153</code>&#13;
    <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">TCP</code>&#13;
    <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">9153</code>&#13;
  <code class="nt">selector</code><code class="p">:</code>&#13;
    <code class="nt">k8s-app</code><code class="p">:</code> <code class="l-Scalar-Plain">kube-dns</code>&#13;
  <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">ClusterIP</code></pre>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611989674376">&#13;
<h5>Protocols and Troubleshooting Services</h5>&#13;
<p>While working with Kubernetes, you might have tried to use <code>ping</code> to troubleshoot Kubernetes Services.<a data-primary="Services" data-secondary="troubleshooting" data-type="indexterm" id="idm45611989589624"/><a data-primary="ping" data-type="indexterm" id="idm45611989588680"/> You probably found that any attempts to ping a Service resulted in 100% packet loss. The problem with using <code>ping</code> is that it uses ICMP datagrams, which are not supported in Kubernetes Services.</p>&#13;
&#13;
<p>Instead of using <code>ping</code>, you must resort to alternative tools when it comes to troubleshooting Services. If you are looking to test connectivity, choose a tool that works with the Service’s protocol. For example, if you need to troubleshoot a web server, you can use <code>telnet</code> to test whether you can establish a TCP connection to the server.<a data-primary="telnet" data-type="indexterm" id="idm45611989585832"/></p>&#13;
&#13;
<p>Another quick way to troubleshoot Services is to verify that the Pod selector is selecting at least one Pod by checking the corresponding Endpoints resource.<a data-primary="Pods" data-secondary="Service Pod selector" data-tertiary="invalid selectors" data-type="indexterm" id="idm45611989584568"/> Invalid selectors are a common issue with Services.</p>&#13;
</div></aside>&#13;
&#13;
<p>As we’ve discussed up to this point, Services load balance traffic across Pods.<a data-primary="load balancing" data-secondary="Services load balancing traffic across Pods" data-type="indexterm" id="idm45611989582680"/> The Service API resource represents the frontend of the load balancer.<a data-primary="Service API" data-type="indexterm" id="idm45611989581480"/> The backend, or the collection of Pods that are behind the load balancer, are tracked by the Endpoints resource and controller, which we will discuss next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Endpoints" data-type="sect2"><div class="sect2" id="idm45611989580376">&#13;
<h2>Endpoints</h2>&#13;
&#13;
<p>The Endpoints resource is another API object that is involved in the implementation of Kubernetes Services.<a data-primary="endpoints" data-type="indexterm" id="ix_Endpt"/><a data-primary="Services" data-secondary="endpoints" data-type="indexterm" id="ix_Serend"/> Every Service resource has a sibling Endpoints resource. If you recall the load balancer analogy, you can think of the Endpoints object as the pool of IP addresses that receive traffic. <a data-type="xref" href="#relationship_between_the_service_and_the_endpoint_resources">Figure 6-5</a> shows the relationship between a Service and an Endpoint.</p>&#13;
&#13;
<figure><div class="figure" id="relationship_between_the_service_and_the_endpoint_resources">&#13;
<img alt="prku 0605" src="assets/prku_0605.png"/>&#13;
<h6><span class="label">Figure 6-5. </span>Relationship between the Service and the Endpoints resources.</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Endpoints resource" data-type="sect3"><div class="sect3" id="idm45611989573304">&#13;
<h3>The Endpoints resource</h3>&#13;
&#13;
<p>An example Endpoints <a data-primary="Services" data-secondary="endpoints" data-tertiary="Endpoints resource" data-type="indexterm" id="idm45611989571896"/><a data-primary="NGNIX" data-secondary="Endpoints resource for nginx Service" data-type="indexterm" id="idm45611989570616"/><a data-primary="endpoints" data-secondary="Endpoints resource" data-type="indexterm" id="idm45611989569704"/>resource for the <code>nginx</code> Service in <a data-type="xref" href="#service_definitions_that_exposes_nginz_on_a_clusterip">Example 6-1</a> might look like this (some extraneous fields have been removed):</p>&#13;
&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Endpoints&#13;
metadata:&#13;
  labels:&#13;
    run: nginx&#13;
  name: nginx&#13;
  namespace: default&#13;
subsets:&#13;
- addresses:&#13;
  - ip: 10.244.0.10&#13;
    nodeName: kube03&#13;
    targetRef:&#13;
      kind: Pod&#13;
      name: nginx-76df748b9-gblnn&#13;
      namespace: default&#13;
  - ip: 10.244.0.9&#13;
    nodeName: kube04&#13;
    targetRef:&#13;
      kind: Pod&#13;
      name: nginx-76df748b9-gb7wl&#13;
      namespace: default&#13;
  ports:&#13;
  - port: 8080&#13;
    protocol: TCP</pre>&#13;
&#13;
<p>In this example, there are two Pods backing the <code>nginx</code> Service. Network traffic destined to the <code>nginx</code> ClusterIP is load balanced across these two Pods. Also notice how the port is 8080 and not 80. This port matches the <code>targetPort</code> field specified in the Service. It is the port that the backend Pods are listening on.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Endpoints controller" data-type="sect3"><div class="sect3" id="idm45611989564168">&#13;
<h3>The Endpoints controller</h3>&#13;
&#13;
<p>An interesting thing about the Endpoints resource is that Kubernetes creates it automatically when you create a Service.<a data-primary="endpoints" data-secondary="Endpoints controller" data-type="indexterm" id="idm45611989562488"/><a data-primary="Services" data-secondary="endpoints" data-tertiary="Endpoints controller" data-type="indexterm" id="idm45611989561512"/> This is somewhat different from other API resources that you usually interact with.</p>&#13;
&#13;
<p>The Endpoints controller is responsible for creating and maintaining the Endpoints objects. Whenever you create a Service, the Endpoints controller creates the sibling Endpoints resource. More importantly, it also updates the list of IPs within the Endpoints object as necessary.</p>&#13;
&#13;
<p>The controller uses the Service’s Pod selector to find the Pods that belong to the Service. Once it has the set of Pods, the controller grabs the Pod IP addresses and updates the Endpoints resource accordingly.</p>&#13;
&#13;
<p>Addresses in the Endpoints resource can be in one of two sets: (ready) addresses and notReadyAddresses.<a data-primary="IP addresses" data-secondary="ready and not ready addresses for Pods" data-type="indexterm" id="idm45611989558328"/> The Endpoints controller determines whether an address is ready by inspecting the corresponding Pod’s Ready condition. The Pod’s Ready condition, in turn, depends on multiple factors. One of them, for example, is whether the Pod was scheduled.<a data-primary="Pods" data-secondary="readiness and readiness probes" data-type="indexterm" id="idm45611989556936"/> If the Pod is pending (not scheduled), its Ready condition is false. Ultimately, a Pod is considered ready when it is running and passing its readiness probe.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod readiness and readiness probes" data-type="sect3"><div class="sect3" id="idm45611989555544">&#13;
<h3>Pod readiness and readiness probes</h3>&#13;
&#13;
<p>In the previous section, we discussed how the Endpoints controller determines whether a Pod IP address is ready to accept traffic.<a data-primary="Services" data-secondary="endpoints" data-tertiary="Pod readiness and readiness probes" data-type="indexterm" id="idm45611989554056"/> But how does Kubernetes tell whether a Pod is ready or not?</p>&#13;
&#13;
<p>There are two complementary methods that Kubernetes uses to determine Pod &#13;
<span class="keep-together">readiness</span>:</p>&#13;
<dl>&#13;
<dt>Platform information</dt>&#13;
<dd>&#13;
<p>Kubernetes has information about the workloads under its management. For example, the system knows whether the Pod has been successfully scheduled to a node. It also knows whether the Pod’s containers are up and running.</p>&#13;
</dd>&#13;
<dt>Readiness probes</dt>&#13;
<dd>&#13;
<p>Developers can configure readiness probes on their workloads.<a data-primary="readiness probes" data-type="indexterm" id="idm45611989548280"/> When set, the kubelet probes the workload periodically to determine if it is ready to receive traffic. Probing Pods for readiness is more powerful than determining readiness based on platform information because the probe checks for application-specific concerns. For example, the probe can check whether the application’s internal initialization process has completed.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Readiness probes are essential. Without them, the cluster would route traffic to workloads that might not be able to handle it, which would result in application errors and irritated end users. Ensure that you always define readiness probes in the applications you deploy to Kubernetes. In <a data-type="xref" href="ch14.html#application_considerations_chapter">Chapter 14</a>, we will further discuss readiness probes.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The EndpointSlices resource" data-type="sect3"><div class="sect3" id="idm45611989545000">&#13;
<h3>The EndpointSlices resource</h3>&#13;
&#13;
<p>The EndpointSlices resource is an optimization implemented in Kubernetes v1.16.<a data-primary="endpoints" data-secondary="EndpointSlices resource" data-type="indexterm" id="idm45611989543496"/><a data-primary="Services" data-secondary="endpoints" data-tertiary="EndpointSlices resource" data-type="indexterm" id="idm45611989542520"/><a data-primary="scalability" data-secondary="issues with Endpoints resource in large cluster deployments" data-type="indexterm" id="idm45611989541304"/> It addresses scalability issues that can arise with the Endpoints resource in <a href="https://oreil.ly/H8rHC">large cluster deployments</a>. Let’s review these issues and explore how EndpointSlices help.<a data-primary="large clusters, scalability issues with Endpoints" data-type="indexterm" id="idm45611989539432"/><a data-primary="clusters" data-secondary="large deployments, scalability issues with Endpoints resource" data-type="indexterm" id="idm45611989538648"/></p>&#13;
&#13;
<p>To implement Services and make them routable, each node in the cluster watches the Endpoints API and subscribes for changes. Whenever an Endpoints resource is updated, it must be propagated to all nodes in the cluster to take effect. A scaling event is a good example. Whenever there is a change to the set of Pods in the Endpoints resource, the API server sends the entire updated object to all the cluster nodes.<a data-primary="updates" data-secondary="Endpoints resources" data-type="indexterm" id="idm45611989536840"/><a data-primary="API server" data-secondary="Endpoints resource updates" data-type="indexterm" id="idm45611989535864"/></p>&#13;
&#13;
<p>This approach to handling the Endpoints API does not scale well with larger clusters for multiple reasons:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Large clusters contain many nodes. The more nodes in the cluster, the more updates need to be sent when Endpoints objects change.</p>&#13;
</li>&#13;
<li>&#13;
<p>The larger the cluster, the more Pods (and Services) you can host. As the number of Pods increases, the frequency of Endpoints resource updates also grows.</p>&#13;
</li>&#13;
<li>&#13;
<p>The size of Endpoints resources increases as the number of Pods that belong to the Service grows. Larger Endpoints objects require more network and storage resources.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The EndpointSlices resource fixes these issues by splitting the set of endpoints across multiple resources. Instead of placing all the Pod IP addresses in a single Endpoints resource, Kubernetes splits the addresses across various EndpointSlice objects. By default, EndpointSlice objects are limited to 100 endpoints.</p>&#13;
&#13;
<p>Let’s explore a scenario to better understand the impact of EndpointSlices. Consider a Service with 10,000 endpoints, which would result in 100 EndpointSlice objects. If one of the endpoints is removed (due to a scale-in event, for example), the API server sends the affected EndpointSlice object to each node. Sending a single EndpointSlice with 100 endpoints is much more efficient than sending a single Endpoints resource with thousands of endpoints.</p>&#13;
&#13;
<p>To summarize, the EndpointSlices resource improves the scalability of Kubernetes by splitting a large number of endpoints into a set of EndpointSlice objects. If you are running a platform that has Services with hundreds of endpoints, you might benefit from the EndpointSlice improvements. Depending on your Kubernetes version, the EndpointSlice functionality is opt-in. If you are running Kubernetes v1.18, you must set a feature flag in kube-proxy to enable the use of EndpointSlice resources. Starting with Kubernetes v1.19, the EndpointSlice functionality will be enabled by default.<a data-primary="endpoints" data-startref="ix_Endpt" data-type="indexterm" id="idm45611989528056"/><a data-primary="Services" data-secondary="endpoints" data-startref="ix_Serend" data-type="indexterm" id="idm45611989527080"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service Implementation Details" data-type="sect2"><div class="sect2" id="idm45611989525736">&#13;
<h2>Service Implementation Details</h2>&#13;
&#13;
<p>Until now, we’ve talked about Services, Endpoints, and what they provide to workloads in a Kubernetes cluster.<a data-primary="Services" data-secondary="implementation details" data-type="indexterm" id="ix_Serimp"/> But how does Kubernetes implement Services? How does it all work?</p>&#13;
&#13;
<p>In this section, we will discuss the different approaches available when it comes to realizing Services in Kubernetes. First, we will talk about the overall kube-proxy architecture. Next, we will review the different kube-proxy data plane modes. Finally, we will discuss alternatives to kube-proxy, such as CNI plug-ins that are capable of taking over kube-proxy’s responsibilities.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kube-proxy" data-type="sect3"><div class="sect3" id="idm45611989521736">&#13;
<h3>Kube-proxy</h3>&#13;
&#13;
<p>Kube-proxy is an agent that runs on every cluster node.<a data-primary="kube-proxy" data-type="indexterm" id="idm45611989520408"/><a data-primary="Services" data-secondary="implementation details" data-tertiary="kube-proxy" data-type="indexterm" id="idm45611989519704"/> It is primarily responsible for making Services available to the Pods running on the local node. It achieves this by watching the API server for Services and Endpoints and programming the Linux networking stack (using iptables, for example) to handle packets accordingly.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Historically, kube-proxy acted as a network proxy between Pods running on the node and Services. This is where the kube-proxy name came from. As the Kubernetes project evolved, however, kube-proxy stopped being a proxy and became more of a node agent or localized control plane.</p>&#13;
</div>&#13;
&#13;
<p>Kube-proxy supports three modes of operation: userspace, iptables, and IPVS. The userspace proxy mode is seldom used, since iptables and IPVS are better alternatives. Thus, we will only cover the iptables and IPVS modes in the following sections of this chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kube-proxy: iptables mode" data-type="sect3"><div class="sect3" id="idm45611989515736">&#13;
<h3>Kube-proxy: iptables mode</h3>&#13;
&#13;
<p>The iptables mode is the default kube-proxy mode at the time of writing (Kubernetes v1.18).<a data-primary="kube-proxy" data-secondary="iptables mode" data-type="indexterm" id="idm45611989514088"/><a data-primary="iptables mode (kube-proxy)" data-type="indexterm" id="idm45611989513112"/> It is safe to say that the iptables mode is the most prevalent across cluster installations today.</p>&#13;
&#13;
<p>In the iptables mode, kube-proxy leverages the network address translation (NAT) features of iptables.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ClusterIP Services" data-type="sect4"><div class="sect4" id="idm45611989470712">&#13;
<h4>ClusterIP Services</h4>&#13;
&#13;
<p>To realize ClusterIP Services, kube-proxy programs the Linux kernel’s NAT table to perform Destination NAT (DNAT) on packets destined for Services.<a data-primary="Services" data-secondary="implementation details" data-tertiary="ClusterIP" data-type="indexterm" id="idm45611989469176"/><a data-primary="ClusterIP Service" data-secondary="implementation details" data-type="indexterm" id="idm45611989467928"/><a data-primary="Destination NAT (DNAT)" data-type="indexterm" id="idm45611989466984"/><a data-primary="network address translation (NAT)" data-secondary="Destination NAT (DNAT)" data-type="indexterm" id="idm45611989466312"/> The DNAT rules replace the packet’s destination IP address with the IP address of a Service endpoint (a Pod IP address). Once replaced, the network handles the packet as if it was originally sent to the Pod.</p>&#13;
&#13;
<p>To load balance traffic across <a data-primary="kube-proxy" data-secondary="configuring iptables rules for ClusterIP Service" data-type="indexterm" id="idm45611989464616"/>multiple Service endpoints, kube-proxy uses multiple iptables chains:</p>&#13;
<dl>&#13;
<dt>Services chain</dt>&#13;
<dd>&#13;
<p>Top-level chain that contains a rule for each Service. Each rule checks whether the destination IP of the packet matches the ClusterIP of the Service. If it does, the packet is sent to the Service-specific chain.</p>&#13;
</dd>&#13;
<dt>Service-specific chain</dt>&#13;
<dd>&#13;
<p>Each Service has its iptables chain. This chain contains a rule per Service endpoint. Each rule uses the <code>statistic</code> iptables extension to select a target endpoint randomly. Each endpoint has 1/n probability of being selected, where n is the number of endpoints. Once selected, the packet is sent to the Service endpoint chain.</p>&#13;
</dd>&#13;
<dt>Service endpoint chain</dt>&#13;
<dd>&#13;
<p>Each Service endpoint has an iptables chain that performs DNAT on the packet.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The following listing of iptables rules shows an example of a ClusterIP Service.<a data-primary="iptables" data-secondary="rules, ClusterIP Service example" data-type="indexterm" id="idm45611989457512"/> The Service is called <code>nginx</code> and has three endpoints (extraneous iptables rules have been removed for brevity):</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code><code>iptables</code><code> </code><code>--list</code><code> </code><code>--table</code><code> </code><code>nat</code><code>&#13;
</code><code>Chain</code><code> </code><code>KUBE-SERVICES</code><code> </code><code class="o">(</code><code class="m">2</code><code> </code><code>references</code><code class="o">)</code><code> </code><a class="co" href="#callout_service_routing_CO2-1" id="co_service_routing_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>target</code><code>     </code><code>prot</code><code> </code><code>opt</code><code> </code><code class="nb">source               </code><code>destination</code><code>&#13;
</code><code>KUBE-MARK-MASQ</code><code>  </code><code>tcp</code><code>  </code><code>--</code><code> </code><code>!10.244.0.0/16</code><code>        </code><code>10.97.85.96</code><code>&#13;
    </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>cluster</code><code> </code><code>IP</code><code> </code><code>*/</code><code> </code><code>tcp</code><code> </code><code>dpt:80</code><code>&#13;
</code><code>KUBE-SVC-4N57TFCL4MD7ZTDA</code><code>  </code><code>tcp</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>             </code><code>10.97.85.96</code><code>&#13;
    </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>cluster</code><code> </code><code>IP</code><code> </code><code>*/</code><code> </code><code>tcp</code><code> </code><code>dpt:80</code><code>&#13;
</code><code>KUBE-NODEPORTS</code><code>  </code><code>all</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>             </code><code>anywhere</code><code>&#13;
    </code><code>/*</code><code> </code><code>kubernetes</code><code> </code><code>service</code><code> </code><code>nodeports</code><code class="p">;</code><code> </code><code>NOTE:</code><code> </code><code>this</code><code> </code><code>must</code><code> </code><code>be</code><code> </code><code>the</code><code> </code><code>last</code><code> </code><code>rule</code><code> </code><code>in</code><code>&#13;
    </code><code>this</code><code> </code><code>chain</code><code> </code><code>*/</code><code> </code><code>ADDRTYPE</code><code> </code><code>match</code><code> </code><code>dst-type</code><code> </code><code>LOCAL</code><code>&#13;
&#13;
</code><code>Chain</code><code> </code><code>KUBE-SVC-4N57TFCL4MD7ZTDA</code><code> </code><code class="o">(</code><code class="m">1</code><code> </code><code>references</code><code class="o">)</code><code> </code><a class="co" href="#callout_service_routing_CO2-2" id="co_service_routing_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>target</code><code>     </code><code>prot</code><code> </code><code>opt</code><code> </code><code class="nb">source               </code><code>destination</code><code>&#13;
</code><code>KUBE-SEP-VUJFIIOGYVVPH7Q4</code><code>  </code><code>all</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>    </code><code>anywhere</code><code>    </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
    </code><code>statistic</code><code> </code><code>mode</code><code> </code><code>random</code><code> </code><code>probability</code><code> </code><code>0.33333333349</code><code>&#13;
</code><code>KUBE-SEP-Y42457KCQHG7FFWI</code><code>  </code><code>all</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>    </code><code>anywhere</code><code>    </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
    </code><code>statistic</code><code> </code><code>mode</code><code> </code><code>random</code><code> </code><code>probability</code><code> </code><code>0.50000000000</code><code>&#13;
</code><code>KUBE-SEP-UOUQBAIW4Z676WKH</code><code>  </code><code>all</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>    </code><code>anywhere</code><code>    </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
&#13;
</code><code>Chain</code><code> </code><code>KUBE-SEP-UOUQBAIW4Z676WKH</code><code> </code><code class="o">(</code><code class="m">1</code><code> </code><code>references</code><code class="o">)</code><code>  </code><a class="co" href="#callout_service_routing_CO2-3" id="co_service_routing_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>target</code><code>     </code><code>prot</code><code> </code><code>opt</code><code> </code><code class="nb">source               </code><code>destination</code><code>&#13;
</code><code>KUBE-MARK-MASQ</code><code>  </code><code>all</code><code>  </code><code>--</code><code>  </code><code>10.244.0.8</code><code>    </code><code>anywhere</code><code>             </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
</code><code>DNAT</code><code>       </code><code>tcp</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>           </code><code>anywhere</code><code>             </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
    </code><code>tcp</code><code> </code><code>to:10.244.0.8:80</code><code>&#13;
&#13;
</code><code>Chain</code><code> </code><code>KUBE-SEP-VUJFIIOGYVVPH7Q4</code><code> </code><code class="o">(</code><code class="m">1</code><code> </code><code>references</code><code class="o">)</code><code>&#13;
</code><code>target</code><code>     </code><code>prot</code><code> </code><code>opt</code><code> </code><code class="nb">source               </code><code>destination</code><code>&#13;
</code><code>KUBE-MARK-MASQ</code><code>  </code><code>all</code><code>  </code><code>--</code><code>  </code><code>10.244.0.108</code><code>    </code><code>anywhere</code><code>           </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
</code><code>DNAT</code><code>       </code><code>tcp</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>             </code><code>anywhere</code><code>           </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
    </code><code>tcp</code><code> </code><code>to:10.244.0.108:80</code><code>&#13;
&#13;
</code><code>Chain</code><code> </code><code>KUBE-SEP-Y42457KCQHG7FFWI</code><code> </code><code class="o">(</code><code class="m">1</code><code> </code><code>references</code><code class="o">)</code><code>&#13;
</code><code>target</code><code>     </code><code>prot</code><code> </code><code>opt</code><code> </code><code class="nb">source               </code><code>destination</code><code>&#13;
</code><code>KUBE-MARK-MASQ</code><code>  </code><code>all</code><code>  </code><code>--</code><code>  </code><code>10.244.0.6</code><code>           </code><code>anywhere</code><code>     </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
</code><code>DNAT</code><code>       </code><code>tcp</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>             </code><code>anywhere</code><code>          </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
    </code><code>tcp</code><code> </code><code>to:10.244.0.6:80</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO2-1" id="callout_service_routing_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This is the top-level chain. It has rules for all the Services in the cluster. Notice how the <code>KUBE-SVC-4N57TFCL4MD7ZTDA</code> rule specifies a destination IP of 10.97.85.96. This is the <code>nginx</code> Service’s ClusterIP.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO2-2" id="callout_service_routing_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This is the chain of the <code>nginx</code> Service. Notice how there is a rule for each Service endpoint with a given probability of matching the rule.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO2-3" id="callout_service_routing_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>This chain corresponds to one of the Service endpoints. (SEP stands for Service endpoint.) The last rule in this chain is the one that performs DNAT to forward the packet to the endpoint (or Pod).</p></dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="NodePort and LoadBalancer Services" data-type="sect4"><div class="sect4" id="idm45611988998104">&#13;
<h4>NodePort and LoadBalancer Services</h4>&#13;
&#13;
<p>When it comes to NodePort and LoadBalancer Services, kube-proxy configures iptables rules similar to those used for ClusterIP Services.<a data-primary="NodePort Service" data-secondary="implementation details" data-type="indexterm" id="idm45611988995720"/><a data-primary="LoadBalancer Service" data-secondary="implementation details" data-type="indexterm" id="idm45611988994744"/><a data-primary="kube-proxy" data-secondary="iptables rules for NodePort and LoadBalancer Services" data-type="indexterm" id="idm45611989031528"/><a data-primary="iptables" data-secondary="rules for NodeProxy and LoadBalancer Services" data-type="indexterm" id="idm45611989030616"/> The main difference is that the rules match packets based on their destination port number. If they match, the rule sends the packet to the Service-specific chain where DNAT happens. The snippet below shows the iptables rules for a NodePort Service called <code>nginx</code> listening on port 31767.</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code><code>iptables</code><code> </code><code>--list</code><code> </code><code>--table</code><code> </code><code>nat</code><code>&#13;
</code><code>Chain</code><code> </code><code>KUBE-NODEPORTS</code><code> </code><code class="o">(</code><code class="m">1</code><code> </code><code>references</code><code class="o">)</code><code> </code><a class="co" href="#callout_service_routing_CO3-1" id="co_service_routing_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>target</code><code>     </code><code>prot</code><code> </code><code>opt</code><code> </code><code class="nb">source               </code><code>destination</code><code>&#13;
</code><code>KUBE-MARK-MASQ</code><code>  </code><code>tcp</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>             </code><code>anywhere</code><code>      </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
    </code><code>tcp</code><code> </code><code>dpt:31767</code><code>&#13;
</code><code>KUBE-SVC-4N57TFCL4MD7ZTDA</code><code>  </code><code>tcp</code><code>  </code><code>--</code><code>  </code><code>anywhere</code><code>     </code><code>anywhere</code><code>   </code><code>/*</code><code> </code><code>default/nginx:</code><code> </code><code>*/</code><code>&#13;
    </code><code>tcp</code><code> </code><code>dpt:31767</code><code> </code><a class="co" href="#callout_service_routing_CO3-2" id="co_service_routing_CO3-2"><img alt="2" src="assets/2.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO3-1" id="callout_service_routing_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Kube-proxy programs iptables rules for NodePort Services in the <code>KUBE-NODEPORTS</code> chain.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO3-2" id="callout_service_routing_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>If the packet has <code>tcp: 31767</code> as the destination port, it is sent to the Service-specific chain. This chain is the Service-specific chain we saw in callout 2 in the previous code snippet.</p></dd>&#13;
</dl>&#13;
&#13;
<p>In addition to programming the iptables rules, kube-proxy opens the port assigned to the NodePort Service and holds it open.<a data-primary="kube-proxy" data-secondary="opening and holding port for NodePort Service" data-type="indexterm" id="idm45611989148248"/> Holding on to the port has no function from a routing perspective. It merely prevents other processes from claiming it.</p>&#13;
&#13;
<p>A key consideration to make when using NodePort and LoadBalancer Services is the Service’s external traffic policy setting.<a data-primary="external traffic policy" data-secondary="setting on NodePort and LoadBalancer" data-type="indexterm" id="idm45611989118856"/> The external traffic policy determines whether the Service routes external traffic to node-local endpoints (<code>externalTrafficPolicy: Local</code>) or cluster-wide endpoints (<code>externalTrafficPolicy: Cluster</code>). Each policy has benefits and trade-offs, as discussed next.</p>&#13;
&#13;
<p>When the policy is set to <code>Local</code>, the Service routes traffic to endpoints (Pods) running on the node receiving the traffic. Routing to a local endpoint has two important benefits. First, there is no SNAT involved so the source IP is preserved, making it available to the workload. And second, there is no additional network hop that you would otherwise incur when forwarding traffic to another node. With that said, the <code>Local</code> policy also has downsides. Mainly, traffic that reaches a node that lacks Service endpoints is dropped. For this reason, the <code>Local</code> policy is usually combined with an external load balancer that health-checks the nodes. When the node doesn’t have an endpoint for the Service, the load balancer does not send traffic to the node, given that the health check fails. <a data-type="xref" href="#loadbalancer_service_with_local_external_traffic_policy">Figure 6-6</a> illustrates this functionality. Another downside of the <code>Local</code> policy is the potential for unbalanced application load. For example, if a node has three Service endpoints, each endpoint receives 33% of the traffic. If another node has a single endpoint, it receives 100% of the traffic. This imbalance can be mitigated by spreading the Pods with anti-affinity rules or using a DaemonSet to schedule the Pods.</p>&#13;
&#13;
<figure><div class="figure" id="loadbalancer_service_with_local_external_traffic_policy">&#13;
<img alt="prku 0606" src="assets/prku_0606.png"/>&#13;
<h6><span class="label">Figure 6-6. </span>LoadBalancer Service with <code>Local</code> external traffic policy. The external load balancer runs health checks against the nodes. Any node that does not have a Service endpoint is removed from the load balancer’s backend pool.</h6>&#13;
</div></figure>&#13;
&#13;
<p>If you have a Service that handles a ton of external traffic, using the <code>Local</code> external policy is usually the right choice.<a data-primary="Local external traffic policy setting" data-type="indexterm" id="idm45611989388728"/><a data-primary="Cluster external traffic policy setting" data-type="indexterm" id="idm45611989388056"/> However, if you do not have a load balancer at your disposal, you should use the <code>Cluster</code> external traffic policy. With this policy, traffic is load balanced across all endpoints in the cluster, as shown in <a data-type="xref" href="#loadbalancer_service_with_cluster_external_traffic_policy">Figure 6-7</a>. As you can imagine, the load balancing results in the loss of the Source IP due to SNAT. It can also result in an additional network hop. However, the <code>Cluster</code> policy does not drop external traffic, regardless of where the endpoint Pods are running.</p>&#13;
&#13;
<figure><div class="figure" id="loadbalancer_service_with_cluster_external_traffic_policy">&#13;
<img alt="prku 0607" src="assets/prku_0607.png"/>&#13;
<h6><span class="label">Figure 6-7. </span>LoadBalancer Service with <code>Cluster</code> external traffic policy. Nodes that do not have node-local endpoints forward the traffic to an endpoint on another node.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Connection tracking (conntrack)" data-type="sect4"><div class="sect4" id="idm45611988996648">&#13;
<h4>Connection tracking (conntrack)</h4>&#13;
&#13;
<p>When the kernel’s networking stack performs DNAT on a packet destined to a Service, it adds an entry to the connection tracking (conntrack) table.<a data-primary="network address translation (NAT)" data-secondary="conntrack table and" data-type="indexterm" id="idm45611989410904"/><a data-primary="connection tracking (conntrack)" data-type="indexterm" id="idm45611989409912"/><a data-primary="Services" data-secondary="implementation details" data-tertiary="connection tracking (conntrack)" data-type="indexterm" id="idm45611989409224"/> The table tracks the translation performed so that it is applied to any additional packet destined to the same Service. The table is also used to remove the NAT from response packets before sending them to the source Pod.</p>&#13;
&#13;
<p>Each entry in the table maps the pre-NAT protocol, source IP, source port, destination IP, and destination port onto the post-NAT protocol, source IP, source port, destination IP, and destination port. (Entries include additional information but are not relevant in this context.) <a data-type="xref" href="#connection_tracking_conntrack_table_entry_that_tracks">Figure 6-8</a> depicts a table entry that tracks the connection from a Pod (<code>192.168.0.9</code>) to a Service (<code>10.96.0.14</code>). Notice how the destination IP and port change after the DNAT.</p>&#13;
&#13;
<figure><div class="figure" id="connection_tracking_conntrack_table_entry_that_tracks">&#13;
<img alt="prku 0608" src="assets/prku_0608.png"/>&#13;
<h6><span class="label">Figure 6-8. </span>Connection tracking (conntrack) table entry that tracks the connection from a Pod (<code>192.168.0.9</code>) to a Service (<code>10.96.0.14</code>).</h6>&#13;
</div></figure>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>When the conntrack table fills up, the kernel starts dropping or rejecting connections, which can be problematic for some applications. If you are running workloads that handle many connections and notice connection issues, you may need to tune the maximum size of the conntrack table on your nodes. More importantly, you should monitor the conntrack table utilization and alert when the table is close to being full.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Masquerade" data-type="sect4"><div class="sect4" id="idm45611989378776">&#13;
<h4>Masquerade</h4>&#13;
&#13;
<p>You may have noticed that we glossed over the <code>KUBE-MARK-MASQ</code> iptables rules listed in the previous examples. <a data-primary="Services" data-secondary="implementation details" data-tertiary="masquerade" data-type="indexterm" id="idm45611989377144"/><a data-primary="masquerade" data-type="indexterm" id="idm45611989204088"/>These rules are in place for packets that arrive at a node from outside the cluster. To route such packets properly, the Service fabric needs to masquerade/source NAT the packets when forwarding them to another node.<a data-primary="IP addresses" data-secondary="masquerading and" data-type="indexterm" id="idm45611989203048"/> Otherwise, response packets will contain the IP address of the Pod that handled the request. The Pod IP in the packet would cause a connection issue, as the client initiated the connection to the node and not the Pod.</p>&#13;
&#13;
<p>Masquerading is also used to egress from the cluster. When Pods connect to external services, the source IP must be the IP address of the node where the Pod is running instead of the Pod IP. Otherwise, the network would drop response packets because they would have the Pod IP as the destination IP address.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Performance concerns" data-type="sect4"><div class="sect4" id="idm45611989229016">&#13;
<h4>Performance concerns</h4>&#13;
&#13;
<p>The iptables mode <a data-primary="iptables mode (kube-proxy)" data-secondary="performance concerns" data-type="indexterm" id="idm45611989227448"/><a data-primary="kube-proxy" data-secondary="iptables mode" data-tertiary="performance concerns" data-type="indexterm" id="idm45611989226472"/><a data-primary="Services" data-secondary="implementation details" data-tertiary="performance concerns with iptables mode" data-type="indexterm" id="idm45611989223240"/><a data-primary="performance" data-secondary="iptables mode in kube-proxy, concerns with" data-type="indexterm" id="idm45611989222056"/>has served and continues to serve Kubernetes clusters well. With that said, you should be aware of some performance and scalability limitations, as these can arise in large cluster deployments.</p>&#13;
&#13;
<p>Given the structure of the iptables rules and how they work, whenever a Pod establishes a new connection to a Service, the initial packet traverses the iptables rules until it matches one of them. In the worst-case scenario, the packet needs to traverse the entire collection of iptables rules.</p>&#13;
&#13;
<p>The iptables mode suffers from O(n) time complexity when it processes packets. In other words, the iptables mode scales linearly with the number of Services in the cluster. As the number of Services grows, the performance of connecting to Services gets worse.</p>&#13;
&#13;
<p>Perhaps more important, updates to the iptables rules also suffer at large scale.<a data-primary="updates" data-secondary="to iptables rules" data-seealso="iptables" data-type="indexterm" id="idm45611989283256"/><a data-primary="scalability" data-secondary="issues with iptables mode" data-type="indexterm" id="idm45611989282008"/> Because iptables rules are not incremental, kube-proxy needs to write out the entire table for every update. In some cases, these updates can even take minutes to complete, which risks sending traffic to stale endpoints. Furthermore, kube-proxy needs to hold the iptables lock (<code>/run/xtables.lock</code>) during these updates, which can cause contention with other processes that need to update the iptables rules, such as CNI plug-ins.</p>&#13;
&#13;
<p class="pagebreak-before">Linear scaling is an undesirable quality of any system. With that said, based on <a href="https://oreil.ly/YJAu9">tests</a> performed by the Kubernetes community, you should not notice any performance degradation unless you are running clusters with tens of thousands of Services.<a data-primary="performance" data-secondary="IPVS mode in kube-proxy" data-type="indexterm" id="idm45611989171976"/> If you are operating at that scale, however, you might benefit from the IPVS mode in kube-proxy, which we’ll discuss in the following section.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611989170584">&#13;
<h5>Rolling Updates and Service Reconciliation</h5>&#13;
<p>An interesting problem with Services is unexpected request errors during application rolling updates.<a data-primary="Services" data-secondary="problem with rolling updates and Service reconciliation" data-type="indexterm" id="idm45611989278552"/><a data-primary="updates" data-secondary="problem with rolling application updates and Service reconciliation" data-type="indexterm" id="idm45611989277608"/> While this issue is less common in development environments, it can crop up in production clusters that are hosting many workloads.</p>&#13;
&#13;
<p>The crux of the problem is the distributed nature of Kubernetes. As we’ve discussed in this chapter, multiple components work together to make Services work in Kubernetes, mainly the <a data-primary="endpoints" data-secondary="Endpoints controller" data-type="indexterm" id="idm45611989275832"/><a data-primary="kube-proxy" data-secondary="problem with rolling updates and Service reconciliation" data-type="indexterm" id="idm45611989274248"/>Endpoints controller and kube-proxy.</p>&#13;
&#13;
<p>When a Pod is deleted, the following happens simultaneously:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The kubelet initiates the Pod shutdown sequence. It sends a SIGTERM signal to the workload and waits until it terminates. If the process continues to run after the graceful shutdown period, the kubelet sends a SIGKILL to terminate the workload forcefully.</p>&#13;
</li>&#13;
<li>&#13;
<p>The Endpoints controller receives a Pod deletion watch event, which triggers the removal of the Pod IP address from the Endpoints resource. Once the Endpoints resource is updated, kube-proxy removes the IP address from the iptables rules (or IPVS virtual service).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This is a classic distributed system race.<a data-primary="distributed system race" data-type="indexterm" id="idm45611989260040"/> Ideally, the Endpoints controller and kube-proxy finish their updates before the Pod exits. However, ordering is not guaranteed, given that these workflows are running concurrently. There is a chance that the workload exits (and thus stops accepting requests) before kube-proxy on each node removes the Pod from the list of active endpoints. When this happens, in-flight requests fail because they get routed to a Pod that is no longer running.</p>&#13;
&#13;
<p>To solve this, Kubernetes would have to wait until all kube-proxies finish updating endpoints before stopping workloads, but this is not feasible. For example, how would it handle the case of a node becoming unavailable? With that said, we’ve used SIGTERM handlers and <code>sleep</code> pre-stop hooks to mitigate this issue in practice.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kube-proxy: IP Virtual Server (IPVS) mode" data-type="sect3"><div class="sect3" id="idm45611989257528">&#13;
<h3>Kube-proxy: IP Virtual Server (IPVS) mode</h3>&#13;
&#13;
<p>IPVS is a load balancing technology<a data-primary="kube-proxy" data-secondary="IPVS mode" data-type="indexterm" id="ix_kpIPVS"/><a data-primary="IPVS (IP Virtual Server) mode (kube-proxy)" data-type="indexterm" id="ix_IPVS"/><a data-primary="Services" data-secondary="kube-proxy in IPVS mode" data-type="indexterm" id="ix_SerkpIPVS"/> built into the Linux kernel. Kubernetes added support for IPVS in kube-proxy to address the scalability limitations and performance issues of the iptables mode.</p>&#13;
&#13;
<p>As discussed in the previous section, the iptables mode uses iptables rules to implement Kubernetes Services. The iptables rules are stored in a list, which packets need to traverse in its entirety in the worst-case scenario. IPVS does not suffer from this problem because it was originally designed for load balancing use cases.</p>&#13;
&#13;
<p>The IPVS implementation in the Linux kernel uses hash tables to find the destination of a packet. Instead of traversing the list of Services when a new connection is established, IPVS immediately finds the destination Pod based on the Service IP address.</p>&#13;
&#13;
<p>Let’s discuss how kube-proxy in IPVS mode handles each of the Kubernetes Service types.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ClusterIP Services" data-type="sect4"><div class="sect4" id="idm45611989267976">&#13;
<h4>ClusterIP Services</h4>&#13;
&#13;
<p>When handling Services that have a ClusterIP, kube-proxy in <code>ipvs</code> mode does a couple of things.<a data-primary="Services" data-secondary="kube-proxy in IPVS mode" data-tertiary="ClusterIP" data-type="indexterm" id="idm45611989265944"/><a data-primary="ClusterIP Service" data-secondary="kube-proxy in IPVS mode" data-type="indexterm" id="idm45611989325304"/><a data-primary="kube-proxy" data-secondary="IPVS mode" data-tertiary="CluserIP Service" data-type="indexterm" id="idm45611989324360"/> First, it adds the IP address of the ClusterIP Service to a dummy network interface on the node called <code>kube-ipvs0</code>, as shown in the following snippet:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>ip address show dev kube-ipvs0&#13;
28: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu <code class="m">1500</code> qdisc noop state DOWN group default&#13;
    link/ether 96:96:1b:36:32:de brd ff:ff:ff:ff:ff:ff&#13;
    inet 10.110.34.183/32 brd 10.110.34.183 scope global kube-ipvs0&#13;
       valid_lft forever preferred_lft forever&#13;
    inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0&#13;
       valid_lft forever preferred_lft forever&#13;
    inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0&#13;
       valid_lft forever preferred_lft forever</pre>&#13;
&#13;
<p>After updating the dummy interface, kube-proxy creates an IPVS virtual service with the IP address of the ClusterIP Service. Finally, for each Service endpoint, it adds an IPVS real server to the IPVS virtual service. The following snippet shows the IPVS virtual service and real servers for a ClusterIP Service with three endpoints:</p>&#13;
&#13;
<pre data-type="programlisting">$ ipvsadm --list --numeric --tcp-service 10.110.34.183:80&#13;
Prot LocalAddress:Port Scheduler Flags&#13;
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn&#13;
TCP  10.110.34.183:80 rr <a class="co" href="#callout_service_routing_CO4-1" id="co_service_routing_CO4-1"><img alt="1" src="assets/1.png"/></a>&#13;
  -&gt; 192.168.89.153:80            Masq    1      0          0 <a class="co" href="#callout_service_routing_CO4-2" id="co_service_routing_CO4-2"><img alt="2" src="assets/2.png"/></a>&#13;
  -&gt; 192.168.89.154:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.155:80            Masq    1      0          0</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO4-1" id="callout_service_routing_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This is the IPVS virtual service. Its IP address is the IP address of the ClusterIP Service.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO4-2" id="callout_service_routing_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>This is one of the IPVS real servers. It corresponds to one of the Service endpoints (Pods).</p></dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="NodePort and LoadBalancer Services" data-type="sect4"><div class="sect4" id="idm45611989318728">&#13;
<h4>NodePort and LoadBalancer Services</h4>&#13;
&#13;
<p>For NodePort and LoadBalancer Services, kube-proxy creates an IPVS virtual service for the Service’s cluster IP.<a data-primary="kube-proxy" data-secondary="IPVS mode" data-tertiary="NodePort and LoadBalancer Services" data-type="indexterm" id="idm45611989317448"/><a data-primary="Services" data-secondary="kube-proxy in IPVS mode" data-tertiary="NodePort and LoadBalancer" data-type="indexterm" id="idm45611989311096"/> Kube-proxy also &#13;
<span class="keep-together">creates</span> an IPVS virtual service for each of the node’s IP addresses and the loopback address. For example, the following snippet shows a listing of the IPVS virtual services created for a NodePort Service listening on TCP port 30737:</p>&#13;
&#13;
<pre data-type="programlisting">ipvsadm --list --numeric&#13;
IP Virtual Server version 1.2.1 (size=4096)&#13;
Prot LocalAddress:Port Scheduler Flags&#13;
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn&#13;
TCP  10.0.99.67:30737 rr <a class="co" href="#callout_service_routing_CO5-1" id="co_service_routing_CO5-1"><img alt="1" src="assets/1.png"/></a>&#13;
  -&gt; 192.168.89.153:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.154:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.155:80            Masq    1      0          0&#13;
TCP  10.110.34.183:80 rr <a class="co" href="#callout_service_routing_CO5-2" id="co_service_routing_CO5-2"><img alt="2" src="assets/2.png"/></a>&#13;
  -&gt; 192.168.89.153:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.154:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.155:80            Masq    1      0          0&#13;
TCP  127.0.0.1:30737 rr <a class="co" href="#callout_service_routing_CO5-3" id="co_service_routing_CO5-3"><img alt="3" src="assets/3.png"/></a>&#13;
  -&gt; 192.168.89.153:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.154:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.155:80            Masq    1      0          0&#13;
TCP  192.168.246.64:30737 rr <a class="co" href="#callout_service_routing_CO5-4" id="co_service_routing_CO5-4"><img alt="4" src="assets/4.png"/></a>&#13;
  -&gt; 192.168.89.153:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.154:80            Masq    1      0          0&#13;
  -&gt; 192.168.89.155:80            Masq    1      0          0</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO5-1" id="callout_service_routing_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>IPVS virtual service listening on the node’s IP address.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO5-2" id="callout_service_routing_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>IPVS virtual service listening on the Service’s cluster IP address.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO5-3" id="callout_service_routing_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>IPVS virtual service listening on <code>localhost</code>.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO5-4" id="callout_service_routing_CO5-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>IPVS virtual service listening on a secondary network interface on the node.<a data-primary="kube-proxy" data-secondary="IPVS mode" data-startref="ix_kpIPVS" data-type="indexterm" id="idm45611989441880"/><a data-primary="IPVS (IP Virtual Server) mode (kube-proxy)" data-startref="ix_IPVS" data-type="indexterm" id="idm45611989440632"/><a data-primary="Services" data-secondary="kube-proxy in IPVS mode" data-startref="ix_SerkpIPVS" data-type="indexterm" id="idm45611989439720"/></p></dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Running without kube-proxy" data-type="sect3"><div class="sect3" id="idm45611989438248">&#13;
<h3>Running without kube-proxy</h3>&#13;
&#13;
<p>Historically, kube-proxy has been a staple in all Kubernetes deployments.<a data-primary="Services" data-secondary="running without kube-proxy" data-type="indexterm" id="idm45611989437016"/><a data-primary="kube-proxy" data-secondary="running without" data-type="indexterm" id="idm45611989436072"/> It is a vital component that makes Kubernetes Services work. As the community evolves, however, we could start seeing Kubernetes deployments that do not have kube-proxy running. How is this possible? What handles Services instead?</p>&#13;
&#13;
<p>With the<a data-primary="eBPF (extended Berkeley Packet Filter)" data-type="indexterm" id="idm45611988889112"/><a data-primary="Container Networking Interface (CNI)" data-secondary="plug-ins" data-tertiary="absorbing kube-proxy responsibilities" data-type="indexterm" id="idm45611988888408"/> advent of extended Berkeley Packet Filters (eBPF), CNI plug-ins such as <a href="https://oreil.ly/sWoh5">Cilium</a> and <a href="https://oreil.ly/0jrKG">Calico</a> can absorb kube-proxy’s responsibilities.<a data-primary="Cilium" data-secondary="absorbing kube-proxy responsibilities" data-type="indexterm" id="idm45611988885704"/><a data-primary="Calico" data-secondary="absorbing kube-proxy responsibilities" data-type="indexterm" id="idm45611988884712"/> Instead of handling Services with iptables or IPVS, the CNI plug-ins program Services right into the Pod networking data plane. Using eBPF improves the performance and scalability of Services in Kubernetes, given that the eBPF implementation uses hash tables for endpoint lookups.<a data-primary="performance" data-secondary="improvements in Services with eBPF" data-type="indexterm" id="idm45611988883320"/> It also improves Service update processing, as it can handle individual Service updates efficiently.</p>&#13;
&#13;
<p>Removing the need for kube-proxy and optimizing Service routing is a worthy feat, especially for those operating at scale. However, it is still early days when it comes to running these solutions in production. For example, the Cilium implementation requires newer kernel versions to support a kube-proxy-less deployment (at the time of &#13;
<span class="keep-together">writing</span>, the latest Cilium version is <code>v1.8</code>). Similarly, the Calico team discourages the use of eBPF in production because it is still in tech preview. (At the time of writing, the latest calico version is <code>v3.15.1</code>.) Over time, we expect to see kube-proxy replacements become more common.<a data-primary="Container Networking Interface (CNI)" data-secondary="CNI chaining" data-type="indexterm" id="idm45611988879592"/> Cilium even supports running its proxy replacement capabilities alongside other CNI plug-ins (referred to as <a href="https://oreil.ly/jZ-2r">CNI chaining</a>).<a data-primary="Services" data-secondary="implementation details" data-startref="ix_Serimp" data-type="indexterm" id="idm45611988877736"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service Discovery" data-type="sect2"><div class="sect2" id="idm45611988876328">&#13;
<h2>Service Discovery</h2>&#13;
&#13;
<p>Service discovery provides a mechanism for applications to discover services that are available on the network.<a data-primary="service discovery" data-type="indexterm" id="ix_serdis"/> While not a <em>routing</em> concern, service discovery is intimately related to Kubernetes Services.</p>&#13;
&#13;
<p>Platform teams may wonder whether they need to introduce a dedicated service discovery system to a cluster, such as Consul. While possible, it is typically not necessary, as Kubernetes offers service discovery to all workloads running in the cluster. In this section, we will discuss the different service discovery mechanisms available in Kubernetes: DNS-based service discovery, API-based service discovery, and environment variable-based service discovery.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using DNS" data-type="sect3"><div class="sect3" id="idm45611989361032">&#13;
<h3>Using DNS</h3>&#13;
&#13;
<p>Kubernetes provides service discovery over DNS to workloads running inside the cluster.<a data-primary="DNS (Domain Name System)" data-secondary="service discovery over" data-type="indexterm" id="idm45611989359304"/><a data-primary="service discovery" data-secondary="using DNS" data-type="indexterm" id="idm45611989358360"/> Conformant Kubernetes deployments run a DNS server that integrates with the Kubernetes API. <a data-primary="CoreDNS servers" data-secondary="DNS-based service discovery" data-type="indexterm" id="idm45611989357176"/>The most common DNS server in use today is <a href="https://coredns.io">CoreDNS</a>, an open source, extensible DNS server.</p>&#13;
&#13;
<p>CoreDNS watches resources in the Kubernetes API server.<a data-primary="API server" data-secondary="service discovery through DNS" data-type="indexterm" id="idm45611989354840"/> For each Kubernetes Service, CoreDNS creates a DNS record with the following format: <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>. For example, a Service called <code>nginx</code> in the <code>default</code> Namespace gets the DNS record <code>nginx.default.svc.cluster.local</code>. But how can Pods use these DNS records?</p>&#13;
&#13;
<p>To enable DNS-based service discovery, Kubernetes configures CoreDNS as the DNS resolver for Pods. When setting up a Pod’s sandbox, the kubelet writes an <em>/etc/resolv.conf</em> that specifies CoreDNS as the nameserver and injects the config file into the container. The <em>/etc/resolv.conf</em> file of a Pod looks something like this:</p>&#13;
&#13;
<pre data-type="programlisting">$ cat /etc/resolv.conf&#13;
search default.svc.cluster.local svc.cluster.local cluster.local&#13;
nameserver 10.96.0.10&#13;
options ndots:5</pre>&#13;
&#13;
<p>Given this configuration, Pods send DNS queries to CoreDNS whenever they try to connect to a Service by name.</p>&#13;
&#13;
<p>Another interesting trick in the resolver configuration is the use of <code>ndots</code> and <code>search</code> to simplify DNS queries. When a Pod wants to reach a Service that exists in the same Namespace, it can use the Service’s name as the domain name instead of the fully qualified domain name (<code>nginx.default.svc.cluster.local</code>):</p>&#13;
&#13;
<pre data-type="programlisting">$ nslookup nginx&#13;
Server:		10.96.0.10&#13;
Address:	10.96.0.10#53&#13;
&#13;
Name:	nginx.default.svc.cluster.local&#13;
Address: 10.110.34.183</pre>&#13;
&#13;
<p>Similarly, when a Pod wants to reach a Service in another Namespace, it can do so by appending the Namespace name to the Service name:</p>&#13;
&#13;
<pre data-type="programlisting">$ nslookup nginx.default&#13;
Server:		10.96.0.10&#13;
Address:	10.96.0.10#53&#13;
&#13;
Name:	nginx.default.svc.cluster.local&#13;
Address: 10.110.34.183</pre>&#13;
&#13;
<p>One thing to consider with the <code>ndots</code> configuration is its impact on applications that communicate with services outside of the cluster. The <code>ndots</code> parameter specifies how many dots must appear in a domain name for it to be considered an absolute or fully qualified name. When resolving a name that’s not fully qualified, the system attempts various lookups using the items in the <code>search</code> parameter, as seen in the following example. Thus, when applications resolve cluster-external names that are not fully qualified, the resolver consults the cluster DNS server with multiple futile requests before attempting to resolve the name as an absolute name. To avoid this issue, you can use fully qualified domain names in your applications by adding a <code>.</code> at the end of the name. Alternatively, you can tune the DNS configuration of the Pod via the <code>dnsConfig</code> field in the Pod’s specification.</p>&#13;
&#13;
<p>The following snippet shows the impact of the <code>ndots</code> configuration on Pods that resolve external names. Notice how resolving a name that has less dots than the configured <code>ndots</code> results in multiple DNS queries, while resolving an absolute name results in a single query:</p>&#13;
&#13;
<pre data-type="programlisting">$ nslookup -type=A google.com -debug | grep QUESTIONS -A1 <a class="co" href="#callout_service_routing_CO6-1" id="co_service_routing_CO6-1"><img alt="1" src="assets/1.png"/></a>&#13;
  QUESTIONS:&#13;
google.com.default.svc.cluster.local, type = A, class = IN&#13;
--&#13;
  QUESTIONS:&#13;
google.com.svc.cluster.local, type = A, class = IN&#13;
--&#13;
  QUESTIONS:&#13;
google.com.cluster.local, type = A, class = IN&#13;
--&#13;
  QUESTIONS:&#13;
google.com, type = A, class = IN&#13;
&#13;
$ nslookup -type=A -debug google.com. | grep QUESTIONS -A1 <a class="co" href="#callout_service_routing_CO6-2" id="co_service_routing_CO6-2"><img alt="2" src="assets/2.png"/></a>&#13;
  QUESTIONS:&#13;
google.com, type = A, class = IN</pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO6-1" id="callout_service_routing_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Attempt to resolve a name with less than 5 dots (not fully qualified). The resolver performs multiple lookups, one per item in the <code>search</code> field of <em>/etc/resolv.conf</em>.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO6-2" id="callout_service_routing_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Attempt to resolve a fully qualified name. The resolver performs a single lookup.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Overall, service discovery over DNS is extremely useful, as it lowers the barrier for applications to interact with Kubernetes Services.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using the Kubernetes API" data-type="sect3"><div class="sect3" id="idm45611989360472">&#13;
<h3>Using the Kubernetes API</h3>&#13;
&#13;
<p>Another way to discover Services in Kubernetes<a data-primary="service discovery" data-secondary="using Kubernetes API" data-type="indexterm" id="idm45611988930408"/><a data-primary="Kubernetes API" data-secondary="using for service discovery" data-type="indexterm" id="idm45611988929432"/> is by using the Kubernetes API. The community maintains various Kubernetes client libraries in different languages, including Go, Java, Python, and others. Some application frameworks, such as Spring, also support service discovery through the Kubernetes API.</p>&#13;
&#13;
<p>Using the Kubernetes API for service discovery can be useful in specific scenarios. For example, if your applications need to be aware of Service endpoint changes as soon as they happen, they would benefit from watching the API.</p>&#13;
&#13;
<p>The main downside of performing service discovery through the Kubernetes API is that you tightly couple the application to the underlying platform. Ideally, applications should be unaware of the platform. If you do choose to use the Kubernetes API for service discovery, consider building an interface that abstracts the Kubernetes details away from your business logic.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using environment variables" data-type="sect3"><div class="sect3" id="idm45611988926392">&#13;
<h3>Using environment variables</h3>&#13;
&#13;
<p>Kubernetes injects environment variables into Pods to facilitate service discovery.<a data-primary="Pods" data-secondary="environment variables enhancing service discovery" data-type="indexterm" id="idm45611988925096"/><a data-primary="service discovery" data-secondary="using environment variables" data-type="indexterm" id="idm45611988924152"/><a data-primary="environment variables" data-secondary="using for service discovery" data-type="indexterm" id="idm45611988923144"/> For each Service, Kubernetes sets multiple environment variables according to the Service definition. The environment variables for an <code>nginx</code> ClusterIP Service listening on port 80 look as follows:</p>&#13;
&#13;
<pre data-type="programlisting">NGINX_PORT_80_TCP_PORT=80&#13;
NGINX_SERVICE_HOST=10.110.34.183&#13;
NGINX_PORT=tcp://10.110.34.183:80&#13;
NGINX_PORT_80_TCP=tcp://10.110.34.183:80&#13;
NGINX_PORT_80_TCP_PROTO=tcp&#13;
NGINX_SERVICE_PORT=80&#13;
NGINX_PORT_80_TCP_ADDR=10.110.34.183</pre>&#13;
&#13;
<p>The downside to this approach is that environment variables cannot be updated without restarting the Pod. Thus, Services must be in place before the Pod starts up.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DNS Service Performance" data-type="sect2"><div class="sect2" id="idm45611988919736">&#13;
<h2>DNS Service Performance</h2>&#13;
&#13;
<p>As mentioned in the previous section, offering DNS-based service discovery to workloads on your platform is crucial.<a data-primary="DNS (Domain Name System)" data-secondary="service performance" data-type="indexterm" id="idm45611988918392"/><a data-primary="performance" data-secondary="DNS service, concerns with" data-type="indexterm" id="idm45611988917400"/> As the size of your cluster and number of applications grows, the DNS service can become a bottleneck. In this section, we will discuss techniques you can use to provide a performant DNS service.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DNS cache on each node" data-type="sect3"><div class="sect3" id="idm45611988915976">&#13;
<h3>DNS cache on each node</h3>&#13;
&#13;
<p>The Kubernetes community maintains a DNS cache add-on called <a href="https://oreil.ly/lQdTH">NodeLocal DNSCache</a>. <a data-primary="NodeLocal DNSCache" data-type="indexterm" id="idm45611988913992"/><a data-primary="DNS (Domain Name System)" data-secondary="service performance" data-tertiary="DNS cache on each node" data-type="indexterm" id="idm45611988913256"/>The add-on runs a DNS cache on each node to address multiple problems. First, the cache reduces the latency of DNS lookups, given that workloads get their answers from the local cache (assuming a cache hit) instead of reaching out to the DNS server (potentially on another node). Second, the load on the CoreDNS servers goes down, as workloads are leveraging the cache most of the time.<a data-primary="CoreDNS servers" data-secondary="effects of DNS cache on each node" data-type="indexterm" id="idm45611988911496"/> Finally, in the case of a cache miss, the local DNS cache upgrades the DNS query to TCP when reaching out to the central DNS service.<a data-primary="TCP" data-secondary="DNS cache query upgraded to" data-type="indexterm" id="idm45611988910264"/> Using TCP instead of UDP improves the reliability of the DNS query.</p>&#13;
&#13;
<p>The DNS cache runs as a DaemonSet on the cluster. Each replica of the DNS cache intercepts the DNS queries that originate from their node. There’s no need to change application code or configuration to use the cache. The node-level architecture of the NodeLocal DNSCache add-on is depicted in <a data-type="xref" href="#node_level_architecture_of_the_nodelocal_dnscache_add_on">Figure 6-9</a>.</p>&#13;
&#13;
<figure><div class="figure" id="node_level_architecture_of_the_nodelocal_dnscache_add_on">&#13;
<img alt="prku 0609" src="assets/prku_0609.png"/>&#13;
<h6><span class="label">Figure 6-9. </span>Node-level architecture of the NodeLocal DNSCache add-on. The DNS cache intercepts DNS queries and responds immediately if there’s a cache hit. In the case of a cache miss, the DNS cache forwards the query to the cluster DNS service.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Auto-scaling the DNS server deployment" data-type="sect3"><div class="sect3" id="idm45611988905240">&#13;
<h3>Auto-scaling the DNS server deployment</h3>&#13;
&#13;
<p>In addition to running the node-local DNS cache in your cluster, you can automatically scale the DNS Deployment<a data-primary="DNS (Domain Name System)" data-secondary="auto-scaling DNS server deployment" data-type="indexterm" id="idm45611988903352"/> according to the size of the cluster. Note that this strategy does not leverage the Horizontal Pod Autoscaler. Instead, it uses the <a href="https://oreil.ly/432we">cluster Proportional Autoscaler</a>, which scales workloads based on the number of nodes in the cluster.<a data-primary="Cluster Proportional Autoscaler (CPA)" data-type="indexterm" id="idm45611988901368"/></p>&#13;
&#13;
<p>The Cluster Proportional Autoscaler runs as a Pod in the cluster. It has a configuration flag to set the workload that needs autoscaling.<a data-primary="CoreDNS servers" data-secondary="autoscaling the deployment" data-type="indexterm" id="idm45611988900088"/><a data-primary="autoscaling" data-secondary="of DNS server deployment" data-type="indexterm" id="idm45611988899096"/> To autoscale DNS, you must set the target flag to the CoreDNS (or kube-dns) Deployment.<a data-primary="kube-dns" data-type="indexterm" id="idm45611988897896"/> Once running, the autoscaler polls the API server every 10 seconds (by default) to get the number of nodes and CPU cores in the cluster. Then, it adjusts the number of replicas in the CoreDNS Deployment if necessary. The desired number of replicas is governed by a configurable replicas-to-nodes ratio or replicas-to-cores ratio. The ratios to use depend on your workloads and how DNS-intensive they are.</p>&#13;
&#13;
<p>In most cases, using node-local DNS cache is sufficient to offer a reliable DNS service. However, autoscaling DNS is another strategy you can use when autoscaling clusters with a wide-enough range of minimum and maximum nodes.<a data-primary="service discovery" data-startref="ix_serdis" data-type="indexterm" id="idm45611988896056"/><a data-primary="service routing" data-secondary="Kubernetes Services" data-startref="ix_serrtKS" data-type="indexterm" id="idm45611988895080"/><a data-primary="Services" data-startref="ix_SerK" data-type="indexterm" id="idm45611988893864"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress" data-type="sect1"><div class="sect1" id="ingress">&#13;
<h1>Ingress</h1>&#13;
&#13;
<p>As we’ve discussed in <a data-type="xref" href="ch05.html#chapter5">Chapter 5</a>, workloads running in Kubernetes are typically not accessible from outside the cluster.<a data-primary="service routing" data-secondary="Ingress" data-type="indexterm" id="ix_serrtIng"/><a data-primary="Ingress" data-type="indexterm" id="ix_Ingr"/> This is not a problem if your applications do not have external clients. Batch workloads are a great example of such applications. Realistically, however, most Kubernetes deployments host web services that do have end users.</p>&#13;
&#13;
<p>Ingress is an approach to exposing services running in Kubernetes to clients outside of the cluster. Even though Kubernetes does not fulfill the Ingress API out of the box, it is a staple in any Kubernetes-based platform. It is not uncommon for off-the-shelf Kubernetes applications and cluster add-ons to expect that an Ingress controller is running in the cluster. Moreover, your developers will need it to be able to run their applications successfully in Kubernetes.</p>&#13;
&#13;
<p>This section aims to guide you through the considerations you must make when implementing Ingress in your platform. We will review the Ingress API, the most common ingress traffic patterns that you will encounter, and the crucial role of Ingress controllers in a Kubernetes-based platform. We will also discuss different ways to deploy Ingress controllers and their trade-offs. Finally, we will address common challenges you can run into and explore helpful integrations with other tools in the ecosystem.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Case for Ingress" data-type="sect2"><div class="sect2" id="idm45611988810120">&#13;
<h2>The Case for Ingress</h2>&#13;
&#13;
<p>Kubernetes Services already provide ways to route traffic to Pods, so why would you need an additional strategy to achieve the same thing?<a data-primary="Ingress" data-secondary="case for" data-type="indexterm" id="idm45611988808584"/><a data-primary="service routing" data-secondary="Ingress" data-tertiary="case for" data-type="indexterm" id="idm45611988807736"/> As much as we are fans of keeping <a data-primary="Services" data-secondary="limitations and downsides" data-type="indexterm" id="idm45611988806520"/>platforms simple, the reality is that Services have important limitations and downsides:</p>&#13;
<dl>&#13;
<dt>Limited routing capabilities</dt>&#13;
<dd>&#13;
<p>Services route traffic according to the destination IP and port of incoming requests.<a data-primary="routing" data-secondary="limited capabilities of Services" data-type="indexterm" id="idm45611988804152"/> This can be useful for small and relatively simple applications, but it quickly breaks down for more substantial, microservices-based applications. These kinds of applications require smarter routing features and other advanced capabilities.</p>&#13;
</dd>&#13;
<dt>Cost</dt>&#13;
<dd>&#13;
<p>If you are running in a cloud environment, each LoadBalancer Service in your cluster creates an external load balancer, such as an ELB in the case of AWS. Running a separate load balancer for each Service in your platform can quickly become cost-prohibitive.<a data-primary="load balancers" data-secondary="multiple, Ingress removing need for" data-type="indexterm" id="idm45611988801496"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Ingress addresses both these limitations. Instead of being limited to load balancing at layer 3/4 of the OSI model, Ingress provides load balancing and routing capabilities at layer 7. In other words, Ingress operates at the application layer, which results in more advanced routing features.</p>&#13;
&#13;
<p>Another benefit of Ingress is that it removes the need to have multiple load balancers or entry points into the platform.<a data-primary="HTTP requests, Ingress routing of" data-type="indexterm" id="idm45611988799176"/> Because of the advanced routing capabilities available in Ingress, such as the ability to route HTTP requests based on the <code>Host</code> header, you can route all the service traffic to a single entry point and let the Ingress controller take care of demultiplexing the traffic.<a data-primary="Host header, routing based on" data-type="indexterm" id="idm45611988797752"/> This dramatically reduces the cost of bringing traffic into your platform.</p>&#13;
&#13;
<p>The ability to have a single ingress point into the platform also reduces the complexity of noncloud deployments. Instead of potentially having to manage multiple external load balancers with a multitude of NodePort Services, you can operate a single external load balancer that routes traffic to the Ingress controllers.</p>&#13;
&#13;
<p>Even though Ingress solves most of the downsides related to Kubernetes Services, the latter are still needed. Ingress controllers themselves run inside the platform and thus need to be exposed to clients that exist outside. And you can use a Service (either a NodePort or LoadBalancer) to do so. Besides, most Ingress controllers shine when it comes to load balancing HTTP traffic. If you want to be able to host applications that use other protocols, you might have to use Services alongside Ingress, depending on the capabilities of your Ingress controller.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Ingress API" data-type="sect2"><div class="sect2" id="idm45611988795208">&#13;
<h2>The Ingress API</h2>&#13;
&#13;
<p>The Ingress API enables application teams to expose their services and configure request routing according to their needs.<a data-primary="Ingress" data-secondary="Ingress API" data-type="indexterm" id="idm45611988793688"/><a data-primary="service routing" data-secondary="Ingress" data-tertiary="Ingress API" data-type="indexterm" id="idm45611988792840"/> Because of Ingress’s focus on HTTP routing, the Ingress API resource provides different ways to route traffic according to the properties of incoming HTTP requests.</p>&#13;
&#13;
<p>A common routing technique is routing traffic according to the <code>Host</code> header of HTTP requests. For example, given the following Ingress configuration, HTTP requests with the <code>Host</code> header set to <code>bookhotels.com</code> are routed to one service, while requests destined to <code>bookflights.com</code> are routed to another:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">hotels-ingress</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">rules</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">bookhotels.com</code>&#13;
    <code class="nt">http</code><code class="p">:</code>&#13;
      <code class="nt">paths</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>&#13;
        <code class="nt">backend</code><code class="p">:</code>&#13;
          <code class="nt">serviceName</code><code class="p">:</code> <code class="l-Scalar-Plain">hotels</code>&#13;
          <code class="nt">servicePort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>&#13;
<code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">flights-ingress</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">rules</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">bookflights.com</code>&#13;
    <code class="nt">http</code><code class="p">:</code>&#13;
      <code class="nt">paths</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>&#13;
        <code class="nt">backend</code><code class="p">:</code>&#13;
          <code class="nt">serviceName</code><code class="p">:</code> <code class="l-Scalar-Plain">flights</code>&#13;
          <code class="nt">servicePort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
&#13;
<p>Hosting applications on specific subdomains of a cluster-wide domain name is a common approach we encounter in Kubernetes. In this case, you assign a domain name to the platform, and each application gets a subdomain.<a data-primary="subdomain-based routing" data-type="indexterm" id="idm45611988787464"/> Keeping with the travel theme in the previous example, an example of subdomain-based routing for a travel booking application could have <code>hotels.cluster1.useast.example.com</code> and <code>flights.cluster1.useast.example.com</code>. Subdomain-based routing is one of the best strategies you can employ. It also enables other interesting use cases, such as hosting tenants of a software-as-a-service (SaaS) application on tenant-specific domain names (<code>tenantA.example.com</code> and <code>tenantB.example.com</code>, for example). We will further discuss how to implement subdomain-based routing in a later section.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611988711688">&#13;
<h5>Ingress Configuration Collisions and How to Avoid Them</h5>&#13;
<p>The Ingress API is prone to configuration collisions in multiteam or multitenant clusters.<a data-primary="Ingress" data-secondary="Ingress API" data-tertiary="configuration collisions, avoiding" data-type="indexterm" id="idm45611988710216"/> The primary example is different teams trying to use the same domain name to expose their applications. Consider a scenario where an application team creates an Ingress resource with the host set to <code>app.bearcanoe.com</code>. What happens when another team creates an Ingress with the same host? The Ingress API does not specify how to handle this scenario. Instead, it is up to the Ingress controller to decide what to do. Some controllers merge the configuration when possible, while others reject the new Ingress resource and log an error message. In any case, overlapping Ingress resources can result in surprising behavior, and even outages!</p>&#13;
&#13;
<p>Usually, we tackle this problem in one of two ways. The first is using an admission controller that validates the incoming Ingress resource and ensures the hostname is unique across the cluster. We’ve built many of these admission controllers over time while working in the field.<a data-primary="Open Policy Agent (OPA)" data-type="indexterm" id="idm45611988707112"/> These days, we use the Open Policy Agent (OPA) to handle this concern. The OPA community even maintains a <a href="https://oreil.ly/wnN0V">policy</a> for this use case.</p>&#13;
&#13;
<p>The Contour Ingress controller approaches this with a different solution.<a data-primary="Contour Ingress controller" data-type="indexterm" id="idm45611988704824"/><a data-primary="HTTPProxy custom resources" data-type="indexterm" id="idm45611988704152"/> The HTTPProxy Custom Resource handles this use case with <em>root</em> HTTPProxy resources and <em>inclusion</em>. In short, a root HTTPProxy specifies the host and then <em>includes</em> other HTTPProxy resources that are hosted under that domain. The idea is that the operator manages root HTTPProxy resources and assigns them to specific teams. For example, the operator would create a root HTTPProxy with the host <code>app1.bearcanoe.com</code> and <em>include</em> all HTTPProxy resources in the <code>app1</code> Namespace. See <a href="https://oreil.ly/xOzBF">Contour’s documentation</a> for more details.</p>&#13;
</div></aside>&#13;
&#13;
<p>The Ingress API supports features beyond host-based routing. Through the evolution of the Kubernetes project, Ingress controllers extended the Ingress API. Unfortunately, these extensions were made using annotations instead of evolving the Ingress resource. The problem with using annotations is that they don’t have a schema. This can result in a poor user experience, as there is no way for the API server to catch misconfigurations. To address this issue, some Ingress controllers provide Custom Resource Definitions (CRDs). These resources have well-defined APIs offering features otherwise not available through Ingress. Contour, for example, provides an <code>HTTPProxy</code> custom resource. While leveraging these CRDs gives you access to a broader array of features, you give up the ability to swap Ingress controllers if necessary. In other words, you “lock” yourself into a specific controller.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress Controllers and How They Work" data-type="sect2"><div class="sect2" id="idm45611988698056">&#13;
<h2>Ingress Controllers and How They Work</h2>&#13;
&#13;
<p>If you can recall the first time you played with Kubernetes, you probably ran into a puzzling scenario with Ingress.<a data-primary="service routing" data-secondary="Ingress" data-tertiary="Ingress controllers, how they work" data-type="indexterm" id="idm45611988696360"/><a data-primary="Ingress" data-secondary="Ingress controllers" data-type="indexterm" id="idm45611988695096"/> You downloaded a bunch of sample YAML files that included a Deployment and an Ingress and applied them to your cluster. You noticed that the Pod came up just fine, but you were not able to reach it. The Ingress resource was essentially doing nothing. You probably wondered, What’s going on here?</p>&#13;
&#13;
<p>Ingress is one of those APIs in Kubernetes that are left to the platform builder to implement. In other words, Kubernetes exposes the Ingress interface and expects another component to provide the implementation. This component is commonly called an <em>Ingress</em> &#13;
<span class="keep-together"><em>controller</em></span>.</p>&#13;
&#13;
<p>An Ingress controller is a platform component that runs in the cluster. The controller is responsible for watching the Ingress API and acting according to the configuration defined in Ingress resources.<a data-primary="reverse proxies, Ingress controllers paired with" data-type="indexterm" id="idm45611988690888"/> In most implementations, the Ingress controller is paired with a reverse proxy, such as NGINX or Envoy.<a data-primary="control plane" data-secondary="Ingress controllers" data-type="indexterm" id="idm45611988689896"/><a data-primary="data plane" data-secondary="Ingress controllers" data-type="indexterm" id="idm45611988688952"/> This two-component architecture is comparable to other software-defined networking systems, in that the controller is the <em>control plane</em> of the Ingress controller, while the proxy is the <em>data plane</em> component. <a data-type="xref" href="#the_ingress_controller_watches_various_resources_in_the_api">Figure 6-10</a> shows the control plane and data plane of an Ingress &#13;
<span class="keep-together">controller</span>.</p>&#13;
&#13;
<figure><div class="figure" id="the_ingress_controller_watches_various_resources_in_the_api">&#13;
<img alt="prku 0610" src="assets/prku_0610.png"/>&#13;
<h6><span class="label">Figure 6-10. </span>The Ingress controller watches various resources in the API server and configures the proxy accordingly. The proxy handles incoming traffic and forwards it to Pods, according to the Ingress configuration.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The control plane of the Ingress controller connects to the Kubernetes API and watches a variety of resources, such as Ingress, Services, Endpoints, and others. Whenever these resources change, the controller receives a watch notification and configures the data plane to act according to the desired state declared in the Kubernetes API.<a data-primary="API server" data-secondary="Ingress controllers and" data-type="indexterm" id="idm45611988682472"/></p>&#13;
&#13;
<p>The data plane handles the routing and load balancing of network traffic. As mentioned before, the data plane is usually implemented with an off-the-shelf proxy.</p>&#13;
&#13;
<p>Because the Ingress API builds on top of the Service abstraction, Ingress controllers have a choice between forwarding traffic through Services or sending it directly to Pods.<a data-primary="Pods" data-secondary="Ingress controllers routing traffic directly to" data-type="indexterm" id="idm45611988680328"/> Most Ingress controllers opt for the latter. They don’t use the Service resource, other than to validate that the Service referenced in the Ingress resource exists. When it comes to routing, most controllers forward traffic to the Pod IP addresses listed in the corresponding Endpoints object. Routing traffic directly to the Pod bypasses the Service layer, which reduces latency and adds different load balancing strategies.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress Traffic Patterns" data-type="sect2"><div class="sect2" id="idm45611988678552">&#13;
<h2>Ingress Traffic Patterns</h2>&#13;
&#13;
<p>A great aspect of Ingress is that each application gets to configure routing according to its needs.<a data-primary="service routing" data-secondary="Ingress" data-tertiary="Ingress traffic patterns" data-type="indexterm" id="ix_serrtIngTP"/><a data-primary="Ingress" data-secondary="traffic patterns" data-type="indexterm" id="idm45611988675400"/> Typically, each application has different requirements when it comes to handling incoming traffic. Some might require TLS termination at the edge. Some might want to handle TLS themselves, while others might not support TLS at all (hopefully, this is not the case).</p>&#13;
&#13;
<p>In this section, we will explore the common ingress traffic patterns that we have encountered. This should give you an idea of what Ingress can provide to your developers and how Ingress can fit into your platform offering.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="HTTP proxying" data-type="sect3"><div class="sect3" id="idm45611988673288">&#13;
<h3>HTTP proxying</h3>&#13;
&#13;
<p>HTTP proxying is the bread-and-butter of Ingress.<a data-primary="Ingress" data-secondary="traffic patterns" data-tertiary="HTTP proxying" data-type="indexterm" id="idm45611988671720"/><a data-primary="HTTP proxying" data-type="indexterm" id="idm45611988670472"/><a data-primary="proxies" data-secondary="HTTP proxying by Ingress" data-type="indexterm" id="idm45611988669800"/> This pattern involves exposing one or more HTTP-based services and routing traffic according to the HTTP requests’ properties. We have already discussed routing based on the <code>Host</code> header. Other properties that can influence routing decisions include the URL path, the request method, request headers, and more, depending on the Ingress controller.</p>&#13;
&#13;
<p>The following Ingress resource exposes the <code>app1</code> Service at <code>app1.example.com</code>. Any incoming request that has a matching <code>Host</code> HTTP header is sent to an <code>app1</code> Pod.</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">app1</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">rules</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">app1.example.com</code>&#13;
    <code class="nt">http</code><code class="p">:</code>&#13;
      <code class="nt">paths</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>&#13;
        <code class="nt">backend</code><code class="p">:</code>&#13;
          <code class="nt">serviceName</code><code class="p">:</code> <code class="l-Scalar-Plain">app1</code>&#13;
          <code class="nt">servicePort</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code></pre>&#13;
&#13;
<p>Once applied, the preceding configuration results in the data plane flow depicted in <a data-type="xref" href="#path_of_an_http_request_from_the_client_to_the_target_pod">Figure 6-11</a>.</p>&#13;
&#13;
<figure><div class="figure" id="path_of_an_http_request_from_the_client_to_the_target_pod">&#13;
<img alt="prku 0611" src="assets/prku_0611.png"/>&#13;
<h6><span class="label">Figure 6-11. </span>Path of an HTTP request from the client to the target Pod through the Ingress controller.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="HTTP proxying with TLS" data-type="sect3"><div class="sect3" id="idm45611988588312">&#13;
<h3>HTTP proxying with TLS</h3>&#13;
&#13;
<p>Supporting TLS encryption is table-stakes for Ingress controllers.<a data-primary="Ingress" data-secondary="traffic patterns" data-tertiary="HTTP proxying with TLS" data-type="indexterm" id="idm45611988586744"/><a data-primary="HTTP proxying" data-secondary="with TLS" data-type="indexterm" id="idm45611988585496"/><a data-primary="TLS" data-secondary="HTTP proxying with" data-type="indexterm" id="idm45611988584552"/> This ingress traffic pattern is the same as HTTP proxying from a routing perspective. However, clients communicate with the Ingress controller over a secure TLS connection instead of plain-text HTTP.</p>&#13;
&#13;
<p>The following example shows an Ingress resource that exposes <code>app1</code> with TLS. The controller gets the TLS serving certificate and key from the referenced Kubernetes Secret.</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.k8s.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Ingress</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">app1</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">tls</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">hosts</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">app1.example.com</code>&#13;
    <code class="nt">secretName</code><code class="p">:</code> <code class="l-Scalar-Plain">app1-tls-cert</code>&#13;
  <code class="nt">rules</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">app1.example.com</code>&#13;
    <code class="nt">http</code><code class="p">:</code>&#13;
      <code class="nt">paths</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>&#13;
        <code class="nt">backend</code><code class="p">:</code>&#13;
          <code class="nt">serviceName</code><code class="p">:</code> <code class="l-Scalar-Plain">app1</code>&#13;
          <code class="nt">servicePort</code><code class="p">:</code> <code class="l-Scalar-Plain">443</code></pre>&#13;
&#13;
<p>Ingress controllers support different configurations when it comes to the connection between the Ingress controller and the backend service. The connection between the external client and the controller is secure (TLS), while the connection between &#13;
<span class="keep-together">the Ingress</span> controller and the backend application does not have to be. Whether the &#13;
<span class="keep-together">connection</span> between the controller and the backend is secure depends on whether the application is listening for TLS connections. By default, most Ingress controllers terminate TLS and forward requests over an unencrypted connection, as depicted in <a data-type="xref" href="#Ingress_controller_handling_an_HTTPS">Figure 6-12</a>.</p>&#13;
&#13;
<figure><div class="figure" id="Ingress_controller_handling_an_HTTPS">&#13;
<img alt="prku 0612" src="assets/prku_0612.png"/>&#13;
<h6><span class="label">Figure 6-12. </span>Ingress controller handling an HTTPS request by terminating TLS and forwarding the request to the backend Pod over an unencrypted connection.</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the case where a secure connection to the backend is required, the Ingress controller terminates the TLS connection at the edge and establishes a new TLS connection with the backend (illustrated in <a data-type="xref" href="#ingress_controller_terminating_tls_and_establishing_a_new_tls">Figure 6-13</a>). The reestablishment of the TLS connection is sometimes not appropriate for certain applications, such as those that need to perform the TLS handshake with their clients. In these situations, TLS passthrough, which we will discuss further later, is a viable alternative.</p>&#13;
&#13;
<figure><div class="figure" id="ingress_controller_terminating_tls_and_establishing_a_new_tls">&#13;
<img alt="prku 0613" src="assets/prku_0613.png"/>&#13;
<h6><span class="label">Figure 6-13. </span>Ingress controller terminating TLS and establishing a new TLS connection with the backend Pod when handling HTTPS requests.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Layer 3/4 proxying" data-type="sect3"><div class="sect3" id="idm45611988496040">&#13;
<h3>Layer 3/4 proxying</h3>&#13;
&#13;
<p>Even though the Ingress API’s primary<a data-primary="layer 7 proxying" data-type="indexterm" id="idm45611988494312"/> focus is layer 7 proxying (HTTP traffic), some Ingress controllers can proxy traffic at layer 3/4 (TCP/UDP traffic).<a data-primary="proxies" data-secondary="layer 3/4 proxying (TCP/UDP traffic)" data-type="indexterm" id="idm45611988493352"/><a data-primary="Ingress" data-secondary="traffic patterns" data-tertiary="layer 3/4 proxying" data-type="indexterm" id="idm45611988492440"/><a data-primary="TCP" data-secondary="proxying by Ingress controllers" data-type="indexterm" id="idm45611988491224"/><a data-primary="UDP" data-secondary="proxying by Ingress controllers" data-type="indexterm" id="idm45611988490312"/><a data-primary="layer 3/4 proxying" data-type="indexterm" id="idm45611988489352"/> This can be useful if you need to expose applications that do not speak HTTP. When evaluating Ingress controllers, you must keep this in mind, as support for layer 3/4 proxying varies across controllers.</p>&#13;
&#13;
<p>The main challenge with proxying TCP or UDP services is that Ingress controllers listen on a limited number of ports, usually 80 and 443. As you can imagine, exposing different TCP or UDP services on the same port is impossible without a strategy to distinguish the traffic. Ingress controllers solve this problem in different ways.<a data-primary="Contour Ingress controller" data-secondary="proxying of TLS encrypted TCP connections" data-type="indexterm" id="idm45611988487592"/> Some, such as Contour, support proxying of only TLS encrypted TCP connections that use the Server Name Indication (SNI) TLS extension.<a data-primary="TLS" data-secondary="proxying of TLS encrypted TCP connections" data-type="indexterm" id="idm45611988486360"/><a data-primary="Server Name Indication (SNI) TLS extension" data-type="indexterm" id="idm45611988485384"/> The reason for this is that Contour needs to know where the traffic is headed. And when using SNI, the target domain name is available (unencrypted) in the ClientHello message of the TLS handshake. Because TLS and SNI are dependent on TCP, Contour does not support UDP &#13;
<span class="keep-together">proxying</span>.</p>&#13;
&#13;
<p>The following is a sample HTTPProxy Custom Resource, which is supported by Contour.<a data-primary="HTTPProxy custom resources" data-type="indexterm" id="idm45611988483160"/> Layer 3/4 proxying is one of those cases where a Custom Resource provides a better experience than the Ingress API:</p>&#13;
&#13;
<pre data-type="programlisting">apiVersion: projectcontour.io/v1&#13;
kind: HTTPProxy&#13;
metadata:&#13;
  name: tcp-proxy&#13;
spec:&#13;
  virtualhost:&#13;
    fqdn: tcp.bearcanoe.com&#13;
    tls:&#13;
      secretName: secret&#13;
  tcpproxy:&#13;
    services:&#13;
    - name: tcp-app&#13;
      port: 8080</pre>&#13;
&#13;
<p>With the preceding configuration, Contour reads the server name in the SNI extension and proxies the traffic to the backend TCP service. <a data-type="xref" href="#the_ingress_controller_inspects_the_sni_header_to_determine_the_backend_terminates_the_tls_connection">Figure 6-14</a> illustrates this capability.</p>&#13;
&#13;
<figure><div class="figure" id="the_ingress_controller_inspects_the_sni_header_to_determine_the_backend_terminates_the_tls_connection">&#13;
<img alt="prku 0614" src="assets/prku_0614.png"/>&#13;
<h6><span class="label">Figure 6-14. </span>The Ingress controller inspects the SNI header to determine the backend, terminates the TLS connection, and forwards the TCP traffic to the Pod.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Other Ingress controllers expose configuration parameters that you can use to tell the underlying proxy to bind additional ports for layer 3/4 proxying. You then map these additional ports to specific services running in the cluster. This is the approach that the community-led NGINX Ingress controller takes for layer 3/4 proxying.</p>&#13;
&#13;
<p>A common use case of layer 3/4 proxying is TLS passthrough.<a data-primary="TLS" data-secondary="TLS passthrough" data-type="indexterm" id="idm45611988476440"/> TLS passthrough involves an application that exposes a TLS endpoint and the need to handle the TLS handshake directly with the client. As we discussed in the “HTTP proxying with TLS” pattern, the Ingress controller usually terminates the client-facing TLS connection. The TLS termination is necessary so that the Ingress controller can inspect the HTTP request, which would otherwise be encrypted. However, with TLS passthrough, the Ingress controller does not terminate TLS and instead proxies the secure connection to a backend Pod. <a data-type="xref" href="#when_tls_passthrough_is_enabled_the_ingress_controller_inspects">Figure 6-15</a> depicts TLS passthrough.</p>&#13;
&#13;
<figure><div class="figure" id="when_tls_passthrough_is_enabled_the_ingress_controller_inspects">&#13;
<img alt="prku 0615" src="assets/prku_0615.png"/>&#13;
<h6><span class="label">Figure 6-15. </span>When TLS passthrough is enabled, the Ingress controller inspects the SNI header to determine the backend and forwards the TLS connection accordingly.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Choosing an Ingress Controller" data-type="sect2"><div class="sect2" id="idm45611988495416">&#13;
<h2>Choosing an Ingress Controller</h2>&#13;
&#13;
<p>There are several<a data-primary="Ingress" data-secondary="Ingress controllers" data-tertiary="choosing a controller" data-type="indexterm" id="idm45611988470392"/><a data-primary="service routing" data-secondary="Ingress" data-tertiary="choosing an Ingress controller" data-type="indexterm" id="idm45611988469112"/> Ingress controllers that you can choose from.<a data-primary="service routing" data-secondary="Ingress" data-startref="ix_serrtIngTP" data-tertiary="Ingress traffic patterns" data-type="indexterm" id="idm45611988467752"/> In our experience, the NGINX Ingress controller is one of the most commonly used.<a data-primary="NGINX Ingress controller" data-type="indexterm" id="idm45611988466120"/> However, that does not mean it is best for your application platform. Other choices include Contour, HAProxy, Traefik, and more. In keeping with this book’s theme, our goal is not to tell you which to use. Instead, we aim to equip you with the information you need to make this decision. We will also highlight significant trade-offs where applicable.</p>&#13;
&#13;
<p>Stepping back a bit, the primary goal of an Ingress controller is to handle application traffic. Thus, it is natural to turn to the applications as the primary factor when selecting an Ingress controller. Specifically, what are the features and requirements that your applications need? The following is a list of criteria that you can use to evaluate Ingress controllers from an application support perspective:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Do applications expose HTTPS endpoints? Do they need to handle the TLS handshake with the client directly, or is it okay to terminate TLS at the edge?</p>&#13;
</li>&#13;
<li>&#13;
<p>What SSL ciphers do the applications use?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do applications need session affinity or sticky sessions?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do applications need advanced request routing capabilities, such as HTTP header-based routing, cookie-based routing, HTTP method-based routing, and others?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do applications have different load balancing algorithm requirements, such as round-robin, weighted least request, or random?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do applications need support for Cross-Origin Resource Sharing (CORS)?</p>&#13;
</li>&#13;
<li>&#13;
<p>Do applications offload authentication concerns to an external system? Some Ingress controllers provide authentication features that you can leverage to provide a common authentication mechanism across applications.</p>&#13;
</li>&#13;
<li>&#13;
<p>Are there any applications that need to expose TCP or UDP endpoints?</p>&#13;
</li>&#13;
<li>&#13;
<p>Does the application need the ability to rate-limit incoming traffic?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In addition to application requirements, another crucial consideration to make is your organization’s experience with the data plane technology. If you are already intimately familiar with a specific proxy, it is usually a safe bet to start there. You will already have a good understanding of how it works, and more importantly, you will know its limitations and how to troubleshoot it.</p>&#13;
&#13;
<p>Supportability is another critical factor to consider. Ingress is an essential component of your platform. It exists right in the middle of your customers and the services they are trying to reach. When things go wrong with your Ingress controller, you want to have access to the support you need when facing an outage.</p>&#13;
&#13;
<p>Finally, remember that you can run multiple Ingress controllers in your platform using Ingress classes. Doing so increases the complexity and management of your platform, but it is necessary in some cases. The higher the adoption of your platform and the more production workloads you are running, the more features they will demand from your Ingress tier. It is entirely possible that you will end up having a set of requirements that cannot be fulfilled with a single Ingress controller.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress Controller Deployment Considerations" data-type="sect2"><div class="sect2" id="idm45611988422632">&#13;
<h2>Ingress Controller Deployment Considerations</h2>&#13;
&#13;
<p>Regardless of the Ingress controller, there is a set <a data-primary="service routing" data-secondary="Ingress" data-tertiary="Ingress controller deployment considerations" data-type="indexterm" id="ix_serrtIngdply"/><a data-primary="Ingress" data-secondary="Ingress controller deployment considerations" data-type="indexterm" id="ix_Ingdply"/>of considerations that you should keep in mind when it comes to deploying and operating the Ingress tier. Some of these considerations can also have an impact on the applications running on the &#13;
<span class="keep-together">platform</span>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dedicated Ingress nodes" data-type="sect3"><div class="sect3" id="idm45611988417048">&#13;
<h3>Dedicated Ingress nodes</h3>&#13;
&#13;
<p>Dedicating (or reserving) a set of<a data-primary="nodes" data-secondary="dedicated to running Ingress controller" data-type="indexterm" id="idm45611988415512"/><a data-primary="Ingress" data-secondary="Ingress controller deployment considerations" data-tertiary="dedicated Ingress nodes" data-type="indexterm" id="idm45611988414568"/> nodes to run the Ingress controller and thus serve as the cluster’s “edge” is a pattern that we have found very successful. <a data-type="xref" href="#dedicated_ingress_nodes_are_reserved_for_the_ingress_controller">Figure 6-16</a> illustrates this deployment pattern. At first, it might seem wasteful to use dedicated ingress nodes. However, our philosophy is, if you can afford to run dedicated control plane nodes, you can probably afford to dedicate nodes to the layer that is in the critical path for all workloads on the cluster. Using a dedicated node pool for Ingress brings considerable benefits.</p>&#13;
&#13;
<figure><div class="figure" id="dedicated_ingress_nodes_are_reserved_for_the_ingress_controller">&#13;
<img alt="prku 0616" src="assets/prku_0616.png"/>&#13;
<h6><span class="label">Figure 6-16. </span>Dedicated ingress nodes are reserved for the Ingress controller. The ingress nodes serve as the “edge” of the cluster or the Ingress tier.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The primary benefit is resource isolation. Even though Kubernetes has support for configuring resource requests and limits, we have found that platform teams can struggle with getting those parameters right. This is especially true when the platform team is at the beginning of their Kubernetes journey and is unaware of the implementation details that underpin resource management (e.g., the Completely Fair Scheduler, cgroups). Furthermore, at the time of writing, Kubernetes does not support resource isolation for network I/O or file descriptors, making it challenging to guarantee the fair sharing of these resources.</p>&#13;
&#13;
<p>Another reason for running Ingress controllers on dedicated nodes is compliance. We have encountered that a large number of organizations have pre-established firewall rules and other security practices that can be incompatible with Ingress controllers. Dedicated ingress nodes are useful in these environments, as it is typically easier to get exceptions for a subset of cluster nodes instead of all of them.</p>&#13;
&#13;
<p>Finally, limiting the number of nodes that run the Ingress controller can be helpful in bare-metal or on-premises installations. In such deployments, the Ingress tier is fronted by a hardware load balancer. In most cases, these are traditional load balancers that lack APIs and must be statically configured to route traffic to a specific set of backends. Having a small number of ingress nodes eases the configuration and management of these external load balancers.</p>&#13;
&#13;
<p>Overall, dedicating nodes to Ingress can help with performance, compliance, and managing external load balancers.<a data-primary="performance" data-secondary="dedicating nodes to Ingress" data-type="indexterm" id="idm45611988406376"/> The best approach to implement dedicated ingress nodes is to label and taint the ingress nodes. Then, deploy the Ingress controller as a DaemonSet that (1) tolerates the taint, and (2) has a node selector that targets the ingress nodes. With this approach, ingress node failures must be accounted for, as Ingress controllers will not run on nodes other than those reserved for Ingress. In the ideal case, failed nodes are automatically replaced with new nodes that can continue handling Ingress traffic.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Binding to the host network" data-type="sect3"><div class="sect3" id="idm45611988404648">&#13;
<h3>Binding to the host network</h3>&#13;
&#13;
<p>To optimize the ingress traffic path, you can bind your Ingress controller to the underlying host’s network.<a data-primary="host network, binding Ingress controller to" data-type="indexterm" id="idm45611988403112"/><a data-primary="Ingress" data-secondary="Ingress controller deployment considerations" data-tertiary="binding to the host network" data-type="indexterm" id="idm45611988402312"/> By doing so, incoming requests bypass the Kubernetes Service fabric and reach the Ingress controller directly.<a data-primary="DNS (Domain Name System)" data-secondary="DNS policy of Ingress controller" data-type="indexterm" id="idm45611988400744"/> When enabling host networking, ensure that the Ingress controller’s DNS policy is set to <code>ClusterFirstWithHostNet</code>. <a data-primary="ClusterFirstWithHostNet DNS policy" data-type="indexterm" id="idm45611988399144"/>The following snippet shows the host networking and DNS policy settings in a Pod template:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">containers</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
  <code class="nt">dnsPolicy</code><code class="p">:</code> <code class="l-Scalar-Plain">ClusterFirstWithHostNet</code>&#13;
  <code class="nt">hostNetwork</code><code class="p">:</code> <code class="l-Scalar-Plain">true</code></pre>&#13;
&#13;
<p>While running the Ingress controller directly on the host network can increase performance, you must keep in mind that doing so removes the network namespace boundary between the Ingress controller and the node. In other words, the Ingress controller has full access to all network interfaces and network services available on the host. This has implications on the Ingress controller’s threat model. Namely, it lowers the bar for an adversary to perform lateral movement in the case of a data plane proxy vulnerability. Additionally, attaching to the host network is a privileged operation. Thus, the Ingress controller needs elevated privileges or exceptions to run as a privileged workload.</p>&#13;
&#13;
<p>Even then, we’ve found that binding to the host network is worth the trade-off and is usually the best way to expose the platform’s Ingress controllers. The ingress traffic arrives directly at the controller’s gate, instead of traversing the Service stack (which can be suboptimal, as discussed in <a data-type="xref" href="#kubernetes_services">“Kubernetes Services”</a>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingress controllers and external traffic policy" data-type="sect3"><div class="sect3" id="idm45611988353912">&#13;
<h3>Ingress controllers and external traffic policy</h3>&#13;
&#13;
<p>Unless configured properly, using a Kubernetes Service to expose the Ingress controller impacts the performance of the Ingress data plane.<a data-primary="performance" data-secondary="use of Service to expose Ingress controller" data-type="indexterm" id="idm45611988376056"/><a data-primary="Ingress" data-secondary="Ingress controller deployment considerations" data-tertiary="external traffic policy" data-type="indexterm" id="idm45611988374984"/></p>&#13;
&#13;
<p>If you recall from <a data-type="xref" href="#kubernetes_services">“Kubernetes Services”</a>, a Service’s external traffic policy determines<a data-primary="external traffic policy" data-type="indexterm" id="idm45611988372488"/> how to handle traffic that’s coming from outside the cluster. If you are using a NodePort or LoadBalancer Service to expose the Ingress controller, ensure that you set the external traffic policy to <code>Local</code><a data-primary="Local external traffic policy setting" data-type="indexterm" id="idm45611988371208"/>.<a data-primary="LoadBalancer Service" data-secondary="external traffic policy setting" data-type="indexterm" id="idm45611988370408"/><a data-primary="NodePort Service" data-secondary="external traffic policy setting" data-type="indexterm" id="idm45611988369464"/></p>&#13;
&#13;
<p>Using the <code>Local</code> policy avoids unnecessary network hops, as the external traffic reaches the local Ingress controller instead of hopping to another node. Furthermore, the <code>Local</code> policy doesn’t use SNAT, which means the client IP address is visible to applications handling the requests.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Spread Ingress controllers across failure domains" data-type="sect3"><div class="sect3" id="idm45611988366744">&#13;
<h3>Spread Ingress controllers across failure domains</h3>&#13;
&#13;
<p>To ensure the high-availability of your <a data-primary="Ingress" data-secondary="Ingress controller deployment considerations" data-tertiary="spreading controllers across failure domains" data-type="indexterm" id="idm45611988365320"/>Ingress controller fleet, use Pod anti-affinity rules to spread the Ingress controllers across different failure domains.<a data-primary="Pods" data-secondary="anti-affinity rules" data-type="indexterm" id="idm45611988363672"/><a data-primary="service routing" data-secondary="Ingress" data-startref="ix_serrtIngdply" data-tertiary="Ingress controller deployment considerations" data-type="indexterm" id="idm45611988362728"/><a data-primary="Ingress" data-secondary="Ingress controller deployment considerations" data-startref="ix_Ingdply" data-type="indexterm" id="idm45611988361208"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DNS and Its Role in Ingress" data-type="sect2"><div class="sect2" id="idm45611988359704">&#13;
<h2>DNS and Its Role in Ingress</h2>&#13;
&#13;
<p>As we have discussed in this chapter, applications running on the platform share the<a data-primary="service routing" data-secondary="Ingress" data-tertiary="DNS and its role in" data-type="indexterm" id="ix_serrtIngDNS"/><a data-primary="DNS (Domain Name System)" data-secondary="and role in Ingress" data-type="indexterm" id="ix_DNSIng"/><a data-primary="Ingress" data-secondary="DNS and its role in" data-type="indexterm" id="ix_IngDNS"/> ingress data plane, and thus share that single entry point into the platform’s network. As requests come in, the Ingress controller’s primary responsibility is to disambiguate traffic and route it according to the Ingress configuration.</p>&#13;
&#13;
<p>One of the primary ways to determine the destination of a request is by the target<a data-primary="Host header, routing based on" data-type="indexterm" id="idm45611988329064"/> hostname (the <code>Host</code> header in the case of HTTP or SNI in the case of TCP), turning DNS into an essential player of your Ingress implementation. We will discuss two of the main approaches that are available when it comes to DNS and Ingress.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Wildcard DNS record" data-type="sect3"><div class="sect3" id="idm45611988327688">&#13;
<h3>Wildcard DNS record</h3>&#13;
&#13;
<p>One of the most successful patterns we continuously use is to assign a domain name to the environment and slice it up by assigning subdomains to different applications.<a data-primary="Ingress" data-secondary="DNS and its role in" data-tertiary="wildcard DNS record" data-type="indexterm" id="idm45611988326264"/><a data-primary="DNS (Domain Name System)" data-secondary="and role in Ingress" data-tertiary="wildcard DNS record" data-type="indexterm" id="idm45611988325016"/><a data-primary="wildcard DNS records" data-type="indexterm" id="idm45611988323832"/> We sometimes call this “subdomain-based routing.” The implementation of this pattern involves creating a wildcard DNS record (e.g., <code>*.bearcanoe.com</code>) that resolves to the Ingress tier of the cluster. Typically, this is a load balancer that is in front of the Ingress controllers.</p>&#13;
&#13;
<p>There are several benefits to using a wildcard DNS record for your Ingress &#13;
<span class="keep-together">controllers</span>:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Applications can use any path under their subdomain, including the root path (<code>/</code>). Developers don’t have to spend engineering hours to make their apps work on subpaths. In some cases, applications expect to be hosted at the root path and do not work otherwise.</p>&#13;
</li>&#13;
<li>&#13;
<p>The DNS implementation is relatively straightforward. There is no integration necessary between Kubernetes and your DNS provider.</p>&#13;
</li>&#13;
<li>&#13;
<p>The single wildcard DNS record removes DNS propagation concerns that could arise when using different domain names for each application.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes and DNS integration" data-type="sect3"><div class="sect3" id="idm45611988316808">&#13;
<h3>Kubernetes and DNS integration</h3>&#13;
&#13;
<p>An alternative to using a wildcard DNS record is to integrate your platform with your DNS provider.<a data-primary="DNS (Domain Name System)" data-secondary="and role in Ingress" data-tertiary="Kubernetes and DNS integration" data-type="indexterm" id="idm45611988315224"/><a data-primary="Ingress" data-secondary="DNS and its role in" data-tertiary="Kubernetes and DNS integration" data-type="indexterm" id="idm45611988313896"/> The Kubernetes community maintains a controller that offers this integration called <a href="https://github.com/kubernetes-sigs/external-dns">external-dns</a>. <a data-primary="external-dns controller" data-type="indexterm" id="idm45611988311848"/>If you are using a DNS provider that is supported, consider using this controller to automate the creation of domain names.</p>&#13;
&#13;
<p>As you might expect from a Kubernetes controller, external-dns continuously reconciles the DNS records in your upstream DNS provider and the configuration defined in Ingress resources. In other words, external-dns creates, updates, and deletes DNS records according to changes that happen in the Ingress API. External-dns needs two pieces of information to configure the DNS records, both of which are part of the Ingress resource: the desired hostname, which is in the Ingress specification, and the target IP address, which is available in the status field of the Ingress resource.</p>&#13;
&#13;
<p>Integrating the platform with your DNS provider can be useful if you need to support multiple domain names. The controller takes care of automatically creating DNS records as needed. However, it is important to keep the following trade-offs in mind:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You have to deploy an additional component (external-dns) into your cluster. An additional add-on brings about more complexity into your deployments, given that you have to operate, maintain, monitor, version, and upgrade one more component in your platform.</p>&#13;
</li>&#13;
<li>&#13;
<p>If external-dns does not support your DNS provider, you have to develop your own controller. Building and maintaining a controller requires engineering effort that could be spent on higher-value efforts. In these situations, it is best to simply implement a wildcard DNS record.<a data-primary="service routing" data-secondary="Ingress" data-startref="ix_serrtIngDNS" data-tertiary="DNS and its role in" data-type="indexterm" id="idm45611988306696"/><a data-primary="DNS (Domain Name System)" data-secondary="and role in Ingress" data-startref="ix_DNSIng" data-type="indexterm" id="idm45611988305176"/><a data-primary="Ingress" data-secondary="DNS and its role in" data-startref="ix_IngDNS" data-type="indexterm" id="idm45611988303944"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Handling TLS Certificates" data-type="sect2"><div class="sect2" id="idm45611988302344">&#13;
<h2>Handling TLS Certificates</h2>&#13;
&#13;
<p>Ingress controllers need certificates and their corresponding private keys to serve applications over TLS.<a data-primary="TLS" data-secondary="handling TLS certificates" data-type="indexterm" id="ix_TLScert"/><a data-primary="service routing" data-secondary="Ingress" data-tertiary="handling TLS certificates" data-type="indexterm" id="ix_serrtIngTLS"/><a data-primary="Ingress" data-secondary="handling TLS certificates" data-type="indexterm" id="ix_IngTLS"/><a data-primary="cert-manager controller" data-type="indexterm" id="ix_cert"/> Depending on your Ingress strategy, managing certificates can be cumbersome. If your cluster hosts a single domain name and implements subdomain-based routing, you can use a single wildcard TLS certificate. In some cases, however, clusters host applications across a variety of domains, making it challenging to manage certificates efficiently. Furthermore, your security team might frown upon the usage of wildcard certificates. In any case, the Kubernetes community has rallied around a certificate management add-on that eases the minting and management of certificates. The add-on is aptly called <a href="https://cert-manager.io">cert-manager</a>.</p>&#13;
&#13;
<p>Cert-manager is a controller that runs in your cluster. It installs a set of CRDs that enable declarative management of Certificate Authorities (CAs) and Certificates via the Kubernetes API. More importantly, it supports different certificate issuers, including ACME-based CAs, HashiCorp Vault, Venafi, etc. It also offers an extension point to implement custom issuers, when necessary.</p>&#13;
&#13;
<p>The certificate minting features of cert-manager revolve around issuers and certificates. Cert-manager has two issuer Custom Resources.<a data-primary="Issuer resource" data-type="indexterm" id="idm45611988292904"/> The Issuer resource represents a CA that signs certificates in a specific Kubernetes Namespace. If you want to issue certificates across all Namespaces, you can use the ClusterIssuer resource.<a data-primary="ClusterIssuer resource" data-type="indexterm" id="idm45611988291864"/> The following is a sample ClusterIssuer definition that uses a private key stored in a Kubernetes Secret named <code>platform-ca-key-pair</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">cert-manager.io/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">ClusterIssuer</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">prod-ca-issuer</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">ca</code><code class="p">:</code>&#13;
    <code class="nt">secretName</code><code class="p">:</code> <code class="l-Scalar-Plain">platform-ca-key-pair</code></pre>&#13;
&#13;
<p>The great thing about cert-manager is that it integrates with the Ingress API to automatically mint certificates for Ingress resources. For example, given the following Ingress object, cert-manager automatically creates a certificate key pair suitable for TLS:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">networking.k8s.io/v1</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Ingress</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">annotations</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="nt">cert-manager.io/cluster-issuer</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">prod-ca-issuer</code><code> </code><a class="co" href="#callout_service_routing_CO7-1" id="co_service_routing_CO7-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">bearcanoe-com</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">tls</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">hosts</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">bearcanoe.com</code><code>&#13;
</code><code>    </code><code class="nt">secretName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">bearcanoe-cert-key-pair</code><code> </code><a class="co" href="#callout_service_routing_CO7-2" id="co_service_routing_CO7-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="nt">rules</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">host</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">bearcanoe.com</code><code>&#13;
</code><code>    </code><code class="nt">http</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="nt">paths</code><code class="p">:</code><code>&#13;
</code><code>      </code><code class="p-Indicator">-</code><code> </code><code class="nt">path</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">/</code><code>&#13;
</code><code>        </code><code class="nt">backend</code><code class="p">:</code><code>&#13;
</code><code>          </code><code class="nt">serviceName</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">nginx</code><code>&#13;
</code><code>          </code><code class="nt">servicePort</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">80</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO7-1" id="callout_service_routing_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>cert-manager.io/cluster-issuer</code> annotation tells cert-manager to use the <code>prod-ca-issuer</code> to mint the certificate.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO7-2" id="callout_service_routing_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Cert-manager stores the certificate and private key in a Kubernetes Secret called <code>bearcanoe-cert-key-pair</code>.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Behind the scenes, cert-manager handles the certificate request process, which includes generating a private key, creating a certificate signing request (CSR), and submitting the CSR to the CA. Once the issuer mints the certificate, cert-manager stores it in the <code>bearcanoe-cert-key-pair</code> certificate. The Ingress controller can then pick it up and start serving the application over TLS. <a data-type="xref" href="#cert_manager_watches_the_ingress_api_and_requests_a_certificate">Figure 6-17</a> depicts the process in more detail.</p>&#13;
&#13;
<figure><div class="figure" id="cert_manager_watches_the_ingress_api_and_requests_a_certificate">&#13;
<img alt="prku 0617" src="assets/prku_0617.png"/>&#13;
<h6><span class="label">Figure 6-17. </span>Cert-manager watches the Ingress API and requests a certificate from a Certificate Authority when the Ingress resource has the <code>cert-manager.io/cluster-issuer</code> annotation.</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you can see, cert-manager simplifies certificate management on Kubernetes. Most platforms we’ve encountered use cert-manager in some capacity. If you leverage cert-manager in your platform, consider using an external system such as Vault as the CA. Integrating cert-manager with an external system instead of using a CA backed by a Kubernetes Secret is a more robust and secure solution.<a data-primary="cert-manager controller" data-startref="ix_cert" data-type="indexterm" id="idm45611988127512"/><a data-primary="TLS" data-secondary="handling TLS certificates" data-startref="ix_TLScert" data-type="indexterm" id="idm45611988126664"/><a data-primary="service routing" data-secondary="Ingress" data-startref="ix_serrtIngTLS" data-tertiary="handling TLS certificates" data-type="indexterm" id="idm45611988125576"/><a data-primary="Ingress" data-secondary="handling TLS certificates" data-startref="ix_IngTLS" data-type="indexterm" id="idm45611988124248"/><a data-primary="service routing" data-secondary="Ingress" data-startref="ix_serrtIng" data-type="indexterm" id="idm45611988123160"/><a data-primary="Ingress" data-startref="ix_Ingr" data-type="indexterm" id="idm45611988122072"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service Mesh" data-type="sect1"><div class="sect1" id="idm45611988816408">&#13;
<h1>Service Mesh</h1>&#13;
&#13;
<p>As the industry continues to adopt containers and microservices, service meshes have gained immense popularity.<a data-primary="service routing" data-secondary="service meshes" data-type="indexterm" id="ix_serrtSM"/><a data-primary="service meshes" data-type="indexterm" id="ix_sermsh"/> While the term “service mesh” is relatively new, the concepts that it encompasses are not. Service meshes are a rehash of preexisting ideas in service routing, load balancing, and telemetry. Before the rise of containers and Kubernetes, hyperscale internet companies implemented service mesh precursors as they ran into challenges with microservices.<a data-primary="microservices" data-type="indexterm" id="idm45611988117224"/> Twitter, for example, created <a href="https://twitter.github.io/finagle">Finagle</a>, a Scala library that all its microservices embedded. It handled load balancing, circuit breaking, automatic retries, telemetry, and more. Netflix developed <a href="https://github.com/Netflix/Hystrix">Hystrix</a>, a similar library for Java applications.</p>&#13;
&#13;
<p>Containers and Kubernetes changed the landscape. Service meshes are no longer language-specific libraries like their precursors. Today, service meshes are distributed systems themselves. <a data-primary="proxies" data-secondary="service mesh" data-type="indexterm" id="idm45611988114472"/>They consist of a control plane that configures a collection of proxies that implement the data plane. The routing, load balancing, telemetry, and other capabilities are built into the proxy instead of the application. The move to the proxy model has enabled even more apps to take advantage of these features, as there’s no need to make code changes to participate in a mesh.</p>&#13;
&#13;
<p>Service meshes provide a broad set of features that can be categorized across three<a data-primary="service meshes" data-secondary="features provided by" data-type="indexterm" id="idm45611988112712"/> pillars:</p>&#13;
<dl>&#13;
<dt>Routing and reliability</dt>&#13;
<dd>&#13;
<p>Advanced traffic routing and reliability features such as traffic shifting, traffic mirroring, retries, and circuit breaking.<a data-primary="routing" data-secondary="service mesh features" data-type="indexterm" id="idm45611988110088"/></p>&#13;
</dd>&#13;
<dt>Security</dt>&#13;
<dd>&#13;
<p>Identity and access control features that enable secure communication between services, including<a data-primary="identity" data-secondary="service mesh features" data-type="indexterm" id="idm45611988107976"/><a data-primary="access control" data-secondary="service mesh features" data-type="indexterm" id="idm45611988107128"/> identity, certificate management, and mutual TLS (mTLS).<a data-primary="certificate management" data-secondary="in service meshes" data-secondary-sortas="service" data-type="indexterm" id="idm45611988106152"/><a data-primary="mTLS (mutual TLS)" data-type="indexterm" id="idm45611988105064"/></p>&#13;
</dd>&#13;
<dt>Observability</dt>&#13;
<dd>&#13;
<p>Automated gathering of metrics and traces from all the interactions happening in the mesh.<a data-primary="observability" data-secondary="service mesh features" data-type="indexterm" id="idm45611988103192"/></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Throughout the rest of this chapter, we are going to discuss service mesh in more detail. Before we do so, however, let’s return to this book’s central theme and ask “Do we need a service mesh?” Service meshes have risen in popularity as some organizations see them as a golden bullet to implement the aforementioned features. However, we have found that organizations should carefully consider the impact of adopting a service mesh.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When (Not) to Use a Service Mesh" data-type="sect2"><div class="sect2" id="idm45611988101240">&#13;
<h2>When (Not) to Use a Service Mesh</h2>&#13;
&#13;
<p>A service mesh can provide immense value to an application platform and the &#13;
<span class="keep-together">applications</span> that run atop.<a data-primary="service meshes" data-secondary="deciding when to use" data-type="indexterm" id="idm45611988099240"/> It offers an attractive feature set that your developers will appreciate. At the same time, a service mesh brings a ton of complexity that you must deal with.</p>&#13;
&#13;
<p>Kubernetes is a complex distributed system. Up to this point in the book, we have touched on some of the building blocks you need to create an application platform atop Kubernetes, and there are still a bunch of chapters left. The reality is that building a successful Kubernetes-based application platform is a lot of work. Keep this in mind when you are considering a service mesh. Tackling a service mesh implementation while you are beginning your Kubernetes journey will slow you down, if not take you down the path to failure.</p>&#13;
&#13;
<p>We have seen these cases firsthand while working in the field. We have worked with platform teams who were blinded by the shiny features of a service mesh. Granted, those features would make their platform more attractive to developers and thus increase the platform’s adoption. However, timing is important. Wait until you gain operational experience in production before thinking about service mesh.</p>&#13;
&#13;
<p>Perhaps more critical is for you to understand your requirements or the problems you are trying to solve. Putting the cart before the horse will not only increase the chances of your platform failing but also result in wasted engineering effort. A fitting example of this mistake is an organization that dove into service mesh while developing a Kubernetes-based platform that was not yet in production. “We want a service mesh because we need everything it provides,” they said. Twelve months later, the only feature they were using was the mesh’s Ingress capabilities. No mutual TLS, no fancy routing, no tracing. Just Ingress. The engineering effort to get a dedicated Ingress controller ready for production is far less than a full-featured mesh implementation. There’s something to be said for getting a minimum viable product into production and then iterating to add features moving forward.</p>&#13;
&#13;
<p>After reading this, you might feel like we think there’s no place for a service mesh in an application platform. Quite the opposite. A service mesh can solve a ton of problems if you have them, and it can bring a ton of value if you take advantage of it. In the end, we have found that a successful service mesh implementation boils down to timing it right and doing it for the right reasons.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Service Mesh Interface (SMI)" data-type="sect2"><div class="sect2" id="idm45611988094120">&#13;
<h2>The Service Mesh Interface (SMI)</h2>&#13;
&#13;
<p>Kubernetes provides<a data-primary="service meshes" data-secondary="Service Mesh Interface" data-type="indexterm" id="ix_sermshSMI"/> interfaces for a variety of pluggable components. These interfaces include the Container Runtime Interface (CRI), the Container Networking Interface (CNI), and others. As we’ve seen throughout the book, these interfaces are what makes Kubernetes such an extensible foundation. Service mesh is slowly but surely becoming an important ingredient of a Kubernetes platform. Thus, the service mesh community collaborated to build the Service Mesh Interface, or SMI.</p>&#13;
&#13;
<p>Similar to the other interfaces we’ve already discussed, the SMI specifies the interaction between Kubernetes and a service mesh. With that said, the SMI is different than other Kubernetes interfaces in that it is not part of the core Kubernetes project. Instead, the SMI project leverages CRDs to specify the interface. The SMI project also houses libraries to implement the interface, such as the SMI SDK for Go.</p>&#13;
&#13;
<p>The SMI covers the three pillars we discussed in the previous section with a set of CRDs.<a data-primary="Traffic Split API" data-type="indexterm" id="idm45611988089480"/> The Traffic Split API is concerned with routing and splitting traffic across a number of services. It enables percent-based traffic splitting, which enables different deployment scenarios such as blue-green deployments and A/B testing. The following snippet is an example of a TrafficSplit that performs a canary deployment of the “flights” web service:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">split.smi-spec.io/v1alpha3</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TrafficSplit</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">flights-canary</code><code>&#13;
</code><code>  </code><code class="nt">namespace</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">bookings</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">service</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">flights</code><code> </code><a class="co" href="#callout_service_routing_CO8-1" id="co_service_routing_CO8-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="nt">backends</code><code class="p">:</code><code> </code><a class="co" href="#callout_service_routing_CO8-2" id="co_service_routing_CO8-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">service</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">flights-v1</code><code>&#13;
</code><code>    </code><code class="nt">weight</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">70</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">service</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">flights-v2</code><code>&#13;
</code><code>    </code><code class="nt">weight</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">30</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO8-1" id="callout_service_routing_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The top-level Service that clients connect to (i.e., <code>flights.bookings.cluster.svc.local</code>).</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO8-2" id="callout_service_routing_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The backend Services that receive the traffic. The v1 version receives 70% of traffic and the v2 version receives the rest.</p></dd>&#13;
</dl>&#13;
&#13;
<p>The Traffic Access Control and Traffic Specs APIs work together to implement security features such as access control.<a data-primary="Traffic Specs API" data-type="indexterm" id="idm45611988027720"/><a data-primary="Traffic Access Control APIs" data-type="indexterm" id="idm45611988027016"/> The Traffic Access Control API provides CRDs to control the service interactions that are allowed in the mesh. With these CRDs, developers can specify access control policy that determines which services can talk to each other and under what conditions (list of allowed HTTP methods, for example). The Traffic Specs API offers a way to describe traffic, including an <code>HTTPRouteGroup</code> CRD for HTTP traffic and a <code>TCPRoute</code> for TCP traffic.<a data-primary="TCPRoute CRD" data-type="indexterm" id="idm45611988025064"/><a data-primary="HTTPRouteGroup CRD" data-type="indexterm" id="idm45611988024328"/> Together with the Traffic Access Control CRDs, these apply policy at the application level.</p>&#13;
&#13;
<p>For example, the following HTTPRouteGroup and TrafficTarget allow all requests from the bookings service to the payments service.<a data-primary="TrafficTarget" data-type="indexterm" id="idm45611987994888"/> The HTTPRouteGroup resource describes the traffic, while the TrafficTarget specifies the source and destination &#13;
<span class="keep-together">services</span>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">specs.smi-spec.io/v1alpha3</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">HTTPRouteGroup</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">payment-processing</code><code>&#13;
</code><code>  </code><code class="nt">namespace</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">payments</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">matches</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">everything</code><code> </code><a class="co" href="#callout_service_routing_CO9-1" id="co_service_routing_CO9-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>    </code><code class="nt">pathRegex</code><code class="p">:</code><code> </code><code class="s">"</code><code class="s">.*</code><code class="s">"</code><code>&#13;
</code><code>    </code><code class="nt">methods</code><code class="p">:</code><code> </code><code class="p-Indicator">[</code><code class="s">"</code><code class="s">*</code><code class="s">"</code><code class="p-Indicator">]</code><code>&#13;
</code><code class="nn">---</code><code>&#13;
</code><code class="nt">apiVersion</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">access.smi-spec.io/v1alpha2</code><code>&#13;
</code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">TrafficTarget</code><code>&#13;
</code><code class="nt">metadata</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">allow-bookings</code><code>&#13;
</code><code>  </code><code class="nt">namespace</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">payments</code><code>&#13;
</code><code class="nt">spec</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="nt">destination</code><code class="p">:</code><code> </code><a class="co" href="#callout_service_routing_CO9-2" id="co_service_routing_CO9-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>    </code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ServiceAccount</code><code>&#13;
</code><code>    </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">payments</code><code>&#13;
</code><code>    </code><code class="nt">namespace</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">payments</code><code>&#13;
</code><code>    </code><code class="nt">port</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">8080</code><code>&#13;
</code><code>  </code><code class="nt">rules</code><code class="p">:</code><code> </code><a class="co" href="#callout_service_routing_CO9-3" id="co_service_routing_CO9-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">HTTPRouteGroup</code><code>&#13;
</code><code>    </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">payment-processing</code><code>&#13;
</code><code>    </code><code class="nt">matches</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">everything</code><code>&#13;
</code><code>  </code><code class="nt">sources</code><code class="p">:</code><code> </code><a class="co" href="#callout_service_routing_CO9-4" id="co_service_routing_CO9-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="nt">kind</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">ServiceAccount</code><code>&#13;
</code><code>    </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">flights</code><code>&#13;
</code><code>    </code><code class="nt">namespace</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">bookings</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO9-1" id="callout_service_routing_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Allow all requests in this HTTPRouteGroup.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO9-2" id="callout_service_routing_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The destination service. In this case, the Pods using the <code>payments</code> Service Account in the <code>payments</code> Namespace.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO9-3" id="callout_service_routing_CO9-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The HTTPRouteGroups that control the traffic between the source and destination services.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO9-4" id="callout_service_routing_CO9-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The source service. In this case, the Pods using the <code>flights</code> Service Account in the <code>bookings</code> Namespace.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Finally, the Traffic Metrics API provides the telemetry functionality of a service mesh.<a data-primary="Traffic Metrics API" data-type="indexterm" id="idm45611987850936"/> This API is somewhat different than the rest in that it defines outputs instead of mechanisms to provide inputs. The Traffic Metrics API defines a standard to expose service metrics. Systems that need these metrics, such as monitoring systems, autoscalers, dashboards, and others, can consume them in a standardized fashion.<a data-primary="TrafficMetrics resource" data-type="indexterm" id="idm45611987804024"/> The following snippet shows an example TrafficMetrics resource that exposes metrics for traffic between two Pods:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">metrics.smi-spec.io/v1alpha1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">TrafficMetrics</code>&#13;
<code class="nt">resource</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">flights-19sk18sj11-a9od2</code>&#13;
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">bookings</code>&#13;
  <code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>&#13;
<code class="nt">edge</code><code class="p">:</code>&#13;
  <code class="nt">direction</code><code class="p">:</code> <code class="l-Scalar-Plain">to</code>&#13;
  <code class="nt">side</code><code class="p">:</code> <code class="l-Scalar-Plain">client</code>&#13;
  <code class="nt">resource</code><code class="p">:</code>&#13;
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">payments-ks8xoa999x-xkop0</code>&#13;
    <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">payments</code>&#13;
    <code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>&#13;
<code class="nt">timestamp</code><code class="p">:</code> <code class="l-Scalar-Plain">2020-08-09T01:07:23Z</code>&#13;
<code class="nt">window</code><code class="p">:</code> <code class="l-Scalar-Plain">30s</code>&#13;
<code class="nt">metrics</code><code class="p">:</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">p99_response_latency</code>&#13;
  <code class="nt">unit</code><code class="p">:</code> <code class="l-Scalar-Plain">seconds</code>&#13;
  <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">13m</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">p90_response_latency</code>&#13;
  <code class="nt">unit</code><code class="p">:</code> <code class="l-Scalar-Plain">seconds</code>&#13;
  <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">7m</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">p50_response_latency</code>&#13;
  <code class="nt">unit</code><code class="p">:</code> <code class="l-Scalar-Plain">seconds</code>&#13;
  <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">3m</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">success_count</code>&#13;
  <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">100</code>&#13;
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">failure_count</code>&#13;
  <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">0</code></pre>&#13;
&#13;
<p>The SMI is one of the newest interfaces in the Kubernetes community. While still under development and iteration, it paints the picture of where we are headed as a community. As with other interfaces in Kubernetes, the SMI enables platform builders to offer a service mesh using portable and provider-agnostic APIs, further increasing the value, flexibility, and power of Kubernetes.<a data-primary="service meshes" data-secondary="Service Mesh Interface" data-startref="ix_sermshSMI" data-type="indexterm" id="idm45611987800856"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Data Plane Proxy" data-type="sect2"><div class="sect2" id="idm45611988093528">&#13;
<h2>The Data Plane Proxy</h2>&#13;
&#13;
<p>The data plane of a service mesh is a collection of proxies that connect services together. <a data-primary="proxies" data-secondary="data plane proxy in service mesh" data-type="indexterm" id="ix_prxDP"/><a data-primary="service meshes" data-secondary="data plane proxy" data-type="indexterm" id="ix_sermshDPP"/>The <a href="https://www.envoyproxy.io">Envoy proxy</a> is one of the most popular service proxies in the cloud native ecosystem.<a data-primary="Envoy proxy" data-type="indexterm" id="ix_Env"/> Originally developed at Lyft, it has quickly become a prevalent building block in cloud native systems since it was open sourced in <a href="https://oreil.ly/u5fCD">late 2016</a>.</p>&#13;
&#13;
<p>Envoy is used in Ingress controllers (<a href="https://projectcontour.io">Contour</a>), API gateways (<a href="https://www.getambassador.io">Ambassador</a>, <a href="https://docs.solo.io/gloo/latest">Gloo</a>), and, you guessed it, service meshes (<a href="https://istio.io">Istio</a>, <a href="https://github.com/openservicemesh/osm">OSM</a>).</p>&#13;
&#13;
<p>One of the reasons why Envoy is such a good building block is its support for dynamic configuration over a gRPC/REST API. Open source proxies that predate Envoy were not designed for environments as dynamic as Kubernetes. They used static configuration files and required restarts for configuration changes to take effect.<a data-primary="xDS APIs (Envoy)" data-type="indexterm" id="idm45611987698488"/> Envoy, on the other <a data-primary="Envoy proxy" data-secondary="dynamic configuration via xDS APIs" data-type="indexterm" id="idm45611987697656"/>hand, offers the xDS (* discovery service) APIs for dynamic configuration (depicted in <a data-type="xref" href="#envoy_supports_dynamic_configuration_via_the_xds_api">Figure 6-18</a>). It also supports hot restarts, which allow Envoy to reinitialize without dropping any active connections.</p>&#13;
&#13;
<figure><div class="figure" id="envoy_supports_dynamic_configuration_via_the_xds_api">&#13;
<img alt="prku 0618" src="assets/prku_0618.png"/>&#13;
<h6><span class="label">Figure 6-18. </span>Envoy supports dynamic configuration via the XDS APIs. Envoy connects to a configuration server and requests its configuration using LDS, RDS, EDS, CDS, and other xDS APIs.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Envoy’s xDS is a collection of APIs that includes the Listener Discovery Service (LDS), the Cluster Discovery Service (CDS), the Endpoints Discovery Service (EDS), the Route Discovery Service (RDS), and more.<a data-primary="Route Discovery Service (RDS)" data-type="indexterm" id="idm45611987692824"/><a data-primary="endpoints" data-secondary="Endpoints Discovery Service (EDS) in Envoy" data-type="indexterm" id="idm45611987692104"/><a data-primary="Cluster Discovery Service (CDS)" data-type="indexterm" id="idm45611987691128"/><a data-primary="Listener Discovery Service (LDS)" data-type="indexterm" id="idm45611987690440"/> An Envoy <em>configuration server</em> implements these APIs and behaves as the source of dynamic configuration for Envoy. During startup, Envoy reaches out to a configuration server (typically over gRPC) and subscribes to configuration changes. As things change in the environment, the configuration server streams changes to Envoy. Let’s review the xDS APIs in more detail.</p>&#13;
&#13;
<p>The LDS API configures Envoy’s <em>Listeners</em>.<a data-primary="Listeners (Envoy)" data-type="indexterm" id="idm45611987688024"/> Listeners are the entry point into the proxy. Envoy can open multiple Listeners that clients can connect to. A typical example is listening on ports 80 and 443 for HTTP and HTTPS traffic.</p>&#13;
&#13;
<p>Each Listener has a set of filter chains that determine how to handle incoming traffic. The HTTP connection manager filter leverages the RDS API to obtain routing configuration. The routing configuration tells Envoy how to route incoming HTTP requests. It provides details around virtual hosts and request matching (path-based, header-based, and others).</p>&#13;
&#13;
<p>Each route in the routing configuration references a <em>Cluster</em>. A cluster is a collection of <em>Endpoints</em> that belong to the same service. Envoy discovers Clusters and Endpoints using the CDS and EDS APIs, respectively. Interestingly enough, the EDS API does not have an Endpoint object per se. Instead, Endpoints are assigned to clusters using <em>ClusterLoadAssignment</em> objects.</p>&#13;
&#13;
<p>While digging into the details of the xDS APIs merits its own book, we hope the preceding overview gives you an idea of how Envoy works and its capabilities. To summarize, listeners bind to ports and accept connections from clients. Listeners have filter chains that determine what to do with incoming connections. For example, the HTTP filter inspects requests and maps them to clusters. Each cluster has one or more endpoints that end up receiving and handling the traffic. <a data-type="xref" href="#envoy_configuration_with_a_listener_that_binds_to_port_80">Figure 6-19</a> shows a graphical representation of these concepts and how they relate to each other.</p>&#13;
&#13;
<figure><div class="figure" id="envoy_configuration_with_a_listener_that_binds_to_port_80">&#13;
<img alt="prku 0619" src="assets/prku_0619.png"/>&#13;
<h6><span class="label">Figure 6-19. </span>Envoy configuration with a Listener that binds to port 80. The Listener has an HTTP connection manager filter that references a routing configuration. The routing config matches requests with <code>/</code> prefix and forwards requests to the <code>my_service</code> cluster, which has three endpoints.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Service Mesh on Kubernetes" data-type="sect2"><div class="sect2" id="idm45611987709320">&#13;
<h2>Service Mesh on Kubernetes</h2>&#13;
&#13;
<p>In the previous section, we discussed how the data plane of a service mesh provides connectivity between services.<a data-primary="Envoy proxy" data-startref="ix_Env" data-type="indexterm" id="idm45611987677464"/><a data-primary="proxies" data-secondary="data plane proxy in service mesh" data-startref="ix_prxDP" data-type="indexterm" id="idm45611987676488"/><a data-primary="service meshes" data-secondary="data plane proxy" data-startref="ix_sermshDPP" data-type="indexterm" id="idm45611987675208"/><a data-primary="service meshes" data-secondary="on Kubernetes" data-secondary-sortas="Kubernetes" data-type="indexterm" id="ix_sermshKub"/><a data-primary="Envoy proxy" data-secondary="use with Istio service mesh" data-type="indexterm" id="ix_EnvIst"/> We also talked about Envoy as a data plane proxy and how it supports dynamic configuration through the xDS APIs. To build a service mesh on Kubernetes, we need a control plane that configures the mesh’s data plane according to what’s happening inside the cluster. The control plane needs to understand Services, Endpoints, Pods, etc. Furthermore, it needs to expose Kubernetes Custom Resources that developers can use to configure the service mesh.<a data-primary="Istio" data-type="indexterm" id="ix_Ist"/></p>&#13;
&#13;
<p>One of the most popular service mesh implementations for Kubernetes is Istio. Istio implements a control plane for an Envoy-based service mesh.<a data-primary="control plane" data-secondary="Istio interactions with" data-type="indexterm" id="idm45611987668968"/> The control plane is implemented in a component called istiod, which itself has three primary sub-components: Pilot, Citadel, and Galley.<a data-primary="istiod" data-type="indexterm" id="idm45611987667704"/> Pilot is an Envoy configuration server. It implements the xDS APIs and streams the configuration to the Envoy proxies running alongside the applications.<a data-primary="Citadel" data-type="indexterm" id="idm45611987666728"/><a data-primary="Galley" data-type="indexterm" id="idm45611987666056"/><a data-primary="Pilot" data-type="indexterm" id="idm45611987665384"/><a data-primary="certificate management" data-secondary="in Istio" data-type="indexterm" id="idm45611987664712"/> Citadel is responsible for certificate management inside the mesh. It mints certificates that are used to establish service identity and mutual TLS. Finally, Galley interacts with external systems such as Kubernetes to obtain configuration. It abstracts the underlying platform and translates configuration for the other istiod components. <a data-type="xref" href="#istio_control_plane_interactions">Figure 6-20</a> shows the interactions between the Istio control plane components.</p>&#13;
&#13;
<figure><div class="figure" id="istio_control_plane_interactions">&#13;
<img alt="prku 0620" src="assets/prku_0620.png"/>&#13;
<h6><span class="label">Figure 6-20. </span>Istio control plane interactions.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Istio provides other capabilities besides configuring the data plane of the service mesh. First, Istio includes a mutating admission webhook that injects an Envoy sidecar into Pods. Every Pod that participates in the mesh has an Envoy sidecar that handles all the incoming and outgoing connections. The mutating webhook improves the developer experience on the platform, given that developers don’t have to manually add the sidecar proxy to all of their application deployment manifests. The platform injects the sidecar automatically with both an opt-in and opt-out model. With that said, merely injecting the Envoy proxy sidecar alongside the workload does not mean the workload will automatically start sending traffic through Envoy. Thus, Istio uses an init-container to install iptables rules that intercept the Pod’s network traffic and routes it to Envoy. The following snippet (trimmed for brevity) shows the Istio init-container configuration:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nn">...</code><code>&#13;
</code><code class="nt">initContainers</code><code class="p">:</code><code>&#13;
</code><code class="p-Indicator">-</code><code> </code><code class="nt">args</code><code class="p">:</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">istio-iptables</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--envoy-port</code><code> </code><a class="co" href="#callout_service_routing_CO10-1" id="co_service_routing_CO10-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="s">"</code><code class="s">15001</code><code class="s">"</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--inbound-capture-port</code><code> </code><a class="co" href="#callout_service_routing_CO10-2" id="co_service_routing_CO10-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="s">"</code><code class="s">15006</code><code class="s">"</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--proxy-uid</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="s">"</code><code class="s">1337</code><code class="s">"</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--istio-inbound-interception-mode</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">REDIRECT</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--istio-service-cidr</code><code> </code><a class="co" href="#callout_service_routing_CO10-3" id="co_service_routing_CO10-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">*</code><code class="s">'</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--istio-inbound-ports</code><code> </code><a class="co" href="#callout_service_routing_CO10-4" id="co_service_routing_CO10-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="s">'</code><code class="s">*</code><code class="s">'</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">--istio-local-exclude-ports</code><code>&#13;
</code><code>  </code><code class="p-Indicator">-</code><code> </code><code class="l-Scalar-Plain">15090,15021,15020</code><code>&#13;
</code><code>  </code><code class="nt">image</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">docker.io/istio/proxyv2:1.6.7</code><code>&#13;
</code><code>  </code><code class="nt">imagePullPolicy</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">Always</code><code>&#13;
</code><code>  </code><code class="nt">name</code><code class="p">:</code><code> </code><code class="l-Scalar-Plain">istio-init</code><code>&#13;
</code><code class="nn">...</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_service_routing_CO10-1" id="callout_service_routing_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Istio installs an iptables rule that captures all outbound traffic and sends it to Envoy at this port.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO10-2" id="callout_service_routing_CO10-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Istio installs an iptables rule that captures all inbound traffic and sends it to Envoy at this port.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO10-3" id="callout_service_routing_CO10-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>List of CIDRs to redirect to Envoy. In this case, we are redirecting all CIDRs.</p></dd>&#13;
<dt><a class="co" href="#co_service_routing_CO10-4" id="callout_service_routing_CO10-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>List of ports to redirect to Envoy. In this case, we are redirecting all ports.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Now that we’ve discussed Istio’s architecture, let’s discuss some of the service mesh features that are typically used.<a data-primary="Envoy proxy" data-secondary="use with Istio service mesh" data-startref="ix_EnvIst" data-type="indexterm" id="idm45611987506696"/> One of the more common requirements we run into in the field is service authentication and encryption of service-to-service traffic.<a data-primary="Traffic Access Control APIs" data-type="indexterm" id="idm45611987475976"/> This feature is covered by the Traffic Access Control APIs in the SMI. Istio and most service mesh implementations use mutual TLS to achieve this.<a data-primary="TLS" data-secondary="mTLS (mutual TLS)" data-type="indexterm" id="idm45611987475080"/><a data-primary="mTLS (mutual TLS)" data-secondary="use in Istio" data-type="indexterm" id="idm45611987474232"/> In Istio’s case, mutual TLS is enabled by default for all services that are participating in the mesh. The workload sends unencrypted traffic to the sidecar proxy. The sidecar proxy upgrades the connection to mTLS and sends it along to the sidecar proxy on the other end. By default, the services can still receive non-TLS traffic from other services outside of the mesh. If you want to enforce mTLS for all interactions, Istio supports a <code>STRICT</code> mode that configures all services in the mesh to accept only TLS-encrypted requests. For example, you can enforce strict mTLS at the cluster level with the following configuration in the <code>istio-system</code> Namespace:</p>&#13;
&#13;
<pre data-type="programlisting">apiVersion: "security.istio.io/v1beta1"&#13;
kind: "PeerAuthentication"&#13;
metadata:&#13;
  name: "default"&#13;
  namespace: "istio-system"&#13;
spec:&#13;
  mtls:&#13;
    mode: STRICT</pre>&#13;
&#13;
<p>Traffic management is another key concern handled by a service mesh.<a data-primary="Istio" data-secondary="traffic management in" data-type="indexterm" id="idm45611987470760"/> Traffic management is captured in the Traffic Split API of the SMI, even though Istio’s traffic management features are more advanced. In addition to traffic splitting or shifting, Istio supports fault injection, circuit breaking, mirroring, and more. When it comes to traffic shifting, Istio uses two separate Custom Resources for configuration: VirtualService and DestinationRule.</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The <em>VirtualService</em> resource creates services in the mesh and specifies how traffic is routed to them.<a data-primary="VirtualService resource" data-type="indexterm" id="idm45611987468104"/> It specifies the hostname of the service and rules that control the destination of the requests. For example, the VirtualService can send 90% of traffic to one destination and send the rest to another. Once the VirtualService evaluates the rules and chooses a destination, it sends the traffic along to a specific subset of a <em>DestinationRule</em>.<a data-primary="DestinationRule resource" data-type="indexterm" id="idm45611987466648"/></p>&#13;
</li>&#13;
<li>&#13;
<p>The <em>DestinationRule</em> resource lists the “real” backends that are available for a given Service. Each backend is captured in a separate subset. Each subset can have its own routing configuration, such as load balancing policy, mutual TLS mode, and others.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As an example, let’s consider a scenario where we want to slowly roll out version 2 of a service. We can use the following DestinationRule and VirtualService to achieve this. The DestinationRule creates two service subsets: v1 and v2. The VirtualService references these subsets. It sends 90% of traffic to the v1 subset and 10% of the traffic to the v2 subset:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.istio.io/v1alpha3</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">DestinationRule</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">flights</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">flights</code>&#13;
  <code class="nt">subsets</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
    <code class="nt">labels</code><code class="p">:</code>&#13;
      <code class="nt">version</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">v2</code>&#13;
    <code class="nt">labels</code><code class="p">:</code>&#13;
      <code class="nt">version</code><code class="p">:</code> <code class="l-Scalar-Plain">v2</code>&#13;
<code class="nn">---</code>&#13;
<code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">networking.istio.io/v1alpha3</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">VirtualService</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">flights</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">hosts</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">flights</code>&#13;
  <code class="nt">http</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">route</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">destination</code><code class="p">:</code>&#13;
        <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">flights</code>&#13;
        <code class="nt">subset</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
      <code class="nt">weight</code><code class="p">:</code> <code class="l-Scalar-Plain">90</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">destination</code><code class="p">:</code>&#13;
        <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">flights</code>&#13;
        <code class="nt">subset</code><code class="p">:</code> <code class="l-Scalar-Plain">v2</code>&#13;
      <code class="nt">weight</code><code class="p">:</code> <code class="l-Scalar-Plain">10</code></pre>&#13;
&#13;
<p>Service observability <a data-primary="observability" data-secondary="services in service mesh" data-type="indexterm" id="idm45611987461768"/>is another feature that is commonly sought after. Because there’s a proxy between all services in the mesh, deriving service-level metrics is straightforward. Developers get these metrics without having to instrument their applications. The metrics are exposed in the Prometheus format, which makes them available to a wide range of monitoring systems. The following is an example metric captured by the sidecar proxy (some labels removed for brevity). The metric shows that there have been 7183 successful requests from the flights booking service to the payment processing service:</p>&#13;
&#13;
<pre data-type="programlisting">istio_requests_total{&#13;
  connection_security_policy="mutual_tls",&#13;
  destination_service_name="payments",&#13;
  destination_service_namespace="payments",&#13;
  destination_version="v1",&#13;
  request_protocol="http",&#13;
  ...&#13;
  response_code="200",&#13;
  source_app="bookings",&#13;
  source_version="v1",&#13;
  source_workload="bookings-v1",&#13;
  source_workload_namespace="flights"&#13;
} 7183</pre>&#13;
&#13;
<p>Overall, Istio offers all of the features that are captured in the SMI. However, it does not yet implement the SMI APIs (Istio v1.6). The SMI community maintains an <a href="https://github.com/servicemeshinterface/smi-adapter-istio">adapter</a> that you can use to make the SMI APIs work with Istio. We discussed Istio mainly because it is the service mesh that we’ve most commonly encountered in the field.<a data-primary="service meshes" data-secondary="in Kubernetes ecosystem" data-secondary-sortas="Kubernetes" data-type="indexterm" id="idm45611987372424"/> With that said, there are other meshes available in the Kubernetes ecosystem, including Linkerd, Consul Connect, Maesh, and more. One of the things that varies across these implementations is the data plane architecture, which we’ll discuss next.<a data-primary="Istio" data-startref="ix_Ist" data-type="indexterm" id="idm45611987370776"/><a data-primary="service meshes" data-secondary="on Kubernetes" data-secondary-sortas="Kubernetes" data-startref="ix_sermshKub" data-type="indexterm" id="idm45611987369832"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Plane Architecture" data-type="sect2"><div class="sect2" id="idm45611987678664">&#13;
<h2>Data Plane Architecture</h2>&#13;
&#13;
<p>A service mesh is a highway that services can use to communicate with each other.<a data-primary="service meshes" data-secondary="data plane architecture" data-type="indexterm" id="idm45611987367048"/><a data-primary="data plane" data-secondary="service mesh architecture" data-type="indexterm" id="idm45611987366072"/> To get onto this highway, services use a proxy that serves as the on-ramp. Service meshes follow one of two architecture models when it comes to the data plane: the sidecar proxy or the node proxy.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Sidecar proxy" data-type="sect3"><div class="sect3" id="idm45611987364696">&#13;
<h3>Sidecar proxy</h3>&#13;
&#13;
<p>The sidecar proxy is the most common architecture model among the two. As we discussed<a data-primary="sidecar proxy" data-type="indexterm" id="idm45611987362984"/><a data-primary="proxies" data-secondary="sidecar proxy in service mesh" data-type="indexterm" id="idm45611987362280"/> in the previous section, Istio follows this model to implement its data plane with Envoy proxies. Linkerd uses this approach as well. In essence, service meshes that follow this pattern deploy the proxy inside the workload’s Pod, running alongside the service. Once deployed, the sidecar proxy intercepts all the communications into and out of the service, as depicted in <a data-type="xref" href="#pods_participating_in_the_mesh">Figure 6-21</a>.</p>&#13;
&#13;
<figure><div class="figure" id="pods_participating_in_the_mesh">&#13;
<img alt="prku 0621" src="assets/prku_0621.png"/>&#13;
<h6><span class="label">Figure 6-21. </span>Pods participating in the mesh have a sidecar proxy that intercepts the Pod’s network traffic.</h6>&#13;
</div></figure>&#13;
&#13;
<p>When compared to the node proxy approach, the sidecar proxy architecture can have greater impact on services when it comes to data plane upgrades. The upgrade involves rolling all the service Pods, as there is no way to upgrade the sidecar without re-creating the Pods.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Node proxy" data-type="sect3"><div class="sect3" id="idm45611987357016">&#13;
<h3>Node proxy</h3>&#13;
&#13;
<p>The node proxy is an alternative data plane architecture.<a data-primary="node proxy" data-type="indexterm" id="idm45611987355688"/><a data-primary="proxies" data-secondary="node proxy in service mesh" data-type="indexterm" id="idm45611987354984"/> Instead of injecting a sidecar proxy into each service, the service mesh consists of a single proxy running on each node. Each node proxy handles the traffic for all services running on their node, as depicted in <a data-type="xref" href="#the_node_proxy_model_involves_a_single_service_mesh_proxy">Figure 6-22</a>. Service meshes that follow this architecture include <a href="https://www.consul.io/docs/connect">Consul Connect</a> and <a href="https://containo.us/maesh">Maesh</a>. The first version of Linkerd used node proxies as well, but the project has since moved to the sidecar model in version 2.</p>&#13;
&#13;
<p>When compared to the sidecar proxy architecture, the node proxy approach can have greater performance impact on services.<a data-primary="performance" data-secondary="impact of node proxy on services" data-type="indexterm" id="idm45611987350744"/> Because the proxy is shared by all the services on a node, services can suffer from noisy neighbor problems and the proxy can become a network bottleneck.</p>&#13;
&#13;
<figure><div class="figure" id="the_node_proxy_model_involves_a_single_service_mesh_proxy">&#13;
<img alt="prku 0622" src="assets/prku_0622.png"/>&#13;
<h6><span class="label">Figure 6-22. </span>The node proxy model involves a single service mesh proxy that handles the traffic for all services on the node.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Adopting a Service Mesh" data-type="sect2"><div class="sect2" id="idm45611987347032">&#13;
<h2>Adopting a Service Mesh</h2>&#13;
&#13;
<p>Adopting a service mesh can seem like a daunting task. Should you deploy it to an existing cluster? <a data-primary="service meshes" data-secondary="adopting a service mesh" data-type="indexterm" id="ix_sermshadp"/>How do you avoid affecting workloads that are already running? How can you selectively onboard services for testing?</p>&#13;
&#13;
<p>In this section, we will explore the different considerations you should make when introducing a service mesh to your application platform.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prioritize one of the pillars" data-type="sect3"><div class="sect3" id="idm45611987343176">&#13;
<h3>Prioritize one of the pillars</h3>&#13;
&#13;
<p>One of the first things to do is to prioritize one of the service mesh pillars.<a data-primary="service meshes" data-secondary="adopting a service mesh" data-tertiary="prioritizing one of the pillars" data-type="indexterm" id="idm45611987341880"/> Doing so will allow you to narrow the scope, both from an implementation and testing perspective. Depending on your requirements (which you’ve established if you’re adopting a service mesh, right?), you might prioritize mutual TLS, for example, as the first pillar. In this case, you can focus on deploying the PKI necessary to support this feature. No need to worry about setting up a tracing stack or spending development cycles testing traffic routing and management.</p>&#13;
&#13;
<p>Focusing on one of the pillars enables you to learn about the mesh, understand how it behaves in your platform, and gain operational expertise. Once you feel comfortable, you can implement additional pillars, as necessary. In essence, you will be more successful if you follow a piecemeal deployment instead of a big-bang implementation.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploy to a new or an existing cluster?" data-type="sect3"><div class="sect3" id="idm45611987338952">&#13;
<h3>Deploy to a new or an existing cluster?</h3>&#13;
&#13;
<p>Depending on your platform’s life cycle and topology, you might have a choice between deploying the service mesh to a new, fresh cluster or adding it to an existing cluster.<a data-primary="service meshes" data-secondary="adopting a service mesh" data-tertiary="deploying to new or existing cluster" data-type="indexterm" id="idm45611987337464"/><a data-primary="clusters" data-secondary="deploying service mesh to" data-type="indexterm" id="idm45611987336152"/> When possible, prefer going down the new cluster route. This eliminates any potential disruption to applications that would otherwise be running in an existing cluster. If your clusters are ephemeral, deploying the service mesh to a new cluster should be a natural path to follow.</p>&#13;
&#13;
<p>In situations where you must introduce the service mesh into an existing cluster, make sure to perform extensive testing in your development and testing tiers. More importantly, offer an onboarding window that allows development teams to experiment and test their services with the mesh before rolling it out to the staging and production tiers. Finally, provide a mechanism that allows applications to opt into being part of the mesh. A common way to enable the opt-in mechanism is to provide a Pod annotation. Istio, for example, provides an annotation (<code>sidecar.istio.io/inject</code>) that determines whether the platform should inject the sidecar proxy into the workload, which is visible in the following snippet:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">template</code><code class="p">:</code>&#13;
    <code class="nt">metadata</code><code class="p">:</code>&#13;
      <code class="nt">annotations</code><code class="p">:</code>&#13;
        <code class="nt">sidecar.istio.io/inject</code><code class="p">:</code> <code class="s">"true"</code>&#13;
    <code class="nt">spec</code><code class="p">:</code>&#13;
      <code class="nt">containers</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
        <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Handling upgrades" data-type="sect3"><div class="sect3" id="idm45611987331768">&#13;
<h3>Handling upgrades</h3>&#13;
&#13;
<p>When offering a service mesh as part of your platform, you must have a solid upgrade strategy in place.<a data-primary="upgrades" data-secondary="handling for service mesh" data-type="indexterm" id="idm45611987248536"/><a data-primary="service meshes" data-secondary="adopting a service mesh" data-tertiary="handling upgrades" data-type="indexterm" id="idm45611987247592"/> Keep in mind that the service mesh data plane is in the critical path that connects your services, including your cluster’s edge (regardless of whether you are using the mesh’s Ingress gateway or another Ingress controller). What happens when there’s a CVE that affects the mesh’s proxy? How will you handle upgrades effectively? Do not adopt a service mesh without understanding these concerns and having a well-established upgrade strategy.</p>&#13;
&#13;
<p>The upgrade strategy must account for both the control plane and the data plane.<a data-primary="control plane" data-secondary="service mesh upgrades" data-type="indexterm" id="idm45611987245400"/> The control plane upgrade carries less risk, as the mesh’s data plane should continue to function without it. With that said, do not discount control plane upgrades. You should understand the version compatibility between the control plane and the data plane. If possible, follow a canary upgrade pattern, as recommended by the <a href="https://oreil.ly/TZj7F">Istio project</a>. Also make sure to review any service mesh Custom Resource Definition (CRD) changes and whether they impact your services.</p>&#13;
&#13;
<p>The data plane <a data-primary="data plane" data-secondary="service mesh upgrades" data-type="indexterm" id="idm45611987242728"/>upgrade is more involved, given the number of proxies running on the platform and the fact that the proxy is handling service traffic. When the proxy runs as a sidecar, the entire Pod must be re-created to upgrade the proxy as Kubernetes doesn’t support in-place upgrades of containers. Whether you do a full data plane upgrade or a slow rollout of the new data plane proxy depends on the reason behind the upgrade. One one hand, if you are upgrading the data plane to handle a vulnerability in the proxy, you must re-create every single Pod that participates in the mesh to address the vulnerability. As you can imagine, this can be disruptive to some applications. If, on the other hand, you are upgrading to take advantage of new features or bug fixes, you can let the new version of the proxy roll out as Pods are created or moved around in the cluster. This slower, less disruptive upgrade results in version sprawl of the proxy, which may be acceptable as long as the service mesh supports it. Regardless of why you are upgrading, always use your development and testing tiers to practice and validate service mesh upgrades.</p>&#13;
&#13;
<p>Another thing to keep in mind is that meshes typically have a narrow set of Kubernetes versions they can support. How does a Kubernetes upgrade affect your service mesh? Does leveraging a service mesh hinder your ability to upgrade Kubernetes as soon as a new version is released? Given that Kubernetes APIs are relatively stable, this should not be the case. However, it is possible and something to keep in mind.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource overhead" data-type="sect3"><div class="sect3" id="idm45611987331400">&#13;
<h3>Resource overhead</h3>&#13;
&#13;
<p>One of the primary trade-offs of using a service mesh is the resource overhead that it carries, especially in the sidecar architecture.<a data-primary="service meshes" data-secondary="adopting a service mesh" data-tertiary="resource overhead" data-type="indexterm" id="idm45611987238440"/><a data-primary="resource overhead for service mesh" data-type="indexterm" id="idm45611987237192"/> As we’ve discussed, the service mesh injects a proxy into each Pod in the cluster. To get its job done, the proxy consumes resources (CPU and memory) that would otherwise be available to other services. When adopting a service mesh, you must understand this overhead and whether the trade-off is worth it. If you are running the cluster in a datacenter, the overhead is probably palatable. However, the overhead might prevent you from using a service mesh in edge deployments where resource constraints are tighter.</p>&#13;
&#13;
<p>Perhaps more important, a service mesh introduces latency between services given that the service calls are traversing a proxy on both the source and the destination services. While the proxies used in service meshes are usually highly performant, it is important to understand the latency overhead they introduce and whether your application can function given the overhead.</p>&#13;
&#13;
<p>When evaluating a service mesh, spend time investigating its resource overhead. Even better, run performance tests with your services to understand how the mesh behaves under load.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Certificate Authority for mutual TLS" data-type="sect3"><div class="sect3" id="idm45611987234264">&#13;
<h3>Certificate Authority for mutual TLS</h3>&#13;
&#13;
<p>The identity features of a service mesh are usually based on X.509 certificates.<a data-primary="service meshes" data-secondary="adopting a service mesh" data-tertiary="Certificate Authority for mTLS" data-type="indexterm" id="idm45611987232728"/><a data-primary="mTLS (mutual TLS)" data-secondary="Certificate Authority for, in a service mesh" data-type="indexterm" id="idm45611987231464"/><a data-primary="mTLS (mutual TLS)" data-type="indexterm" id="idm45611987230552"/> Proxies in the mesh use these certificates to establish mutual TLS (mTLS) connections between services.</p>&#13;
&#13;
<p>Before being able to leverage the mTLS features of a service mesh, you must establish a certificate management strategy.<a data-primary="certificate management" data-secondary="in service meshes" data-secondary-sortas="service" data-type="indexterm" id="idm45611987229096"/> While the mesh is usually responsible for minting the service certificates, it is up to you to determine the Certificate Authority (CA). In most cases, a service mesh uses a self-signed certificate as the CA. However, mature service meshes allow you to bring your own CA, if necessary.</p>&#13;
&#13;
<p>Because the service mesh handles service-to-service communications, using a self-signed CA is adequate. The CA is essentially an implementation detail that is invisible to your applications and their clients. With that said, security teams can disapprove of the use of self-signed CAs. When adopting a service mesh, make sure to bring your security team into the conversation.</p>&#13;
&#13;
<p>If using a self-signed CA for mTLS is not viable, you will have to provide a CA certificate and key that the service mesh can use to mint certificates. Alternatively, you can integrate with an external CA, such as Vault, when an integration is available.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multicluster service mesh" data-type="sect3"><div class="sect3" id="idm45611987225816">&#13;
<h3>Multicluster service mesh</h3>&#13;
&#13;
<p>Some service meshes offer multicluster capabilities that you can use to extend the mesh across multiple Kubernetes clusters.<a data-primary="service meshes" data-secondary="adopting a service mesh" data-tertiary="multicluster service mesh" data-type="indexterm" id="idm45611987224376"/><a data-primary="clusters" data-secondary="multicluster service mesh" data-type="indexterm" id="idm45611987223112"/> The goal of these capabilities is to connect services running in different clusters through a secure channel that is transparent to the application. Multicluster meshes increase the complexity of your platform. They can have both performance and fault-domain implications that developers might have to be aware of. <a data-primary="performance" data-secondary="impacts of multicluster service meshes" data-type="indexterm" id="idm45611987221688"/>In any case, while creating multicluster meshes might seem attractive, you should avoid them until you gain the operational knowledge to run a service mesh successfully within a single cluster.<a data-primary="service routing" data-secondary="service meshes" data-startref="ix_serrtSM" data-type="indexterm" id="idm45611987220392"/><a data-primary="service meshes" data-secondary="adopting a service mesh" data-startref="ix_sermshadp" data-type="indexterm" id="idm45611987219176"/><a data-primary="service meshes" data-startref="ix_sermsh" data-type="indexterm" id="idm45611987217960"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611988120632">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Service routing is a crucial concern when building an application platform atop Kubernetes. Services provide layer 3/4 routing and load balancing capabilities to applications. They enable applications to communicate with other services in the cluster without worrying about changing Pod IPs or failing cluster nodes. Furthermore, developers can use NodePort and LoadBalancer Services to expose their applications to clients outside of the cluster.</p>&#13;
&#13;
<p>Ingress builds on top of Services to provide richer routing capabilities. Developers can use the Ingress API to route traffic according to application-level concerns, such as the <code>Host</code> header of the request or the path that the client is trying to reach. The Ingress API is satisfied by an Ingress controller, which you must deploy before using Ingress resources. Once installed, the Ingress controller handles incoming requests and routes them according to the Ingress configuration defined in the API.</p>&#13;
&#13;
<p>If you have a large portfolio of microservices-based applications, your developers might benefit from a service mesh’s capabilities. When using a service mesh, services communicate with each other through proxies that augment the interaction. Service meshes can provide a variety of features, including traffic management, mutual TLS, access control, automated service metrics gathering, and more. Like other interfaces in the Kubernetes ecosystem, the Service Mesh Interface (SMI) aims to enable platform operators to use a service mesh without tying themselves to specific implementations. However, before adopting a service mesh, ensure that you have the operational expertise in your team to operate an additional distributed system on top of Kubernetes.<a data-primary="service routing" data-startref="ix_serrt" data-type="indexterm" id="idm45611987212568"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>