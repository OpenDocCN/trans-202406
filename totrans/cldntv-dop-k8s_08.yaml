- en: Chapter 6\. Operating Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If Tetris has taught me anything, it’s that errors pile up and accomplishments
    disappear.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Andrew Clay Shafer
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once you have a Kubernetes cluster, how do you know it’s in good shape and running
    properly? How do you scale to cope with demand, but keep cloud costs to a minimum?
    In this chapter, we’ll look at the issues involved in operating Kubernetes clusters
    for production workloads, and some of the tools that can help you.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ve seen in [Chapter 3](ch03.html#gettingk8s), there are many important
    things to consider about your Kubernetes cluster: availability, authentication,
    upgrades, and so on. If you’re using a good managed Kubernetes service, as we
    recommend, most of these issues should be taken care of for you.'
  prefs: []
  type: TYPE_NORMAL
- en: However, what you actually do with the cluster is up to you. In this chapter,
    you’ll learn how to size and scale the cluster, check it for *conformance*, and
    test the resilience of your infrastructure with *Chaos Monkeys*.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Sizing and Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How big does your cluster need to be? With self-hosted Kubernetes clusters,
    and almost all managed services, the ongoing cost of your cluster depends directly
    on the number and size of its nodes. If the capacity of the cluster is too small,
    your workloads won’t run properly, or will fail under heavy traffic. If the capacity
    is too large, you’re wasting money.
  prefs: []
  type: TYPE_NORMAL
- en: Sizing and scaling your cluster appropriately is very important, so let’s look
    at some of the decisions involved.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to make an initial estimate of the capacity you need is to think about
    how many traditional servers you would need to run the same applications. For
    example, if your current architecture runs on 10 separate cloud virtual machine
    instances, you probably won’t need more than 10 nodes of similar sizes in your
    Kubernetes cluster to run the same workload, plus another 1 or 2 for redundancy.
    In fact, you might not even need that many because Kubernetes will balance containers
    evenly across all machines, and can therefore achieve higher utilization than
    with traditional servers. But it may take some time and practical experience to
    tune your cluster for optimal capacity.
  prefs: []
  type: TYPE_NORMAL
- en: The smallest cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you’re first setting up a cluster, you will probably be using it to play
    around and experiment, and figure out how to run your application. So you probably
    don’t need to burn money on a large cluster until you have some idea what capacity
    you’re going to need.
  prefs: []
  type: TYPE_NORMAL
- en: The smallest possible Kubernetes cluster is a single node. This will allow you
    to try out Kubernetes and run small workloads for development, as we saw in [Chapter 2](ch02.html#firststeps).
    However, a single-node cluster has no resilience against the failure of the node
    hardware, or of the Kubernetes API server or the kubelet (the agent daemon that
    is responsible for running workloads on each node).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re using a managed Kubernetes service like GKE, EKS, or AKS (see [“Managed
    Kubernetes Services”](ch03.html#managedclusters)), then you don’t need to worry
    about provisioning control plane nodes: this is done for you. If, on the other
    hand, you’re building your own cluster, you’ll need to decide how to lay out the
    control plane.'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum number of control plane nodes for a resilient Kubernetes cluster
    is three. One wouldn’t be resilient, and two could disagree about which was the
    leader, so at least three nodes are needed.
  prefs: []
  type: TYPE_NORMAL
- en: While you can do useful work on a Kubernetes cluster this small, it’s not recommended.
    A better idea is to add some worker nodes so that your own workloads aren’t competing
    for resources with the Kubernetes control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Provided your cluster control plane is highly available, you *can* get by with
    a single worker node, but two nodes is the sensible minimum to protect against
    node failure and to allow Kubernetes to run at least two replicas of every Pod.
    The more nodes there are the better, especially as the Kubernetes scheduler cannot
    always ensure that workloads are fully balanced across available nodes (see [“Keeping
    Your Workloads Balanced”](ch05.html#balanced)).
  prefs: []
  type: TYPE_NORMAL
- en: K3S
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to lightweight clusters, a tool like [K3s](https://k3s.io) is
    worth checking out. It packages all Kubernetes components into a single binary,
    making it great for environments that are isolated and/or resource constrained.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes clusters need at least three nodes running the control plane components
    in order to be highly available, and you may need more to handle the work of larger
    clusters. Two worker nodes is the minimum required to make your workloads fault
    tolerant to the failure of a single node, and three worker nodes is even better.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Is there a limit to how large Kubernetes clusters can be? The short answer is
    yes, but you almost certainly won’t have to worry about it; Kubernetes version
    1.22 officially supports clusters of up to 5,000 nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Because clustering requires communication between nodes, the number of possible
    communication paths, and the cumulative load on the underlying database, grows
    exponentially with the size of the cluster. While Kubernetes *may* still function
    with more than 5,000 nodes, it’s not *guaranteed* to work, or at least to be responsive
    enough to deal with production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes documentation advises that [supported cluster configurations](https://oreil.ly/NapFi)
    must have no more than 5,000 nodes, no more than 150,000 total Pods, no more than
    300,000 total containers, and no more than 100 Pods per node. It’s worth bearing
    in mind that the larger the cluster, the bigger the load on the control plane
    nodes; if you’re responsible for running your own control plane, they’ll need
    to be pretty powerful machines to cope with a cluster of thousands of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For maximum reliability, keep your Kubernetes clusters smaller than 5,000 nodes
    and 150,000 Pods (this isn’t an issue for most users). If you need more resources,
    run multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Federated clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have extremely demanding workloads or need to operate at huge scale,
    these limits may become a practical problem for you. In this case, you can run
    multiple Kubernetes clusters, and, if necessary, *federate* them so that workloads
    can be replicated across clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Federation provides the ability to keep two or more clusters synchronized, running
    identical workloads. This can be useful if you need Kubernetes clusters in different
    cloud providers, for resilience, or in different geographical locations, to reduce
    latency for your users. A group of federated clusters can keep running even if
    an individual cluster fails.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about *cluster federation* in the Kubernetes [documentation](https://oreil.ly/QdAB2).
  prefs: []
  type: TYPE_NORMAL
- en: For most Kubernetes users, federation isn’t something they need to be concerned
    with, and, in practice, most users at very large scale are able to handle their
    workloads with multiple unfederated clusters of a few hundred to a few thousand
    nodes each. Often, provisioning smaller separated clusters across team or application
    boundaries ends up being easier to manage compared to large, centralized federated
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you need to replicate workloads across multiple clusters, perhaps for geographical
    redundancy or latency reasons, use federation. Most users don’t need to federate
    their clusters, though.
  prefs: []
  type: TYPE_NORMAL
- en: Do I need multiple clusters?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unless you’re operating at very large scale, as we mentioned in the previous
    section and in [“Multicloud Kubernetes Clusters”](ch03.html#multicloud), you probably
    don’t need more than one or two clusters: maybe one for production, and one for
    staging and testing.'
  prefs: []
  type: TYPE_NORMAL
- en: For convenience and ease of resource management, you can divide your cluster
    into logical partitions using namespaces, which we covered in more detail in [“Using
    Namespaces”](ch05.html#namespaces). With a few exceptions, it’s not usually worth
    the administration overhead of managing multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: There are some specific situations, such as security and regulatory compliance,
    where you might want to ensure that services in one cluster are absolutely isolated
    from those in another (for example, when dealing with protected health information,
    or when data can’t be transmitted from one geographical location to another for
    legal reasons). In those cases, you need to create separate clusters. For most
    Kubernetes users, this won’t be an issue.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use a single production and a single staging cluster, unless you really need
    complete isolation of one set of workloads or teams from another. If you just
    want to partition your cluster for ease of management, use namespaces instead.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes and Instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The more capacity a given node has, the more work it can do, where capacity
    is expressed in terms of the number of CPU cores (virtual or otherwise), available
    memory, and to a lesser extent, disk space. But is it better to run 10 very large
    nodes, for example, rather than 100 much smaller ones?
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right node size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s no universally correct node size for Kubernetes clusters. The answer
    depends on your cloud or hardware provider, and on your specific workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The cost per capacity of different instance sizes can have an effect on the
    way you decide to size your nodes. For example, some cloud providers may offer
    a slight discount on larger instance sizes so that if your workloads are very
    compute intensive, it may be cheaper to run them on a few very large nodes instead
    of many smaller ones.
  prefs: []
  type: TYPE_NORMAL
- en: The number of nodes required in the cluster also affects the choice of node
    size. To get the advantages that Kubernetes offers, such as Pod replication and
    high availability, you need to spread work across several nodes. But if nodes
    have too much spare capacity, that’s a waste of money.
  prefs: []
  type: TYPE_NORMAL
- en: If you need, say, at least 10 nodes for high availability, but each node only
    needs to run a couple of Pods, the node instances can be very small. On the other
    hand, if you only need two nodes, you can make them quite large and potentially
    save money with more favorable instance pricing.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use the most cost-effective node type that your provider offers. Often, larger
    nodes work out cheaper, but if you only have a handful of nodes, you might want
    to add some smaller ones, to help with redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud instance types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because the Kubernetes components themselves, such as the kubelet, use a given
    amount of resources, and you will need some spare capacity to do useful work,
    the smallest instance sizes offered by your cloud provider will probably not be
    suitable for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: A control plane for small clusters (up to around five total nodes) should have
    at least two CPUs and two GiB of memory, with larger clusters requiring more memory
    and CPUs for each control plane node.
  prefs: []
  type: TYPE_NORMAL
- en: For larger clusters, with perhaps a few tens of nodes, it may make sense for
    you to provision a mix of two or three different instance sizes. This means that
    Pods with compute-intensive workloads requiring a lot of memory can be scheduled
    by Kubernetes on large nodes, leaving smaller nodes free to handle smaller Pods
    (see [“Node Affinities”](ch09.html#nodeaffinities)). This gives the Kubernetes
    scheduler the maximum freedom of choice when deciding where to run a given Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Heterogeneous nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all nodes are created equal. You may need some nodes with special properties,
    such as a graphics processing unit (GPU). GPUs are high-performance parallel processors
    that are widely used for compute-intensive problems that have nothing to do with
    graphics, such as machine learning or data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the *resource limits* functionality in Kubernetes (see [“Resource
    Limits”](ch05.html#resourcelimits)) to specify that a given Pod needs at least
    one GPU, for example. This will ensure that those Pods will run only on GPU-enabled
    nodes, and get priority over Pods that can run on any node.
  prefs: []
  type: TYPE_NORMAL
- en: Most Kubernetes nodes probably run Linux of one kind or another, which is suitable
    for almost all applications. Recall that containers are *not* virtual machines,
    so the process inside a container runs directly on the kernel of the operating
    system on the underlying node. A Windows binary will not run on a Linux Kubernetes
    node, for example, so if you need to run Windows containers, you will have to
    provision Windows nodes for them.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most containers are built for Linux, so you’ll probably want to run mostly Linux-based
    nodes. You may need to add one or two special types of nodes for specific requirements,
    such as GPUs or Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Bare-metal servers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most useful properties of Kubernetes is its ability to connect all
    sorts of machines of different sizes, architectures, and capabilities to provide
    a single, unified, logical machine on which workloads can run. While Kubernetes
    is usually associated with cloud servers, many organizations have large numbers
    of physical, bare-metal machines in datacenters that can potentially be harnessed
    into Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw in [Chapter 1](ch01.html#revolution) that cloud technology transforms
    *capex* infrastructure (purchasing machines as a capital expense) to *opex* infrastructure
    (leasing compute capacity as an operating expense), and this makes financial sense.
    However, if your business already owns a large number of bare-metal servers, you
    don’t need to write them off just yet: instead, consider joining them into a Kubernetes
    cluster (see [“Bare-Metal and On-Prem”](ch03.html#baremetal)).'
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have hardware servers with spare capacity, or you’re not ready to migrate
    completely to the cloud yet, use Kubernetes to run container workloads on your
    existing machines.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having chosen a sensible starting size for your cluster, and picked the right
    mix of instance sizes for your worker nodes, is that the end of the story? Almost
    certainly not: over time, you may need to grow or shrink the cluster to match
    changes in demand, or in business requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Instance groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s easy to add nodes to a Kubernetes cluster. If you’re running a self-hosted
    cluster, a cluster management tool such as kops (see [“kops”](ch03.html#kops))
    can do it for you. kops has the concept of an *instance group*, which is a set
    of nodes of a given instance type (for example, `m5.large`). Managed services,
    such as GKE, have the same facility, called *node pools*. The Elastic Kubernetes
    Service (EKS) tool `eksctl` refers to this concept as a [*nodegroup*](https://oreil.ly/mRr0j).
  prefs: []
  type: TYPE_NORMAL
- en: You can scale instance groups or node pools either by changing the minimum and
    maximum size for the group, or by changing the specified instance type, or both.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling down
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In principle, there’s no problem with scaling down a Kubernetes cluster either.
    You can tell Kubernetes to *drain* the nodes you want to remove, which will gradually
    shut down or move any running Pods on those nodes elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Most cluster management tools will do the node draining for you automatically,
    or you can use the `kubectl drain` command to do it yourself. Provided there is
    enough spare capacity in the rest of the cluster to reschedule the doomed Pods,
    once the nodes have been successfully drained, you can terminate them.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid overly reducing the number of Pod replicas for a given service, you
    can use PodDisruptionBudgets to specify a minimum number of available Pods, or
    the maximum number of Pods that can be *unavailable* at any time (see [“Pod Disruption
    Budgets”](ch05.html#poddisruptionbudgets)).
  prefs: []
  type: TYPE_NORMAL
- en: If draining a node would cause Kubernetes to exceed these limits, the drain
    operation will block until you change the limits or free up some more resources
    in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Draining allows Pods to shut down gracefully, cleaning up after themselves and
    saving any necessary state. For most applications, this is preferable to simply
    shutting down the node, which will terminate the Pods immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t just shut down nodes when you don’t need them anymore. Drain them first
    to ensure their workloads are migrated to other nodes, and to make sure you have
    enough spare capacity remaining in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most cloud providers support autoscaling: automatically increasing or reducing
    the number of instances in a group according to some metric or schedule. For example,
    AWS autoscaling groups (ASGs) can maintain a minimum and maximum number of instances
    so that if one instance fails, another will be started to take its place, or if
    too many instances are running, some will be shut down.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, if your demand fluctuates according to the time of day, you
    can schedule the group to grow and shrink at specified times. You can also configure
    the scaling group to grow or shrink dynamically on demand: if the average CPU
    utilization exceeds 90% over a 15-minute period, for example, instances can be
    added automatically until the CPU usage falls below the threshold. When demand
    falls again, the group can be scaled down to save money.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has a Cluster Autoscaler add-on that cluster management tools like
    kops can take advantage of to enable cloud autoscaling, and managed clusters such
    as AKS also offer autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: However, it can take some time and experimentation to get your autoscaling settings
    right, and for many users it may not be necessary at all. Most Kubernetes clusters
    start small and grow gradually and monotonically by adding a node here and there
    as resource usage grows.
  prefs: []
  type: TYPE_NORMAL
- en: For large-scale users, though, or applications where demand is highly variable,
    cluster autoscaling is a very useful feature.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t enable cluster autoscaling just because it’s there unless you already
    know you need it. You probably won’t need it unless your demands or workloads
    are extremely variable. Start by scaling your cluster manually and getting comfortable
    monitoring usage as you get a sense of how your scale requirements are changing
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: Conformance Checking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When is Kubernetes not Kubernetes? The flexibility of Kubernetes means there
    are lots of different ways to set up Kubernetes clusters, and this presents a
    potential problem. If Kubernetes is to be a universal platform, you should be
    able to take a workload and run it on any Kubernetes cluster and have it work
    the way you expect. That means the same API calls and Kubernetes objects have
    to be available, they have to have the same behavior, they have to work as advertised,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Kubernetes itself includes a test suite that verifies that a given
    Kubernetes cluster is *conformant*; that is, it satisfies a core set of requirements
    for a given Kubernetes version. These conformance tests are very useful for Kubernetes
    administrators.
  prefs: []
  type: TYPE_NORMAL
- en: If your cluster doesn’t pass them, then there is a problem with your setup that
    needs to be addressed. If it does pass, knowing that it’s conformant gives you
    confidence that applications designed for Kubernetes will work with your cluster,
    and that things you build on your cluster will work elsewhere too.
  prefs: []
  type: TYPE_NORMAL
- en: CNCF Certification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Cloud Native Computing Foundation (CNCF) is the official owner of the Kubernetes
    project and trademark (see [“Cloud Native”](ch01.html#cloudnative)), and it provides
    various kinds of certifications for Kubernetes-related products, engineers, and
    vendors.
  prefs: []
  type: TYPE_NORMAL
- en: Certified Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you use a managed, or partially managed, Kubernetes service, check whether
    it carries the Certified Kubernetes mark and logo (see [Figure 6-1](#img-certifiedk8s)).
    This indicates that the vendor and service meet the [Certified Kubernetes standard](https://oreil.ly/NKgpp),
    as specified by the CNCF.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Certified Kubernetes logo](assets/cnd2_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The Certified Kubernetes mark means that the product or service
    is approved by the CNCF
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the product has *Kubernetes* in the name, it must be [certified by the CNCF](https://oreil.ly/A5Mku).
    This means customers know exactly what they’re getting and can be satisfied that
    it will be interoperable with other conformant Kubernetes services. Vendors can
    self-certify their products by running the Sonobuoy conformance checking tool
    (see [“Conformance Testing with Sonobuoy”](#sonobuoy)).
  prefs: []
  type: TYPE_NORMAL
- en: Certified Kubernetes products also have to track the latest version of Kubernetes,
    providing updates at least annually. It’s not just managed services that can carry
    the Certified Kubernetes mark; distributions and installer tools can, too.
  prefs: []
  type: TYPE_NORMAL
- en: Certified Kubernetes Administrator (CKA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To become a Certified Kubernetes Administrator (CKA), you need to demonstrate
    that you have the key skills to manage Kubernetes clusters in production, including
    installation and configuration, networking, maintenance, knowledge of the API,
    security, and troubleshooting. Anyone can take the CKA exam, which is administered
    online and includes a series of challenging practical tests. The [CNCF website](https://oreil.ly/810Ot)
    has more info about how to train and how to register to take the exam.
  prefs: []
  type: TYPE_NORMAL
- en: The CKA exam has a reputation as a tough, comprehensive exam that really tests
    your skills and knowledge. You can be confident that any engineer who is CKA certified
    really knows Kubernetes. If you run your business on Kubernetes, consider putting
    some of your staff through the CKA program, especially those directly responsible
    for managing clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Certified Service Provider (KCSP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vendors themselves can apply for the Kubernetes Certified Service Provider (KCSP)
    program. To be eligible, the vendor has to be a CNCF member, provide enterprise
    support (for example by supplying field engineers to a customer site), contribute
    actively to the Kubernetes community, and employ three or more CKA-certified engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Look for the Certified Kubernetes mark to make sure that a product meets CNCF
    standards. Look for vendors to be KCSP-certified, and if you’re hiring Kubernetes
    administrators, look for a CKA qualification.
  prefs: []
  type: TYPE_NORMAL
- en: Conformance Testing with Sonobuoy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re managing your own cluster, or even if you’re using a managed service
    but want to double-check that it’s configured properly and up-to-date, you can
    run the Kubernetes conformance tests to prove it. The standard tool for running
    these tests is [*Sonobuoy*](https://oreil.ly/dS40G).
  prefs: []
  type: TYPE_NORMAL
- en: 'Sonobuoy uses a CLI tool and your `kubectl` authentication to run tests inside
    of your cluster. Once installed, you can run the test suite using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A new namespace is created, the Sonobuoy pods are launched, and they start
    running the tests. You can see this with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The full test suite can take an hour or more to complete! You can add the `--mode
    quick` flag to the `sonobuoy run` command to run a single test to verify connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the conformance tests are complete, you can view the results using the
    `retrieve` command, which saves the output locally to a file. You can then inspect
    that output using the `results` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Later, in [“Cluster Security Scanning”](ch11.html#cluster-security-scanning),
    we will cover similar tools that are focused on scanning your clusters for potential
    security issues. When used alongside Sonobuoy, these tools can give you a better
    picture of where your clusters may be out of line with current industry best practices
    for Kubernetes compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Run Sonobuoy once your cluster is set up for the first time, to verify that
    it’s standards compliant and that everything works. Run it again every so often
    to make sure there are no conformance problems.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Audit Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you find a problem on your cluster, such as a Pod you don’t recognize,
    and you want to know where it came from. How do you find out who did what on the
    cluster when? The [Kubernetes audit log](https://oreil.ly/3NS5l) will tell you.
  prefs: []
  type: TYPE_NORMAL
- en: With audit logging enabled, all requests to the cluster API will be recorded,
    with a timestamp, saying who made the request (which service account), the details
    of the request (such as the resources it queried), and what the response was.
  prefs: []
  type: TYPE_NORMAL
- en: The audit events can be sent to your central logging system, where you can filter
    and alert on them as you would for other log data (see [Chapter 15](ch15.html#observability)).
    A good managed service such as GKE will include audit logging by default, but
    otherwise you may need to configure the cluster yourself to enable it.
  prefs: []
  type: TYPE_NORMAL
- en: Chaos Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We pointed out in [“Trust, but verify”](ch03.html#trustbutverify) that the only
    real way to verify high availability is to kill one or more of your cluster nodes
    and see what happens. The same applies to the high availability of your Kubernetes
    Pods and applications. You could pick a Pod at random, for example, terminate
    it, and check that Kubernetes restarts it, and that your error rate is unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: Doing this manually is time-consuming, and, without realizing it, you may be
    unconsciously sparing resources that you know are application-critical. To make
    it a fair test, the process must be automated.
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of automated, random interference with production services is sometimes
    known as *Chaos Monkey* testing, after the tool of the same name developed by
    Netflix to test its infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a monkey entering a data center, these farms of servers that host all
    the critical functions of our online activities. The monkey randomly rips cables,
    destroys devices...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The challenge for IT managers is to design the information system they are responsible
    for so that it can work despite these monkeys, which no one ever knows when they
    arrive and what they will destroy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Antonio Garcia Martinez, *Chaos Monkeys*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Apart from Chaos Monkey itself, which terminates random cloud servers, the Netflix
    *Simian Army* also includes other *chaos engineering* tools such as Latency Monkey,
    which introduces communication delays to simulate network issues, Security Monkey,
    which looks for known vulnerabilities, and Chaos Gorilla, which drops a whole
    AWS availability zone.
  prefs: []
  type: TYPE_NORMAL
- en: Only Production Is Production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can apply the Chaos Monkey idea to Kubernetes applications, too. While
    you can run chaos engineering tools on a staging cluster to avoid disrupting production,
    that can only tell you so much. To learn about your production environment, you
    need to test production:'
  prefs: []
  type: TYPE_NORMAL
- en: Many systems are too big, complex, and cost-prohibitive to clone. Imagine trying
    to spin up a copy of Facebook for testing (with its multiple, globally distributed
    data centers).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The unpredictability of user traffic makes it impossible to mock; even if you
    could perfectly reproduce yesterday’s traffic, you still can’t predict tomorrow’s.
    Only production is production.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Charity Majors](https://oreil.ly/88wsu)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It’s also important to note that your chaos experiments, to be most useful,
    need to be automated and continuous. It’s no good doing it once and deciding that
    your system is reliable for evermore:'
  prefs: []
  type: TYPE_NORMAL
- en: The whole point of automating your chaos experiments is so that you can run
    them again and again to build trust and confidence in your system. Not just surfacing
    new weaknesses, but also ensuring that you’ve overcome a weakness in the first
    place.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Russ Miles (ChaosIQ)](https://oreil.ly/dq0Ij)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are several tools you can use for automatically chaos engineering your
    cluster. Here are a few options.
  prefs: []
  type: TYPE_NORMAL
- en: chaoskube
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*chaoskube*](https://oreil.ly/ffcdM) randomly kills Pods in your cluster.
    By default it operates in dry-run mode, which shows you what it would have done,
    but doesn’t actually terminate anything.'
  prefs: []
  type: TYPE_NORMAL
- en: You can configure chaoskube to include or exclude Pods based on labels (see
    [“Labels”](ch09.html#labels)), annotations, and namespaces, and to avoid certain
    time periods or dates (for example, don’t kill anything on Christmas Eve). By
    default, though, it will potentially kill any Pod in any namespace, including
    Kubernetes system Pods, and even chaoskube itself.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’re happy with your chaoskube filter configuration, you can disable
    dry-run mode and let it do its work.
  prefs: []
  type: TYPE_NORMAL
- en: chaoskube is simple to install and set up, and it’s an ideal tool for getting
    started with chaos engineering.
  prefs: []
  type: TYPE_NORMAL
- en: kube-monkey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*kube-monkey*](https://oreil.ly/HGmrb) runs at a preset time (by default,
    8 a.m. on weekdays), and builds a schedule of Deployments that will be targeted
    during the rest of the day (by default, 10 a.m. to 4 p.m.). Unlike some other
    tools, kube-monkey works on an opt-in basis: only those Pods that specifically
    enable kube-monkey using annotations will be targeted.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that you can add kube-monkey testing to specific apps or services
    during their development, and set different levels of frequency and aggression
    depending on the service. For example, the following annotation on a Pod will
    set a mean time between failures (MTBF) of two days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kill-mode` annotation lets you specify how many of a Deployment’s Pods
    will be killed, or a maximum percentage. The following annotations will kill up
    to 50% of the Pods in the targeted Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: PowerfulSeal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*PowerfulSeal*](https://oreil.ly/Wwe31) is an open source Kubernetes chaos
    engineering tool that works in two modes: interactive and autonomous. Interactive
    mode lets you explore your cluster and manually break things to see what happens.
    It can terminate nodes, namespaces, Deployments, and individual Pods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Autonomous mode uses a set of policies specified by you: which resources to
    operate on, which to avoid, when to run (you can configure it to only operate
    during working hours Monday–Friday, for example), and how aggressive to be (kill
    a given percentage of all matching Deployments, for example). PowerfulSeal’s policy
    files are very flexible and let you set up almost any imaginable chaos engineering
    scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: Best Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your applications require high availability, run a chaos testing tool such
    as chaoskube regularly to make sure that unexpected node or Pod failures don’t
    cause problems. Make sure you clear this first with the people responsible for
    operating the cluster and the applications under test.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It can be really difficult to know how to size and configure your first Kubernetes
    clusters. There are a lot of choices you can make, and you don’t really know what
    you’ll need until you’ve actually gained some production experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can’t make those decisions for you, but we hope we’ve at least given you
    some helpful things to think about when making them:'
  prefs: []
  type: TYPE_NORMAL
- en: Before provisioning your production Kubernetes cluster, think about how many
    nodes you’ll need, and of what size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need at least three control plane nodes (unless you’re using a managed service)
    and at least two (ideally three) worker nodes. This can make Kubernetes clusters
    seem a little expensive at first when you’re only running a few small workloads,
    but don’t forget the advantages of built-in resilience and scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes clusters can scale to many thousands of nodes and hundreds of thousands
    of containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need to scale beyond that, use multiple clusters (sometimes you need
    to do this for security or compliance reasons too). You can join clusters together
    using federation if you need to replicate workloads across clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes isn’t just for the cloud; it runs on bare-metal servers too. If you’ve
    got metal sitting around, why not use it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can scale your cluster up and down manually without too much trouble, and
    you probably won’t have to do it very often. Autoscaling is nice to have when
    your workloads grow and shrink.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There’s a well-defined standard for Kubernetes vendors and products: the CNCF
    Certified Kubernetes mark. If you don’t see this, ask why not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaos testing is a process of knocking out Pods at random and seeing if your
    application still works. It’s useful, but the cloud also has a way of doing its
    own chaos testing anyway, without you asking for it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
