- en: Chapter 11\. Policy and Governance for Your Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever wondered how you might ensure that all containers running on a
    cluster come only from an approved container registry? Or maybe you’ve been asked
    by the security team to enforce a policy that services are never exposed to the
    internet. These are precisely the challenges that policy and governance for your
    cluster set out to address. As Kubernetes continues to mature and becomes adopted
    by more enterprises, the question of how to apply policy and governance to Kubernetes
    resources is increasing in frequency. In this chapter we share what you can do
    and the tools to use to make sure that your cluster is in compliance with the
    defined policies, whether you work at a startup or an enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: Why Policy and Governance Are Important
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you operate in a highly regulated environment—for example, health care
    or financial services—or you simply want to make sure that you maintain a level
    of control over what’s running on your clusters, you’re going to need a way to
    implement the company-specific policies. Once your policy is defined, you will
    need to determine how to implement it and maintain clusters that are compliant
    to these policies. These policies may be required to meet regulatory compliance
    or simply to enforce best practices. Whatever the reason, you must be sure that
    you do not sacrifice developer agility and self-service when implementing these
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: How Is This Policy Different?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, policy is everywhere. Whether it be network policy or pod security,
    we’ve all come to understand what policy is and when to use it. We trust that
    whatever is declared in Kubernetes resource specifications is implemented as per
    the policy definition. Both network policy and pod security are implemented at
    runtime. However, what policy restricts the field values in these Kubernetes resource
    specifications? That’s the job of policy and governance. Rather than implementing
    policy at runtime, when we talk about policy in the context of governance, what
    we mean (or at least what we are trying to achieve) is the ability to limit the
    way fields are configured in Kubernetes resources. Only Kubernetes resource specifications
    that are compliant when evaluated by policies are allowed and committed to the
    cluster state.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Native Policy Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to evaluate which resources are compliant, we need a policy engine
    that is flexible enough to meet a variety of needs. [The Open Policy Agent (OPA)](https://oreil.ly/xzN2p)
    is an open source, flexible, lightweight policy engine that has become increasingly
    popular in the cloud native ecosystem. Having OPA in the ecosystem has allowed
    many implementations of different Kubernetes governance tools to appear. One such
    Kubernetes policy and governance project the community is rallying around is called
    [Gatekeeper](https://oreil.ly/RvKUw). For the rest of this chapter, we use Gatekeeper
    as the canonical example to illustrate how you might achieve policy and governance
    for your cluster. Although there are other implementations of policy and governance
    tools in the ecosystem, they all seek to provide the same user experience (UX)
    by allowing only compliant Kubernetes resource specifications to be committed
    to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Gatekeeper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gatekeeper is an open source, customizable Kubernetes admission webhook for
    cluster policy and governance. Gatekeeper takes advantage of the OPA constraint
    framework to enforce custom resource definition (CRD)-based policies. Using CRDs
    allows for an integrated Kubernetes experience that decouples policy authoring
    from implementation. Policy templates are referred to as *constraint templates*,
    which can be shared and reused across clusters. Gatekeeper enables resource validation
    and audit functionality. One of the great things about Gatekeeper is that it’s
    portable, which means that you can implement it on any Kubernetes clusters, and
    if you are already using OPA, you might be able to port that policy over to Gatekeeper.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gatekeeper is a production-ready open source project. For the latest stable
    version, please visit the official [upstream repository](https://oreil.ly/Rk8dc).
  prefs: []
  type: TYPE_NORMAL
- en: Example Policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into how to configure Gatekeeper, it’s important to keep the
    problem we are trying to solve in focus. While every organization/team will need
    to optimize their policies for their needs, some fairly universal policies serve
    as best practices. Let’s look at some policies that solve the most common compliance
    issues for context:'
  prefs: []
  type: TYPE_NORMAL
- en: Services must not be exposed publicly on the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow containers only from trusted container registries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All containers must have resource limits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress hostnames must not overlap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingresses must use only HTTPS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gatekeeper Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gatekeeper has adopted much of the same terminology as OPA. It’s important
    that we cover that terminology so you can understand how Gatekeeper operates.
    Gatekeeper uses the OPA constraint framework, which introduces three new terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Constraint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rego
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constraint template
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constraint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The best way to think about constraints is as restrictions that you apply to
    specific fields and values of Kubernetes resource specifications. This is really
    just a long way of saying policy. When constraints are defined, you are effectively
    stating that you *DO NOT* want to allow this. The implications of this approach
    mean that resources are implicitly allowed without a constraint that issues a
    deny. This is an important nuance because rather then allowing the Kubernetes
    resources specification fields and values you want, you are denying only the ones
    you *DO NOT* want. This architectural decision suits Kubernetes resource specifications
    nicely because they are ever changing.
  prefs: []
  type: TYPE_NORMAL
- en: Rego
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rego is an OPA-native query language. Rego queries are assertions on the data
    stored in OPA. Gatekeeper stores rego in the constraint template.
  prefs: []
  type: TYPE_NORMAL
- en: Constraint template
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Think of this as a policy template. It’s portable and reusable. Constraint templates
    consist of typed parameters and the target rego that is parameterized for reuse.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Constraint Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Constraint templates are a [custom resource definition](https://oreil.ly/LQSAH)
    (CRD) that provide a means of templating policy so that it can be shared or reused.
    In addition, parameters for the policy can be validated. Let’s look at a constraint
    template, from the upstream [Gatekeeper policy library](https://oreil.ly/HksnE),
    in the context of the earlier examples. In the following example, we share a constraint
    template that provides the policy “Only allow containers from trusted container
    registries”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The constraint template consists of three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes-required CRD metadata
  prefs: []
  type: TYPE_NORMAL
- en: The name is the most important part. It’s best practice to make it descriptive
    enough to easily identify the purpose of the policy. We reference this later.
  prefs: []
  type: TYPE_NORMAL
- en: Schema for input parameters
  prefs: []
  type: TYPE_NORMAL
- en: Indicated by the validation field, this section defines the input parameters
    and their associated types. In this example, we have a single parameter called
    `repos` that is an array of strings.
  prefs: []
  type: TYPE_NORMAL
- en: Policy definition
  prefs: []
  type: TYPE_NORMAL
- en: Indicated by the `target` field, this section contains templated rego (the language
    to define policy in OPA). Using a constraint template allows the templated rego
    to be reused and means that generic policy can be shared. If the rule matches,
    the constraint is violated.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use the previous constraint template, we must create a constraint resource.
    The purpose of the constraint resource is to provide the necessary parameters
    to the constraint template that we created earlier. You can see that the `kind`
    of the resource defined in the following example is `K8sAllowedRepos`, which maps
    to the constraint template defined in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The constraint consists of two main sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes metadata
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this constraint is of `kind K8sAllowedRepos`, which matches the
    name of the constraint template.
  prefs: []
  type: TYPE_NORMAL
- en: The spec
  prefs: []
  type: TYPE_NORMAL
- en: The `match` field defines the scope of intent for the policy. In this example,
    we are matching pods only in the production namespace.
  prefs: []
  type: TYPE_NORMAL
- en: The parameters define the intent for the policy. Notice that they match the
    type from the constraint template schema from the previous section. In this case,
    we allow only container images that start with `openpolicyagent/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Constraints have the following operational characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Logical AND
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When multiple policies validate the same field, if one violates then the whole
    request is rejected
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Schema validation that allows early error detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selection criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can use label selectors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Constrain only certain kinds
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Constrain only in certain namespaces
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Replication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, you might want to compare the current resource against other
    resources that are in the cluster, for example, in the case of “Ingress hostnames
    must not overlap.” OPA needs to have all the other Ingress resources in its cache
    in order to evaluate the rule. Gatekeeper uses a `config` resource to manage which
    data is cached in OPA in order to perform evaluations such as the one previously
    mentioned. In addition, `config` resources are also used in the audit functionality,
    which we explore a bit later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example `config` resource caches v1 service, pods, and namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: UX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gatekeeper enables real-time feedback to cluster users for resources that violate
    defined policy. If we consider the example from the previous sections, we allow
    containers only from repositories that start with `openpolicyagent/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to create the following resource; it is not compliant given the current
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives you the violation message that’s defined in the constraint template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using Enforcement Action and Audit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, we have discussed only how to define policy and have it enforced as
    part of the request admission process. Constraints include the ability to configure
    an `enforcementAction`, which by default is set to `deny`. In addition to `deny`,
    `enforcementAction` also allows accepted values of `warn` and `dryrun`. When we
    think about rolling out policy, it’s not always the case that you are applying
    to a cluster or namespace without resources already deployed. It’s therefore important
    to understand how to deploy policy to a cluster that already has resources deployed
    with the confidence that you can identify and remediate policy violations without
    necessarily breaking deployed workloads. The `enforcementAction` field allows
    you to define the behavior. When set to `deny`, a resource that violates policy
    will not be created and an error message will both be audit logged and sent back
    to the user. If set to `warn`, the resource will be created; however, a warning
    message will be audit logged and sent back to the user. Finally, if `dryrun` is
    set, the resource will be created and resources that violate the policy will be
    available in the audit log.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever `enforcementAction` you decide to use, Gatekeeper will periodically
    evaluate resources against any configured policy and provide an audit log. This
    helps with the detection of misconfigured resources according to policy and allows
    for remediation. The audit results are stored in the status field of the constraint,
    making them easy to find by simply using `kubectl`. To use audit, the resources
    to be audited must be replicated. For more details, refer to [“Data Replication”](#data_replication).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the constraint called `prod-repo-is-openpolicyagent` that you
    defined in the previous section. In this case, imagine we already had a pod called
    nginx running in the production namespace and we would like to check its compliance
    to the policy using audit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Upon inspection, you can see the last time the audit ran in the `auditTimestamp`
    field. We also see all the resources that violate this constraint, only the nginx
    pod in this case, under the `violations` along with the `enforcementAction`.
  prefs: []
  type: TYPE_NORMAL
- en: Mutation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to resource validation, Gatekeeper also allows you to configure
    mutation policies. Mutation policies allow you to modify Kubernetes resources
    at admission time. Generally, mutating resources at admission time is not considered
    best practice. Having resources “magically” modified by Gatekeeper is a cloud
    native antipattern as this is counter to the declarative nature of Kubernetes.
    Mutation policies are simply mentioned here to provide guidance to avoid them
    unless you feel your use case absolutely requires them and that you have exhausted
    other best practices. Refer to [Chapter 18](ch18.html#gitops) for more details
    on how to implement declarative best practices for Kubernetes resources.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the GitOps philosophy has become widely adopted, testing policy and evaluation
    as part of local testing or CI/CD pipelines has become a must have. Gatekeeper
    ships with a `gator` CLI that enables you to take the constraint templates and
    constraints and run a local evaluation. This is a great tool for building new
    policies, testing them against your resources, and remediating any issues prior
    to deploying them to your production clusters. The [Gatekeeper documentation](https://oreil.ly/Qj4p8)
    provides a practical guide to using the `gator` CLI to test policy.
  prefs: []
  type: TYPE_NORMAL
- en: Becoming Familiar with Gatekeeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’d like to explore Gatekeeper further, the repository ships with fantastic
    demonstration content that walks you through a detailed example of building policies
    to meet compliance for a bank. We would strongly recommend walking through the
    demonstration for a hands-on approach to how Gatekeeper operates. You can find
    the demonstration in [this Git repository](https://oreil.ly/GcR3i). Gatekeeper
    also maintains a [public library](https://oreil.ly/e8ESD) of policies that you
    can apply to your cluster with easy installation guidance via [ArtifactHub](https://oreil.ly/uEcfn).
  prefs: []
  type: TYPE_NORMAL
- en: Policy and Governance Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should consider the following best practices when implementing policy and
    governance on your clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to enforce a specific field in a pod, you need to determine which
    Kubernetes resource specification you want to inspect and enforce. Let’s consider
    the case of Deployments, for example. Deployments manage ReplicaSets, which manage
    pods. We could enforce at all three levels, but the best choice is the one that
    is the lowest handoff point before the runtime, which in this case is the pod.
    This decision, however, has implications. The user-friendly error message when
    we try to deploy a noncompliant pod, as seen in [“UX”](#user_x), is not going
    to be displayed. This is because the user is not creating the noncompliant resource,
    the ReplicaSet is. This experience means that the user would need to determine
    that the resource is not compliant by running a `kubectl describe` on the current
    ReplicaSet associated with the Deployment. Although this might seem cumbersome,
    this is consistent behavior with other Kubernetes features, such as pod security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Constraints can be applied to Kubernetes resources on the following criteria:
    kinds, namespaces, and label selectors. We would strongly recommend scoping the
    constraint to the resources to which you want it to be applied as tightly as possible.
    This ensures consistent policy behavior as the resources on the cluster grow,
    and means that resources that don’t need to be evaluated aren’t being passed to
    OPA, which can result in other inefficiencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On clusters with resources that are already deployed, utilize `warn` and `dryrun`
    along with audit to remediate resources that violate policy before setting the
    `enforcementAction` to `deny`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t use mutation policies; instead consider other declarative approaches,
    including GitOps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronizing and enforcing on potentially sensitive data such as Kubernetes
    secrets is *not* recommended. Given that OPA will hold this in its cache (if it
    is configured to replicate that data) and resources will be passed to Gatekeeper,
    it leaves surface area for a potential attack vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have many constraints defined, a deny of constraint means that the entire
    request is denied. There is no way to make this function as a logical OR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered why policy and governance are important and walked
    through a project that’s built upon OPA, a cloud native ecosystem policy engine,
    to provide a Kubernetes-native approach to policy and governance. You should now
    be prepared and confident the next time the security teams asks, “Are our clusters
    in compliance with our defined policy?”
  prefs: []
  type: TYPE_NORMAL
