- en: Chapter 13\. Administration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers replica set administration, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing maintenance on individual members
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring sets under a variety of circumstances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting information about and resizing your oplog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing some more exotic set configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting from master/slave to a replica set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting Members in Standalone Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of maintenance tasks cannot be performed on secondaries (because they
    involve writes) and shouldn’t be performed on primaries because of the impact
    this could have on application performance. Thus, the following sections frequently
    mention starting up a server in standalone mode. This means restarting the member
    so that it is a standalone server, not a member of a replica set (temporarily).
  prefs: []
  type: TYPE_NORMAL
- en: 'To start up a member in standalone mode, first look at the command-line options
    used to start it. Suppose they look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To perform maintenance on this server we can restart it without the `replSet`
    option. This will allow us to read and write to it as a normal standalone *mongod*.
    We don’t want the other servers in the set to be able to contact it, so we’ll
    make it listen on a different port (so that the other members won’t be able to
    find it). Finally, we want to keep the `dbpath` the same, as we are presumably
    starting it up this way to manipulate the server’s data somehow.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we shut down the server from the *mongo* shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in an operating system shell (e.g., bash), we restart *mongod* on another
    port and without the `replSet` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It will now be running as a standalone server, listening on port 30000 for connections.
    The other members of the set will attempt to connect to it on port 27017 and assume
    that it is down.
  prefs: []
  type: TYPE_NORMAL
- en: When we have finished performing maintenance on the server, we can shut it down
    and restart it with its original options. It will automatically sync up with the
    rest of the set, replicating any operations that it missed while it was “away.”
  prefs: []
  type: TYPE_NORMAL
- en: Replica Set Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Replica set configuration is always kept in a document in the *local.system.replset*
    collection. This document is the same on all members of the set. Never update
    this document using `update`. Always use an `rs` helper or the `replSetReconfig`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Replica Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You create a replica set by starting up the *mongods* that you want to be members
    and then passing one of them a configuration through `rs.initiate()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You should always pass a config object to `rs.initiate()`. If you do not, MongoDB
    will attempt to automatically generate a config for a one-member replica set;
    it might not use the hostname that you want or correctly configure the set.
  prefs: []
  type: TYPE_NORMAL
- en: You only call `rs.initiate()` on one member of the set. The member that receives
    the configuration will pass it on to the other members.
  prefs: []
  type: TYPE_NORMAL
- en: Changing Set Members
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you add a new set member, it should either have nothing in its data directory—in
    which case it will perform an initial sync—or have a copy of the data from another
    member (see [Chapter 23](ch23.xhtml#chapter-backup) for more information about
    backing up and restoring replica set members).
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to the primary and add a new member as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can specify a more complex member config as a document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also remove members by their `"host"` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can change a member’s settings by reconfiguring. There are a few restrictions
    in changing a member’s settings:'
  prefs: []
  type: TYPE_NORMAL
- en: You cannot change a member’s `"_id"`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot make the member you’re sending the reconfig to (generally the primary)
    priority 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot turn an arbiter into a nonarbiter, or vice versa.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot change a member’s `"buildIndexes"` field from `false` to `true`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notably, you *can* change a member’s `"host"` field. Thus, if you incorrectly
    specify a host (say, if you use a public IP instead of a private one) you can
    later go back and simply change the config to use the correct IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'To change a hostname, you could do something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This same strategy applies to changing any other option: fetch the config with
    `rs.config()`, modify any parts of it that you wish, and reconfigure the set by
    passing `rs.reconfig()` the new configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Larger Sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Replica sets are limited to 50 members in total and only 7 voting members. This
    is to reduce the amount of network traffic required for everyone to heartbeat
    everyone else and to limit the amount of time elections take.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are creating a replica set that has more than seven members, every additional
    member must be given zero votes. You can do this by specifying it in the member’s
    config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This prevents these members from casting positive votes in elections.
  prefs: []
  type: TYPE_NORMAL
- en: Forcing Reconfiguration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you permanently lose a majority of a set, you may want to reconfigure
    the set while it doesn’t have a primary. This is a little tricky, as usually you’d
    send the reconfig to the primary. In this case, you can force-reconfigure the
    set by sending a reconfig command to a secondary. Connect to a secondary in the
    shell and pass it a reconfig with the `"force"` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Forced reconfigurations follow the same rules as a normal reconfiguration:
    you must send a valid, well-formed configuration with the correct options. The
    `"force"` option doesn’t allow invalid configs; it just allows a secondary to
    accept a reconfig.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Forced reconfigurations bump the replica set `"version"` number by a large
    amount. You may see it jump by tens or hundreds of thousands. This is normal:
    it is to prevent version number collisions (just in case there’s a reconfig on
    either side of a network partition).'
  prefs: []
  type: TYPE_NORMAL
- en: When the secondary receives the reconfig, it will update its configuration and
    pass the new config along to the other members. The other members of the set will
    only pick up on a change of config if they recognize the sending server as a member
    of their current config. Thus, if some of your members have changed hostnames,
    you should force reconfig from a member that kept its old hostname. If every member
    has a new hostname, you should shut down each member of the set, start a new one
    up in standalone mode, change its *local.system.replset* document manually, and
    then restart the member.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating Member State
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several ways to manually change a member’s state for maintenance or
    in response to load. Note that there is no way to force a member to become primary,
    however, other than configuring the set appropriately—in this case, by giving
    the replica set member a priority higher than any other member of the set.
  prefs: []
  type: TYPE_NORMAL
- en: Turning Primaries into Secondaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can demote a primary to a secondary using the `stepDown` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This makes the primary step down into SECONDARY state for 60 seconds. If no
    other primary is elected in that time period, it will be able to attempt a reelection.
    If you would like it to remain a secondary for a longer or shorter amount of time,
    you can specify your own number of seconds for it to stay in SECONDARY state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Preventing Elections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you need to do some maintenance on the primary but don’t want any of the
    other eligible members to become primary in the interim, you can force them to
    stay secondaries by running `freeze` on each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Again, this takes a number of seconds for the member to remain a secondary.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you finish whatever maintenance you’re doing on the primary before this
    time elapses and want to unfreeze the other members, simply run the command again
    on each of them, giving a timeout of 0 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: An unfrozen member will be able to hold an election, if it chooses.
  prefs: []
  type: TYPE_NORMAL
- en: You can also unfreeze primaries that have been stepped down by running `rs.freeze(0)`.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to be able to monitor the status of a set: not only that all
    members are up, but what states they are in and how up to date the replication
    is. There are several commands you can use to see replica set information. MongoDB
    hosting services and management tools including Atlas, Cloud Manager, and Ops
    Manager (see [Chapter 22](ch22.xhtml#chapter-mms)) also provide mechanisms to
    monitor replication and dashboards on the key replication metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Often issues with replication are transient: a server could not reach another
    server, but now it can. The easiest way to see issues like this is to look at
    the logs. Make sure you know where the logs are being stored (and that they *are*
    being stored) and that you can access them.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the Status
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most useful commands you can run is `replSetGetStatus`, which gets
    the current information about every member of the set (from the view of the member
    you’re running it on). There is a helper for this command in the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'These are some of the most useful fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"self"`'
  prefs: []
  type: TYPE_NORMAL
- en: This field is only present in the member `rs.status()` was run on—in this case,
    *server-2* (*m1.example.net:27017*).
  prefs: []
  type: TYPE_NORMAL
- en: '`"stateStr"`'
  prefs: []
  type: TYPE_NORMAL
- en: A string describing the state of the server. See [“Member States”](ch11.xhtml#sect2-rs-states)
    for descriptions of the various states.
  prefs: []
  type: TYPE_NORMAL
- en: '`"uptime"`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of seconds a member has been reachable, or the time since this server
    was started for the `"self"` member. Thus, *server-1* has been up for 269 seconds,
    and *server-2* and *server-3* for 14 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '`"optimeDate"`'
  prefs: []
  type: TYPE_NORMAL
- en: The last optime in each member’s oplog (where that member is synced to). Note
    that this is the state of each member as reported by the heartbeat, so the optime
    reported here may be off by a couple of seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '`"lastHeartbeat"`'
  prefs: []
  type: TYPE_NORMAL
- en: The time this server last received a heartbeat from the `"self"` member. If
    there have been network issues or the server has been busy, this may be longer
    than two seconds ago.
  prefs: []
  type: TYPE_NORMAL
- en: '`"pingMs"`'
  prefs: []
  type: TYPE_NORMAL
- en: The running average of how long heartbeats to this server have taken. This is
    used in determining which member to sync from.
  prefs: []
  type: TYPE_NORMAL
- en: '`"errmsg"`'
  prefs: []
  type: TYPE_NORMAL
- en: Any status message that the member chose to return in the heartbeat request.
    These are often merely informational, not error messages. For example, the `"errmsg"`
    field in *server-3* indicates that this server is in the process of initial syncing.
    The hexadecimal number 507e9a30:851 is the timestamp of the operation this member
    needs to get to to complete the initial sync.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several fields that give overlapping information. `"state"` is the
    same as `"stateStr"`; it’s simply the internal ID for the state. `"health"` merely
    reflects whether a given server is reachable (`1`) or unreachable (`0`), which
    is also shown by `"state"` and `"stateStr"` (they’ll be `UNKNOWN` or `DOWN` if
    the server is unreachable). Similarly, `"optime"` and `"optimeDate"` are the same
    value represented in two ways: one represents milliseconds since the epoch (`"t"
    : 135...`) and the other is a more human-readable date.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note that this report is from the point of view of whichever member of the
    set you run it on: the information it contains may be incorrect or out of date
    due to network issues.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Replication Graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you run `rs.status()` on a secondary, there will be a top-level field called
    `"syncingTo"`. This gives the host that this member is replicating from. By running
    the `replSetGetStatus` command on each member of the set, you can figure out the
    replication graph. For example, assuming `server1` was a connection to *server1*,
    `server2` was a connection to *server2*, and so on, you might have something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Thus, *server0* is the replication source for *server1*, *server1* is the replication
    source for *server2* and *server3*, and *server2* is the replication source for
    *server4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'MongoDB determines who to sync to based on ping time. When one member heartbeats
    another, it times how long that request takes. MongoDB keeps a running average
    of these times. When a member has to choose another member to sync from, it looks
    for the one that is closest to it and ahead of it in replication (thus, you cannot
    end up with a replication cycle: members will only replicate from the primary
    or secondaries that are further ahead).'
  prefs: []
  type: TYPE_NORMAL
- en: This means that if you bring up a new member in a secondary data center, it
    is more likely to sync from another member in that data center than a member in
    your primary data center (thus minimizing WAN traffic), as shown in [Figure 13-1](#repl-admin-1).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a downside to automatic replication chaining: more replication
    hops means that it takes a bit longer to replicate writes to all servers. For
    example, let’s say that everything is in one data center but, due to the vagaries
    of network speeds when you added members, MongoDB ends up replicating in a line,
    as shown in [Figure 13-2](#repl-admin-2).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. New secondaries will generally choose to sync from a member in
    the same data center
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/mdb3_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. As replication chains get longer, it takes longer for all members
    to get a copy of the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is highly unlikely, but not impossible. It is, however, probably undesirable:
    each secondary in the chain will have to be a bit further behind than the secondary
    “in front” of it. You can fix this by modifying the replication source for a member
    using the `replSetSyncFrom` command (or the `rs.syncFrom()` helper).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to the secondary whose replication source you want to change and run
    this command, passing it the server you’d prefer this member to sync from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: It may take a few seconds to switch sync sources, but if you run `rs.status()`
    on that member again, you should see that the `"syncingTo"` field now says `"server0:27017"`.
  prefs: []
  type: TYPE_NORMAL
- en: This member (*server4*) will now continue replicating from *server0* until *server0*
    becomes unavailable or, if it happened to be a secondary, falls significantly
    behind the other members.
  prefs: []
  type: TYPE_NORMAL
- en: Replication Loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *replication loop* is when members end up replicating from one another—for
    example, *A* is syncing from *B* who is syncing from *C* who is syncing from *A*.
    As none of the members in a replication loop can be a primary, the members will
    not receive any new operations to replicate and will fall behind.
  prefs: []
  type: TYPE_NORMAL
- en: Replication loops should be impossible when members choose who to sync from
    automatically. However, you can force replication loops using the `replSetSyncFrom`
    command. Inspect the `rs.status()` output carefully before manually changing sync
    targets, and be careful not to create loops. The `replSetSyncFrom` command will
    warn you if you do not choose to sync from a member that is strictly ahead, but
    it will allow it.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling Chaining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Chaining* is when a secondary syncs from another secondary (instead of the
    primary). As mentioned earlier, members may decide to sync from other members
    automatically. You can disable chaining, forcing everyone to sync from the primary,
    by changing the `"chainingAllowed"` setting to `false` (if not specified, it defaults
    to `true`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With `"chainingAllowed"` set to `false`, all members will sync from the primary.
    If the primary becomes unavailable, they will fall back to syncing from secondaries.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Lag
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important metrics to track for replication is how well the secondaries
    are keeping up with the primary. *Lag* is how far behind a secondary is, which
    means the difference between the timestamp of the last operation the primary has
    performed and the timestamp of the last operation the secondary has applied.
  prefs: []
  type: TYPE_NORMAL
- en: You can use `rs.status()` to see a member’s replication state, but you can also
    get a quick summary by running `rs.printReplicationInfo()` or `rs.printSlaveReplicationInfo()`.
  prefs: []
  type: TYPE_NORMAL
- en: '`rs.printReplicationInfo()` gives a summary of the primary’s oplog, including
    its size and the date range of its operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the oplog is about 10 MB (10 MiB) and is only able to fit about
    an hour of operations.
  prefs: []
  type: TYPE_NORMAL
- en: If this were a real deployment, the oplog should probably be larger (see the
    next section for instructions on changing oplog size). We want the log length
    to be *at least* as long as the time it takes to do a full resync. That way, we
    don’t run into a case where a secondary falls off the end of the oplog before
    finishing its initial sync.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The log length is computed by taking the time difference between the first and
    last operation in the oplog once the oplog has filled up. If the server has just
    started with nothing in the oplog, then the earliest operation will be relatively
    recent. In that case, the log length will be small, even though the oplog probably
    still has free space available. The length is a more useful metric for servers
    that have been operating long enough to write through their entire oplog at least
    once.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `rs.printSlaveReplicationInfo()` function to get the `syncedTo`
    value for each member and the time when the last oplog entry was written to each
    secondary, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Remember that a replica set member’s lag is calculated relative to the primary,
    not against “wall time.” This usually is irrelevant, but on very low-write systems,
    this can cause phantom replication lag “spikes.” For example, suppose you do a
    write once an hour. Right after that write, before it’s replicated, the secondary
    will look like it’s an hour behind the primary. However, it’ll be able to catch
    up with that “hour” of operations in a few milliseconds. This can sometimes cause
    confusion when monitoring a low-throughput system.
  prefs: []
  type: TYPE_NORMAL
- en: Resizing the Oplog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your primary’s oplog should be thought of as your maintenance window. If your
    primary has an oplog that is an hour long, then you only have one hour to fix
    anything that goes wrong before your secondaries fall too far behind and must
    be resynced from scratch. Thus, you generally want to have an oplog that can hold
    a couple days’ to a week’s worth of data, to give yourself some breathing room
    if something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there’s no easy way to tell how long your oplog is going to be
    before it fills up. The WiredTiger storage engine allows online resizing of your
    oplog while your server is running. You should perform these steps on each secondary
    replica set member first; once these have been changed, then and only then should
    you make the changes to your primary. Remember that each server that could become
    a primary should have a large enough oplog to give you a sane maintenance window.
  prefs: []
  type: TYPE_NORMAL
- en: 'To increase the size of your oplog, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to the replica set member. If authentication is enabled, be sure to
    use a user with privileges that can modify the `local` database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify the current size of the oplog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This will display the collection size in megabytes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change the oplog size of the replica set member:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The following operation changes the oplog size of the replica set member to
    16 gigabytes, or 16000 megabytes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, if you have reduced the size of the oplog, you may need to run the
    `compact` to reclaim the disk space allocated. This should not be run against
    a member while it is a primary. Please see the [“Change the Size of the Oplog”
    tutorial in the MongoDB documentation](https://oreil.ly/krv0R) for more details
    on this case and on the entire procedure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You generally should not decrease the size of your oplog: although it may be
    months long, there is usually ample disk space for it and it does not use up any
    valuable resources like RAM or CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: Building Indexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you send an index build to the primary, the primary will build the index
    normally and then the secondaries will build the index when they replicate the
    “build index” operation. Although this is the easiest way to build an index, index
    builds are resource-intensive operations that can make members unavailable. If
    all of your secondaries start building an index at the same time, almost every
    member of your set will be offline until the index build completes. This process
    is only for replica sets; for a sharded cluster, please see [the MongoDB documentation
    tutorial about building indexes on a sharded cluster](https://oreil.ly/wJNeE).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You must stop all writes to a collection when you are creating a `"unique"`
    index. If the writes are not stopped, you can end up with inconsistent data across
    the replica set members.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you may want to build an index on one member at a time to minimize
    the impact on your application. To accomplish this, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Shut down a secondary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart it as a standalone server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the index on the standalone server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the index build is complete, restart the server as a member of the replica
    set. When restarting this member, you need to remove the `disableLogicalSessionCacheRefresh`
    parameter if it is present in your command-line options or configuration file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 through 4 for each secondary in the replica set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should now have a set where every member other than the primary has the
    index built. Now there are two options, and you should choose the one that will
    impact your production system the least:'
  prefs: []
  type: TYPE_NORMAL
- en: Build the index on the primary. If you have an “off” time when you have less
    traffic, that would probably be a good time to build it. You also might want to
    modify read preferences to temporarily shunt more load onto secondaries while
    the build is in progress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The primary will replicate the index build to the secondaries, but they will
    already have the index so it will be a no-op for them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Step down the primary, then follow steps 2 through 4 of the procedure outlined
    previously. This requires a failover, but you will have a normally functioning
    primary while the old primary is building its index. After its index build is
    complete, you can reintroduce it to the set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that you could also use this technique to build different indexes on a
    secondary than you have on the rest of the set. This could be useful for offline
    processing, but make sure a member with different indexes can never become primary:
    its priority should always be `0`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are building a unique index, make sure that the primary is not inserting
    duplicates or that you build the index on the primary first. Otherwise, the primary
    could be inserting duplicates that would then cause replication errors on secondaries.
    If this occurs, the secondary will shut itself down. You will have to restart
    it as a standalone server, remove the unique index, and restart it.
  prefs: []
  type: TYPE_NORMAL
- en: Replication on a Budget
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If it is difficult to get more than one high-quality server, consider getting
    a secondary server that is strictly for disaster recovery, with less RAM and CPU,
    slower disk I/O, etc. The good server will always be your primary and the cheaper
    server will never handle any client traffic (configure your clients to send all
    reads to the primary). Here are the options to set for the cheaper box:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"priority" : 0`'
  prefs: []
  type: TYPE_NORMAL
- en: You do not want this server to ever become primary.
  prefs: []
  type: TYPE_NORMAL
- en: '`"hidden" : true`'
  prefs: []
  type: TYPE_NORMAL
- en: You do not want clients ever sending reads to this secondary.
  prefs: []
  type: TYPE_NORMAL
- en: '`"buildIndexes" : false`'
  prefs: []
  type: TYPE_NORMAL
- en: This is optional, but it can decrease the load this server has to handle considerably.
    If you ever need to restore from this server, you’ll need to rebuild the indexes.
  prefs: []
  type: TYPE_NORMAL
- en: '`"votes" : 0`'
  prefs: []
  type: TYPE_NORMAL
- en: If you only have two machines, set `"votes"` on this secondary to `0` so that
    the primary can stay primary if this machine goes down. If you have a third server
    (even just your application server), run an arbiter on that instead of setting
    `"votes"` to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: This will give you the safety and security of having a secondary without having
    to invest in two high-performance servers.
  prefs: []
  type: TYPE_NORMAL
