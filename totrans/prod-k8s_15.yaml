- en: Chapter 14\. Application Considerations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 应用考虑因素
- en: Kubernetes is rather flexible when it comes to the type of applications it can
    run and manage. Barring operating system and processor type limitations, Kubernetes
    can essentially run anything. Large monoliths, distributed microservices, batch
    workloads, you name it. The only requirement that Kubernetes imposes on workloads
    is that they are distributed as container images. With that said, there are certain
    steps you can take to make your applications better Kubernetes citizens.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到Kubernetes可以运行和管理的应用程序类型时，它相当灵活。除了操作系统和处理器类型的限制外，Kubernetes基本上可以运行任何东西。大型单体应用、分布式微服务、批处理工作负载，应有尽有。Kubernetes对工作负载施加的唯一要求是它们以容器镜像的形式分发。话虽如此，您可以采取一些步骤来使您的应用程序成为更好的Kubernetes公民。
- en: In this chapter, we will pivot our discussions to focus on the application instead
    of the platform. If you are part of a platform team, don’t skip this chapter.
    While you might think it only applies to developers, it also applies to you. As
    a platform team member, you will most likely get to build applications to provide
    custom services on your platform. Even if you don’t, the discussions in this chapter
    will help you better align with development teams consuming the platform, and
    even educate those teams that might be unfamiliar with container-based platforms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论重点转移到应用程序，而不是平台。如果您是平台团队的一员，请不要跳过本章。虽然您可能认为这仅适用于开发人员，但它也适用于您。作为平台团队的成员，您很可能会构建应用程序，为平台上提供定制服务。即使您不这样做，本章的讨论也将帮助您更好地与使用平台的开发团队对齐，甚至教育那些可能对基于容器的平台不熟悉的团队。
- en: 'This chapter covers various considerations you should make when running applications
    on Kubernetes. Mainly:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了在Kubernetes上运行应用程序时应考虑的各种因素。主要包括：
- en: Deploying applications onto the platform, and mechanisms to manage deployment
    manifests, such as templating and packaging.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将应用程序部署到平台上，并管理部署清单的机制，如模板化和打包。
- en: Approaches to configure applications, such as using Kubernetes APIs (ConfigMaps/Secrets),
    and integrating with external systems for config and secret management.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置应用程序的方法，例如使用Kubernetes API（ConfigMaps/Secrets），并集成外部系统进行配置和密钥管理。
- en: Kubernetes features that improve the availability of your workloads, such as
    pre-stop container hooks, graceful termination, and scheduling constraints.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes的功能，可提高工作负载的可用性，如预停止容器钩子、优雅终止和调度约束。
- en: State probes, a feature of Kubernetes that enables you to surface application
    health information to the platform.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: State probes，这是Kubernetes的一个功能，使您能够向平台展示应用程序的健康信息。
- en: Resource requests and limits, which are critical to ensure your applications
    run properly on the platform.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源请求和限制对确保应用程序在平台上正常运行至关重要。
- en: Logs, metrics, and tracing as mechanisms to debug, troubleshoot, and operate
    your workloads effectively.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志、指标和跟踪作为调试、故障排除和有效操作工作负载的机制。
- en: Deploying Applications to Kubernetes
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将应用程序部署到Kubernetes
- en: 'Once your application is containerized and available in a container image registry,
    you are ready to deploy it onto Kubernetes. In most cases, deploying the application
    involves writing YAML manifests that describe the Kubernetes resources required
    to run the app, such as Deployments, Services, ConfigMaps, CRDs, etc. Then, you
    send the manifests to the API server, and Kubernetes takes care of the rest. Using
    raw YAML manifests is a great way to get started, but it can quickly become impractical,
    especially when deploying the application onto different clusters or environments.
    You will most likely encounter questions similar to the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的应用程序被容器化并存储在容器镜像注册表中，您就可以准备将其部署到Kubernetes上。在大多数情况下，部署应用程序涉及编写描述运行应用程序所需的Kubernetes资源的YAML清单，例如Deployments、Services、ConfigMaps、CRDs等。然后，您将清单发送到API服务器，Kubernetes会处理其余工作。使用原始YAML清单是入门的好方法，但在将应用程序部署到不同集群或环境时，它很快可能变得不切实际。您很可能会遇到类似以下问题：
- en: How do I provide different credentials when running in staging versus production?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在演练和生产环境中运行时如何提供不同的凭证？
- en: How can I use a different image registry when deploying in various datacenters?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同数据中心部署时如何使用不同的镜像注册表？
- en: How do I set different replica counts in development versus production?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在开发和生产环境中设置不同的副本计数？
- en: How can I ensure all port numbers match up across the different manifests?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何确保不同清单中的所有端口号匹配？
- en: The list goes on and on. And while you could have multiple sets of manifests
    to solve for each of these concerns, the permutations make it rather challenging
    to manage. In this section, we will discuss approaches you can take to address
    the issue of manifest management. Mainly, we will cover templating manifests and
    packaging applications for Kubernetes. We will not, however, discuss the gamut
    of tools available in the community. More often than not, we find that teams get
    stuck in analysis paralysis when considering the different options. Our advice
    is to choose *something* and move on to solving higher-value concerns.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 清单不胜枚举。虽然您可以为每个问题解决方案创建多组清单，但排列组合使得管理变得相当具有挑战性。在本节中，我们将讨论您可以采取的方法来解决清单管理问题。主要包括清单模板化和为
    Kubernetes 打包应用程序。然而，我们不会详细讨论社区中可用的各种工具。我们经常发现，团队在考虑不同选择时陷入分析瘫痪。我们的建议是选择*某种*方法，然后继续解决更高价值的问题。
- en: Templating Deployment Manifests
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模板化部署清单
- en: Templating involves introducing placeholders in your deployment manifests. Instead
    of hardcoding values in the manifests, the placeholders provide a mechanism for
    you to inject values as necessary. For example, the following templated manifest
    enables you to set replica counts to different values. Perhaps you need one replica
    in development, but five in production.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模板化涉及在您的部署清单中引入占位符。占位符提供了一种机制，可以根据需要注入值，而不是在清单中硬编码值。例如，以下模板化清单使您可以将复制数量设置为不同的值。也许在开发环境中您只需要一个副本，但在生产环境中需要五个副本。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Packaging Applications for Kubernetes
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 应用程序打包
- en: Creating self-contained software packages is another mechanism you can use to
    deploy your application while addressing the manifest management. Packaging solutions
    usually build upon templating, but they introduce additional functionality that
    can be useful, such as the ability to push the package to OCI-compatible registries,
    life cycle management hooks, and more.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自包含软件包是另一种可以用来部署应用程序并解决清单管理问题的机制。打包解决方案通常建立在模板化的基础上，但它们引入了其他有用的附加功能，例如能够将包推送到兼容
    OCI 的注册表、生命周期管理钩子等等。
- en: 'Packages are a great mechanism to consume software maintained by a third party
    or deliver software to third parties. If you’ve used Helm to install software
    into a Kubernetes cluster, you’ve already leveraged the benefits of packaging.
    If you are unfamiliar with Helm, the following snippet gives you an idea of what
    it takes to install a package:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 软件包是消耗由第三方维护的软件或将软件交付给第三方的重要机制。如果您已经使用 Helm 将软件安装到 Kubernetes 集群中，则已经利用了打包的好处。如果您对
    Helm 不熟悉，以下片段为您展示了安装软件包所需的步骤：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, packages can be a great way to deploy and manage software on
    Kubernetes. With that said, packages can fall short when it comes to complex applications
    that require advanced life cycle management. For such applications, we find operators
    to be a better solution. We discuss operators extensively in [Chapter 2](ch02.html#deployment_models).
    Even though the chapter focuses on platform services, the concepts discussed apply
    when building operators for complex applications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，软件包是在 Kubernetes 上部署和管理软件的好方法。话虽如此，当涉及到需要高级生命周期管理的复杂应用程序时，软件包可能会显得力不从心。对于这样的应用程序，我们发现操作员是更好的解决方案。我们在[第
    2 章](ch02.html#deployment_models)中详细讨论了操作员。尽管该章节侧重于平台服务，但所讨论的概念在构建用于复杂应用程序的操作员时同样适用。
- en: Ingesting Configuration and Secrets
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置和密钥摄入
- en: Applications typically have configuration that tells them how to behave at runtime.
    Configuration commonly includes logging levels, hostnames of dependencies (e.g.,
    DNS record for a database), timeouts, and more. Some of these settings can contain
    sensitive information, such as passwords, that we usually call secrets. In this
    section, we will discuss the different methods you can use to configure applications
    on a Kubernetes-based platform. First, we will review the ConfigMap and Secret
    APIs available in core Kubernetes. Then, we will explore an alternative to the
    Kubernetes API, mainly integrating with an external system. Finally, we will provide
    guidance on these approaches based on what we’ve seen work best in the field.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序通常具有告知它们在运行时如何行为的配置。配置通常包括日志记录级别、依赖项的主机名（例如，数据库的 DNS 记录）、超时等内容。其中一些设置可能包含敏感信息，例如密码，通常称为密钥。在本节中，我们将讨论在基于
    Kubernetes 平台的应用程序配置的不同方法。首先，我们将审查 Kubernetes 核心中提供的 ConfigMap 和 Secret API。然后，我们将探讨与外部系统集成的
    Kubernetes API 替代方法。最后，我们将根据实地经验提供关于这些方法的指导，介绍哪些方法最有效。
- en: Before digging in, it is worth mentioning that you should avoid bundling configuration
    or secrets inside your application’s container image. The tight coupling between
    the application binary and its configuration defeats the purpose of runtime configuration.
    Furthermore, it poses a security problem in the case of secrets, as the image
    might be accessible to actors that should otherwise not have access to the secrets.
    Instead of including config in the image, you should leverage platform features
    to inject configuration at runtime.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论之前，值得一提的是，您应该避免将配置或密钥捆绑在应用程序的容器镜像中。应用程序二进制文件与其配置的紧密耦合会破坏运行时配置的目的。此外，对于密钥而言，这种做法还存在安全问题，因为镜像可能被不应访问密钥的角色访问到。您应该利用平台特性在运行时注入配置，而不是将配置包含在镜像中。
- en: Kubernetes ConfigMaps and Secrets
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 的 ConfigMaps 和 Secrets
- en: 'ConfigMaps and Secrets are core resources in the Kubernetes API that enable
    you to configure your applications at runtime. As with any other resource in Kubernetes,
    they are created via the API server and are usually declared in YAML, such as
    the following example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMaps 和 Secrets 是 Kubernetes API 中的核心资源，使您能够在运行时配置您的应用程序。与 Kubernetes 中的任何其他资源一样，它们通过
    API 服务器创建，并通常以 YAML 形式声明，例如以下示例：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s discuss how you can consume ConfigMaps and Secrets in your applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何在您的应用程序中使用 ConfigMaps 和 Secrets。
- en: 'The first method is to mount ConfigMaps and Secrets as files in the Pod’s filesystem.
    When building your Pod specification, you can add volumes that reference ConfigMaps
    or Secrets by name and mount them into containers at specific locations. For example,
    the following snippet defines a Pod that mounts the ConfigMap named `my-config`
    into the container named `my-app` at `/etc/my-app/config.json`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是将 ConfigMaps 和 Secrets 作为文件挂载到 Pod 文件系统中。在构建 Pod 规范时，您可以添加引用 ConfigMaps
    或 Secrets 的卷，并将它们挂载到容器的特定位置。例如，以下片段定义了一个 Pod，将名为 `my-config` 的 ConfigMap 挂载到名为
    `my-app` 的容器中，路径为 `/etc/my-app/config.json`：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Leveraging volume mounts is the preferred method when it comes to consuming
    ConfigMaps and Secrets. The reason is that the files in the Pod are dynamically
    updated, which allows you to reconfigure applications without restarting the app
    or re-creating the Pod. With that said, this is something that the application
    must support. The application must watch the configuration files on disk and apply
    new configuration when the files change. Many libraries and frameworks make it
    easy to implement this functionality. When this is not possible, you can introduce
    a sidecar container that watches the config files and signals the main process
    (with a SIGHUP, for example) when new configuration is available.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 利用卷挂载是在消费 ConfigMaps 和 Secrets 时的首选方法。原因在于 Pod 中的文件是动态更新的，这使您能够重新配置应用程序而无需重新启动应用程序或重新创建
    Pod。尽管如此，应用程序必须支持这一功能。应用程序必须监视磁盘上的配置文件，并在文件更改时应用新配置。许多库和框架使实现此功能变得容易。当这不可行时，您可以引入一个
    sidecar 容器来监视配置文件，并在有新配置可用时向主进程发信号（例如，使用 SIGHUP）。
- en: 'Consuming ConfigMaps and Secrets via environment variables is another method
    you can use. If your application expects configuration through environment variables,
    this is the natural approach to follow. Environment variables can also be helpful
    if you need to provide settings via command-line flags. In the following example,
    the Pod sets the `DEBUG` environment variable using a ConfigMap named `my-config`,
    which has a key called `debug` that contains the value:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过环境变量消费ConfigMaps和Secrets是另一种可以使用的方法。如果您的应用程序通过环境变量来期望配置，这是一个自然的方法。环境变量还可以在需要通过命令行标志提供设置时提供帮助。在以下示例中，Pod使用名为`my-config`的ConfigMap设置了`DEBUG`环境变量，该ConfigMap有一个名为`debug`的键，其包含以下值：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: One of the downsides of using environment variables is that changes to the ConfigMaps
    or Secrets are not reflected in the running Pod until it restarts. This might
    not be a problem for some applications, but you should keep it in mind. Another
    downside, mainly for Secrets, is that some applications or frameworks may dump
    the environment details into logs during startup or when they crash. This poses
    a security risk as secrets can be leaked into logfiles inadvertently.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用环境变量的一个缺点是，对于ConfigMaps或Secrets的更改在运行的Pod中不会反映出来，直到它重新启动。这对某些应用程序可能并非问题，但您应该牢记这一点。另一个缺点，主要针对Secrets，是某些应用程序或框架在启动时或崩溃时可能会将环境细节转储到日志中。这会造成安全风险，因为秘密可能会不经意地泄露到日志文件中。
- en: These first two ConfigMap and Secret consumption methods rely on Kubernetes
    injecting the configuration into the workload. Another option is for the application
    to communicate with the Kubernetes API to get its configuration. Instead of using
    config files or environment variables, the application reads ConfigMaps and Secrets
    straight from the Kubernetes API server. The app can also watch the API so that
    it can act whenever the configuration changes. Developers can use one of the many
    Kubernetes libraries or SDKs to implement this functionality or leverage application
    frameworks that support this capability, such as Spring Cloud Kubernetes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种首选的ConfigMap和Secret消费方法依赖于Kubernetes将配置注入到工作负载中。另一个选择是应用程序与Kubernetes API通信来获取其配置。应用程序不使用配置文件或环境变量，而是直接从Kubernetes
    API服务器读取ConfigMaps和Secrets。应用程序还可以监听API，以便在配置更改时执行操作。开发人员可以使用许多Kubernetes库或SDK来实现此功能，或者利用支持此功能的应用程序框架，如Spring
    Cloud Kubernetes。
- en: While leveraging the Kubernetes API for application configuration can be convenient,
    we find that there are important downsides you should consider. First, the need
    to connect to the API server to get configuration creates a tight coupling between
    the application and the Kubernetes platform. This coupling raises some interesting
    questions. What happens if the API server goes down? Will your application experience
    downtime when your platform team upgrades the API server?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然利用Kubernetes API进行应用程序配置可能很方便，但我们发现有一些重要的缺点需要考虑。首先，连接到API服务器以获取配置会在应用程序和Kubernetes平台之间创建紧密耦合关系。这种耦合会引发一些有趣的问题。如果API服务器宕机会发生什么？当平台团队升级API服务器时，您的应用程序会经历停机时间吗？
- en: Second, for the application to get its configuration from the API, it needs
    credentials, and it needs to have the right permissions. These requirements increase
    your deployment complexity, as you now have to provide a Service Account and define
    RBAC roles for your workload.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，应用程序要从API获取其配置时，需要凭据，并且需要具有适当的权限。这些要求增加了部署的复杂性，因为现在您必须提供一个服务账户，并为您的工作负载定义RBAC角色。
- en: Last, the more applications using this method to get config, the more undue
    load is imposed on the API server. Since the API server is a critical component
    of the cluster’s control plane, this approach to app configuration can be at odds
    with the overall scalability of the cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，越多应用程序使用此方法获取配置，API服务器上的负载就越大。由于API服务器是集群控制平面的关键组件，这种应用程序配置方法可能与集群的整体可扩展性相冲突。
- en: Overall, when it comes to consuming ConfigMaps and Secrets, we prefer using
    volume mounts and environment variables over integrating with the Kubernetes API
    directly. In this way, the applications remain decoupled from the underlying platform.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，当涉及到消费ConfigMaps和Secrets时，我们更喜欢使用卷挂载和环境变量，而不是直接集成Kubernetes API。这样一来，应用程序与底层平台保持解耦。
- en: Obtaining Configuration from External Systems
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从外部系统获取配置
- en: ConfigMaps and Secrets can be convenient when it comes to configuring applications.
    They are built into the Kubernetes API and are readily available for you to consume.
    With that said, configuration and secrets have been a concern that application
    developers had faced well before Kubernetes existed. While Kubernetes provides
    features to solve this concern, nothing is stopping you from using external systems
    instead.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及配置应用程序时，ConfigMaps和Secrets非常方便。它们内置于Kubernetes API中，可以随时供您使用。话虽如此，配置和密钥在Kubernetes出现之前就一直是应用程序开发者面临的问题。虽然Kubernetes提供了解决这些问题的功能，但并没有阻止您使用外部系统。
- en: One of the most prevalent examples of an external configuration or secrets management
    system we run into in the field is [HashiCorp Vault](https://www.vaultproject.io).
    Vault provides advanced secret management functionality that is unavailable in
    Kubernetes Secrets. For example, Vault provides dynamic secrets, secret rotation,
    time-based tokens, and more. If your application is already leveraging Vault,
    you can continue to do so when running your application on Kubernetes. Even if
    not yet using Vault, it is worth evaluating as a more robust alternative to Kubernetes
    Secrets. We discussed secret management considerations and the Vault integration
    with Kubernetes extensively in [Chapter 7](ch07.html#chapter7). If you want to
    learn more about secret management in Kubernetes and the lower-level details of
    the Vault integration, we recommend you check out that chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在现场遇到的外部配置或密钥管理系统中，最普遍的一个例子是[HashiCorp Vault](https://www.vaultproject.io)。Vault提供了在Kubernetes
    Secrets中无法获得的高级密钥管理功能。例如，Vault提供动态密钥、密钥轮换、基于时间的令牌等功能。如果您的应用程序已经利用了Vault，那么在Kubernetes上运行应用程序时可以继续使用。即使尚未使用Vault，评估其作为Kubernetes
    Secrets更强大的替代方案也是值得的。我们在[第7章](ch07.html#chapter7)中广泛讨论了密钥管理考虑因素和Vault与Kubernetes的集成。如果您想了解更多关于Kubernetes中的密钥管理以及Vault集成的低级细节，请查阅该章节。
- en: When leveraging an external system for configuration or secrets, we find that
    offloading the integration (as much as possible) to the platform is beneficial.
    Integrations with external systems such as Vault can be offered as a platform
    service to expose Secrets as volumes or environment variables in Pods. The platform
    service abstracts the external system and enables your application to consume
    the Secret without worrying about the implementation details of the integration.
    Overall, leveraging such a platform service reduces the application’s complexity
    and results in standardization across your applications.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在利用外部系统进行配置或密钥时，我们发现尽可能将集成任务转移到平台上是有益的。与Vault等外部系统的集成可以作为平台服务提供，以将Secrets作为Pods中的卷或环境变量暴露出来。平台服务抽象了外部系统，并使您的应用程序能够消费Secret，而无需担心集成的实现细节。总体而言，利用这样的平台服务可以降低应用程序的复杂性，并实现应用程序间的标准化。
- en: Handling Rescheduling Events
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理重新调度事件
- en: Kubernetes is a highly dynamic environment where workloads are moved around
    for different reasons. Cluster nodes can come and go; they can run out of resources
    or even fail. Platform teams can drain, cordon, or remove nodes to perform cluster
    life cycle operations (e.g., upgrades). These are examples of situations in which
    your workload might be killed and rescheduled, and there are many others.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个高度动态的环境，在这里工作负载会因各种原因而被移动。集群节点可能会来来去去；它们可能会耗尽资源，甚至失败。平台团队可以排空、隔离或删除节点以执行集群生命周期操作（例如升级）。这些都是可能导致您的工作负载被终止并重新调度的情况的示例，还有许多其他情况。
- en: Regardless of the reason, the dynamic nature of Kubernetes can impact your application’s
    availability and operation. Even though the application’s architecture has the
    highest bearing in determining the impact of disturbances, there are features
    in Kubernetes you can leverage to minimize that impact. We will explore these
    features in this section. First, we will dig into pre-stop container life cycle
    hooks. As indicated by the name, these hooks enable you to act before Kubernetes
    stops your containers. We will then discuss how you can shut down containers gracefully,
    which involves handling signals from within the application in response to shutdown
    events. Finally, we will review Pod anti-affinity rules, a mechanism you can use
    to spread your application across failure domains. As mentioned before, these
    mechanisms can help *minimize* the impact of disturbances but cannot eliminate
    the potential for failure. Keep that in mind as you read through this section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不论原因如何，Kubernetes的动态特性都可能影响您应用的可用性和操作。尽管应用架构对干扰影响最大，但Kubernetes中的功能可以帮助您最小化这种影响。本节将探讨这些功能。首先，我们将深入了解预停止容器生命周期钩子。如其名称所示，这些钩子允许您在Kubernetes停止容器之前采取行动。然后，我们将讨论如何优雅地关闭容器，包括在应用程序内响应关闭事件的信号处理。最后，我们将审查Pod反亲和规则，这是一个可以用来跨故障域分布应用程序的机制。正如前面提到的，这些机制可以帮助*最小化*干扰的影响，但不能消除故障的潜在可能性。阅读本节时请牢记这一点。
- en: Pre-stop Container Life Cycle Hook
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预停止容器生命周期钩子
- en: Kubernetes can terminate workloads for any number of reasons. If you need to
    perform an action before your container is terminated, you can leverage the pre-stop
    container life cycle hook. Kubernetes provides two types of hooks. The `exec`
    life cycle hook runs a command within the container, while the `HTTP` life cycle
    hook issues an HTTP request against an endpoint you specify (typically the container
    itself). Which hook to use depends on your specific requirements and what you
    are trying to achieve.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以出于任何原因终止工作负载。如果您需要在终止容器之前执行操作，可以利用预停止容器生命周期钩子。Kubernetes提供两种类型的钩子。`exec`生命周期钩子在容器内部运行命令，而`HTTP`生命周期钩子则针对您指定的端点（通常是容器本身）发出HTTP请求。使用哪种钩子取决于您的具体要求和您试图实现的目标。
- en: 'The pre-stop hook in the [Contour](https://projectcontour.io) Ingress controller
    is a great example that showcases the power of pre-stop hooks. To avoid dropping
    in-flight client requests, Contour includes a container pre-stop hook that tells
    Kubernetes to execute a command before stopping the container. The following snippet
    from the Contour Deployment YAML file shows the pre-stop hook configuration:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[Contour](https://projectcontour.io)中的预停止钩子是展示预停止钩子强大功能的一个绝佳示例。为了避免中断进行中的客户端请求，Contour包含一个容器预停止钩子，告诉Kubernetes在停止容器之前执行一个命令。以下是来自Contour
    Deployment YAML文件的预停止钩子配置片段：'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Container pre-stop hooks enable you to take action before Kubernetes stops your
    container. They allow you to run commands or scripts that exist within the container
    but are otherwise not part of the running process. One key consideration to keep
    in mind is that these hooks are executed only in the face of planned life cycle
    or re-scheduling events. The hooks will not run if a node fails, for example.
    Furthermore, any action performed as part of the pre-stop hook is governed by
    the Pod’s graceful shutdown period, which we will discuss next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 容器预停止钩子使您能够在Kubernetes停止容器之前采取操作。它们允许您运行容器内存在但不是运行进程的命令或脚本。需要牢记的一个关键考虑因素是这些钩子仅在计划的生命周期或重新调度事件面前执行。例如，如果节点失败，这些钩子将不会运行。此外，作为预停止钩子的一部分执行的任何操作受Pod的优雅关闭期限的控制，我们将在接下来讨论。
- en: Graceful Container Shutdown
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优雅的容器关闭
- en: After executing pre-stop hooks (when provided), Kubernetes initiates the container
    shutdown process by sending a SIGTERM signal to the workload. This signal lets
    the container know that it is being stopped. It also starts running down the clock
    of the termination shutdown period, which is 30 seconds by default. You can tune
    this period using the `terminationGracePeriodSeconds` field of the Pod specification.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行预停止钩子（如果提供）后，Kubernetes通过向工作负载发送SIGTERM信号启动容器关闭过程。此信号告知容器它即将被停止。同时，它开始计时终止关闭期限，默认为30秒。您可以使用Pod规范的`terminationGracePeriodSeconds`字段来调整此期限。
- en: During the graceful termination period, the application can complete any necessary
    actions before shutting down. Depending on the application, these actions can
    be persisting data, closing open connections, flushing files to disk, etc. Once
    done, the application should exit with a successful exit code. The graceful termination
    is illustrated in [Figure 14-1](#application_termination_in_kubernetes), where
    we can see the kubelet sending the SIGTERM signal and waiting for the container(s)
    to terminate within the grace period.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在优雅终止期间，应用程序可以在关闭之前完成任何必要的操作。根据应用程序的不同，这些操作可以包括持久化数据、关闭开放连接、将文件刷新到磁盘等。完成这些操作后，应用程序应以成功的退出代码退出。优雅终止的示例在[图 14-1](#application_termination_in_kubernetes)中有所说明，我们可以看到
    kubelet 发送 SIGTERM 信号并等待容器在优雅终止期间内终止。
- en: If the application shuts down within the termination period, Kubernetes completes
    the shutdown process and moves on. Otherwise, it forcefully stops the process
    by sending a SIGKILL signal. [Figure 14-1](#application_termination_in_kubernetes)
    also shows this forceful termination toward the bottom right of the diagram.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序在终止期间内关闭，Kubernetes 将完成关闭流程并继续。否则，它将通过发送 SIGKILL 信号强制停止进程。[图 14-1](#application_termination_in_kubernetes)还显示了这种强制终止在图表右下角。
- en: '![prku 1401](assets/prku_1401.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![prku 1401](assets/prku_1401.png)'
- en: Figure 14-1\. Application termination in Kubernetes. The kubelet first sends
    a SIGTERM signal to the workload and waits up to the configured graceful termination
    period. If the process is still running after the period expires, the kubelet
    sends a SIGKILL to terminate the process.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-1\. Kubernetes 中的应用程序终止。kubelet 首先向工作负载发送 SIGTERM 信号，并等待配置的优雅终止期间。如果进程在期间结束后仍在运行，kubelet
    将发送 SIGKILL 信号终止进程。
- en: 'For your application to terminate gracefully, it must handle the SIGTERM signal.
    Each programming language or framework has its own way of configuring signal handlers.
    Some application frameworks might even take care of it for you. The following
    snippet shows a Go application that configures a SIGTERM signal handler, which
    stops the application’s HTTP server upon receipt of the signal:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要使您的应用程序能够优雅地终止，它必须处理 SIGTERM 信号。每种编程语言或框架都有其配置信号处理程序的方法。有些应用程序框架甚至可能会为您处理这些事务。以下代码片段展示了一个配置
    SIGTERM 信号处理程序的 Go 应用程序，在接收到信号后停止应用程序的 HTTP 服务器：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When running your applications on Kubernetes, we recommend you configure signal
    handlers for the SIGTERM signal. Even if there are no shutdown actions to take,
    handling the signal makes your workload a better Kubernetes citizen, as it reduces
    the time it takes to stop the application and thus free up the resources for other
    workloads.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上运行应用程序时，我们建议您为 SIGTERM 信号配置信号处理程序。即使没有关闭操作需要执行，处理信号也会使您的工作负载成为更好的
    Kubernetes 元素，因为它减少了停止应用程序所需的时间，从而为其他工作负载释放资源。
- en: Satisfying Availability Requirements
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 满足可用性要求
- en: Container pre-stop hooks and graceful termination are concerned with a single
    instance or replica of your application. If your application is horizontally scalable,
    you will most likely have multiple replicas running in the cluster to satisfy
    availability requirements. Running more than one instance of your workload can
    provide increased fault tolerance. For example, if a cluster node fails and takes
    one of the application instances with it, the other replicas can pick up the work.
    With that said, having multiple replicas does not help if they are running in
    the same failure domain.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 容器预停钩和优雅终止与应用程序的单个实例或副本相关。如果您的应用程序具有水平扩展性，那么您很可能在集群中运行多个副本以满足可用性要求。运行多个工作负载实例可以提高容错能力。例如，如果群集节点故障并将其中一个应用程序实例带走，其他副本可以继续工作。但是，请注意，如果这些副本运行在相同的故障域中，则拥有多个副本并不能提高可用性。
- en: 'One way to ensure your Pods are spread across failure domains is by using Pod
    anti-affinity rules. With Pod anti-affinity rules, you tell the Kubernetes scheduler
    that you want to schedule your Pods according to constraints you define in the
    Pod definition. More specifically, you ask the scheduler to avoid placing your
    Pod on nodes that are already running a replica of your workload. Consider a web
    server that has three replicas. To ensure the three replicas are not placed in
    the same failure domain, you can use Pod anti-affinity as in the following snippet.
    In this case, the anti-affinity rule tells the scheduler that it should prefer
    placing Pods across zones, as determined by the `zone` label on cluster nodes:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的 Pods 分布在不同故障域的一种方法是使用 Pod 反亲和性规则。通过 Pod 反亲和性规则，您告诉 Kubernetes 调度器，您希望根据
    Pod 定义中定义的约束条件来调度您的 Pods。更具体地说，您要求调度器避免将 Pod 放置在已经运行负载副本的节点上。考虑一个具有三个副本的 Web 服务器。为了确保这三个副本不会放置在同一故障域中，您可以像下面的片段中使用
    Pod 反亲和性规则一样。在这种情况下，反亲和性规则告诉调度器，它应优先将 Pods 放置在按照集群节点上的 `zone` 标签确定的区域内：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In addition to Pod anti-affinity, Kubernetes provides Pod Topology Spread Constraints,
    which are an improvement to Pod anti-affinity rules when it comes to spreading
    Pods across failure domains. The problem with anti-affinity rules is that there
    is no way to guarantee Pods are spread *evenly* across the domains. You can either
    “prefer” scheduling them based on the topology key, or you can guarantee a single
    replica per failure domain.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Pod 反亲和性之外，Kubernetes 还提供了 Pod 拓扑扩展约束，这是在将 Pods 分布在不同故障域方面对 Pod 反亲和性规则的改进。反亲和性规则的问题在于无法保证
    Pods 在域内均匀分布。您可以“偏好”根据拓扑键调度它们，或者可以保证每个故障域内只有一个副本。
- en: The Pod Topology Spread Constraints provide a way for you to tell the scheduler
    to spread your workload. Similar to Pod anti-affinity rules, they are only evaluated
    against new Pods that need scheduling, and thus they are not retroactively enforced.
    The following snippet shows an example Pod Topology Spread Constraint that results
    in Pods spread across zones (based on the `zone` label of nodes). If the constraint
    cannot be satisfied, Pods will not be scheduled.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 拓扑扩展约束提供了一种方式告诉调度器如何分布您的工作负载。与 Pod 反亲和性规则类似，它们仅对需要调度的新 Pods 进行评估，因此不会对已存在的
    Pods 进行反向强制执行。以下代码片段展示了一个 Pod 拓扑扩展约束的示例，该约束导致 Pods 在不同区域（基于节点的 `zone` 标签）中分布。如果无法满足约束条件，则不会调度
    Pods。
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When running multiple instances of an application, you should leverage these
    Pod placement features to improve the application’s tolerance of infrastructure
    failure. Otherwise, you risk Kubernetes scheduling your workload in a way that
    does not achieve the failure tolerance you are looking for.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行多个应用程序实例时，应利用这些 Pod 放置特性来提高应用程序对基础设施故障的容忍度。否则，您面临的风险是 Kubernetes 可能以未能实现所需故障容忍度的方式调度您的工作负载。
- en: State Probes
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态探针
- en: Kubernetes uses many signals to determine the state and health of applications
    running on the platform. When it comes to health, Kubernetes treats workloads
    as opaque boxes. It knows whether the process is up or not. While this information
    is helpful, it is typically not enough to run and manage applications effectively.
    This is where probes come in. Probes provide Kubernetes with increased visibility
    of the application’s condition.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用许多信号来确定平台上运行的应用程序的状态和健康状况。在健康方面，Kubernetes 将工作负载视为不透明的盒子。它知道进程是否运行。尽管这些信息很有帮助，但通常不足以有效地运行和管理应用程序。这就是探针的作用所在。探针为
    Kubernetes 提供了增加应用程序状态可见性的功能。
- en: 'Kubernetes provides three probe types: liveness, readiness, and startup probes.
    Before discussing each type in detail, let’s review the different probing mechanisms
    that are common to all probe types:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供三种探针类型：活跃探针（liveness）、就绪探针（readiness）和启动探针（startup）。在详细讨论每种类型之前，让我们回顾一下所有探针类型共有的不同探测机制：
- en: Exec
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Exec
- en: The kubelet executes a command inside the container. The probe is deemed successful
    if the command returns a zero exit code. Otherwise, the kubelet considers the
    container unhealthy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet 在容器内部执行命令。如果命令返回零退出代码，则探针被视为成功。否则，kubelet 将容器视为不健康。
- en: HTTP
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP
- en: The kubelet sends an HTTP request to an endpoint in the Pod. As long as the
    HTTP response code is greater than or equal to 200 and less than 400, the probe
    is deemed successful.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet 向 Pod 中的端点发送 HTTP 请求。只要 HTTP 响应代码大于或等于 200 且小于 400，探针就被视为成功。
- en: TCP
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: TCP
- en: The kubelet establishes a TCP connection with the container on a configurable
    port. The container is deemed healthy if the connection is established successfully.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet 在可配置端口与容器建立 TCP 连接。如果连接成功建立，则容器被视为健康。
- en: In addition to sharing the probing mechanisms, all probes have a common set
    of parameters that you can use to tune the probe according to your workload. These
    parameters include success and failure thresholds, timeout periods, and others.
    The Kubernetes documentation describes each setting in detail, so we will not
    dive into them here.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了共享探测机制外，所有探测都有一组通用的参数，您可以根据工作负载调整探测。这些参数包括成功和失败的阈值、超时时间等。Kubernetes 文档详细描述了每个设置，因此我们在这里不会深入讨论它们。
- en: Liveness Probes
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活跃探测
- en: Liveness probes help Kubernetes understand the health of Pods on the cluster.
    At the node level, the kubelet continuously probes Pods that have the liveness
    probe configured. When the liveness probe exceeds the failure threshold, the kubelet
    deems the Pod unhealthy and restarts it. [Figure 14-2](#flowchart_that_shows_an_http_based_liveness_probe)
    shows a flowchart that depicts an HTTP liveness probe. The kubelet probes the
    container every 10 seconds. If the kubelet finds that the last 10 probes have
    failed, it restarts the container.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃探测帮助 Kubernetes 了解集群中 Pod 的健康状况。在节点级别，kubelet 会持续探测配置了活跃探测的 Pod。当活跃探测超过失败阈值时，kubelet
    将认为 Pod 不健康，并重新启动它。[图 14-2](#flowchart_that_shows_an_http_based_liveness_probe)
    显示了一个展示 HTTP 活跃探测的流程图。kubelet 每 10 秒探测一次容器。如果 kubelet 发现最近的 10 次探测失败，它将重新启动容器。
- en: '![prku 1402](assets/prku_1402.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![prku 1402](assets/prku_1402.png)'
- en: Figure 14-2\. Flowchart that shows an HTTP-based liveness probe with a period
    of 10 seconds. If the probe fails 10 times consecutively, the Pod is deemed unhealthy
    and the kubelet restarts it.
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-2\. 显示基于 HTTP 的活跃探测的流程图，间隔为 10 秒。如果连续失败 10 次，则认为 Pod 不健康，kubelet 会重新启动它。
- en: Tip
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Given that liveness probe failures result in container restarts, we typically
    suggest that liveness probe implementations should not check for the workload’s
    external dependencies. By keeping the liveness probe local to your workload and
    not checking external dependencies, you prevent cascading failures that could
    otherwise occur. For example, a service that interacts with a database should
    not perform a “database availability” check as part of its liveness probe, as
    restarting the workload will most likely not fix the problem. If the app detects
    an issue with the database, the app can enter a read-only mode or gracefully disable
    the functionality that depends on the database. Another option is for the app
    to fail its readiness probe, which we discuss next.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于活跃探测失败会导致容器重新启动，我们通常建议活跃探测实现不要检查工作负载的外部依赖项。通过将活跃探测限制在您的工作负载范围内，并且不检查外部依赖项，您可以防止可能发生的级联故障。例如，与数据库交互的服务不应将“数据库可用性”检查作为其活跃探测的一部分，因为重新启动工作负载通常不会解决问题。如果应用程序检测到与数据库的问题，应用程序可以进入只读模式或优雅地禁用依赖于数据库的功能。另一种选择是使应用程序在其就绪探测中失败，我们将在下面讨论。
- en: Readiness Probes
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 就绪探测
- en: The readiness probe is perhaps the most common and most important probe type
    in Kubernetes, especially for services that handle requests. Kubernetes uses readiness
    probes to control whether to route Service traffic to Pods. Thus, readiness probes
    provide a mechanism for the application to tell the platform that they’re ready
    to accept requests.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探测可能是 Kubernetes 中最常见和最重要的探测类型，特别是用于处理请求的服务。Kubernetes 使用就绪探测来控制是否将服务流量路由到
    Pod。因此，就绪探测为应用程序提供了一种机制，告知平台它们已准备好接受请求。
- en: As with liveness probes, the kubelet is responsible for probing the application
    and updating the Pod’s status according to the probe results. When the probe fails,
    the platform removes the failing Pod from the list of available endpoints, effectively
    diverting traffic to other replicas that are ready. [Figure 14-3](#flowchart_that_shows_an_http_based_readiness)
    shows a flowchart that explains an HTTP-based readiness probe. The probe has a
    5-second initial delay and a probing period of 10 seconds. On startup, the application
    begins receiving traffic only when the readiness probe succeeds. Then, the platform
    stops sending traffic to the Pod if the probe fails twice consecutively.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与生存探测一样，kubelet 负责对应用程序进行探测，并根据探测结果更新 Pod 的状态。当探测失败时，平台将失败的 Pod 从可用端点列表中移除，有效地将流量重定向到其他准备好的副本。[图
    14-3](#flowchart_that_shows_an_http_based_readiness) 显示了一个解释基于 HTTP 的可用性探测的流程图。探测具有
    5 秒的初始延迟和 10 秒的探测周期。在启动时，应用程序只有在可用性探测成功后才开始接收流量。然后，如果连续两次探测失败，平台将停止向 Pod 发送流量。
- en: When deploying service-type workloads, make sure that you configure a readiness
    probe to avoid sending requests to replicas that cannot handle them. The readiness
    probe is not only critical when the Pod is starting up but also important during
    the lifetime of the Pod to prevent routing clients to replicas that have become
    unready.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署服务型工作负载时，请确保配置一个可用性探测，以避免将请求发送到无法处理的副本上。可用性探测不仅在 Pod 启动时至关重要，在 Pod 的生命周期中也很重要，以防止将客户端路由到已变得不可用的副本上。
- en: '![prku 1403](assets/prku_1403.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![prku 1403](assets/prku_1403.png)'
- en: Figure 14-3\. Flowchart that shows an HTTP-based readiness probe with a period
    of 10 seconds. If the probe fails twice consecutively, the Pod is deemed not ready
    and it is taken out of the set of ready endpoints.
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-3\. 显示基于 HTTP 的可用性探测流程图，间隔为 10 秒。如果连续两次探测失败，Pod 被视为未准备好，并从准备好的端点集中移除。
- en: Startup Probes
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动探测
- en: Liveness and readiness probes have been available since the first version of
    Kubernetes. As the system gained popularity, the community identified a need to
    implement an additional probe, the startup probe. The startup probe provides extra
    time for slow-starting applications to initialize. Similar to liveness probes,
    failed startup probes result in the container being restarted. Unlike liveness
    probes, however, startup probes are executed only until they succeed, at which
    point the liveness and readiness probes take over.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Kubernetes 的第一个版本以来，生存探测和可用性探测一直可用。随着系统的普及，社区认识到需要实现一种额外的探测方法，即启动探测。启动探测为启动缓慢的应用程序提供了额外的初始化时间。与生存探测类似，启动探测失败会导致容器重新启动。然而，与生存探测不同的是，启动探测仅在成功之前执行，此后由生存探测和可用性探测接管。
- en: If you are wondering why a liveness probe is not enough, let’s consider an application
    that takes, on average, 300 seconds to initialize. You could indeed use a liveness
    probe that waits 300 seconds before stopping the container. During startup, this
    liveness probe would work. But what about later on when the application is running?
    If the application entered an unhealthy state, the platform would wait 300 seconds
    before restarting it! This is the problem that the startup probe solves. It looks
    after the workload during startup but then gets out of the way. [Figure 14-4](#flowchart_that_shows_an_http_based_startup_probe_with_a_period_of_10_seconds)
    shows a flowchart that walks through a startup probe like the one we just discussed.
    It has a failure threshold of 30 times and a probing period of 10 seconds.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道为什么仅仅有一个生存探测还不够，让我们考虑一个平均需要 300 秒初始化时间的应用程序。你确实可以使用一个等待 300 秒的生存探测来停止容器。在启动期间，这个生存探测可以工作。但是当应用程序运行后呢？如果应用程序进入不健康状态，平台将等待
    300 秒后再重启它！这就是启动探测解决的问题。它在启动期间看管工作负载，然后自动退出。[图 14-4](#flowchart_that_shows_an_http_based_startup_probe_with_a_period_of_10_seconds)
    显示了一个流程图，详细说明了一个类似我们刚讨论的启动探测。它的失败阈值是 30 次，探测周期是 10 秒。
- en: '![prku 1404](assets/prku_1404.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![prku 1404](assets/prku_1404.png)'
- en: Figure 14-4\. Flowchart that shows an HTTP-based startup probe with a period
    of 10 seconds. If the probe returns a successful response, the startup probe is
    disabled and the liveness/readiness probes are enabled. Otherwise, if the probe
    fails 30 times consecutively, the kubelet restarts the Pod.
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-4\. 显示基于 HTTP 的启动探测流程图，探测周期为 10 秒。如果探测返回成功响应，则禁用启动探测并启用生存/可用性探测。否则，如果连续失败
    30 次，则 kubelet 重新启动 Pod。
- en: While startup probes can be useful for certain applications, we usually recommend
    avoiding them unless absolutely necessary. We find liveness and readiness probes
    to be appropriate most of the time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管启动探针对某些应用程序可能有用，但我们通常建议除非绝对必要，否则应避免使用它们。我们发现，大多数情况下使用活跃性和就绪性探针是合适的。
- en: Implementing Probes
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施探针
- en: Now that we have covered the different probe types, let’s dive into how you
    should approach them in your application, specifically liveness and readiness
    probes. We know that failed liveness probes result in the platform restarting
    the Pod, while failed readiness probes prevent traffic from being routed to the
    Pod. Given these different outcomes, we find that most applications that leverage
    both liveness and readiness probes should configure different probe endpoints
    or commands.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了不同的探针类型，让我们深入探讨在您的应用程序中如何处理它们，特别是活跃性探针和就绪性探针。我们知道，失败的活跃性探针会导致平台重新启动
    Pod，而失败的就绪性探针会阻止流量路由到 Pod。鉴于这些不同的结果，我们发现大多数同时使用活跃性和就绪性探针的应用程序应该配置不同的探针端点或命令。
- en: Ideally, the liveness probe fails only when there is a problem that requires
    a restart, such as a deadlock or some other condition that permanently prevents
    the app from making progress. Applications that expose an HTTP server commonly
    implement a liveness endpoint that unconditionally returns a 200 status code.
    As long as the HTTP server is healthy and the app can respond, there’s no need
    to restart it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，只有在存在需要重新启动的问题时，例如死锁或其他永久阻止应用程序进展的条件时，活跃性探针才会失败。通常，暴露 HTTP 服务器的应用程序会实现一个无条件返回
    200 状态代码的活跃性端点。只要 HTTP 服务器健康且应用程序能够响应，就无需重新启动它。
- en: In contrast to the liveness endpoint, the readiness endpoint can check for different
    conditions within the application. For example, if the application warms internal
    caches on startup, the readiness endpoint can return false unless the caches are
    warm. Another example is service overload, a condition under which the app can
    fail the readiness probe as a mechanism to shed load. As you can probably imagine,
    the checked conditions vary from one application to the next. In general, however,
    they are temporary conditions that resolve with the passing of time.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与活跃性端点相比，就绪性端点可以检查应用程序内部的不同条件。例如，如果应用程序在启动时预热内部缓存，则在缓存未预热时就绪性端点可以返回 false。另一个例子是服务过载，这种情况下应用程序可以通过失败就绪性探针来卸载负载。可以想象，检查的条件因应用程序而异。但通常，它们都是随时间推移解决的临时条件。
- en: To summarize, we typically recommend using readiness probes for workloads that
    handle requests, given that readiness probes are meaningless in other application
    types, such as controllers, jobs, etc. When it comes to liveness probes, we recommend
    considering them only when restarting the application would help fix the problem.
    Lastly, we tend to avoid startup probes unless absolutely necessary.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们通常建议对处理请求的工作负载使用就绪性探针，因为在其他应用程序类型（如控制器、作业等）中，就绪性探针是没有意义的。至于活跃性探针，我们建议仅在重新启动应用程序有助于解决问题时考虑使用。最后，我们倾向于避免启动探针，除非绝对必要。
- en: Pod Resource Requests and Limits
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 资源请求与限制
- en: One of Kubernetes’ primary functions is to schedule applications across cluster
    nodes. The scheduling process involves, among other things, finding candidate
    nodes that have enough resources to host the workload. To place workloads effectively,
    the Kubernetes scheduler first needs to know the resource needs of your application.
    Typically, these resources encompass CPU and memory, but can also include other
    resource types such as ephemeral storage and even custom or extended resources.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的主要功能之一是在集群节点上调度应用程序。调度过程包括寻找能够托管工作负载的候选节点，除了其他因素外，还需要找到足够的资源来容纳工作负载。为了有效地放置工作负载，Kubernetes
    调度器首先需要知道您的应用程序的资源需求。通常，这些资源包括 CPU 和内存，但也可以包括其他资源类型，如临时存储甚至自定义或扩展资源。
- en: In addition to scheduling your applications, Kubernetes also needs resource
    information to guarantee those resources at runtime. After all, the platform has
    limited resources that are shared across applications. Providing resource requirements
    is critical to your application’s ability to *use* those resources.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调度应用程序外，Kubernetes 还需要在运行时提供资源信息，以确保这些资源。毕竟，平台上有限的资源需要在应用程序之间共享。提供资源需求对于您的应用程序能够*使用*这些资源至关重要。
- en: In this section, we will discuss resource requests and resource limits, and
    how they can impact your application. We will not dig into the details of how
    the platform implements resource requests and limits, as we have already discussed
    it in [Chapter 12](ch12.html#multi_tenancy_chapter).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论资源请求和资源限制，以及它们如何影响您的应用程序。我们不会深入讨论平台如何实现资源请求和限制的详细信息，因为我们已经在[第12章](ch12.html#multi_tenancy_chapter)中讨论过。
- en: Resource Requests
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源请求
- en: Resource requests specify the minimum amount of resources your application needs
    to run. In most cases, you should specify resource requests when deploying applications
    to Kubernetes. By doing so, you ensure your workload will have access to the requested
    resources at runtime. If you don’t specify resource requests, you might find your
    application’s performance diminishes significantly when resources on the node
    come under contention. You even risk the possibility of your application being
    terminated if the node needs to reclaim memory for other workloads. [Figure 14-5](#pod_1_and_pod_2_share_the_nodes_memory)
    shows the termination of an application because another workload with memory requests
    starts consuming additional memory.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 资源请求指定应用程序运行所需的最小资源量。在大多数情况下，部署应用程序到Kubernetes时，您应该指定资源请求。通过这样做，您可以确保您的工作负载在运行时能够访问所请求的资源。如果您不指定资源请求，当节点上的资源竞争时，您可能会发现应用程序的性能显著下降。甚至可能会因为节点需要回收内存以供其他工作负载使用而终止您的应用程序。[图14-5](#pod_1_and_pod_2_share_the_nodes_memory)显示了一个应用程序被终止的情况，因为另一个带有内存请求的工作负载开始消耗额外内存。
- en: '![prku 1405](assets/prku_1405.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![prku 1405](assets/prku_1405.png)'
- en: Figure 14-5\. Pod 1 and Pod 2 share the node’s memory. Each Pod is initially
    consuming 200 MiB out of the total 500 MiB. Pod 2 is terminated when Pod 1 needs
    to consume additional memory, as Pod 2 does not have memory requests in its specification.
    Pod 2 enters a crash loop as it cannot allocate enough memory to start up.
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-5\. Pod 1和Pod 2共享节点的内存。每个Pod最初消耗了500 MiB总内存中的200 MiB。当Pod 1需要消耗额外内存时，Pod
    2因其规格中没有内存请求而被终止。Pod 2进入崩溃循环，因为它无法分配足够内存启动。
- en: One of the main challenges with resource requests is finding the right numbers
    to use. If you are deploying an existing application, you might already have data
    you can analyze to determine the app’s resource requests, such as the application’s
    actual utilization over time or perhaps the size of the VMs hosting it. When you
    don’t have historical data, you will have to use an educated guess and gather
    data over time. Another option is to use the Vertical Pod Autoscaler (VPA), which
    can suggest values for CPU and memory requests and even adjust those values over
    time. For more information about the VPA, see [Chapter 13](ch13.html#autoscaling_chapter).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 资源请求的主要挑战之一是找到合适的数字。如果您正在部署现有应用程序，则可能已经有数据可以分析以确定应用程序的资源请求，例如应用程序随时间的实际利用率或托管其的VM大小。当您没有历史数据时，您将不得不使用合理的猜测，并随时间收集数据。另一个选项是使用垂直Pod自动缩放器（VPA），它可以建议CPU和内存请求的值，并随时间调整这些值。有关VPA的更多信息，请参见[第13章](ch13.html#autoscaling_chapter)。
- en: Resource Limits
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源限制
- en: Resource limits allow you to specify the maximum amount of resources your workload
    can consume. You might be wondering why you would impose an artificial limit.
    After all, the more resources available, the better. While this is true for some
    workloads, having unbound access to resources can result in unpredictable performance,
    as the Pod will have access to extra resources when available but will not when
    other Pods need the resources on the node. It gets even worse with memory. Given
    that memory is an incompressible resource, the platform has no other choice but
    to kill Pods when it needs to reclaim memory that was opportunistically consumed.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 资源限制允许您指定工作负载可以消耗的最大资源量。您可能想知道为什么要设置人为限制。毕竟，资源越多越好。尽管对于某些工作负载来说是正确的，但无限制地访问资源可能会导致性能不可预测，因为Pod在资源可用时可以访问额外资源，但当其他Pod需要节点上的资源时则不能。内存情况更糟。鉴于内存是不可压缩的资源，平台在需要回收被机会性消耗的内存时别无选择，只能终止Pod。
- en: An important consideration to make when setting resource limits is whether you
    need to propagate those limits to the workload itself. Java applications are a
    good example. If the application uses an older version of Java (JDK version 8u131
    or earlier), you need to propagate the memory limit down to the Java Virtual Machine
    (JVM). Otherwise, the JVM remains unaware of the limit and attempts to consume
    more memory than allowed. In the case of Java, you can configure the memory settings
    of the JVM using the `JAVA_OPTIONS` environment variable. Another option, although
    not always feasible, is to update the version of the JVM, as more recent versions
    gained the ability to detect the memory limits within containers. If you are deploying
    an application that leverages a runtime, consider whether you need to propagate
    the resource limits for the application to understand them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理应用程序日志时，首先要弄清楚的一件事是 *什么* 应该包含在日志中。尽管开发团队通常有他们自己的哲学，但我们发现他们往往在日志记录方面过度。如果记录过多，会导致噪音过多，可能会错过重要信息。相反，如果记录过少，可能会影响有效地排查应用程序故障。像大多数事情一样，在这里需要取得平衡。
- en: Limits are also important if you are trying to run performance tests or benchmarks
    against your workload. As you can imagine, it is likely that each test run will
    execute against Pods scheduled on different nodes at different times. If resource
    limits are not enforced on the workload, the test results can be highly variable
    as the workload under test can burst above its resource requests when the nodes
    have idle resources.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试针对工作负载运行性能测试或基准测试，限制也很重要。可以想象，每次测试运行将在不同时间调度在不同节点上的 Pod 上执行。如果工作负载上未强制执行资源限制，则测试结果可能会因为工作负载超出资源请求而高度变化，特别是当节点有空闲资源时。
- en: Usually, you should set your resource limits equal to your resource requests,
    which ensures your application will always have the same amount of resources,
    no matter what’s happening with other Pods running beside it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，你应该将资源限制设置为与资源请求相等，这样可以确保你的应用程序在旁边运行的其他 Pod 发生变化时仍然拥有相同数量的资源。
- en: Application Logs
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在设置资源限制时需要考虑的一个重要因素是是否需要将这些限制传播到工作负载本身。Java 应用程序是一个很好的例子。如果应用程序使用较旧的 Java 版本（JDK
    版本 8u131 或更早版本），则需要将内存限制传播到 Java 虚拟机（JVM）。否则，JVM 将不知道限制并尝试消耗超出允许的内存。对于 Java 应用程序，可以使用
    `JAVA_OPTIONS` 环境变量配置 JVM 的内存设置。另一个选项，虽然并非总是可行，是更新 JVM 版本，因为较新版本可以在容器中检测到内存限制。如果部署的应用程序利用运行时环境，请考虑是否需要传播资源限制以便应用程序理解它们。
- en: Application logs are critical to troubleshoot and debug applications both during
    development and in production. Applications running on Kubernetes should, as much
    as possible, log to the standard out and standard error streams (STDOUT/STDERR).
    This not only removes complexity in the application but also is the least complex
    solution from the platform’s perspective when it comes to shipping logs to a central
    location. We covered this concern in [Chapter 9](ch09.html#observability_chapter),
    where we also discussed different log processing strategies, systems, and tools.
    In this section, we will touch on some of the considerations to make when thinking
    about application logs. The first thing we’ll talk about is what you should log
    in the first place. Then, we will discuss unstructured versus structured logs.
    Finally, we will touch on improving the usefulness of logs by including contextual
    information in log messages.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序日志对于开发和生产环境中应用程序的故障排除和调试至关重要。运行在 Kubernetes 上的应用程序应尽可能日志记录到标准输出和标准错误流（STDOUT/STDERR）。这不仅简化了应用程序的复杂性，而且从平台的角度来看，也是将日志发送到中央位置时的最简解决方案。我们在
    [第 9 章](ch09.html#observability_chapter) 中讨论了这个问题，我们还讨论了不同的日志处理策略、系统和工具。在本节中，我们将涉及一些在考虑应用程序日志时需要考虑的因素。首先，我们将讨论首先应该记录什么。然后，我们将讨论非结构化与结构化日志。最后，我们将讨论通过在日志消息中包含上下文信息来提高日志的实用性。
- en: What to Log
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时记录日志
- en: One of the first things to figure out when it comes to application logs is *what*
    to include in the logs. While development teams typically have their own philosophy,
    we have found that they tend to go overboard with logs. If you log too much, you
    run the risk of having too much noise and missing out on important information.
    On the flip side, if you log too little, it can become difficult to troubleshoot
    your application effectively. As with most things, there’s a balance to strike
    here.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序日志
- en: While working with application teams, we have found that a good rule of thumb
    that helps determine whether to log something is to ask the question, Is this
    log message actionable? If the answer is yes, this is a good indicator that it
    is worth logging that message. Otherwise, it is an indicator that the log message
    might not be useful.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在与应用团队合作时，我们发现一个有用的经验法则是询问是否可以采取行动来确定是否记录某条消息。如果答案是肯定的，这是一个很好的指示，值得记录该消息。否则，这表明该日志消息可能没有用处。
- en: Unstructured Versus Structured Logs
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非结构化与结构化日志的对比
- en: Application logs can be categorized as either unstructured or structured. Unstructured
    logs are, as the name suggests, strings of text that lack a specific format. They
    are arguably the most prevalent, as there is zero upfront planning that teams
    need to make. While the team might have generic guidelines, developers get to
    log messages in whatever format they like.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序日志可以分为非结构化或结构化。如其名称所示，非结构化日志是缺乏特定格式的文本字符串。它们可以说是最常见的日志类型，因为团队不需要预先进行任何计划。尽管团队可能有一些通用的指导方针，但开发人员可以按照自己喜欢的格式记录消息。
- en: Structured logs, on the other hand, have predetermined fields that must be provided
    when logging events. They are typically formatted as JSON lines or key-value lines
    (e.g., `time="2015-08-09T03:41:12-03:21" msg="hello world!" thread="13" batchId="5"`).
    The main benefit of structured logs is that they are written in a machine-readable
    format, making them easier to query and analyze. With that said, structured logs
    tend to be harder for humans to read, so you must carefully consider this trade-off
    as you implement logging in your application.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化日志与非结构化日志相反，其具有预定义的字段，在记录事件时必须提供这些字段。通常格式为JSON行或键-值行（例如，`time="2015-08-09T03:41:12-03:21"
    msg="hello world!" thread="13" batchId="5"`）。结构化日志的主要优点在于其采用机器可读格式，便于查询和分析。话虽如此，结构化日志往往较难以人类可读的方式呈现，因此在实施应用程序日志记录时必须仔细权衡这一利弊。
- en: Contextual Information in Logs
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志中的上下文信息
- en: The primary purpose of logs is to provide insight into what happened within
    your application at a certain point in time. Perhaps you are troubleshooting a
    production issue in a live application, or maybe you are performing a root cause
    analysis to understand why something happened. To be able to complete such tasks,
    you typically need contextual information in log messages, in addition to what
    happened.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 日志的主要目的是提供关于应用程序在某一时间点发生了什么的见解。也许您正在解决生产环境中的问题，或者您正在执行根本原因分析，以了解为什么会发生某些事情。要完成这些任务，通常需要日志消息中的上下文信息，除了事件本身。
- en: Let’s consider a payment application as an example. When the application request
    serving pipeline encounters an error, in addition to logging the error itself,
    try to include the context surrounding the error as well. For example, if an error
    occurs because the payee was not found, include the payee name or ID, the user
    ID attempting to make the payment, the payment amount, etc. Such contextual information
    will improve your experience troubleshooting issues and will help you prevent
    such problems in the future. Having said that, avoid including sensitive information
    in your logs. You don’t want to leak a user’s password or credit card information.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以支付应用为例。当应用程序请求服务管道遇到错误时，除了记录错误本身外，尝试包括围绕错误的上下文信息。例如，如果出现错误是因为未找到收款人，则包括收款人姓名或ID、尝试进行付款的用户ID、付款金额等。这样的上下文信息将提升您解决问题和预防此类问题的能力。话虽如此，在日志中避免包含敏感信息。您不希望泄露用户的密码或信用卡信息。
- en: Exposing Metrics
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 暴露指标
- en: In addition to logs, metrics provide critical insight about how your application
    is behaving. Once you have application metrics, you can configure alerts to let
    you know when your application needs attention. Furthermore, by aggregating metrics
    over time, you can discover trends, improvements, and regressions as you roll
    out new versions of your software. This section discusses application instrumentation
    and some of the metrics you can capture, including RED (Rate, Errors, Duration),
    USE (Utilization, Saturation, Errors), and app-specific metrics. If you are interested
    in the platform components that enable monitoring and more additional discussions
    on metrics, check out [Chapter 9](ch09.html#observability_chapter).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 除了日志外，指标提供了关于应用程序行为的关键见解。一旦您拥有应用程序指标，您可以配置警报以在应用程序需要关注时通知您。此外，通过随时间聚合指标，您可以发现随着软件新版本的发布而出现的趋势、改进和退化。本节讨论了应用程序仪表化及您可以捕获的一些指标，包括
    RED（速率、错误、持续时间）、USE（利用率、饱和度、错误）和特定于应用程序的指标。如果您对启用监控的平台组件以及有关指标的更多讨论感兴趣，请查阅 [第9章](ch09.html#observability_chapter)。
- en: Instrumenting Applications
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仪表化应用程序
- en: In most cases, the platform can measure and surface metrics about your application’s
    externally visible behavior. Metrics such as CPU usage, memory usage, disk IOPS,
    and others are readily available from the node that is running your application.
    While these metrics are useful, instrumenting your application to expose key metrics
    from within is worthwhile.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，平台可以测量和展示有关应用程序外部可见行为的指标。例如 CPU 使用率、内存使用率、磁盘 IOPS 等指标可以直接从运行应用程序的节点获取。虽然这些指标很有用，但是为了从内部暴露关键指标，对应用程序进行仪表化是值得的。
- en: Prometheus is one of the most popular monitoring systems for Kubernetes-based
    platforms that we run into in the field. We have extensively covered Prometheus
    and its components in [Chapter 9](ch09.html#observability_chapter). In this section,
    we will focus our discussions on instrumenting apps for Prometheus.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 是 Kubernetes 平台上最流行的监控系统之一，我们在现场经常遇到它。我们在 [第9章](ch09.html#observability_chapter)
    中广泛讨论了 Prometheus 及其组件。在本节中，我们将重点讨论为 Prometheus 仪表化应用程序的相关内容。
- en: 'Prometheus pulls metrics from your application using an HTTP request on a configurable
    endpoint (typically `/metrics`). This means that your application must expose
    this endpoint for Prometheus to scrape. More importantly, the endpoint’s response
    must contain Prometheus-formatted metrics. Depending on the type of software you
    want to monitor, there are two approaches you can take to expose metrics:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 通过 HTTP 请求在可配置的端点（通常为 `/metrics`）从您的应用程序中提取指标。这意味着您的应用程序必须暴露此端点以供
    Prometheus 抓取。更重要的是，端点的响应必须包含 Prometheus 格式的指标。根据您想要监视的软件类型，可以采取两种方法来暴露指标：
- en: Native instrumentation
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本地仪表化
- en: This option involves instrumenting your application using the Prometheus client
    libraries so that metrics are exposed from within the application process. This
    is an excellent approach when you have control over the source code of the application.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该选项涉及使用 Prometheus 客户端库对应用程序进行仪表化，以便从应用程序进程内部暴露指标。当您可以控制应用程序源代码时，这是一个很好的方法。
- en: Out-of-process exporter
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 外部导出器
- en: This is an additional process running beside your workload that transforms preexisting
    metrics and exposes them in a Prometheus-compatible format. This approach is best
    suited for off-the-shelf software that you cannot instrument directly and is typically
    implemented using the sidecar container pattern. Examples include the [NGINX Prometheus
    Exporter](https://oreil.ly/g0ZCt) and the [MySQL Server Exporter](https://oreil.ly/SJOka).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个额外的进程，与您的工作负载并行运行，转换预先存在的指标并以兼容 Prometheus 的格式暴露它们。这种方法最适合无法直接进行仪表化的现成软件，并且通常使用
    sidecar 容器模式实现。例如 [NGINX Prometheus Exporter](https://oreil.ly/g0ZCt) 和 [MySQL
    Server Exporter](https://oreil.ly/SJOka) 就是这种情况的例子。
- en: 'The Prometheus instrumentation libraries supports four metric types: Counters,
    Gauges, Histograms, and Summaries. Counters are metrics that can only increase,
    while Gauges are metrics that can go up or down. Histograms and Summaries are
    more advanced metrics than Counters and Gauges. Histograms place observations
    into configurable buckets that you can then use to compute quantiles (e.g., 95th
    percentile) on the Prometheus server. Summaries are similar to Histograms, except
    that they compute quantiles on the client side over a sliding time window. The
    [Prometheus documentation](https://oreil.ly/epvwC) explains the metric types in
    more depth.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus仪表化库支持四种指标类型：计数器（Counters）、仪表（Gauges）、直方图（Histograms）和摘要（Summaries）。计数器是只能增加的指标，而仪表是可以上升或下降的指标。直方图和摘要比计数器和仪表更高级。直方图将观察结果放入可配置的桶中，您可以使用这些桶来计算Prometheus服务器上的分位数（例如95th百分位数）。摘要与直方图类似，但它们在滑动时间窗口内在客户端上计算分位数。[Prometheus文档](https://oreil.ly/epvwC)更深入地解释了这些指标类型。
- en: 'There are three primary things you must do to instrument an application with
    the Prometheus libraries. Let’s work through an example of instrumenting a Go
    service. First, you need to start an HTTP server to expose the metrics for Prometheus
    to scrape. The library provides an HTTP handler that takes care of encoding the
    metrics into the Prometheus format. Adding the handler would look something like
    this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Prometheus库对应用程序进行仪表化有三个主要步骤。让我们通过一个Go服务的仪表化示例来详细了解。首先，您需要启动一个HTTP服务器来暴露Prometheus需要抓取的指标。该库提供了一个HTTP处理程序，负责将指标编码成Prometheus格式。添加处理程序会类似于以下内容：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, you need to create and register metrics. For example, if you wanted to
    expose a Counter metric called `items_handled_total`, you would use code similar
    to the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要创建并注册指标。例如，如果您想暴露一个名为`items_handled_total`的计数器指标，您可以使用类似以下的代码：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, you need to update the metric according to what’s happening in the
    application. Continuing the Counter example, you would use the `Inc()` method
    of the Counter to increment it:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，根据应用程序的实际情况更新指标。继续上面的计数器示例，您将使用计数器的`Inc()`方法来增加它：
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Instrumenting an application using the Prometheus libraries is relatively simple.
    The more complicated task is to determine the metrics that your application should
    expose. In the following sections, we will discuss different methods or philosophies
    you can use as a starting point to select metrics.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Prometheus库来对应用程序进行仪表化相对简单。更复杂的任务是确定应用程序应该公开的指标。在接下来的几节中，我们将讨论可以作为选择指标起点的不同方法或哲学。
- en: USE Method
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: USE方法
- en: The USE method, proposed by [Brendan Gregg](http://www.brendangregg.com/usemethod.html),
    focuses on system resources. When using this method, you capture Utilization,
    Saturation, and Errors (USE) for each of the resources your application uses.
    These resources typically include CPU, memory, disk, etc. They can also include
    resources that exist within the application software, such as queues, thread pools,
    etc.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: USE方法由[Brendan Gregg](http://www.brendangregg.com/usemethod.html)提出，侧重于系统资源。使用此方法时，您可以捕获应用程序使用的每个资源的利用率、饱和度和错误（USE）。这些资源通常包括CPU、内存、磁盘等，也可以包括存在于应用软件内部的资源，如队列、线程池等。
- en: RED Method
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RED方法
- en: In contrast to the USE method, the RED method focuses more on the services themselves
    instead of the underlying resources. Initially proposed by [Tom Wilkie](https://oreil.ly/sW3al),
    the RED method captures the Rate, Errors, and Durations of requests that the service
    handles. The RED method can be better suited for online services, as the metrics
    provide insight into your users’ experience and how they perceive the service
    from their standpoint.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与USE方法相比，RED方法更注重服务本身，而不是底层资源。最初由[Tom Wilkie](https://oreil.ly/sW3al)提出，RED方法捕捉服务处理的请求速率、错误和持续时间。对于在线服务来说，RED方法可能更合适，因为这些指标可以揭示用户的体验以及他们如何从自己的角度看待服务。
- en: The Four Golden Signals
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 四个黄金信号
- en: 'Another philosophy you can adopt is to measure the four golden signals, as
    proposed by Google in [*Site Reliability Engineering*](https://oreil.ly/iv1bJ)
    (O’Reilly). Google suggests you measure four critical signals for every service:
    Latency, Traffic, Errors, and Saturation. You might notice that these are somewhat
    similar to the metrics captured as part of the RED method, with the addition of
    Saturation.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以采用另一种哲学，即测量四个黄金信号，正如谷歌在《*网站可靠性工程*》（O’Reilly）中提出的那样。谷歌建议您为每个服务测量四个关键信号：延迟、流量、错误和饱和度。您可能会注意到，这些信号与RED方法捕获的指标有些相似，但增加了饱和度。
- en: App-Specific Metrics
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用特定指标
- en: The USE method, RED method, and four golden signals capture generic metrics
    that are applicable across most if not all applications. There is an additional
    class of metrics that surface app-specific information. For example, how long
    does it take to add an item to the shopping cart? Or how much time does it take
    to connect a customer with an agent? Typically, these metrics are correlated with
    business key performance indicators (KPIs).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: USE 方法、RED 方法和四个黄金信号捕获了适用于大多数（如果不是所有）应用程序的通用指标。还有一类指标显示了特定于应用程序的信息。例如，将商品添加到购物车需要多长时间？或者客户与代理人连接需要多长时间？通常，这些指标与业务关键绩效指标（KPI）相关联。
- en: Regardless of the method you choose, exporting metrics from your application
    is critical to its success. Once you have access to those metrics, you can build
    dashboards to visualize the behavior of your system, set up alerts to notify your
    on-call teams when something goes wrong, and perform trend analysis to derive
    business intelligence that can advance your organization.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪种方法，从应用程序导出指标对其成功至关重要。一旦您可以访问这些指标，您可以构建仪表板来可视化系统的行为，设置警报以在发生问题时通知负责团队，并执行趋势分析以推导能够推进组织的业务智能。
- en: Instrumenting Services for Distributed Tracing
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为分布式跟踪服务添加仪表
- en: Distributed tracing enables you to analyze applications that are composed of
    multiple services. They provide visibility into the execution flow of a request
    as it traverses the different services that make up the application. As discussed
    in [Chapter 9](ch09.html#observability_chapter), Kubernetes-based platforms can
    offer distributed tracing as a platform service using systems such as [Jaeger](https://www.jaegertracing.io)
    or [Zipkin](https://zipkin.io). However, similar to monitoring and metrics, you
    must instrument services to take advantage of distributed tracing. In this section,
    we will explore how to instrument services using Jaeger and [OpenTracing](https://opentracing.io).
    First, we will discuss how to initialize a tracer. Then, we will dive into how
    to create spans within a service. A *span* is a named, timed operation that is
    the building block of a distributed trace. Finally, we will explore how to propagate
    tracing context from one service to another. We will use Go and the Go libraries
    for examples, but the concepts are applicable to other programming languages.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式跟踪使您能够分析由多个服务组成的应用程序。它们提供了在请求穿越组成应用程序的不同服务时执行流程的可见性。正如在[第9章](ch09.html#observability_chapter)中讨论的那样，基于
    Kubernetes 的平台可以使用诸如[Jaeger](https://www.jaegertracing.io)或[Zipkin](https://zipkin.io)等系统作为平台服务提供分布式跟踪。然而，与监控和指标类似，您必须为服务添加仪表以利用分布式跟踪。在本节中，我们将探讨如何使用Jaeger和[OpenTracing](https://opentracing.io)来为服务添加仪表。首先，我们将讨论如何初始化跟踪器。然后，我们将深入探讨如何在服务中创建跨度。*跨度*是构成分布式跟踪的命名、计时操作的基本单元。最后，我们将探讨如何从一个服务传播跟踪上下文。我们将使用Go语言和Go库作为示例，但这些概念适用于其他编程语言。
- en: Initializing the Tracer
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化跟踪器
- en: Before being able to create spans within the service, you must initialize the
    tracer. Part of the initialization involves configuring the tracer according to
    the environment the application is running. The tracer needs to know the service
    name, the URL to send trace information, etc. For these settings, we recommend
    using the Jaeger client library environment variables. For example, you can set
    the service name using the `JAEGER_SERVICE_NAME` environment variable.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在能够在服务内创建跨度之前，您必须初始化跟踪器。初始化的一部分涉及根据应用程序运行的环境配置跟踪器。跟踪器需要知道服务名称、发送跟踪信息的URL等。对于这些设置，我们建议使用Jaeger客户端库的环境变量。例如，您可以使用`JAEGER_SERVICE_NAME`环境变量设置服务名称。
- en: In addition to configuring the tracer, you can integrate the tracer with your
    metrics and logging libraries as you initialize the tracer. The tracer uses the
    metrics library to emit metrics about what’s happening with the tracer, such as
    the number of traces and spans sampled, the number of successfully reported spans,
    and others. On the other hand, the tracer leverages the logging libraries to emit
    logs when it encounters errors. You can also configure the tracer to log spans,
    which is rather useful in development.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 除了配置追踪器外，你还可以在初始化追踪器时将其与你的度量和日志库集成。追踪器使用度量库来发布关于追踪器活动的度量，例如跟踪和采样的跨度数量，成功报告的跨度数量等。另一方面，当遇到错误时，追踪器利用日志库来记录日志。你还可以配置追踪器来记录跨度，这在开发中非常有用。
- en: 'To initialize a Jaeger tracer in a Go service, you would add code to your application
    similar to the following. In this case, we are using Prometheus as the metrics
    library and Go’s standard logging library:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Go 服务中初始化 Jaeger 追踪器，你需要向应用程序添加类似以下的代码。在这种情况下，我们使用 Prometheus 作为度量库和 Go 的标准日志库：
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_application_considerations_CO1-1)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_application_considerations_CO1-1)'
- en: Create a Prometheus metrics factory that Jaeger can use to emit metrics.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 Prometheus 的度量工厂，供 Jaeger 使用来发布度量。
- en: '[![2](assets/2.png)](#co_application_considerations_CO1-2)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_application_considerations_CO1-2)'
- en: Create a default Jaeger configuration with no hardcoded configuration (use environment
    variables instead).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个默认的 Jaeger 配置，不要硬编码配置（使用环境变量替代）。
- en: '[![3](assets/3.png)](#co_application_considerations_CO1-3)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_application_considerations_CO1-3)'
- en: Create a new tracer from the configuration, and provide the metrics factory
    and the Go standard library logger.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从配置创建一个新的追踪器，并提供度量工厂和 Go 标准库日志记录器。
- en: With the tracer initialized, we can start creating spans in our service.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有了初始化的追踪器，我们可以开始在我们的服务中创建跨度。
- en: Creating Spans
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建跨度
- en: 'Now that we have a tracer, we can start creating spans within our service.
    Assuming the service is somewhere in the middle of the request processing flow,
    the service needs to deserialize the incoming span information from the previous
    service and create a child span. Our example is an HTTP service, so the span context
    is propagated via HTTP headers. The following code extracts the context from the
    headers and creates a new span. Note that the tracer we initialized in the previous
    section must be in scope:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个追踪器，我们可以开始在我们的服务中创建跨度。假设服务在请求处理流程的中间位置，服务需要从前一个服务的传入跨度信息中反序列化，并创建一个子跨度。我们的示例是一个
    HTTP 服务，因此跨度上下文通过 HTTP 头部传播。以下代码从头部提取上下文信息并创建一个新的跨度。请注意，我们在前一节初始化的追踪器必须在作用域内：
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_application_considerations_CO2-1)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_application_considerations_CO2-1)'
- en: Extract the context information from the HTTP headers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从 HTTP 头部提取上下文信息。
- en: '[![2](assets/2.png)](#co_application_considerations_CO2-2)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_application_considerations_CO2-2)'
- en: Create a new span using the extracted span context.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提取的跨度上下文创建一个新的跨度。
- en: 'As the service handles the request, it can add child spans to the span we just
    created. As an example, let’s assume the service calls a function to perform a
    SQL query. We can use the following code to create a child span for the function
    and set the operation name to `listPayments`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务处理请求时，它可以为我们刚刚创建的跨度添加子跨度。例如，假设服务调用一个执行 SQL 查询的函数。我们可以使用以下代码为该函数创建一个子跨度，并将操作名称设置为
    `listPayments`：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Propagate Context
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传播上下文
- en: Up to this point, we’ve created spans within the same service or process. When
    there are other services involved in processing a request, we need to propagate
    the trace context over the wire for the service on the other end. As discussed
    in the previous section, you can use HTTP headers to propagate the context.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在同一个服务或进程内创建了跨度。当其他服务参与请求处理时，我们需要通过网络传播跟踪上下文给另一端的服务。如前文所述，你可以使用 HTTP
    头部来传播上下文。
- en: 'The OpenTracing libraries provide helper functions you can use to inject the
    context into HTTP headers. The following code shows an example that uses the Go
    standard library HTTP client to create and send the request:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: OpenTracing 库提供了一些辅助函数，你可以用来将上下文注入到 HTTP 头部中。以下代码展示了一个使用 Go 标准库 HTTP 客户端来创建和发送请求的示例：
- en: '[PRE15]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_application_considerations_CO3-1)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_application_considerations_CO3-1)'
- en: Adds a tag to mark the span as the client side of a service call.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个标签，将该跨度标记为服务调用的客户端端点。
- en: '[![2](assets/2.png)](#co_application_considerations_CO3-2)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_application_considerations_CO3-2)'
- en: Injects the span context into the request’s HTTP headers.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 将跨度上下文注入到请求的 HTTP 头中。
- en: As we’ve discussed through these sections, instrumenting an application for
    tracing involves initializing a tracer, creating spans within the service, and
    propagating the span context to other services. There is additional functionality
    that you should explore, including tagging, logging, and baggage. If the platform
    team offers tracing as a platform service, now you have an idea of what it takes
    to take advantage of it.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这些章节中讨论的那样，为了对应用程序进行跟踪，需要初始化跟踪器，在服务内创建跨度，并将跨度上下文传播到其他服务。还有其他功能需要您探索，包括标记、日志记录和行李。如果平台团队提供跟踪作为平台服务，那么现在您已经了解如何利用它了。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: There are multiple things you can do to make your applications run better in
    Kubernetes. While most require investing time and effort to implement, we find
    that they are critical to achieve production-grade outcomes for your applications.
    As you onboard applications to your platform, make sure to consider the guidance
    provided in this chapter, including injecting configuration and secrets at runtime,
    specifying resource requests and limits, exposing application health information
    using probes, and instrumenting applications with logs, metrics, and traces.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有多项措施可以使您的应用程序在 Kubernetes 中运行更加顺畅。尽管大多数需要投入时间和精力来实施，但我们发现它们对于实现应用程序的生产级成果至关重要。在将应用程序引入到您的平台时，请务必考虑本章提供的指导，包括在运行时注入配置和密钥、指定资源请求和限制、使用探针公开应用程序健康信息，以及为应用程序添加日志、度量和跟踪。
