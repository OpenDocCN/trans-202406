<html><head></head><body><section data-pdf-bookmark="Chapter 3. Databases on Kubernetes the Hard Way" data-type="chapter" epub:type="chapter"><div class="chapter" id="databases_on_kubernetes_the_hard_way">&#13;
<h1><span class="label">Chapter 3. </span>Databases on Kubernetes the Hard Way</h1>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="about" data-type="indexterm" id="idm46183198295360"/>As we discussed in <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>, Kubernetes was designed for stateless workloads. A corollary to this is that stateless workloads are what Kubernetes does best. Because of this, some have argued that you shouldn’t try to run stateful workloads on Kubernetes, and you may hear various recommendations about what you should do instead: “Use a managed service,” or “Leave data in legacy databases in your on-premises datacenter,” or perhaps even “Run your databases in the cloud, but in traditional VMs instead of containers.”</p>&#13;
<p><a contenteditable="false" data-primary="“A Case for Databases on Kubernetes from a Former Skeptic” (Bradford)" data-primary-sortas="case" data-type="indexterm" id="idm46183199285584"/><a contenteditable="false" data-primary="Bradford, Christopher, “A Case for Databases on Kubernetes from a Former Skeptic”" data-type="indexterm" id="idm46183199179216"/><a contenteditable="false" data-primary="Kurktchiev, Boris, “3 Reasons to Bring Stateful Applications to Kubernetes”" data-type="indexterm" id="idm46183199182288"/><a contenteditable="false" data-primary="“3 Reasons to Bring Stateful Applications to Kubernetes” (Kurktchiev)" data-primary-sortas="three" data-type="indexterm" id="idm46183199000288"/>While these recommendations are still viable options, one of our main goals in this book is to demonstrate that running data infrastructure in Kubernetes has become not only a viable option, but a preferred option. In his article <a href="https://oreil.ly/SjQV0">“A Case for Databases on Kubernetes from a Former Skeptic”</a>, Christopher Bradford describes his journey from being skeptical of running any stateful workload in Kubernetes, to grudging acceptance of running data infrastructure on Kubernetes for development and test workloads, to enthusiastic evangelism around deploying databases on Kubernetes in production. This journey is typical of many in the Data on Kubernetes Community (DoKC). By the middle of 2020, Boris Kurktchiev was able to cite a growing consensus that managing stateful workloads on Kubernetes had reached a point of viability, and even maturity, in his article <a href="https://oreil.ly/xtm89">“3 Reasons to Bring Stateful Applications to Kubernetes”</a>.</p>&#13;
<p>How did this change come about? Over the past several years, the Kubernetes community has shifted focus toward adding features that support the ability to manage state in a cloud native way on Kubernetes. The storage elements represent a big part of this shift we introduced in the previous chapter, including the Kubernetes PersistentVolume subsystem and the adoption of the CSI. In this chapter, we’ll complete this part of the story by looking at Kubernetes resources for building stateful applications on top of this storage foundation. We’ll focus in particular on a specific type of stateful application: data infrastructure.</p>&#13;
<section data-pdf-bookmark="The Hard Way" data-type="sect1"><div class="sect1" id="the_hard_way">&#13;
<h1>The Hard Way</h1>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="“the hard way”" data-secondary-sortas="hard" data-type="indexterm" id="idm46183198267216"/>The phrase “doing it the hard way” has come to be associated with avoiding the easy option in favor of putting in the detailed work required to accomplish a result that will have lasting significance. Throughout history, pioneers of all persuasions are well known for taking pride in having made the sacrifice of blood, sweat, and tears that made life just that little bit more bearable for the generations that follow. These elders are often heard to lament when their protégés fail to comprehend the depth of what they had to go through.</p>&#13;
<p>In the tech world, it’s no different. While new innovations such as APIs and <span class="keep-together">“no code”</span> environments have massive potential to grow a new crop of developers worldwide, a deeper understanding of the underlying technology is still required in order <span class="keep-together">to manage</span> highly available and secure systems at worldwide scale. It’s when things <span class="keep-together">go wrong</span> that this detailed knowledge proves its worth. This is why many of us who are software developers and never touch a physical server in our day jobs gain <span class="keep-together">so much</span> from building our own PC by wiring chips and boards by hand. It’s also one <span class="keep-together">of the</span> hidden benefits of serving as informal IT consultants for our friends and family.</p>&#13;
<p><a contenteditable="false" data-primary="Hightower, Kelsey" data-type="indexterm" id="idm46183198249328"/>For the Kubernetes community, of course, “the hard way” has an even more specific connotation. Google engineer Kelsey Hightower’s <a href="https://oreil.ly/xd6ne">“Kubernetes the Hard Way”</a> has become a sort of rite of passage for those who want a deeper understanding of the elements that make up a Kubernetes cluster. This popular tutorial walks you through downloading, installing, and configuring each of the components that make up the Kubernetes control plane. The result is a working Kubernetes cluster that, although not suitable for deploying a production workload, is certainly functional enough for development and learning. The appeal of the approach is that all of the instructions are typed by hand. Rather than downloading a bunch of scripts that do everything for you, you must understand what is happening at each step.</p>&#13;
<p>In this chapter, we’ll emulate this approach and walk you through deploying some example data infrastructure the hard way ourselves. Along the way, you’ll get more hands-on experience with the storage resources you learned about in <a data-type="xref" href="ch02.html#managing_data_storage_on_kubernetes">Chapter 2</a>, and we’ll introduce additional Kubernetes resource types for managing compute and network to complete the compute, network, storage triad we introduced in <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>. Are you ready to get your hands dirty? Let’s go!</p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Examples Are Not Production-Grade</h1>&#13;
<p>The examples we present in this chapter are primarily for introducing new elements of the Kubernetes API and are not intended to represent deployments we’d recommend running in production. We’ll make sure to highlight any gaps so that we can demonstrate how to fill them in upcoming chapters.</p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Prerequisites for Running Data Infrastructure on Kubernetes" data-type="sect1"><div class="sect1" id="prerequisites_for_running_data_infrastr">&#13;
<h1>Prerequisites for Running Data Infrastructure <span class="keep-together">on Kubernetes</span></h1>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="prerequisites for running infrastructure on Kubernetes" data-type="indexterm" id="idm46183198591984"/><a contenteditable="false" data-primary="data" data-secondary="infrastructure of" data-type="indexterm" id="idm46183198322080"/>To follow along with the examples in this chapter, you’ll want to have a Kubernetes cluster to work on. If you’ve never tried it before, perhaps you’ll want to build a cluster using the <a href="https://oreil.ly/sLopS">“Kubernetes the Hard Way”</a> instructions, and then use that same cluster to add data infrastructure the hard way as well. You could also use a simple desktop Kubernetes, since we won’t be using a large amount of resources. If you’re using a shared cluster, you might want to install these examples in their own Namespace to isolate them from the work of others:</p>&#13;
<pre data-type="programlisting"><strong>kubectl config set-context --current --namespace=&lt;<em>insert-namespace-name-here</em>&gt;</strong></pre>&#13;
<p><a contenteditable="false" data-primary="SC (StorageClasses)" data-type="indexterm" id="idm46183198285664"/><a contenteditable="false" data-primary="volumeBindingMode" data-type="indexterm" id="idm46183198332256"/>You’ll also need to make sure you have a StorageClass in your cluster. If you’re starting from a cluster built the hard way, you won’t have one. You may want to follow the instructions in <a data-type="xref" href="ch02.html#storageclasses">“StorageClasses”</a> for installing a simple StorageClass and provisioner that expose local storage. The <a href="https://oreil.ly/iV1Tg">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: storage.k8s.io/v1&#13;
kind: StorageClass&#13;
metadata:&#13;
name: local-path&#13;
provisioner: rancher.io/local-path&#13;
volumeBindingMode: WaitForFirstConsumer&#13;
reclaimPolicy: Delete</pre>&#13;
<p>You’ll want to use a StorageClass that supports a <a href="https://oreil.ly/rpNyc"><code>volumeBindingMode</code></a> of <code>WaitForFirstConsumer</code>. This gives Kubernetes the flexibility to defer provisioning storage until we need it. This behavior is generally preferred for production deployments, so you might as well start getting in the habit.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Running MySQL on Kubernetes" data-type="sect1"><div class="sect1" id="running_mysql_on_kubernetes">&#13;
<h1>Running MySQL on Kubernetes</h1>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="running MySQL on Kubernetes" data-type="indexterm" id="db_my"/><a contenteditable="false" data-primary="MySQL" data-secondary="running on Kubernetes" data-type="indexterm" id="my_db"/>First, let’s start with a super simple example. MySQL is one of the most widely used relational databases because of its reliability and usability. For this example, we’ll build on the <a href="https://oreil.ly/cY6cv">MySQL tutorial</a> in the official Kubernetes documentation, with a couple of twists. You can find the source code used in this section at <a href="https://oreil.ly/YfjiG">“Deploying MySQL Example—Data on Kubernetes the Hard Way”</a>. The tutorial includes two Kubernetes deployments: one to run a MySQL Pod, and another to run a sample client—in this case, WordPress. This configuration is shown in <a data-type="xref" href="#sample_kubernetes_deployment_of_mysql">Figure 3-1</a>.</p>&#13;
<figure><div class="figure" id="sample_kubernetes_deployment_of_mysql">&#13;
<img alt="Sample Kubernetes deployment of MySQL" src="assets/mcdk_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>Sample Kubernetes deployment of MySQL</h6>&#13;
</div></figure>&#13;
<p>In this example, we see that there is a PersistentVolumeClaim for each Pod. For the purposes of this example, we’ll assume these claims are satisfied by a single volume provided by the default StorageClass. You’ll also notice that each Pod is shown as part of a ReplicaSet and that there is a service exposed for the MySQL database. Let’s take a pause and introduce these concepts.</p>&#13;
<section data-pdf-bookmark="ReplicaSets" data-type="sect2"><div class="sect2" id="replicasets">&#13;
<h2>ReplicaSets</h2>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="ReplicaSets" data-type="indexterm" id="idm46183198498688"/><a contenteditable="false" data-primary="ReplicaSets" data-type="indexterm" id="idm46183198497216"/>Production application deployments on Kubernetes do not typically deploy individual Pods, because an individual Pod could easily be lost when the node disappears. Instead, Pods are typically deployed in the context of a Kubernetes resource that manages their lifecycle. ReplicaSet is one of these resources, and the other is StatefulSet, which we’ll look at later in the chapter.</p>&#13;
<p>The purpose of a <em>ReplicaSet</em> is to ensure that a specified number of replicas of a given Pod are kept running at any given time. As Pods are destroyed, others are created to replace them in order to satisfy the desired number of replicas. A ReplicaSet is defined by a Pod template, a number of replicas, and a selector. The Pod template defines a specification for Pods that will be managed by the ReplicaSet, similar to what we saw for individual Pods created in the examples in <a data-type="xref" href="ch02.html#managing_data_storage_on_kubernetes">Chapter 2</a>. The number of replicas can be zero or more. The selector identifies Pods that are part of the ReplicaSet.</p>&#13;
<p>Let’s look at a portion of an example definition of a ReplicaSet for the WordPress application shown in <a data-type="xref" href="#sample_kubernetes_deployment_of_mysql">Figure 3-1</a>:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: ReplicaSet&#13;
metadata:&#13;
  name: wordpress-mysql&#13;
  labels:&#13;
    app: wordpress&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: wordpress&#13;
      tier: mysql&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: wordpress&#13;
        tier: mysql&#13;
    spec:&#13;
      containers:&#13;
      - image: mysql:8.0&#13;
        name: mysql&#13;
        ...</pre>&#13;
<p>A ReplicaSet is responsible for creating or deleting Pods in order to meet the specified number of replicas. You can scale the size of a ReplicaSet up or down by changing this value. The Pod template is used when creating new Pods. Pods that are managed by a ReplicaSet contain a reference to the ReplicaSet in their <code>metadata.ownerReferences</code> field. A ReplicaSet can actually take responsibility for managing a Pod that it did not create if the selector matches and the Pod does not reference another owner. This behavior of a ReplicaSet is known as <em>acquiring</em> a Pod.</p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Define ReplicaSet Selectors Carefully</h1>&#13;
<p>If you do create ReplicaSets directly, make sure that the selector you use is unique and does not match any bare Pods that you do not intend to be acquired. Pods that do not match the Pod template could be acquired if the selectors match. For more information about managing the lifecycle of ReplicaSets and the Pods they manage, see the <a href="https://oreil.ly/8Bc9D">Kubernetes documentation</a>.</p>&#13;
</div>&#13;
<p class="pagebreak-before">You might be wondering why we didn’t provide a full definition of a ReplicaSet. As it turns out, most application developers do not end up using ReplicaSets directly, because Kubernetes provides another resource type that manages ReplicaSets declaratively: Deployments.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Deployments" data-type="sect2"><div class="sect2" id="deployments">&#13;
<h2>Deployments</h2>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="Deployments" data-type="indexterm" id="db_dep"/><a contenteditable="false" data-primary="Deployments" data-type="indexterm" id="dep_ab"/>A Kubernetes <em>Deployment</em> is a resource that builds on top of ReplicaSets with additional features for lifecycle management, including the ability to roll out new versions and roll back to previous versions. As shown in <a data-type="xref" href="#deployments_and_replicasets">Figure 3-2</a>, creating a Deployment results in the creation of a ReplicaSet as well.</p>&#13;
<figure><div class="figure" id="deployments_and_replicasets">&#13;
<img alt="Deployments and ReplicaSets" src="assets/mcdk_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Deployments and ReplicaSets</h6>&#13;
</div></figure>&#13;
<p>This figure highlights that ReplicaSets (and therefore the Deployments that manage them) operate on cloned replicas of Pods, meaning that the definitions of the Pods are the same, even down to the level of PersistentVolumeClaims. The definition of a ReplicaSet references a single PVC that is provided to it, and there is no mechanism provided to clone the PVC definition for additional Pods. For this reason, Deployments and ReplicaSets are not a good choice if your intent is for each Pod to have access to its own dedicated storage.</p>&#13;
<p>Deployments are a good choice if your application Pods do not need access to storage, or if your intent is that they access the same piece of storage. However, the cases where this would be desirable are pretty rare, since you likely don’t want a situation in which you could have multiple simultaneous writers to the same storage.</p>&#13;
<p class="pagebreak-before">Let’s create an example Deployment. First, create a Secret that will represent the database password (substitute in whatever string you want for the password):</p>&#13;
<pre data-type="programlisting">&#13;
<strong>kubectl create secret generic mysql-root-password \&#13;
  --from-literal=password=&lt;your password&gt;</strong></pre>&#13;
<p>Next, create a PVC that represents the storage that the database can use. The <a href="https://oreil.ly/CHccy">source code</a> is in this book’s repository. A single PVC is sufficient in this case since you are creating a single node. This should work as long as you have an appropriate StorageClass, as referenced earlier:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  name: mysql-pv-claim&#13;
  labels:&#13;
    app: wordpress&#13;
spec:&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      storage: 1Gi</pre>&#13;
<p>Next, create a Deployment with a Pod template spec that runs MySQL. The <a href="https://oreil.ly/v9TEt">source code</a> is in this book’s repository. Note that it includes a reference to the PVC you just created as well as the Secret containing the root password for the database:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: wordpress-mysql&#13;
  labels:&#13;
    app: wordpress&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: wordpress&#13;
      tier: mysql&#13;
  strategy:&#13;
    type: Recreate&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: wordpress&#13;
        tier: mysql&#13;
    spec:&#13;
      containers:&#13;
      - image: mysql:8.0&#13;
        name: mysql&#13;
        env:&#13;
        - name: MYSQL_ROOT_PASSWORD&#13;
          valueFrom:&#13;
            secretKeyRef:&#13;
              name: mysql-root-password&#13;
              key: password&#13;
        ports:&#13;
        - containerPort: 3306&#13;
          name: mysql&#13;
        volumeMounts:&#13;
        - name: mysql-persistent-storage&#13;
          mountPath: /var/lib/mysql&#13;
      volumes:&#13;
      - name: mysql-persistent-storage&#13;
        persistentVolumeClaim:&#13;
          claimName: mysql-pv-claim</pre>&#13;
<p>We have a few interesting things to note about this Deployment’s specification:</p>&#13;
<ul>&#13;
<li><p><a contenteditable="false" data-primary="Recreate" data-type="indexterm" id="idm46183198752192"/>The Deployment has a <code>Recreate</code> strategy. This refers to the way the Deployment handles the replacement of Pods when the Pod template is updated; we’ll discuss this shortly.</p></li>&#13;
<li><p>Under the Pod template, the password is passed to the Pod as an environment variable extracted from the Secret you created in this example. Overriding the default password is an important aspect of securing any database deployment.</p></li>&#13;
<li><p>A single port is exposed on the MySQL image for database access, since this is a relatively simple example. In other samples in this book, we’ll see cases of Pods that expose additional ports for administrative operations, metrics collection, and more. The fact that access is disabled by default is a great feature of Kubernetes.</p></li>&#13;
<li><p>The MySQL image mounts a volume for its persistent storage using the PVC defined in this example.</p></li>&#13;
<li><p>The number of replicas was not provided in the specification. This means that the default value of 1 will be used.</p></li>&#13;
</ul>&#13;
<p><a contenteditable="false" data-primary="kubectl get deployment command" data-type="indexterm" id="idm46183198196368"/>After applying the configuration, try using a command like <code>kubectl get deployments,rs,pods</code> to see the items that Kubernetes created for you. You’ll notice a single ReplicaSet named after the Deployment that includes a random string (for example, <code>wordpress-mysql-655c8d9c54</code>). The Pod’s name references the name of the ReplicaSet, adding some additional random characters (for example, <code>wordpress-mysql-655c8d9c54-tgswd</code>). These names provide a quick way to identify the relationships between these resources.</p>&#13;
<p>Here are a few of the actions that a Deployment takes to manage the lifecycle of ReplicaSets. In keeping with the Kubernetes emphasis on declarative operations, most of these are triggered by updating the specification of the Deployment:</p>&#13;
<dl>&#13;
<dt>Initial rollout</dt>&#13;
<dd><a contenteditable="false" data-primary="rollout" data-type="indexterm" id="idm46183198420704"/><a contenteditable="false" data-primary="initial rollout" data-type="indexterm" id="idm46183198406640"/>When you create a Deployment, Kubernetes uses the specification you provide to create a ReplicaSet. The process of creating this ReplicaSet and its Pods is known as a <em>rollout</em>. A rollout is also performed as part of a rolling update.</dd>&#13;
<dt>Scaling up or down</dt>&#13;
<dd><a contenteditable="false" data-primary="scalability" data-secondary="about" data-type="indexterm" id="idm46183198194640"/>When you update a Deployment to change the number of replicas, the underlying ReplicaSet is scaled up or down accordingly.</dd>&#13;
<dt>Rolling update</dt>&#13;
<dd><a contenteditable="false" data-primary="RollingUpdate" data-type="indexterm" id="idm46183198330384"/>When you update the Deployment’s Pod template (for example, by specifying a different container image for the Pod), Kubernetes creates a new ReplicaSet based on the new Pod template. The way that Kubernetes manages the <span class="keep-together">transition</span> between the old and new ReplicaSets is described by the Deployment’s <code>spec.strategy</code> property, which defaults to a value called <code>RollingUpdate</code>. In <span class="keep-together">a rolling</span> update, the new ReplicaSet is slowly scaled up by creating Pods <span class="keep-together">conforming to</span> the new template, as the number of Pods in the existing ReplicaSet is scaled down. During this transition, the Deployment enforces a maximum and minimum number of Pods, expressed as percentages, as set by the <code>spec.strategy.rollingupdate.maxSurge</code> and <code>maxUnavailable</code> properties. Each of these values defaults to 25%.</dd>&#13;
<dt>Recreate update</dt>&#13;
<dd>The other option for use when you update the Pod template is <code>Recreate</code>. This is the option that was set in the preceding Deployment. With this option, the existing ReplicaSet is terminated immediately before the new ReplicaSet is created. This strategy is useful for development environments since it completes the update more quickly, whereas <code>RollingUpdate</code> is more suitable for production environments since it emphasizes high availability. This is also useful for data migration.</dd>&#13;
<dt>Rollback update</dt>&#13;
<dd><p><a contenteditable="false" data-primary="rollback update" data-type="indexterm" id="idm46183198854768"/>When creating or updating a Deployment, you could introduce an error—for example, by updating a container image in a Pod with a version that contains a bug. In this case, the Pods managed by the Deployment might not even initialize fully. You can detect these types of errors using commands such as <code>kubectl</code> <span class="keep-together"><code>rollout </code></span> <code>status</code>. Kubernetes provides a series of operations for managing the history of rollouts of a Deployment. You can access these via <code>kubectl</code> commands such as <code>kubectl rollout history</code>, which provides a numbered history of rollouts for a Deployment, and <code>kubectl rollout undo</code>, which reverts a Deployment to the previous rollout. You can also <code>undo</code> to a specific rollout version with the <span class="keep-together"><code>--to-version</code></span> option. Because <code>kubectl</code> supports rollouts for other resource types we’ll cover later in this chapter (StatefulSets and DaemonSets), you’ll need to include the resource type and name when using these commands—for <span class="keep-together">example:</span></p>&#13;
<pre data-type="programlisting"><strong>kubectl rollout history deployment/wordpress-mysql</strong></pre>&#13;
<p>This produces output such as the following:</p>&#13;
<pre data-type="programlisting">deployment.apps/wordpress-mysql&#13;
REVISION  CHANGE-CAUSE&#13;
1         &lt;none&gt;</pre></dd>&#13;
</dl>&#13;
<p>As you can see, Kubernetes Deployments provide some sophisticated behaviors for managing the lifecycle of a set of cloned Pods. You can test out these lifecycle operations (other than rollback) by changing the Deployment’s YAML specification and reapplying it. Try scaling the number of replicas to 2 and back again, or using a different MySQL image. After updating the Deployment, you can use a command like <code>kubectl describe deployment</code> <code>wordpress-mysql</code> to observe the events that Kubernetes initiates to bring your Deployment to your desired state.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="db_dep" data-type="indexterm" id="idm46183198335008"/><a contenteditable="false" data-primary="" data-startref="dep_ab" data-type="indexterm" id="idm46183198173696"/>Other options are available for Deployments that we don’t have space to go into here—for example, how to specify what Kubernetes does if you attempt an update that fails. For a more in-depth explanation of the behavior of Deployments, see the <a href="https://oreil.ly/ibjpA">Kubernetes documentation</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Services" data-type="sect2"><div class="sect2" id="services">&#13;
<h2>Services</h2>&#13;
<p><a contenteditable="false" data-primary="Services" data-secondary="about" data-type="indexterm" id="idm46183198191600"/>In the preceding steps, you’ve created a PVC to specify the storage needs of the database, a Secret to provide administrator credentials, and a Deployment to manage the lifecycle of a single MySQL Pod. Now that you have a running database, you’ll want to make it accessible to applications. In our scheme of compute, network, and storage that we introduced in <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>, this is the networking part.</p>&#13;
<p>Kubernetes <em>Services</em> are the primitive that we need to use to expose access to our database as a network service. A Service provides an abstraction for a group of Pods running behind it. In the case of a single MySQL node as in this example, you might wonder why we’d bother creating this abstraction. One key feature that a Service supports is to provide a consistently named endpoint that doesn’t change. You don’t want to be in a situation of having to update your clients whenever the database Pod is restarted and gets a new IP address. You can create a Service for accessing MySQL by using a YAML configuration like this. The <a href="https://oreil.ly/FyR9E">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: wordpress-mysql&#13;
  labels:&#13;
    app: wordpress&#13;
spec:&#13;
  ports:&#13;
    - port: 3306&#13;
  selector:&#13;
    app: wordpress&#13;
    tier: mysql&#13;
  clusterIP: None</pre>&#13;
<p>Here are a couple of things to note about this configuration:</p>&#13;
<ul>&#13;
<li><p>This configuration specifies a <code>port</code> that is exposed on the Service: 3306. In defining a Service, two ports are actually involved: the <code>port</code> exposed to clients of the Service, and the <code>targetPort</code> exposed by the underlying Pods that the Service is fronting. Since you haven’t specified a <code>targetPort</code>, it defaults to the <code>port</code> value.</p></li>&#13;
<li><p>The <code>selector</code> defines what Pods the Service will direct traffic to. In this configuration, there will be only a single MySQL Pod managed by the Deployment, and that’s just fine.</p></li>&#13;
<li><p>If you have worked with Kubernetes Services before, you may note that there is no <code>serviceType</code> defined for this Service, which means that it is of the default type, known as <code>ClusterIP</code>. Furthermore, since the <code>clusterIP</code> property is set to <code>None</code>, this is what is known as a <em>headless Service</em>—that is, the Service’s DNS name is mapped directly to the IP addresses of the selected Pods.</p></li>&#13;
</ul>&#13;
<p>Kubernetes supports several types of Services to address different use cases, which are shown in <a data-type="xref" href="#kubernetes_service_types">Figure 3-3</a>. We’ll introduce them briefly here in order to highlight their applicability to data infrastructure:</p>&#13;
<dl>&#13;
<dt>ClusterIP Service</dt>&#13;
<dd><a contenteditable="false" data-primary="ClusterIP Service" data-type="indexterm" id="idm46183198166096"/>This type of Service is exposed on a cluster-internal IP address. ClusterIP Services are the type used most often for data infrastructure such as databases in Kubernetes, especially headless services, since this infrastructure is typically deployed in Kubernetes alongside the application that uses it.</dd>&#13;
<dt>NodePort Service</dt>&#13;
<dd><a contenteditable="false" data-primary="NodePort Service" data-type="indexterm" id="idm46183198148592"/>A NodePort Service is exposed externally to the cluster on the IP address of each Worker Node. A ClusterIP service is also created internally, to which the NodePort routes traffic. You can allow Kubernetes to select what external port is used from a range of ports (30000–32767 by default), or specify the one you desire by using the <code>NodePort</code> property. NodePort services are most suitable for development environments, when you need to debug what is happening on a specific instance of a data infrastructure application.</dd>&#13;
<dt>LoadBalancer</dt>&#13;
<dd><a contenteditable="false" data-primary="LoadBalancer Service" data-type="indexterm" id="idm46183198197616"/><a contenteditable="false" data-primary="EKS (Elastic Kubernetes Service)" data-type="indexterm" id="idm46183198233680"/><a contenteditable="false" data-primary="ELB (Elastic Load Balancer)" data-type="indexterm" id="idm46183198142736"/>LoadBalancer Services represent a request from the Kubernetes runtime to set up an external load balancer provided by the underlying cloud provider. For <span class="keep-together">example,</span> on Amazon’s Elastic Kubernetes Service (EKS), requesting a <span class="keep-together">LoadBalancer</span> Service causes an instance of an Elastic Load Balancer (ELB) to be created. Usage of LoadBalancers in front of multinode data infrastructure deployments is typically not required, as these data technologies often have their own approaches for distributing load. For example, Apache Cassandra drivers are aware of the topology of a Cassandra cluster and provide load-balancing features to client applications, eliminating the need for a load balancer.</dd>&#13;
<dt>ExternalName Service</dt>&#13;
<dd><a contenteditable="false" data-primary="ExternalName Service" data-type="indexterm" id="idm46183198139104"/>An ExternalName Service is typically used to represent access to a Service that is outside your cluster—for example, a database that is running externally to Kubernetes. An ExternalName Service does not have a selector, as it is not mapping to any Pods. Instead, it maps the Service name to a CNAME record. For example, if you create a <code>my-external-database</code> Service with an <code>externalName</code> of <code>database.mydomain.com</code>, references in your application Pods to <code>my-external-database</code> will be mapped to <code>database.mydomain.com</code>.</dd>&#13;
</dl>&#13;
<figure><div class="figure" id="kubernetes_service_types">&#13;
<img alt="Kubernetes Service types" src="assets/mcdk_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>Kubernetes Service types</h6>&#13;
</div></figure>&#13;
<p><a contenteditable="false" data-primary="Kubernetes Ingress" data-type="indexterm" id="idm46183198132832"/>Note also the inclusion of <em>Ingress</em> in the figure. While Kubernetes Ingress is not a type of Service, it is related. An Ingress is used to provide access to Kubernetes services from outside the cluster, typically via HTTP. Multiple Ingress implementations are available, including Nginx, Traefik, Ambassador (based on Envoy) and others. Ingress implementations typically provide features including Secure Sockets Layer (SSL) termination and load balancing, even across multiple Kubernetes Services. As with LoadBalancer Services, Ingresses are more typically used at the application tier.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Accessing MySQL" data-type="sect2"><div class="sect2" id="accessing_mysql">&#13;
<h2>Accessing MySQL</h2>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="accessing MySQL" data-type="indexterm" id="idm46183198922656"/><a contenteditable="false" data-primary="MySQL" data-secondary="accessing" data-type="indexterm" id="idm46183198920528"/>Now that you have deployed the database, you’re ready to deploy an application that uses it—the WordPress server. First, the server will need its own PVC. This helps illustrate that some applications leverage storage directly—perhaps for storing files, applications that use data infrastructure, and applications that do both. You can make a small request since this is just for demonstration purposes. The <a href="https://oreil.ly/smKtM">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  name: wp-pv-claim&#13;
  labels:&#13;
    app: wordpress&#13;
spec:&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      storage: 1Gi</pre>&#13;
<p>Next, create a Deployment for a single WordPress node. The <a href="https://oreil.ly/hLPdW">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: wordpress&#13;
  labels:&#13;
    app: wordpress&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: wordpress&#13;
      tier: frontend&#13;
  strategy:&#13;
    type: Recreate&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: wordpress&#13;
        tier: frontend&#13;
    spec:&#13;
      containers:&#13;
      - image: wordpress:4.8-apache&#13;
        name: wordpress&#13;
        env:&#13;
        - name: WORDPRESS_DB_HOST&#13;
          value: wordpress-mysql&#13;
        - name: WORDPRESS_DB_PASSWORD&#13;
          valueFrom:&#13;
            secretKeyRef:&#13;
              name: mysql-root-password&#13;
              key: password&#13;
        ports:&#13;
        - containerPort: 80&#13;
          name: wordpress&#13;
        volumeMounts:&#13;
        - name: wordpress-persistent-storage&#13;
          mountPath: /var/www/html&#13;
      volumes:&#13;
      - name: wordpress-persistent-storage&#13;
        persistentVolumeClaim:&#13;
          claimName: wp-pv-claim</pre>&#13;
<p>Notice that the database host and password for accessing MySQL are passed to WordPress as environment variables. The value of the host is the name of the Service you created for MySQL above. This is all that is needed for the database connection to be routed to your MySQL instance. The value for the password is extracted from the Secret, as with the preceding configuration of the MySQL Deployment.</p>&#13;
<p>You’ll also notice that WordPress exposes an HTTP interface at port 80, so let’s create a service to expose the WordPress server. The <a href="https://oreil.ly/tEigE">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: wordpress&#13;
  labels:&#13;
    app: wordpress&#13;
spec:&#13;
  ports:&#13;
    - port: 80&#13;
  selector:&#13;
    app: wordpress&#13;
    tier: frontend&#13;
  type: LoadBalancer</pre>&#13;
<p><a contenteditable="false" data-primary="kubectl get services command" data-type="indexterm" id="idm46183198313728"/>Note that the service is of type LoadBalancer, which should make it fairly simple to access from your local machine. Execute the command <code>kubectl get services</code> to get the LoadBalancer’s IP address; then you can open the WordPress instance in your browser with the URL <code>http://<em>&lt;ip&gt;</em></code>. Try logging in and creating some pages.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Accessing Services from Kubernetes Distributions</h1>&#13;
<p><a contenteditable="false" data-primary="NodePort Service" data-type="indexterm" id="idm46183198493696"/>The exact details of accessing Services will depend on the Kubernetes distribution you’re using and whether you’re deploying apps in production, or just testing something quickly as we’re doing here. If you’re using a desktop Kubernetes distribution, you may wish to use a NodePort Service instead of LoadBalancer for simplicity. You can also consult the documentation for instructions on accessing services, such as those provided for <a href="https://oreil.ly/euQLB">minikube</a> or <a href="https://k3d.io">k3d</a>.</p>&#13;
</div>&#13;
<p>When you’re done experimenting with your WordPress instance, clean up the resources specified in the configuration files you’ve used in the local directory using the following command, including the data stored in your PersistentVolumeClaim:</p>&#13;
<pre data-type="programlisting"><strong>kubectl delete -k ./</strong></pre>&#13;
<p><a contenteditable="false" data-primary="" data-startref="db_my" data-type="indexterm" id="idm46183198407136"/><a contenteditable="false" data-primary="" data-startref="my_db" data-type="indexterm" id="idm46183198137216"/>At this point, you might be feeling like this was relatively easy, despite our claim to be doing things “the hard way.” And in a sense, you’d be right. So far, we’ve deployed a single Node of a simple database with sane defaults that we didn’t have to spend much time configuring. Creating a single Node is, of course, fine if your application is going to store only a small amount of data. Is that all there is to deploying databases on Kubernetes? Of course not! Now that we’ve introduced a few of the basic Kubernetes resources via this simple database deployment, it’s time to step up the complexity a bit. Let’s get down to business!</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Running Apache Cassandra on Kubernetes" data-type="sect1"><div class="sect1" id="running_apache_cassandra_on_kubernetes">&#13;
<h1>Running Apache Cassandra on Kubernetes</h1>&#13;
<p><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="running on Kubernetes" data-type="indexterm" id="ac_run"/><a contenteditable="false" data-primary="databases" data-secondary="running Apache Cassandra on Kubernetes" data-type="indexterm" id="db_run"/><a contenteditable="false" data-primary="CQL (Cassandra Query Language)" data-type="indexterm" id="idm46183198276928"/>In this section, we’ll look at running a multinode database on Kubernetes using Apache Cassandra. Cassandra is a NoSQL database first developed at Facebook that became a top-level project of the Apache Software Foundation (ASF) in 2010. Cassandra is an operational database that provides a tabular data model, and its Cassandra Query Language (CQL) is similar to SQL.</p>&#13;
<p>Cassandra is a database designed for the cloud, as it scales horizontally by adding nodes, where each node is a peer. This decentralized design has been proven to have near-linear scalability. Cassandra supports high availability by storing multiple copies of data or <em>replicas</em>, including logic to distribute those replicas across multiple Datacenters and cloud regions. Cassandra is built on similar principles to Kubernetes in that it is designed to detect failures and continue operating while the system can recover to its intended state in the background. All of these features make Cassandra an excellent fit for deploying on Kubernetes.</p>&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="racks" data-type="indexterm" id="idm46183198088720"/><a contenteditable="false" data-primary="token" data-type="indexterm" id="idm46183198184528"/><a contenteditable="false" data-primary="datacenters" data-type="indexterm" id="idm46183198086624"/><a contenteditable="false" data-primary="Cassandra: The Definitive Guide" data-type="indexterm" id="idm46183198083152"/><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183198082176"/><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183198081200"/>To discuss how this deployment works, it’s helpful to understand Cassandra’s approach to distributing data from two perspectives: physical and logical. Borrowing some of the visuals from <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/cassandra-the-definitive/9781492097136"><em>Cassandra: The Definitive Guide</em></a> by Jeff Carpenter and Eben Hewitt (O’Reilly), you can see these perspectives in <a data-type="xref" href="#physical_and_logical_views_of_cassandra">Figure 3-4</a>. From a physical perspective, Cassandra nodes (not to be confused with Kubernetes Worker Nodes) are organized using <em>racks</em> and <em>Datacenters</em>. While the terms betray Cassandra’s origin during a time when on-premise datacenters were the dominant way software was deployed in the mid-2000s, they can be flexibly applied. In cloud deployments, racks often represent an availability zone, while Datacenters represent a cloud region. However these are represented, the important part is that they represent physically separate failure domains. Cassandra uses awareness of this topology to make sure that it stores replicas in multiple physical locations to maximize the availability of data in the event of failures, whether those failures are a single machine, a rack of servers, an availability zone, or an entire region.</p>&#13;
<figure><div class="figure" id="physical_and_logical_views_of_cassandra">&#13;
<img alt="Physical and logical views of Cassandra’s distributed architecture" src="assets/mcdk_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Physical and logical views of Cassandra’s distributed architecture</h6>&#13;
</div></figure>&#13;
<p>The logical view helps us understand how Cassandra determines what data will be placed on each node. Each row of data in Cassandra is identified by a primary key, which consists of one or more partition-key columns used to allocate data across nodes, as well as optional clustering columns, which can be used to organize multiple rows of data within a partition for efficient access. Each write in Cassandra (and most reads) references a specific partition by providing the partition-key values, which Cassandra hashes together to produce a <em>token</em>, which is a value between −2<sup>63</sup> and 2<sup>63</sup><sup>−1</sup>. Cassandra assigns each of its nodes responsibility for one or more token ranges (shown as a single range per node labeled with letters A–H in <a data-type="xref" href="#physical_and_logical_views_of_cassandra">Figure 3-4</a> for simplicity). The physical topology is taken into account in the assignment of token ranges in order to ensure that copies of your data are distributed across racks and datacenters.</p>&#13;
<p>Now we’re ready to consider how Cassandra maps onto Kubernetes. It’s important to consider two implications of Cassandra’s architecture:</p>&#13;
<dl>&#13;
<dt>Statefulness</dt>&#13;
<dd><a contenteditable="false" data-primary="Statefulness" data-type="indexterm" id="idm46183198264256"/>Each Cassandra node has state that it is responsible for maintaining. Cassandra has mechanisms for replacing a node by streaming data from other replicas to a new node, which means that a configuration in which nodes use local ephemeral storage is possible, at the cost of longer startup time. However, it’s more common to configure each Cassandra node to use persistent storage. In either case, each Cassandra node needs to have its own unique PersistentVolumeClaim.</dd>&#13;
<dt>Identity</dt>&#13;
<dd><a contenteditable="false" data-primary="Identity" data-type="indexterm" id="idm46183198062608"/>Although each Cassandra node is the same in terms of its code, configuration, and functionality in a fully peer-to-peer architecture, the nodes are different in terms of their actual role. Each node has an identity in terms of where it fits in the topology of Datacenters and racks, and its assigned token ranges.</dd>&#13;
</dl>&#13;
<p>These requirements for identity and an association with a specific PersistentVolumeClaim present some challenges for Deployments and ReplicaSets that they weren’t designed to handle. Starting early in Kubernetes’ existence, there was an awareness that another mechanism was needed to manage stateful workloads like Cassandra.</p>&#13;
<section data-pdf-bookmark="StatefulSets" data-type="sect2"><div class="sect2" id="statefulsets">&#13;
<h2>StatefulSets</h2>&#13;
<p><a contenteditable="false" data-primary="databases" data-secondary="StatefulSets" data-type="indexterm" id="db_ss"/><a contenteditable="false" data-primary="StatefulSets" data-secondary="about" data-type="indexterm" id="idm46183198134000"/>Kubernetes began providing a resource to manage stateful workloads with the alpha release of PetSets in the 1.3 release. This capability has matured over time and is now known as <em>StatefulSets</em> (see <a data-type="xref" href="#are_your_stateful_workloads_pets_or_cat">“Are Your Stateful Workloads Pets or Cattle?”</a>). A StatefulSet has some similarities to a ReplicaSet in that it is responsible for managing the lifecycle of a set of Pods, but the way in which it goes about this management has some significant differences. To address the needs of stateful applications, like those of Cassandra that we’ve listed, StatefulSets demonstrate the following key properties:</p>&#13;
<dl>&#13;
<dt>Stable identity for Pods</dt>&#13;
<dd><p><a contenteditable="false" data-primary="Pods" data-secondary="stable identity for" data-type="indexterm" id="idm46183198050608"/>First, StatefulSets provide a stable name and network identity for Pods. Each Pod is assigned a name based on the name of the StatefulSet, plus an ordinal number. For example, a StatefulSet called <code>cassandra</code> would have Pods named <code>cassandra-0</code>, <code>cassandra-1</code>, <code>cassandra-2</code>, and so on, as shown in <a data-type="xref" href="#sample_deployment_of_cassandra_on_kuber">Figure 3-5</a>. These are stable names, so if a Pod is lost and needs replacing, the replacement will have the same name, even if it’s started on a different Worker Node. A Pod’s name is set as its hostname, so if you create a headless service, you can actually address individual Pods as needed—for example: <code>cassandra-1.cqlservice.default.svc.cluster.local</code>. The figure also includes a seed service, which we’ll discuss in  <a data-type="xref" href="#accessing_cassandra">“Accessing Cassandra”</a>.</p>&#13;
<figure><div class="figure" id="sample_deployment_of_cassandra_on_kuber">&#13;
<img alt="Sample Deployment of Cassandra on Kubernetes with StatefulSets" src="assets/mcdk_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>Sample Deployment of Cassandra on Kubernetes with StatefulSets</h6>&#13;
</div></figure></dd>&#13;
<dt>Ordered lifecycle management</dt>&#13;
<dd><a contenteditable="false" data-primary="Pods" data-secondary="lifecycle management" data-type="indexterm" id="idm46183198039280"/><a contenteditable="false" data-primary="lifecycle management" data-secondary="about" data-type="indexterm" id="idm46183198038176"/>StatefulSets provide predictable behaviors for managing the lifecycle of Pods. When scaling up the number of Pods in a StatefulSet, new Pods are added according to the next available number, unlike ReplicaSets, where Pod name suffixes are based on universally unique identifiers (UUIDs). For example, expanding the StatefulSet in <a data-type="xref" href="#sample_deployment_of_cassandra_on_kuber">Figure 3-5</a> would cause the creation of Pods such as <code>cassandra-4</code> and <code>cassandra-5</code>. Scaling down has the reverse behavior, as the Pods with the highest ordinal numbers are deleted first. This predictability simplifies management—for example, by making it obvious which Nodes should be backed up before reducing cluster size.</dd>&#13;
<dt>Persistent disks</dt>&#13;
<dd><a contenteditable="false" data-primary="PDs (persistent disks)" data-type="indexterm" id="idm46183198408560"/><a contenteditable="false" data-primary="PVCs (PersistentVolumeClaims)" data-type="indexterm" id="idm46183198403408"/>Unlike ReplicaSets, which create a single PersistentVolumeClaim shared across all their Pods, StatefulSets create a PVC associated with each Pod. If a Pod in a StatefulSet is replaced, the replacement is bound to the PVC that has the state it is replacing. Replacement could occur because of a Pod failing or the scheduler choosing to run a Pod on another node in order to balance the load. For a database like Cassandra, this enables quick recovery when a Cassandra node is lost, as the replacement node can recover its state immediately from the associated PersistentVolume rather than needing data streamed from other replicas.</dd>&#13;
</dl>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Managing Data Replication</h1>&#13;
<p><a contenteditable="false" data-primary="data" data-secondary="managing replication of" data-type="indexterm" id="idm46183198160208"/>When planning your application deployment, make sure you consider whether data is being replicated at the data tier or the storage tier. A distributed database like Cassandra manages replication itself, storing copies of your data on multiple nodes according to the replication factor you request, typically three per Cassandra Datacenter. The storage provider you select may also offer replication. If the Kubernetes volume for each Cassandra Pod has three replicas, you could end up storing nine copies of your data. While this certainly promotes high data survivability, this might cost more than you intend.</p>&#13;
</div>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="are_your_stateful_workloads_pets_or_cat">&#13;
<h5>Are Your Stateful Workloads Pets or Cattle?</h5>&#13;
<p><a contenteditable="false" data-primary="PetSet" data-type="indexterm" id="idm46183198157920"/><a contenteditable="false" data-primary="Baker, Bill" data-type="indexterm" id="idm46183198156688"/><em>PetSet</em> might seem like an odd name for a Kubernetes resource, and it has since been replaced, but it provides interesting insights into the thought process of the Kubernetes community in supporting stateful workloads. The name PetSets is a reference to a discussion that has been active in the DevOps world since at least 2012. The original concept has been attributed to Bill Baker, formerly of Microsoft.</p>&#13;
<p>The basic idea is that there are two ways of handling servers: to treat them as pets that require care, feeding, and nurture, or to treat them as cattle, to which you don’t develop an attachment or provide a lot of individual attention. If you’re logging into a server regularly to perform maintenance activities, you’re treating it as a pet.</p>&#13;
<p>The implication is that the life of an operations engineer can be greatly improved by being able to treat more elements as cattle than as pets. With the move to modern cloud native architectures, this concept has extended from servers to VMs and containers, and even to individual microservices. We now design systems to avoid single points of failure so they can survive the loss of individual components. Architectural approaches for high availability have made technologies like Kubernetes and Cassandra successful.</p>&#13;
<p>As you can see, naming a Kubernetes resource <em>PetSets</em> carried a lot of weight and perhaps even a bit of skepticism to running stateful workloads on Kubernetes at all. In the end, PetSets helped take the care and feeding out of managing state on Kubernetes but the name change to <em>StatefulSets</em> was appropriate. Taken together, capabilities like StatefulSets, the PersistentVolume subsystem introduced in <a data-type="xref" href="ch02.html#managing_data_storage_on_kubernetes">Chapter 2</a>, and operators (coming in <a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a>) are bringing a level of automation that promises a day in the near future when we will manage data on Kubernetes like cattle.</p>&#13;
</div></aside>&#13;
<section data-pdf-bookmark="Defining StatefulSets" data-type="sect3"><div class="sect3" id="defining_statefulsets">&#13;
<h3>Defining StatefulSets</h3>&#13;
<p><a contenteditable="false" data-primary="StatefulSets" data-secondary="defining" data-type="indexterm" id="idm46183198010112"/>Now that you’ve learned a bit about StatefulSets, let’s examine how they can be used to run Cassandra. You’ll configure a simple three-node cluster the “hard way” using a Kubernetes StatefulSet to represent a single Cassandra datacenter containing a single rack. The source code used in this section is located in <a href="https://oreil.ly/yhg3w">the book’s repository</a>. This approximates the configuration shown in <a data-type="xref" href="#sample_deployment_of_cassandra_on_kuber">Figure 3-5</a>.</p>&#13;
<p>To set up a Cassandra cluster in Kubernetes, you’ll first need a headless service. This service represents the CQL Service shown in <a data-type="xref" href="#sample_deployment_of_cassandra_on_kuber">Figure 3-5</a>, providing an endpoint that clients can use to obtain addresses of all the Cassandra nodes in the StatefulSet. The <a href="https://oreil.ly/7nXxZ">source code</a> is in this book’s repository:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  labels:&#13;
    app: cassandra&#13;
  name: cassandra&#13;
spec:&#13;
  clusterIP: None&#13;
  ports:&#13;
  - port: 9042&#13;
  selector:&#13;
    app: cassandra</pre>&#13;
<p>You’ll reference this service in the definition of a StatefulSet which will manage your Cassandra nodes. The <a href="https://oreil.ly/0r6Cr">source code</a> is located in this book’s repository. Rather than applying this configuration immediately, you may want to wait until after we do some quick explanations. The configuration looks like this:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: StatefulSet&#13;
metadata:&#13;
  name: cassandra&#13;
  labels:&#13;
    app: cassandra&#13;
spec:&#13;
  serviceName: cassandra&#13;
  replicas: 3&#13;
  podManagementPolicy: OrderedReady&#13;
  updateStrategy:&#13;
    type: RollingUpdate&#13;
  selector:&#13;
    matchLabels:&#13;
      app: cassandra&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: cassandra&#13;
    spec:&#13;
      containers:&#13;
      - name: cassandra&#13;
        image: cassandra&#13;
        ports:&#13;
        - containerPort: 7000&#13;
          name: intra-node&#13;
        - containerPort: 7001&#13;
          name: tls-intra-node&#13;
        - containerPort: 7199&#13;
          name: jmx&#13;
        - containerPort: 9042&#13;
          name: cql&#13;
        lifecycle:&#13;
          preStop:&#13;
            exec:&#13;
              command:&#13;
              - /bin/sh&#13;
              - -c&#13;
              - nodetool drain&#13;
        env:&#13;
          - name: CASSANDRA_CLUSTER_NAME&#13;
            value: "cluster1"&#13;
          - name: CASSANDRA_DC&#13;
            value: "dc1"&#13;
          - name: CASSANDRA_RACK&#13;
            value: "rack1"&#13;
          - name: CASSANDRA_SEEDS&#13;
            value: "cassandra-0.cassandra.default.svc.cluster.local"&#13;
        volumeMounts:&#13;
        - name: cassandra-data&#13;
          mountPath: /var/lib/cassandra&#13;
  volumeClaimTemplates:&#13;
  - metadata:&#13;
      name: cassandra-data&#13;
    spec:&#13;
      accessModes: [ "ReadWriteOnce" ]&#13;
      storageClassName: standard&#13;
      resources:&#13;
        requests:&#13;
          storage: 1Gi</pre>&#13;
<p>This is the most complex configuration we’ve looked at together so far, so let’s simplify it by looking at one portion at a time:</p>&#13;
<dl>&#13;
<dt>StatefulSet metadata</dt>&#13;
<dd><a contenteditable="false" data-primary="metadata" data-type="indexterm" id="idm46183198019360"/><a contenteditable="false" data-primary="StatefulSets" data-secondary="metadata" data-type="indexterm" id="idm46183198018528"/>We’ve named and labeled this StatefulSet <code>cassandra</code>, and that same string will be used as the selector for Pods belonging to the StatefulSet.</dd>&#13;
<dt>Exposing StatefulSet Pods via a Service</dt>&#13;
<dd><a contenteditable="false" data-primary="Pods" data-secondary="StatefulSets" data-type="indexterm" id="idm46183198006272"/><a contenteditable="false" data-primary="Services" data-secondary="exposing StatefulSet Pods via" data-type="indexterm" id="idm46183198004112"/><a contenteditable="false" data-primary="StatefulSets" data-secondary="Pods" data-type="indexterm" id="idm46183198003344"/>The <code>spec</code> of the StatefulSet starts with a reference to the headless service you created. While <code>serviceName</code> is not a required field according to the Kubernetes specification, some Kubernetes distributions and tools such as Helm expect it to be populated and will generate warnings or errors if you fail to provide a value.</dd>&#13;
<dt>Number of replicas</dt>&#13;
<dd><a contenteditable="false" data-primary="replicas" data-type="indexterm" id="idm46183198199744"/>The <code>replicas</code> field identifies the number of Pods that should be available in this StatefulSet. The value provided (<code>3</code>) reflects the smallest Cassandra cluster that one might see in an actual production deployment, and most deployments are significantly larger, which is when Cassandra’s ability to deliver high performance and availability at scale really begin to shine through.</dd>&#13;
<dt>Lifecycle management options</dt>&#13;
<dd><a contenteditable="false" data-primary="podManagementPolicy" data-type="indexterm" id="idm46183198302272"/><a contenteditable="false" data-primary="updateStrategy" data-type="indexterm" id="idm46183198170528"/><a contenteditable="false" data-primary="lifecycle management" data-secondary="options for" data-type="indexterm" id="idm46183198167248"/>The <code>podManagementPolicy</code> and <code>updateStrategy</code> describe how Kubernetes should manage the rollout of Pods when the cluster is scaling up or down, and how updates to the Pods in the StatefulSet should be managed, respectively. We’ll examine the significance of these values in  <a data-type="xref" href="#statefulset_life_cycle_management">“StatefulSet lifecycle management”</a>.</dd>&#13;
<dt>Pod specification</dt>&#13;
<dd><p><a contenteditable="false" data-primary="template" data-type="indexterm" id="idm46183198887472"/><a contenteditable="false" data-primary="Pods" data-secondary="specification" data-type="indexterm" id="idm46183198886272"/>The next section of the StatefulSet specification is the <code>template</code> used to create each Pod that is managed by the StatefulSet. The template has several subsections. First, under <code>metadata</code>, each Pod includes a label <code>cassandra</code> that identifies it as being part of the set.</p>&#13;
<p>This template includes a single item in the <code>containers</code> field, a specification for a Cassandra container. The <code>image</code> field selects the latest version of the official Cassandra <a href="https://oreil.ly/arYaE">Docker image</a>, which at the time of writing is Cassandra 4.0. This is where we diverge with the Kubernetes StatefulSet tutorial referenced previously, which uses a custom Cassandra 3.11 image created specifically for that tutorial. Because the image we’ve chosen to use here is an official Docker image, you do not need to include registry or account information to reference it, and the name <code>cassandra</code> by itself is sufficient to identify the image that will be used.</p>&#13;
<p>Each Pod will expose <code>ports</code> for various interfaces: a <code>cql</code> port for client use, <code>intra-node</code> and <code>tls-intra-node</code> ports for communication between nodes in the Cassandra cluster, and a <code>jmx</code> port for management via the Java Management Extensions (JMX).</p>&#13;
<p><a contenteditable="false" data-primary="livenessProbe command" data-type="indexterm" id="idm46183197986256"/><a contenteditable="false" data-primary="preStop command" data-type="indexterm" id="idm46183197985280"/>The Pod specification also includes instructions that help Kubernetes manage Pod lifecycles, including a <code>livenessProbe</code> and a <code>preStop</code> command. You’ll learn how each of these are used next.</p>&#13;
<p>According to its <a href="https://oreil.ly/WuTZo">documentation</a>, the image we’re using has been constructed to provide two ways to customize Cassandra’s configuration, which is stored in the <em>cassandra.yaml</em> file within the image. One way is to override the entire contents of the <em>cassandra.yaml</em> with a file that you provide. The second is to use environment variables that the image exposes to override a subset of Cassandra configuration options that are used most frequently. Setting these values in the <code>env</code> field causes the corresponding settings in the <em>cassandra.yaml</em> file to be <span class="keep-together">updated:</span></p>&#13;
<dl>&#13;
<dt><code>CASSANDRA_CLUSTER_NAME</code></dt>&#13;
<dd><a contenteditable="false" data-primary="CASSANDRA_CLUSTER_NAME" data-type="indexterm" id="idm46183197978256"/>This setting is used to distinguish which nodes belong to a cluster. Should a Cassandra node come into contact with nodes that don’t match its cluster name, it will ignore them.</dd>&#13;
<dt><code>CASSANDRA_DC</code> and <code>CASSANDRA_RACK</code></dt>&#13;
<dd><a contenteditable="false" data-primary="CASSANDRA_RACK" data-type="indexterm" id="idm46183198051312"/><a contenteditable="false" data-primary="CASSANDRA_DC" data-type="indexterm" id="idm46183198126864"/>These settings identify the Datacenter and rack that each node will be a part of. This serves to highlight one interesting wrinkle in the way that StatefulSets expose a Pod specification. Since the template is applied to each Pod and container, there is no way to vary the configured Datacenter and rack names between Cassandra Pods. For this reason, it is typical to deploy Cassandra on Kubernetes using a StatefulSet per rack.</dd>&#13;
<dt><code>CASSANDRA_SEEDS</code></dt>&#13;
<dd><a contenteditable="false" data-primary="CASSANDRA_SEEDS" data-type="indexterm" id="idm46183198356208"/>These define well-known locations of nodes in a Cassandra cluster that new nodes can use to bootstrap themselves into the cluster. The best practice is to specify multiple seeds in case one of them happens to be down or offline when a new node is joining. However, for this initial example, it’s enough to specify the initial Cassandra replica as a seed via the DNS name <code>cassandra-0.cassandra.default.svc.cluster.local</code>. We’ll look at a more robust way of specifying seeds in <a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a> using a service, as implied by the Seed service shown in <a data-type="xref" href="#sample_deployment_of_cassandra_on_kuber">Figure 3-5</a>.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="volumeMount" data-type="indexterm" id="idm46183198184272"/>The last item in the container specification is a <code>volumeMount</code> which requesting that a PersistentVolume be mounted at the <em>/var/lib/cassandra</em> directory, which is where the Cassandra image is configured to store its datafiles. Since each Pod will need its own PersistentVolumeClaim, the name <code>cassandra-data</code> is a reference to a PersistentVolumeClaim template, which is defined next.</p></dd>&#13;
<dt>volumeClaimTemplates</dt>&#13;
<dd><a contenteditable="false" data-primary="volumeClaimTemplates" data-type="indexterm" id="idm46183198250736"/>The final piece of the StatefulSet specification is the volumeClaimTemplates. The specification must include a template definition for each name referenced in one of the preceding container specifications. In this case, the <code>cassandra-data</code> template references the <code>standard</code> StorageClass we’ve been using in these <span class="keep-together">examples.</span> Kubernetes will use this template to create a PersistentVolumeClaim of the requested size of 1 GB whenever it spins up a new Pod within this StatefulSet.</dd>&#13;
</dl>&#13;
</div></section>&#13;
<section data-pdf-bookmark="StatefulSet lifecycle management" data-type="sect3"><div class="sect3" id="statefulset_life_cycle_management">&#13;
<h3>StatefulSet lifecycle management</h3>&#13;
<p><a contenteditable="false" data-primary="lifecycle management" data-secondary="StatefulSets" data-type="indexterm" id="idm46183197962640"/><a contenteditable="false" data-primary="StatefulSets" data-secondary="lifecycle management" data-type="indexterm" id="idm46183197961424"/>Now that we’ve had a chance to discuss the components of a StatefulSet specification, you can go ahead and apply the source:</p>&#13;
<pre data-type="programlisting"><strong>kubectl apply -f cassandra-statefulset.yaml</strong></pre>&#13;
<p>As this gets applied, you can execute the following to watch as the StatefulSet spins up Cassandra Pods:</p>&#13;
<pre data-type="programlisting"><strong>kubectl get pods -w</strong></pre>&#13;
<p>Let’s describe some of the behavior you can observe from the output of this command. First, you’ll see a single Pod, <code>cassandra-0</code>. Once that Pod has progressed to <code>Ready</code> status, you’ll see the <code>cassandra-1</code> Pod, followed by <code>cassandra-2</code> after <code>cassandra-1</code> is ready. This behavior is specified by the selection of <code>podManagementPolicy</code> for the StatefulSet. Let’s explore the available options and some of the other settings that help define how Pods in a StatefulSet are managed:</p>&#13;
<dl>&#13;
<dt>Pod management policies</dt>&#13;
<dd><p><a contenteditable="false" data-primary="Pods" data-secondary="management policies" data-type="indexterm" id="idm46183197968160"/><a contenteditable="false" data-primary="podManagementPolicy" data-type="indexterm" id="idm46183197951296"/><a contenteditable="false" data-primary="OrderedReady policy" data-type="indexterm" id="idm46183197950192"/>The <code>podManagementPolicy</code> determines the timing for adding or removing Pods from a StatefulSet. The <code>OrderedReady</code> policy applied in our Cassandra example is the default. When this policy is in place and Pods are added, whether on initial creation or scaling up, Kubernetes expands the StatefulSet one Pod at a time. As each Pod is added, Kubernetes waits until the Pod reports a status of <code>Ready</code> before adding subsequent Pods. If the Pod specification contains a <code>readinessProbe</code>, Kubernetes executes the provided command iteratively to determine when the Pod is ready to receive traffic. When the probe completes successfully (i.e., with a zero return code), it moves on to creating the next Pod. For Cassandra, readiness is typically measured by the availability of the CQL port (9042), which means the node is able to respond to CQL queries.</p>&#13;
<p><a contenteditable="false" data-primary="nodetool drain command" data-type="indexterm" id="idm46183198079584"/><a contenteditable="false" data-primary="preStop command" data-type="indexterm" id="idm46183198302816"/>Similarly, when a StatefulSet is removed or scaled down, Pods are removed one at a time. As a Pod is being removed, any provided <code>preStop</code> commands for its containers are executed to give them a chance to shut down gracefully. In our current example, the <code>nodetool drain</code> command is executed to help the Cassandra node exit the cluster cleanly, assigning responsibilities for its token range(s) to other nodes. as Kubernetes waits until a Pod has been completely terminated before removing the next Pod. The command specified in the <code>livenessProbe</code> is used to determine when the Pod is alive, and when it no longer completes without error, Kubernetes can proceed to removing the next Pod. See the <a href="https://oreil.ly/SsIuO">Kubernetes documentation</a> for more information on configuring readiness and liveness probes.</p>&#13;
<p class="pagebreak-before">The other Pod management policy is <code>Parallel</code>. When this policy is in effect, Kubernetes launches or terminates multiple Pods at the same time in order to scale up or down. This has the effect of bringing your StatefulSet to the desired number of replicas more quickly, but it may also result in some stateful workloads taking longer to stabilize. For example, a database like Cassandra shuffles data between nodes when the cluster size changes in order to balance the load, and will tend to stabilize more quickly when nodes are added or removed one at a time.</p>&#13;
<p>With either policy, Kubernetes manages Pods according to the ordinal numbers, always adding Pods with the next unused ordinal numbers when scaling up, and deleting the Pods with the highest ordinal numbers when scaling down.</p></dd>&#13;
<dt>Update strategies</dt>&#13;
<dd><p><a contenteditable="false" data-primary="updateStrategy" data-type="indexterm" id="idm46183197965536"/><a contenteditable="false" data-primary="RollingUpdate" data-type="indexterm" id="idm46183197964448"/>The <code>updateStrategy</code> describes how Pods in the StatefulSet will be updated if a change is made in the Pod template specification, such as changing a container image. The default strategy is <code>RollingUpdate</code>, as selected in this example. With the other option, <code>OnDelete</code>, you must manually delete Pods in order for the new Pod template to be applied.</p>&#13;
<p>In a rolling update, Kubernetes will delete and re-create each Pod in the StatefulSet, starting with the Pod with the largest ordinal number and working toward the smallest. Pods are updated one at a time, and you can specify a number of Pods, called a <em>partition</em>, in order to perform a phased rollout or canary. Note that if you discover a bad Pod configuration during a rollout, you’ll need to update the Pod template specification to a known good state and then manually delete any Pods that were created using the bad specification. Since these Pods will not ever reach a <code>Ready</code> state, Kubernetes will not decide they are ready to replace with the good configuration.</p></dd>&#13;
</dl>&#13;
<p>Note that Kubernetes offers similar lifecycle management options for Deployments, ReplicaSets, and DaemonSets, including revision history.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="“StatefulSets: Run and Scale Stateful Applications Easily in Kubernetes”" data-primary-sortas="stateful" data-type="indexterm" id="idm46183198586480"/>We recommend getting more hands-on experience with managing StatefulSets in order to reinforce your knowledge. For example, you can monitor the creation of PersistentVolumeClaims as a StatefulSet scales up. Another thing to try: delete a StatefulSet and re-create it, verifying that the new Pods recover previously stored data from the original StatefulSet. For more ideas, you may find these guided tutorials helpful: <a href="https://oreil.ly/dOovM">“StatefulSet Basics”</a> from the Kubernetes documentation, and <a href="https://oreil.ly/TyJj2">“StatefulSet: Run and Scale Stateful Applications Easily in Kubernetes”</a> from the Kubernetes blog.</p>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>More Sophisticated Lifecycle Management for StatefulSets</h1>&#13;
<p><a contenteditable="false" data-primary="Advanced StatefulSet" data-type="indexterm" id="idm46183198120112"/>One interesting set of opinions on additional lifecycle options for StatefulSets comes from OpenKruise, a CNCF Sandbox project, which provides an <a href="https://oreil.ly/xEqYf">Advanced StatefulSet</a>. The Advanced StatefulSet adds capabilities including these:</p>&#13;
<ul>&#13;
<li><p>Parallel updates with a maximum number of unavailable Pods</p></li>&#13;
<li><p>Rolling updates with an alternate order for replacement, based on a provided prioritization policy</p></li>&#13;
<li><p>Updating Pods “in place” by restarting their containers according to an updated Pod template specification</p></li>&#13;
</ul>&#13;
<p>This Kubernetes resource is also named <code>StatefulSet</code> to facilitate its use with minimal impact to your existing configurations. You just need to change the <code>apiVersion</code>: from <code>apps/v1</code> to <code>apps.kruise.io/v1beta1</code>.</p>&#13;
</div>&#13;
&#13;
<p><a contenteditable="false" data-primary="PDBs (PodDisruptionBudgets)" data-type="indexterm" id="idm46183197916592"/>StatefulSets are extremely useful for managing stateful workloads on Kubernetes, and that’s not even counting some capabilities we didn’t address, such as affinity and anti-affinity, managing resource requests for memory and CPU, and availability constraints such as PodDisruptionBudgets (PDBs). On the other hand, you might desire capabilities that StatefulSets don’t provide, such as backup/restore of PersistentVolumes, or secure provisioning of access credentials. We’ll discuss how to leverage or build these capabilities on top of Kubernetes in <a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a> and beyond.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="statefulsets_pastcomma_presentcomma_and">&#13;
<h5>StatefulSets: Past, Present, and Future</h5>&#13;
<p><em>With Maciej Szulik, Red Hat Engineer and Kubernetes SIG Apps member</em></p>&#13;
<p><a contenteditable="false" data-primary="StatefulSets" data-secondary="past, present, and future" data-type="indexterm" id="idm46183197905632"/><a contenteditable="false" data-primary="Szulik, Maziej" data-type="indexterm" id="idm46183197911264"/><a contenteditable="false" data-primary="SIG Apps (Special Interest Group for Applications)" data-type="indexterm" id="idm46183197898496"/>The Kubernetes <a href="https://oreil.ly/uec9G">Special Interest Group for Applications (SIG Apps)</a> is responsible for development of the controllers that help manage application workloads on Kubernetes. This includes batch workloads like Jobs and CronJobs, other stateless workloads like Deployments and ReplicaSets, and of course StatefulSets for stateful workloads.</p>&#13;
<p>The StatefulSet controller has a slightly different way of working from the other controllers. When you’re thinking about Deployments, or Jobs, the controller just has to manage Pods. You don’t have to worry about the underlying data, either because that’s handled by PersistenVolumes or you’re OK with just throwing each Pod’s data away when you destroy and re-create it. But that behavior is not acceptable when you’re trying to run a database, or any kind of workload that requires the state to be persisted between runs. This results in significant additional complexity in the StatefulSet controller. The main challenge in writing and maturing Kubernetes controllers has been handling edge cases. StatefulSets are similar in this regard, but it’s even more urgent for StatefulSets to handle the failure cases correctly, so that we don’t lose data.</p>&#13;
<p>We’ve encountered some interesting use cases for StatefulSets, and some users would like to change boundaries that have been set in the core implementation. For example, we’ve had pull requests submitted to change the way StatefulSets handle Pods during an update. In the original implementation, the StatefulSet controllers update Pods one at a time, and if something breaks during the rollout, the entire rollout is paused, and the StatefulSet requires manual intervention to make sure that data is not corrupted or lost. Some users would like the StatefulSet controller to ignore issues where a Pod is stuck in a pending state or cannot run, and just restart these Pods. However, the thing to remember with StatefulSets is that protecting the underlying data is the most important priority. We could end up making the suggested change in order to allow faster updates in parallel for development environments where data protection is less of a concern, but require opting in with a feature flag.</p>&#13;
<p><a contenteditable="false" data-primary="auto-deletion" data-type="indexterm" id="idm46183197881216"/><a contenteditable="false" data-primary="KEP (Kubernetes Enhancement Proposal)" data-type="indexterm" id="idm46183197889888"/>Another frequently requested feature is the ability to auto-delete the PersistentVolumeClaims of a StatefulSet when the StatefulSet is deleted. The original behavior is to preserve the PVCs, again as a data protection mechanism, but a Kubernetes Enhancement Proposal (KEP) for <a href="https://oreil.ly/XO0fv">auto-deletion</a> was included as an alpha feature for the Kubernetes 1.23 release.</p>&#13;
<p><a contenteditable="false" data-primary="minReady Seconds setting" data-type="indexterm" id="idm46183197896816"/>Even though some significant differences exist in the way StatefulSets manage Pods versus other controllers, we are working to make the behaviors more similar across the controllers as much as possible. One example is the addition of a <a href="https://oreil.ly/6Qwsz"><code>minReadySeconds</code> setting</a> in the Pod template, which allows you to say, “I’d like this application to be unavailable for a little bit of extra time before sending traffic to it.” This is helpful for some stateful workloads that need a bit more time to initialize themselves (e.g., to warm up caches) and brings StatefulSets in line with other controllers.</p>&#13;
<p>Another example is the work that is in progress to unify status reporting across all of the application controllers. Currently, if you’re building any kind of higher-level orchestration or management tools, you need to have different behavior to handle the status of StatefulSets, Deployments, DaemonSets, and so on, because each was written by a different author. Each author had a different requirement for what should be in the status, how the resource should express information about whether it’s available, or whether it’s in a rolling update, or it’s unavailable, or whatever is happening with it. DaemonSets are especially different in how they report status.</p>&#13;
<p>Another feature in progress allows you to set a <a href="https://oreil.ly/44jlT"><code>maxUnavailable</code> number of Pods</a> for a StatefulSet. This number would be applied during the initial rollout of a StatefulSet and allow the number of replicas to be scaled up more quickly. This is another feature that brings StatefulSets into greater alignment with the way the other controllers work. The best way to understand the work that is in progress from the SIG Apps team, is to look at <a href="https://oreil.ly/Mmlp2">Kubernetes open issues</a> that are labeled <code>sig/apps</code>.</p>&#13;
<p>It can be difficult to build StatefulSets as a capability that will meet the needs of all stateful workloads; we’ve tried to build them in such a way as to consistently handle the most common requirements. We could obviously add support for more and more edge cases, but this tends to make the functionality significantly more complicated for users to grasp. There will always be users who are dissatisfied because their use case is not covered, and there’s always a balance of how much we can put in without affecting both functionality and performance.</p>&#13;
<p><a contenteditable="false" data-primary="sample controller" data-type="indexterm" id="idm46183197944704"/><a contenteditable="false" data-primary="Programming Kubernetes (Hausenblas and Schimanski)" data-type="indexterm" id="idm46183198070128"/><a contenteditable="false" data-primary="Hausenblas, Michael, Programming Kubernetes" data-type="indexterm" id="idm46183197903184"/><a contenteditable="false" data-primary="Schimanski, Stefan, Programming Kubernetes" data-type="indexterm" id="idm46183197901392"/>In most cases where users need more specific behaviors (for example, to handle edge cases), it’s because they’re trying to manage a complex application like Postgres or Cassandra. That’s where there’s a great argument for creating your own controllers and even operators to deal with those specific cases. Even though it might sound super scary, it’s really not that difficult to write your own controller. You can start reasonably quickly and get a basic controller up and running in a couple of days by using simple examples including the <a href="https://oreil.ly/NB8wk">sample controller</a>, which is part of the Kubernetes codebase and maintained by the project. <a class="orm:hideurl" href="https://oreil.ly/Ad4Ga"><em>Programming Kubernetes</em></a> by Michael Hausenblas and Stefan Schimanski (O’Reilly), also has a chapter on writing controllers. Don’t assume you’re stuck with the behavior that comes out of the box. Kubernetes is meant to be open and extensible; whether it’s networking, controllers, CSI, plug-ins, or something else you need to customize Kubernetes, you should go for it!<a contenteditable="false" data-primary="" data-startref="db_ss" data-type="indexterm" id="idm46183197900320"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Accessing Cassandra" data-type="sect2"><div class="sect2" id="accessing_cassandra">&#13;
<h2>Accessing Cassandra</h2>&#13;
<p><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="accessing" data-type="indexterm" id="idm46183198013568"/><a contenteditable="false" data-primary="databases" data-secondary="accessing Apache Cassandra" data-type="indexterm" id="idm46183197853920"/>Once you have applied the configurations we’ve listed, you can use Cassandra’s CQL shell <code>cqlsh</code> to execute CQL commands. If you happen to be a Cassandra user and have a copy of <code>cqlsh</code> installed on your local machine, you could access Cassandra as a client application would, using the CQL Service associated with the StatefulSet. However, since each Cassandra node contains <code>cqlsh</code> as well, this gives us a chance to demonstrate a different way to interact with infrastructure in Kubernetes, by connecting directly to an individual Pod in a StatefulSet:</p>&#13;
<pre data-type="programlisting"><strong>kubectl exec -it cassandra-0 -- cqlsh</strong></pre>&#13;
<p>This should bring up the <code>cqlsh</code> prompt, and you can then explore the contents of Cassandra’s built-in tables using <code>DESCRIBE KEYSPACES</code> and then <code>USE</code> to select a particular keyspace and run <code>DESCRIBE TABLES</code>. Many Cassandra tutorials available online can guide you through more examples of creating your own tables, inserting and querying data, and more. When you’re done experimenting with <code>cqlsh</code>, you can type <code>exit</code> to exit the shell.</p>&#13;
<p>Removing a StatefulSet is the same as any other Kubernetes resource—you can delete it by name, for example:</p>&#13;
<pre data-type="programlisting"><strong>kubectl delete sts cassandra</strong></pre>&#13;
<p>You could also delete the StatefulSet referencing the file used to create it:</p>&#13;
<pre data-type="programlisting"><strong>kubectl delete -f cassandra-statefulset.yaml</strong></pre>&#13;
<p>When you delete a StatefulSet with a policy of <code>Retain</code> as in this example, the PersistentVolumeClaims it creates are not deleted. If you re-create the StatefulSet, it will bind to the same PVCs and reuse the existing data. When you no longer need the claims, you’ll need to delete them manually. The final cleanup from this exercise you’ll want to perform is to delete the CQL Service:</p>&#13;
<pre data-type="programlisting"><strong>kubectl delete service cassandra</strong></pre>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="what_about_daemonsetsquestion_mark">&#13;
<h5>What About DaemonSets?</h5>&#13;
<p><a contenteditable="false" data-primary="DaemonSets" data-type="indexterm" id="idm46183199058288"/><a contenteditable="false" data-primary="databases" data-secondary="DaemonSets" data-type="indexterm" id="idm46183197842864"/><a contenteditable="false" data-primary="taints and tolerations" data-type="indexterm" id="idm46183197841648"/><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183197840672"/><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183197839696"/>If you’re familiar with the resources Kubernetes offers for managing workloads, you may have noticed that we haven’t yet mentioned <a href="https://oreil.ly/487vb"><em>DaemonSets</em></a>. These allow you to request that a Pod be run on each Worker Node in a Kubernetes cluster, as shown in <a data-type="xref" href="#daemon_sets_run_a_single_pod_on_selecte">Figure 3-6</a>.</p> &#13;
&#13;
&#13;
<figure><div class="figure" id="daemon_sets_run_a_single_pod_on_selecte">&#13;
<img alt="Daemon Sets run a single Pod on selected Worker Nodes" src="assets/mcdk_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>DaemonSets run a single Pod on selected Worker Nodes</h6>&#13;
</div></figure>&#13;
&#13;
  <p>Instead of specifying a number of replicas, a DaemonSet scales up or down as Worker Nodes are added or removed from the cluster. By default, a DaemonSet will run your Pod on each Worker Node, but you can use <a href="https://oreil.ly/kLM6t">taints and tolerations</a> to override this behavior (for example, limiting some Worker Nodes). DaemonSets support rolling updates in a similar way to StatefulSets.</p>&#13;
&#13;
<p>On the surface, DaemonSets might sound useful for running databases or other data infrastructure, but this does not seem to be a widespread practice. Instead, DaemonSets are most frequently used for functionality related to Worker Nodes and their relationship to the underlying Kubernetes provider. For example, many of the CSI implementations that we saw in <a data-type="xref" href="ch02.html#managing_data_storage_on_kubernetes">Chapter 2</a> use DaemonSets to run a storage driver on each Worker Node. Another common usage is to run Pods that perform monitoring tasks on Worker Nodes, such as log and metrics collectors<a contenteditable="false" data-primary="" data-startref="ac_run" data-type="indexterm" id="idm46183197855536"/><a contenteditable="false" data-primary="" data-startref="db_run" data-type="indexterm" id="idm46183198022512"/>.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000002">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, you’ve learned how to deploy both single-node and multinode distributed databases on Kubernetes with hands-on examples. Along the way, you’ve gained familiarity with Kubernetes resources such as Deployments, ReplicaSets, StatefulSets, and DaemonSets, and learned about the best use cases for each:</p>&#13;
<ul>&#13;
<li><p>Use Deployments/ReplicaSets to manage stateless workloads or simple stateful workloads like single-node databases or caches that can rely on ephemeral <span class="keep-together">storage.</span></p></li>&#13;
<li><p>Use StatefulSets to manage stateful workloads that involve multiple nodes and require association with specific storage locations.</p></li>&#13;
<li><p>Use DaemonSets to manage workloads that leverage specific Worker Node <span class="keep-together">functionality.</span></p></li>&#13;
</ul>&#13;
<p>You’ve also learned the limits of what each of these resources can provide. Now that you’ve gained experience in deploying stateful workloads on Kubernetes, the next step is to learn how to automate the so-called “day two” operations involved in keeping this data infrastructure running.</p>&#13;
</div></section>&#13;
</div></section></body></html>